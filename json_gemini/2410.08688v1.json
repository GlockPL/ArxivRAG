{"title": "Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot Step-by-Step Universal Image Restorers", "authors": ["Jin Cao", "Deyu Meng", "Xiangyong Cao"], "abstract": "Despite previous works typically targeting isolated degradation types, recent research has increasingly focused on addressing composite degradations which involve a complex interplay of multiple different isolated degradations. Recognizing the challenges posed by the exponential number of possible degradation combinations, we propose Universal Image Restoration (UIR), a new task setting that requires models to be trained on a set of degradation bases and then remove any degradation that these bases can potentially compose in a zero-shot manner. Inspired by the Chain-of-Thought which prompts LLMs to address problems step-by-step, we propose the Chain-of-Restoration (CoR), which instructs models to step-by-step remove unknown composite degradations. By integrating a simple Degradation Discriminator into pre-trained multi-task models, CoR facilitates the process where models remove one degradation basis per step, continuing this process until the image is fully restored from the unknown composite degradation. Extensive experiments show that CoR significantly improves model performance in removing composite degradations, achieving results comparable to or surpassing those of State-of-The-Art (SoTA) methods trained on all degradations. The code will be released at this url.", "sections": [{"title": "1. Introduction", "content": "Image restoration plays a crucial role in the retrieval of high-fidelity imagery from corrupted sources and is extensively applied across various fields, including autonomous navigation, medical imaging, and surveillance systems. Considerable progress has been made in addressing isolated degradation scenarios by a One-to-One single-task model (Fig. 2a) [6, 10, 11, 13, 23, 37, 44, 60, 66, 67], such as low-light conditions [24, 40, 62, 63, 74], haze [22, 27, 51, 54, 72], rain [9, 18, 20, 31, 42], noise [16, 33, 55, 69, 70] and snow [7, 8, 12, 52, 58]. Despite their remarkable achievements in specific contexts, real-world conditions often involve unpredictable and variable degradations, posing significant challenges to single-task methods [36].\nTo overcome the limitations of One-to-One methods,"}, {"title": "2. Related Works", "content": "Image Restoration for Composite Degradations. While One-to-One and One-to-Many models address specific degradations, recent research has shifted focus towards composite degradation removal. OneRestore [25] pioneers an unified model leveraging visual and textual embeddings to confront the challenge of composite degradation removal. Nonetheless, it requires comprehensive training data across all degradation types, resulting in high costs and being limited by the model's capacity. Uniprocessor [19] and RestoreAgent [5] both identify step-by-step removal of composite degradation as feasible for their specialized models. However, [19] only mentions this without further investigation. Moreover, [5] necessitates individual model training for each degradation type and intensity, followed by extra fine-tuning of a large vision-text model for every possible sequence permutation of isolated degradations, rendering the approach prohibitively expensive and impractical. In contrast, our CoR, with a single pre-trained multi-task model, necessitates merely a simple and cheap Degradation Discriminator for step-by-step composite degradation removal, proving its simplicity and efficacy, and demonstrating that any single multi-task model possesses the generalization capability to remove composite degradations in a zero-shot manner. Considering the constraints of previous task settings involving composite degradation, we also introduce Universal Image Restoration (UIR), a task setting where models are trained on a set of degradation bases and evaluated on their ability"}, {"title": "3. Method", "content": "3.1. Chain of Restoration\nAs previously introduced, we observed that multi-task models trained on multiple degradations typically address only one degradation when faced with an image containing composite degradations. Our proposed Chain of Restoration (CoR) capitalizes on this behavior. For an input image $X_0$ with composite degradation comprising $T$ components and a multi-task model $M$ trained on these components, CoR operates step-by-step as follows:\n$type = Degradation Discriminator(X_{i-1})$, \n$X_i = M(X_{i-1}, type), i = 1,2,\u2026\u2026,T$ \nHere, type represents the degradation type identified from $X_{i-1}$ by the Degradation Discriminator. Ideally, the model remove one degradation per step, and after $T$ steps, outputs a clearly restored image $X_T$ with the degradations removed.\n3.2. Degradation Basis\nThe essence of CoR is the progressive elimination of composite degradations by addressing their individual components."}, {"title": "3.3. Degradation Discriminator", "content": "To ascertain the degradation state of an input image, a Degradation Discriminator (DD) is essential. Specifically, with a multi-task image restoration model pre-trained on degradation bases $b_1, b_2,..., b_n$, the DD operates in two scenarios:\n(1) The given model is blind. In the case of a blind multi-task image restoration model, (like All-in-One models), DD serves to only identify if the input image is clean or degraded. This DD is trained on clean images as well as those with any degradation among $b_1, b_2,..., b_n$, effectively acting as a binary classifier. Specifically, if DD outputs \"clean\", the iterative process of CoR will stop and return the restored image, otherwise it continues.\n(2) The given model is non-blind. For non-blind multi-task image restoration models, (like multi-head/tail methods), the DD must recognize the specific type of degradation present. So this DD is a multiple classifier, designed to differentiate between clean images and those with one of degradations bases $b_1, b_2,..., b_n$. Two challenges persist: (I) The identified degradation type may not be unique, e.g., an image with \"haze+rain\" could correctly be labeled as \"rain,\" \"haze,\" or \"haze+rain.\" We aim to address higher-order degradations first to optimize performance. (II) The restoration sequence of the bases is not predetermined or controllable, which can impact the outcome. To address these issues, pre-defined hyperparameters soft margins $\u03f5_o$ and $\u03f5_b$, are introduced. Given an input image $X$ with an unknown degradation, the DD generates a probabilistic vector $v \u2208 R^{n+1}$:\n$v = Softmax(DD(X))$\n(5)\nThe $i$-th element $v_i$ represents the probability of selecting the degradation basis $b_i$ in this step, while the $(n + 1)$-th element corresponds to the probability of the image being in a \"clean\" state. Given that the order of $b_i$ is $o_i$, we can derive the revised probabilistic vector $v' \u2208 R^n$ as follows:\n$V'_i = V_i + O_i * \u03f5_o + \u03f5_{bi}$\n(6)\nHere, $\u03f5_o$ represents the soft margin for degradation order, a positive value that favors the selection of higher-order bases. Meanwhile, $\u03f5_{bi}$ is the soft margin for the degradation basis $b_i$, used to give preference to specific degradation bases. The final choice of basis $b_t$ is determined by:\n$t = arg \\underset{t}{max} V'_t$\n(7)"}, {"title": "3.4. Method Complexity", "content": "Given $n$ isolated degradations $S_1, S_2,..., S_n$ and their corresponding combined degradations ${S_{k_1} + S_{k_2}+...+ S_{k_m} | 1 < k_1 < k_2 < ... < k_m \u2264 n, 1 < m < n}$, we first define $\u03a6_n(k)$ as follows:\n$\u03a6_n(k) = \u2211_{t=1}^{k} C_n^t$, where $C_n^t = \\frac{n!}{t!(n - t)!}, 1 \u2264 k \u2264 n.$\n(8)\nHere, $\u03a6_n(k)$ describes the number of bases a k-order model (trained on all given degradations of order \u2264 k) will be trained to, given n isolated degradations. Assuming each degradation has N training pairs, a k-order model will be trained on $\u03a6_n(k) \u00d7 N$ images for each epoch. We then define the training ratio $TR_n(k)$ as:\n$TR_n(k) = \\frac{\u03a6_n(k)}{\u03a6_n(1)}, 1 \u2264 k \u2264 n.$\n(9)\nHere, $TR_n(k)$ indicates the factor by which the training time for a k-order model exceeds that for a corresponding 1-order model, per training epoch. Although $TR_n(k)$ can't simplify to a concise formula, we know the following: (1) $TR_n(1) = 1$. (2) $TR_n(n) = 2^{n-1}$. (3) $TR_n(k + 1) > TR_n(k)$.\nNext, we consider the inference time. We start by defining $\u03c6_n(k)$ as follows:\n$\u03c6_n(k) = \u2211_{t=1}^n \\lceil \\frac{n}{k} \\rceil, 1 \u2264 k \u2264 n,$\n(10)\nwhere $\\lceil * \\rceil$ denotes the ceiling function. Similarly, we define the inference ratio $IR_n(k)$ as follows:\n$IR_n(k) = \\frac{\u03c6_n(k)}{\u03c6_n(n)} = \\frac{\u03c6_n(k)}{2^{n-1}}, 1 \u2264 k \u2264 n.$\n(11)\nAssuming all degradations have the same probability of occurrence and the model always prioritizes bases with higher orders, on average, the inference time of a k-order model is $IR_n(k)$ times that of an n-order model (i.e., the end-to-end model like [25]). Although $IR_n(k)$ does not have a simple expression, we know that: (1) $IR_n(1) = \\frac{n}{2^{n-1}} \u2248 0$. (2) $IR_n(n) = 1$. (3) $IR_n(k + 1) < IR_n(k)$.\nAnalysis. Eq. (11) and Eq. (9) show that as the order k of a model increases, it tends to gain more training time and less inference time, as shown in Fig. 4. It can be seen that when k increases, $TR_n(k)$ can grow at an exponential rate and $IR_n(k)$ decreases from fast to slow. Still, the addition of inference time when k = 1 is acceptable and the addition of training time when k = n is not acceptable. What's more, a higher order doesn't mean better performance since the capacity of the models is always limited; too much training data can impair the model's overall performance. Considering these, it's recommended to use a relatively low order model when using CoR."}, {"title": "4. Experiment", "content": "4.1. Experiment Settings\nImplementation Details Most results and pre-trained models in this paper are directly from previous works [3, 25, 35, 49]. All the re-trained models (including the image classifiers) are trained with a batch size of 32 (64 for the classifiers) on 8 NVIDIA GeForce RTX 3090 Ti GPUs. The network optimization is guided by an $L_1$ loss function, employing the AdamW optimizer [41] with parameters $\u03b2_1 = 0.9$ and $B_2 = 0.999$. The learning rate is set to 2e \u2013 4 (2\u0435 3 for the classifiers). To enhance the training data, input patches of size 128 x 128 are utilized, with random horizontal and vertical flips applied to the images to augment the dataset. All the classifiers in this paper are MobileNetV3 small [29].\n4.1.1 Datasets\nThe Synthesised Dataset UIRD-12. We propose a new dataset for UIR, i.e. Universal Image Restoration Dataset-12 (UIRD-12). The training set of UIRD-12 closely follows previous All-in-One works [35, 49]: BSD400 [1] and WED [43] datasets for training on Gaussian denoising (\u03c3 = {15, 25, 50}); Rain100L dataset [64] for derain; SOTS dataset [34] for dehaze. For the test set, we use BSD68 [45], Urban100 [30], Rain100L [64], SOTS [34] to synthesise 12 categories of image degradations and their clear counterparts. These degradations include n1, n2, n5, r, h, h+r, h+n1, h+n5, r+n1, r+n5, h+r+n1, h+r+n5. (n1: noise(\u03c3 = 15), n2: noise(\u03c3 = 25), n5: noise(\u03c3 = 50), r: rain, h: haze.) Each category contains 100 images. All models are trained on the 5 isolated degradations and tested on the 12 degradations.\nDatasets Settings. To better verify the effect of CoR, we utilize not only the UIRD-12 dataset in our experiments but also the CDD-11 dataset [25], which encompasses 11 degradation types including l, h, r, s, l+h, l+r, l+s, h+r, h+s, l+h+r, and l+h+s. (l: low-light, s: snow, r: rain, h: haze.) Unlike UIRD-12, CDD-11 contains all the corresponding training and testing data for each degradation type. Specifically, 1-order models are trained only on 1-order degradations (i.e. l, h, r, s), 2-order models are trained on all degradations except l+h+r and l+h+s, and 3-order models are end-to-end models trained on all degradations.\nCompared Methods and Evaluation Metrics. To validate the effectiveness of our proposed CoR, we experiment it with a range of methods. In the experiment on UIRD-12, we primarily select 4 One-to-Many methods including AirNet [35], PromptIR [49], InstructIR [14], HAIR [3], and a One-to-Composite method OneRestore [25]. We combine these methods with CoR to demonstrate its effectiveness. In the experiment on CDD-11, we compare low-order methods integrated with CoR against end-to-end methods, comprising 9 One-to-One image restoration methods (MIRNet [65], MPRNet [66], MIRNetv2 [68], Restormer [67], DGUNet [46], NAFNet [6], SRUDC [53], Fourmer [73], OKNet [15]) and 6 One-to-Many image restoration methods (AirNet [35], Trans Weather [56], WeatherDiff [48], PromptIR [49], WG-WSNet [75], HAIR [3], and the One-to-Composite method OneRestore [25]). Notably, some methods are trained on different orders. Additionally, we employ the Peak Signal-to-"}, {"title": "4.2. Results", "content": "Results on UIRD-12 The results of all the methods on UIRD-12 are presented in Tab. 1 and Fig. 6. It's obvious that our proposed CoR can significantly improve the performance of these 1-order multi-task models on composite degradation removal with only the addition of a simple Degradation Discriminator, while having nearly no impact on isolated degradation removal. What's more, we find that non-blind models gain obviously more increment from CoR than blind models. This is because non-blind methods know the exact degradation type they are processing, thus alleviating the Degradation Coupling issue described in Sec. 5. The results show the effectiveness and feasibility of CoR.\nResults on CDD-11 The results of all the methods on CDD-11 are presented in Tab. 2 and Fig. 7. It can be seen that with CoR, models with low orders can achieve comparable or even superior performance to models trained on all degradations. Notably, the 2-order OneRestore with CoR achieves comparable performance with the 3-order OneRestore, and the 2-order HAIR with CoR surpasses the 3-order HAIR in both PSNR and SSIM, demonstrating the effectiveness of CoR. As previously discussed, due to limited capacity, training the model on all degradations can lead to decreased performance on each degradation, which is why 2-order methods with CoR can perform better with less training. However, as shown in Fig. 7, 1-order HAIR with CoR fail to achieve satisfactory results in composite degradations with low-light due to the Degradation Coupling described in Sec. 5. Additionally, we provide a training time comparison in Tab. 3. It is evident that models with lower orders require less time and thus have lower training costs to converge, not only due to the reduced time cost per epoch but also because less data requires fewer epochs to converge.\nVisual Results We present visual results in Fig. 5 and Fig. 8. Fig. 5 illustrates how CoR assists multi-task models in step-by-step removal of composite degradations. It is evident that each step typically addresses only one degradation basis that the models are trained on, which is a common"}, {"title": "4.3. Ablation Study", "content": "Ablation Study of Degradation Discriminator. As shown in Tab. 4, we investigate the influence of the proposed Degradation Discriminator across five distinct settings: (a) Without the Degradation Discriminator, the model randomly selects a degradation basis or halts the restoration upon receiving an input. (b) A pristine classifier is employed each time to determine the degradation basis or to decide when to stop the restoration process. (c) The model incorporates $\u03f5_0$. (d) The model incorporates $\u03f5_{b_i}$. (e) The model utilizes both $\u03f5_o$ and $\u03f5_b$. The significance of $\u03f5_o$ is evident in its ability to prioritize degradation bases of higher orders. Similarly, $\u03f5_{b_i}$ underscores the value of selecting an appropriate restoration sequence, as illustrated in Fig. 9. Collectively, these findings validate the rationale behind our design of the Degradation Discriminator.\nEffect of Bases. As discussed in Sec. 3, a fundamental assumption of CoR is that the composite degradations encountered must be directly combinable from the bases on"}, {"title": "5. Limitation & Future Prospect", "content": "Before delving into the limitations, it is crucial to define a key concept: Degradation Coupling. Consider input images $x \u223c p(x|s_1, s_2,\u2026, s_n)$, where $p(x|s_1, s_2,\u2026, s_n)$ represents the distribution of images subjected to composite degradations $s_1 + s_2 + \u00b7\u00b7\u00b7 + s_n$. Let M be a model trained on degradation bases $b_1, b_2, \u2026, b_m$. Suppose that $s_1 + s_2 + \u00b7\u00b7\u00b7 + s_n$ can be expressed as $b_{k_1} + b_{k_2}+...+b_{k_t}$, and the model M sequentially removes these degradations starting with $b_{k_1}$. If the following condition holds:\n$p(x|M[b_{k_1}], b_{k_2},\u2026, b_{k_t}) \u2260 p(x|b_{k_2},\u2026, b_{k_t})$\n(12)\nwe term this phenomenon Degradation Coupling. Here, $M[b_{k_1}]$ indicates that the model M has been applied to eliminate the degradation $b_{k_1}$ from x. In essence, Degradation Coupling occurs when the model's removal of one degradation inadvertently affects other unintended degradations. This issue arises because the model wasn't trained to handle the scenario where $p(x|M[b_{k_1}], b_{k_2},\u2026, b_{k_t})$. For example, as depicted in Fig. 9, when the model first attempts to remove low-light conditions, it inadvertently enhances other degradations like snow and haze, which it hasn't learned to address. Conversely, if the model first removes snow, which affects other degradations less, it can then more effectively restore the image. Tables 1 and 2 show CoR performs better with non-blind methods, which can control the removal of one degradation at a time, thus reducing Degradation Coupling. Therefore, CoR is more suitable for non-blind methods.\nGiven the inherent interdependencies among degradations in composite degradations, Degradation Coupling is an unavoidable issue. While adjusting the restoration sequence may offer some relief, its effectiveness is limited. The direction of future works should be to develop strategies that (1) enable the model to clearly remove the targeted degradation $b_i$, and (2) minimize the impact on other degradations. We look forward to future work that can develop algorithms to more effectively address this limitation."}, {"title": "6. Conclusion", "content": "In this paper, we introduce a new task setting called Universal Image Restoration (UIR) to address the limitations of previous settings. UIR challenges the model to be trained on a set of degradation bases and then tested on images with isolated or composite degradations in a zero-shot manner. To meet this challenge, we propose the first algorithm for UIR, known as Chain-of-Restoration (CoR). CoR enhances a pre-trained multi-task model with a Degradation Discriminator, enabling it to remove one degradation basis at a time and restore degraded images step by step. Our extensive experiments indicate that CoR significantly boosts model performance in removing composite degradations, rivaling or even outperforming end-to-end methods trained on all degradations, as demonstrated by both quantitative and visual results. Lastly, we discuss the limitations and future prospects of CoR. We are confident that this work will offer new insights to the research community."}]}