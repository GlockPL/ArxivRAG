{"title": "LEARNING WITH INSTANCE-DEPENDENT NOISY LABELS BY ANCHOR HALLUCINATION AND HARD SAMPLE LABEL CORRECTION", "authors": ["Po-Hsuan Huang", "Chia-Ching Lin", "Chih-Fan Hsu", "Ming-Ching Chang", "Wei-Chao Chen"], "abstract": "Learning from noisy-labeled data is crucial for real-world applications. Traditional Noisy-Label Learning (NLL) methods categorize training data into clean and noisy sets based on the loss distribution of training samples. However, they often neglect that clean samples, especially those with intricate visual patterns, may also yield substantial losses. This oversight is particularly significant in datasets with Instance-Dependent Noise (IDN), where mislabeling probabilities correlate with visual appearance. Our approach explicitly distinguishes between clean vs. noisy and easy vs. hard samples. We identify training samples with small losses, assuming they have simple patterns and correct labels. Utilizing these easy samples, we hallucinate multiple anchors to select hard samples for label correction. Corrected hard samples, along with the easy samples, are used as labeled data in subsequent semi-supervised training. Experiments on synthetic and real-world IDN datasets demonstrate the superior performance of our method over other state-of-the-art NLL methods.", "sections": [{"title": "1. INTRODUCTION", "content": "The success of Deep Neural Networks (DNNs) heavily relies on extensively annotated datasets. However, data annotation is often costly and inevitably comes with label noise [1, 2]. The correction of label errors and the exploration of robust representations have become focal points in recent research [3]. In this paper, we consider practical real-world scenarios of training an image classification model from a dataset with noisy labels [2], where the probability of mislabeling each image is contingent on its visual appearance, characterized as Instance-Dependent Noise (IDN).\nTraditional Noisy-Label Learning (NLL) methods primarily rely on sample selection [4, 5]. These methods identify correctly-labeled (i.e., clean) samples in the training set by employing the small-loss criterion during an initial training. Samples with small classification loss are assumed to have correct labels, and samples with large loss have potentially"}, {"title": "2. RELATED WORKS", "content": "Noisy-Label Learning (NLL). In NLL literature, Instance-Independent Noise (IIN) stands out as the most prevalent type of label noise. IIN is characterized by the probability of an image being mislabeled, which depends solely on the involved class pair, regardless of its visual content. Notable examples of IIN include symmetric and asymmetric noise patterns proposed in [10], which have gained widespread adoption in related fields. Various NLL approaches are developed based on this noise assumption, encompassing the design of noise-robust loss functions [11], loss correction [12], label correction [13], and sample selection [4].\nRecent prominent studies regarding IIN combine sample selection with Semi-Supervised Learning (SSL), yielding notable progress [4, 5]. The majority of methods resort to the small-loss criterion and consider samples with small training loss as clean. Subsequently, an off-the-shelf SSL algorithm, such as [14], is applied, treating the selected samples as labeled data and the remainder as unlabeled. However, these approaches often overfit to a small training subset of easy samples chosen based on the small-loss criterion [8]. This limitation hampers their ability to fully exploit critical labeling information contained in hard samples near the decision boundary. Despite their success on various IIN benchmarks, performance under the IDN assumption remains unclear.\nLearning from Instance-Dependent Noise (IDN) Data. In contrast to the naive assumption of IIN, recent works [8, 15, 16, 2, 17, 18, 9] contend that real-world noise patterns are more likely to depend on visual content, prompting a shift towards addressing IDN. Some methods combat IDN by estimating the noise transition matrix [18, 19], requiring additional information and achieving mediocre performance on real-world data. Others adopt a selection-based method combined with SSL, similar to previous works for IIN [9], and have reached SoTA results on several IDN benchmarks. However, the effective utilization of valuable hard samples with potentially noisy labels remain an unsolved challenge. Our work aligns with this research line, focusing on leveraging robust information from the easy samples to: (1) identify hard"}, {"title": "3. THE PROPOSED METHOD", "content": "This paper addresses the noisy label learning of image classification for data with Instance-Dependent Noise (IDN). We work with a noisy training set $D = \\{(x_n, \\tilde{y}_n)\\}$, where $x_n$ denotes the n-th image, and $\\tilde{y}_n \\in \\{1,2,..., C\\}$ is the corresponding label for $C$ classes. The given label $\\tilde{y}_n$ may differ from the real ground truth label $y_n$, which remains unobservable during training. The goal is to train an image classification model on $D$ that performs well on a clean test set.\nOur approach operates by distinguishing between easy and hard training samples, using the cleanly-labeled easy samples to identify hard samples for label correction. This process is achieved through anchor hallucination, where features are synthesized from the easy samples to create anchors. These anchors are then employed to select hard samples via majority voting for label correction. Overall, our model is structured with a classification module, comprising a feature extractor $f_e$ and a linear classifier $g_p$, along with an anchor hallucinator $h_p$.\nOur training framework comprises two iterative phases, as in Fig. 2. First, in the classification phase, we keep $h_p$ fixed and optimize $f_e$ and $g_p$ via semi-supervised training (\u00a7 3.4) using clean-labeled easy sample and label-corrected hard samples. The pipeline comprises three steps: easy sample selection (\u00a7 3.1), hard anchor hallucination (\u00a7 3.2), and hard sample selection (\u00a7 3.3). Secondly, in the hallucinator training phase, $f_e$ and $g_p$ remain fixed, and only $h_p$ undergoes updates based on a hallucination loss outlined in \u00a7 3.2. Further details regarding the steps and losses are elaborated in the subsequent sections."}, {"title": "3.1. Easy sample selection", "content": "Our method starts with selecting easy samples using small-loss criterion [4]. Drawing on the insight that DNNs tend to learn simple patterns faster than complex ones [6], we identify easy samples by analyzing the distribution of classification losses during initial training. Specifically, we compute the cross-entropy loss for each sample $(x_n, \\tilde{y}_n) \\in D$ and fit the loss distribution across all training samples using a two-component Gaussian Mixture Model (GMM), which provides greater flexibility in capturing sharpness of the fitted distribution [4]. The Gaussian component with the smaller mean represents the distribution of smaller losses. The probability of each sample belonging to this Gaussian component is then calculated as the easiness score $w_n$ for that sample.\nSubsequently, we select a fixed fraction of samples with the top-$P\\%$ easiness scores to form the set of easy training samples. The hyperparameter $P$ defaults to 60 and is fine-tuned using a small clean validation set. To maintain balance across all classes, we ensure a sufficient number of easy samples for each class. Let $N_j$ denote the total number of samples of the $j$-th class in the training set $D$. We control the number of easy samples $M_j$ for the $j$-th class to be:\n$M_j = \\text{min}(\\left[ \\frac{|D| \\times P\\%}{C} \\right], N_j)$ (1)\nWe thus obtain class-balanced easy training samples with assumed clean labels. The remaining samples with potentially noisy labels are regarded as hard samples. We next utilize the feature extractor $f_e$ to embed both easy and hard samples into a $d$-dimensional feature space, resulting in the easy feature set $S_e$ and hard feature set $S_h$, which will be used for anchor hallucination, hard sample selection and label correction."}, {"title": "3.2. Hard anchor hallucination", "content": "The easy feature set $S_e$ comprises feature vectors of training samples with simple visual patterns and clean labels. By combining features from $S_e$, we can form complex visual patterns, which form the basis for hard anchor hallucination. Through feature concatenation and generating a substantial number of feature anchors spanning the feature space, we can employ majority voting of nearby anchors to search for the most matching feature vectors in the hard feature set $S_h$.\nAnchors are hallucinated by aggregating features of easy samples in an automatic, data-driven process. Specifically, for each $s_u \\in S_e$ from class $\\tilde{y}_u$, we randomly select another feature $s_v \\in S_e$ from a different class $\\tilde{y}$ (where $\\tilde{y}_v \\neq \\tilde{y}_u$) for mixing. Subsequently, we concatenate $s_u$ and $s_v$, and feed it to the hallucinator $h_p$ to produce a hallucinated anchor, $s_a = h_p(s_u, s_v)$. To ensure that $s_a$ is transformed into a hard anchor of the desired class, we formulate the hallucination loss $L_{hal}$ according to the following two designs.\nFirst, to encourage the hallucinated anchor $s_a$ represents a hard instance, we optimize the hallucinator $h_p$ by regularizing the similarities between $s_a$ and both $s_u$ and $s_v$. Specifically, we define a similarity loss based on the cosine distances between features as $L_{sim} = -\\lambda_p \\langle s_a, s_u \\rangle - (1 - \\lambda_p) \\langle s_a, s_v \\rangle$, where $\\lambda_p \\in [0.5, 1.0]$ is a hyperparameter controlling the difficulty level of $s_a$, and $\\langle,\\rangle$ computes the cosine similarity between its arguments. By minimizing $L_{sim}$, the hallucinated anchor $s_a$ will be encouraged to reside in the area between $s_u$ and $s_v$ in the feature space, and thus share visual patterns from both classes $\\tilde{y}_v$ and $\\tilde{y}_u$.\nSecond, to ensure hallucinated anchor $s_a$ belongs to a known, desired class, we follow the work of [20] and define a classification loss using its target label $\\tilde{y}_a = \\tilde{y}_u$. The overall hallucination loss $L_{hal}$ is calculated as:\n$L_{hal} = L_{sim} + H(s_a, \\tilde{y}_u)$, (2)\nwhere $H(\\cdot,\\cdot)$ computes the cross-entropy loss. Minimizing Eq. (2) prompts the hallucinator to generate an anchor $s_a = h_p(s_u, s_v)$ with complex visual patterns positioned near the decision boundary between classes $\\tilde{y}_u$ and $\\tilde{y}_v$, while remaining on the side closer to $\\tilde{y}_u$. Multiple hallucinated anchors can"}, {"title": "3.3. Hard sample selection", "content": "The hallucinated anchors extend throughout the feature space, with varying degrees of representative qualities. Computationally, a hallucinated anchor $s_a \\in S_{hal}$ is considered representative of a real hard feature sample $s_h\\in S_h$, if they are sufficiently close in the feature space. We measure this proximity between $s_a$ and $s_h$ using cosine similarity $\\langle\\cdot,\\cdot\\rangle$. To filter out the representative $s_a$, we identify the nearest hard feature sample of a hallucinated anchor $s_a$, denoted as $s_r = \\arg \\max_{s_h \\in S_h} \\langle s_a, s_h\\rangle$. The anchor $s_a$ is considered a valid representative of $s_r$, if $\\langle s_a, s_r\\rangle$ exceeds a threshold $\\lambda_{conf}$. The hyperparameter $\\lambda_{conf}$ can be tuned based on a small validation set with clean labels. These valid representatives $s_a$ disperse near the hard feature samples and can be used to match them. For each $s_h \\in S_h$, we collect up to $K$ of its surrounding valid representatives. These $K$ valid representatives are leveraged to determine the correct label of $s_h$ by majority voting.\nBy identifying hard samples with corrected labels in this manner, the classification module ($f_e$ and $g_p$) is trained on more valuable labeling information contained in the hard samples, leading to improved performance."}, {"title": "3.4. Semi-supervised learning", "content": "The set of hard samples with corrected labels, denoted $S'_h$, is combined with the easy feature set $S_e$ to form the labeled dataset $S_{labeled} = S'_h \\cup S_e$ for model training, while the remaining noisy hard features, denoted as $S_n$, constitute the unlabeled dataset $S_{unlabeled} = S_h$ for semi-supervised learning (SSL).\nWe adopt the classic SSL method MixMatch [14] to augment samples from $S_{labeled}$ and $S_{unlabeled}$. We obtain pseudo labels for samples in $S_{unlabeled}$ from the average of model predictions across its two augmented copies. Similarly, we refine the labels of samples in $S_{labeled}$ by using a linear combination of their given labels in $D$ and the average of model predictions across the two augmented images. The weight assigned to the given label is determined by the easiness score $w_n$ from \u00a7 3.1.\nAfter obtaining the refined pseudo labels, we then perform SSL using the combined classification loss:\n$L_{SSL} = L_{CE} + \\lambda_{MSE} L_{MSE}$, (3)\nwhere $L_{CE}$ is the cross-entropy loss for the labeled data, $L_{MSE}$ is the mean squared error for the unlabeled data, and $\\lambda_{MSE}$ is a hyperparameter set through validation. By minimizing Eq. (3), the resulting image classifier comprising feature extractor $f_e$ and linear classifier $g_p$ would become more robust, as it incorporates information from critical hard samples with clean labels during training."}, {"title": "3.5. Iterative model training", "content": "To prevent the hallucinator $h_p$ from degeneration, i.e., always producing identical hallucinated anchors $s_a$ regardless of the input pair $(s_u, s_v)$, we adopt an iterative training procedure, as illustrated in Fig. 2. After a few epochs of warm-up training, we start the iterative training stage, which consists of two training phases. In the classification phase, we freeze the hallucinator $h_p$ and train the feature extractor $f_e$ and the linear classifier $g_p$ jointly using the corrected labeled and unlabeled training sets ($S_{labeled}$ and $S_{unlabeled}$) in \u00a7 3.4 by minimizing Eq. (3). In the hallucinator training phase, we freeze $f_e$ and $g_p$ and train $h_p$ by minimizing Eq. (2). The two phases are performed iteratively until sufficient epochs are reached."}, {"title": "4. EXPERIMENTS", "content": "We follow previous NLL works on learning from datasets with IDN labels [15, 8, 17, 9] to conduct the experiments on both synthetic and real-world IDN datasets.\nSynthetic IDN datasets. We conduct experiments on synthetic IDN datasets created from the CIFAR-10 dataset [7], which contains 50,000 training images and 10,000 test images from 10 cleanly annotated classes. We considered two approaches in generating IDN noises: 1) part-dependent label noise (PTD) [15], which is generated according to a combination of multiple noise transition matrices of different parts of an image; 2) classification-based label noise [8], which is generated by averaging the collected softmax outputs during training using a standard CNN trained on all the training data for multiple epochs.\nReal-world IDN datasets. To evaluate the effectiveness of our method on real-world IDN datasets, we conducted experiments using the CIFAR-10N/100N [2] and the real-world Clothing1M [1] datasets. CIFAR-10N/100N were generated from CIFAR-10/100 by collecting labels from three human annotations for each training image through Amazon Mechanical Turk. The three noisy labels for each image are denoted as Random 1/2/3, and are further aggregated by majority vote (denoted as Aggregate) and by random selection of one wrong label if there is any (denoted as Worst). The Clothing1M dataset contains over 1 million training images of 14 different types of clothing collected online, with labels extracted from the surrounding text of images. We use the 14K clean validation set for hyperparameter tuning and the 10K clean test set to evaluate the model performance. These IDN datasets present real-world scenarios with various noise sources and thus provide a suitable testbed for comparing our method with the SoTA methods."}, {"title": "4.2. Baselines and implementation details", "content": "We compare our framework with recent SoTA NLL works, including those focusing on IIN datasets such as DivideMix [4], and those focusing on IDN datasets such as TSCSI [9]. It is worth noting that both DivideMix and TSCSI employ two networks in a co-training fashion for model ensemble thus incurring higher computational costs, whereas our framework only trains a single network in most of our experiment settings except on Clothing1M. For CIFAR-10 with IDN and the CIFAR-10N/100N datasets, we follow previous works [2, 9] and adopt ResNet-34 network as our classification module, and a two-layer Multi-Layer Perceptron (MLP) as our hallucinator $h$. We evaluate our method on a clean testing set and report the best testing accuracy on average over three runs. As for Clothing 1M, we adopt an ImageNet-pretrained ResNet-50 network as per the prior works [4, 9] while also implementing $h$ as a two-layer MLP. We also adopt the same procedures as those used in DivideMix to select easy samples(GMM-based selection without class balancing) for better comparison. During training, we use the 14K clean validation set to choose the best model, which is applied to the 10K clean test to get the test accuracy. More implementation details are available in the supplementary materials."}, {"title": "4.3. Quantitative results", "content": "Results on PTD label noise. Table 1 shows experimental results on the CIFAR-10 datasets with PTD noise [15]. Our proposed method achieves significant performance improvement compared to prior state-of-the-art methods under both 20% and 40% noise ratios. Our model also shows robustness against the increasing noise rate under PTD.\nResults on classification-based label noise. Table 2 provides performance comparisons on CIFAR-10 with classification-based label noise [8] under different noise levels. The classification-based label noise is considered challenging due to its originating from a classification model [9]. Our method consistently demonstrates significantly superior performance compared to previous methods across all noise levels. Our method exhibits remarkable resistance to higher levels of label noise (40%) on classification-based label noise, while other methods suffer substantial performance degradation.\nResults on CIFAR-N. Table 3 shows performance comparisons on the CIFAR-10N/CIFAR-100N datasets [2]. Our method consistently outperforms other methods on CIFAR-10N with all the noise settings of Random 1, Random 2, Random 3, and Worst. Notably, our method achieves comparable performance against DivideMix [4] on CIFAR-100N while only trained a single network. This demonstrates the efficacy of our method in learning from real-world IDN datasets.\nResults on Clothing1M. Table 4 shows performance comparisons on the Clothing1M dataset. Our method achieves competitive results compared with TSCSI and is superior to DivideMix and other methods. Since our method adopts similar strategies with DivideMix in easy sample selection (\u00a7 3.1), the superior performance compared to DivideMix indicates the effectiveness of our hallucination-based hard sample selection (\u00a7 3.2 and \u00a7 3.3) in learning from such a large-scale IDN dataset.\nAblation study. To evaluate the effectiveness of each design component, we conducted an ablation analysis of our proposed framework on the CIFAR-10 dataset with 40% classification-based IDN. We compared the performance of three different settings: (1) vanilla GMM selection-based method, which is essentially DivideMix [4] without co-training and model ensemble, (2) our method with only the easy sample selection stage as described in \u00a7 3.1, and (3) our method with both stages of easy sample selection and hard sample correction in \u00a7 3.3. Table 5 presents the comparison results. As can be seen from the table, the design of each of the two stages contributes to the performance improvement of our framework. Notably, the easy sample selection stage contributed the most to the performance boost, indicating the importance of obtaining a class-balanced easy subset for effective model training. The second stage of hard sample selection further improved the performance to the SoTA level of 92.47%. This validates and confirms our proposal that information contained in hard samples is valuable for the model to learn a robust representation."}, {"title": "4.4. Visualization of Hard Anchor Hallucination", "content": "To demonstrate the effectiveness of our method, Fig. 3 presents the t-SNE visualization of our hallucination on the CIFAR-10 dataset with 40% classification-based label noise in various training epochs. For simplicity, we limit the display to 25 hallucinated and 500 real samples for each class randomly sampled from $D$. The colors of darker hues indicate the hallucinated anchors with pseudo-labels that match the corresponding lighter shades. Observe that the features of hallucinated anchors for each class align with the corresponding cluster of real features. This demonstrates that our hallucinated anchors can effectively mimic the desired hard samples with appropriate pseudo-labels, which can facilitate the subsequent hard sample selection for improved decision boundary training. We provide additional visualization on the hard anchors in the supplementary materials."}, {"title": "5. CONCLUSIONS", "content": "In this paper, we present a novel framework to tackle the underestimation of hard samples in classic selection-based Noisy-Label Learning (NLL) methods. By leveraging easy samples to hallucinate the hard anchors, our approach captures crucial information from hard samples in the presence of instance-dependent noise. While we could not cover all the possible works, we compared with the most similar ones and demonstrated the effectiveness of our model on several benchmark datasets, achieving superior performance compared to state-of-the-art methods. We believe that our work offers a fresh perspective on the significance of hard samples in training models under label noise, a factor frequently overlooked by conventional NLL methods. We show that leveraging the critical labeling information in clean hard samples can enhance the robustness of the decision boundary. Other domains may also benefit from our proposal, such as active learning, which also focuses on leveraging the information of the data effectively.\nLimitations. Our framework identifies hard samples and corrects their labels through hard anchor hallucination, with the assumption that the selected easy feature set $S_e$ (and hence the hallucinated anchor set $S_{hal}$) span the class of interest. As a result, the proposed hallucination process might not work well for highly imbalanced datasets.\nFuture work. A thorough investigation and evaluation of the proposed framework on larger real-world datasets will preferably generate new insights to improve the current solution. We also plan to integrate the proposed framework into other domains beyond image classification to enhance the generalizability of our work."}]}