{"title": "An explainable transformer circuit for compositional generalization", "authors": ["Cheng Tang", "Brenden Lake", "Mehrdad Jazayeri"], "abstract": "Compositional generalization-the systematic combination of known components into novel structures-remains a core challenge in cognitive science and machine learning. Although transformer-based large language models can exhibit strong performance on certain compositional tasks, the underlying mechanisms driving these abilities remain opaque, calling into question their interpretability. In this work, we identify and mechanistically interpret the circuit responsible for compositional induction in a compact transformer. Using causal ablations, we validate the circuit and formalize its operation using a program-like description. We further demonstrate that this mechanistic understanding enables precise activation edits to steer the model's behavior predictably. Our findings advance the understanding of complex behaviors in transformers and highlight such insights can provide a direct pathway for model control.", "sections": [{"title": "Introduction", "content": "Transformers, first introduced by Vaswani et al. (2017), excel at tasks requiring complex reasoning such as code synthesis (Chen et al., 2021) and mathematical problem-solving (Hendrycks et al., 2020). This capability stems not merely from memorization, but from their ability to perform compositional generalization-systematically combining learned primitives into novel structures via in-context learning (ICL) (Brown et al., 2020; Lake & Baroni, 2023). While humans inherently excel at such abstraction (Fodor, 1979), traditional neural architectures struggle with out-of-distribution (OOD) compositional tasks (Hupkes et al., 2019; Lake et al., 2016). Understanding how neural systems accomplish compositionality has become a focus of both machine learning and cognitive science research.\nMechanistic interpretability-a field dedicated to reverse-engineering neural networks into human-understandable algorithms-has begun unraveling these dynamics. Seminal work identified induction heads as a critical component for ICL (Elhage et al., 2021; Olsson et al., 2022), enabling transformers to dynamically bind and retrieve contextual patterns rather than relying on shallow \"lazy\" heuristics like n-gram matching. However, prior works mostly focused on isolated mechanisms (K. R. Wang et al., 2022; Hanna et al., 2023) or over-simplified models (e.g., attention-only transformer in Olsson et al. (2022) , single layer transformer in Nanda et al. (2022)), leaving interpretation of complex induction mechanisms in full-circuit transformers rarely explored. Furthermore, while studies have shown that hyper-parameters (e.g., number of attention layers) can causally affect model's compositional ability (Sanford et al., 2024; He et al., 2024), a microscopic inspection to the internal circuitry is still lacking.\nIn this case study, we provide an end-to-end mechanistic interpretation of how a compact transformer solves a compositional induction task. We rigorously trace down the minimal circuit responsible for the model's behavior and fully reverse-engineer the attention mechanism into human-readable pseudocode. We also bridge mechanistic interpretation and model control by showing that we can steer the model's behavior with activation edits guided by the circuit mechanism."}, {"title": "Related Work", "content": "Transformer circuit interpretation. Mechanistic inter-pretability of transformers began with analysis of simplified models, identifying attention heads as modular components that implement specific functions. In their seminal work, Elhage et al. (2021) and Olsson et al. (2022) introduced \"induction heads\" as critical components for in-context learning in small attention-only models. These heads perform pattern completion by attending to prior token sequences, forming the basis for later work on compositional generalization. Case studies have dissected transformer circuits for specific functions, such as the 'greater than' circuit (Hanna et al., 2023), the 'docstring' circuit (Heimersheim & Janiak, 2023), the 'indirect object' circuit (M. Wang et al., 2024), and the 'max of list' circuit (Hofst\u00e4tter, 2023). These case studies successfully reverse-engineered the transformer into the minimal-algorithm responsible for the target behavior.\nTo facilitate identification of relevant circuits, researchers have proposed circuit discovery methods such as logit lens (nostalgebraist, 2020), path patching (Goldowsky-Dill et al., 2023), causal scrubbing LawrenceC et al. (2022). For large-scale transformers, automated circuit discovery methods are also proposed (Conmy et al., 2023; Hsu et al., 2024; Bhaskar et al., 2024). So far, transformer interpretability work still requires extensive human efforts in the loop for hypothesis generation and testing. We point to a review paper for a more comprehensive review (Rai et al., 2024)."}, {"title": "Compositional generalization in transformers", "content": "In their study, Hupkes et al. (2019) evaluated compositional generalization ability on different families of models, and found that transformer outperformed RNN and ConvNet in systematic generalization, i.e., recombination of known elements, but still uncomparable to human performance. D. Zhang et al. (2024) pointed out that transformers struggle with composing recursive structures. Recently, Lake & Baroni (2023) showed that after being pre-trained with data generated by a 'meta-grammar', small transformers (less than 1 million parameters) can exhibit human-like compositional ability in novel in-context learning cases. This is in line with the success of commercial large language models (LLM) in solving complex out-of-distribution reasoning tasks (Bubeck et al., 2023; DeepSeek-Al et al., 2024), where compositional genralization is necessary.\nSeveral studies highlighted factors that facilitate transformer's compositional ability. M. Wang & E (2024) identified initialization scales as a critical factor in determining whether models rely on memorization or rule-based reasoning for compositional tasks. Z. Zhang et al. (2025) revealed that low-complexity circuits enable out-of-distribution generalization by condensing primitive-level rules. (Sanford et al., 2024) identified logarithmic depth as a key constraint for transformers to emulate computations within a sequence. Here, we offer a complementary mechanistic understanding of how trasnform-ers perform compositional computations."}, {"title": "Experimental Setup", "content": "Our experimental setup involves a synthetic function composition task (Figure 1) designed to probe compositional induction in a compact Transformer. We outline the task structure, the Transformer basics (including attention mechanisms), and the training protocol."}, {"title": "Task Structure", "content": "Each episode consists of a support set and a question (Figure 1b):\n\u2022 Support Set: Specifies (i) Primitives as symbol-to-color mappings (e.g., A = red, D = pink), and (ii) Functions as symbolic operations over these primitives (e.g., A S D = pink red, where S indicates swapping adjacent symbols).\n\u2022 Question: Presents a new composition of primitives and functions from the support-set.\nThe model generates answers to the question as token sequences emitted from the decoder, with a SOS (start of sentence) token as the first input to the decoder and an EOS (end of sentence) marking the end of the emission. The model operates strictly via in-context learning-weights remain frozen during inference, and test episodes are disjoint from training data. The model must infer latent variable bindings (primitives and functions) from the support set and dynamically compose these bindings to solve the novel question."}, {"title": "Model", "content": "Transformer Basics Our transformer uses an encoder-decoder architecture that involves two types of attentions:\n\u2022 Self-Attention: Captures within-sequence interactions. The token embedding matrix\n$$X \\in \\mathbb{R}^{n_{input} \\times d_{model}}$$\nis projected into Queries, Keys, and Values:\n$$Q = XW_Q, K = XW_K, V = XW_V,$$\nwhere $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_{head}}$ are learnable weight matrices.\n\u2022 Cross-Attention: Enables the decoder to attend to encoder outputs. Here, the Queries (Q) come from the decoder tokens, while the Keys (K) and Values (V) come from the encoder tokens.\nThe attention mechanism operates through two separate circuits on embedding $X \\in \\mathbb{R}^{input \\times d_{model}}$ for each attention head:\n\u2022 QK Circuit ($W_QW_K$): Determines from where information flows to each token by computing attention scores between"}, {"title": null, "content": "token pairs, with higher scores indicate stronger token-to-token relationships:\n$$Attention(Q, K) = softmax\\left(\\frac{X_QW_Q(X_KW_K)^T}{\\sqrt{d_{head}}}\\right)$$\nwhere softmax is applied along the dimension for Key and independently for each head.\n\u2022 OV Circuit ($W_VW_O$): Controls what information gets written to each token position. Combined with the QK Circuit, this produces the output of the attention head:\n$$Z = Attention(Q, K)X_VW_VW_O \\in \\mathbb{R}^{n_{query} \\times d_{model}}$$\nwhere $W_O \\in \\mathbb{R}^{d_{head} \\times d_{model}}$ is learnable weight.\nOur analysis focuses on how these circuits in attention heads together implement the compositional induction algorithm."}, {"title": "Model Training", "content": "We adopt an encoder-decoder Transformer with 2 layers in the encoder and 2 layers in the decoder (Figure 1a) with each layer containing 8 attention heads. Further model details appear in the Appendix.\nFor each episode, we randomly generate:\n\u2022 Primitive Assignments: A mapping from symbol tokens (e.g., A, B) to color tokens (e.g., red, pink).\n\u2022 Function Definitions: Symbolic transformations by randomly sampling primitive arguments to a function to produce color sequences (e.g., A S B might be expanded into a sequence [A] [B] [A] [A] [B], maximum length=5).\nWe train on 10,000 such episodes for 50 epochs and evaluate on 2,000 test (held-out) episodes. The model achieves 98% accuracy on this test set, indicating strong compositional induction capabilities. In the test set, primitive assignments and function definitions are conjunctively different from those in the training set (i.e., some primitives or some functions might be in the training set, but not the whole combination of them), preventing a memorization strategy. Please refer to the Appendix for additional details."}, {"title": "Results", "content": "First, we give an intuitive overview of the effective algorithm the model appears to implement. Next, we describe our circuit discovery procedure, where we use causal methods to pinpoint the exact attention heads responsible for compositional induction. Finally, we validate this mechanism by applying targeted perturbations that predictably alter the model's behavior."}, {"title": "The Effective Algorithm", "content": "General Solution. We first provide a general solution to this type of compositional problem in a python-like pseudocode for intuitive understanding (Algorithm 1). We use 1-indexing (count from 1) for tokens throughout.\nTransformer Solution. Next, we describe the actual implementation of the algorithm with attention operations in Figure 2 through a guidance episode."}, {"title": "Circuit Discovery", "content": "Nomenclature: for attention heads, Enc-self-0.5 stands for Encoder, self-attention, layer 0, head 5; similarly, Dec-cross-1.5 stands for Decoder, cross-attention, layer 1, head 5.\nOutput Head (Dec-cross-1.5; Figure 3b) We discovered the model's circuit backwards from the unembedding layer using logit attribution (nostalgebraist, 2020), which measures each decoder attention head's linear contribution to the final token logits (adjusted by the decoder's output LayerNorm). We identified Dec-cross-1.5 (decoder cross attention layer 1 head 5) as the primary contributor (Figure 3\u0430).\nDec-cross-1.5's Q tokens always attend to the K tokens from the Encoder that are the next predicted ones. For example, in Figure 3b, the SOS token attends to instances of red in the support set, which is indeed the correct next output prediction. This attention accuracy (i.e., max-attended token being the next-emitted token) of Dec-cross-1.5 remains above 90% for the first three tokens in the responses across all test episodes (Figure 3c), with Dec-cross-1.1 and -1.3 partially compensating beyond that point.\nThese observations suggest that Dec-cross-1.5's OV circuit feeds token identities directly to the decoder unembed-ding layer (output layer). Specifically, we observe that the output of the OV circuit, $XW_VW_O$, align closely (strong inner product) with the unembedding vectors of the corresponding tokens (Figure 3d). Hence, we designate Dec-cross-1.5 as the Output Head (while Dec-cross-1.1 and -1.3 perform similar but less dominant roles) (Algorithm Step 3).\nNext, we show how the Output Head identifies the correct token through QK interactions.\nThe K-Circuit to the Output Head We first determine which encoder heads critically feed into the Output Head's K. To do this, we performed path-patching (K. R. Wang et al., 2022) by ablating all but one single encoder head and then measuring how much of Output Head's QK behavior (i.e., attention accuracy) remained. During these experiments, Output Head's Q were frozen using clean-run activations. Here we report patching results with mean-ablation (qualitative similar to random-sample ablation) (details in Appendix).\nThrough this process, we identified Enc-self-1.1 and Enc-self-0.5 as the primary contributors to Output Head's K, acting in a sequential chain (Figure 4). Next, we show how they sequentially encode symbols' index-in-question critical for the QK alignment.\nPrimitive-Pairing Head (Enc-self-1.1; Figure 5a) This head exhibits a distinct attention pattern that pairs each color token with its associated primitive symbol token (e.g., in the support set, all instances of red attend to C). In other words, Enc-self-1.1 relays information (described below, as computed by e.g., Enc-self-0.5) from the primitive symbols to their corresponding color tokens via its QK circuit. Hence, we call Enc-self-1.1 the Primitive-Pairing Head.\nTo investigate which upstream heads feed into the OV circuit of the Primitive-Pairing Head, we applied a sequential variant of path-patching, isolating the chain:\nUpstream heads (e.g. Enc-self-0.5)\nPrimitive-Pairing Head (V)"}, {"title": null, "content": "question-related information (including token identity and position) across symbols in the support-set (henceforth the Question-Broadcast Head). We hypothesize that the primitive symbols' index-in-question is the critical information passed from the Question-Broadcast Head's Z through the Primitive-Pairing Head's Z and lastly into the Output Head's K.\nIndex-In-Question Tracing To validate this hypothesis, we examined the Question-Broadcast Head's Z for each primitive-symbol token. We reduced these outputs to two principal components and colored each point by its index-in-question. As illustrated in Figure 6a, the Question-Broadcast Head's Z exhibit clear clustering, indicating that the index-in-question is robustly encoded at this stage (quantified by the $R^2$ score, i.e., the amount of variance explained by index identity, details in Appendix). We further confirmed that the Primitive-Pairing Head's Z preserves index-in-question (Figure 6b) and that the resulting Output Head's K also reflect the same clustering (Figure 6c).\nCausal Ablation Finally, we verified that this circuit indeed causally propagates index-in-question. Ablating the Question-Broadcast Head's Z (together with the similarly functioning Enc-self-0.7) obliterates the clustering in the Primitive-Pairing Head's Z; ablating the Primitive-Pairing Head's Z (together with similarly functioning Enc-self-1.0) disrupts the clustering in the Output Head's K (Figure 6). We therefore conclude that the Question-Broadcast Head, the Primitive-Pairing Head and heads with similar functions form a crucial K-circuit pathway, passing index-in-question information from primitive tokens to their associated color tokens in the Output Head's K.\nThe Q-Circuit to the Output Head Having established the role of the K-circuit, we next investigate where its Q originates. We again relied on sequential path-patching to pinpoint which decoder heads ultimately provide the Output Head's Q. We identified Dec-cross-0.6 as the main conduit for the Q values of the Output Head. Enc-self-1.0 and -1.2 supply positional embeddings that enable the decoder to track primitive symbol's relative-index-on-LHS, thereby completing the QK alignment for correct predictions (Figure 7).\nRHS-Scanner Head (Dec-cross-0.6; Figure 8b) We identify Dec-cross-0.6 as the dominant contributor to the the Output Head's Q (Figure 8a). Analyzing Dec-cross-0.6's attention patterns reveals that each Q token (from Decoder in the cross-attention) sequentially attends to the color tokens (in the support set) on the function's RHS (Figure 8b). For example, the first Decoder token (SOS) attends to the first RHS tokens (purple, red, yellow), and the second query token (red) attends to the second RHS tokens (red, purple, red), and so on. This iterative scanning mechanism enables the decoder to reconstruct the transformation defined by the function. Hence we call Dec-cross-0.6 the RHS-Scanner Head.\nPrimitive-Retrival Head (Enc-self-1.0; Figure 9b) and Function-Retrival Head (Enc-self-1.2; Figure 9c) Next,"}, {"title": "Relative-Index-On-LHS Tracing", "content": "To confirm that our discovered circuit genuinely encodes the relative-index-on-LHS in the Output Head's Q, we conducted three complementary ablation experiments summarized in Figure 10:\n\u2022 Retaining only the Primitive- and Function-Retrieval Heads When all other encoder heads are ablated, the RHS-Scanner Head's Z still carries relative-index-on-LHS that propagate to the Output Head's Q, indicating that these two heads alone provide sufficient index information.\n\u2022 Ablating the Primitive- or Function-Retrieval Head individually Ablating either head disrupts the clustering by relative-index-on-LHS in the RHS-Scanner Head's Z, demonstrating that both heads are necessary to preserve the full index information.\n\u2022 Ablating the RHS-Scanner Head (together with Dec-cross-0.0 and -0.3) These decoder heads share similar"}, {"title": "Targeted Perturbation Steers Behavior", "content": "So far, our circuit tracing indicates that the K-circuit of the Output Head encodes the primitive symbols' index-in-question, and that the Q-circuit encodes primitive symbols' relative-index-on-LHS. We reason that if the QK circuit of the Output Head truly leverages on the primitive symbol index to predict the next word, then swapping those index information across different color tokens should also swap the corresponding attention patterns observed in the Output Head.\nSwapping Index Information Concretely, we select two primitive symbols in the question (e.g., 'BSA | A=red | B=blue |...'). The red token will have index-in-quesiton=3rd from A (similarly blue will have '1st') on the K-side of the Output Head. If the Q-side expects a particular index from the K-side (e.g., 'SOS' in Q may carry relative-index-on-LHS=3rd and expects tokens carrying index-in-question=3rd from K), a swap of the index information in K should lead to a predictable shift in which tokens the head attends to. We performed this perturbation in the K-circuit of the Output Head while freezing its Q-circuit. Indeed, when we swap only the position embedding of B and A on the Question-Broadcast Head's V (the most upstream node in the K-circuit), with everything else intact, we observe that the Output Head systematically \"reverts\u201d the attention from red to blue based on their swapped positions (Figure 11).\nThis intervention thus provides causal evidence that the Output Head's QK alignment relies on the index information on both sides passed through the sub-circuits. It does not merely degrade or randomly scramble the output head's behavior; rather, the predictions shift in a way directly consistent with our interpretation of how index information is encoded and matched between Q and K. The model's predictable response to this precise manipulation underscores that we have correctly identified the sufficient pathways."}, {"title": "Discussion", "content": "In this work, we investigated how a compact transformer model achieves compositional induction on a synthetic function composition task. By combining path-patching analyses with causal ablations, we uncovered a detailed QK circuit that encodes index information from both the question and the function's LHS. We further demonstrated that precisely swapping these positional embeddings in the model's activations leads to predictable changes to behavior, thereby confirming the causal relevance of the discovered circuit. These results show that, even for complex functions, transformers can implement a structured and interpretable mechanism."}, {"title": "Limitations and Future Work", "content": "Model Scale. Our circuit analysis focused on a relatively small transformer. Establishing whether similar interpretable circuits exist in larger models remains an important open question to follow up.\nManual Circuit Discovery. The techniques employed here required substantial human effort-path-patching, ablations, and extensive interpretation of attention heads. For large-scale models, such manual approaches become less feasible. We therefore see a need for automated or semi-automated"}, {"title": "Partial Perturbations", "content": "Although our targeted activations swaps successfully steered the Output Head's behavior, we have not demonstrated a complete perturbation of its predicted tokens. This is due to the distributed nature of the underlying mechanism (multiple heads fulfill similar roles). Coordinating interventions across all such heads will require systematic workflows, which we aim to develop in the future.\nDespite these constraints, our work shows that disassembling transformer circuitry can yield two key benefits. First, it illuminates how compositional functions are mechanistically instantiated at the attention-head level. Second, it enables targeted, activation-based interventions that reliably steer model behavior. We hope these contributions will encourage further research on scalable circuit discovery methods and more automated interpretability approaches for large-scale models."}, {"title": "Appendix", "content": "Transformer Model\nWe adopt an encoder-decoder architecture, which naturally fits the task by allowing the encoder to process the prompt (question + support) with bidirectional self-attention and the decoder to generate an output sequence with causal and cross-attention. Specific hyperparameters include:\n\u2022 Token embedding dimension: $d_{model}$ = 128\n\u2022 Attention embedding dimension: $d_{head}$ = 16\n\u2022 Eight attention heads per layer (both encoder and decoder)\n\u2022 Pre-LayerNorm (applied to attention/MLP modules) plus an additional LayerNorm at the encoder and decoder outputs\n\u2022 Standard sinusoidal positional embeddings\nThe encoder comprises two layers of bidirectional self-attention + MLP, while the decoder comprises two layers of causal self-attention + cross-attention + MLP. We train the model by minimizing the cross-entropy loss (averaged over tokens) using the Adam optimizer. The learning rate is initialized at 0.001 with a warm-up phase over the first epoch, then linearly decays to 0.00005 over training. We apply dropout of 0.1 to both input embeddings and internal Transformer layers, and train with a batch size of 25 episodes. All experiments are performed on an NVIDIA A100 GPU."}, {"title": "Task Structure", "content": "In each episode, the support set and question are concatenated into a single prompt for the encoder, with question tokens placed at the start. Question, primitive assignments, and function assignments are separated by '|' tokens, while primitive and function assignments are identified by '='. Overall, there are 6 possible colors and 9 symbols that may serve as either color primitives or function symbols. Each episode contains 2-4 function assignments and 3-4 color assignments.\nA function may be a single-argument (arg func) or double-argument ($arg_1$ func $arg_2$) function. The function's right-hand side (RHS) describes how arguments are transformed, generated by randomly sampling up to length-5 sequences of arguments and mapping them to color tokens. Each prompt ends with an 'EOS' token. During decoding, the model begins with an 'SOS' token and iteratively appends each newly generated token until it emits 'EOS'."}, {"title": "Path Patching", "content": "Path patching is a method for isolating how a specific source node in the network influences a particular target node. It proceeds in three runs:"}, {"title": null, "content": "1. Clean Run: Feed the input through the model normally and cache all intermediate activations (including those of the source and target nodes).\n2. Perturbed Run: Freeze all direct paths into the target node using their cached activations from the clean run. For the source node alone, replace its cached activation with mean-ablated values. Record the new, perturbed activation at the target node.\n3. Evaluation Run: Supply the target node with the perturbed activation from Step 2, then measure any resulting changes in the model's output. This quantifies how the source node's contribution (altered via mean-ablation) affects the target node's behavior.\nChained Path Patching. When analyzing circuits that span multiple nodes in sequence, we extend path patching in a chain-like manner. For instance, to evaluate a chain A\u2192B\u2192C:\n\u2022 We first perform path patching on the sub-path B \u2192 Cas usual.\n\u2022 Next, to capture how A specifically influences B, we isolate and record A's effect on B via mean-ablation on all other inputs to B.\n\u2022 Finally, we patch that recorded activation into B and evaluate its effect on C.\nFor a chain of length N, we run N + 1 forward passes, ensuring the measured impact on the target node reflects only the chained pathway. This approach precisely attributes the model's behavior to the intended sequence of dependencies.\nTwo Modes of Ablation. To assess how individual heads or nodes contribute to the target node, we use two complementary modes:\n1. Keep-only-one-head: Mean-ablate all direct paths to the target node except for one node, which retains its clean-run activation. If the target node's performance remains stable, this single node is sufficient for driving the relevant behavior. However, this method may fail when multiple heads each provide partial information that is only collectively sufficient.\n2. Ablate-only-one-head: Keep all source nodes from the clean run except one, which is mean-ablated. If performance degrades, that ablated node is necessary. However, if the node's information is redundant or duplicated across other paths, the target node's performance will not significantly change.\nBy combining both modes, we identified the putative QK-circuit of the output head. We then validate the circuits by inspecting the information they propagates and causally erasing the information by ablating specific upstream nodes."}, {"title": "$R^2$ Score", "content": "To quantify how much an activation dataset Y encodes a particular latent variable Z, we compute a linear regression of Z (one-hot encoded) onto Y and measure the explained variance:\n$$R^2 = 1 - \\frac{SS_{res}}{SS_{total}}$$\nAn $R^2$ value of 1.0 indicates that Z fully explains the variance in Y, whereas an $R^2$ near 0.0 implies Z provides no information about Y."}]}