{"title": "Enabling Energy-Efficient Deployment of Large Language Models on Memristor Crossbar: A Synergy of Large and Small", "authors": ["Zhehui Wang", "Tao Luo", "Cheng Liu", "Weichen Liu", "Rick Siow Mong Goh", "Weng-Fai Wong"], "abstract": "Large language models (LLMs) have garnered substantial attention due to their promising applications in diverse domains. Nevertheless, the increasing size of LLMs comes with a significant surge in the computational requirements for training and deployment. Memristor crossbars have emerged as a promising solution, which demonstrated a small footprint and remarkably high energy efficiency in computer vision (CV) models. Memristors possess higher density compared to conventional memory technologies, making them highly suitable for effectively managing the extreme model size associated with LLMs. However, deploying LLMs on memristor crossbars faces three major challenges. Firstly, the size of LLMs increases rapidly, already surpassing the capabilities of state-of-the-art memristor chips. Secondly, LLMs often incorporate multi-head attention blocks, which involve non-weight stationary multiplications that traditional memristor crossbars cannot support. Third, while memristor crossbars excel at performing linear operations, they are not capable of executing complex nonlinear operations in LLM such as softmax and layer normalization. To address these challenges, we present a novel architecture for the memristor crossbar that enables the deployment of state-of-the-art LLM on a single chip or package, eliminating the energy and time inefficiencies associated with off-chip communication. Our testing on BERT Large showed negligible accuracy loss. Compared to traditional memristor crossbars, our architecture achieves enhancements of up to 39x in area overhead and 18x in energy consumption. Compared to modern TPU/GPU systems, our architecture demonstrates at least a 68x reduction in the area-delay product and a significant 69% energy consumption reduction.", "sections": [{"title": "INTRODUCTION", "content": "LARGE language models (LLMs), such as ChatGPT, LLaMA and PaLM, have become increasingly popular in recent years due to their ability to leverage vast amounts of professional knowledge by fine-tuning the model. This potential has been demonstrated in various domains, including medical and finance technologies. However, the size of LLMs has been growing rapidly with their increasing accuracy, resulting in huge computational complexity. Even inferring LLMs require powerful computing systems that consume a significant amount of energy. For example, the inference of ChatGPT requires approximately eight A100 GPU cards [1], which poses challenges for local deployment, particularly in mobile environments. The OpenAI data center for ChatGPT consumes 23 million kWh based on monthly re- quests [2]. This high energy consumption and cost for model inference could constrain its further development, particularly as models become even larger. Thus, there is a significant demand for small-size LLM accelerators that can perform model inference more efficiently and cost-effectively.\nMemristor crossbars are widely considered strong competitors for traditional machine learning accelerators [3]. Numerous mem- ristor crossbar systems have been proposed [4\u20138], showcasing low power consumption and low latency compared to classical accelerators. By taking advantage of the physical characteristics of memristive storage technology, the analog computation can be performed using memristors [9-11], which greatly boosts the accelerator's performance. Different types of memristors using various Non-volatile memory (NVM) technologies [12] exist, in- cluding resistive random access memory (RRAM), phase-change memory (PCM), spin-transfer torque magnetic random access memory (STT-RAM), and Flash memory. All of them are promis- ing candidates for building high-efficiency accelerators. For the purposes of this discussion, we will use RRAM memristors as a representative of these NVM technologies.\nCompared with the current leading memory technologies, such as DRAM (dynamic random-access memory) and SRAM (static random-access memory), memristors have higher density. For instance, each RRAM memristor occupies only 4f2 [13] area, where f refers to the feature size of the chip. Considering the use of 4-bit memristors, a footprint of 274 mm\u00b2 memory cells area is sufficient to store all 175 billion parameters of GPT-3, assuming the 14 nm technology. In contrast, the DRAM and SRAM require 6f2 [13] and 100f2 [13] for each bit of information. As a comparison, they demand significantly larger areas of 1646 mm\u00b2 and 27440 mm\u00b2 as memory cell area for GPT-3, assuming the same 14 nm technology node. By achieving a substantial decrease in physical size, there is a high chance to store the whole neural network models within a single chip or package, thus effectively eliminating the inefficiencies of off-chip communication in terms of both time and energy [14].\nDespite the benefits of memristor-based machine learning ac- celerators and their wide applications in neural networks, it is still difficult to directly deploy LLMs on memristor-based accelerators due to three major challenges that constrain their usage."}, {"title": "RELATED WORK", "content": "Before the emergence of non-volatile memory, two major cat- egories of memory devices were prevalent [13]: SRAM (static random-access memory) and DRAM (dynamic random-access memory). An SRAM cell consists of six transistors, occupying a relatively larger area [15]. On the other hand, a DRAM cell comprises a transistor and a capacitor, storing one bit of informa- tion [16]. With the development of emerging memory systems like RRAM (resistive random access memory), even higher density has been achieved. Each RRAM cell consists of a memristor (3D- stacked) and a transistor [17]. Unlike DRAM and SRAM, the RRAM is capable of storing multiple bits in each cell, rather than just one bit [18]. Utilizing RRAM increases the chance to deploy large-scale neural network models within a single chip or package, thus effectively bypassing inefficient off-chip communication.\nMemristor crossbars have shown great potential in computing deep neural networks (DNNs) with higher energy efficiency than traditional neural network accelerators such as TPU and GPU, in traditional computer vision applications [17]. There are two basic architectures using two different formats of data storage: multi- bit memristor and single-bit memristor. In the multi-bit design, each memristor stores multiple bits of information. Examples include [18, 19]. This architecture has a very high density of data storage as each weight only occupies one or a few memristors. Another design is the single-bit memristor, where we need mul- tiple memristors to store a single weight. Examples are [20, 21]. Since the data is computed in a digital way with binarized data, The design excels in handling unstable environments like noise, but it sacrifices density for robustness.\nIn the literature, several improvements have been made to the basic architectures of memristor crossbars to increase their computing efficiency on DNNs. Among them, PRIME [22], ISAAC [9], and PipeLayers [23] are the most popular and typical designs. These architectures combine features from both multi- bit and single-bit designs and demonstrate higher energy effi- ciency, throughput, and computation density compared to tradi- tional accelerators. Another type of device that people commonly used to compute neural networks analogously today is SRAM crossbar. For example, Vesti is an SRAM-based neural network accelerator [24]. Compared to NVM, SRAM crossbar is easier to reconfigure the memory content. However, its density is only 4% of traditional NVM devices such as RRAM [13], making it unsuitable for storing extremely large neural network models."}, {"title": "PRELIMINARIES", "content": "In this section, we will first introduce the background knowledge of language models and their internal operations. Next, we will present the typical architecture of the memristor crossbar and discuss its characteristics."}, {"title": "Operations in Large Language Models", "content": "Take the language model BERT as an example. BERT utilizes a series of encoder layers to process input data. Each encoder layer in BERT consists of two fundamental components, represented as component A and component B in Figure 1. In other language models, the layers are also structured using the same two com- ponents (Component A is masked in some models). Component A comprises a multi-head attention block followed by an add and normalization block. Within this component, the multi-head attention block takes the input denoted as X and produces an output denoted as Z. The add and normalization block then takes the sum of X and Z as input and generates an output denoted as U. On the other hand, Component B consists of a feed-forward block followed by an add and normalization block. The notation for the input X, output Z, and final output U in Component B follow a similar convention as used in Component A.\nThe details of the multi-head attention block are shown in Table 1(a). Assuming the input sequence matrix X, it is first multiplied with three weight matrices individually: Wq, Wk, and Wv. This generates matrices Q, K, and V, respectively. The next step is to multiply the matrix Q with KT/\u221adk, which represents the transposed K scaled by a factor /\u221adk. Here dk is a fixed value indicating the width of the matrix K. This multiplication operation results in a matrix S, representing the attention scores between the query and the key. Next, we apply the softmax function to normalize S, and the result is denoted as Softmax(S). This softmax function ensures that the values in each row of Softmax(S) range from 0 to 1 and sum up to 1, representing the importance of relative elements. It is defined in Equation (1).\nSij = esij /(esi1 + e8i2 + ... + esin) (1)\nHere, sij represents the element in matrix S, while s'; represents the corresponding element in matrix Softmax(S). Once we ob- tained Softmax(S), we multiply it with matrix V, yielding a matrix Y. Subsequently, we perform a matrix multiplication between the matrix Y and another weight matrix Wo, producing the output matrix Z.\nThe details of the feed-forward block are illustrated in Ta- ble 1(b). The process begins with the input matrix X, which undergoes multiplication by the weight matrix Wa, resulting in an intermediate matrix Y. Subsequently, Y is multiplied by another weight matrix Wb, producing the final output matrix Z.\nThe details of the add and normalization block can be found in Table 1(c). There is one operation called LayerNorm, which transforms the matrix U (the sum of X and Z) into U. This transformation is defined by Equation (2).\nUij = (Uij - E(Ui*))/\u221aVar(ui*) + \u0454 \u00b7 \u03b3 + \u03b2 (2)\nIn this equation, Uij and \u016bij represent the corresponding elements in matrix U and \u016a, respectively. The parameters \u03f5, \u03b3, and \u03b2 are trainable parameters and remain fixed during inference. The terms E(ui\u2217) and Var(ui\u2217) denote the mean and variance, respectively, of the elements in the i-th row of the matrix U.\nAs indicated in Table 1, operations 4 and 5 in (a), as well as operation 1 in (c), are classified as non-weight stationary (NW) computations. On the other hand, the remaining operations are all categorized as weight-stationary (WS) computations."}, {"title": "Traditional Memristor Crossbar", "content": "Traditional memristor-based machine learning accelerators are op- timized for weight-stationary matrix multiplications, making them well-suited for deploying most computer vision (CV) models. Figure 2(a) provides an example of a memristor crossbar with 2\u00d72 cells. This is a multi-bit design. Each cell stores a single parameter of the neural network, with the conductance of the cell represent- ing the weight stored. A digital-to-analog converter (DAC) is used to transform activation values into voltages applied to the memory cell. By Ohm's Law, the current flowing through the cell equals the applied voltage multiplied by the conductance. Finally, an analog- to-digital converter (ADC) converts the combined current from various memory cells back into digital data. Kirchhoff's Current Law ensures that the current sum is equivalent to the summation of the products.\nDespite the higher density of memristors compared to DRAM and SRAM, the effective density of the entire system remains low due to the necessity of implementing high-bit DACs/ADCs in the input and output circuits of the crossbar, which occupy a significant area on the chip. The area breakdown of a classical 128 \u00d7 128 memristor crossbar in Figure 2 (b) illustrates this. Even with the sharing of the ADC among the 128 columns [9], the DACs and ADCs still demand approximately 51% and 45% of the total area, respectively. The ISAAC [9] crossbar archi- tecture reduces the area overhead of the DAC at the expense of longer computation time. Despite this, only around 2% of the area being allocated to memristors. While the PRIME [22] and PipeLayer [23] approaches eliminate the need for ADCs, their input or output circuits still contain numerous capacitors, which consume a significant amount of chip area.\nThe substantial area overhead caused by implementing input and output circuits (e.g., DACs and ADCs) within the memristor crossbar, as noted in previous studies [25, 26], diminishes the advantage of the high density offered by RRAM devices in comparison to alternative techniques. Due to the considerable time and energy consumption associated with memristor program- ming, dynamic modification of stored values during inference is infrequent. It becomes necessary to pre-store the entire set of parameters within the memristors. However, because of the"}, {"title": "STANDARDIZED OPERATION DECOMPOSITION", "content": "The LLM consists of a wide range of operations, including both linear operations such as weight stationery and non-weight station- ary multiplication, as well as non-linear operations like softmax and layer normalization. The diversity of operations within the LLM presents challenges for a single hardware module to effi- ciently perform all the required functions. Without optimization, it would be necessary to employ separate hardware modules to handle the diverse computational requirements. To enable the seamless implementation of LLM on a unified hardware module, we decompose all the operations of LLM into standardized sub- operations, as illustrated in Equation (3).\nF(XY) = F(Z) (3)\nAs depicted in Figure 3, each standardized sub-operation within the LLM consists of a fundamental linear operation executed by memristor-based crossbars and an additional F executed by peripheral module. The linear operation involves multiplying a matrix X with a matrix Y, resulting in a matrix Z. Depending on the specific context, this linear operation can be either weight sta- tionary or non-weight stationary. In the case of weight-stationary multiplication, the matrix Y can be replaced by a weight matrix denoted as W. The additional operator F can be either linear, such as multiplication, addition, or non-linear, incorporating func- tions like the exponential function (EXP), Rectified Linear Unit (ReLU), division, and various others.\nFor easier hardware implementation, we can further decom- pose each sub-operation into multiple sessions, as shown in Equation (4). We denote colt (Y) and colt (Z) as the t-th column of matrices Y and Z, respectively. In each session, we only compute the multiplication between X and the vector colt (Y). This operation can be performed by any hardware that supports matrix vector multiplications.\nF(X colt (Y)) = F(colt(Z)), t = 1,2,3... (4)\nThe standardized operation decomposition for LLM is listed in Table 2 for the multi-head attention block, Table 3 for the feed- forward block, and Table 4 for the layer normalization function."}, {"title": "Softmax Operation", "content": "The softmax operation is a crucial component in multi-head attention blocks. After multiplying the matrix Q with KT/\u221adk, the resulting matrix S undergoes a softmax operation along each row. Subsequently, the softmax result is multiplied by the matrix V. The aforementioned computation process can be divided into three standardized sub-operations, represented as sub-operations 4, 5, and 6 in Table 2. All of these sub-operations are non- weight stationary. The details of the decomposition process can be summarized in the following steps.\n1) In sub-operation 4, we multiply matrix Q with KT/\u221adk to obtain matrix S. This operation is performed in n sessions. During session t, matrix Q is multiplied with the t-th column of matrix KT/\u221adk. We incorporate the additional function F as EXP, which transforms each element in matrix S from Sij to esij. As a result, we can obtain the matrix EXP(S) from this sub-operation.\n2) In sub-operation 5, we multiply the matrix EXP(S) with vector 1, where vector 1 is defined as a vector with all elements being 1. This sub-operation is performed in a single session (t \u2208 {1}), with no additional operation F involved. Each element in the final output vector, denoted by at, is the summation of all the elements in the i-th row of matrix EXP(S), as illustrated in Equation (5).\naj = esil + esi2 + ... + esin = \u2211jesij (5)\n3) In sub-operation 6, we multiply matrix EXP(S) with the matrix V, resulting in matrix R. This operation is also performed in m sessions. During session t, matrix EXP(S) is multiplied with the t-th column of matrix V. In the output matrix, element rik in matrix R is calculated as \u2211j (esij \u00b7Vjk).\n4) Finally, we perform scaling on the matrix R. This is done by applying division as the additional operation F in sub- operation 6. Specifically, the i-th column of matrix R is scaled by ai. Mathematically, matrix R is element-wise divided (symbol) by (a\u00b7 1T). This results in matrix Y (Ta- ble 1(a)), whose element Yik becomes \u2211j(esij\u00b7Vjk)/\u2211jesij, corresponding to the softmax results.\nFigure 4 outlines the transformation process for the softmax operation, enabling its execution on memristor-based crossbars. We begin by decomposing the softmax into three components. Afterwards, two of these components can be integrated with the matrix multiplication operations that precede and follow them, yielding three standardized sub-operations."}, {"title": "Layer Normalization", "content": "Layer normalization is essential for both multi-head attention blocks and feed-forward blocks. Let's assume the input to the normalization block is U = Z + X. The normalization process is applied to each token individually, where the t-th token is represented by rowt(U) (i.e., the t-th row of the matrix U). We use ut* to denote elements in rowt (U). Prior to normalization, it is necessary to determine the means and variances of these elements, denoted as E(ut*) and E(uz), respectively. These values enable us to compute the variance using Equation (6).\nVar(ut*) = E(u) - (E(Ut*))2 (6)\nThe calculation of the means of ut* and ut can be decomposed into two standardized sub-operations, as shown in Table 4. Both of them are non-weight stationary. To calculate the sum of elements Ut* in rowt(U), we can multiply this row vector with a vector 1 that contains all elements as 1. Similarly, by multiplying rowt (U) with itself, we can obtain the sum of squared elements u\u00b2. In both sub-operations, we incorporate additional operations F multiplied by 1/m, where m represents the number of elements in rowt (U). Consequently, we can obtain E(ut*) and E(u*) from the sub- operations. Since there are n tokens in the sequence, the above computations are executed in n sessions, where each session corresponds to processing one token. Specifically, the t-th token in the input sequence is processed during session t.\nAfter obtaining E(ut*) and E(u\u2217), we can utilize equation (6) to calculate Var(ut*). To normalize the vector, we begin by subtracting each element in the vector by its mean E(ut*) and then multiply it by a scaling factor, denoted as \u03b1 in Equation (7).\n\u03b1 = \u03b3/ Var(ut*) + \u20ac (7)\nHere, \u03b3 and \u03f5 are known parameters from the model. A dedicated module can be utilized to perform the square root and division operations. Due to the need for computation of the aforementioned process only n times within each LLM layer, the requirement for such modules is minimal, resulting in a negligible area overhead. Once we obtain \u03b1, we have the option of directly multiplying it with ut*. Alternatively, we can incorporate this multiplication as an operation F within the subsequent computation blocks."}, {"title": "MEMRISTOR CROSSBAR ARCHITECTURE FOR STANDARDIZED SUB-OPERATIONS IN LLM", "content": "We have developed an advanced architecture for memristor cross- bars that enables efficient computation of the standardized sub- operations (Equation (4)) in LLM. Our proposed system utilizes two types of crossbars: the computation crossbar, which is opti- mized for low-energy computing, and the dense crossbar, which is designed specifically for deploying large-scale neural networks. Both crossbar types are integrated onto the same chip to eliminate the need for inefficient off-chip communication."}, {"title": "Architecture Overview", "content": "Figure 5(a) illustrates the overview of our proposed architecture, while Figure 5(b) shows an example of a traditional architecture with the same weight capacity. To simplify the analysis, we assume that the model contains only three weight matrices. In both figures, we use unique colors to indicate the locations of weight matrices, denoted as W1, W2, and W3. In traditional architecture (Figure 5(b)), the weights stored within the crossbars are fixed. Weight matrices W1 to W3 are stored in Crossbar-1 to Crossbar- 3, respectively. The dark arrows among the crossbars indicate the data flow direction. In contrast, our architecture (Figure 5(a)) consists of computation crossbars and dense crossbars. The com- putation crossbar executes both weight-stationary and non-weight-stationary multiplications, where weight matrices W1 to W3 are stored in the dense crossbar. This results in a much smaller area overhead than the traditional architecture, due to the high area efficiency of the dense crossbars.\nIn Figure 6, we present a breakdown of the computation process for both architectures across multiple stages. At the initial state, the computation crossbar is empty and contains no data information. To address this, we have developed a mechanism to instantly reconfigure the memory storage in the computation crossbar. For weight-stationary computation, the weight matrix Wi is transferred into the computation crossbar at Stage i. In total, three stages are required to execute all the computations for task Ta. For non-weight stationary multiplication, another operand Y needs to be transferred into the compute crossbar."}, {"title": "Efficient Encoding for the Sub-Operation", "content": "We have developed a mechanism that achieves instantaneous reconfigurability in the memristor crossbar. It is illustrated in Figure 7. In this approach, each memristor can exist in either an \"on\" or an \"off\" state. When in the \"on\" state, we can read a fixed data value, denoted as X, from the memristor. It is important to note that the value of x remains unchanged throughout the entire computation process. On the other hand, when in the \"off\" state, the memristor can only be read as 0. To perform any multiply-accumulate (MAC) operation, we employ an encoding technique where the weights serve as input, while the activations control the state of the memristors. The activation data needs to be encoded into multiple digits to enable digit-by-digit computation. In our example, we utilize the balanced septenary (base-7) numeral system for this encoding.\nIn the balanced septenary numeral system, each digit can take one of seven possible values: -3, -2, -1, 0, 1, 2, or 3. For instance, a given data value, let's say 78, can be encoded into three digits: 2, -3, 1. The expansion of 78 in the balanced septenary numeral system can be expressed as 2 \u00d7 72 \u2013 3 \u00d7 71 + 1 \u00d7 7\u00ba = 78.\nBy employing this type of encoding scheme, we can perform MAC operations efficiently, optimizing area overhead and energy consumption within the memristor crossbars. To cover all possible values of a single digit, we implement four memristors for each activation. These memristors are arranged as follows: two mem- ristor in the positive (left) column, storing 1 and 2, and two in the negative (right) column, also storing -1 and -2.\nTo illustrate the multiplication of a positive weight w\u2081 with the activation 78, we first obtain the three digits representing the value 78: 2, -3, 1. Each digit corresponds to a specific computing time step. In Figure 7, the top four memristors (colored purple) represent the activation 78. During step-0 of computation, only the memristors storing 2 are turned on, representing the first digit 2. The remaining memristors are turned off and indicating value 0. During step-1, the memristors storing -1 and -2 are activated, representing the second digit -3. The other memristors are turned off. Step-2 follows a similar rule, with the corresponding mem- ristors being activated based on the third digit 1. Between every two steps, we multiply the accumulated result by the base value 7 since the previously processed digit holds higher significance in the overall value. If w\u2081 is negative, we exchange the states of the memristors in the two columns, reflecting the sign change. By following this process, we can effectively perform multiplication operations between the weight and the activation values, utilizing the four memristors per activation to cover all possible digit values."}, {"title": "Robust Computation Crossbar", "content": "We introduce a computation crossbar that is compatible with the above encoding scheme. In this design, the memristors within the computation crossbar always store the same data value, re- gardless of the values of the activations. Once we establish the encoding format, there is no need to update the data stored in the memristors. This holds true even when we change neural network models. Consequently, there is no requirement to implement actual memristors and program them prior to usage. Instead, we employ regular resistors with fixed resistance to function as memristors, effectively storing the specified data. This approach enhances the resilience of the computation crossbar against random telegraph noise (RTN) [27]. Unlike memristor, the likelihood of defects occurring in the resistors is relatively low after undergoing post- fabrication examination [28]. Consequently, the resistors do not need to operate in the low-resistance mode [29] to counteract RTN, resulting in energy savings [30].\nWe have devised the computation crossbar that builds upon the classic 1T1R (one-transistor-one-resistor) design [31]. The structure of our computation crossbar can be seen in Figure 8(a). In addition to substituting memristors with conventional resistors, we have made four significant modifications to our design. These modifications are as follows:\n1) In the classical 1T1R structure [31], each memristor is con- nected in series with a transistor, serving as a control switch to regulate the current flow through the memristor. Typically, developers use this switch to enable or disable the program- ming functionality of the memristor during the computation phase [31]. In our new architecture, we retain the transistor- switch design, which is attached to the memristor/resistor, and utilize the transistor to control the on-and-off state of the memristor/resistor.\n2) In the balanced septenary (base-7) system, we utilize four resistors to handle each input data. These four resistors are placed into two rows (\u201ca\u201d, \u201cb\u201d) and two columns (\"+\",\"-\"). To optimize the switch control, we introduce dedicated registers that are directly connected to the memristor switches, storing the control information. The register responsible for storing the encoded data only requires three binary bits. One bit is used to select the column, while the remaining two bits control the resistors within the selected column. The resistors in the unselected column are effectively cut off or deactivated.\n3) In our computation crossbar design, we decompose the computation into multiple time steps. At each time step, the output needs to be multiplied by the base value of the encoding scheme before proceeding to the next time step. For instance, in the balanced septenary encoding system, the base value is 7. To optimize the processing time, we can perform the multiplication by 7 in two steps. First, we utilize shifting operations to the original value by three bits, which effectively multiplies the output by 8. Then, we subtract the original value from this result to obtain the final multiplication by 7. Other base values can use similar rules to optimize because all of them can be expressed as  \u03a32\u2212i. These operations are executed within shift-and-add (S+A) units, whose energy consumption is relatively small compared to other components in the system [9]\n4) An module for processing additional function F is imple- mented at the end of the linear computation. This function can be implemented using either a digital circuit or an analog circuit. The analog circuit also offers well-established solu- tions for basic operations such as exponential, multiplication, summation, and more [32][33][34]. Similar to the ADC, the peripheral module for function F is shared among the columns in the crossbar in an interleaved manner [9]. Hence, its overall impact on the area cost is minimal.\nAn encoder is employed to convert the activation from the original binary system into the new encoding system. Our design supports any type of balanced numeral system for encoding. In general, each activation can be encoded using 2S resistors within a balanced base 2S+1 - 1 system. The scaling factor, represented by S, is a crucial parameter that influences the characteristics of the encoding. We should choose the right value of S based on the required precision of the activation. In this example, S = 2 is utilized, representing the balanced septenary (base-7) encoding system. In Section 6 of our study, we will conduct a comparative analysis of different encoding schemes, ranging from S = 1 to 7, in order to identify the ideal encoding base value while considering a specific precision requirement for activation data."}, {"title": "Dense Crossbar with High Capacity", "content": "Our system is specifically designed to support large-scale neural network models by utilizing an additional crossbar with a substan- tial storage capacity. This new hardware is referred to as the \"dense crossbar\" due to its high density of memristors. The capacity of the dense crossbar is easy to accommodate the size requirements of various neural network models, due to the high density of the memristors. The structure of the dense crossbar closely resembles a traditional memory design, as depicted in Figure 8(b). There are two significant features:\n1) Low-resolution DAC and ADC: Both the Digital-to-Analog Converter (DAC) and Analog-to-Digital Converter (ADC) employed in our system have low resolutions of 1 bit and 2 bits, respectively. The DAC functions by enabling or disabling the entire column of memristors, while the ADC incorporates a sense amplifier to recover the signal. Utilizing low-bit DAC and ADC resolutions significantly enhances energy efficiency and reduces the required area compared to higher-resolution alternatives [25].\n2) Individual column activation: At any given time, only one column of memristors is activated within the dense crossbar. This means that the current flowing through one memristor does not interfere with the current from other memristors in adjacent columns. This unique characteristic enables accurate data retrieval from large-scale crossbars without signal interference or degradation.\nBy incorporating the dense crossbar and computation crossbar within a single chip or package, we eliminate the inefficiency of off-chip communication. This design enables energy-efficient deployment of large-scale neural network models, particularly extremely large language models. Furthermore, our dense cross- bar provides comprehensive support for various configurations of memristors in any number of bits. In the specific example illustrated in Figure 8(b), we assume that each memristor can store two bits of information. This particular configuration represents a balance between the complexity of ADC and the additional area needed for implementing memristors.\nOur architecture is compatible with a wide variety of memris- tors types. Memristors are implemented within the dense crossbar and arranged as a traditional memory bank. Considering the robustness of the dense crossbar in this particular organizational structure, the resolution and accuracy requirements for the mem- ristors are relatively flexible. Therefore, our architecture can also support future advanced memristors with greater performance."}, {"title": "Computation Process of Sub-Operation", "content": "We illustrate the computation process of the sub-operation in Fig- ure 9, depicting the sequential steps involved, assuming the linear part of the sub-operation is a weight-stationary multiplication. We use register Wi to store the weight, register Xi (including Xia at row a and Xib at row b) to store the activation and register Z to store the computation result. The locations of these registers in the computation crossbar are shown in Figure 8(a). In this example, we adopt the balanced septenary (base-7) encoding system. Each activation Xi,j (highlighted in red) is encoded into three digits. For each digit, we utilize four memristors. Two memristors in the positive column (storing 1 in row a and 2 in row b) and two in the negative column (storing -1 in row a and -2 in row b). The states of the four memristors are denoted as a\u2021, a, b, and 6, where i represents the index of the digital, and a/b along with +/- denote the location of the memristors. As three digits are required to encode each activation xi,j, a total of 4 \u00d7 3 = 12 memristor states are required. During computation, we load the 12 memristor states of activation xi,j into the respective registers Xia and Xib digit by digit.\nAs depicted by Equation (4), our proposed approach partitions the computation of sub-operations into multiple sessions. In ses- sion t, we load multiple weights from the dense crossbar to the computation crossbar. For instance, weight W1,t is loaded into Wi and sequentially multiplied with activations X1,1, X2,1, and so on. As an example, the system performs the multiply-accumulate (MAC) operation by adding the product of w\u2081,t and X1,1 to other multiplication products. The column generates outputs 21,t, Z2,t, and so forth. They are passed to the additional function block, denoted as F, and we obtain the final result F(21,t), F(22,t), and so on. Once all possible activations have been traversed in session t, session t + 1 begins. New weights are loaded from the dense crossbar to the computation crossbar and the same sequence of activations from session t is repeated. This process continues for subsequent sessions.\nTo expedite the computation time, we have employed a du- plication technique for the computation columns associated with the same set of weights. This duplication approach significantly enhances the level of parallelism within each session. For instance, as illustrated in Figure 8(a), concurrently, we can calculate the multiplication between weights from Wi and activations from another activation register X in the second column of the compu- tation crossbar. We load activations with even index X2i,t into X1a and X16, while activations with odd indices X2i+1,t are loaded into X1a and X16. This approach can reduce the processing time for each session by half. If the crossbar allows the implementation of de columns, the processing time of each session can be further reduced to 1/de of the original value.\nOur architecture is also capable of performing non-weight-"}, {"title": "Cache Management System", "content": "To temporarily store the intermediate results of the model infer- ence, we need to implement five caches named Cache D1, D2, T1, T2, and S. When executing the multi-head attention (MHA) layers, T1 and D2 are used to store the input and output data, while caches T1 and T2 are employed to store temporary results. Cache S, on the other hand, is utilized to store the softmax result. When executing the"}]}