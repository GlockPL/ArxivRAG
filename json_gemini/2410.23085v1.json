{"title": "S3PT: Scene Semantics and Structure Guided Clustering to Boost Self-Supervised Pre-Training for Autonomous Driving", "authors": ["Maciej K. Wozniak", "Camille Maurice", "Hariprasath Govindarajan", "Ravi Kiran", "Marvin Klingner", "Senthil Yogamani"], "abstract": "Recent self-supervised clustering-based pre-training techniques like DINO and CrIBo have shown impressive results for downstream detection and segmentation tasks. However, real-world applications such as autonomous driving face challenges with imbalanced object class and size distributions and complex scene geometries. In this paper, we propose S3PT a novel scene semantics and structure guided clustering to provide more scene-consistent objectives for self-supervised training. Specifically, our contributions are threefold: First, we incorporate semantic distribution consistent clustering to encourage better representation of rare classes such as motorcycles or animals. Second, we introduce object diversity consistent spatial clustering, to handle imbalanced and diverse object sizes, ranging from large background areas to small objects such as pedestrians and traffic signs. Third, we propose a depth-guided spatial clustering to regularize learning based on geometric information of the scene, thus further refining region separation on the feature level. Our learned representations significantly improve performance in downstream semantic segmentation and 3D object detection tasks on the nuScenes, nulmages, and Cityscapes datasets and show promising domain translation properties.", "sections": [{"title": "1. Introduction", "content": "Recent research has focused a lot on improving the understanding of vehicle surroundings to increase the safety and autonomy of the vehicles. However, with constant updates to sensor types, setups, and differences between vehicles, it is highly challenging to create one perception system that will achieve high performance regardless of the inference environment, vehicle or sensor setup. While there are many causes, we can think of weather conditions, sensor setup or driving environment that differ during inference and training. This problem is highly relevant for autonomous driving (AD) companies, that iterate between different sensor setups and car models. Another motivation is the challenging scenario when different sensors are available during the training compared to inference. Most modern consumer cars (inference) are equipped with various cameras but do not have LiDAR due to design and cost barriers, while data collection vehicles (training) typically have it for accurate 3D data collection and annotation. Leveraging the full data collection sensor suite to create optimal models on the target sensor setup is a problem with many highly relevant practical applications, particularly in autonomous driving.\nOne promising solution to this problem is self-supervised pre-training, which has proven to be effective at learning informative representations which are transferable to a range of downstream tasks and generalize across different datasets in a similar domain [28]. While these methods easily outperform supervised baselines on image recognition tasks, they are not as strong at dense prediction tasks such as image segmentation and object detection, especially in more challenging settings such as autonomous driving [11]. There is only a limited research focus on developing self-supervised pre-training strategies to learn representations suitable for dense prediction tasks by pre-training directly on complex scene data [4,32,35,49,57,62]. These methods are also evaluated by pre-training on curated datasets such as COCO [41]. Training on driving datasets bring up other challenges such as limited diversity (mostly street view, quite repetitive) [45,53,59]. Additionally, those datasets are much smaller than ImageNet or COCO since it is much more costly to collect the data. Methods such as CrIBo assume and encourage uniformly sized object segments and a uniform distribution over the pseudo-labels assigned to these segments. This is in contrast to the distributions typically found in autonomous driving datasets, which do not focus on the caveats presented by self-driving car data, such as small, far away objects, heavy occlusions, and long-tail classes. In general, the applicability of such methods to autonomous driving datasets is not currently known.\nIn this work, we build upon the recently published CrIBo [35], and propose an improved version, S3PT, better adapted to autonomous driving datasets, showed in Fig. 1. Autonomous driving scenes contain a variety of object sizes and often have long-tailed distributions (e.g., motorcycles, cyclists, construction vehicles). Additionally, large areas of the image are road or background, making small objects like pedestrians or cyclists easy to miss. First, we incorporate semantic distribution consistent clustering to encourage better representation of rare classes such as motorcycles or animals in the learned features, as it is important to autonomous driving systems to correctly understand their surroundings. Second, we introduce object diversity consistent spatial clustering, to handle imbalanced and diverse object sizes and counts, ranging from large background areas to numerous small objects such as pedestrians and traffic signs. Finally, we propose a depth-guided spatial clustering by incorporating the depth information from LiDAR point clouds and find that this further improves learned representations by accounting for 3D scene structure, helping in 3D perception tasks as well as accurate 2D segmentation of occluded objects. We achieve this by directly using the depth information without requiring additional compute for a LiDAR encoder, as in multi-modal models.\nWe show that our improved representations perform well on downstream image-based 2D semantic segmentation and 3D object detection tasks. We demonstrate improved generalization capabilities through transfer learning experiments. We conclude by studying the scaling effect of pre-training on AD data and highlight the untapped potential that can be realized by improving data diversity."}, {"title": "2. Related work", "content": "Self-Supervised Pre-Training Contrastive learning methods have proven to be highly effective in self-supervised learning, popularized by SimCLR [13]. The usage of negative samples and triplet losses help mitigate the representation collapse issue but require large batch sizes, which can be managed with a memory bank [29]. Some approaches address this by avoiding explicit negative samples through asymmetry, such as additional predictors, stop-gradients, or momentum encoders [15, 27]. Clustering-based methods further prevent trivial solutions by regularizing sample assignments across clusters [1, 7\u20139, 39]. Another set of methods extend self-supervised pre-training to learn representations better suited for dense downstream tasks. These methods use the dense spatial features in their formulation. Local-level approaches contrast dense features [4, 35, 46, 56, 61, 64], while object-level approaches promote similarity between semantically coherent feature groups [17,31,50,58,61].\nCrIBo [35], builds upon DINO [9] and CROC [49], to enhance dense visual representation learning by employing object-level nearest neighbor bootstrapping. CrIBo showed state-of-the-art performance in various segmentation tasks. While these pre-training methods often show good performance when trained on large-scale curated datasets such as ImageNet [22] or COCO [41], their applicability to more challenging autonomous driving datasets is currently under explored, with the exception of the older work, MultiSiam [11]. ImageNet pre-trained models are used as initialization in autonomous driving but this transfer of models involves a domain shift and performs worse on tasks like traffic sign classification, which heavily deviate from ImageNet [28]. With this motivation, it is of interest to study and develop pre-training methods for autonomous driving that can generalize between different driving datasets. In this work, we investigate the recent advances achieved by CrIBo and propose S3PT- scene semantics and structure guided clustering self-supervised pre-training for autonomous driving data."}, {"title": "Self-Supervised Pre-Training in Autonomous Driving", "content": "Self-supervised learning in autonomous driving often focuses on either LiDAR [5, 10, 37, 48, 54, 59, 63] or camera [12, 63, 65], with fewer studies integrating both [24, 42, 44, 47, 52, 60]. Li et al. [40] introduced a method with a spatial perception module and a feature interaction module, aligning features using contrastive loss. BEVDistill [16] improved this approach with dense and sparse feature distillation, utilizing a bird's-eye view (BEV) feature plane for consistent representation. CALICO [52], an unsupervised solution, employs multi-stage contrastive losses and region-aware distillation to align BEV features from LiDAR and camera. Methods using both camera and LiDAR often rely on pre-trained image backbones. Our method, trained solely on autonomous driving data, excels despite the lack of diversity in datasets. It incorporates LiDAR or depth-predicted cues during learning and is entirely unsupervised. Our approach ensures robust camera features that work independently of LiDAR during inference, unlike many fusion methods that heavily depend on LiDAR data during both training and inference. We integrate LiDAR depth information directly into the model without needing a separate LiDAR encoder. This focus on unsupervised pre-training allows us to address the unique challenges of autonomous driving, both in research and in consumer vehicles, which rarely have LiDAR."}, {"title": "Handling Imbalance in Self-Supervised Learning", "content": "Self-supervised methods are usually pre-trained on well-curated datasets such as ImageNet [22] which contains object-centric images (one predominant object per image) and contains a uniform distribution over the object classes. The uniform distribution of visual concepts is particularly important for several methods. Contrastive learning methods such as SimCLR [13] and MoCo [14, 30] attempt to spread the features uniformly over the latent space [55]. Clustering-based methods such as DeepCluster [7], SwAV [8], DINO [9] and MSN [3] encourage a uniform distribution over the clusters or pseudo-classes. This is shown to have a negative impact when pre-training on long-tailed datasets [2]. Using temperature schedules [34] and explicit matching to non-uniform prior distributions, PMSN [2] are proposed to address this issue. Recent work indicates that using a clustering formulation based on the von Mises-Fisher distribution can learn reasonably from long-tailed data distributions [26], generally performing even better than PMSN. KoLeo-regularization of the prototypes is shown to further improve performance when pre-training on long-tailed data [26]. While recent methods such as CrIBo [35] have demonstrated excellent performance when pre-training from scratch on scene data, they still use ImageNet and COCO [41] datasets, which have more uniform distributions of objects."}, {"title": "3. Method", "content": "The DINO-family of methods [3, 9, 25, 38, 64] use the pretext task of assigning object-centric images to K latent classes. Given a dataset D, consider an image, $I \\in D$. Consider an encoder model with parameters $\\theta$, that produces an L2-normalized global [CLS] (class token) [23] representation $z = g_\\theta(I)$ such that $z \\in R^D$ and $||z|| = 1$. The probability of assigning an image to a latent class k under the assumption of a uniform latent class prior is formulated using a softmax operation: $P_k(z) = Pr(y = k|z) = \\frac{exp((W_k, z)/T)}{\\Sigma_{j=1}^K exp((W_j, z)/T)}$ where $W_k \\in R^D$ denotes the weights of the last linear layer, also known as prototype vectors and T is the temperature. We will refer to this as semantic clustering. The teacher produces the targets and the student is trained to match the outputs of the teacher. Then, the teacher weights are updated as an exponential moving average (ema) of the student weights.\nWhile DINO only used the global representation in its formulation (appropriate when each image contains a single object), CrIBo [35] extends it to scene data, by considering each scene as a set of objects. The goal is to learn a representation ${z,\\tilde{z}} = f_\\theta(I)$. Here, $\\tilde{z} \\in R^d$ is the global representation and dense representation $z \\in R^{H\\times W\\times d}$ with downsampled spatial dimensions H and W. CrIBo identifies the objects in a scene through a spatial clustering of the dense spatial features into M clusters to get object-level representations, $o_k \\in R^d$, obtained by mean pooling the spatial features within each cluster. These global and object-level representations are assigned to K different semantic clusters, to obtain probability distributions similar to DINO. The CrIBo loss objective is obtained as a combination of three DINO-based loss terms:\n$L_{CrIBo} = L_{cv} + L_{oci} + L_o$\\Each of these three loss terms are formulated as a cross-entropy loss between the teacher and student probability distributions, $H(P^{(t)}, P^{(s)})$. The three terms $L_{cv}$, $L_{oci}$ and $L_o$ correspond to global cross-view, object-level cross-view and object-level cross-image consistencies respectively.\nPre-training with CrIBo off-the-shelf on an autonomous driving dataset produces poor representations due to the inappropriate uniformity assumptions made regarding the data distribution over the semantic clusters and area distribution over the spatial clusters. As illustrated in Fig. 2, we propose a novel scene semantics and structure guided clustering (S3PT). The semantic clustering is improved by using a vMF normalized formulation [25] to improve the pre-training on long-tailed distributions over objects. The spatial clustering is improved to enable diverse object sizes by relaxing the cluster uniformity assumption and using a depth-guided clustering that incorporates the depth information of scenes to further enhance the spatial clustering of objects (see Fig. 4 for details). We discuss each of these contributions in detail below."}, {"title": "3.1. Semantic distribution consistent clustering", "content": "Autonomous driving (AD) data typically comprise of a long-tail distribution over the different objects, where some objects such as cars are frequent whereas other objects such as bicycles or motorcycles are relatively rare, as shown in Fig. 3. DINO-based methods assume and encourage a uniform distribution over the K semantic clusters [2], which matches object distributions in pre-training datasets such as ImageNet [22] or COCO [41]. However, this poses a challenge when pre-training on AD datasets. Using vMF normalized formulation of DINO enables more flexible cluster spread [25] and is shown to perform well when learning from such long-tailed datasets [26]. This formulation is obtained by using unnormalized prototypes and modifying the probabilities over the K pseudo-classes as follows:\n$P_k(z) = \\frac{C(||W_k||/T) exp ((W_k, z)/T)}{\\Sigma_{j=1}^K C(||W_j||/T) exp ((W_j, z)/T)}$\nand applying the centering operation in the probability space instead of the logit space as in DINO. Here, C(.) refers to the normalization constant of the vMF distribution. This enhances the semantic clustering by being consistent to the observed long-tail distribution [26]. We investigate the benefit of using this formulation in Section 4.1."}, {"title": "3.2. Scene Distribution consistent clustering", "content": "CrIBo [35] performs spatial clustering of dense features to identify the object regions. This is done by computing a cost matrix based on the dense teacher features. Then, the clustering problem is solved as an optimal transport problem by iteratively running the Sinkhorn-Knopp (SK) optimization algorithm [21] for multiple iterations. Running the algorithm for multiple iterations strongly encourages a uniform distribution of areas over the spatial clusters. This makes the implicit assumption that all the objects should be approximately equally sized. This is not a meaningful assumption for autonomous driving scenes which feature objects of diverse sizes in a scene roads and buildings are large regions whereas several objects of interest such as pedestrians, bicycles, traffic cones and animals are small in size. Also, we observe more objects per image in autonomous driving compared to other datasets. To this end, we propose to modify the spatial clustering to remain consistent to the object diversity observed in AD scenes. Firstly, we propose to use a much larger number of spatial clusters compared to CrIBo and then, we relax the strong uniformity assumption by limiting the number of SK iterations to just one iteration. We demonstrate in Section 4.1 that this enhances the spatial clustering through empirical analysis and improves performance on rarer objects."}, {"title": "3.3. Depth-guided spatial clustering", "content": "The spatial clustering is formulated as a joint-view clustering of 2P spatial tokens from the two views of an image into M clusters. This is done by solving an optimal transportation problem. Consider the transportation cost matrix $T\\in R^{2P\\times M}$. We modify this transportation cost to also account for the depth information, available through the LiDAR data as: $T = T^{(CrIBO)} + \\beta T^{(depth)}$. The depth-based cost corresponding to token i and cluster centroid j, with depths $d_i$ and $d_j$ respectively, is computed as $T_{ij}^{(depth)} = ||d_i - d_j||_2$. Since the LiDAR point clouds are sparse, we consider a depth completion approach to obtain depth information at all image pixels. This enables the spatial clustering to account for the scene structure made apparent through depth guidance."}, {"title": "4. Experiments", "content": "We conducted extensive experiments with longer training and different models (ViT-S/16 and ViT-B/16) to benchmark our proposed method with competitive baselines DINO [9] and CrIBo [35]. CrIBo is current SOTA method for SSL pre-training on complex scenes and an improvement of CroC [49], while DINO is one of the most popular image-based SSL approaches. All the models in this section are pre-trained in a fully self-supervised manner on the nuScenes [6]. ViT-S/16 models undergo 500 epochs of training, while ViT-B/16 models are trained for 100 epochs only due to computational constraints (see supplementary for full training setup). Our method, S3PT, not only performs well on the original datasets but also excels in domain adaptation and 3D object detection tasks, indicating that our method learns generalized and robust features, making it a versatile solution for various computer vision tasks."}, {"title": "Semantic Segmentation", "content": "In Tab. 2, we compare our models with DINO and CrIBo baseline methods. We freeze the backbone model and evaluate the learned features on the semantic segmentation downstream task using nuImages [6] and Cityscapes [20] datasets. We train the Mask Transformer and Linear probing heads for 160K iterations similar to CrIBo [35] (see supplementary for complete evaluation protocol). This setup allows us to assess the effectiveness of our method in learning robust features that generalize well across different datasets. With all the datasets and heads, we significantly improve over CrIBo, which is our main baseline. Interestingly, CrIBo outperforms DINO in linear probing but significantly underperforms DINO when using a more complex Mask Transformer head. We hypothesize that DINO features are more suitable for more complex segmentation models and are not as generic as S3PT. On nuImages, our model with the Mask Transformer head achieves mIoU lower than that of DINO, 55.70 vs. 58.19, but still significantly improving over CrIBo (50.31). However, in linear probing, our model significantly outperformed DINO, scoring 42.13 compared to 35.27, indicating superior raw feature quality. On the Cityscapes dataset, our model excelled, achieving the highest scores with both Mask Transformer head (55.41) and linear probing (37.32). This demonstrates our model's robustness and ability to generalize across different domains (we further explore this in Tab. 3). Furthermore, in Fig. 6 we can observe that in comparison to other methods, our approach significantly improves the segmentation of long-tail classes and distant objects (motorcycles or pedestrians), even though the majority of the image consists of background or surface. It also excels in segmenting distant or occluded objects, thanks to the enhanced spatial features."}, {"title": "Domain Generalization", "content": "To further test the generalization capabilities of our models, we perform domain transfer experiments in Tab. 3 between nuImages and Cityscapes. We use frozen backbone (trained with S3PT or baseline method) with segmentation head. Segmentation heads are trained on nuImages, then tested on Cityscapes and vice versa. S3PT demonstrates superior performance in these domain adaptation tasks, indicating that it learns more generic and effective features compared to other methods. Following the results we can observe that S3PT learns better spatial features than DINO or CrIBo and achieves higher performance across all metrics."}, {"title": "3D Object Detection", "content": "Finally, in Tab. 4 we evaluate the performance of our method on 3D object detection tasks. We use the frozen pre-trained backbones and fine-tune the PETR detection head, reporting the mean Average Precision (mAP) and nuScenes Detection Score (NDS). This comparison highlights the effectiveness of our method in learning transferable features that perform well across different tasks, including 3D object detection. Unlike other methods, such as DINO and CrIBo, our method considers the depth information that enables an improved understanding of the 3D scene geometry. This enables our method to learn representations that perform well not only in 2D segmentation tasks, but also in 3D object detection. Our model demonstrates superior domain generalization capabilities due to its ability to effectively learn spatial and object-specific features. This enhanced learning capability allows our model to perform better across different autonomous driving domains, making it a more robust and versatile solution for various applications."}, {"title": "4.3. Study on data diversity and training length", "content": "We evaluate the importance of pre-training length (in terms of number of epochs) and the size of the pre-training datasets. We consider smaller subsets of the nuScenes dataset by randomly sampling 25% and 50% of the sequences and using all the frames from all cameras in the selected sequences. The linear probing results are reported in Fig. 7. Although longer trainings are beneficial, we find that larger and more diverse datasets yield higher performance gains. While performance begins to plateau with extended training (right subfigure), there remains potential for improvement through the use of larger, more varied datasets (left subfigure, we can see that performance has not saturated yet). This can enable pre-training on autonomous driving datasets to not only improve further but also become the superior de-facto choice over using Imagenet pre-trained models for AD tasks."}, {"title": "5. Conclusions", "content": "We introduce S3PT, a novel scene semantics and structure guided clustering to improve clustering-based self-supervised learning methods pre-trained on scene data. Through extensive evaluation on a wide range of downstream tasks that depend on different types of information, we demonstrate that our proposed method learns effective feature representations that generalize across various tasks and autonomous driving datasets. Our experiments show improved representation of rarer objects on segmentation and 3D object detection tasks. Finally, we highlight the potential to significantly improve the performance by scaling the pre-training dataset size. We also could expect that in combination with domain adaptation method, S3PT would generalize even better. While this study focuses on autonomous driving applications, the results in terms of domain generalization and pre-training design choices can also have high impact on other applications domains."}]}