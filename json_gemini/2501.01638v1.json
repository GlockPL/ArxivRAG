{"title": "A non-ergodic framework for understanding emergent capabilities in Large Language Models", "authors": ["Javier Mar\u00edn"], "abstract": "Large language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.", "sections": [{"title": "1. Introduction", "content": "Current research in large language models have unveiled increasingly complex capabilities that emerge unpredictably at scale. These emergent capabilities, from complex reasoning to zero-shot learning, appear without explicit training (Ganguli et al., 2022). While observing these phenomena, we found patterns suggesting fundamental similarities with complex biological systems: capabilities emerged through sudden transitions rather than gradual improvements (Wei et al., 2022), model behavior showed strong dependence on context history (Press et al., 2021), and next-token predictions varied significantly based on the path taken to reach a particular state (Levine et al., 2022). These observations suggested that language models, like biological systems, might be fundamentally non-ergodic in nature. This insight led us to explore theoretical biology frameworks, particularly Stuart Kauffman's theory of adjacent possible (TAP), which describes how biological systems navigate their possibility spaces through restricted exploration (Kauffman, 2000). Similar to how a cell in a developing organism navigates a limited array of possibilities influenced by its environment and history rather than choosing its next state at random, a language model's next-token prediction arises from a complex interaction of collected patterns and existing constraints. This paper presents both theoretical proof of language models' non-ergodicity and a novel framework based on TAP that explains their emergent capabilities."}, {"title": "1.1. Current frameworks for emergent capabilities in language models", "content": "Actual advances in Al systems research primarily emphasizes empirical observations of increases in capability (Chowdhery et al., 2023; Wei et al., 2022) and scaling laws (Kaplan et al., 2020), still we lack a unified theoretical framework to explain the underlying mechanisms.\nSome studies approaching this challenge has shown the stochastic nature of these emergent properties. Research on next-token prediction (NTP) proves that these capabilities arise from intrinsically stochastic processes (Radford et al., 2019), challenging deterministic interpretations of model behavior. Analyses of probability spaces in language models, including softmax distributions (Holtzman et al., 2020), entropy in token predictions (Holtzman et al., 2020; Zhang, 2019), and temperature sampling effects (Meister & Cotterell, 2021), suggest that this emergence follows complex probabilistic patterns that current frameworks struggle to explain.\nAn important restriction in evaluating these emergent capabilities is the underlying assumption of ergodicity in current approaches (Basu et al., 2023; Ziemann et al., 2024). This assumption breaks down when observing how capabilities emerge through path-dependent processes. In language models, each new capability depends on the assembled context and previous token sequences, leading to different probability distributions for the same token in different contexts. This context-dependent behavior contravenes fundamental ergodic principles, which require time and ensemble averages to be equivalent (Cornfeld et al., 2012). The non-linear dynamics seen in language models (Ramscar et al., 2014) create a kind of \"memory system\" through the context window. This leads to path dependence and temporal asymmetry, which are qualities of non-ergodic systems (Katok, 1995). This fundamental feature of language models suggests that a framework that explicitly accounts for the historical path by which capabilities emerge is necessary to describe emergence rather than relying on simple averages of states."}, {"title": "1.2. Motivation for a biological-inspired approach", "content": "One of the most important differences between research in physics or disciplines such as theoretical biology and research in mathematics or computer science is that theoretical physicists tend to ignore everything that they consider irrelevant, focusing only on the fundamentals. When trying to describe a phenomenon, physicists do not take into account the observable details but rather try to find the fundamental laws that underlie it. This is why fundamental laws, such as conservation laws, symmetries or phase transitions are present in different fields, such as astrophysics, general relativity, particle physics, or quantum mechanics (Glattfelder, 2019). In this research, we aim to identify some fundamental laws that govern Al systems. To do this, we will apply some of the current fundamental laws of physics and biology to artificial intelligent systems, particularly large language models.\nSimilar to how a cell in a developing organism navigates a limited array of possibilities influenced by its environment and history rather than choosing its next state at random (Kauffman, 2000), a language model's next-token prediction arises from a complex interaction of collected patterns and contextual constraints. This idea suggests that some theoretical biology frameworks, particularly Stuart Kauffman's theory of adjacent possible or TAP (Kauffman, 2000, 2019) might offer a valid framework to understand how language models navigate their possibility spaces when predicting next token. In biological systems evolution is considered a process evolving within a set of constrained possibilities and random events. This evolution creates, without selection acting to do so, new \"adjacent possible empty niches\" which enable new possible directions of evolution. These paths are accessible based on the system's current state and guide future evolution by removing incompatible random explorations through selective exclusion (Longo et al., 2012). Longo & Mont\u00e9vil (2014) further develop this understanding through their theory of \"extended critical transitions\" arguing that biological systems perpetually operate in a critical state, continuously redefining their own phase space. They emphasize that biological systems don't just explore pre-existing possibility spaces, but actively create new dimensions of these spaces through their evolution (Goldenfeld & Woese, 2011). This idea connects with how language models generate next token, where each possible selection not only explores but also reshapes the space of possible continuations.\nThis biological approach to systems organization and adaptation has interesting connections with current artificial intelligence architectures. LeCun's H-JEPA framework -Hierarchical Joint Embedding Predictive Architecture- (LeCun, 2022) establishes a framework for autonomous intelligence that shows many similarities with the adaptive mechanisms of biological systems. H-JEPA builds hierarchical world models via prediction-based learning just like how living organisms form and maintain their organizational structure. H-JEPA emphasis on learning world models through prediction is consistent with theoretical biologist Stuart Kauffman's theory of how living systems explore their possibility spaces via restricted exploration (Kauffman, 2000).\nNovel research in Al systems has further strengthened these connections between biological systems and AI systems. For example Zador (2019) provides relevant insights into how biological neural networks' efficiency and sparsity could inform artificial systems design. Richards et al. (2019) explain how hierarchical learning in deep networks parallels biological development, showing similar behaviors in how both systems build increasingly complex representations. This biological inspiration extends to architecture design showing how principles from neuroscience can guide the development of more adaptive and efficient artificial neural networks (Hassabis et al., 2017). These findings illustrate how, while modern AI architectures are still far from biological complexity, they are beginning to reflect biological system behavior.\nAnother relevant research area in Al systems is continual learning, which clearly draws parallels between the ability of biological and artificial systems to acquire new knowledge while retaining existing capabilities (Wang et al., 2024). Research on catastrophic forgetting and solutions inspired by biological memory consolidation also provides valuable insights into how systems can maintain and expand their possibility spaces over time (Hadsell et al., 2020). Researchers have further shown how artificial systems can achieve the kind of open-ended learning observed in biological systems (Parisi et al., 2019). The biological perspective is likewise enriched by the work of Van de Ven & Tolias (2019), providing a theoretical framework for identifying diverse forms of continuous learning that closely resemble biological adaptation processes.\nThe intersection of these areas, from biological self-organization to modern AI architectures, suggests a deeper connection between how biological and artificial systems navigate their possibility spaces. This connection becomes clearer when we consider how language models explore their semantic spaces by combining learned patterns with contextual constraints in ways that are similar to living system's restricted creativity."}, {"title": "1.3. Empirical evidences in LLMs", "content": "Recent empirical research provides compelling evidence for how language models navigate their possibility spaces in ways that mirror biological systems' constrained exploration. Recent work presented at NeurIPS 2024 on in-context exploration in large language models demonstrates how these systems' exploration capabilities are fundamentally shaped by constraints similar to those observed in biological systems (Krishnamurthy et al., 2024). The finding that models need clear exploration hints and external memory support to exhibit strong exploration behavior fits with Kauffman's theory of how systems navigate their adjacent possible spaces (Kauffman, 2000).\nThe poster presented at NeurIPS 2024 by A. Krishnamurthy, K. Harris, D. J Foster, C. Zhang, and A. Slivkins makes a relevant observation of \"suffix failures\", where models fail to explore optimal choices even after initial exposure. This evidence suggests that, like in complex biological systems, language models operate within constrained possibility spaces where exploration is limited by both architectural and contextual factors. The need for external history evaluation corresponds with how biological systems needs ambient framing to increase their exploration ability. These empirical results support our idea by proving how different constraints in language models interact to shape the exploration of the next token space akin to the interacting constraints in biological systems that shape possibility spaces (Steel et al., 2020). Finally, the identified exploration failures provide evidence for the non-ergodic nature of these systems (Kauffman, 2022), where past trajectories fundamentally influence future exploration capabilities. The convergence con between theoretical predictions and empirical observations strengthens our intention for applying complex biological systems frameworks to analyze language model behavior."}, {"title": "2. Probabilistic spaces in language models", "content": "The foundations of probabilistic modeling in language trace back to Shannon's information theory (Shannon, 1948) and early statistical NLP (Church & Mercer, 1993). This framework initially treated language as a stochastic process where words could be predicted based on their statistical co-occurrence patterns. Modern language models have progressed past traditional static statistical methods, using dynamic, context-sensitive probability distributions via transformer architectures (Vaswani, 2017). This evolution represents a fundamental shift from considering language as an essentially statistical phenomenon to viewing it as a dynamic, context-dependent system.\nCurrent transformer-based models provide probability distributions that differ fundamentally from traditional statistical language models in several aspects like contextual dependency or sampling dynamics. Probability distributions are computed through complex attention mechanisms (Vaswani, 2017) capturing semantic uncertainty and ambiguity (Wang et al., 2022). The formation process involves complex interactions between attention heads (Elhage et al., 2021). Beyond basic temperature sampling, actual innovations include nucleus sampling scheme, top-p sampling (Holtzman et al., 2020), and top-k sampling (Fan et al., 2018). Meister & Cotterell (2021) observed that models learn \"only a subset of the tendencies\" rather than complete theoretical distributions. The success of these approaches unveil the non-uniform nature of the probability space."}, {"title": "2.1. Evidences of non-ergodicity in language models", "content": "Ergodicity in dynamical systems implies that a system, if left to itself for long enough, will pass close to almost all the dynamical states consistent with energy conservation. Though this is a very simplistic view to define ergodicity. A dynamical system may have a hierarchy of properties, each of which implies the one before it (Lebowitz & Penrose, 1973). Ergodicity is only the first. Central to understanding ergodicity is the notion of symmetry in temporal evolution (Birkhoff, 1931; Neumann, 1932). The invariance of statistical properties under time translation in ergodic systems reveals time symmetry; the system's behavior remains consistent whether observed now, at a later time, or in a forward or backward temporal direction (Ruelle, 2004; Walters, 2000). This temporal symmetry guarantees that time averages are equivalent to ensemble averages, a fundamental principle of statistical mechanics (Kullback, 1997). Non-ergodic systems break this symmetry, which fundamentally influences the system's future possibilities and creates distinct temporal phases that are impossible to average (Prigogine & Stengers, 2018).\nConsider how language models generate text: each token prediction depends not just on direct context, but on the entire sequence of previous tokens. Unlike classical ergodic systems, where future states are independent of the path taken to reach the current state, language models exhibit strong path dependence. A word appearing early in a sequence can fundamentally alter the probability distribution of all subsequent tokens. This creates an intrinsic asymmetry in time that violates the basic premise of ergodicity. For example, when a language model builds a story, the context and characters selected at the outset limit all potential scenarios. Past choices introduce semantic and logical constraints for coherence, preventing the model from exploring all possible story states. This is similar to how biological systems expand within constrained spaces, where every possibility during development bounds and shapes what might happen in the future. Language models show this non-ergodic behavior by using their attention mechanisms and analysis of context. The cumulative context window shapes the model's state space, creating what we might call 'semantic valleys' that guide and constrain predictions about future tokens. Although time and ensemble averages converge in ergodic systems, language models have persistent memory effects that make certain semantic paths more probable based on the past evolution.\nMeister & Cotterell (2021) observation that models learn \"only a subset of the tendencies\" rather than complete theoretical distributions provides clear evidence for non-ergodicity in language model (Cornfeld et al., 2012; S. Kauffman, 2022; Lebowitz & Penrose, 1973; Mazur, 1969). This would imply that models follows a constrained exploration, thus not operating in a fully ergodic space where all states are equally accessible (Peters, 2019). This aligns with Kauffman's theory of constrained possibility spaces (Kauffman, 2019). Language models, as complex biological systems, show preference for empirically observed patterns over theoretical possibilities (Kauffman, 2022). These models also capture path-dependent patterns emerging from actual language use. The fact that the probability space is shaped by training history rather than theoretical distributions creates a fundamental asymmetry in how models explore their possibility space (Castellani, 2003; Glattfelder, 2019; Marsden & Ratiu, 2013)."}, {"title": "3. Complex dynamics and emergence in language models", "content": "The evolution of probability distributions across tokens shows patterns that go beyond simple statistical dependencies. For example classical statistical measures like perplexity fail to capture emergent capabilities in language models (Wei et al., 2022). This limitation is not new in complex systems where reductionist statistical methods fail to capture emergent behaviors (Anderson, 1972)."}, {"title": "3.1. Limitation of classical statistics", "content": "In systems exhibiting non-linear behaviors resulting from the interaction of multiple parts or sub-systems, basic aggregation of probabilities fails to describe coherent, long-range dependencies. In LLMs, complex interactions between context layers create capabilities not predictable from individual components (Press et al., 2021). These capabilities often appear swiftly at certain scales (Wei et al., 2022), suggesting phase transitions in model behavior (Kauffman, 2022; Mazur, 1969; Stanley, 1971). In language models, probability distributions are likely to evolve through paths that preserve coherence throughout long sequences, suggesting they operate as complex adaptive systems CAS. In CAS non-linear behaviors take place from multiple interacting components showing a high sensitivity to initial conditions. These systems also presents self-organizing properties emerging at multiple scales."}, {"title": "3.2. Complex system dynamics and emergent behaviors in language models", "content": "The emergence of capabilities in language models suggests some characteristics common in complex adaptive systems as hierarchical organization and phase transitions (Stanley, 1971). Token-level interactions give rise to higher-order semantic structures where multiple scales of organization emerge simultaneously (Reed & Simon, 1980). These hierarchies resist reductionist analysis. LLMs demonstrates capabilities that emerge at certain model scales (Wei et al., 2022). These emergences imply phase transitions, revealing the presence of critical phenomena in model behavior.\nRecent research demonstrates that context significantly influences model behavior due to the creation of context-dependent representations through sequential processing (Press et al., 2021). Dynamic memory effects influence long-range dependencies (Ainslie et al., 2023; Pang et al., 2023), and context modifications show non-linear effects on model output (Levine et al., 2022). These mechanisms suggests an adaptive behavior where models seems to adapt to changing contexts. Furthermore, long-range coherence emerges from local interactions (Press et al., 2021) and self-organization appears at multiple scales (Wei et al., 2022)."}, {"title": "4. Complex adaptive systems and biological evolution", "content": "Complex adaptive systems, or CAS, are defined by some fundamental mathematical properties that discern them from simple dynamical systems (Gell-Mann, 1995; Holland, 1992). CAS create and apply internal models to predict the future, taking current actions according expected outcomes. This characteristic differentiates CAS from other types of complex systems, as well as making the emergent behavior of CAS more difficult to understand (Mitchell, 2009). When Haken (1989) coined the term \"synergetics\" he gave a very simple definition, referring to self-organizing systems (a property of CAS) as those characterized by the fact that the system finds its organization or function on its own, without direct external guidance (England, 2015).\nAnalogously, language models develop internal representations during pre-training capturing statistical patterns of language, semantic relationships, contextual dependencies, and domain knowledge. These internal models are encoded in the weights and attention patterns of the neural network (Vaswani, 2017). Internal models are able to generate next tokens based on predicted probability distributions using attention mechanisms to \"look back\" at context and then \"predict forward\". Model's decisions about token selection are based on these predictions that can also be changed based on the evolving context. As a result, language models capabilities \"emerge\" from interactions between different layers, attention heads, and learned patterns. During its training, language models adjust weights based on training text data, and during inference adapt their internal model to the specific context of the current conversation or task.\nAn important practical limitation of CAS is that they don't have a single governing equation, or rule, that controls the system (Holland, 1992). Thus a direct approach to analyze these systems is by evaluating its different properties as non-linearity, emergence, self-organization, and phase transitions (Crutchfield, 2012). Non-linearity implies that system's behavior cannot be derived from linear relations (Crutchfield, 2012). The following sections will elaborate on the meaning of emergence, self-organization, and phase transitions."}, {"title": "4.1. Emergence in complex systems", "content": "Emergence appears when complex behaviors arise from simple rules and interactions in a system (Anderson, 1972). The CAS theory (M. Cross & Greenside, 2009; Holland, 1992, 2006) together with synergetics (Haken, 1989) provide a comprehensible theoretical framework that can be used to study emergence. Mathematically emergence can be formalized through the interaction between fast and slow variables in a system. Slow variables are the high-level patterns that emerge and guide the system, meanwhile fast variables are the detailed, moment-to-moment changes in the system (Haken, 1973). We can consider these variables as the macroscopic parameters (Holland, 2006). In these systems, the link between order parameters and components is complex because multiple components (fast variables) influence, and sometimes, define the order parameters. This is known as the slaving principle, which results in the notion of circular causality. The limited order parameters govern the behavior of the individual components, whereas the components influence the behavior of the order parameters (Haken, 1989). Haken (1993) defined the slaving concept, which links both rapid and slow variables, as follows:\n$\\frac{d}{dt}= q = N(q, V, a) + F(t)$\nwhere q is the state vector (microscopic level variables), N is a nonlinear vector function, \u2207 is a grading operator acting on q, a represents control parameters, and F(t) denotes fluctuating forces that characterize the external or internal noise affecting the system. The transition from Equation 2 to a simpler parametric equation describing the system's collective behavior is not evident. The complete mathematical derivation, including all necessary assumptions and detailed proofs, can be found in literature (Crawford, 1991; Guckenheimer & Holmes, 2013; Kielh\u00f6fer, 2006; Troger & Steindl, 2012). In short, we need to find a time-independent solution q\u00ba for a specific set of control parameters. Then when the system operates near an instability point small perturbations around this solution can be analyzed. This allows us to sort the system's behavior into different time scales, identifying fast-decaying stable modes and slowly-evolving unstable modes that become critical near the instability point. In language model slow variables would be equivalent to the overall flow of a story in language generation and fast variables could be individual word choices."}, {"title": "4.2. Self-organization and phase transitions", "content": "In statistical mechanics, the underlying assumption behind the theory of self-organized criticality (SOC) is that a complex system will naturally organize itself into a state on the edge of two different regimes, without intervention from outside the system (Markovi\u0107 & Gros, 2014; Sornette, 2006). The mathematical formalization of self-organizing systems can be expressed through the concept of pattern formation and symmetry breaking (Cross & Hohenberg, 1993). Systems exhibit spontaneous pattern generation governed by equations of this type:\n$\\frac{\\partial u}{\\partial t} = DV^2u + f(u, \\alpha)$\nwhere u represents the pattern-forming field, D is a diffusion coefficient, and a is a control parameter. When a reaches critical values, the system undergoes spontaneous symmetry breaking, leading to pattern formation. Function u could represent for example temperature variations in thermal convection, or population density in ecological systems. In language models u could represent the distribution of attention weights, the activation patterns across layers, or the probability distributions over tokens. Equation 2 is divided in two terms: $DV^2u$ calculates how the field changes over time from diffusion or spatial spreading, and $f(u, a)$ represents the local dynamics.\nPhase transitions are another fundamental property of complex adaptive systems, marked by abrupt changes in system behavior at critical points (Cilliers, 2002). The universality principle categorizes different physical systems based on their behavior near critical points, leading to the emergence of universal scaling laws, also known as power laws (Amaral et al., 1998). Physical quantities follow power laws as systems approach critical points. At these phase transitions, key parameters like correlation length and susceptibility shows divergent behavior, characterizing the critical phenomena. The correlation length \u00a7 near a critical point, Tc, represents the scale at which a system's general properties begin to diverge from its main properties. It can be defined as ~|T - Tc-, where v is the critical exponent governing the divergence of the correlation length. The characteristic length scale \u00a7 diverges as the system T approaches a critical point Te with a negative exponent -v by following a power-law behavior.\nIn language models, self-organization arises through the emergence of coherent text structures from local token interactions, whereas phase transitions are characterized by sudden improvements in model capabilities at certain scales (Arnold et al., 2024; Nakaishi et al., 2024). The correlation between successive tokens follows power-law scaling near critical points, suggesting similar underlying mechanisms to phase transitions occurring in many natural phenomena."}, {"title": "4.3. Non-ergodicity in biological systems", "content": "The ergodic hypothesis articulates the notion that a point within a moving system, whether it be a dynamical system or a stochastic process, will eventually go through every part of the space in which it works, in a way that is both uniform and random (Walters, 2000). This suggests that we can infer the overall behavior of the system from the path taken by a representative point. Classical statistical mechanics relies on the ergodic hypothesis, which states that time averages equal ensemble averages (Lebowitz & Penrose, 1973). We can define an ergodic system as one in which, for any property A, the time average and ensemble average are equivalent: A = (A). In ergodic systems events occur quickly relative to an observation time Tint << tobserved. When the system may be evolving at a very slow rate too for an observer (Tint >> tobserved), the system enters in a non-ergodic state. The hypothesis that, given enough time, a system will explore its entire phase space implies that a system will eventually explore all accessible states with equal probability. Ergodic breakdown can be probed by either measuring the evolution of Tint for some properties at fixed tobserved, or by changing tobserved for fixed Tint (Bossen & Mauro, 2024).\nBiological systems challenge this assumption via two primary mechanisms: historical contingency, since the system's current state depends critically on its past trajectory and not only on current configuration (Gould, 1989), and through adaptive dynamics, where the whole space of possible states evolves as the system advances (Kauffman, 2019). These mechanisms can be formalized with the following equations:\nP(st + 1 st) + P(st + 1 s't)\n\u03a9(t + 1) + \u03a9(t)\nEquation 3 shows that, even when st and s't have the same energy, path-dependent transition probabilities P differs due to historical unfolding (Villani, 2009). Equation 4 shows the adaptive dynamic nature of the phase space of accessible states \u03a9. According these mechanisms, biological systems cannot be understood through the statistical ensemble averages (Marshall, 2011)."}, {"title": "4.4. The adjacent possible theory (TAP)", "content": "Theoretical biologist Stuart Kauffman worked to figure out fundamental principles that govern a specific category of non-equilibrium systems, particularly those involving coevolutionary self-constructing communities of autonomous agents (Kauffman, 2000). The adjacent possible theory or TAP appeared as an important advance for understanding how biological and other complex systems explore and expand their possibility spaces. In his book \"Investigations\" Kauffman presents this concept by initially considering an important question: How do biological systems perpetually generate novelty in an apparently limitless way? The solution lies in understanding how each current state of a system defines a collection of possible subsequent states-a concept he refers to as the adjacent possible. The adjacent possible suggests not every conceivable state, but specifically those states that exist just one step away from the present reality, unveiling the potential transformations that can arise from the existing organization (Kauffman, 1993). However, it is important to note that, in contrast to phase spaces in physics, each expression of an adjacent possible state generates new additional adjacent possible. In Kauffman words, \"The adjacent possible consists of all those molecular species that are not members of the actual but are one reaction step away from the actual\" (Kauffman, 2000).\nTAP provides a theoretical framework for understanding how systems can be both constrained by their current state and perpetually creative. It suggests that evolution, rather than exploring a fixed space of possibilities, indeed expands the very space of what is possible. This expansion follows what Kauffman defines as \"the laws of the construction of the possibilities of the biosphere\" (Kauffman, 2000). TAP also provides a mathematical framework for understanding non-ergodic evolution in biological systems (Kauffman, 2019).\nAccording TAP, complex systems evolution could be described with the following equation:\n$Mt+1 = Mt + \\sum_{i=1}^{M} \\alpha^i  \\binom{Mt}{i}$"}, {"title": "5. Application of TAP framework to language models", "content": "Conceptual framework introduced by TAP equation could be applicable to understand how language models could navigate their possibility spaces via restricted combinations of existing elements rather than through random search of all potential possibilities."}, {"title": "5.1. Mathematical framework", "content": "We need to define the necessary mathematical structures for mapping TAP to language models. Let (\u03a9, F, P) be the probability space associated with language model token predictions, where is the sample space of all possible token sequences, F is the o-algebra of measurable events (Ash & Dol\u00e9ans-Dade, 2000), and P is the probability measure generated by the model."}, {"title": "5.1.1. Model's state space", "content": "While biological systems operate in continuous state spaces, language models work with discrete token sets. Let Me represent the state of the system at time t. For a language model, we can define M\u2081 as a tuple (Vt, St, Pt) where V V is the active vocabulary subset at time t, St is the semantic state space at time t, and Pt: V \u2192 [0,1] is the probability distribution over tokens. For a mapping between continuous semantic representations and discrete tokens, we define the discretization operator De: St V. Formally, we can define St as a manifold in a high-dimensional space:\n$S = {s \u2208 R : V \u2192 R and D\u2081\u03bf\u03c6}$$\nwhere o is smooth and locally invertible. The dimensionality of St, dim(St) is the number of independent semantic features actively involved in token prediction at time t.\nIn language models, while the lexical space V is constrained by a fixed vocabulary or lexicon, the semantic space S reveals a complex system with hierarchical organization (Simon, 2012). Just as a book represents a hierarchy from words to complete narratives, language models process and generate language across multiple hierarchical levels: starting from individual tokens as elementary units to phrases, clauses, sentences, paragraphs, and even broader narrative structures. These models create this organization through attention mechanisms and contextual relationships (Schlag et al., 2021), where each level emerges from combinations of lower-level elements. The possibilities for these combinations expand as we move up the hierarchy. This is analogous to the slaving principle defined by Haken (1973) where fast variables (lower-level elements) generates slow variables (high-order elements)."}, {"title": "5.1.2. Computational resources space", "content": "We can define computational resource utilization as the vector-valued function C\u2081 = (Memt, At, Ht), where Mem, \u2208 R\u207ais the model memory use, At \u2208 R is the attention computation cost, and HER+ the hidden state computation cost at time t. The total computational cost can be represented as:\n$R(C) = min\\frac{(1, Cmax-Ct)}{||C||}$$\nwhere ||C|| = W\u2081Memt + W2At + W3Ht is a weighted norm, Cmax is the maximum computational capacity, and w are weight coefficients for different resource types. Resource bound function R(Ct) connects to model architecture as follows:\n$R(C) = min (1, (C))$\nwhere ri(C) is the individual resource constraints from memory capacity, attention computation, context window size, and hidden state dimension."}, {"title": "5.1.3. Mapping TAP equation to language models", "content": "Lemma 1. Let V be the vocabulary space and S be the semantic space of a language model. There exists a measurable mapping\n\u03c6: V \u00d7 S - \u03a9\nthat satisfies the following properties:\nFor any state space M\u2081 E V \u00d7 S, the mapping preserves the combinatorial structure of Equation 6 from TAP:\n$Mt+1= (Vt+1,St+1, Pt+1)$\nwhere\n$St+1 = \u03c6(\\delta \u03b5) \u03c5\u03c6 (\\sum\\limits_{a}(\u039c\u03b5))}$$\n$Vt+1 = Dt(St+1)$\nand Pt+1: Vt+1 \u2192 [0,1]\nFor any x \u2208 V \u00d7 S, the mapping is bounded by computational resources:\n||(x)||2 \u2264 R(C)\nThe mapping preserves the dimensionality constraints:\ndim(Im()) \u2264 dim(V) \u00d7 dim(St)\nProof. Given the measurement space (V x S, B, \u00b5) where B is the Borel o -algebra, and u is the product measure (Ash & Dol\u00e9ans-Dade, 2000). We define o trough the composition q = \u03c0\u03bf\u03c5, where \u03c8 is the attention mechanism, and n is the projection onto the probability simplex. The mapping o preserves TAP equation structure:\n\u03c6(\u039c\u03b5 + \u0394\u039c) = \u03c6(\u039c\u03b5) + \u03a0\u03c6(\u039c)\u0394\u039c + 0(||AM||\u00b2)\nEquation 14 illustrates that when we make small changes to the model's state, the resulting changes in the mapped space are well-behaved and predictable - they consist mainly of a linear component plus some small higher-order corrections. This is central for showing that the mapping is compatible with how the TAP equation describes system evolution. The equation essentially establishes that o is differentiable and provides a Taylor expansion around Mt, which is necessary for proving the mapping preserves the mathematical structure needed for the TAP framework."}, {"title": "5.1.4. Resources limits", "content": "Lemma 2. For the mapping : V \u00d7 S \u2192 2 defined in Lemma 1, there exists a positive constant K such that:\n||||2 \u2264 KR(C)\u2200x \u2208 V \u00d7 S\nThe proof of this lemma can be developed in three basic steps, each dependent on the previous one in order to set a fitting bound. First, we are going to define the combinatorial framework of token prediction in large language models and its relation to TAP theory. Second, we will define an isomorphism between attention processes and the combinatorial space of TAP. Finally, we will verify resource boundedness through analyzing model capacity constraints.\nFirst step. Next token prediction in LLMs follows a combinatorial structure analogous to TAP. The probability of the next token given a context can be represented as:\nP(token+1 context) = \u03a3 Wig(tokens)\nwhere g denote the attention mechanism operations and w\u2081 are learned weights. This structure directly corresponds to the combinatorial summation in TAP.\nSecond step. The attention mechanism provides a natural isomorphism to TAP's combinatorial space. For any query Q, keys K, and values V:\nAttention(Q, K, V) = softmax  V\u03a3\u03af\u03b1 (M)"}, {"title": "5.1.5. Semantic space evolution", "content": "To model the semantic space evolution we have to extend Kauffman's idea of expanding possibility spaces by explicitly incorporating computational limitations. The evolving of semantic space can be modelled with the following equation:\n$\\frac{d}{dim(S)} = g^{-1}(t)dim(S_t)$$\nwhere dim(St) represents the number of dimensions the semantic space has at time t, and (t) is a decay term ensuring computational feasibility. The term \u2207g, captures how the hierarchical functions influences dimensionality at each level l."}, {"title": "5.2. Constraints in language models", "content": "Equation 6 provides a framework that can be naturally mapped to token space growth. We identify three key types of constraints that influence language model behavior: architectural constraints (Brown et al., 2020; Vaswani, 2017), training data constraints (Kaplan et al., 2020), and contextual constraints (Press et al., 2021). These constraints interact to"}]}