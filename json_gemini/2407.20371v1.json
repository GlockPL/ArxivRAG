{"title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval", "authors": ["Kyra Wilson", "Aylin Caliskan"], "abstract": "Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1% of cases and female-associated names in only 11.1% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have implications for widely used AI tools that are automating employment, fairness, and tech policy.", "sections": [{"title": "Introduction", "content": "One of the widespread practical applications of artificial intelligence (AI) tools has been their use in hiring processes. It is estimated that 99% of Fortune 500 companies are already using some sort of AI assistance when making hiring decisions (Schellmann 2024), due to their potential to increase recruitment quality and efficiency (Chen 2023). This could potentially alleviate discrimination based on people's unconscious biases or stereotypes, such as associating Black male job seekers with criminals (Pager 2003) or female job seekers with lower productivity due to motherhood (Gonz\u00e1lez, Cortina, and Rodr\u00edguez 2019). However, many AI hiring tools do still exhibit biased outcomes, such as a resume screening tool developed at Amazon which had to be scrapped when it was revealed that it unfairly discriminated against women (Dastin 2018).\nLarge language models (LLMs), which are trained on a general corpus of language data rather than data specific to hiring tasks, also have the potential to be used in these scenarios. In fact, resume screening tasks have already been observed in interactions between users and ChatGPT (Ouyang et al. 2023). Additionally, the accessibility of these models (both in terms of cost and user interfaces) lend them well to adoption by companies which have either not yet incorporated AI assistance into their hiring pipeline due to cost or technological complexity or to replace models that are currently in use and whose biases may be better understood.\nWith the potential for increased use of LLMs in hiring, it is essential to document the extent to which they exhibit biases against particular social groups. In the US, it is illegal to make hiring decisions on the basis of race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age (40 or older), disability or genetic information. A number of these biases have been documented in LLMs already, including gender, race, religion, and disability biases, as well as their intersections (Kotek, Dockum, and Sun 2023; Narayanan Venkit 2023; Kirk et al. 2021; Guo and Caliskan 2021). Therefore, it is crucial to investigate whether these models exhibit discriminatory biases related to protected attributes such as race and gender or their intersections in order to evaluate how they can be used for resume screening tasks.\nAdditionally, it is also essential to investigate low-level textual features such as term frequencies and document lengths which, although not direct signals of social groups, play a significant role in the performance and outputs of language models (Jones and Steinhardt 2022; Anil et al. 2022; Wolfe and Caliskan 2021). These features can vary widely across documents such as resumes, and evaluating their relationship to social group outcomes in automated resume screening is essential in order to accurately represent real-world usage.\nIn this study, we formulate resume screening as a practical zero-shot document retrieval task. Using this approach, we seek to address the following questions:\n\u2022 RQ1: Are identical resumes with different race (Black vs. White) or gender (male vs. female) sig-nals selected at equal rates when using three state-"}, {"title": "of-the-art LLMs for resume screening via a practical retrieval task?", "content": "\u2022 RQ2: Are identical resumes with different intersectional group signals selected at equal rates when using three language models for the same resume screening task?\n\u2022 RQ3: How do the features of race and gender signals such as name frequency and resume length impact screening outcomes?\nWe introduce a simulation for LLMs' usage as resume screening tools and analyze outcomes with respect to race and gender. In a hiring decision pipeline, resume screening is generally considered to be the second stage, after sourcing potential candidates, and prior to interviewing and selecting a candidate (Bogen and Rieke 2018). LLM assistance is particularly useful for this stage because it involves the analysis of many text documents (resumes) to identify those that are most relevant relative to a particular job description.\nWe investigate LLM-mediated resume screening using publicly-available English datasets of over 500 resumes and 500 job descriptions that are collected from real-world examples of these documents. They span nine occupations (Chief Executive, Marketing and Sales Manager, Miscellaneous Manager, Human Resources Worker, Accountant and Auditor, Miscellaneous Engineer, Secondary School Teacher, Designer, and Miscellaneous Sales and Related Worker). Resume screening is analyzed within three different Massive Text Embedding (MTE) models, a special class of LLMs which are trained for representational tasks such as document retrieval, classification, and clustering after pre-training on general language corpora. While many studies have characterized the biases of foundation or instruction-tuned LLMs, very few have investigated the biases of MTEs or their use in resume screening, adding further novelty and importance to this study.\nTo audit the MTE models for biases in resume screening scenarios, we augment resumes with 120 frequency-controlled names that are associated with White, Black, male, and female identities. Using document retrieval and selecting subsets of resumes which are most similar to job descriptions, we are able analyze whether outcomes differ across identity groups. The code and data produced by this research are available at https://github.com/ kyrawilson/Resume-Screening-Bias. We make the following knowledge contributions:\n1. In all of the models, we find that resumes that belong to the same occupation category as a given job description have significantly higher cosine similarities than resumes that belong to a different occupation category (0.0437 higher on average), justifying the use of cosine similarity to determine retrieval in order to analyze an initial stage of resume screening where a pool of the most relevant candidates are identified for further evaluation by a hiring professional."}, {"title": "2. Using more than three million comparisons between resumes and job descriptions, we find that resumes with White or male names are preferred to those with Black or female names in up to 85.1% of cases.", "content": "3. Intersectional comparisons reveal resumes that contain Black male names are highly unfavored in resume screening, with other groups being preferred in up to 100% of cases. Gender differences are driven largely by disparate preferences of Black females over Black males, as White males and White females have much smaller selection rate differences. These findings validate three hypotheses of intersectionality.\n4. Features such as resume length and name frequency significantly impact bias measurements in LLM resume audits, such that increasing the ratio of signals that are proxies to race or gender information in a document by decreasing its length can increase the number of biased outcomes by 22.2%, and changing frequency matching strategies can alter whether Black names or White names are favored in a majority of cases.\nFinally, we discuss how the resume screening patterns found in LLMs echo societal patterns of hiring discrimination and how the particular features of resumes such as names should be considered when using LLMs for resume screening."}, {"title": "Related Work", "content": "There has been limited work addressing and documenting the potential risks of using LLMs for hiring decisions, despite the documentation of biases in real-world resume screening scenarios and in AI models specially trained for hiring tasks. In economic labor market studies, bias in resume screening is usually investigated through resume audit or correspondence studies (Baert 2018). In these experiments, artificial resumes which differ only on some protected attribute are created and then sent in response to real job postings. Hiring procedures are determined to be discriminatory if the response rates differ significantly between groups that vary on the key dimension. This paradigm has been used to identify bias related to a number of protected attributes such as race (Bertrand and Mullainathan 2004), queerness (Mishel 2016), religion (Wright et al. 2013), disability (Hipes et al. 2016), and age (Neumark, Burn, and Button 2016, 2019; Lahey 2008).\nTo date, there are no external gender or racial bias audits of AI-mediated resume screening tools, the majority of which are typically closed-source, propriety, and not accessible for external review (Li and Goel 2024). Limited work has addressed this issue by reviewing publicly available statements and model descriptions (Raghavan et al. 2020; S\u00e1nchez-Monedero, Dencik, and Edwards 2020), finding that the majority of vendors do not make explicit statements regarding their models' compliance with anti-discrimination law, and those that do are typically only within a US"}, {"title": "context. Wilson et al. (2021), using a cooperative audit, found that the system of interest did not exhibit adverse impact, but key assumptions make this result difficult to generalize without additional testing. A final external audit found that closed-source models are often unstable, but they did not investigate any protected characteristics (Rhea et al. 2022).", "content": "Only very preliminary work has been done to investigate LLMs used for resume screening. A team of Bloomberg reporters investigated OpenAI's GPT-3.5 and GPT-4 and found that Black women were only ranked as top candidates for software engineering roles in 11% of tests, and Hispanic women were twice as likely as men to be ranked as top candidates for human resources workers. The only career in which no group was disadvantaged was retail workers (Yin, Alba, and Nicoletti 2024). Another study investigating OpenAI's ChatGPT found that resumes which mentioned disability in the context of an award were only ranked highest in 25% of cases (Glazko et al. 2024). While both studies demonstrate biased outcomes when using LLMs or chatbots as resume screeners, the models investigated were all \"black boxes,\" meaning the analysis was limited to model outputs only and could not investigate internal representations. Additionally, researchers did not rigorously investigate low-level document features such as term frequency or length in their studies, which are related to model biases (Esiobu et al. 2023).\nAnother limitation of studies investigating chatbots' use as resume screeners is that they are less transparent and rely on the model's generative capabilities for resume screening. Language generation is relatively computationally expensive as new tokens are produced iteratively, and outputs are also highly sensitive to features of the prompt which are unrelated to the task itself (Sclar et al. 2023). An alternative is to use embeddings of documents directly, which are less computationally intensive to process although still potentially sensitive to spurious prompt features."}, {"title": "Data", "content": "The present study examines race and gender bias in resume screening using a corpus of resumes and job descriptions as well as three MTEs (illustrated in Figure 1) with a Mistral-7B-like architecture (Jiang et al. 2023). Mistral-7B is based on the transformer architecture (Vaswani et al. 2017), but it innovates using grouped-query attention and sliding-window attention to decrease computational costs while also being able to process long sequences more effectively, a key innovation for the resume screening setting where long documents are common."}, {"title": "Massive Text Embedding Models", "content": "We select three high-performing MTEs to illustrate the potential range in outcomes from a set of architecturally similar models. The chosen models were fine-tuned for representational tasks (including document retrieval, classification, and clustering) on either Mistral-7B-v0.1, a foundation model whose learning objective is solely next-token prediction, or on models which were themselves fine-tuned on Mistral-7B-v0.1. In order to achieve high performance on these tasks, LLMs must undergo an additional round of training (fine-tuning) in which a specialized loss function is used to update model parameters. Typically, the loss function used for representational tasks is a contrastive loss (CL), in which embeddings of examples that are positive examples of a query or label are brought closer together spatially, while those that are negative examples are pushed further apart (Jones and Steinhardt 2022).\nE5-mistral-7b-instruct (e5) (Wang et al. 2023) is trained on Mistral-7B-v0.1 and fine-tuned using using a CL objective with natural and synthetically generated training data for retrieval tasks. GritLM-7B (GritLM) (Muennighoff et al. 2024) is also fine-tuned on top of Mistral-7B-v0.1; however, it has no synthetic data in its training set and uses a joint objective with both CL and next-token prediction in order to train for both representational and generative functions. Finally, SFREmbedding-Mistral (SFR) (Meng et al. 2024) is fine-tuned with CL on top of e5, which itself is already a finetuned MTE. Its training data includes both retrieval tasks as well as classification and clustering tasks.\nAll three MTEs achieve state-of-the-art performance according to the Massive Text Embedding Benchmark (MTEB), which aims to quantify the performance of MTES on a large set of representational tasks and datasets (Muennighoff et al. 2022). On document retrieval tasks, they are the highest performing open-source MTEs with at least one billion parameters\u00b9."}, {"title": "Job Description and Resume Stimuli", "content": "In order to simulate resume screening, a selection of job descriptions as well as candidate resumes are necessary. While examples of job descriptions are widely available"}, {"title": "Approach", "content": "The present study describes a generalizable and scalable approach to resume screening based on document retrieval. This involves using MTEs to create embeddings for job descriptions and resumes, and then using a simple cosine similarity comparison to capture which resumes are most similar to a given job description. A summary of this approach is given in Figure 2. A chi-square test is then used to determine whether the most similar resumes are distributed equally amongst relevant groups, or whether certain groups are favored over others, indicating bias. Chi-square tests require a minimum of five observed values for valid population estimates (Franke, Ho, and Christie 2012); our dataset far exceeds that, with at least 160 resume documents for every bias test, demonstrating the legitimacy of the results at scale."}, {"title": "Task-Augmented Job Descriptions", "content": "In document retrieval with MTEs, query texts are encoded with an additional instruction describing the particular setting. We created a set of 10 instructions, shown in Table 2, to be encoded with job descriptions according to templates specific to each model. We used ChatGPT in this procedure to develop alternatives for the phrases \"job description\" and \"resume.\""}, {"title": "Name-Augmented Resumes", "content": "To measure bias in resume screening, resumes were augmented with a name, comprised of a variable first name and constant last name, by prepending the complete name to the beginning of the document. Williams was selected as a last name because it is both frequent (third most common name in the US) and approximately equally likely to be used either by a Black or White person (47.68% vs. 45.75%) (Census 2024). The\n^{5}The text formatting used to encode instructions and query texts varies between MTEs. For each model, we followed the recommended structure as described in that model's documentation for these experiments."}, {"title": "last name was kept constant across all resumes in order to maximize experimental control and document realism while also minimizing required computation.", "content": "We use the name database introduced in Elder and Hayes (2023) to select names associated with one of four groups: Black males, Black females, White males, or White females. Of these, the Black male group had the fewest potential names, and the top 20 most distinctive names (33% of all Black male names in the database) were chosen for use in resume augmentation.\nAn equal number of names corresponding to other groups were then selected in order to closely match or be proportional to the corpus frequencies of the Black male names. Corpus frequencies were determined using infini-gram (Liu et al. 2024), a tool that facilitates n-gram searches for arbitrarily large corpora, and the DOLMA corpus (Soldaini et al. 2024).\nThe first set of names was selected in order to be proportional to the relative population differences between Black and White people in the US, replicating the distribution of names that would likely be seen in real-world resume screening. According to 2023 US Census estimates, those who identify as White alone comprise 75.5% of the US population, while those who identify as Black alone comprise 13.6%. Accordingly,\n^{6}Distinctiveness was measured via the difference in ratings from one to five between the most likely racial group versus the second most likely racial group, with all names having a score of at least 0.66.\n^{7}Although Mistral model weights are available publicly, the training dataset is proprietary. The DOLMA frequencies are meant only to approximate the frequencies of names in the corpus used to train Mistral. DOLMA contains 3.1 trillion tokens and is currently the largest available to search using infini-gram."}, {"title": "Resume Screening", "content": "Zero-shot dense retrieval, which uses contextualized embeddings to compare documents rather than exact term matches, provides a natural analog for resume screening. In the initial stages of retrieval, relevance scores computed from text embeddings are used to select a set of documents from a large corpus that best match a user query. Cosine similarity is commonly used as a relevance metric (Zhao et al. 2024). Similarly for resume screening, resumes r which are similar to a job description d can be identified via their respective embeddings $\\mathbf{v}_{r}$ and $\\mathbf{v}_{d}$ using the equation in (1). Furthermore, using a retrieval approach for resume screening allows for the direct analysis of textual embeddings to determine whether the representations are potentially biased in a way that could influence model outputs. If the resumes which are most similar to a particular job description consistently belong to a certain group, this is evidence that the representations are biased in favor"}, {"title": "of that group.", "content": "$\\operatorname{sim}(r, d)=\\left(\\mathbf{v}_{r}, \\mathbf{v}_{d}\\right)$\nEmbeddings for document retrieval were generated using the MTEs in Figure 1. Texts were truncated due to computational limitations, and 1,300 tokens was chosen as a length which captured the majority of resume content while still being computationally feasible. A summary of unaltered document lengths is available in the appendix. Document embeddings were extracted from the last hidden state of a model and normalized before computing their cosine similarity. Cosine similarity scores were averaged over task instructions, so the updated similarity computation is as in Equation (2), where $t$ corresponds to the index of the task instructions in Table 2 used to form job description embeddings. For completeness, results for individual task instructions are also provided in the Appendix.\n$\\operatorname{sim}(r, d)=\\frac{1}{10} \\sum_{t=1}^{10}\\left(\\mathbf{v}_{r}, \\mathbf{v}_{d_{t}}\\right)$\nTo simulate candidate selection, we select a percentage of the most similar of resumes for each job description for further analysis. A chi-square test is used to determine whether the selected resumes are distributed uniformly amongst relevant groups or whether particular groups are represented at significantly higher rates than others, indicating bias in resume screening outcomes. Results for resume screening outcomes are presented primarily in terms of difference in selection rates; intermediate cosine similarity results are provided in the Appendix."}, {"title": "Experiments", "content": "Detailed information is provided for four experiments. Experiment 1 evaluates the document retrieval for resume screening framework by comparing the similarity of resumes which belong to the same occupation category as a given job description to those that belong to different categories. Experiment 2 and 3 investigate bias in resume screening, first using gender and race categories separately, then investigating intersectional identities. Finally, Experiment 4 examines the effect of low-level features such as name frequency and resume length on bias measurements. In all experiments, the initial pools of resumes are balanced with respect to identity groups, and the expected outcome is that all groups should be represented uniformly if the MTEs are unbiased. Any significant deviations from this represent biased outcomes against particular identity groups in resume screening."}, {"title": "Evaluating Retrieval for Resume Screening", "content": "Cosine similarity of embeddings for resumes without names and job description embeddings was calculated for both resumes whose occupation category corresponded to the job description (matched) and those which did not (unmatched), simulating the initial stage of screening resumes for relevance. The cosine similarity scores of these two groups were compared to verify that the document retrieval approach is suitable for resume screening with LLMs."}, {"title": "Evaluating Race and Gender Bias", "content": "Gender and race groups were formed by combining names with population proportional frequencies from the four intersectional groups into four groups corresponding to only one race or gender identity (Black, White, male, or female). Each group was comprised of 40 names. Embeddings for job descriptions and name-augmented resumes were created using the three MTE models and cosine similarities were computed.\nFor each model and occupation, we performed a bias test by selecting the top 10% of most similar resumes for every job description and determining whether race or gender groups were represented at significantly higher rates. At this threshold, a minimum of 160 resumes were selected for each job description, and a total of 27 bias tests were conducted for both gender and race."}, {"title": "Evaluating Intersectional Bias", "content": "Using the 20 names with population proportional frequencies from each intersectional group (Black female, Black male, White female, White male), we repeated the embedding procedures, selection of top 10% of resumes, and 27 chi-square bias tests from Experiment 2 for each pair of intersectional identities, excluding those in which no race or gender dimension was shared."}, {"title": "Evaluating Effects of Length and Frequency", "content": "An additional set of name-augmented resumes was created in which the document contained a name and occupation title only (title-only) in contrast to the original name-augmented resumes which contained a name, occupation title, and additional content (full-length). Cosine similarities and bias tests were repeated as described above, and the results of using title-only versus full-length resumes were compared for non-intersectional race and gender categories."}, {"title": "Results", "content": "First, evidence indicates successful resume screening via retrieval. Additional evidence indicates underlying biases favoring resumes with White or male names when race and gender are analyzed independently. Intersectional analyses indicate this bias is strongest against resumes with Black female or male names. Finally, attributes such as name frequency and resume length also effect the similarity of resumes to job descriptions."}, {"title": "Verification of Retrieval", "content": "Resumes with no names belonging to the same occupation category as the job description have significantly higher cosine similarities (p<0.001) than resumes with no names belonging to different occupation categories for every occupation, as seen in Figure 3, verifying the use of document retrieval for resume screening."}, {"title": "White and Male Names are Preferred", "content": "The majority of experiments reveal a preference for White names over Black names. In 85.1% of 27 tests for racial bias in resume screening, White names were preferred, and only in 8.6% of tests were Black names preferred, as seen in Figure 4. Notably, the only cases in which Black-associated"}, {"title": "Intersectional Identities", "content": "Intersectional comparisons reveal that the smallest disparities exist between White names of different genders. Comparisons between Black names of different genders or Black and White names of the same gender exhibit larger disparities. Comparisons between resumes with White male and White female names reveal significant differences (p<0.05) between the two groups in only 44.4% of 27 tests, as shown in Figure 6. White males are only preferred in 18.5% of tests and White females are preferred in 25.9%. No model exhibits consistent behavior in preferring one group where other models do not.\nLarger differences appear in comparisons including Black names. Tests of resumes with White female names versus Black female names show a statistically significant preference for the former in 48.1% of cases and the latter in only 25.9% of cases (p<0.05), as shown in Figure 7. Conversely, tests of resumes with Black female names versus Black male names reveal a significant preference for the former in 66.7% of tests and the latter in only 14.8% of tests (p<0.05), as"}, {"title": "Shorter Resumes Result in More Bias", "content": "Overall, we find that resumes with names and title only (title-only) lead to more biased outcomes than those with names, titles, and content (full-length). For title-only race tests, significant differences are observed in 96.2% of 27 bias tests, compared to 93.7% of bias tests of full-length resumes (p<0.05). Of these, resumes with White names were significantly preferred in 62.9% of tests; Black names were preferred in 33.3%.\nFor title-only gender tests, significant differences are identified in 85.2% of cases, compared to 63% for full-length cases; this increase is entirely attributable to an increase of significant preferences for resumes with female names. Additionally, for both gender and race tests, the difference between group selection rates increases for title-only resumes. Detailed results for title-only resumes can be seen in the Appendix."}, {"title": "Frequency Effects Bias Measurements", "content": "Finally, the frequency of names also had a significant impact on outcomes. When names which had approximately equivalent frequencies in the DOLMA corpus were used, we find that resumes with Black names are preferred to those"}, {"title": "Discussion", "content": "The results of these experiments illuminate the potential for biased outcomes when using LLMs as resume screeners, as each of the three MTEs had outcomes which favor certain social groups over others. When analyzing race and gender independently, we find that the MTEs show an overall preference for resumes with White and male names, rather than preferences that align with societal patterns. For example, resumes with Black names were preferred only for the occupations Miscellaneous Managers and Secondary School Teachers; resumes with female names were preferred for Miscellaneous Engineers, Miscellaneous Sales and Related Workers, and Marketing and Sales Managers. In neither of these cases are the occupations those that have the largest proportions of workers belonging to the preferred group according to the BLS, and further investigation is needed to explain these preferences.\nThe lack of correlation between population statistics and model preferences suggests that group disparities in resume screening are a consequence of default model preferences rather than occupational patterns learned during training. This contrasts to work which finds evidence of occupational biases in LLMs (Kotek, Dockum, and Sun 2023), and instead aligns with research suggesting that LLMs have an"}, {"title": "an impact on LLM outcomes and may further disadvantage certain groups (Jones and Steinhardt 2022; Anil et al. 2022; Wolfe and Caliskan 2021).", "content": "While there are a number of factors contributing to biased outcomes in resume screening via LLMs, one naive approach to mitigation might be removing names from resumes altogether. However, resumes from real-world job seekers differ on many additional dimensions which can signal social group membership, including educational institutions, locations, and even lexical content choices. For example, Parasurama and Sedoc (2022) find that resumes written by women were more likely to use words like cared or volunteered, while men used words like repaired or competed, and these differences correlated with differences in hiring outcomes. While this study manipulated names alone due to their strong associations with certain protected characteristics, LLMs used for resume screening are likely sensitive to such signals stemming from additional sources as well.\nOther approaches to bias mitigation have identified methods which can minimize nuanced social group signals from words other than names (Deshpande, Pan, and Foulds 2020; Parasurama and Sedoc 2022). Additionally, other methods such as debiasing embeddings or reranking documents have also been proposed to counteract biases in hiring and retrieval settings (Gerritse and de Vries 2020; Sundararaman and Subramanian 2022; Parasurama and Sedoc 2021). However, these methods all rely on views of race and gender in which the ideal relationship between groups is one of sameness rather than difference. According to Drage and Mackereth (2022), addressing and solving inequities in resume screening mediated by AI or LLMs requires accounting for structural power imbalances that underpin the conceptualization and use of these tools.\nThe primary limitations of this study were data-related."}, {"title": "The resumes may diverge from those being used by realworld applicants due to the need to truncate them for computational feasibility. Furthermore, we relied on external tools to determine occupation categories for each document. The codes may not be as accurate as manual coding, which limits the conclusions which can be drawn for particular occupations.", "content": "The results presented here exemplify the risks associated with using LLMs for resume screening. They consistently replicate existing societal patterns of discrimination, further disadvantaging the groups already experiencing inequity in resume screening (Bertrand and Mullainathan 2004; Pager 2003). Additionally, screening outcomes are highly variable based on low-level features such as name frequency and resume length. Rather than LLMs having the potential to counteract people's unconscious biases, we find these are reinforced in ways that are unpredictable and difficult to control in a real-world resume screening setting. Therefore, policy and mechanisms to comprehensively audit resume screening systems, whether proprietary or open source, are essential in order to evaluate their fairness and improve or remove these systems accordingly."}, {"title": "Future Work", "content": "Given the novelty of using LLMs as resume screeners, there is still much work to do in documenting their potential risks and improving their transparency in ways that can potentially help to identify and reduce bias and discrimination in hiring scenarios. Areas for future research include assessing additional MTEs, increasing the diversity of social group signals in resumes, as well as the range of social groups investigated. This study was limited to analyzing only two of the most commonly studied race (White and Black) and gender (male and female) groups via associated names. Hiring discrimination is not limited to these groups or signals, thus it will be important to investigate additional groups in order to fully quantify the risks in using LLMs for hiring. Finally, investigating realistic variations in resume length is an important direction as well."}, {"title": "Conclusion", "content": "We proposed using retrieval to simulate resume screening via LLMs to investigate the potential for biased outcomes. We found that the models do not exhibit occupational biases, but rather reinforce societal \"defaults,\" such as the preference for White and male identities. Further intersectional analyses showed that Black males are at the greatest disadvantage, and three hypotheses of intersectionality were also confirmed in this setting. Finally, we investigated how features like resume length and name frequency can also impact biased outcomes."}, {"title": "Ethical Considerations Statement", "content": "Resume screening using LLMs can be potentially difficult to research and audit ethically. Of primary importance is the preservation of privacy and confidentiality when using documents, such as resumes, which contain large amounts of identifiable information. Researchers interested in transparency and reproducibility must negotiate tensions between the distribution and use of documents which accurately reflect signals of identity such as race and gender as they would be present in real-world resumes, while also preserving privacy of the candidates represented by the documents."}]}