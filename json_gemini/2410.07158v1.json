{"title": "quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond", "authors": ["Dilyara Bareeva", "Galip \u00dcmit Yolcu", "Anna Hedstr\u00f6m", "Niklas Schmolenski", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "abstract": "In recent years, training data attribution (TDA) methods have emerged as a promising direction for the interpretability of neural networks. While research around TDA is thriving, limited effort has been dedicated to the evaluation of attributions. Similar to the development of evaluation metrics for the traditional feature attribution approaches, several standalone metrics have been proposed to evaluate the quality of TDA methods across various contexts. However, the lack of a unified framework that allows for systematic comparison limits the trust in TDA methods and stunts their widespread adoption. To address this research gap, we introduce quanda, a Python toolkit designed to facilitate the evaluation of TDA methods. Beyond offering a comprehensive set of evaluation metrics, quanda provides a uniform interface for seamless integration with existing TDA implementations across different repositories, thus enabling systematic benchmarking. The toolkit is user-friendly, thoroughly tested, well-documented, and available as an open-source library on PyPi and under https://github.com/dilyabareeva/quanda.", "sections": [{"title": "1 Introduction", "content": "As neural networks become increasingly complex and widely adopted, the field of Explainable AI (XAI) has emerged to address the need for elucidating the decision-making processes of these black-box models [Longo et al., 2024]. Initially, research in XAI predominantly focused on feature attribution methods, which highlight features in the input space that are responsible for a specific prediction [Simonyan et al., 2014, Bach et al., 2015, Lundberg and Lee, 2017, Sundararajan et al., 2017]. However, limitations in these methods' reliability [Adebayo et al., 2018, Ghorbani et al., 2019, Bilodeau et al., 2024] and their lack of informativeness [Rudin, 2019, Achtibat et al., 2023] have prompted a surge in alternative approaches. One category of such methods is mechanistic interpretability [Bereska and Gavves, 2024], which aims to reverse-engineer neural networks by identifying and understanding the features and their connections within the model. Similarly, concept-based explainability approaches seek to explain model predictions via high-level, human-understandable concepts that the model uses during inference [Poeta et al., 2023].\nThis study puts its focus on the interpretability approach known as training data attribution (TDA) [Hammoudeh and Lowd, 2024]. Broadly, TDA methods aim to relate a model's inference on a specific sample to its training data [Koh and Liang, 2017, Yeh et al., 2018, Pruthi et al., 2020, Park et al., 2023, Bae et al., 2024]. We denote by D = {z\u2081, ..., zn} \u2208 Z\u207f an ordered set composed of n training samples, where each sample zi = (xi, Yi) \u2208 Z. Here, xi \u2208 \u211d\u1d48 represents the model input and yi represents the target variable. The space Z corresponds to the set of all possible input-target"}, {"title": "2 Related Works", "content": "The need to assess the reliability of explanations and select the most suitable TDA method for a given intent and application raises the critical question of how TDA methods should be effectively evaluated. As some of the methods are designed to approximate LOO effects, ground truth can often be computed for TDA evaluation [e.g., Koh and Liang, 2017, Koh et al., 2019, Basu et al., 2021, Park et al., 2023]. However, this counterfactual ground truth approach requires retraining the model multiple times on different subsets of the training data, which quickly becomes computationally expensive. Furthermore, this ground truth is shown to be dominated by noise in practical deep learning settings, due to the inherent stochasticity of a typical training process [Nguyen et al., 2023]. The model parameters after optimization are often sensitive to hyperparameters and initialization, introducing further uncertainty, as discussed in Appendix A of Bae et al. [2024].\nTo address these challenges, alternative approaches have been introduced to make the evaluation of TDA methods practically feasible. Downstream task evaluators assess the utility of a TDA method within the context of an end-task, such as model debugging or data selection [e.g., Koh and Liang, 2017, Yeh et al., 2018, Pruthi et al., 2020]. Heuristics evaluate whether a TDA explanation meets certain expected properties, such as its dependence on model weights or the variance of the explanations for different test samples [e.g., Barshan et al., 2020, Hanawa et al., 2021, Singla et al., 2023]. However, these evaluation strategies, developed independently by various researchers, often rely on distinct assumptions and implementational nuances that can affect their outcomes. As a result, the evaluation results reported in independent studies are frequently not directly comparable.\nThere exist well-established libraries in the broader interpretability space, like Captum [Kokhlikyan et al., 2020], Xplique [Fel et al., 2022], TransformerLens [Nanda and Bloom, 2022], InterpretML [Nori et al., 2019], and Alibi Explain [Klaise et al., 2021]. However, TDA has not seen much attention in terms of dedicated software packages. Out of the libraries listed above, only Captum provides a small number of TDA methods, but it lacks dedicated TDA evaluators. The Influenci\u00e6 library [Picard et al., 2024] offers implementations of TDA methods in TensorFlow [Abadi, 2016]. However, unlike quanda, it has a limited focus on evaluation and includes only a single evaluation benchmark. Similarly, PyDVL[TransferLab, 2024] provides influence function implementations with a focus on data valuation [Sim et al., 2022]. quanda addresses the need for an open-source, standardized evaluation procedure for data attribution. In this respect, quanda draws significant inspiration from previous work on feature attribution evaluation, notably Quantus [Hedstr\u00f6m et al., 2023] and OpenXAI [Agarwal et al., 2022]. quanda, in the vein of these two libraries, was designed to unify disparate evaluation strategies and to provide benchmarking tools for systematic comparison of different techniques."}, {"title": "3 Library Overview", "content": "quanda is designed to facilitate the evaluation of data attribution methods for practitioners, model developers, and researchers. \nThe following sections describe the library's core components, key features, and API design. The first iterations of the library mainly focus on (but are not limited to) image classification and post-hoc data attribution. As such, quanda currently supports attributing decisions of a single model, as opposed to computing average attributions for different instantiations of the model architecture as sometimes done in the literature [K and S\u00f8gaard, 2021, Park et al., 2023]."}, {"title": "3.1 Library Design", "content": "The design philosophy of quanda emphasizes creating modular interfaces that represent distinct functional units. The three main components of quanda are explainers, evaluation metrics, and benchmarks. Each component is implemented as a base class that defines the minimal functionalities required to implement a new instance. The design of the base classes prioritizes easy extension and integration with other components. \nExplainers An Explainer is a class representing a single TDA method. An Explainer instance maintains information about the specific model architecture, model weights, training dataset, and in some cases the training hyperparameters, such as the loss function. As the initialization of a TDA method can be computationally intense , it is delegated to the initialization of Explainer instances. Attribution of a test batch is then performed on-demand with the explain method, while the self_influence method returns a vector of self-influences of training samples. quanda also provides functional interfaces to the explain and self_influence methods, encapsulating the process of initializing and using the explainers in a single function.\nMetrics A metric is a method that summarizes the performance and reliability of a TDA method in a compact representation, typically as a single number. Three categories of Metric classes can be found in quanda: ground_truth, downstream_eval and heuristics. To provide flexibility and efficiency, quanda adopts a stateful Metric design that supports the incremental addition of new test batches via the update method. This allows the user to directly evaluate precomputed explanations, or to conduct the evaluation process alongside the explanation process. The final metric score is obtained by calling the compute method after the Metric instance has been updated with the explanations.\nBenchmarks A quanda benchmark is a combination of a specific model architecture, model weights, training and evaluation datasets, and a metric with its arguments, similar to the definition from Raji et al. [2021]. Benchmark classes enable standardized comparison of different TDA methods. The library interface supports the initialization of Benchmark instances by using user-provided assets with the assemble method, creating controlled settings with the generate method, and loading pre-configured benchmarks that are made available with the download method. The precomputed benchmark suites allow users to speed up the evaluation process by using datasets and models that are already properly set up for each specific metric. An Explainer class implementation with a set of hyperparameters can be evaluated with a call to the evaluate method to compute the associated metric score."}, {"title": "3.2 Maintaining Library Quality and Accessibility", "content": "We ensure high code quality through thorough integration and unit testing with pytest, reaching 90% test coverage at submission time. To maintain style consistency and compliance with PEP8 conventions, the static type checker mypy, the automatic code formatter black, the styling tool isort, and the linter flake8 are used. Each pull request triggers a full test run, coverage, type, and linting checks through a continuous integration (CI) pipeline on GitHub.\nThe quanda toolkit leverages widely-used libraries such as PyTorch Lightning, HuggingFace Datasets, torchvision, torcheval and torchmetrics to ensure seamless integration into users' pipelines while avoid-ing the reimplementation of functionality that is already available in established libraries. The Explainer classes provide wrappers for existing PyTorch implementations of TDA methods, including those from Captum [Kokhlikyan et al., 2020] and the official TRAK implementation [Park et al., 2023]. We plan to expand the number of supported TDA method wrappers in the future.\nIn designing the quanda library, the primary focus is to ensure ease of use by providing clear and consistent APIs. Comprehensive documentation, installation guides, and extensive tutorials are provided to help users navigate the library for attribution, evaluation, and benchmarking. The library documentation is available at the following URL: https://quanda.readthedocs.io.\nThe library source code is made public as a GitHub repository and a PyPi package under an open-source license. The benchmarks used in the library are also made available under this license and can be easily loaded using the provided tools. We encourage bug reports and contributions through GitHub and offer detailed guidelines to facilitate collaboration and community engagement."}, {"title": "4 Conclusion and Future Plans", "content": "Despite the rising interest in training data attribution (TDA) methods within the interpretability community, the lack of comprehensive evaluation tooling has hindered broader adoption. quanda directly addresses this gap by providing a versatile open-source Python library designed to streamline the evaluation of TDA methods. In addition to offering a suite of metrics and ready-to-use controlled setups to speed up complicated evaluation processes, quanda provides unified and consistent wrappers for existing implementations of major TDA methods, which are currently scattered across repositories.\nWith quanda, we aim to significantly lower the barrier to entry for researchers applying TDA methods, thereby promoting TDA methods as valuable tools within the interpretability community. In future releases, we plan to extend quanda's capabilities to additional domains, including natural language processing. Our active development efforts will also continue to focus on integrating more TDA method wrappers, metrics, and tasks, releasing new benchmarks, as well as building our own implementations of TDA methods."}, {"title": "A Notation", "content": "We consider an ordered training dataset of size n, denoted as D = {zi}\u1d62=\u2081 \u2208 Z\u207f, where each training sample zi \u2208 Z represents an individual datapoint. In the context of classification tasks, a datapoint zi consists of a pair zi = (xi, Yi), where xi \u2208 \u211d\u1d48 is the input sample and yi \u2208 [C] is the corresponding label. Here, C \u2208 \u2124 denotes the number of classes, and [C] represents the set {1, 2, ..., C}. For test samples z = (x, y) \u2208 Z, the label y indicates the output of the network that was used for attributions.\nThe model we aim to explain is assumed to be a trained neural network, which implements the function f( \u00b7 ; \u03b8), where \u03b8 denotes the set of learned parameters of the model. We denote the empirical risk:\nJ(\u03b8, D) = \u2211\u1d62\u208c\u2081\u207f L(zi, \u03b8),\nwhere L(\u00b7, \u00b7) is a sample-wise loss function, such as the cross entropy loss.\nAccording to the assumptions of certain TDA methods , we consider the penultimate layer of the model as a feature extractor, denoted by h( \u00b7 ; \u03b8) : \u211d\u1d48 \u2192 \u211d\u1d56, with parameters \u03b8. This is followed by a linear classifier characterized by a weight matrix W \u2208 \u211d\u1d56\u00d7\u1d9c and a bias term b \u2208 \u211d\u1d9c. Consequently, we can express the model output as f(z; \u03b8) = f(z; \u03b8, W) = W\u00afh(z; \u03b8) + b. For brevity, we will denote the final hidden features as h(z) and the model output as f(z) in the following sections. In the context of classification tasks, the model output corresponds to the value associated with the correct class. The attribution for a test sample z, computed by a TDA method, is denoted as \u03c4(z, D)\u1d62 for the training sample zi."}, {"title": "B Training Data Attribution Methods", "content": "In this section, we present the details on TDA method implementations currently included in quanda. We provide a theoretical definition for each method, along with insights into their specific implementations and the original codebases that quanda wraps in its Explainer interface."}, {"title": "B.1 Similarity of Representations", "content": "Gaining insights into what makes two samples similar from a model's perspective can aid practitioners in understanding the model's underlying reasoning and potentially detecting unwanted behaviours . The similarity between hidden representations of a given sample and those of training samples can be framed as an attribution method :\n\u03c4SIM(z, D)\u1d62 = \u03c3(h(z), h(zi)),                                                                                           (1)\nwhere \u03c3(\u00b7, \u00b7) is a similarity measure such as the dot product or the cosine distance.\nThis explanation method is implemented as SimilarityExplainer in Captum\u00b9. In quanda, we provide a wrapper around this Captum implementation."}, {"title": "B.2 Influence Functions", "content": "Influence Functions (IF) [Koh and Liang, 2017] is a TDA method that approximates the counterfactual effect of retraining the model after removing a single training sample from the dataset. The approximation is computed by performing a single Newton optimization step on the counterfactual loss landscape, which results in the following attribution:\n\u03c4IF(z, D)\u1d62 = \u2207\u03b8L(z, \u03b8)\u1d40H\u03b8\u207b\u00b9\u2207\u03b8L(zi, \u03b8),                                                                        (2)\nwhere H\u03b8 = \u2207\u00b2L(\u03b8, D) is the Hessian of the total loss.\nComputing the inverse Hessian is a computationally demanding task, and the approach introduced in the original paper [Koh and Liang, 2017] incurs a substantial memory footprint. To mitigate these challenges, recent work has proposed to speed up the inverse Hessian calculation using Arnoldi iterations . This method is named Arnoldi Influence Function in Captum. quanda provides a wrapper around this implementation\u00b2.\nTo generate a global ranking of training samples, such as for detecting mislabeled examples, [Koh and Liang, 2017] proposes evaluating the influence of each sample on itself, following Equation 2. In quanda, we also provide a wrapper around the Captum implementation to compute these self-influences."}, {"title": "B.3 TracIn", "content": "TracIn [Pruthi et al., 2020] builds on the idea of tracking how the loss on a test point evolves throughout the training process. The change in the loss for a test sample z caused by a training sample zi is approximated via a linear approximation:\n\u03c4TracIn(z, D)\u1d62 = \u2211\u209c \u03b7\u209c \u2207\u03b8L(z, \u03b8\u209c)\u1d40\u2207\u03b8L(zi, \u03b8\u209c),\nwhere t indexes the training epochs where the training sample zi was used, \u03b8t represents the parameters and \u03b7t the learning rate at epoch t.\nThe full TracIn attributor as defined in Equation 3 is prohibitively computationally expensive, as it requires storing the parameters at each training step. To address this, attributions are computed using only a selected set of checkpoints, a method known as TracInCP. Captum implements this approach\u00b3 with two separate extensions, as described in the original paper [Pruthi et al., 2020]:\n\u2022 TracInCPFast: This variant simplifies the process further by considering only the final layer parame-ters when computing the gradients.\n\u2022 TracInCPFastRandProj: This version additionally uses random projections to reduce the dimen-sionality of the gradients, thereby speeding up the computations.\nquanda provides wrappers for all the aforementioned variants\u2074."}, {"title": "B.4 Representer Points", "content": "The Representer Points method leverages a representer theorem for gradient-descent-based training of deep neural networks. Under the assumption that the model is trained to convergence with L2 regularization, they demonstrate that the final layer parameters W can be written as a linear combination of final layer features h(zi). This formulation reduces the model to a kernel machine, enabling a decomposition of the model output to the contributions of each training point. This results in the following attribution function:\n\u03c4RP (z, D)\u1d62 = \u2202L(zi; \u03b8) / \u2202f(z\u1d62) h(zi)\u1d40h(z).\nGiven a model that is not originally trained with L2 regularization, the Representer Points approach suggests training the final layer parameters W with L2 regularization, using backtracking line search to ensure convergence.\nquanda provides a wrapper around the official code release by the original paper's authors."}, {"title": "B.5 TRAK", "content": "TRAK is a method that approximates the counterfactual effect of retraining on a subset of training points. It achieves this by linearizing the model around the test point using a first-order Taylor decomposition of the model output. This approach corresponds to using the empirical Neural Tangent Kernel [Jacot et al., 2018] to define a surrogate for the model.\nTRAK is defined to use multiple independent instantiations of the model, trained on the same dataset, or different subsets of the dataset. Since quanda currently considers post-hoc attribution of a single model, we consider the case of using only a single model, whose behavior we want to attribute to the training data.\nLet G = [g\u2081; g\u2082; . . . ; g\u2099] be a matrix of gradients, where each column gi = \u2207\u03b8 f(zi) represents the gradient of the model output corresponding to a training point. Additionally, let Q be the diagonal matrix such that Q\u1d62,\u1d62 = 1 \u2212 p\u1d62, where p\u1d62 is the probability corresponding to the ground truth label of data point zi. In the binary classification scenario, TRAK can then be formulated as follows:\n\u03c4TRAK(z, D)\u1d62 = \u2207\u03b8f(z)\u1d40(GTG)\u207b\u00b9G\u1d40Q.\nTo extend the formulation to the multiclass case, instead of using the model output f(\u00b7), we consider the gradients of the function r(z) = log (p(z; \u03b8)/p(z)), where p(z; \u03b8) denotes the softmax probability corresponding to the ground truth label. For test points, this function represents the probability of the output that is chosen for explanation."}, {"title": "C Evaluation Metrics", "content": "In this section, we summarize the evaluation metrics that are currently implemented in quanda and provide references for related work."}, {"title": "C.1 Ground Truth Metrics", "content": "Ground truth metrics evaluate the attributions against the ground truth values that the respective TDA methods aim to approximate, e.g., the counterfactual effects of modifying the training dataset."}, {"title": "C.1.1 Linear Datamodeling Score", "content": "Proposed in , the Linear Datamodeling Score (LDS) evaluates the ability of a TDA method to make accurate counterfactual predictions about the model's output when trained on a subset of training data points. Methods like Influence Functions (IF) [Koh and Liang, 2017] and TRAK [Park et al., 2023] explicitly aim to approximate these counterfactual values, making LDS a ground truth metric for their evaluation. The metric relies on the assumption that the attributions are linear, implying that the attribution for a subset of training samples can be represented as the sum of the individual attributions from those samples.\nLet D\u2032 \u2282 D be a training dataset, and g+(z, D\u2032) be the attribution-based output prediction of the model trained on D\u2032:\ng+(z, D\u2032) = \u2211 \u1d62:z\u1d62\u2208D\u2032 \u03c4(z, D)\u1d62,\nLet f(z; S) denote the output of a model trained on the dataset S.\nWe randomly sample m subsets of the training data: {D\u2032\u2081, D\u2032\u2082, . . . D\u2032\u2098: D\u2032\u2c7c \u2282 D \u2200j \u2208 [m]}. For each subset, we compute the counterfactual prediction from the original attributions. We then compute the Spearman rank correlation of these predictions with the actual outputs after retraining models on each subset. This gives the LDS score for a single test point z:\nLDS(\u03c4, z) = \u03c1({g+(z, D\u2032\u2c7c): j \u2208 [m]}, {f(z; D\u2032\u2c7c): j \u2208 [m]},\nwhere \u03c1 denotes the Spearman rank correlation function.\nThe final LDS score is the average LDS score over the test samples."}, {"title": "C.2 Downstream Evaluation Tasks", "content": "Downstream tasks assess the effectiveness of attributions in addressing a specific end-task."}, {"title": "C.2.1 Class Detection", "content": "Class detection is the task of inferring a test sample's label based on the labels of the training data points that have the highest attributions in the corresponding TDA explanations. As defined in , the class detection task is grounded in the intuition that same-class training data points are more likely to assist the model in making a correct decision than those of differing classes.\nFollowing [Hanawa et al., 2021], the quanda implementation of ClassDetectionMetric computes the ratio of test samples for which the training sample with the highest attribution corresponds to a datapoint of the correct class among the supplied attributions."}, {"title": "C.2.2 Subclass Detection", "content": "Built on the assumption that a model learns distinct representations for different sub-groups within the same class, introduces a subclass detection test. The original formulation of the test involves creating a modified training dataset where labels are randomly grouped to form new labels, followed by training a model. In quanda, this functionality is handled by the respective Benchmark class, while the Metric requires ground-truth labels for the sub-groups. It is assumed that the model develops separate mechanisms for classifying data points from different sub-groups, which should be reflected in the attributions. Particularly, in evaluating similarity-based attributions, Hanawa et al. [2021] recommend calculating the ratio of test data points for which the highest attributed training sample belongs to the correct (original, sub-) class.\nquanda implements this metric as defined in the paper, allowing for random grouping of classes, as well as user-defined groupings."}, {"title": "C.2.3 Mislabeling Detection", "content": "Mislabeling Detection is widely used as an evaluation strategy for TDA methods . This approach measures the effectiveness of TDA methods in identifying training samples that have been incorrectly labeled, also referred to as noisy labels.\nUnder the assumption that mislabeled samples will be strong evidence for their changed label for the model , the metric calculation procedure involves ordering the training samples by their self-influence ranking and assessing each label for potential mislabeling one-by-one based on the ground-truth labels. The resulting cumulative mislabeling detection curve is expected to rise sharply for more effective TDA methods. The metric scores are derived from the corresponding AUC score. As an additional feature, quanda enables users to generate a global ranking of the training samples from TDA attributions for test samples using aggregators."}, {"title": "C.2.4 Shortcut Detection", "content": "A shortcut, or a Clever-Hans effect, refers to decision rules learned by a model that allows it to perform well on a specific test data distribution while failing to generalize to more challenging testing conditions. The Shortcut Detection metric assesses the ability of TDA methods to identify test samples for which the model relies on shortcut features for its predictions. This metric is referred to as domain mismatch debugging in [Koh and Liang, 2017] and .\nThe metric evaluates the explanations of test samples that trigger the shortcut effect in a model. To confirm that the model is relying on shortcut features, the quanda implementation of the metric enables users to filter out samples where the shortcut effect is unlikely to have occurred during inference following . Assuming the indices of the training samples exhibiting a shortcut are known, this metric quantifies the ranking of the attributions of \"shortcutted\" samples relative to clean samples for the predictions of \"shortcutted\" test samples. The quanda implementation utilizes the area under the precision-recall curve (AUPRC) for calculation, as per . AUPRC is chosen because it provides better insight into performance in highly skewed classification tasks where false positives are common."}, {"title": "C.3 Heuristics", "content": "Heuristics are metrics designed to estimate desirable properties of explanations or serve as sanity checks for their validity."}, {"title": "C.3.1 Mixed Datasets", "content": "In scenarios where a model is trained on multiple datasets, this metric assesses the effectiveness of a given TDA method in identifying samples from the correct dataset as the most influential for a specific prediction, as outlined by [Hammoudeh and Lowd, 2022].\nThe metric assumes that a model has been trained on two distinct datasets: a base dataset and an adversarial dataset. These datasets are assumed to have substantially distinct underlying data distributions. The number of samples in the base dataset is significantly larger than the number of samples in the adversarial dataset. All \"adversarial\" samples are assigned a single \"adversarial\" label from the base dataset. The evaluation score is calculated based on explanations of \"adversarial\" test samples where the model correctly predicts the \"adversarial\" label. The AUPRC score quantifies the ranking of the attributions of \"adversarial\" samples in relation to other training samples."}, {"title": "C.3.2 Model Randomization", "content": "This metric, proposed in [Hanawa et al., 2021], draws inspiration from a sanity check for feature attributions introduced by [Adebayo et al., 2018]. The underlying intuition is that attributions should be sensitive to changes in model parameters. If the attributions remain unchanged despite randomizing the model parameters, this suggests a weak connection between the attributions and the model behavior. Accordingly, the metric procedure involves randomizing the model parameters and generating explanations for each test sample using"}, {"title": "C.3.3 Top-K Cardinality", "content": "The attributions should depend on the test samples used as input. However, a suboptimal TDA method may yield the same top influential samples for a given explanation target, regardless of the test sample being explained. The Top-K Cardinality metric, proposed in quantifies this dependence by calculating the ratio of the cardinality of the set of top-k attributed training samples across all test sample attributions to the maximum possible cardinality of this set (the product of the number of test samples and k). A higher ratio indicates better performance of the TDA method, reflecting a greater dependence of attributions on the specific test samples being examined."}, {"title": "D Benchmarks", "content": "All metrics in quanda are associated with corresponding benchmarks. One key use case for these benchmarks is the generation of controlled environments that metric definitions often require. While Metric objects necessitate that users provide all components such as modified datasets, models trained on these datasets, and attributions from these models-Benchmark objects facilitate the creation of these components. They take vanilla components and manage dataset manipulation, model training, and the generation of explanations as needed by the associated metrics. They utilize the respective Metric objects to handle the entire evaluation process.\nAdditionally, quanda enables users to download pre-computed benchmarks, allowing them to bypass the setup process for controlled environments. By utilizing the download method to initialize a Benchmark, users can immediately begin their evaluation. This feature also offers a standardized benchmark environment for researchers, facilitating consistent assessments of their methods."}, {"title": "E Experimental Details", "content": "In this section, we outline the experimental setup used for the evaluation displayed in Figure 1. Following this, we highlight important caveats regarding the implementation of the TDA methods employed, as well as the evaluation metrics used, to provide better context and clarity for the results."}, {"title": "E.1 Experimental Setup", "content": "To achieve the controlled conditions required for various evaluation strategies, we modified the original Tiny ImageNet dataset [Le and Yang, 2015], incorporating several special features to create two distinct datasets.\nThe first dataset includes the following modifications:\n\u2022 For the subclass detection task, all cat subclasses are merged into a single cat class, and similarly, all dog subclasses are grouped into a single dog class.\n\u2022 For the shortcut detection test, a yellow box is added to 20% of the input images in the pomegranate class.\n\u2022 For the mixed dataset test, we introduced 200 images of panda sketches from the ImageNet-Sketch [Wang et al., 2019] dataset, all labeled as basketball.\nIn the second dataset:\n\u2022 For the mislabeling detection test, 30% of images are intentionally mislabeled.\nWe fine-tuned ResNet18 models pre-trained on ImageNet using these datasets. Both models were trained for 10 epochs using AdamW [Loshchilov and Hutter, 2019] optimizer with a learning rate of 0.0003, weight decay w = 0.01, and a CosineAnnealingLR learning rate scheduler. The model trained on the noisy-label dataset achieved a top-1 accuracy of 43%, while the model trained on the transformed dataset achieved a top-1 accuracy of 56%.\nThese modified datasets and the trained models were then used to generate attributions using the following methods: Representer Points, Influence Functions with Arnoldi Iterations, TracIn (performed only on the last layer), TRAK, and a baseline random explainer. The displayed attributions are normalized by dividing the attribution score vector of each TDA method by its maximum absolute value. The first dataset, along with the"}, {"title": "E.2 Discussion of the Results", "content": "In the final subsection, we aim to provide further explanations and potential reasoning for the evaluation outcomes depicted in Figure 1.\nFirstly, TRAK is a TDA method designed to leverage multiple trained models to mitigate the noisiness of attributions [Park et al., 2023]. However, quanda currently emphasizes post-hoc attribution of model decisions. Consequently, our implementation of TRAK utilizes only a single model instance, which reduces the quality of attributions, as noted in the original paper [Park et al., 2023].\nFurthermore, we focus solely on the parameters of the final linear layer for Arnoldi Influence Function and TracIn attributions. This choice is made to ensure feasible computation times, following the recommendations from the original papers [Koh and Liang, 2017, Pruthi et al., 2020]. This means that the TDA methods have less information about the model behaviour, which results in suboptimal attributions."}]}