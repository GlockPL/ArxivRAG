{"title": "Towards Enhancing Linked Data Retrieval in Conversational UIs using Large Language Models", "authors": ["Omar Mussa", "Omer Rana", "Beno\u00eet Goossens", "Pablo Orozco-terWengel", "Charith Perera"], "abstract": "Despite the recent broad adoption of Large Language Models (LLMs) across various domains, their potential for enriching information systems in extracting and exploring Linked Data (LD) and Resource Description Framework (RDF) triplestores has not been extensively explored. This paper examines the integration of LLMs within existing systems, emphasising the enhancement of conversational user interfaces (UIs) and their capabilities for data extraction by producing more accurate SPARQL queries without the requirement for model retraining. Typically, conversational UI models necessitate retraining with the introduction of new datasets or updates, limiting their functionality as general-purpose extraction tools. Our approach addresses this limitation by incorporating LLMs into the conversational UI workflow, significantly enhancing their ability to comprehend and process user queries effectively. By leveraging the advanced natural language understanding capabilities of LLMs, our method improves RDF entity extraction within web systems employing conventional chatbots. This integration facilitates a more nuanced and context-aware interaction model, critical for handling the complex query patterns often encountered in RDF datasets and Linked Open Data (LOD) endpoints. The evaluation of this methodology shows a marked enhancement in system expressivity and the accuracy of responses to user queries, indicating a promising direction for future research in this area. This investigation not only underscores the versatility of LLMs in enhancing existing information systems but also sets the stage for further explorations into their potential applications within more specialised domains of web information systems.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant advancements have been made in the development of large language models (LLMs), which have shown robust capabilities across a broad spectrum of natural language processing (NLP) tasks. These models can execute tasks without the need for specific fine-tuning, as they can be directed through instructional prompts embedded within the requests, thereby facilitating their application across various domains [24,5]. This scientific progress has catalysed a surge in research aimed at examining their applicability and efficacy across a diverse array of topics and methodologies such as education [3], healthcare [21], and finance [7], as well as for specific NLP tasks like replacing traditional question-answering models [22,9]. In particular, commercial versions of these models, such as GPT, have received considerable attention and have been promoted as optimal solutions for integration within chatbot applications and backend text processing solutions. This has opened up new avenues for leveraging artificial intelligence to enhance user interactions and streamline data processing.\nDespite these advancements, one area that still needs to be explored is the potential of LLMs to serve as entity extractors within existing linked data (LD) systems and Resource Description Framework (RDF) triplestores. The integration of LLMs in such capacities could potentially revolutionise the way entities are extracted and managed, offering more dynamic and context-aware data handling capabilities. However, the efficacy and reliability of LLMs in this specific role have not been thoroughly tested, raising questions about their practicality and performance in real-world settings.\nIn this study, we employ a novel toolkit known as ForestQB [16], specifically developed to explore observational LD to support bioscientists and wildlife conservation efforts. This system integrates an interface that combines a chatbot with a traditional form-based graphical user interface (GUI). It facilitates the extraction of data from diverse observational LD endpoints. The chatbot within this toolkit is configured to interpret user queries and handle entity extraction. This process automates populating the GUI with appropriate selections and executes the search process. Consequently, the chatbot, as a general-purpose tool, is designed to adapt to changes in the dataset, thereby not being restricted to specific names or sentences within its training data in an ontology-independent manner. Therefore, the model used by the chatbot lacks the ability to comprehend the relationships within the knowledge graph concerning the sensors and their observations. As a result, constructing queries that incorporate an arbitrary array of filters and entities through the Conversational UI is unfeasible. In addition, general queries about the descriptions of the sensors and entities within the dataset cannot be accommodated.\nThe objective of our research is to critically evaluate the validity and effectiveness of implementing this approach within current systems. Our study aims to explore how the integration of LLMs can enhance the expressivity and functionality of systems dealing with LD and RDF triplestores. By conducting empirical tests and analysing the outcomes, we hope to provide insights into the"}, {"title": "2 Related Work", "content": "Entity Extraction. Traditional entity extraction, which identifies and classifies named entities within unstructured text, relies on rule-based systems and machine learning algorithms. These methods depend on predefined patterns, dictionaries and statistical models but are hindered by the need for frequent updates and extensive annotated datasets [11]. They also struggle with context-specific nuances, limiting their ability to integrate additional reference data to improve accuracy [11]. For instance, distinguishing that \"heat\" signifies a temperature entity in RDF data remains problematic. Therefore, LLMs' proficiency in NLP tasks supports our approach to enhancing entity extraction and reducing manual and data-intensive dependencies [4], as further discussed in Section 3.\nQuestion-Answering over LD and RDF Triplestores. Traditional approaches to developing models for question-answering systems require specific training for each dataset or adjustments due to structural changes in the data. This process necessitates developing and maintaining models trained at comprehending and interpreting data, which is both resource-intensive and time-consuming due to the complexity and variability inherent in the data [6]. As a result, conventional methods are impractical for creating general-purpose solutions that require adaptability, such as toolkits that can interface with multiple datasets or manage changes within the same dataset. In response to these challenges, the adoption of LLMs provides a promising alternative. These models enhance the adaptability and efficiency of question-answering systems by reducing the need for extensive retraining and facilitating more flexible integration with diverse and dynamically evolving datasets, such as those in observational LD [18,4,10,22].\nLLMs Limitations. While LLMs exhibit significant capabilities, they have notable limitations that render them unsuitable for directly replacing chatbots employed within conversational UIs. These UIs, such as the one utilised in our experiment, are designed to automate the process of populating user selections on the UI, thereby constructing SPARQL queries to extract information from"}, {"title": "3 Proposed Approach", "content": "ForestQB [16] is a novel SPARQL query builder that combines a chatbot interface with a form-based GUI to construct and execute SPARQL queries, enhancing the extraction of relevant information from observational LD; the source code is available at (https://github.com/i3omar/ForestQB). The integration of a chatbot has sped up the query construction, allowing users to seamlessly translate inquiries into the GUI (see Figure 1). Testing of ForestQB shows increased user satisfaction and a significantly reduced learning curve. However, the chatbot has limitations, notably in its inability to perform all tasks achievable by the form-based GUI, such as extracting and filtering an arbitrary set of entities from user queries. These limitations arise from a design choice to keep the chatbot ontology independent, ensuring compatibility with various datasets but restricting the complexity of questions and accuracy.\nTo overcome these limitations, it is proposed that an LLM be integrated as an advanced entity extractor and reasoning tool, as shown in Figure 1. This"}, {"title": "4 LLM Integration", "content": "4.1 Generating SPARQL query using LLMs\nWe initially examined the use of LLMs to generate accurate SPARQL queries, encountering significant challenges [8]. Our experiment with GPT-4 involved generating queries from a range of simple inputs specified by the toolkit, along with the required RDF definition file to guide the model. However, the results were largely unsuccessful; all generated queries failed to yield results except for the first, which was incorrect. The queries were structurally sound and adhered to SPARQL standards. However, the model reverted to its intrinsic understanding of the SOSA [12] ontology inherent to the dataset, thereby neglecting the specific context and data provided. Despite demonstrating precise encoding capabilities, effective entity extraction, and appropriate filter application, the findings suggest that LLMs are not yet reliable for directly converting user queries into SPARQL, though they may enhance data extraction methods. More details available at (https://github.com/i3omar/LLM-Integration-Data)."}, {"title": "4.2 RDF Embeddings", "content": "Incorporating RDF data into the prompts of LLMs like GPT presents significant challenges primarily due to the voluminous nature of RDF datasets. The principal issue arises from the economic implications of embedding large RDF datasets into LLM prompts, especially with fee-based services such as OpenAI's GPT, where costs are proportional to the number of tokens processed. Additionally, although the maximum allowable prompt size has recently expanded, it remains insufficient for embedding substantial RDF data. For instance, models like Llama-3 accommodate a context length of up to 8192 tokens, which is considerably smaller than even a modest-sized RDF graph, particularly those containing observational data. To address these limitations, we have adopted the following approach:\n1. Selective Data Inclusion: The definitions of sensors, their properties, and the corresponding properties each sensor monitors are the essential data for the LLM to identify entities and understand their relationships within the graph. Consequently, we omit less critical observational data from the LLM context. This selective inclusion significantly reduces the volume of RDF data while still transmitting valuable information. However, this does not imply that the entire data definition can be included, as it may still be excessively large.\n2. Subgraph Generation through RDF Walks: Unlike document embeddings, which are tailored for unstructured text, RDF embeddings are specifically designed to manage structured graph data. A viable approach involves decomposing extensive RDF graphs into smaller, more manageable subgraphs. This decomposition is achieved by using RDF walks, inspired by the RDF2Vec technique [20], to create subgraphs that encapsulate each triple's directly connected nodes. However, RDF2Vec was found to be impractical for our specific applications due to the high computational demands associated with dynamically generating subgraphs from user queries. For example, take the query, 'What is the latest reading of Sensor A?' Within this query, the identifiable entities are 'Sensor A' and 'latest reading.' Subsequently, it becomes imperative to construct a simplified RDF graph or subgraph that accurately encapsulates the semantic structure of the user query. Instead, we decompose the RDF graph into individual triples stored in memory as an array. These triples are then iteratively processed to identify and amalgamate directly connected triples into smaller subgraph arrays.\n3. Vector Embedding of RDF Triples: As each RDF triple selected in this step is enriched with textual annotations, including labels and descriptive comments specific to the entity, we utilised the \u2018paraphrase-TinyBERT-L6-v2', a pre-trained model capable of capturing semantic similarities within textual data [19], to represent each triple as a vector. These vectors are then stored in Qdrant, an open-source vector database. As summarised in Figure 4, when a user query is received, it is similarly encoded, and a cosine similarity search is performed"}, {"title": "4.3 Prompts templates", "content": "An essential step in enhancing the accuracy of outputs generated by LLMs is the preparation of an appropriate prompt. Recent studies have demonstrated the high sensitivity of LLMs to the specificity of prompts [25,14,1]. Consequently, to facilitate the generation of valid JSON from user queries, a JSON template has been embedded within the prompt to provide the LLM with a clear indication of the expected output. In addition, two distinct types of prompts were developed to evaluate the most effective method for extracting entities using zero-shot examples. The first prompt was succinct, offering minimal instruction, while the second was more elaborate, clarifying relevant filters to assess whether this would lead to enhanced output accuracy. We conducted experiments using 'gpt-3.5-turbo-0125' on a set of 20 questions, equally divided between 10 straightforward and 10 complex inquiries, to ascertain whether the outputs differed significantly between the two prompts. As shown in Table 2, the results indicated a 42.86% improvement in the outputs generated by the more detailed second prompt compared to the first. This finding suggests that more comprehensive instructions significantly aid the model in producing more accurate outputs. Therefore, the more detailed prompt will be utilised in our subsequent experiments."}, {"title": "5 Experiment Evaluation", "content": "In this section, the effectiveness of incorporating LLMs into the existing system to improve information retrieval is evaluated, alongside an analysis of the experimental results. The results and evaluation metrics are available at (https://github.com/i3omar/LLM-Integration-Data).\n5.1 Dataset\nWe conducted the experiment using a private SPARQL endpoint hosting observational LD, structured in accordance with the SOSA [12] ontology. The dataset is publicly available and can be accessed through the GitHub repository at (https://github.com/i3omar/ForestRDF).\n5.2 Selected Large Language Models\nIn order to rigorously evaluate our approach, we have selected a diverse range of state-of-the-art language models. Our selection primarily includes six versions of OpenAI's latest GPT-3.5 and GPT-4 models (https://platform.openai.com/docs/models). Additionally, we integrate two open-source models into our study: openchat-3.5-0106 [23] and Meta-Llama-3-8B-Instruct [2]. The former has been finetuned from the 'Mistral-7B-v0.1' base model, while the latter is the most recent iteration within the Meta Llama-3 series. Both open-source models were chosen due to their lightweight architecture, making them practical for real-world applications. Furthermore, all selected models have demonstrated superior performance in various NLP tasks relevant to our study. The primary objective of this selection is not to compare these models directly; rather, it is to validate our approach's effectiveness and achieve high accuracy in its application.\n5.3 Experiment Setup\nWe have formulated a set of 40 questions designed to assess the application of LLMs across use cases 1 and 2, where the LLM was prompted with relevant RDF data. These questions are strategically divided into four groups to gauge the effectiveness of the LLM model in various scenarios:\n1. Simple Direct Queries: Involves a single entity clearly identified in the RDF data.\n2. Complex Direct Queries: Encompasses questions with multiple entities, all explicitly mentioned in the RDF data.\n3. Simple Indirect Queries: Queries about a single entity not directly mentioned in the data, using synonyms and different phrasing to evaluate the model's inference abilities.\n4. Complex Indirect Queries: More involved queries that include multiple entities not explicitly stated, utilising synonyms and varied wording to assess the model's comprehension and reasoning capabilities."}, {"title": "5.4 Evaluation Metrics", "content": "To evaluate our approach, we conducted manual evaluations for both use cases and automatic validation of the JSON output for Use Case 1, applying targeted metrics to ensure practical and technical validity, as follows:\nA. Manual Evaluation (Accuracy) The effectiveness of the LLM in enhancing entity extraction and RDF reasoning capabilities was manually evaluated through meticulous testing in practical scenarios. The output is manually classified in a binary manner as correct or incorrect. For Use Case 1, the output is deemed correct if the generated JSON output includes the correct answer and does not exhibit issues such as incorrect key naming or invalid JSON format. Meanwhile, for Use Case 2, the output comprises plain text that has been manually evaluated against reference data to verify the accuracy of the generated output.\nB. Automatic Structural Evaluation In practical settings for Use Case 1, two models can generate outputs that are both correct and compatible with the system, although one model may introduce additional keys (which remain operational). This evaluation aims to determine the extent to which each model complies with specified instructions to yield a more reliable JSON output. These adapted metrics, based on foundational work in the field as detailed by [15,19], evaluate performance by comparing JSON outputs from LLMs to a reference answer, enabling a deeper examination of quality beyond mere accuracy. The metrics used are as follows:\nB.1. Structural Similarity Score (StrSS): This metric measures the similarity between the structure of the reference JSON and the generated JSON output. Both JSON objects are flattened, with each key representing a path through the original structure. URIs in the generated JSON are simplified to their abbreviated forms used in the reference, e.g., converting 'http://.../sosa/resultTime' to 'sosa:resultTime' to prevent scoring penalties for acceptable URI variations. Extra keys in the output incur a lesser penalty, recognising that models may generate correct but additional keys, slightly lowering the match score for a more nuanced comparison. The metric equation for evaluating outputs is as follows:\nStrSS = $\\frac{K_{ref} \\cap K_{gen}}{K_{ref} + \\beta \\cdot (K_{gen} - K_{ref})}$"}, {"title": "B.2. Semantic Similarity Score (SemSS)", "content": "This metric evaluates the semantic similarity between values in reference and generated JSONs by converting them into vector embeddings using the 'paraphrase-TinyBERT-L6-v2' model. Cosine similarity between these embeddings measures their semantic closeness. The average cosine similarity across corresponding key-value pairs in the JSON structures quantifies semantic alignment, defining the SemSS score. The SemSS score is defined as:\nSemSS = $\\begin{cases} \\frac{1}{N} \\sum_{i=1}^{N} cos\\_sim (emb(U_{ref,i}), emb(U_{gen,i})) & \\text{if } N > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\nwhere N is the count of keys shared between Kref and Kgen. For the i-th matching key, Uref,i and Ugen,i denote the associated values in the reference and generated JSONs, respectively. The function emb(v) computes the embedding of value v, and cos_sim(a, b) calculates the cosine similarity between vectors a and b."}, {"title": "B.3. Overall JSON Similarity Score (OJSS)", "content": "This score is a weighted average that combines the two distinct similarity scores: the SemSS Score and the StrSS Score. Both scores are assigned equal weight, each contributing 50% to the final calculation, denoted by a coefficient of 0.5 for each. This equal weighting underscores the balanced importance of both the structural integrity of the keys and the semantic accuracy of the values in determining the overall score. The following is the OJSS complete equation:\nOJSS = $\u03b1 \\times StrSS + (1 \u2212 \u03b1) \\times SemSS$\nwhere a = 0.5, a is a weighting factor between 0 and 1 that balances the importance of structural and semantic similarity."}, {"title": "5.5 Use Case 1: Results", "content": "Table 3 presents the performance of the selected LLMs in generating accurate JSON outputs, reflecting the queries employed in our research. This practical experiment relied on the system's capability to retrieve and process the generated responses accurately and to construct the queries correctly to be deemed successful. Figure 5 illustrates responses across question groups."}, {"title": "Zero-shot", "content": "During the zero-shot testing phase, the performance levels were notably low as most LLMs did not adhere precisely to the instructions. For instance, Llama-3, despite explicit instructions to generate solely JSON outputs, produced additional explanatory text alongside the JSON, thereby rendering the outputs impractical for further processing, culminating in a complete failure rate. In contrast, GPT-4-turbo achieved an accuracy of 68.3%, with a StrSS score of 71.9%, indicating a well-structured and highly practical output."}, {"title": "Few-shot", "content": "In the few-shot tests, there was a substantial improvement in results. LLMs utilised the provided examples as historical references, enhancing their ability to generate structured outputs, as evidenced by the StrSS scores. GPT-40 achieved the highest accuracy rate at 89.2%, followed by GPT-4-0613 and GPT-4-turbo, with scores of 88.3% and 87.5% respectively. GPT-4-turbo also scored 87.4% in StrSS, 84.2% in SemSS, and 85.8% in OJSS, demonstrating fewer errors in practical applications within web information systems, despite not being the most accurate model. Furthermore, Llama-3 showed a notable increase in accuracy to 71.7%, a commendable achievement for a lightweight model, illustrating greater adherence to instructions when provided with few-shot examples."}, {"title": "5.6 Use Case 2: Results", "content": "The application of LLMs for reasoning over RDF data to respond to queries related to the data schema has demonstrated notable efficacy. By using the GPT-4-turbo model, we achieved a remarkable accuracy rate of 100% (see Table 4). As seen in Figure 6, the performance of most models, including open-source ones, was generally robust; however, the 'GPT-3.5-turbo-0613' model underperformed significantly due to its limitations in processing RDF data as textual content, reflecting its lack of training for such data types."}, {"title": "5.7 Result Analysis", "content": "While a majority of the models demonstrated competent performance in zero-shot scenarios, particularly with Use Case 2, the challenges of executing Use Case 1 in similar settings were pronounced. In the analysis of error patterns within Use Case 1 across various LLMs, multiple shortcomings became evident. Notably, models frequently misinterpreted 'Horizontal Dilution of Precision"}, {"title": "6 Conclusion", "content": "In this paper, we propose a technique for integrating LLMs into existing conversational UIs, enhancing their functionality without requiring retraining. Our approach leverages the intrinsic strengths of LLMs to complement traditional chatbot models and improve entity extraction over LD and RDF triplestores. We initiated our discussion by outlining the system's limitations with the proposed use cases, followed by a critical analysis of why direct SPARQL generation or outright replacement of the existing chatbot model with LLMs was suboptimal. Our proposed solution includes embedding RDF schema into LLM prompts to ensure the question is contextually enriched with relevant RDF data. Furthermore, we enhanced the quality of LLM responses through optimised prompt templates, leading to more accurate and context-aware interactions. Our experimental evaluations corroborate that LLMs can effectively function as both an entity extraction component and a reasoning tool within our system, significantly augmenting user experience and improving the overall information retrieval process. This integration addresses key limitations of prior systems, such as scalability and contextual understanding, and sets a new benchmark for advancements in conversational UIs and LD retrieval. Our approach, enhancing semantic query handling and entity recognition, paves the way for further research into LLM applications across diverse datasets and domains."}]}