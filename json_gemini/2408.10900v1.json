{"title": "Towards Efficient Formal Verification of Spiking Neural Network", "authors": ["Baekryun Seong", "Jieung Kim", "Sang-Ki Ko"], "abstract": "Recently, AI research has primarily focused on large language models (LLMs), and increasing accuracy often involves scaling up and consuming more power. The power consumption of AI has become a significant societal issue; in this context, spiking neural networks (SNNs) offer a promising solution. SNNs operate event-driven, like the human brain, and compress information temporally. These characteristics allow SNNs to significantly reduce power consumption compared to perceptron-based artificial neural networks (ANNs), highlighting them as a next-generation neural network technology. However, societal concerns regarding AI go beyond power consumption, with the reliability of AI models being a global issue. For instance, adversarial attacks on Al models are a well-studied problem in the context of traditional neural networks. Despite their importance, the stability and property verification of SNNs remains in the early stages of research. Most SNN verification methods are time-consuming and barely scalable, making practical applications challenging. In this paper, we introduce temporal encoding to achieve practical performance in verifying the adversarial robustness of SNNs. We conduct a theoretical analysis of this approach and demonstrate its success in verifying SNNs at previously unmanageable scales. Our contribution advances SNN verification to a practical level, facilitating the safer application of SNNs.", "sections": [{"title": "1 Introduction", "content": "Currently, AI research is primarily focused on large language models (LLMs) since the advent of powerful models such as OpenAI's ChatGPT (Brown et al. 2020). It is well known that the performance of AI models generally improves as the number of parameters increases (Montufar et al. 2014; Goodfellow et al. 2013). However, this also leads to a significant increase in power consumption (Desislavov, Mart\u00ednez-Plumed, and Hern\u00e1ndez-Orallo 2023; Samsi et al. 2023), which leads to environmental side effects such as carbon emissions and climate change.\nSpiking neural networks (SNNs) are neural networks that mimic the human brain, and computing SNNs by mimicking neurons in electronic circuits consumes less power than typical perceptron-based artificial neural networks (ANNs) (Kim et al. 2020). Many researchers claim that SNNs are the future of AI research and could be an effective alternative to address the excessive power consumption problem caused by current ANNs (Eshraghian et al. 2023b; Tavanaei et al. 2019). However, studies supporting these claims are still in their early stages.\nOn the other hand, many people suggested the potential risk of adversarial attacks on neural networks by introducing a carefully designed perturbation to the input data, which causes severe problems with neural networks' security and safety (Akhtar and Mian 2018; Moosavi-Dezfooli et al. 2017; Szegedy et al. 2013). Since neural networks are essentially function approximators (Hornik, Stinchcombe, and White 1989), it is important to verify the approximation ratio, which is the range of error in their output, to estimate the robustness of the networks to be deployed in safety-critical applications. To date, there have been a lot of attempts to formally verify ANNs (Albarghouthi et al. 2021; Katz et al. 2022; Tjeng, Xiao, and Tedrake 2017; Bunel et al. 2020).\nHowever, it is still an early stage for SNNs, and several researchers have only suggested the plausibility of formal verification of SNNs (De Maria, Di Giusto, and Laversa 2020; Banerjee et al. 2023; De Maria, Di Giusto, and Ciatto 2017). Recently, there has been a promising study that aims to verify the local adversarial robustness of SNNs using satisfiability modulo theories (SMT) solvers (Banerjee et al. 2023). They have shown that it is theoretically possible to verify the robustness of SNNs, but it is notoriously difficult to apply in practice due to its high computational cost, even at small scales. It is not difficult to speculate that the rigorous formal verification of SNNs is generally more difficult than that of traditional ANNs because of the temporal and discrete nature of the network. For instance, the state space of SNNs involves one more dimension, the temporal dimension, as the input is described as a spike train along the time axis rather than an instantaneous one. In addition, the activation of SNNs is a discrete function of the potential, which is not differentiable, unlike ANNs. These two properties make rigorous verification and analytic interpretation more difficult for SNNs than for ANNS.\nIn this paper, we present an efficient verification algorithm for SNNs, and the following are our key contributions:\n1. Formulate spiking neural network with temporal encoding as SMT solver constraints.\n2. Theoretically analyze the impact of temporal encoding on the speed of formal verification."}, {"title": "2 Related Work", "content": "Formal verification of neural networks is an active area of research, mainly related to robustness (Albarghouthi et al. 2021). There are two main ways to verify the properties of neural networks, abstraction-based verification and constraint-based verification. Methods based on constraint-based verification approximate the neural network using abstract interpretation theory. Katz et al. (Katz et al. 2022) have extended the simplex method to perform verification for neural networks with ReLU activation functions, which are frequently employed in deep neural networks. Later, Katz et al. (Katz et al. 2019) introduced the Marabou framework by extending Reluplex by considering arbitrary piecewise-linear activation functions. Tjeng et al. (Tjeng, Xiao, and Tedrake 2017) have utilized mixed-integer linear programming (MILP) to represent ReLU and achieved better robustness verification performance than Reluplex. Bunel et al. (Bunel et al. 2020) have utilized a branch-and-bound algorithm to perform formal verification. Most of abstraction-based verification methods for ANNs are based on piecewise linear activation in real space, which is hard to be applied to the discrete spikes of SNNs. On the other hand, constraint-based methods use SMT solver, which can also applied in discrete activations. Pulina and Tacchella used an SMT solver to verify the properties of ANN (Pulina and Tacchella 2012). Amir et al. (Amir et al. 2021) applied an SMT solver to verify the properties of a binarized neural network, extending ReLUplex. Guo et al. (Guo et al. 2023) proposed an efficient SMT-based verification method for occlusion robustness."}, {"title": "2.2 Mathematical Modeling of SNNs", "content": "Due to its event-driven and time-varying nature, the mathematical models for SNNs differ substantially from those for ANNs. De Maria et al. (De Maria, Di Giusto, and Ciatto 2017; De Maria, Di Giusto, and Laversa 2020) have proposed a formalization of SNNs based on timed automata (Alur and Dill 1994; Waez, Dingel, and Rudie 2013) networks. As each neuron of SNN can be modeled by a timed automaton, they have constructed a network that consists of a set of timed automata running in parallel and sharing channels according to the structure of the SNN. The authors have validated the constructed model against several properties of SNNs via temporal logic formulae.\nMore recently, SMT-based encoding of SNN for adversarial robustness verification has been introduced (Banerjee et al. 2023). The authors have adapted the quantifier-free linear real arithmetic constraints to encode the overall mathematical operations in SNNs. They have further encoded the adversarial robustness conditions based on potential perturbations on input as logical constraints and demonstrated that SMT solvers can be successfully used for modeling and verifying the robustness of SNNs. However, the experiments are conducted on relatively small SNNs with a restricted benchmark due to the limited scalability of the proposed approach."}, {"title": "2.3 Information Encoding in SNNs", "content": "Three main ways to convert input data in SNNs are rate encoding, temporal encoding, and delta modulation (Eshraghian et al. 2023b). Rate encoding is a commonly used method in SNNs that interprets the size of the input as the probability of spiking and generates random spikes based on this probability (Adrian 1926). This method, which follows the simple intuition that the size of the information is proportional to the number of spikes, is simple to implement because the gradient descent of the ANN can be applied through a surrogate derivative (Guerguiev, Lillicrap, and Richards 2017). However, it consumes more power because the number of spikes is large, and neuromorphic chips implementing SNNs consume more power as the number of spikes increases. Temporal encoding generally forces each input neuron to encode only once, and the larger the input, the faster it spikes (Johansson and Birznieks 2004; Gollisch and Meister 2008). It consumes significantly less power but is generally harder to train because it is harder to use the gradient descent of an ANN as is. Delta modulation encodes data to spike when it increases over time. It requires sequential data, so it is not used in most cases (Petro, Kasabov, and Kiss 2019). Mostafa (Mostafa 2017) introduced the gradient descent learning algorithm on temporal-encoded SNN, using the property that the input-output latency relation in temporal-encoded SNN is differentiable almost everywhere. Kheradpisheh et al. (Kheradpisheh and Masquelier 2020) improved Mostafa's algorithm (Mostafa 2017) and published an implementation in a public online repository. Yamamoto et al. (Yamamoto, Sakemi, and Aihara 2022) proposed temporal encoding without single spike restriction."}, {"title": "2.4 Adversarial Robustness in SNNS", "content": "The adversarial robustness of neural networks is important in practical use, so various research studies have been conducted to increase the adversarial robustness of SNNs, like ANNs. Zhou (Zhou et al. 2021) has suggested the method to train robust temporal encoded SNN. Liang et al. designed S-IBP and S-CROWN for robust training of SNN. (Liang et al. 2022) \u00d6zdenizci et al. presented an algorithm that converts ANNs to SNNs and fine-tunes it to strengthen robustness. (\u00d6zdenizci and Legenstein 2023) Ding et al. showed a stochastic gating spiking neural model, which is biologically plausible, to increase robustness. (Ding et al. 2024) Chen et al. proposed a way to train robust SNN using the inspiration from the visual masking effect and filtering theory. (Chen, Sun, and Xu 2024)"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Spiking Neural Networks", "content": "Variable definitions We need to define variables to describe SNN. T is the number of simulated time steps, L is the total number of layers where the first layer is the input layer, and $N_l$ is the number of neurons in layer l. For simplicity,"}, {"title": "Spiking neurons", "content": "Leaky Integrate-and-Fire (LIF) neurons and Integrate-and-Fire (IF) neurons are artificial neuron models that integrate binary presynaptic input spikes over time and fire to postsynaptic neurons if potential exceeds the threshold. In general, a spike firing is represented as 1, and 0 otherwise. On discrete time steps, we can define LIF neurons in the form of multivariate functions that map the state of the past to the future. First, the LIF neuron integrates the input current and some of its past potential $p^{(t-1)}$.\n$\\text{Integrate}(x, w, p^{(t-1)}; \\gamma) = (x, w) + \\gamma p^{(t-1)}$\nThen, the neuron determines whether to generate a spike according to the integrated current and updates its potential. Note that the spike timing of the next layer depends on the synaptic delay $\\tau$, but the potential does not.\n$\\begin{aligned}x_{\\text{out}}^{(t)} = \\begin{cases}1 & \\text{if } \\text{Integrate} (x_{\\text{in}}^{(t-\\tau)}, p^{(t-\\tau-1)}; W_{\\text{in}}, \\gamma) \\geq 0\\\\0 & \\text{otherwise}.\\end{cases}\\\\p^{(t)} = \\text{Integrate} (x_{\\text{in}}^{(t)}, W_{\\text{in}}, p^{(t-1)}; \\gamma) - x_{\\text{out}}^{(t)}\\end{aligned}$\nBy substituting $\\gamma$ to 1, they become the definition of IF neurons. Our method uses IF neurons due to the ease of training. A spiking neural network (SNN) is a neural network that employs biologically plausible neuron models such as LIF and IF neurons. In general, SNNs are simulated over many time steps to generate outputs."}, {"title": "Spike encoding", "content": "Due to the nature of SNNs described above, inputs targeting any SNN must be encoded in temporal and binary form, and we must interpret the output spike train for prediction. We utilize two ways to encode inputs: rate and temporal (latency) encoding.\nIn SNNs, rate encoding generates a spike train based on the Poisson or Bernoulli random process, whereby inputs are interpreted as probabilities. If an input is not between 0 and 1, it must be scaled to fit within this range. On the other hand, temporal encoding makes a spike train by interpreting the intensity of inputs to input quickness. The larger the inputs, the faster the spikes.\nGiven that the meanings of time steps vary according to input encoding, the methods for making predictions based on the output must differ. In rate encoding, we count each neuron's spikes and choose the neuron with the most spikes. In contrast, in temporal encoding, we utilize the time-to-first-spike (TTFS) method, which selects the fastest spiking neuron. (Note that neurons in the hidden and output layer can spike multiple times.) Taking TTFS into account, we do not need whole spike train $x_{\\text{out}}^{(t)}$. Instead, we can use the spike times $s_\\xi \\in \\mathbb{N}_{N_l}$."}, {"title": "3.2 Adversarial Attack", "content": "In ANNs, adversarial attacks are generated by adding a real-valued perturbation to the original input. However, we cannot use this approach to SNNs because the spike trains do not consist of a real number but binary. Moreover, considering that the inputs of rate are far from the inputs of temporal encodings, the way to generate adversarial attacks must be different.\nSpike train perturbation In rate encoding, previous work (Banerjee et al. 2023) proposed the definition of perturbation based on L1 distance. The elements in the spike train are randomly flipped, with the number of flips being less than or equal to the value of A.\nHowever, it is not possible to employ this method in temporal encoding, as it is essential that all neurons in the input layer fire only once; a random flip could corrupt this property. Instead, we randomly shift the spike times, and the sum of the distances between original and perturbed spike times is less than or equal to A.\nPerturbation space For a theoretical analogy, we need a measure to compare two different encodings. A-perturbation space is the size of the set of possible perturbed inputs. We have a different definition of perturbation set for the two encoding cases because the spike can not be flipped in temporal encoding.\nDefinition 1 (A-Perturbation Space). For a non-negative integer $\\Delta$ the A-perturbation set of a rate-encoded input spike train X is,\n$\\epsilon(X, \\Delta) = \\{x| \\sum_{n=0}^{N_0-1} ||x_n - x_n||_1 \\leq \\Delta\\}$\nFurthermore, the A-perturbation set of an input spike times s is,\n$\\epsilon(s, \\Delta) = \\{\\tilde{s}| \\sum_{n=0}^{N_0-1} |\\tilde{s}_n - s_n| < \\Delta\\}$\nThen, the A-perturbation space of the input spike train or spike times is defined as the size of the A-perturbation set of it.\nConsidering that spike times have a similar meaning to ANNs' intensity, the L1 distance between the perturbed spike times and the original is very similar to formal verification in ANNs. Assuming that the spike ratio in rate encoding is an estimate of ground truth probability, the meanings of L1 distance from two different encodings are almost the same. However, the space complexity of the two encodings is very different, as shown in the theoretical analysis section, and it is one of the main contributions of our paper."}, {"title": "4 Methodology", "content": "First, we introduce our SMT encoding for temporal encoded SNN. We describe mathematical behavior and adversarial robustness conditions in quantifier-free linear real arithmetic formulas for the sound and complete verification of SNNs. Second, we theoretically compare the proposed encoding and the previous SMT encoding (Banerjee et al. 2023) for SNNs with rate encoding in terms of combinatorial complexity of adversarial perturbations on input."}, {"title": "4.1 SMT Encoding of Temporal Encoded SNN", "content": "We encode expressions to describe SNN, which consists of IF neurons. In the following paragraphs, we denote the nth neuron in the lth layer by the pair (n, l). Spike time $s_{l,n}$ of the neuron (n, l) is described by a non-negative integer with the following constraint:\n$\\xi_1 = \\bigwedge_{l=1}^{L} \\bigwedge_{n=0}^{N_l-1} [s_{l,n} \\geq \\tau_l \\land s_{l,n} < T - 1] $.\nThe condition $\\xi_1$ asserts that spike time must be in the range $[\\tau_l, T - 1]$ since we assume that there are synaptic delays in the spike time at each layer. Simply speaking, a neuron (n, l) for all $0 \\leq n \\leq N_l - 1$ cannot spike before $\\tau_l$ as it requires at least $l$ synaptic delays.\nPotential $p_{l,t,n}$ of the neuron (n, l) at time step t is a real number satisfying the following conditions:\n$\\begin{aligned}\\xi_2 &= \\bigwedge_{l=1}^{L} \\bigwedge_{n=0}^{N_l-1} [p_{l,0,n} = 0]\\\\\\xi_3 &= \\bigwedge_{l=1}^{L} \\bigwedge_{n=0}^{N_l-1} \\bigwedge_{t=1}^{T-1} \\left[ p_{l,t,n} = \\sum_{m=0}^{N_{l-1}-1} w_{l-1,m,n} \\cdot 1(s_{l-1,m} \\leq t) \\right]\\\\\\end{aligned}$\nwhere 1 is an indicator function and $w_{l,m,n}$ is a learnable weight of the neuron (n, l) to the neuron (n, l + 1). $\\xi_2$ and $\\xi_3$ assert that neurons integrate the sum of weighted spikes from the previous layer.\n$\\begin{aligned}\\xi_4 &= \\bigwedge_{l=1}^{L} \\bigwedge_{n=0}^{N_l-1} \\bigwedge_{t=1}^{T-1} \\left[ a_{l,t,n} = \\bigvee_{t'=0}^{t-1} p_{l,t',n} \\geq 0 \\right]\\\\\\xi_5 &= \\bigwedge_{l=1}^{L} \\bigwedge_{n=0}^{N_l-1} \\bigwedge_{t=\\tau_l}^{T-2} [(-a_{l,t-\\tau,n} \\land p_{l,t-\\tau,n} \\geq 0) = (s_{l,n} = t)]\\\\\\xi_6 &= \\bigwedge_{l=1}^{L} \\bigwedge_{n=0}^{N_l-1} [a_{l,\\tau-1-\\tau,n} = (s_{l,n} = T - 1)]\\\\\\end{aligned}$\nThe condition $\\xi_4$ utilizes a flag variable $a_{l,t,n}$ to indicate whether the neuron (n, l) has ever spiked before time step t and $\\xi_5$ states that if (n, l) has not spiked before time step $t-\\tau-1$ and its potential reaches the threshold at $t - \\tau$, it spikes at t. The condition $\\xi_6$ enforces the neuron (n, l) to spike at the last time step T \u2013 1 if (n,l) has never spiked before.\nTemporal perturbation about an input x is expressed as:\n$\\xi_7 = \\sum_{n=0}^{N_0-1} |\\tilde{s}_{0,n} - x_n| \\leq \\Delta$.\nThe condition $\\xi_7$ asserts a constraint that L1 distance between $s_0$ and x is not greater than $\\Delta$. SNN makes the prediction $\\hat{y}$ for an input x by choosing the neuron with the fastest input response.\nLocal robustness for the input x and prediction $\\hat{y}$ is encoded as:\n$\\xi_8 = \\bigwedge_{n=0}^{N_{L-1}} [n \\neq \\hat{y} \\Rightarrow s_{L,n} > s_{L,\\hat{y}}]$.\nWe can verify whether or not a given SNN is robust by checking if $\\xi_1 \\land \\xi_2 \\land ... \\land \\xi_7 \\land \\neg \\xi_8$ is satisfiable using the SMT solver. Namely, the SNN model is not robust if the condition is satisfiable as it implies the existence of an adversarial counterexample."}, {"title": "4.2 Theoretical Analysis", "content": "Here, we provide a theoretical explanation for why our SMT encoding is more efficient regarding the adversarial robustness verification performance than the one formulated in the previous work (Banerjee et al. 2023). The main intuition is to compare the perturbation space, which is the number of possible adversarial A-perturbations on input, of the rate and temporal encoding. Now, we are ready to provide the theoretical analysis of the exponential complexity advantage by adopting temporal encoding instead of rate encoding.\nLemma 1. Let $\\Delta = \\alpha TN$, where $0 \\leq \\alpha \\leq 1$. The perturbation space ratio of rate encoding to temporal encoding is $O((T^{\\Delta T} /(1 + 2\\alpha T))^N)$.\nProof. Let us assume the input layers have the number of neurons N. The perturbation space of rate encoding can be calculated by counting the ways to flip less than or equal to $\\Delta$ spikes as follows:\n$\\sum_{d=1}^{\\Delta T} \\binom{NT}{d} = O\\left(\\binom{NT}{\\Delta T}\\right)$\nWe can also calculate the perturbation space of temporal encoding. We define delta partitions D($\\Delta$) and count the number of the delta partitions sequences as we can distribute $\\Delta$ to N input neurons:\n$\\begin{aligned}D(\\Delta) &= \\left\\{ \\{\\Delta_n\\}_{n=0}^{N-1} | \\sum_{n=0}^{N-1} {\\Delta_n} = \\Delta \\right\\\\\\{s\\}_{n=0}^{N-1} &= \\left\\{ \\{\\Delta_n\\} \\in {\\mathbb{N}}^{N} | \\forall n \\Delta_n \\leq \\Delta \\right\\}\\\\ |D(\\Delta)| &= \\binom{N + \\Delta -1}{\\Delta} = O\\left(\\binom{N}{\\Delta}\\right) \\end{aligned}$\nWe can get delta sequences S($\\cdot$) along to input neurons, for each sequence in a delta partition ${\\Delta_n\\}_{n=0}^{N-1}$. We can estimate the upper bound using the Lagrangian multiplier method with logarithm:\n$\\begin{aligned}S(\\{\\Delta_n\\}) &= \\left\\{\\{\\Delta_n\\} \\in {\\mathbb{Z}}^{N} | \\forall n \\forall \\delta_n \\delta_n \\leq \\Delta_n\\right\\}\\\\ |S(\\{\\Delta_n\\}_{n=0}^{N-1})| &\\leq \\prod_{n=0}^{N-1}(1 + 2\\Delta_n) < (1 + 2\\frac{\\Delta}{N})^N.\\end{aligned}$\nTherefore, we can get the asymptotic upper bound on the number of perturbations as follows:\n$\\begin{aligned}|\\{s|s \\in S(d), d \\in D(\\Delta)\\}| \\leq \\binom{N+\\Delta-1}{\\Delta} \\left(1 + 2\\frac{\\Delta}{N}\\right)^N\\\\= O\\left(\\binom{N}{\\Delta} \\left(1 + 2\\frac{\\Delta}{N}\\right)^N\\right)\\end{aligned}$\nFrom the results above, we can compute the perturbation space ratio function $f = T^{\\Delta T}/(1 + 2\\Delta/N)^N$ and prove the statement."}, {"title": "Theorem 1.", "content": "The perturbation space of temporal encoding is exponentially smaller than that of rate encoding for T \u2265 8.\nProof. From Lemma 1, we compute the partial derivative of ln f over a as follows:\n$\\frac{\\partial}{\\partial \\alpha} \\text{ln } f = NT \\text{ ln }T - \\frac{2T}{1 + 2 \\alpha T}$\nObserve that the derivative over a is always positive where $T > e^2 \\approx 7.39$. As $f = 1$ at N = 0 and T is an integer, f increases over T and N monotonically and exponentially for T > 8.\nNote that there still exists $\\alpha$, which makes the ratio exponential for T < 8. Most of our experiments are conducted in T = 5, N = 10, and $\\Delta$ = 1. In T = 5 and N = 10, the space of rate encoding is greater than that of temporal, where $\\alpha \\geq 0.05$ and $\\Delta = \\alpha NT \\approx 2.5$."}, {"title": "4.3 Direct Counterexample Search", "content": "Using Lemma 1, we can predict that verification times of SNNs only depend on $\\Delta$ and N, which is the number of input neurons. However, the experiment section of our paper shows that verification time increases exponentially as the number of time steps does. Assuming that the verification time inefficiency is from the SMT solver, we propose a direct counterexample search (DCS) algorithm that does not use an SMT solver but brings entirely the same result with an SMT solver. Algorithm 1 describes DCS in pseudocode. This method provides a solution to the exponentially increasing size of the model space of SMT solvers, which grows along with the number of hidden neurons and the number of time steps. This phenomenon is even observed in temporal encoding."}, {"title": "5 Experiments", "content": "We have implemented the proposed verification algorithm based on the high-performance SMT solver Z3Py (De Moura and Bj\u00f8rner 2008), the Z3 API in Python developed by Microsoft Research. To train and infer the model, we have used NumPy (Harris et al. 2020), snnTorch (Eshraghian et al. 2023a) and PyTorch (Paszke et al. 2019), S4NN (Kheradpisheh and Masquelier 2020). Experiments were conducted on the AMD EPYC 7763 2.45GHz CPU and 1TB of RAM.\nTo balance the practicality and ease of the experiment, we used the MNIST and FMNIST datasets. Each verification of a model was conducted with 14 fixed inputs and ran once, except for MNIST DCS with $\\Delta$ = 1, which had 5 runs. To get an appropriate performance of the SNNs in reasonable verification time, we set the number of hidden neurons to 20 in the experiment on time steps and the number of time steps to 5 on the number of neurons. A was set to 1. The model accuracies of rate encoding range from 52% to 67%, and those of temporal encoding range from 70% to 80%. The lower model accuracy in rate encoding can affect its mean verification time by the ratio of robust and not robust instances, but we can ignore this because rate encoding was extremely slow even on not robust instances, which is generally faster than robust instances.\nVerification time with different number of hidden neurons With rate encoding, most MNIST instances failed verification in N > 12 within 180000 seconds, and only 8 of 84 instances are verified to be not robust in FashionMNIST within 150,000 seconds. In contrast, presents that verification time is very small in temporal encoding. The verification time does not look like the following exponential time complexity. Using Lemma 1, we can infer that it is because perturbation space does not depend on the number of hidden neurons.\nVerification time with different time steps Rate encoding has not had a single successful verification in about 250000 seconds on T > 7 and has succeeded in only 14 instances in T < 6 at the MNIST dataset. Moreover, there was no successful verification in the FashionMNIST dataset in about 150000 seconds. On the contrary, shows verification time at temporal encoding over different time steps. The result differs from the theoretical prediction: temporal encoding performs well even in T < 8. Verification time in temporal encoding still increases exponentially but shows much less verification time than the baseline. In contrast to lemma 1, It shows exponential time complexity.\nVerification time of DCS We found an inconsistency between the theoretical prediction and the experiment results in the time complexity in the number of time steps. We assumed the problem was caused by the inefficiency of the SMT solver, especially by the number of the time step terms. We devised DCS algorithm that directly generates the perturbation set and checks whether any adversarial counterexample exists. The number of time steps and hidden neurons are different from the temporal encoding experiment. This is because the verification times were too low, so the results could be noisy. We also conducted a simple experiment in DCS, using the perturbation space as inputs and the original prediction as labels. We succeeded in making not robust instances to be robust."}, {"title": "6 Conclusions and Future Work", "content": "We have proposed the SMT encoding of the temporally encoded SNNs for efficient robustness verification of SNNs. The key contributions of the paper are to exponentially reduce the size of perturbation space by exploiting temporal encoding of input instead of rate encoding and to reduce the inefficiency of the SMT solver. We have achieved the exponential reduction in the perturbation space by restricting each neuron to fire only once, which we have proved theoretically while also providing empirical evidence. However, as the time step increases, we observe an exponential increase in verification time, which is contrary to our prediction and can be interpreted as the SMT solver's inability to narrow the search space properly. So we designed the DCS algorithm and succeeded in making the verification times barely depend on the number of time steps. We also succeeded in getting reasonable verification time in large models, and now we can also use GPUs to accelerate formal verification of SNNs due to separable SNN inference.\nWhile this study has achieved impressive progress in the formal verification of SNNs, further research is still needed. Despite the verification time dependency on the number of input neurons, we could not try formal verification to more input neurons because of the training difficulty of temporal SNNs. Additionally, experiments on more complex datasets and more complex network architectures, such as convolutional SNNs, are necessary. We tried to train SNNs in cifar10 (Krizhevsky, Hinton et al. 2009), but without convolutional architecture, it was hard to train. In view of adversarial training, we also have challenges. Fine-tuning the network with the counterexamples will help the adversarial training of SNNs."}, {"title": "A Appendix", "content": "Here we provide detailed information about the experiments introduced in the main paper, which could not be included in the main text due to space constraints."}, {"title": "A.1 Hyperparameter Setting for Experiments", "content": ""}, {"title": "A.2 DCS Experiment Results", "content": "In this section, we report the experimental results of the proposed DCS algorithm in more detail."}]}