{"title": "Towards Efficient Formal Verification of Spiking Neural Network", "authors": ["Baekryun Seong", "Jieung Kim", "Sang-Ki Ko"], "abstract": "Recently, AI research has primarily focused on large lan-\nguage models (LLMs), and increasing accuracy often in-\nvolves scaling up and consuming more power. The power\nconsumption of AI has become a significant societal issue; in\nthis context, spiking neural networks (SNNs) offer a promis-\ning solution. SNNs operate event-driven, like the human\nbrain, and compress information temporally. These charac-\nteristics allow SNNs to significantly reduce power consump-\ntion compared to perceptron-based artificial neural networks\n(ANNs), highlighting them as a next-generation neural net-\nwork technology. However, societal concerns regarding AI go\nbeyond power consumption, with the reliability of AI mod-\nels being a global issue. For instance, adversarial attacks on\nAl models are a well-studied problem in the context of tra-\nditional neural networks. Despite their importance, the sta-\nbility and property verification of SNNs remains in the early\nstages of research. Most SNN verification methods are time-\nconsuming and barely scalable, making practical applications\nchallenging. In this paper, we introduce temporal encoding to\nachieve practical performance in verifying the adversarial ro-\nbustness of SNNs. We conduct a theoretical analysis of this\napproach and demonstrate its success in verifying SNNs at\npreviously unmanageable scales. Our contribution advances\nSNN verification to a practical level, facilitating the safer ap-\nplication of SNNs.", "sections": [{"title": "1 Introduction", "content": "Currently, AI research is primarily focused on large lan-\nguage models (LLMs) since the advent of powerful mod-\nels such as OpenAI's ChatGPT (Brown et al. 2020). It is\nwell known that the performance of AI models generally\nimproves as the number of parameters increases (Montufar\net al. 2014; Goodfellow et al. 2013). However, this also leads\nto a significant increase in power consumption (Desislavov,\nMart\u00ednez-Plumed, and Hern\u00e1ndez-Orallo 2023; Samsi et al.\n2023), which leads to environmental side effects such as car-\nbon emissions and climate change.\nSpiking neural networks (SNNs) are neural networks\nthat mimic the human brain, and computing SNNs by\nmimicking neurons in electronic circuits consumes less\npower than typical perceptron-based artificial neural net-\nworks (ANNs) (Kim et al. 2020). Many researchers claim\nthat SNNs are the future of AI research and could be an ef-\nfective alternative to address the excessive power consump-\ntion problem caused by current ANNs (Eshraghian et al.\n2023b; Tavanaei et al. 2019). However, studies supporting\nthese claims are still in their early stages.\nOn the other hand, many people suggested the poten-\ntial risk of adversarial attacks on neural networks by intro-\nducing a carefully designed perturbation to the input data,\nwhich causes severe problems with neural networks' secu-\nrity and safety (Akhtar and Mian 2018; Moosavi-Dezfooli\net al. 2017; Szegedy et al. 2013). Since neural networks are\nessentially function approximators (Hornik, Stinchcombe,\nand White 1989), it is important to verify the approxima-\ntion ratio, which is the range of error in their output, to\nestimate the robustness of the networks to be deployed in\nsafety-critical applications. To date, there have been a lot of\nattempts to formally verify ANNs (Albarghouthi et al. 2021;\nKatz et al. 2022; Tjeng, Xiao, and Tedrake 2017; Bunel et al.\n2020).\nHowever, it is still an early stage for SNNs, and several re-\nsearchers have only suggested the plausibility of formal ver-\nification of SNNs (De Maria, Di Giusto, and Laversa 2020;\nBanerjee et al. 2023; De Maria, Di Giusto, and Ciatto 2017).\nRecently, there has been a promising study that aims to ver-\nify the local adversarial robustness of SNNs using satisfia-\nbility modulo theories (SMT) solvers (Banerjee et al. 2023).\nThey have shown that it is theoretically possible to verify the\nrobustness of SNNs, but it is notoriously difficult to apply in\npractice due to its high computational cost, even at small\nscales. It is not difficult to speculate that the rigorous formal\nverification of SNNs is generally more difficult than that of\ntraditional ANNs because of the temporal and discrete na-\nture of the network. For instance, the state space of SNNs\ninvolves one more dimension, the temporal dimension, as\nthe input is described as a spike train along the time axis\nrather than an instantaneous one. In addition, the activation\nof SNNs is a discrete function of the potential, which is not\ndifferentiable, unlike ANNs. These two properties make rig-\norous verification and analytic interpretation more difficult\nfor SNNs than for ANNS.\nIn this paper, we present an efficient verification algorithm\nfor SNNs, and the following are our key contributions:\n1. Formulate spiking neural network with temporal encod-\ning as SMT solver constraints.\n2. Theoretically analyze the impact of temporal encoding\non the speed of formal verification."}, {"title": "2 Related Work", "content": "2.1 Formal Verification of Neural Networks\nFormal verification of neural networks is an active area of\nresearch, mainly related to robustness (Albarghouthi et al.\n2021). There are two main ways to verify the proper-\nties of neural networks, abstraction-based verification and\nconstraint-based verification. Methods based on constraint-\nbased verification approximate the neural network using ab-\nstract interpretation theory. Katz et al. (Katz et al. 2022) have\nextended the simplex method to perform verification for\nneural networks with ReLU activation functions, which are\nfrequently employed in deep neural networks. Later, Katz\net al. (Katz et al. 2019) introduced the Marabou framework\nby extending Reluplex by considering arbitrary piecewise-\nlinear activation functions. Tjeng et al. (Tjeng, Xiao, and\nTedrake 2017) have utilized mixed-integer linear program-\nming (MILP) to represent ReLU and achieved better ro-\nbustness verification performance than Reluplex. Bunel et\nal. (Bunel et al. 2020) have utilized a branch-and-bound al-\ngorithm to perform formal verification. Most of abstraction-\nbased verification methods for ANNs are based on piece-\nwise linear activation in real space, which is hard to be ap-\nplied to the discrete spikes of SNNs. On the other hand,\nconstraint-based methods use SMT solver, which can also\napplied in discrete activations. Pulina and Tacchella used an\nSMT solver to verify the properties of ANN (Pulina and Tac-\nchella 2012). Amir et al. (Amir et al. 2021) applied an SMT\nsolver to verify the properties of a binarized neural network,\nextending ReLUplex. Guo et al. (Guo et al. 2023) proposed\nan efficient SMT-based verification method for occlusion ro-\nbustness.\n2.2 Mathematical Modeling of SNNs\nDue to its event-driven and time-varying nature, the math-\nematical models for SNNs differ substantially from those\nfor ANNs. De Maria et al. (De Maria, Di Giusto, and\nCiatto 2017; De Maria, Di Giusto, and Laversa 2020) have\nproposed a formalization of SNNs based on timed au-\ntomata (Alur and Dill 1994; Waez, Dingel, and Rudie 2013)\nnetworks. As each neuron of SNN can be modeled by a\ntimed automaton, they have constructed a network that con-\nsists of a set of timed automata running in parallel and shar-\ning channels according to the structure of the SNN. The au-\nthors have validated the constructed model against several\nproperties of SNNs via temporal logic formulae.\nMore recently, SMT-based encoding of SNN for adver-\nsarial robustness verification has been introduced (Banerjee\net al. 2023). The authors have adapted the quantifier-free lin-\near real arithmetic constraints to encode the overall mathe-\nmatical operations in SNNs. They have further encoded the\nadversarial robustness conditions based on potential pertur-\nbations on input as logical constraints and demonstrated that\nSMT solvers can be successfully used for modeling and veri-\nfying the robustness of SNNs. However, the experiments are\nconducted on relatively small SNNs with a restricted bench-\nmark due to the limited scalability of the proposed approach.\n2.3 Information Encoding in SNNs\nThree main ways to convert input data in SNNs are\nrate encoding, temporal encoding, and delta modula-\ntion (Eshraghian et al. 2023b). Rate encoding is a commonly\nused method in SNNs that interprets the size of the input\nas the probability of spiking and generates random spikes\nbased on this probability (Adrian 1926). This method, which\nfollows the simple intuition that the size of the informa-\ntion is proportional to the number of spikes, is simple to\nimplement because the gradient descent of the ANN can\nbe applied through a surrogate derivative (Guerguiev, Lilli-\ncrap, and Richards 2017). However, it consumes more power\nbecause the number of spikes is large, and neuromorphic\nchips implementing SNNs consume more power as the num-\nber of spikes increases. Temporal encoding generally forces\neach input neuron to encode only once, and the larger the\ninput, the faster it spikes (Johansson and Birznieks 2004;\nGollisch and Meister 2008). It consumes significantly less\npower but is generally harder to train because it is harder\nto use the gradient descent of an ANN as is. Delta modula-\ntion encodes data to spike when it increases over time. It re-\nquires sequential data, so it is not used in most cases (Petro,\nKasabov, and Kiss 2019). Mostafa (Mostafa 2017) intro-\nduced the gradient descent learning algorithm on temporal-\nencoded SNN, using the property that the input-output la-\ntency relation in temporal-encoded SNN is differentiable al-\nmost everywhere. Kheradpisheh et al. (Kheradpisheh and\nMasquelier 2020) improved Mostafa's algorithm (Mostafa\n2017) and published an implementation in a public online\nrepository. Yamamoto et al. (Yamamoto, Sakemi, and Ai-\nhara 2022) proposed temporal encoding without single spike\nrestriction.\n2.4 Adversarial Robustness in SNNS\nThe adversarial robustness of neural networks is important\nin practical use, so various research studies have been con-\nducted to increase the adversarial robustness of SNNs, like\nANNs. Zhou (Zhou et al. 2021) has suggested the method\nto train robust temporal encoded SNN. Liang et al. designed\nS-IBP and S-CROWN for robust training of SNN. (Liang\net al. 2022) \u00d6zdenizci et al. presented an algorithm that con-\nverts ANNs to SNNs and fine-tunes it to strengthen robust-\nness. (\u00d6zdenizci and Legenstein 2023) Ding et al. showed a\nstochastic gating spiking neural model, which is biologically\nplausible, to increase robustness. (Ding et al. 2024) Chen et\nal. proposed a way to train robust SNN using the inspiration\nfrom the visual masking effect and filtering theory. (Chen,\nSun, and Xu 2024)"}, {"title": "3 Preliminaries", "content": "3.1 Spiking Neural Networks\nVariable definitions We need to define variables to de-\nscribe SNN. \\(T\\) is the number of simulated time steps, \\(L\\) is the\ntotal number of layers where the first layer is the input layer,\nand \\(N_l\\) is the number of neurons in layer \\(l\\). For simplicity,"}, {"title": "3.2 Adversarial Attack", "content": "In ANNs, adversarial attacks are generated by adding a real-\nvalued perturbation to the original input. However, we can-\nnot use this approach to SNNs because the spike trains do\nnot consist of a real number but binary. Moreover, consider-\ning that the inputs of rate are far from the inputs of temporal"}, {"title": "4 Methodology", "content": "First, we introduce our SMT encoding for temporal encoded\nSNN. We describe mathematical behavior and adversarial\nrobustness conditions in quantifier-free linear real arithmetic\nformulas for the sound and complete verification of SNNs.\nSecond, we theoretically compare the proposed encoding\nand the previous SMT encoding (Banerjee et al. 2023) for\nSNNs with rate encoding in terms of combinatorial com-\nplexity of adversarial perturbations on input.\n4.1 SMT Encoding of Temporal Encoded SNN\nWe encode expressions to describe SNN, which consists of\nIF neurons. In the following paragraphs, we denote the nth\nneuron in the lth layer by the pair (n, l). Spike time \\(s_{l,n}\\) of\nthe neuron (n, l) is described by a non-negative integer with\nthe following constraint:\n\\[\\pounds_1 = \\bigwedge_{l=1}^{L-1} \\bigwedge_{n=0}^{N_l-1} [s_{l,n} \\geq \\tau_l \\land s_{l,n} < T - 1] .\\]\nThe condition \\( \\pounds_1 \\) asserts that spike time must be in the range\n[\\( \\tau_l \\), T \u2013 1] since we assume that there are synaptic delays\nin the spike time at each layer. Simply speaking, a neuron\n(n, l) for all 0 \u2264 n \u2264 \\(N_l\\) - 1 cannot spike before \\( \\tau_l \\) as it\nrequires at least l synaptic delays.\nPotential \\(p_{l,t,n}\\) of the neuron (n, l) at time step t is a real\nnumber satisfying the following conditions:\n\\[\\pounds_2 = \\bigwedge_{l=1}^{L-1} \\bigwedge_{n=0}^{N_l-1} [p_{l,0,n} = 0]\\]\n\\[\\pounds_3 = \\bigwedge_{l=1}^{L-1} \\bigwedge_{n=0}^{N_l-1} \\bigwedge_{t=1}^{T-1}\np_{l,t,n} = \\sum_{m=0}^{N_{l-1}-1} w_{l-1,m,n} \\cdot 1(s_{l-1,m} \\leq t)\\]\nwhere 1 is an indicator function and \\(w_{l,m,n}\\) is a learnable\nweight of the neuron (n, l) to the neuron (n, l + 1). \\( \\pounds_2 \\) and\n\\( \\pounds_3 \\) assert that neurons integrate the sum of weighted spikes\nfrom the previous layer.\n\\[\\pounds_4 = \\bigwedge_{l=1}^{L-1} \\bigwedge_{n=0}^{N_l-1} \\bigwedge_{t=1}^{T-1} \\left(a_{l,t,n} = \\bigvee_{t'=0}^{t-1} p_{l,t',n} \\geq \\theta \\right)\\]\n\\[\\pounds_5 = \\bigwedge_{l=1}^{L-1} \\bigwedge_{n=0}^{N_l-1} \\bigwedge_{t= \\tau_l}^{T-2} [(\\neg a_{l,t-\\tau,n} \\land p_{l,t-\\tau,n} \\geq \\theta) = (s_{l,n} = t)]\\]\n\\[\\pounds_6 = \\bigwedge_{l=1}^{L-1} \\bigwedge_{n=0}^{N_l-1} [a_{l,\\tau-1-\\tau,n} = (s_{l,n} = T - 1)]\\]\nThe condition \\( \\pounds_4 \\) utilizes a flag variable \\(a_{l,t,n}\\) to indicate\nwhether the neuron (n, l) has ever spiked before time step\nt and \\( \\pounds_5 \\) states that if (n, l) has not spiked before time\nstep t \u2013 \\( \\tau \\) - 1 and its potential reaches the threshold at t - \\( \\tau \\),"}, {"title": "4.2 Theoretical Analysis", "content": "Here, we provide a theoretical explanation for why our SMT\nencoding is more efficient regarding the adversarial robust-\nness verification performance than the one formulated in the\nprevious work (Banerjee et al. 2023). The main intuition is\nto compare the perturbation space, which is the number of\npossible adversarial \u0394-perturbations on input, of the rate and\ntemporal encoding. Now, we are ready to provide the theo-\nretical analysis of the exponential complexity advantage by\nadopting temporal encoding instead of rate encoding.\nLemma 1. Let \u0394 = \u03b1TN, where 0 \u2264 \u03b1 \u2264 1. The pertur-\nbation space ratio of rate encoding to temporal encoding is\n\\(O((T^{\\alpha T} /(1 + 2 \\alpha T))^{N})\\).\nProof. Let us assume the input layers have the number of\nneurons N. The perturbation space of rate encoding can be\ncalculated by counting the ways to flip less than or equal to\n\u0394 spikes as follows:\n\\[\\sum_{d=0}^{\\Delta} \\binom{NT}{d} = O \\left(\\binom{NT}{\\Delta} \\right)\\]\nWe can also calculate the perturbation space of temporal\nencoding. We define delta partitions D(\u0394) and count the\nnumber of the delta partitions sequences as we can distribute\n\u0394 to N input neurons:\n\\[D(\\Delta) = \\left\\{\\{\\Delta_n\\}_{n=0}^{N-1} \\in \\mathbb{N}^{N} | \\sum_{n=0}^{N-1} \\Delta_n = \\Delta\\right\\}\\]\n\\[|D(\\Delta)| = \\binom{N + \\Delta - 1}{\\Delta} = O\\left(\\binom{N}{\\Delta}\\right)\\]\nWe can get delta sequences S(\u00b7) along to input neurons,\nfor each sequence in a delta partition \\({\\Delta_n}_{n=0}^{N-1}\\). We can"}, {"title": "4.3 Direct Counterexample Search", "content": "Using Lemma 1, we can predict that verification times of\nSNNs only depend on \u0394 and N, which is the number of\ninput neurons. However, the experiment section of our pa-\nper shows that verification time increases exponentially as\nthe number of time steps does. Assuming that the verifica-\ntion time inefficiency is from the SMT solver, we propose\na direct counterexample search (DCS) algorithm that does\nnot use an SMT solver but brings entirely the same result\nwith an SMT solver. Algorithm 1 describes DCS in pseu-\ndocode. This method provides a solution to the exponen-\ntially increasing size of the model space of SMT solvers,\nwhich grows along with the number of hidden neurons and\nthe number of time steps. This phenomenon is even observed\nin temporal encoding."}, {"title": "5 Experiments", "content": "We have implemented the proposed verification al-\ngorithm based on the high-performance SMT solver\nZ3Py (De Moura and Bj\u00f8rner 2008), the Z3 API in\nPython developed by Microsoft Research. To train and in-\nfer the model, we have used NumPy (Harris et al. 2020),\nsnnTorch (Eshraghian et al. 2023a) and PyTorch (Paszke\net al. 2019), S4NN (Kheradpisheh and Masquelier 2020).\nExperiments were conducted on the AMD EPYC 7763\n2.45GHz CPU and 1TB of RAM.\nTo balance the practicality and ease of the experiment, we\nused the MNIST and FMNIST datasets. Each verification of\na model was conducted with 14 fixed inputs and ran once,\nexcept for MNIST DCS with \u0394 = 1, which had 5 runs. To\nget an appropriate performance of the SNNs in reasonable\nverification time, we set the number of hidden neurons to 20\nin the experiment on time steps and the number of time steps\nto 5 on the number of neurons. \u0394 was set to 1. The model ac-\ncuracies of rate encoding range from 52% to 67%, and those\nof temporal encoding range from 70% to 80%. The lower\nmodel accuracy in rate encoding can affect its mean verifi-\ncation time by the ratio of robust and not robust instances,\nbut we can ignore this because rate encoding was extremely\nslow even on not robust instances, which is generally faster\nthan robust instances.\nVerification time with different number of hidden neu-\nrons With rate encoding, most MNIST instances failed\nverification in N > 12 within 180000 seconds, and only\n8 of 84 instances are verified to be not robust in FashionM-\nNIST within 150,000 seconds. In contrast, Figure 4 (right)\npresents that verification time is very small in temporal en-"}, {"title": "6 Conclusions and Future Work", "content": "We have proposed the SMT encoding of the temporally en-\ncoded SNNs for efficient robustness verification of SNNs.\nThe key contributions of the paper are to exponentially re-\nduce the size of perturbation space by exploiting temporal\nencoding of input instead of rate encoding and to reduce the\ninefficiency of the SMT solver. We have achieved the ex-\nponential reduction in the perturbation space by restricting\neach neuron to fire only once, which we have proved theoret-\nically while also providing empirical evidence. However, as\nthe time step increases, we observe an exponential increase\nin verification time, which is contrary to our prediction and\ncan be interpreted as the SMT solver's inability to narrow\nthe search space properly. So we designed the DCS algo-\nrithm and succeeded in making the verification times barely\ndepend on the number of time steps. We also succeeded in\ngetting reasonable verification time in large models, and now\nwe can also use GPUs to accelerate formal verification of\nSNNs due to separable SNN inference.\nWhile this study has achieved impressive progress in the\nformal verification of SNNs, further research is still needed.\nDespite the verification time dependency on the number of\ninput neurons, we could not try formal verification to more\ninput neurons because of the training difficulty of temporal\nSNNs. Additionally, experiments on more complex datasets\nand more complex network architectures, such as convolu-\ntional SNNs, are necessary. We tried to train SNNs in ci-\nfar10 (Krizhevsky, Hinton et al. 2009), but without convolu-\ntional architecture, it was hard to train. In view of adversarial\ntraining, we also have challenges. Fine-tuning the network\nwith the counterexamples will help the adversarial training\nof SNNs."}, {"title": "A Appendix", "content": "Here we provide detailed information about the experiments introduced in the main paper, which could not be included in the\nmain text due to space constraints.\nA.1 Hyperparameter Setting for Experiments\nTable 2 provides the hyperparameter settings used in our experiments. We use the Adam optimizer (Kingma and Ba 2015)\nwith a learning rate of 5e-4 for training the baseline SNN. Following the configurations from S4NN (Kheradpisheh and Masque-\nlier 2020), we set the learning rate of temporal SNN to 0.2 and initialize the input-hidden and hidden-output synaptic weights\nwith random values drawn from uniform distributions in the ranges [0, 5] and [0, 50]. Note that we use the off-the-shelf optimizer\nfor S4NN, which is publicly available1, for training the temporal SNN in our experiments.\nA.2 DCS Experiment Results\nIn this section, we report the experimental results of the proposed DCS algorithm in more detail."}]}