{"title": "COAST: INTELLIGENT TIME-ADAPTIVE NEURAL OPERATORS", "authors": ["Zhikai Wu", "Shiyang Zhang", "Sizhuang He", "Sifan Wang", "Min Zhu", "Anran Jiao", "Lu Lu*", "David van Dijk*"], "abstract": "We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. Across different systems, more complex dynamics receive more granular time steps. Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems.", "sections": [{"title": "1 Introduction", "content": "Partial differential equations (PDEs) are fundamental to modeling complex physical phenomena, from fluid dynamics to quantum mechanics [1]. While traditional numerical methods such as finite differences and finite elements have been the cornerstone of scientific computing [2], they often struggle with modern challenges involving multi-scale dynamics and complex geometries. Adaptive solvers have emerged as a powerful solution, dynamically adjusting spatial and temporal discretization based on local error estimates [3]. These methods significantly improve computational efficiency by concentrating computational resources where needed most. However, they face inherent limitations: high computational overhead for frequent refinement, careful parameter tuning requirements, and challenges with stiff problems [4].\nRecent advances in machine learning, particularly operator learning frameworks, have opened new avenues for PDE solving [5]. Instead of traditional numerical approximations, these approaches learn mappings between function spaces, enabling rapid solution prediction for entire families of PDEs. Several architectures have demonstrated remarkable success: Fourier Neural Operators (FNO) leverage the spectral domain for efficient learning [5], DeepONet employs the universal approximation theorem for operators [6], and transformer-based models like Oformer [7], DPOT [8], CViT [9], Transolver [10] adapt attention mechanisms for PDE solving. These methods have achieved impressive results across various applications, from fluid dynamics [11], solid mechanics [12], to heat transfer [13], often leading to reasonable accuracy and great computational efficiency.\nDespite these advances, current machine learning approaches for PDE solving face a significant limitation: they typically operate with fixed time steps. This constraint becomes particularly problematic when dealing with problems exhibiting multiple time scales or rapid temporal variations. To address this limitation, we design Causal Operator with Adaptive Solver Transformer (COAST) as shown in fig. 1, which is motivated by adaptive numerical solvers and can dynamically adjust their temporal resolution based on solution dynamics. Our contribution is summarized as below:\n\u2022 Causal Operator with Adaptive Solver Transformer (COAST). We introduce COAST, a neural operator that self-determines the solution time step sizes. COAST can autonomously decide the prediction step and solve efficiently in continuous time using interpolation methods."}, {"title": "2 Method", "content": "COAST is a novel neural operator architecture in the core of a causal language model (CLM). COAST gives a time-adaptive and temporally continuous approach to solve time-dependent physical systems by leveraging a trainable spatiotemporal encoding for time-sequential spatial inputs on continuous time points, a causal language model to output spatially-temporally coupled embedding of the next predicted step, and an interpret-modify mechanism to interpret the next solving time step from the coupled embedding and modify the embedding using the interpreted time step to obtain the spatial embedding of the predicted solution."}, {"title": "2.1 Architecture Description", "content": "The COAST architecture consists of four main components: a spatio-temporal encoder, a causal language model, an interpret-modify mechanism, and an interpolation decoder.\nSpatio-temporal Embeddings The COAST encoder takes as input a gridded representation of the input states u, yielding a spatio-temporal data tensor $u \\in \\mathbb{R}^{T\\times H \\times W \\times D}$ with D channels, and a time series $T_{seq}$ made up of T float numbers standing for the relative times of each of the 2D spatial frames $S_i \\in \\mathbb{R}^{H \\times W \\times C}$. 1 We patchify our input into $u_p \\in \\mathbb{R}^{T\\times \\frac{H}{H_p} \\times \\frac{W}{W_p} \\times C}$ by tokenizing each $S_i$ independently using CNN with overlapped kernels. We then add trainable 2D spatial positional embeddings to each $S_i$ and use a common FiLM layer to embed the time series $T_{seq}$ into each $S_i$ and then we get $u_{st} \\in \\mathbb{R}^{T\\times \\frac{H}{H_p} \\times \\frac{W}{W_p} \\times C}$. For convenience in the next step, we transpose $u_{st}$ to $u'_{st} \\in \\mathbb{R}^{(\\frac{H}{H_p} \\times \\frac{W}{W_p})\\times T \\times C}$.\n$u_p = u_s + PE_s, PE_s \\in \\mathbb{R}^{\\frac{H}{H_p} \\times \\frac{W}{W_p} \\times C}$ \n$u'_{st} = transpose(FiLM(T_{seq}, u_s))$ \nWhen the input frames are all on fixed time points, the $T_{seq}$ can be omitted because now the $T_{seq}$ can be computed mechanically by the model itself."}, {"title": "Causal Language Model Predictor", "content": "We use a causal language model(CLM), specified as GPT-2, as the core of our model. The channel number C of $u'_{st}$ should be equal to the embedding dimension of the CLM. Then the input embedding $u'_{st}$ can be considered as $(\\frac{H}{H_p} \\times \\frac{W}{W_p})$ sentences made up of T tokens. We input the $u'_{st}$, where batch size is $(\\frac{H}{H_p} \\times \\frac{W}{W_p})$, into the first hidden states of the CLM. For each sentence, we take the last token of its last hidden states as output. We assert that the output tokens $z\\in \\mathbb{R}^{(\\frac{H}{H_p} \\times \\frac{W}{W_p})\\times 1 \\times C}$ carries information on both the predicted spatial solution and the predicted proper time-step size."}, {"title": "Interpret-Modify Mechanism", "content": "To interpret the predicted time step size from z, we apply an MLP model, named interprator, on each token $z_i$ in z independently to get $(\\frac{H}{H_p} \\times \\frac{W}{W_p})$ time step sizes. Given that each output token is actually a spatial patch of the total space and they should have the same status in terms of time, one interprator is enough to be applied on all tokens. We take the average of the $(\\frac{H}{H_p} \\times \\frac{W}{W_p})$ time step sizes as the predicted time step size $\\delta t_{orig}$.\n$\\delta t_{orig} = mean(MLP(z_i))$ \nIn many cases, $\\delta t_{orig}$ is likely to converge to 0 during training in order to make it easier to predict the spatial solution more correctly. But too small $\\delta t_{orig}$ will impede the extension of the rollout and make our adaptive time-stepping meaningless. In addition, sometimes too large $\\delta t_{orig}$ can also push the model's training and prediction in a direction we don't expect. Therefore, we set a lower bound and a upper bound on $\\delta t_{orig}$ to get $\\delta t$ and add a term\u00b2 to the loss function to penalize comparatively small $\\delta t$.\n$\\delta t = clip(\\delta t_{orig}, LB, UB)$ \n$Loss = Loss_{spatial} + Loss_{\\delta t}$ \nTo get the final predicted spatial embedding, we use $\\delta t$ to modify the output tokens z. We apply another FiLM layer on each token of z and rearrange the first dimension of z in two dimensions to get $z_s \\in \\mathbb{R}^{\\frac{H}{H_p} \\times \\frac{W}{W_p} \\times 1 \\times C}$ \n$z_s = FiLM(\\delta t, z)$"}, {"title": "Interpolation Decoder", "content": "To reconstruct the output spatial state $u' \\in \\mathbb{R}^{H \\times W \\times D}$ from $z_s$, we learn a transposed-convolution model with overlapped kernels just mirror-symmetric with the encoder's structure.\nFinally, if we want to get a solution state $u_s$ on any continuous time point $t = t_s$ between [0, $\\delta t$], just use $t_s$ to perform linear interpolation between the last input state $u_{-1}$, where t = 0, and $u'$, where t = $\\delta t$.\nIf the target time point $t_s$ is larger than $\\delta t$, just forward the model over and over again until the sum of time step sizes is over $t_s$."}, {"title": "2.2 COAST vs. Traditional Adaptive Solvers", "content": "Traditional adaptive solvers have been widely used in numerical computation to address challenges in solving differential equations, optimization problems, and other complex models. Key methodologies include Adaptive Mesh Refinement (AMR) [14, 15], which dynamically refines grids for PDEs with localized features like shocks, and adaptive time-stepping [4], which adjusts step sizes in ODE solvers based on error estimates. Error estimation techniques, such as those by [16], have been critical in guiding adaptive refinement in finite element methods, while adaptive quadrature methods [17] optimize numerical integration by adjusting sampling points. In optimization, adaptive strategies like the Nelder-Mead method [18] dynamically modify search parameters to improve convergence. Despite their success, traditional adaptive solvers face challenges such as computational overhead and scalability, which have spurred recent efforts to integrate them with machine learning techniques. For instance, physics-informed neural networks [19] and data-driven sparse identification [20] have shown promise in enhancing adaptive solvers by leveraging data to predict optimal refinement strategies or model dynamics. This work builds on these foundations, exploring the synergy between classical adaptive methods and modern data-driven approaches to overcome existing limitations."}, {"title": "3 Experiments", "content": "We compare COAST against popular neural operators on four challenging benchmarks in physical sciences. In addition to demonstrating COAST's performance against strong baselines, we analyze its loss accumulation behavior and its efficiency in terms of inference time. Furthermore, we investigate the relationship between the adaptive time-stepping behavior and the underlying physical intrinsicities, which lie in different temporal regions and varying system parameters. Finally, we perform ablation studies to evaluate the scalability of COAST in terms of adaptivity and accuracy."}, {"title": "COAST model setup", "content": "For all experiments, unless otherwise stated, we use a patch size of 32 \u00d7 32 for tokenizing inputs and an embedding dimension of 768 for CLM predictor. We use a piecewise quadratic-type function as $Loss_{dt}$ to impose a penalty on $\\delta t$."}, {"title": "Baselines", "content": "We benckmark our framework against operatoer learning framework Fourier Neural Operator [21] and DPOT [22] and vision models Dilated ResNet [23], CNextU-Net [24]and AViT [25]. For all baselines, we adhere to the training and evaluation protocols outlined in the The Well datasets [26] or follow the configurations recommended in the original papers. Detailed implementation settings for the baseline models are provided in appendix A.2."}, {"title": "Evaluation method for adaptive solvers", "content": "We can find that the existing datasets and the operator learning models are all on fixed time points. But for COAST, as an adaptive solver, we can query an arbitrary continuous future time $t_s$ as the target time point where the expected solution is located. In order to make a more reasonable comparison with other baselines, we propose a method to evaluate the accuracy of adaptive solvers by obtaining the longest fixed-step sequence after an adaptive prediction.\nWe now set the time interval between two temporally-neighboring 2D frames in the original input data $u_{in} \\in \\mathbb{R}^{T \\times H \\times W \\times D}$ to be the unit time 1, and the time of the last 2D frame $u_{-1}$ is set to be 0 as a starting time point. We use an interpolation method to obtain the output solution $u_{pred}$ of COAST on fixed time points. When we get $\\delta t$ from COAST, let $T' = [\\delta t]$. Then for each $t_s$ from 1 to $T'$, perform linear interpolation between the last input state $u_{-1}$, where t = 0, and $u'$, where t = $\\delta t$. Note that now the lower bound for $\\delta t$ can be set slightly larger than 1 to ensure that we can obtain at least one solution on the t = 1 time point. Details are shown in algorithm 1."}, {"title": "Task description", "content": "We let models (including all baselines and our COAST) accept input data $u_{in} \\in \\mathbb{R}^{T \\times H \\times W \\times D}$ for T = 4 moments and generates a predicted sequence $u_{pred} \\in$ $[][\\mathbb{R}^{T' \\times H \\times W \\times D}$ for the next T' moments. In the training phase, we use T' to stand for the default output sequence length, which is minimized to 1 and does not exceed 8, for the baseline models, or the adaptive output sequence length of the COAST. We use the real data $u_{true}$ of these next T' moments as labels and calculate the average loss across the T' predicted solutions. 3 In the validation phase, we make T' = 8 so that the model should rollout to autoregressively generates $u_{pred}$ to find the average loss between $u_{pred}$ and $u_{true}$."}, {"title": "3.1 Rollout Simulation Results", "content": "Here we provide our main results across four challenged benchmarks from the dataset The-Well [26]. The full details on the underlying equations, dataset generation and problem setup for each case are provided in appendix A.3.\n\u2022 Active matter (AM) [27]: Active matter systems, composed of energy-transforming agents that generate orientation-dependent viscosity and transmit forces, exhibit complex nonlinear spatiotemporal dynamics in viscous fluids.\n\u2022 Turbulent radiative layer (TR) [28]: Turbulent mixing between cold dense gas clumps and hot ambient gas generates rapidly cooling intermediate-temperature regions, where the competition between radiative energy loss and turbulent velocity fields nonlinearly regulates cold phase growth or dissolution.\n\u2022 Viscoelastic fluids (VF) [29]: viscoelastic FENE-P fluid flow in wall-bounded geometries, resolving coupled Navier-Stokes and nonlinear conformation tensor dynamics to study multiscale elasto-inertial phenomena.\n\u2022 Rayleigh-B\u00e9nard convection (RB) [30]: A buoyancy-driven turbulent flow arising from thermally induced density gradients in fluid layers bounded by contrasting thermal boundary conditions, exhibits nonlinear multiscale transport phenomena critical to geophysical, astrophysical, and engineered systems.\nWe presents the results of COAST against several competitive and heavily optimized baselines in table 1. Our proposed method achieves the lowest VRMSE loss on almost all baselines. The best results are shown in bold and the next best results are underlined. Additional visualizations of our models are shown in fig. 2, fig. 3 and appendix A.4."}, {"title": "Loss accumulation", "content": "Loss accumulation is a major problem for operator learning models. During the rollout process, the model adds the predicted results to the input sequence to predict the solution further in time, so the error of the previous prediction results with the real data will exacerbate the error of the later prediction. Moreover, none of the models use the full length of rollout predictions in training, so examining the behavior of loss accumulation is important for evaluating the capability of models.\nFor the four benchmarks, we show the average loss for each moment of the rollout process of COAST and other baseline models in fig. 4. As shown in fig. 4, COAST not only achieves the lowest loss on the four benchmarks, but also exhibits the lowest level of error accumulation behavior. This illustrates the stability of our model in long time period prediction."}, {"title": "3.2 Inference Time", "content": "The speed of rollout inference is another important metric for evaluating operator learning models, which can reflect the efficiency of a operator-learning solver. As an adaptive solver, COAST can use fewer prediction steps for rollout inference. The distribution of the number of steps used by COAST to perform rollout prediction on each of the four benchmark datasets is shown in fig. 5 (a), where the rollout length is 8.\nThus, COAST has the potential to have higher solving efficiency than other baseline models, although the CLM structure makes it have a relatively large number of parameters. We compared the average rollout inference time of COAST and other baseline models over the 4 benchmarks when the rollout length is 8. The results are presented in fig. 5 (b).\nFurther, a rollout process of length 8 does not seem to be sufficient to fully utilize the high inferential efficiency of COAST as an adaptive solver. Our interpolation method makes COAST predict step lengths that tend to exceed the time span of the interpolation, and the percentage of this exceeding interval will be larger in rollouts of shorter lengths. In addition, in longer intervals, COAST will have more opportunities to generate more longer step lengths. Therefore, to more fully evaluate the inference efficiency of COAST, we extend the length of the rollout to 16, 32, and 64 to compute the average inference time of COAST and other baseline models, and the results can be seen in fig. 5 (c)."}, {"title": "3.3 Significance of Variable Step Sizes", "content": "As shown in fig. 5 (a), the number of rollout steps used by COAST shows a distribution on the four benchmarks, which comes from the fact that the predicted step size given by COAST is variable. We next explore the relationship between this variability and the nature of the systems themselves. And in doing so, we can explain the reasons for this variability in the length of steps."}, {"title": "Cross dynamics analysis", "content": "fig. 6 shows the distributions of time step sizes given by the model when rolling out inferences for subdatasets with different parameters. Here the step sizes are averaged over each rollout trajectory. It can be seen that under the same dataset, different subdatasets can be distinguished according to the distribution of step sizes predicted by COAST.\nFurther, we explore the physical meaning behind these parameters or types. In the cases we have shown, these parameters or types can characterize the complexity of their systems. Specifically, the absolute value of $\\alpha$ in the active matter (AM) problem has a negative correlation with system complexity, while the Rayleigh number (Ra) in the Rayleigh-B\u00e9nard convection (RB) problem has a positive correlation with system complexity. The specific impact of each parameter on their systems can be seen in appendix A.3.1 and appendix A.3.4. As shown in fig. 6, for systems of higher complexity, the model tends to predict smaller step sizes and more concentrated distributions, and vice versa. This indicates that the model learns to predict the step size based on the complexity of the system. For systems of higher complexity, where changes between temporally-neighboring frames are generally more complex, the model exhibits more \"cautious\" characteristics. On the other hand, for systems of lower complexity, the model tends to make more \"bold\" predictions with longer step sizes to improve prediction efficiency. This is a strong indication of the intelligence of COAST, and in particular its core model CLM, in dealing with the solution of PDE equations."}, {"title": "Cross time regions analysis", "content": "Similarly, we explore the adaptive behavior of the model in different time intervals of the same dynamics. fig. 7 shows the average time step sizes predicted by COAST for each moment in the same type of dynamics. 5"}, {"title": "3.4 Scalability", "content": "Here we verify the effect of the model's scalability on the turbulent radiative layer (TR) benchmark.\nWe tested our model with three different parameter sizes and obtained the rollout accuracy as shown in fig. 8 (a). The COAST configurations are detailed in table 2. This reflects that the prediction accuracy of our model is positively correlated with the number of parameters and has a good scalability.\nInterestingly, as shown in fig. 8 (b), a smaller number of parameters leads to a larger average step size. One potential explanation for this is as follows. In training our loss function contains two terms, spatial a spatial loss$Loss_{spatial}$ and a step-size loss $Loss_{\\delta t}$, where $Loss_{\\delta t}$ imposes a greater penalty on smaller step sizes. When the model is not sufficiently large to extract the complex dependencies in the space, it becomes challenging to continue decreasing $Loss_{spatial}$. Consequently, the model shifts to lowering $Loss_{\\delta t}$ by lengthening the step size."}, {"title": "4 Discussion", "content": "Summary This work introduces COAST, a new neural operator architecture that utilizes causal language models at its core to address the challenges of learning complex physical systems. COAST combines the strengths of causal language models and adaptive solution methods to achieve state-of-the-art accuracy and minimal error accumulation behavior on challenging benchmarks in energy transformation, fluid dynamics, and thermodynamic processes. Our approach demonstrates the potential of employing advanced causal language models to develop more flexible and accurate machine learning models for the physical sciences. Key innovations of our work include: (a) an efficient solver on continuous time for autonomous decision prediction step sizes, (b) a rational method for evaluating time-adaptive solvers, and (c) an exploration of the deep understanding of PDE systems embodied in causal language models for operator learning.\nOur empirical results in various PDE benchmark tests show that COAST's time-adaptive approach endows it with higher solving efficiency in prediction over longer sequences. This time-adaptive behavior helps to build more general solvers. In addition, the step size distributions predicted by COAST when confronted with different systems and states can also be used to explore some of the more intrinsic properties of dynamical systems. The broader impact of this work based on COAST is that it has the potential to accelerate scientific discovery by more efficiently and accurately modeling complex physical systems over longer time horizons, with applications ranging from energy transformation modeling to engineering design.\nLimitations & Future Work While COAST advances neural operator capabilities, several limitations need attention. First, current experiments focus on systems with regular geometries and uniform grids, leaving performance on complex geometries (e.g., fractured porous media, turbulent multiphase flows) unexplored. Second, while the architecture shows empirical stability in moderate rollout lengths, its error propagation behavior under extended autoregressive prediction horizons remains unexamined. Third, the current implementation operates as a specialized solver rather than a generalizable framework, limiting direct applicability to PDE systems requiring coupled multi-physics modeling.\nFuture research should prioritize extending COAST's framework toward multimodal PDE foundation models capable of unifying diverse physical systems under a single architecture. This could involve integrating physical constraints via hybrid symbolic-neural frameworks that enforce various physical laws. Another particularly promising direction lies in coupling COAST with LLMs\u2014such integration could enable cross-modal reasoning where textual system descriptions guide dynamics prediction, or conversely, where learned physical representations enhance LLMs' capacity for quantitative scientific reasoning. We believe that addressing these challenges will enable the synergistic integration of physics-informed machine learning and foundation models, paving the way for next-generation computational tools across scientific domains."}, {"title": "A Experimental Details", "content": "A.1 Training and Evaluation\nA.1.1 Training recipe\nWe use a unified training recipe for all COAST experiments. We employ AdamW optimizer [31] with a weight decay 10-5. Our learning rate schedule includes an initial linear warm up phase of 5 epochs, starting from zero and gradually increasing to 5 \u00d7 10\u22125, followed by an exponential decay at a rate of 0.9 for every 5,000 steps. The spatial loss function is a one step mean squared error (MSE) between the model predictions and the corresponding targets at the predicted time steps, average over all coordinates:\n$MSE = \\frac{1}{BQD} \\sum_{i=1}^B \\sum_{j=1}^Q \\sum_{k=1}^D (\\hat{s}_k(y_j) - s_k(y_j))^2$,\nwhere $s_k(y_j)$ denotes the k-th variable of the i-th sample in the training dataset, evaluated at a query coordinate yj, and $\\hat{s}$ denotes the corresponding model prediction. All models are trained for the same number of epochs with a equal batch size B on each benchmark.\nFor COAST, we need to consider the add a term to the loss function to penalize comparatively small dt the model outputs. Here we choose to use a piecewise power-exponential function of dt as Lossdt, where 0 < \u0454 < 1 and n is the power that larger than 1:\n$Loss_{dt} = \\begin{cases} (1 + \\epsilon - \\delta t)^n  & \\delta t < 1 + \\epsilon \\\\ 0 & \\delta t > 1 + \\epsilon  \\end{cases}$\nBased on our ablation study, changing the values of e and n does not affect the training results. Without loss of generality, we take e to be 0.5 and n to be 2."}, {"title": "A.1.2 Evaluation", "content": "After training, we obtain the predicted trajectory by performing an auto-regressive rollout of 8 time points on the test dataset. We evaluate model accuracy using the VRMSE loss (Variance Scaled Root Mean Squared Error), recommended in The-Well [26]:\n$VRMSE(u, v) = <|u - v|\u3009^2/(<|u - \\bar{u}|^2) + \\epsilon)^{1/2}$ Note that, since VRMSE(u, \u016b) \u2248 1, having VRMSE > 1 indicates worse results than an accurate estimation of the spatial mean \u016b."}, {"title": "A.2 Model Details", "content": "COAST. We use a patch size of 32 \u00d7 32 for embedding data. The encoder consists of 3-layer convolutional neural network (CNN) layers with padding and overlapped kernels and the decoder just mirrors it. We use an 2-layer MLP to intepret dt from output tokens by CLM. We use two FiLM layer in COAST, one is for embedding time series and another is for output tokens modification using output dt. We use a GPT-2 structure with multi-attention heads as our core causal language model. We use GELU as active function in the structures above.\nFNO. Our FNO model employs 3 Fourier neural blocks, each with 128 channels and 16 Fourier modes.\nDilated ResNet. We adopt the Dilated ResNet which has a hidden dimention of 96 and 32 residual blocks.\nCNextU-Net. We employs a U-Net with a Conv Next structure reported by The -Well[26], which has 8 blocks per stage and 42 initial features.\nAVIT. We adopt an AViT model with a hidden dimension of 768 and 12 attention heads.\nDPOT. We employs a DPOT with an AFNO as mixing type and Exp-MLP time aggregation. The patch size is 4 and blocks number is 16. For challenging comparison, We set its embedding dimension to 1024, depth to 16 and number of attention head to 16. The out layer of the model has a dimension of 32. The active function in it is also GELU."}, {"title": "A.3 Benchmarks", "content": "Dataset. In all the following benchmarks, we make use of the datasets released by The Well [26]. This dataset consists of 15T data of discretized initial conditions on diverse types and parameter-sets.\nBelow, we collect and summarize the descrition from The Well of each dataset we used.\nProblem setup. We follow the problem setup by The Well [26]. Our objective is to predict the future solution within 8 moments. We compare the COAST model's performance against several strong baseline neural operators: FNO[5], Dilated Resnet[32], CNextU-Net[24], AViT[25] and DPOT[8], most of those are well performed on The Well datasets reported by (the well). We use different training, validation and testing data split[26]."}, {"title": "A.3.1 Active Matter (AM)", "content": "This dataset comprises simulations of a continuum theory describing the dynamics of N rod-like active particles in a Stokes fluid within a two-dimensional domain of linear size L. The data include 81 time-steps of 256 \u00d7 256 resolution per trajectory, with fields such as concentration (scalar), velocity (vector), orientation tensor, and strain-rate tensor. Simulations explore parameter variations in alignment (C), dipole strength (a), and other coefficients, capturing phenomena like energy transfer across scales, vorticity-orientation coupling, and the isotropic-to-nematic phase transition. Periodic boundary conditions and uniform Cartesian grids are employed, with data stored at 0.25-second intervals over a 20-second timespan. Refer to [27] for details on problem formulation and detailed equations.\nNote that a is the dimensionless active dipole strength. Base on the original paper[27], the greater the absolute value of a, the faster the system approaches order/stability. It can be analogous to viscosity in the fluid problem to some extent. So smaller a means higher complexity."}, {"title": "A.3.2 Turbulent Radiative Layer (TR)", "content": "This dataset explores the dynamics of turbulent radiative layers in astrophysical systems, where hot and cold gases mix, leading to the formation of intermediate-temperature gas that rapidly cools. The simulations model the Kelvin-Helmholtz instability in a 2D domain, with cold, dense gas at the bottom and hot, dilute gas at the top. The data capture key phenomena such as mass flux from the hot to cold phase, turbulent velocities, and the distribution of mass across temperature bins. The dataset includes 101 timesteps of 384\u00d7128 resolution for 90 trajectories, varying the cooling time tcool across nine values. Simulations were performed using Athena++ on a uniform Cartesian grid with periodic boundary conditions in the x-direction and zero-gradient in the y-direction. This dataset provides insights into the phase structure, energetics, and dynamics of multiphase gas in astrophysical environments, such as the interstellar and circumgalactic media.\n$\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v}) = 0$ \n$\\frac{\\partial \\rho \\mathbf{v}}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v} \\mathbf{v} + P) = 0$ \n$\\frac{\\partial E}{\\partial t} + \\nabla \\cdot ((E + P)\\mathbf{v}) = - \\frac{E}{t_{cool}}$ \n$E = \\frac{P}{\\gamma - 1}; \\gamma = 5/3$"}, {"content": "where p is the density, is the 2D velocity, P is the pressure, E is the total energy, and tcool is the cooling time."}, {"title": "A.3.3 Viscoelastic Fluids (VF)", "content": "This dataset explores the multistability of viscoelastic fluids in a two-dimensional channel flow, capturing four distinct attractors: the laminar state (LAM), a steady arrowhead regime (SAR), Elasto-inertial turbulence (EIT), and a chaotic arrowhead regime (CAR). These states coexist for the same set of parameters, with their emergence dependent on initial conditions. The dataset includes snapshots of these attractors as well as edge states, which lie on the boundary between basins of attraction and provide insight into transitions between flow regimes. The data were generated using direct numerical simulations of the FENE-P model, solving for velocity, pressure, and the conformation tensor fields. Key phenomena include chaotic dynamics in EIT and CAR, as well as the multistability of the system. The dataset, comprising 260 trajectories with 512\u00d7512 resolution, is valuable for studying viscoelastic turbulence and"}, {"title": "A.3.4 Rayleigh-B\u00e9nard Convection (RB)", "content": "This dataset comprises simulations of two-dimensional, horizontally periodic Rayleigh-B\u00e9nard convection, capturing the dynamics of fluid motion driven by thermal gradients. The system consists of a fluid layer heated from below and cooled from above, leading to the formation of convective cells and complex flow patterns. The dataset includes 200 timesteps of 512 \u00d7 128 resolution for 1,750 simulations, varying the Rayleigh number (106 to 1010), Prandtl number (0.1 to 10), and initial buoyancy perturbations. Fields such as buoyancy (scalar), pressure (scalar), and velocity (vector) are provided, with periodic boundary conditions horizontally and Dirichlet conditions vertically. The data, generated using the Dedalus framework, offer insights into turbulent eddies, convection cells, and the sensitivity of flow structures to initial conditions. This dataset is valuable for studying thermal convection phenomena and validating numerical models in fluid dynamics.\nThe time domain problem is formulated as\n$\\frac{\\partial b}{\\partial t} - \\kappa \\Delta b = -u \\nabla b$,\n$\\frac{\\partial u}{\\partial t} - \\nu \\Delta u + \\nabla p - bez = -u \\nabla u$,\nwith boundary conditions\n$b(z = 0) = L_z , b(z = L_z) = 0$,\n$u(z = 0) = u(z = L_z) = 0$\nNote that the Rayleigh number (Ra) satisfy the relation: $\\frac{Ra}{(\\nu\\kappa)} = (prandtl)$. It means that the greater Ra means smaller viscosity, then the system will approaches order/stability slower. So greater Ra means higher complexity."}]}