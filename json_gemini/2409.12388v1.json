{"title": "Disentangling Speakers in Multi-Talker Speech Recognition with Speaker-Aware CTC", "authors": ["Jiawen Kang", "Lingwei Meng", "Mingyu Cui", "Yuejiao Wang", "Xixin Wu", "Xunying Liu", "Helen Meng"], "abstract": "Multi-talker speech recognition (MTASR) faces unique challenges in disentangling and transcribing overlapping speech. To address these challenges, this paper investigates the role of Connectionist Temporal Classification (CTC) in speaker disentanglement when incorporated with Serialized Output Training (SOT) for MTASR. Our visualization reveals that CTC guides the encoder to represent different speakers in distinct temporal regions of acoustic embeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC (SACTC) training objective, based on the Bayes risk CTC framework. SACTC is a tailored CTC variant for multi-talker scenarios, it explicitly models speaker disentanglement by constraining the encoder to represent different speakers' tokens at specific time frames. When integrated with SOT, the SOT-SACTC model consistently outperforms standard SOT-CTC across various degrees of speech overlap. Specifically, we observe relative word error rate reductions of 10% overall and 15% on low-overlap speech. This work represents an initial exploration of CTC-based enhancements for MTASR tasks, offering a new perspective on speaker disentanglement in multi-talker speech recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "Natural human conversations always involve multiple speakers, with varying degrees of speech overlaps. Multi-talker speech recognition (MTASR) has emerged as a critical field, aiming to transcribe these natural conversational speech. While traditional automatic speech recognition (ASR) tasks typically perform monotonic speech-to-text sequence mapping, MTASR presents unique challenges: recognition models are required to disentangle speech from distinct speakers, and separately transcribe their speech.\nIn recent years, many approaches have been proposed to address this challenge. These approaches can be categorized into two types based on their ways of differentiating speakers: branched acoustic encoder (BAE) based models and serialized output training [1] based models. BAE models leverage structural priors to disentangle speakers: they separate mixed speech into independent branches, then use shared recognition blocks to transcribe different speakers in parallel. To align branches with respective speakers, permutation invariant training (PIT) [2], [3] is applied to calculate ASR loss. Yu et al. [4] first adopt a BAE-style model with PIT loss. Seki et al. [5] further extend this approach in a fully end-to-end manner. Subsequently, Chang et al. [6] incorporated transformer backbone into this framework. Further works [7]\u2013[9] explored streaming ASR following the BAE framework. More recently, sidecar separator-based methods [10]\u2013[12] were proposed to convert a single-talker ASR system into a multi-talker one through model-internal separation.\nAnother line of research lies on Serialized Output Training (SOT) [1]. The SOT approach serializes text from different speakers as a single stream, with a special token (sc) as a delimiter between speakers. In contrast to the structural priors of BAE models, this approach relies on attention mechanisms in attention encoder-decoder (AED) [13] to disambiguate speakers. This confers an advantage in that it does not require pre-defining speakers and branch numbers, allowing it to handle a variable number of speakers. The superiority of SOT methods has been demonstrated in the M2Met challenges [14], [15], which provided challenging \"in the wild\" multi-talker meeting speech. SOT methods have been further enhanced with speaker information [16], [17], time boundary [18], [19], learnable speaker orders [20], and integrated with BAE structures as a hybrid branched SOT model [21].\nIn contrast to these two paradigms, there is a lack of investigation on the role of connectionist temporal classification (CTC) [22] in MTASR. CTC has become a fundamental training criterion for sequence-to-sequence tasks including speech recognition. Specifically, CTC introduces a blank token to construct alignments between input and target sequences, providing a method to compute posterior probabilities by summing over all possible alignment paths between inputs and target sequences. Compared to other ASR architecture of AED and Neural Transducer [23], [24], CTC generates all tokens in the sequence simultaneously in a non-autoregressive manner, thus enabling faster decoding speeds. CTC was also used together with AED models as a joint CTC/Attention model [25], which has long been considered one of the state-of-the-art approaches for speech recognition. In the MTASR domain, although the original SOT adopted the AED architecture without including CTC loss, many studies have empirically demonstrated that the joint CTC/Attention SOT model can effectively improve recognition accuracy on overlapped speech [18], [26]\u2013[28]. However, given CTC's monotonicity assumption, it is counter-intuitive that CTC can non-monotonically map overlapped speech to serialized transcriptions, and there is a lack of literature investigating these results.\nIn this work, we investigated the effect of CTC, especially when incorporated with SOT. Our experiments with conformer encoders reveal that CTC loss enables the acoustic encoder to represent different speakers at distinct temporal within the acoustic embeddings. Distinct from existing BAE and SOT approaches, we attribute CTC's speaker distinction capability to its non-autoregressive reordering capability [29], [30], which is potentially a novel direction for speaker disentanglement in MTASR. Building on this insight, we proposed a novel speaker-aware CTC (SACTC) as an enhanced and tailored CTC variant for multi-talker scenarios. This SACTC explicitly models speaker disentanglement by constraining the encoder to represent different speakers' tokens at specific temporal locations. This is achieved by the Bayes risk CTC framework, where we introduced a speaker-aware risk function to penalty CTC paths with undesired token emit. In experiments, SACTC was used as an auxiliary loss for SOT-based MTASR models. Experimental results demonstrate that the SOT-SACTC model consistently outperforms the standard SOT- CTC approach across various degrees of speech overlap. Notably, we observe word error rate reductions of 10% overall and of 15% on low-overlap speech. To our knowledge, this work represents the first exploration of CTC-based enhancements for MTASR tasks."}, {"title": "II. METHODS", "content": "CTC loss guides sequence-to-sequence models by maximizing the posterior probability $p(l|x)$ of the target sequence, where $x = [x_1,..., x_T]$ represents the input acoustic embedding, e.g., from an acoustic encoder, and $l = [l_1,..., l_u]$ represents the transcription label sequence. To compensate for the length discrepancy between x and l, CTC introduces blank tokens \u00d8 into the label sequence I to construct alignment paths (also as known as CTC labels) $\u03c0 = [\u03c0_1, ..., \u03c0_T]$ between x and l. $\u03c0_t$ denotes the output token at time step t. A collapsing function \u0392(\u03c0) = l maps alignment paths to text labels by collapsing repeated consecutive labels into a single label and removing all blank labels (e.g., B(\u00d8a\u00d8aabb) = aab). Subsequently, the posterior probability $p(l|x)$ of the label sequence can be calculated by summing up the posteriors of all possible alignments:\n$P(l|x) = \\sum_{\u03c0\u2208B^{-1}(l)}p(\u03c0|x)$\nwhere $\u03c0\u2208 B^{-1}(l)$ if B(\u03c0) = l. $p(\u03c0|x)$ denotes the posterior probability of path \u3160, calculated by the product of posterior probabilities of $\u03c0_t$ cross T time steps:\n$p(\u03c0|x) = \\prod_{t=1}^{T}p(\u03c0_t|x_t) = \\prod_{t=1}^{T}y^\u03c0_t$\nHere $p(\u03c0_t|x_t)$ typically consists of linear projection and softmax function to generate frame-wise posterior of t at the t-th frame. We denote the output as $y_t$.\nConsidering the combinational explosion of permutating all alignment paths, forward-backward algorithm [31] is commonly used to calculate $P(l|x)$ effectively. First, the original label sequence l is extended by inserting symbol between any two non-blank tokens: $l' = [\u00d8, l_1, \u00d8, ..., \u00d8, l_u, \u00d8]$. Then recursively compute the forward-backward variables $\u03b1(t, v)$ and $\u03b2(t, v)$:\n$\u03b1(t, v) = \\sum_{\u03c0:B(\u03c0_{1:t})=B(l_{1:v})} \\prod_{t'=1}^{t}y^{\u03c0_{t'}}_{t'}$\n$\u03b2(t, v) = \\sum_{\u03c0:B(\u03c0_{t:T})=B(l'_{v:2U+1})} \\prod_{t'=t}^{T}y^{\u03c0_{t'}}_{t'}$\nwith (t, v) is a node in CTC lattice and 1<t<T, 1 <v <2U + 1. These two variables summarized posterior of all paths passing through node (t, v), with exact 1 : v prefix and v : 2U + 1 suffix alignment. Subsequently, for any consent time step t, enumerating all possible tokens v in l' will consider all possible paths. Therefore the CTC posteriors can be calculated by:\n$P(l|x) = \\sum_{\u03c0\u2208B^{-1}(l)} p(\u03c0|x) = \\sum_{v=1}^{2U+1} \u03b1(t,v) \u00b7 \u03b2(t, v) y^{l'}_t$\nConsider a two-talker scenario with serialized target label $l = [l_1, ..., l_M, (sc), l'_1,...,l'_N]$, where a and b stand for 2 speakers and (sc) token separates them. We denote the alignment path as $\u03c0 = [\u03c0_1, ..., \u03c0_K, \u03c0_{K+1},..., \u03c0_T]$, where $\u03c0_K$ represent the last (sc) token. During CTC training, we consider the case where multi-talker overlapped speech was encoded by an acoustic encoder, resulting in embedding x. Eq. 2 suggests that the information carried by x is inherently encouraged to align with \u03c0. I.e., $[x_1,...,x_{K\u22121}]$ encodes speaker a and $[x_{K+1}, ..., x_T]$ encodes speaker b. We note that the \"speaker boundary\" K varies across alignment paths, potentially confusing the encoder on how different speakers are represented. Furthermore, embedding x with a nondeterministic speaker boundary may complicate subsequent processing, e.g., hindering a cascaded ASR decoder from recognizing different speakers.\nAddressing this issue, we propose a speaker-aware CTC training objective as an enhanced and tailored loss function for MTASR. The core idea is to constrain the encoder model to represent different speakers' tokens at specific time frames, which explicitly models speaker disentanglement. To control CTC prediction, the Bayes risk CTC (BRCTC) framework [32] was used to introduce preference over alignment paths. Specifically, BRCTC defined Bayes risk function r(\u03c0) over posteriors of alignment paths, and the training objective is:\n$I_{br} (l, x) = \\sum_{\u03c0\u2208B^{-1}(l)}r(\u03c0)\u00b7p(\u03c0|x)$\nAs paths with the same concerned property could share the same risk value, we group paths by the ending point (frame) of a certain non-blank token and use group-wise risk functions, to control the encoding frames of specific speakers. Given a constant non-blank token $l_u = l'_{2u}$, the ending point of $l_u$ is exclusive over time frames t, thus CTC posterior can be alternatively calculated by enumerating all possible frames, and the training objective can be reformulated as:\n$I_{brctc} (l, x) = \\sum_{t=1}^{T}rg(t)\u00b7 \\frac{\u03b1(t, 2u) \u00b7 \u03b2(t, 2u)}{y^{l'}_{t}}$\nin which $\u03b2(t, 2u)$ is a revised backward variable, summarizing posteriors of the paths where $l_u$ ends at t, i.e., t = argmaxt for 1<\u03c4<T, s.t. $\u03c0_\u03c4 = l_{2u}$. This can be achieved by eliminating non-ending paths such that $\u03c0_{t+1} = l_{2u}$ for t <T. Accordingly:\n$\u03b2'(t, 2u) = \\begin{cases} \u03b2(t, 2u) - \u03b2(t + 1, 2u) \u00b7 y^{l'}_{t}, & \\text{if } t < T \\\\ \u03b2(t, 2u), & \\text{Otherwise} \\end{cases}$\nThe above grouping strategy inherits the original derivation in [32]. With Eq. 7 enables computing the training objective by summing over time steps, we define the following speaker-aware risk function to constrain the emitting time of tokens with consideration of their belonging speakers:\n$r_{sa}(s,t) = \\begin{cases} \\frac{1}{1+e^{(A(t-b))}}, & \\text{if } s = 1 \\\\ \\frac{1}{1+e^{-(A(t-b))}}, & \\text{Otherwise} \\end{cases}, b = M / \\frac{M}{M+N}$\nin which s represents speaker index where speakers were ordered chronologically following the first-in-first-out serialization strategy. A is an adjustable Bayes risk factor controlling the sharpness of risks, X = 0 leads to uniform risks for all paths. And b is a ratio of speaker utterance lengths, used to determine a speaker boundary K invariant to alignment paths. The $r_{sa}(s,t)$ overall is a conditional Sigmoid function, assigning high or low risks according to the established speaker boundary. Subsequently, the training objective for a certain $l_u$ is:\n$I_{sa} (l, x, s, u) = \\sum_{t=1}^{T}r_{sa}(s,t)\u00b7 \\frac{\u03b1(t, 2u) \u00b7 \u03b2(t, 2u)}{y^{l'}_{t}}$\nNote that $r_{sa} (s, t)$ is consistently < 0, so here we minimize expected Bayes risk $I_{sa}$, which contrasts with maximizing a posterior as in"}, {"title": "III. EXPERIMENTAL SETUP", "content": "Dataset Our experiments employed LibriSpeechMix (LSM) [1] as a benchmark dataset. This dataset is derived from the LibriSpeech (LS) [33] corpus, simulated both 2-speaker (LSM-2mix) and 3-speaker (LSM-3mix) mixed speech. As LSM only provides development and test sets, we generated 2-speaker mixed speeches for training using the similar protocol as in [1], [21]. Specifically, for each sample in the LS 960-hour training set, we randomly sample another sample with a random offset to mix with. We expect a practical MTASR model can simultaneously handle single- and multi-talker scenarios. Thus the generated mixed data was combined with the single-talker LS training set, resulting in our training set containing around 560k utterances with 1.7k hours of speech. To prob model performance on varying degrees of overlapped speech, we further divided the LSM test set into three subsets, representing low, medium, and high overlap conditions. The corresponding overlap ratios are (0, 0.2], (0.2, 0.5], and (0.5, 1.0] respectively. The overlap ratio here is defined as the duration of overlaps divided by the total duration of mixed speech. Besides, we concatenate transcriptions from separate speakers as text labels, using the first-in-first-out serialization strategy.\nModel settings We implemented CTC and AED ASR models with the ESPnet2 toolkit [34]. For the CTC model, we use a conformer encoder with 12 conformer blocks. Each block has 4-head self-attention with 256 hidden units and two 1024-dimensional feed-forward layers (macaron style). The AED model has an additional transformer decoder, comprising 8 transformer blocks with also 4 heads self-attention and 256 hidden units, but a 2048-dimensional feed-forward layer. As a result, CTC model has 22.14M parameters and AED model has 34.18M parameters.\nTraining settings The CTC models were trained with vanilla CTC or proposed speaker-aware CTC (SACTC) objectives. And AED model was trained with sole AED loss (w/o CTC) or joint-CTC/attention"}, {"title": "IV. RESULTS AND DISCUSSIONS", "content": "In this section, we first analyze the effect of vanilla CTC in SOT-based MTASR. We then present and discuss the experimental results of the proposed SACTC approach, comparing it to vanilla CTC.\nPrevious research has demonstrated that integrating CTC with SOT improves MTASR performance. We reproduced these experiments, with results presented in Table I, systems A to C. Comparing CTC with SOT, we observe that while CTC generally performed worse, it achieved better WER on the low-overlap subset (7.5 vs. 9.0). Moreover, incorporating CTC with SOT (C1) didn't enhance single-talker performance but improved multi-talker recognition, particularly on the low-overlap subset (9.0\u21927.5). However, the addition of CTC led to decreased performance on mid- and high-overlap speech. These results validate that CTC could assist in recognizing low-overlap speech, while it degrades performance when encountering more severe overlaps.\nTo better understand the interaction of CTC and multi-talker speech, we examined the attention patterns in the conformer encoder for different speakers. In detail, we visualized the top 50 attended frames in self-attention for each CTC-emitted token, then accumulated attentions for tokens from two speakers in distinct colors. Fig. 2 illustrates an interesting pattern: two speakers generally attended all frames in shallower blocks, while from layer 10 onwards, two speakers began to focus on distinct regions. This aligns with our derivation in Section II-B. Notably, we did not observe this phenomenon in the sole SOT system. Fig. 3 further visualize the attention matrices in the last conformer block. Compared to the sole SOT system, the use of CTC leads to information re-ordering: certain portions of the input embedding are attended to distinct regions of"}, {"title": "B. Performance of SACTC", "content": "We evaluated the proposed SACTC approach using a default risk factor parameter of 15. Initially, we trained a model with the SACTC objective alone. As shown in Table I, SACTC by itself did not outperform the vanilla CTC model (B1 vs. D1). However, when combined with SOT (D1), the model showed significant improvements over vanilla CTC: overall LSM-2mix WER improved from 8.8 to 8.0, and mid-overlap WER from 12.4 to 8.4. This outcome is understandable, as SACTC is designed to enhance MTASR embedding with deterministic speaker disentangling, thus not necessarily improving token-level recognition\u00b2. When integrated with SOT, SACTC enhanced low-overlap recognition similar to vanilla CTC, while mitigating performance degradation on more severe overlaps. Experiment E2 also supports this interpretation. For the model"}, {"title": "V. CONCLUSIONS", "content": "In this work, we investigated the effect of CTC in multi-talker speech recognition (MTASR) based on serialized output training (SOT). Our findings reveal that the CTC training objective guides the ASR encoder to encode different speakers into distinct temporal regions within acoustic embeddings. Building upon this insight, we leveraged the Bayes risk CTC framework and proposed a speaker-aware CTC (SACTC), an enhanced CTC variant tailored for MTASR. The core idea of SACTC is to constrain the encoder model to represent different speakers' tokens at specific time frames, explicitly modeling speaker disentanglement. SACTC was used as an auxiliary loss for SOT-based MTASR models in our experiments. Experimental results demonstrate that the SOT-SACTC model consistently outperforms the standard SOT-CTC approach across various degrees of speech overlap. Notably, we observe relative WER reductions of 10% overall and of 15% on low-overlap speech. To our knowledge, this work represents the first exploration of CTC-based enhancements for MTASR tasks. Future research directions may include extending SACTC to streaming seniors and exploring its potential in non-autoregressive speech recognition."}]}