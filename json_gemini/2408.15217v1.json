{"title": "Fundus2Video: Cross-Modal Angiography Video\nGeneration from Static Fundus Photography with\nClinical Knowledge Guidance", "authors": ["Weiyi Zhang", "Siyu Huang", "Jiancheng Yang", "Ruoyu Chen", "Zongyuan Ge", "Yingfeng Zheng", "Danli Shi", "Mingguang He"], "abstract": "Fundus Fluorescein Angiography (FFA) is a critical tool for\nassessing retinal vascular dynamics and aiding in the diagnosis of eye\ndiseases. However, its invasive nature and less accessibility compared\nto Color Fundus (CF) images pose significant challenges. Current CF\nto FFA translation methods are limited to static generation. In this\nwork, we pioneer dynamic FFA video generation from static CF im-\nages. We introduce an autoregressive GAN for smooth, memory-saving\nframe-by-frame FFA synthesis. To enhance the focus on dynamic le-\nsion changes in FFA regions, we design a knowledge mask based on\nclinical experience. Leveraging this mask, our approach integrates inno-\nvative knowledge mask-guided techniques, including knowledge-boosted\nattention, knowledge-aware discriminators, and mask-enhanced patch-\nNCE loss, aimed at refining generation in critical areas and addressing\nthe pixel misalignment challenge. Our method achieves the best FVD of\n1503.21 and PSNR of 11.81 compared to other common video generation\napproaches. Human assessment by an ophthalmologist confirms its high\ngeneration quality. Notably, our knowledge mask surpasses supervised\nlesion segmentation masks, offering a promising non-invasive alternative\nto traditional FFA for research and clinical applications. The code is\navailable at https://github.com/Michi-3000/Fundus2Video.", "sections": [{"title": "1 Introduction", "content": "Fundus Fluorescein Angiography (FFA) is an essential examination in ophthal-\nmology clinics, providing a dynamic view of retinal blood flow and lesion changes.\nIt offers critical insights into retinal circulatory dynamics, aiding in the identifi-\ncation of conditions such as diabetic retinopathy, hypertensive retinopathy, and"}, {"title": "2 Methods", "content": "We aim to generate a realistic FFA video \u0176 from a given CF image x, with\nthe ground-truth FFA video during training represented as Y. Considering the\ntemporal nature of FFA series, we adopt an autoregressive GAN architecture to\ncapture temporal dependencies and generate coherent video sequences. An au-\ntoregressive GAN generates image samples sequentially, conditioning each new\nimage on previously generated images and additional inputs. In our context of\ngenerating FFA videos from CF images, our autoregressive GAN, named Fun-\ndus2Video, based on the image-to-image translation GAN pix2pixHD, sequen-\ntially generates each frame of the FFA video, incorporating the CF image itself\nand the preceding frames. Building upon the generator, discriminator, and loss\ndesigns of pix2pixHD, our approach incorporates specific modifications to enable\nautoregressive and smooth generation. The architecture is as shown in Fig. 1 (a).\nTo ensure smooth output in Fundus2Video, we incorporate multi-frame input\nand smoothing techniques for longer temporal considerations. Specifically, we in-\nput three consecutive frames from the ground-truth FFA series to the model in"}, {"title": "2.2\nUnsupervised Clinically Supported Knowledge Mask", "content": "The baseline Fundus2Video can generate smooth and continuous FFA videos.\nHowever, it falls short of accurately depicting details like lesions and critical\nstructures as shown in Fig. 2 right, which are of utmost clinical importance. To\naddress this, we leveraged clinical insights to analyze ground-truth FFA videos,\nwhich tell us regions undergoing significant morphological changes during the\nFFA process often corresponded to crucial lesions or retinal structure areas that\npose challenges for the model. The theoretical basis is from [3]:\nDuring the FFA process, as the fluorescent dye flows through retinal ves-\nsels, significant leakage always occurs around the lesions, leading to visible\ndifferences between early and late stages.\nBuilding upon this knowledge, we devised a simple binary mask by computing the\ndifference between the first frame (representing the arterial phase) and the last\nframe (representing the late phase) and setting a specific threshold d determined\nthrough comparative experiments, which can be formulated as m = \u03b4(Yo\n\u04ae\u0442), where Yo represents the first frame of the ground-truth FFA video and YT\nrepresents the last frame. The process is depicted in Fig. 2. Unlike supervised\nlesion/structure segmentation masks, this knowledge mask requires no additional\nmanual annotation or segmentation model training. It can be easily derived from\nraw data, making it simple yet effective."}, {"title": "2.3 Knowledge Mask-Guided Video Generation", "content": "Knowledge-boosted Attention. Some types of lesions may be challenging to\ndetect in CF images due to low contrast, leading to synthesized FFA slices lacking"}, {"title": "3 Experiments", "content": "Dataset. Our dataset comprises 350 CF images and 18,180 corresponding FFA\nimages from 350 anonymous patients sampled from a large paired dataset. The\nFFA images were obtained using Zeiss FF450 Plus and Heidelberg Spectralis\nsystems, with a resolution of 768\u00d7768 pixels. Meanwhile, the CF images were\ncaptured by Topcon TRC-50XF and Zeiss FF450 Plus instruments, with resolu-\ntions ranging from 1,110\u00d71,467 to 2,600\u00d73,200 pixels. The Institutional Review\nBoard approved the study.\nImplementation Details. The final objective function (Eq. 7) was utilized to\ntrain the generative model, with AUP, ASP, Att, and AGAN set to 1, 1, 4, and\n2, respectively. The threshold & for obtaining the knowledge mask was set to 45.\nDuring training, each ground-truth FFA series produced 12 frames, with 4 slices\nrandomly selected from the vascular, venous, and late phases, respectively. Data\naugmentation techniques including random cropping, scaling, and color augmen-\ntation. The input images were resized to 512\u00d7512. Additionally, the model was\ntrained to randomly select either generated or ground-truth frames as input, en-\nhancing its adaptability and robustness. We employed the Adam optimizer with"}, {"title": "4 Conclusion", "content": "In this study, we propose Fundus2Video, which pioneers dynamic FFA video gen-\neration from static CF images using an autoregressive GAN architecture. With a"}], "equations": ["LAtt (A,m) = \\frac{1}{n} \\sum(A^{2} \u2013 m^{2})^{2}.", "LInfoNCE (v, v^{+}, v^{-}) = \u2212 log \\frac{e^{sim(v,v^{+})}}{e^{sim(v,v^{+})} + \\sum_{j=1}^{N} e^{sim(v,v_{j}^{-})}}.", "LMasked UP = LInfoNCE(m\u2299 z_{y}, m\u2299 z_{x}, m\u2299 z_{\\bar{x}}).", "LMasked SP LInfoNCE(m\u2299 z_{y}, m\u2299 z_{y}, m\u2299 z_{\\bar{y}}).", "LD (a, b, G(a)) = Ea,b[logDk(a,b)] + Ea[log(1 \u2013 Dk(a, G(a)))],", "LGAN (G, Dk) = LD_{1} (x, y, G(x)) + LD_{1} (x\u2299m, y \u2299m, G(x) \u2299 \u0442),", "L = AUPL MaskedUP + ASPL MaskedSP + Att LAtt + AGAN LGAN."]}