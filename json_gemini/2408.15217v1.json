{"title": "Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance", "authors": ["Weiyi Zhang", "Siyu Huang", "Jiancheng Yang", "Ruoyu Chen", "Zongyuan Ge", "Yingfeng Zheng", "Danli Shi", "Mingguang He"], "abstract": "Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal vascular dynamics and aiding in the diagnosis of eye diseases. However, its invasive nature and less accessibility compared to Color Fundus (CF) images pose significant challenges. Current CF to FFA translation methods are limited to static generation. In this work, we pioneer dynamic FFA video generation from static CF images. We introduce an autoregressive GAN for smooth, memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic lesion changes in FFA regions, we design a knowledge mask based on clinical experience. Leveraging this mask, our approach integrates innovative knowledge mask-guided techniques, including knowledge-boosted attention, knowledge-aware discriminators, and mask-enhanced patch-NCE loss, aimed at refining generation in critical areas and addressing the pixel misalignment challenge. Our method achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common video generation approaches. Human assessment by an ophthalmologist confirms its high generation quality. Notably, our knowledge mask surpasses supervised lesion segmentation masks, offering a promising non-invasive alternative to traditional FFA for research and clinical applications.", "sections": [{"title": "1 Introduction", "content": "Fundus Fluorescein Angiography (FFA) is an essential examination in ophthalmology clinics, providing a dynamic view of retinal blood flow and lesion changes. It offers critical insights into retinal circulatory dynamics, aiding in the identification of conditions such as diabetic retinopathy, hypertensive retinopathy, and macular degeneration [13]. Unlike Color Fundus (CF) images, FFA videos capture the dynamic filling process and real-time changes in retinal vascular abnormalities with greater clarity and depth, thereby enhancing diagnostic precision and facilitating a deeper understanding of disease progression and treatment response. However, due to its invasive nature and potential side effects, FFA\u2019s use is limited for certain individuals. In contrast, CF photography is non-invasive, readily available [27], and has been utilized in some deep-learning methods [6,21] for disease diagnosis. Therefore, generating realistic FFA videos from CF images holds significant research and application potential.\nWhen considering the generative models for FFA synthesis, the majority of existing methods [23,11,12,16,20] focus on specific phases, like the venous and late phase, using various Generative Adversarial Networks (GAN). However, they overlook the changes occurring throughout the entire FFA process, which includes multiple phases. While some approaches [2] can generate multiple discrete FFA images from different phases simultaneously, they still cannot capture the fully dynamic changes of retinal structures and lesions. Capturing lesional changes accurately is another challenge in FFA generation. While using lesion labels for conditional supervision could potentially enhance image details, the manual annotation of these labels is highly time-consuming and impractical for segmenting all possible lesion changes. Additionally, the time-consuming nature of FFA procedures makes it difficult to align FFA images precisely with CF images in clinical practice, due to blinking and movement, even with good patient cooperation [4,7]. This misalignment poses a significant challenge for pixel-to-pixel-based video generation processes.\nTo tackle these challenges, we propose a model leveraging an image-to-image GAN framework, specifically pix2pixHD [25], to generate smooth and stable FFA videos from single CF images autoregressively. Through clinical knowledge analysis of ground-truth FFA series, regions with significant lesion changes during the early and late FFA series examination are lesional changes, reflecting the damage in vascular or retinal pigment epithelium structure [3,28]. The larger the changes, the more important they are. Leveraging this insight, we design a knowledge mask that requires no additional manual labeling and enhances the generation of regions with high variability. Using this mask, we introduce novel knowledge mask-guided techniques into the baseline model to guide the model to focus more on key regions during learning and generation. Specifically, we propose a mask-enhanced patchNCE loss to address the pixel misalignment issue. This model holds the potential to generate FFA videos from CF images to other modalities and improve downstream tasks [1,22,19].\nIn summary, our research contributes as follows: 1. We are the first to generate dynamic FFA video directly from CF images, marking a significant advancement in ophthalmic imaging. Specifically, we introduce Fundus2Video, an autoregressive GAN architecture tailored for frame-by-frame FFA video synthesis from CF images. This architecture optimizes memory usage and ensures smooth output. 2. We introduce a knowledge mask derived from clinical insights to enhance focus on regions undergoing significant changes during dynamic FFA processes."}, {"title": "2 Methods", "content": "2.1 Overview\nWe aim to generate a realistic FFA video $\\hat{Y}$ from a given CF image $x$, with the ground-truth FFA video during training represented as $Y$. Considering the temporal nature of FFA series, we adopt an autoregressive GAN architecture to capture temporal dependencies and generate coherent video sequences. An autoregressive GAN generates image samples sequentially, conditioning each new image on previously generated images and additional inputs. In our context of generating FFA videos from CF images, our autoregressive GAN, named Fundus2Video, based on the image-to-image translation GAN pix2pixHD, sequentially generates each frame of the FFA video, incorporating the CF image itself and the preceding frames. Building upon the generator, discriminator, and loss designs of pix2pixHD, our approach incorporates specific modifications to enable autoregressive and smooth generation. \nTo ensure smooth output in Fundus2Video, we incorporate multi-frame input and smoothing techniques for longer temporal considerations. Specifically, we input three consecutive frames from the ground-truth FFA series to the model in\na sliding window fashion to provide longer temporal context for each generated frame. Instead of generating each frame independently, we aggregate the generated frames over a sliding window and perform triple-frame averaging. This approach smooths out abrupt transitions between frames and ensures continuity in the generated video sequence.\n2.2 Unsupervised Clinically Supported Knowledge Mask\nThe baseline Fundus2Video can generate smooth and continuous FFA videos. However, it falls short of accurately depicting details like lesions and critical structures , which are of utmost clinical importance. To address this, we leveraged clinical insights to analyze ground-truth FFA videos, which tell us regions undergoing significant morphological changes during the FFA process often corresponded to crucial lesions or retinal structure areas that pose challenges for the model. The theoretical basis is from [3]:\nDuring the FFA process, as the fluorescent dye flows through retinal vessels, significant leakage always occurs around the lesions, leading to visible differences between early and late stages.\nBuilding upon this knowledge, we devised a simple binary mask by computing the difference between the first frame (representing the arterial phase) and the last frame (representing the late phase) and setting a specific threshold $\\delta$ determined through comparative experiments, which can be formulated as $m = \\delta(Y_0 - Y_T)$, where $Y_0$ represents the first frame of the ground-truth FFA video and $Y_T$ represents the last frame. Unlike supervised lesion/structure segmentation masks, this knowledge mask requires no additional manual annotation or segmentation model training. It can be easily derived from raw data, making it simple yet effective.\n2.3 Knowledge Mask-Guided Video Generation\nKnowledge-boosted Attention. Some types of lesions may be challenging to detect in CF images due to low contrast, leading to synthesized FFA slices lacking details in these areas. To address this limitation and improve the generator's ability to capture specific regions, we introduce additional supervision into the learning process. Our approach, termed knowledge-boosted attention, involves guiding the network's attention toward focal regions during training. To quantify this guidance, we define an attention loss $\\mathcal{L}_{Att}$ as follows:\n$\\mathcal{L}_{Att}(A, m) = \\frac{1}{n} \\sum (A^2 - m^2)^2.$\nHere, $m$ represents the knowledge mask described in Section 2.2. $A$ denotes the attention map obtained by element-wise multiplication of the semantic-rich activation map $f_l$ from the last convolutional layer $l$ in the generator and the mask $m$. We then apply a rectified linear operation to $A$, resulting in $A = ReLU(f_l \\odot m)$.\nMask-enhanced PatchNCE Losses. To address pixel misalignment in ground-truth FFA series and CF images caused by motion artifacts during acquisition, we introduce the PatchNCE loss [14], inspired by contrastive learning techniques known for boosting model robustness against label noise. However, we observed that the model's primary focus should be on reducing jitter in clinically relevant regions, such as lesions and vasculature, which are of greater clinical significance. To further tackle this issue, we propose mask-enhanced PatchNCE losses as a replacement for traditional PatchNCE losses. This method extends traditional PatchNCE losses by incorporating a knowledge mask $m$, highlighting critical regions within the FFA series. Mathematically, the proposed mask-enhanced PatchNCE losses are based on the InfoNCE loss, which is defined as:\n$\\mathcal{L}_{InfoNCE}(v, v^+, v^-) = -log(\\frac{e^{sim(v, v^+)}}{e^{sim(v, v^+)}+ \\sum_{j=1}^{N} e^{sim(v, v_j^-)}}).$\nHere, $v$, $v^+$, and $v^-$ represent the embeddings of the anchor, positive, and negative samples, respectively.\nThe mask-enhanced unsupervised PatchNCE (UP) loss compares the anchor patch $z_y$ in the generated output with a corresponding positive patch $z_x$ from the input CF image and negative patches $z_{x_j}$ under the guidance of knowledge mask $m$. It is defined as:\n$\\mathcal{L}_{Masked\\ UP} = \\mathcal{L}_{InfoNCE}(m \\odot z_y, m \\odot z_x, m \\odot z_{x_j}).$\nwhere $\\odot$ denotes element-wise multiplication. In contrast, the mask-enhanced supervised PatchNCE (SP) loss ensures consistency between generated and ground-truth patches. It designates the corresponding patch in the ground-truth image $z_y$ as positive, while non-corresponding patches $z_{\\tilde{y}}$ are considered negatives. It's defined as:\n$\\mathcal{L}_{Masked\\ SP} = \\mathcal{L}_{InfoNCE}(m \\odot z_y, m \\odot z_y, m \\odot z_{\\tilde{y}}).$\nThe illumination . By integrating the knowledge mask into the PatchNCE loss, our method directs the model's focus during training, improving its ability to capture clinically significant features.\nKnowledge-aware Discriminators. We employ 3 discriminators $D = \\{D_1, D_2, D_3\\}$ [9,25] with the same patchGAN architecture [10] to evaluate images at scales of 1, 0.5 and 0.25 for different receptive fields. The discriminator objective function for $D_k$ with generator $G$ is given by:\n$\\mathcal{L}_{D_k}(a, b, G(a)) = \\mathbb{E}_{a, b}[logD_k(a, b)] + \\mathbb{E}_a[log(1 - D_k(a, G(a)))].$\nwhere $a$, $b$ and $G(a)$ are the input, ground-truth, and generated images.\nHowever, solely discriminating the entire image may not ensure the authenticity of lesion regions in generated FFA frames. Hence, we introduce discrimination guided by knowledge across scales. By combining knowledge masks $m$ with corresponding FFA images, we tailor inputs for the discriminators to focus on lesions. According to Eq. 5, the combined discriminator loss $\\mathcal{L}_{GAN}(G, D_k)$ for scale $k$ is defined as:\n$\\mathcal{L}_{GAN}(G, D_k) = \\mathcal{L}_{D_1}(x, y, G(x)) + \\mathcal{L}_{D_1}(x \\odot m, y \\odot m, G(x) \\odot m).$\nwhere $\\odot$ denotes element-wise multiplication, and $x$ and $y$ are the input CF images and ground-truth FFA images, respectively.\nConsequently, the final loss function is as follows:\n$\\mathcal{L} = \\lambda_{UPL\\ Masked\\ UP} \\mathcal{L}_{Masked\\ UP} + \\lambda_{SPL\\ Masked\\ SP} \\mathcal{L}_{Masked\\ SP} + \\lambda_{Att} \\mathcal{L}_{Att} + \\lambda_{GAN} \\mathcal{L}_{GAN}.$"}, {"title": "3 Experiments", "content": "Dataset. Our dataset comprises 350 CF images and 18,180 corresponding FFA images from 350 anonymous patients sampled from a large paired dataset. The FFA images were obtained using Zeiss FF450 Plus and Heidelberg Spectralis systems, with a resolution of 768\u00d7768 pixels. Meanwhile, the CF images were captured by Topcon TRC-50XF and Zeiss FF450 Plus instruments, with resolutions ranging from 1,110\u00d71,467 to 2,600\u00d73,200 pixels. The Institutional Review Board approved the study.\nImplementation Details. The final objective function (Eq. 7) was utilized to train the generative model, with $\\lambda_{UP}$, $\\lambda_{SP}$, $\\lambda_{Att}$, and $\\lambda_{GAN}$ set to 1, 1, 4, and 2, respectively. The threshold $\\delta$ for obtaining the knowledge mask was set to 45. During training, each ground-truth FFA series produced 12 frames, with 4 slices randomly selected from the vascular, venous, and late phases, respectively. Data augmentation techniques including random cropping, scaling, and color augmentation. The input images were resized to 512\u00d7512. Additionally, the model was trained to randomly select either generated or ground-truth frames as input, enhancing its adaptability and robustness. We employed the Adam optimizer with beta1 = 0.5 and beta2 = 0.999, adjusting the learning rate every 50 iterations using the PyTorch [17] lr-scheduler. The initial learning rate was set to 2e-3, with a batch size of 1. Training was conducted for 50 epochs on an NVIDIA GeForce RTX3090. For evaluation, 70% of the data was reserved for training at the patient level, while the remaining data was evenly split into validation and test sets.\nEvaluation Criteria. Our video evaluation criteria include Fr\u00e9chet Video Distance (FVD) [24], Structural Similarity Index (SSIM) [26], Peak Signal-to-Noise Ratio (PSNR) [8], and Learned Perceptual Image Patch Similarity (LPIPS) [29]. They measure feature distribution similarity, video structural similarity, reconstruction quality, and perceptual similarity, respectively.\nModel Comparisons. We evaluate Fundus2Video against existing image-to-video translation methods, including the auto-encoder-based Seg2vid [15], and the diffusion model-based Med-ddpm [5] and ConsistI2V [18]. shows our model's superior performance across all metrics. Qualitative comparison in Fig. 3 reveals clearer images and discernible lesion areas in our approach versus others.\nAblation Studies. Additionally, we conduct comprehensive ablation studies to assess the effectiveness of our proposed knowledge mask and related techniques, detailed in the latter part of Table 1 and Fig. 3. Firstly, we show that our designed mask-enhanced patchNCE loss, knowledge-boost attention, and knowledge-aware discriminators, when combined with mask information, outperform the baseline Fundus2Video. Moreover, our mask-enhanced patchNCE loss yields better results than patchNCE loss alone. Secondly, by replacing the knowledge mask with the ground-truth lesion segmentation mask for comparison, we observe that utilizing our KM-guided techniques can enhance performance even with the lesion segmentation mask. However, our knowledge mask yields better results without the need for additional training or labeling efforts.\nHuman Assessment. An ophthalmologist reviewed the results of all methods in Table 1 and found that our Fundus2Video significantly outperformed others. Then the ophthalmologist conducted a quality assessment of 50 randomly selected FFA videos generated by Fundus2Video from the test set, evaluating them based on their corresponding CF images and ground-truth FFA videos. The evaluation focused on vascular perfusion, lesion dynamics, overall coherence, stability, and presence of artifacts. Scores ranged from 1 to 5, with 1 indicating excellent quality and 5 indicating very poor quality. Our model received a score of 2.12 with a standard deviation of 1.07, indicating good overall quality of the generated videos."}, {"title": "4 Conclusion", "content": "In this study, we propose Fundus2Video, which pioneers dynamic FFA video generation from static CF images using an autoregressive GAN architecture. With a knowledge mask derived from clinical experience, we enhance focus on dynamic lesion regions, outperforming supervised lesion segmentation masks. Our method incorporates knowledge-boosted attention, knowledge-aware discriminators, and mask-enhanced patchNCE loss to address challenges in lesion generation and pixel misalignment. Fundus2Video emerges as a promising alternative to traditional FFA, surpassing recent state-of-the-art approaches with its non-invasive, intuitive, and dynamic features."}]}