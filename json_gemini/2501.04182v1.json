{"title": "FIXED POINTS OF DEEP NEURAL NETWORKS:\nEMERGENCE, STABILITY, AND APPLICATIONS", "authors": ["L. Berlyand", "V. Slavin"], "abstract": "We present numerical and analytical results on the formation and stability of a family of fixed\npoints of deep neural networks (DNNs). Such fixed points appear in a class of DNNs when dimen-\nsions of input and output vectors are the same. We demonstrate examples of applications of such\nnetworks in supervised, semi-supervised and unsupervised learning such as encoding/decoding of\nimages, restoration of damaged images among others.\nWe present several numerical and analytical results. First, we show that for untrained DNN's\nwith weights and biases initialized by normally distributed random variables the only one fixed\npoint exists. This result holds for DNN with any depth (number of layers) L, any layer width\nN, and sigmoid-type activation functions. Second, it has been shown that for a DNN whose pa-\nrameters (weights and biases) are initialized by \u201clight-tailed\u201d distribution of weights (e.g. normal\ndistribution), after training the distribution of these parameters become \u201cheavy-tailed\u201d. This mo-\ntivates our study of DNNs with \"heavy-tailed\" initialization. For such DNNs we show numerically\nthat training leads to emergence of Q(N, L) fixed points, where Q(N, L) is a positive integer which\ndepends on the number of layers L and layer width N. We further observe numerically that for\nfixed N = No the function Q(No, L) is non-monotone, that is it initially grows as L increases and\nthen decreases to 1.\nThis non-monotone behavior of Q(No, L) is also obtained by analytical derivation of equation\nfor Empirical Spectral Distribution (ESD) of input-output Jacobian followed by numerical solution\nof this equation.\nKeywords: Deep Neural Network, Fixed Point, Basin of Attraction, Input-Output Jacobian.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, a variety of new technologies based on Deep Neural Networks (DNNs) also\nknown as Artificial Neural networks (ANNs) have been developed. AI-based technologies\nhave been successfully used in physics, medicine, business and everyday life (see e.g.,\u00b9). The\ntwo keys theoretical directions in DNN's theory are the development of novel (i) types of\nDNNs and (ii) training algorithms.\nOne of the most important applications of DNN's is the processing of visual (image or\nvideo) information\u00b2,3. Image transformation (also known as image-to-image translation)\nmeans a transform of the original image into another image according to the goals, for\ninstance, stretching the picture without scarifying the image quality. In this example the\ndimensions of input and output vectors are of the same order of magnitude or just the\nsame. Such DNNs are also called self-mapping transformations and fixed points (FP) play\nan important role in their studies. For example, in digital image restoration\u2074, the presence\nof FP means that the corresponding image is free of defects and does not require processing.\nThe closeness of DNN's output vector to a FP can be used as a stopping criterion for DNNs'\ntraining.\nFurthermore, we consider the problem of stability of FP and the formation of basins of\nattractions. Belonging of the input vector to a basins of attraction can be interpreted as\nbelonging of the corresponding image to a certain class of images (e.g., classes may be dogs\nbreeds, if we consider dog images).\nNote, that the area of FP applications in DNNs is much wider than image-to-image\ntransformations. For example, FPs play an important role in studying the properties of\nHopfield network6,7 proposed by Nobel Prize winner John Hopfield. This model is widely\nused in different branches of science from life science, where FPs are responsible for memory\nformation, up to quantum physics, where FPs may be important in the study of phase\ntransitions\u00ba.\nIn this work, using numerical and analytical methods, we study the influence of DNN\narchitecture (widths of the layers, DNN's depth, and activation function), as well as prob-\nabilistic distributions of weight matrices on the existence of FPs, their number and their\nbasins of attractions."}, {"title": "II. IMAGE-TO-IMAGE TRANSFORMATIONS AND FIXED POINTS OF DNN.", "content": "A DNN can be written in terms of a sequence of non-linear vector-to-vector transforma-\ntions. Let $x^l$ be an output vector of the l-th layer of the DNN.\n\n$x^l = \\{x_i^l\\}_{i=1}^{n_l} \\in \\mathbb{R}^{n_l}, l = 0, 1, ..., L.$\n\nHere L is the DNN's depth, $x^0/x^L$ is a DNN input/output vector. Layer-to-layer transfor-\nmation has the form:\n\n$x^{l+1} = \\Phi^l(x^l) = \\varphi (W^l x^l + b^l)$,\n\nwhere\n\n$W^l = \\{W_{i,j}^{l+1,l}\\}_{i,j=1}^{n_{l+1},n_l}, l = 0, ..., L - 1$\n\nare real-valued $n_{l+1} \\times n_l$ rectangular weight matrices,\n\n$b^l = \\{b_i^l\\}_{i=1}^{n_l}, l = 0, 1, ..., L - 1$\n\nare $n_l$-component bias vectors. The function $\\varphi: \\mathbb{R} \\rightarrow \\mathbb{R}$ is the component-wise activation\nfunction nonlinearity\u00b9\u2070.\nThus, input vector $x^0$ to output vector $x^L$ transformation provided by DNN is defined\nas the following function:\n\n$\\Phi(x^0) = \\Phi^{L-1} \\circ \\Phi^{L-2} \\circ ... \\circ \\Phi^1 \\circ \\Phi^0 (x^0) = x^L.$\n\nNote, that actually function $\\Phi$ depends on input vector $x^0$ and also on weights and biases,\ni.e. $\\Phi(x) = \\Phi(\\alpha, a)$, where $a$ is a set of all weights and biases. Everywhere below we omit\nthe dependence on a for brevity.\nOne of the most impressive features of artificial intelligence structures (DNNs, in partic-\nular) is a wide area of their applications. The same structures of matrices and vectors can\nbe applied in different branches of sciences, industry and art. The only difference is in the\nweights and biases. All these values are formed as a result of training \u2014 one of the most\nimportant ingredient of deep learning. On the other hand, a lot of the modern investigations\nare devoted to to the study of properties of DNNs at initialization with random weights and\nbiases, see, e.g.11-17. One of the reasons of such interest is that the random initialization\nof DNN parameters increases the speed and enhances quality of training significantly. In"}, {"title": null, "content": "context of such an approach we will assume that the weights and the biases are independent\nand identically distributed (i.i.d.) random variables.\nFor DNN solving a classification problem the input vector $x^0$ represents an object (e.g.\nimage of a digit, image of a letter, etc.) and the output vector $x^L$ represents probabilities of\nthe input $x^0$ belonging to each class. If objects are images of symbols, then classes may be\nthe sets containing all images of one digit or letter, etc. Usually, the number of classes K is\nknown apriori e.g., for digits classification problem K = 10 and for classification of letters of\nsome alphabet K is the number of letters in this alphabet. For such classification problems\nthe size of input vector equals the number of pixels of an image and is much greater than\nnumber of classes, i.e. the size of output vector: $n_0 \\gg n_L$. At the same time there are\nseveral important problems, for which $n_0$ and $n_L$ are of same order of magnitude or even\ncoincide. For example:\n1. Image to Image transformations, where DNNs are used for old photo restoration, for\nremoving films defects (e.g., in old movies), for various visual effects, etc2,3. In this\ncase, the result of DNN training is the ability of the network to restore damaged areas\nof photographs, remove noise, etc. In the particular case of $n_0 = n_L$, fixed points (FP)\n$x^*$, defined by\n\n$\\Phi(x^*) = x^*,$\n\nplay an important role in determining when an image is free of defects. Fulfillment of\nthis condition means that a picture(s), which is (are) free of defects, $x^*$, should turn\ninto itself during image processing using a DNN. This is so called image self-mapping.\n2. Image encoding/decoding\u00b9\u2078 (for example, verifying IDs of employees using Face ID\ntechnology\u00b9\u2079). In this case input vectors $x^0 \\in \\mathbb{R}^{n_0}$ (integer $n_0$ is the size of $x^0$) are\nthe photos of employees, obtained from cameras in a building \u2014 input pictures. For\nsecure picture transmission to a remote server, in order to provide access control and\nstop potential, one can use the following DNN-based approach. The example of image\nencoding/decoding is presented in Fig. 1.\n(a) For picture encoding, a DNN $\\Phi_e: \\mathbb{R}^{n_0} \\rightarrow \\mathbb{R}^{n_1}$ is used:\n\n$\\Phi_e(x^0) = x^1,$"}, {"title": null, "content": "Here $x^1 \\in \\mathbb{R}^{n_1}$ is encoded picture (integer $n_1$ is the size of $x^1$, generally, $n_0 \\ne n_1$).\n(b) For pictures decoding one use the other DNN $\\Phi_d: \\mathbb{R}^{n_1} \\rightarrow \\mathbb{R}^{n_0}$. Then $\\Phi_d(x^1) =$\n$x^2$, where $x^2 \\in \\mathbb{R}^{n_0}$ are the decoded (restored) pictures.\nThe goal of the training process for DNN\n\n$\\Phi(x^0) = (\\Phi_d \\circ \\Phi_e) (x^0) = x^2$\n\nis to obtain $x^2 = x^0$, i.e.,\n\n$\\Phi(x^0) = x^0.$\n\nFor simplicity, consider first the case of one employee (i.e. the number of employees\nK = 1). To perform training we start with input vectors $x_{T_1} \\in T_1$, where $T_1$ is training\nset containing the first employee's photos. We then minimize the MSE loss\n\n$L_1(a) = \\sum_{x\\in T_1} l_1 (x, a)$,\n\nwhere\n\n$l_{T_1} (x, a) = ||\\Phi(x, a) - x^*||^2 .$\n\nHere $x^* \\in T_1$ is \u201cstandard\u201d photo of employee stored on access control server. Ulti-\nmately we have: $\\Phi(x^*) = x^*$, i.e., vector $x^*$ is a fixed point of DNN mapping $\\Phi$\u00b2\u2070.\nIf number of employees K > 1 then the loss function is:\n\n$L(a) = \\sum_{k=1}^K \\sum_{x\\in T_k} l_{T_k} (x, a), l_{T_k} (x, a) = ||\\Phi(x, a) - x_k^*||^2 ,$\n\nand $x_1^*, x_2^*,...,x_K^*$ are the fixed points of $\\Phi$. The choice of loss function in this\nexample is quite arbitrary. Instead of MSE (5) one can use, for example, cross-entropy\nloss function\u00b9\u2070\u2019\u00b2\u00b9."}, {"title": null, "content": "There are several remarks:\n\u2022 The advantage of such method is absence of explicit expressions for data encod-\ning and decoding. This significantly reduces the complicates hacking of encod-\ning/decoding.\n\u2022 The natural disadvantage is impossibility to provide exact (bit-to-bit) data re-\ncovery, but it is not required in the case of picture encoding/decoding.\n\u2022 Another important problem of such approach is stability of encoding/decoding\nwith respect to small fluctuations of input vector (input picture). Such fluctua-\ntions are caused by a number of external factors, such as lighting, distance from\ncamera, etc.\n\u2022 In particular, this example demonstrates that for DNNs where input and output\nvector have the same dimensions the number of employees K has nothing in\ncommon with the dimension of DNN's output vector (decoded picture).\n3. Unsupervised image classification, i.e. the number of classes K is a priori unknown\n(number of dog breeds if objects are images of dogs, number of painters in a painting\ncollection, etc.) In this setup, we use DNN $\\Phi$ to transform the image to the generalized\nimage of an object being classified, e.g. \u201cgeneralized dog\", deprived of any breed\nspecific features. Then our DNN $\\Phi$ maps $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, where n is a size of an input\nimage. The loss function for training is chosen as\n\n$L(a) = \\sum_{x\\in T} ||\\Phi(x, a) - \\tilde{x}||^2\n$\n\nwhere x is an input vector, a is the set of parameters of DNN $\\Phi$ (see above), and $\\tilde{x}$\nis the given target image of a \"generalized dog\".\nEmpirically, training leads to the formation of classes via a partition of input vectors\nspace $\\Omega$ into disjoint sets $\\Omega_k, k = 1, 2, . . ., K$ in the following way: for each FP $x_k^*$\nlet $\\Omega_k$ be its basin of attraction. In current example the number of classes K is the\nnumber of dogs breeds in $\\Omega$. Each $x_k^*$ can be considered as \u201cideal\u201d representative of\ncorresponding breed. The rest of $\\Omega$, which does not have FP (if such area exists) we\nwill mark as $\\Omega_0$."}, {"title": null, "content": "As in the previous example, the number of classes K has nothing in common with the\ndimension of the DNN's output $x^L$.\n4. Semi-supervised learning for image classification problem. Again, the number of classes\nK is a priori unknown. However, a training set T is given. The training set T contains\nonly images from $K'< K$ classes and for each image $x \\in T$ we know the class where\nthe image is from. Proposed solution of the problem:\n(a) For each $k = 1, . . ., K$ randomly select an image $\\psi_k \\in T$ in the class k.\n(b) Train DNN $\\Phi$ with $n_0 = n_L = n$ using the following loss function\n\n$L(a) = \\sum_{x\\in T}|\\Phi(x, a) - \\psi_{k(x)} ||^2,$\n\nwhere k(x) is the correct class corresponding to the image x.\n(c) To classify images from unknown classes, use FPs and basins of attraction as\ndescribed in the previous example."}, {"title": "III. HEAVY-TAILED DISTRIBUTIONS AND DNN'S TRAINING: WHAT AND\nWHY.", "content": "In this section we explain where \"heavy-tailed\" distributions arise in DNNs. Usually, for\ninitialization of DNN's weights and biases random variables with \"light-tailed\" (exponential-\ntailed) distribution are used. The reason is that the DNNs training is based on stochastic\ngradient descent (SGD)\u00b2\u00b9. Note, there are several modifications of training process aiming\nat improving DNNs' performance. For instance, it was shown that Marchenko-Pastur\u00b2\u00b2\npruning of singular values of weight matrices enhances DNN's accuracy\u00b2\u00b3. The initialization\nby random variables with \u201clight-tailed\u201d distribution provides optimal initial state for this\nmethod. Actually, such initialization minimizes the number of very large and very small\ngradients of DNN function $\\Phi$ with respect to input vector x. That is why we start our\ninvestigations of FP from untrained DNN with weight matrices entries and bias vectors\ncomponents initialized by random variables with normal distribution.\nOur aim is to investigate how training affects the FPs and their basins of attraction. It\nis well established numerically that for trained DNNs the ESD of weight matrices acquires\n\"heavy-tailed\" form, regardless of initial distribution of a. This phenomenon is a so called"}, {"title": null, "content": "Heavy-Tailed Self-Regularization\u00b2\u2074. For large finite-size matrices ESD $\\rho(\\lambda)$ appears fully\n(or partially) \"heavy-tailed\u201d, but with finite support of $\\lambda$. Moreover, recently it was shown\nthat input-output Jacobian (see also Section V) of a trained DNN has also a \u201cheavy-tailed\"\nESD \u00b2\u2075.\nThe effect of Heavy-Tailed Self-Regularization is very important because allow us to use\nall the tools of Random Matrix Theory for studying spectral properties of DNN's matrices\nand their combinations. For example, in\u00b2\u2076 was shown that \u201cheavy-tailed\" distribution of a\nleads to similar distribution of singular values of input-output Jacobian justifying Heavy-\nTailed Self-Regularization.\nHence, in our investigation of FP we use Heavy-Tailed Self-Regularization for modeling\nthe properties of trained DNNs."}, {"title": "IV. FIXED POINTS AND THEIR BASINS OF ATTRACTION.", "content": "As shown above, FPs play an important role for DNNs in Image to Image transformations.\nWe begin the study with numerical calculations. For simplicity of presentation the dimension\nof input/output vectors x is taken n = 2. The space of input vectors x was chosen as a\nsquare: $\\Omega = [-1, 1] \\times [-1, 1] \\subset \\mathbb{R}^2$.\nThis square was divided using grid with step d (in our calculations $\\delta = 0.05$):\n\n$\\begin{cases}\nx_{j,l} = \\{\\quad x = -1 + d j, j = 0, 1, . . ., [2/\\delta] \\\\\ny = -1 + d l, l = 0, 1, . . ., [2/\\delta] \\end{cases}$\n\nHere [...] denotes an integer part.\nFor each $x_{j,l}$ the iterative process was executed:\n\n$x^{m+1} = \\Phi(x^m), m = 1, 2, 3, . . . ,$\n\nwhere $x^1 = x_{j,l}$. If $\\Phi$ provides contracting mapping on some domain $\\omega \\in \\Omega$, so that $x^1 \\in\\omega$,\nthen the process (8) has a limit:\n\n$\\lim_{m\\rightarrow\\infty} x^{m+1} = \\Phi(x^m) = x^* \\in \\omega.$\n\nThe existence of the limit was checked numerically via Cauchy criteria:\n\n$|x^{m+1} - x^m| < \\epsilon, m < N_0.$"}, {"title": null, "content": "In our calculations $\\epsilon = 10^{-5}$, and $N_0 = 50$. If the limits exists, then $x^{m}$ is the fixed point\n$x^*$ corresponding to starting grid point $x^1 = x_{j,l}$, defined in (7). As a result one can form a\nlist of different fixed points:\n\n$x_1^*, x_2^*,...,x_Q^*,$\n\nwhere all initial points $x_{j,l}$ corresponding to $x_k^*$ form basin of attraction (partition) $\\Omega_k$.\nWe start from untrained DNN with depth L = 2. Matrix entries (2) and bias vector\ncomponents (3) are initialized by random variables with normal distribution $N(0, \\sigma_l)$, where\n$\\sigma_l = (n_l)^{-1}, l = 0, 1$. Activation function is \u201cHardTanh\u201d. The results are presented in\nFig. 2a. As it seen, the only one fixed point x = 0 exists, i.e. Q = 1, and the corresponding\nbasin of attraction is whole set $\\Omega$. This result can be interpreted as follows: the untrained\nDNN can not distinguish input images of employees (see e.g., example 2 in section II).\nNext, using the approach based on Heavy-Tailed Self-Regularization (see\u00b2\u2074\u2019\u00b2\u2076 and Sec-\ntion III) we model trained DNN by untrained DNN with weights and bias initialized by\nrandom variables with Cauchy distribution. In our calculations central point $x_0 = 0$ and\nscale $\\gamma_l = (n_l)^{-1}$. The results are presented in Fig. 2b, c, and d. Fig. 2b corresponds to the\nsame architecture as in Fig. 2a (L = 2, $n_l$ = [2,100]), but we see two FP, Q = 2. Fig. 2c\ncorresponds to L = 3, $n_l$ = [2, 100, 100], Q = 3. In Fig. 2d we present the results of calcula-\ntions for L = 5, $n_l$ = [2, 100, . . ., 100], Q = 5. It is important, that further increase in depth\nL leads to decrease in Q and the result for L = 20 is the same as for L = 2 and normal\ndistribution \u2014 the only one FP, Q = 1. Due to the \"weak similarity\" effect, discovered in\u00b2\u2076\nthe choice of activation function $\\varphi$ does not change the number of FPs.\nNote, that the number of FP, Q, and the shapes of basins of attraction $\\Omega_k (k = 1, 2, . . ., Q)$\ndepend on realization of random variables, e.g. weights and biases. In this regard, the\nquestion of a possible deterministic limit of Q and stabilization of $\\Omega_k$ in the random matrix\ntheory (RMT) limit of the weight matrices (2) and bias vectors (3) sizes infinite-width\nlimit $n_l \\rightarrow \\infty$ appears. However, the following pattern can be traced: the number of\npartitions Q first grows with DNN depth L, but then decreases. The dependence of FP\nnumber = Q(No, L) on layers number L for DNNs with layers widths No = 100 and with\nweights and biases initialized by random variables with Cauchy distribution is presented\nschematically in Fig. 3. This non-monotonic behavior of FP number Q is discussed in the\nnext section."}, {"title": "V. INPUT-OUTPUT JACOBIAN", "content": "An important tool for studying DNN properties is the input-output Jacobian\u00b9\u00b9\u2019\u00b2\u2076\u2019\u00b2\u2077, char-\nacterizing the response of DNN output vector to a small perturbation of an input vector.\nThis operator is closely related to the backpropagation operator, mapping output errors to\nweight matrices, which is the main ingredient of training process. Besides, spectral proper-\nties of input-output Jacobian \u2014 the ESD \u2014 allow one to extract information about DNN\ntraining and, hence, to formulate stopping criteria for training.\nIt occurs, that input-output Jacobian also plays an important role in the study of DNN's\nFP (e.g., the conditions for the DNN mapping being a contraction can be determined in\nterms of this operator). That is why in this paper we perform additional calculations of\ndistribution of input-output Jacobian singular values. Following\u00b2\u2076, one can write the input-\noutput Jacobian as the following $n_L \\times n_0$ random matrix:\n\n$J^L = \\{\\frac{\\partial x^L}{\\partial x^0}\\}_{j_0,j_L=1}^{n_0,n_L} = \\prod_{l=0}^{L-1} D^l W^l,$\n\nwhere diagonal random matrices $D^l$ are\n\n$D^l = \\{D_{j_l}^l\\}_{j_l=1}^{n_l}, D_{j_l,i_l}^l = \\{\\varphi'(W^l x^l + b^l)\\}_{j_l}, l = 0, ..., L - 1.$"}, {"title": null, "content": "Here $\\varphi'$ means the derivative of activation function $\\varphi$.\nWe will assume, that\n\n$E\\{W'\\} = 0, E\\{(W')^2\\} = \\sigma^2 < \\infty, E\\{(W')^4\\} = m_4 < \\infty,$\n\nwhere $E\\{\\dots\\}$ denotes a mathematical expectation w.r.t. all random variables.\nThe functional equation for the resolvent $M_{ML}$ of matrix\n\n$M^L = J^L(J^L)^T,$\n\nwas obtained in\u00b9\u00b9\u2019\u00b2\u2076 and has the form (see, e.g. (1.24) in\u00b2\u2076):\n\n$M_{ML}(z) = m_K(z^{1/L} \\Psi_L(M_{ML}(z)))$,\\\n$\\Psi_L(z) = (1 + z)^{1/L} z^{1-1/L}.$\n\nHere $m_{ML}(z)$ is the moment generating function:\n\n$M_{ML} (z) = \\sum_{k=1}^{\\infty} m_k z^k, m_k = \\int_{-\\infty}^{+\\infty} \\lambda^k \\nu_{ML}(d\\lambda),$\n\nwith Normalized Counting Measure (NCM) $\\nu_{ML}$ (see (1.16)-(1.19) of\u00b2\u2076). The relation be-\ntween $M_{ML}(z)$ and $\\nu_{ML}$ is\n\n$M_{ML}(z) = -1 - z^{-1}f_{ML}(z^{-1}),$"}, {"title": null, "content": "where $f_{ML}$ is the Stieltjes transform of $\\nu_{ML}$:\n\n$f_{ML}(z) = \\int_{-\\infty}^{+\\infty} \\frac{\\nu_{ML}(d\\lambda)}{\\lambda - z}, \\quad \\Im z \\ne 0$\n\nA. Numerical solution of functional equation.\nIn general case the equation (14) can only be solved numerically. The main problem is the\nexistence of multiple roots of (14). To overcome this issue we will use existing asymptotics\nfor $f_{ML}(z)$:\n\n$f_{ML}(z) \\sim -1/z, \\quad |z| \\gg 1.$\n\nFor simplicity let us define:\n\n$w(z) = M_{ML} (1/z) = -1 - zf_{ML} (z).$\n\nThen (14) acquires the form:\n\n$w(z) = m_K(z^{-1/L} \\Psi_L(w(z))),$\n$\\Psi_L(z) = (1 + z)^{1/L} z^{1-1/L}.$\n\nUsing Banach fixed-point theorem we will find the solution of (19) as the limit of the se-\nquence:\n\n$w_{n+1} = m_K(z^{-1/L} \\Psi_L(w_n))), \\quad n = 0, 1, ...$\n\nLet us note that z here is a parameter of this iteration scheme and the only variable is w. If\nmapping $m_K(w)$ is contracting on some set $\\Omega$, then $\\forall w_0 \\in \\Omega$\n\n$\\lim_{n\\rightarrow\\infty} w_n = w,$\n\nand then w is the root of (19):\n\n$w = m_K(z^{-1/L} \\Psi_L(w)).$\n\nFor numerical solution of (20) the criterion for finding the solution is chosen as\n\n$|w_n - w_{n-1}| < \\epsilon, \\quad n < N_0,$"}, {"title": null, "content": "where $\\epsilon = 10^{-5}$ and the maximal number of iterations $N_0 = 50$.\nAs was mentioned above, the equation (14) has, in general, multiple solutions. To chose\ncorrect one let us introduce the sequence of complex $z^{(k)}$ as the following:\n\n$z^{(k)} = \\lambda + ib^{N-k},$\n\nwhere $b > 1; b, \\lambda \\in \\mathbb{R}$ and $N \\gg 1, N \\in \\mathbb{Z}, k = 0, 1, 2, . . ., 2N$. For k = 0, $z = z^{(0)} = \\lambda + ib^N$\nis so that $|z| \\gg 1$, and according to (18), (17)\n\n$w_0^{(0)} = 0.$\n\nThe superscript \u201c(0)\u201d means that this initial approximation, i.e. n = 0 in (20), cor-\nresponds to the iteration step with k = 0. We denote the numerical solution of (20) as\n$w^{(0)}$.\nFollowing\u00b9\u00b9 we will decrease $\\Im z$, fixing $\\Re z = \\lambda$, i.e. we will increase k in (22). For k = 1\nthe initial approximation $w_0^{(1)}$ should be chosen as the numerical solution of (20) obtained\nfor k = 0 (i.e. $w_0^{(1)} = w^{(0)}$). This choice of initial approximation provides asymptotics (17)\nof $f_{ML}$ at $|z| \\gg 1$ and, thus, desired root of (19)."}, {"title": null, "content": "Further, this scheme is repeated for k = 2, 3, ..., 2N, with $w_0^{(k)} = w^{(k-1)}$. As the result\nwe obtain the $w^{(2N)}$ for $z = z^{(2N)} = \\lambda + ib^{-N} \\approx \\lambda + i0$. The ESD of $M^L$ (13) according to\nthe inverse Stieltjes transform has a form\n\n$\\rho(\\lambda) \\sim \\frac{1}{\\pi} \\Im (M^L).$\n\nSubstituting $f_{ML}$ from (18)\n\n$f_{ML} = \\frac{1 + w^{(2N)}}{z^{(2N)}},$\n\nwe finally obtain\n\n$\\rho(\\lambda) \\approx \\frac{1}{\\pi} \\Im \\frac{1 + w^{(2N)}}{z^{(2N)}}.$\n\nSchematic illustration of the proposed algorithm is presented in Fig. 4.\nLet us note that in\u00b2\u2076 the equation (19) was solved in the same way, but with random\nchoice of initial approximation $w_0$ in (20). Such initialization does not guarantee the desired\nroot of the equation and the criterion for whether the root is correct is smoothness of $\\rho(\\lambda)$\nin (23). The approach proposed here guarantees the correct choice of root of (19) and does\nnot require any additional verification.\nThe solution of (14) for linear activation function (24), is presented in Fig. 5 (black curve).\nRed curve corresponds to the direct numerical calculation of the eigenvalues distribution of"}, {"title": null, "content": "the random matrix (13), corresponding to DNN with L = 2, layer width $N = 10^3$. The\ndistribution of eigenvalues was obtained by averaging over $10^3$ realizations. One can see\ngood agreement between the the results for finite-size system and infinite-size one. The only\ndifference is in the neighborhood of soft-edge of spectra (maximal value of $\\lambda$). For finite-size\nsystem this edge is random, for infinite-size system \u2014 deterministic. On the other hand,\nthe behavior of Jacobian (10) eigenvalues determines the existence of FP and boundaries of\npartitions (basins of attraction). For example, $||J^L|| < 1$ is sufficient for the DNN mapping\nto be a contraction. The obtained result confirms our assumption that in the RMT limit\nthe number of FPs Q and corresponding basins $\\Omega_k, k = 1, 2, . . ., Q$ may be non-random.\nIn Fig. 6, the solutions of (14) corresponding to initialization of weights by random\nvariables with normal distribution and various numbers of layers L are shown. The activation\nfunction $\\varphi$ is \u201cLinear\u201d. The case of single layer L = 1 corresponds to Marchenko-Pastur\ndistribution\u00b2\u00b2. One can notice that, with increase of L the curve is pressed towards the\n$\\lambda$-axis, forming a narrow peak at $\\lambda = 0$. So, as L $\\rightarrow \\infty$ the ESD $\\rho(\\lambda) \\rightarrow \\delta(\\lambda)$. It means that\nthe eigenvalues of Jacobian (10) tend to zero, providing contraction mapping $\\forall x \\in \\Omega$ and,\nhence, only one FP $x^*$ may exists. This result is also in full agreement with our numerical\nsimulations, demonstrating that the number of FPs Q $\\rightarrow$ 1 as L $\\rightarrow \\infty$.\nThe equation (14) is not applicable for \u201cheavy-tailed\" distributions due to violation of"}, {"title": null, "content": "(12)"}]}