{"title": "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features", "authors": ["Arnab Dey", "Cheng-You Lu", "Andrew I. Comport", "Jean Martinet", "Srinath Sridhar", "Chin-Teng Lin"], "abstract": "Recent advancements in radiance field rendering show promising results in 3D scene representation, where Gaussian splatting-based techniques emerge as state-of-the-art due to their quality and efficiency. Gaussian splatting is widely used for various applications, including 3D human representation. However, previous 3D Gaussian splatting methods either use parametric body models as additional information or fail to provide any underlying structure, like human biomechanical features, which are essential for different applications. In this paper, we present a novel approach called HFGaussian that can estimate novel views and human features, such as the 3D skeleton, 3D key points, and dense pose, from sparse input images in real time at 25 FPS. The proposed method leverages generalizable Gaussian splatting technique to represent the human subject and its associated features, enabling efficient and generalizable reconstruction. By incorporating a pose regression network and the feature splatting technique with Gaussian splatting, HFGaussian demonstrates improved capabilities over existing 3D human methods, showcasing the potential of 3D human representations with integrated biomechanics. We thoroughly evaluate our HFGaussian method against the latest state-of-the-art techniques in human Gaussian splatting and pose estimation, demonstrating its real-time, state-of-the-art performance.", "sections": [{"title": "1. Introduction", "content": "Generating virtual photorealistic 3D human avatars is a long-standing challenge in the field of computer vision [1, 23]. These 3D models have diverse applications in fields such as augmented reality, virtual reality [32], entertainment, and the medical domain [6, 53]. The task of reconstructing a complete 3D human model with integrated structural properties [5, 76] in real time from images alone presents significant challenges. Classical approaches rely on complex multiview capture systems and body markers [24, 48] to obtain 3D models of humans, incorporating structural properties such as 3D pose involve fitting parametric body models such as SMPL [34] and STAR [43]. However, these methods require substantial resources and computational effort to generate each 3D model.\nIn recent years, radiance field rendering becomes significantly popular [55, 63] for the scene representation capabilities. More recently, 3D Gaussian splatting (3DGS) [29] provides a new research direction and demonstrates notable improvements compared to neural rendering-based methods. 3D Gaussian splatting proposes a novel explicit representation that represents the scene using a set of 3D Gaussians for point-based rendering. The efficient representation of Gaussian splatting makes it particularly well-suited for real-time rendering applications. Subsequent researches apply 3DGS to various applications [10, 37, 62] including 3D human reconstructions [22, 73]. The existing methods employing 3D Gaussian Splatting (3DGS) for human avatar reconstruction either rely on parametric body models or fail to incorporate any underlying biomechanical features crucial for downstream applications [20, 65].\nIn this work, we propose a novel, generalizable approach for estimating a 3D human representation with integrated 3D pose and dense pose in real time, given sparse input images of the human subject. The proposed method, named Human Feature Gaussian (HFGaussian), uses Gaussian splatting to represent the human subject and its associated biomechanical features\u00b9, which include the 3D skeleton, 3D keypoints, and dense pose. These biomechanical properties are essential for recreating natural human movements and interactions in the virtual world [15, 27]. One straightforward approach to representing human features while maintaining real-time rendering speed, is to directly parameterize the 3D Gaussian with additional human features. However, we point out that simply parameterizing"}, {"title": "2. Related works", "content": "The proposed method HFGaussian uses the Gaussian splatting technique to estimate 3D human avatars with integrated biomechanical features in real time from sparse mul-\ntiview images. In this section, we review the previous studies relevant to this research."}, {"title": "2.1. Radiance field rendering", "content": "Radiance field rendering-based techniques become popular in recent years because of the photorealistic scene representation capability. NeRF [38], introduced in 2020, proposes a coordinate-based neural network to represent a 3D scene. The neural networks based on MLP map 3D coordinates and 2D view directions into density and color. Several follow-up works [3, 4, 44] are proposed and achieved impressive results, further verifying the capability. In addition, further studies are conducted to address the limitations, including long training [12, 16, 33, 41] and inference time [40, 51], scene specificity [66], and static scene constraints [49, 54]. In recent years, Gaussian splatting [29] techniques emerge as an alternative to implicit neural radiance fields by utilizing a set of 3D Gaussians to learn fast explicit scene representations."}, {"title": "2.2. Radiance field rendering for human", "content": "Radiance field rendering techniques demonstrate promising results in various applications for 3D human representation. Early studies [54, 64, 71] employ neural radiance fields to produce 3D human avatars from a spare set of images. Subsequent studies [9,21,25,64,68] improve the generalizability of these models by incorporating the SMPL [34] parameters as input. Likewise, [26, 46, 54, 61] utilize existing skeletal data, state-of-the-art pose estimators, or pose data to generate novel views and poses.\nRecently, thanks to the fast rendering speed of Gaussian splatting techniques, radiance field-based methods [22, 30, 73] have been able to represent 3D humans in real-time. More recently, [13] proposes generalized radiance fields for versatile human features that extend beyond RGB rendering by integrating additional human features, although the rendering speed is not real time. Our approach stands out from prior works by combining generalizable Gaussian splatting techniques with feature splitting, which maintains the quality of both low- and high-frequency features, and a dedicated pose regression network to estimate 3D human avatars with integrated biomechanical features in real time."}, {"title": "2.3. Human pose estimation", "content": "Human features such as 3D pose and dense pose estimation are a long-standing problem in computer vision research. Many popular 2D and 3D pose estimation from images is based on supervised training. Algorithms for 2D pose estimation [7, 11, 14, 19,31] employ 2D CNN architecture to estimate 2D poses from images. Works such as [39,42,52,58] employ person detectors to estimate the poses of multiple persons. Bottom-up approaches like [7, 11, 31] identify joints using heatmaps and link body parts, but face"}, {"title": "3. Method", "content": "We present HFGaussian, a unified framework that leverages Gaussian splatting to estimate human features and 3D pose in real-time."}, {"title": "3.1. Preliminary", "content": "In this study, we present a novel method for reconstructing the 3D human model while incorporating biomechanical characteristics through the use of Gaussian splatting. Here, we briefly outline the fundamental concepts behind 3D Gaussian splatting [29] and generalizable Gaussian splatting [73] techniques. 3DGS is introduced as an alternative explicit representation in contrast to continuous NeRF-based approaches. A scene in 3DGS is represented through a collection of 3D Gaussians characterized by certain properties: the 3D position of Gaussian \u00b5, color c represented using spherical harmonics, opacity \u03b1, and a 3D covariance matrix \u03a3. \u0391 3D Gaussian in 3DGS is represented as:\n$G(x) = e^{-(x-\\mu)^T\\Sigma^{-1}(x-{\\mu})}$\nThe covariance matrix \u03a3 can be broken down into a rotation matrix R and a scaling matrix S. The 3D Gaussians are projected into 2D space using a view transformation matrix W and Jacobian of an affine approximation of the projective transform J. The 2D covariance matrix \u03a3' is represented as \u03a3' = JW\u03a3WJT. The alpha blending technique similar to NeRF, is used to rendering final pixel colors from Gaussians:\n$C = \\sum_{i\\in N} c_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j)$\nwhere $c_i$ represents the learned color, \u03b1 denotes the result of the multiplication between the opacity $\u03b1_i$ and the 2D Gaussian. The 3DGS technique demonstrates notably faster rendering speeds compared to continuous methods based on NeRF, primarily because it can directly project and blend 3D Gaussian into color.\nAlthough 3DGS is efficient and produces high-quality results, vanilla 3DGS is scene-specific and is not generalizable to new scenes. To address this issue, GPS-Gaussian [73] proposes Gaussian parameter maps on the"}, {"title": "3.2. Learning human feature with 3DGS", "content": "We propose a novel method for real-time 3D human avatar estimation with biomechanic properties. Although recent advancements in 3D Gaussian splatting techniques [29] show remarkable efficiency in 3D scene representation compared to previous approaches, the vanilla method still requires per-subject optimization for scene representation. To address this limitation, GPS-Gaussian [73] introduces a generalizable approach for 3D human representation that takes advantage of 2D Gaussian parameter maps to estimate 3D Gaussians. This approach enables the"}, {"title": "3.3. Pose regression Network", "content": "A key component of the architecture is the pose regression network, which aims to estimate the 3D pose of the human subject from the partial point cloud data generated from depth maps (Di, Dr), also referred to as the subset of the 3D Gaussians. Estimation of 3D human keypoints from point cloud data is an active research area, with prior work exploring various approaches to address these challenges [8,69,75]. As mentioned in the previous section, the depth estimation models predict depth maps (Di, Dr) corresponding to each source view (I1, Ir). These depth maps of the source views and their associated camera parameters can be used to generate point clouds P\u2081, Pr. The 2D masks of each source view are then used to extract only the point cloud of the human. The point clouds from both views are then combined as they are in the same 3D frame to generate"}, {"title": "3.4. Human feature estimation", "content": "The proposed method also estimates human features. We demonstrate the capabilities of our approach by estimating dense pose, which involves predicting Continuous Surface Embeddings for the human subject. We include an additional branch in the Gaussian parameter estimator to predict human feature maps Mf for each Gaussian, similar to the rotation and opacity maps. Inspired by feature splatting [36], we apply a similar technique that estimates human feature vectors fp by splatting Gaussian features fi in the image plane, and then blending the feature vectors using alpha composition:\n$f_p = \\sum_{i\\in N} f_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j)$\nThe blended feature vectors fp are decoded by a MLP consisting of two linear layers with ReLU activation functions, followed by a final layer with a sigmoid activation function, to render the continuous surface embeddings."}, {"title": "3.5. Optimization", "content": "The proposed HFGaussian method comprises three key components: generalizable Gaussian splatting, 3D pose estimator, and human feature estimator module. The model is trained in two stages. First, the depth estimator module is trained on both source views. Then, the Gaussian parameter estimator module is trained using the depth maps and image features, along with the feature estimator MLP and the 3D pose estimator. A combined loss function is utilized to train all three parts simultaneously, as described:\n$L = L_{image} + L_{depth} + L_{pose} + L_{feature}$\nwhere $L_{image}$ is the photometric loss between the ground truth and the rendered image represented as $L_{image}$ = $\\beta L_{mae}$ + $\\gamma L_{ssim}$ where \u03b2 and \u03b3 are 1.6 and 0.4 respectively. The depth loss $L_{depth}$ is defined as:\n$L_{depth} = \\sum_{t=1}^{T} \\mu^{T-t} || d_{gt}^t - d^t ||_1$\nwhere d represents the depth and \u03bc is set to 0.9 for our experiments. The 3D pose estimation loss $L_{pose}$ is the L2 loss between ground truth and estimated 3D keypoints. Lastly, the feature estimation loss $L_{feature}$ is the L1 loss between the ground truth and the predicted human features, which in this case are the continuous surface embeddings."}, {"title": "4. Experimental Results", "content": "We conduct a comprehensive evaluation of our model's ability to learn a generalized Gaussian representation of humans, including both 3D human poses and features. Extensive experiments are conducted on a variety of datasets, and we evaluate our results with other state-of-the-art NeRF and Gaussian splatting-based methods."}, {"title": "4.1. Implementation details", "content": "The proposed HFGaussian method is implemented using the PyTorch framework and the AdamW optimizer [35]"}, {"title": "4.2. Dataset", "content": "In this work, we create a custom dataset similar to the one utilized in GPS-Gaussian [73]. The GPS-Gaussian dataset is created using 526 scans from the THuman2.0 [67] dataset and includes images, masks, depth information, and camera parameters. The dataset is insufficient for this study and required additional ground-truth data on various human features, such as keypoints and dense pose, in order to train the proposed method. To this extent, we extend the GPS-Gaussian dataset by generating additional ground-truth information. We start with 526 human scans and SMPLX [34] parameters from THuman2.0. Next, we employ the SMPLX model fitted to the scan to produce accurate 3D poses. We produced 19 key points representing the main joints of the human body (nose, neck, right shoulder, right elbow, right wrist, left shoulder, left elbow, left wrist, pelvis, right hip, right knee, right ankle, left hip, left knee, left ankle, right eye, left eye, right ear, left ear). To produce the 2D keypoints, we map the 3D keypoints into image coordinates utilizing the camera's parameters. We use DensePose [17] to generate ground-truth Continuous Surface Embeddings for each image in the dataset. To render images from human scans, we follow the same convention of GPS-Gaussian, where 8 images are rendered in a circle around the human at 45-degree intervals, and those are used as source views. Additionally, 3 random viewpoints are generated as target views. We generate all images in the dataset at a resolution of 512x 512. We also use the real-world dataset captured by [73] to evaluate our methods. This real dataset does not provide any ground-truth 3D or 2D keypoint information. To further evaluate the generalization ability of our model, we also preprocess and utilize the THuman4.0 dataset [74], which contains three clips of real-world subjects, resulting in test sets with 19656, 40464, and 24880 samples, respectively. We will release the preprocessed THuman2.0 and THuman4.0 after the paper is published."}, {"title": "4.3. Baseline", "content": "The proposed HFGaussian method is compared with other state-of-the-art generalizable approaches for human subjects. Regarding the benchmarking of human features and pose estimation, many previous generalizable tech-"}, {"title": "4.4. Novel view synthesis", "content": "Our proposed methods are compared with other state-of-the-art generalizable radiance field rendering techniques for human representation. The quantitative results for novel"}, {"title": "4.5. 3D/2D Pose Estimation", "content": "The proposed HFGaussian method estimates 3D keypoints using a pose regression network, which takes the point cloud generated by the position of 3D Gaussians as input. In contrast, many state-of-the-art approaches do not provide 3D keypoints or any human biomechanical features. Our method is the first to directly estimate 3D pose without relying on prior supervision or parametric body models. The quantitative results of our approach are presented in Ta-"}, {"title": "4.6. Dense Pose Estimation", "content": "The proposed HFGaussian method is capable of estimating various human features using the feature splatting technique discussed previously. To showcase the feature"}, {"title": "4.7. Real Time Performance", "content": "One of the primary objectives of this work is to achieve real-time performance during inference and estimate novel views with biomechanical features on unseen data. We have compared the real-time performance of our method with GPS-Gaussian [73] and GHNERF [13], among which only GHNERF is capable of estimating additional human features such as 2D keypoints. The results presented in Table 3 demonstrate the performance of different methods in terms of frames per second. In our method, we experiment with various backbones for 3D pose estimation. The relationship between the different backbones, the inference speed, and the precision is shown in Figure 5. The figure indicates that the proposed architecture for the 3D pose estimation backbone achieves the best balance between speed and accuracy."}, {"title": "4.8. Ablation Studies", "content": "To assess the importance of feature splatting for estimating human features, we conduct an ablation study. We compare the performance of our method in estimating dense pose, a key human feature, using feature splatting versus directly predicting feature values similar to RGB color. In both cases, we keep the other Gaussian parameters constant.\nThe qualitative results presented in Figure 6 demonstrate that the method without feature splatting is unable to accurately estimate dense pose, as the same Gaussians are unable to effectively represent both RGB values and feature attributes simultaneously."}, {"title": "5. Conclusion", "content": "This paper presents a novel framework, HFGaussian, for the real-time rendering of human avatars with biomechanical properties. The proposed method utilizes Gaussian splatting to generate novel views from sparse source views of human subjects in real time. Extensive experimentation demonstrates the effectiveness of this approach, which represents a significant improvement over previous Gaussian splatting-based methods for human representation. While the primary focus of this work is on 3D human pose estimation and feature splatting-based dense pose estimation, we believe that feature splatting can be leveraged to learn other human-centric features, such as body part segmentation. In summary, this work presents promising results and opens up new avenues for research in 3D human representation."}]}