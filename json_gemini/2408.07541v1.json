{"title": "DifuzCam: Replacing Camera Lens with a Mask and a Diffusion Model", "authors": ["Erez Yosef", "Raja Giryes"], "abstract": "The flat lensless camera design reduces the camera size and weight significantly. In this design, the camera lens is replaced by another optical element that interferes with the incoming light. The image is recovered from the raw sensor measurements using a reconstruction algorithm. Yet, the quality of the reconstructed images is not satisfactory. To mitigate this, we propose utilizing a pre-trained diffusion model with a control network and a learned separable transformation for reconstruction. This allows us to build a prototype flat camera with high-quality imaging, presenting state-of-the-art results in both terms of quality and perceptuality. We demonstrate its ability to leverage also textual descriptions of the captured scene to further enhance reconstruction. Our reconstruction method which leverages the strong capabilities of a pre-trained diffusion model can be used in other imaging systems for improved reconstruction results.", "sections": [{"title": "1. Introduction", "content": "Cameras have become very popular and common in recent years, especially in small handheld devices. Despite that, reducing the size of the camera remains a difficult problem since a camera requires lenses and optical elements to get a high-quality image. Flat cameras [43] is a computational photography method to reduce camera size by replacing the camera lens with a diffuser, namely, an amplitude mask placed very close to the sensor. Thus, the image on the sensor consists of multiplexed projections of the scene reflections on all the sensors area such that the captured image is not visually understandable. Using a computational algorithm, the scene image can be retrieved. Yet, while achieving massive camera size reduction, reconstructing high-quality images from flat camera measurements is an ill-posed task and hard to achieve.\nPrevious approaches tried to reconstruct the scene image using different techniques including direct optimization [43] and deep learning [20]. Despite these attempts, the resulting images are not of sufficient quality. High-quality image reconstruction from flat camera measurements is still not solved and better algorithms are required to reproduce better images.\nWe propose DifuzCam, a novel strategy for flat camera image reconstruction using a strong image prior that relies on a pre-trained diffusion model [9,38]. An overview diagram of DifuzCam is presented in Figure 2. Using a pre-trained image generation model, which is trained on a huge amount of images, we utilize a strong prior for natural images perceptually. This facilitates reconstructing state-of-the-art quality images from flat camera measurements (Figure 1b). Moreover, we leveraged the diffusion model text-guidance property for generation to improve even further image reconstruction through a description of the captured scene from the photographer."}, {"title": "2. Related Work", "content": "Lensless imaging has garnered significant attention in recent years since it offers a more compact, light, and cost-effective imaging system. Different approaches to lens-less camera design were proposed, such as static amplitude mask [43], modulation with LCD [18], Fresnel zone aperture (FZA) with SLM [11, 29, 45], phase mask [3, 6], programmable devices [17, 27, 53, 55], and more as described in the following review [7].\nIn this paper, we rely on the FlatCam design [43]. It employs a static amplitude mask placed near the sensor. The mask pattern is designed in a separable manner such that the imaging linear model can be simplified to a separable operation:\n$$Y = \\Phi_L X \\Phi_R,$$\nwhere  $\\Phi_L$ and $\\Phi_R$ are the separable operations on the image\u2019s rows and columns, respectively. X is the scene intensity and Y is the sensor measurements.\nIn this case, the measurements of the camera consist of multiplexed projections of the scene reflectance such that reconstruction becomes an ill-posed problem. Prior methods for image reconstruction from flat camera measurements were designed as an optimization problem [17, 43, 53]. Such model-based methods are heavily dependent on the imaging model and rely on accurate calibration. Thus, they were very limited and the reconstruction quality was low.\nTo overcome some of these limitations, data-driven algorithms were proposed using deep learning to reconstruct high-quality images from multiplexed measurements. For example, deep learning was used to optimize the alternating direction method of multipliers (ADMM) parameters and reconstruct the image [36]. FlatNet [20, 21] used learned separable transform followed by a Unet [40] architecture which perceptually enhances the resulting image in a generative adversarial network (GAN) approach. GAN is also presented for lensless point spread function (PSF) estimation and robust reconstruction [37]. In [48], a Unet architecture followed by a deep back-projection network (DBPN)\n[15] was proposed for FZA lensless camera image reconstruction. To address the mismatch between the ideal imaging model and the real-world model, the work in [23] presented multi-Wiener deconvolution networks in multi-scale feature spaces. The research in [25] presented a multi-level image restoration with the pix2pix generative adversarial network. A recent work claimed improved results by incorporating the transformer architecture into the reconstruction model [33].\nDiffusion models [9,16,32] are generative models that have gained a lot of popularity in recent years due to their impressive ability to learn the natural image distribution which are used for image generation [12, 31, 38], segmentation [2, 4], inpainting [26], image super-resolution [42], and general image reconstruction [1, 5, 9, 14, 19, 41, 54]. The diffusion model is a parameterized Markov chain that produces samples of a certain data distribution after a predefined T number of steps. In each step of the forward diffusion process, a Gaussian noise is added to the data, such that after a large number of steps, the sample is mapped to an isotropic Gaussian. In the reverse step, a deep neural network is trained to denoise the clean image from the noisy sample. To sample an image, we apply T steps in the reverse direction starting with pure Gaussian noise and denoising it until getting a clean natural image from the learned distribution of the data.\nIn the context of low-level image restoration, diffusion models were recently used for image restoration of linear inverse problems [8, 10, 19], spatially-variant noise removal [34] and low-light image enhancement and denoising [46,49]. The diffusion method was also presented for a low-light text recognition task [30]. The use of text guidance for conventional imaging reconstruction and enhancement was recently proposed [22, 35, 50]. For non-conventional imaging, such as a flat camera, adopting diffusion models for the recovery process is not trivial and additional adjustments and considerations are required. This work bridges this gap and presents a novel approach to integrating diffusion models for this imaging task.\nLatent diffusion model (LDM) [38] is a method for applying the diffusion process in a latent space instead of the pixel space to get a more computationally efficient model for high-resolution images. In this approach, a pre-trained autoencoder consisting of encoder  $\\mathcal{E}$ and decoder  $\\mathcal{D}$ is used to train a diffusion model in a low-dimension (latent) space.\nThe data samples are denoted as $x \\sim q(x)$, where q is the data distribution we learn, and the latent sample is obtained by the encoder as $z = \\mathcal{E}(x)$. A denoiser network $\\epsilon_{\\theta}$ is trained to predict the added noise (at the backward diffusion step) from the noisy sample $z_t$ at the timestamp $t \\in [0,..., T]$. With an additional text input y for text guid-"}, {"title": "3. Proposed Method", "content": "We turn to describe our DifuzCam strategy. The flat camera we use employs a similar implementation to previous works [21,43]. A separable pattern obtained from an outer product of M-sequence binary signals in the length of 255 was used for the amplitude mask. We printed it by lithography on a chrome plate on glass with a thickness of 0.2mm.\nEach feature in the pattern (i.e. a single pinhole) is of size 25 um. Figure 3 shows the mask pattern and the measured camera PSF achieved while the mask is attached to the sensor hot mirror.\nThe acquired image by a flat camera consists of multiplexed measurements of the light reflected from the scene across the sensor area. The light projections on the sensor create long-range correspondences in the captured images. To convert the image from the projections (i.e. \"projection space\") to the pixel space of the target image we apply a learned separable linear transformation to the measurements. This transformation is crucial since the input image serves as guidance for the diffusion model process, which was trained and worked in the domain of natural images. The diffusion model is ignorant about the flat camera mask projections on the image, and the control network we use to guide the reconstruction process exhibits better results when operating in the image pixel domain rather than the long-range projections.\nThe RAW image captured by the camera (in RGGB Bayer pattern) is split into four color channels: R, Gr, Gb and B. Each channel $C_k$ is linearly transformed as\n$$C_k^o = \\phi_l C_k \\phi_r,$$ \nwhere $k \\in [R, Gr, Gb, B]$, each color channel $C_k$ of size $h_i \\times w_i$ and $\\phi_l \\in \\mathbb{R}^{h_o \\times h_i}$ and $\\phi_r \\in \\mathbb{R}^{w_i \\times w_o}$ are two learnable matrices. The output features were stacked onto a single 4-channel tensor $C^o \\in \\mathbb{R}^{4 \\times h_o \\times w_o}$.\nSince the flat camera image reconstruction is highly ill-posed we utilized a pre-trained diffusion model as our image prior. It is a strong prior for natural images since it is trained on a huge number of samples for the image generation task.\nTo leverage the diffusion model for our task we need to control its generation process such that we can generate the captured scene from the measurements of the flat\ncamera. To do so we use a ControlNet [51] network $\\mathcal{C}_{\\theta}$ which we train for our goal (as presented in Figure 2). This network is initialized as a copy of the encoder of the diffusion model UNet with zero convolutions, such that the pre-trained weights performance is not affected and during training non-zero weights are learned for the reconstruction task.\nThe input of the control network is the output of the separable transform ($C^o$) and we use the control network loss\n$$l_c = \\mathbb{E}_{x(x), \\epsilon \\sim \\mathcal{N}(0, 1), t \\sim \\mathcal{U}(0, T)}[||\\epsilon - \\epsilon_{\\theta}(z_t, t, y, C_{\\psi} (C^o)||^2],$$\nwhich is applied for both $\\psi$, the set of parameters of the control network $C_{\\psi}$, and the learned separable transform weights. The diffusion model parameters $\\theta$ are pre-trained and fixed.\nTo guide the training for better results, a separable reconstruction loss term is added to the diffusion conventional loss (Equation (4)). This loss is applied to the output of the separable transformation as\n$$l_{sep} = ||I - f_{conv}(C^o)||^2,$$\nwhere $f_{conv}$ is a learned 3\u00d73 convolution layer which maps the 4-channels $C^o$ to 3 channels image, and I is the target (GT) image in RGB channels format. We present in Section 4.2 the improvement and significance of this term.\nThe diffusion model that we use is a text-guided model for image generation. Thus, we employ this ability to improve the image reconstruction process by giving the model a text description of the captured scene. Giving additional information about the scene content enables the algorithm to have better prior knowledge of the resulting image and reconstruct better images. In this approach the photographer describes the captured scene and this information is input into the reconstruction algorithm. The contribution of the text to the results is presented in Section 4.2."}, {"title": "3.1. Data Acquisition and Training", "content": "As we train our DifuzCam model (separable transform and control network) in a supervised way, we need pairs of RGB images with their corresponding measurements of the flat camera. Simulating the flat camera imaging process to get realistic measurements such that the trained model will generalize to the real world is very hard since the captured measurements are very dependent on camera properties, alignment, calibration, and mask placement. Because of the small feature size (25um), minor movement of the mask will result in totally different measurements. Due to this sensitivity of the system, we obtained using the optical setup real-world measurements that contain all the imaging properties, including space-variant PSF, diffraction, and non-ideal effects which are hard to simulate such as mask manufacturing inaccuracies, dust, image noise, etc.\nTo get a large dataset using our flat camera, we captured images from the LAION-aesthetics dataset [44], which were projected on a PC screen of size 34cm \u00d7 34cm at a distance of 60cm from the camera. We used 12ms exposure time and saved the raw Bayer pattern images in 12-bit depth. We used LAION-aesthetics [44] as it consists of a large amount of high-resolution images and their corresponding textual captions. In total, we captured about 55k images. 500 images were saved for testing while the rest were used for training. To compensate for stray light, a black screen with no image projected on it was captured. In the post-processing stage, the measured black levels were subtracted from the captured measurements.\nIn addition to the screen images, we also capture real objects to show that we are not restricted only to \"objects on screen\". In this case, we just show the qualitative reconstruction result as we do not have an exact RGB match as in the screen measurement case.\nFor the pre-trained diffusion model, we used stable-diffusion 2.1 [39]. We trained the model for 500k steps on the captured dataset using the looses in Equation (4) and Equation (2) and learning rate of 5. 10-5 with the AdamW optimizer."}, {"title": "4. Experimental Results", "content": "Our DifuzCam method was evaluated using several image quality and text similarity metrics. The results of the proposed reconstruction algorithm were compared to the ground through images using PSNR, SSIM [47], and LPIPS [52] metrics. Since we also present a text guidance approach, we measured the CLIP score [36] compared to a GT text description of the captured scene, a metric that evaluates the similarity of an image to a text. For this metric, we used the latest published and most accurate model (ViT-L/14@336px). On the test dataset that we captured, our method achieved 21.89 in PSNR, 0.541 in SSIM, 0.276 in\nLPIPS, and a CLIP score of 24.38. Figure 4 presents examples of our reconstruction results.\nTo have a fair comparison to previous works, tests should be performed on the same dataset. Since FlatNet [20] published the dataset of their camera we conducted experiments also on their data to present a valid comparison. Unfortunately, we could not compare to [23, 33] as they did not publish their code or model weight for comparison. The comparisons to [20] are presented quantitatively in Table 1 and visually in Figure 5. Our method achieved superior results in all evaluated metrics for non-text-guided models. Adding a text to the reconstruction process improves the results in the perceptual and textual similarity metrics. Note that when we just add text without fine-tuning the model to use it a minor degradation in PSNR is observed. When we fine-tune the model to be text-guided we get the best improvement (Table 1). Figure 6 presents results for reconstructions of real scenes (not captured from a screen) that are provided in [21] and it compares to Flatnet [21] and Tikhonov [43]. Figure 7 presents real scene reconstructions using our prototype camera."}, {"title": "4.1. Implementation details", "content": "In the Laion dataset, the given text captions for the images are not always accurate or relevant. We acknowledge that this noise in the data might harm the results we get in\nthe training process when using these text captions. Since we identified that these inaccuracies might be critical in the tests, we manually checked the test dataset captions to verify the accuracy and correctness of the data. This verification is very important for the text guidance reconstruction results and also for the textual CLIP score evaluation we made. Despite the potential disadvantage of training on incorrect captions, we did not manually verify the training dataset since it is not feasible to manually check such a very large dataset.\nTo compare our results to FlatNet [20], we trained our method on their published dataset which consists of 10k images for training and 100 for testing. This data does not contain captions to the images. Thus, to train our method with text guidance on this data we use a large language model (LLM) for the auto image captioning process. We used llava1.5 [24] LLM and generated captions for all the images in the data. Here we also might have the problem of incorrect captions, which is also known as LLM hallucinations. Also in this case, the test samples captions were manually verified due to the high importance of the test captions' correctness. For this data we trained the model for 700k steps with a similar optimizer setup to what we mentioned in Section 3.1. We used the Allied Vision 1800 U-500 board-level camera with a pixel size of 2.2um and 5 megapixels overall for the prototype camera."}, {"title": "4.2. Ablations", "content": "We present ablation results in Table 2 and Figure 8. First of all, it is noticeable from Table 2 that without our proposed separable loss the reconstructed images are not similar to the target image. We observe that the reconstructed images contain the information of the text caption, according to the high CLIP score, but do not succeed in extracting additional information from the camera measurements for the reconstruction process, i.e., the reconstructions become independent of the input camera measurements. When we do use the separable loss, the measurements are taken into account. Adding text information as input improves the reconstruction even further, compared to the non-text-guided model. The visual ablation images in Figure 8 show that the text captions contribute to the high frequency details in the reconstructed images. When we supply a text caption, the reconstructed image details are aligned with the text. This is noticeable also when a wrong text caption is provided. For example, the reconstruction gains a painting style when the caption mentions a painting and elephant shapes are visible when elephants are described in the caption."}, {"title": "5. Conclusion", "content": "A novel method for image reconstruction from flat camera measurements was presented, achieving high quality reconstructions, with and without text guidance. The method leverages the strong capabilities of a pre-trained diffusion model for image prior. Such an approach can be integrated into other imaging systems to improve the reconstructions. Even though we get perceptually pleasant recon-"}]}