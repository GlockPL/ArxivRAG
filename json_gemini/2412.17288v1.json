{"title": "Multi-Modal Grounded Planning and Efficient Replanning\nFor Learning Embodied Agents with A Few Examples", "authors": ["Taewoong Kim", "Byeonghwi Kim", "Jonghyun Choi"], "abstract": "Learning a perception and reasoning module for robotic assis-\ntants to plan steps to perform complex tasks based on natural\nlanguage instructions often requires large free-form language\nannotations, especially for short high-level instructions. To\nreduce the cost of annotation, large language models (LLMs)\nare used as a planner with few data. However, when elab-\norating the steps, even the state-of-the-art planner that uses\nLLMs mostly relies on linguistic common sense, often ne-\nglecting the status of the environment at command reception,\nresulting in inappropriate plans. To generate plans grounded\nin the environment, we propose FLARE (FEW-SHOT LAN-\nGUAGE WITH ENVIRONMENTAL ADAPTIVE REPLANNING\nEMBODIED AGENT), which improves task planning using\nboth language command and environmental perception. As\nlanguage instructions often contain ambiguities or incorrect\nexpressions, we additionally propose to correct the mistakes\nusing visual cues from the agent. The proposed scheme al-\nlows us to use a few language pairs thanks to the visual\ncues and outperforms state-of-the-art approaches. Our code\nis available at https://github.com/snumprlab/flare.", "sections": [{"title": "1 Introduction", "content": "By the rapid advancement in the fields of computer vision,\nnatural language processing, and embodied AI, we are wit-\nnessing a significant improvement in key functionalities of\nrobotic assistants that can perform daily tasks. These func-\ntions include navigation (Anderson et al. 2018; Chaplot et al.\n2017; Uppal et al. 2024), object manipulation (Zhu et al.\n2017; Ryu et al. 2024), and responsive reasoning (Das et al.\n2018; Gordon et al. 2018; Majumdar et al. 2024) in sim-\nulated 3D spaces (Ge et al. 2024; Chang et al. 2017; Xia\net al. 2018; Kim et al. 2024). Practical robotic assistants\nrequire all of the aforementioned capabilities to understand\nlanguage instructions and actively perceive the environment.\nTo learn an agent that performs such complex tasks, a\nstraightforward approach is to train an agent in a supervised\nmanner by a large amount of natural language instruction\nand action pairs (Shridhar et al. 2020; Ehsani et al. 2024;\nPashevich et al. 2021; Blukis et al. 2021; Kim et al. 2023).\nHowever, annotating instructions and providing expert ac-\ntion sequences (i.e., navigating trajectories) is costly and"}, {"title": "2 Related Work", "content": "We first review the attempts to use an LLM in robotics, espe-\ncially for task planning. Then, we discuss recent approaches\nto tackle complex instruction-following tasks.\nFoundation models for task planning. With a recent\ndevelopment in large foundation models (i.e., LLMs and\nVLMs) (Brown et al. 2020; Chen et al. 2021; Zhang et al.\n2022; Liu et al. 2023), they are used as a tool for reason-\ning (Zeng et al. 2023; Singh et al. 2023; Driess et al. 2023),\nplanning (Song et al. 2023; Sarch et al. 2023; Yang et al.\n2024; Szot et al. 2024), and manipulation (Wu et al. 2023;\nFang et al. 2024) in robot systems. Early approaches in\nrobotic planning (Huang et al. 2022) using LLMs plan the\nsubtasks by iterative enhancement of input prompts. For ex-\nample, when the agent does not execute a planned action,\n(Huang et al. 2023) uses multiple environmental feedbacks\nto adjust the initial plan to recover its failure.\nSimilarly, (Ahn et al. 2022) enabled robot planning with\nskill affordance value functions for planning. To directly\nproduce actionable robot policies, (Singh et al. 2023; Liang\net al. 2023) structured a programmatic LLM prompt. Mean- while, VIMA (Jiang et al. 2023) and PaLM-E (Driess et al.\n2023) use multimodal prompts to control robots."}, {"title": "3 Approach", "content": "Generating executable grounded plans is one of the key com-\nponents in developing a successful embodied AI agent (Mur-\nray and Cakmak 2022; Inoue and Ohashi 2022; Kim et al.\n2023). State-of-the-art methods (Kim et al. 2023; Min et al.\n2022; Blukis et al. 2021; Pashevich et al. 2021) rely heavily\non extensive data, implying that they would not be effec-\ntive in data-scarce learning scenarios. However, given the\nhigh costs of annotating free-form language instructions, it\nis desirable to develop a more practical approach to learn an\nagent using small amounts of data. In addition to efforts to\nuse LLMs as planners (Ahn et al. 2022; Huang et al. 2022,\n2023; Sarch et al. 2023), (Song et al. 2023) uses them to\nlearn an agent with a few examples.\nHowever, LLMs do not always generate plausible plans\nwithout proper prompt, resulting in the generation of non-\nsensical or impractical subgoals. For example, for the task\nof 'Put a cooked potato in the fridge,' the LLM may tell\nan agent to 'wrap the potato with a foil,' where 'wrapping'\nis not supported by the agent. Although LLMs create exe-\ncutable plans quite successfully, the inherent ambiguity and\nlexical diversity of open-vocabulary descriptions often make\nthe connection of language-based instructions to the physi-\ncal world less clear. To be specific, an agent that has learned"}, {"title": "3.1 Multi-Modal Planner", "content": "To generate interpretable subgoal sequences for an agent by\nnatural language instructions, LLMs are widely used (Zeng\net al. 2023; Singh et al. 2023; Driess et al. 2023; Wu et al.\n2023; Sarch et al. 2023). For example, (Song et al. 2023;\nSarch et al. 2023) retrieves in-context examples from the\nsimilarity of language instructions to prompt an LLM. In-\nspired by them, we propose 'Multi-Modal Planner (MMP)'\nthat considers both the natural language instruction and the\nagent's egocentric surrounding views at the moment of re-\nceiving the command to reflect the environment status only\nwith a few annotated data. We illustrate an MMP in Figure 3.\nMulti-modal Similarity. In-context learning for LLM\nlargely improves model performance for a wide spectrum of\nlanguage tasks (Brown et al. 2020). It uses explicit context\nwithin the prompt to refine the model's comprehension and\nresponsiveness to detailed language instructions. To capital-\nize on LLM as a few-shot learner, we need to carefully select\nexamples that are relevant to the task at hand. When avail-\nable, such relevant examples assist the LLM's ability to gen-\nerate appropriate subgoals. For example, when the task is to\n'clean a cloth,' it is strategically sound to prompt the model\nwith examples themed in analogy such as 'cleaning a fork'\nor 'washing dishes' over unrelated tasks such as 'heating ap-\nples.' While (Song et al. 2023; Sarch et al. 2023) achieve\nthis by measuring the distances of the embedded language\ninstructions, they do not consider the environment state (i.e.,"}, {"title": "3.2 Environment Adaptive Replanning", "content": "Despite the emergent ability of planning by large language\nmodels (LLMs), they may generate plans that are not well\ngrounded in environments where agents are deployed. This\nissue can be attributed to the lexical variation inherent in nat-\nural language instructions. For example, consider the task,\n\"Place a tray with a butter knife and slice of the fruit on the\ntable.\" To complete the task, an agent needs to find the fruit\nto be sliced. However, if the agent has not learned the fruit\nobject class for navigation during training, this may lead to\nnavigation failure and possibly, task failure.\nTo address this issue, the proposed \u2018Environment Adap-\ntive Replanning (EAR)' revises subgoals by replacing an\nundetected object with the most semantically similar object\namong those observed so far. To revise the subgoal, EAR\nfirst maintains a list of all detected objects that have been\nobserved so far while completing the task. For each subgoal,\nif the agent cannot reach a navigation target (i.e., On or Rn),\nEAR infers that the specified object is absent from the envi-\nronment and replaces it with a semantically analogous one.\nTo replace a current unavailable object with another one,\nEAR finds the most semantically similar object among the\ncandidates (i.e., objects observed so far). To measure seman-\ntic similarity, we compute the cosine similarity of language\nrepresentations of two object class names. Specifically, EAR"}, {"title": "3.3 Action Policy", "content": "For object interaction, the agent first navigates to a target ob-\nject and reaches it in a close vicinity. For navigation, a viable\napproach is to use imitation learning (Shridhar et al. 2020;\nSingh et al. 2021; Pashevich et al. 2021; Nguyen et al. 2021).\nHowever, it requires a large number of training episodes for\nacceptable performance, but collecting these episodes may\nnot always be available, especially in our case where train- ing data collection is often costly and time-consuming.\nTo avoid this issue, recent approaches (Inoue and Ohashi\n2022; Kim et al. 2023) incorporate deterministic algorithms\n(e.g., A* algorithm, FMM (Sethian 1996), etc.) obstacle-free\npath planning, leading to significant performance improve- ments compared to those learned by imitation learning. In- spired by recent observations, we adopt the deterministic ap- proach (Sethian 1996) for effective path planning."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nWe employ four large language models for our FLARE\nto validate the compatibility of the proposed methods with\ndifferent models, incorporating both proprietary and open-\nsource models. Specifically, we use GPT-4 and GPT-3.5 as\nproprietary models, and LLaMA2-13B (Touvron et al. 2023)\nand Vicuna-13B (Zheng et al. 2023) as open-source models.\nWe select k 9 in-context examples, following (Song et al."}, {"title": "4.2 Dataset and metrics", "content": "We evaluate the effectiveness of our FLARE in the AL-\nFRED (Shridhar et al. 2020) benchmark. It requires agents\nto complete household tasks based on language instructions\nand egocentric observations within interactive 3D environ-\nments (Kolve et al. 2017). Both validation and test sets in-\nclude seen and unseen scenarios, where the seen scenario is\npart of the training data, while the unseen scenario repre-\nsents a new and unfamiliar environment for evaluation.\nTo evaluate the efficiency of FLARE where human lan-\nguage pairs are scarce, we followed the same few-shot\nsetting(0.5%) as in the previous work (Song et al. 2023).\nFor a fair comparison with the previous methods, we use the\nsame number of examples (Song et al. 2023) (i.e., 100 ex-\namples). The selected 100 examples contain all 7 task types\nfor fair representations of 21,023 training examples.\nFor evaluation, we follow the same evaluation protocol as\n(Shridhar et al. 2020). The primary metric is a success rate\n(SR), measuring the percentage of completed tasks. A goal-\ncondition success rate (GC) measures the percentage of sat-\nisfied goal conditions. Furthermore, we assess the efficiency\nof agents penalizing SR and GC (i.e., PLWSR and PLWGC)\nwith the path length of a trajectory taken by the agents. More\ndetails on the dataset and metrics are provided in Section A."}, {"title": "4.3 Comparison with State of the Arts", "content": "We first compare our method with state-of-the-art meth-\nods (Blukis et al. 2021; Min et al. 2022; Kim et al. 2023;\nSong et al. 2023) and summarize the result in Table 1. Fol- lowing (Min et al. 2022; Kim et al. 2023; Blukis et al. 2021), we report the performance of agents using 1) only a goal statement, denoted by 'Goal instruction only,' and 2) both goal statement and step-by-step instructions, denoted by 'Goal instructions+Sequential instructions.'\nFirst, we observe significant performance drops from the full-shot setting to the few-shot setting from methods that re- quire a large amount of data to train planners (HLSM, FILM, and CAPEAM). This implies that learning task-performing agents with limited training examples poses a significant challenge, as this data scarcity can hinder the learning of"}, {"title": "4.4 Ablation Study", "content": "We conduct a quantitative ablation study to analyze compo- nents proposed in FLARE and summarize the result in Ta- ble 3. We choose GPT-3.5 over GPT-4 as the language model due to the latter's significantly higher token generation cost."}, {"title": "Without Multi-Modal Planner", "content": "First, we ablate the 'MMP' from our method and the agent considers unimodal similarity to retrieve in-context examples from the dataset, neglecting the environment state for planning. Without the proposed component, we select in-context examples based on instruction similarity. Since a prompt reflects a single modality, the agent may omit environmental cues and mis- interpret task requirements, leading to performance drops in both seen and unseen splits, as shown in (#(a) vs. (#b))."}, {"title": "Without Environment Adaptive Replanning", "content": "We then ablate 'EAR' from our agent.Without EAR, an agent can- not handle language variation and often misinterprets nat- ural language instruction, leading to an erroneous subgoal. We observe noticeable performance drops (1.76%p, 1.51%p in SR) in both seen and unseen splits, as shown in (#(a) vs. (#c)). This implies that LLMs often fail to generate grounded plans in the environment where the agent is de- ployed, causing the agent to wander in search of an object that may not be present, eventually leading to task failure."}, {"title": "Without both", "content": "Without any of the proposed components, the agent adheres to the initial plan, which may not corre- spond to the current task. As expected, our agent without both 'MMP' and 'EAR' achieves the lowest performance among the agents equipped with either or both (#(d) vs. (#a, b, c)). Furthermore, we observe that using both multi-modal planning and adaptive replanning of the environment improves performance compared to using only either of them ((#(d) \u2192 #(b, c)) vs. (#(d) \u2192 #(a)), implying that both components are complementary to each other."}, {"title": "4.5 Qualitative Analysis", "content": "We analyze our method with several qualitative results and illustrate the result in Figure 5, 6 and in Section C."}, {"title": "4.6 Application in Robotic Task Planning", "content": "We demonstrate the generalizability of the proposed FLARE to other robotic task applications. Specifically, we use a simulated tabletop environment with an UR5 robot arm and illustrate a comparison between FLARE and the base- line model in Figure 7. We choose (Zeng et al. 2023) as the baseline model for its effectiveness in few-shot robot plan- ning. Both models use GPT-3.5 as an LLM to generate sub- goals and employ a privileged low-level policy which uses the environment's metadata for end effector pose prediction. We observe that FLARE successfully rearranges objects as instructed, demonstrating its capability in planning for grounded execution. In contrast, the baseline (Zeng et al. 2023) fails due to an ungrounded plan (e.g., attempting to pick a ScrewDriver that is not present in the environment)."}, {"title": "5 Conclusion", "content": "We propose FLARE with a multi-modal planner that re- flects both environmental status by visual input and language instruction to generate detailed plans (i.e., subgoals) to ac- complish a long-horizon tasks with a few data. Addition- ally, it revises only the subset of the subgoals that are in- correct to generate physically grounded plans without us- ing LLMs, leading to computationally efficient replanning. We empirically validate the effectiveness of the proposed components in ALFRED (Shridhar et al. 2020) and observe that our FLARE outperforms the state-of-the-art methods in few-shot settings by significant margins in all metrics.\nLimitations and future work. Although our method re- quires a very few fraction of training data (0.5%), it still re- quires the training data. We aim to develop an agent that learns about environments through exploration, assisted by large language models, without needing any training data."}, {"title": "A Details of ALFRED Benchmark", "content": "The ALFRED (Action Learning From Realistic Environ- ments and Directives) benchmark (Shridhar et al. 2020) is a benchmark designed to test embodied agents in under- standing and executing a variety of natural language instruc- tions within a simulated household environment. It consists of seven task types with 115 distinct object types. The aim is to understand natural language instructions and to complete long-horizon tasks. To satisfy predefined task conditions, an agent must execute a sequence of actions and generate object masks for interacting with objects in the environment. Each task comes with a high-level natural language instruction, accompanied by detailed, low-level directives that specifi- cally guide the agent's actions. Failure to meet any of these conditions results in the task being deemed unsuccessful.\nALFRED provides three distinct splits: 'training,' 'vali- dation,' and 'test.' Agents can be trained with the 'training' split and can have their approaches verified within the 'val- idation' split where they have access to the ground-truth in- formation of the tasks in those splits. The agents are then evaluated in the 'validation' and 'test' splits, without any ground-truth data pertaining to the tasks.\nAt every timestep, an agent within the environment oper- ates based on an egocentric RGB visual input in the shape of the 300 \u00d7 300 image. From this input, the agent must select an appropriate action from a predefined action space, which includes both navigational and object interaction commands. Alongside these actions, the agent also generates a binary object mask to specify interaction targets, corresponding to the same resolution (i.e., 300 \u00d7 300) as the visual input.\nAction commands consist of navigational actions such as MOVEAHEAD, ROTATERIGHT, ROTATELEFT, LOOKUP, and LOOKDOWN. Interaction actions encompass PICK- UPOBJECT, PUTOBJECT, OPENOBJECT, CLOSEOBJECT, TOGGLEOBJECTON, TOGGLEOBJECTOFF, and SLICEOB- JECT. The action STOP signifies the agent's decision to end the task, ideally once all conditions are met.\nALFRED employs multiple metrics to comprehensively quantify an agent's performance. The primary metric is the Success Rate (SR), which measures the proportion of fully completed tasks. A secondary metric, the Goal-Condition Success Rate (GC), accounts for partially completed tasks where the agent satisfies some but not all of the required con- ditions. Finally, Path Length Weighted (PLW) scores adjust the SR and GC metrics (i.e., PLWSR and PLWGC) based on the length of the action sequences undertaken by the agent. Expert demonstrations, which use the shortest path for nav- igation without unnecessary exploration, are generally con- sidered optimal. If an agent takes twice as long as the expert to complete a task, it gets only half the credit."}, {"title": "B Additional Implementation Details", "content": "B.1 Semantic Mapping\nWe first predict an instance segmentation and a depth from the agent's egocentric RGB input. Then we transform these predictions into a point cloud, where each point is assigned a corresponding semantic label, resulting in labeled voxels. To generate the 2D semantic map, we finally aggregate these 3D voxels by summing them across their vertical dimension At every step, the model continuously updates the global map by incorporating the newly obtained partial maps.\nB.2 LLM and Prompt\nWe use four large language models for the implementa- tion of FLARE. For proprietary models\u00b9, we use GPT-3.5- TURBO-INSTRUCT (referred to as GPT-3.5 in the main text) and GPT-4-0125-PREVIEW (referred to as GPT-4 in the main text). We set the temperature to 0 to ensure repro- ductivity and apply a logit bias of 0.1 to all allowable out- put tokens (e.g., allowable A, O, R in Sec. 3.1). For open- source models, we use LLAMA-2-13B-CHAT2 (referred to as LLaMa2-13B in the main text) and VICUNA-13B-V1.53 (referred to as Vicuna-13B in the main text). For both open- source models, we set the temperature value to default.\nWe provide an example of prompt used in MMP (Sec. 3.1) in Figure 8. We first provide an explanation of the task and a list of all allowed actions and objects (block denoted with pink). Then, we present retrieved examples as in-context examples with headers \u2018Task description,' 'Step-by-step in- structions,' and 'Next plan' (block denoted with blue). Fi- nally, we show the current task in the same format as in-context examples, leaving the blank after the 'Next plan' header (block denoted with green). For 'Goal instruction only' setup, which prohibits the use of step-by-step instruc- tions, we remove step-by-step instructions in the prompt for both retrieved example and the current task."}, {"title": "C_Additional Qualitative Examples", "content": "We provide additional qualitative examples in Figures 9 and 10 in the same manner as in Figure 6. To success- fully complete a task, not even a single subgoal should fail. Figure 9 shows an example of an agent without EAR (i.e., FLARE w/o EAR) successfully finding the mug and heat- ing it up. However, it fails to place the object in the intended receptacle and the task is considered a failure. In contrast, as FLARE cannot locate CoffeeMaker, it requests replanning to EAR. EAR infers that CoffeMachine is the most semanti- cally similar to CoffeeMaker, and revises its subgoal.\nIf the first subgoal is incorrect, an agent would fail to achieve subsequent subgoals, aimlessly searching for an un- detectable object. Figure 10 illustrates that the agent without EAR (i.e., FLARE w/o EAR) achieves zero goal conditions (GC). Due to the operator misidentifying the object as a Cup, not a Mug, based on its similar appearances and functional- ity, the agent roams the room in endless search of a Cup."}], "equations": ["Sm = w_l \\frac{\\sum_{i=1}^N S_{l,i}}{N} + w_e \\frac{\\sum_{i=1}^N S_{e,i}}{N},", "S_n = (A_n, O_n, R_n),", "V^* = \\arg \\max_{V_i} S_c(\\text{Enc}(O_k), \\text{Enc}(V_i)),"]}