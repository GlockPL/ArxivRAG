{"title": "ROBIN: A SUITE OF MULTI-SCALE VISION-LANGUAGE MODELS AND THE CHIRP EVALUATION BENCHMARK", "authors": ["Alexis Roger", "Prateek Humane", "Daniel Z. Kaplan", "Kshitij Gupta", "Qi Sun", "George Adamopoulos", "Jonathan Siu Chi Lim", "Quentin Anthony", "Edwin Fennell", "Irina Rish"], "abstract": "The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, a lot of significant advances have been made in Vision-Language Models (VLMs), driven by breakthroughs in computer vision and natural language processing. However, existing VLM benchmarks, often designed for specific tasks (e.g., VQAv2), struggle to accurately reflect real-world VLM performance and capture nuanced differences between models. This is particularly evident when evaluating models with significant architectural variations, where standard benchmark scores remain similar despite noticeable differences in human-perceived model quality.\nTo address this issue, we introduce CHIRP, a hybrid VLM benchmark that combines automated metrics' scalability with human evaluators' nuanced judgment. We argue that this approach is crucial for capturing the complexities of VLM behavior, which traditional benchmarks often fail to represent.\nTo demonstrate the limitations of existing benchmarks and the efficacy of our proposed method, we introduce Robin, a suite of VLMs trained at various scales, inspired by the Pythia language model suite. By systematically varying the Vision Encoder (VE) and the Large Language Model (LLM) sizes, we will show that while benchmark scores remain largely unaffected, human evaluations reveal significant differences in the models' outputs quality.\nOur findings underscore the need for more robust and human-centric VLM evaluation methodologies. CHIRP paves the way for developing more reliable and informative VLM benchmarks, ultimately leading to the creation of more effective and impactful VLMs.\nOur Contributions:\n\u2022 We investigate the drawbacks of relying on automatic metrics and show the benefits of AI-based and human-based evaluations of VLMs.\n\u2022 We present CHIRP, an open-ended question-and-answer benchmark."}, {"title": "2 RELATED WORK", "content": "Scaling Suites. Scaling laws have recently emerged as one of the central research areas in large foundation models. These laws enable performance prediction based on variations in compute time, dataset size, and model parameters, facilitating efficient resource allocation by extrapolating results from small-scale experiments.\nKaplan et al. Kaplan et al. (2020) pioneered the application of scaling laws to language models, demonstrating a power-law relationships between loss and model size, dataset size, and compute time. This has led to practical applications, such as the Pythia suite which comprises of identically trained language models with varying parameter sizes, empirically verifying these scaling laws.\nCherti et al. Cherti et al. (2023) investigated the scaling laws of the CLIP vision encoders, training and comparing different sizes of the CLIP vision encoders on the same data. These models indeed verified the aforementioned scaling laws and have become a very popular suite of models.\nAI-based Evaluation. The advent of powerful foundation models like GPT-4V offers a new way to evaluate weaker models, moving beyond traditional, rigid metrics such as exact string matching, as done in Hudson & Manning (2019); Mishra et al. (2019); Singh et al. (2019). Early evidence from benchmarks like MM-Vet and VQA tasks suggests that evaluating with stronger models offers a promising path towards more comprehensive and insightful evaluation, surpassing the limitations of static, string-based methods. This shift towards leveraging the semantic understanding of LLMs for evaluation promises to unlock a better understanding of model capabilities.\nZheng et al. Zheng et al. (2023) introduce two bench-\nmarks, MT-Bench and Chatbot Arena, to explore the feasibility of employing LLMs as judges. Their findings indicate that advanced LLMs, such as GPT-4, closely align with human preferences, achieving over 80% of agreement. Similarly, AlpacaEval utilizes LLMs to assess instruction-following models.\nWu et al. Wu & Aji (2023) focused on the bias in evaluations conducted by both human and LLM annotators, particularly noting a preference for flawed content if it avoids brevity or grammatical errors, and introduced the Multi-Elo Rating System (MERS) for more nuanced assessments. A study by Koo et al. Koo et al. (2023) pointed out significant biases of LLMs evaluators, with an average Rank-Biased Overlap (RBO) score of 49.6%, suggesting a misalignment between machine and human preferences."}, {"title": "3 ROBIN VLM SUITE: TRAINING METHODOLOGY", "content": "We review the methodology used to train our scaling suite, and the different experiments conducted with the trained models.\n3.1 MODEL ARCHITECTURES\nOur models are based on the LLaVA architecture and consist of three components: a pretrained vision encoder, a MultiLayer Perceptron (MLP) projection that converts image features"}, {"title": "3.2 EXPERIMENTAL DESIGN", "content": "To design a scaling suite for VLMs, we vary the language encoder and vision encoder. Our setup is based on the Pythia suite which maintains consistent training data and order, leaving model size as the only variable. Similarly, the CLIP vision encoders released by LAION follow this pattern. We train VLMs using 5 Pythia sizes (410M, 1.4B, 2.8B, 6.9B, and 12B parameters) paired with 4 CLIP models (Base, Large, Huge, and gigantic). The sizes of these CLIP models are detailed in Table 1, resulting in 20 Robin models. The scaling laws for the Robin suite over VE size and LLM size is shown in Figure 1.\nWe run experiments across all Robin models or across two main ablations:\n1. LLM Size ablation - ablate the Pythia model size across the Robin models with the gigantic CLIP vision encoder (ViT-g)\n2. VE Size ablation - ablate the CLIP model size across the Robin models with a 12B parameter Pythia LLM"}, {"title": "4 BENCHMARK RESULTS", "content": "We ran our suite of models on the following benchmarks: ScienceQA , GQA , VQAv2, TextVQA, MM-Vet, and LLaVA-Bench. The complete results of the models on these benchmarks are detailed in Appendix A.4, which includes a complete score table (Table 5) and heatmaps for all benchmarks (Figure 10). Figure 2 shows the scaled average scores. Due to varying score distributions across benchmarks, we use a scaled average. For example, VQAv2 scores range from 40 to 60, while MM-Vet scores range from 6 to 18. The scaled score is calculated as follows: let S be the matrix of scores, with each row Si: representing the scores model i obtained on all N benchmarks, and S.,j representing the scores of all models on benchmark j. Let S* be the scaled scores vector.\n$S_{i}^{*}=\\frac{1}{N}\\sum_{j}\\frac{S_{i,j} - min(S_{:,j})}{max(S_{:,j}) - min(S_{:,j})}$\nThe scaled average is plotted in Figure 2. As it is shown, there is no clear relationship between VE size and model performance. However, a slight trend between LLM size and performance is"}, {"title": "5 INVESTIGATING EXISTING BENCHMARKS", "content": "Empirical testing suggested that existing benchmarks might not capture all observed model capabilities. We aimed to determine whether the standard evaluation methods were inaccurate or if the benchmarks themselves were flawed.\nTo rigorously assess the reliability of existing benchmarks, we sampled 100 random questions from GQA as well as 100 from TextVQA. These questions require the model to observe the image and answer objective facts. We examined the questions, the provided ground truth answers, and model responses across all model size combinations.\nIn 100 questions sampled from GQA, we found that 9 questions had incorrect ground truth answers. If we want to estimate the error of this value, the actual percentage of incorrect prompts p is\n$p \\in p \\pm z \\sqrt{\\frac{p*(1-p)}{n}}$.\nFor a 95% confidence interval: z = 1.96, and with our sample size n of 100, we measured p = 0.09, we are 95% certain: 3.4% < p < 14.6%. This equates to 770,769 to 3,309,773 questions of the 22,669,678 GQA questions being incorrect. Although this is a large spread, this result rmeains quite significant, as most improvements on State of The Art (SoTA) models are very small, regularly under 3%. These findings lead to the conclusion that if 2 models score within 3% of each other on GQA, they could very well be equal in actual performance on it. Representative examples of the aforementioned questions are shown in Appendix B.4.1.\nConducting the same study for TextVQA, we identified only 5 problematic questions in the sample that either did not require reading the text in the image, or were too vague and did not correspond to a clear correct answer. Redoing our previous calculations, we conclude with 95% certainty that 0.73% < p < 9.27%. Although SoTA models are indeed close in performance, we are not as confident as in the case of GQA. However, two SoTA models scoring within 0.7% of each other on TextVQA can be considered equally good on the benchmark. Representative examples of the aforementioned questions are shown in Appendix B.4.2.\nUltimately, after examining benchmarks and responses, we propose the following hypotheses for why our models did not exhibit expected scaling trends:\n\u2022 short responses don't convey enough information to thoroughly evaluate model performance\n\u2022 benchmarks were graded inaccurately\n\u2022 vague questions with multiple possible answers and incorrect ground truth answers\n\u2022 questions themselves don't demand a detailed examination of images\nIn the following subsections, we test each of the above hypothesis to see if addressing these issues reveal trends in model scale we hadn't observed previously."}, {"title": "5.1 LONG VS SHORT RESPONSES (LVSR)", "content": "Most benchmarks were evaluated on short responses; with explicit instructions to \"respond with one word or phrase\". However, we hypothesize that short responses do not convey sufficient information to evaluate model performance in detail. To test this theory, we allowed models to generate longer responses without prompting for brevity. We then collected, manually evaluated, and compared these LvSR to see if they offered a more nuanced assessment of the models.\nThe GQA benchmark provides an evaluation script that grades responses using string matching on single phrase responses. On the sample of 100 GQA questions, we prompted and manually graded our models for LvSR to see if new trends across the LLM size ablation appear with longer responses, the results of which are shown in Figure 3. For sufficiently large models, we did not notice a significant improvement in overall model accuracy. However, models often got different questions correct when responding with LvSR. To show this, we calculated a superscore, in which responses were marked correct if either the long or short response was correct (See Figure 3). The improved results of the superscore indicate that while long and short responses achieve a similar overall accuracy, they tend to be accurate for a different set of questions. This suggests that evaluating"}, {"title": "5.2 INACCURATE GRADING AND LLM EVALUATIONS", "content": "Most existing automatic metrics are incapable of evaluating longer responses, and often fail in scenarios where the models being tested do not output the answer in the expected format. For example, models may respond with a synonym for the ground truth, which can cause issues with exact string matching based evaluation. These issues can be especially prevalent with non instruction tuned models, or small scale models.\nTo address responses that automated evaluations cannot recognize, we utilize a the GPT-4 LLM OpenAI (2023) to evaluate whether a given response matches the correct answer or not. We ran this LLM evaluations on both the long and short responses, using the prompting detailed in Appendix B.3.\nOn short responses, LLMs tend to mark more answers as correct when compared to existing automated evaluations. An example of this behaviour can be found in Appendix B.4.3. By comparing LLM evaluations to manual evaluations of LvSR in Figure 4, we calculated the accuracy of LLM evaluations on LLM size. This analysis shows that LLM evaluations can be slightly more accurate than automated evaluations, though not enough to reveal new model capabilities."}, {"title": "5.3 MULTIPLE POSSIBILITIES AND VLM EVALUATION", "content": "Our empirical analysis revealed that ground truth answers are not always representative of all possible correct answers. In GQA and TextVQA, this issue arises from ambiguous questions that can have multiple valid answers, as shown in Appendix B.4.1 and B.4.2. In questions where ground truth answers don't encompass all valid answers, LLMs don't have sufficient information to accurately responsed.\nWe explore using stronger VLMs, namely LLaVA-34B and GPT-4V, to evaluate our models responses in order to account for such cases. We ask the VLM to individually evaluate each model's long response, question by question. The exact prompts used for LLaVA-34B and GPT-4V are in Appendix B.3.\nA comparison of the accuracy of LLaVA-34B and GPT-4V over LLM scale can be seen in Figure 4. GPT-4V evaluations of GQA differed from human evaluations more than LLaVA-34B due to GPT-4V applying stricter grading criteria. Appendix B.4.3 presents a few such examples. Although LLaVA-34B had higher accuracy, we hypothesize that further work could align GPT-4V's grading schema closer to the LLaVA-34B grading by prompting for a looser grading, likely leading to improved results for GQA evaluation."}, {"title": "5.4 COMPARISON OF AUTOMATED, LLM, AND VLM EVALUATIONS", "content": "We graph scaling across LLM size, and VE size using all AI evaluation methods in Figure 4. AI evaluations seem to yield different and more accurate results across the largest LLM and VE sizes."}, {"title": "6 CHIRP BENCHMARK", "content": "To address the drawbacks of existing benchmarks outlined in Section 5, we introduce CHIRP, a new evaluation benchmark, which grades long form responses. CHIRP comprises of 104 open ended questions, evaluated by either humans or VLMs. These free form questions do not correspond to a single \"correct\" answer. Instead, they require models to generate flexible, creative and complex responses. Consequently, we evaluate models using a preference based rating in which two model's responses are compared side by side. Instructions on downloading the CHIRP benchmark can be found in Appendix C.\n6.1 GENERATING THE DATASET\nWe wrote questions along with image descriptions, which we then refined with the help of GPT-4 OpenAI (2023). The image descriptions were given to Dalle-E 3 to generate the associated images. We would then iterate and finetune the description by hand in order to get the desired image.\nThe questions created are classified in 8 distinct categories: descriptive analysis, inferential reasoning, contextual understanding, emotional and psychological understanding, ethical evaluations, abstract understanding, creative and subjective analysis, and visual aesthetics evaluation. Detailed descriptions and examples of these categories can be found in Appendix C.1."}, {"title": "6.2 HUMAN BASED EVALUATIONS", "content": "We utilized CloudResearch for large scale human evaluation of our model's responses. To this end, we presented users with the responses of two models and asked them to indicate their preferred response on a set of criteria. There are 5 criteria: overall preference, relevance and completeness, understanding and reasoning, hallucinations, and details. These criteria were chosen as empirical evidence showed that these were under-evaluated in other benchmarks and the most important to a user's perception of the model quality. An example of the user interface as well as a detailed description of each criterion can be found in Appendix C.2.1.\nWe validated this evaluation method by evaluating our suite of models on all five criteria across the LLM size and VE size ablations. Due to limitations in time and budget, for each question of the dataset, we randomly sample five model matchups out of all the model pairwise combinations. We also ran evaluations across our entire suite of VLMs to judge the overall preference criteria. To this end, we randomly selected 25 matchups from the 190 possible pairs of Robin models. Full details on the human evaluation setup can be found in Appendix section C.2.2."}, {"title": "6.3 VLM BASED EVALUATIONS", "content": "To evaluate our models on CHIRP at scale, we experiment with the use of VLMs: GPT-4V and LLaVA-34B. Rather than asking a human for model preferences, we asked the VLMs to indicate their preferred response for each criterion. For GPT-4V, we utilized two distinct prompts: GPT-4V (S) (simple), which directly solicited model preferences, and GPT-4V (R) (reasoning), which prompted the VLM to reason before making a decision. We extracted the VLMs final choice using GPT-3.5. Detailed explanations of these prompts are provided in Appendix C.2.3.\nWe evaluated all combinations of matchups from the LLM size and VE size ablations across all criteria. We also ran GPT-4V (R) evaluations on a random sample of 50 matchups from all combinations of Robin models in the overall preference criteria."}, {"title": "6.4 ELO RATINGS", "content": "To benchmark our models using CHIRP, we calculated Elo scores based on the evaluators' indicated preferences. Because Elo calculations are not order-agnostic, we performed 500 bootstrap iterations for each Elo score.\nThe results from the human and VLM evaluations using this average Elo rating is shown in Figure 5.\nWith regards to the LLM size ablation, we note a clear scaling trend, with all the evaluators ranking the bigger models the best performers across all categories. However, we do note that the biggest marginal improvement occurs from the 410M Pythia-based Robin to the 1.4B Pythia-based Robin.\nWith regards to the VE size ablation, only the human survey results exhibit a strictly monotonically increasing trend with scale. Indeed, AI evaluations of CHIRP do not correlate VE size with model performance. GPT-4V (R) evaluations of CHIRP demonstrate some scaling with model size, with ViT-L performing surprisingly well. To the contrary, LLaVA-34B gives a very consistent score to all models across all categories, with the exception of the \u201challucination\u201d evaluation where the trend is similar to the one from GPT-4V (R). It is worth noting however that human surveys exhibit high variance in Elo trends, mostly due to different evaluators having very different preferences.\nThe heatmap of median Elo scores in Figure 6 allows us to directly compare GPT-4V (R) and human surveys results. In the following sections, we will explore why GPT-4V (R) evaluations seem to capture some trends more distinctly while not others."}, {"title": "6.5 AGREEMENT", "content": "To evaluate the efficacy of AI evaluations, we first examine the agreement between AI and human preferences. To this end, we use Cohen's Kappa.\n6.5.1 COHEN'S KAPPA\nCohen's Kappa Cohen (1960) is a method used for calculating inter-rater reliability, that takes into account random chance agreement. A Cohen's Kappa score of 1 indicates a complete agreement between reviewers, while a Kappa of 0 indicates no agreements other than a random chance of agreement. Further details on the calculation of Cohen's Kappa can be found in Appendix C.2.5.\nLooking at Table 2, the results indicate that both GPT-4V (S) and GPT-4V (R) have higher agreement with human surveys compared to LLaVA-34B. We also note that GPT-4V (R) exhibits the most agreement to the human surveys of the both of them. However, according to Landis & Koch's interpretation of Cohen's Kappa Landis & Koch (1977), GPT-4V (R) only achieves \u201cslight\u201d to \u201cfair\""}, {"title": "6.5.2 MODEL SIZE AGREEMENT", "content": "For each of our evaluation methods, we calcu-lated the frequency with which the evaluator preferred the larger model in any given matchup. The results in Table 3 show that GPT-4V evaluations favor models with more parameters more frequently than human evaluators. We also see that although users tend to prefer larger mod-els, this is not as systematic as we had initially believed."}, {"title": "6.5.3 CONTRADICTIONS", "content": "One hypothesis for why trends are better captured using AI evaluations is that a single AI evaluator is more consistent than the combined evaluations of many different humans, as different humans may have different preferences or leniency. We tried negating this by aggregating multiple human surveys together however it is possible this still influenced the results. To evaluate the consistency of AI versus human evaluators, we introduce a concept to measure contradictions in their rankings. A contradiction occurs when an evaluator's preferences form a cycle, such as preferring A over B, B over C, but then C over A. A more exhaustive explanation along which sample graphs is given in Appendix C.2.8. This inconsistency suggests a lack of transitivity in their judgments. By counting these contradictions, we can determine how reliably an evaluator ranks models.\nThe results presented in Figure 7, indicate that human and LLaVA-34B based evaluations tend to have the most contradictions, requiring more runs to average out human or model inconsistencies. GPT-4V (R) however is the model with the least contradictions, leading us to the conclusion that a single run is sufficient as the model is highly consistent in its responses."}, {"title": "6.6 OBSERVATIONS AND INSIGHTS", "content": "Although AI-based evaluations don't consistently agree with human evaluations on a case-by-case basis, GPT-4V (R) displays both higher agreement with humans preferences and less contradictions than GPT-4V (S).\nIn general, GPT-based evaluations tend to produce lower variance results which correlate better with training loss, as shown Appendix C.2.6. We hypothesize that these smoother results are attributed to the fact that GPT employs a more consistent approach to grading across evaluations, whereas multiple different human evaluators lead to more variability, as indicated by the higher rates of contradictions. This variability could be affecting our ability to accurately measure how well human evaluations correlate with training loss and we hope to address this in future work.\nAnother possibility is that AI evaluations favor models with larger LLMs because the LLMs generate preferable strings of words irrespective of the content of the image. However, we rule out this possibility by showing that GPT-4V (R) preferences do not align with the more likely logit probabilities of question-answer strings in Appendix C.2.7."}, {"title": "6.7 LIMITATIONS", "content": "Although the CHIRP benchmark revealed scaling trends in our models that other benchmarks did not, it has several notable limitations. First, it heavily relies on the strong language proficiency of the evaluator, to evaluate a models' perceptual capabilities.\nSecond, the benchmark is not very extensive as it only contains 104 questions on 104 images. However, the small size is a deliberate choice based on the cost of evaluations. As grading the responses requires VLM or human evaluations, cost is a major consideration when deciding the size and 104 was seen as a good balance between evaluating the models performance and the cost or evaluating. This is in line with other small, high quality, and well respected datasets like MM-Vet , 218 questions on 200 images, and LLaVA-Bench, 60 questions on 24 images, which both require LLM evaluations, which itself is cheaper than VLM or human evaluations.\nFinally, models are benchmarked via pairwise matchups. Therefore models can only be compared via a direct matchup or mutual matchups. This requires more work when validating a new model, requiring matchups which each of the most performant models, however we believe this is a valuable trade-off for a considerably more accurate evaluation and ranking."}, {"title": "7 CONCLUSIONS", "content": "In this paper, we explore the limitations of existing vision-language model (VLM) benchmarks, and introduce CHIRP, a novel benchmark designed to address these shortcomings. Our analysis reveals that a longer-form benchmark with open-ended questions quantifies multimodal understanding in ways that existing benchmarks do not. While current benchmarks evaluate contextually relevant responses, they often fail to capture the subtleties that humans value in long-form content.\nExpensive evaluations. We acknowledge that generating and evaluating long-form responses, especially with human evaluators, can be resource-intensive. To mitigate this challenge, we have designed CHIRP to remain effective even at a smaller scale. Additionally, our findings suggest that AI evaluations can serve as a reliable proxy for human assessments, demonstrating similar overall trends in the same unique skill we aim to test for.\nAs VLMs continue to advance towards and beyond human-level performance on quantitative tasks, we emphasize the need to assess models on qualitative tasks that reflect the nuances of human preferences. Our work demonstrates that CHIRP is a viable benchmark for evaluating skills that have not been previously reported."}]}