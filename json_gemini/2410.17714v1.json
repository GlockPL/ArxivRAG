{"title": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient Semantic Steering in Large Language Models", "authors": ["Xintong Wang", "Jingheng Pan", "Longqin Jiang", "Liang Ding", "Xingshan Li", "Chris Biemann"], "abstract": "Despite their impressive capabilities, large language models (LLMs) often lack interpretabil-ity and can generate toxic content. While using LLMs as foundation models and applying semantic steering methods are widely practiced, we believe that efficient methods should be based on a thorough understanding of LLM behavior. To this end, we propose using eye movement measures to interpret LLM behavior across layers. We find that LLMs exhibit pat-terns similar to human gaze across layers and different layers function differently. Inspired by these findings, we introduce a heuristic steering layer selection and apply it to layer intervention methods via fine-tuning and inference. Using language toxification and detoxification as test beds, we demonstrate that our proposed Cog-Steer methods achieve better results in terms of toxicity scores while efficiently saving 97% of the computational resources and 60% of the training time. Our model-agnostic approach can be adopted into various LLMs, contributing to their interpretability and promoting trustworthiness for safe deployment.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Touvron et al., 2023; Achiam et al., 2023) have demonstrated strong abilities in natural language understanding and reasoning (Zhao et al., 2023). Despite the continuous increase in performance and the emergence of new capabilities from scaling LLMs (Wei et al., 2022), there are significant concerns about interpretability, safety, and trustworthiness in LLMs, leading to issues such as hallucinations (Huang et al., 2023) and toxicity (Schick et al., 2021). As the trend of using pre-trained LLMs as foundation models, followed by fine-tuning to align with human preferences (Ouyang et al., 2022; Rafailov et al., 2024), the lack of transparency in LLMs has further limited the development of efficient post-training and inference methods.\nPrevious work has introduced diverse methods of interpretability, including training linear classifiers as probes on top of hidden representations (Belinkov, 2022), projecting representations into vocabularies (Geva et al., 2022), and intervening on the computation path, such as knowledge neurons (Dai et al., 2022) and circuits (Conmy et al., 2023; Ghandeharioun et al., 2024). However, these methods exhibit practical shortcomings. Probings rely on limited pre-defined classes, projection methods depend on human effort to interpret limited concepts, and interventions require well-designed prompts, making them difficult to scale and generalize. Furthermore, Bills et al. (2023) uses large language models to interpret smaller language models through natural language explanations, which we find problematic, as it involves using one black box to understand another.\nWe believe that a more promising approach to understanding LLMs is to analyze how they operate and behave similarly to the way humans process tokens and integrate information during reading. This can be achieved by using highly interpretable"}, {"title": "2 Related Work", "content": "Interpretability of Large Language Models Interpretability research is essential for uncovering the mechanisms of LLMs, ensuring their safe and trustworthy deployment as foundation models (Zhao et al., 2024). Various studies have analyzed how knowledge is stored in LLMs, focusing on concepts like knowledge neurons (Dai et al., 2022) and the circuits (Conmy et al., 2023; Yao et al., 2024). Knowledge neurons serve as a tool for localizing specific activated neurons, while the circuits framework identifies information propagation paths (Goldowsky-Dill et al., 2023; Stolfo et al., 2023). We argue that these methods are insufficient as they rely on projections into the vocabulary space (Rai et al., 2024) and well-designed prompts (Bills et al., 2023), using humans to interpret limited samples as concepts (Geva et al., 2022). Moreover, Wang et al. (2024a) preliminarily reveals that GPT predicts tokens more similarly to humans compared with shallow language models but lacks engagement with LLMs and a broad discussion on internal mechanisms. Unlike previous works, our method explores the interpretability of LLMs through human-interpretable indicators involving eye movement theory, to precisely understand and control model behavior (see \u00a7 3.1, 3.2).\nParameter-Efficient Semantic Steering Parameter efficient fine-tuning (PEFT) (Han et al., 2024; He et al., 2022; Wang et al., 2023) has gained popularity due to the large number of frozen parameters in LLMs for generality, while adding only a few trainable parameters per task. Adapter tuning methods (Houlsby et al., 2019; Poth et al., 2023) introduce new modules between layers of LLMs, whereas prompt tuning methods (Liu et al., 2022) train prompt embeddings instead. Additionally, LORA (Hu et al., 2022) improves efficiency and helps mitigate overfitting. Our method for steering semantics via fine-tuning enhances adapter tuning by incorporating a selective layer intervention strat-"}, {"title": "3 Method", "content": "egy, saving computational resources while delivering superior performance (see \u00a7 3.3). Moreover, steering semantics during inference presents an even more efficient approach. Contrastive decoding (Li et al., 2023; Sennrich et al., 2024) guides the generation direction by comparing two output distributions. Wang et al. (2024b) proposed using instruction contrast to mitigate hallucinations. In contrast, our proposed implicit layer contrastive intervention efficiently identifies and steers the semantics of LLMs toward safe directions during inference (see \u00a7 3.4).\n3.1 LLMs Show Similar Pattern to Human Gaze Across Layers\nReading is a crucial ability for humans to process and integrate information, and eye movement studies enable the examination of the cognitive processes involved in reading (Rayner, 1998). Eye movements are monitored using various techniques, including eye-tracking systems that rely on infrared pupil monitoring. In eye-tracking studies, changes in the reader's gaze are recorded in relation to eye movements. Similarly, tokens are processed, and information is integrated through layers in large language models (Chuang et al., 2023). This similarity offers us the opportunity to leverage the extensive data previously collected and analyzed from eye movement studies, not only to infer human reading and information-processing behavior, but now to also understand the behavioral patterns of large language models.\nRecent studies reveal that feed-forward networks (FFNs) function similarly to neural memory networks (Geva et al., 2021), embedding syntactic features, semantic features, and even factual knowledge (Chuang et al., 2023; Zhang et al., 2024). The early exit concept (Elbayad et al., 2020; Schuster et al., 2022) further demonstrates that the hidden states of FFNs can be directly applied to predict words over a vocabulary. At the same time, eye movement measures provide insights into the time required for human reading to process syntax, semantics, and integrate information.\nMotivated by this, we utilize eye movement measures from various datasets (Luke and Christianson, 2018; Hollenstein et al., 2020; Colman et al., 2022) to establish correlations with the hidden states in FFNs across layers of LLMs. Unlike traditional eye movement studies, which focus on differences in eye movement measures under experimental conditions such as natural reading or task-specific reading, we observe changes in correlation values between eye movement measures and hidden states across layers. Our approach, inspired by cognitive theory, aims to better understand the behavioral patterns of LLMs.\nFormally, let $S_j$ denote the $j$-th sentence, which consists of $n_j$ words $w_1, w_2,..., w_{n_j}$. For each word $w_i$ in sentence $S_j$, we have five eye movement measures for each word $w_i$ in sentence $S_j$, denoted as $E_{sfd}, E_{ffd}, E_{gd}, E_{trt}, E_{gpt}$. Each of these represents a single scalar value per word. We also denote the hidden state at layer $l$ of the LLM for word $w_i$ as $H_{l,i} \\in \\mathbb{R}^d$, where $d$ is the dimensionality of the hidden state vectors.\nSince the eye movement measures are scalar values and each hidden state is a high-dimensional vector, we apply Principal Component Analysis (PCA) to reduce the dimensionality of the hidden states $H_{l,i}$ to a scalar that can be aligned with the eye movement measure for each word $w_i$, yielding a one-dimensional representation: $H_{l,i}^{PCA} = PCA(H_{l,i})$. We concatenate the eye movement measures and the reduced hidden states across all words in all sentences. Specifically, for the eye movement measure $k \\in \\{sfd,ffd, gd, trt, gpt\\}$, we concatenate the measures for all words in all sentences to form a vector $E^{(k)}$:\n$E^{(k)} = [E_1^{(k)}, E_2^{(k)},..., E_{n_{total}}^{(k)}] \\in \\mathbb{R}^{n_{total}},$ (1)\nwhere $n_{total} = \\sum_{j=1}^m n_j$ is the total number of words across all sentences. Similarly, for the PCA-reduced hidden states at layer $l$, we concatenate the reduced hidden states:\n$H_l^{PCA} = [H_{l,1}^{PCA}, H_{l,2}^{PCA},..., H_{l,n_{total}}^{PCA}] \\in \\mathbb{R}^{n_{total}}.$ (2)\nFinally, we compute the Pearson correlation $\\rho_{l,k}$ between the hidden states at layer $l$ and the eye movement measure $k$:\n$\\rho_{l,k} = \\frac{\\sum_{i=1}^{n_{total}} (H_{l,i}^{PCA} - \\overline{H_l^{PCA}}) (E_i^{(k)} - \\overline{E^{(k)}})}{\\sqrt{\\sum_{i=1}^{n_{total}} (H_{l,i}^{PCA} - \\overline{H_l^{PCA}})^2 \\sum_{i=1}^{n_{total}} (E_i^{(k)} - \\overline{E^{(k)}})^2}}.$ (3)\nwhere $\\overline{H_l^{PCA}}$ and $\\overline{E^{(k)}}$ are the mean values of the reduced hidden states and the eye movement measure $k$, respectively."}, {"title": "3.2 Heuristic Steering Layer Selection", "content": "A better understanding of LLM mechanisms will help in precisely and efficiently controlling their behaviors, particularly for semantic steering. We argue that the predominant parameter-efficient fine-tuning (PEFT) methods (Han et al., 2024), which by default intervene in the last layer or across all layers, are not optimal\u00b9. Instead, we propose an efficient heuristic steering layer selection strategy for intervention, based on our cognition-inspired interpretability analysis detailed in Section 3.1.\nFor semantic steering in LLMs, the layers in the middle bucket are the most suitable candidates for intervention. These layers handle further token processing, information integration, and preliminary reasoning. Additionally, the residual connections (He et al., 2016) in transformer layers allow the semantic intervention to flow and evolve gradually, avoiding abrupt changes in the final prediction (Chuang et al., 2023).\nFrom a task-oriented perspective, we apply PEFT methods, such as Adapters (Poth et al., 2023), or inference-only methods to the candidate layers in the middle bucket, using a small portion of data, like the validation set, to search for and select the best-performing layer that suits the task scenario. Formally, given $M$ as the best layer for intervention, $J$ represents a set of candidate layers in the middle bucket $(\\frac{N}{3} \\leq M < \\frac{2N}{3})$. D denotes the"}, {"title": "3.3 Layer Intervention via Fine-tuning", "content": "Adapter methods are among the most popular and widely used PEFT approaches due to their efficiency, requiring only a small set of new parameters for task-specific fine-tuning. Let the parameters of an LLM consist of a set of pre-trained, frozen parameters $\\phi(\\cdot)$ and a set of newly introduced parameters in the adapter block $\\varphi(\\cdot)$. Our layer intervention via fine-tuning to steer semantics in LLM and predicts the next token as follows:\n$y(x_t) = softmax(\\sigma( W_{logit} (FFN_{\\phi}(x_t) | FFN_{\\varphi}(x_t), y_{<t}))).$ (5)\nHere, $FFN_{\\phi}(x_t)$ represents the hidden state of the FFN in the final layer with the frozen parameters $\\phi(\\cdot)$, used for token prediction over a vocabulary. $M$ denotes the best layer for semantic intervention, determined by Equation 4. $FFN_{\\varphi}(x_t)$ indicates that the fusion layer in the LLM incorporates new parameters $\\varphi(\\cdot)$ from the adapter block, aligned with the frozen parameters $\\phi(\\cdot)$.\nOur cognitive-inspired selective layer intervention method is an adaptive fine-tuning strategy that identifies the best layer for both effective semantic steering and task performance. Moreover, as our method only operates on a single layer rather than all layers, it significantly reduces computational resources and time, while also avoiding catastrophic forgetting (Luo et al., 2023; Li et al., 2024)."}, {"title": "3.4 Layer Intervention during Inference", "content": "Efficient semantic steering can be achieved via fine-tuning. However, an even more efficient approach is to steer the semantics of LLMs during inference without introducing additional parameters. Motivated by Li et al. (2023); Leong et al. (2023), which contrast outputs from either a less capable model or outputs induced from a negative prompt, we propose an implicit layer contrastive intervention method during inference. First, we fine-tune a contrast model that generates either the desired output"}, {"title": "4 Experiments", "content": "or the output we aim to mitigate. In our case, to mitigate toxic token generation, we fine-tune a toxic LLM as the contrast model. Unlike Li et al. (2023); Leong et al. (2023), which contrast the outputs explicitly in the last layer, our method operates on the contextualized value vectors derived from the weight matrices K, Q, V of the attention modules within LLMs (see Elhage et al. (2021) for mathematical derivation). We perform this operation on the best layer for intervention as described in Equation 4. Formally, our layer intervention during inference finds the semantic steering direction by contrasting the value vectors as follows:\n$\\Delta v_M = v_M' - v_M,$ (6)\nwhere $v_M'$ and $v_M$ are the contextualized value vectors of the contrast LLM and the original LLM at the best layer $M$ for semantic intervention. We then update the value vector in the layer $M$ of the original LLM:\n$v_M' = v_M - \\lambda \\alpha \\cdot \\Delta v_M.$ (7)\nHere, $\\alpha_{norm} = \\frac{1}{1+||\\Delta v_M||^2}$ is a normalization term that adaptively affects the steering effect, and $\\lambda$ is a hyperparameter that further controls the steering strength. Finally, we keep the updated steering direction and renormalize the adapted value vector to ensure its representation is close to the original vector:\n$v_M'' = \\frac{v_M'}{||v_M'||_2} \\cdot ||v_M||_2$ (8)\n4.1 Datasets and Evaluation\nDatasets We consider natural language toxification and detoxification tasks as dual tasks to examine our proposed efficient semantic steering methods through selective layer intervention. To train toxic adapters and contrast models, as described in Sections 3.3 and 3.4, we use the Toxic Comment Classification Challenge Dataset (Jigsaw, 2018), which contains 15,294 annotated toxic comments. We randomly split the dataset into 13,764 comments for fine-tuning and training the contrast models, and 1,530 comments for the validation set, aimed at identifying the best layer for intervention as described in Section 3.2. For evaluation, we use the RealToxicityPrompts (RTP) dataset (Gehman et al., 2020). Following the setup of prior work (Leong et al., 2023), we sample 2,122 toxic prompts to assess the performance of our proposed CogSteer method during fine-tuning and inference.\nEvaluation For each toxic prompt in the RTP dataset, we generate 25 continuations, following the setup in Li et al. (2023); Leong et al. (2023). We then evaluate the toxicity of these continuations using the Perspective API 2, which assigns a toxicity score to each one. This score represents the probability that a continuation is perceived as toxic, with higher scores indicating a greater likelihood of toxicity. Finally, we use the average maximum toxicity as our evaluation metric. This metric involves identifying the highest toxicity score from the 25 continuations for each prompt, and then calculating the average of these maximum scores across all prompts.\n4.2 Models and Baselines\nWe evaluate our efficient semantic steering methods using three sizes of GPT-2 models (12-layer small, 24-layer medium, and 36-layer large) and LLaMa2-7B. We selected the GPT-2 model and LLaMa2-7B as the former represents earlier classical LLMs, while the latter represents modern LLMs with advanced techniques such as ICL and RHLF. For generalizability, we believe our proposed method is LLM-agnostic and can be applied to other LLMs. For the Adapter block in GPT-2 models, we used the adapter implementation (Poth et al., 2023) from HuggingFace, while for LLaMa2-7B we used the LLaMa-Adapter implementation (Zhang et al., 2023). Our experiments focus on the performance of selective layer intervention. Specifically, we compare the toxicity scores and detoxify margins when applying our methods to different layers of the models, with the best intervention layer selected under the same task. Since applying semantic intervention to the last layer and all layers of LLMs is a conventional approach, we use these two methods as baselines for comparison.\n4.3 Analysis on Language Toxification\nWe evaluate our selective layer intervention via fine-tuning using the language toxification task. We generate continuations for each toxic prompt in the RTP dataset and compute and compare the average maximum toxicity. Figure 4 (green bars) shows the toxicity score for inserting a toxic adapter into"}, {"title": "4.4 Analysis on Language Detoxification", "content": "As a dual task, we evaluate our selective layer intervention during inference using the language detoxification task. This task serves as an adversarial task to mitigate the toxic tokens that are amplified by layer intervention methods during fine-tuning. Thus, we observe the detoxify margin in each intervention layer and compare the performance with intervention in the last layer. Figure 4 (blue bars in the upper chart) shows the results of the detoxify margin scores. The best layer $M$ for semantic intervention in the middle bucket is L23 for GPT-2 and L16 for LLaMa2-7B, both outperforming the last layer results by a large margin (+2.9% for GPT and +24% for LLaMa). Notably, the best layer $M$ for LLaMa2-7B in the toxification task is L13, whereas the best layer for detoxification is L16, demonstrating that our heuristic layer selec-"}, {"title": "4.5 Efficiency Analysis", "content": "The efficiency analysis of our proposed cognition-inspired selective layer intervention is presented in Table 1. By selecting the best layer for semantic steering in LLMs, the training time and number of parameters involved in fine-tuning are significantly reduced compared to full-layer intervention. Specifically, our method requires only 40% of the time and 3% of the parameters needed for full-layer settings. Importantly, this reduction in computational cost does not compromise performance; in fact, layer intervention is comparable to, and in some cases even better than, full-layer intervention across all models."}, {"title": "5 Conclusion", "content": "In this paper, we introduce an efficient semantic steering method for LLMs using cognition-inspired selective layer intervention. Our approach is inspired by correlation analysis with eye movement measures, making our method understandable and highly interpretable. Specifically, we select the optimal layer for intervention based on heuristic steering layer selection and then apply it to efficient steering methods during fine-tuning and inference. Experimental results show that CogSteer significantly mitigates the toxicity of LLMs while being efficient in terms of training time and parameters involved. Overall, our proposed method is an important step toward making LLMs more interpretable and contributes to the safe deployment of LLMs."}, {"title": "Limitations", "content": "In this paper, we conduct a thorough analysis of the correlation between the hidden states in the feed-forward network and eye movement measures. However, there is another key component in Transformers: the attention block. We limit our discussion to the FFN blocks because the hidden states in FFNs are more responsible for token prediction. For future work, we will explore the mechanisms of the attention block in greater depth. Additionally, we conclude that the layers in the mature buckets are involved in reasoning based on the task-specific reading dataset. However, factual knowledge also seems to be integrated in this bucket. We will seek further eye movement studies that involve factual information processing to refine our analysis."}, {"title": "Ethics Statement", "content": "We propose an efficient semantic steering method in LLMs through cognition-inspired selective layer intervention to better understand the behavior of LLMs and address safety concerns, thereby enhancing their safety and reliability within the community. Additionally, the eye-tracking data used in this study, derived from the ZuCo 2.0, GeCo, and Provo datasets, are publicly available and adhere to established ethical protocols, promoting transparency and reproducibility in our research. Furthermore, we have made our code publicly accessible, ensuring that researchers and practitioners can easily access and implement our methods."}, {"title": "Qualitative Analysis", "content": "Warning: Some examples have harmful or offensive language."}, {"title": "A Implementation Details", "content": "For training the GPT-2 adapters, we set the learning rate to $5 \\times 10^{-4}$ and train for 5 epochs. For the LLaMA2-7B adapters, we use the default settings provided by LLaMA-Adapter (Zhang et al., 2023). In all models, we apply the implicit layer contrastive intervention approach with $\\alpha = 0.4$. Following previous works (Liu et al., 2021; Leong et al., 2023), the model generates 25 continuations per prompt using nucleus sampling with $p = 0.9$, with each continuation limited to a maximum of 20 tokens."}]}