{"title": "Goal-Conditioned Data Augmentation for Offline Reinforcement Learning", "authors": ["Xingshuai Huang", "Di Wu", "Benoit Boulet"], "abstract": "Offline reinforcement learning (RL) enables policy learning from pre-collected offline datasets, relaxing the need to interact directly with the environment. However, limited by the quality of offline datasets, it generally fails to learn well-qualified policies in suboptimal datasets. To address datasets with insufficient optimal demonstrations, we introduce Goal-cOnditioned Data Augmentation (GODA), a novel goal-conditioned diffusion-based method for augmenting samples with higher quality. Leveraging recent advancements in generative modeling, GODA incorporates a novel return-oriented goal condition with various selection mechanisms. Specifically, we introduce a controllable scaling technique to provide enhanced return-based guidance during data sampling. GODA learns a comprehensive distribution representation of the original offline datasets while generating new data with selectively higher-return goals, thereby maximizing the utility of limited optimal demonstrations. Furthermore, we propose a novel adaptive gated conditioning method for processing noised inputs and conditions, enhancing the capture of goal-oriented guidance. We conduct experiments on the D4RL benchmark and real-world challenges, specifically traffic signal control (TSC) tasks, to demonstrate GODA's effectiveness in enhancing data quality and superior performance compared to state-of-the-art data augmentation methods across various offline RL algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning [1] aims to learn a control policy from trial and error through interacting with the environment. While demonstrating remarkable performance in various domains, this approach typically requires vast amounts of training data collected from these interactions. Such data-intensive requirements become impractical in applications where environmental interactions are costly, risky, or time-consuming, such as robotics [2], [3], autonomous driving [4], [5] and TSC [6], [7]. Offline RL offers a feasible solution to these challenges by enabling policy learning directly from pre-collected historical datasets, thus significantly reducing the need to interact directly with the environment.\nAlthough offline RL makes policy learning less expensive, its performance is highly dependent on the quality of the pre-collected datasets and may suffer from lack of diversity, behavior policy bias, distributional shift, and suboptimal demonstrations [8]. The performance of offline RL tends to decline drastically when trained with suboptimal offline"}, {"title": "II. RELATED WORK", "content": "Offline RL involves learning policies from pre-collected offline datasets comprising trajectory rollouts generated by behavior policies. This approach is promising because it avoids the costs and risks associated with direct interactions with the environment. Conventional offline RL methods aim to alleviate the distributional shift problem, i.e., a significant drop in performance due to deviations between learned policy and the behavior policy used for generating the offline data [17]. To address this issue, various strategies have been employed, including explicit correction [18], such as constraining the policy to a restricted action space [19], and making conservative estimates of the value function [20], [21], with the aim of aligning the behavior policy with the learned policy.\nSome recent studies exploit the strong sequence modeling ability of Transformer models to solve offline RL with trajectory optimization. For instance, Decision Transformer [16] and its variants [22], [23] utilize a GPT model to autoregressive predict actions given the recent subtrajectories composed of historical RTGs, states, and actions. These approaches integrate hindsight return information, i.e., RTG, with sequence modeling, eliminating the necessity for dynamic programming.\nDiffusion models have also been adopted in offline RL given its exceptional capability of multi-modal distribution modeling. Diffuser [24] employs diffusion models for long-horizon planning, effectively bypassing the compounding errors associated with classical model-based RL. Hierarchical Diffuser [25] enhances this approach by introducing a hierarchical structure, specifically a jumpy planning method, to improve planning effectiveness further."}, {"title": "B. Data Augmentation in Offline Reinforcement Learning", "content": "Rather than passively reusing data and concentrating on enhancing learning algorithms, data augmentation proactively generates more diverse data to improve policy optimization. Some model-based RL methods employ learned world models from offline datasets to simulate the environment and iteratively generate synthetic trajectories, facilitating policy optimization [11]. For instance, TATU [11] uses world models to produce synthetic trajectories and truncates those with high accumulated uncertainty. However, model-based RL often suffers from compounding errors in the learned world models."}, {"title": "III. PRELIMINARIES", "content": "In RL, the task environment is generally formulated as a Markov decision process (MDP) \\{S, A,R,P,\\gamma\\} [1]. s \\in S, s' \\in S, a \\in A, r = R(s,a), P(s'|s, a), and \\gamma \\in [0, 1) represent state, next state, action, reward function, state transition, and discount factor, respectively. RL aims to train an agent to interact with the environment and learn a policy from experience. The objective of RL is to maximize the expected discounted cumulative rewards over time:\n$J = E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t}R(s_{t}, a_{t})]$,\nwhere t denotes the timestep in a trajectory. For offline RL, the policy is learned directly from offline datasets pre-"}, {"title": "B. Diffusion Models", "content": "Diffusion models [28], [14], [15], a class of well-known generative modeling methods, aim to learn a comprehensive representation of the data distribution $P_{data}(x)$ with a standard deviation $\\sigma_{data}$ from a given dataset. Diffusion models generally have two primary processes, the forward process, also known as the diffusion process, and the reverse/sampling process.\nThe forward process is characterized by a Markov chain in which the original data distribution $x \\in P_{data}(x)$ is progressively perturbed with a predefined i.i.d. Gaussian noise schedule $\\sigma_{N} = 0 < \\sigma_{N-1} < ... < \\sigma_{0 max}$. Therefore, we can obtain a sequence of noised distributions $p(x^{t}; \\sigma^{2})$ for each nose level $\\sigma^{2}$, where the last noised distribution $p(x^{0}; \\sigma_{max})$ can be seen as pure Guassion noise when $\\sigma_{max} \\gg \\sigma_{data}$.\nFor reverse process, Denoising Diffusion Probabilistic Model (DDPM) [14] models it as a Markov chain that involves denoising an initial noise $p(x^{0}) = N(x^{0}; 0, I)$ to the original"}, {"title": "IV. METHODOLOGY", "content": "In this section, we introduce GODA, a goal-conditioned data augmentation method utilizing generative modeling for augmenting higher-quality synthetic transition data. Our adopted diffusion model learns comprehensive data distribution from the initial offline dataset, subsequently sampling new data towards higher return with controllable selective goal conditions. In this part, we define a representative goal for GODA and introduce different selective mechanisms for goal conditions. To proactively control the sampling direction, a controllable goal scaling factor is introduced. For better integrating goal conditions as guidance within the diffusion model, we further propose a novel adaptive gated conditioning approach that introduces condition-adaptive gate mechanism into long skip connection and residual connection."}, {"title": "A. Return-oriented goal", "content": "Prior diffusion-based work [13] lacks the ability to guide the sampling process in the desired direction. We attempt to introduce a return-oriented goal as a condition of our diffusion model. Inspired by Decision Transformer, we adopt RTG [16], cumulative rewards from the current step till the end, as the explicit goal condition\n$g_{t} = \\sum_{t'=t}^{T} r_{t'}$,\nFor each transition sample represented as a tuple (s, a, s',r) within a trajectory, RTG quantifies the expected future rewards for the current behavior, effectively serving as a goal. In other words, a higher RTG at a specific timestep typically"}, {"title": "B. Selective goal conditions", "content": "During dataset preprocessing, we first organize offline samples into trajectories, compute the RTG for each, and append timesteps to every sample. To fully leverage well-performing samples and augment samples with higher returns, we propose three distinct condition selection mechanisms: return-prior, RTG-prior, and random goal conditions.\n(1) Return-prior goal condition. In this approach, we rank all trajectories based on their return values and select the top n trajectories. During the sampling process of the diffusion model, the RTG and timestep pairs $(\\hat{g}_{t},t)$ from these top n trajectories are selected as the sampling goal conditions. This method filters high-return trajectories from the initial offline datasets and reuses them to sample more well-optimized transitions.\n(2) RTG-prior goal condition. We group RTGs by their associated timesteps and then sort them to select the top n RTGs along with their corresponding timesteps as goal conditions. This approach selectively reuses high-RTG transitions for data augmentation, focusing on transitions that are most likely to yield higher returns.\n(3) Random goal condition. We randomly select m RTG and timestep pairs $(\\hat{g}_{t}, t)$ as sampling goal conditions for each batch of samples. This increases the diversity of the augmented data while paying less attention to the optimal trajectories for improving performance."}, {"title": "C. Controllable goal scaling", "content": "Selective goal conditions offer high-return guidance during the sampling process but are limited in generating data with returns or quality beyond the initial offline datasets. To overcome this limitation, we introduce a controllable goal scaling factor,"}, {"title": "D. Adaptive Gated Conditioning", "content": "To better capture goal guidance and seamlessly integrate conditions into the diffusion model, we propose a novel adaptive gated conditioning approach, as shown in Figure 2. This structure significantly enhances the ability to guide the diffusion and sampling processes using goal conditions. The conditional inputs include both the noise level condition and goal condition, which are embedded separately, then element-wise added, and fed into the neural network. The noise input is processed with several gated residual multi-layer perception (MLP) blocks with novel adaptive gated skip connections between shallow and deep layers.\n1) Condition Embedding: The noise level \\sigma for diffusion is encoded using Random Fourier Feature [30] embedding. The RTG is processed with a linear transformation to get a hidden representation. The timestep of each RTG is embedded with Sinusoidal positional embedding [31]. We concatenate the RTG and timestep embeddings to form the goal condition, which is then element-wise added to the noise level embedding and used as the conditional input.\n2) Adaptive Gated Long Skip Connection: As shown in the left part of Figure 2, we adopt a long skip connection similar to U-Net [32] to connect MLP blocks at different levels. To capture different information with varying importance weights, we propose a novel adaptive gated long skip connection structure by adding the previous information with an adaptive gate mechanism based on the given conditions.\n$x_{out} = (1 - w) * x_{skip} + w * x$,\nwhere $x_{skip}$ and x are outputs of a shallower and the previous block, and w denotes a learnable weight calculated by regressing the conditional input with an MLP and a sigmoid layer.\n3) Gated Residual MLP Block: The right part of Figure 2 depicts the structure of each gated residual MLP block. We adopt the widely used adaptive layer normalization (adaLN) method [33] to learn dimension-wise scale \\gamma and shift \\beta based on the conditional information. Besides, we explore a modification of the residual connection [34] and introduce a"}, {"title": "E. Model Implementation", "content": "Given the strong ability of diffusion models to capture complex data distribution and generate high-dimension data, we adopt EDM [15] as our generative model for augmenting offline data. The neural network equipped with adaptive gated conditioning as illustrated in Figure 2 is used as the denoiser function. We train the generative model to approximate the data distribution of the offline dataset and use every transition tuple as a training sample. Given the non-sequence input format, we do not consider complicated structures, e.g., attention mechanisms, but use simple MLPs to process inputs. Algorithm 1 shows the learning process of our GODA method. With goal conditions c and transition data x from the original datasets, the generative model $G_{\\theta}$ with a learnable denoiser neural network $D_{\\theta}$ is trained by\n$\\min_{\\theta} E_{x, c \\sim p_{data}; \\eta \\sim N(0, \\sigma^{2}I)} ||D_{\\theta}(x + \\eta; \\sigma; c) - x||^{2}$.\nAfter obtaining a well-trained conditional diffusion model, we leverage it for sampling data and store data in augmentation dataset D* for further policy training."}, {"title": "V. EXPERIMENTAL SETTINGS", "content": "In this section, we provide a comprehensive overview of the experiments conducted to evaluate the performance of our proposed GODA method."}, {"title": "A. D4RL Tasks and Datasets.", "content": "We adopt three popular Mujoco locomotion tasks from Gym 1, i.e., HalfCheetah, Hopper, and Walker2D, and a navigation task, i.e., Maze2D [37], as well as more complex tasks, specifically the Pen and Door tasks from the Adroit benchmark [38], [37].\n1) Gym: In the case of Gym tasks where dense rewards are available, we employ three distinct data configurations from the D4RL datasets: Random, Medium-Replay, and Medium. To elaborate, Random datasets contain transition data generated by a randomly initialized policy. Medium datasets consist of a million data points gathered using a policy that achieves"}, {"title": "B. Traffic Signal Control Tasks and Datasets.", "content": "To evaluate GODA's applicability to real-world challenges, we further test it on TSC tasks with much fewer training"}, {"title": "VI. AUGMENTED DATA QUALITY MEASUREMENT", "content": "Since our GODA is built upon SynthER, we compare the quality of the datasets augmented by both SynthER and GODA to assess whether the goal conditions incorporated by GODA enhance data quality. We adopt two metrics from SynthER [13]:\n$Dynamics \\ MSE = \\frac{1}{M} \\sum_{i=1}^{M} ||(\\hat{s}_{i+1}, \\hat{r}_{i}) - (s_{i+1}, r_{i})||^{2}$,\n$L2 \\ Distance = ||(s_{i}, a_{i}) - (\\hat{s}_{i}, \\hat{a}_{i})||_{2}$,\nand introduce an Average Reward for evaluating reward distributions of augmented datasets\n$Average \\ Reward = \\frac{1}{M} \\sum_{i=1}^{M} \\hat{r}_{i}$,\nwhere M is the selected number of samples, $s_{i}, a_{i}, \\hat{s}_{i+1}, r_{i}$ denote the samples from augmented datasets, $\\hat{s}_{i+1}$ and $\\hat{r}_{i}$ denote the next state and reward generated by the simulator given states and actions from augmented datasets, and $s$ and $a$ are the state and action from original datasets.\nDynamics MSE measures how well the augmentation models capture the dynamics of the environment by learning patterns from the original datasets and generating data that aligns with those dynamics. L2 Distance assesses the models' exploration capabilities and data diversity by calculating the Euclidean distance between the augmented dataset and the original dataset, reflecting how diverse the generated data is. Average Reward compares the ground-truth reward distributions produced by the simulator given states and actions in datasets augmented by SynthER and GODA.\nThe left part of Figure 5 presents a scatter plot of 10K points sampled from the augmented datasets. Results show that"}, {"title": "VII. EXPERIMENTAL RESULTS", "content": "1) Gym tasks.: Table 1 presents a performance comparison between GODA and other state-of-the-art data augmentation methods trained on the D4RL Gym and Maze2D. We adopt the results of Original and SyntheER from the SynthER paper [13], and those of TATU and DStitch from the DStitch [4] paper. We further conduct experiments for tasks not covered in the literature. As shown in the results, GODA consistently outperforms other methods across most Gym locomotion tasks when evaluated with both IQL and TD3+BC, resulting in higher average scores. Notably, even for tasks using Random datasets, GODA successfully leverages limited high-quality samples to enhance data quality, leading to improved final performance.\n2) Maze2D tasks.: For Maze2D tasks where rewards are sparse, GODA demonstrates significant improvements across"}, {"title": "B. Extended Experiments on Traffic Signal Control", "content": "Table VI presents a comparison of average travel times across different methods for TSC tasks. As shown, while SynthER achieves modest improvements over the performance of models trained on the original datasets, it fails to surpass the original datasets on JN tasks when using the CQL algorithm. In contrast, our GODA consistently outperforms both the original datasets and SynthER across most tasks and all average evaluations. These extended experiments on TSC tasks further validate that GODA is not only applicable to diverse tasks but also capable of improving performance in real-world challenges."}, {"title": "C. Ablation Study", "content": "To validate the effectiveness of GODA's components, we conduct experiments using different configurations.\n1) Ablation on Condition Selection: We test three condition selection mechanisms as described in Section IV-B: return-prior, RTG-prior, and random goal conditions. As shown in"}, {"title": "VIII. CONCLUSION AND DISCUSSION", "content": "This paper proposes a novel goal-conditioned data augmentation method for offline RL, namely GODA, which integrates goal guidance into the data augmentation process. We define the easily obtainable return-to-go signal, along with its corresponding timestep in a trajectory, as the goal condition. To achieve high-return augmentation, we introduce several goal selection mechanisms and a scaling method. Additionally, we propose a novel adaptive gated conditioning structure to better incorporate goal conditions into our diffusion model. We demonstrate that data augmented by GODA shows higher quality than SynthER without goal conditions on different evaluation metrics. Extensive experiments on the D4RL benchmark confirm that GODA enhances the performance of classic offline RL methods when trained on GODA-augmented datasets. Furthermore, we evaluate GODA on real-world traffic signal control tasks. The results demonstrate that GODA is highly applicable to TSC problems, even with very small real-world training datasets, making RL-based methods more practical for real-world applications. In the future, we plan to improve the performance of GODA and explore the application of GODA on more complex real-world challeges."}]}