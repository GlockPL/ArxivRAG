{"title": "How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments", "authors": ["Yusuke Ide", "Yuto Nishida", "Miyu Oba", "Yusuke Sakai", "Justin Vasselli", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "The grammatical knowledge of language models (LMs) is often measured using a benchmark of linguistic minimal pairs, where LMs are presented with a pair of acceptable and unacceptable sentences and required to judge which is acceptable. The existing dominant approach, however, naively calculates and compares the probabilities of paired sentences using LMs. Additionally, large language models (LLMs) have yet to be thoroughly examined in this field. We thus investigate how to make the most of LLMs' grammatical knowledge to comprehensively evaluate it. Through extensive experiments of nine judgment methods in English and Chinese, we demonstrate that a probability readout method, in-template LP, and a prompting-based method, Yes/No probability computing, achieve particularly high performance, surpassing the conventional approach. Our analysis reveals their different strengths, e.g., Yes/No probability computing is robust against token-length bias, suggesting that they harness different aspects of LLMs' grammatical knowledge. Consequently, we recommend using diverse judgment methods to evaluate LLMs comprehensively.", "sections": [{"title": "1 Introduction", "content": "Acceptability judgments have been widely used to measure grammatical knowledge of language models (Lau et al., 2017; Warstadt et al., 2019). Of two major categories of acceptability judgment benchmarks, we focus on the minimal-pair (MP) benchmark, where LMs are tested to see if they will prefer the more acceptable sentence from a pair of minimally different sentences. An example minimal pair extracted from Warstadt et al. (2020) is shown below.\nHere, sentence (a) is acceptable or grammatically correct, while (b) is not, as its underlined verb violates the subject-verb agreement. As such, MP benchmarks can evaluate any LMs including ase models and nstruct models, without fine-tuning for acceptability judgments.\nMeanwhile, recent scaling up of model sizes and training data for LMs has made it possible to solve a wide range of tasks by few-shot or zero-shot prompting, without task-specific finetuning (Brown et al., 2020; Liu et al., 2021), popularizing the term large language models (LLMs). Incorporating learning techniques such as instruction-tuning (Wei et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023) has further improved the alignment of LLM outputs with human preferences and expectations. The LLMs trained by such techniques achieve good performance through prompting. In other words, LLMs show high performance when provided with guidance on what knowledge to elicit.\nIn this light, one can conceive various methods of obtaining acceptability judgments from LLMs, including prompting. However, no previous studies have thoroughly explored them; most of them naively input the given sentences to an (L)LM, calculate their probabilities, and deem the sentence with the higher probability of the pair to be acceptable for the (L)LM. Consequently, it is unclear what methods are effective in obtaining acceptability judgments using LLMs and what their strengths or weaknesses are.\nWe thus investigate how to make the most of LLMs for acceptability judgments, comparing (1) conventional sentence probability readout methods, (2) novel probability readout methods in in-template settings, and (3) prompting-based methods. In in-template probability readout, we insert each target sentence into a template before inputting it to an LLM, instructing the LLM to focus on its grammaticality. As prompting-based methods, we investigate Yes/No probability computing (Yes/No prob comp) as well as simple prompting. In Yes/No prob comp, we compute the normalized probability of \u201cYes\u201d versus \u201cNo\u201d, inspired by UniEval (Zhong et al., 2022), which is shown to be strong in evaluating natural language generation. We conduct rigorous experiments and analysis using six state-of-the-art LLMs and two MP benchmarks (one for English and one for Chinese) to demonstrate the following key findings.\n1. An in-template probability readout method, in-template LP, and Yes/No prob comp show top performance, surpassing the conventional methods.\n2. In-template LP and Yes/No prob comp have different strengths; for example, Yes/No prob comp is robust against token-length bias. This indicates that they harness different aspects of LLMs' grammatical knowledge, helping comprehensive evaluation of LLMs.\n3. Ensembling the two methods further improves the accuracy, revealing their complementary capabilities. The highest score by Mix-P3 with Qwen2 is 1.6 percentage points higher than humans on the English benchmark.\n4. Even with the top two methods, all the LLMs have trouble making correct judgments where the unacceptable sentence can be obtained by shuffling the words in the acceptable one.\nIn conclusion, we recommend employing diverse judgment methods instead of relying on conventional sentence probability readout methods."}, {"title": "2 Related Work", "content": "2.1 Acceptability Judgments\nBenchmarks of acceptability judgments have two categories, one of which is a single-sentence binary classification as seen in CoLA, a dataset composed of sentences each tagged as acceptable or unacceptable (Warstadt et al., 2019). CoLA was incorporated into the natural language understanding benchmark GLUE (Wang et al., 2018) and has been widely used to evaluate models. However, single-sentence benchmarks are limited in their ability to measure LMs' grammatical knowledge directly because they require training a supervised classifier before the evaluation. This makes it difficult to distinguish between the knowledge of the model itself and what is learned through training the classifier (Warstadt et al., 2020).\nIn contrast, MP benchmarks do not need task-specific training as they present minimally different pairs, asking which is acceptable. As another advantage of the MP benchmark, the minimal pairs are automatically generated in a controlled manner, which provides a sufficient amount of quality data for model evaluation (Linzen et al., 2016). In conventional experiments using an MP benchmark, judgments are made based on sentence probabilities. Models are evaluated by whether they assign a higher probability to the acceptable sentence in each minimal pair. This method, which we call sentence probability readout, has been dominantly employed for MP acceptability judgments across languages (Marvin and Linzen, 2018; Warstadt et al., 2020; Mueller et al., 2020; Xiang et al., 2021; Someya and Oseki, 2023, inter alia).\nAs an exception in such studies, Hu and Levy (2023) compared the sentence probability readout and prompting. They conducted experiments using LLMs and an MP benchmark, which is composed of subsets of BLiMP (Warstadt et al., 2020) and SyntaxGym (Gauthier et al., 2020). They showed that sentence probability readout generally outperforms prompting. However, this study is limited in that their readout method remained conventional sentence probability readout and that instruction-tuned models were not investigated.\nAnother line of work has revealed that the token length influences the performance of sentence probability readout; normalized measures such as PenLP (Wu et al., 2016) have been shown to mitigate some of this bias (Lau et al., 2020), but they do not eliminate it (Ueda et al., 2024)."}, {"title": "3 Methods", "content": "We compare three different groups of methods to extract acceptability judgments from the LLMs.\n3.1 Sentence Probability Readout\nIn sentence probability readout, we input each sentence of a given pair into a model to obtain the probabilities assigned to each token. The probabilities are then used to compute a probability score for each sentence, and the sentence given the higher score is predicted to be acceptable.\nWe experiment with three measures to compute the probability scores: LP, MeanLP, and PenLP. LP is the unnormalized log probability of the sentence\n$\\text{LP}(s) = \\log P(s).$ (1)\nBecause LP tends to get smaller as the sentence gets longer (Ueda et al., 2024), we also compute two normalized measures, MeanLP and PenLP (Lau et al., 2020; Wu et al., 2016),\n$\\text{MeanLP}(s) = \\frac{\\log P(s)}{|s|}$ (2)\n$\\text{PenLP}(s) = \\frac{\\log P(s)}{((5+ |s|)/(5+1))^\\alpha}$ (3)\nwhere $s$ is the input sequence of tokens and $P(s)$ is the probability assigned to $s$ by the model. $\\alpha$ is a hyperparameter to scale the token-length; we set $\\alpha = 0.8$ following Lau et al. (2020); Ueda et al. (2024). We hereafter refer to the three judgment methods simply by the name of the corresponding measures: LP, MeanLP, PenLP.\n3.2 In-template Probability Readout\nIn-template probability readout follows the same steps of computing and comparing probabilities as sentence probability readout. Its input string, however, is built by embedding the sentences in a template designed to draw focus to their grammaticality. The input has two types: in-template single and in-template comparative. For each type, we prepare five templates per language because the performance can vary due to minor differences in expressions within prompts (Zheng et al., 2023). The templates were created based on those of Flan (Wei et al., 2022). For Chinese experiments, we use translation of English templates.\nIn-template single In-template single templates have one placeholder where the target sentence is inserted. The performance of in-template single inputs depends on the normalization, like the sentence probability readout. We thus apply each of the three measures explained above to the method, dubbing the corresponding methods in-template LP, in-template MeanLP, and in-template PenLP, respectively. The final measure also depends on whether we let $s$ the whole input string or the target sentence only. We report the result of the former because it performed better in our preliminary experiments.\nIn-template comparative In-template comparative inputs are built by filling two placeholders; we insert the target sentence into the first one and the other sentence of the minimal pair into the second. Note that the other sentence is supplementary, and the main aim here is to measure the acceptability of the target sentence.\nMeanwhile, in-template comparative does not need normalization because the token length of the whole input string is constant no matter which of the paired sentences enter the first placeholder. We thus only calculate LP for the in-template comparative input, referring to this method as in-template comparative LP.\n3.3 Prompting-based Methods\nIn prompting-based methods, we provide the models with prompts that include a question. Specifically, we examine A/B prompting and Yes/No prob comp. In both methods, we prepare a system message and a user message. The system message describes the task to be solved, which has been shown to enhance the performance (Peng et al., 2023). The user message includes the main question and has five versions per language for each method. Each user message is built by inserting one or two sentences into a template, as we do for in-template probability readout. When prompting a base model, we concatenate the two messages and append the string \\nAnswer: at the end. When prompting an instruct model, we apply chat templates to maximize the performance. As a result, actual inputs into the model include control tokens like <|begin_of_text|>.\nA/B prompting A/B prompting inputs a prompt containing the paired sentences to the models and asks which sentence is acceptable. The user message contains one acceptable and one unacceptable sentence. Their order (which sentence goes to A or B) is randomized to eliminate the potential bias from the order (Pezeshkpour and Hruschka, 2023). We perform constrained decoding by outlines (Willard and Louf, 2023) to ensure that the model outputs either A or B. We turn off sampling in decoding.\nYes/No probability computing In Yes/No prob comp, we compute the score of each sentence as the normalized probability of \u201cYes\u201d versus \u201cNo\u201d given a prompt asking its acceptability. An example prompt is shown. We predict the sentence that resulted in a higher \u201cYes\u201d probability to be acceptable. This method is inspired by UniEval (Zhong et al., 2022), which shows strong performance in evaluating natural language generation tasks. We formulate the probability given a sentence $s$ as follows,\n$P(\\text{``Yes''} |s) = \\frac{P_{\\text{LLM}}(\\text{``Yes''} |s)}{P_{\\text{LLM}}(\\text{``Yes''} |s) + P_{\\text{LLM}}(\\text{``No''}|s)}$ (4)\nwhere $P_{\\text{LLM}}()$ is the probability of a token assigned by the model. For Chinese, we substitute \u201c\u662f\u201d and \u201c\u5426\u201d for \u201cYes\u201d and \u201cNo\u201d, respectively. In all our experiments, no tokenizers segment these words into subwords."}, {"title": "4 Experimental Setup", "content": "Models We use six LLMs, among which Llama-3-70B, Mixtral-8x7B-v0.1, and Qwen2-57B-A14B are base models, while Llama-3-70B-Instruct, Mixtral-8x7B-Instruct-v0.1, and Qwen2-57B-A14B-Instruct are instruct models based on the pre-trained counterparts. We hereafter abbreviate these models, e.g., to Llama-3, omitting the model sizes and minor versions. Post-training for the three instruct models includes supervised fine-tuning on an instruction dataset, i.e., instruction-tuning, and aforementioned DPO (Meta, 2024; Jiang et al., 2024; Team, 2024). They are models publicly available on Hugging Face Hub. All six models are used for English experiments, while Chinese experiments examine only Qwen2 and Qwen2-Instruct, which are trained on Chinese texts. On inference, we perform 4-bit quantization using bitsandbytes to compress the models."}, {"title": "5 Results", "content": "Table 3 summarizes the results. The statistics of the in-template probability readout methods and prompting methods are the average of the five scores by the five versions of templates. They show that Yes/No prob comp and in-template LP are particularly strong in both languages. On BLiMP, Yes/No prob comp achieves the highest mean accuracy for five out of six models. The mean accuracies of Llama-3-Instruct and Qwen2 exceed that of humans (the majority vote of 20 crowd workers) reported in Warstadt et al. (2020), 88.6%. In-template LP scores in the top two methods for accuracy on all but one model. The results on CLIMP find similar conclusions; the top two methods are in-template LP and Yes/No prompting.\nSentence readout methods, LP, MeanLP, and PenLP, which have been dominant in previous studies, underperformed in-template LP for all settings. This indicates that including some guidance about the task in the input to LLMs improves acceptability judgment performance.\nMethods giving two sentences to the model, i.e., A/B prompting and in-template comparative LP, also underperformed Yes/No prob comp and in-template LP, respectively, though giving multiple choices is a common approach to harness LLMs\u2019 knowledge for a classification task as seen in studies such as Hendrycks et al. (2021)."}, {"title": "6 Analysis", "content": "Given the excellence of Yes/No prob comp and in-template LP, this section provides further analysis to reveal their strengths and weaknesses.\nYes/No prob comp is robust against token-length bias. Figure 2 illustrates the correlations between the token-length difference and the accuracy. The token-length difference is $|s_{\\text{acceptable}}| - |s_{\\text{unacceptable}}|$ where $s_x$ denotes the token sequence of either sentence. A level line denotes that the token-length difference does not affect the method. Across the models, the following trends are observed. (1) The token-length difference biases the readout methods. The accuracy of in-template LP decreases as the difference grows because the acceptable sentence is less likely to be given a high probability. In-template PenLP suffers a reversed tendency; due to normalization, it becomes weaker as the unacceptable sentence gets longer than the acceptable one. (2) Yes/No prob comp is relatively robust against the bias. Its accuracy does not drop as much as that of the other methods, even when the token lengths differ by a large margin.\nThese observations are quantitatively supported by the correlation coefficient between the token-length difference and the dichotomous variable that gets 1 for a successful prediction and 0 for a failure. Table 4 shows the average coefficients of Yes/No prob comp are much closer to zero than those of readout methods on both benchmarks, demonstrating its robustness against the token-length bias. This, in turn, indicates that the readout methods need better normalization techniques.\nIn-template readout and Yes/No prob comp excel in different phenomena. Figure 3 illustrates the accuracy of in-template LP, Yes/No prob comp, and the humans by linguistic phenomenon; the scores of humans are from Warstadt et al. (2020) and Xiang et al. (2021). For in-template LP and Yes/No prob comp, the result of the best-performing template is shown. Here we find that the two methods have different strengths. On BLiMP, Yes/No prob comp excels at phenomena such as Subject-verb agreement (S-v agr.) and Binding for most (at least five out of six) models. In contrast, in-template LP is superior in Ellipsis and Quantifiers for most models. On CLIMP, Yes/No prob comp is good at Coverb and in-template LP at NP head finality (NP head). This indicates that each method harnesses different aspects of the models' grammatical knowledge.\nGiven the aforementioned token-length bias, one hypothesis to explain this difference would be that Yes/No prob comp is stronger in phenomena with large token-length differences. Our analysis on BLIMP, however, does not support this. See Appendix C.3 for the details.\nMeanwhile, some phenomena are challenging for both methods. As Figure 3 shows, on BLiMP, the two methods underperform humans for all models in Island effects and Quantifiers, which were shown to be challenging also by Warstadt et al. (2020). On CLIMP, our methods struggle with phenomena such as Binding and Passive, lagging far behind human performance.\nVoting ensembles of the top two methods further improve the performance. Given the different strengths of in-template LP and Yes/No prob comp, we ensemble these methods to see if they can complement each other to achieve higher accuracy.\nNow we have 10 sets of predictions by the two methods, as each has five templates. To compare ensembling single-method predictions and ensembling multi-method predictions on equal terms, we sample five without replacement from the 10 and perform majority voting by the five. We prepare the following four settings, which differ in the balance between the two methods: P-only, Mix-P3, Mix-L3, and L-only. P-only and L-only are ensembles of predictions by Yes/No prob comp only and in-template LP only, respectively. Mix-P3 and Mix-L3 use three predictions from Yes/No prob comp and in-template LP, respectively, with two predictions from the other method. We report the mean accuracy of 10 trials for these settings because the result is non-deterministic due to sampling.\nTable 5 demonstrates that ensembles of the two methods, either Mix-P3 or Mix-L3, yield the best results across models, surpassing the oracle (max) accuracies of methods without ensembling, except for Mixtral-Instruct. The highest score by Mix-P3 with Qwen2 is 1.6 points higher than humans (described in Section 5). This indicates that the two methods have complementary capabilities.\nAttractors in a relative clause lower the performance. Attractors refer to material intervening agreement dependencies, and their effects on acceptability judgments have been studied. Below are examples of different attractor types in S-v agr., from Warstadt et al. (2020); (a) contains no attractor, (b) has an attractor as a relational noun, and (c) has an attractor in a relative clause.\nFigure 4 shows both top methods suffer the same issue across models. The accuracy averaged over methods and models drops from 94.5% for the agreement with no attractors to 90.4% for the agreement with attractors in a relative clause. In contrast, attractors as relational nouns do not necessarily lower the performance.\nWord-shuffling paradigms are challenging. BLiMP's 67 paradigms can be divided into two categories based on whether minimal-pair sentences of the paradigm have the same bag of words when the cases are ignored. We call the paradigms where this is true word-shuffling paradigms. Following is an example pair from a word-shuffling paradigm, existential_there_quantifiers_2. Figure 5 shows the accuracy by paradigm, paradigm type-word-shuffling or not, method, and model, demonstrating that the word-shuffling paradigms have much lower accuracy than other phenomena across methods and models. The accuracy of word-shuffling paradigms averaged over models and methods is 71.6% compared to 87.9% of other paradigms. The paradigm marking the lowest accuracy is aforementioned existential_there_quantifiers_2, whose accuracy is only 39.9% on average. Note that such a large difference is not observed for humans according to the data by Warstadt et al. (2020); humans' accuracy on word-shuffling paradigms and other paradigms are, on average, 83.1% and 89.7%, respectively. This suggests that word-shuffling paradigms remain a challenge for the current LLMs, as they have trouble recognizing word shuffling that corrupts grammar even with our best-performing methods."}, {"title": "7 Conclusion", "content": "To investigate how best to measure LLMs' grammatical knowledge, we compared nine acceptability judgment methods across six LLMs and two languages. We found that in-template LP and Yes/No prob comp consistently outperform conventional sentence probability readout methods. This indicates that sentence probability readout methods are suboptimal and should be replaced in an evaluation of LLMs. Meanwhile, the performant two methods excel in different phenomena, suggesting they harness different aspects of LLMs' grammatical knowledge. We thus recommend using diverse judgment methods for a more comprehensive and appropriate evaluation of LLMs."}, {"title": "8 Limitations", "content": "One of this paper's key findings is that in-template LP and Yes/No prob comp excel in different linguistic phenomena. To investigate the reasons for the differences, we examined hypotheses that Yes/No prob comp is stronger in phenomena where the acceptable sentence is, on average, longer than the unacceptable one (See Appendix C.3). Yet the hypotheses were not supported, leaving the cause of their different strengths an open question.\nThroughout the paper, we focused on experiments in the zero-shot setting, aligning the conditions with conventional probability readout methods. It is notable that some methods nonetheless achieved accuracies surpassing humans. However, providing few-shot examples in in-template LP and Yes/No prob comp might increase accuracy even further, which is worth investigating in future work."}, {"title": "A Benchmarks", "content": "A.1 Linguistic Phenomena"}, {"title": "B Experiments", "content": "B.1 Computational Budgets\nFor each method or combination of methods and templates, we used a single NVIDIA A6000 GPU with 48GB RAM. The total GPU hours are estimated to be about 126 hours and 7 hours for the BLIMP and CLiMP experiments, respectively."}, {"title": "C Results and Analysis", "content": "C.1 Max Accuracy\nC.2 Why A/B prompting does not perform well"}, {"title": "C.3 Is Yes/No prob comp strong where the acceptable sentence is longer than the unacceptable?", "content": ""}]}