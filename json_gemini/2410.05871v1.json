{"title": "A SECOND-ORDER-LIKE OPTIMIZER WITH ADAPTIVE\nGRADIENT SCALING FOR DEEP LEARNING", "authors": ["J\u00e9r\u00f4me Bolte", "Ryan Boustany", "Edouard Pauwels", "Andrei Purica"], "abstract": "In this empirical article, we introduce INNAprop, an optimization algorithm that\ncombines the INNA method with the RMSprop adaptive gradient scaling. It lever-\nages second-order information and rescaling while keeping the memory require-\nments of standard DL methods as AdamW or SGD with momentum. After having\nrecalled our geometrical motivations, we provide quite extensive experiments. On\nimage classification (CIFAR-10, ImageNet) and language modeling (GPT-2), IN-\nNAprop consistently matches or outperforms AdamW both in training speed and\naccuracy, with minimal hyperparameter tuning in large-scale settings. Our code is\npublicly available at https://github.com/innaprop/innaprop.", "sections": [{"title": "INTRODUCTION", "content": "As deep learning models grow in size, massive computational resources are needed for training,\nrepresenting significant challenges in terms of financial costs, energy consumption, and processing\ntime (Susnjak et al., 2024; Varoquaux et al., 2024). According to the UN's Environment Programme\nTraining, the Big Tech sector produced between two and three percent of the world's carbon emissions\nin 2021; some estimations for the year 2023 go beyond 4%, see the latest Stand.earth reports, and also\n(Schwartz et al., 2020; Strubell et al., 2020; Patterson et al., 2021) for related issues. For instance,\ntraining GPT-3 is estimated to require 1,287 megawatt-hours (MWh) of electricity, equivalent to the\nannual usage of over 100 U.S. households (Anthony et al., 2020; Patterson et al., 2021). Similarly,\nthe financial cost of specialized hardware and cloud computing is extremely high. OpenAI claimed\nthat the training cost for GPT-4 (Achiam et al., 2023) exceeded 100 million dollars. The PaLM\nmodel developed by Google AI was trained for two months using 6144 TPUs \u00b9 for 10 million dollars\n(Chowdhery et al., 2023). All this implies a need for faster and more cost-efficient optimization\nalgorithms. It also suggests that early stopping (Prechelt, 2002; Bai et al., 2021) in the training phase\nis a desirable feature whenever possible.\nWe focus in this work on computational efficiency during the training phase and consider the problem\nof unconstrained minimization of a loss function J: RP \u2192 R, as follows\n\nmin J(0).\nDERP\n\nContinuous dynamical systems as optimization models. To achieve higher efficiency, it is\nnecessary to deeply understand how algorithms work and how they relate to each other. A useful way\nto do this is by interpreting optimization algorithms as discrete versions of continuous dynamical\nsystems (Ljung, 1977), further developed in (Harold et al., 1997; Bena\u00efm, 2006; Borkar & Borkar,\n2008; Attouch et al., 2016; Aujol et al., 2019; Castera et al., 2024). In deep learning, this approach is\nalso quite fruitful; it has, in particular, been used to provide convergence proofs or further geometric\ninsights (Davis et al., 2020; Bolte & Pauwels, 2020; Barakat & Bianchi, 2021; Chen et al., 2023a).\nIn the spirit of Castera et al. (2021; 2024), we consider the following continuous-time dynamical\nsystem introduced in Alvarez et al. (2002) and referred to as DIN (standing for \"dynamical inertial"}, {"title": null, "content": "\n\n(t) + \u03b1\u03b8(t) + \u03b2 \u22072J (0(t))0(t) + \u221aJ(0(t)) = 0,\nt \u2265 0,\n\nInertial term Friction term\nNewtonian effects\nGravity effect\n\nwhere t is the time, J: RP \u2192 R is a loss function to be minimized (e.g., empirical loss in DL\napplications) as in Equation (1), assumed C2 with gradient VI and Hessian \u22072J. A key aspect of\nEquation (2) that places it between first- and second-order optimization is that a change of variables\nallows to describe it using only the gradient VI, since \u22072J(0(t))0(t) = +\u2207I(0(t)) (see Section\n2.2 for details). This greatly reduces computational costs, as it can be discretized as a difference of\ngradients which does not require Hessian vector product, making it possible to design more practical\nalgorithms, as shown in Chen & Luo (2019); Castera et al. (2021); Attouch et al. (2022).\nWe recover the continuous-time heavy ball system by assuming a > 0, and removing the geometrical\n\"damping", "system": "n\n(t) +\n1\nG(t, 0(t)) + \u20ac\nVJ(0(t)) = 0, t \u2265 0,\n\nwhere \u20ac > 0, G(t,0(t)) \u2208 R\u00ba represents accumulated information. The scalar addition, square\nroot, and division are understood coordinatewise and denotes the coordinate-wise product for\nvectors in IRP. In AdaGrad or RMSprop, G(t, 0(t)) is defined as an accumulation of squared gradient\ncoordinates of the form:\n\nG(t, 0(t)) := \\int_0^t \u25bdJ(\u03b8(\u03c4))\u00b2 d\u03bc\u03b5(\u03c4),\n\nfor different choices of \u00b5t (uniform for AdaGrad and moving average for RMSprop). Both approaches\nscale the gradient based on accumulated information on past gradient magnitudes, improving per-\nformance, particularly in settings with sparse or noisy gradients (Duchi et al., 2011; Tieleman et al.,\n2012).\nOur approach. We combine the \"dynamical inertial Newton", "follows": "n\nd\ndt\n(t) + \u03b1\u03b8(t) + \u03b2RMSprop(J(0(t))) + RMSprop(J(0(t))) = 0,\nt\u2265 0\n\nwhere RMSprop(J(0(t))) =\n1\n\u221aG(t,0(t)) + \u0454\n\u25bdJ(0(t))\n\nwith G of the form (4) with an adequate time-weight distribution \u03bc\u03b5 corresponding to the RMSProp\nscaling. A discretization of this continuous time system, combined with careful memory management,\nresults in our new optimizer INNAprop, see Section 2.1.\nRelation with existing work. To improve the efficiency of stochastic gradient descent (SGD), two\nprimary strategies are used: leverage local geometry for having clever directions and incorporate mo-\nmentum to accelerate convergence. These approaches include accelerated methods (e.g., Nesterov's\nacceleration (Nesterov, 1983; Dozat, 2016), momentum SGD (Polyak, 1964; Qian, 1999; Sutskever\net al., 2013), and adaptive methods (e.g., Adagrad (Duchi et al., 2011), RMSProp (Tieleman et al.,\n2012)), which adjust learning rates per parameter."}, {"title": null, "content": "Adam remains the dominant optimizer in deep learning. It comes under numerous variants proposed\nto improve its performance or to adapt it to specific cases (Dozat, 2016; Shazeer & Stern, 2018;\nReddi et al., 2019; Loshchilov & Hutter, 2017; Zhuang et al., 2020). Adafactor (Shazeer & Stern,\n2018) improves memory efficiency, Lamb (You et al., 2019) adds layerwise normalization, and Lion\n(Chen et al., 2023b) uses sign-based momentum updates. AdEMAMix (Pagliardini et al., 2024)\ncombines two EMAs, while Defazio et al. (Defazio et al., 2024) introduced a schedule-free method\nincorporating Polyak-Ruppert averaging with momentum.\nOne of the motivations of our work is the introduction of second-order properties in the dynamics\nakin to Newton's method. Second-order optimizers are computationally expensive due to frequent\nHessian computations (Gupta et al., 2018; Martens & Grosse, 2015) and their adaptation to large scale\nlearning settings require specific developments (Jahani et al., 2021; Qian et al., 2021). For example,\nthe Sophia optimizer (Liu et al., 2023), designed for large language models, uses a Hessian-based\npre-conditioner to penalize high-curvature directions. In this work, we draw inspiration from the\nINNA optimizer (Castera et al., 2021), based on the continuous time dynamics introduced by (Alvarez\net al., 2002), which combines gradient descent with a Newtonian mechanism for first-order stochastic\napproximations.\nOur proposed method, INNAProp, integrates the algorithm INNA, which features a Newtonian\neffect with cheap computational cost, with the gradient scaling mechanism of RMSprop. This\nframework preserves the efficiency of second-order methods and the adaptive features of RMSprop\nwhile significantly reducing the computational overhead caused by Hessian evaluation. Specific\nhyperparameter choices for our method allow us to recover several existing optimizers as special\ncases.\nContributions. They can be summarized as follows:\n\u2022 We introduce INNAprop, a new optimization algorithm that combines the Dynamical Inertial\nNewton (DIN) method with RMSprop's adaptive gradient scaling, efficiently using second-order\ninformation for large-scale machine learning tasks. We obtain a second-order optimizer with\ncomputational requirements similar to first-order methods like AdamW, making it suitable for\ndeep learning (see Section 2.2 and Appendix B).\n\u2022 We provide a continuous-time explanation of INNAprop, connecting it to second-order ordinary\ndifferential equations (see Section 2 and Equation (5)). We discuss many natural possible\ndiscretizations and show that INNAprop is empirically the most efficient. Let us highlight a key\nfeature of our method: it incorporates second-order terms in space without relying on Hessian\ncomputations or inversions of linear systems which are both prohibitive in deep learning.\n\u2022 We show through extensive experiments that INNAprop matches or outperforms AdamW in\nboth training speed and final accuracy on benchmarks such as image classification (CIFAR-10,\nImageNet) and language modeling (GPT-2) (see Section 3).\nWe describe our algorithm and its derivation in Section 2. Hyperparameter tuning recommendations\nand our experimental results are provided in Section 3."}, {"title": "INNAPROP: A SECOND-ORDER METHOD IN SPACE AND TIME BASED ON\nRMSPROP", "content": null}, {"title": "THE ALGORITHM", "content": "Our method is built on the following Algorithm 1, itself derived from a combination of INNA\n(Castera et al., 2021) and RMSprop (Tieleman et al., 2012) (refer to Section 2.2 for more details). The\nfollowing version of the method is the one we used in all experiments. It includes the usual ingredients\nof deep-learning training: mini-batching, decoupled weight-decay, and scheduler procedure. For a\nsimpler, \"non-deep learning", "scheduler\" for step-sizes; it is defined as a custom procedure for\nhandling learning rate sequences for different networks and databases. To provide a full description of\nour algorithm, we provide detailed explanations of the scheduler procedures used in our experiments\n(Section 3) in Appendix D, along with the corresponding benchmarks.\"\n    },\n    {\n      \"title\"": "DERIVATION OF THE ALGORITHM"}, {"content": "There are several ways to combine RMSprop and INNA, or DIN its second-order form, as there exist\nseveral ways to do so with the heavy ball method and RMSprop. We opted for the approach below\nbecause of its mechanical and geometrical appeal and its numerical success (see Appendix B for\nfurther details). Consider the following dynamical inertial Newton method (Alvarez et al., 2002):\n\n(t) + \u03b1\u03b8(t) + \u03b2J((t)) + \u221aJ(0(t)) = 0, t\u2265 0,\n\nas in Equation (2) and replacing \u22072J(0(t))0(t) by \u2207J(0(t)). We use finite differences with a\nfixed time step y to discretize this system, replacing in particular the gradient derivatives by gradient\ndifferences:\n\nd\ndt\nVI(0(t)) ~\\frac{\u25bdI(0k+1) \u2013 \u2207I(0k)}{7},\n\nwhere \u03b8k, 0k+1 correspond to two successive states around the time t.\nSetting VI(0k) = gk, we obtain\n\n\\frac{0k+1 - 20k + 0k-1}{\u03b32} + \u03b1\\frac{0k - 0k-1}{7} + \u03b2\\frac{9k}{\\sqrt{U_k+1}+\u20ac} + \\frac{9k-1}{\\sqrt{U_k+\u20ac}} + gk-1 = 0.\n\nTo provide our algorithm with an extra second-order geometrical intelligence, we use the proxy of\nRMSprop direction in place of the gradient.\nChoose \u03c3 > 0 and \u20ac > 0, and consider:\n\nUk+1 = \u03c3\u03c5\u03ba + (1 \u2212 \u03c3)g\n\n\\frac{0k+1 - 20k + \u03b8\u03ba-1}{\u03b32} + \u03b1\\frac{\u03b8\u03ba - 0k-1}{7} + \u03b2\\frac{9k}{\\sqrt{U_k+1}+\u20ac} + \\frac{9k-1}{\\sqrt{U_k+\u20ac}} + gk-1 = 0.\n\nAlthough this system has a natural mechanical interpretation, its memory footprint is abnormally\nimportant for this type of algorithm: for one iteration of the system (8)-(9), it culminates at 6 full\ndimension memory slots, namely gk-1, gk, Ok\u22121, \u03b8\u03ba, Uk, and Uk+1 before the evaluation of (9)."}, {"title": null, "content": "Therefore, we proceed to rewrite the algorithm in another system of coordinates. The computations\nand the variable changes are provided in Appendix B. We eventually obtain:\n\nUk+1 = \u03c3\u03c5\u03ba + (1 \u2212 \u03c3)g\n\nWk+1 = \u03c8\u03ba (1-\\frac{\u03b2-\u03b3}{\u03b2})+(\\frac{\u03b3}{\u03b2}-\u03b1) \u03b8\u03ba,\n\n0k+1\n=\n(1+(1-\\frac{\u03b3}{\u03b2}\u03b1))\\\n\u03b8\u03ba\n-\n\u03c8k+1\n-\n\u03b3\u03b2\\\n\u03b2\ngk\n\u221aUk+1+\n\u20ac\n\nwhich only freezes 3 full dimension memory slots corresponding to \u03c5\u03ba, \u03c8\u03ba, \u03b8\u03ba. As a result, the\nmemory footprint is equivalent to that of the Adam optimizer (see Table 2)."}, {"title": "EMPIRICAL EVALUATION OF INNAPROP", "content": "We conduct extensive comparisons of the proposed algorithm and the AdamW optimizer, which\nis dominantly used in image classification (Chen et al., 2018; Zhuang et al., 2020; Touvron et al.,\n2021; Mishchenko & Defazio, 2023) and language modeling tasks (Brown et al., 2020; Hu et al.,\n2021; Liu et al., 2023). Hyperparameter tuning (Sivaprasad et al., 2020) is a crucial issue for this\ncomparison, and we start with this. As a general rule, we strive to choose the hyperparameters that\ngive a strong baseline for AdamW (based on literature or using grid search). Unless stated differently,\nour experiments use the AdamW optimizer 2 with its default settings as defined in widely-used\nlibraries like PyTorch (Paszke et al., 2019), Jax (Bradbury et al., 2018), and TensorFlow (Abadi\net al., 2016): \u03b2\u2081 = 0.9, \u03b22 = 0.999, X = 0.01 and \u0454 = 1e \u2013 8. For INNAprop, unless otherwise\nspecified, the default settings for the RMSprop component align with those of AdamW: \u03c3 = 0.999\nand e le - 8.\nThe INNAprop method and the AdamW optimizer involve different classes of hyperparameters; some\nof them are common to both algorithms, and some are specific.\nOur hyperparameter tuning strategy for both algorithms is summarized in Table 1.\nWe begin this section with the tuning of parameters \u03b1, \u03b2 for INNAprop on CIFAR10 with VGG\nand ResNet architectures and then use these parameters on larger datasets and models. We use as\nmuch as possible the step size scheduler and weight decay settings reported in the literature for the\nAdamW optimizer, which we believe to be well-adjusted and provide adequate references for each\nexperiment. These are used both for AdamW and INNAprop. With this protocol, we only perform\nminimal hyperparameter tuning for INNAprop for larger-scale experiments. This is due to constrained\ncomputational resources. We aim to demonstrate the typical performance of the Algorithm 1, rather\nthan its peak performance with extensive tuning."}, {"title": "TUNING INNAPROP ON CIFAR-10 WITH VGG11 AND RESNET18", "content": "Hyperparameter tuning: We tune (\u03b1, \u03b2) using VGG11 (Simonyan & Zisserman, 2014) and\nResNet18 (He et al., 2016) models trained on CIFAR10 (Krizhevsky & Hinton, 2010), together with\nthe initial learning rate Yo to ensure proper training. We fix a cosine scheduler where Tmax = 200 and\nYmin = 0 (see Appendix D for more details) and we consider two weight decay parameters x = 0 or\nX = 0.01. Our experiment suggests using an initial learning rate % = 10\u22123, which is the baseline\nvalue reported for AdamW in this experiment (see Appendix E). For INNAprop, we optimize only\nthe hyperparameters a and \u03b2, using test accuracy and training loss as the optimization criteria. A grid\nsearch is performed over (\u03b1, \u03b2) \u2208 {0.1, 0.5, 0.9, . . ., 3.5, 4.0} using optuna (Akiba et al., 2019).\nIn Figure 1, we detail the obtained training loss and test accuracy for various (\u03b1, \u03b2) configurations\nover short training durations (20 epochs) and long training durations (200 epochs) for VGG11 with\nweight decay X = 0.01. Our criteria (short and long training duration) are chosen to find parameters\n(\u03b1, \u03b2) that provide a rapid decrease in training loss in the early stages and the best test accuracy for\nlong training duration.\nThese results highlight a tendency for efficient couples; we choose for further experiments the values\n(\u03b1, \u03b2) = (0.1, 0.9) which correspond to aggressive optimization of the training loss for short training\ndurations, and (\u03b1, \u03b2) = (2.0, 2.0) which provides very good results for longer training durations.\nAdditional results for VGG11 and ResNet18 with and without weight decay are in Appendix F.4,\nwhich are qualitatively similar.\nValidation and comparison with AdamW: We confirm our hyperparameter choices (% = 10\u22123,\nX = 0.01) by reproducing the experiment with 8 random seeds and comparing with AdamW using the\nsame settings. Based on hyperparameter tuning, we select two pairs of (\u03b1, \u03b2) with different training\nspeeds. As shown in Figure 2 (and Appendix F for ResNet18), with (\u03b1, \u03b2) = (0.1, 0.9), INNAprop\nimproves training loss and test accuracy rapidly by the 100th epoch, maintaining the highest training"}, {"title": "EXTENSIVE EXPERIMENTS ON LARGE-SCALE VISION MODELS", "content": "In this section, we present experimental results on large-scale vision benchmarks, using the hyperpa-\nrameters selected as described in Section 3.1.\nResnets on ImageNet: We consider the larger scale ImageNet-1k benchmark (Krizhevsky et al.,\n2012). We train a ResNet-18 and a ResNet-50 (He et al., 2016) for 90 epochs with a mini-batch\nof size of 256 as in Chen et al. (2023b); Zhuang et al. (2020). We used the same cosine scheduler\nfor both AdamW and INNAprop with initial learning rate % = 10-3 as reported in Chen et al.\n(2023b); Zhuang et al. (2020); Chen et al. (2018). The weight decay of AdamW is set to X = 0.01\nfor the ResNet18, instead of X = 0.05 reported in Zhuang et al. (2020); Chen et al. (2018) because\nit improved the test accuracy from 67.93 to 69.43. The results of the ResNet18 experiment are\npresented in Figure 11 in Appendix F. The figure shows that our algorithm with (\u03b1, \u03b2) = (0.1, 0.9)\noutperforms AdamW in test accuracy (70.12 vs 69.43), though the training loss decreases faster\ninitially but slows down towards the end of training.\nFor the ResNet50, we kept the value X = 0.1 as reported in Zhuang et al. (2020); Chen et al. (2018).\nFor INNAprop, we tried two weight decay values {0.1, 0.01} and selected x = 0.01 as it resulted\nin a faster decrease in training loss. We report the results in Figure 3, illustrating the advantage\nof INNAprop. As noted in Section 3.1, INNAprop with (\u03b1, \u03b2) = (0.1, 0.9) reduces training loss\nquickly but has lower test accuracy compared to AdamW or INNAprop with (\u03b1, \u03b2) = (2.0, 2.0). For\n(\u03b1, \u03b2) = (2.0, 2.0), the loss decrease is similar to AdamW, with no clear advantage for either method.\nThis obviously suggests developing scheduling strategies for damping parameters (\u03b1, \u03b2). This would\nrequire a much more computation-intensive tuning, far beyond the numerical resources used in the\ncurrent work.\nVision transformer (ViT) on ImageNet: We performed the same experiment with a ViT-B/32\narchitecture over 300 epochs with a mini-batch size of 1024, following Defazio & Mishchenko (2023);\nMishchenko & Defazio (2023). For AdamW, we used a cosine scheduler with a linear warmup (30\nepochs) and the initial learning rate and weight decay from Defazio & Mishchenko (2023). For\nINNAprop, we tested weight decay values of {0.1, 0.01}, selecting X = 0.1 for better test accuracy.\nResults in Figure 3 show the advantage of INNAprop. For faster convergence using INNAprop\n(0.1, 0.9), we recommend a weight decay of X = 0.01 (see Figure 12 in the Appendix).\nIn the ImageNet experiments, we evaluated INNAprop for rapid early training and optimal final test\naccuracy without tuning (\u03b3\u03bf, \u03b1, \u03b2). For ViT-B/32 with x = 0.1, INNAprop achieved lower training"}, {"title": "PRE-TRAINING AND FINE-TUNING GPT2", "content": "In this section, we present experimental results on large-scale language benchmarks, using the\nhyperparameters selected as outlined in Section 3.1.\nTraining GPT-2 from scratch: We train various GPT-2 transformer models from scratch (Radford\net al., 2019) using the nanoGPT repository\u00b3 on the OpenWebText dataset. For all models, gradients\nare clipped to a norm of 1, following Mishchenko & Defazio (2023); Liu et al. (2023); Brown et al.\n(2020). We use AdamW with hyperparameters from the literature (Liu et al., 2023; Brown et al.,\n2020), the standard configuration for LLM pre-training. The reported RMSprop parameter \u1e9e2 = 0.95\nis different from Adam's default (0.999), the weight decay is X = 0.1 and yo depending on the\nnetwork size (see Brown et al. (2020); Liu et al. (2023)). For example, GPT-2 small works with\nan initial learning rate % = 6 \u00d7 10\u22124. For INNAprop, we keep the same values for \u5165 and o as\nAdamW, and use the RMSprop parameter \u03c3 = 0.99 (corresponding to \u1e9e2 for Adam), which provides\nthe best results among values {0.9, 0.95, 0.99} on GPT-2 mini. We use this setting for all our GPT-2\nexperiments with (\u03b1, \u03b2) = (0.1, 0.9) and (\u03b1, \u03b2) = (2.0, 2.0). The results are in Figure 5. INNAprop\nleads to a faster decrease in validation loss during the early stages compared to the baseline for GPT-2\nmodels of Mini (30M), Small (125M), and Medium (355M) sizes. Its performance could be further\nimproved with more thorough tuning of hyperparameters (\u03b1, \u03b2, \u03c3, \u03bb).\nFine-tune GPT-2 with LoRA: Using LoRA (Hu et al., 2021), we fine-tune the same GPT-2 models\non the E2E dataset, consisting of roughly 42000 training 4600 validation, and 4600 test examples\nfrom the restauration domain. We compare AdamW and INNAprop for 5 epochs, as recommended in\nHu et al. (2021). We use for both algorithms the same linear learning rate schedule, the recommended\nmini-batch size, and the RMSprop parameter (\u03b22 = \u03c3 = 0.999); these are listed in Table 11 in Hu\net al. (2021). The results are displayed in Figure 6, where we see the perplexity mean result over\n3 random seeds. INNAprop with (\u03b1, \u03b2) = (0.1, 0.9) consistently achieves lower perplexity loss\ncompared to AdamW across all GPT-2 fine-tuning experiments."}, {"title": "CONCLUSION", "content": "We introduce INNAprop, an optimizer that leverages second-order geometric information while\nmaintaining memory and computational footprints similar to AdamW. Experiments on text modeling\nand image classification show that INNAprop consistently matches or exceeds AdamW's performance.\nIn our approach, we systematically favored AdamW through the choice of recommended hyperparam-\neters (scheduler, learning rates, weight decay). Hyperparameter tuning for friction parameters (\u03b1, \u03b2)\nwas conducted using a grid search on CIFAR-10 (see Figure 15). We suspect that further experiments\nin that direction could greatly improve the efficiency of INNAprop. In particular, a dynamic scheduler\nfor (\u03b1, \u03b2) could be very beneficial, but we were unable to explore this due to resource limitations.\nFor language models, INNAprop with (\u03b1, \u03b2) = (0.1, 0.9) performs consistently well across all\ntraining durations, both for pre-training from scratch and for fine-tuning. In image classification,\nthe same hyperparameter choice accelerates short-term learning, while higher values like (\u03b1, \u03b2) =\n(2.0, 2.0) improve test accuracy during longer training runs. Moreover, (\u03b1, \u03b2) = (2.0, 2.0) proves\neffective for fine-tuning, offering a good balance between convergence speed and final accuracy.\nThese experiments illustrate consistent performances of the proposed method over a diversity of\nbenchmarks, architecture, and model scales, making INNAprop a promising competitor for the\ntraining of large neural networks. Future research will be focused on the design of schedulers for the\nhyperparameters a and \u03b2."}, {"title": "DERIVATION OF INNAPROP FROM DIN", "content": "We consider (9) which was a discretization of (6), namely:\n\nUk+1 = \u03c32Uk + (1 \u2212 02)g\n\n\\frac{0k+1 - 20k + \u03b8\u03ba-1}{\u03b32} + \u03b1\\frac{0k - 0k-1}{7} + \u03b2\\frac{9k}{\\sqrt{U_k+1}+\u20ac} + \\frac{9k-1}{\\sqrt{U_k+\u20ac}} + gk-1 = 0.\n\nThis gives\n\n\\frac{1}{7}(\\frac{\u03b2}{9k+1-0+\u03b29k}-(\\frac{9k}{\\sqrt{U_k+1}+\u20ac}+3\\frac{9k-1}{\\sqrt{U_k+\u20ac}})+ \u03b1\\frac{0k - 0k-1}{7}gk-1-\u03b10\\frac{9k-1}{\\sqrt{U_k+\u20ac}}\n\nand thus\n\n\\frac{1}{\u03b1}(\\frac{9k+1}{9k-0} (06-\\frac{10+\u03b29k}{-1} +\u03b2gk)-\n\nMultiplying by \u03b2, we obtain\n\n\\frac{\u03b2-\u03b1}{9k-0(\\frac{9k}{\\sqrt{U_k+1}+\u20ac}+\u03b2\\frac{9k-1}{\\sqrt{U_k+\u20ac}}),\n\n\u03b2(\\frac{\\sqrt{U_k+\u20ac}} (\u03b2-\u03b1))\u03b8\u03ba-1\\frac{9k}{\\sqrt{U_k+1}+\u20ac} + \u03b22{\\frac{9k-1}{\\sqrt{U_k+\u20ac}}\n\nafter rearranging all terms\n\n(\\) \u03b8\u03ba\n\nSetting k\u22121 \u03b2V"}, {"title": "AN APPROACH \u00c0 LA ADAM", "content": "In this section, we mimic the process for deriving Adam from the heavy ball with a RMSprop proxy,\nsee, e.g., Kingma & Ba (2014); Ruder (2016), by simply replacing the heavy ball by DIN4. We call\nthis optimizer DINAdam.\nFrom (6), we infer the discretization:\n\n\\frac{0k+1 - 20k + \u03b8\u03ba-1}{\u03b32} + \u03b1\\frac{0k+1 - \u03b8\u03ba}{\u0e0a}+\u03b2\\frac{gk-gk-1}{\u0e0a} + gk = 0.\n\nRearranging terms, we have\n\n9k+1+1+\u03b1+ (\u03b8\u03ba \u2013 \u03b8\u03ba\u22121) \u20131+(9k \u2013 gk-1) (\u03b8\u03ba\u22121 \u2013 \u03b8\u03ba) + \u03b1 \u03b2\u03b7+ (\u03b8\u03ba\u22121 \u2013 \u03b8\u03ba) +2 + 3 = 0.\n\nBy introducing the new variable mk = (\u03b8k\u22121 \u2212 \u03b8k)/\u03b7 and setting \u03b7 > 0, we can rewrite equation\n(25) as:\n\n\u03b3+1=110 (\u03b8\u03ba \u2013 \u03b8\u03ba\u22121) +2 + 3 = 0."}, {"title": "SCHEDULER PROCEDURES", "content": "Cosine annealing (Loshchilov & Hutter, 2016). Let Yk represent the learning rate at iteration\nk, Tmax be the maximum number of iterations (or epochs), and min be the minimum learning rate\n(default value is 0). The learning rate Yk at iteration k is given by:\n\nYk\n\nThis scheduler was employed in all image classification experiments except for ViT."}]}