{"title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving", "authors": ["Andreas Kosmas Kakolyris", "Dimosthenis Masouros", "Petros Vavaroutsos", "Sotirios Xydis", "Dimitrios Soudris"], "abstract": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present throttLL'eM, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. throttLL'eM features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, throttLL'eM manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that throttLL'eM achieves up to 43.8% lower energy consumption and an energy efficiency improvement of at least 1.71x under SLOs, when compared to NVIDIA's Triton server.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have surged in popularity over the last couple of years, driven by advancements in deep learning and highlighted by the remarkable success of ChatGPT from OpenAI [3]. This rapid adoption, combined with the reliance of LLMs on high-end GPUs with significant power and energy consumption has raised concerns about their environmental impact. [41], [48]. Recent studies indicate that inference dominates the computational workload, accounting for over 90% of the overall LLM compute cycles within data centers [42], [44]. Although individual inference tasks are short-lived, the energy required for a single response from an LLM like GPT-4 can be significant, reaching \u2248 3Wh [7]. As userbases for LLM services grow into the millions [1], their energy footprint is projected to become substantial, potentially reaching 1050 TWh by 2026 [22], rivaling entire countries.\nMinimizing the energy footprint of LLM inference serving systems is crucial not only for environmental reasons but also from the providers' perspective. Efficient utilization of resources allows data center providers to reduce their operating costs, while also facilitating compliance to power budget constraints, enforced by contracts with utility companies [67]. However, reducing energy consumption often conflicts with the need for high performance. LLM-based services (e.g., ChatGPT) are highly interactive, as users submit their queries (e.g., questions) and await for \u201con-the-fly\u201d responses. This user expectation of low latency is often quantified through Service-Level-Objectives (SLO), e.g., maintaining the 99th percentile of response latency under a specific threshold [43].\nDesigning energy-efficient LLM serving systems that operate under SLOs is challenging due to their inherent unpredictability compared to traditional neural networks. We identify three major barriers that complicate this task:\n1) Autoregressive nature of LLMs: Unlike traditional feed-forward neural networks (e.g., CNNs) with fixed execution steps, LLMs exhibit inherent stochasticity in their inference process. Response generation proceeds iteratively, processing one token (word or sub-word unit) at a time until an End-of-Sequence (EOS) token is generated or a predefined maximum token limit is reached. This dynamic token generation length makes it difficult to precisely pre-allocate computational resources for each inference request.\n2) Dynamic Batch Composition: Techniques like inflight batching [65] leverage the autoregressive nature of LLMs to improve throughput by allowing requests to enter and exit a batch dynamically. However, as shown in Sec. III-A1, different batch sizes introduce significant performance variability, possibly increasing time-between-tokens and end-to-end latency by up to 45%, thus complicating energy optimization under SLOs.\n3) Variable Memory Footprint: LLMs exhibit variable memory footprint due to their autoregressive nature. Within each iteration, memory is dynamically reserved for the growing sequence length and only deallocated once a request is completed. This takes the form of a variably sized Key-Value (KV) cache, which stores the information associated with each token. While techniques like paged attention [28] have mitigated memory fragmentation issues, managing the KV cache remains challenging. Notably, increased KV cache usage can lead to an up to 18.2% performance degradation in the inference process (cf. Sec. III-B).\nManaging the energy consumption of inference serving systems through traditional power-capping [31] and power-oversubscription [27], [41] techniques falls short in the case of LLMs. These solutions often operate in a static manner, leading to high tail latencies under load [68], potentially conflicting with the specified SLOs set by providers [48]. Moreover, techniques designed for energy optimization of traditional neural network inference serving systems [64], such as delayed [14] or coordinated [37] batching, cannot be directly applied to LLMs due to their unpredictability. Finally, race-to-idle strategies [26], [29] are also rendered inefficient since LLM inference serving systems typically experience a continuous stream of requests [43], [59], making it challenging to achieve true idle states for extended durations.\nGiven these limitations, dynamic GPU frequency scaling (throttling) forms a promising approach for optimizing energy efficiency in LLM inference serving. Unlike static power management techniques, frequency scaling offers a dynamic control knob, allowing the system to adjust its power consumption to the real-time workload demands of incoming inference requests. Crucially, frequency scaling can be applied at the granularity of individual LLM iterations (which typically span milliseconds), whereas traditional techniques operate on the scale of entire queries (which span several seconds). This fine-grained, iteration-level control allows throttling to be applied in a more granular manner, thus greatly enhancing energy efficiency while meeting predefined SLOs.\nHowever, effectively applying frequency scaling to LLM inference remains challenging due to the inherent unpredictability in the text generation process exhibited by these models. While recent works address generation length prediction [21], [24], [46], [71], there is a notable gap on modeling the impact of LLM-intrinsic characteristics and leveraging these insights for energy-efficient LLM inference serving.\nThis paper presents throttLL'eM, an energy-efficient LLM inference serving framework. By leveraging dynamic GPU frequency scaling, throttLL'eM minimizes energy consumption while adhering to latency and throughput SLOs. The system relies on a projection mechanism that estimates KV cache utilization and batch size and a performance prediction model that forecasts system throughput at future LLM iterations. These predictions guide a throttling mechanism, which identifies the minimum frequency that meets target SLOs, thereby optimizing energy usage. throttLL'eM also features an autoscaling mechanism that adjusts the engine's parallelism level based on the incoming workload. Our contributions are:\n\u2022 Comprehensive analysis: We conduct a thorough analysis of GPU frequency, KV cache usage, batch size, and instance parallelism, providing key insights into LLM performance and energy efficiency.\n\u2022 Predictive modeling: We develop an analytical model that forecasts KV cache usage and batch size for future LLM iterations with mean absolute prediction errors of 2.26% and 0.19%, respectively. Additionally, we introduce a machine learning model that predicts iteration-level LLM performance with $R^2$ scores exceeding 0.97.\n\u2022 throttLL'eM framework: We introduce throttLL'eM, a framework that leverages the aforementioned models for instance autoscaling and GPU frequency throttling, optimizing LLM inference energy consumption while meeting SLOs.\nthrottLL'eM reduces energy consumption by an average of 24.7% without autoscaling, reaching 43.8% with autoscaling enabled, when compared to the state-of-the-art Triton inference server [40]. The proposed framework targets a p99th response time SLO equal to that of the baseline system running at peak load and a TBT SLO matching the human reading rate."}, {"title": "II. BACKGROUND ON LLM INFERENCE", "content": "LLMs are neural networks with billions of parameters that heavily rely on the Transformer architecture [56]. Figure 1a shows an overview of a typical LLM inference serving stack. Inference servers (e.g., Triton [40], LMDeploy [13]) bridge the gap between users and pre-trained LLMs. They act as a frontend, receiving user queries and interacting with inference serving backends (e.g., TensorRT-LLM backend [39], vLLM [28]), which handle the deployment of LLM instances on the underlying infrastructure.\nLLM text generation process: LLMs generate text in a step-by-step manner, one token (word or subword) at a time, as shown in Fig 1b. The process begins with a prompt from the user, which is tokenized before being fed into the model. The tokenization vector is passed to the LLM, generating the first token. This is known as the prefill or prompt phase and is compute-bound [43]. The LLM then enters an iterative loop until it generates an End-of-Sequence (EOS) token or reaches a maximum length limit ($max\\_tokens$). During each iteration, the LLM considers the prompt and previously generated tokens to calculate token occurrence probabilities and select a new one. The generated token is appended to the sequence, and the process repeats. This phase is referred to as decoding or generation and is known to be memory bound [4].\nKey-Value (KV) cache: During the prefill phase the input tokens are processed, generating their initial representations (values) stored with their corresponding token identities (keys) in the KV cache. These key-value pairs are high-dimensional tensors, containing vectors for each attention head of the Transformer. For the decoder-only Transformer architecture used by many LLMs, these vectors remain identical across iterations and can thus be cached. In the generation phase, the LLM iteratively predicts tokens, checking the KV cache for existing representations. If found, the cached representation is used; otherwise, a new representation is computed and stored.\nInflight Batching: Inflight batching [65] is an optimization technique which capitalizes on the autoregressive nature of LLMs, by allowing new requests to be added/removed to/from an existing batch while it is being processed."}, {"title": "III. ENERGY EFFICIENT LLM INFERENCE SERVING: CHALLENGES & OPPORTUNITIES", "content": "In this section we examine the impact of various factors that affect LLM performance and energy efficiency. We perform this analysis on a NVIDIA DGX A100 system, utilizing the Llama2-13B model as a representative example of LLMs\u00b9, with a tensor parallelism of two, unless otherwise stated.\nA. Performance-Energy Trade-offs in LLM Inference\nWe investigate the interplay between performance and energy efficiency in LLM inference, aiming to identify the optimal operating points that align with specific performance metrics. Our analysis focuses on the following parameters: i) batch size, indicating the number of requests currently\n\u00b9Since LLMs typically consist of continuous Attention blocks, similar conclusions can be drawn for other models."}, {"title": "1) Impact of Batch Size and GPU Frequency:", "content": "We first examine the impact of batch size and GPU frequency on the performance and power consumption of LLM inference. To this end, we spawn batches of different sizes, ranging from 1 to 32 requests wide. To avoid any variation or bias due to the context of the queries, all queries are identical with their prompt and generation lengths fixed to a specific value of 1 input token and 1024 generation tokens. Figures 2a-2e show the average throughput, E2E latency, TBT, power consumption and energy efficiency for the lifetime of each batch.\nPerformance Analysis: Regarding performance related metrics (Fig.2a-2c), we first observe that throughput increases with batch size and GPU frequency, as shown in Figure 2a. Higher GPU frequencies enable faster processing of tokens, while larger batch sizes increase parallelism, contributing to higher overall throughput (TPS). Notably, when changing from a batch size of 1 at 210 MHz to a batch size of 32 at 1410 MHz, throughput experiences a threefold increase. This trend partially aligns with those observed for the E2E latency and TBT metrics. Figures 2b and 2c show that both E2E latency and TBT generally decrease with higher GPU frequencies due to reduced computation times. On the other hand, both metrics worsen as batch size increases, indicating that larger batch sizes incur overheads due to increased concurrent parallel processing and resource contention. Overall, both E2E and TBT approximately double when moving from the \"high frequency, low batch size\u201d to the \u201clow frequency, high batch size\" area.\nPower Analysis: Figure 2d shows the power consumption for different GPU frequencies and batch sizes. As expected, power increases with higher GPU frequencies due to increased switching activity and operating voltage required to sustain higher processing speeds. We note a greater than twofold increase in power draw between the lowest and highest available GPU frequencies. Interestingly, power consumption remains relatively constant across different batch sizes for a given GPU frequency, indicating that power draw is primarily influenced"}, {"title": "Instance Parallelism:", "content": "Modern LLM serving frameworks [28], [39] support partitioning (sharding), enabling model distribution among many GPUs. This allows for executing large models on GPUs that would otherwise not be able to store LLM weights individually. Three key approaches exist to distribute the workload across multiple GPUs [38]. Distributed Data parallelism (DDP) creates identical copies of the model across multiple GPUs. Tensor parallelism (TP) splits the model's weight tensors across multiple GPUs, enabling parallel computation for specific operations. Finally, Pipeline parallelism (PP) assigns consecutive LLM layers or segments to different GPUs, similar to instruction pipelining in CPUs.\nLLM inference performance metrics: Several metrics have been proposed to evaluate LLM performance and serve as potential SLOs [43], [54]. Time To First Token (TTFT) measures the responsiveness of the inference engine, i.e., the time to generate the first token after query submission. Throughput metrics include Time Between Tokens (TBT), LLM iterations per second (IPS), and tokens per second (TPS). End To End latency (E2E) represents the total time from query submission to completion. To quantify energy efficiency, we consider tokens per Joule (TPJ), measuring the number of tokens generated per unit of energy expended."}, {"title": "III. ENERGY EFFICIENT LLM INFERENCE SERVING: CHALLENGES & OPPORTUNITIES", "content": "In this section we examine the impact of various factors that affect LLM performance and energy efficiency. We perform this analysis on a NVIDIA DGX A100 system, utilizing the Llama2-13B model as a representative example of LLMs\u00b9, with a tensor parallelism of two, unless otherwise stated.\nA. Performance-Energy Trade-offs in LLM Inference\nWe investigate the interplay between performance and energy efficiency in LLM inference, aiming to identify the optimal operating points that align with specific performance metrics. Our analysis focuses on the following parameters: i) batch size, indicating the number of requests currently"}, {"title": "2) Impact of KV cache usage on LLM inference", "content": "Impact of KV on performance: Figures 3a and 3b show the impact of KV cache utilization on throughput (IPS) and TBT for different batch sizes as the token generation process progresses and the number of KV blocks allocated in the GPU's DRAM increases. The results reveal that both the number of allocated KV blocks and the batch size affect performance. We observe that as the number of blocks increases there is clear degradation in Throughput and a linear increase in TBT. This trend is consistent across all batch sizes. While this trend is similar across all batch sizes, smaller batches achieve better performance for the same number of allocated KV blocks, due to the fewer tokens that need to be generated at each iteration, resulting in reduced resource contention within the GPU.\nImpact of KV on power consumption: Figure 3c shows the power consumption for a batch of 32 queries as the KV block count increases (reflecting the generation process) under different frequency levels. A positive correlation between"}, {"title": "Takeaways:", "content": "KV cache utilization significantly impacts both performance and power consumption. The high correlation between KV utilization and performance-related SLOs emphasizes its effectiveness as a proxy for performance modeling."}, {"title": "C. LLM partitioning implications on performance and energy", "content": "As described in Sec. II, LLMs support various techniques for partitioning the model across multiple GPUs, each im-"}, {"title": "Takeaways:", "content": "TP configuration outperforms DDP and PP in both performance and energy efficiency. The trade-off between performance and energy efficiency at different parallelism levels underscores the need for dynamic engine sizing based on real-time workload characteristics."}, {"title": "D. Unpredictability in LLM inference serving systems", "content": "Inference serving systems exhibit significant unpredictability due to the diversity of input prompts and workload traffic variations [43], [59]. To illustrate this, we analyze a 60-minute production trace from Azure LLM inference services [43].\nLength Distribution: Figure 5a shows the distribution of prompt tokens (top) and generated tokens (bottom). Query prompts can reach up to 4000 tokens, with most queries falling between 0-1500 tokens. Similarly, generated tokens range from 10 to 700, with the majority between 100 and 400 tokens. The long-tail distributions observed for both prompt and generated tokens underscore the necessity for dynamic resource allocation strategies to handle the varying computational demands of different queries. Moreover, the unpredictable nature of token generation lengths further complicates resource management"}, {"title": "Takeaways:", "content": "The long tails of token length distributions and high variability in arrival patterns necessitate dynamic resource allocation for optimal management. LLM inference systems rarely reach idle states, rendering race-to-idle techniques incapable of providing optimal energy efficiency."}, {"title": "IV. throttLL'eM: SLO-AWARE & ENERGY-EFFICIENT LLM INFERENCE SERVING", "content": "Building on insights from Sec. III, we introduce throttLL'eM, an energy-efficient, SLO-aware LLM inference serving system. throttLL\u2019eM prioritizes critical SLOs, namely E2E and TBT, delivering energy-efficient inference by dynamically adjusting GPU frequency to match workload demands. Figure 6 shows an overview of throttLL\u2019eM. New queries are processed by the generation length prediction component, which estimates the number of generated tokens. This prediction enables accurate KV cache usage and batch size projections, allowing the system to evaluate potential SLO violations. If no violations are anticipated, the query is scheduled; otherwise, it is queued. Finally, throttLL'eM also includes an autoscaling mechanism to dynamically adjust LLM instance parallelism based on workload, and a throttling controller to optimize GPU frequency and energy consumption. Table I summarizes the terminology used in this section."}, {"title": "A. Generation Length Prediction", "content": "Accurate generation length prediction is essential for optimizing LLM inference, as it can mitigate KV cache overheads (cf. Sec. III-B) and prevent out-of-memory errors [24] due to dynamic memory allocations [28]. Given the direct correlation between the number of generated tokens and allocated KV blocks, length prediction is crucial for preventing such issues. However, accurately predicting generation length is challenging due to the stochasticity of LLMs, the diversity of input queries, and other factors influencing output length.\nRecent scientific works have proposed various approaches to tackle the aforementioned problem [21], [24], [46], [71]. In [21], [24], the authors model length prediction as a classification task, fine-tuning DistilBERT [49] and OPT-125M [69] respectively, to classify output length into \u201cbuckets\u201d. In contrast, the authors of [46] use a regression model, appending a two-layer fully connected network to a BERT base model [17]. In [71], the authors propose the \"Perception in Advance\" method, where the model estimates the length of its response before generating the output. All approaches report errors up to \u2248 30%. In the context of throttLL'eM, we consider a reliable length predictor that can accurately estimate the expected generation length ($r_q$) of each input query. This predicted token generation length ($|r_q|$) is propagated to the KV usage & Batch Size projection component."}, {"title": "B. KV usage & Batch Size projection", "content": "The KV usage & Batch Size projection component generates vectors containing the projected KV cache utilization (KV) and batch size (B) at each future iteration until all currently scheduled requests terminate. By assuming an oracle-based generation length predictor and considering the iterative nature of the token generation process, it is possible to create accurate projections for these values over time. Specifically, since each request in the batch generates one token per iteration (each corresponding to a fixed KV cache block allocation size), we can predict the cumulative KV cache demands and batch sizes for upcoming iterations using an analytical model.\nTo keep track of scheduled requests, a Scoreboard is kept, containing the following metadata for each query ($q_i$) in the batch: i) the iteration at which the query was scheduled ($s_i$), ii) the input length of the query ($|q_i|$) and iii) its predicted generation length ($|r_i|$). Based on these values, we can determine"}, {"title": "that a query will complete its generation process at iteration $s_i+|r_i|$, i.e., $|r_i|$ iterations after it was initially scheduled. By repeating this process for each entry in the Scoreboard, we can identify the final iteration of all scheduled queries, allowing us to generate a vector $B = [B_1, B_2, ..., B_j, ..., B_n]$, where $B_j$ is the projected batch size at each future iteration $j\u2208 [1, n]$ and $n$ is the iteration at which the last query will be completed.", "content": "A similar approach can be followed to calculate the KV cache utilization for each iteration. At iteration $j$, request $q_i$ will have generated $j \u2013 s_i$ tokens\u00b2. The KV cache also stores the precomputed values for the $q_i$ tokens that constitute the input of $q_i$. As each block has a capacity of $N$ tokens, where $N$ is a compile time parameter, the KV cache blocks allocated to request $q_i$ ($KV_{q_i}[j]$) are given by Equation 1.\n$KV_{q_i}[i] = \\begin{cases}\n[(j \u2212 s_i + |q_i|)/N] & , s_i < j < (s_i + |r_i|)\\\\\n0 & , otherwise.\\end{cases}$ (1)\nThe total KV cache block usage of the inference engine at each future iteration $j$ can finally be calculated by summing all $KV_{q_i}$. This is the KV of Figure 6, given by Equation 2, where $KV = [KV_1, KV_2, ...KV_j, ..., KV_n]$.\n$KV [j] = \u2211KV_{q_i}[j]$ (2)\nLast, when a query terminates, its entry is struck from the Scoreboard, signalling the deallocation of all associated KV cache blocks. The introduction of this component allows us to project the usage of the KV cache and the batch size into the\n\u00b2Under the constraint $s_i < j < s_i+r_i$ which ensures that $q_i$ is scheduled."}, {"title": "C. LLM Query Scheduler", "content": "The Query Scheduler is responsible for orchestrating the execution of incoming requests on the LLM engine. To identify potential SLO violations, a Performance Prediction Model is employed. If no SLO violations are predicted, the query is scheduled for execution; otherwise, it is placed in a waiting queue until it can be served without compromising any SLO.\n1) Performance Prediction Model: The Performance Prediction Model (M) predicts the throughput of the system in terms of iterations per second (IPS). Building on the high correlation of throughput to KV cache usage, GPU frequency, and batch size demonstrated in Sec. III-A1 and Sec. III-B, the model leverages these metrics as input features to accurately forecast system performance at the iteration level.\nTraining data collection: Capturing the entire design space for training the predictive model is prohibitive due to i) the fine granularity of GPU frequencies (15MHz steps), ii) the extensive range of possible values for KV block usage, and iii) the large batch sizes, which can reach up to 512 instances in few-parameter models with a high degree of parallelism, or on GPU infrastructures with higher memory capacity [4]. To address this challenge, we employ a systematic sampling approach to acquire a representative subset of the design space. Specifically, we employ a request generator that profiles the LLM inference engine across different batch sizes by spawning queries with predefined generation lengths, ensuring maximum utilization of the KV cache at the final iteration. For each batch size, the generator spawns a number of requests equal to the batch size to cover the entire KV cache usage range, ensuring that the edges of the profiling space are present in the dataset.\nA monitoring agent is deployed to continuously log KV cache usage, GPU frequency, batch size, and the experienced IPS every second. During this process, the GPU frequency is randomly changed (for high frequency range coverage) after each measurement is completed and before starting a new one, in order to ensure that the frequency remains consistent for the duration of each measurement. This data collection process is repeated for all supported tensor parallelism (TP) levels, generating a dataset for each LLM that captures the relationship between the input variables and IPS.\nModel selection & training: throttLL'eM adopts a machine learning (ML) approach to model the relationship between system metrics and IPS. Given that all the input features are upper and lower bounded for a given LLM, we employ a Gradient Boosted Decision Tree [10], [20] as our ML model, which"}, {"title": "has been shown to provide accurate interpolation predictions for bounded datasets with multiple interacting features [33]. The model is lightweight (\u2248 3 ms inference latency on CPU), meeting the design constraint of low inference latency. This is vital not only for enabling iteration-level predictions but also because the model resides in the critical path of the Scheduler. We use the following features as input to the model: i) engine size, ii) batch size, iii) KV cache usage and iv) GPU frequency, training it to predict the throughput (IPS).", "content": "2) Query Scheduling & Admission Control: Upon receiving a new query, the Scheduler initiates an admission control process, using the projected batch (B) and KV cache size (KV) vectors, generated by the KV usage & Batch Size projection mechanism using the \u201cvirtual\u201d Scoreboard. The Scheduler must determine if the new query can be scheduled without causing any SLO violations. This involves three checks:\n1. KV cache assessment: The projected KV cache utilization (KV) is compared to the available KV cache capacity of the inference engine. If any value in KV exceeds this capacity, the query is queued to prevent significant performance degradation from KV block swapping to main system memory.\n2. TBT SLO compliance: For queries meeting KV cache constraints, the Scheduler uses M to estimate throughput for each projected batch size ($B_j \u2208 B$) and KV cache usage ($KV_j \u2208 KV$) pair. The model is queried with: i) the current size of the LLM inference engine, ii) projected batch sizes B, iii) projected KV cache utilization KV, and iv) the maximum available GPU frequency, reflecting peak theoretical performance. The model predicts a vector T of maximum attainable throughput (IPS) per future iteration. Since TBT = 1/IPS, the Scheduler calculates the inverse values of T to form a vector T', representing TBT per iteration. If the average value of T' exceeds the TBT SLO, the request is queued.\n3. E2E SLO compliance: Given the TBT vector T', we can determine the estimated time to reach a future iteration $j$, by calculating the vector $TR = [TR_1,TR_2, ...TR_j, ...,TR_n]$, i.e., performing a cumulative sum on the values of T' as follows:\n$TR_j = \\sum_{i=k}^{i<j} T'_i = \\sum_{i=k}^{i<j} T_i^{-1}$ (3)\nFor request $q_i$, finishing at iteration $l = s_i + r_i - k$, the remaining time is given from element $TR_l$. The current iteration $k$ is subtracted as future iterations are relative to $k$. Thus, for each $q_i$ which is predicted to complete at iteration $l$, the current time ($t_{cur}$) is a added to $TR_l$ and compared to the deadline imposed by the E2E SLO ($t_{dead(q_i)}$):\n$q_i, TR_l + t_{cur} < t_{dead(q_i)}$ (4)\nIf all checks are successful, the newly arrived request is scheduled and any changes to the Scoreboard are committed. Otherwise, the request is queued and changes are rolled back. If a new request cannot be scheduled without violation of its own E2E SLO, but does not cause violations, it is scheduled but marked as \u201clost\u201d, signalling the Scheduler to ignore it during future SLO validations."}, {"title": "D. LLM autoscaling mechanism", "content": "Effective autoscaling is crucial for optimizing both performance and energy efficiency in LLM inference systems. As shown in Sec. III-C, tensor parallelism (TP) forms the most efficient approach for scaling the LLM engine to multiple GPUs on the same node. To this end, throttLL\u2019eM's autoscaling mechanism relies on managing the TP of the LLM engine to match the incoming request rate (RPS), ensuring that the system operates efficiently under varying loads.\nAutoscaling is driven by a 10-second interval monitoring agent that determines the minimum engine size required to handle the current workload. This decision is based on pre-characterized performance profiles, which define the maximum load each engine can sustain without introducing long tail latencies (cf. Sec. V-A). To mitigate the significant provisioning latency (>20s) of new inference servers, we employ \"shadow instancing\" [12], [18], thus, masking switching overheads. This involves a \"warm-up\" phase, where new requests are served by the current engine until the new one becomes operational. Afterwards, a \u201ctransition\u201d phase occurs, during which the old engine gracefully shuts down, while the new engine takes over, serving all new requests. During shadow instancing, both engines run simultaneously, consuming energy without providing peak performance capacity until fully loaded.\nTo regulate autoscaling and minimize energy waste, we establish a policy where newly spawned engines are given a \"grace period\" equal to their spawn time. During this period, the autoscaler may switch to a larger engine to adapt to sudden load spikes but may not switch to a smaller engine, preventing premature downscaling and unnecessary shadow instancing. The grace period is renewed each time the measured RPS is within the engine's workload constraints. This policy prioritizes adherence to SLOs by always allowing scale-up decisions while minimizing scale-downs and unnecessary shadow instancing, thereby reducing energy consumption."}, {"title": "E. GPU Frequency Throttling Controller", "content": "The throttling controller is triggered upon admission of a new query to the inference engine. Its goal is to identify the minimum GPU frequency that adheres to the target SLOs, unlike the Scheduler, which validates SLO compliance at maximum frequency. Since this component is invoked subsequent to successful SLO validation at maximum frequency, the existence of at least one SLO-compliant GPU frequency is guaranteed. The controller receives the KV and B vectors from the KV and Batch projection mechanism, performing a binary search over the GPU frequency range. This involves: 1) Recalculating T, T', and TR for each frequency; and 2) Checking TBT and E2E SLO compliance using the same procedure as the Scheduler. This process determines the minimum SLO-satisfying frequency. If a \u201clost\u201d request is present in the current batch, the binary search is bypassed, and the maximum GPU frequency is selected in an attempt to meet the \u201clost\u201d SLO. We measure the combined latency of the Scheduler and GPU frequency throttling components at 35 ms under heavy load."}, {"title": "F. Discussion", "content": "Addressing generation length prediction errors: The effectiveness of throttLL'eM relies on accurate generation length predictions, yet such errors are unavoidable. Queries terminating earlier than expected primarily impact energy savings, as their SLOs could have been met at lower frequencies. Conversely, queries exceeding the predicted length risk violating SLOs. To mitigate this, we conservatively adjust the predicted generation length ($r_i$) by a factor proportional to the error induced by the predictor. If the actual length surpasses the adjusted $r_i$ during the generation process, the Scoreboard entry for the request is updated to the maximum length limit ($max\\_tokens$) supported by the model.\nthrottLL'eM and TTFT optimization: throttLL'eM does not explicitly optimize the prefill phase, a compute-intensive part of LLM inference. While the prefill phase typically constitutes a smaller proportion of overall latency compared to the generation phase [43], it still represents a potential performance bottleneck. Increasing GPU frequency for a single iteration (upon scheduling a new query) would be an inefficient way to manage TTFT due to the frequency switching overhead (avg. 200 ms) compared to the short prefill phase duration (avg. \u2248175 ms). The Splitwise framework [43] offers a promising alternative by decoupling the prefill and generation stages onto separate GPUs, allowing for independent optimization."}, {"title": "A. Experimental Setup", "content": "HW/SW stack: We deploy throttLL\u2019eM on a p4d.24xlarge AWS EC2 instance equipped with 8\u00d7 NVIDIA A100 40GB GPUs [5]. For the software stack, we use NVIDIA's Triton server [40] and TensorRT-LLM backend [39] which provides high-performance inference on NVIDIA GPUs. We enable typical LLM optimizations, i.e., i) inflight fused batching [65], ii) paged KV caching [28] and iii) Flash Attention [16].\nExamined LLMs and SLO definition: We examine LLMs of different sizes and configurations from the LLaMa family [55], presented in Table II. The TBT SLO is set as to an average latency below 200 ms, corresponding to the average human reading speed of 250 words per minute [8], a value adopted as a target constraint by MLPerf [36], [47]. To define the E2E SLO, we profile the LLMs at maximum GPU frequency using the MLPerf benchmark [47], gradually increasing the RPS until reaching saturation, characterized by long tail latencies in response times. The E2E SLO is set to the p99th response time under maximum load. Table II summarizes this information."}, {"title": "D. throttLL'eM end-to-end Evaluation", "content": "1) throttLL'eM without autoscaling: We first evaluate throttLL'eM without autoscaling, under the assumption of i) an oracle generation length predictor (predicted length $|r_q|$ equals actual generation length) and ii) error-prone predictors with 15% and 30% $p95^{th}$ error, reflecting typical prediction errors reported in the literature [21", "24": [46], "40": "and throttLL'eM are deployed on the scaled trace. In almost all cases, with the exception of llama2-13b-TP1, throttLL\u2019eM achieves the specified $p99^{th}$ latency SLO, being on average 1.45 seconds under the deadline. We attribute the inability of throttLL'eM to achieve the E2E SLO for llama2-13b-TP1 to two factors. First, even the default Triton"}]}