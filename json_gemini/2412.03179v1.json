{"title": "Optimizing Dense Visual Predictions Through Multi-Task Coherence and Prioritization", "authors": ["Maxime Fontana", "Michael Spratling", "Miaojing Shi"], "abstract": "Multi-Task Learning (MTL) involves the concurrent training of multiple tasks, offering notable advantages for dense prediction tasks in computer vision. MTL not only reduces training and inference time as opposed to having multiple single-task models, but also enhances task accuracy through the interaction of multiple tasks. However, existing methods face limitations. They often rely on suboptimal cross-task interactions, resulting in task-specific predictions with poor geometric and predictive coherence. In addition, many approaches use inadequate loss weighting strategies, which do not address the inherent variability in task evolution during training. To overcome these challenges, we propose an advanced MTL model specifically designed for dense vision tasks. Our model leverages state-of-the-art vision transformers with task-specific decoders. To enhance cross-task coherence, we introduce a trace-back method that improves both cross-task geometric and predictive features. Furthermore, we present a novel dynamic task balancing approach that projects task losses onto a common scale and prioritizes more challenging tasks during training. Extensive experiments demonstrate the superiority of our method, establishing new state-of-the-art performance across two benchmark datasets. The code is available at: https://github.com/Klodivio355/MT-CP", "sections": [{"title": "1. Introduction", "content": "Dense vision tasks, which involve pixel-wise predictions, are essential to achieve a thorough understanding of scenes. These tasks encompass image segmentation [5, 26], depth estimation [32, 48], and boundary detection [14, 20], among others. They provide critical information that is fundamental for detailed scene analysis. Traditionally, independent models have been developed to tackle each specific task separately [20, 40, 48]. However, there is increasing interest in developing unified models that can predict multiple tasks simultaneously. This approach, known as Multitask Learning (MTL) [1, 15, 38], aims to improve the efficiency and coherence of predictions in different tasks by leveraging shared information and representations, resulting in substantial advantages over traditional methods [7,29,53].\nMTL frameworks allow interactions between tasks at various stages within the model with the aim of enhancing overall multi-task performance. On the one hand, many previous attempts consist in implementing Cross-Task Prediction Coherence, either through distillation [8, 27, 30] or attention mechanisms [23, 46, 51]. However, these methods often result in a poor geometry consistency throughout task representations. On the other hand, we draw inspiration from [12] to define the notion of Cross-Task Geometric Coherence. [12] leverages auxiliary task's geometric information to optimize the main semantic segmentation task; here, our goal is to preserve spatial relationships and geometric properties among task representations to ensure consistent geometry across all tasks. We believe that successfully solving both types of coherence as part of MTL frameworks is the key.\nAnother aim of MTL is for concurrent training of multiple tasks to improve parameter efficiency and create robust, transferable representations. However, training multiple tasks together comes with major challenges: (1) some tasks can dominate in terms of gradient magnitudes due to their task-specific loss scales, resulting in larger gradients on the shared parameters and causing hyperfocus on the larger-scaled task functions; (2) tasks do not naturally evolve at the same pace, making it crucial to control the learning pace of each task while keeping the diverse task losses on the same scale. Previous MTL approaches typically opt for one of two solutions; however, each has significant issues: (1) manually choosing weights for each task, which requires extensive trial-and-error optimization [15, 46, 51]; (2) learning parameters, which are practically nontrivial and difficult to interpret during training [13, 18, 46]. To remedy these issues, we instead propose a dynamic loss prioritization scheme which balances tasks for efficient multi-task training.\nIn this study, we introduce a method that explicitly addresses the aforementioned Multi-Task Coherence and Prioritization issues, and therefore name our method MT-CP. The MT-CP architecture distinguishes itself from existing multi-task learning (MTL) models for dense predictions in two key ways. Firstly, it ensures geometric coherence of tasks by aligning the directions of task vectors in feature spaces; then, to tackle the coherence of prediction of tasks, it propagates non-linear pixel relationships through task-specific decoders back to the shared backbone (see Fig. 1); we name this whole procedure Trace-Back. Secondly, it employs a parameter-free loss prioritization technique that normalizes task-specific losses and dynamically emphasizes more challenging tasks throughout training. Experiments on two benchmark datasets demonstrate that MT-CP achieves state-of-the-art performance on the NYUD-v2 [28] and PASCAL-Context [6] datasets."}, {"title": "2. Related Work", "content": "In this section, we review key areas relevant to our research: MTL in Sec. 2.1, cross-task interactions for dense prediction in Sec. 2.2 and loss weighting strategies in Sec. 2.2. Firstly, MTL allows for simultaneous training of multiple tasks, enhancing model performance and generalization. Secondly, cross-task interactions improve the accuracy and efficiency of predictions in pixel-wise visual tasks through information sharing. Lastly, loss weighting strategies balance the contributions of different tasks, ensuring effective MTL optimization."}, {"title": "2.1. Multi-Task Learning", "content": "Multi-Task Learning (MTL) has become increasingly popular due to its ability to leverage information across multiple tasks. MTL aims to partition features into shared and task-specific subsets. Architectures for MTL can be broadly categorized based on their approach to information sharing: (1) Soft-parameter sharing [8, 27, 30, 31] involves distinct task-specific data paths, for which each task has its own set of parameters, encouraging parameter partitioning through regularization. For example, cross-stitch networks [27] originally introduce this paradigm and propose to fuse parameters by performing a linear combination of activation maps from each layer of task-specific networks. Later, MTAN [21] suggested the use of attention mechanisms to derive a shared set of parameters from the task-specific parameters. This framework, while computationally intensive and complex, is preferred for unrelated tasks. (2) Hard-parameter sharing [15, 21, 25, 46] uses a shared backbone that is branched into lightweight task-specific decoders. This design, with its extensive feature sharing, is ideal for closely related tasks. In this work, we use a hard-parameter sharing backbone with state-of-the-art transformers, based on the idea that this simple framework is well suited for dense prediction tasks because of their related nature."}, {"title": "2.2. Cross-Task Interactions for Dense Prediction", "content": "Dense visual tasks in computer vision involve complex, pixel-wise, and semantically related tasks such as object detection [35], semantic segmentation [40], panoptic segmentation [57], depth estimation [48], surface normal estimation [36] etc.. They present extremely valuable information for scene understanding. Previous MTL works have explored cross-task relationships through distillation and affinity patterns [2, 39, 45, 54]. Additionally, many approaches have employed visual attention mechanisms to learn non-linear relationships across tasks [11, 21, 23, 46, 51]. However, these methods frequently fall short in explicitly identifying the high-level embeddings utilized in cross-task operations and the rationale behind their effectiveness. In contrast, we emphasize that cross-task coherence, within the context of dense visual tasks, entails maintaining both pixel-wise consistency and preserving spatial relationships across task representations. The work most closely related to ours is [12], which leverages geometric information from depth estimation to improve semantic segmentation. While our approach is inspired by this objective, it differs by addressing the intrinsic challenge of multi-task learning (MTL), which involves optimizing all tasks equally within a unified framework, thereby ensuring balanced performance across all tasks."}, {"title": "2.3. Loss-Weighting Strategies", "content": "In MTL training, shared parameters aggregate task-specific gradients, necessitating careful gradient design in terms of magnitudes [3, 43] and directions [43, 52]. A common strategy is to tweak task-specific loss magnitudes to indirectly manage gradient magnitudes. Many methods manually select task weights for a weighted average of gradients [9, 15, 51], an inefficient process requiring trial-and-error optimization. Alternatively, learning task weights during training has been explored, such as in [13], which adjusts task scalars based on uncertainty. Dynamically adjusting losses based on task difficulty is another approach, focusing on more challenging tasks during optimization [10, 16, 19, 34]. In this study, we adhere to the paradigm of dynamically adjusting the focus on challenging tasks throughout training. However, we extend this approach by also normalizing task losses to a consistent scale. Additionally, we introduce a method that enables controllable task learning paces during training. Implementing such dynamic approach enhances cross-task interactions and results in improved overall performance."}, {"title": "3. Method", "content": "In this section, we introduce the MT-CP Model. We present an overview of our model in Sec. 3.1. Next we present the technical aspects of the forward pass of our model in Sec. 3.2; we then illustrate how we enforce geometric coherence through the task representations in Sec. 3.3; afterwards, we introduce in Sec. 3.4 how we perform the trace-back which propagates cross-task information through the task-specific decoders to help enhance predictive performance. We finally present our loss prioritization scheme in Sec. 3.5."}, {"title": "3.1. Overview", "content": "The overview method is illustrated in Fig. 2. Our MT-CP model uses a Mask2Former as a shared backbone [4] to process the RGB input. The resulting representation is then divided into task-specific heads. The representation is individually run through a pyramid transformer which provides a multi-scale representation of each task. The different scales are then concatenated by using Pyramid Feature Fusion (PFF), resulting in the task features $X$, and $X_d$. Subsequently, Coherence Fusion Modules (CFMs) use the aforementioned representations from both tasks to enforce pixel-wise coherence. Then, the learned embeddings are then traced back through our task decoder stages via the Spatial Refinement Modules (SRMs) attached to each stage. Throughout this prediction refinement procedure, intermediate predictions are kept and added to the MTL loss. Finally, predictions are obtained from the output of the final SRM module. Finally, we present a Loss Prioritization Scheme (LPS) that dynamically optimizes the learning process by prioritizing more challenging tasks. This scheme periodically updates task-specific weights based on their relative progress over a performance history. It is designed to normalize tasks on a common scale, and we further regulate task progression through the implementation of a spread parameter."}, {"title": "3.2. Forward Pass", "content": "Shared Backbone. A single input image $I \\in \\mathbb{R}^{3 \\times H \\times W}$, is passed through a Mask2Former backbone [4]. This backbone consists of 3 elements: an encoder, a pixel decoder, and a transformer decoder. Firstly, $I$ will pass through the encoder and the pixel decoder to produce the pixel embeddings $P \\in \\mathbb{R}^{C \\times H \\times W}$. Secondly, we obtain $N$ object mask predictions from each layer in the transformer decoder, we denote those masks as $M \\in \\mathbb{R}^{N \\times H \\times W}$. We finally project the masks onto the pixel embeddings by performing matrix multiplication between the two representations: $A = PM$, then the elements in $A$ are summed over the dimension of the instance $N$, thus aggregating the contributions of each instance to produce a final representation $R \\in \\mathbb{R}^{N \\times H \\times W}$. This final representation encapsulates both the pixel-level details and the instance-level contextual information, providing a rich and informative feature map which we further utilize in the task-specific decoders.\nTask Decoders. Given $T$ tasks, we implement task-specific decoders $F_{T}^{1}$. As our model is targeted towards dense prediction tasks, we choose to leverage lightweight transformer models that use Hierarchical Feature Processing (HFP) [22, 37, 41, 42]. As a result, we obtain the multi-scale representations throughout the $K$ intermediate down-sampling stages $X_{K}^{1}(R i E T) \\in \\mathbb{R}^{(H / P) \\times (W / P) \\times (P^{2} . C)}$, where $P$ is the hyperparameter for window size inherent to HFP transformers. Subsequently, we merge features by performing Dynamic Feature Pyramid Fusion (DFPN) [17], which is a technique to integrate information across multiple scales by learning adaptive weights to selectively integrate features. The DPFN module consists of a series of Interpolation and Conv2D operations. Finally, as part of the forward pass, the coherence fusion module (CFM) uses the resulting concatenated representation to enforce geometric coherence throughout task representations. We present this method in the next section."}, {"title": "3.3. Coherence Fusion Module", "content": "We aim to enforce geometric coherence between tasks by using our coherence fusion module, illustrated in Fig. 3. CFM modules are placed at the end of each task-specific decoder and take as input (1) a main task representation $X_T$ and (2) a gated concatenation of all other (auxiliary) task representations $X_{T}^{2...T}$. Specifically, we design the gates as sigmoid-activated pixel-wise convolutions, which we later multiply element-wise with the original representations. We then concatenate these representations and denote the resulting representation as $X_{T}^{aux}$. Subsequently, $X_{T}$ and $X_{T}^{aux}$ are individually processed by lightweight learnable convolution-based modules that consist of a $1 \\times 1$ Convolutional Block Attention Module (CBAM) [44], followed by a batch normalization and a ReLU activation function. We use the notation $X_{T_{1}}^{'}$ and $X_{T_{aux}}^{'}$ to describe the resulting representations. Then, we design two strategies to enforce geometric coherence to help enhance the main task. Firstly, we minimize the cosine distance between $X_{T_{1}}^{'}$ and $X_{T_{aux}}^{'}$, the cosine distance ensures that the vectors in each representation are attracted together towards the same direction. This conceptually helps ensure that the geometric structure (e.g., edges, boundaries) of the scenes is similarly captured in both representations. Secondly, the features are merged via matrix multiplication. This conceptually ensures that not only are the structural features aligned but also vector magnitudes help maintain consistency as using matrix multiplication to project onto a common space serves this purpose. Finally, the resulting representation is passed through a 1x1 CBAM [44] and batch normalization. We note the output of the CFM : $H_{i \\in T}, T$ being the set of tasks."}, {"title": "3.4. Prediction Refinement via Trace-Back", "content": "We further leverage pixel-wise cross-task relationships for better cross-task prediction coherence. Specifically, we choose to trace back our cross-task representation from our initial representation $H_{i \\in T}$ through the associated task-specific decoder blocks. This trace-back is performed through the use of the spatial refinement module, illustrated in Fig. 4. Specifically, to give an example, we design our SRM to recursively propagate the cross-task representation $T_1$ back through Task 1 and the block scales $K$ in a bottom-up manner. Therefore, our first SRM takes as input $T_1$ and $T_K$. Subsequently, the CBAM [44] convolutions are run to learn discriminative characteristics to better suit task 1. $T_1$ is resized to match the size of $T_K$. The learned features are then concatenated along the channel dimension before parallel and independent lightweight learnable modules consisting of pixelwise convolution, batch normalization, and the ReLU activation function are applied to produce the input $T_{K-1}$ to the next SRM module, which will also take as input $T_{K}$ and so on... In addition, as proposed by [12], we retain intermediate task-specific predictions to contribute to the MTL loss that aims to further improve discriminative power."}, {"title": "3.5. Loss Prioritization Scheme", "content": "This section describes the design of our Loss Prioritization Scheme (LPS) to tackle the loss imbalance problem. To further improve performance by enhancing cross-task interactions throughout training, we believe that difficult tasks should not only be prioritized but also projected onto a similar scale. To this end, we first introduce the minimization objective inherent to MTL training and explain why designing an LPS is central to our challenge. Then, we introduce how we project losses onto a similar scale. Finally, we present our LPS algorithm and present our MTL loss.\nObjective and Problem. We describe a MTL objective, as finding a set of parameters $0*$ such as :\n$0* = \\arg \\min _{01, ..., 0T} (L^{1} (\\theta^{sh}, \\theta^{1}), ..., L^{T} (\\theta^{sh}, \\theta^{i})), $ (1)\nwhere task specific losses $L^{i}$ take as parameters both the shared parameters $\\theta^{sh}$ and task-specific parameters $\\theta^{i \\in T}$, where $T$ is the set of tasks. To achieve this objective, existing MTL methods weigh the tasks according to pre-defined weights $w_i$ as follows:\n$L_{MTL} = \\sum_{i=1}^{T} W_i L_i$ (2)\nwhen $w_i = \\frac{1}{T} \\forall i$, this is an Equal Weighting (EW) loss scheme. Otherwise, if the weights have different values, we consider this to be the Manual Annotation (MA) loss scheme. However, both loss schemes have drawbacks, EW completely overlooks the different scales, leading to a domination of the semantic segmentation task on NYUD-v2 [28] for instance. This leads to undesirable overall performance caused by the faster convergence of the segmentation task. One may be interested in having tasks trained at a similar pace. Therefore, some works have chosen to perform MA to compensate for that scale difference [15, 51], however, this requires a lot of trial-and-error tuning and it is also heavily dependent on the model complexity. We stress therefore the importance of both (1) projecting tasks onto a similar scale, (2) dynamically prioritising the more challenging tasks.\nLoss Scale Projection. Similar to previous work [13, 18, 19], we choose to project tasks onto a similar scale by using the log transformation. Precisely, we choose to formulate our overall objective as follows:\n$L_{Log-MTL} = \\sum_{i=1}^{T} \\log(1 + w_i)L_i,$ (3)\nwhere the $\\log(1 + w_i)$ is necessary to avoid values for $w_i \\in [0, 1]$ leading negative weights, therefore leading to negative loss values. This scaling method has the effect to remove the scale imbalance problem.\nTask Prioritization. In addition to projecting tasks onto a similar scale through the log transformation, dynamically adjusting the learning of some tasks over others might improve the learned cross-task relationships in our CFM module. We choose to prioritise challenging tasks, which might change over training to further smooth out the training of tasks and increase overall performance. We periodically adjust the rate of tasks, at each epoch $n$. For the sake of simplicity, we denote $L_i$ to be the loss for a task $i \\in T$ according to Eq. (3), where $T$ is the set of tasks. Moreover, we define the ratio to which a task $i$ contributes to the overall loss as $\\frac{L_{i}}{L}$. We then define an arbitrary task history length $H$. Then, we dynamically adjust our task-specific weights $\\tilde{w}$ over our history size $H$ such that:\n$\\tilde{w}_{i}^{n}=\\prod_{k=1}^{H} \\frac{\\frac{L_{i}}{L}_{n-k+1}}{\\frac{L_{i}}{L}_{n-k}}$ (4)\nAs a result, the weights $\\tilde{w}_{i}^{n}$ indicate whether the task-specific loss decreases quickly ($\\tilde{w}_{i}^{n}<1$) or slowly ($\\tilde{w}_{i}^{n}>1$)). This indicates whether a task is easy or difficult, therefore assigning more weight to the slower or difficult task, respectively.\nControlling Spread. As our experiments show that weights tend to be different at start and then close together as training continues. We implement a penalty term that encourages the spread of the weights around their mean. Firstly, let us consider the mean of the weights $\\mu_{i}^{n}$ for a given epoch $n$ and task $i$. Secondly, we calculate the deviations from the mean as follows :\n$\\sigma_{i}^{n} = w_{i}^{n} - \\mu_{i}^{n}$ (5)\nFinally, we design a hyper-parameter $k$ to scale the deviations $\\sigma_{i}^{n}$ to update our weights such as :\n$w_{i}^{n} = \\mu_{i}^{n} + k\\sigma_{i}^{n}$ (6)\nAs a result, $k_{i}$ is a hyper parameter which controls the convergence of task losses by controlling the spread of our task-specific weights. Increasing $\\kappa$ will lead to a higher penalty in the weights normalization.\nMTL Loss. We summarise our overall MTL loss used for training. In addition to $L_{Log-MTL}$ defined in Eq. (3), we keep track of intermediate task-specific predictions to further improve the performance. Formally, our MTL loss, for a given epoch $n$ can be formulated as below:\n$L_{IPS} = L_{Log-MTL}(w^n, L^n) + \\sum_{i=1}^{T} \\sum_{j=1}^{TK} L_{k}^{i}$ (7)\ns.t. w* = LPS(w,$\\kappa$)\nwhere K is the number of down-sampling stages in our task-specific decoder, and $w^n$ and $L^n$ represent the list of weights and losses for all tasks, for a given epoch n, respectively."}, {"title": "4. Experiments", "content": "We apply our model on two widely used MTL datasets.\nNYUD-v2. [28] This dataset comprises 1449 labeled images drawn from indoor scene videos for which each pixel is annotated with a depth value and an object class. Additionally, there are 407,024 unlabeled images which contain RGB, depth and accelerometer data, rendering this dataset useful for real-time applications as well. This dataset comprises 3 different tasks: Semantic Segmentation, Monocular Depth Estimation and Surface Normal Estimation.\nPascal-Context. [6] A dataset of 1464 of regular object-centered scenes. This dataset comprises 3 different tasks: Semantic Segmentation, Human Part Parsing which is a type of semantic segmentation task where objects are defined as body parts, and Saliency Detection."}, {"title": "4.2. Implementation", "content": "Semantic Segmentation / Human Parsing: To train this task, we choose to employ the Cross Entropy loss. To evaluate this task, we choose to leverage the mean Intersection over Union (mIoU).\nMonocular Depth Estimation: We leverage the L1 loss for training. We report the results of depth estimation using the Root Mean Squared Error (RMSE) value.\nSurface Normal Estimation: Similarly, we choose to use the L1 loss with normalisation during training. We evaluate this task by using the mean Error (mErr).\nSaliency Detection: We leverage the Balanced Cross Entropy loss function. We also adopt the maximum F-measure (maxF) to evaluate saliency detection results.\nBackbone. We fine-tune our backbone which is a Mask2Former [4] pre-trained on the ADE20K dataset [55] on the semantic segmentation task. This backbone uses a small Swin transformer encoder [22]. This backbone network channel size is 256 which operates on image sizes of (480, 640) for NYUD-v2 [28] and (512,512) for Pascal-Context [6].\nTask Decoders. Furthermore, we design lightweight task-specific decoders consisting of 3 down-sampling stages with a lightweight configuration of (2, 2, 2) blocks per head with depth (1, 2, 1).\nNetwork Parameters. We validate and train our model on a NVIDIA A100 GPU. We choose to use a learning rate of 5 x 10-5 on a batch size of 2. We also choose an Adam optimizer with weight decay [24] with a weight decay value of 1 \u00d7 10-4. We empirically choose the value of k to be 2.5. Similarly, we choose the history length to be H = 3."}, {"title": "4.3. Comparison with State-of-the-art", "content": "In this section, we compare our method with several state-of-the-art (SOTA) models on two benchmark datasets: NYUD-v2 [28] and Pascal-Context [6]. Our comparison focuses on multi-task learning performance, using only RGB input, across different tasks within these datasets.\nNYUD-v2. [28] presents the performance comparison of various SOTA methods on the NYUD-v2 dataset for three tasks: semantic segmentation (Semseg), depth estimation (Depth), and surface normal estimation (Normal). Our method achieves the best performance in semantic segmentation and depth estimation, with mIoU of 56.25 and RMSE of 0.4316, respectively. Furthermore, our method shows competitive performance in normal estimation with an mErr of 18.60. Compared to the previous method with the best performance, MLORE [49], our model exceeds it in both Semseg and Depth tasks. Specifically, our model improves the mIoU from 55.96 to 56.25 and reduces the RMSE from 0.5076 to 0.4316, demonstrating significant advancements. Although MLORE [49] achieves the best mErr of 18.33 in Normal estimation, the performance of our method is close to an mErr of 18.60.\nPascal-Context. [6] showcases the comparison on the Pascal-Context dataset, focusing on semantic segmentation (Semseg), human part parsing (Parsing), and saliency detection (Saliency). Our approach yields top-tier results in parsing and semseg, achieving the highest mIoU of 69.13 and 79.96 respectively. In saliency detection, our method scores a maxF of 84.20, closely trailing the leading score of 84.94 by Bi-MTDP [33].\nOverall, our approach demonstrates substantial improvements and competitive results across both datasets, establishing it as a strong contender in the multi-task learning domain. These results highlight the effectiveness of both our model architecture and our loss-balancing strategy in enhancing performance across diverse tasks. Some visualizations of our model predictions on this dataset are shown in Fig. 5."}, {"title": "4.4. Ablation Analysis", "content": "MT-CP Architecture. illustrates the impact of key architectural components, CFM (Coherence Fusion Module) and SRM (Spatial Refinement Module), on the performance of our MT-CP model on the NYUD-v2 dataset.\nThe complete MT-CP model, with both CFM and SRM, delivers the best results across all metrics, indicating their crucial role in the architecture. Removing CFM results in a noticeable decline in performance, particularly in semantic segmentation (mIoU drops to 52.78) and depth estimation (RMSE increases to 0.4803), highlighting its importance in feature integration to enhance geometric coherence between tasks. The absence of SRM also degrades performance, though less severely.suggesting its role in refining spatial features for better cross-task predictive coherence. The combined removal of both CFM and SRM leads to the most significant performance drop, demonstrating the synergistic effect of these components in the MT-CP architecture. This ablation study confirms the critical contributions of CFM and SRM to the overall performance and robustness of the model.\nLPS. presents a comparative study of various loss schemes on the NYUD-v2 dataset [28]. MT-CP, using the Loss Prioritization Scheme (LPS), achieves superior results on all tasks. In contrast, the Equal Weights (EW) scheme significantly underperforms, demonstrating the necessity of a balanced loss approach. The log smoothing scheme, which consists of a simple log transform as presented in Sec. 3.5, offers notable improvements, yet falls short of LPS, while the Loss Prioritization (without log smoothing) configuration, although effective, does not match the consistency between tasks achieved by LPS. This analysis underscores the effectiveness of LPS in enhancing multi-task learning performance by appropriately balancing task contributions, hence resulting in a better optimization and learning of cross-task information.\nVarying K. We illustrate the effect of varying the hyper-parameter K in Fig. 6. We show the effect of the heuristic values of $k = 2.5$ and $k = 7.5$ on our MTL optimization. For each given epoch, we notice that if a task-specific loss decreases slowly, the respective weights go up. We also show how a higher value of $k = 7.5$ acts a stronger penalty, as opposed to k = 2.5 to the convergence of the weights."}, {"title": "5. Conclusion", "content": "This paper introduces MT-CP, a multi-task learning model designed for dense prediction tasks. MT-CP effectively leverages pixel-wise cross-task information through each task-specific decoder, ensuring coherent predictions in both semantic and geometric contexts. Furthermore, we propose a loss prioritization scheme that dynamically focuses on more challenging tasks during training. Experimental results on two benchmark datasets demonstrate the superior performance of MT-CP, surpassing current state-of-the-art methods in certain tasks and maintaining competitive results in others."}]}