{"title": "Mitigating Gradient Overlap in Deep Residual Networks with Gradient Normalization for Improved Non-Convex Optimization", "authors": ["Juyoung Yun"], "abstract": "In deep learning, Residual Networks (ResNets) have proven effective in addressing the vanishing gradient problem, allowing for the successful training of very deep networks. However, skip connections in ResNets can lead to gradient overlap, where gradients from both the learned transformation and the skip connection combine, potentially resulting in overestimated gradients. This overestimation can cause inefficiencies in optimization, as some updates may overshoot optimal regions, affecting weight updates. To address this, we examine Z-score Normalization (ZNorm) as a technique to manage gradient overlap. ZNorm adjusts the gradient scale, standardizing gradients across layers and reducing the negative impact of overlapping gradients. Our experiments demonstrate that ZNorm improves training process, especially in non-convex optimization scenarios common in deep learning, where finding optimal solutions is challenging. These findings suggest that ZNorm can affect the gradient flow, enhancing performance in large-scale data processing where accuracy is critical.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has advanced significantly in recent years, enabling breakthroughs across a wide range of applications. While newer architectures continue to emerge, Residual Networks (ResNets) [1] remain widely used in areas like medical imaging and computer vision, valued for their effectiveness [2]\u2013[5]. ResNets represented a major milestone in deep learning by successfully addressing the vanishing gradient problem through skip connections [6]. These connections enable gradients to flow more consistently through deep layers, stabilizing training even in networks with hundreds of layers. This capability has allowed ResNets to outperform traditional models in tasks ranging from image classification to complex data analysis.\nDespite their success, We found that the deep residual networks [1] have an issue: gradient overlap. This phenomenon results from interactions between gradients from both the learned transformation and the identity mapping (skip connections) during backpropagation. While gradient overlap helps mitigate the vanishing gradient problem by preserving gradient flow, it can also lead to overestimated gradients, resulting in less effective optimization. In particular, during the training, overlapping gradients may cause updates that overshoot optimal regions, potentially slowing convergence. Additionally, in non-convex optimization scenarios, such as those common in deep learning, gradient overlap in ResNets can hinder the model's ability to find optimal solutions.\nCan we mitigate the gradient overlap in Residual\nNetworks by applying gradient normalization to control\noverestimated gradients for the better optimization?\nTo address this challenge, our gradient analysis reveals that Z-score Normalization (ZNorm) [7] can effectively mitigate the effects of gradient overlap. Originally developed as a gradient normalization technique to enhance neural network performance, ZNorm adjusts gradient scales, helping prevent excessive updates by maintaining consistent gradient magnitudes across layers [7]. This normalization approach not only reduces the risk of overestimated gradients but also accelerating training, aligning gradient scales across layers more effectively.\nOur hypothesis is that ZNorm mitigates the gradient overlap phenomenon in ResNets, providing a more balanced and accurate gradient flow that accelerates convergence and enhances training accuracy. This is particularly crucial in fields such as medical imaging, where handling large-scale data accurately is essential, and even minor improvements in model accuracy can have significant implications. By addressing gradient overlap, ZNorm establishes a foundation for understanding why it has proven effective in enhancing performance in CNNs.\nIn this paper, we provide the examination of the impact of gradient overlap in ResNets and show how ZNorm offers a solution by stabilizing the gradient flow. Our experimental results demonstrate that applying ZNorm improves optimization stability and leads to more effective deep learning models, particularly in applications requiring high accuracy and robust performance."}, {"title": "II. RELATED WORKS", "content": "Convolutional Neural Networks (ConvNets) have been pivotal in deep learning advancements, especially in computer vision. LeNet-5 by LeCun et al. [8] laid the groundwork for ConvNets in handwritten digit recognition. The introduction of AlexNet [9] by Krizhevsky et al. marked a significant leap in image classification, utilizing deeper architectures and GPU acceleration. Following this, the VGG network [10] by Simonyan and Zisserman explored the impact of network depth, using small convolutional filters and increasing layers to enhance accuracy. However, deeper networks faced challenges like vanishing gradients. To address this, He et al. proposed Residual Networks (ResNets) [1], introducing skip connections that allow training of much deeper networks by mitigating these issues. ResNets have become fundamental in various computer vision tasks due to their robust performance and training efficiency. Extensions like Wide ResNets [11], which trade depth for width, and ResNeXt [12], which aggregates transformations, have further improved performance. DenseNets [13] enhance feature reuse by connecting each layer to every other layer.\nOptimization techniques are fundamental for effectively training deep neural networks. Stochastic Gradient Descent (SGD) and its variants, including momentum-based SGD [14] and adaptive methods like Adam [15], have been instrumental in minimizing loss functions through iterative parameter updates. Adaptive optimizers such as Adagrad [16] and RMSProp [17] adjust learning rates dynamically based on gradient magnitudes and history, enhancing training stability for deep models. Normalization methods mitigate training instability and smooth optimization landscapes. While Batch Normalization (BN) [18] normalizes activations to reduce internal covariate shifts, other techniques like Layer Normalization (LN) [19] and Group Normalization (GN) [20] normalize along different dimensions, such as per layer or group of channels. Weight Normalization (WN) [21] and Weight Standardization (WS) [22] focus on consistent scaling of weights, contributing to faster and more stable convergence during training. Gradient adjustment methods stabilize training by controlling gradient updates. Gradient Clipping [23] caps large gradients, preventing instability. AdamW [24] separates weight decay from gradient updates in adaptive optimizers, improving generalization. Stochastic Gradient Sampling focus on improving the generalization effect of Residual Networks [25]. Gradient Centralization [26] normalizes gradients to zero mean, enhancing convergence. Z-score Normalization (ZNorm) [7] standardizes gradients across layers, mitigating vanishing and exploding gradients for greater stability and performance. In this work, we leverage ZNorm to counteract gradient overlap in ResNets, aiming to further stabilize training by addressing this specific challenge in gradient flow."}, {"title": "III. GRADIENT OVERLAP", "content": "Residual networks [1] are well-known for their ability to prevent the vanishing gradient problem using skip connections [13]. These connections improve gradient flow and create a smoother loss landscape [27], leading to more stable training convergence [28]. However, our investigation reveals a potential drawback of skip connections: gradient overlap. This phenomenon occurs when gradients from the learned transformation and the skip connection combine during backpropagation. This overlap may result in the loss of important gradient details, affecting the network's capacity for precise updates. To analyze this, we present several lemmas that describe gradient behavior in normal networks [9], [29] and residual networks [1]. We conclude with a theorem demonstrating the impact of gradient overlap in Residual Networks.\nLemma 2.1. Let $Y_i = X_{i-1}+ f_i(x_{i-1}; W_i)$ represent the output of the i-th residual block in a Residual Network [27]. The gradient of the loss L with respect to the input $x_0$ is bounded by:\n$\\frac{\\partial L}{\\partial x_0} \\leq \\prod_{i=1}^{N}(1 + a_i)\\frac{\\partial L}{\\partial Y_N}$ (1)\nwhere $a_i = ||\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}||$ is the operator norm of the gradient of the learned function in the i-th residual block.\nProof. Consider the forward propagation through each residual block [27]:\n$Y_i = X_{i-1} + f_i(x_{i-1}; W_i), for i = 1, 2, ..., N$ (2)\nHere, $X_{i-1}$ is the input to the i-th block, and $f_i$ is the residual function of that block. To compute the gradient of the loss function L with respect to $x_{i-1}$ during backpropagation, we apply the chain rule:\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} \\frac{\\partial Y_i}{\\partial x_{i-1}}$ (3)\nSince $Y_i = x_{i-1} + f_i(x_{i-1}; W_i)$, the derivative $\\frac{\\partial Y_i}{\\partial x_{i-1}}$ is:\n$\\frac{\\partial Y_i}{\\partial x_{i-1}} = I + \\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}$ (4)\nwhere I is the identity matrix, representing the derivative of $x_{i-1}$ with respect to itself.\nThe first term, $\\frac{\\partial L}{\\partial Y_i}$, simplifies to $\\frac{\\partial L}{\\partial Y_i}$, and substituting this expression back into the chain rule gives:\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} (I+ \\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}})$ (5)\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} + \\frac{\\partial L}{\\partial Y_i} \\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}$ (6)\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} \\Big(I+\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}\\Big)$ (7)\nFactoring out $\\frac{\\partial L}{\\partial Y_i}$ gives the final expression:\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} \\Big(I+\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}\\Big)$ (8)"}, {"title": null, "content": "Taking the norm on both sides of the inequality and using the sub-multiplicative property of norms, we have:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial L}{\\partial Y_i} \\Big(I+\\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}\\Big)||$ (9)\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial L}{\\partial Y_i}|| ||I+\\frac{\\partial f_i (x_{i-1}; W_i)}{\\partial x_{i-1}}||$ (10)\nHere, we have applied the sub-multiplicative property of norms, which states that for any matrices A and B, the inequality $||AB|| \\leq ||A|| \\cdot ||B||$ holds. This results in:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial L}{\\partial Y_i}|| ||I+\\frac{\\partial f_i (x_{i-1}; W_i)}{\\partial x_{i-1}}||$ (11)\nNext, to handle the term $||I+\\frac{\\partial f_i (x_{i-1}; W_i)}{\\partial x_{i-1}}||$, we use the triangle inequality for operator norms. The triangle inequality states that for any two matrices A and B, we have:\n$||A + B|| \\leq ||A|| + ||B||$ (12)\nApplying this to our case, since the operator norm of the identity matrix I is 1 (i.e., $||I|| = 1$), this simplifies to::\n$||I + \\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}|| +||I||$ (13)\n$||I + \\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}|| + 1$ (14)\nWe define $a_i = ||\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}||$ which represents the operator norm of the gradient of the learned function in the i-th residual block. Thus, the inequality becomes:\n$||I + \\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}|| \\leq 1 + a_i$ (15)\nSubstituting this result back into equation (11), we obtain:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial L}{\\partial Y_i}|| \\cdot (1 + a_i).$"}, {}, {"title": null, "content": "We can apply this inequality recursively for i = N, N - 1,..., 1. Starting from i = N, we have:\n$||\\frac{\\partial L}{\\partial x_{0}}|| \\leq ||\\frac{\\partial L}{\\partial Y_N}|| \\prod_{i=1}^{N}(1 + a_i)$ (17)\nThis completes the proof that the gradient norm with respect to the input $x_0$ is bounded by the product of (1 + ai) terms across all residual blocks.\nLemma 2.2. For a plain neural network without skip connections, where the output of the i-th block is defined as $Y_i = f_i(x_{i-1}; W_i)$, the gradient of the loss function L with respect to the input $x_0$ is [27]:\n$\\frac{\\partial L}{\\partial x_{0}} = (\\prod_{i=1}^{N} a_i) \\frac{\\partial L}{\\partial Y_N}$ (18)\nwhere $a_i = ||\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}||$\nProof. Consider a plain network without skip connections, where the output of the i-th block is:\n$Y_i = f_i(x_{i-1}; W_i)$ (19)\nFor the i-th block, the gradient of the loss L with respect to the input $X_{i-1}$ is given by the chain rule:\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} \\frac{\\partial f_i (x_{i-1}; W_i)}{\\partial x_{i-1}}$ (20)\nTaking the norm of both sides:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| = ||\\frac{\\partial L}{\\partial Y_i} \\frac{\\partial f_i (x_{i-1}; W_i)}{\\partial x_{i-1}}||$ (21)\nBy setting $a_i = ||\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}||$, we have:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| = a_i ||\\frac{\\partial L}{\\partial Y_i}||$ (22)"}, {"title": null, "content": "Recursively applying this formula for all N blocks, we obtain: By recursively applying this inequality from i = N down to i = 1, we obtain:\n$||\\frac{\\partial L}{\\partial x_{0}}|| = (\\prod_{i=1}^{N} a_i) ||\\frac{\\partial L}{\\partial Y_N}||$ (23)\nTheorem 2.3. In a Residual Network with N residual blocks, the presence of skip connections causes gradient overlap due to the additive term (1 + ai) in the gradient propagation. Specifically, the gradient of the loss function L with respect to the input $x_0$ is bounded by:\n$||\\frac{\\partial L}{\\partial x_{0}}|| \\leq (\\prod_{i=1}^{N} (1 + a_i)) ||\\frac{\\partial L}{\\partial Y_N}||$ (24)\nwhere $a_i = ||\\frac{\\partial f_i (x_{i-1};W_i)}{\\partial x_{i-1}}||$ is the operator norm of the gradient of the learned transformation in the i-th residual block. This inequality demonstrates that gradients from both the identity mapping and the learned transformation are combined at each layer, leading to gradient overlap.\nProof. From Lemma 2.1, we have established that for each residual block:\n$\\frac{\\partial L}{\\partial x_{i-1}} = \\frac{\\partial L}{\\partial Y_i} \\Big(I+\\frac{\\partial f_i(x_{i-1}; W_i)}{\\partial x_{i-1}}\\Big)$ (25)\nTaking the norm and using the sub-multiplicative property of norms:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| \\leq ||\\frac{\\partial L}{\\partial Y_i}|| \\Big(||I|| + ||\\frac{\\partial f_i}{\\partial x_{i-1}}||\\Big)$ (26)\nSince $||I|| = 1$, this simplifies to:\n$||\\frac{\\partial L}{\\partial x_{i-1}}|| \\leq (1+a_i) ||\\frac{\\partial L}{\\partial Y_i}||$ (27)\n$||\\frac{\\partial L}{\\partial x_{0}}|| \\leq \\prod_{i=1}^{N} (1 + a_i) ||\\frac{\\partial L}{\\partial Y_N}||$ (28)\nThis result shows that at each layer, the gradient includes contributions from both the identity mapping (1) and the learned transformation ($a_i$), encapsulated in the term (1+ai). This additive term indicates that gradients from these two sources are combined, leading to gradient overlap.\nIn contrast, for a plain network without skip connections (from Lemma 2.2), the gradient propagation is:\n$||\\frac{\\partial L}{\\partial x_{0}}|| = (\\prod_{i=1}^{N} a_i) ||\\frac{\\partial L}{\\partial Y_N}||$ (29)\nHere, the gradient at each layer depends solely on the learned transformation's gradient norm ai. Comparing the two results, we observe that:\n$\\prod_{i=1}^{N}(1 + a_i) > \\prod_{i=1}^{N} a_i$ (30)\nsince 1 + ai > ai for all ai > 0.\nThe additive 1 from the identity mapping in residual networks increases the gradient norm compared to plain networks, leading to what we term gradient overlap. This overlap can result in an overestimation of the gradient norm, amplifying certain components beyond what is necessary for accurate updates. Although batch normalization [19] helps control the gradient magnitude, it cannot fully mitigate the directional influence of the additive 1, which can cause a loss of precision in the gradient direction. This loss of gradient directionality can impair optimization efficiency, especially when the learned transformation fi is highly sensitive (i.e., when ai is large), leading to compounded contributions from both the identity path and the learned transformation. These"}, {"title": null, "content": "compounded contributions can influence the optimization in skip-connected networks. ZNorm [7] has demonstrated superior performance compared to traditional methods like gradient centralization [26], gradient clipping [23], and weight decay [24]. By normalizing gradient magnitudes, ZNorm addresses the accumulation of overlapping gradients in skip connections, leading to more stable and efficient training.\nNotations. Consider a deep neural network with L layers, where each layer l is associated with weights $\\Theta^{(l)}$. For fully connected layers, the weights are represented as $\\Theta_{fc}^{(l)} \\in R^{D_l \\times M_l}$, where $D_l$ is the number of neurons and $M_l$ is the input dimension to the layer. In convolutional layers, the weights $\\Theta_{conv}^{(l)}$ are described by a 4-dimensional tensor: $\\Theta_{conv}^{(l)} \\in R^{C_{in}^{(l)} \\times C_{out}^{(l)} \\times k_1 \\times k_2}$, where $C_{in}^{(l)}$ and $C_{out}^{(l)}$ represent the number of input and output channels, and $k_1, k_2$ are the kernel sizes. Let $\\nabla L(\\Theta^{(l)})$ denote the gradient of the loss function L with respect to the weights $\\Theta^{(l)}$ of layer l. The overall gradient tensor is represented by $\\nabla L(\\Theta)$.\nZ-Score Gradient Normalization (ZNorm). Z-score nor-malization is applied independently to each layer's gradient $\\nabla L(\\Theta^{(l)})$, defined as:\n$\\Phi(\\nabla L(\\Theta^{(l)})) = \\frac{\\nabla L(\\Theta^{(l)}) - \\mu_{\\nabla(\\Theta^{(l)})}}{\\sigma_{\\nabla(\\Theta^{(l)})} + \\epsilon}$ (32)\nwhere $\\mu_{\\nabla(\\Theta^{(l)})}$ is the mean of the gradients in layer l, and $\\sigma_{\\nabla(\\Theta^{(l)})}$ is the standard deviation of these gradients. A small"}, {"title": null, "content": "constant $ \\epsilon$ (commonly set to $1 \\times 10^{-10}$) is added to avoid has momentum 0.9, with the baseline referring to the standard Adam and SGDM without gradient adjustment. The experiments involved Gradient Clipping [23] with 0.1, Gradient Centralization [26], and ZNorm [7]. We train none-augmented CIFAR-10 for 100 epochs with 256 of batch size.\ndivision by zero.\nEffect on Overlapped Gradients. In Figure 1 and Figure 2, we examine the effect of skip connections and normalization techniques on gradient flow within deep networks. Subfigure (A,E) illustrates the baseline gradients without skip con-nections, providing a reference for comparison. In contrast, Subfigure (B,F) shows the gradients when skip connections are applied, highlighting gradient overlap, where gradients from both the learned transformation and the skip connection merge. This overlap can lead to overestimated gradients, potentially affecting the gradient descent process by causing larger-than-necessary updates.\nTo address this, we apply ZNorm to the skip-connected gradients, as shown in Subfigure (C,G). ZNorm effectively reduces the impact of overlapping gradients by re-centering and scaling them, mitigating the overestimation present in Subfigure (B,F). The difference between the ZNorm-applied and skip-connected gradients is visualized in Subfigure (D,H), showing how ZNorm counters the overestimated components and helps maintain gradient direction consistency. As a result, applying ZNorm allows the model to better manage overlapped gradients, leading to more optimal gradient updates and improved training process. This demonstrates that ZNorm can effectively mitigate the gradient overlap problem, enhancing the model's ability to reach optimal solutions.\nV. EXPERIMENTS\nIn this section, we conducted a series of experiments to train Residual Networks using ZNorm method, comparing its performance to other approaches. These experiments aim to test our hypothesis that ZNorm enhances performance in Residual Networks by mitigating gradient overlap, ultimately leading to improved accuracy with optimal directions.\nExperimental Settings. All experiments were performed using the Adam [15] and SGDM [14] optimizer which"}, {"title": null, "content": "Experimental Results. Figure 3 shows the test accuracy progression across 100 epochs for each model and technique. ZNorm consistently led to faster convergence and higher test accuracy compared to baseline and other normaliza-tion techniques across both optimizers. Specifically, ZNorm demonstrated its effectiveness in mitigating gradient overlap by maintaining stable gradient flows and minimizing the risk of overestimated gradients, which often disrupt the training in deep residual networks. Table I summarizes the top-1 test accuracy and training loss achieved by each model and normalization technique. Notably, ZNorm improved both test accuracy validating its capability to mitigate the gradient overlap problem inherent in residual networks. These results confirm that ZNorm not only enhances model performance but also provides a robust solution for managing gradient flow, particularly beneficial in non-convex optimization scenarios typical of deep neural networks.\nVI. CONCLUSION\nZ-score Gradient Normalization (ZNorm) effectively ad-dresses the gradient overlap issue in Residual Networks (ResNets) by standardizing gradient magnitudes across layers, thus minimizing the risk of overestimated gradients from skip connections. This results in more accelerated and optimal training, particularly in deep networks facing non-convex optimization challenges. Experimental results on ResNet archi-tectures showed that ZNorm improves convergence rates and achieves higher test accuracy compared to baseline methods and other gradient normalization techniques. These findings confirm ZNorm's utility in optimizing ResNet-based models across various domains."}]}