{"title": "From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World Navigation Task", "authors": ["Nicolas Martorell"], "abstract": "Understanding how large language models (LLMs) represent and reason about spatial information is crucial for building robust agentic systems that can navigate real and simulated environments. In this work, we investigate the influence of different text-based spatial representations on LLM performance and internal activations in a grid-world navigation task. By evaluating models of various sizes on a task that requires navigating toward a goal, we examine how the format used to encode spatial information impacts decision-making. Our experiments reveal that cartesian representations of space consistently yield higher success rates and path efficiency, with performance scaling markedly with model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal units primarily located in intermediate layers that robustly correlate with spatial features, such as the position of the agent in the grid or action correctness, regardless of how that information is represented, and are also activated by unrelated spatial reasoning tasks. This work advances our understanding of how LLMs process spatial information and provides valuable insights for developing more interpretable and robust agentic AI systems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated impressive capabilities in processing and generating text [1,2], yet a critical question remains: do these models develop abstract internal representations of the world, or do they simply memorize typical reasoning paths? This debate is particularly heated in non- textual domains such as spatial or perceptual tasks where the models have only been exposed to such concepts indirectly through text [3-5]. While several studies have shown that LLMs can learn and solve tasks in these domains [6-8], and that their internal representations can be linearly mapped to those external structures [9], a clear understanding of how these internal models are composed and how they influence behavior is still lacking."}, {"title": "2 Related Work", "content": "This question is of increasing importance as LLMs are being deployed as agents that interact with environments by taking sequential actions based on textual inputs [10,11]. In non-multimodal settings, the representation of spa- tial information as text is crucial, as language models are known to be highly sensitive to prompting [12-14]; the format in which this information is encoded may profoundly influence a model's ability to extract relevant knowledge and make correct decisions. Furthermore, studying distinct representations of spatial information can help unveil if and how LLMs encode abstract models of space (i.e. internal activations that contain spatial information and are invariant to changes in prompting and context), and whether they leverage those internal world models to make decisions.\nTo address these issues, we investigated how LLM behavior and understanding of space change based on how spatial information is provided in the prompt, and whether there exists an internal model of space that is invariant to how that information is represented. We evaluated the LLaMA-3 family of models [15], spanning model sizes from 1B to 90B parameters, on a Grid-World Spatial Orientation Task (GWSOT). In this task, models were required to navigate a 2D grid toward a goal by selecting one of four possible moves. We tested several different Spatial Information Representations (SIRs) in text form. We categorized these representations into three types: Cartesian representations, in which the (x, y) coordinates of both the agent and the goal are explicitly encoded; Topographic representations, which preserve the grid-like spatial structure in the text; and Textual representations, which describe the world state in prose. Across model sizes, we found that Cartesian representations consistently yielded better performance.\nFurthermore, to gain insight into the internal processing of spatial information, we probed the activations of the LLaMA-3.1-8B model during the GWSOT. Our analysis revealed parameters that significantly predicted the position of the agent in the grid, as well as parameters that predicted action correctness in the subsequent step, regardless of how spatial information was represented in the prompt. Intriguingly, a subset of these units, primarily located in the model's middle layers, were also more active when the model tackled spatial reasoning questions in unrelated contexts. These findings point to the existence of core units that form an internal spatial model invariant to prompt variations, context, and task specificity.\nOur contributions can be summarized as follows:\n\u2022 We demonstrate that spatial orientation performance scales with model size.\n\u2022 We show that specific ways of encoding spatial information have significant effects on model performance, even when the conveyed information is equivalent.\n\u2022 We reveal that LLMs can represent the position of an agent in 2D space in ways that are partially invariant to prompting, with units that encode specific spatial features abstractly.\n\u2022 We find specific units that predict action correctness during spatial navigation, which are also activated in unrelated spatial reasoning contexts."}, {"title": "3 Evaluating LLMs in a Spatial Orientation Task", "content": "World Models in LLMs. World models have been defined as a compact, coherent, and interpretable representation of the generative process underlying the training data [9,16,17]. Some authors argue that LLMs lack internal world models capable of predicting world states and simulating action outcomes, which can impair their performance in agentic and planning tasks [4,5]. On the other hand, some studies have identified specific neurons that encode space and time, demonstrating internal models that remain robust to variations in prompting [9]. Other investigations have found evidence for internal models of the non-linguistic world-ranging from perceptual structures such as color to spatial orientation concepts, cardinal directions, and object properties [6,8,18]. Implicit world models have also been described in goal-oriented contexts where the representation is influenced by an agent's objectives [19].\nInternal Representations and Mechanistic Interpretability. Mechanistic interpretability has emerged as a promising avenue for understanding the inner workings of LLMs [20-22]. This approach has been used to analyze emergent behaviors in larger models and to trace how internal representations influence output, providing essential insights for causality and AI safety [23-25]. For example, research has shown that neurons across different layers tend to specialize, with middle layers often containing neurons that represent higher-level contextual features [26-29]. Furthermore, some neurons exhibit task-specific activations and can be predictive of model performance on these tasks [28,30-32].\nPrompting Techniques and Prompt Influence on Outcome. A growing body of work has established that LLM performance is highly sensitive to the specific prompting techniques employed, across a variety of benchmarks and tasks [12-14]. Even variations in prompt formatting have been found to lead to notable differences in model behavior [33]. This has driven the development of a wide range of prompting techniques which become highly relevant when attempting to improve LLM performance [34,35].\nLLMs for Spatial Navigation. LLMs have also been applied to spatial navigation tasks despite being trained solely on text [36,37]. These tasks have commonly been represented as text-based sequences, where models are asked to provide information about the environment or actions that have effects in the world [38-40]. Previous studies have evaluated LLMs as agents navigating grid- world environments, where models must plan a full path in advance to locate goals and avoid obstacles [41,42]. These approaches demonstrate that LLMs can solve spatial navigation tasks, although small models have shown limited generalization to variations such as differing grid sizes or obstacle configurations [41]."}, {"title": "4 Prompting and Scale as Factors in Performance", "content": "To evaluate spatial orientation in an agentic context, we designed the Grid-World Spatial Orientation Task (GWSOT), as illustrated in Fig. 1. This is a simple variation of a classic reinforcement learning task [49,50], where an agent and a goal are placed in an 5\u00d75 grid. The two left-most panels of Fig. 1 depict example trials, where green arrows indicate a correct path leading to the goal, whereas red arrows show a path that result in failure by reaching a maximum number of steps.\nAt every step, the agent is provided with the current world state in a conversation-style prompt-where user messages describe the grid state and assistant messages record previous decisions. The LLM is required to respond with a JSON object containing a single key, \"action\", whose value is one of four commands: \"UP\", \"DOWN\", \"LEFT\", or \"RIGHT\". An action is defined as correct if it reduces the Manhattan distance between the agent and the goal. A trial ends when the agent reaches the goal or when it exceeds a maximum step limit.\nWe use different Spatial Information Representations (SIRs) to convey the current world state to the LLM. We categorized SIRs into three classes:\n\u2022 Cartesian representations: The LLM receives explicit (x, y) coordinates for the agent and the goal, along with the grid size.\n\u2022 Topographic representations: The two-dimensional structure of the grid is preserved by representing cells and objects using characters or words. This requires LLMs to be able to interpret the text layout as structure, which previous research has shown these models are able to achieve [51].\n\u2022 Textual representations: The world state is described in prose-like language.\nTo ensure that our findings are attributable to the SIR class rather than to a specific formatting style, we implemented two variants for each class: JSON and Chess Notation for Cartesian, Symbol and Word Grid for Topographic, and Row and Column Description for Textual (see Fig. 1, right panels). Despite their differences, all six SIR types encode identical spatial information.\nFor additional details on the task configuration and system prompt see Appendix A."}, {"title": "5 Activations Encode Spatial Features in a Mid-Sized LLM", "content": "We evaluated the LLaMA 3 family of models\u2014specifically the 3.2-1B, 3.2-3B, 3.1-8B, 3.2-11B, 3.1-70B, and 3.2-90B variants in the 5\u00d75 GWSOT. For each model and each of the six SIR types, we conducted 100 trials, resulting in a dataset of 3,600 trials. In addition, we ran 100 trials with a random policy agent that uniformly selected one of the four possible actions at each step, providing a baseline for comparison. A complete account of each experiment performed in this work, along with their features and number of trials per condition can be found in Appendix B.\nModel performance was quantified using three metrics. First, we computed the success rate as the proportion of trials in which the agent reached the goal. Second, for successful trials, we measured path efficiency by dividing the minimum number of steps required to reach the goal (i.e., the initial Manhattan distance) by the actual number of steps taken by the agent; an efficiency of 1 indicates a perfect, direct path. Third, for unsuccessful trials, we calculated the final distance ratio by dividing the final Manhattan distance from the agent to the goal by their initial Manhattan distance. A ratio of 1 implies no improvement, while lower ratios indicate that the agent moved closer to the goal despite not reaching it.\nThe first three panels of Figure 2 summarize how both model scale and SIR type influence these performance metrics. The left panel shows that success rates increase with model size for every SIR type (Linear Regression,  \\( \\beta = 0.008\\), p < .001). The smallest models (1B and 3B) perform near chance level\u2014around 10% for all SIR types except for JSON, while the largest models (70B and 90B) always exceed a 74% success rate. Particularly, the 90B model receiving the JSON representation achieved an outstanding 98% success rate. Notably, Cartesian SIRs consistently outperformed Topographic and Textual SIRs across model sizes (Binomial GLM: success ~ representation + model_size, \\(\\beta_{topographic} = 1.385\\), p < .001; \\(\\beta_{textual} = 1.270\\), p < .001), although the mid-sized models (8B and 11B) exhibited the most pronounced performance differences between SIRs. For example, the 8B model achieves a 66% success rate with the JSON SIR but only 30% with its best non-cartesian SIR (Symbol Grid). Importantly, this pattern holds regardless of formatting variations within each SIR class (compare same-color curves in the left panel of Fig. 2-both JSON and Chess Notation outperform Topographic and Textual SIRs).\nA similar trend was observed in the efficiency metric, shown in the second panel of Fig. 2. This metric also improved with the scale of the model (Gaussian GLM: efficiency ~ model_size, \\(\\beta_{model\\_size} = 0.001\\), p < .001), and Cartesian SIRS consistently resulted in models taking more efficient paths to the goal during"}, {"title": "6 Robust Units Encode Spatial Reasoning Across Unrelated Tasks", "content": "Spatial Maps in Neuroscience. The neuroscience literature provides a foun- dational perspective on spatial representations through studies of spatial maps in the brain. Early work on place cells in the hippocampus of rats revealed that specific neurons encode an animal's position in a two-dimensional space [44, 45]. Subsequent research identified other spatially tuned cells such as grid cells, which encode grid-like patterns; head-direction cells, which signal the direction of gaze; and boundary cells, which respond to environmental edges [46-48]. The specialization observed in biological systems suggests that generalist artificial systems, including LLMs, might similarly develop internal representations for spatial orientation and navigation."}, {"title": "7 Discussion", "content": "successful trials (Gaussian GLM: efficiency ~ representation + model_size, \\(\\beta_{topographic} = \u22120.159\\), p < .001; \\(\\beta_{textual} = \u22120.116\\), p < .001), particularly in the mid-sized models where performance is above chance and not yet saturated. Notably, the largest models approached near-optimal efficiency in this task when receiving the JSON SIR (see green continuous curve).\nFurthermore, the final distance ratio (third panel of Fig. 2) also improves with scale (Gaussian GLM: final_distance_ratio ~ model_size, \\(\\beta_{model\\_size} = -0.006\\), p < .001). Smaller models (1B and 3B) fail to improve their positions in unsuccessful trials regardless of SIR type, while in mid-sized models (8B and 11B) only Cartesian representations led to improvements (Welch's t-test; Cartesian: t = -1.96, p = 0.053, marginally significant; Topographic: t = 1.06, p = 0.291; Textual: t = 1.41, p = 0.162). For the largest models (70B and 90B), all SIR types demonstrated improvements relative to the random policy (Welch's t-test; Cartesian: t = \u22123.055, p = 0.004; Topographic: t = -9.223, p < 0.001; Textual: t = -7.324, p < 0.001). Overall, these metrics reveal a scaling law for spatial navigation performance, and show that Cartesian representations are better tuned to convey spatial information to the models.\nTo gain further insight into the models' spatial reasoning, we constructed policy maps the four right-most panels of Fig. 2-where cells represent positions relative to the goal (recentered to the middle of the grid to allow visualization), and arrows indicate the most common action chosen in each cell, colored by the relative frequency with which that action was chosen at each position in the grid. For brevity, only maps for the LLaMA-3.1-8B and LLaMA-3.2-90B models using JSON and Symbol Grid SIRs are shown. Presenting the models with a Cartesian SIR resulted in a higher proportion of correct policies across positions and model sizes, compared to other SIR types. However, larger models found the correct policy in more positions and were more consistent in their behavior. This aligns with the quantitative performance metrics, reinforcing the conclusion that Cartesian representations (and especially the JSON SIR) lead to more reliable spatial decision-making."}, {"title": "Our work aims to enhance the explainability of LLM-based agentic systems by elucidating how the representation of spatial information influences model behavior. These insights have practical implications for the design of agentic systems. For instance, when deploying LLMs in environments where spatial reasoning is critical, explicitly formatting spatial data in a Cartesian manner can lead to more robust and efficient decision-making. Moreover, understanding the internal structure of spatial representations may inform future strategies for mechanistic analysis and interventions. These might involve reweighing or isolating identifiable units to mitigate unexpected behaviors, as well as using information encoded in these specific groups of neurons to make agentic decision- making more transparent, thus leading to safer and more reliable AI systems.", "content": "To investigate whether specific parameters within LLMs are involved in represent- ing spatial information, we conducted a detailed analysis of the LLaMA-3.1-8B model the smallest variant that demonstrated above-chance performance across all SIR types (see left-most panel of Fig. 2). Our primary aim was to determine whether activations in individual layers encode the grid configuration up to a linear transformation and, if so, what are the components of that spatial model and how it depends on the SIR type.\nWe then trained linear regression models to predict the complete configuration of a 5\u00d75 grid, represented as a 50-dimensional binary vector (with 25 dimensions encoding the presence or absence of the agent and 25 for the goal-details on this analysis can be found on Appendix C). The \\(R^2\\) values of these linear models generally peaked in early middle layers and then declined in later layers, as depicted in the left-most top panel of Fig. 3. This was the case for all SIR types. However, the layer at which the \\(R^2\\) peaked depended on the SIR class. For Cartesian and Textual representations, \\(R^2\\) peaked around layer 5, exhibited a dip, and then showed a modest recovery in the final layers. In contrast, Topographic representations reached their maximum performance deeper in the model (around layer 10) before declining monotonically. These observations suggest that the way the model represents spatial information, and how this representation changes across layers, depends on how that information is encoded in text. Despite these differences, the test-set performance of every layer for each SIR type consistently exceeded that of a null model (Permutation Test: one per model against 10 shuffles, p < .001 for all cases), indicating that information about the spatial configuration of the grid is present throughout the network regardless of SIR type."}]}