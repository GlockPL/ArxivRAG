{"title": "AnDB: Breaking Boundaries with an Al-Native Database for Universal Semantic Analysis", "authors": ["Tianqing Wang", "Xun Xue", "Guoliang Li", "Yong Wang"], "abstract": "In this demonstration, we present AnDB, an AI-native database that supports traditional OLTP workloads and innovative AI-driven tasks, enabling unified semantic analysis across structured and unstructured data. While structured data analytics is mature, challenges remain in bridging the semantic gap between user queries and unstructured data. AnDB addresses these issues by leveraging cutting-edge AI-native technologies, allowing users to perform semantic queries using intuitive SQL-like statements without requiring Al expertise. This approach eliminates the ambiguity of traditional text-to-SQL systems and provides a seamless end-to-end optimization for analyzing all data types. AnDB automates query processing by generating multiple execution plans and selecting the optimal one through its optimizer, which balances accuracy, execution time, and financial cost based on user policies and internal optimizing mechanisms. AnDB future-proofs data management infrastructure, empowering users to effectively and efficiently harness the full potential of all kinds of data without starting from scratch.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the data landscape has evolved with the emergence of data lakehouses, a hybrid architecture combining the scalability of data lakes with the performance and management features of data warehouses [9]. However, despite their strengths in data storage, data lakehouses struggle to effectively analyze unstructured data, which constitutes 90% of global data [2] and remains a largely untapped resource [8]. Unstructured data, encompassing text, images, audio, and video, presents significant challenges due to its complexity and heterogeneity, often leading to data silos, inefficiencies, and missed opportunities for actionable insights [6].\nThe rapid advancement of Artificial Intelligence (AI), particularly Large Language Models (LLMs), offers promising solutions to bridge the gap between semantic queries and unstructured data. Techniques like Retrieval-Augmented Generation (RAG) enhance LLMs by integrating external knowledge, reducing hallucinations, and improving accuracy [3]. However, RAG systems are limited by their focus on retrieval, filtering, and ranking, with inadequate support for advanced operations like aggregation and join.\nRecent research has sought to integrate traditional database methodologies with AI technologies. For instance, the \"LOTUS\" system [7] introduces semantic operators (e.g., semantic join, semantic group) to enhance AI-driven analysis, while \"PALIMPSEST\" [5] enables LLM-powered queries through a declarative process. Notwithstanding these advancements, significant challenges remain in the precise interpretation of user requirements, and the implementation of effective feedback mechanisms for system calibration. A case in point is the \"PALIMPSEST\" system, which necessitates user programming intervention, demonstrates inadequate coverage in its execution plan generation, and disregards conventional optimization strategies for data scan operations, etc.\nIn this demonstration, AnDB addresses these limitations by providing an intuitive SQL-like interface through its SQL Engine, enabling seamless analysis of structured and unstructured data. The Query Optimization component generates multiple execution plans, with an optimizer selecting the most efficient one. Results are delivered relationally via the Execution Engine, ensuring clarity regardless of data structure. Additionally, the Calibration component estimates result accuracy and core metrics (e.g., execution time, token usage), feeding calibration information back into the optimizer to improve future performance."}, {"title": "2 System Overview", "content": "As illustrated in Figure 1, AnDB comprises several core components, including the SQL engine, query optimization engine, execution engine, storage component, and auxiliary modules such as the model management and Knowledge Graph (KG) module. Users can seamlessly interact with AnDB using SQL-like statements for unstructured data analysis and standard SQL for relational data queries."}, {"title": "2.1 SQL Engine", "content": "SQL Grammar Tokens. In addition to standard SQL statement tokens, AnDB introduces additional tokens to express user requirements for semantic analysis tasks accurately. In Table 1, these tokens are categorized into Semantic Tokens and Auxiliary Tokens, enabling end-to-end semantic analysis with enhanced flexibility and precision.\nLogical Plan. AnDB processes relational data in strict adherence to traditional relational concepts, ensuring compatibility with existing systems. Semantic tokens in AnDB are internally translated into logical plans, naturally bridging the gap between unstructured data query semantics and standard database operators. To achieve this, AnDB introduces several novel operators, e.g., Transform, which resembles a map operation but supports flexible schema based on user prompts. Unlike traditional map operations in MapReduce, which preserve dimensionality, the Transform operator can expand or contract relation dimensions, necessitating a new conceptual framework.\nParticularly, systems that rely exclusively on isolated prompt fragments while failing to incorporate global contextual information consistently exhibit coherence deficiencies. In contrast, AnDB addresses this limitation by comprehensively considering all prompt text parameters (e.g., PROMPT, SEM_GROUP, and SEM_MATCH), better understanding user requirements.\nPhysical Plan. AnDB integrates prompt requirements into conventional relational operators, leveraging their inherent flexibility. However, a key challenge lies in designing efficient and effective physical operators to implement logical plans. For example, a logical join operator can be implemented through multiple physical operators, such as nested-loop-join, hash-join, and sorted-merge join. As semantically equivalent execution paths, providing diverse physical operator implementations is essential to support the query optimizer in selecting an optimal execution plan."}, {"title": "2.2 Query Optimization", "content": "Although several innovations, like \"PALIMPSEST\", propose balancing financial cost, accuracy, and runtime, we contend that runtime is merely an intermediate factor, as the financial cost strongly correlates with end-to-end execution time. Therefore, we adopt a \"less-is-more\" strategy to redefine the problem. Notably, we introduce a normalized cost model that defines the trade-off between these factors, which is interpretable and natural to integrate with the existing relational database optimizer. Specifically, we define the accuracy-cost matrix and token vector as follows.\nLet M represent the accuracy-cost matrix, where the elements correspond to the coefficients for accuracy (A) and cost (C) for both input and output tokens:\nWe also define the token vector t as the number of input and output tokens processed by a semantic operator o:\nGiven that semantic operators process N data tuples, the cost-accuracy trade-off for a semantic operator o is formalized as:\n$F(o) = C_{input} t_{input} + C_{output} t_{output} + \\lambda \\cdot (1 - \\frac{A_{input} t_{input} + A_{output} t_{output}}{t})$\nwhere $\\lambda \\in [0, +\\infty)$ is a tuning parameter that controls the trade-off between cost and accuracy. A higher value of $\\lambda$ prioritizes accuracy, while a lower value emphasizes cost reduction.\nThe core idea of this formulation is to explicitly minimize the weighted total cost while introducing accuracy as a loss term into the optimization objective, which penalizes low accuracy resulting from cost reduction. This design aligns more closely with the traditional database optimization logic. Hence, the optimization of a hybrid execution plan, P, combining both semantic and traditional operators, where x(o) is the execution cost of a traditional operator o, is then formulated as:\n$argmin\\limits_{P \\in P} {\\sum\\limits_{o \\in P_{semantic}} {\\Gamma(o)} + \\sum\\limits_{o \\in P_{traditional}} {x(o)}}$"}, {"title": "2.3 Execution Engine", "content": "LLM invocations primarily rely on API integration. However, if local deployment is required, there are numerous of significant potential for optimization in areas such as resource scheduling, GPU utilization, and in-memory data caching. Additionally, AnDB has a lightweight knowledge graph module, inspired by GraphRAG [1], to aid in reasoning, as graphs effectively represent entity relationships.\nThe execution engine can retrieve structured, semi-structured, and unstructured data from disk. A key optimization involves indexing unstructured files, such as extracting keywords from images using multimodal models (e.g., FUYU-8b [4]) and preprocessing documents (e.g., chunking) before storage because performing these tasks during execution would be inefficient."}, {"title": "2.4 Calibration", "content": "The calibration component parallels the reflection and self-critique aspects of the agentic Al framework, which integrates two approaches: (1) User-based: This approach utilizes accuracy feedback provided by the user following query execution. (2) Model-based: This method employs cross-validation across multiple LLMs and built-in rules, offering a higher level of automation."}, {"title": "3 DEMONSTRATION OVERVIEW", "content": "We showcase AnDB's functionalities for processing unstructured data.\nDataset. Two unstructured text documents, neurips_2023.txt, and neurips_2024.txt, were scraped from papers published at NeurIPS conferences in 2023 and 2024.\nScenario 1: Classification and Aggregation. The following three statements are nearly of semantic equivalence but differ in the execution plans. Statement a represents a simple retrieval scenario, similar to what is typically processed by RAG systems. Statement b demonstrates a semantic retrieval scenario that does not provide a specific schema. AnDB is required to implicitly infer the schema and perform retrieval based on global semantics. Statement c is a scenario where the user specifies a schema. AnDB only needs to transform unstructured data into structured data according to the user's requirements and then apply traditional data operators for aggregation.\nScenario 2: Analyzing Research Trends in NeurIPS (2023-2024). Cross-conference analysis of research trends between NeurIPS 2023 and 2024, involving retrieval, aggregation, and join operations.\nHere, we demonstrate an end-to-end workflow for a use case as Figure 2 shown. Step 1: The user inputs an SQL-like query in the client interface. Step 2: AnDB scans the relevant unstructured data and invokes the LLM to perform the analysis. Step 3: The results are returned to the user, and presented in a structured format.\nThe use case only involves two feasible plans in Figure 3. Plan A: First, unstructured data is transformed into tabular data, followed by aggregation using traditional join methods. Plan B: During the tabular transformation process, a domain-specific model is used to vectorize the corresponding text contents and embed them into a vector value in a hidden column. Subsequently, during the semantic join, vector similarity is utilized for judgment."}]}