{"title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint", "authors": ["Qianli Ma", "Dongrui Liu", "Qian Chen", "Linfeng Zhang", "Jing Shao"], "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: neuron misidentification due to simplistic parameter magnitude-based selection, and cross-task neuron interference during merging. To address these challenges, we propose LED-Merging, a three-stage framework that Locates task-specific neurons via gradient-based attribution, dynamically Elects critical neurons through multi-model importance fusion, and Disjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(e.g., a 31.4% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95% of utility performance(e.g., 52.39% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks (Brown et al., 2020; OpenAI, 2024; Cai et al., 2024; Touvron et al., 2023; Bai et al., 2023). Although post-training is widely used to improve LLMs' performances on downstream tasks, training task-specific models for different tasks leads to significant storage and training costs. To this end, model merging (Wortsman et al., 2022a; Akiba et al., 2024; Xiao et al., 2024), a training-free technique that combines parameters from multiple fine-tuned models into a unified model, has emerged as a promising solution.\nPrevious research has shown that merging methods can lead to safety-utility conflicts, where improvements in general ability (e.g., mathematical reasoning) degrade safety safeguards (Hammoud et al., 2024). For instance, merging safety-aligned and math-specific fine-tuned models get an unsafe mathematical AI expert (left conversation in Fig. 1a), reducing safety capabilities by over 30%, as shown in Fig. 1b. To address this problem, additional alignment training has been employed to improve the safety capabilities of the merged model (Thakkar et al., 2024; Aakanksha et al., 2024). However, such consequential safety-specific training requires labeled data and training costs, limiting their applicability in privacy-sensitive or resource-constrained scenarios. More critically, these methods address symptoms rather than root causes-they neither analyze neuron-level conflicts nor resolve interference mechanisms.\nThe safety-utility conflicts stem from two fundamental limitations in existing methods: (i) Neuron misidentification: Previous merging methods rely on simplistic metrics like parameter magnitude to select neurons, failing to distinguish safety-related regions from LLMs and impair safety capacity (ii) Neuron interference: Neurons optimized for different tasks (e.g., safety and code generation) exhibit antagonistic updates during merging, causing destructive parameter collisions and severely reduced performance, as shown in Fig. 1b, and Fig. 1c.\nIn this paper, we propose LED-Merging, a simple and effective merging method to address the above problems. Specifically, LED-Merging has three steps, including Location, Election, and Disjoint Merging. For the Location, LED-Merging identifies critical neurons in both base and fine-tuned models using gradient-based attribution scores to avoid neuron misidentification. For the"}, {"title": "2 Related Work", "content": "Model merging combines multiple fine-tuned models into one without additional training, reducing storage and computational costs (Jiang et al., 2024; Ma et al., 2024; Du et al., 2024a; Huang et al., 2024). Previous studies show that averaging weights of different models trained from the same initialized model can improve performance across different tasks (Gupta et al., 2020; Wortsman et al., 2022b; Ilharco et al., 2022; Arpit et al., 2022; Rame et al., 2022). Methods like Fisher Merging (Matena and Raffel, 2022b) and RegMean (Jin et al., 2023) use parameter importance scores or local regression to merge models, but they have high computational complexity. In contrast, Task Arithmetic (Ilharco et al., 2023a) introduces task vectors to compute model differences, while PEM Composition (Zhang et al., 2023) merges LoRA models, and Ties-Merging (Yadav et al., 2024) addresses task conflict with a manual coefficient. Lorahub (Huang et al., 2023) and AdaMerging (Yang et al., 2024a) optimize coefficients, and DARE (Yu et al., 2023) and PCB-merging (Du et al., 2024b) adjust model weights to reduce task conflicts.\nIdentifying task-related regions in LLMs. Identifying task-related regions and neurons in models is crucial for understanding AI models (Tjoa and Guan, 2020; Liu et al., 2024a; Ren et al., 2024a; Dang et al., 2024). Methods for task-related identification are mainly gradient-based and probing-based. Gradient-based methods estimate the importance of weights via back-propagation gradients (Springenberg et al., 2015; Sundararajan et al., 2017; Shrikumar et al., 2017; Michel et al., 2019; Maini et al., 2023; Wang et al., 2023b; Wei et al., 2024a; Liu et al., 2024b). Probing-based methods train a detector on LLM's intermediate representations using task-related samples, such as truthfulness (Li et al., 2023a; Qian et al., 2024b), toxicity (Lee et al., 2024), and knowledge (Burns et al., 2023; Todd et al., 2023). However, these methods focus on single LLMs, failing to capture task-related regions across multiple LLM versions, such as base and instruct versions. This paper aims to identify task-related regions by considering multiple LLM versions.\nLLMs' safety. With the rapid development of LLMs, safety concerns of LLMs in different dimensions (e.g., reliability, toxicity, privacy, and fairness) have attracted a lot of attention (Liu et al., 2023; Wang et al., 2023a; Sun et al., 2024a; Mazeika et al., 2024; Xie et al., 2024; Ren et al., 2024b). To align the LLM with human value, numerous post-training methods have been proposed, including supervised fine-tuning (SFT) (Zong et al., 2024; Hu et al., 2024), reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Zhang et al., 2024b), direct preference optimization"}, {"title": "3 Methodology", "content": "Preliminaries. We primarily focus on the homologous model merging, in which all  \u03b8i come from the same base model \u03b8base. Given K tasks {T1, T2,\u2026,TK} and K corresponding fine-tuned models with parameters {\u03b81, \u03b82,\u00b7\u00b7\u00b7, \u03b8\u03ba}, model merging aims to combine K fine-tuned models into one single model simultaneously performing on {T1, T2,\uff65\uff65\uff65,TK} without post-training (Choshen et al., 2022; Matena and Raffel, 2022a). Task vector (Ilharco et al., 2023b; Yang et al., 2024b) is a key element in merging method which could enhances the base model's ability or enable the model to handle other tasks. Specifically, for task Ti, the task vector \u03c4i \u2208 RD is defined as the vector obtained by subtracting the SFT weights \u03b8i from the base model weight \u03b8base, i.e., \u03c4i = \u03b8i \u2014 \u03b8base. The merged model could be denoted as \u03b8m = \u03b8base + \u03a3i\u03bb\u03af\u03c4i, which \u03bbi is the scaling factor measuring the importance of task vector. For clarification, we also denote the neuron set in \u03b8i as Ni, the neuron set in Ti as Ti.\nLED-Merging: Location, Election, and Disjoint Merging To address the neuron misidentification and interference issues in existing model merging methods, we propose LED-Merging (Location, Election, and Disjoint Merging). Specifically, previous studies (Jang et al., 2024; Ilharco et al., 2023b; Yadav et al., 2023) fail to accurately identify safety-related neurons in task vectors with a single magnitude score, namely neuron misidentification. Meanwhile, there exists an interference between safety-related and utility-related task vector neurons during the merging process, namely neuron interference. To address neuron misidentification, we first locate important neurons both in the base and fine-tuned models and then elect neurons from the task vector considering these two scores together. Subsequently, to mitigate the interference, we introduce a disjoint step, isolating these important neurons so that they influence different base neurons. The whole process is illustrated in Figure 2."}, {"title": "Location.", "content": "We first calculate importance scores for each neuron in a base/fine-tuned model. Given a location dataset Xi = {(x,y)k}, where x is the question and y is the answer, we calculate the importance scores for the weight \u03b8i \u2208 RD in any layer as follows (Lee et al., 2019; Frantar and Alistarh, 2023; Sun et al., 2024b):\nI(\u03b8i) = Ex~Xi[\u03b8i\u2207\u03b8iL(x)], (1)\nwhich L(x) = - log p(y | x) is the conditional negative log-likelihood loss. We choose the SNIP score (Lee et al., 2019) because it balances computational efficiency and performance (Qian et al., 2024a). Please refer to Sec. 4.3 for the comparison between different location methods. After computing importance scores, we choose top-ri neurons as the important neuron subset Ni from I(\u03b8i).\nElection. A natural question is how to select important neurons in the task vector Ti based on I(\u03b8base) and I(\u03b8i). The important neurons in the base model may be different from neurons in the fine-tuned model. Therefore, we introduce the following election strategy to select neurons with high scores in both base and fine-tuned models:\nTri = Ni \u2229Nbasei  (2)\nRemark. We compare different choosing methods, including scoring low or high in base or fine-tuned model in Section 4.3 and find that Equation 2 achieves the best performance."}, {"title": "Disjoint.", "content": "As important neurons from different task vectors may conflict with each other at the same position, we use the set difference to disjoint the neurons from others to prevent interference:\nDisjoint(T/'^*) = T/** - U J\u20ac[K],|J|\u22652 (TJ\u2229j\u20acJT ) (3)\nNext, we construct a mask mi \u2208 RD to implement disjoint in the merging process. Specifically, this mask m\u2081 is used to select neurons from T. The mask ratio is ri, where r \u2208 (0,1]. The mask mi can be derived from:\nmi,d = { 1, if d\u2208 Disjoint(T), 0, otherwise. (4)\nMerging. The final merged task vector \u012bm is as follows:\n\u03c4m = \u03a3 \u03bb\u03af\u03c4i \u2299 m. (5)\ni=1\nWe summarize the workflow in Algorithm 1."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nBaselines. We compare LED-Merging with multiple merging methods: Model Stock (Jang et al., 2024), Model Breadcrumbs (Davari and Belilovsky, 2024), Task Arithmetic (Ilharco et al., 2023b), Ties-Merging (Yadav et al., 2023). Please see Appendix A.1 for more discussions.\nDatasets & Metrics. We assess safety-utility trade-offs through three pillars: (1) Safety via HarmBench (Mazeika et al., 2024) and SORRY-Bench (Xie et al., 2024) (Attack Success Rate, ASR\u2193). (2) Mathematical reasoning using GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) (Accuracy\u2191 with chain-of-thought. (3) Code Generation evaluated by MBPP (Austin et al., 2021) and HumanEvalPack (Muennighoff et al., 2023) (Pass@1\u2191). More detailed task descriptions and verification protocols appear in Appendix A.2\nModels. We evaluate three model families: (1) Llama-3 (8B base/instruct/math/code variants), (2) Wizard-LM (13B base/instruct/math/code), and (3) Mistral (7B base/instruct/math). All models use base architectures paired with safety-aligned or task-specialized versions. Further Details of SFT models are in Appendix A.3."}, {"title": "4.2 Main Results", "content": "LED-Merging presents superior safety capacity. LED-Merging achieves SOTA safety performance across all evaluated benchmarks, surpassing both existing merging methods and even the original safety-aligned models. On HarmBench, merging safety-aligned and code-specialized models for Llama3-8B reduces the ASR to 14.75%, a 75.9% improvement over the standalone code model (ASR=61.25%) and a 31.4% enhancement compared to the original LM model (ASR=21.50%). This indicates that LED-Merging not only mitigates conflicts but actively strengthens safety through gradient-informed neuron election. Similar trends hold for Mistral-7B, where merging safety and math models achieves ASR=16%, outperforming Task Arithmetic (ASR=55.75%) and Ties-Merging (ASR=62%) while surpassing the original Mistral-7B-Instruct (ASR=54.75%) by 70.8%. For larger models like Llama2-13B, merging multiple specific fine-tuned models maintains an exceptionally low ASR=4%, significantly better than both baselines (Task Arithmetic: 35%) and the standalone safety model (28.25%), proving its capacity to resolve cross-task interference at scale.\nLED-Merging preserves utility performance with safety alignment. Beyond superior safety-alignment performance, LED-Merging maintains comparable utility performance to specialized models and merging baselines across mathematical reasoning and code generation tasks. When merging Llama3-8B's safety-aligned model with its math-specialized counterpart, our method retains 52.39% accuracy on GSM8K\u2014significantly outperforming Task Arithmetic (13.12%) and closely matching Ties-Merging (53.01%)\u2014while preserving 66.3% of the math-specialized model's capability (79.00%). Similar advantages emerge in code generation, where merging safety and code models yields 47.2% MBPP Pass@1 (40.2% higher than the code-specialized model's 33.6%), demonstrating effective preservation of specialized capabilities. Crucially, for safety, math, and code multi-task"}, {"title": "LED-Merging demonstrates cross-architecture robustness.", "content": "LED-Merging demonstrates consistent effectiveness across distinct model architectures, including Llama-2, Llama-3, and Mistral families. For Llama3-8B, merging safety-aligned and math-specialized models preserves 52.39% GSM8K accuracy while maintaining 20.75% ASR. Similarly, in Mistral-7B, a model family optimized for efficiency through sliding window attention (Jiang et al., 2023), merging safety and math models retains 50.34% GSM8K accuracy with 16% ASR on HarmBench, proving compatibility with diverse architectural designs.\nLED-Merging presents model-scale agnosticism. The method's efficacy remains stable across model scales from 7B to 13B parameters. For smaller"}, {"title": "4.3 Ablation Study", "content": "Different location methods. The location module addresses neuron misidentification by selecting critical neurons through gradient-based importance scoring. Tab. 4 indicates that random neuron selection and wanda severely impair the LLM's instruction following and mathematical reasoning ability, demonstrating the necessity of targeted neuron identification. SNIP achieves optimal balance, reducing HarmBench ASR to 16.00%, while maintaining 50.34% GSM8K accuracy. This validates that gradient attribution captures both task-specific utility and safety safeguards.\nDifferent election type. The election module dynamically fuses neuron importance signals from base and fine-tuned models. Tab. 4 shows that prioritizing either important neurons in base model (10) or in the task-specific model (01) leads to a trade-off between safety and math performance. Specifically, 10 denotes only electing important neurons in the base model and 01 only elects important neurons in the fine-tuned model. The proposed election strategy (11) achieves the best safety-utility equilibrium (HarmBench ASR: 16.00%, GSM8K: 50.34%).\nEffects of disjoint merging. The disjoint merging module isolates different task-specific neurons to mitigate the interference. Tab. 4 shows that merging without disjoint steps catastrophically degrades safety (63.00% ASR on HarmBench), despite improved GSM8K performance (72.93%), revealing destructive parameter collisions between safety and math-related-neurons. Enabling disjoint merging restores safety ability (ASR 16.00%) while maintaining reasonable utility (GSM8K: 50.34%). Such experimental results verify the disjoint merging"}, {"title": "4.4 Hyperparameter Analysis", "content": "LED-Merging's robustness stems from its ability to balance safety-utility trade-offs through two key hyperparameters: mask ratios ri controlling neuron retention, and scaling factors \u03bbi governing task vector contributions. Experiments on Mistral-7B reveal distinct operational regimes and design principles.\nMask ratio dynamics. As shown in Fig. 3a, varying LM(safety) and Math (utility) reveals three critical regimes. In safety-centric mode (ri \u2264 0.3), prioritizing base model neurons(rlm = 0.1) minimizes ASR to 7.75%, but suppresses math capabilities (42.38% accuracy). Conversely, utility-centric mode(ri \u2265 0.5) maximizes accuracy on GSM8K to 53.68% by retaining task-specific neurons, yet compromises safety (ASR > 25%). The Pareto-optimal regime (ri = 0.3 \u2013 0.5, labeled by white dashed line) strikes a balance. When rLM, Math = 0.5, 18.75% ASR and 44.81% accuracy are achieved through spatially disjoint neuron updates, confirming that moderate ratios maximize conflict-free parameter fusion.\nScaling factor trade-offs. Scaling factors \u03bbi dictate the dominance hierarchy between safety and utility gradients, shown in Fig. 3b. Amplifying safety contributions (\u03bbLM \u2265 0.7) suppresses harmful behaviors but over-penalizes mathematical ability. Prioritizing utility boosts accuracy on GSM8K, yet reintroduces safety risks. The equilibrium configuration, labeled by a star marker, achieves 11% ASR and 49.66% accuracy, demonstrating a balanced task coexistence."}, {"title": "5 Conclusion", "content": "In this paper, we propose LED-Merging, a training-free framework to address the critical safety-utility conflicts inherent in model merging for LLMs. By integrating gradient-based neuron localization, dynamic importance election, and parameter space isolation, our method achieves robust safety alignment, LED-Merging achieves robust safety alignment (e.g., 75.9% reduction in harmful responses for code-specialized models) while preserving task performance (e.g. 52.39% GSM8K accuracy). Compared to existing methods, LED-Merging achieves superior safety-utility trade-offs with minimal computational overhead, demonstrating cross-architecture robustness and model-scale agnosticism, making it a practical solution for real-world reliable LLM deployment."}, {"title": "6 Limitations and Future Work", "content": "While our focus is on homologous model merging, extending this framework to heterogeneous architectures (e.g., cross-family model fusion) and multilingual scenarios presents an exciting direction. Additionally, exploring tokenization divergence effects in multilingual models could uncover new safety-utility dynamics. We advocate for community efforts to establish standardized benchmarks for merged model evaluation, ensuring transparency and reproducibility in this rapidly evolving field."}, {"title": "7 Broader Impact and Ethics Statement", "content": "This research tackles the pivotal challenge of balancing safety alignment and functional utility in large language models (LLM) merging techniques. Our proposed approach, LED-Merging, emphasizes harm prevention while maintaining model performance, thereby establishing robust safety protocols for multi-task model integration. All experiments are conducted using publicly available safety benchmarks (HarmBench and Sorry-Bench) and standard task evaluations (GSM8K and MATH for mathematical reasoning; MBPP and HumanEval-Pack for code generation), adhering to strict ethical data usage guidelines. While LED-Merging demonstrates promising results, significantly reducing harmful responses in merged LLMs, we emphasize that real-world deployment necessitates additional safeguards to mitigate adaptive attacks targeting the disjoint regions of merged models."}]}