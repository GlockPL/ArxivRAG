{"title": "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "authors": ["G M Shahariar", "Jia Chen", "Jiachen Li", "Yue Dong"], "abstract": "Recent studies show that text-to-image (T2I) models are vulnerable to adversarial attacks, especially with noun perturbations in text prompts. In this study, we investigate the impact of adversarial attacks on different POS tags within text prompts on the images generated by T2I models. We create a high-quality dataset for realistic POS tag token swapping and perform gradient-based attacks to find adversarial suffixes that mislead T2I models into generating images with altered tokens. Our empirical results show that the attack success rate (ASR) varies significantly among different POS tag categories, with nouns, proper nouns, and adjectives being the easiest to attack. We explore the mechanism behind the steering effect of adversarial suffixes, finding that the number of critical tokens and content fusion vary among POS tags, while features like suffix transferability are consistent across categories. We have made our implementation publicly available at https://github.com/shahariar-shibli/\nAdversarial-Attack-on-POS-Tags.", "sections": [{"title": "Introduction", "content": "Text-to-Image (T2I) generation models such as Stable Diffusion (Rombach et al., 2022; Podell et al., 2023), DALL-E2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022), ediff-i (Balaji et al., 2022) have made steady progress in the field of image generation by bridging the semantic gap between textual descriptions and visual representations. Unlike traditional methods reliant solely on pixel manipulation, these models leverage multi-model alignments in latent spaces to interpret and synthesize complex visual content from textual prompts. Recent studies, such as Tang et al. (2023), have interpreted how cross-alignment from texts to images is transformed through text-image attribution analysis, demonstrating that different POS tags are well captured by cross-modal attention during synthesis.\nOn the other hand, recent research shows that T2I models are vulnerable to adversarial perturbations in text prompts, such as inserting nonsensical words (Milli\u00e8re, 2022), phrases (Maus et al., 2023), or irrelevant characters (Zhuang et al., 2023), which can significantly bias the generated images (Chefer et al., 2023; Salman et al., 2023). However, current adversarial attacks on T2I generation models, either manual heuristic-based methods (Zhuang et al., 2023; Gao et al., 2023; Maus et al., 2023) or automatic gradient-based approaches (Zhuang et al., 2023; Liang et al., 2023; Liu et al., 2023; Shahgir et al., 2023; Yang et al., 2024a,b; Du et al., 2024; Zhai et al., 2024), are specifically targeting entities"}, {"title": "Related Work", "content": "Text-to-Image Diffusion Models. Nichol et al. (2021) formalized the initial text-to-image (T2I) diffusion model (GLIDE) that substituted class labels with text in class-conditioned diffusion models (i.e. Ablated Diffusion Models (Dhariwal and Nichol, 2021)). The authors explored two types of text conditioning methods: classifier guidance"}, {"title": "Dataset Creation", "content": "In this section, we outline the procedure for constructing our dataset. We first specify the dataset source and then describe the steps involved in its construction.\nData Collection. The first obstacle we encountered in evaluating adversarial attacks across different POS categories beyond nouns was that there was no existing dataset for fair comparison.  To construct our dataset, we chose MS-COCO (Lin et al., 2014) as the data source for its diverse and complex captions making it suitable for testing the robustness of SD. In the train split of MS-COCO, each image has five captions. We collected only the first caption among the five resulting in 118,287 rows.\nInput Prompts Selection. We identified the POS tags in each caption from the initially collected data using the NLTK library (Bird, 2006) and a pre-trained POS tagging model (Sajjad et al., 2022). We only focused on six parts of speech tags: noun, verb, adverb, adjective, numeral, and proper noun. For each POS tag, we then randomly selected 20 unique captions, each containing at least one word from the corresponding POS tag, to be used as input prompts.\nTarget Prompts Generation. For each input prompt of every POS tag, we generated five target prompts, resulting in 100 prompt pairs per POS tag. Each input and target prompt differed by only one word, with the target words chosen from a pool of candidate words. The process of generating target prompts starts by extracting the POS-tagged word from the input prompt using the same NLTK library and pre-trained POS tagging model employed during the input prompt selection. Then, we compile a set of candidate words by gathering other words of the same POS category, identifying antonyms to introduce variety, [MASK] prediction to acquire the top-5 words, and exploring the CLIP token embedding space to find the top-k distant neighbors of the word. To extract antonyms, we use the"}, {"title": "Experiment", "content": "In this section, we outline the gradient-based adversarial attack method, describe the experimental setup, and report the results to assess the effectiveness of the attack.\n4.1 Attack Method\nGradient-based attacks on Stable Diffusion (Zhuang et al., 2023; Shahgir et al., 2023; Yang et al., 2024a,b; Du et al., 2024) utilize the gradient information to perturb the input prompt in a way that maximizes the divergence from the intended output, effectively manipulating the image synthesis process. While previous studies have predominantly focused on nouns, our analysis extends this approach to other parts of speech by applying the gradient-based attack framework proposed by (Shahgir et al., 2023). The process of such an attack on T2I models generally starts with an initial prompt, which is modified iteratively to create an adversarial prompt that maximizes a predefined score function. This involves embedding the target prompt and the adversarial prompt using a token embedder and processing them through a text encoder. The core mechanism focuses on creating multiple candidate prompts by replacing tokens and computing the top-k token candidates. The best candidate prompt, which maximizes the score function, is selected, and the gradient of the loss function concerning the adversarial prompt is used to iteratively refine the prompt. This iterative optimization adjusts the adversarial prompt to gradually increase the discrepancy between the model's output for the target prompt and the adversarial prompt, effectively fooling the T2I generation model. Further details of the attack are provided in Appendix B. We conducted the targeted attack under two distinct settings: with and without restrictions. In the unrestricted setting, we"}, {"title": "Experimental Setup", "content": "We followed the setup of Shahgir et al. (2023) and conducted the attack five times for each pair, with 100 steps per run, employing 10 adversarial tokens. For each step, we selected the top 256 tokens as candidate tokens and generated 512 new prompts by randomly substituting tokens using these candidates. Subsequently, we generated seven images per attack, resulting in the evaluation of a total of 21,000 generated images (600 pairs, 5 runs, and 7 images per run). During image generation, we set the resolution to 512 \u00d7 512, the number of inference steps to 50, and the scale of classifier-free guidance to 7.5. As the victim model, we utilized Stable Diffusion v1.5 (SD v15) for both image generation and performance assessment, leveraging a pre-trained CLIP model trained on a dataset comprising text-image pairs. All experiments (attack execution, evaluation, and image generation) were conducted using a single Nvidia RTX 3090 GPU, totaling approximately 600 GPU hours. The execution time to attack a single input-target prompt pair is approximately 8 minutes."}, {"title": "Evaluation Metrics", "content": "Attack Success Rate. We consider an attack successful if the image generated by the adversarial prompt matches the target text; otherwise, we consider it unsuccessful. Since we generate 7 images per adversarial prompt, to measure the attack success rate (ASR), we consider the attack as successful if at least 4 images have a higher matching score than a threshold. Following (Shahgir et al., 2023), we set this threshold value at 3.41. We determine the matching score by calculating the difference between the CLIP score of the input prompt and the generated image, and the CLIP score of the target text and the generated image. CLIP score measures the cosine similarity between the visual CLIP embedding of an image and the textual CLIP embedding of a text. For each input-target prompt pair, we run the attack five times, generating five adversarial prompts, and consider the attack successful if at least one of them succeeds.\nSemantic Shift Rate. For a quantitative measure to evaluate the efficacy of adversarial suffix tokens, we utilized SemSR (Semantic Shift Rate) (Zhai et al., 2024) which measures the semantics between a generated image and a text prompt. SemSR utilizes CLIP's multi-modal embedding space and computes the similarity in semantics between a generated image and a prompt using cosine similarity. This metric quantifies the displacement in the vector space of the generated image after appending adversarial suffix tokens compared to the image generated using the input prompt. Since the amount of deviation necessary to attain diverse target semantics differs, it is adjusted by the maximum deviation. The SemSR equation is provided below:\n$SemSR = \\frac{CS(E_{Ia}, E_{Pa}) \u2013 CS(E_{I_i}, E_{P_i})}{CS(E_{I_t}, E_{P_t}) \u2013 CS(E_{I_i}, E_{P_i})}$\nwhere CS denotes CLIP_Score, $I_a$ represents the generated image from the adversarial prompt $P_a$, $I_i$ denotes the generated image from the input prompt $P_i$, and $I_t$ denotes the generated image from the target prompt $P_t$. For a single input-target prompt pair, we measure the average of SemSR scores over five runs."}, {"title": "Results", "content": "In Figure 11, we showcase a few examples of images generated through both the unrestricted and restricted attack methods.  Table 2 displays the average attack success rate (ASR) and average semantic shift rate (SemSR) over all the prompt pairs for each POS tag under both attack conditions. Below, we present both quantitative analysis and human evaluation of our experiments."}, {"title": "Attack Success Mechanism", "content": "In this section, we explore the mechanism behind the steering effect of adversarial suffixes. We identify (a) features that vary across POS categories and explain differences in attack success rates (ASR), such as the number of critical tokens and content fusion, and (b) features that are consistent across"}, {"title": "Limitations", "content": "We utilized the Stable Diffusion model for the gradient-based attack. It is important to note that the attack approach might not generalize effectively to other closed-source T2I generation models like Imagen (Saharia et al., 2022) or DALL-E2 (Ramesh et al., 2022), owing to differences in architecture, text encoder, and training data. Moreover, the metrics utilized in this study to assess the attack may not fully capture the visual plausibility or semantic accuracy of images after the attack. We evaluated the attack only on six specific POS tags, which may not encompass all possible scenarios, such as prepositions, conjunctions, interjections, articles, and determiners. Furthermore, the approach relies on appending suffix tokens to the original prompt, which may not always be the most optimal method for manipulating the image generation process, considering the T2I model's sensitivity to the order of tokens. As the appended adversarial suffix tokens may lack meaning, the resulting adversarial prompt found by the attack methods exhibits reduced naturalness."}, {"title": "Preliminaries of Stable Diffusion", "content": "Stable diffusion is a latent diffusion model comprising three key components: a Variational Autoencoder (VAE), a UNet, and a CLIP text encoder (Radford et al., 2021) for conditioning. The VAE consists of an encoder E and a decoder D, where the encoder compresses an image y into a lower-dimensional latent space representation E(y), while the decoder reconstructs the image from the latent space \u1ef3 = D(E(y)). During the T2I generation process, at first CLIP tokenizer to-kenizes a text prompt into a sequence of tokens W = {W1, W2, ....Wn}, ensuring uniform length by"}, {"title": "Details of Adversarial Attack", "content": "Adversarial Prompt Generation. We start the process by considering the input prompt as the adversarial prompt. Subsequently, we extract the embeddings for both the adversarial prompt tokens and the target prompt tokens. These embeddings\nare then fed into the CLIP text encoder to obtain the\nfinal hidden state representations. Following this,\nwe compute the loss using a loss function and calcu-\nlate gradients with respect to one hot token vector\nto determine the top-k candidate tokens for substitu-\ntion. Then, we generate several candidate prompts\nby randomly replacing multiple tokens of the ini-\ntial adversarial prompt from the pool of candidate\ntokens. The candidate prompt that maximizes the\nscore function is chosen as the adversarial prompt\nfor the subsequent optimization step. This iterative\nprocess continues for a set number of iterations un-\ntil a final adversarial prompt is obtained. Notably,\nthis attack method relies only on the text encoder\nand does not necessitate access to the image gen-\neration model. An illustration of the adversarial\nattack is depicted in Figure 3. From the adversary\u2019s\nviewpoint, the concatenated suffix tokens in the ad-\nversarial prompt should be nonsensical to humans\nyet encode specific semantics predetermined by the\nadversary.\nLoss Function. The attack focuses on manipu-\nlating the CLIP embedding space to optimize a\nscore function, which quantifies how much the ad-\nversarial token embeddings at an intermediate op-\ntimization stage deviate towards the target token\nembeddings using cosine similarity. The objective\nis to steer away from the embeddings of input to-\nkens and progressively approach those of the target\ntokens by discovering more effective adversarial\ntokens. This process of maximizing the score func-\ntion is similar to Shahgir et al. (2023). To compute\nthe loss, we adopt the negated score function. Max-\nimizing the score is equivalent to minimizing the\nloss.\nGradient-based Search. The attack employs an\neffective greedy coordinated gradient-based search\nalgorithm (Zou et al., 2023), utilizing the loss func-\ntion discussed above. At each optimization step,\nthe algorithm selects k tokens with the highest neg-\native loss and computes gradients with respect to"}, {"title": "Vulnerabilities Observed across POS Tags", "content": "In this section, we present some vulnerabilities ob-served on Stable Diffusion across a few POS tags.We noticed that the SD model inherently faces diffi-culty generating images from prompts that includenumerals. Specifically, the model struggles to pro-duce images with a precise count of identical ob-jects. For instance, if the prompt is to generate animage of five birds, the SD model will fail to createexactly five birds and instead produce images witha random number of birds, such as three, four, ormore than five. Examples of this issue are shownin Figure 5. Images generated by the model usingprompts where the adverb tokens have shared lin-guistic structures, close semantic representation inthe feature space, and unrelated to emotions gen-erally have minimal impact on visual output. Weshow such an example in Figure 4(a). In this ex-ample, substituting \u201cbeautifully\u201d with \u201cpartly\u201d in"}, {"title": "Impact of Semantic Distance on Attack Success", "content": "In this section, we examine why certain POS tagsare easier to attack by considering the impact ofsemantic distance. To explore this, we plotted textembeddings of all the data across six POS tagsfrom the dataset in Figure 6 using t-DistributedStochastic Neighbor Embedding (t-SNE) (Van derMaaten and Hinton, 2008). Nouns, proper nouns,and adjectives show clear clustering with visibledistances between markers, indicating considerabledifferences in their text embeddings. This distanceallows the gradient algorithm to minimize the gapfrom input to target, making attacks on these POS"}]}