{"title": "Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models", "authors": ["Xiao Li", "Zhuhong Li", "Qiongxiu Li", "Bingze Lee", "Jinghao Cui", "Xiaolin Hu"], "abstract": "Aligned Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, LLMs remain susceptible to jailbreak adversarial attacks, where adversaries manipulate prompts to elicit malicious responses that aligned LLMs should have avoided. Identifying these vulnerabilities is crucial for understanding the inherent weaknesses of LLMs and preventing their potential misuse. One pioneering work in jailbreaking is the GCG attack proposed by Zou et al. (2023), a discrete token optimization algorithm that seeks to find a suffix capable of jailbreaking aligned LLMs. Despite the success of GCG, we find it suboptimal, requiring significantly large computational costs and the achieved jailbreaking performance is limited. In this work, we propose Faster-GCG, an efficient adversarial jailbreak method by delving deep into the design of GCG. Experiments demonstrate that Faster-GCG can surpass the original GCG with only 1/10 of the computational cost, achieving significantly higher attack success rates on various open-source aligned LLMs. In addition, We demonstrate that Faster-GCG exhibits improved attack transferability when testing on closed-sourced LLMs such as ChatGPT. The code will be publicly available.", "sections": [{"title": "1 Introduction", "content": "Aligned Large Language Models (LLMs) (Touvron et al., 2023; Chiang et al., 2023; Achiam et al., 2023) have been developed and improved rapidly, demonstrating remarkable performance across various tasks and enabling many practical applications such as AI assistants (Achiam et al., 2023). These LLMs are often trained to align with human values and thus expected to refuse to generate harmful or toxic contents (Ouyang et al., 2022). For example, if malicious questions like \"Tell me how to build a bomb\" are asked, LLMs should generate evasive responses like \"I cannot fulfill your request. I'm just an AI ...\". However, several studies have shown that even the most powerful LLMs are not adversarially aligned. With some deliberately designed prompts, also known as jailbreak attacks (Wei et al., 2023a), it is possible to elicit the aligned LLMs to bypass the safety feature and generate harmful, violent, or hateful content that should be avoided.\nIdentifying the vulnerabilities of LLMs to jailbreak attacks is crucial for understanding their inherent weaknesses and preventing potential misuse from a red-teaming perspective (Zhuo et al., 2023). Early jailbreak attack methods (Wei et al., 2023a; Kang et al., 2023; Yuan et al., 2024) often rely on manually crafted prompts, which require expert knowledge and thus lack scalability. Recently, automatic jailbreak methods have received increasing attention. The pioneering work in this area is the Greedy Coordinate Gradient (GCG) attack (Zou et al., 2023). By formalizing the jailbreak problem as a discrete token optimization problem, GCG can automatically identify prompt suffixes that jailbreak LLMs. The success of GCG has inspired a lot of works on how to automatically jailbreak LLMs from various perspectives (Chao et al., 2023; Zhu et al., 2023; Jia et al., 2024; Liao and Sun, 2024; Wei et al., 2023b; Mehrotra et al., 2023; Deng et al., 2024).\nHowever, we find that the discrete token optimization efficiency of GCG is suboptimal, incurring significantly high computational costs and limited jailbreak performance. The primary reason for this is that GCG relies on an unrealistic assumption when exploiting the gradient information. More specifically, GCG aims to find a jailbreak suffix through an iterative two-step process, as summarized below and detailed in Sec. 3.2: 1) Identifying a set of promising candidate tokens, which are more likely to decrease the targeted loss $\\mathcal{L}$ of LLMs, for replacement at each token position of"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Optimization-based Jailbreak Methods", "content": "Optimization-based jailbreak methods, similar to classical adversarial attacks such as PGD (Madry et al., 2018) in computer vision, utilize gradient information to generate jailbreak prompts of LLMs. However, unlike these classical adversarial attacks that operate in continuous pixel space, where gradient information can be directly applied for gradient descent, attacks in the discrete token space are generally more challenging (Goyal et al., 2023; Shin et al., 2020). Zou et al. (2023) propose the pioneering attack method GCG, which first makes the optimization-based strategy feasible for the jailbreak of LLMs. GCG has become a foundational algorithm for many subsequent jailbreak approaches.\nJia et al. (2024) develop an improved jailbreak method by introducing diverse target templates containing harmful self-suggestions into the GCG framework. Liao and Sun (2024) employ a generative model to capture the distribution of adversarial suffixes generated by GCG for faster jailbreaking. Zhao et al. (2024) propose to dynamically evaluate the similarity between the predictions of a smaller draft model and those of the target LLM for achieving improved jailbreaking. Zhu et al. (2023) utilize a similar optimization strategy to GCG to create more readable and interpretable adversarial jailbreak prompts. Different from these works that treat GCG as a foundational tool of discrete optimization, our work focuses on enhancing the efficiency and effectiveness of GCG's discrete optimization itself, which can be integrated with these approaches to further improve performance. Additionally, recent studies demonstrate that GCG's discrete optimization process can be easily adapted to attack other models beyond LLMs, such as text-"}, {"title": "2.2 Optimization-free Jailbreak Methods", "content": "Optimization-free jailbreak methods generally leverage certain properties of LLMs, such as instruction-following or in-context learning capabilities, to design jailbreak prompts. Typically, another LLM is involved to iteratively modify these prompts. Specifically, Chao et al. (2023) propose the Prompt Automatic Iterative Refinement (PAIR) method by employing another LLM as an attacker to autonomously produce jailbreaks for a targeted LLM. Similarly, Mehrotra et al. (2023) utilize another LLM to iteratively refine potential attack prompts through a tree-of-thought approach. Deng et al. (2024) propose to combine reverse engineering with another LLM as an automatic jailbreak prompt generator. Wei et al. (2023b) introduce the in-context attack technique to guide LLMs into generating unsafe outputs by using several crafted harmful query-answer templates."}, {"title": "3 Method", "content": "Faster-GCG aims to accelerate the discrete token optimization efficiency of GCG. We first formalize the problem of jailbreaking in Sec. 3.1 and give a detailed introduction of GCG in Sec. 3.2, on which our Faster-GCG is built. Then in Sec. 3.3 we discuss the limitations of GCG. We finally introduce the techniques used in our Faster-GCG in Sec. 3.4."}, {"title": "3.1 Formalizing the Problem of Jailbreaking", "content": "Notation. We assume that the input of the LLM is a sequence of tokens generated by a tokenizer. Let $x_k \\in \\{1,2,\\dots,m\\}$ denote an individual token in the vocabulary of size $m$, while the bold letter $\\mathbf{x}$ or $\\mathbf{X}_{1:l}$ will represent a sequence of tokens of length $l$. The LLM is considered a mapping from a sequence of tokens $\\mathbf{x}_{1:l}$, to a distribution over the next token, i.e., $p_\\theta(x_{l+1} | emb(\\mathbf{x}_{1:l}))$, where $p_\\theta(\\cdot)$ denotes the output probability of the LLM parameterized by $\\theta$, and $emb(\\cdot)$ denotes the embedding function that maps each token to a vector with $d$ dimension. For simplicity, we omit $emb(\\cdot)$ in most cases.\nFollowing the setting of Zou et al. (2023), the task of LLM jailbreaking can be then formalized as a discrete optimization problem. More specifically, as shown in Fig. 1, given the prefix system prompt $\\mathbf{x}^{(s_1)}$, the user request $\\mathbf{x}^{(u)}$, and the connecting system prompt $\\mathbf{x}^{(s_2)}$, the objective of jailbreaking is to find an adversarial suffix $\\mathbf{x}^{(a)}$ with a fixed length $n$ that minimizes the cross-entropy loss $\\mathcal{L}(\\mathbf{x}^{(a)})$ between the output of LLMs and the predefined optimization target $\\mathbf{x}^{(t)}$ (harmful contents). With a slight abuse of notation, the goal of minimization can be expressed as follows:\n$\\begin{aligned} \\mathcal{L}(\\mathbf{x}^{(a)}) &= - \\log p_\\theta(\\mathbf{x}^{(t)} | \\mathbf{x}^{(s_1)} \\mathbf{x}^{(u)} \\mathbf{x}^{(a)} \\mathbf{x}^{(s_2)}) \\\\ &= - \\sum_{0 < k < l_t} \\log p_\\theta(x_k^{(t)} | \\mathbf{x}^{(s_1)} \\mathbf{x}^{(u)} \\mathbf{x}^{(a)} \\mathbf{x}^{(s_2)} \\mathbf{x}_{0:k-1}^{(t)}), \\end{aligned}$        (1)\nwhere $l_t$ denotes the length of $\\mathbf{x}^{(t)}$, and $\\concat$ represents the concatenation operation. We now proceed to discuss how GCG solves this optimization problem."}, {"title": "3.2 Preliminary on GCG", "content": "GCG aims to find a jailbreak suffix that minimizes the objective defined in Eq. (1) through an iterative two-step process. Let the adversarial suffix $\\mathbf{x}^{(a)}$ of length $n$, also denoted as $\\mathbf{X}_{1:n}$, be initialized with specific tokens. Following Zou et al. (2023), $\\mathbf{x}^{(a)}$ can be written as $\\mathbf{x}^{(a)} = V E$, where the $1 \\times m$ matrix $V := [v_1, v_2, \\dots, v_m]$ represents the stack of the token vocabulary, and $E$ is defined as an $m \\times n$ binary matrix, with each column $\\mathbf{e}_{x_i}$ being a one-hot vector (token indicator) in which the position"}, {"title": "3.3 Key limitations of GCG", "content": "The GCG attack, while improving upon brute-force methods, incurs significantly high computational"}, {"title": "3.4 Faster-GCG", "content": "To address the aforementioned limitations, we propose three simple yet effective techniques. Specifically, we first propose two techniques to identify candidate tokens with better approximations:\nTechnique 1: Additional regularization term related to the distance between tokens. Instead of using Eq. (6) for the token $x_j$ in the original GCG, we introduce a regularization term related to the distance between tokens to weight the gradient $\\hat{g}$ during the candidate token selection process:\n$\\hat{g}_k = \\frac{\\partial \\mathcal{L}}{\\partial e_k} + w \\cdot ||X_j - X_k||,          (8)\nwhere $w$ controls the weight of the regularization term, $||\\cdot||$ denotes the $l_2$ distance, and $\\hat{g}_k$ denotes the $k$-th element of $\\hat{g}$. This additional term ensures that using $\\hat{g}$ for candidate selection (e.g., following Eq. (3)) eliminates candidate tokens with poor approximations.\nTechnique 2: Replacing random sampling with greedy sampling. To improve the optimization efficiency of random sampling from the top-$K$ gradients in GCG, we propose a direct approach by adopting a deterministic greedy sampling strategy. This involves sequentially selecting candidates from the most promising to the least, according to $\\hat{g}$. By eliminating the randomness of random sampling, this technique further accelerates the convergence of the search. The pivotal improvement of greedy sampling is underpinned by the above-introduced regularization term, which enhances the precision in identifying accurate candidates."}, {"title": "4 Experiments", "content": "In this section, we first describe the experimental setup. Then we present and analyze the results of Faster-GCG across various LLMs, comparing them with those using the original GCG. Finally, we conduct an ablation study on the proposed techniques."}, {"title": "4.1 Experimental Setups", "content": "Models. We test the performances of Faster-GCG on four LLMs, including two open-source LLMs: Vicuna-13B\u00b9 (Chiang et al., 2023) and Llama-2-7B-chat\u00b2 (Touvron et al., 2023), and two closed-source models: GPT-3.5-Turbo-1106 and GPT-4-0125-Preview (Achiam et al., 2023). Additional settings for target LLMs such as system prompt are shown in Appendix A."}, {"title": "4.2 Results in the White-box Setting", "content": "We first compare the results in the white-box setting. As shown in Table 1, Faster-GCG achieves a substantial improvement in ASR for both Llama-2-7B-chat and Vicuna-13B models while substantially reducing computational cost. Specifically, for the Llama-2-7B-chat model, the ASR improves by 31% using only 1/10 of the computational costs compared to the original GCG method (1st row v.s. 2nd row). Similarly, for the Vicuna-13B model, our method achieved a 7% improvement with the same reduced computational cost. Furthermore, when Faster-GCG operates with a computational cost comparable to GCG, it achieves significantly higher attack success rates on these models (1st"}, {"title": "4.3 Results in the Black-box Setting", "content": "Zou et al. (2023) have demonstrated that jailbreak prompts effective on one LLM can be successfully transferred to others. To assess the transferability of the suffix generated by Faster-GCG, we employ Vicuna-13B as the source white-box model to generate the suffix, which is then directly fed into the target closed-source LLMs. Surprisingly, the results shown in Table 2 indicate that the suffix generated by Faster-GCG exhibits better transferability to closed-source models compared to GCG. We note that Faster-GCG is not specifically designed for the black-box transfer-based setting. We speculate that this may be due to the lower loss associated with the suffixes generated by Faster-GCG."}, {"title": "4.4 Ablation Study", "content": "The Effect of Each Technique. Faster-GCG introduces four key modifications compared to GCG: distance regularization term, greedy sampling, avoidance of the self-loop (deduplication), and the use of CW loss. To quantify the impact of each, we perform an ablation study on Llama-2-7B-chat, selectively disabling one modification at a time. Table 3 presents the ASR for Llama-2-7B-chat under various configurations. As we can see, disabling the distance regularization term reduces ASR by 14%, demonstrating its role in improving gradient approximation. Removing self-loop avoidance leads to the largest ASR drop of 20%, highlighting the need to prevent redundant cycles for effective optimization. Eliminating greedy sampling decreases ASR by 8%, underscoring its contribution to attack stability. Excluding the CW loss results in a 6% ASR reduction, indicating its importance in optimizing the loss landscape. Overall, each modification uniquely enhances the effectiveness of Faster-GCG."}, {"title": "5 Conclusion and Discussion", "content": "In this paper, we introduce Faster-GCG, an optimized and efficient adversarial jailbreak method for LLMs. By identifying and addressing key bottlenecks in the original GCG, Faster-GCG achieves significantly higher attack success rates while reducing computational cost by an order of magnitude. Our experiments demonstrate that Faster-GCG not only outperforms GCG on open-source LLMs like Llama-2-7B-chat and Vicuna-13B but also exhibits improved transferability when applied to closed-source models. These results indicate that, despite advancements in aligning LLMs to adhere more closely to intended behaviors, adversarial jailbreak attacks remain a critical vulnerability. The superior performance of Faster-GCG underscores the need for continuous advancements"}, {"title": "Limitations.", "content": "As shown in Appendix C, the adversarial suffix optimized by Faster-GCG has a higher perplexity than natural language, making it easily detectable via perplexity-based metrics. Thus, similar to GCG, the optimized suffixes produced by Faster-GCG are unlikely to bypass perplexity-based defenses (Jain et al., 2023). However, this issue may be mitigated by following the method proposed by (Zhu et al., 2023), where Faster-GCG can be combined with other techniques to create more readable suffixes. Additionally, unlike Zou et al. (2023), we do not employ the ensemble technique in the transfer-based attack setting, which typically significantly enhances the ASR of black-box attacks. Our focus is on improving the discrete optimization itself, and the ensemble experiments are beyond the scope of this paper. We leave these two limitations for future work."}, {"title": "Impact Statement.", "content": "This work proposes several enhanced techniques for generating jailbreak suffixes for LLMs, which may potentially produce harmful texts. But similar to previous jailbreak methods, we investigate jailbreak prompts with the objective of uncovering inherent weaknesses in LLMs. This endeavor aims to inform and guide future research focused on improving human preference safeguards in LLMs while advancing more effective defense strategies against misuse. For example, the generated jailbreak suffixes can be employed for adversarial training of LLMs, thereby enhancing their overall robustness and security (Mazeika et al., 2024).\nIn addition, as a foundational discrete optimization algorithm, Faster-GCG has the potential to be adapted to more models beyond LLMs, such as text-to-image diffusion models (Yang et al., 2024). This could contribute to a broader understanding and collaborative development of AI security across various domains."}]}