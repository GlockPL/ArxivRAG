{"title": "Computing Ex ANTE EQUILIBRIUM IN HETEROGE-\nNEOUS ZERO-SUM TEAM GAMES", "authors": ["Naming Liu", "Mingzhi Wang", "Xihuai Wang", "Weinan Zhang", "Yaodong Yang", "Youzhi Zhang", "Bo An", "Ying Wen"], "abstract": "The ex ante equilibrium for two-team zero-sum games, where agents within each\nteam collaborate to compete against the opposing team, is known to be the best a\nteam can do for coordination. Many existing works on ex ante equilibrium solu-\ntions are aiming to extend the scope of ex ante equilibrium solving to large-scale\nteam games based on Policy Space Response Oracle (PSRO). However, the joint\nteam policy space constructed by the most prominent method, Team PSRO, cannot\ncover the entire team policy space in heterogeneous team games where teammates\nplay distinct roles. Such insufficient policy expressiveness causes Team PSRO\nto be trapped into a sub-optimal ex ante equilibrium with significantly higher ex-\nploitability and never converges to the global ex ante equilibrium. To find the\nglobal ex ante equilibrium without introducing additional computational complex-\nity, we first parameterize heterogeneous policies for teammates, and we prove that\noptimizing the heterogeneous teammates' policies sequentially can guarantee a\nmonotonic improvement in team rewards. We further propose Heterogeneous-\nPSRO (H-PSRO), a novel framework for heterogeneous team games, which\nintegrates the sequential correlation mechanism into the PSRO framework and\nserves as the first PSRO framework for heterogeneous team games. We prove\nthat H-PSRO achieves lower exploitability than Team PSRO in heterogeneous\nteam games. Empirically, H-PSRO achieves convergence in matrix heterogeneous\ngames that are unsolvable by non-heterogeneous baselines. Further experiments\nreveal that H-PSRO outperforms non-heterogeneous baselines in both heteroge-\nneous team games and homogeneous settings.", "sections": [{"title": "1 INTRODUCTION", "content": "In this paper, we focus on a class of multiplayer games where a team of agents competes against\nan adversarial team. Specifically, we focus on a team of heterogeneously skilled agents against an\nopposing team, which is referred to as heterogeneous team games. These games model competitions\nbetween two entities (the team and the opponent team) and are natural extensions of two-player\ngames to multiplayer games. To this day, algorithms have achieved superhuman performance in\ntwo-player games, including Go (Silver et al., 2017) and heads-up no-limit Texas hold'em poker\ngames (Bowling et al., 2015; Brown & Sandholm, 2018).\nHeterogeneous team games introduce distinct challenges not presented in two-player games, espe-\ncially in terms of coordinating team members with distinct roles. For example, in StarCraft, how\nshould distinct species (e.g., Marine, Stalker, and Medivac), each with unique skills, collaborate\nto defeat an opposing team? Similarly, in soccer, how can forwards, midfielders, and defenders,\nwho cannot communicate during the game except through public observable actions, play optimally\nagainst their opponents? A common solution for these challenges is that of ex ante coordination\nintroduced by Celli & Gatti (2018), in which team members can correlate their strategies before\nthe game starts but cannot communicate during the gameplay. This form of ex ante coordination\nis known to be the best a heterogeneous team can do for coordination and makes the problem of\noptimization convex. However, computing an ex ante equilibrium in heterogeneous team games"}, {"title": "2 PRELIMINARIES", "content": "Two-team zero-sum game (Littman, 1994) 1 can be defined as a tuple $G = (T, O, A, R, P, \\gamma)$, where we write the team set as $T = {T_1, T_2}$, where $T_1$ is a finite set of players playing cooperatively against an adversary team denoted by $T_2$. Let $O = O_1 \\times O_2$ be the product of locaob-servation spaces of two teams, namely the joint observation space, where $O_1 = \\times_{i=1}^{n_1}O_{1,i}$, and $O_2 = \\times_{j=1}^{n_2}O_{2,j}$ denote the product of local observation spaces of the players in team $T_1$ and $T_2$, namely team's joint observation space. $O_{1,i}$, $O_{2,j}$ is the local observation spaces of players $i \\in T_1$ and $j \\in T_2$. $A = A_1 \\times A_2$ is the product of action spaces of two teams, namely the joint action space, where $A_1 = \\times_{i=1}^{n_1}A_{1,i}$ and $A_2 = \\times_{j=1}^{n_2}A_{2,j}$ denote the product of action space of players in team $T_1$ and $T_2$, namely team's joint action space. $A_{k,i}$ is the action spaces of players $i \\in T_k \\in T$. We define the joint action of team $T_k$ as $a_k = (a^1,\\dots,a^{n_k}) \\in A_k$. A team strategy is a vector of individual strategies of team players, denoted by $\\pi_k = (\\pi_{k,1}, ..., \\pi_{k,n_1})$, where $\\pi_{k,i} \\in \\Pi_{k,i}: O_{k,i} \\rightarrow \\Delta A_{k,i}$ is an individual strategy of player $i \\in T_k$. $R$ is a pair of reward functions $(R_1, R_2)$, where we use the notation of $R_t: O \\times A \\rightarrow [-R_{max}, R_{max}]$, $t \\in {1,2}$ to represent the reward function of two teams. Note that players within the same team share the team reward with $R_{k,1} = R_{k,2} = \u00b7\u00b7\u00b7 = R_{k,n_k} = R_k/n_k$, and the rewards of two teams sum to zero $R_1 + R_2 = 0$. Let $P: O \\times A \\times O \\rightarrow R$ be the transition probability function, and $\\gamma \\in [0,1)$. The transition probabililty function $P$, team policy $\\pi_1$, opponent team policy $\\pi_2$, and the initial observation distribution $d$, induce a marginal observation distribution at time $t$, denoted by $p_t^\\pi$.\nAt time step $t \\in R$, team $T_k$ observes its local observations $o_{k,t} \\in O_k$ ($o_{k,t} = (o^1_t, ..., o^{n_k}_t)$ is the \"joint\" observations) and take team joint actions $a_{k,t} \\in A_k$ according to its policy $\\pi$. At each time step, two teams take actions simultaneously based on their observations with no sequential dependency. At the end of each time step, team $T_k$ receives its joint reward $R_k(o_{1,t}, a_{1,t}, o_{2,t}, a_{2,t})$, and observes $o_{k,t+1}$. Following this process infinitely long, team $T_1$ and $T_2$ earn a discounted cumulative return of $R_1 = \\sum_{t=0}^{\\infty} \\gamma^tR_1(o_{1,t}, a_{1,t}, o_{2,t}, a_{2,t})$ and of $R_2 = \\sum_{t=0}^{\\infty} \\gamma^tR_2(o_{1,t}, a_{1,t}, o_{2,t}, a_{2,t})$ respectively. The expected reward of the team can be written as the following function:\n$R_1 (\\pi_1, \\pi_2) := E_{o_{1,0}:o_{1,\\infty} \\sim p_1, o_{2,0}:o_{2,\\infty} \\sim p_2, a_{1,0}:a_{1,\\infty} \\sim \\pi_1, a_{2,0}:a_{2,\\infty} \\sim \\pi_2}[\\sum_{t=0}^{\\infty} \\gamma^t R_1(o_{1,t}, a_{1,t}, o_{2,t}, a_{2,t})]$\nHeterogeneous team games are a specialized subset of two-team games where each team is com-posed of agents with differing characteristics and abilities. Formally, $\\exists i, i' \\in T_k \\in T$ such that player $i$ and player $i'$ perform distinct roles and are not exchangeable. Consequently, the distribu-tion of observation space $O_{k,i}$ is different from the distribution of observation space $O_{k,i'}$. Further,"}, {"title": "TMECor as a Maxmin Problem.", "content": "The central solution concept in heterogeneous team games is the\nTeam-Maxmin Equilibrium with correlation (TMECor) (Basilico et al., 2017; Celli & Gatti, 2018).\nTMECor is a Nash equilibrium where the team $T_1$ plays according to the ex ante coordinated strategy\n$\\pi_1 \\in \\Pi_1: O_1 \\rightarrow \\Delta A_1$ and the opponent team $T_2$ plays according to the ex ante coordinated\nstrategy $\\pi_2 \\in \\Pi_2: O_2 \\rightarrow \\Delta A_2$. According to definition, a TMECor is reached if, for each team\n$T \\in T$, its coordinated team strategy is a best response to the coordinated team strategies of teams\n$\\tau \\in T\\setminus T$. Upon reaching a TMECor $(\\pi_1, \\pi)$, players in both $T_1$ and $T_2$ cannot cooperatively deviate\nfrom their team strategies to obtain a higher team reward:\n$R_1 (\\pi_1^*, \\pi_2^*) \\geq R_1 (\\pi'_1, \\pi_2^*) \\forall \\pi'_1 \\in \\Pi_1,$\n$R_2(\\pi_1^*, \\pi_2^*) \\geq R_2(\\pi_1^*, \\pi'_2) \\forall \\pi'_2 \\in \\Pi_2.$\n(1a)\n(1b)\nDefine the exploitability of a pair of coordinated team strategies $(\\pi_1,\\pi_2)$ as $e(\\pi_1, \\pi_2) =R_2(\\pi_1, BR(\\pi_1)) + R_1(BR(\\pi_2), \\pi_2)$, where $BR(\\pi_1)$ is the coordinated opponent team strat-egy which achieves the highest reward responding to the team coordinated strategy $\\pi_1$ and $BR(\\pi_2)$is the coordinated team strategy that achieves the highest reward responding to the coordinated opponent team strategy $\\pi_2$. A coordinated team strategy pair $(\\pi_1, \\pi_2)$ is a TMECor if $e(\\pi_1, \\pi_2) = 0$, and is an $\\epsilon$-approximate TMECor if $e(\\pi_1, \\pi_2) < \\epsilon$."}, {"title": "Policy Space Response Oracle (PSRO)", "content": "PSRO (Lanctot et al., 2017) provides an iterative mecha-\nnism for finding a Nash equilibrium approximation in two-player zero-sum games. These algorithms\nwork in expanding a restricted policy set $\\Pi^e$ for each team $T_k \\in T$ iteratively. At each epoch, a\nlocal TMECor $\\sigma = (\\sigma_k, \\sigma_{-k})$ is computed for a restricted game which is formed by a tuple of re-stricted policy sets $\\Pi^e = (\\Pi^e_k, \\Pi^e_{-k})$. Then, a best response to the local TMECor $\\sigma_{-k}$ is computed\nand added to team $T_k$'s restricted policy set $\\Pi^e_k = \\Pi^e_k \\cup {BR(\\sigma_{-k})}$. When the iteration termi-nates with ${BR(\\sigma_{-k})} \\subseteq \\Pi^e_k$ and ${BR(\\sigma_k)} \\subseteq \\Pi^e_k$, the local TMECor $\\sigma^* = (\\sigma^*_k, \\sigma^*_{-k})$ for theresricted game is approximating an TMECor in the original team game."}, {"title": "3 RELATED WORK", "content": "Team Games as Two Player Games To compute TMECor in heterogeneous team games, it is\nstraightforward to treat each heterogeneous team as a single player with a joint strategy space\n(Carminati et al., 2022). By transforming a heterogeneous team game into an equivalent two-player\nzero-sum game (2p0s), the problem of finding a TMECor becomes equivalent to the problem of\nfinding a Nash equilibrium in two-player zero-sum games, thus more amenable to the techniques\nthat have been developed over the past 80 years (Robinson, 1951; McMahan et al., 2003; Zinkevich\net al., 2007; Lanctot et al., 2017; McAleer et al., 2020; Liu et al., 2021; Zhou et al., 2022). Celli\n& Gatti (2018) propose Column Generation (CG), which designs a hybrid representation to reduce\nthe space of the join team plans and builds a subset of jointly-reduced plans progressively to avoid\nenumerating the whole space. While these algorithms perform well in small to medium-scale het-erogeneous team games, scaling them to larger games is challenging because the joint policy space\nof both teams grows exponentially with the increasing number of players.\nTeam Games as MARL Problems Another perspective for solving heterogeneous team games is\nto formulate it as a multiplayer cooperative challenge, e.g., considering opponent team part of the\nenvironment and modeling the problem of solving TMECor as an optimization problem, which aims\nto maximize the reward of team $T_1$ and find an optimal ex ante correlation solutions for heteroge-neous players in $T_1$. To achieve this goal, various Multi-Agent Reinforcement Learning (MARL)\nalgorithms (Yu et al., 2022; Kuba et al., 2022; Wen et al., 2022; Wang et al., 2023) have been pro-posed. While these algorithms achieve remarkable performance in games like StarCraft II, they\nsuffer from unsteady performance when applied to real-world scenarios, where diverse opponent\nteams are encountered (see results in Table 2).\nTeam Games as Mixed Cooperative-Competitive Games To overcome the above challenges, re-searchers model team games as mixed cooperative-competitive games and integrate the cooperative\nreinforcement learning techniques with competitive frameworks like Policy Space Response Ora-\ncle (PSRO) (Lanctot et al., 2017) to solve the mixed cooperative-competitive games. For example,\nMcAleer et al. (2023) integrate PSRO with a homogeneous-agent based cooperative algorithms,\niteratively constructing a population of shared policies to find an approximate TMECor. However,"}, {"title": "4 COMPUTING TMECOR IN HETEROGENEOUS TEAM GAMES", "content": "We focus on the problem of computing a global TMECor in heterogeneous team games. One of\nthe most promising algorithms that can scale to large team games is Team PSRO (McAleer et al.,\n2023). However, Team PSRO heavily relies on a policy sharing mechanism for team coordination,\nwhich causes Team PSRO to be trapped into a sub-optimal ex ante equilibrium with significantly\nhigher exploitability and never converges to the global TMECor in heterogeneous team games. In\nthis section, we analyze and formulate the convergence problem that Team PSRO encounters in het-\nerogeneous team games, and propose the first PSRO framework for heterogeneous team games to\naddress the convergence issue. We analyze the reasons that cause the convergence issue of Team\nPSRO in heterogeneous team games in Section 4.1; then we formulate the convergence issue in\ngeneral heterogeneous team games in Section 4.2. To address the convergence problem and find a\nglobal TMECor in heterogeneous team games, we maintain heterogeneous policies for teammates\nand propose a mechanism to reduce the high complexity brought by optimizing over multiple play-\ners' policy spaces in Section 4.3. Inspired by this, we propose a general framework named het-\nerogeneous PSRO (H-PSRO) for heterogeneous team games in Section 4.4 and prove that H-PSRO\nachieves lower exploitability than Team PSRO in heterogeneous team games."}, {"title": "4.1 INSUFFICIENT EQUILIBRIUM EXPRESSIVE ABILITY OF NON-HETEROGENEOUS\nALGORITHMS", "content": "Team PSRO relies on a policy sharing mechanism for team correlation. That is, players in team\n$T_k \\in T$ share a policy $\\pi_{k,share}$, which forms a team policy $\\pi_{k,share} = {\\pi_{k,share},...,\\pi_{k,share}}$.\nWe define the space of team policy $\\pi_{k,share}$ as $\\Pi_{k, share}$. Team PSRO iteratively expands a re-\nstricted policy set $\\Pi^e, share$ by computing a best response to the meta policy $\\sigma_{-k}$ and adding it to\nthe restricted policy set $\\Pi^e, share = \\Pi^e,share \\cup {BR_{k,share}(\\sigma_{-k})}$, where the Best Response Ora-\ncle under a policy sharing based correlation is defined as $BR_{k,share}: \\Pi_{-k} \\rightarrow \\Pi_{k, share}$. When\n$BR_{k, share}(\\sigma_{-k})$ already exists in $\\Pi^e_k, share$, $\\forall T_k \\in T$, Team PSRO terminates with a pair of meta\npolicies $\\sigma^*_{ share} = (\\sigma^*_{k,share}, \\sigma^*_{-k,share}) \\in \\Delta\\Pi^e_{k,share} \\times \\Delta\\Pi^e_{-k,share}$. While this mechanism does not in-troduce additional computational complexity when the number of teammates increases, it causes\ninsufficient policy expressive ability and insufficient equilibrium expressive ability in heterogeneous\nteam games."}, {"title": "Definition 1", "content": "The Policy Expressive Ability of team $T_1$ is defined as $PEA_{1,c} = \\frac{\\Pi_{1, c}}{\\Pi_{1}} < 1$, where$\\Pi_{1, c}$ is the corresponding team policy space under a correlation method $c$, and $\\Pi_{1}$ is the entire teampolicy space."}, {"title": "Definition 2", "content": "A TMECor is a vector of distributions over policy spaces of team $T_1$ and opponent\nteam $T_2$. We define the set of TMECor within the joint policy space $S = \\Pi_1 \\times \\Pi_2$ as $E$, and the\nset of TMECor within the corresponding joint policy space $S_c = \\Pi_{1,c} \\times \\Pi_{2,c}$ under a correlation\nmethod $c$ as $E_c$, where $\\Pi_{k,c}$ is the team policy space of Team $T_k \\in T$ under the correlation method$c$."}, {"title": "Proposition 1", "content": "In any two team games, Policy Expressive Ability of team $T_1$ under a policy sharing\nbased correlation $PEA_{1, share} < 1$, and Policy Expressive Ability of opponent team $T_2$ under a policy\nsharing based correlation $PEA_{2, share} < 1$, leading to insufficient policy expressive ability."}, {"title": "Proposition 2", "content": "In heterogeneous team games, at most $E_{share} \\subseteq E$; in some cases, $E_{share} \\neq E$, leading\nto insufficient equilibrium expressive ability."}, {"title": "4.2 CONVERGENCE ISSUE OF NON-HETEROGENEOUS ALGORITHMS", "content": "Due to the insufficient policy expressive ability and equilibrium expressive ability caused by policy\nsharing, the non-heterogeneous PSRO framework encounters severe convergence issue in hetero-geneous team games. There are two primary reasons. Firstly, the homogeneous PSRO frameworkiteratively computes a Best Response policy within team policy space $\\Pi_{1, share}$ and $\\Pi_{2, share}$, and ter-minates if and only if Best Response policies of team $T_k$ already exist in $\\Pi^e_{ share}$, $\\forall T_k \\in T$. Whenthe iteration terminates, it does not mean convergence to a TMECor in the original game because itis highly possible that the Best Response policy is in the space $\\Pi_k\\setminus\\Pi_{k, share}$ because the policy ex-pressive ability $PEA_1 < 1$ and $PEA_2 < 1$ (Proposition 1). Secondly, due to the insufficient equi-librium expressive ability, the homogeneous PSRO framework can only converge to a sub-optimalTMECor with no guarantee that deviating to policies within $\\Pi_1\\setminus\\Pi_{1,share}$ (or $\\Pi_2\\setminus\\Pi_{2, share}$) will notdecrease (or increase) the team reward $R_1$."}, {"title": "Proposition 3", "content": "In heterogeneous team games, the homogeneous PSRO framework is trapped into asub-optimal TMECor within a subset of joint policy space $S_{share} \\subset S$."}, {"title": "4.3 THEOREM FOR CORRELATING HETEROGENEOUS POLICIES", "content": "To tackle the convergence issues described in Section 4.2, which is caused by insufficient policyexpressive ability and equilibrium expressive ability under a policy sharing based correlation, onestraightforward idea is to parameterize heterogeneous policies for teammates. For example, we canparameterize $\\pi_{1,i}$ by $\\vartheta_i$, which, together with other agents in team $T_1$, forms a joint team policy $\\pi_1$parameterized by $\\Theta_1 = (\\vartheta_1, ..., \\vartheta_{n_1})$. We prove that a global TMECor can be achieved with theheterogeneously parameterized policies."}, {"title": "Theorem 1", "content": "The joint policy space with heterogeneous policies under PSRO framework is equal to$S$, therefore enabling the PSRO framework to achieve a global TMECor."}, {"title": "However", "content": "the heterogeneous policies bring in new difficulties for finding the optimal ex ante corre-lated solution. Optimizing over multiple players' policy space simultaneously is significantly harderthan optimizing over a single shared policy space (e.g., $\\Delta$-complete) (Zhang et al., 2022b), andoptimizing over the correlated team policy space consisting of heterogeneous policies makes theoptimization space grow exponentially with the increasing number of players. To find an optimal exante correlation solution for heterogeneous policies without introducing additional complexity, wepropose a sequential correlation method for the Best Response Oracle (BRO) for computing a bestresponse ex ante correlation strategy for a given opposing team's strategy. The proposed sequentialcorrelation method is based on the observation in Lemma 1 (see Appendix A.1)."}, {"title": "Lemma 1", "content": "confirms that a sequential update is an effective approach for sequential BRO to search forthe direction of ex ante coordination improvement (i.e., joint actions with positive advantage values)in two-team games given the opposing team's policy. That is, agents take actions sequentially byfollowing an arbitrary order $i_{1:n_1} = (i_1,..., i_{n_1}) = T_1$ (or $j_{1:n_2} = (j_1,..., j_{n_2}) = T_2$). Let agent$i_1 \\in T_1$(or $j_1 \\in T_2$) take action $a_{1,i_1}$ (or $a_{2,j_1}$) such that the value of the advantage function of"}, {"title": "Theorem 2", "content": "Given an opponent team policy $\\pi_2 \\in \\Pi_2$ (or a team policy $\\pi_1 \\in \\Pi_1$),the sequential BRO can achieve better ex ante team coordination than the policy sharingbased BRO with $R_1 (BR_{1, seq} (\\pi_2), \\pi_2) \\geq R_1 (BR_{1, share} (\\pi_2), \\pi_2)$ and $R_2(\\pi_1, BR_{2, seq}(\\pi_1)) \\geqR_2 (\\pi_1, BR_{2, share} (\\pi_1)$, where $BR_{k,share}: \\Pi_{-k} \\rightarrow \\Pi_{k,share}$ is the policy sharing based BROof team $T_k \\in T$, and $BR_{k,seq}: \\Pi_{-k} \\rightarrow \\Pi_{k,seq}$ is the sequential BRO of team $T_k \\in T$.In some cases, $R_1 (BR_{1, seq} (\\pi_2), \\pi_2) > R_1 (BR_{1, share} (\\pi_2), \\pi_2)$ and $R_2(\\pi_1, BR_{2, seq}(\\pi_1)) >R_2 (\\pi_1, BR_{2, share} (\\pi_1))$ hold."}, {"title": "4.4 A GENERAL FRAMEWORK FOR HETEROGENEOUS TEAM GAMES", "content": "Inspired by the results in Section 4.3, we introduce a general framework, Heterogeneous PSRO(H-PSRO). H-PSRO integrates the sequential correlation mechanism derived from lemma 1 intoan iterative procedure and serves as the first PSRO framework for heterogeneous team games. Weprove that H-PSRO can achieve lower exploitability than the homogeneous PSRO framework inheterogeneous team games. H-PSRO iteratively expands a restricted set of team policies $\\Pi^e_{k,hete}$for team $T_k \\in T$, where each team policy consists of coordinated heterogeneous policies denotedby $\\pi_{k,hete} = {\\pi_{k,1}, \\pi_{k,2},..., \\pi_{k,n_k} }$, where heterogeneous policies $\\pi_{k,1}, \\pi_{k,2},..., \\pi_{k,n_k}$ are playedindependently and we define the space of team policy $\\pi_{k, hete}$ as $\\Pi_{k, hete} = \\Pi_k$. As proved inTheorem 1, the joint policy space under the PSRO framework $S_{hete} = \\Delta\\Pi^e_{1, hete} \\times \\Delta\\Pi^e_{2, hete} = S$,thus mitigating the problem of insufficient equilibrium expressive ability in Proposition 2.\nH-PSRO starts with randomly initialized team policies $\\pi_{1,hete}, \\pi_{2, hete}$, and the restricted sets of teampolicies and opponent team policies are $\\Pi^e_{1,hete} = {\\pi_{1,hete} }, \\Pi^e_{2, hete} = {\\pi_{1,hete} }$. Consider the re-stricted game where the team policy space is $\\Pi_{1,hete}$ and the opponent team policy space is $\\Pi_{2, hete}$.We denote the payoff matrix of this restricted game as $U_{1x2}$. If the game is symmetric, we also have"}, {"title": "Theorem 3", "content": "In heterogeneous team games, H-PSRO achieves lower exploitability than Team PSRO.\nFormally, $e(\\sigma_{1, seq}^*, \\sigma_{2, seq}^*) \\leq e(\\sigma_{1, share}^*, \\sigma_{2, share}^*)$."}, {"title": "5 EXPERIMENTS", "content": "The main purpose of the experiments is to compare H-PSRO with existing state-of-the-art PSROvariants in terms of approximating a full game TMECor. The baseline methods include Team PSRO(McAleer et al., 2023), PSRO (Lanctot et al., 2017), Indep-PSRO\u00b2, Self Play, Fictitious Self Play(FSP) (Heinrich et al., 2015). The benchmarks consist of single-state heterogeneous team games(Team Rock-Paper-Scissors and Matrix heterogeneous Team games) and complex stochastic hetero-geneous team games (Competitive StarCraft and Google Research Football). For Team Rock-Paper-Scissors, Matrix heterogeneous Team games and Competitive StarCraft, we report the exploitabilityof the meta TMECor or learning trajectories through the training process. For Google ResearchFootball where the exact exploitability is intractable, we report the performance of the final strate-gies. First, we analyze the empirical convergence performance of H-PSRO and several baselines ina case study of Team Rock-Paper-Scissors, which is an extended heterogeneous team games of theclassic two-player zero-sum game Rock-Paper-Scissors. In addition, we illustrate how the perfor-mance evolves for each method using the Competitive StarCraft Benchmark in Section 5.2.2 andthe MAgent game (Zheng et al., 2018; Terry et al., 2020) in Appendix B.1, where H-PSRO is moreeffective at approximating a TMECor with the enlarging task scales. An ablation study on rela-tive performance against state-of-the-art MARL algorithms of H-PSRO in Appendix B.2 reveals that, with different MARL opponent strategies, H-PSRO exhibits superior win rate and more steadyperformance. The competitive videos against state-of-the-art MARL algorithms are available athttps://sites.google.com/view/h-psro-2024/h-psro."}, {"title": "5.1 A CASE STUDY: TEAM ROCK-PAPER-SCISSORS", "content": "We analyze the convergence property of H-PSRO and other baselines in Team Rock-Paper-Scissors(team RPS), which extends the classic 2-player zero-sum game Rock-Paper-Scissors to a 4-playerheterogeneous team setting (see details in Example 1). This task requires agents in the same teamto cooperatively choose Rock, Paper, Scissors to compete against the opposing team. Clearly, thisgame has a unique TMECor where the team chooses Rock, Paper, Scissors with equal probability.However, as analyzed in Example 1, Team PSRO fails to find such a TMECor because the insuffi-cient policy expressive ability caused by the policy sharing based correlation makes the equilibriumset $E_{share} \\neq E$.\nWe visualize the trajectories of Self Play (SP), Fictitious Self Play (FSP), Team PSRO, and H-PSRO in Team RPS, and the learning dynamics are shown in Figure 2. The orange star in eachsubfigure is the TMECor of the team RPS game. The black lines in SP subfigure are the traces of"}, {"title": "5.2 HETEROGENEOUS TEAM GAMES", "content": "In this section, we study the impacts of the insufficient policy expressive ability in different het-erogeneous team games including a matrix heterogeneous team game, competitive StarCraft, andGoogle Research Football."}, {"title": "5.2.1 MATRIX HETEROGENEOUS TEAM GAMES", "content": "We conduct our matrix experiment on a carefully designed heterogeneous team game, which in-volves two teams: $T_1 = {M_1, M_2}$ and $T_2 = {O_1, O_2}$ with joint team action spaces $A_1 =\\{0,1\\} \\times \\{0,2\\}$ and $A_2 = \\{0,1\\} \\times \\{0,3\\}$, and the reward structure of this game is defined inEq (5). The heterogeneity lies in the different action spaces of team players in $T_1$ and $T_2$. Theglobal TMECor in this game requires team $T_1$ to take joint action (0,0) with probability 0.6, (0, 2)with probability 0.4, and all other joint actions with probability 0, and requires opponent team$T_2$ to take joint action (0,0) with probability 0.4, (1,0) with probability 0.6, and all joint otheractions with probability 0. We visualize the trajectory of the 8-dimensional joint policies of twoteams in a compressed 2D space in Figure 3(b) and Figure 3(c) in order to compare the conver-gence properties of H-PSRO and Team PSRO. The results show that Team PSRO gets stuck ina sub-optimal point with $\\sigma^*_{1,share}$ = (0.81,0.09,0.09,0.01) and $\\sigma^*_{2, share}$ = (1., 0., 0., 0.), where$R_1 (\\sigma^*_{1, share}, BR(\\sigma^*_{1,share})) \\approx 4.0$ and $R_2 (BR(\\sigma^*_{2, share}), \\sigma^*_{2,share}) \\approx -1.05$, leading to exploitability$\\approx 2.95$ (see Figure 3(a)). In contrast, H-PSRO approximates the global TMECor with exploitabil-ity < 10-6. The exploitability outcomes nicely align with Theorem 3, demonstrating H-PSRO'ssuperior ability to explore sufficient policy spaces, and to approximate the global TMECor in het-erogeneous team games."}, {"title": "5.2.2 \u0421\u043eMPETITIVE STARCRAFT", "content": "Competitive StarCraft (Leroy et al., 2022) is a variant of the classical StarCraft Multi-Agent Chal-lenge (SMAC, Samvelyan et al., 2019), which allows both team $T_1$ and opponent team $T_2$ to controltheir actions and naturally serves a heterogeneous team game benchmark. The Competitive StarCraftprovides a set of StarCraft II maps to evaluate the effectiveness of H-PSRO. These maps feature ateam of mostly heterogeneous ally units that aim to defeat a team of heterogeneous enemy units, andchallenges algorithms to ensure internal team cooperation while learning robust equilibrium strate-gies that are capable of facing diverse opponent team strategies. We compare H-PSRO and TeamPSRO in Competitive StarCraft maps with different scales, including 2s3z_compet, 3s5z_compet,and MMM_compet (see details in Table 2), and the exploitability results are shown in Figure 4.\nThese results show that H-PSRO achieves superior convergence with an exploitability of approx-imate 0 across different tasks while Team PSRO converges to strategies with significant higherexploitability. With the task scale increases"}]}