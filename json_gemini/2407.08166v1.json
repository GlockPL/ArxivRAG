{"title": "SYNTHETIC ELECTRORETINOGRAM\nSIGNAL GENERATION USING\nCONDITIONAL GENERATIVE\nADVERSARIAL NETWORK FOR\nENHANCING CLASSIFICATION OF AUTISM\nSPECTRUM DISORDER", "authors": ["Mikhail Kulyabin", "Paul A. Constable", "Aleksei Zhdanov", "Irene O. Lee", "David H. Skuse", "Dorothy A. Thompson", "Andreas Maier"], "abstract": "The electroretinogram (ERG) is a clinical test that records\nthe retina's electrical response to light. The ERG is a promising way\nto study different neurodevelopmental and neurodegenerative disorders,\nincluding autism spectrum disorder (ASD) - a neurodevelopmental con-\ndition that impacts language, communication, and reciprocal social inter-\nactions. However, in heterogeneous populations, such as ASD, where the\nability to collect large datasets is limited, the application of artificial in-\ntelligence (AI) is complicated. Synthetic ERG signals generated from real\nERG recordings carry similar information as natural ERGs and, there-\nfore, could be used as an extension for natural data to increase datasets\nso that AI applications can be fully utilized. As proof of principle, this\nstudy presents a Generative Adversarial Network capable of generating\nsynthetic ERG signals of children with ASD and typically developing\ncontrol individuals. We applied a Time Series Transformer and Visual\nTransformer with Continuous Wavelet Transform to enhance classifica-\ntion results on the extended synthetic signals dataset. This approach may\nsupport classification models in related psychiatric conditions where the\nERG may help classify disorders.", "sections": [{"title": "1 Introduction", "content": "The electroretinogram (ERG) is the waveform recorded from the eye under dark-\nor light-adapted conditions in response to a brief flash of light. The stimulus pa-\nrameters for the recording of the ERG are described by the International Society\nfor Clinical Electrophysiology of Vision (ISCEV) standard, which details flash\nstrength, duration, amplification, patient preparation, and reporting of the clin-\nical ERG [30]. Typically, the clinical ERG is used to support the diagnosis of\ninherited or acquired retinal disease using time-domain parameters based on\nthe amplitude and timing of the waveform peaks [31]. However, in conditions\nwhere subtle changes in the ERG may not be apparent through analysis of time-\ndomain parameters, such as in heterogeneous conditions, that may co-occur,\nsuch as autism spectrum disorder (ASD) [37] and attention deficit hyperactivity\ndisorder (ADHD) [2] then alternative approaches may be required using signal\nanalysis of the ERG to identify features for better classification [9,26]. One way\nof addressing the difficulty of heterogeneity is by using large datasets and artifi-\ncial intelligence (AI) to improve classification models. To support this aim, the\ngeneration of synthetic ERGs may help to advance this field for the identification\nand classification of neurodevelopmental and neurodegenerative disorders where\nthe ERG is anomalous [8].\nThe amplitude of the ERG waveform is affected by various patient factors,\nincluding age [29], sex [5], iris color [1], pupil diameter [11], dark adaptation in-\nterval [4], and in the case of skin electrodes, their position below the eye [18]. In\naddition to these subject factors, the choice of electrode, fiber, gold foil, contact\nlens, or skin will also affect the amplitude of the recorded signal [6]. Conse-\nquently, the ISCEV standard recommends that clinical sites establish reference\nranges that reflect the local population and recording parameters [30]. Decom-\nposing the signal may mitigate against some of these effects, such as age and iris\ncolor, by providing features that are less dependent on the patient's age, ocular\npigmentation, and the testing parameters. Benefits of signal analysis have been\ndemonstrated in ECG recordings [39] but is yet to be routinely used in ophthal-\nmology to analyze the ERG in detail and enhance the potential of the ERG as\nan early marker for retinal or neurological diseases [25].\nThrough demonstration of harnessing the utility of synthetic waveform gen-\neration to the ERG, this technique could support fields of ophthalmic research.\nThese may include, but are not limited to, clinical trials to ensure appropriate\nbalancing between case and control groups based on age, sex, electrode type,\nand iris color. In addition, the generation of synthetic ERG waveforms based\non signals from natural data recorded in different populations would help with\nensuring comparison reference waveforms are available to sites regardless of their\nlocal patient population and contribute to the expansion of the clinical utility\nof the ERG [23]. Furthermore, the ERG is only one test that is often used in\nconjunction with other tests of the visual pathways that may also be amenable\nto synthetic waveform generation, such as the multifocal and pattern ERG [31].\nThus, this paper intends to introduce this field to ophthalmology, using the ERG\nand autism as models from which future studies may build."}, {"title": "2 Related Work", "content": "The field of medical signals generation with AI has received significant atten-\ntion in recent years, prompting numerous advancements and this being the first\nwork to propose a synthetic ERG generation method using AI for ASD classifi-\ncation. Despite the previous significant body of work in this field, such as syn-\nthesizing electrocardiogram (ECG) and electroencephalogram (EEG) signals in\none-dimensional space, no other studies have sought to utilize these methods for\nclassifying neurodevelopmental disorders using the ERG signal. Previous works\nin neurology include those of Hartmann et al. [17], who utilized EEG-GAN as a\nframework to generate EEG brain signals. They reported that the modification\nof the Wasserstein GAN stabilized training and were then able to investigate a\nrange of architectural choices critical for time series generation. In cardiology,\nGolany et al. [14] applied a DCGAN architecture to generate synthetic ECG sig-\nnals independently to five different heartbeat classes. The authors showed that\nadding generated signals to the LSTM classifier improved accuracy significantly.\nWang et al. [36] augmented an imbalanced ECG dataset using signals generated\nby a 1D auxiliary classifier generative adversarial network (AC-GAN). In the\nsame field, Zhu et al. [41] employed a Long Short-Term Memory (LSTM) layer\nin GAN to generate synthetic ECG signals. Thambawita et al. [34] developed and\ncompared two methods, named WaveGAN and Pulse2Pulse, based on GAN ar-\nchitecture. According to the authors, conditional GAN (CGAN) improved ECG\nsignal generation by paying more attention to minor signal features' importance.\nMost recently, we have used GAN to increase the dataset sample size in\nan under-represented class based on sex in a control population using standard\nlight-adapted ERGs as a precursor to applying these methods to a classification\nframework that we present here [23]."}, {"title": "3 Dataset", "content": "The original dataset [7] is shown in Table 1. It includes nine types of LA-ERG\nwaveform recordings at different flash strengths, including 1.204, 1.114, 0.949,\n0.799, 0.602, 0.398, 0.114, -0.119, -0.367 (log cd.s.m-2) from 30 ASD and 20\ntypically developing control individuals. Data was collected at two sites based\nin London (UK) and Adelaide (Australia) as described previously in detail in\nprevious works [9,24,10].\nFull-field LA-ERG recordings were performed on each eye (right always first),\nand followed the guidelines of the ISCEV ERG standard [30]. A series of white\nflashes at each strength were presented to the eye at 2 Hz on a 40 (cd.m-2) white\nbackground. Recordings were performed with the RETeval (LKC Technologies,\nGaithersburg, MD, USA) with a custom nine-step randomized Troland-based\nprotocol with skin electrodes placed 2-3 mm below the lower eyelid. ERG wave-\nforms were averaged from 30-60 traces per eye to generate the reported averaged\nwaveform signal that was used in the analysis. Waveforms with artifacts such as\nblinks were automatically rejected if they fell within the upper or lower quartile"}, {"title": "4 Method", "content": "In this section, we present the proposed method for the generation of synthetic\nERG signals and their further classification, shown in Fig. 1. Initially, 25% of\nthe data was partitioned as a test dataset and stored untouched for unbiased\nevaluation outcomes. The remaining subset was utilized to train the conditional\nGAN for synthesizing ERG signals. These generated signals were then combined\nwith real or 'natural' signals to form an extended dataset for training the clas-\nsification Transformer models. For the training of ViT, a CWT transformation\nwas applied, generating a wavelet representation for each signal. For the TST\ntraining, signals were used in their original time-series representation. The evalu-\nation process used a five-fold cross-validation, and the performance metrics were\naveraged over the test subsets."}, {"title": "4.1 Conditional GAN", "content": "The framework for synthetic waveform generation used CGAN [28] to obtain\nthe synthetic signals from the natural ERG waveforms. The CGAN architecture\ncomprised two sub-networks: Generator (G) and Discriminator (D). The goal of\nthe Generator was to learn the transformation between the latent distribution pz\nand the real-world data distribution pa. The Discriminator learned to distinguish\nreal signals from synthesized ones. The Generator and the Discriminator were\nprovided with auxiliary class information y as an additional input layer. The\ncomplete CGAN architecture was trained in a min-max optimization game as in\nthe standard GAN loss function: the Discriminator tried to maximize the score\nfor real signals D(xy) and minimize the score for generated D(G(z|y)); and\nvice versa, the Generator tried to minimize log(1 \u2013 D(G(z|y))), such that the\nminimizing of the term was only possible if the Generator synthesized realistic\nsynthetic signals. The only difference with standard GAN is that the conditional\nprobability was used for both the Generator and the Discriminator instead of\nthe regular one. The complete optimization process is represented below [28]:\nminmaxV (D, G) = Ex~pa(x)[logD(x|y)] + Ez~pz(z)[log(1 \u2013 D(G(z|y)))] (1)\nRecurrent Neural Networks (RNNs) have gained widespread adoption in di-\nverse applications, including time series data processing, speech recognition, and\nimage generation [33]. While RNNs exhibit proficiency in handling short-term\ndependencies, they have limitations in effectively addressing long-term depen-\ndencies. To overcome these limitations, LSTM networks were introduced as an\nextension of RNNs [19]. LSTM incorporates a memory cell architecture that re-\ntains prior contextual information, thus accommodating issues such as gradient\nexpansion or disappearance during training.\nBLSTM is a neural network technique that facilitates bidirectional sequence\ninformation flow, encompassing both backward and forward directions [15]. In"}, {"title": "4.2 Continuous Wavelet Transform", "content": "Continuous Wavelet Transform (CWT) is an instrument that provides an over-\ncomplete representation of a signal by letting the translation and scale param-\neter of the wavelets vary continuously. CWT of the function x(t) at a scale\n(a > 0) \u2208 R+* and translational value b\u2208 R is expressed by the following in-\ntegral (2), where y(t) is the continuous function called the mother wavelet, and\nthe overline represents the operation of the complex conjugate [16]. The primary\nobjective of the mother wavelet is to serve as the foundational function for gen-\nerating daughter wavelets, which are simply the translated and scaled versions\nof the mother wavelet. The output of the CWT consists of a two-dimensional\ntime-scale representation of the signal.\n\u03a7\u03c9(a,b) = a/1/2 x(t) (tab) dt (2)\nUsing the method [21], we determined the three most optimal mother func-\ntions for our dataset: Ricker, Gaussian, and Morlet. To increase the efficiency\n[3], the input wavelet image for further classification consisted of three wavelets\nas three channels [22]."}, {"title": "4.3 Visual Transformer classification model", "content": "Transformers have emerged as a preferred model for image classification tasks,\nprimarily attributed to their computational efficiency and scalability [35]. In\nthis study, we applied ViT to the wavelet scalograms using real and synthetic\ndatasets. ViT demonstrated superior classification performance compared to\ntraining solely on the smaller natural waveform dataset.\nInitially, the model processes a 2D input image (wavelet in our case) by"}, {"title": "4.4 Time Series Transformer classification model", "content": "TST is a neural network architecture designed to process and analyze time series\ndata. It is based on the Transformer architecture introduced by Vaswani et\nal. for natural language processing tasks [35], but without the decoder part\nof the architecture [40]. TST adapts the Transformer architecture to handle\nsequences of temporal data points effectively. As with the original Transformer\nmodel, TST uses self-attention mechanisms to weigh the importance of different\nelements within the input time series sequence. This attention mechanism allows\nthe model to learn dependencies between different time steps and capture long-\nrange dependencies. Positional encodings are crucial to TST and are added to"}, {"title": "4.5 Training", "content": "CGAN was trained with a batch size of 15 on 10000 epochs. The dropout was\nset to 0.2. The Adam optimizer was used with a learning rate of 0.0002 for the\nGenerator and Discriminator, with the BCELoss used as the criterion function.\nThe training time of the CGAN model was two hours using an AMD Ryzen\n95900HX\u00d716 processor with an NVIDIA GeForce RTX 3070 graphics card. The\nmodel had 106 trainable parameters with a complexity of 0.026 GFLOPS.\nFor the classification, two ResNet - ViT hybrid image models were used,\nwhich differed in the number of parameters used and have shown effective-\nness in ERG wavelet classification previously [22]. The models were ViT S\n(ViT_small_r26_s32_224) and ViT T (ViT_tiny_r_s16_p8_224), available\nat the HuggingFace 'transformers' repository [38]. Both models were pretrained\non ImageNet-21k and fine-tuned with ImageNet-lk with additional augmenta-\ntion and regularization with a resolution of 224x224 pixels. An SGD optimiza-\ntion with a 0.001 initial learning rate was used. ViT S and ViT T have 36.4 \u00d7 106\nand 10.4 \u00d7 106 trainable parameters with complexities of 3.5 and 0.4 GFLOPS,\nrespectively. For TST, a cross-entropy loss function with class weights adapted\nto address the class imbalance and an Adam optimizer with an initial learning\nrate of 0.0001 was used. Each model was then trained until convergence using\nthe early stopping criteria on the validation loss with a batch size of 32.\nThe models were evaluated using a five-fold cross-validation. Performance\nmetrics were Balanced Accuracy (BA), Precision (P), Recall (R), F1-score, and\nAUC. The performance outcomes were averaged across the five-folds and are\npresented in Table 2. TST was trained for each strength independently, as well\nas on all strengths simultaneously. ViT was trained only on the entire signal\ndataset (all flash strengths together). The input for the ViT model was the\nwavelet scalograms obtained using CWT, and the input for TST was the ERG\nsignals in the time-series representation."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "The experiments evaluated the performance of TST and ViT models under var-\nious training subset conditions. We found the highest metrics of the TST model\nfor training on signals with the higher (brightest) flash strength: BA = 0.805\nfor 1.114 (log cd.s.m\u00af\u00b2) flash strength. The second-best accuracy was with ViT\ntrained on wavelets of transformed mixed signals: BA = 0.777. Introducing syn-\nthetic signals into the training dataset significantly enhanced the performance\nof all models. Specifically, the BA of TST trained on the flash strength data at\n1.114 (log cd.s.m-2) increased by 10% and ViT by 13%."}, {"title": "6 Conclusions", "content": "Synthetic reference signals can enhance medical operational efficiency by offering\na feasible alternative to natural. Synthesizing facilitates dataset expansion within\nspecialized domains, enabling training resource-intensive networks such as trans-\nformers. Incorporating synthetic signals generated by the proposed conditional\nGAN offers a promising solution to address existing challenges in the domain\nof AI applied to the ERG. Through expanding the number of samples collected\nfrom subjects and generating synthetic waveforms provides a new opportunity\nto expand AI modeling of the ERG in rare or heterogeneous populations where\nlarge datasets are required. The augmentation with synthetic signals will provide\nthe opportunity to train heavy models such as Transformers to support the early\ndetection of retinal disorders. Furthermore, the non-personal nature of synthetic\nsignals permits their open-source publication, making them suitable for sharing\nwithout violating patient privacy concerns. We demonstrate the first applica-"}]}