{"title": "SYNTHETIC ELECTRORETINOGRAM SIGNAL GENERATION USING CONDITIONAL GENERATIVE ADVERSARIAL NETWORK FOR ENHANCING CLASSIFICATION OF AUTISM SPECTRUM DISORDER", "authors": ["Mikhail Kulyabin", "Paul A. Constable", "Aleksei Zhdanov", "Irene O. Lee", "David H. Skuse", "Dorothy A. Thompson", "Andreas Maier"], "abstract": "The electroretinogram (ERG) is a clinical test that records the retina's electrical response to light. The ERG is a promising way to study different neurodevelopmental and neurodegenerative disorders, including autism spectrum disorder (ASD) - a neurodevelopmental con- dition that impacts language, communication, and reciprocal social inter- actions. However, in heterogeneous populations, such as ASD, where the ability to collect large datasets is limited, the application of artificial in- telligence (AI) is complicated. Synthetic ERG signals generated from real ERG recordings carry similar information as natural ERGs and, there- fore, could be used as an extension for natural data to increase datasets so that AI applications can be fully utilized. As proof of principle, this study presents a Generative Adversarial Network capable of generating synthetic ERG signals of children with ASD and typically developing control individuals. We applied a Time Series Transformer and Visual Transformer with Continuous Wavelet Transform to enhance classifica- tion results on the extended synthetic signals dataset. This approach may support classification models in related psychiatric conditions where the ERG may help classify disorders.", "sections": [{"title": "1 Introduction", "content": "The electroretinogram (ERG) is the waveform recorded from the eye under dark- or light-adapted conditions in response to a brief flash of light. The stimulus pa- rameters for the recording of the ERG are described by the International Society for Clinical Electrophysiology of Vision (ISCEV) standard, which details flash strength, duration, amplification, patient preparation, and reporting of the clin- ical ERG [30]. Typically, the clinical ERG is used to support the diagnosis of inherited or acquired retinal disease using time-domain parameters based on the amplitude and timing of the waveform peaks [31]. However, in conditions where subtle changes in the ERG may not be apparent through analysis of time- domain parameters, such as in heterogeneous conditions, that may co-occur, such as autism spectrum disorder (ASD) [37] and attention deficit hyperactivity disorder (ADHD) [2] then alternative approaches may be required using signal analysis of the ERG to identify features for better classification [9,26]. One way of addressing the difficulty of heterogeneity is by using large datasets and artifi- cial intelligence (AI) to improve classification models. To support this aim, the generation of synthetic ERGs may help to advance this field for the identification and classification of neurodevelopmental and neurodegenerative disorders where the ERG is anomalous [8]. The amplitude of the ERG waveform is affected by various patient factors, including age [29], sex [5], iris color [1], pupil diameter [11], dark adaptation in- terval [4], and in the case of skin electrodes, their position below the eye [18]. In addition to these subject factors, the choice of electrode, fiber, gold foil, contact lens, or skin will also affect the amplitude of the recorded signal [6]. Conse- quently, the ISCEV standard recommends that clinical sites establish reference ranges that reflect the local population and recording parameters [30]. Decom- posing the signal may mitigate against some of these effects, such as age and iris color, by providing features that are less dependent on the patient's age, ocular pigmentation, and the testing parameters. Benefits of signal analysis have been demonstrated in ECG recordings [39] but is yet to be routinely used in ophthal- mology to analyze the ERG in detail and enhance the potential of the ERG as an early marker for retinal or neurological diseases [25]. Through demonstration of harnessing the utility of synthetic waveform gen- eration to the ERG, this technique could support fields of ophthalmic research. These may include, but are not limited to, clinical trials to ensure appropriate balancing between case and control groups based on age, sex, electrode type, and iris color. In addition, the generation of synthetic ERG waveforms based on signals from natural data recorded in different populations would help with ensuring comparison reference waveforms are available to sites regardless of their local patient population and contribute to the expansion of the clinical utility of the ERG [23]. Furthermore, the ERG is only one test that is often used in conjunction with other tests of the visual pathways that may also be amenable to synthetic waveform generation, such as the multifocal and pattern ERG [31]. Thus, this paper intends to introduce this field to ophthalmology, using the ERG and autism as models from which future studies may build."}, {"title": "2 Related Work", "content": "The field of medical signals generation with AI has received significant atten- tion in recent years, prompting numerous advancements and this being the first work to propose a synthetic ERG generation method using AI for ASD classifi- cation. Despite the previous significant body of work in this field, such as syn- thesizing electrocardiogram (ECG) and electroencephalogram (EEG) signals in one-dimensional space, no other studies have sought to utilize these methods for classifying neurodevelopmental disorders using the ERG signal. Previous works in neurology include those of Hartmann et al. [17], who utilized EEG-GAN as a framework to generate EEG brain signals. They reported that the modification of the Wasserstein GAN stabilized training and were then able to investigate a range of architectural choices critical for time series generation. In cardiology, Golany et al. [14] applied a DCGAN architecture to generate synthetic ECG sig- nals independently to five different heartbeat classes. The authors showed that adding generated signals to the LSTM classifier improved accuracy significantly. Wang et al. [36] augmented an imbalanced ECG dataset using signals generated by a 1D auxiliary classifier generative adversarial network (AC-GAN). In the same field, Zhu et al. [41] employed a Long Short-Term Memory (LSTM) layer in GAN to generate synthetic ECG signals. Thambawita et al. [34] developed and compared two methods, named WaveGAN and Pulse2Pulse, based on GAN ar- chitecture. According to the authors, conditional GAN (CGAN) improved ECG signal generation by paying more attention to minor signal features' importance. Most recently, we have used GAN to increase the dataset sample size in an under-represented class based on sex in a control population using standard light-adapted ERGs as a precursor to applying these methods to a classification framework that we present here [23]."}, {"title": "3 Dataset", "content": "The original dataset [7] is shown in Table 1. It includes nine types of LA-ERG waveform recordings at different flash strengths, including 1.204, 1.114, 0.949, 0.799, 0.602, 0.398, 0.114, -0.119, -0.367 (log cd.s.m-2) from 30 ASD and 20 typically developing control individuals. Data was collected at two sites based in London (UK) and Adelaide (Australia) as described previously in detail in previous works [9,24,10]. Full-field LA-ERG recordings were performed on each eye (right always first), and followed the guidelines of the ISCEV ERG standard [30]. A series of white flashes at each strength were presented to the eye at 2 Hz on a 40 (cd.m-2) white background. Recordings were performed with the RETeval (LKC Technologies, Gaithersburg, MD, USA) with a custom nine-step randomized Troland-based protocol with skin electrodes placed 2-3 mm below the lower eyelid. ERG wave- forms were averaged from 30-60 traces per eye to generate the reported averaged waveform signal that was used in the analysis. Waveforms with artifacts such as blinks were automatically rejected if they fell within the upper or lower quartile"}, {"title": "4 Method", "content": "In this section, we present the proposed method for the generation of synthetic ERG signals and their further classification, shown in Fig. 1. Initially, 25% of the data was partitioned as a test dataset and stored untouched for unbiased evaluation outcomes. The remaining subset was utilized to train the conditional GAN for synthesizing ERG signals. These generated signals were then combined with real or 'natural' signals to form an extended dataset for training the clas- sification Transformer models. For the training of ViT, a CWT transformation was applied, generating a wavelet representation for each signal. For the TST training, signals were used in their original time-series representation. The evalu- ation process used a five-fold cross-validation, and the performance metrics were averaged over the test subsets."}, {"title": "4.1 Conditional GAN", "content": "The framework for synthetic waveform generation used CGAN [28] to obtain the synthetic signals from the natural ERG waveforms. The CGAN architecture comprised two sub-networks: Generator (G) and Discriminator (D). The goal of the Generator was to learn the transformation between the latent distribution $p_z$ and the real-world data distribution $p_a$. The Discriminator learned to distinguish real signals from synthesized ones. The Generator and the Discriminator were provided with auxiliary class information y as an additional input layer. The"}, {"title": "4.2 Continuous Wavelet Transform", "content": "Continuous Wavelet Transform (CWT) is an instrument that provides an over- complete representation of a signal by letting the translation and scale param- eter of the wavelets vary continuously. CWT of the function x(t) at a scale $(a > 0) \\in \\mathbb{R}^{+*}$ and translational value $b\\in \\mathbb{R}$ is expressed by the following in- tegral (2), where $\\psi(t)$ is the continuous function called the mother wavelet, and the overline represents the operation of the complex conjugate [16]. The primary objective of the mother wavelet is to serve as the foundational function for gen- erating daughter wavelets, which are simply the translated and scaled versions of the mother wavelet. The output of the CWT consists of a two-dimensional time-scale representation of the signal.\n$\\chi_{\\omega}(a,b) = \\frac{1}{\\sqrt{a}} \\int_{-\\infty}^{\\infty} x(t) \\overline{\\psi \\left(\\frac{t-b}{a}\\right)} dt$ (2)\nUsing the method [21], we determined the three most optimal mother func- tions for our dataset: Ricker, Gaussian, and Morlet. To increase the efficiency [3], the input wavelet image for further classification consisted of three wavelets as three channels [22]."}, {"title": "4.3 Visual Transformer classification model", "content": "Transformers have emerged as a preferred model for image classification tasks, primarily attributed to their computational efficiency and scalability [35]. In this study, we applied ViT to the wavelet scalograms using real and synthetic datasets. ViT demonstrated superior classification performance compared to training solely on the smaller natural waveform dataset."}, {"title": "4.4 Time Series Transformer classification model", "content": "TST is a neural network architecture designed to process and analyze time series data. It is based on the Transformer architecture introduced by Vaswani et al. for natural language processing tasks [35], but without the decoder part of the architecture [40]. TST adapts the Transformer architecture to handle sequences of temporal data points effectively. As with the original Transformer model, TST uses self-attention mechanisms to weigh the importance of different elements within the input time series sequence. This attention mechanism allows the model to learn dependencies between different time steps and capture long- range dependencies. Positional encodings are crucial to TST and are added to"}, {"title": "4.5 Training", "content": "CGAN was trained with a batch size of 15 on 10000 epochs. The dropout was set to 0.2. The Adam optimizer was used with a learning rate of 0.0002 for the Generator and Discriminator, with the BCELoss used as the criterion function. The training time of the CGAN model was two hours using an AMD Ryzen 95900HX\u00d716 processor with an NVIDIA GeForce RTX 3070 graphics card. The model had 106 trainable parameters with a complexity of 0.026 GFLOPS. For the classification, two ResNet - ViT hybrid image models were used, which differed in the number of parameters used and have shown effective- ness in ERG wavelet classification previously [22]. The models were ViT S (ViT_small_r26_s32_224) and ViT T (ViT_tiny_r_s16_p8_224), available at the HuggingFace 'transformers' repository [38]. Both models were pretrained on ImageNet-21k and fine-tuned with ImageNet-1k with additional augmenta- tion and regularization with a resolution of 224x224 pixels. An SGD optimiza- tion with a 0.001 initial learning rate was used. ViT S and ViT T have 36.4 \u00d7 106 and 10.4 \u00d7 106 trainable parameters with complexities of 3.5 and 0.4 GFLOPS, respectively. For TST, a cross-entropy loss function with class weights adapted to address the class imbalance and an Adam optimizer with an initial learning rate of 0.0001 was used. Each model was then trained until convergence using the early stopping criteria on the validation loss with a batch size of 32. The models were evaluated using a five-fold cross-validation. Performance metrics were Balanced Accuracy (BA), Precision (P), Recall (R), F1-score, and AUC. The performance outcomes were averaged across the five-folds and are presented in Table 2. TST was trained for each strength independently, as well as on all strengths simultaneously. ViT was trained only on the entire signal dataset (all flash strengths together). The input for the ViT model was the wavelet scalograms obtained using CWT, and the input for TST was the ERG signals in the time-series representation."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "The experiments evaluated the performance of TST and ViT models under var- ious training subset conditions. We found the highest metrics of the TST model for training on signals with the higher (brightest) flash strength: BA = 0.805 for 1.114 (log cd.s.m\u00af\u00b2) flash strength. The second-best accuracy was with ViT trained on wavelets of transformed mixed signals: BA = 0.777. Introducing syn- thetic signals into the training dataset significantly enhanced the performance of all models. Specifically, the BA of TST trained on the flash strength data at 1.114 (log cd.s.m-2) increased by 10% and ViT by 13%."}, {"title": "6 Conclusions", "content": "Synthetic reference signals can enhance medical operational efficiency by offering a feasible alternative to natural. Synthesizing facilitates dataset expansion within specialized domains, enabling training resource-intensive networks such as trans- formers. Incorporating synthetic signals generated by the proposed conditional GAN offers a promising solution to address existing challenges in the domain of AI applied to the ERG. Through expanding the number of samples collected from subjects and generating synthetic waveforms provides a new opportunity to expand AI modeling of the ERG in rare or heterogeneous populations where large datasets are required. The augmentation with synthetic signals will provide the opportunity to train heavy models such as Transformers to support the early detection of retinal disorders. Furthermore, the non-personal nature of synthetic signals permits their open-source publication, making them suitable for sharing without violating patient privacy concerns. We demonstrate the first applica-"}]}