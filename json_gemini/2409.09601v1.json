{"title": "A Survey of Foundation Models for Music Understanding", "authors": ["Wenjun Li", "Ying Cai", "Ziyang Wu", "Wenyi Zhang", "Yifan Chen", "Rundong Qi", "Mengqi Dong", "Peigen Chen", "Xiao Dong", "Fenghao Shi", "Lei Guo", "Junwei Han", "Bao Ge", "Tianming Liu", "Lin Gan", "Tuo Zhang"], "abstract": "Music is essential in daily life, fulfilling emotional and entertainment needs, and connecting us personally, socially, and culturally. A better understanding of music can enhance our emotions, cognitive skills, and cultural connections. The rapid advancement of artificial intelligence (AI) has introduced new ways to analyze music, aiming to replicate human understanding of music and provide related services. While the traditional models focused on audio features and simple tasks, the recent development of large language models (LLMs) and foundation models (FMs), which excel in various fields by integrating semantic information and demonstrating strong reasoning abilities, could capture complex musical features and patterns, integrate music with language and incorporate rich musical, emotional and psychological knowledge. Therefore, they have the potential in handling complex music understanding tasks from a semantic perspective, producing outputs closer to human perception. This work, to our best knowledge, is one of the early reviews of the intersection of AI techniques and music understanding. We investigated, analyzed, and tested recent large-scale music foundation models in respect of their music comprehension abilities. We also discussed their limitations and proposed possible future directions, offering insights for researchers in this field.", "sections": [{"title": "1. Introduction", "content": "Music is intricately woven into people's daily life, meeting both emotion and entertainment needs. Its significance is clear not only on a personal level but also within social and cultural contexts, making it an essential aspect of human existence, culture, and society. In essence, efforts in understanding music could help deepen our emotional experiences, enhance cognitive abilities and enrich our lives, and further connect us with others and diverse cultures [1, 2, 3, 4, 5, 6, 7]. Nowadays, the rapid development of AI provides a new and exciting avenue to music understanding [8, 9]. A variety of techniques and models have been developed by researchers to fulfill tasks including music theory analysis, music information retrieval, music perception and cognition, performance analysis[10, 11], attempting to enable machines to understand music as intricately and authentically as humans do and provide services that people need, such as music creation, analytical research, music education, and emotional recognition in music[12].\nWith the rapid development and breakthrough of deep learning models, such as Convolutional Neural Networks (CNNs)[13], Generative Adversarial Networks (GANs)[14], Recurrent Neural Networks (RNNs)[15], and Transformers, significant success in the fields of computer vision and natural language processing (NLP) has been achieved. This success includes applications such as machine translation, speech recognition, and text generation, fundamentally transforming our way of life [16]. Meanwhile, music, often being referred as a universal language [17], shares similarities with natural language in terms of both format as input to machine and their abstract functions in emotional and conceptional expression. Therefore, researchers have become interested in applying these aforementioned deep learning models to the field of music. For instance, in the realm of music understanding, these earlier technologies were adept at handling basic tasks, including music classification and generating simple music captions. Below, we present some examples of such early AI methods."}, {"title": "2. Dataset and Evaluation Metrics", "content": "To develop and implement an artificial intelligence model algorithm, it is essential to first understand the datasets and evaluation metrics relevant to the specific task. Only after having these two elements can we proceed with the actual algorithm development. Therefore, in this section, we will introduce several representative open-source music datasets we have collected, as well as the relevant evaluation metrics in the music understanding domain, and provide detailed explanations."}, {"title": "2.1. Datasets", "content": "Selecting the appropriate dataset for different music artificial intelligence tasks is crucial, as the quality and type of the dataset will directly impact the effectiveness of the developed algorithms. We have gathered several representative music datasets to provide readers with a comprehensive understanding of music-related datasets. We describe these datasets based on various aspects, including popularity, data format, the duration of every music sample, label type, and the uniqueness of annotations (tags). By gaining an understanding of the existing music datasets, readers can quickly comprehend and choose the suitable dataset according to their specific needs to develop music understanding AI applications.\nFor clarity, the datasets were grouped or ordered according to the tags they provided in the following sections and Table 1, including genre, music mood, music caption, instruments and mixture of them. Other features of these datasets including file format and duration of samples were also reported.\nGenre\nGTZAN Genre Collection (GTZAN)5[36, 37] contains 1000 music pieces in .wav format, divided into 10 different music genres: Blues, Classical, Country, Disco, Hip-Hop, Jazz, Metal, Popular, Reggae, and Rock. There are 100 examples of each of these music genres, each of which contains a piece of music with a duration of 30 seconds. The GTZAN dataset has been downloaded up to 3.1k times.\nThe Music Genre Dataset contains 1700 music works in .mp3 format, which are derived from NetEase Music, divided into nine music genres: Symphony, Opera, Solo, Chamber, Pop, Dance and House, Indie, Soul or R&B, Rock. The duration of each clip ranges from 270 seconds~300 seconds.\nThe EDM Music Genres (EDM) is derived from YouTube music clips and contains 40,000 pieces of music which divided into 16 different genres: Ambient, Big Room House, Drum and Bass, Dubstep, Future Garage/Wave Trap, Hardcore, Hardstyle, House, Lo-fi, Moombahton/Reggaeton, Phonk, Psytrance, Synthwave, Techno, Trance, Trap. There are 2,500 examples for each music genre, 2,000 for training features, and 500 for testing features, each containing a music clip with a duration of 3 seconds.\nThe Free Music Archive (FMA) dataset 8 [38] has a large amount of data that provides a diverse sample of more than 100,000 pieces of music in .mp3 formats and is designed to facilitate research in music classification, recommender systems, and other related fields. The FMA dataset contains 161 genres, such as popular, classical, folk, etc. Each of these music samples lasts for 30 seconds.\nMusic mood\nThe Music-Classification dataset 9 is in .wav format and contains 5 different musical moods: aggressive, dramatic, happy, romantic, sad. There are a total of 10,133 examples, each of which contains a music clip with a duration of 5 seconds.\nThe Multi-modal MIREX Emotion Dataset 10 [39] has three formats, including .mp3, .txt, and .midi, and the same music clip is named in the same way in different formats. This dataset contains audio, midi and lyrics and 5 different musical moods: passionate, rollicking, literate, humorous and aggressive. There are a total of 903 examples, each of which contains a music clip with a duration of 30 seconds.\nInstruments\nChinese Traditional Instrument Sound (CTIS) 11[40, 41, 42] is a dataset containing sound information about Chinese traditional musical instruments. It includes 287 Chinese national musical instruments, including traditional musical instruments, improved musical instruments and ethnic minority musical instruments. The music files of dataset is saved by .wav format. It selects the typical sound of all conventional performance techniques, and the representative fragments in music. Meanwhile, it makes comprehensive annotation on the playing position, pitch and performance techniques of musical instruments. The duration of each track is 3 seconds.\nMIDI annotations\nThe Lakh MIDI dataset\u00b92[43] is a collection of 176,581 unique midi files, using information extracted from midi files as annotations for matching audio files. You can get a transcription of the song, as well as meter information such as beats and keys. The duration of each track ranges from 180 seconds to 300 seconds. The author did not provide the total duration of all the data.\nMusic caption\nMusicCaps 13 [44] is a dataset of 5.5k music-text pairs, containing 5521 music examples in .wav format, each labeled with a list in English (eg.pop, tinny wide hi hats, mellow piano melody, high pitched female vocal melody, sustained pulsating synth lead) and free-text descriptive subtitles written by musicians (e.g., \"This folk song features a male voice singing the main melody in an emotional mood. This is accompanied by an accordion playing fills in the background. A violin plays a droning melody. There is no percussion in this song. This song can be played at a Central Asian classical concert.\"). These texts focus on the sound of the music, not metadata. Each instance contains a piece of music with a duration of 10 seconds.\nMultiple tags\nThe MTG-Jamendo dataset14[45] is an open music auto-tagging dataset built using music available on the Jamendo platform under Creative Commons licenses and tags provided by content uploaders. This dataset contains over 55,000 full-length audio tracks annotated with 195 tags across categories such as genre, instrument, and mood/theme. The dataset offers detailed data splits and benchmarking, with audio tracks encoded in 320kbps mp3, totaling 3,777 hours of music."}, {"title": "2.2. Evaluation Metrics", "content": "When developing a music understanding model, we need to clarify the relevant evaluation indicators and metrics, the appropriate selection of which is equally vital. The evaluation system comprises both objective and subjective metrics: objective metrics provide clear, data-based comparisons, while subjective metrics help to gauge the popularity of the music generated by each model among listeners and their assessment of its artistic value. Typically, a comprehensive evaluation system combines these two types of metrics to thoroughly assess and compare the performance of various music models."}, {"title": "2.2.1. Subjective Evaluation Metrics", "content": "In testing the music comprehension ability of the large music model, subjective evaluation is crucial because it provides direct feedback on the perceived quality, emotional expression, cultural relevance, and creativity of a musical piece, aspects that are often not fully captured by objective metrics. Subjective evaluation of music datasets is typically performed using human listening tests to measure performance on various aspects of the music. The method is used in a similar way to a Turing test or a questionnaire. The mean opinion score(MOS)[46] is one of the subjective evaluation method that determines the quality of audio by subjective score of human raters, so it is flexible to test different aspects of music. The MOS score range is 0~5 points that following metrics in a five-point Likert scale. The larger the score, the better the music quality. When evaluating the MusicCaps [44] dataset, the researchers collected a total of 1,200 ratings involving 600 pairwise comparisons from each source. In the music test, participants were shown two 10-second clips and a text title and were asked which clip best described the title text on a 5-point Likert scale (e.g., \"Which of the music clips is best described by the text? A. Strong preference for option 1. B. Weak preference for option 1. C. No preference. D. Strong preference for option 2. E. Weak preference for option 2.\") They were also instructed to ignore audio quality and focus only on how well the text fit with the music."}, {"title": "2.2.2. Objective Evaluation Metrics", "content": "While human rating of music may be a reasonable method of subjective assessment of music, it faces several challenges in machine learning tasks: it is difficult to standardize, results lack consistency due to individual differences, implementation can be costly, and preference may influence outcomes. Therefore, it is necessary to add objective evaluation indicators and combine them with subjective evaluation ones.\nThe objective evaluation of a music understanding system's performance includes assessing the model's accuracy in identifying qualitative indicators such as genre and instruments, as well as its alignment with quantitative indicators based on music rules and the actual music itself. Here are metrics used for objective evaluation in the literature.\nRhythm\na) Beat Per Minute(BPM)[47]: The number of beats per minute, used to measure the speed of music. BPM is an accurate value that objectively indicates the speed of music and is a basic indicator of rhythm.\nb) Time Signature[48, 49]: The denominator indicates the note value that represents one beat, and the numerator indicates the number of beats per measure, such as 4/4, 3/4, 6/8. The time signature clearly defines the basic rhythmic unit and structure of a measure, forming the basis of rhythm.\nc)Rhythmic Complexity [50]: The complexity of rhythm, which can reflect the compositional techniques and expressiveness of the music. It is an important metric for music performance and analysis.\nKey and Harmony\na) Key[51]: The tonic and scale of the music, such as C major, A minor, etc. The key defines the pitch system and harmonic relationships of the music, forming the basis for understanding melody and harmony.\nb) Harmonic Complexity [52, 53, 54]: The complexity of chords in music. Harmonic complexity can reflect the technical difficulty and depth of expression in music, making it an important indicator for music analysis.\nInstrumentation\na) Instrumentation: The types of instruments used in music. Different combinations of instruments affect the timbre and expressiveness of the music, making instrumentation a crucial element in musical arrangement.\nb)Timbre[55]: The tonal characteristics of instruments, including frequency and time domain indicators. Timbre is an important feature distinguishing different sounds, affecting the personality and recognizability of music.\nc)Number of Voices: The number of independent parts in the music. The number of voices influences the texture and richness of the music, making it an important element in arranging and composing.\nGenre"}, {"title": "The style of music, such as Rock, Classical, Jazz, etc. Genre is a classification standard for music, helping to understand the historical background and cultural characteristics of the music.\nMusical Structure", "content": "The sectional structure of music[49], such as A-B-A form. Musical structure determines the overall layout and development of the music, making it a crucial aspect of understanding and analyzing musical works."}, {"title": "3. Large Foundation Models for Music Understanding", "content": "We have collected several large foundation models that have been released in the past years and are capable of understanding music. As shown in Table 2."}, {"title": "3.1. Music Specialized LLMs", "content": null}, {"title": "3.1.1. ChatMusician", "content": "ChatMusician [56] is a large language model that combines music comprehension and generation. Its design is based on the LLaMA 2[57] model with continuous pre-training and fine-tuning. To enable music understanding and generation, ChatMusician employs an ABC note representation which is compatible with text. Unlike traditional methods, this model does not require external multimodal neural architectures or dedicated taggers. With this approach, ChatMusician is able to understand and generate diverse musical content including text, chords, melodies, themes, and musical forms, outperforming baseline models such as GPT-4[58].\nChatMusician uses a pre-training dataset called MusicPile, which is specifically designed to inject musical capabilities into LLMs. In addition, metadata of 2 million music tracks were crawled from YouTube and associated question-answer pairs were generated. During training, the model adopts continuous pre-training and fine-tuning, initializes LLaMA 2-7B-Base with fp16 accuracy, and integrates LORA adapter [59]. In training, the ratio of music score to music knowledge and summary data is determined to be 2:1, which ensures the efficiency of music generation and understanding.\nIn terms of music understanding, ChatMusician demonstrates significant advantages and superior performance. By converting music from .wav or .mp3 files to ABC notation, music can be represented in a highly compressed format, effectively shortening the length of the music sequence and improving the model's processing efficiency. Additionally, this method avoids quantization issues, ensuring that the generated music maintains rhythmic accuracy. Furthermore, the high compatibility of ABC notation with language models enables ChatMusician to perform advanced music analysis in large language model applications."}, {"title": "3.1.2. M2UGen", "content": "M\u00b2UGen[60] is a multimodal framework that integrates large language models to understand and generate music across different modalities. This framework relies on pre-trained MERT[61], ViT[62], ViViT[63] to support comprehensive understanding and generation of music, images, and videos, respectively, and LLaMA 2[57] model to serve as the core large language model, collaborates with these encoders through a multimodal understanding adapter, enhancing capabilities in music comprehension, question answering, and generation. In particular, MERT is used as the music encoder due to its superior performance in music annotation tasks. The framework also employs specialized audio tokens, denoted as $[AUD_i]$ where $i \\in \\{1, 2, . . ., K\\}$ (with K as a hyperparameter representing the number of special audio tokens added to the LLAMA 2 model's vocabulary), to distinguish between music question-answering and generation tasks, ensuring the model can correctly produce either audio or plain text outputs.\nThe training strategy involves freezing the pre-trained encoders and the generation model, focusing on training the multimodal understanding adapter and the output projector to reduce computational load and cost. The LLaMA 2 model is trained using the LoRA [59] method, which simplifies the training process and minimizes the number of trainable parameters. For data generation, the framework utilizes MU-LLaMA [64] and MPT-7B(MosaicPretrainedTransformer) [65] models to create a multimodal-music paired dataset, addressing the scarcity of music-related task datasets. The generated MUCaps [60] dataset is used to align the encoders and decoders, ensuring effective multimodal understanding.\nBy employing the high-performing MERT model as the music encoder, the M2UGen framework ensures high-quality music input feature embeddings. Additionally, with the use of specialized audio tokens, M2UGen can flexibly produce plain text outputs for music question-answering and audio outputs for music generation within the same framework, catering to different task requirements. This versatility enables the model to excel in music understanding tasks."}, {"title": "3.1.3. MU-LLaMA", "content": "MU-LLaMA [64] is a multimodal model that utilizes large language models (LLMs) to answer music-related questions and generate music captions. The model uses pre-trained MERT and LLaMA models for encoding music features and answering music-related questions, respectively. The model also proposes a music understanding adapter to fuse features and feed them into LLaMA. In addition, to improve the model's question-answering capabilities, the model proposes a systematic approach to creating music question-answer datasets, which are essential for training the MU-LLaMA model.\nThe training strategy involves freezing the pre-trained encoders and the generation model, focusing on training the music understanding adapter to reduce computational load. The LLaMA model is the top 19 layers of the LLaMA 2-7B model. The LLaMA 2 model was fine tuned using the LORA method. For data generation, the framework utilizes MPT model to create a multimodal-music paired dataset, addressing the scarcity of question-answering task datasets. The generated MusicQA dataset [64] is designed to answer open-ended questions related to music.\nBy employing the high-performing MERT model as the music encoder, the MU-LLaMA model ensures high-quality music input feature embeddings. Additionally, with the use of MPT, MU-LLAMA can generate music question-answer pairs for training the model. This design enables the model to excel in question-answering tasks."}, {"title": "3.2. General Multimodal Large Language Models", "content": null}, {"title": "3.2.1. Qwen-Audio", "content": "Qwen-Audio15 [66] contains an audio encoder and a large language model. It is a multi-task audio large language model based on audio and text input. Qwen-Audio can provide a variety of audio (including speaker's voice, natural sounds, music, songs) and text as input, and use text as output. It can host more than 30 different audio tasks for training.\nQwen-Audio is initialized with Whisper-large-v2[67] as the audio encoder, capable of processing various types of audio, such as human speech, natural sounds, music, and songs. The encoder consists of 640M parameters, which is able to convert the original audio waveform into an 80-channel mel-spectrogram, and reduce the length of the audio representation through the pooling layer, so that each frame output by the encoder corresponds to approximately a 40 millisecond segment of the original audio signal. In addition, Qwen-Audio uses the Qwen-7B pre-training model as the initialization of the language model.\nTo overcome one-to-many interference, Qwen-Audio uses a wide range of audio datasets for collaborative training, designing a multi-task training framework to encourage knowledge sharing by regulating a series of hierarchical labels of the decoder, and through shared and specified Label separately to avoid interference."}, {"title": "3.2.2. LTU", "content": "LTU[68] is a multimodal large language model that focus on audio understanding. The model consists of four components, Audio Spectrogram Transformer (AST) [21] for encoding music, Audio Projection Layer (APL) for reshaping the feature representation, LLaMA for performing audio understanding tasks, and low-rank adapters (LoRA) for indirectly fine-tuning LLaMA parameters. In particular, LLaMA is pre-trained on a combination of a natural language corpus and a programming language corpus. In addition, to accommodate a variety of open-ended audio understanding tasks, LTU creates the OpenAQA-5M dataset, which consists of eight mainstream audio datasets and all of the data are (audio, question, answer) tuples.\nIn order for LTU to learn to answer free-form, open-ended questions based on the given audio, rather than just using the linguistic capabilities of LLaMA, the model divides the training into four phases. In the first phase, LTU freezes the parameters of AST, LORA and LLaMA, then updates the parameters of the audio projection layer only with the closed-form classification task and the acoustic feature description task. In the second phase, LTU sets the parameters of all parts except LLaMA as trainable and updates the parameters by running the training tasks from the first phase. The third and fourth phases are similar to the second, except that the third phase extends the training tasks to all closed tasks and the fourth phase extends the training tasks to all closed and open tasks. This design of gradually increasing the difficulty of the training tasks helps the model to perceive and understand the audio, which helps improve the model's performance on open-ended tasks."}, {"title": "3.2.3. SALMONN", "content": "SALMONN [69] is a multimodal model designed to understand speech, audio events, and music. It uses a dual encoder architecture with a Whisper speech encoder [67] and a BEATs audio encoder [70].Its language model module uses Vicuna LLM[71], which is a LLAMA LLM fine-tuned to follow instructions. A window-level Q-Former integrates outputs from both encoders into augmented audio tokens, improving its ability to process diverse audio inputs.\nSALMONN uses a three-stage cross-modal training method. A large number of tasks that contain key auditory information but do not require complex reasoning and understanding is used for pre-training, which enables SALMONN to learn high-quality alignment between auditory and textual information. In addition, thanks to the Activation Tuning Stage, SALMONN has significant emergent abilities."}, {"title": "3.2.4. AnyGPT", "content": "AnyGPT 16[72] builds a text-centric multi-modal alignment dataset AnyInstruct-108k. The dataset consists of 108K multi-turn dialogue samples, which are intricately intertwined with various modes. It aims to use text as a bridge to achieve all the mutual alignment between modalities and enables the model to handle any combination of multimodal inputs and outputs. It is an any-to-any multi-channel language model which can understand and generate a variety of channels, including speech, text, images and music.\nAnyGPT employs encoder (a convolutional autoencoder with a latent space quantized using residual vector quantization (RVQ)). This variant processes 32kHz mono audio and achieves a 50Hz frame rate. The generated embeddings are quantized using RVQ with four quantizers, each with a codebook size of 2048, resulting in a combined music vocabulary size of 8192. AnyGPT uses LLaMA as its language model module. In order to enable the language model to predict the entire music piece, the 4-layer music code is flattened into a causal sequence in a frame-by-frame manner. The language model starts by predicting the first four tokens of the first frame and continues predicting subsequent frames in a similar manner.\nThe field of arbitrary multi-channel large language models (LLMs) is an emerging research field. Therefore, AnyGPT lacks a dedicated benchmark to evaluate the model's capabilities in multiple dimensions, as well as to mitigate potential risks. Although multi-modal LLM with discrete representation can be trained stably, there is a higher loss compared to single-modal training, thus hindering the best performance in each mode. Moreover, AnyGPT limits the music duration to 5 seconds, which greatly limits the practical value of its audio output."}, {"title": "3.2.5. Moda Verse", "content": "ModaVerse17 [73] is a multimodal (including audio, image and video) large language model. It employs an Adaptor+Agent training strategy, aligning input features to the language model's textual space through linear projection layers while using existing text-to-X (audio, image, video) models to generate non-text outputs. For musical modality, ModaVerse utilizes a unified encoder, ImageBind[1], to process various types of inputs, which are then transformed into textual features the model can understand through linear projection layers. ModaVerse uses Vicuna LLM and LoRA fine-tuning technique to reduce training costs and achieve corresponding fine-tuning effects.\nAdditionally, ModaVerse avoids complex feature-level alignment by directly operating at the natural language level. It completes model training in a single phase through instruction-following tuning, reducing the need for multi-stage training and thus improving training efficiency.\nIn terms of audio input processing, ModaVerse not only converts audio into textual descriptions but also understands and generates related images or videos. For instance, it can transform the sound of an animal into an image of that animal. This multimodal comprehension capability stems from the efficient alignment of different modal data at the model's input stage, enabling ModaVerse to perform well in music understanding."}, {"title": "4. Experiments and Analysis", "content": null}, {"title": "4.1. Implementation Details", "content": "We conducted tests on eight existing open-source large foundation models to evaluate their music understanding capabilities, covering both simple and complex music understanding tasks. Due to AnyGPT's inability to perform specific prompts, it was only tested for Music Caption according to the official method.\nFor simple music understanding tasks, we employed music classification methods. We randomly selected 100 samples from each of the four datasets: GTZAN, Free Music Archive, Music_Classification, and Multi-modal MIREX Emotion, totaling 400 samples. These samples were then input into the models using a standardized prompt, and the models' responses were recorded.\nFor advanced music understanding tasks, we used the MusicCaps dataset. This dataset features professional music evaluations provided by music experts, including sound quality, instruments and performance, emotions, and suitable contexts, making it a high-quality labeled dataset. We randomly selected 147 samples from this dataset, input the test samples along with standardized prompts into the models, and obtained the corresponding captions. These responses were then compared with the reference texts of the actual labels using rouge scores to evaluate the models' advanced music understanding capabilities.\nClassification and text similarity metrics are as follows:"}, {"title": "Classification Metrics", "content": "We use accuracy to characterize the ability of the model for music classification. The reason why we do not use precision and recall is that in some cases, the response of the model may not belong to any of the data labels. But this situation cannot be abandoned, as it also reflects to some extent the ability of the model."}, {"title": "Accuracy", "content": "Accuracy is the proportion of correctly classified samples to the total number of samples. As shown in equations (1):\nAccuracy = $\\frac{\\text{Number of Correctly Classified Samples}}{\\text{Total Number of Samples}}$                                                                                                                                                                                                                                                                                                                           (1)"}, {"title": "Text similarity Metrics", "content": "For the text similarity metrics, we use the ROUGE, because the ROUGE has multiple variants such as ROUGE-N and ROUGE-L, which can measure the text similarity at different granularity. This multidimensional method enables the ROUGE to comprehensively reflect the similarity of the text at different levels such as words, phrases and subsequences.\nROUGE\nROUGE (Recall-Oriented Understudy for Gisting Evaluation)[74] is a set of metrics used to evaluate the quality of summaries and translations in natural language processing. ROUGE measures the overlap between the produced texts and a set of reference texts.\nROUGE-N: Measures n-gram overlap. It calculates the number of matching n-grams between the candidate and the reference texts. As shown in equations (2).\nROUGE-N=$\\frac{\\sum_{S \\in \\text{References}} \\sum_{\\text{gram}, eS} \\text{Count}_{\\text{match}}(\\text{gram})}{\\sum_{S \\in \\text{References}} \\sum_{\\text{gram}, eS} \\text{Count}(\\text{gram})}$             (2)\nROUGE-L: Measures the longest common subsequence (LCS). It captures sentence-level structure similarity by measuring the longest matching sequence of words between the candidate and reference texts. As shown in equations (3)-(5)."}, {"title": "ROUGE-L = F\u2081 =", "content": "ROUGE-L = F\u2081 = $\\frac{(1 + \\beta^2) \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}$       (3)\nPrecision = $\\frac{\\text{LCS}(X, Y)}{|X|}$                                                                                                                                                                                                                                                                                                                                          (4)\nRecall = $\\frac{\\text{LCS}(X, Y)}{|Y|}$                                                                                                                                                                                                                                                                                                                                          (5)"}, {"title": "4.2. Experimental Results", "content": "We conducted tests on eight open-source multimodal large foundation models for music under- standing. The downstream tasks performed include genre classification, mood classification, and music captioning.\nDue to the fact that objective evaluation metrics based on the music itself, such as rhythm and musical structure, are generally only included in .midi format with relevant labels and not in more common .wav and .mp3 format datasets, we did not perform tasks related to music and rhythm understanding. For genre classification, we used the GTZAN and FMA datasets. For mood classification, we used the Music_Classification(MC) and Multi-modal MIREX Emotion(MMME) datasets. Note that, due to the models sometimes providing results outside the given label categories during the identification of music genres, we are unable to calculate metrics such as recall and precision. Therefore, we have analyzed the classification performance solely based on the accuracy of identification. In Table 3, we can observe the performance of various models in Genre and Mood classification tasks. Qwen-Audio excelled in the genre classification task, achieving an accuracy of 80% and 75% on the GTZAN and FMA datasets, respectively. However, its performance in mood classification was weaker, particularly on the MC dataset. LTU performed exceptionally well in both genre and mood classification tasks. Its genre classification accuracy on the GTZAN and FMA datasets was 61% and 70%, respectively, and its mood classification accuracy on the MC and MMME datasets was 58% and 79%, the best among all models. MU-LLaMA performed well in the mood classification task but had average performance in the genre classification task. ModaVerse and M2UGen performed poorly in both tasks."}, {"title": "4.3. Analysis", "content": "Intuitively, it was anticipated that models specifically designed for music would perform better across various tasks. However, the results showed that general multimodal language models outperformed dedicated music models in music understanding tasks.\nMusic Classification\nThe high classification accuracy of these two models is due to their extensive task training on large datasets, which significantly enhances their generalization ability. These models also provide good answers when asked different questions. In contrast, M\u00b2UGen and ModaVerse have not addressed the basic music understanding task of music classification and lack training on music datasets with classification labels, resulting in poor performance in music classification tasks. Specifically, M2UGen often produces results in a single category or fails to provide any options, while Moda Verse inaccurately generates results across multiple labels and cannot provide accurate answers.\nMusic Caption\nIn general, the ROUGE score is relatively low because the dataset references include advanced musical reasoning, not just descriptions of musical phenomena (e.g., a piece of music suitable for opera house performance). The best-performing model is still Qwen-Audio, due to its multi-task pre-training, which includes captioning and question-answering tasks, and allows it to excel in long-form Q&A. Another key factor is its supervised fine-tuning, which improves the model's alignment with human intentions.\nM2UGen, which performed poorly in classification tasks, shows decent performance in music caption- ing because it was trained on a large number of music-caption data pairs. However, these data pairs were generated based on MU-LLaMA and lacked fine-tuning for advanced reasoning, limiting M2UGen's per- formance. The most unsatisfactory performance among all models is from AnyGPT. Although AnyGPT can handle multimodal inputs and outputs and perform related tasks, it cannot use prompts in a per- sonalized manner for specific tasks, and it has higher losses compared to single-modal training. These factors contribute to AnyGPT's poor performance in specific modal tasks.\nIn summary, we find that models with more input and output modalities, such as AnyGPT and M2UGen, do not perform well in music understanding. This may be due to the increase in modalities without a corresponding increase in the number of parameters, or because the weights of different modal- ities interfere with each other. Models that perform better generally do not include image modalities, meaning they do not have weights for visual information. Additionally, among models with similar input and output modalities, those that perform poorly often lack diverse training data. Some models focus on a single category of music data, such as ChatMusician, which primarily covers Irish music, leading to bias in the model. Another reason for poor performance is the lack of diversity in music instructions. One important reason why Qwen-Audio and the LTU models perform well is that both cover a variety of tasks during training, enabling them to handle different tasks effectively. These factors are significant contributors to the performance differences among models."}, {"title": "5. Discussion", "content": "After comparing and studying existing general multimodal large foundation models and specialized music understanding models, we suggest that AI holds immense potential in the field of music under- standing. Although our comparative results have demonstrated that AI methods can enable machines to understand and appreciate music in"}]}