{"title": "Random Tree Model of Meaningful Memory", "authors": ["Weishun Zhong", "Tankut Can", "Antonis Georgiou", "Ilya Shnayderman", "Mikhail Katkov", "Misha Tsodyks"], "abstract": "Traditional studies of memory for meaningful narratives focus on specific stories and their seman- tic structures but do not address common quantitative features of recall across different narratives. We introduce a statistical ensemble of random trees to represent narratives as hierarchies of key points, where each node is a compressed representation of its descendant leaves, which are the orig- inal narrative segments. Recall is modeled as constrained by working memory capacity from this hierarchical structure. Our analytical solution aligns with observations from large-scale narrative recall experiments. Specifically, our model explains that (1) average recall length increases sublin- early with narrative length, and (2) individuals summarize increasingly longer narrative segments in each recall sentence. Additionally, the theory predicts that for sufficiently long narratives, a universal, scale-invariant limit emerges, where the fraction of a narrative summarized by a single recall sentence follows a distribution independent of narrative length.", "sections": [{"title": "Introduction", "content": "Narratives are a naturalistic form of stimuli for prob- ing the structure and organization of human cognitive functions [1\u20136]. In the classic work of Bartlett [7], a close link between narrative memory and comprehension was established, and both were shown to vary greatly across individuals. Subsequent studies established that narra- tive comprehension and recall is strongly linked to a sub- ject's semantic knowledge, such as schemas, that help to interpret new information in the context of prior knowl- edge (see e.g. [8\u201311]). It appears therefore that the com- plexity of memory for meaningful information cannot be captured by simple physics-style models with a few gen- eral postulates and mathematical tractability. Indeed, in the psychological literature, most studies aim to relate memory for narratives to their linguistic organization and do not predict generic quantitative features of narrative recall (see e.g. [12, 13]), in contrast to studies of memory for random material where many quantitative experimen- tal and theoretical results are obtained (see e.g. [14]). In particular, some of the authors previously developed a model that predicted the universal relation between the average number of recalled items and the number of items retained in memory after list presentation, and this rela- tion was well supported by experimental results [15]. \u03a4\u03bf enable a similar quantitative description of memory for meaningful material, we recently performed large-scale experiments on recognition and recall of narratives of various lengths [16]. Unsurprisingly, we observed that recall of narratives is better than that of random lists in terms of the fraction of material being recalled. More in- teresting however are the qualitative differences between these two cases; in particular, people tend to recall in- formation from the narrative in the same order as it is presented [17], as opposed to much more variable recall order of random lists [18\u201320]; for longer narratives, peo- ple compress progressively bigger pieces of information from the narrative into single sentences, resulting in a sublinear growth of recall length with the narrative size [16].\nIn the current study, we show that despite the more complex nature of meaningful recall, the above statistical regularities can be captured by mathematical modeling. To this end, we propose a simplified model of narrative recall that focuses on describing the statistical aspects of recall across large populations of people rather than addressing individual differences between people and nar- ratives.\nOur model is based on two basic principles. The first principle describes the memory encoding of a narrative. Following multiple previous studies, we consider a nar- rative as a linear sequence of clauses, i.e., the shortest meaningful pieces of text (see e.g. [21]). We assume, how- ever, that after people comprehend the narrative, they form a tree-like memory representation of it where, at each level of the tree, information units of a given level of abstraction are represented [22\u201324]. In particular, the root of the tree represents the whole narrative; the upper-level nodes represent pieces of the narrative that corre- spond to its main keypoints [25]; and lower levels rep- resent shorter pieces corresponding to progressively finer points [26\u201329]. Moreover, we assume that a piece corre- sponding to a particular keypoint is split into pieces cor- responding to the finer points that elaborate the original keypoint, resulting in a tree structure of a narrative. We see the evidence for this type of memory representation in the fact that people can easily summarize a familiar narrative at different levels of detail, which we believe is achieved by retrieving the node representations at dif- ferent tree levels, e.g., retrieving upper nodes evokes the most abstract memory of a narrative in terms of its most general points. An implicit assumption made here is that continuous stimuli, such as a text, is represented by a dis-"}, {"title": "Ensemble of random trees", "content": "crete set of nodes. This is also supported by experiments that show humans naturally segment sequential stimuli (e.g. movies, personal experiences) into discrete units [30\u201335]. There is also extensive support for the claim that narratives, from folk tales [36] to oral narratives [21, 37], have such a hierarchical, tree-like structure. We argue that this hierarchy is reflected in the mental rep- resentation.\nThe second principle reflects the way people recall a given narrative based on its memory representation de- scribed above. We assume that people mentally traverse the tree from upper to lower levels, keeping the interme- diate nodes in working memory, in order to constantly maintain the integrity of the narrative in mind. It is in this respect that we believe the recall of meaningful narratives is fundamentally different from the recall of random lists of words. In particular, working memory capacity limits the number of nodes that can simulta- neously be kept in mind, thus limiting the number of levels that can be reached during recall. We further as- sume that when reporting a particular node during recall, people use a single clause to express the content of the corresponding piece.\nIt seems plausible that a specific instantiation of the memory tree depends both on the narrative structure and on the way a particular person comprehends and"}, {"title": "Comparison between theory and experiment", "content": "subsequently memorizes it. Hence, in order to translate the above two principles into a quantitatively constrained model, we make a highly simplifying assumption about the statistical ensemble of trees encoding different narra- tives of a particular length in the minds of different peo- ple. We build this ensemble by a recursive self-similar process in which a narrative is split into up to K pieces by the random placement of K \u2212 1 boundaries, and each piece is similarly split into up to K pieces, and so on. We begin this process from the whole narrative and continue it recursively from one layer to the next until splitting stops (see details in SI). K is the parameter of the model, and for the rest of the paper, we choose K = 4, i.e., the typical working memory capacity [38], because we believe that it corresponds to the most probable way for people to comprehend the narrative at the most abstract level given limitations of working memory (see [39] for a pos- sible neural mechanism of chunking subject to working memory constraints). In Fig. 1(a), we show an example tree constructed by this process for a narrative consisting of 42 clauses.\nWe then illustrate the recall process playing out on this tree, assuming that people systematically traverse the memory representation by following each route be- ginning from the root, until they either reach a leaf or a lowest reachable (Dth) level (the levels above the grey shaded area in Fig. 1(a)). We again consider D = 4 be- cause it corresponds to working memory capacity. The retrieval trajectory is shown in red on Fig. 1(a). One can see that some of the nodes reached by a retrieval trajectory contain single narrative clauses, while others contain multiple clauses (up to 12 for the illustrated tree). Since we assume that each node, no matter its level, is recalled with a single recall clause, the length of recall is defined by the number of retrieved nodes, while each re- call clause summarizes the corresponding number of nar- rative clauses. It is important to emphasize that while the ensemble of memory representations described above involves a random set of trees, the hypothesized recall process itself is deterministic, i.e. the number of recall clauses and the pieces of the narrative that each one of them corresponds to are fully determined by the memory representation of the narrative.\nThe model described above can be simulated numeri- cally and allows for an asymptotic mathematical solution as we show below. The length of recall (the number of clauses in recall, C) averaged over the ensemble of trees with N clauses exhibits a characteristic saturating form for increasing N, see Fig. 1(b). We also computed the distribution of the number of narrative clauses summa- rized by single recall clauses (i.e., compression ratios of different recall clauses), shown in Fig. 1(c) for compres- sion ratios up to 10 and for different N. One can see that, as expected, for longer narratives compression ratios of"}, {"title": "Analytical approximations", "content": "recall clauses tend to increase.\nTo obtain the analytical approximations for these nu- merical results, we derive a recurrence relation for the dis- tribution of the number of clauses represented by nodes at a given level l, P(n^{(l)}), with\n$$P(n^{(1)}) = \\delta(n^{(1)} \u2013 N),$$\n since the upper node represents all of the narrative clauses retained in memory after acquisition. When a node at level l with n^{(l)} clauses is split randomly into K nodes of level l + 1 (some of them empty), the distri- bution of the size of each of the resulting nodes can be computed by the \u201cstars and bars\" method ([40]) by con- sidering all possible configurations of n^{(l)} stars and K-1 bars and only counting the number of stars to the left of the left-most bar. This results in the following recursive expression (for n^{(l+1)} < n^{(l)}, 0 otherwise):\n$$P(n^{(l+1)}\\vert n^{(l)}) = \\frac{Z_{K-1}(n^{(l)} \u2013 n^{(l+1)})}{Z_K(n^{(l)})}$$\nwhere $$Z_K(n) := \\binom{n+K-1}{K-1}$$.\nApplying this recursive expression and integrating over intermediate levels results in the distribution of the node sizes in level D,\n$$P(n^{(D)}) = \\sum_{0<n^{(D-1)}\nNote that Eq. (4) defines a Markov chain, see SI for more details. In this version of the stars and bars model, some of the nodes may end up being empty (if two or more bars are adjacent), and the average number of nonempty nodes at level D will define the average length of recall:\n$$C = K^{D-1}(1 \u2013 P(n^{(D)} = 0)),$$\nwhile the re-normalized distribution of non-zero node sizes corresponds to the distribution of compression ra- tios introduced above. For K = D = 4, these analytical results are very close to numerical simulations, as shown in Fig. 1(b,c). In particular, one can see from Fig. 1(c) that as the narrative gets longer, the distribution of com- pression ratios shifts to the right, i.e. recall clauses tend to summarize progressively bigger pieces of the narrative. For long narratives, the distribution of compression ra- tios, normalized by narrative size (s = \\frac{n^{(D)}}{N}), asymptotes to a universal scaling function f(s) (see derivations in SI):\n$$P(n^{(D)}) = \\frac{1}{N}f(\\frac{n^{(D)}}{N}),$$\n$$f(s_D) = \\int \\frac{1}{s} \\prod_{l=1}^{D-1} ds_l \\rho (s_{l+1}|s_l) p(s_1),$$\n$$ \\rho (s_{l+1}|s_l) := \\frac{K-1}{K} \\binom{s_l}{s_{l+1}} \\left(\\frac{s_{l+1}}{s_l}\\right)^{K-2}.$$\nIn this limit, f(s) is a scale-invariant probability den- sity function for the normalized compression ratio s, which in particular implies that the average compres- sion ratios grow proportionally to narrative length. The shape of the scaling function computed with Eq. (7) with K = D = 4 is shown in Fig. 1(d) in red, together with finite-N simulations from the random tree model."}, {"title": "Experimental data collection", "content": "We now compare our theoretical predictions to exper- imental data on 11 narratives ranging from 19 to 194 clauses in length, including 8 narratives from our pre- vious study [16] and 3 new narratives selected for this study. All of the narratives, except for two, were chosen from the study by Labov [21] or generated by a Large Language Model (LLM), GPT-4 from OpenAI [41], using Labov's narratives as templates. The original narratives were edited for clarity and spelling. The two remaining narratives were borrowed from [42]. We used the clause segmentation from [21] for narratives sourced from that work, and this clause segmentation was also transferred to GPT4-generated narratives that used the originals as a templates. Each narrative was presented to 100 sub- jects on their computer screens via the internet platform Prolific (www.prolific.com), who were instructed to re- call it as closely as possible to the original. We followed the free recall experimental protocol described in detail in [16]. We used GPT-4 to segment the recalls into single clauses, using the same prompts as in [16] (reproduced in SI). To obtain compression ratios for all recall clauses, we instructed GPT-4 to map each recall clause back to the original narrative and then counted the number of nar- rative clauses it was mapped to (see SI for details of this analysis). We reason that the narrative clauses to which a given recall clause is mapped correspond precisely to the segment of the narrative encoded by the correspond- ing node, which was recalled by this clause. The crucial remaining step needed to compare these results to the theoretical predictions above is to estimate the number of clauses in the memory representation of a given nar- rative for each subject (N). We compute this estimate as the total number of narrative clauses into which at least one recall clause is mapped. We assume that the remaining narrative clauses are either not remembered by the subject, or are not integrated into the tree mem- ory representation of the narrative and hence cannot be recalled. The average values of N obtained for each nar- rative are plotted against their lengths in Fig. 2(a) and are observed to be close to half the total length of the narrative. With this assumption, experimental results for the average length of recall and compression ratios up to 10 are quite similar to theoretical predictions, as shown in Fig. 2(b,c), even though no parameters were tuned after the fact to fit the model predictions to the data. Fig. 2(d) shows that for longer narratives, the dis- tribution of compression ratios relative to N indeed ap- proaches the scale-invariant form predicted by Eqs. (6)- (7), with the speed similar to corresponding simulations in the same range of Ns shown in Fig. 1(d).\nWe emphasize here the crucial role of LLM-assisted"}, {"title": "Discussion", "content": "techniques in analyzing recall data, particularly the pro- jection from each individual recall into the narrative de- scribed above. Since there is no objective ground truth for this procedure, the authors manually performed the mapping for one of the recalls (see SI for details). We observed a substantial variability between individual pro- jections; however, the LLM-computed compression ratios for different recall clauses were closely aligned with the average compression ratios obtained from the authors' projections.\nIn summary, we showed that despite its highly com- plex nature, some statistical trends in meaningful nar- rative recall can be captured by a mathematical model based on two basic principles: random tree-like memory representations and a working memory-limited determin- istic recall process. In particular, the model correctly predicts the average length of recall and the distribution of compression ratios over recall clauses as a function of the size of the narrative tree representation. While we do not have a theoretical way to calculate the latter for each narrative/subject, we could estimate the size of the tree using LLM-performed mapping of each recall back into the narrative. We found that the average tree size of a narrative, estimated this way, is very close to one half of the total number of clauses in a narrative for our set of narratives. We currently do not have any theoretical understanding of this observation, and it would be in- teresting to study whether this simple relation holds for other types of narratives.\nThe model has two parameters: the maximal splitting ratio in the random tree ensemble of narrative represen- tations and the maximal depth of the recall process. Im-"}, {"title": "Supplementary Information for \"Random Tree Model of Meaningful Memory\"", "content": "The ensemble of random trees is built by a self-similar splitting process: it begins with the root node that contains all N clauses and progressively splits it into smaller and smaller nodes at lower levels. At each level l, a node of size n^{(l)} randomly splits into K nodes at level l + 1, of sizes n_i^{(l+1)} \u2265 0, i = 1, ..., K, such that\n$$\\eta^{(l)} = \\sum_{i=1}^K \\eta_i^{(l+1)}.$$\nThis is achieved by randomly inserting K \u2212 1 dividing bars among n^{(l)} clauses. More precisely, n^{(l)} clauses and K \u2212 1 divisions are randomly placed in n^{(l)} + K \u2013 1 positions, and n_i^{(l+1)} is defined as the number of clauses to the left of the first bar for i = 1, between (i - 1)th and ith bars for i = 2, ..., K \u2212 1 and to the right of the last bar for i = K. A child node with n_i^{(l+1)} clauses stops splitting further, and becomes a leaf, if\n$$n_i^{(l+1)} = n^{(l)}$$ or $$n_i^{(l+1)} \\leq 1.$$\nThe entire splitting process ends once all the children nodes have met the stopping condition.\nTo simulate recall, as described in the main text, we cut all the nodes below the Dth level and retrieve all the leaves of the remaining tree. The size of a retrieved node is then considered to be the compression ratio for the corresponding recall clause, as explained in the paper. To obtain the numerical results presented in Fig. 1(b)-(c), for each value of N, 10,000 random tree realizations were constructed with the splitting parameter K = 4, and for each tree, recall was simulated with D = 4. For the results shown in Fig. 1(d), 20 different values of N were sampled uniformly on a log scale from N = 10 to N = 100, and 10,000N random realizations were simulated for each N in order to obtain good statistics for the tail of the compression ratios distribution."}, {"title": "Analytical solution of the model", "content": "Mathematically, the splitting process described above is akin to the problem of the weak composition of an integer n into K-tuples of nonnegative integers, a process called \"Stars and Bars\" [40] (S&B), which counts all possible configurations of splitting n \"stars\" into K bins by placing K - 1 \"bars\" among them. However, the S&B process is only an approximation of the tree generation process described in Eq. (S.1)-(S.2) because of the first stopping condition in Eq. (S.2). In S&B, a node stops splitting only when it becomes non-splittable, i.e., when the second condition in Eq. (S.2) is met. However, as n^{(l)} becomes large, the probability of having n_i^{(l+1)} = n^{(l)} becomes increasingly small, and the approximation using S&B converges to the exact distribution. In fact, for the range of N considered in this paper (main text Fig. 1-2), there is little difference between the theory and the simulation, and the approximation is nearly exact. It is in this sense that we refer to the solution obtained below using S&B as an analytical solution to Eq. (S.1)-(S.2).\nThe total number of weak compositions is given by the S&B Theorem 2 [40],\n$$\u0396\u03ba(n) = \\binom{n+K-1}{K-1}$$\nwhich states that there are ZK(n) ways to partition n indistinguishable objects (stars) into K distinguishable adjacent bins (whose boundaries correspond to bars), by placing K \u2212 1 bars randomly among the n + K \u2212 1 total positions, where each position can either be occupied by a star or a bar.\nNow consider splitting n^{(l)} stars into K bins. Due to the symmetry among the bins, without loss of generality, we can focus on the first bin. The total number of configurations with the first bin having n^{(l+1)} stars is ZK-1(n^{(l)} \u2013 n^{(l+1)}),"}, {"title": null, "content": "since there are n^{(l)} \u2013 n^{(l+1)} remaining stars that have to be divided into K 1 remaining bins. Therefore, the probability of observing n^{(l+1)} stars in the first bin of the (l + 1)th level is given by\n$$P(n^{(l+1)}\\vert n^{(l)}) = \\frac{Z_{K-1}(n^{(l)} \u2013 n^{(l+1)})}{Z_K(n^{(l)})}$$\nWe are interested in computing the distribution of node sizes (or compression ratios) at level D, p(n^{(D)}), which can be obtained by repeatedly applying Eq. (S.4) starting from level 1:\n$$P(n^{(D)}) = \\sum_{\\begin{subarray}{c}0<n^{(D-1)}\\\\2 Recall Mapping"}, {"content": "System Prompt\nYou are an AI designed to map a human recall split into clauses and an original narrative split into segments, both provided in JSON format. Your task is to generate a JSON output that maps the clauses to the relevant segments. Clauses can be mapped to multiple segments, and the same segment can be mapped multiple times. If a recall clause cannot be mapped to any narrative segment, return an empty list. Here is an example of the desired output format:\nUser Prompt"}, {"title": "Example narrative (L = 194)", "content": "Segmented clauses from the narrative\nOriginally from Mary Costa: \u201cThe death of her youngest daughter\""}, {"title": "Comparison between LLM- and Human-generated Mappings", "content": "To compare the mappings performed by GPT-4 with those by humans, seven human evaluators (all the authors and one colleague) performed the projections described above for the example recall in Section E2 of the \"The death of her youngest daughter\" narrative presented in Section E1, and compression ratios for all recall clauses were obtained as described in Section D. The results are shown in Fig. S2. The scatter plots display the compression ratios for consecutive recall clauses based on the individual human mappings, overlaid on the corresponding compression ratios obtained with the GPT-generated map.\nIn general, mappings can be highly subjective, resulting in significant variance among human participants. Despite this substantial variability, the compression ratios produced by the LLM closely align with the corresponding averages over human evaluators, as shown in Fig. S2."}]}