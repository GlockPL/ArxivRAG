{"title": "Leveraging Speaker Embeddings in End-to-End Neural Diarization for Two-Speaker Scenarios", "authors": ["Juan Ignacio Alvarez-Trejos", "Beltr\u00e1n Labrador", "Alicia Lozano-Diez"], "abstract": "End-to-end neural speaker diarization systems are able to address the speaker diarization task while effectively handling speech overlap. This work explores the incorporation of speaker information embeddings into the end-to-end systems to enhance the speaker discriminative capabilities, while maintaining their overlap handling strengths. To achieve this, we propose several methods for incorporating these embeddings along the acoustic features. Furthermore, we delve into an analysis of the correct handling of silence frames, the window length for extracting speaker embeddings and the transformer encoder size. The effectiveness of our proposed approach is thoroughly evaluated on the CallHome dataset for the two-speaker diarization task, with results that demonstrate a significant reduction in diarization error rates achieving a relative improvement of a 10.78% compared to the baseline end-to-end model.", "sections": [{"title": "1. Introduction", "content": "Speaker diarization, a crucial task in multiple speakers scenarios, involves identifying different speaker segments within audio recordings [1]. Traditional approaches typically segment audio into speech and non-speech chunks using Voice Activity Detection (VAD) [2] before extracting speaker features from each speech segment. Utilizing VAD is essential as speaker embedding extractors are not typically trained to effectively represent silence. Among the commonly used speaker representations are i-vectors [3, 4], d-vectors [5], x-vectors [6] and ECAPA-TDNN x-vectors [7], which encapsulate information about speaker identity, speech style, and other related attributes [8].\nIn the traditional modular diarization systems, these speaker embeddings are then clustered using conventional clustering algorithm such as K-means [9], Agglomerative Hierarchical Clustering [4, 6], spectral clustering [10] and Bayesian HMM clustering of x-vector sequences (VBx) [11]. However, traditional diarization strategies often struggle with overlapping speech, where multiple speakers talking simultaneously pose a challenge to isolate and cluster individual speaker information. Also, these speaker representations are often extracted using a fixed-size sliding window. The window size can significantly affect the diarization performance: while using larger window sizes helps capture more speaker information, it increases the likelihood of including multiple speakers, complicating clustering approaches. In contrast, smaller window sizes reduce this risk, but may not provide enough data to extract reliable speaker information. This trade-off between capturing speaker-specific details and minimizing overlap-induced errors underscores the importance of carefully selecting window sizes in diarization systems [12].\nTo address the challenge of speaker overlap, recent advancements in speaker diarization have introduced end-to-end models, treating the task as a multi-label classification problem [13, 14]. Models like the self-attentive end-to-end neural diarization (SA-EEND) [15, 16] integrate various aspects like voice activity and overlap detection, showing promising results at handling these issues natively. However, these models encounter difficulties with scenarios involving varying numbers of speakers. To overcome this limitation, newer models like end-to-end neural diarization with encoder-decoder attractors (EEND-EDA) [17, 18] have been developed to adapt dynamically to a variable speaker number. Additionally, hybrid systems combining end-to-end approaches with clustering algorithms have emerged achieving improved performance across diverse scenarios [19, 20, 21, 22, 23]. Similarly, in [24, 25] speaker turns are detected at word level by an automatic speech recognition (ASR) system to subsequently perform speaker clustering diarization.\nAnother method to tackle the diarization task is to condition voice activity detection with speaker embedding information. This approach leverages speaker embeddings like i-vectors or x-vectors, concatenating them with acoustic features of speech segments and feeding them into the model. Systems such as Target-Speaker Voice Activity Detection (TS-VAD) [26] utilize this technique. Additionally, methods described in [27, 28] demonstrate the effectiveness of jointly training EEND and TS-VAD components.\nIn this study, inspired by the capabilities of TS-VAD, we aim to enhance the speaker discriminative potential of the EEND-EDA diarization system by incorporating speaker information extracted from a pre-trained ECAPA-TDNN [7] x-vector extractor.\nWe have explored different ways to integrate the speaker embeddings at various points of the architecture, aiming to help the model to better leverage the speaker information. Additionally, we have focused on critical speaker embedding extraction hyperparameters such as the window size, for optimal performance in this task. Furthermore, to address the potential limitations of speaker embeddings in representing silent segments, we applied an oracle VAD to the speaker embeddings during the diarization training process, ensuring a better handling of silent segments within the model. Finally, we used an external VAD during testing for a comprehensive evaluation independent of oracle VAD labels.\nThe rest of this paper is structured as follows: Section 2 explains the speaker diarization pipeline, detailing the architecture and training scheme of the proposed systems. Section 3"}, {"title": "2. Methods", "content": "2.1. Review of End-to-End Neural Diarization with Encoder-Decoder Attractors\nHere we provide a concise overview of the end-to-end diarization framework known as EEND-EDA [17]. The EEND module processes input data comprising a sequence of log-scaled Mel-Filterbank features $\\mathbf{X} = [\\mathbf{X}_1, \\mathbf{X}_2, ..., \\mathbf{X}_T] \\in \\mathbb{R}^{T \\times F}$, where T is the sequence length and F the feature dimension. These features are passed through either bi-directional long short-term memory (BLSTM) [13], Transformer [15, 16], or Conformer [29] encoders to obtain embeddings $\\mathbf{e} = [\\mathbf{e}_1, \\mathbf{e}_2, ..., \\mathbf{e}_T] \\in \\mathbb{R}^{T \\times D}$ at each time step.\nTo determine a flexible number of attractor points from variable-length embedding sequences, an LSTM-based encoder-decoder architecture is employed. The embedding sequence $(\\mathbf{e}_t)_{t=1}^{T}$, with D dimensions, is fed into the unidirectional LSTM encoder.\n$\\mathbf{h}_0, \\mathbf{c}_0 = \\text{LSTM}_{\\text{encoder}}(\\mathbf{e}_1, ..., \\mathbf{e}_T)$ (1)\nAttractors are generated during the decoding process.\n$\\mathbf{h}_s, \\mathbf{c}_s, \\mathbf{a}_s = \\text{LSTM}_{\\text{decoder}}(\\mathbf{h}_{s-1}, \\mathbf{c}_{s-1}, \\mathbf{0})$ (2)\nThese attractors are defined as $\\mathbf{A} = [\\mathbf{a}_1, ..., \\mathbf{a}_S] \\in \\mathbb{R}^{D \\times S}$ where S is the number of speakers. The probability of the presence of an attractor $\\mathbf{a}$ is determined using a fully-connected layer with a sigmoid activation function, defined as:\n$p_s = \\frac{1}{1 + \\text{exp}(-(\\mathbf{w}^T \\mathbf{a}_s + b))}$ (3)\nHere, $\\mathbf{w}$ and $b$ represent the trainable weights and bias of the fully-connected layer, respectively. During training, ground truth labels are defined based on the actual number of speakers S.\n$l_s = \\begin{cases} 1 & \\text{if } s \\in \\{1, ..., S\\} \\\\ 0 & \\text{if } s > S + 1 \\end{cases}$ (4)\nThe attractor existence loss $L_a$ is calculated between $\\hat{p}$ and the ground truth labels $y$ using the binary cross entropy defined as:\n$\\mathcal{H}(y_t, \\hat{y}_t) := \\sum_{s=1}^{S} (-y_{t,s} \\log \\hat{y}_{t,s} - (1 - y_{t,s}) \\log(1 - \\hat{y}_{t,s}))$ (5)\nTherefore:\n$L_a = \\frac{1}{1 + S} \\mathcal{H}(\\mathbf{l}, \\mathbf{p})$ (6)\nFinally, the diarization loss is computed using the Permutation Invariant Training (PIT) scheme [30]. This loss is calculated between $\\hat{y}_t$ and the ground truth labels $\\mathbf{y}_t = [y_{t,1}, ..., y_{t,S}]^T \\in \\{0, 1\\}^S$ as:\n$L_d = \\frac{1}{T} \\min_{\\pi \\in \\text{Perm}(1,...,S)} \\sum_{t=1}^T \\mathcal{H}(y_t, \\hat{y}_{\\pi(t)})$ (7)\nThe final loss is defined by the diarization loss and the attractor existence loss.\n$\\mathcal{L} = L_d + \\alpha L_a$ (8)\nWhere $\\alpha$ is the weighting parameter for the attractor loss and it has a value of 1 for training and 0.1 for adaptation.\n2.2. ECAPA-TDNN speaker embedding extractor\nThe ECAPA-TDNN architecture [7] builds upon the well-established x-vector topology [6] and integrates several enhancements to produce more robust speaker embeddings. During training, the network is optimized to directly minimize the cosine distance between speaker embeddings by optimizing the AAM- softmax loss [31]. This approach makes it less dependent on complex scoring backends such as Probabilistic Linear Discriminant Analysis (PLDA) [32]. The ECAPA-TDNN model is trained using data from VoxCeleb1 and VoxCeleb2 [33, 34], with additional data augmentation from the RIRs 1 and MUSAN [35] datasets, further enhancing its performance and generalization capabilities.\nTo ensure alignment between acoustic features and speaker embeddings, we construct two sequences with the same number of frames. The acoustic feature sequence is $\\mathbf{X} = [\\mathbf{X}_1, \\mathbf{X}_2, ..., \\mathbf{X}_T] \\in \\mathbb{R}^{T \\times F}$, and the corresponding speaker embedding sequence is $\\mathbf{B} = [\\mathbf{b}_1, \\mathbf{b}_2, ..., \\mathbf{b}_T] \\in \\mathbb{R}^{T \\times E}$. We use the same acoustic features as in [17], and we adjust the sliding window of the ECAPA-TDNN embedding extractor to match the number of frames of the acoustic feature sequence."}, {"title": "2.3. Proposed methods", "content": "We propose three methods to combine the acoustic features $\\mathbf{X}$ and the speaker embedding $\\mathbf{B}$ sequences as input to the SA-EEND-EDA model. In Figure 1, the three proposed methods are depicted, distinguishable by the color of the arrows (i.e., red arrows indicate method a), where Mel-Filterbank (MFbank) goes to the SA-EEND module and the speaker embeddings to the EDA module).\n2.3.1. Speaker embeddings into EDA module\nAs previously discussed in Section 2.1, the original encoder-decoder attractor (EDA) module receives as input the output vector sequence from the last SA-EEND transformer encoder block. This vector sequence, is then used for both the diarization task (optimized using the PIT loss) and the speaker identification or existence task within the EDA module. Specifically, the EDA aims to identify the active speakers within the corresponding frame segment. Once the attractors are calculated, they are used to condition the computation of the diarization posteriors applying the dot-product operation with the same encoder output vector sequence $\\mathbf{e}$. Our objective is to examine the behavior of the model by directly introducing the speaker embeddings $\\mathbf{B}$ calculated with the ECAPA-TDNN extractor into the EDA module as shown in Figure 1 a). These embedding sequence $\\mathbf{B}$ are encoded with a single transformer encoder block to maintain the original input dimension of the EDA module, ensuring that:\n$\\mathbf{e}_{spk} = \\text{Emb Encoder}(\\mathbf{B}) \\in \\mathbb{R}^{T \\times D}$ (9)\nIn this setup, the SA-EEND transformer encoder takes just the Mel-Filterbank acoustic feature sequence $\\mathbf{X}$ as input. Thus, we aim to determine if including the speaker embedding information in the EDA module helps to better determine the attractors and represent speaker variability.\n2.3.2. Speaker embeddings into SA-EEND encoder\nWe also explore how effective speaker embeddings $\\mathbf{B}$ are on their own as input to the end-to-end speaker diarization system, without using any acoustic features, and if the model is able to refine the speaker discriminative information within these embeddings for the task of diarization. To do this, we use these ECAPA-TDNN embeddings $\\mathbf{B}$ as input features, extracted at the same time resolution as $\\mathbf{X}$. In this setup, as in the SA-EEND-EDA baseline [17], encoder output vector sequence $\\mathbf{e}$ are directly fed into the EDA module as shown in Figure 1 b).\n2.3.3. Concatenation of speaker embeddings and MFbank into SA-EEND encoder\nTaking inspiration from the TS-VAD model's approach of combining speaker embeddings with acoustic features, our setup aims to work on this idea by concatenating the ECAPA-TDNN embedding sequence $\\mathbf{B}$ with Mel-filterbank acoustic feature sequence $\\mathbf{X}$, and using it as input to the EEND-EDA model. This concatenated sequence is denoted as:\n$\\mathbf{C} = \\mathbf{X} \\oplus \\mathbf{B} \\in \\mathbb{R}^{T \\times (F+E)}$ (10)\nWhere $\\oplus$ refers to the concatenation operation. With this configuration, the model can utilize both feature sequences simultaneously, leading to an integration of acoustic and speaker-related information. This setup is illustrated in Figure 1 c)."}, {"title": "2.4. Silence frames handling", "content": "Speaker embeddings are not typically trained to effectively discriminate silence [36]. Extracting these embeddings directly with a sliding window without applying VAD introduces the silence variability into the speaker embedding sequence, complicating the subsequent speaker modeling within the diarization system. Hence, to train the three proposed methods, we use the oracle VAD segmentation to the speaker embeddings beforehand, replacing the embeddings that correspond to silent segments by a zero vector of the same dimension. We also examined the scenario where this oracle VAD segmentation was not used during training to assess whether the model could learn to discard the silence segments without needing the information of the VAD.\nAdditionally, we evaluated the models in CallHome without using VAD, using the oracle VAD segmentation, and with an external VAD. As external VAD, we used Kaldi's [37] energy-based VAD."}, {"title": "3. Experimental setup", "content": "3.1. Data\nFor training the models we generated the sim2spk (simulated conversations with 2 speakers) dataset, using the generation algorithm outlined in [38], choosing an speech overlap ratio of 34.4%. Specifically, sim2spk was constructed from recordings extracted from Switchboard-2 (Phases I, II, and III), Switchboard Cellular (Part1 and Part2) [39], and NIST Speaker Recognition Evaluation (2004, 2005, 2006, and 2008) corpora [40, 41, 42, 43], all sampled at 8 kHz. As in [16], each utterance was augmented with background noise from the MUSAN dataset [35], as well as having a 50% probability of being convoluted with a randomly chosen simulated room impulse response from the RIR\u00b2 dataset [44]. To adapt the models trained on simulated data to real conversations, we reserved the \"Adapt\" subset of the 2-speaker CallHome CH1-2spk telephone conversation dataset [45]. Finally, for evaluation purposes, two distinct datasets were created: a simulated one, generated with the same process as described before; and the remaining \"Test\" CH2-2spk subset of the 2-speakers CallHome dataset.\nThe details of the different datasets for training, testing and adaptation are shown in Table 1.\n3.2. Experimental settings\nThe acoustic features extracted are 23-dimensional log-Mel-Filterbanks with a frame length of 25 ms and a frame shift of 10 ms. Each feature vector was concatenated with context data from the previous seven and subsequent seven frames. Concatenated features are subsampled by a factor of ten. Consequently,"}, {"title": "4. Results", "content": "Our first proposed method, described in section 2.3.1 and referred to as \"Speaker Embeddings into EDA Module,\" investigates how speaker embeddings influence the diarization performance of the EEND-EDA model when used as input to the EDA module. Results from our experiments (see table 3) indicate that, despite an increase in model parameters, incorporating speaker embeddings into the EDA module does not enhance performance compared to the baseline EEND-EDA model. This suggests that the speaker embeddings may not provide more discriminative information when used in this manner within the diarization task, than the one learned by the original EEND-EDA model.\nOur second proposed method, explained in section 2.3.2 and referred to as \"Speaker Embeddings into SA-EEND Encoder,\" evaluates the impact of utilizing only these speaker embeddings as input to the SA-EEND encoder, and therefore as the"}, {"title": "4.1. Results of the concatenation of speaker embeddings with acoustic features", "content": "In Table 5, the results obtained by the models using the proposed method described in section 2.3.3 and so-called \"Concatenation of speaker embeddings and MFbank into SA-EEND encoder,\" are presented.\nThe performance without finetuning the models to the Call-Home \"Adapt\" subset is very similar to the baseline, although experiments trained and evaluated with oracle VAD segmentation, using 3 encoder blocks and a 3-second extraction window, as well as with 4 encoder blocks and a 1-second extraction window, achieve a DER of 9.01% and 9.86%. This represents a relative improvement over the baseline (10.09% DER in this condition) of 10.7% and 2.27%, respectively.\nWhen finetuning the models to real data using the Call-Home \"Adapt\" subset, and evaluating using oracle VAD segmentation, their performance is significantly improved, surpassing the baseline. Specifically, the best-performing model uses 4 encoder transformer blocks and extracts embeddings with a 1-second sized window. It achieves a DER of 7.2%, while the baseline EEND-EDA has a performance of 8.07% after adaptation to real conversations. This represents a relative improvement of 10.78%. For a more realistic perspective, without oracle VAD segmentation during inference and using instead the external energy-based VAD, this model achieves a DER of 7.6%. The relative improvement in this case reduces to 5.82%, highlighting the importance of a reliable VAD for leveraging proper information from these embeddings in the diarization task.\nExperiments where oracle VAD segmentation was not used during training, were not as satisfactory due to the silence variability leaking into the speaker embeddings. In this case, the experiment yielding the best DER uses 4 encoder layers and a 1-second window (the same configuration of the best model when trained with oracle VAD segmentation). The results obtained evaluating the model without oracle VAD is 12.55%, whereas with oracle VAD segmentation, it is 8.34%. It should be noted that in all experiments, whether trained with oracle VAD segmentation or not, an improvement is observed when using oracle VAD or an external VAD at evaluation time. This indicates that the EEND-EDA model's ability to discern between speech and non-speech segments is quite limited and that using external models to assist in this task improves the overall performance. Additionally, our experiments confirm the hypothesis that using"}, {"title": "5. Conclusions", "content": "In this study, we explore the integration of speaker embeddings with acoustic features for the diarization of two speakers in telephonic data within end-to-end neural diarization approaches. Our findings indicate that incorporating speaker information embeddings into the well-known EEND-EDA model leads to substantial improvements in diarization performance, in particular when using as input a concatenation of speaker embeddings with acoustic feature. Our results also showcase the importance of correctly handling silence segments and optimizing window sizes for extracting speaker information for the diarization task. The use of speaker information enhances the model's ability to discriminate between speakers, while keeping the overlap handling capabilities of end-to-end neural systems, leading to further improvements in speaker diarization performance with respect to the EEND-EDA baseline."}]}