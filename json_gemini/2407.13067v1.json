{"title": "Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness", "authors": ["Harsh Kumar", "Suhyeon Yoo", "Angela Zavaleta Bernuy", "Jiakai Shi", "Huayin Luo", "Joseph Williams", "Anastasia Kuzminykh", "Ashton Anderson", "Rachel Kornfield"], "abstract": "Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.", "sections": [{"title": "1 INTRODUCTION", "content": "Changing behavior to improve wellness is challenging, even when the potential benefits are obvious [21, 131, 132]. For example, people are often motivated to participate in traditional yoga classes, try meditation routines discovered in books or magazines, participate in community-led mindfulness sessions, or explore wellness exercises on digital platforms such as Instagram and YouTube. However, forming habits and following through with these exercises proves to be a significant hurdle [136, 199]. This lack of perseverance can be attributed to several factors, including lack of accountability and the struggle to overcome ambivalence [21, 89, 90]. Nevertheless, there is evidence that social support can contribute to the success of behavior change initiatives by increasing accountability, motivation, and emotional sustenance [5, 19, 109, 137, 145].\nThe 'Supportive Accountability' model proposed by Mohr et al. [135] suggests that digital health promotion tools are more likely to be effective when complemented by human support. This support goes beyond simple companionship, incorporating benevolence, relevant expertise, and tailored guidance. An example could be a coach that provides information, scaffolding the learning process with factual and procedural knowledge [135, 161]. On the other hand, there can be support that operates through inquiry and reflection, which encourages users to engage in self-exploration and critical thinking about their behaviors and underlying motivations [93, 182]. Integrating social and expertise-based support into technology may facilitate long-term behavioral change. However, traditional forms of support are inaccessible to many due to the costs and complexities of providing support [3]. Human support is often the most expensive and complex part of a digital health intervention [10, 106, 134]. Some people also have preferences not to interact directly with others or lack existing supportive relationships that can be leveraged to support new practices.\nRecently, advances in deep learning have led to the development of LLMs such as GPT-4 [2, 26]. These models are large neural networks trained on vast collections of textual data, enabling them to process inputs in natural language and generate human-like text. Conversations, essential for providing social support among humans, can now be convincingly mimicked by LLM-based conversational agents, simulating the personalized and open-ended dimensions of human-to-human interactions that, until now, have been challenging to achieve in automated conversation systems [129, 164, 176]. LLMs have potential to play a multifaceted role in helping individuals achieve behavior change. Although significant research has explored social support in various contexts, less is known about the effectiveness of LLM-based agents for long-term behavior change in ecologically valid settings [176].\nOur research draws from existing CSCW and social support literature to understand the role of LLM-based agents in providing social support to adopt wellness practices [3, 109, 133]. Specifically, we focus on evaluating the impact of LLM agents on two key aspects: the users' intention to participate in wellness exercises and their actual engagement with these exercises. We also gather user perspectives on the support provided by LLM agents to understand how people think about collaborating with LLM agents around behavior change. We address the following research questions.\nRQ1 (Intention to Practice Exercises): How do LLM agents influence users' intention to engage with wellness exercises?\nRQ2 (Engagement with Exercises): What impact do LLM agents have on the actual engagement with wellness exercises, as indicated by the initiation and completion rates of exercises?\nRQ3 (Measures of Well-being): What impact do LLM agents have on immediate and long-term well-being measures (such as stress and mindfulness scores) related to wellness exercises?\nWe focus on mindfulness as a case study, a practice increasingly recognized for its extensive benefits in improving mental health, reducing stress, improving emotional regulation, and fostering overall well-being [107, 113, 142, 179]. A popular method of learning mindfulness is through tutorial videos. This approach allows people to learn about mindfulness and practice it at their own pace and convenience. However, despite the accessibility and flexibility of these self-guided mindfulness programs, they often face challenges with attrition. Studies have shown that consistent engagement in such self-directed mindfulness interventions can be challenging, and many participants do not complete the programs [13, 162, 190]. Estimates suggest that 23-39% people drop out within 10 weeks of the program [114].\nThere is an opportunity to enhance engagement with self-directed wellness practices through additional social support. The nature of this social support can vary to meet individual needs at any point in the behavior change process [130]. Informational support and validation can play crucial"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "roles in helping people understand the benefits of change and motivate them to begin practicing. This type of support can provide the necessary clarity and encouragement that individuals might need at various times during their wellness journey. In addition, planning support and decisional reinforcement can help individuals stay on track and effectively integrate practices into their daily routines. Self-reflection can allow participants to introspect and evaluate their progress [62, 112, 150]. Reflective support is particularly valuable as it can deepen the understanding of personal experiences and promote sustained behavior change [14, 15, 17]. In this paper, we use LLM agents to provide informational and reflective support to investigate whether these agents can meet varying needs for support throughout the mindfulness journey of an individual.\nWe conducted a formative study involving online crowdworkers (N = 502) to capture user perspectives and intentions about mindfulness practice when supplemented with different forms of LLM agent support (RQ1). As design probes, we developed two types of LLM agents using GPT-3: the Information Chatbot, designed to provide detailed information about mindfulness practices, and the Reflection Chatbot, designed to help participants reflect on their mindfulness experiences and plan their future mindfulness sessions. Additionally, our study incorporated a tutorial video as a fundamental element of many digital interventions promoting mindfulness and similar wellness behaviors. Overall, the design of the study was a 2 (Information Chatbot: present vs. absent) x 2 (Tutorial Video: present vs. absent) x 2 (Reflection Chatbot: present vs. absent) factorial between-subjects experiment. Although both chatbots were perceived to be useful, user feedback suggested a preference for more relaxed and friendly conversational interactions (small talk), which they found particularly lacking in the Information Chatbot. Self-reported intentions to practice mindfulness were higher among participants who first interacted with the Information Chatbot before watching the video and those who engaged with the Reflection Chatbot after the video, compared to the control group that did not use the chatbots. Participants mentioned repetitiveness in the responses as a point of discontent for both chatbots.\nHowever, as highlighted in previous research [153, 170], such preliminary insights require validation through real-world deployments to gauge the alignment between expressed intentions and actual user behavior. To this end, we conducted a 3-week deployment study with 54 participants interested in learning about mindfulness (RQ2). The core intervention included instructional videos and email reminders every two days to practice mindfulness. In a 2 (Sociable Information LLM agent: present vs. absent) x 2 (Reflection LLM agent post Exercise: present vs. absent) between-subjects experiment, participants were randomized to have access to the two LLM chatbots. Based on the participants' feedback in the formative study, the Information Chatbot was developed into a sociable informational companion chatbot, Mindy, characterized by a friendly persona. Half of the participants were randomly assigned to receive access to Mindy in their reminder emails. We continued using the Reflection LLM agent, which was enhanced based on initial feedback and used immediately after each instructional video. Again, half of the participants were randomly given access to this reflective component after each session. We upgraded to GPT-4 from GPT-3 to address issues like repetitiveness mentioned by participants in the formative study. Participants who had access to the Sociable Information LLM, Mindy, showed a notable increase in engagement with a 18% higher initiation rate and a 12% higher completion rate of daily exercises compared to those without access to the agent (p < 0.01). Interestingly, incorporating the Reflection LLM agent following video exercises did not significantly affect initiation or completion rates. This finding was somewhat unexpected, as the Reflection LLM, with its specific objective of aiding in planning and reflection, was expected to play a more active role in enhancing exercise adherence.\nThese findings suggest that LLM agents can potentially improve people's intentions to participate in self-guided wellness exercises and their engagement with them. However, the findings also highlight that these LLM agents should be tested in a longitudinal setting to gauge the actual"}, {"title": "2 RELATED WORK", "content": "Our work builds on long-standing research on influencing engagement in behavior change interventions, the role of social support in maintaining this engagement, technology-enabled mindfulness, conversational agents, and the growing literature on the interaction between humans and LLMs.\n2.1 User Engagement in Behavior Change Interventions\nEnsuring sustained engagement with behavior change tools is crucial for their effectiveness [87, 199]. Most behavior change interventions, especially those delivered through the digital medium without human support, suffer from high dropout and attrition [53, 94, 199]. Engagement alone may not directly result in behavior change, but it serves as an essential precondition [68, 87, 199]. In an era marked by fragmented attention, distractions, and shifting priorities, a key challenge is designing technologies that maintain user interest without contributing to the noise of daily life [44, 70, 180]. Karapanos [87] proposed the use of \u2018Creating checking habits' powered by instant information rewards, similar to those in social media updates and emails. Although there is a risk that these habits could lead to addictive behaviors [67, 87], they also promise to foster beneficial engagement, such as encouraging more frequent exercise through interactions with the app.\nGouveia et al. [66] explored how behavior-change applications might create checking habits by continuously updating their feedback, similar to strategies employed in the computer-gaming and airline industries. This continuous update could maintain the informational reward from app checking, which is believed to be a primary driver of the formation of checking habits. Furthermore, enhancing engagement through social interactions, which we cover in the following subsection, could be another strategy to sustain engagement. Erickson and Kellogg's [52] concept of \u201cSocial Translucence\u201d suggests shifting the responsibility of behavior change to families and social ties, promoting awareness of each other's behaviors through technology, rather than relying solely on the technology's persuasive capabilities. This shift towards social awareness and nudging within personal relationships could lead to a more sustainable change in behavior [12, 88, 135].\nBehavior change is a broad concept that can capture changes in how individuals prevent or manage health conditions, and their day-to-day practices and habits that have implications for general well-being. Here, we focus on the latter, considering the specific context of in initiating and sustaining mindfulness practice.\n2.2 Social Support in Wellness Behavior Change Journeys\nSocial support is essential in influencing health behaviors and outcomes, acting as a cornerstone of physical and mental well-being [109, 181]. Studies, such as those conducted by Tay et al. [178], have highlighted the multifaceted benefits of social relationships, highlighting their importance in chronic illness self-management and mitigating suicidal tendencies. The expanding research in this field has led to diverse conceptualizations and operational frameworks for social support, reflecting its complexity and the variety of forms it can take [75, 116].\nBarrera and Ainlay [11] provided a comprehensive classification of social support into six categories, including: Material Aid, signifying the provision of tangible resources like money or goods; Behavioral Assistance, which involves sharing tasks or providing physical labor; Intimate Interaction, encompassing traditional counseling behaviors such as listening and expressing empathy; Guidance, offering advice or information; Feedback, providing insights into an individual's behavior, thoughts, or feelings; and Positive Social Interaction, facilitating leisure and relaxation through"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "social activities. Some forms of support are particularly relevant to the context of supporting wellness exercises. Material aid and behavioral assistance are less relevant for our specific context of self-directed mindfulness exercises. Guidance and Feedback can be synthesized into a singular category as 'Directive Guidance' [73]. In contrast, intimate and positive social interactions can be viewed as elements of \u2018Non-directive Support' [65].\nPositive social interaction may be especially critical in well-being interventions, offering fun and relaxation opportunities essential for a balanced lifestyle. Supportive peer relationships, as found by Roach [155], significantly improved adolescents' mental health, indicating the broader implications of positive social interactions across different demographics. Although several self-directed programs and apps are available in the particular case of mindfulness, a common challenge remains in the lack of sustained use of these tools or strategies in daily life [114, 190]. Mohr et al. [135] proposed a model for \u201cSupportive Accountability\u201d, highlighting the critical role of social support in enhancing adherence and engagement in behavior change programs. The model suggests that incorporating social support elements in technology-enabled services significantly increases their effectiveness in promoting long-term behavioral changes [177, 189]. Recent CSCW research underscores the importance of interpersonal relationships in the management of mental health concerns and the crucial role of technology in providing ongoing support [28, 138].\nHowever, the scalability of providing high-quality social support presents a major challenge, as traditional methods, such as individual, in-person instruction, are not feasible on a large scale. Therefore, there is a need to explore and implement scalable solutions that can provide high-quality social support effectively and sustainably to a broader audience. Our research looks into how LLM-based agents can effectively embody different forms of Directive and Non-directive support, one that is informational versus one that encourages self-reflection, for a nuanced understanding of the varying needs for social support during behavior change.\n2.3 Technology-Enabled Mindfulness\nThe HCI and CSCW literature has extensively explored the role of technology in supporting meditation to practice mindfulness [179]. Technology serves as a valuable tool, allowing users to practice in their own time and space and aiding users in achieving and sustaining focus on inner or outer experiences, such as breathing [146, 148], tactile movements [80, 158], or even monitoring brain activity [96, 97, 159]. These techniques encompass a spectrum of applications, including Virtual Reality (VR), wearables, and online interventions. Wearable devices have been investigated for their role in mindfulness practice, with a focus on monitoring biometrics using integrated sensors. Research has explored sensor placement options like the thumb, head, or infraclavicular locations to determine the most sensitive accelerometer location for measuring body motion during short meditation sessions [156]. Additionally, MeditAid, a headset providing binaural feedback for enhancing meditative states [159], and the MindfulWatch, which tracks real-time breathing patterns during meditation using smartwatch technology [71], have emerged. These wearables extend to wrist-worn devices that can distinguish meditation periods by analyzing heart rate variability [6, 35]. In workplace settings, brain-sensing wearable devices like the MUSE-S\u2122\u2122 have been utilized for guided meditation sessions to reduce stress [64].\nWhile wearable devices, including VR head-mounted units, show promise in various applications, their reach among diverse users in daily contexts, particularly those new to mindfulness seeking accessible entry points, remains limited. Conversely, numerous online programs and applications have been developed to promote mindfulness practices, leveraging the convenience of smart-phones, tablets, and desktop systems for personal use. These apps offer various features, including guided audio meditations, daily mindfulness activities, and training programs. For instance, apps offering guided audio meditations were associated with reduced blood pressure and significant"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "enhancements in overall well-being, distress reduction, and the alleviation of job-related stress [20]. Additionally, smartphone-based interventions provide users with simple daily mindfulness activities, further contributing to improved mental well-being [77]. Walsh et al. [186] highlighted that mindfulness training through smartphone apps can enhance subjective well-being, attentional control, and interoceptive integration. An intervention using a mindfulness meditation app, Calm, was associated with improved sleep quality, reduced alcohol consumption (particularly binge drinking), increased physical activity, and healthier dietary choices, including increased fruit and vegetable consumption [79]. However, these apps suffer from high attrition rates [114].\n2.4 Conversational Agents for Behavioral Interventions\nConversational agents, or chatbots, have been increasingly deployed in various domains to positively influence user behavior, from improving sleep habits [154] and offering psychiatric counseling [144] to promoting healthier diets, exercise routines [78], smoking cessation [30], and other health-related behavior change. Lukoff et al. [118] introduced a chat-based food journaling tool to help families achieve their healthy eating goals, exemplifying the utility of chatbots in nurturing healthier lifestyle choices. In addition, chatbots have been used in professional settings to help users reflect on experiences [93] or manage psychological transitions related to work [188], ultimately strengthening recovery and overall well-being. Educational initiatives have also seen the integration of pedagogically focused chatbots designed to improve learning outcomes [101]. For example, Zvereva et al. [205] proposed a dialogue-based approach to assess and improve student motivation.\nThe role of chatbots extends into the mental health domain, where they have been used to manage symptoms and improve quality of life [1]. In particular, Woebot [57], a chatbot based on Cognitive Behavioral Therapy (CBT), demonstrated effectiveness in a randomized controlled trial, highlighting the potential of chatbots to provide mental health support. In addition, chatbots have facilitated self-disclosure [110, 111] and promoted self-compassion [108], crucial components of personal well-being and behavioral change interventions.\nOur investigation is based on existing research surrounding the design and implementation of conversational agents for behavior change and well-being [31]. Using LLMs for chatbots presents promising prospects and novel challenges [82]. LLMs' ability to generate adaptive human-like responses offers an exciting frontier for behavior change technologies. However, LLMs also introduce ethical and safety concerns that surpass those associated with traditional chatbots, necessitating rigorous evaluations to understand their implications fully [45, 61, 197]. There are also questions regarding how users feel about social support delivered by an LLM. Through laboratory and field deployments, our work addresses these critical questions, exploring the effectiveness of LLM-based chatbots in real-world settings. In addition, we explore the applicability and desirability of different forms of LLM-provided support, contributing to the literature on the types of automated social support to which users are the most receptive [22, 103, 128, 139].\n2.5 Situating Work in Broader CSCW & HCI Literature\nAs we saw in previous sections, the current literature emphasizes the pivotal role of human support in facilitating wellness practices, particularly in self-directed exercises (Section 2.1 and 2.2). Within the CSCW community, there is a growing emphasis on understanding the needs and preferences of people who pursue self-directed wellness and the development of supportive technologies [95, 105, 130, 133]. Human facilitators in various formats, from personal coaches available in popular mindfulness apps to instructors leading group sessions, have significantly improved engagement with interventions and outcomes [63, 109, 135]. These facilitators offer personalized guidance, emotional support, and accountability, aspects crucial to the success of such interventions [63]. Despite their effectiveness, human-led support faces limitations in cost, scalability, and accessibility"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "[3, 10, 106, 134]. In contrast, LLM-based agents, which can be made available everywhere and at a lower cost, emerge as a promising alternative. These agents can provide continuous judgment-free social support without time and location constraints and maintain user anonymity and privacy, making them a practical solution for wide-scale implementations [127, 198].\nTrained on extensive datasets, LLMs have the potential to adapt their guidance to various user contexts and challenges, a capability that is still being explored and refined [34]. They can also support people with social anxiety or who, for various reasons, are uncomfortable interacting with human coaches about their well-being [122]. LLMs can serve as informative guides [98], help users acquire new skills by offering precise and detailed explanations that reflect the user's specific contexts and concerns [202], and resolve uncertainties about effectively carrying out wellness practices [119, 197]. They can provide proactive check-ins, inquire about progress, and assist in planning future wellness activities, thus promoting accountability. Furthermore, their ability to engage in empathetic and supportive dialogues positions them as a 'friendly presence, offering encouragement and reflection [122].\nIn our research, we investigate the capabilities of LLM agents within the context of behavior change interventions through a large-scale single-session study and a longitudinal field deployment. Our investigation focuses on two forms of LLM agents: an informational agent, designed to provide relevant knowledge and guidance, and a reflection-promoting agent, designed to facilitate self-reflection. These functions mirror the crucial roles traditionally fulfilled by human coaches in guiding people through behavior change processes [16, 135]. While acknowledging the irreplaceable value of human interaction, there is an emerging interest in examining how human-AI collaboration can improve digital support systems [166\u2013168]. Such hybrid models represent a future direction where AI and human efforts collaborate with each other in providing wellness support.\n3 FORMATIVE STUDY DESIGN\nWe conducted our formative study with 502 participants recruited from Amazon Mechanical Turk [92, 165] to understand user intentions regarding use of LLM agents for mindfulness. We asked users about their perceptions of the strengths and weaknesses of the video and the two LLM agents. The study was conducted in January 2023 and was approved by the Research Ethics Board of a public university in North America. Figure 1 summarizes the overall design of our formative study. The methodologies and rationale behind the study design are elaborated upon in the following sections.\n3.1 Information Chatbot (Using LLM to Provide Information on Mindfulness)\nSince the release of ChatGPT, there has been a surge in the use of LLMs for information retrieval. LLMs have been increasingly integrated into web search platforms [175], such as by BingChat\u00b9 and PerplexityAI\u00b2. However, exploring LLM-based search engines' implications and capabilities in facilitating behavior change remains limited. In our study, we employed Information Chatbot as a design probe to provide mindfulness-related information to participants via dialogues. Drawing from existing literature on LLM prompts [40, 102], educational theories [38], and extensive preliminary testing, we crafted a specific system prompt for the GPT-3 model:\nThe following is a conversation with a Mindfulness instructor. The instructor teaches and provides information about different mindfulness activities to the Human. The instructor"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "explains different activities clearly and provides examples wherever possible. The instructor has a sense of humor, is fair, and is empathetic.\nIn addition to the system prompt, the model began with an initial greeting to establish the conversational context: \u201cHello. I am an Al agent designed to act as your Mindfulness instructor. I can answer any questions you might have related to Mindfulness. How can I help you?\u201d. To guide participants in engaging with the chatbot, we provided example queries such as \u201cCan you help me learn about Mindfulness?\u201d. A typical interaction (taken from this study) between a participant and LLM1 (Information Chatbot) is depicted in Figure 1, illustrating the nature and flow of the dialogue.\n3.2 Instructional/Tutorial Video for Mindful Breathing\nThe instructional video we used was modified from a previous study validated by experts, focusing on mindfulness-based practices. It introduced participants to mindfulness meditation and included exercises emphasizing the internal awareness of bodily sensations or breath in a curious and non-evaluative manner. We specifically selected mindful breathing for its foundational role in mindfulness practices.\nWhile the video specifically addressed mindfulness, its application extends as a representative model for various behavioral change interventions. Similar video-based approaches are instrumental in physical fitness, mental resilience training, dietary habits, and more, where visual and instructional components are essential for the participation and learning of participants [56, 124]."}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "3.3 Reflection Chatbot (Using LLM to Aid in Mindfulness-Related Self-Reflection)\nIn addition to providing informational support, LLMs can facilitate self-reflection and planning for post-learning activity practice. Research indicates that students' learning outcomes improve when they reflect on their metacognitive strategies and beliefs [151, 194]. Similarly, self-reflection has been recognized as an effective mechanism to encourage behavior change [69, 104, 150]. However, the role of LLMs in enhancing self-reflection to change behavior remains relatively unexplored. To investigate this, we used LLM2 (Reflection Chatbot) as a probe to assist participants in reflecting on their understanding of mindfulness and scheduling their daily mindfulness exercises. With GPT-3, we configured this chatbot with a system prompt analogous to that used with Information Chatbot:\nThe following is a conversation with a Mindfulness instructor. The instructor asks open-ended reflection questions to the Human to solidify the Human's understanding of Mindfulness and helps them plan when they can practice Mindfulness in their daily lives. The instructor has a sense of humour, is fair, and empathetic.\nAdditionally, we incorporated an initial greeting into the bot's prompt, \u201cHello. I am an Al agent designed to act as your Mindfulness instructor. I am here to help you reflect on your learnings. How can I help you?\u201d. To facilitate the start of the conversation, we provided sample questions to participants such as \u201cCan you help me plan when I can practice Mindfulness?\u201d. An illustrative dialogue between a participant (part of the study) and the bot is presented in Figure 1.\n3.4 Participants\nOn Mechanical Turk, 866 individuals responded to our Qualtrics survey. Post-attention check filtering resulted in 502 legitimate responses. Table 1 displays the diverse demographic profile of the participants, with a significant majority (93.12%) having prior experience in mindfulness. Participants were randomly assigned to one of eight experimental conditions (2x2x2). They dedicated up to 20 minutes to designated activities and another 10 minutes to subsequent follow-up questions.\n3.5 Study Procedure\nParticipants navigated to a Qualtrics\u00b3 survey, where they were briefed on the study objectives: exploring and evaluating various mindfulness introduction techniques to foster well-being. Upon understanding the purpose of the study, the participants gave their informed consent. They were also provided contact information for crisis management centers, a precautionary measure for any emotional distress encountered during the task. Participants reported their experience with mindfulness and were then introduced to the Core Mindfulness Message (Figure 1). The message described the concept of mindfulness and its benefits. This message served as a proxy for the type of mindfulness descriptions that one might typically find on social media platforms or news articles. Subsequently, the participants were randomly assigned to one of the 2 (Information Chatbot: Present vs. Absent) x 2 (Tutorial Video: Present vs. Absent) x 2 (Reflection Chatbot: Present vs. Absent) experimental conditions (depicted in Figure 1). They interacted and provided qualitative feedback for each assigned component, one after the other. After interaction with designated conditions, participants were asked to express their likelihood of engaging in mindfulness practices, rated on a scale from 1 (not likely at all) to 7 (extremely likely), with the prompt \u201cHow likely are you to practice Mindfulness after undergoing these learning exercises?\u201d. Furthermore, to deepen our understanding of their experience, we posed several open-ended questions, after each bot/video, and after the entire process, such as:"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "3.6 Ethical Considerations\nConducting research in the domain of mental health inherently presents various ethical considerations. A primary concern is that experimental interventions might cause negative emotional responses, either as a direct result of introspective processes, inadvertently through misunderstandings of the material presented, or through inaccurate responses generated by LLMs. To minimize these risks, we proactively cautioned crowdworkers about potentially encountering unsettling content and communicated their option to discontinue participation at any point. Additionally, we provided mental health support resources, such as contact information for the National Suicide Prevention Lifeline, at the beginning of the study.\n3.7 Data Analysis\nThe qualitative data analysis was carried out by two researchers employing the open coding method [32, 33], which involved a detailed examination of the responses to identify significant patterns and concepts. Subsequently, thematic analysis was applied to distill these patterns into clear, recurring themes [23]. The process included familiarizing with the data, generating initial codes, identifying recurring themes, and defining and naming these themes. The example themes include the benefits, challenges, and use cases of practicing mindfulness with chatbots and videos. The researchers engaged in two rounds of collaborative discussions to ensure an unbiased analysis, focusing on validating the coding process and the identified themes.\nThe composition of our research team reflects a broad spectrum of perspectives, which we acknowledge may influence the interpretation and direction of our findings. The gender representation within the team is approximately balanced. Team members hail from various cultural backgrounds, including North America, eastern Europe, the global south, and Asia. Educational levels among the team range from undergraduate students to doctoral students and professors. The areas of research represented include computer science, psychology, education studies, communication, and behavior change. This composition may help triangulate a more comprehensive understanding of the collected data.\nTo analyze the Likert-style ratings, we conducted a factorial ANOVA with interactions to evaluate the impact of the information chatbot, tutorial video, and reflection chatbot on participants' self-reported intentions to practice mindfulness. Further, we conducted exploratory post-hoc comparisons of different conditions using the Tukey method, adjusting for multiple comparisons."}, {"title": "4 FORMATIVE STUDY FINDINGS", "content": "4.1 Comparing Intentions for Practicing Mindfulness Across Different Conditions (RQ1)\nFigure 2 compares the intentions of the participants to practice mindfulness in various conditions, including the control group. The results of ANOVA indicated a significant main effect of Information Chatbot (F(1,494) = 5.489, p < .05) suggesting that the interaction with the information chatbot affected the participants' intention to practice mindfulness. Similarly, the main effect of Tutorial Video was significant (F(1,494) = 5.506, p < .05) indicating that the inclusion of video also influenced participants' intention. Furthermore, Reflection Chatbot also had a significant main effect (F(1,494) = 7.603, p < .01).\nNo significant interaction effects were found between Information Chatbot and Tutorial Video (F(1,494) = 0.458, p = .50), Information Chatbot and Reflection Chatbot (F(1, 494) = 0.524, p = .47), or Tutorial Video and Reflection Chatbot (F(1,494) = 2.252, p = .13). Additionally, the three-way interaction between Information Chatbot, Tutorial Video, and Reflection Chatbot was not significant (F(1,494) = 1.509, p = .22).\n4.2 User Perspectives on Different Modes of Interaction\n4.2.1 Instructional Video. Most participants shared their willingness to watch the video to relieve stress, concentrate, and practice mindfulness. Participants mentioned social contexts (such as before a work presentation) and specific times in their daily routines when the video would be helpful. This includes contexts throughout the day, such as in the morning, before starting work, after finishing work, and before bedtime.\nParticipants expressed a positive response to the video. They appreciated the calm and quiet atmosphere in the video, devoid of distracting music, and with soothing vocal tones. Many found the video instrumental for focusing and relaxation."}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "\u201cI liked the way she used her voice to give me instructions on what I should be doing. I also liked how she gave me things to look for. Also giving me tips on how to fall into mindfulness. I wish the video was longer!\u201d (P16)\nHowever, a few participants (N=3) provided negative feedback regarding their willingness to rewatch a specific video on mindful breathing, citing reasons such as a lack of perceived need for repetition due to already experiencing the benefits of deep breathing, dissatisfaction with the video's effectiveness in alleviating stress, and a sense of familiarity with the mindfulness technique, making further viewings unnecessary.\n4.2.2 Information Chatbot. Participants found the information provided by the chatbot beneficial for practicing mindfulness, especially in stressful situations, and when seeking relaxation, focus, and comfort. The willingness to use chatbots for acquiring knowledge, assistance, and guidance in enhancing mindfulness practice was also discussed.\nParticipants favored the informational chatbot for its clear, straightforward, and detailed responses, appreciating the fact-based and unbiased nature of the information. The bot's non-judgmental tone was also highly valued. However, some participants noted occasional irregularities in the information chatbot's responses, finding them less consistently interactive. Specifically, repetition of the same definition or response could lead participants to perceive the interaction as mechanical and artificial. P393 and P114 illustrated these mixed sentiments:\n\u201cI liked that it had fast responses that totally made sense, and were open and vague enough that it was easy to understand, even if I didn't know anything about mindfulness. I didn't like that it felt like perhaps the bot's knowledge was limited, and that it might have repeated the phrase \u2018whatever works for you' more than twice.\u201d (P393)"}, {"title": "LLM Agents for Improving Engagement with Interventions", "content": "\u201cThe instructions from the bot were quite straightforward. However, I don't see the need to interact with it again. I'm uncertain if repeating the interaction would offer more mindfulness insights; it might only serve to redirect my focus to breathing exercises during stressful moments. Interacting with the bot in such instances could be beneficial for guiding my breathing exercise.", "conversing with a human,": "haracterized by clear explanations and sensible responses. For example", "described": "n\u201cI liked the conversation very much. It provided me with a clear understanding of mindfulness", "contexts": "n\u201cIt seemed to quickly and effectively answer my questions without some of the typical aggravations of interacting with chatbots...I can't think of many negatives other than talking to an actual person would probably"}]}