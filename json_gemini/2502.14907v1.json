{"title": "GneissWeb: Preparing High Quality Data for LLMs at Scale", "authors": ["Hajar Emami Gohari", "Swanand Ravindra Kadhe", "Syed Yousaf Shah", "Constantin Adam", "Abdulhamid Adebayo", "Praneet Adusumilli", "Farhan Ahmed", "Nathalie Baracaldo Angel", "Santosh Borse", "Yuan-Chi Chang", "Xuan-Hong Dang", "Nirmit Desai", "Ravital Eres", "Ran Iwamoto", "Alexei Karve", "Yan Koyfman", "Wei-Han Lee", "Changchang Liu", "Boris Lublinsky", "Takuyo Ohko", "Pablo Pesce", "Maroun Touma", "Shiqiang Wang", "Shalisha Witherspoon", "Herbert Woisetschl\u00e4ger", "David Wood", "Kun-Lung Wu", "Issei Yoshida", "Syed Zawad", "Petros Zerfos", "Yi Zhou", "Bishwaranjan Bhattacharjee"], "abstract": "Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.\nIn this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously con-structed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens). We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLM) are becoming pervasive in many aspects of life. The performance of these models are dictated by several factors including the model architecture, model size, training data size as well as training data quality.\nHow much data should one use to train an LLM of certain size? The answer is typically governed by scaling laws - empirical formulas that estimate optimal models sizes and data sizes for a given compute budget. For instance, the widely adopted Chinchilla law [1] suggested a compute optimal token-to-parameter-ratio of roughly 20. However, recent state-of-the-art LLMs have been trained on far more data than what the scaling laws would deem as optimal. For instance, Llama3 family of models are trained on 15 trillion (15T) tokens (compared to 1.8T tokens for Llama2) [2, 3], Gemma2 family of models are trained on 13T tokens [4], and Granite-3.0 family of models are trained on 12T tokens [5]. At the time of writing of this paper, the pre-training datasets for leading LLMs, such as Llama3 [2] and Mixtral [6], remain inaccessible to the public, with limited information available on their creation process."}, {"title": "Related Work", "content": "In this work we aim to create a large dataset capable for pre-training of a LLM. There are several related works in this space. Prior public pre-training datasets are typically derived from the Common Crawl [12]. Early works include the C4 dataset with 160 billion tokens [14] and the Pile dataset with billion tokens [15]. The C4 dataset is curated from the April 2009 snapshot of the Common Crawl. It uses langdetect [16] to detect English text, applies a series of heuristic filters including discarding any page with less than 3 sentences, removing lines without any terminal punctuation mark, removing any page containing any word in a list of dirty, naughty, obscene or bad words etc, and also performs deduplication by removing all but one of any three-sentence span occurring more than once in the dataset. The Pile is a composite dataset that includes the Pile-CC, which is based on Common Crawl. It uses pycld2 [17] for language detection, removes boilerplate using jusText [18], applies classifier-based filtering and performs fuzzy deduplication.\nMultilingual models like XLM ROBERTa [19] used the CC100 dataset [20]. This dataset was curated using the CCNET [21] processing pipeline on one year of Common Crawl snapshots. CCNet uses the data processing methods introduced in fastText [22], which include deduplicating documents and applying LangID filtering. It then adds a filtering step to select documents that are similar to high-quality corpora like Wikipedia by utilizing a 5-gram KenLM filter.\nRedPajama dataset [9] is an open source attempt to recreate the dataset used to train Llama models. It is a composite dataset which includes text obtained from the Common Crawl by using the CCNet pipeline [21] and a classifier trained to identify documents similar to Wikipedia articles or references. SlimPajama with 627B tokens [8] further refines RedPajama by removing short documents and performing additional fuzzy dedupllication. RedPajama-V2 [9] with 30 trillion tokens is entirely based on the Common Crawl and contains annotations without applying any filtering. These annotations cover filtering techniques from CCNet, C4, and others, and also labels identifying deduplicates using exact and fuzzy deduplication.\nRefinedWeb dataset [7] is a Common Crawl-based dataset, using trafilatura [23] for text extraction, fastText-based language identification [22], heuristic rules for quality filtering, and fuzzy and exact deduplication. Dolma [10] is a 3 trillion token composite dataset with a Common Crawl-based portion, which employs fastText for language identification, primarily uses heuristic rules from Massive Web [24] for quality filtering, applies toxicity filtering based on rules and classifiers and performs deduplication at URL, document and paragraph levels.\nMore recent datasets include FineWeb datasets [13], DCLM-Baseline [11], and TxT360 [25]. FineWeb consists of 15T tokens derived from the Common Crawl by applying a series of processing steps, mainly including language classification, fuzzy deduplication at snapshot level and heuristic rule-based quality filters. Subsequently, two smaller but higher quality versions called FineWeb-Edu (1.3 trillion tokens) and FineWeb-Edu-Score2 (5.4 trillion tokens) derived from FineWeb were released [13]. These smaller high quality derivatives of FineWeb are created by retaining documents perceived to have higher educational value from FineWeb. See Appendix A for more details on FineWeb.\nDCLM-Baseline is obtained from the Common Crawl snapshots by using resiliparse [26] for text extraction, heuristic quality filters from RefinedWeb, fuzzy deduplication with Bloom filter [27], model-based quality filtering using a specially trained fastText classifier. TxT360 is a composite dataset obtained from Common Crawl snapshots and 14 high-quality datasets (e.g. FreeLaw, Ubuntu IRC, etc). TxT360 is obtained by first applying local exact deduplication, global fuzzy deduplication, and quality filtering to both web and curated datasets, resulting in approximately 5 trillion tokens, which are then up-sampled to over 15 trillion tokens. The mixing and up-sampling approach is shown essential to boosting TxT360 performance.\nNemotron-CC [28] and Zyda2 [29] are concurrent works published recently. Zyda-2 is a 5 trillion"}, {"title": "GneissWeb Dataset in a Nutshell", "content": "Building on Top of FineWeb: We use FineWeb-V1.1.0 as base dataset for GneissWeb, with the goal of obtaining sufficiently large number of quality tokens that are suitable for Stage-1 pre-training. We developed the GneissWeb recipe to distill ~10T high quality tokens from FineWeb. We produced the GneissWeb dataset with nearly 10T tokens by applying the GneissWeb recipe to the 15T tokens of FineWeb-V1.1.0, however, FineWeb dataset is not a requirement for our GneissWeb recipe neither is it tied to FineWeb.\nA key differentiator of the GneissWeb recipe is that it employs a multi-faceted ensemble of quality annotators and thresholds can be adjusted at annotator level to filter documents based on use-case. This is in contrast with recent high-quality datasets [13, 11], which rely on a single model-based quality annotator and perform aggressive filtering which removes around 90% of data. Such aggressive filtering, although improves data quality, results in substantial reduction in data quantity and limits the applicability of these datasets for Stage-1 pre-training. The ensemble of quality annotators in the GneissWeb recipe enables fine-grained quality filtering and achieves a favorable trade-off between the data quality and quantity.\nWe note that, while the GneissWeb recipe is focused at obtaining nearly 10T high quality tokens suitable for Stage-1 pre-training, it is also possible to adapt the recipe by tuning filtering parameters to produce smaller and higher quality datasets fit for Stage-2 type of pre-training.\nThe GneissWeb Recipe consists of the following ingredients:\n\u2022 Exact substring deduplication at line level (Sec. 4.1)"}, {"title": "The GneissWeb Recipe", "content": "In this section we provide details of individual components of the GneissWeb recipe."}, {"title": "Exact Substring Deduplication", "content": "Removing duplicates from training data has been shown to reduce memorization [31, 32] and improve model performance [33, 7]. FineWeb applied per snapshot fuzzy deduplication and removed near-duplicate documents using the MinHash algorithm [13]. Furthermore, FineWeb also applied repetition filter, intra-document deduplication [24] which removes documents with many repeated lines and paragraphs. (See Appendix A for details.) However, duplicates still remain at sequence-level within and across documents. Such repeated substrings bypass the document level deduplication steps of FineWeb for several reasons: they may not represent a significant enough portion of a document or a single document may include repeated sections from various documents.\nWe apply exact substring deduplication to remove any substring of predetermined length that repeats verbatim more than once by adapting the implementation from [33] based on Suffix arrays [34]. Exact substring deduplication can be fine tuned through two hyper-parameters: length-threshold (the minimum length of repeated text sequences) and frequency-threshold. We utilize a length-threshold of 50, consistent with the implementation in [33, 7].\nWe make several modifications to the exact substring deduplication implementation from [33] to run at scale. Furthermore, we adapt it to remove exact substring duplicates in a sharded manner. In particular, we shard each snapshot of FineWeb-V1.1.0 into sets of roughly equal size and apply exact substring deduplication on each shard independently. Also, rather than removing all copies of a duplicate substring, we retain the first occurrence of each duplicate substring and remove any subsequent matches exceeding 50 consecutive tokens."}, {"title": "FastText Classifiers", "content": "FastText [22] family of binary classifiers have been used in prior datasets [9, 11] for identifying high-quality pre-training documents. Recently, [11] showed that fastText classifier trained on carefully selected data can outperform sophisticated model-based filtering approaches such as AskLLM (prompting an LLM to ask if a document is helpful). Inspired by their effectiveness coupled with the computational efficiency of fastText classifiers, we use fastText classifiers for quality annotations.\nWe employ two fastText classifiers: (i) the fastText classifier from [11] trained on a mix of instruction-formatted data (OpenHermes-2.5 [35]) and high-scoring posts from ELI5 subreddit [36] and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value.\nSpecifically, we use the supervised fastText package from [22] to train a classifier on 400k documents, equality split between positive (i.e., high-quality) and negative (i.e., low-quality) classes, selected as follows.\n\u2022 Positive documents:\n190k synthetic documents randomly sampled from the Cosmopedia dataset an open synthetic dataset consisting of textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1 [37].\n10k documents with high educational value selected as follows: we annotated 600k random documents from FineWeb-V1.1.0 asking Mixtral-8x22B-Instruct to score each document between 1 to 5 for its educational quality (with 5 being the highest quality), using a prompt similar to the one used by FineWeb-Edu. Next, we selected 10k random documents from the documents with scores \u2265 4.\n\u2022 Negative documents: 200k random documents out of the 600k Mixtral-annotated documents with scores < 2.\nWe denote the DCLM-fastText as $\\mathcal{O}_{DCLM}$ and our custom fastText as $\\mathcal{O}_{Cosmo}$. Each fastText classifier takes as input a document $D$ and produces a confidence score between [0,1] for the document to have positive label (i.e., high-quality).3 In Appendix C, we present several examples showing how our custom fastText filter complements the DCLM-fastText filter."}, {"title": "Readability Scores", "content": "Readability scores are formulas based on text statistics (such as sentence length, average number of words, number of syllables etc.) designed to assess how easily the text can be read and understood [38]. We apply readability scores as a novel quality metric to facilitate identifying and filtering hard-to-read low-quality documents.\nA large number of readability score formulas have been developed to asses text difficulty [39, 40]. We experimented with a number of readability score formulas and selected McAlpine-EFLAW readability score [41, 42]. McAlpine-EFLAW readability score of a document is a numerical score computed as a function of the number of words in a document plus the number of mini-words (consisting of < 3 characters) divided by the number of sentences. Lower score means the document is easier to understand for a reader with English as a foreign language. Unlike other readability score formulas (such as Flesch-Kincaid [43] or Gunning Fog [44]) which are restricted to estimate a grade level for the text, McAlpine-EFLAW produces a numerical score assessing readability for a global audience [39], making it more suitable for document quality annotation. We also demonstrate the effectiveness of the McAlpine-EFLAW score compared to other readability scores through ablation experiments. Specifically, we tested a few of readability score metrics including Flesch-Kincaid-grade level [43], Automated Readability Index (ARI) [45], Gunning Fog [44] and McAlpine-EFLAW, and determined that McAlpine-EFLAW yields the best results.\nWe analyzed readability score distributions of the documents grouped by categories. Specifically, we considered the documents from the following 3 snapshots from FineWeb-V1.1.0: CC-MAIN-2024-10, CC-MAIN-2023-40 and CC-MAIN-2023-14 and computed the top-level category for each document"}, {"title": "Extreme-Tokenized Documents", "content": "After manually inspecting fastText model-quality annotations and readability scores of large number of low-quality documents, we found that several abnormal documents were mislabeled by these annotators. We observed a peculiar pattern after tokenizing these documents: while most of these documents had similar lengths, they produced significantly different token counts. To quantify this effect, we propose novel annotations that effectively leverages information from the \"pre-tokenization\" stage (document char length, document size) and the \"post-tokenization\" stage (token counts) to identify potential low-quality documents.\nSpecifically, for each document $D$, we compute the the following two annotations:\nTokensPerChar(D) = $\\frac{\\text{Number of Tokens in D}}{\\text{Number of Characters in D}}$, TokensPerByte(D) = $\\frac{\\text{Number of Tokens in D}}{\\text{Size of D (in bytes)}}$\nWe refer to the the documents with extremely high or low number of tokens per character (or tokens per byte) as extreme-tokenized documents (see Fig. 4 for a schematic).\nData quality filtering based on tokenized data has been used in other works [48, 10] to improve the data quality by filtering out documents with too few tokens [10] or removing the sequences containing fewer tokens than a specified threshold. However, the effectiveness of these approaches in detecting low-quality documents is limited because of their sole reliance on the token count. Our extreme-tokenized quality filter does not solely rely on token count but also effectively leverages both information from the \"pre-tokenization\" stage and the \u201cpost-tokenization\" stage to identify and filter out low-quality documents.\nWe analyzed the distributions of TokensPerChar and TokensPerByte for documents grouped by category. Specifically, we considered the documents from the following 3 snapshots from FineWeb-V1.1.0: CC-MAIN-2024-10, CC-MAIN-2023-40 and CC-MAIN-2023-14, and computed the top-level category for each document using the WatsonNLP hierarchical text categorization [46], which is based on the Interactive Advertising Bureau (IAB) Tech Lab categories taxonomy [47]. We observe that the distributions are generally bell-shaped for each category, but the values of the mean and variance differ by category. Furthermore, we observe that low-quality documents typically fall into the two extremes of the distribution. Therefore, we characterize extreme-tokenized documents of a given category as those falling into the two extremes of the TokensPerChar (or TokensPerByte) distribution for the category. Guided by the distributions of TokensPerChar and TokensPerByte in different categories, we leverage the category information of documents and develop a category-aware extreme-tokenized quality filter as part of our ensemble quality filter (Section 4.6). At a high level, we use stricter thresholds on TokensPerChar/TokensPerByte for documents outside the key categories and use more lenient thresholds for documents in these key categories (Section 5.4.3). In Appendix C, we present several low quality examples detected and filtered out by our category-aware Extreme-Tokenized documents filter."}, {"title": "Document Category Classifiers", "content": "As mentioned in previous sections, the quality score distributions of documents in certain categories, which tend to contain documents with high educational-level, differ from the overall distribution across all categories. In particular, we observe that the following IAB categories supported by WatsonNLP categorization have significantly different distributions than the overall distribution across all categories: science, education, technology & computing, and medical health. Thus, for each of these key categories, we annotate whether each document falls into the category.\nTo perform category classification on the 96 snapshots in FineWeb-V1.1.0 at scale, we train four binary fastText category classifiers for each of the four key categories. Specifically, we generated labeled data using the WatsonNLP hierarchical categorization [46], and used the supervised fastText package from [22] to train the fastText classifiers on the following documents:\n\u2022 Positive documents: 400k documents randomly sampled from the documents labeled with that specific category with a confidence score 0.95 and above.\n\u2022 Negative documents: 400k documents randomly sampled from the documents labeled with any category other than these four categories with a confidence score of 0.95 and above.\nWe denote the fastText classifiers as $\\mathcal{O}_{sci}$, $\\mathcal{O}_{edu}$, $\\mathcal{O}_{tech}$, and $\\mathcal{O}_{med}$. Each classifier takes as input a document and produces a label whether the document belongs to the category, along with a confidence score between [0, 1].\nWe use our trained document category classifiers to annotate all the snapshots from FineWeb-V1.1.0. We leverage these category annotations in our category-aware readability score quality filtering and extreme-tokenized quality filtering which results in better performance compared to filtering without leveraging category information."}, {"title": "Ensemble Quality Filter", "content": "Equipped with multiple quality annotators, we develop an ensemble quality filter with the aim of maximizing data quality under the constraint of retaining nearly 10T tokens from FineWeb-V1.1.0. We construct our ensemble quality filter by selecting thresholds for individual annotators and then designing an ensemble filtering rule for aggregating the filter outputs.\nSpecifically, we select the thresholds on readability scores integrating the category annotations to design Category-Aware Readability Score filter. We choose our initial thresholds based on the readability score distributions for key categories (computed on entire FineWeb-V1.1.0), and subsequently fine-tune them through ablation experiments to identify the best set of thresholds that result in maximum performance gain (see Section 5.4.2). Similarly, we select the thresholds for Category-Aware Extreme-Tokenized Documents filter (see Section 5.4.3). Then, given an aggregation rule, we choose the thresholds for fastText filters such that we retain nearly 10T tokes from FineWeb-V1.1.0. As an example, a simple aggregation rule is to apply each filter sequentially (which essentially is a logical AND of filter outputs).\nWe perform ablations on a variety of aggregation rules and determine the best aggregation rule that provides the maximum performance gain. We provide the details of our ensemble quality filter"}, {"title": "Putting It All Together", "content": "The GneissWeb recipe consists of first applying the exact substring deduplication, computing category and quality annotations, and then applying the ensemble quality filter as shown in Fig. 2. We obtain the GneissWeb dataset of 10T tokens by applying the GneissWeb recipe to the 15T tokens in the 96 snapshots of FineWeb-V1.1.0. We prepared GneissWeb using a version of IBM's DataPrep kit library [49] which will be released in open source in future.\nWe note that, while the GneissWeb recipe is designed with the goal of obtaining ~10T high quality tokens suitable for Stage-1 pre-training, it is also possible to adapt the recipe by tuning filtering parameters to produce smaller and higher quality datasets fit for Stage-2 type of pre-training."}, {"title": "Experiments", "content": "We analyze our recipe ingredients and design choices by training data ablation models that are identical in terms of architecture and training parameters, except for the data they were trained on. We evaluate the ablation models on a wide range of downstream benchmarks (details below).\nTraining: To minimize the impact of random data subset selection on evaluation scores, we use three equal-sized random subsets of the full data to train three models, and compute average scores along with standard deviation. More specifically, when comparing two dataset versions $D_1$ and $D_2$, we select three equal-sized random subsets $D_1^i$, $D_1^j$, $D_1^k$ from each $D_i$, i \u2208 {1,2}, and train three models using the random subsets. We compare the average scores across the three models and also report standard deviations."}, {"title": "Evaluating the GneissWeb Dataset", "content": "We compare our GneissWeb dataset with the following state-of-the-art open-source, web-scale datasets: FineWeb (15T tokens) [13]4, FineWeb-Edu-Score-2 (5.4T tokens) [13], DCLM-Baseline (3.8T tokens) [11], Dolma (3T tokens), FineWeb-Edu (1.3T tokens) [13], and RefinedWeb (600B tokens) [7]."}, {"title": "1.4B Models Trained on 350B Tokens", "content": "Table 1 shows the average scores on high-signal tasks and extended tasks for 1.4 billion parameter models trained on three randomly sampled sets of 350B tokens from each dataset. The datasets evaluated are broken down into those which are above 5 trillion tokens (highlighted in blue) in size and those below 5 trillion. The former are useful for Stage-1 kind of training and are the primary focus of this study. The latter are useful for Stage-2 kind of training and with certain tuning of parameters of filtering a version of GneissWeb can be produced for this space. GneissWeb demonstrates the best performance among large datasets. Specifically, models trained on the GneissWeb outperform those trained on FineWeb-V1.1.0 by 2.14 percent points on high-signal tasks, and by 1.49 percent points on extended tasks.\nFor datasets that are greater than 5 trillion token set size, in Table 2, we show the performance broken down into the various categories of tasks \u2013 Commonsense Reasoning (CR), Language Understanding (LU), Reading Comprehension (RC), World Knowledge (WK) and Symbolic Problem Solving (SPS). As shown in Table 2, GneissWeb is not only the best overall but in fact performs the best in all categories of tasks except World Knowledge.\nIn Figure 7, we show the progression of average score over high-signal tasks with training for 1.4 billion parameter model for 350 billion tokens. We see that for all three datasets compared, the accuracy increases over time and the accuracy of GneissWeb is consistently higher than FineWeb.V1.1.0 and FineWeb-Edu-score-2."}, {"title": "3B and 7B Models Trained on 350B Tokens", "content": "To evaluate the GneissWeb for training larger models, we perform controlled ablations by training models with 3 billion and 7 billion parameters on 350 billion tokens. Given that training models of size 3 and 7 billion parameters require lot more compute and so does evaluation, we have restricted comparison with large datasets (FineWeb and FineWeb-Edu-Score-2). Specifically, we train models on three randomly sampled sets of 350 billion tokens from each dataset and compute the average scores.\nTable 3 and Fig. 8 depict the results for 3B model size. We observe that models trained on GneissWeb outperform those trained on FineWeb.V1.1.0 by 2.52 percent points in terms of the average score computed on high-signal benchmarks (both zero-shot and few-shot), and 1.95 percent points on Extended benchmarks (both zero-shot and few-shot).\nTable 4 and Fig. 9 show the results for 7B model size. Models trained on GneissWeb outperform those trained on FineWeb.V1.1 by 2.73 percent points in terms of the average score computed on a"}, {"title": "Exact Substring Deduplication", "content": "We conduct an ablation experiment to evaluate the impact of exact substring deduplication on the model performance. As discussed in [13], the impact of deduplication is not typically visible for small number of tokens. Thus, we train two 1.4B models each on 350B tokens as follows. The baseline model is trained on 350B tokens randomly sampled from FineWeb-V1.1.0, and the second model is trained on the 350B tokens randomly sampled after applying sharded exact substring deduplication to FineWeb-V1.1.0 as discussed in Sec. 4.1.\nIn Fig. 10, we compare average evaluation score on high-signal tasks for the two models. We see that for both datasets compared, the average score increases as the training progresses, and the score of the model trained on the dataset with exact substring deduplication is consistently higher (especially"}, {"title": "Category-Aware Readability Score Filter", "content": "As discussed in Section 4.3, our analysis of readability score distributions of documents grouped by categories depicts that distributions of certain categories differ from the overall distribution across categories. These specific categories tend to contain many documents with educational-style content, resulting in higher values of readability scores. Equipped with this observation, we design category-aware readability score filter wherein we select lenient filtering threshold on readability scores for documents from these educational-style categories, and stricter filtering threshold for documents outside of these categories. We select initial thresholds based on readability score distributions, and then perform ablations to tune the thresholds. We use lenient threshold for the following educational-style categories: science, education, technology and computing, and medical health. We also performed ablations to include other categories, for instance, adding \"news and politics\", \"business and finance\" as well as \"personal finance\u201d to the hard-to-read categories, but it degraded performance.\nIn Figure 11, we plot the average score over high-signal tasks for the best thresholds. Specifically, we train two 1.4B parameter models - the baseline model is trained on 35B tokens randomly sampled from FineWeb-V1.1.0, and the second model is trained on the 35B tokens randomly sampled after applying category-aware readability score filter to FineWeb-V1.1.0. We see that for both datasets compared, the average accuracy increases with training and the accuracy of the dataset with readability score quality filter is consistently higher than the baseline, achieving the final score of 53.20 percent as compare to the score of 51.94 percent for the baseline."}, {"title": "Category-Aware Extreme-Tokenized Filter", "content": "As mentioned in Section 4.4, we annotate each document with two parameters defined using pre-tokenization and post-tokenization document properties: TokensPerChar (number of tokens divided by number of characters) and TokensPerByte (number of tokens divided by the document size in bytes). When we plot distributions of TokensPerChar and TokensPerByte, we observe that distributions of the documents in specific education-style categories differ than the overall distribution across categories. Guided by this observation, we design our category-aware extreme-tokenized documents filter, in which, we select lenient thresholds on TokensPerChar/TokensPerByte for the specific categories and stricter thresholds for the other categories. Specifically, we select lenient thresholds for the same categories as in the case of readability scores: science, education, technology and computing, and medical health. Our ablations show that adding other categories (where distributions differ) such as personal finance degrade performance. We choose initial thresholds based on the TokensPerChar/TokensPerByte distributions, and then perform ablations to tune the thresholds.\nFigure 12 shows the results of the ablation experiment with the best thresholds. We show the progression of average accuracy on high-signal tasks with training for two models a baseline model trained on 35B tokens randomly sampled from FineWeb-V1.1.0, and the second model trained on"}, {"title": "Ensemble Quality Filtering", "content": "Equipped with fastText classifiers, category-aware readability score filter, and category-aware extreme-tokenized documents filter, we perform ablations over various ensemble filtering rules. We first select the thresholds for category-aware readability score filter and category-aware extreme-tokenized filter as discussed in the above sections. Then, we tune the thresholds for fastText classifiers for a given ensemble filtering rule such that at least 10T tokens are retained from the 15T tokens of FineWeb-V1.1.0. Specifically, we consider the following five ensemble aggregation rules, described using the notation in Fig. 5. The Venn diagram in Figure 13 is helpful to visualize the filtering rules.\nEnsemble filtering rule 1: A document is retained if either of the fastText classifiers agrees and category-aware readability score filter agrees and category-aware extreme tokenized filter agrees (illustrated as D in Fig. 13). Note that this rule is equivalent to sequentially applying the filters (in arbitrary order).\n[($\\mathcal{O}_{DCLM}(D) > T_{DCLM} \\text{ OR } \\mathcal{O}_{Cosmo}(D) > T_{Cosmo}) \\text{ AND } (Readability(D) < r_c)] \\text{AND } (T_{Low}^c < TokensPerChar(D) < T_{High}^c)$\nEnsemble filtering rule 2: A document is retained if any two of the three filters-fastText classifier combination with logical OR, category-aware readability score filter, category-aware extreme tokenized filter-agree (illustrated as D, B, C, and A areas in Fig. 13).\n[($\\mathcal{O}_{DCLM}(D) > T_{BCLM} \\text{ OR } \\mathcal{O}_{Cosmo}(D) > T_{Cosmo}) \\text{ AND } (Readability(D) < r_c)] \\text{OR } [(\\mathcal{O}_{DCLM}(D) > T_{BCLM} \\text{ OR } \\mathcal{O}_{Cosmo}(D) > T_{Cosmo}) \\text{ AND } (T_{LOW}^c < TokensPerChar(D) < T_{High}^c)] \\text{OR } [(Readability(D) < r_c) \\text{ AND } (T_{Low}^c < TokensPerChar(D) < T_{High}^c)]$\nEnsemble filtering rule 3: A document is retained if either the fastText combination agrees, or both category-aware readability score filter and category-aware extreme tokenized filter agree (illustrated"}, {"title": "Conclusion", "content": "In this paper, we introduced the GneissWeb dataset and demonstrated how to improve upon state-of-the-art dasets of similar size, achieving a better trade-off between data quality and quantity. The GneissWeb dataset consists of 10T high quality tokens distilled from 96 common-crawl snapshots of FineWeb. GneissWeb is created through a series of experiments that provided evidence for our choice of exact substring deduplication, and quality filters. The GneissWeb recipe goes beyond simple model-based quality filtering used in recent datasets and design an ensemble of filters incorporating novel quality filters based on characteristics of the text contents. Our experiments show the effectiveness of our novel category-aware extreme-tokenized documents quality filter and category-aware quality filter based on human readabilty. GneissWeb is prepared using a version of IBM Data Prep Kit which will be released in open source in the near future."}, {"title": "Limitations", "content": "Due to resource constraints, we could not perform ablation experiments to determine the optimal threshold sets for all processing steps in the GneissWeb recipe, and there is likely room for improvement. Moreover, due to resource constraints, we could only experiment with a subset of ensemble filtering rules, and investingating a broader spectrum of ensemble rules is an interesting future work. Although comparison with other state-of-the-art datasets of comparative size has demonstrated the the effectiveness of the GneissWeb ensemble quality filter, it still has the potential for improvement in future work. For example, for the readability score quality filter, we tested a few of readability score metrics and through our ablation experiments, we found that McAlpine-EFLAW yields the best results. It could be interesting to explore testing other readability scores in future work. We tested our processing steps and illustrated their impact only on English data. More work is needed to adapt our processing steps"}]}