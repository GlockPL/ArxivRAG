{"title": "LLM Safety Alignment is Divergence Estimation in Disguise", "authors": ["Rajdeep Haldar", "Ziyi Wang", "Qifan Song", "Guang Lin", "Yue Xing"], "abstract": "We propose a theoretical framework demonstrating that popular Large Language Model (LLM) alignment methods, including Reinforcement Learning from Human Feedback (RLHF) and alternatives, fundamentally function as divergence estimators between aligned (preferred or safe) and unaligned (less-preferred or harmful) distributions. This explains the separation phenomenon between safe and harmful prompts in the model hidden representation after alignment. Inspired by the theoretical results, we identify that some alignment methods are better than others in terms of separation and, introduce a new method, KLDO, and further demonstrate the implication of our theories. We advocate for compliance-refusal datasets over preference datasets to enhance safety alignment, supported by both theoretical reasoning and empirical evidence. Additionally, to quantify safety separation, we leverage a distance metric in the representation space and statistically validate its efficacy as a statistical significant indicator of LLM resilience against jailbreak attacks.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) are powerful generative tools capable of understanding human language and performing specific tasks. After the pre-training and supervised fine-tuning stages, alignment methods are used to align the LLMs' outputs with human preferences or ethics. In the literature, methods such as reinforcement learning with human feedback (RLHF), direct preference optimization (DPO), and their variants serve as common approaches.\nWithin alignment research, a significant focus is on safety alignment, which aims to ensure that LLMs avoid responding to malicious user inputs and generate only safe responses. Studies, such as (Lin et al., 2024b; Xu et al., 2024), observe that for aligned LLMs, the hidden representations"}, {"title": "2. Related Works", "content": "Empirical Studies Various methods have been proposed to align LLMs with human preferences. For instance, RLHF with the BT and PL models was first introduced in (Ziegler et al., 2019) and (Ouyang et al., 2022), respectively. In RLHF, a reward model is trained and is further used in the alignment of the LLM. In contrast, DPO (Rafailov et al., 2024) designs its loss function (training objective) to avoid the need for a separate reward model. Later, BCO (Jung et al., 2024) and KTO (Ethayarajh et al., 2024) were proposed to further enhance alignment performance. In addition to the alignment methods mentioned above, several others have been developed to enhance performance in various ways. For example, ORPO (Hong et al., 2024) incorporates the SFT loss into DPO, and (Yuan et al., 2024) uses a preference tree. Other techniques can be found in (Xiong et al., 2024b; Amini et al., 2024; Lu et al., 2024; Wang et al., 2024b; Zhou et al., 2024; Zhang et al., 2024; Fr\u00e4nken et al., 2024; Yin et al., 2024).\nTheoretical Investigations Beside the empirical studies, some other works focus on the theoretical properties of alignment and develop new algorithms based on their analysis. For example, (Xiao et al., 2024) addresses preference bias in RLHF through preference matching. (He et al., 2024) accelerates convergence by applying momentum, and (Liu et al., 2024) proposes an algorithm that uses active learning to select the appropriate human for RLHF. Other studies can be found in (Wang et al., 2024a; Xiong et al., 2024a; Wang et al., 2023; Du et al., 2024). Different from existing literature, we have a emphasis on the separation effect between aligned and unaligned data."}, {"title": "2.2. Jailbreak Attack", "content": "Aligned LLMs, despite their intended safety measures, can still produce harmful content, as highlighted in studies like (Zhou et al., 2023), (Hazell, 2023), and (Kang et al., 2024). Jailbreak attacks, which exploit vulnerabilities in these models, have been explored in (Wei et al., 2024) and (Carlini"}, {"title": "3. Preliminaries", "content": null}, {"title": "3.1. Notation", "content": "Let x and y represent the prompt and response, respectively. Specifically, yw denotes an aligned response, while y1 corresponds to an unaligned response. The joint distribution of prompts, aligned responses, and unaligned responses is represented by (x, yw, y\u0131) ~ D. (x,y) ~ D+ represents samples of the aligned distribution, which is obtained by marginalizing the joint D over unaligned responses y\u0131, while the unaligned distribution, D\u00af, is obtained by marginalizing D over aligned responses yw. Note that the meaning of aligned and unaligned distributions can vary across different contexts (please refer to Sec. 3.3).\nThe trainable model policy for generating a response given a prompt is represented by \u03c0\u03bf, parameterized by 0, with 0* denoting the parameters at convergence. The reference model policy before alignment training is denoted by \u03c0ref.\nFor a probability distribution G, the probability mass function (p.m.f.) or probability density function (p.d.f.) is represented by pg, and the corresponding conditional probability of y | x is written as pg(y | x). The sigmoid function is defined as \u03c3(u) = (1 + exp(-u))-1. Finally, \u03a9 denotes the usual asymptotic notation such that f(u) = \u03a9(g(u)) if there exists c > 0, with f(u) > c. g(u) for all u."}, {"title": "3.2. Alignment Methods", "content": "RLHF: RLHF is one of the foundational methods for model alignment. It consists of the following two steps:\n1. Reward Modeling: A reward function ro (x, y) is learned to encode human preferences, where & represents the parameters (usually neural networks). Reward modeling requires paired alignment data of the form of (x, yw, y\u0131) ~ D.\nThe reward function is trained to maximize the likelihood under the Bradley-Terry (BT) model (Bradley & Terry, 1952):\n$p(Yw > y\u0131 | x) = \\frac{expr(x, yw)}{expr(x, yw) + expr(x, y1)}.$ (1)\n2. Reward Maximization: The learned reward function r(x, y) is then used to guide the policy \u03c0\u03bf by maximizing the following objective:\n$\\underset{\\theta}{\\text{sup}} \\underset{x,y}{\\mathbb{E}} (r(x, y) \u2013 \\beta D_{KL}(\\pi_{\\theta} || \\pi_{ref})),$ (2)\nwhere Tref denotes the reference policy, and $D_{KL}(\\pi_{\\theta} || \\Pi_{ref})$ is a regularization term (e.g., KL divergence), with \u1e9e con-"}, {"title": "DPO:", "content": "Direct Preference Optimization (DPO) (Rafailov et al., 2024) reduces the reward modeling and reward maximization steps of RLHF into a single minimization objective w.r.t. \u03b8. This is achieved by substituting the theoretically optimal reward of (2) in terms of the \u03c0\u03bf policy, i.e., $r_{\\theta}(x,y) = r_{\\theta}(x,y) = \\beta \\cdot ln[\\pi_{\\theta}(y|x)/\\pi_{ref}(y|x)]$, into (1). The DPO loss is as follows:\n$L_{DPO}(\\theta) = \\underset{X, Yw, Y1~D}{\\mathbb{E}} lno (r_{\\theta}(x, Yw) \u2013 r_{\\theta}(x,y\u0131)).$\nTheoretically, DPO is equivalent to RLHF (Rafailov et al., 2024), and our main results will focus on DPO, which implicitly applies to RLHF as well. Alignment under DPO maximizes the likelihood of aligned responses while minimizing it for unaligned ones.\nUnlike RLHF and DPO, which rely on pairwise data, methods such as KTO (Ethayarajh et al., 2024) and BCO (Jung et al., 2024) reformulate alignment as a binary classification problem (aligned vs. unaligned) using unpaired data of the form (x, y). In our paper, $r_{\\theta}(x,y) = \\beta \\cdot ln \\pi_{\\theta}(y|x)/\\pi_{ref}(y|x)$ is defined as reward unless stated otherwise."}, {"title": "KTO:", "content": "Given data in the form (x, y), along with a binary signal indicating whether x, y is sampled from aligned (D+) or unaligned (D\u00af), the KTO loss with reference constant zo (Defition A.1) is defined as follows:\n$L_{KTO} (\\theta) = \\underset{x,y~D^+}{\\mathbb{E}} [1 \u2013 \u03c3(r_{\\theta}(x, y) - z_0)] + \\underset{x,y~D^-}{\\mathbb{E}} [1 \u2013 \u03c3(z_0 \u2013 r_{\\theta}(x, y))].$"}, {"title": "BCO:", "content": "Using the same data format as KTO, the Binary Classification Optimizer (BCO) loss with reference constant \u03b4 (Defn. A.2) is defined as follows:\n$L_{BCO} (\\theta) = \\underset{x,y~D^+}{\\mathbb{E}} ln [\u03c3(r_{\\theta}(x, y) \u2013 \u03b4)] \u2013 \\underset{x,y~D^-}{\\mathbb{E}}ln [\u03c3(\u03b4 \u2013 r_{\\theta}(x, y))].$"}, {"title": "3.3. Data", "content": "In this section, we define the data generation process to relate LLM alignment and safety classification, which separates harmful and safe prompts in the hidden representation.\nEach prompt x has a binary latent variable zx, denoting its safety: 2x = 1 corresponds to a safe prompt, while zx = 0 indicates a harmful prompt. For any prompt, responses could be compliant or refusing, leading to two fundamental distributions: samples x, y ~ C represent the compliance distribution, where y is a compliant response for x, and samples x, y ~ R correspond to the rejection distribution, where y is a refusal response.\nIn the following, we introduce two data models: compliance-refusal (CR) and preference (Pref) data,"}, {"title": "4. Main Results", "content": null}, {"title": "4.1. Alignment Losses as Divergence Estimators", "content": "To connect alignment losses and divergence metrics, we first layout some details about divergences. Popular divergences, such as KL, TV, and JS, measure the difference between any two probability distributions P, Q over a random variable v \u2208 V. They can be expressed in terms of an optimization problem over an arbitrary functional T(v) as follows:\n$1_{DKL}(P || Q) = \\underset{T}{\\text{sup}} \\underset{v~P}{\\mathbb{E}}T(v) - In \\underset{v~Q}{\\mathbb{E}}e^{T(v)},$ (3)\n$D_{TV} (P || Q) = \\underset{T:|T|<1/2}{\\text{sup}} \\underset{v~P}{\\mathbb{E}}T(v) - \\underset{v~Q}{\\mathbb{E}}T(v),$ (4)\n$2 . D_{JS} (P || Q) - ln 4 = \\underset{T:0<T<1}{\\text{sup}} \\underset{v~P}{\\mathbb{E}}lnT(v) - \\underset{v~Q}{\\mathbb{E}}ln (1 \u2013 T(v)).$ (5)\nFor the general class of f-divergences (Defn. A.3) we have the crude variational bound:\n$D_f (P || Q) = \\underset{T:V\\rightarrow ED(f^*)}{\\text{sup}} \\underset{v~P}{\\mathbb{E}}T(v) - \\underset{v~Q}{\\mathbb{E}} E_{f^*oT(v)},$ (6)\nwhere f* is the convex conjugate (Defn. A.4) of f, and $ED(f^*) = \\{u : f^*(u) < \\infty\\}."}, {"title": "4.1.1. ANALYZING DPO INDUCED DIVERGENCE", "content": "Based on the DPO loss, we define a variational representation of a non-parametric candidate divergence as follows:\n$D_{DPO}(P || Q) = \\underset{T}{\\text{sup}} \\underset{V1~P,V2~Q}{\\mathbb{E}}  lno (T(v1) \u2013 T(v2)).$\nTo analyze the behavior of this candidate divergence, we conduct simulation to compare it with other divergences. We compute DDPO and Drv, DJs and DKL for range of normal distribution pairs P = N(0,1), Q = N(\u03bc,1) where we vary \u03bc. The results are summarized in Fig. 2."}, {"title": "4.2. Alignment Losses from General Divergences", "content": "While Sec 4.1 discusses KTO, BCO, and DPO, there are some other divergences (e.g., KL in Fig. 2) which are not included in Thm 4.1. In the following, we use KL to design a new method, KLDO, to demonstrate how the insights in Sec 4.1 can further inspire new methods. Similarly, we also consider f-divergence in addition to KL."}, {"title": "4.2.1. KLDO", "content": "We introduce a new alignment method to connect to the KL divergence. On one hand, from Fig. 2, KL is the most sensitive in capturing farthest distribution separation. On the other hand, none of the standard optimizers estimate it (Thm 4.1). Therefore, we borrow KL and design KLDO: the KL-Divergence Optimizer.\nTo derive the loss objective for alignment, we parameterize the functional Te(x,y) = ro(x,y) in the variational representation (3), resulting in the following loss function:\n$L_{KLDO}(\\theta) =  - \\underset{x,y~D^+}{\\mathbb{E}}r_{\\theta}(x, y) + In \\underset{x,y~D^-}{\\mathbb{E}}e^{r_{\\theta}(x,y)}.$ (7)\nBy construction, KLDO estimates the KL divergence, such that $L_{KLDO}(\\theta^*) = \u2212D_{KL}(D^+ || D^\u2212)$."}, {"title": "Dealing with Biased Gradient Updates", "content": "The gradient of the loss is given by:\n$\\nabla \\theta L_{KLDO} (\\theta) =  \\underset{x,y~D^-}{\\mathbb{E}} \\nabla \\theta r_\\theta(x,y) e^{r_\\theta \\theta} \\underset{x,y~D^+}{\\mathbb{E}} \\nabla \\theta r_{\\theta} + \\frac{\\underset{x,y~D^-}{\\mathbb{E}} e^{r_{\\theta}}}{(8)}$\nThe second term of (8) involves a scaling factor in the denominator, which is an expectation over the complete unaligned distribution. In practice, directly applying SGD updates with a minibatch B = B+ U B\u00ae of aligned and unaligned samples will induce bias. To mitigate this, we estimate the denominator by computing a moving average over the unaligned minibatch, inspired by (Belghazi et al., 2018), where mutual information is estimated using a DV representation in a contrastive learning framework."}, {"title": "4.2.2. FDO", "content": "Extending from KLDO, in theory, we can also construct an alignment loss based on general class of f-divergences. We parametrize To (x,y) = g(re(x, y)) in variational representation (6), where g : R \u2192 ED(f*) is a strictly increasing and invertible function. We refer to this class of alignment losses as FDO(f, g), or the f-divergence optimizer with the link function g, defined as follows:\n$L_{FDO} (\\theta) = - \\underset{x,y~D^+}{\\mathbb{E}}g (r_{\\theta}(x, y)) + \\underset{x,y~D^-}{\\mathbb{E}} f^*og (r_{\\theta}(x, y)).$ (9)\nTo align with Thm 4.1, by definition, $L_{FDO(f,g)}(\\theta^*) = D_f(D^+ || D^-)$."}, {"title": "4.3. Alignment Consistency", "content": "While the above connects alignment methods with divergence metrics, and divergence intuitively induces separation of the aligned and unaligned data, there is still a missing piece on how to explicitly show the separation in theory. Therefore, in this section, we introduce the concept of alignment consistency, and utilize this concept in the later Sec 4.4 to show the separation.\nBefore alignment, the probabilities of responses given prompts are governed by the reference policy Tref. An effective alignment method should learn a policy \u03c0\u03bf* that appropriately adjusts the reference policy based on the likelihood of a response being aligned or unaligned. The following definition formalizes this concept, which we refer to as alignment consistency.\nDefinition 4.2 (Alignment Consistent). An alignment method is \"consistent\" if the optimal policy follows\n$\\pi_{\\theta^*}(y|x) = Z(x)^{-1} \\cdot \\pi_{ref}(y|x) \\cdot h(R(x, y)),$\nwhere $R(x, y) = \\frac{p_{D^+}(y|x)}{p_{D^-}(y|x)}$, h : R \u2192 R is a non-decreasing, non-constant function, and Z(x) is a normalizing constant so that the total probability is 1."}, {"title": "Theorem 4.3.", "content": "The following alignment methods are \u2018consistent\u2019 (Defn. 4.2) with corresponding h(u):\n$h(\u0438)_{KTO} = \\begin{cases}\n1 + sign (u - 1)\\\\\n1 - sign (u \u2013 1)\n\\end{cases}$\n$h(u)_{KLDO, BCO} = u , h(u)_{FDO} = e^{\\frac{g^{-1}of^{\\prime}(u)}{\\beta}}$\nFrom Thm 4.3, all the methods enforce \u03c0\u03c1* \u2192 Tref as \u03b2\u2192\u221e. This behavior aligns with Equation 2, where large values of \u1e9e heavily penalize deviations from Tref during the reward maximization step in RL. Conversely, as \u1e9e \u2192 0 (i.e., no regularization), \u03c0\u03c1* \u03b1\u221e or 0 depending on whether R(x, y) > 1 or R(x, y) \u2264 1. In this regime, the optimal policy eliminates all probability mass from unaligned responses and distributes it uniformly among aligned responses.\nFor KTO, the function h(u) is discrete in R(x, y), which is a characteristic of TV divergence. In contrast, KLDO and BCO exhibit smoother dependencies on R(x, y), allowing these methods to capture subtle discrepancies between aligned and unaligned distributions, if present.\nFinally, as a sanity check, the FDO framework recovers the formulations for KTO, BCO, and KLDO with appropriate choices of f and g. For example, expressing KTO as an FDO with f(u) = 1|u - 1| and $g(u) = \u03c3(u \u2212 z_0) - \\frac{1}{2}$ reproduces the same result.\nRemark 4.4 (Is DPO Alignment Consistent?). Theoretically, proving is challenging due to the lack of a closed-form solution for the divergence it estimates (Sec 4.1.1). However, as a valid divergence, we believe its consistency arises as a by-product of divergence estimation, supported heuristically and by DPO's empirical success as an alignment method."}, {"title": "4.4. Alignment Consistent Loss induces Separation", "content": "To understand the clustering or separation of prompts based on safety by LLMs after alignment, we model this behavior as a classification problem. Specifically, we aim to predict the safety label z given the prompt x and the optimal parameter 0* obtained after alignment. Using a Bayesian framework with no prior bias towards the safety label, we define the probability of a response being safe, given the prompt-response pair and the optimal policy, as:\n$p(z = 1 | x, y, \\theta^*) = \\frac{\\pi_{\\theta^*}(y | x, z = 1)}{\\sum_{t\\in\\{0,1\\}} \\pi_{\\theta^*}(y | x, z = t)}$ (10)"}, {"title": "LLM Safety Alignment", "content": "To eliminate the dependence on y in the conditional model, we normalize over the set of all feasible responses, FR(x) = {y : ($p_C(y/x)/p_R(y|x))^{2zx\u22121} > 1}, for a given prompt x. This set consists of all responses likely to comply or refuse based on whether the prompt is safe or harmful.\n$p(z = t | x,\\theta^*) = \\sum_{y\\in FR(x)}p(z = t | x, y, \\theta^*)/|FR(x)|.$ (11)\nUsing this conditional model, we define a Naive Bayes Classifier for safety, 2(x, 0*) as:\n$2(x,\\theta^*) = arg \\underset{t\\in\\{0,1\\}}{max} p(z=t|x, \\theta^*).$ (12)\nThe following theorem demonstrates how alignment consistency is related to separation.\nTheorem 4.5 (Separation). If an alignment method is alignment consistent, then the Naive Bayes Classifier (12) is always accurate, z(x, 0*) = zx, \u2200x. Moreover, conditional probability p(z = t | x,0*) in Equation (11) is improved for 'CR' vs 'Pref' type data, i.e.,\n$p^{CR}(z = z_x | x, \\theta^*) \u2265 p^{Pref} (z = z_x | x, \\theta^*) > 0.5.$\nThe above theorem asserts that the Naive Bayes Classifier based on \u03c0\u03c1* accurately classifies the true safety label for all given prompts. Thus, the model learns to distinguish safe and harmful prompts. Moreover, the separation between safe and harmful clusters is stronger when using compliance-refusal data compared to preference data."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Experiments Setup", "content": "Model In our experiments, we leverage various LLMs to showcase the universality of our theoretical insights across different models. Specifically, we use Llama3.2-1B (Dubey et al., 2024), Llama2-7B (Touvron et al., 2023; Ethayarajh et al., 2024) trained on SHP (Ethayarajh et al., 2022; 2024; Touvron et al., 2023), Anthropic HH (Bai et al., 2022; Ganguli et al., 2022), OASST1 (K\u00f6pf et al., 2024), Gemma2-2B (Team et al., 2024), Mistral-7B-v0.1 (Jiang et al., 2023), and Qwen2.5-1.5B (Yang et al., 2024).\nData and Training We leverage the SafeAligner (Huang et al., 2024) and Alpaca-GPT4-Data (Peng et al., 2023) datasets to create two versions based on CR and Pref (Sec 3.3). We train LLMs on these data, using different alignment methods (Github repo.). Detailed data and training description can be found in Appendix C.\nCompliance Refusal Dataset For the responses in the compliance refusal dataset, we handle safe and unsafe prompts differently. For safe prompts, the aligned responses"}, {"title": "5.2. Separation and robustness", "content": "Unlike (Lin et al., 2024a), which focuses on analyzing already aligned models from a jailbreak perspective, our work examines how alignment impacts separation in latent space. Nevertheless, we adopt the visualization methodology from (Lin et al., 2024a) (details in Appendix C.1) to illustrate separation in aligned models. Our findings reveal that post-alignment, models consistently exhibit separation in latent space, across all methods. For instance, Fig. 4 demonstrates this for Qwen-2.5-1.5B. Additional visualizations for other models can be found in Appendix C.4. However, visualizations are qualitative, and we propose numerical metrics to evaluate separation, clustering, and robustness for detailed quantification."}, {"title": "5.2.1. METRICS", "content": "Separation & Clustering: To quantify the separation between safe and harmful prompts in the hidden representation space, we employ the Bhattacharyya Distance ($D_B \u2208 R^+$), a metric that measures the distance between two distributions or clusters. This distance directly reflects the extent of overlap between clusters, making it particularly suited to our context, where unaligned base models exhibit significant overlap, while aligned models show minimal or no overlap. Additionally, we assess clustering quality using the Silhouette Score (s \u2208 [-1,1]), which measures how well an object matches its own cluster compared to others. A higher s indicates denser and more distinct clusters. Together, DB and s serve as sufficient metrics to quantitatively capture the separation and clustering quality observed in visualizations such as Fig. 4. Details on these metrics and their implementation are provided in Appendix C.2.\nRobustness: We formally assess robustness using the At-"}, {"title": "5.2.2. COMPARING ALIGNMENT METHODS", "content": "In this section, we compare how different alignment methods perform on the separation, clustering, and robustness metrics. The results are summarized in Table 2.\nSeparation and Clustering: KLDO and BCO consistently outperform KTO and DPO in terms of both the Bhattacharyya Distance ($D_B$) and the Silhouette Score (s). For instance, KLDO achieves the highest DB and s for Llama2-7B-SFT and Gemma2-2B, while BCO leads for Llama3.2-1B and Mistral-7B, with KLDO as a close second. The persistent underperformance of DPO can be attributed to the less sensitive divergence it induces, as discussed in Sec. 4.1.1. For KTO, its reliance on the TV distance, which lacks strict convexity with respect to separation, limits its effectiveness. In contrast, the JS and KL divergences induced by BCO and KLDO (Fig. 2) are more sensitive to larger separations between aligned and unaligned distributions, making BCO and KLDO better suited for improving alignment.\nRobustness (ASR): KLDO and BCO generally achieve lower ASR percentages, indicating stronger robustness. For example, KLDO demonstrates the lowest ASR for Qwen2.5-1.5B and Mistral-7B-v0.1, as well as the second-lowest ASR for Gemma2-2B. Even in cases where KLDO does not achieve the lowest ASR, it consistently demonstrates superior utility and win rates, as detailed in Sec. 5.3. This highlights KLDO's ability to strike a favorable balance between robustness and utility.\nOmission of Base Model ASR: We omit ASR percentages for base models since they are text-completion models, not designed for instructions or Q&A. However, our aligned models, fine-tuned with LoRA and high learning rates, successfully enable Q&A capabilities. To illustrate this, we compare base and aligned models (DPO, KTO, KLDO, BCO) on adversarial (Appendix Table 6) and clean"}, {"title": "5.2.3. RELATIONSHIP BETWEEN SEPARATION AND ASR", "content": "Using the values from Table 2 across various configurations, we evaluate the Pearson correlation between the Bhattacharyya Distance ($D_B$) or Silhouette Score (s) and the Attack Success Rate (ASR). As shown in Fig. 5, both metrics are negatively correlated with ASR. Specifically, DB exhibits a Pearson correlation of -0.44 (p-value = 0.027), while s shows a weaker correlation of -0.13 (p-value = 0.52). These results indicate that LLMs with better separation between safe and unsafe prompts are generally more robust to adversarial prompts, aligning with our initial hypothesis. However, the clustering metric s is not statistically significant, which is expected since robustness is dictated by the separation between clusters rather than their density."}, {"title": "5.3. Balance between Utility & Robustness", "content": "An effective alignment method must balance robustness and utility. To evaluate this, we measure win rates on the"}, {"title": "5.4. Compliance Refusal vs Preference Data", "content": "In this section, we evaluate the impact of the Compliance Refusal Dataset versus the Preference Dataset in improving LLMs' ability to distinguish safe and unsafe prompts, as suggested by Theorem 4.5.\nWe aligned Llama3.2-1B on both datasets and measured the Bhattacharyya distance between safe and unsafe prompt representations, along with the ASR. Results in Table 4 show that alignment with the Compliance Refusal Dataset significantly increases the Bhattacharyya distance, achieving better separation between safe and unsafe prompts while imporov-ing robustness across the board (decreasing the ASR %). This aligns with Theorem 4.5 and highlights the advantages of Compliance Refusal Dataset for safety alignment."}, {"title": "6. Conclusion & Future Direction", "content": "We established a theoretical framework linking LLM alignment methods to divergence estimation, explaining the separation between safe and harmful prompts in latent space after alignment. This insight also led to development of KL divergence based alignment: KLDO, and a broader class of f-divergence optimizers (FDO) in theory. We also addressed weakness of DPO compared to other alignment methods from a divergence perspective. Empirical results validated our theory, and we demonstrated the importance of compliance-refusal datasets for safety alignment compared to the common preference based datasets used in RLHF.\nOne possible future direction is that, the FDO class enables plug-and-play optimizers for custom choices of f, g (Sec. 4.2.2). Moving forward, one can analyze the empirical properties of FDO-based methods, refining their effectiveness and robustness in LLM alignment."}, {"title": "C. Experimental Details", "content": "Link to our KLDO repo for implementation.\nData We utilize the SafeAligner (Huang et al., 2024) and Alpaca-GPT4-Data (Peng et al., 2023) datasets in our experiments. As described in (Huang et al., 2024), the SafeAligner dataset includes 628 unsafe prompts sourced from open platforms, with safe responses generated by GPT-4 and unsafe responses created by a fine-tuned Llama-3-8B-Instruct model designed to produce harmful content in response to unsafe prompts. The Alpaca-GPT4-Data dataset consists of 52,000 safe prompts from Alpaca citepalpaca, paired with aligned responses generated by GPT-4. We randomly sample 628 prompts from Alpaca-GPT4-Data, and combined with the 628 unsafe prompts from SafeAligner, we create a half-safe and unsafe set of prompts.\nTraining We train LLMs using different alignment methods (DPO, KTO, BCO, KLDO) on the above data. The training spans 5 epochs with a learning rate of 5 \u00d7 10\u22125, a batch size of 32, \u03b2 = 0.1, and the Adam optimizer (Kingma & Ba, 2015; Zhang, 2018). We apply Low-Rank Adaptation (LoRA) (Hu et al., 2022; Zhang et al., 2023; Dettmers et al., 2023) with \u03b1 = 256, rank = 64, and dropout = 0.05. Combining LoRA with a high learning rate proved highly effective for achieving strong alignment while requiring less computation compared to full parameter training. We perform all our training on 2 Nvidia A100-80 GB Gpus."}, {"title": "C.1. Visalization Methodology", "content": "For each model, both safe and unsafe prompts are fed into the model, and the last-layer embeddings of the full prompts are extracted. These embeddings are then reduced to two dimensions using PCA for visualization purposes for each model separately."}, {"title": "C.2. Metrics for Separation & Clustering", "content": "Bhattacharyya Distance: The Bhattacharyya Distance between two probability distributions P", "following": "n$D_B = \\frac{1"}, {"Score": "The Silhouette Score quantifies the relative distance of a data point to its own cluster compared to the other cluster. For the i-th data point", "as": "n$s(i) = \\frac{b(i) \u2013 a(i)"}, {"where": "n$a(i) = \\frac{1}{|C_i|-1}\\sum_{j\u2208C_i,j\u2260i}d(i, j)$,\n$b(i) = \\underset{C\u2208C_i}{\\text{min}}\\frac{1}{|C|}\\sum"}]}