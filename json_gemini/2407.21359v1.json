{"title": "ProSpec RL: Plan Ahead, then Execute.", "authors": ["Liangliang Liu", "Yi Guan", "BoRan Wang", "Rujia Shen", "Yi Lin", "Chaoran Kong", "Lian Yan", "Jingchi Jiang"], "abstract": "Imagining potential outcomes of actions before execution helps agents make more\ninformed decisions, a prospective thinking ability fundamental to human cognition.\nHowever, mainstream model-free Reinforcement Learning (RL) methods lack\nthe ability to proactively envision future scenarios, plan, and guide strategies.\nThese methods typically rely on trial and error to adjust policy functions, aiming\nto maximize cumulative rewards or long-term value, even if such high-reward\ndecisions place the environment in extremely dangerous states. To address this,\nwe propose the Prospective (ProSpec) RL method, which makes higher-value,\nlower-risk optimal decisions by imagining future n-stream trajectories. Specifically,\nProSpec employs a dynamic model to predict future states (termed \u201cimagined\nstates\") based on the current state and a series of sampled actions. Furthermore, we\nintegrate the concept of Model Predictive Control and introduce a cycle consistency\nconstraint that allows the agent to evaluate and select the optimal actions from\nthese trajectories. Moreover, ProSpec employs cycle consistency to mitigate two\nfundamental issues in RL: augmenting state reversibility to avoid irreversible events\n(low risk) and augmenting actions to generate numerous virtual trajectories, thereby\nimproving data efficiency. We validated the effectiveness of our method on the\nDMControl benchmarks, where our approach achieved significant performance\nimprovements. Code will be open-sourced upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has proven indispensable for solving continuous decision problems\n[1, 2, 3]. Current RL research falls into two main categories: model-based and model-free methods.\nModel-based methods involve constructing a world model to simulate an interactive environment,\nwhich helps the agent predict future trajectories for optimal decision-making. However, these methods\ncan be expensive and prone to failure due to discrepancies between the world model and reality [4].\nOn the other hand, model-free methods refine the policy function through trial-and-error learning,\noften outperforming model-based techniques in continuous control tasks [5]. However, these methods\nrequire frequent interaction with the environment, which is risky or costly in real-world applications\n[6], and suffer from low data efficiency when learning from complex, high-dimensional observations\n(such as image pixels) based on limited experience.\nTraining model-free methods using low data efficiency and sparse signals to fit high-performance\nfeature encoders is challenging and prone to suboptimal convergence [7]. In contrast, humans can\nlearn to play simple arcade games in minutes [8], while even state-of-the-art RL algorithms require\nbillions of frames to reach human-level performance [9]. Moreover, collecting such large amounts of\ninteraction data is impractical, especially in domains such as autonomous driving and robot control."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Prospective thinking", "content": "The capacity to anticipate the future represents as a crucial facet of human cognition, recently\ngarnering substantial attention from researchers [20, 22, 23]. In psychology, prospective thinking\nis defined as the ability to mentally simulate the future, involving the extraction of information\nfrom episodic and semantic memory (such as detailed information about past states) as well as\nmore abstract, schematic, and conceptual knowledge (such as envisioning general goals or events)\n[24, 25, 26]. Broadly, prospective thinking involves two main aspects: 1) predicting future scenarios\nand 2) guiding decisions for those scenarios. In deep learning, methods such as Model Predictive\nControl (MPC) and model-based Reinforcement Learning (RL) appear to encapsulate certain aspects\nof prospective thinking.\nModel-based RL methods aim to construct an internal model, often referred to as a \"World Model,\"\nto simulate the environment. This model helps predict future states and rewards, guiding the agent's\ndecision-making and planning. Examples include PILCO [27], Dyna-Q [28], and World models [29].\nOn the other hand, MPC-based methods utilize the world model to optimize trajectories within a\nshorter time horizon. Examples of such methods include MuZero, Eff.Zero [30], and TD-MPC [5].\nHowever, these approaches heavily rely on accurate environmental modeling and are limited by the\ncomplexity of state spaces. Consequently, model-based approaches often struggle to outperform\nsimpler model-free methods in continuous control tasks [5]. Model-free methods, in contrast, learn\npolicies through trial and error, directly interacting with the environment without the need for a\nWorld Model. This method is easy to implement and adapts well to complex environments. However,\ndue to the frequent interaction with the environment and the lack of prediction of future states or\nconsequences, it can lead to risky exploration of the environment [6].\nBuilding upon this, we merge the planning capabilities of model-based methods to enhance model-\nfree methods. To achieve this, we introduce a dynamics model into model-free methods, enabling the\nprediction of future scenarios. Additionally, we integrate the concept of MPC with value consistency\nto aid decision-making and guidance for future scenarios. Our objective is to leverage prospective\nthinking in selecting the optimal path while maintaining the trial-and-error learning approach of\nmodel-free methods. This integration aims to enhance the effectiveness of model-free methods in\nlearning."}, {"title": "2.2 Data-Efficient Reinforcement Learning", "content": "When confronted with a limited amount of interaction data, the performance of both model-based\nand model-free methods experiences a significant decline. Researchers have extensively investigated\nthis issue from various angles [3, 10, 11, 14, 15, 16, 17]. For example, SiMPLe utilizes collected\ndata to learn a world model and generate imagined trajectories. They achieved outstanding results in\nmultiple games under the setting of 100k frames, albeit at the cost of several weeks of training time.\nSubsequent studies, such as DataEfficient Rainbow [31] and OTRainbow [32], have demonstrated that\nmerely increasing the number of steps in multi-step returns and conducting more frequent parameter\nupdates can lead to improved performance under conditions of low data efficiency.\nConsequently, there is growing interest in using computer vision techniques to enhance representation\nlearning in RL [12, 13, 7]. For example, DrQ [7] and RAD [13] showed that moderate image\naugmentation can significantly improve the efficiency of RL data, outperforming previous methods\nusing world models in model-based approaches. Yarats et al. [12] introduced image reconstruction\nas an auxiliary loss function in RL to improve data efficiency. Inspired by contrastive learning in\nrepresentation learning, CURL [33] incorporated contrastive loss into RL, allowing the algorithm to\ndistinguish representations of different states through contrastive loss and promote the generation\nof similar embeddings for similar states. In addition, some studies aim to incorporate dynamical\nmodels to help agents predict future events, thereby generating multiple virtual trajectories to improve"}, {"title": "3 Methods", "content": "In this section, we will systematically introduce the proposed ProSpec method. First, we will review\nsome preliminary work. Then, we will present the overall framework of ProSpec."}, {"title": "3.1 Preliminaries: Reinforcement Learning", "content": "Reinforcement learning (RL) typically uses Markov decision processes (MDPs) to solve continuous\ndecision problems, denoted as < S, A, T, R, \u03b3 >. Here S represents a finite set of states; A is the\naction space; T(St, at, St+1) = P(st+1|St, at) is the dynamic function, which defines the probability\nof transition from state st to st+1 after taking action at; R(st, at) is the reward function, and\n\u03b3\u2208 (0, 1] is the discount factor. The goal of RL is to enable the agent to learn how to maximize the\nexpected discounted cumulative payoff Gt = \u03a3\u03c4=0YR($t+7, at++) by choosing actions at each\ntime t. The agent's decision process is guided by the policy \u03c0(at st), which maps the current state to\nthe action selection. The action-value function Q\u03c0(St, at) = \u0395\u03c0[Gt|st, at] evaluates the expected\npayoff of taking action at in state st and following policy \u03c0. This paper focuses on value-based\nRL methods, specifically Q-learning, which approximates the optimal policy \u03c0* using the Bellman\nequation [37]. Therefore, Soft Actor-Critic (SAC) is employed as the policy algorithm, given that it\nis a widely utilized gradient-based algorithm for continuous control that incorporates policy entropy\nas an additional reward to encourage exploration. The overall training objective of SAC is as follows:\nTo = Lcritic + Lactor + La\nwhere, Lcritic, Lactor and La represent the critic loss, actor loss and temperature loss, respectively (see\nthe supplementary material for details)."}, {"title": "3.2 Overall Framework", "content": "In this paper, we introduce the ProSpec method, which aims to incorporate prospective thinking into\nmodel-free methods. ProSpec consists of two main components: first, the Flow-based Dynamics\nModel for predicting future scenarios, and second, the prospective unit that integrates Model Predictive\nControl and value consistency to guide decision-making. Figure1 shows the process of ProSpec RL.\nIn the following sections, we provide detailed explanations of each component.\nFlow-based Dynamics Model (FDM). The FDM predicts the future and retroactively infers previous\nlatent states, crucial in ProSpec. Unlike PlayVirtual [14], which employs separate models for these"}, {"title": "4 Experiments", "content": "In this section, we will provide a detailed introduction to the evaluation environment used in this paper,\nincluding experimental settings, experimental details, and evaluation metrics. Then, we perform a\nseries of ablation experiments to analyze the key components of our method."}, {"title": "4.1 Setup for Evaluation", "content": "Environments. We conducted benchmark tests on ProSpec in environments with limited interaction,\nusing the DeepMind Control Suite (DMControl) for continuous control testing [40]. Following the\npractices of most researchers [7, 12, 14, 31, 41, 42, 43], we evaluated the performance after training\nfor 100k and 500k interaction steps in the DMControl environment, selecting a total of 6 games for\nperformance evaluation.\nBaseline. In this work, we consider SPR [16] as a strong baseline. We also selected Dreamer [44],\nSAC+AE [7], SLAC [34], CURL [33], DrQ [12], SPR [16], PlayVirtual [14], and VCR [17] as\nbaselines. It is worth noting that since SPR was originally designed for offline tasks, we adapted SPR\nfor continuous tasks using a SAC-based approach (SPR).\nImplementation Datails. We used the encoder and policy network architecture of CURL [33] as\nthe basic network. The contrastive loss component was removed from CURL, and we introduced\nBYOL [45], creating a baseline solution similar to SPR. To generate imaginary trajectories in this\nenvironment, the prediction horizon was set to 6, and action generation was achieved by randomly\nsampling from a uniform distribution over the continuous action space. Additional implementation\ndetails are provided in the Appendix.\nEvalation Metrics. In DMControl, the maximum achievable score for each environment is capped\nat 1000 [40]. Consistent with previous studies [7, 33, 34, 46, 44], we evaluated model performance\nacross six common environments and used the median score to represent overall performance. To\nreduce potential variance in benchmarking results, we performed ProSpec evaluations on DMControl\nusing 10 random seeds for each game. For the 100k step, we reran the official code for SPR and\nPlay Virtual with 10 random seeds and recorded the final results. We did not rerun the code for VCR\nas we were unable to obtain the official implementation. Due to limited computational resources,\nwe report the 500k results directly from the respective papers. The entire implementation was using\nPyTorch [47], with network training performed on an NVIDIA RTX 3090 GPU."}, {"title": "4.2 Performance Comparison with State-of-the-Arts", "content": "As shown in Table 1, our method surpassed the performance of most other methods by achieving five\nof the top scores in six environments, regardless of the limited 100k or 500k interactions. Furthermore,\nwhen considering the median scores, our method achieved the highest rankings in the following two\naspects: (i) In the DMControl-100k environment with limited interaction data, our method achieves\nthe highest median score of 807.5. This outperforms PlayVirtual by 8.32%, VCR by 3.66%, SPR by\n9.64%, DrQ by 17.80%, and CURL by 44.20%. (ii) Our method achieves a median score of 959.5\nand comes close to the maximum score of 1000 in four environments. These results demonstrate the\nexceptional performance of our method even with limited data. Furthermore, our method shows a\nsignificant improvement in performance across different environments."}, {"title": "4.3 Analysis", "content": "In this section, we performed an ablation study in the DMControl-100k to analyze the key components\nof ProSpec. Due to limited computing resources, we experimented using 5 random seeds."}, {"title": "5 Conclusion and Limitation", "content": "To imbue deep RL with cognitive abilities akin to those of humans\u2014namely, prospective thinking-we\npropose a novel method named ProSpec. ProSpec employs a reversible flow-based dynamics model\nto predict latent future outcomes, drawing on the concept of Model Predictive Control to guide\ndecision-making in anticipation of future scenarios. This approach enables RL to simulate future n-\nstream trajectories, facilitating high-value, low-risk optimal decision-making. Additionally, ProSpec\naugments state traceability through cycle consistency, reducing environmental risks and leveraging\nthis to augment actions, thus generating a wealth of virtual trajectories to improve data efficiency. We\nconducted a large number of experiments at DMControl and the results confirmed the effectiveness\nof ProSpec. However, due to the necessity of exploring future scenarios from multiple perspectives,\nour method incurs additional overhead in training the agent. In addition, ProSpec is currently tailored\nfor value-based RL methods; future research endeavors will aim to discover a universally applicable\nRL approach that incorporates prospective thinking."}]}