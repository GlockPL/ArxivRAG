{"title": "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning", "authors": ["DataTager"], "abstract": "The pervasive deployment of Large Language Models-LLMs in various sectors often neglects the nuanced requirements of individuals and small organizations, who benefit more from models precisely tailored to their specific business contexts rather than those with broadly superior general capabilities. This work introduces AnyTaskTune, a novel fine-tuning methodology coined as Task-Fine-Tune, specif-ically developed to elevate model performance on a diverse array of domain-specific tasks. This method involves a meticulous process to identify and define targeted sub-tasks within a domain, followed by the creation of specialized enhancement datasets for fine-tuning, thereby optimizing task-specific model performance. We conducted comprehensive fine-tuning experiments not only in the legal domain for tasks such as keyword extraction and sentence prediction but across over twenty different sub-tasks derived from the domains of finance, healthcare, law, psychology, consumer services, and human resources. To substantiate our approach and facilitate community engagement, we will open-source these bilingual task datasets. Our findings demonstrate that models fine-tuned using the Task-Fine-Tune methodology not only achieve superior performance on these specific tasks but also significantly outperform models with higher general capabilities in their respective domains. Our work is publicly available at https://github.com/PandaVT/DataTager.", "sections": [{"title": "Introduction", "content": "Recently, the rapid advancement and deployment of Large Language Models (LLMs) have transformed various sectors by providing unprecedented natural language processing capabilities [21]. Traditionally, these models have been developed with a focus on enhancing their general abilities, aiming to create universally powerful tools that excel across a broad spectrum of tasks and domains. However, this generalized approach often fails to meet the specific requirements of individual users and small organizations, whose needs are intricately tied to their unique operational contexts. To address this issue, one very simple way is to fine-tune LLMs for different tasks. In the medical field, researchers have optimized LLMs using medical datasets to support functionalities such as medical document interpretation and diagnostic consultations. For example, ChatMed [23], DISC-MedLLM [2], HyKGE [9], IvyGPT [12], and HuatuoGPT [20] are notable examples, demonstrating significant advancements within their specialized medical domains compared to generic LLMs. Similarly, in the legal field, researchers have introduced fine-tuned LLMs trained on legal corpora to support activities such as legal research, contract analysis, and legal document summarization. Models like ChatLaw [7], LawGPT [22], and DISC-LawLLM [19] exemplify this trend, demonstrating a deeper grasp of legal language and principles compared to their generic counterparts. Within finance, researchers are leveraging diverse financial datasets, including conversational data and market reports, to fine-tune LLMs for applications such as financial forecasting, risk assessment, and fraud detection. Examples include FinGPT [16], DISC-FinLLM [4], and PIXIU [13]. Additionally, in other fields, models like FaiMA [17], designed for Multi-domain applications, and Kuaiji [10], tailored for accounting tasks, illustrate the versatility and potential of fine-tuned LLMs in specialized domains.\nTo address this discrepancy, we introduce a novel fine-tuning paradigm specifically designed for Explicit Data Sets, which we term \"task fine tune.\" This method diverges from conventional training techniques by emphasizing precision and specificity over general performance. By focusing on Explicit Data Sets that contain clear, directive input-output pairs with specific instructions, this approach allows for precise tailoring of models to perform designated tasks effectively. Such fine-tuning not only enhances the model's ability to execute particular functions but also significantly improves its applicability and efficiency in real-world scenarios.\nThe cornerstone of this work, \"AnyTaskTune,\" is to validate and elaborate on the \"task fine tune\" method, utilizing Explicit Data Sets across various domains. This method involves identifying the spectrum of tasks required in different fields and scenarios and meticulously developing numerous Explicit Data Sets to conduct task-specific fine-tuning. The aim is to optimize the model's performance for individual and corporate use, ensuring that it meets the nuanced demands of users in fields such as finance, healthcare, law, psychology, consumer services, and human resources.\nTo showcase the effectiveness and versatility of this approach, we conducted experiments using over twenty different Explicit Data Sets that we will open-source. These datasets are bilingual and designed to cater to a diverse global audience, providing a valuable resource for the community to engage in further research and enhance application-specific model improvements.\nBy concentrating on specific, clearly defined tasks rather than general capabilities, \"AnyTaskTune\" significantly outperforms traditional models that prioritize breadth over depth. This strategic shift towards more specialized, task-oriented model training represents a critical evolution in how we develop and deploy LLMs, making them more relevant and valuable in industry-specific applications.\nThe primary contributions of this paper can be summarized as follows:\n\u2022 Introduction of the \"task fine tune\" paradigm, specifically designed for Explicit Data Sets, emphasizing precision and specificity over general performance.\n\u2022 Development and utilization of over twenty bilingual Explicit Data Sets across various domains to validate the effectiveness of the \"task fine tune\" method.\n\u2022 Evaluation of multiple models and datasets across various domains, demonstrating that task-specific fine-tuning yields better results.\n\u2022 Open-sourcing of the developed Explicit Data Sets, providing a valuable resource for the com-munity to engage in further research and enhance application-specific model improvements.\n\u2022 Implementation of our data processing model, DataTager-LLM, as an online accessible website, https://datatager.com."}, {"title": "AnyTaskTune", "content": "2.1 Model Requirements in Business Applications\nIn practical applications, simply pursuing the performance of a general model or deploying a domain-specific model alone is often insufficient to address real-world problems. While scientific research outputs need to be diverse and comprehensive, businesses require standardized and controllable solutions. For most enterprises and organizations, their needs are highly specific and contextualized, which cannot be fully met by a single general language model. In such cases, general models may underperform on specific tasks or fail to achieve the expected efficiency and accuracy. Moreover, while some domain-specific models may provide good performance within their fields, they lack flexibility and scalability, making it difficult to adapt to ever-changing business needs. Therefore, there is a need for a new method to fine-tune and optimize models, ensuring they not only possess the broad applicability of general models but also meet the specific needs of particular domains and tasks. AnyTaskTune addresses this issue by using specially designed explicit datasets for task fine-tuning, ensuring the model's precision and efficiency in specific tasks, thereby enhancing its value in real-world business applications.\n2.2 DataTager\nFirst, leveraging our prior experience and expertise in vertical domains, we collected raw data from mul-tiple fields, including finance, healthcare, law, psychology, consumer services, and human resources. By surveying industry professionals to understand their expectations and requirements for models, we syn-thesized multiple branch datasets. These datasets cover core tasks across various domains and include detailed and variant data from real-world scenarios. Through the training of these branch datasets, we developed DataTager-LLM, a large-scale data synthesis model. DataTager-LLM forms the foundation of our product and serves as a core tool for further research and application. Currently, we have no plans to open-source DataTager-LLM to better protect our intellectual property and commercial interests.\n2.3 Explicit and Implicit DataSets\nWe defined what constitutes a \"good\" dataset for businesses and distinguished between these datasets based on the presence of explicit instructions. Specifically, datasets with clear instructions, such as \"instruction: Please summarize this news article and extract the key points; input: xxx,\" are designed for models to perform specific tasks. Explicit datasets provide models with clear guidelines, enhancing their ability to understand and execute specific tasks accurately and efficiently in practical applications. On the other hand, datasets without explicit instructions, such as \"instruction:, input: xxx,\" involve random queries without fixed task types, thus considered implicit datasets. Implicit datasets are better suited for handling open-ended questions and unstructured data but are more challenging to train and apply. For businesses looking to enhance their operations with LLMs, the first step is to identify the various task types within their current operational context. This helps determine the number of explicit datasets needed. Each dataset is then synthesized individually, and a general LLM that aligns with the business's usual interaction patterns is fine-tuned across multiple tasks to create a model that best fits the specific business scenarios.\n2.4 AnyTaskTune\nDifferent fields have multiple interaction (action) modes. Take the medical field, for example: if triage is considered an action, the actual tasks it involves include: a. Patients describing their issues in non-standard language; b. Triage staff translating this into medical standard language; c. Staff analyzing the completeness of the information and asking follow-up questions if necessary; d. Extracting medical key points and directing the patient to the appropriate department. This triage action thus includes four sub-tasks (a, b, c, d). We can use DataTager to generate datasets for each sub-task. These datasets collectively represent the triage action process. Then, we train models based on these datasets, with multiple approaches available:\nIf we train a separate model for each sub-task, the model can address that specific sub-task effectively. For instance, a model specifically trained for sub-task a can handle the translation of patient descriptions, while another model trained for sub-task b can convert non-standard language into medical standard language. If we combine these sub-task datasets to train a large mixed model, different instructions will trigger different task types. This method can handle multiple tasks within one model but requires a more complex training process and more computational resources. It is important to note that if we choose to train multiple small models, based on our experience, a 1.5 billion or 3 billion model can handle less complex tasks well without data drift. If the performance of the mixed model declines, adding some general datasets to the training process can help. This way, we can use high-performing models to replace traditional sub-tasks in action interactions, creating a new interaction model and reducing costs."}, {"title": "Experiments", "content": "To validate the effectiveness and versatility of the AnyTaskTune paradigm, we undertook extensive experiments across multiple domains: finance, healthcare, law, psychology, and role-play. We compared the performance of AnyTaskTune against various models including closed-source large language models (LLMs), open-source LLMs, and domain-specific models. Critically, our experiments maintained a strict separation between training and testing datasets to ensure unbiased evaluation and reproducibility of results.\n3.1 Experimental Setup\nOur experiments were structured as follows:\n\u2022 Model Base: We utilized Qwen2-7B [1] as the base model for AnyTaskTune training. This model was fine-tuned on specific sub-tasks within each domain, and then tested to obtain the experimental results.\n\u2022 Model Categories:\n Closed Source LLMs: GPT-4 [11], GPT-3.5, LLaMA3-8b, and LLaMA3-70b.\n Open Source LLMs: Qwen, Baichuan [15], ChatGLM [8], and internalLM [3].\n3.2 Results and Domain-Specific Analysis\nThe results of the experiments are summarized in Table 1. Our AnyTaskTune models, fine-tuned on specific domain data, demonstrated significant performance improvements, even sometimes surpassing well-regarded models like GPT-4 and LLaMA3-70b. The following points detail cross-domain influences and task-specific adaptabilities observed during testing:\n3.3 Detailed Comparative Analysis\n3.3.1 Impact Across Domains\nMedical Domain: The AnyTaskTune-Qwen2-7B-Med model showcased exceptional performance in medical sub-tasks, notably achieving a F1 score of 0.835 in M1, which is a substantial improvement over the base Qwen2-7B model's score of 0.338. This precision underscores its reliability in handling medical data. However, when this model was tested in finance and law domains, its performance dropped to 0.247 and 0.242 respectively, illustrating a significant decline due to the model's parameters being highly specialized to medical data.\nFinancial Domain: AnyTaskTune-Qwen2-7B-Finance performed excellently in financial tasks, especially F2 and F3, with scores of 0.739 and 0.816, outperforming the base model's scores of 0.497 and 0.516 in these tasks. Applying this model to legal tasks resulted in moderate performance drops (from 0.462 in legal tasks down from 0.458 in finance tasks), likely due to some overlap in structured data and terminological precision between the finance and legal fields. However, performance in the medical domain was notably poorer, confirming the domain-specific nature of the fine-tuning.\nLegal Domain: The AnyTaskTune-Qwen2-7B-Law excelled in legal tasks with scores of 0.75, 0.743, 0.793, and 0.798 across L1, L2, L3, and L4. Its performance in the finance domain was competitive, with scores slightly lower but still respectable. However, when applied to the medical and psychological domains, the performance was limited, further highlighting the challenges of cross-domain applicability due to different knowledge bases and data characteristics.\nPsychology and Role-Play Domains The AnyTaskTune-Qwen2-7B-Psychology model showed good results in psychology, particularly in the P2 task with a score of 0.659. The nature of psychological tasks, which often involve complex language and emotional analysis, allows this model to perform reasonably well in role-play scenarios as well, evidenced by a score of 0.572 in the MBTI task.\nRole-Play Domain: Similar to the psychology model, AnyTaskTune-Qwen2-7B-RolePlay demon-strated good performance due to overlapping requirements in emotional and language processing between the two fields.\n3.3.2 Task Adaptability and Generalizability\nWhile domain-specific fine-tuning led to exceptional performance within respective fields, it generally resulted in reduced effectiveness when models were applied across unrelated domains. For instance, while AnyTaskTune-Qwen2-7B-Med excelled in medical tasks, its application in non-medical fields showed significant performance reductions, a clear indication of the trade-offs involved in specialized fine-tuning.\nComparison with Base Model: The original Qwen2-7B model, without fine-tuning, generally showed lower performance across all specific tasks compared to the fine-tuned AnyTaskTune models. This highlights the effectiveness of task-specific fine-tuning in enhancing model capabilities, although it restricts the model's flexibility across various domains.\n3.4 Sub-task Details\nThe specific types of sub-tasks evaluated in this experiment will be detailed in the accompanying GitHub repository. This repository will provide comprehensive descriptions and examples of each sub-task, offering valuable insights for further research and application development.\n3.5 Analysis\nThis analysis confirms that while AnyTaskTune can dramatically improve performance on specific tasks, its cross-domain applicability is limited. This highlights the importance of task-specific datasets in fine-tuning processes, ensuring that models are not only highly effective but also tailored to the particular characteristics and requirements of their intended operational domains."}, {"title": "Conclusion", "content": "In this work, we introduced AnyTaskTune, a novel fine-tuning paradigm specifically designed for Explicit Data Sets. Our approach addresses the limitations of traditional general models and domain-specific models by emphasizing precision and specificity, which are critical for real-world applications. By leveraging our extensive experience in various vertical domains, we developed and utilized over twenty bilingual Explicit Data Sets, enabling us to fine-tune models for specific tasks with high accuracy and efficiency.\nOur experiments demonstrated that AnyTaskTune significantly outperforms both closed-source and open-source LLMs, as well as existing domain-specific models, across a range of tasks in finance, health-care, law, psychology, and role-play. The results showcased the superior performance of AnyTaskTune in handling complex and nuanced tasks, thereby validating the effectiveness of task-specific fine-tuning.\nMoreover, we highlighted the practical implications of AnyTaskTune for businesses and organizations. By defining clear and directive input-output pairs through Explicit Data Sets, we enable precise model tailoring, which not only enhances task performance but also improves the model's applicability and efficiency in specific operational contexts. Our methodology ensures that models are not just broadly capable but are finely tuned to meet the exact needs of various business scenarios.\nWe also introduced DataTager, our foundational data synthesis model, which forms the basis for generating high-quality datasets used in AnyTaskTune. While we have not open-sourced DataTager-LLM, we have provided comprehensive documentation and resources in our accompanying GitHub repository, detailing the sub-tasks and datasets used in our experiments. This transparency aims to foster further research and development in the community.\nIn conclusion, AnyTaskTune represents a significant advancement in the development and deployment of Large Language Models. By shifting the focus from general capabilities to specialized, task-oriented model training, AnyTaskTune offers a robust and efficient solution for industry-specific applications. This strategic evolution in model fine-tuning not only enhances performance but also ensures that LLMs are more relevant and valuable in real-world business contexts.\nWe believe that AnyTaskTune will pave the way for more precise and effective applications of AI in various domains, ultimately contributing to the broader adoption and integration of AI technologies in everyday business operations."}]}