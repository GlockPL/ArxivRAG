{"title": "Sign-Symmetry Learning Rules are Robust Fine-Tuners", "authors": ["Aymene Berriche", "Mehdi Zakaria Adjal", "Riyadh Baghdadi"], "abstract": "Backpropagation (BP) has long been the predominant method for training neural networks due to its effectiveness. However, numerous alternative approaches, broadly categorized under feedback alignment, have been proposed many of which are motivated by the search for biologically plausible learning mechanisms. Despite their theoretical appeal, these methods have consistently underperformed compared to BP, leading to a decline in research interest. In this work, we revisit the role of such methods and explore how they can be integrated into standard neural network training pipelines. Specifically, we propose fine-tuning BP-pre-trained models using sign-symmetry learning rules and demonstrate that this approach not only maintains performance parity with BP but also enhances robustness. Through extensive experiments across multiple tasks and benchmarks, we establish the validity of our approach. Our findings introduce a novel perspective on neural network training and open new research directions for leveraging biologically inspired learning rules in deep learning.", "sections": [{"title": "1. Introduction", "content": "The human cerebral cortex learns through updating synapses based on input signals that activate multiple regions involved in the learning process. The complex multilayered structure of our brain's neural network makes it challenging to determine the exact mechanism responsible for learning in the brain. A common way to address this problem in Deep Artificial Neural Networks is through a credit assignment algorithm responsible for updating synaptic connections based on feedback signals. These algorithms typically differ in the way the feedback signal is backpropagated. The most widely used credit assignment algorithm is Backpropagation (BP)."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Backpropagation and Bio-plausible Learning Rules", "content": "Backpropagation, while efficient, is often criticized for its biological implausibility, especially regarding the weight transport problem. Unlike the brain, which uses asymmetrical feedback signals, backpropagation employs identical weights for forward and backward passes. Efforts to create biologically plausible credit assignment methods focus on reducing weight transport. Backpropagation relies entirely on a symmetrical feedback structure, while Sign-Symmetry employs weight sign matrices, reducing the extent of weight transport. Variants include Uniform Sign-Concordant Feedback (uSF), Fixed Random Magnitude Sign-Concordant Feedback (frSF), and Batchwise Random Magnitude Sign-Concordant Feedback (brSF). These approximate gradients enable learning without temporally synchronized gradients. Feedback Alignment (FA) uses fixed random feedback matrices, avoids weight transport entirely and demonstrates that symmetry is unnecessary for training. This mechanism aligns forward synaptic connections with synthetic feedback, making errors derived by feedforward weights converge toward those calculated by synthetic backward matrices. Additionally, biologically plausible methods could overcome backpropagation's sequential nature, which limits computational efficiency. Techniques like Target Propagation use local updates but perform poorly at scale, discouraging further exploration."}, {"title": "3. Background", "content": ""}, {"title": "3.1. Learning Algorithms", "content": "In the following, we consider a fully connected neural network with L layers. W\u2081 is the weight matrix for layer l. a denotes the pre-activation of layer l, and satisfies h\u2081 = f(ar), where hi is the activation vector for layer l and f is a non-linearity. The network final output is denoted \u0177. We calculate the error using the squared error \\(E = \\sum_k (\\hat{Y_k} - Y_k)^2\\).\nThe Backpropagation algorithm In backpropagation, we compute the exact error for each parameter of the network and update its value using the negative of its gradient, given in vector/matrix notation:\n\\[ \\Delta W_l = -\\eta \\frac{\\partial E}{\\partial W_l} = -\\eta \\delta_l h_{l-1}^T \\] \nWhere n is the learning rate and 8 is referred to as the error signal and is computed recursively via the chain rule:\n\\[ \\delta_l = (W_{l+1}^T \\delta_{l+1}) \\circ f'(a_l) \\] \nWhere is the Hadamard product. This equation demonstrates the full symmetry of backpropagation, as the full weight matrix is used to calculate the backward update.\nFeedback Alignment In Feedback Alignment , the update of the synaptic connections is similar to BP given by (1), but instead of using the weight matrix to perform the backward update, a synthetic fixed random matrix (note that, the exact distribution is often not critical, as long as the feedback matrices remain fixed after initialization and provide a roughly consistent direction for error"}, {"title": "3.2. Adversarial Robustness", "content": "Adversarial robustness quantifies a machine learning model's ability to withstand inputs intentionally perturbed to induce erroneous predictions. Evaluating and enhancing this property is of paramount importance, particularly in applications where reliability and trustworthiness are critical. Adversarial attacks are broadly categorized based on the level of access and information available to the attacker:\nWHITE-BOX ATTACKS\nWhite-box attacks leverage access to the gradient information of the model to craft perturbations that maximize the classification loss. The perturbations are tailored to deceive"}, {"title": "4. Method", "content": "Tasks We apply our proposed training method to two tasks: image classification, and hashing-based image retrieval. Since the first is well known, we will provide more details about the second. This task involves mapping high-dimensional image data to compact binary codes that preserve semantic similarity. The process consists of two main steps: first is to learn a hash function to map input images to continuous encodings while preserving semantic relevance , and the second is to generate binary codes by binarizing the hidden representations using a sign function. Retrieval is then performed by comparing binary codes using Hamming distance, enabling efficient matching.\nLearning Method and Model Architectures We propose to use backpropagation for pre-training followed by the use of a Sign-Symmetry credit assignment method for fine-tuning. We use a CNN-based backbone model that was pre-trained on ImageNet using backpropagation (e.g., RestNet-18). We then append a task-specific fully connected layer to adapt the network to the particular task, either a hash layer or a classification layer, with its parameters trained from scratch. Subsequently, we fine-tune all network parameters using a Sign-Symmetry method (e.g., uSF). We do not freeze the weights of the backbone during fine-tuning. We use the hashing loss HyP2 to fine-tune the task of image retrieval, setting the hashing size k to 32, while we use the Cross-Entropy loss function for image classification."}, {"title": "5. Experimental Setup", "content": "Models We evaluate our method using three different backbone architectures: AlexNet , ResNet-18 , and VGG-16 . These pre-trained backbone models were obtained from the torchvision model repository. The motivation behind using these backbones is that they already have established biologically plausible definitions for their respective architectures. In our approach, we propose a new method for training neural networks rather than a biologically plausible version of AlexNet or VGG-16 (work that has already been done).\nDatasets We evaluate our approach across varying complexity levels, following established protocols. For image classification, we use CIFAR-10 , MS-COCO , NUS WIDE , and ImageNet dataset. Adversarial attacks were tested on 5,000 images per dataset, based on the method from.\nFor hashing-based image retrieval, we use CIFAR-10, NUS WIDE , MS-COCO , and ImageNet100. CIFAR-10 has 500 images per class for training, and 100 for testing/validation. NUS-WIDE is filtered to 148,332 images, MS-COCO to 20,000, and ImageNet100 contains 13,000 training and 5,000 testing/validation images. The remaining images are used as the query database.\nTraining Pre-trained backbones, initially trained on ImageNet, were loaded through torchvision. For fine-tuning in both tasks, the same set of hyperparameters was employed. We used ADAM as the optimizer with \u03b2\u2081 = 0.9 and B2 = 0.999, and a weight decay of 0.0005. The learning rate was set to 10-5 for all layers except the classification/hashing layer, which was assigned a learning rate of 10-4. We fine-tuned each dataset for 20 epochs using a batch size of 32, apart from ImageNet and NUS WIDE, that have been fine-tuned for 10 epochs.\nEvaluation Metrics We assess performance using two metrics: accuracy for the classification task and mean average precision (mAP) for the hashing-based image retrieval task, which are both widely used for the tasks we consider. For the task of image retrieval, we follow the practices established in the literature and compute the mAP@k with k = 5000 for CIFAR-10, NUS-WIDE, and MS COCO datasets, and with k = 1000 for ImageNet, The mAP metric is defined as follows:\n\\[ mAP@k = \\frac{1}{|Q|} \\sum_{q \\in Q} AP(q) \\] \nwhere Q represents the set of queries, and APk denotes the average precision of the first k \u2264 n retrieved entries..\nAdverserial Robustness Evaluation Setup Following prior studies on adversarial robustness and the robustness of bio-inspired methods , we adopt a similar experimental setup with some adjustments. Adversarial attacks experiments for classification models were implemented using Foolbox , using the L\u221e variants of FGSM and PGD. FGSM is a single-step attack, while PGD is run for 5 iterations with a step size a = 6/3. Perturbation magnitude e varies from 0 to 0.5, then the robust accuracy is recorded. To demonstrate that Sign-Symmetry learning rules specifically enhance robustness against gradient-based attacks while leaving models as vulnerable as the classical BP fine-tuned ones to gradient-free adversarial strategies, we conduct similar experiments on black-box attacks where we use Hop-SkipJump (HSJA) and Boundary Attack (BA).\nFor hashing-based image retrieval, robustness is assessed against HAG and SDHA non-targeted attacks, using 5 iterations and perturbation magnitudes ranging from 0.001 to 0.5. These settings prioritize computational efficiency over optimal attack strength, focusing on the comparative robustness of learning methods under adversarial perturbations.\nAs gradient-based attacks rely on model gradients to craft adversarial samples, sign symmetry methods with approximate gradients make fine-tuned models inherently harder to fool using such attacks."}, {"title": "6. Results", "content": "We compared our proposed approach and the classical fine-tuning approach where backpropagation is used to fine-tune all the parameters of the model. We evaluate on different backbones and datasets. We compare the performance and robustness of the models in each case (performance is measured using the top-1 accuracy for image classification and the mean average precision for image retrieval)."}, {"title": "6.1. Performance", "content": "Image Classification We first measure the accuracy for the classification task. The results indicate that Sign-Symmetry methods can perform comparably to, or outperform BP in different experiment settings, such as AlexNet-CIFAR10, AlexNet-ImageNet, and ResNet18-CIFAR10. Generally, uSF and frSF are the two methods that show the best results in classification.\nHashing-based Image Retrieval We asses hashing-based image retrieval models fine-tuned using the following credit assignment methods (BP, frSF, and uSF), on four datasets"}, {"title": "6.2. Adverserial Robustness", "content": ""}, {"title": "6.2.1. WHITEBOX ATTACKS (GRADIENT-BASED)", "content": "Image Classification We evaluate the robustness of the image classification models using two gradient-based attacks: FGSM and PGD. We benchmarked the accuracy of the fine-tuned models on three datasets (CIFAR10, MS COCO, and ImageNet), using two backbone architectures (AlexNet and VGG-16). For each backbone-dataset configuration, we compared Sign-Symmetry methods to backpropagation and recorded the robust accuracy for each perturbation distance \u20ac \u2208 {0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5}.\nAll backbones exhibit similar behavior under adversarial attacks, with Sign-Symmetry methods showing consistently greater robustness than BP across configurations. This trend is apparent in most results, for instance, ResNet18 results in Figure 2 show that the robustness gap can surpace 50% for both types of attack. Among Sign-Symmetry variants, brSF and frSF perform best, with frSF demonstrating the most stable performance .\nMost notably, we observe that BP accuracy decreases drastically compared to Sign-Symmetry methods, which exhibit a more gradual decline. In ResNet18 experiments (Figure 2), BP's accuracy drops to 0 rapidly at \u20ac = 0.1. The difference in accuracy between BP and Sign-Symmetry methods can reach up to 71.88% in multiple configurations .\nHashing-based Image Retrieval We have measured the performance of deep hashing models when fine-tuned using either BP or Sign-Symmetry methods (frSF, brSF, uSF). The mean average precision was measured while varying the perturbation distance e from 0 to 0.5. This was done for both HAG and SDHA attacks, and for each experiment combination.\nThe robustness of AlexNet and VGG-16 under adversarial attacks reveals critical insights. As can be seen in figure 3 BP collapses entirely on ImageNet (mAP \u2248 12% at \u20ac = 0.5), while sign symmetry methods retain ~40% mAP. Similar trends hold for CIFAR-10 and MS COCO. Both"}, {"title": "6.2.2. BLACK-BOX ATTACKS", "content": "The results of the experiments presented in tables 3 and 4 indicate that there's no explicit tendency for any of the methods. However, this is expected since black-box attacks do not depend on the nature of the learning algorithm to perform their attack. The other positive takeaway from this experiment is that Sign-Symmetry still performs similarly to BP, which highlights no disadvantage or instability when using it."}, {"title": "7. Discussion", "content": "Sign-Symmetry methods can be thought of as robust fine-tuners, this robustness is attributed to the use of approximate gradients during backpropagation, which obfuscates the use of exact gradients by gradient-based adversarial attacks, making the task more challenging for such optimization techniques. While previous work by has demonstrated the performance limitations of bio-plausible methods compared to backpropagation, no study has yet considered the use of bio-plausible methods along with backpropagation, potentially benefiting from both the effective representation learning of BP and the robustness enhancement of bio-inspired methods. Our research demonstrates that through fine-tuning pre-trained models using bio-plausible methods, we obtain models that achieve a performance comparable to BP while being more robust. Our findings also indicate that Sign-Symmetry methods, when used in fine-tuning, are very effective in terms of performance and offer the greatest potential for results comparable to BP. Among Sign-Symmetry methods, frSF emerged as the most performant and stable learning method. While our research has shown the effectiveness of the proposed approach, it is also important to investigate the direct impact of bio-plausible learning on the robustness of BP-trained methods. We suggest the use of a few fine-tuning steps using a Sign-Symmetry method as a corrective measure for robustness in BP-trained models. To fully explore this approach, more experiments need to be conducted with appropriate settings across various architectures. It should be noted that adversarial attacks are generally designed to target models trained using BP, especially white-box attacks that rely on the model's gradient. Given this, it would be more equitable to compare BP's robustness with attacks specifically designed for bio-plausible learning methods. This observation opens another avenue of research: developing attacks tailored to bio-plausible methods."}, {"title": "8. Conclusion", "content": "In our work, we propose a novel way of training neural networks, that enhances robustness to adversarial attacks while keeping a similar performance to traditional methods. Through our proposed approach, we narrow the gap between bio-plausible learning methods and backpropagation. By integrating backpropagation with Sign-Symmetry methods, we have demonstrated the potential of achieving high robustness while maintaining performance comparable to BP. Improvements in robustness against adversarial attacks were significant and this was achieved on both tasks, image classification and hashing-based image retrieval. These results hold important implications for the field of deep learning, suggesting that biologically inspired learning rules can address some of the limitations of backpropagation, particularly in terms of robustness, without a significant degradation in performance. The observed improvements across various architectures, including AlexNet, VGG-16, and ResNet-18, and datasets of increasing complexity, such as CIFAR-10, ImageNet-100, ImageNet-1000, MS-COCO, and NUS-WIDE, further support the broad applicability and efficacy of this approach. Future research could investigate the application of this hybrid learning approach to a wider range of neural network architectures and tasks. Moreover, understanding the underlying mechanisms that contribute to the increased robustness observed in Sign-Symmetry methods could lead to the development of new biologically inspired learning rules that further enhance both performance and robustness."}, {"title": "A. Additional Experimental Results", "content": "The following plots display the results of other backbones under adversarial attacks for both classification and hashing tasks, as referenced in the main text."}]}