{"title": "Are Language Models Up to Sequential Optimization Problems?\nFrom Evaluation to a Hegelian-Inspired Enhancement", "authors": ["Soheil Abbasloo"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning.", "sections": [{"title": "1. Introduction", "content": "Setting the Context: Optimization is fundamental to decision-making across diverse domains such as engineering, science, economics, healthcare, and even nature. The essence of decision-making lies in choosing the best option from a set of alternatives, driven by objectives such as efficient resource allocation, cost minimization, profit maximization, or performance enhancement of systems and infrastructures (Chong & Zak, 2013). However, solving optimization problems is often intricate, requiring specialized expertise and addressing practical challenges like high dimensionality, nonlinearity, and the dynamic, stochastic nature of real-world environments (Datta et al., 2019). Consequently, there is a continuous quest across various fields to simplify the optimization process.\nA New Opportunity: Simultaneously, Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023a), have shown impressive capabilities in both general assistance and specialized domains, including mathematics, coding, and law (Bubeck et al., 2023). These advancements present a compelling opportunity to revolutionize our approach to solving optimization problems by leveraging LLMs to automate solution processes. This emerging potential prompts a fundamental scientific inquiry: How proficient are current LLMs in handling optimization problems?\nAssessing LLMs in a New Context: Although various benchmarks exist to evaluate LLM performance in general and specialized tasks (e.g., coding, mathematics), their capabilities in solving optimization problems, particularly Sequential Optimization Problems (SOPs), remain under-explored. SOPs involve making a series of decisions over time, where each decision affects subsequent options and outcomes, creating a complex web of interdependencies. Different challenges contribute to this issue, primarily the need for defining a representative set of SOPs and ensuring that observed performance is not influenced by data contamination or prior exposure during the LLMs' training (Dong et al., 2024; Mialon et al., 2023; Chen, 2023).\nOn-Demand SOP Generation: Motivated by these opportunities and challenges, in the first half of this paper we aim to addresse this research question: How can we evaluate the performance of LLMs in SOPs? We introduce a straightforward yet effective framework, WorldGen, capable of generating unseen SOPs with controllable complexities on demand. WorldGen contrasts with most existing static benchmarks (e.g., MMLU (Hendrycks et al., 2021), GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), GSM8k (Cobbe et al., 2021), etc.) which become obsolete as LLMs evolve (Mialon et al., 2023). Utilizing this dynamic framework, we have made two key observations: (1) For relatively simple optimization tasks, with a single global maximum and no local maxima (i.e., simple surfaces and scenarios), current LLMs can solve them efficiently. (2) As the complexity of the optimization problems increases, the performance of LLMs degrades significantly and becomes unsatisfactory.\nImproving LLMs with Roots in Philosophy: Inspired by"}, {"title": "2. Background: Reasoning and Dialectics", "content": "Reasoning: Despite considerable achievements, LLMs' reasoning capability continues to be a subject of intense debate within the AI research community. A key challenge lies in reaching a consensus on what reasoning entails, how it should be defined, and how it can be reliably measured. Interestingly, the concept of reasoning is not new. The domain of philosophy has a rich tradition of exploring and formalizing reasoning through centuries of discourse (Aristotle, -350; -340; Plato, -380; Descartes, 1641; Hume, 1739; Kant, 1781; Mill, 1843; Hegel, 1807; Nietzsche, 1886; Wittgenstein, 1921; Heidegger, 1927; Popper, 1934; Kuhn, 1962; Adorno, 1966). From ancient philosophers such as Aristotle, who developed formal logic as a foundation for reasoning (Aristotle, -350; -340), to more recent thinkers like Hegel, who introduced dialectics as a dynamic framework for understanding processes of thought (Hegel, 1812; 1807), the philosophical study of reasoning has produced a wide range of influential theories and formal systems. These works not only define reasoning but also provide structured frameworks for improving and analyzing it.\nDialectics: As a method of reasoning and philosophical argumentation, dialectics involves the resolution of contradictions through a process of development and transformation. Rooted in ancient philosophy, dialectics was first formalized by thinkers like Socrates and Aristotle, who used it as a tool for logical inquiry. Over time, dialectics evolved into a broader philosophical framework, describing the dynamic process through which contradictions are identified, explored, and resolved (Hegel, 1807; Engels, 1875). At its core, dialectical thinking posits that reality is composed of opposing forces or contradictions, and that these contradictions are not static but dynamic, evolving over time. The resolution of these contradictions leads to the emergence of new, higher forms of understanding or being.\nHegelian Dialectics: Introduced by the German philosopher Georg Wilhelm Friedrich Hegel, Hegelian Dialectics crystallizes the modern notion of dialectics by proposing a structured process of development through three stages: thesis, antithesis, and synthesis (Hegel, 1812; 1807). The thesis represents an initial idea or condition, the antithesis introduces a contradictory or opposing force, and the synthesis resolves the tension by merging elements of both into a higher, more comprehensive understanding. Hegel viewed this triadic process as the driving force of intellectual, historical, and societal progress, emphasizing that contradictions, which he calls \"negations\", are not merely obstacles but necessary components of growth and transformation. His dialectical framework has had profound influence across disciplines, from philosophy to political theory.\nDialectics vs. Debate: From the philosophical point of view, debate is competitive, aiming to persuade an audience of one position's superiority. While effective in contexts like politics or law, it often sacrifices deeper inquiry for rhetoric and winning. Dialectics, however, fosters a cooperative approach, treating opposing perspectives as opportunities for growth. Through structured dialogue, dialectics seeks deeper truths, as seen in the Socratic and Hegelian methods, encouraging intellectual humility and a shared pursuit of wisdom. While debate has been significant in philosophical traditions, figures like Socrates criticized its focus on persuasion over truth. Dialectics, with its emphasis on dialogue and synthesis, is regarded as superior for fostering intellectual growth."}, {"title": "3. Related Work", "content": "Our works overlaps with three groups of related work: (1) Benchmarks for Evaluating LLMs, (2) Prompt Engineering, and (3) Multi-Agency. Here, we briefly overview them to provide a better context for the rest of this paper.\nBenchmarks for Evaluating LLMs: There are numerous benchmarks for evaluating LLMs, ranging from general-purpose (e.g., GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), BIG-bench (Srivastava et al., 2022), GAIA (Mialon et al., 2023)) to domain-specific (e.g., Fin-Ben (Xie et al., 2024) for finance, LegalBench (Guha et al., 2024) for legal reasoning, GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for mathematical reasoning, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for coding, MultiMedQA (Singhal et al., 2023) for healthcare, etc.) and ones requiring professional level knowledge in various fields such as law or science (e.g., MMLU (Hendrycks et al., 2021)). Our framework, World-Gen, falls into the domain-specific category. It addresses the issue of being static and become obsolete with the rapid advancements in LLMs by providing a dynamic tool for generating SOPs with varying controllable complexity.\nPrompt Engineering (PE): Since we treat LLMs as black-boxes and focus on improving the off-the-shelf LLMs without any retraining, it will be natural to mention works in PE domain here. Prompting techniques have become essential in enhancing the performance and versatility of LLMs. These techniques range from Zero-shot Prompting, where models are given tasks without prior examples, to Few-shot Prompting, which provides a few examples to guide responses (Brown et al., 2020). Chain-of-Thought encourages models to generate intermediate reasoning steps (Wei et al., 2022), Self-Consistency (Majority Vote) generates multiple outputs to select the most consistent one (Wang et al., 2022; Lewkowycz et al., 2022), and Generate Knowledge Prompting prompts the model to produce relevant background information before answering (Liu et al., 2021). Tree of Thoughts structures reasoning as a tree to explore different branches (Yao et al., 2024; Long, 2023). Retrieval Augmented Generation combines document retrieval with generation for improved accuracy (Lewis et al., 2020). Automatic Prompt Engineer uses algorithms to refine prompts (Zhou et al., 2022), while Active-Prompt (Diao et al., 2023) adjusts prompts based on performance feedback and Program-Aided Language incorporate programming logic (Gao et al., 2023). Techniques like ReAct combine reasoning and acting steps (Yao et al., 2022), and Self-Reflection prompts models to reflect on their responses for better outcomes (Madaan et al., 2024; Shinn et al., 2024). These diverse techniques collectively enhance the adaptability and effectiveness of LLMs. ACE is an orthogonal approach compared to these techniques and as we later show, it can be combined with them (detailed in section 4.5).\nMulti-Agency: Minsky was among the early pioneers to introduce the idea of multi-agent systems (Minsky, 1988). His notion of multi-agency involves dividing complex cognitive tasks into smaller parts, delegating them to specialized \"agents\", and integrating the results into a coherent solution. Inspired by Minsky's vision, recent works have utilized and implemented multi-agency in LLM-based systems. Some focus on building general infrastructures for autonomous cooperation among communicative agents (e.g., CAMEL (Li et al., 2023) and AutoGen (Wu et al., 2023)). Others focus on specific multi-agent solutions or tailored applications. For instance, MetaGPT (Hong et al., 2023) and ChatDev (Qian et al., 2023) automate software development by assigning distinct roles to different agents. Multi-agent debate frameworks (such as MAD (Liang et al., 2023), which employs a debate cycle among agents moderated by a judge agent, and Du et al. (Du et al., 2024), where agents exchange answers to get a chance to modify their next responses) present another direction.\nWhile these works follow Minsky's multi-agent view, our proposal, ACE takes a different path. ACE focuses on the reasoning process itself rather than focusing on how to delegate tasks to specialized agents or automate their communication. Great performance of ACE (as shown in section 5) suggests that the basic element of intelligence needs to include a Hegelian-inspired triad, not a single entity offering a complementary perspective to Minsky's multi-agent approach. Compared to debate-based proposals, from a philosophical qualitative perspective, as explained earlier in section 2, there are key fundamental differences between debate and dialectics, which ACE draws inspiration from. Additionally, from a quantitative standpoint, our experiments and comparisons in section 5 highlight ACE's superior performance over debate-based works in SOP context."}, {"title": "4. Assessing & Improving LLMs in SOPs", "content": "SOPs are pervasive across diverse domains, ranging from logistics and resource allocation to machine learning and operations research. Requiring specialized knowledge to address practical issues such as high dimensionality, nonlinearity, and the dynamic, unpredictable nature of real-world settings make SOPs complex in their nature (Datta et al., 2019). Automating the solution of such problems is highly desirable, as it can lead to efficiency gains and innovative solutions to complex challenges. On the other hand, LLMs have demonstrated remarkable capabilities, including proficiency in coding and exceptional context-awareness, making them promising candidates for tackling SOPs.\nSo, naturally, exploring the performance of LLMs in addressing these crucial tasks is a significant step forward in understanding their broader applicability and the opportunity to solve these problems automatically. However, evaluating LLMs in this context comes with it own set of challenges. Key concerns include managing data contamination, ensuring that the problems and their solutions were not inadvertently exposed during training, and consequently distinguishing between genuine reasoning and mere memorization. Additionally, access to a set of representative optimization problems is vital to effectively assess LLMs' capabilities in solving sequential tasks. Addressing these challenges will enable a deeper understanding of the role LLMs can play in advancing optimization methodologies.\n4.1. WorldGen\nCore Idea: At the heart of optimization lies the task of finding the optimum point(s) in an n-dimensional world, mathematically expressed as $f(x_1,x_2,...,x_{n-1})$. So, instead of focusing on specific optimization problems, we shift our attention to the worlds that represent these problems. In other words, rather than trying to come up with specific optimization problems, which may inadvertently introduce biases or contamination from training data, we focus on directly generating n-dimensional worlds that can represent the solution spaces for a wide range of SOPs. That said, we do not predefine the optimization problem. Instead, the task naturally emerges as finding the maximum (or other extrema) in the generated n-dimensional world. This setup allows us to define flexible problems while preserving the integrity of the test environment.\nBenefits and Advantages: This approach ensures that neither the optimization problem nor its solution was exposed to the LLM during training. By doing so, we can mimic a real-world scenario where an optimization expert is asked to tackle a newly faced optimization problem using any techniques or strategies they prefer. Utilizing this approach brings some advantages. It offers generative flexibility, allowing the n-dimensional world to represent an infinite variety of optimization problems, from simple to highly complex ones. By abstracting the problem into a generated world, it ensures unbiased evaluation, reducing contamination from known problem-solution pairs and providing a more acceptable measure of the LLM's capability. Our world generator, WorldGen, enables the creation of increasingly complex worlds that test the limits of learning agents and provides a platform for benchmarking them under controlled yet dynamic conditions. \n4.2. LLM and Accessing the World\nAn Interactive Cycle: To enable the LLM agent to perform its task effectively, we provide it with access to the generated world through an interactive cycle as shown in Figure 2. This ensures a dynamic sequential process where the agent iteratively learns and refines its approach based on the information it gathers. Simply put, in each iteration, the LLM agent is allowed to interact with the world by selecting a batch of interested points where each point is a vector, $v_i$ of size $n-1$. Then, the world responds by revealing the corresponding values of $f(v_i)$ to the LLM agent. This will end one iteration/round of the interaction. In the next round, the LLM utilizes this feedback to determine the next set of points to query.\nSupporting Coding & Providing Flexibility: Due to the complexity of SOPs, the LLM agent is permitted to provide a Python code as part of its response and therefore utilize any library it deems necessary for solving the problem. This freedom ensures that the agent can employ a diverse range of tools and techniques to explore and analyze the generated world. As part of its role, the World is responsible for executing the Python code generated by the LLM agent. Once the code is executed, the World provides the results back to the agent as part of its feedback, enabling it to adapt and refine its strategy in subsequent iterations. In case of errors that may arise during execution, the World is responsible to provide the details of the errors and return meaningful feedback. Moreover, the LLM agent is not constrained by a fixed set of queries or techniques. Instead, it has the freedom to decide how to approach the problem, including leveraging mathematical models, heuristics, or machine learning techniques. This flexibility mimics the"}, {"title": "4.3. Notion of Efficiency", "content": "To Solve or to Efficiently Solve: Exhaustively searching through all possible solutions, or brute force, is always a solution to any optimization problem; however, it represents the most inefficient approach. Thus, merely solving an optimization problem is not the primary goal; solving it efficiently is what truly matters. To formalize this, we require the notion of an efficient solution. But how can we define efficiency in a meaningful and practical way here?\nThe Expert Solution: To address this question, we designed a baseline referred to as the Expert Solution. This baseline serves as a reference point for assessing the efficiency of the LLM agent's performance. The Expert Solution is crafted using different optimization techniques, including a combination of Monte Carlo search methods, Bayesian optimization, and Active Learning strategies. An important aspect of the Expert Solution is the introduction of a query budget. This budget represents the number of queries required by the Expert Solution to reliably solve the optimization problem. It provides an upper bound on the number of interactions with the environment that are necessary to achieve a solution. That said, alongside the optimization problem, the LLM agent is provided with the query budget and instructed to not only solve the optimization problem but also do so within the given query budget.\nIncentivizing Efficiency: This setup ensures that the agent is incentivized to prioritize efficiency. It must strategize its queries, balancing exploration and exploitation to maximize the information gained from each interaction. By enforcing a query budget, we can objectively evaluate LLM's efficiency and effectiveness in solving the problem. Ultimately, the notion of efficient solutions pushes the LLM agent beyond simple problem-solving, encouraging it to adopt creative and resource-conscious strategies that align with real-world optimization challenges."}, {"title": "4.4. Evaluating LLMs Performance in SOPs", "content": "The Setup: To evaluate the performance of the LLM agent, we follow a structured approach based on repeated experiments. We begin by generating worlds, characterized by a complexity index. In particular, to simplify experiments, visualizations, and keep overall token usage manageable, we focus on 3-D worlds and three levels of complexity: very simple (L0), simple (L1), and medium (L2). Next, we apply the Expert Solution to solve the corresponding SOPs associated with the generated world. As a result, we find a query budget required to achieve this reliably. Then, LLM agent is asked to solve the problem constrained by the query budget. We repeat this process 100 times and measure the success rate of the LLM agent-defined as the proportion of trials where the agent successfully finds the optimal solution within the given query budget\u00b9. This success rate provides a quantitative measure of the agent's effectiveness and efficiency. In these experiments, we utilize the GPT-4-32k model as a capable baseline model.\nThe Default Scheme: Without involving any prompting techniques, the success rates become very low (close to 0%). Therefore, we borrowed ideas from few-shot learning (Brown et al., 2020), Chain of Thought (CoT) (Wei et al., 2022), and added other techniques such as proper role assignment (Karpathy, 2023) to improve the performance of the LLM agent. We name the resulting scheme LLM+ and treat it as our default scheme from now on. In particular, the prompt given to the LLM agent includes: [Role Assignment], followed by [Problem Definition & Examples], [General Helpful Notes], and [Required Response Format]. The [Required Response Format] itself consists of [Plain Description of Current Strategy], [Python Code Implementation of the Strategy], and [Maximum Value Found So Far] fields (check Appendix A for more details)."}, {"title": "4.5. A Dialectical Perspective to Enhance LLMs in SOPs", "content": "Motivated by the unsatisfactory performance of vanilla LLMs in non-straightforward SOPs, we aim to address a natural follow-up question: Can we enhance the performance of LLMs without relying on retraining, fine-tuning, or post-training modifications?\nThe Core Idea: To that end, we propose a framework inspired by the well-established Hegelian Dialectics, offering a formal structure for improving LLM performance through dynamic dialectical reasoning processes. Using the terminology of Hegelian Dialectics, we can conceptualize a general LLM agent as a Thesis Generator entity-a block that observes a given problem and generates a corresponding solution or response to it\u00b2. However, inspired by Hegel's framework, we suggest that confining to a single Thesis Generator block is not an appropriate model for solving problems or generating ideas. Instead, a more robust model should include additional components: an Antithesis Generator and a Synthesis Block. Together, these three components, as Hegel formally describes (Hegel, 1807; 1812), form a dynamic reasoning cycle, enabling the system to iteratively refine and improve its outputs.\nACE: The Antithesis Generator plays a critical role by challenging the solutions produced by the Thesis Generator. It identifies potential flaws, contradictions, or alternative perspectives that may have been overlooked. This counterbalance forces the system to evaluate its assumptions critically and consider a broader range of possibilities. The Synthesis Block then reconciles the Thesis and Antithesis, combining their insights to produce a more refined and coherent solution. This iterative interplay between Thesis, Antithesis, and Synthesis ensures that the system continuously evolves its understanding and response, ultimately arriving at a more suitable outcome. This dialectical structure led us to introduce our solution, ACE\u00b3, embodying three components: (1) Actor, (2) Critic, and (3) Synthesizer. The relationship between these components is shown in Fig. 3. The iterative cycle of solving a problem starts with the Actor creating an initial thesis. This thesis is then implemented and executed"}, {"title": "5. Evaluation", "content": "5.1. Overall Results\nSettings: To put the performance improvements of ACE in proper context, we implemented several recent related proposals including Self-Reflection (Madaan et al., 2024), Majority Vote (Wang et al., 2022; Lewkowycz et al., 2022), and Debate (Du et al., 2024) and compared them with ACE. To have a fair comparison, for the Majority and Debate schemes we set the total number of agents to three and two agents, respectively, to roughly match the token usage of ACE. Later, in section 5.2, we perform more evaluations with higher number of agents for them. We accompany all these schemes with the additional prompting techniques appeared in LLM+ to have a fair comparison (check Appendix A for more details). As for the LLMs, we use 3 different models: GPT-4-32K (OpenAI, 2023a), Llama-3-70B-Instruct (Dubey et al., 2024), and GPT-3.5-Turbo (OpenAI, 2023b). As before, we repeat evaluations 100 times and report the success rate of different schemes."}, {"title": "5.2. Deep Dive", "content": "Cost Comparison: We evaluate the cost of using ACE by analyzing the total number of tokens consumed and comparing it with other approaches. Table 3 provides the average total token usage for one run of the experiment for various schemes, and their corresponding normalized values (to the default single-agent scheme, LLM+) using GPT-3.5-Turbo as the base model. On average, ACE consumes 2.27\u00d7 the total tokens compared with LLM+; however, it remains more efficient compared to multi-agent schemes like Debate and Majority, which require higher token usage.\nACE vs. Multi-Agent Schemes with More Agents: A natural assumption might be that increasing the number of agents in schemes like Debate and Majority would enhance performance. So, we conducted additional experiments, scaling the number of agents to seven (referred to as Debate* and Majority*) and comparing their performance and token costs with ACE. These experiments were carried out using GPT-4-32K and GPT-3.5-Turbo as base models in the L1 3-D world sample. The results are summarized in Table 4. As shown in Table 4, increasing the number of agents in these schemes results in higher costs without corresponding performance improvements. In fact, the performance of Debate* and Majority degrades when compared to their counterparts with fewer agents. Schemes like Debate*, which require exchanging responses among all agents in each round, can experience exponential growth in token consumption as the number of agents increases. This inefficiency is particularly evident in SOP settings, where sequential decision-making processes require multiple rounds of interaction, as illustrated by the token cost data in Table 4. The key takeaway is that simply increasing the number of agents in tasks involving sequential decision-making, such as those in SOP settings, does not necessarily yield better results. Instead, it often introduces inefficiencies and performance degradation in solutions like Debate* and Majority*.\nACE in Static Settings: Our primary domain and targeted setting (SOPs) possess an important characteristic: the ability of the World to provide feedback during task execution."}, {"title": "6. Limitations & a Brief Discussion", "content": "WorldGen's Limitation: WorldGen effectively generates worlds with adjustable complexity for testing LLMs in SOPs, but relies on manually designed Expert solutions to solve the SOP. This dependence on human expertise for robust baselines can be time-intensive and limit the automation potential of the approach. We leave addressing the fully automated objective to future work.,\nLimitations of ACE: ACE's dialectical framework has demonstrated great performance in our main targeted domain, SOPs, but its effectiveness in other domains, particularly those lacking real-time feedback, remains an open question. In static question-answering or static tasks without iterative refinement, the benefits of ACE may be limited, warranting further evaluation. Additionally, by treating LLMs as black boxes, ACE's performance is inherently bound by the capabilities of the underlying model. Moreover, while its token consumption is lower than multi-agent schemes like Debate or Majority, ACE incurs a slight overhead compared to single-agent approaches. This trade-off is minor in complex SOP tasks but could pose challenges in resource-constrained scenarios.\nOn the Potential of ACE: The potential of ACE, rooted in its Hegelian dialectical framework, extend beyond solving SOPs. Its dialectical approach, mirroring human-like problem-solving processes, fosters solutions that are not only accurate but also deeply contextual and well-reasoned. Furthermore, Hegelian philosophy provides a foundation to explain the effectiveness of other prompt engineering techniques, such as self-reflection, by framing them within a structured dialectical process. This perspective can deepen our understanding of existing methods and their mechanisms. Additionally, the Hegelian-inspired framework offers a powerful structure for generating synthetic data. Its iterative nature facilitates the creation of diverse, high-quality datasets that reflect a broad range of perspectives and solutions, making them invaluable for training and fine-tuning LLMs to tackle complex and nuanced tasks effectively.\nLLM+ Could Have Been Better! A fair criticism might be that LLMs might perform better in solving SOPs with improved prompt engineering. We are not claiming that LLM+ represents the optimal default scheme; rather, we argue that it serves as a robust baseline. Even with carefully designed prompts, LLMs' performance in this setting remains limited, highlighting the need for approaches like ACE to unlock their full potential and deliver superior performance.\nWhat If the Next LLM Becomes Very Capable? A more capable LLM makes ACE even more useful, not less. As demonstrated in Section 5, a better base model serves as a stronger foundation, enabling even greater performance improvements. In essence, ACE with its dialectical base is designed to complement and amplify the capabilities of any LLM, regardless of its initial proficiency in SOP context. By leveraging ACE, we can transform an already impressive LLM into an extraordinary one, pushing what is possible and unlocking new levels of performance in this domain."}, {"title": "7. Final Note", "content": "Our exploration into the capabilities of LLMs in tackling sequential optimization problems has revealed both their potential and their current limitations. Through the development and use of WorldGen, we have shown that while LLMs exhibit impressive abilities, they still face challenges with even relatively simple SOPs. These findings have led us to propose a novel approach inspired by philosophical reasoning frameworks, aiming to enhance LLM performance in innovative yet easy to reason about ways. We believe that this work can open new avenues, encouraging the integration of philosophical reasoning frameworks into AI systems. By fostering a deeper understanding and application of these frameworks, we can pave the way for more robust and intelligent systems. We hope our efforts inspire others to explore these interdisciplinary approaches, ultimately contributing to the advancement of LLMs and its applications across diverse fields."}, {"title": "A. More on the Evaluations and the Prompt Templates Used", "content": "Prompt Templates: Figure 4 shows the main template used for the LLM+ scheme. We use the same template for the main agent of other schemes compared in this paper, including ACE's Actor. Additionally, Figure 5 and 6 demonstrate the initial and transitional prompts used for ACE's Critic, respectively. The task of the Synthesizer, the Actor of the previous and next steps, will be identified through a transitional prompt, as shown in Figure 7.\nMajority Scheme: To implement the Majority scheme and automate the solution, we use another agent called the poll worker. The poll worker checks different agents' responses and identifies the Majority response, which is the one with the highest consensus. Figure 8 shows the prompt template used for the poll worker. Unlike taking the majority vote after every agent completes the task in general scenarios, in our sequential decision-making problems, we need to take the majority vote in every round. Therefore, the poll worker processes the agents' responses at each round of interaction with the World, identifies the response with the majority consensus at each round so that the World can execute it and provide the feedback."}, {"title": "B. A Couple of Samples for LLM+ in Action", "content": "Figures 9 and 10 demonstrate two samples of strategies used by LLM+ in separate runs, utilizing the GPT-4-32K base model in an L1 world. In the first run (Figure 9), the agent begins with a broad grid search strategy, aiming to cover the entire search space and identify regions with high values. This initial phase can form a foundation for subsequent searches. However, the agent does not adapt its strategy in the following rounds, remaining static and failing to find the optimum point by the end of the 16 rounds.\nIn the second example (Figure 10), the agent attempts to adapt its strategy based on the feedback it receives from the World. Initially, the agent starts with a coarse exploration to identify promising regions. It then refines the search around one promising area identified in the initial phase and continues with further refinements. However, it still cannot find the optimum point. This time, the issue lies in the agent spending a significant number of queries exploring around the local maximum found. The search strategy was not sufficiently adaptive to balance between local exploitation and global exploration. Nevertheless, the fact that it found a local maximum is noteworthy, as it demonstrates some degree of understanding about the notion of a maximum point in a region, the direction of increase or decrease of a sequence of observations, and their relation to the actual curve modeling the unknown World function, f."}, {"title": "C. A Sample of the Dialectical Process in ACE", "content": "Figures 11 and 12 demonstrate a sample of a dialectical progress with ACE, utilizing the GPT-4-32K base model in an L1 world. The iterative process of ACE's dialectical method, which involves a cycle of thesis, antithesis, and synthesis allows for systematic refinement and improvement of strategies based on feedback and critique. One of the key advantages of ACE is its ability to adapt and improve through structured feedback. For instance, the initial grid search strategy (Thesis 1 in Figure 11) provided a broad understanding of the search space. However, the corresponding antithesis highlighted the need for refinement in promising areas, the incorporation of adaptive techniques, and a balance between exploration and exploitation. This critical feedback led to a more refined and effective strategy (Synthesis 1 in Figure 12), which combined grid search with adaptive methods like simulated annealing.\nThe dialectical method also ensured that the agent did not become overly focused on a single approach. By evaluating and adjusting strategies through a dialectal cycle, ACE attempted to balance the thorough exploration of high-value areas with the need to investigate less explored regions. This is evident in the transition from Thesis 2 to Synthesis 2 (Figure 12)), where the agent broadened its exploration based on feedback, expanding the use of simulated annealing to uncover potential peaks outside the heavily focused regions.\nMoreover, the dialectical approach fostered a dynamic and flexible search process. The agent's ability to incorporate feedback and adjust its methods in real-time allowed for efficient use of queries and increased the chances of identifying the global maximum. This adaptability is crucial in complex search spaces where the landscape can vary significantly.\nIn short, the Hegelian Dialectics aspect of ACE offered a mechanism for continuous improvement. By leveraging structured feedback and iterative refinement, ACE enhanced the agent's ability to navigate search spaces effectively. The figures illustrate this process, showcasing how each cycle of thesis, antithesis, and synthesis leads to progressively better strategies and outcomes."}]}