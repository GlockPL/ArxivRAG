{"title": "Bridging the User-side Knowledge Gap in Knowledge-aware Recommendations with Large Language Models", "authors": ["Zheng Hu", "Zhe Li", "Ziyun Jiao", "Satoshi Nakagawa", "Jiawen Deng", "Shimin Cai", "Tao Zhou", "Fuji Ren"], "abstract": "In recent years, knowledge graphs have been integrated into recommender systems as item-side auxiliary information, enhancing recommendation accuracy. However, constructing and integrating structural user-side knowledge remains a significant challenge due to the improper granularity and inherent scarcity of user-side features. Recent advancements in Large Language Models (LLMs) offer the potential to bridge this gap by leveraging their human behavior understanding and extensive real-world knowledge. Nevertheless, integrating LLM-generated information into recommender systems presents challenges, including the risk of noisy information and the need for additional knowledge transfer. In this paper, we propose an LLM-based user-side knowledge inference method alongside a carefully designed recommendation framework to address these challenges. Our approach employs LLMs to infer user interests based on historical behaviors, integrating this user-side information with item-side and collaborative data to construct a hybrid structure: the Collaborative Interest Knowledge Graph (CIKG). Furthermore, we propose a CIKG-based recommendation framework that includes a user interest reconstruction module and a cross-domain contrastive learning module to mitigate potential noise and facilitate knowledge transfer. We conduct extensive experiments on three real-world datasets to validate the effectiveness of our method. Our approach achieves state-of-the-art performance compared to competitive baselines, particularly for users with sparse interactions.", "sections": [{"title": "Introduction", "content": "In the era of information explosion, recommender systems play a crucial role in helping individuals obtain relevant information (Gao et al. 2023a; Cheng et al. 2024). However, these systems have long struggled with data sparsity and the cold start problem (Zhou et al. 2007). Recently, knowledge graphs (KGs) have been introduced as auxiliary information on the item side, enhancing recommendation algorithms' performance and alleviating the item cold start issue (Cao et al. 2023). Technically, KGs are integrated into the user-item collaborative graph (CG), as illustrated in Fig. 1(a), to form a hybrid structure known as the collaborative knowledge graph (CKG) (Wang et al. 2019b), depicted in Fig. 1(b). In recent years, significant progress has been made using graph neural networks (GNNs) with the CKG for recommendations (Wang et al. 2019c; Yang et al. 2022, 2023a). However, KGs provide auxiliary information solely on the item side (Wang et al. 2021), and the construction and integration of auxiliary information on the user side into recommendation algorithms remain unexplored.\nUnlike item-side features, which can be naturally abstracted as structured knowledge (e.g., (movie A, has genre, comedy)), user-side features present significant challenges in structuring and integrating into graph-based recommendation algorithms. These challenges arise from issues related to the improper granularity and scarcity of existing user-side features. Specifically, user meta-features, such as gender, age, and nationality, are too coarse to provide precise knowledge, often leading to over-smoothing problems in GNNs. While user interests are crucial for recommendations, they are typically abstract and lack explicit user feedback (Wu et al. 2023), making them difficult to capture and utilize.\nFortunately, the rapid development of large language models (LLMs) has shown impressive capabilities in understanding and simulating human behavior (Tan and Jiang 2023; Hu et al. 2023b; Gao et al. 2024). Leveraging LLMs to interpret users' historical behavior can abstract specific and meaningful user interest knowledge, thereby advancing knowledge-based recommendation algorithms. However, existing LLM-based works primarily focus on integrating recommendation tasks with dialogue systems (Geng et al. 2022; Hou et al. 2024) or using natural language features generated by LLMs as enhanced semantic information (Zhao et al. 2024; Wei et al. 2024). How to prompt LLMs to generate and construct user-side structural knowledge and combine it with knowledge-based recommendation algorithms remains an open problem.\nFurther incorporating LLM-generated information into recommendation algorithms introduces two significant challenges: (1) How to effectively utilize LLM-generated user-side knowledge while minimizing noise interference. The LLM-generated content may contain potential noise due to the hallucination problem (Zhang et al. 2023). (2) How to effectively transfer information from the auxiliary knowledge domain to the recommendation domain. Addressing these"}, {"title": "Related Work", "content": "LLM for Recommender Systems. In recent years, the rapid development of LLMs has garnered increasing attention for their application in recommendation tasks. The role of LLMs in these tasks can be categorized into two primary functions (Fan et al. 2023): LLMs as recommenders and LLMs as enhancers. When used as recommenders, LLMs integrate recommendation tasks into conversational scenarios, tokenizing item IDs and user IDs. Due to their extensive real-world knowledge, LLMs as recommender exhibit advantages in few-shot scenarios (Li et al. 2024), fairness (Jiang et al. 2024) and explainability (Gao et al. 2023b; Xie et al. 2023). However, they struggle to capture the pervasive collaborative signals, resulting in limited advantages over traditional collaborative filtering algorithms (Kang et al. 2023). Some studies have explored enhancing traditional recommendation algorithms by incorporating features generated by LLMs. These models generate missing item features (Ren et al. 2024), user-item interactions (Wei et al. 2024), etc., and encode LLM-generated context from natural language into semantic vectors for enhancing traditional algorithms. Nevertheless, current methods of using LLMs for data enhancement primarily focus on the meta-features, neglecting structural knowledge from the user side. This limitation hinders their ability to facilitate traditional methods to capture higher-order collaborative information.\nKG-based Recommendation Methods. Another closely related area of this work involves KG-based recommendation methods. KB4Rec (Zhao et al. 2019) links knowledge base information with items in the recommendation domain, constructing an item-side KG to facilitate the fusion of KG and the recommendation task. Existing KG-based methods can be broadly categorized into two groups: path-based methods and regularization-based methods. Path-based approaches extract higher-order information paths and input them into recommendation models (Wang et al. 2019c; Hu et al. 2018). Regularization-based techniques introduce additional loss terms to capture the KG's structure, regulating the learning process of recommender models (Zhang et al. 2016; Wang et al. 2019b,a, 2021; Hu et al. 2023a). Recent efforts have focused on enhancing model robustness (Yang et al. 2022) and reconstructing knowledge graphs (Yang et al. 2023a; Tang et al. 2024) with information from the recommendation domain."}, {"title": "Problem Definition", "content": "In this section, we first introduce the concept of the CG and the KG. Then, we formally define the top-K recommendation task.\nDefinition of CG. In a recommendation scenario, historical user-item interactions (e.g., watches and clicks), denoted as $E^+$, are represented as a user-item collaborative graph $G_R = (V_R, E^+)$, where $V_R = (U \\cup I)$, with $U$ representing the set of users and $I$ representing the set of items.\nDefinition of KG. In addition to interactions, item-side information is organized as a knowledge graph $G_k$, a directed graph composed of subject-property-object triples (Shi and Weninger 2017). Formally, $G_k = \\{(h, r, t)|h, t \\in V_E, r \\in R\\}$, where $V_E$ denotes the set of entities and $R$ denotes the set of relations. An item-entity projection set $P = \\{(i, e)|i \\in I, e \\in V_E\\}$ links items to entities.\nTop-K Recommendation. Given $G_R$, $G_K$ and $E^+$, the goal is to recommend the top-K items that each user $u \\in U$ is most likely to interact with, among the items u has not yet interacted with."}, {"title": "Method", "content": "The architecture of our approach is depicted in Fig. 2. Overall, our method comprises two stages: LLM-based CIKG construction (left half of Fig. 2) and CIKG-based recommendation (right half of Fig. 2). The details of these stages are as follows.\nLLM-based CIKG Construction\nThis section introduces the details of LLM-based user-side knowledge generation and how to convert such knowledge from natural language into structured user-side information. Furthermore, we introduce the concept of the CIKG.\nLLM-based User Interest Inference. Modeling and representing user interests is crucial for recommender systems (Gao et al. 2015; Yang et al. 2023b). Traditional recommendation algorithms represent user interests implicitly as dense vectors (Wu et al. 2023). The abstract and complex nature of user interests makes explicitly representing them challenging. This limitation hinders the recommendation model's ability to fully leverage user interest information. Recently, LLMs have shown good ability to understand user behavior (Tan and Jiang 2023; Hu et al. 2023b; Gao et al. 2024), which makes it possible to model user interests explicitly. Inspired by this, we devise prompts derived from the dataset's user behaviors to enable the LLM to infer and generate user interests not originally present in the dataset. Leveraging the LLM's extensive knowledge and reasoning ability, the model can summarize high-level, abstract user interests based on the relevance of items interacted with by users rather than relying on traditional user-side meta-features. Specific examples are shown in Fig. 2. Formally, the LLM-based user interest inference is defined as follows:\n$A = \\{A_1, A_2, ..., A_u\\}, A_u = LLM(P_u, E^+)$, (1)\nwhere $P_u$ represents the textual prompt for user u, and $A$ represents the generated user interest textual information.\nUser Interest Knowledge Structuration. The purpose of structuring user interest knowledge is twofold: (1) Ensuring vector space consistency. Previous works (Ren et al. 2024; Wei et al. 2024) have encoded LLM-generated text as semantic embeddings and incorporated them into traditional recommendation methods. However, direct addition poses the problem of misalignment (Hao et al. 2019; Radford et al. 2021) between semantic signals and collaborative signals, resulting in suboptimal performance. (2) Capturing user-side multi-hop information. Explicitly modeling user interests as"}, {"title": "CIKG-based Recommendation", "content": "structured information facilitates the model mining higher-order relationships between users with common interests. Technically, we first employ text clustering methods, which can be as simple as tf-idf or deep sentence embedding methods, to merge similar interests. In this way, we semantically avoid inconsistencies where the same interests are expressed differently due to the randomness of LLM outputs. We then connect the merged interests with users to build a user interest knowledge graph $G_J$:\n$I = \\{i_1, i_2, \u2026\u2026i_k\\} = f_c(A, k)$, \n$G_J = (V_J, E_J), V_j = (U \\cup I)$, (2)\nwhere $f_c()$ denotes the textual clustering method, and $k$ is a hyperparameter representing the number of clusters. An edge $(u, j) \\in E_J$ exists if user u has the interest j.\nThe CIKG is defined as a unified relational graph $G$, encoding user behaviors, user interest knowledge, and item-side knowledge. Each user behavior in $G_R$ is represented as a triplet $(u, Interact, i)$, where Interact denotes the relationship between user u and item i. User interest information in $G_J$ is represented as triplets $(u, Has Interact, j)$. By integrating the item-entity projection set $P$, the user behaviors graph $G_R$, and user-side information $G_J$ with the knowledge graph $G_K$, we form the unified graph $G = \\{(h, r, t)|h, t \\in V = (U \\cup V_E \\cup J), r \\in R'\\}$, where $R' = R \\cup \\{Interact, Has Interest\\}$, as the CIKG. Let $Z \\in R^{\\mid V \\mid \\times D}$ denotes the initial embedding matrix of $G$, where D is the embedding dimension. $z_i \\in R^{D \\times 1}$ represents the embedding of node i.\nIn this section, we propose a CIKG-based recommendation method. This method consists of three components: user interest reconstruction (addressing the challenge of LLM-generated noisy content), cross-domain contrastive learning (addressing the challenge of knowledge transfer), and model training. The details are as follows.\nUser Interest Reconstruction. Due to the hallucination problem (Zhang et al. 2023) in LLMs and the complexity of user interests, the user interests inferred by LLMs can be inaccurate and noisy. The objective of this module is to reduce the noise interference. Inspired by the robustness of graph representations achieved through GMAEs, we apply masking and reconstruction on user interest nodes, which facilitates the model's focus on crucial information and enhances its generalization ability. Unlike previous GMAE methods that use a fixed mask rate (Hou et al. 2022) or incorporate randomness into the mask rate determination (Wang et al. 2024), we propose two mask rate acceleration scheduling strategies that dynamically control the mask rate. Drawing inspiration from curriculum learning (Wang, Chen, and Zhu 2022), these strategies progressively transition task difficulty from easy to hard. Formally, the two strategies can be expressed as:\n$p_q = \\psi(\\delta(q), \\omega), \\delta(q) = \\begin{cases} \\alpha + q \\frac{(\\omega - \\alpha)}{\\Lambda}, & \\delta_{lin}(q) \\\\\\ \\alpha (\\frac{\\omega}{\\alpha})^{\\frac{q}{\\Lambda}}, & \\delta_{exp}(q) \\end{cases}$, (3)\nwhere $\\delta(.)$ denotes the scheduling function. $\\alpha, \\omega$, and $\\Lambda$ are hyperparameters representing the initial and maximum mask rates, and the required epochs to convergence to the maximum mask rate. q denotes the current epoch number. $p_q$ denotes the mask rate at epoch q. $\\psi(\\cdot)$ is the minimize function. It's easy to prove that:\n$\\delta_{exp}(q) < \\delta_{lin} (q), 0 < q < \\Lambda$. (4)\nTherefore, our proposed exponential growth strategy allows the task difficulty to grow more slowly, as shown in the user interest reconstruction module of Fig. 2. After determining the mask rate $p_q$, we sample a subset of nodes $\\tilde{I} \\in I$ and mask their embeddings with a learnable mask token [M]. For each interest node j, we have:\n$\\tilde{z_j} = \\begin{cases} z_{[M]} & \\text{if } j \\in \\tilde{I}, \\\\ z_j & \\text{if } j \\notin \\tilde{I}. \\end{cases}$ (5)\nThe masked embeddings $Z'$ and $G$ are then sent to the encoder $f^1(\\cdot)$, which is a GNN with (l \u2013 1) layers. Without loss of generality, we use LightGCN (He et al. 2020) as the graph encoder for all modules. The decoder $f^d(\\cdot)$ is then applied to obtain the reconstructed node embeddings. This process can be formulated as:\n$Z'' = f^1(Z', G), Z' = f^d(\\chi(Z''), G)$, (6)\nwhere $\\chi()$ denotes the linear projection layer. Subsequently, we define the user interest reconstruction loss by comparing $Z$ and $Z''$. The loss function, with the scaling factor $\\eta$, is described as follows:\n$L_u = \\frac{1}{\\mid J \\mid} \\sum_{j \\in J} (1 - \\frac{z_j \\cdot z'''_j}{\\mid \\mid z_j \\mid \\mid \\cdot \\mid \\mid z'''_j \\mid \\mid})^\\eta, \\eta \\geq 1$, (7)\nwhere $\\mid \\mid \\cdot \\mid \\mid$ represents the L2 normalization function.\nCross-domain Contrastive Learning. To facilitate the transfer of user-side and item-side knowledge to the recommendation domain, we devise a cross-domain contrastive learning module. Unlike most existing contrastive learning-based models (You et al. 2020; Wu et al. 2021) that augment graphs by perturbing the graph structure and potentially losing useful information, we construct augmented views by adding auxiliary information from the user or item side to $G_R$. Our proposed augmentation approach aligns auxiliary information with the collaborative signals of the recommendation domain, all while preserving the integrity of the graph structure. The encoded embeddings of the augmented views are as follows:\n$Z^{U1} = f(Z, G_R, \\gamma)$,\n$Z^{U2} = f(Z, G_{v2}), G_{v2} = Merge(G_R, G_J)$,\n$Z^{U3} = f(Z, G_{v3}), G_{v3} = Merge(G_R, M(G_K, P))$, (8)\nwhere $\\gamma$ is representation-level perturbations as depicted in SimGCL (Yu et al. 2022), which provides augmentation to $G_R$ without perturb graph structures. $Merge(\\cdot)$ is a function to merge the edges and nodes of graphs and eliminate co-reference nodes or edges. $M(\\cdot)$ is the map function to transfer entity id to item id based on P."}, {"title": "Experiments", "content": "Experimental Settings\nDatasets. We conduct extensive experiments on three real-world datasets: DBbook2014 (Cao et al. 2019), Book-Crossing (Dong et al. 2017), and MovieLens-1M (Noia et al. 2016), which all contains item-side auxiliary knowledge and vary in terms of domain, size, and density. We follow the previous literature (Cao et al. 2019; Hu et al. 2024; Li et al. 2023) filtering out low-frequency users (i.e., lower than 10 in movielens and 5 in DBbook2014 and Book-Crossing). The statistics of the processed datasets are shown in Tab. 2.\nBaselines. We compare the performance of CIKGRec with various baseline methods, which can be broadly categorized into three groups: (1) GNN-based general recommenders, including LightGCN (He et al. 2020) and two contrastive learning-based methods, SGL (Wu et al. 2021) and SimGCL (Yu et al. 2022). (2) Embedding-based knowledge-aware recommenders, including CKE (Zhang et al. 2016) and CFKG (Ai et al. 2018). (3) GNN-based item-side"}, {"title": "Overall Performance", "content": "The positive samples of the user node in $G_{v1}$ are the user node in $G_{v2}$, and the positive examples of the item node in $G_{v1}$ are the item nodes in $G_{v3}$, since $G_{v2}$ and $G_{v3}$ add the knowledge of the user- and item-side, respectively. The contrastive objective $L_c$, which formally maximizes agreement among positive pairs and minimizes agreement among negative pairs, is defined based on the InfoNCE loss (Chen et al. 2020) as follows:\n$L_c = - \\log \\sum_{u \\in U} \\frac{\\exp(s(z_{u1}, z_{u2}) / \\tau)}{\\sum_{w \\in U} \\exp(s(z_{u1}, z_{w2}) / \\tau) + \\sum_{w \\in I} \\exp(s(z_{u1}, z_{w3}) / \\tau)}$,\nwhere s(.) is the cosine function to estimate the similarity of positive and negative pairs. $\\tau$ is the temperature coefficient to control the softness of the probability distribution.\nModel Training. In addition to the two loss functions introduced above, the model training process also includes the recommendation task loss function $L_r$, which can be formulated as:\n$\\hat{z} = f(Z, G)$,\n$L_r = \\sum_{(u, i, i')} - \\ln(\\sigma( -\\hat{z_u} \\hat{z_i} + \\hat{z_u} \\hat{z_{i'}}) )$, (10)\nwhere (u, i) \u2208 $E^+$ and i' is a randomly sampled negative item of user u. \u03c3 is the sigmoid function. The main loss is the weighted sum of the three loss functions, which can be formulated as:\n$L = \\lambda_1 L_r + \\lambda_2 L_u + \\lambda_3 L_c$, (11)\nwhere \u039b is the hyperparameter to control the measures of loss functions. In order to further enhance the multi-relational semantic representation space for entity-item dependencies, we perform the alternative translation-based training (Bordes et al. 2013), which can be formulated as:\n$L_t = \\sum_{(h, r, t, t')} - \\ln(\\sigma(\\mid \\mid z_h + z_r - z_{t'} \\mid \\mid - \\mid \\mid z_h + z_r - z_t \\mid \\mid ))$, (12)\nwhere (h, r, t) \u2208 $G_k$ and the negative tail t' of head h is randomly sampled from $G_K$.\nThe overall performance of CIKGRec and baseline models is shown in Tab. 1. Based on the results, we have the following key observations: (1) Our model achieves superior performance across all metrics and datasets, demonstrating the effectiveness of our model. This also underscores the efficacy of the proposed user interest reconstruction module and cross-domain contrastive learning module. (2) The model shows notable improvements on the two data-sparse datasets (DBbook2014 and Book-Crossing), with densities of 0.44% and 0.19%, respectively. We attribute this to our proposed LLM-generated user-side structural knowledge, which effectively mitigates the data sparsity problem. (3) We observe that the knowledge-aware baselines are sensitive to the availability of item-side knowledge. Specifically, these methods perform well in datasets with abundant item-side knowledge (DBbook2014 and MovieLens-1M) but perform worse in datasets with sparse item-side knowledge (Book-Crossing). Our method performs well on both item-side knowledge-dense and knowledge-sparse datasets, highlighting the effectiveness and necessity of incorporating user-side knowledge."}, {"title": "User-side Data Sparsity Analysis", "content": "One motivation for exploiting user-side knowledge is to alleviate the data sparsity problem, which often limits the expressiveness of recommender systems. It is challenging to learn optimal representations for inactive users with few interactions. Following previous work (Wang et al. 2019b), we divide users into four groups based on their historical interactions in the training set. We then report the average NDCG@50 results for each user group using our model and selected baseline methods from various categories. The results are depicted in Fig. 3, where the horizontal axis represents user interaction from sparse to dense.\nFig. 3 indicates that our model demonstrates superior performance for user groups with sparse interactions across the three datasets. This highlights the advantage of our model in handling user interaction sparsity. We attribute this superiority to our proposed CIKG, which incorporates LLM-generated user-side knowledge. The introduction of user-side knowledge effectively facilitates the mining of user-side collaborative information, thereby improving performance for user groups with sparse interactions. This also demonstrates the effectiveness of our user-side knowledge construction method."}, {"title": "Ablation Study", "content": "To evaluate the contributions of the various modules in our model, we conducted ablation experiments while keeping the hyperparameters fixed. The results, presented in Table 3, highlight the impact of each module.\nFirst, we observe a significant performance decrease across all three datasets in the model without user interest knowledge (UIK). This underscores the importance of the LLM-generated user-side structured knowledge. Similarly, the removal of user interest reconstruction (UIR) and contrastive learning (CL) leads to diminished performance, indicating these modules' roles in mitigating information noise and enhancing knowledge transfer. To investigate the impact of the"}, {"title": "Case Study", "content": "dynamic mask rate strategy within the UIR, we compare two variants: one without the dynamic masking rate (DMR), and one using the linear strategy. Our findings are twofold: (1) Both the linear and exponential growth strategies outperform the fixed masking rate, demonstrating that an easy-to-hard training approach enhances model performance. (2) The exponential growth strategy yields superior results compared to the linear strategy, suggesting that a slower increase in model difficulty improves performance across the datasets, validating the efficacy of our proposed exponential growth strategy.\nTo intuitively assess the effectiveness of the user-side knowledge and the CIKG-based recommendation model, we select user #u2362 from the Book-Crossing dataset, which exhibits the sparsest user-item interactions and item-side knowledge among the three datasets. As depicted in Fig. 4, the left panel illustrates the LLM-based interest knowledge generation process for user #u2362 along with the top-K recommendation list, where higher-ranked items indicate a greater likelihood of user interaction. The right panel demonstrates the process of capturing user similarity based on the shared interest #j9580 between user #u2362 and user #u897 using the CIKG-based recommendation model. Item #i1484, a book with a positive self-improvement narrative, appears in the test set for user #u2362 and in the training set for user #u897. Based on Fig. 4, we draw the following two observations: (1) Our LLM-based user-side knowledge method effectively summarizes the interests of user #u2362, as evidenced by the alignment between the potential interact item #i1484 and interest #j9580 (self-help motivational). This demonstrates that our proposed LLM-based user-side knowledge inference method effectively captures user interests. (2) Our CIKG-based model accurately predicts user #u2362's potential interaction with item #i1484, as it is prominently listed in the top-K recommendation list for this user. This result suggests that our CIKG-based recommendation model effectively leverages user-side knowledge to identify higher-order user-item similarity relationships."}, {"title": "Conclusion", "content": "In this paper, we address the issue of missing user-side structured knowledge, a problem often overlooked by knowledge-based recommender systems. We introduce a LLM-based method for generating user-side knowledge. Additionally, we propose a recommendation framework that effectively leverages the LLM-generated user-side knowledge. This framework includes a user interest reconstruction module and a contrastive learning module, which are designed to mitigate the challenges of noise and knowledge transfer, respectively."}, {"title": "Hyperparameter Sensitivity", "content": "In this section, we investigate the influence of four critical hyperparameters: the initial mask rate a, the maximum mask rate w, the user interest reconstruction loss weight $\\lambda_2$, and the contrastive learning loss weight $\\lambda_3$. The results of the hyperparameter sensitivity experiments on the Book-Crossing dataset are illustrated in Fig.7. The primary vertical axis of the figures shows the Recall@50 results as a line chart, while the secondary vertical axis presents the NDCG@50 results as a bar chart. Based on Fig.7, we derive the following observations: (1) Examining $\\lambda_2$ and $\\lambda_3$, which control the loss weights, reveals that model performance initially increases and then decreases as these parameters increase. Notably, a very high $\\lambda_3$ leads to a significant decline in performance. This indicates that the selection of these hyperparameters should avoid extremes; they must be scaled appropriately during joint learning to achieve optimal performance. (2) Considering the initial mask rate a and the maximum mask rate w, we find that a small initial mask rate, around 0.1, is beneficial for model training. The maximum mask rate w can be quite large, even up to 0.95. We believe that these hyperparameters should be adjusted according to the specific dataset and the particular LLM employed in our model."}, {"title": "Auxiliary Information Incorporation Experiment", "content": "To evaluate the effectiveness of incorporating item-side information (KG) and our proposed user-side information (IG) into the recommendation domain (CG), we employ Light-GCN (He et al. 2020) as the baseline model. We fix all hyperparameters, including random seeds, and use graphs containing various types of auxiliary information to assess the impact of this additional information on model performance. The experimental results are presented in Tab. 4. In this context, LightGCNCG refers to the original model using only collaborative data, LightGCNCIG denotes the variant augmented with LLM-generated user-side information, LightGCNCKG incorporates item-side information, and LightGCNCIKG integrates both user-side and item-side information. The key observations are as follows:\n\u2022 Models incorporating auxiliary information consistently outperform the baseline model (LightGCNCG) across all three datasets. This aligns with our hypothesis that the inclusion of additional information enables GNN-based models to capture higher-order collaborative signals, thereby enhancing performance.\n\u2022 Among the variants incorporating auxiliary information, both user-side and item-side additions improve model performance, although no single variant consistently outperforms the others across all metrics and datasets. This result supports the effectiveness of the user-side information generated by our LLM. However, the lack of a dominant variant suggests that noise in the auxiliary information, combined with the absence of an effective knowledge transfer module in the original LightGCN, limits the model's ability to fully leverage this information. Thus, our findings highlight the necessity of our proposed anti-noise and knowledge transfer modules."}, {"title": "Supplementary Hyperparameter Sensitive Experiments", "content": "To further investigate the impact of different hyperparameter settings on our model's performance, we conduct sensitivity"}, {"title": "Technical Appendix", "content": "The appendix contains the following sections: the proof of Equation (4) from the main paper, the auxiliary information incorporation experiment, supplementary results on the sensitivity of additional hyperparameters, and descriptions of the baseline methods.\nProof of Equation (4)\nLet h(q) =dlin (q) - dexp(q), thus h(0)=0 and h(A) = 0. By taking the first and second derivatives of h(q), we get:\nd\nh(q)\ndq\n=\nd\ndq (Slin (q) \u2013 \u03b4\u03b5xp(q))\ndq\n= (9) \u2013 \u0431\u0435\u0445\u0440 (9)\ndq\n(\u03c9 \u2013 \u03b1)\n=\n\u039b\nd\ndq\n\u03b1\n()ln()\n=(9)= -()*(())\u03b1\nIn our setup, a and w represent the initial and maximum mask rates, both of which are greater than 0. Therefore,\nd2\ndash(q) < 0 in the interval [0, A], which means aah(q) is\ndq"}]}