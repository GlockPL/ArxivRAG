{"title": "Dr. GPT in Campus Counseling: Understanding Higher Education Students' Opinions on LLM-assisted Mental Health Services", "authors": ["OWEN XINGJIAN ZHANG", "SHUYAO ZHOU", "JIAYI GENG", "YUHAN LIU", "SUNNY XUN LIU"], "abstract": "In response to the increasing mental health challenges faced by college students, we sought to understand their perspectives on how AI applications, particularly Large Language Models (LLMs), can be leveraged to enhance their mental well-being. Through pilot interviews with ten diverse students, we explored their opinions on the use of LLMs across five fictional scenarios: General Information Inquiry, Initial Screening, Reshaping Patient-Expert Dynamics, Long-term Care, and Follow-up Care. Our findings revealed that students' acceptance of LLMs varied by scenario, with participants highlighting both potential benefits, such as proactive engagement and personalized follow-up care, and concerns, including limitations in training data and emotional support. These insights inform how AI technology should be designed and implemented to effectively support and enhance students' mental well-being, particularly in scenarios where LLMs can complement traditional methods, while maintaining empathy and respecting individual preferences.", "sections": [{"title": "1 INTRODUCTION", "content": "Today's young adults, including higher education students, are reporting increasingly high levels of depressive symptoms, stress, and loneliness, surpassing those of older cohorts [1, 4]. Studies link these mental health issues to academic pressures, future career concerns, achievement culture, the COVID-19 pandemic, and a lack of mental health resources [10, 11, 26]. On average, it takes about 7.8 days for college students to get an initial appointment with a mental health professional, but the following sessions could extend to several weeks due to a shortage of mental health services on campuses [20]. This shortage is reflected in the counselor-to-student ratio, which often falls short of the recommended 1:500 standard, with many colleges having only one counselor for every 1,000 to 1,500 students [7]. In addition to the shortage of mental health resources, college students are reluctant to seek traditional treatment for multiple reasons, such as financial cost, time constraints, and concerns about stigma [14], . These findings highlight the urgent need for innovative solutions, such as technology, to address mental health challenges in this demographic. Researchers are considering LLM-powered chatbots [16] for mental health support. Large Language Models (LLMs) [6] are advanced AI systems capable of understanding and generating human-like text, which"}, {"title": "2 RELATED WORK", "content": "LLMs present significant opportunities and challenges in the field of mental health services [17]. They offer the potential to enhance accessibility, improve efficiency, and maintain privacy through automated interactions. Previous research suggests that LLMs can support mental health in five distinct approaches: General Information Inquiry, Initial Screening, Reshaping Patient-Expert Dynamics, Long-term Care, and Follow-up Care. However, concerns about their reliability, the potential to generate unpredictable responses, and their ability to provide emotional support as effectively as human interactions remain significant challenges [15, 18].\nGeneral Information Inquiry: Research has shown that computational or predictive analyses of digital data can accurately identify moods and mental health states [8, 9]. These capabilities lay the foundation for LLMs to provide informative support, crucial in this scenario. However, there is a risk of LLMs propagating misinformation due to reliance on online data for fact-checking, which could lead to incorrect assessments or advice [2, 3].\nInitial Screening: The shortage of mental health professionals and the increasing dependence on technology for mental health services highlight the importance of LLMs in offering initial screening and basic counseling [5, 19]. Yet, concerns about the depth of emotional support that LLMs can provide persist, questioning their effectiveness in replacing human interactions [18].\nReshaping Patient-Expert Dynamics: The integration of chatbots in mental health services promises to extend accessibility and reshape traditional healthcare dynamics [12]. Nonetheless, the potential for generating unpredictable responses could undermine the trust required in patient-expert relationships [2].\nLong-term Care: The ability of LLMs to adjust and evolve in response to user interactions is essential for their effectiveness and long-term sustainability in therapeutic settings [25]. However, the reliance on potentially biased online data poses a challenge, as it may lead to the reinforcement of incorrect or harmful patterns [2].\nFollow-up Care: Previous studies have shown that chatbots can effectively deliver follow-up care and maintain continuity through coherent responses to user prompts [23]. Still, the limitations in providing nuanced emotional support could impact the quality of ongoing care [18]."}, {"title": "3 PILOT RESULTS", "content": "We conducted a pilot study using semi-structured interviews with participants to evaluate their acceptance of LLMs across five different scenarios in mental health services. This study revealed varied levels of acceptance for LLMs across these scenarios, providing critical insights into where these technologies may be most effective in supporting mental health services. Participants responded most positively to the \"Initial Screening\" and \"Follow-up Care\" scenarios, which were seen as areas where LLMs could significantly enhance the accessibility and efficiency of care.\nInitial Screening: In the Initial Screening scenario, LLMs were highly accepted by participants for their ability to personalize interactions and tailor questions to the specific needs of users. This customization was identified as a key strength, allowing students to express their mental health concerns more effectively and facilitating a more engaging and responsive screening experience. Participants appreciated how LLMs could streamline the initial stages of mental health support by providing timely and relevant responses, potentially expediting access to further care. The positive reception of LLMs in this scenario underscores their potential to enhance the efficiency and effectiveness of mental health screenings, making them a valuable tool in early intervention efforts."}, {"title": "4 DISCUSSION", "content": "The findings from our pilot study across all five scenarios provide essential insights into how LLMs can be effectively integrated into mental health services to enhance students' mental well-being. These insights inform the strategic design and implementation of AI technology, ensuring that it complements traditional methods while maintaining empathy and respecting user preferences.\nDesigning AI to Complement Human Care: A consistent theme across the scenarios was the importance of designing LLMs to enhance, rather than replace, human care. In scenarios like Initial Screening and Follow-up Care, where LLMs were highly accepted, their ability to improve accessibility and efficiency was clear. Participants appreciated how LLMs could manage routine tasks, offer personalized support, and provide timely assistance, particularly when human resources are limited. However, the success of these technologies hinges on their ability to work in tandem with human providers, recognizing the limits of AI in handling complex emotional issues [5, 18]. LLMs must be equipped with mechanisms to escalate cases to human professionals when necessary, ensuring that their use enhances service quality without compromising the therapeutic relationship. Moreover, in scenarios like General Information Inquiry and Long-term Care, the accuracy and reliability of AI-generated content are crucial. By maintaining high standards of information accuracy and incorporating features that enable seamless collaboration between AI and human experts, LLMs can significantly contribute to more effective mental health support [2].\nMaintaining Empathy and User-Centered Design: Another critical consideration is the need to maintain empathy and personalization in mental health care. Across all scenarios, participants expressed the importance of LLMs being able to provide emotionally intelligent, personalized care, particularly in sensitive contexts like Reshaping Patient-Expert Dynamics and Long-term Care [18, 19]. To address these concerns, LLMs must be designed with advanced emotional intelligence, allowing them to respond to the nuanced needs of students effectively. Additionally, transparency about the AI's role in care is essential for building trust. Ensuring that users understand the capabilities and limitations of AI, and keeping human providers at the center of the therapeutic process, is crucial for fostering a positive relationship between students and AI [12]. By focusing on empathetic, user-centered design, we can create AI tools that not only improve accessibility and efficiency but also enhance the overall quality of mental health care for students. This approach ensures that LLMs serve as supportive, trustworthy tools in the mental health journey, contributing positively to students' mental well-being [15]."}]}