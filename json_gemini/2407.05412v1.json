{"title": "FM-OSD: Foundation Model-Enabled One-Shot Detection of Anatomical Landmarks", "authors": ["Juzheng Miao", "Cheng Chen", "Keli Zhang", "Jie Chuai", "Quanzheng Li", "Pheng-Ann Heng"], "abstract": "One-shot detection of anatomical landmarks is gaining significant attention for its efficiency in using minimal labeled data to produce promising results. However, the success of current methods heavily relies on the employment of extensive unlabeled data to pre-train an effective feature extractor, which limits their applicability in scenarios where a substantial amount of unlabeled data is unavailable. In this paper, we propose the first foundation model-enabled one-shot landmark detection (FM-OSD) framework for accurate landmark detection in medical images by utilizing solely a single template image without any additional unlabeled data. Specifically, we use the frozen image encoder of visual foundation models as the feature extractor, and introduce dual-branch global and local feature decoders to increase the resolution of extracted features in a coarse-to-fine manner. The introduced feature decoders are efficiently trained with a distance-aware similarity learning loss to incorporate domain knowledge from the single template image. Moreover, a novel bidirectional matching strategy is developed to improve both robustness and accuracy of landmark detection in the case of scattered similarity map obtained by foundation models. We validate our method on two public anatomical landmark detection datasets. By using solely a single template image, our method demonstrates significant superiority over strong state-of-the-art one-shot landmark detection methods.", "sections": [{"title": "1 Introduction", "content": "Accurate anatomical landmark detection is an essential step in many clinical applications, such as disease diagnosis [11,17,23], therapy planning [5,19], and registration initialization [9,13,18]. Although deep learning methods have achieved"}, {"title": "2 Method", "content": "Fig. 1 illustrates our one-shot anatomical landmark detection framework with foundation models. By leveraging the powerful feature extraction capabilities of visual foundation models and our proposed novel feature enhancement and matching strategies, our method achieves anatomical landmark detection using only a single template image, without requiring any additional unlabeled images."}, {"title": "2.1 Global and Local Feature Enhancement of Foundation Model", "content": "We aim to leverage deep features extracted from a pre-trained visual foundation model as dense visual descriptors for one-shot landmark detection tasks. However, the image encoder of a pre-trained foundation model often generate feature maps with downsampled resolution, which significantly restricts their effectiveness in landmark detection. Additionally, the feature extractor of the foundation model is trained on natural images, leading to a substantial domain shift when applied to medical images. To address these challenges, we propose a coarse-to-fine landmark detection framework featuring dual-branch global and local learnable decoders. Our approach enhances the feature resolution for landmark detection in a coarse-to-fine manner and improves feature quality by incorporating domain knowledge contained in the template image.\nSpecifically, as shown in Fig. 1(a), for the coarse stage, the entire input image \\(I \\in \\mathbb{R}^{H\\times W}\\) is downsampled into a size with the short side of 224 and is then fed into the frozen image encoder \\(&\\) of a foundation model, which typically undergoes additional downsampling of the feature maps due to the patch encoding process in vision transformers. In order to restore the feature resolution that is"}, {"title": "Distance-aware similarity learning loss.", "content": "To enhance the feature quality extracted from the foundation model with increased discrimination among features from different positions, we aim to effectively optimize the global and local feature decoders by leveraging just one single template image \\(I_t \\in \\mathbb{R}^{H\\times W}\\) annotated with N landmarks \\(P_t = \\{(x_i^t,y_i^t) | i = 1,\\dots, N\\}\\). Intuitively, the"}, {"title": "2.2 Bidirectional Matching Strategy", "content": "With the extracted features of the template and query image, the success of one-shot landmark detection relies on the development of an effective matching strategy to find the corresponding point on the query image based on feature similarity with the ground truth landmark on the template image. The commonly used matching strategy is selecting the point with highest feature similarity to the template landmark. However, the feature similarity map often lacks the accurate focus on a single point since the feature extracted by foundation models like DINO is usually ambiguous [1] in terms of their spatial distribution, reflecting similar features over extensive areas, as shown in the similarity map in Fig. 1. As a result, directly choosing the query point with the highest similarity to template landmark could result in inaccurate landmark detection.\nTo improve the matching accuracy, we propose a novel bidirectional matching strategy (BDM) to find the landmark pair \\((\\mathbf{p}_t^i,\\mathbf{p}_q^i)\\) with not only a high feature similarity from template to query image but also a low inverse-matching error from query to template. \\(\\((\\mathbf{p}_t^i,\\mathbf{p}_q^i)\\)\\) indicates the matched position for the i-th landmark on the query image and the template image, respectively and \\(\\mathbf{p}_q^i\\) is the final prediction for the query image. Such a two-side agreement is similar to the Best-Buddies Similarity method for template matching [7], which has been theoretically and experimentally demonstrated to be a robust and reliable matching method between two sets of points. Moreover, the inverse-matching error for the matching from query to template can be calculated since the ground truths are known and this can be taken as an estimation of the matching error"}, {"title": "3 Experimental Results", "content": "Datasets and Evaluation Metrics. We evaluate the effectiveness of our method using two public X-ray datasets that are commonly utilized in prior one-shot anatomical landmark detection studies. The Head X-ray dataset [17] consists of 400 lateral cephalograms with a resolution of 0.1 mm and 19 target landmarks. This dataset has been officially split, and we use a single labeled image from the training set as our template image, while employing the 250 testing images as our test set. The Hand X-ray dataset, labeled by [15], contains 909 radiographs with 37 labeled landmarks. The length between two endpoints of the wrist is assumed to be 50 mm [15]. Following [15,16,22], we use one image from the first 609 images as the template image, and utilize the remaining images for testing. Two commonly used evaluation metrics are employed, i.e., the mean radial error (MRE) and the successful detection rate (SDR) [16,22]. The MRE measures the Euclidean distance between two points while SDR counts the percentage of predictions with a distance under various thresholds, which are set to be 2 mm, 2.5 mm, 3 mm, 4mm for the Head X-ray dataset and 2 mm, 4 mm, 10 mm for the Hand X-ray dataset, respectively.\nImplementation Details. The image encoder in our method is taken from DINO-S [6], with features extracted from the \"key\" head of the ninth layer. The investigation on using different foundation models and different layers have been provided in our ablation study. The template image is augmented with random"}, {"title": "Comparisons with State-of-the-arts.", "content": "We compare our method with SOTA one-shot anatomical landmark detection methods, including CC2D [20], SAEM [18], SCP [16], EGTNLR [22] and UOD [25]. All these comparison approaches and our method use the same template image as in [20,21,25], with the difference being that our method does not employ additional unlabeled images. We also compare with SOTA fully supervised methods GU2Net [26] as an upper bound. The results for other methods are directly obtained from their original papers since we use the same datasets and template image, except for SAEM and EGTNLR, for which we implement their released code on the two datasets.\nAs shown in Table 1, our FM-OSD method significantly outperforms other SOTA one-shot methods on both datasets, even though we only utilize one"}, {"title": "Ablation Studies.", "content": "Table 2 shows ablation studies on the key components of our method with the Hand dataset. Since BDM is a inference strategy for feature similarity matching without the need of training, we first apply it to the original features extracted by the frozen image encoder of foundation models. Leveraging our proposed BDM strtegy, the performance significantly improves by 0.9 mm in MRE. By introducing the global decoder and the local decoder under a coarse-to-fine scheme, the MRE can be reduced to 1.86 mm and 1.41 mm, respectively.\nMoreover, to show the efficacy of the distance-aware similarity learning loss, we replace it with a contrastive learning loss by taking all the pixels on the feature map except the target point itself as negative samples and an MSE loss posed on the similarity map guided by a ground truth map whose value is 1 for the target point and 0 for others (See Fig. 3). The MRE results for the training of the global decoder on the hand dataset are 4.54 mm and 2.33 mm for the two losses, respectively, whereas using our proposed loss can obtain an MRE of 1.86 mm. Fig. 4 (a) compares the results of various image encoders from several popular foundation models. DINO-S is chosen in this work considering its stable performance on both datasets and the small model size, but our proposed method can be applied to others without modifications. We also compare the impacts of"}, {"title": "4 Conclusion", "content": "This paper proposes a foundation model-enabled one-shot landmark detection framework for medical images. With careful designs to tackle the challenges of adapting foundation models to landmark detection, our proposed method can obtain significantly superior results using solely a single template image than other one-shot methods which even use plenty of unlabeled data. However, we only validate the efficacy of our method on 2D images. More evaluations on 3D datasets and cross-modality settings will be conducted in our future studies."}]}