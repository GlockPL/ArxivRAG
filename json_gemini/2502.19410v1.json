{"title": "Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices", "authors": ["Xinru Wang", "Mengjie Yu", "Hannah Nguyen", "Michael Iuzzolino", "Tianyi Wang", "Peiqi Tang", "Natasha Lynova", "Quoc Co Tran", "Ting Zhang", "Naveen Sendhilnathan", "Hrvoje Benko", "Haijun Xia", "Tanya Jonker"], "abstract": "Large Language Models (LLMs) have shown remarkable potential in recommending everyday actions as personal AI assistants, while Explainable AI (XAI) techniques are being increasingly utilized to help users understand why a recommendation is given. Personal AI assistants today are often located on ultra-small devices such as smartwatches, which have limited screen space. The verbosity of LLM-generated explanations, however, makes it challenging to deliver glanceable LLM explanations on such ultra-small devices. To address this, we explored 1) spatially structuring an LLM's explanation text using defined contextual components during prompting and 2) presenting temporally adaptive explanations to users based on confidence levels. We conducted a user study to understand how these approaches impacted user experiences when interacting with LLM recommendations and explanations on ultra-small devices. The results showed that structured explanations reduced users' time to action and cognitive load when reading an explanation. Always-on structured explanations increased users' acceptance of AI recommendations. However, users were less satisfied with structured explanations compared to unstructured ones due to their lack of sufficient, readable details. Additionally, adaptively presenting structured explanations was less effective at improving user perceptions of the AI compared to the always-on structured explanations. Together with users' interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) advancements over the past decades have led to the development of AI-based personal assistants, which have become a popular way for human users to interact with digital devices and data. More recently, Large Language Models (LLMs) such as the GPT [1] and LLaMa [57] series have shown remarkable performance in generation tasks, which has led to their growing use as a method to make everyday contextual recommendations for AI-based personal assistants [11, 33]. In addition, to ensure transparency and control during human-AI interaction, there has been a growing interest in Explainable AI (XAI) techniques that can help users understand and trust the AI assistance they receive [48, 49, 66, 72, 78]. In the context of LLMs, those models based on transformer architectures are capable of generating explanations that rationalize their reasoning process using natural language [27, 69].\nAn emerging manifestation of AI-based personal assistant has been on ultra-small devices, such as smartwatches, smartglasses [19, 44], and on-cloth gadgets [21], to offer low-friction assistance that is seamlessly integrated into the physical world. These devices rely on vast amount of user contextual data gathered from their sensors (e.g., egocentric video, gaze tracking, and audio) to predict the goal of the user's next action and generate action recommendations [23], such as launching an application, to help users efficiently complete daily tasks.\nInteracting with LLM recommendations and explanations on such ultra-small devices can, however, be challenging for several reasons. First, LLM self-explanations are often verbose and time-consuming to comprehend [22], while the constrained screen space on ultra-small devices exacerbates the challenge of displaying verbose explanations. Real-world examples include smartwatches or Augmented Reality (AR) glasses, which have limited screen real estate that is available, and struggle to display long messages [14]. Designers need to carefully condense the content of an explanation to ensure it remains relevant and informative while not overloading the limited screen space. Additionally, some tasks, such as providing route adjustment recommendations while driving, are inherently time-sensitive and consume a lot of the user's cognitive resources. Thus, it is crucial to optimize the timing of explanations by presenting them only at moments when they would be most useful or when a user requests them to prevent unnecessary user interruptions [72].\nTo overcome these challenges, this work aims to design glanceable explanations for a goal-oriented, contextual LLM action recommendation system for end-users using ultra-small devices. We focused on improving spatial and temporal glanceability via two research questions:\n\u2022 RQ1: To improve the spatial glanceability of LLM explanations on ultra-small devices, how should LLM explanation text be structured?\n\u2022 RQ2: To take a step beyond and further improve the temporal glanceability of LLM explanations on ultra-small devices, how should LLM explanations be adaptively presented?\nTo answer these questions, we designed a web-based interface that enabled us to simulate an LLM recommending and explaining actions to a user when they were using an AI-based personal assistant embedded in a smartwatch, and we recruited participants to interact with this interface. A subset of Ego4D [20] videos featuring digital interactions in people's daily activities was selected for evaluation. We used a learning approach based on Socratic Models [76] with the dataset, where pre-trained vision-language models were used to generate a linguistic summarization (e.g., user physical actions and detected objects) of a video input for downstream processing with an LLM, which then made inferences and generated action recommendations.\nTo improve spatial glanceability (RQ1), we converted an LLM's verbose explanation text into concise concepts and intuitive icons that users could grasp at a glance. We adopted a Chain-of-Thought prompt strategy [27, 69] and broke down the inference process into steps. An LLM first summarized all possible contexts (i.e., the [activity] the user is doing, the [object] the user is interacting with, and the [location] the user is in) from the output of the vision-language models. Then, the LLM inferred the short-term [goal] that the user wanted to achieve and provided a digital action recommendation that the user could take as a next step. Instead of displaying lengthy text, the explanation only included the four structured components.\nTo improve temporal glanceability (RQ2), we focused on situations when the intelligent system became uncertain and needed user confirmation [72]. We employed a hybrid method [6, 71] that combined the consistency among multiple responses and the textual confidence generated by the model to extract the confidence level for each recommendation. Explanations were automatically presented whenever the recommendation confidence was low, otherwise they were only displayed upon the user's request.\nWe then conducted a user study with 44 participants to understand users' experiences when provided with structured and adaptively presented explanations for LLM-driven contextual recommendations. The study was designed with four within-subject conditions: no explanations, always-on unstructured explanations, always-on structured explanations, and adaptive structured explanations whose presentation was dependant on the recommendation confidence level. We found that structured explanations effectively reduced participants' time to select an AI-recommended action and lowered their cognitive load when reading the explanation. Participants were also more likely to accept the recommendations when presented with always-on structured explanations. Participants were, however, less satisfied with structured explanations compared to unstructured explanations. When presented adaptively, the structured explanations were less effective at improving user perceptions of the AI (e.g., trust, satisfaction) compared to the always-on structured explanations. Our analysis of participants' feedback from a semi-structured interview showed that they valued the ease of reading and viewing text in the structured explanations, but also desired the level of detail and naturalness as provided by the unstructured explanations. Additionally, while participants appreciated the control over interface offered by adaptively presented explanations, it introduced extra efforts to interact with the interface.\nTaken together, we make the following contributions in this work:"}, {"title": "2 Related Work", "content": "Our work builds upon three areas of relevant literature, i.e., existing XAI techniques to explain LLMs, empirical studies on AI explanations, and user interface design challenges for ultra-small devices."}, {"title": "2.1 Techniques to Explain Large Language\nModels", "content": "A variety of XAI techniques have been developed to increase the interpretability of AI models. Conventional XAI methods can be categorized into model-specific methods tailored to particular model types, and model-agnostic methods applicable across various model types. Training interpretable models such as rule-based models and generalized additive models [24, 30, 62] are examples of model-specific methods. On the other hand, examples of model-agnostic methods include generating feature contributions [36, 49, 52] or searching for prototypes or counterfactual instances [26, 61].\nThe emergence of LLMs raised unique challenges when applying traditional XAI methods to explain them due to the size of their training datasets and non-deterministic outputs [34]. Open-source LLMs like Meta's LLaMA models [57] provide broader transparency for Al researchers to uncover their internal states. For instance, [60] identified when neurons within LLMs were activated, while [81] analyzed how high-level cognitive processes were represented in LLM neurons. [79] explored how LLMs encoded knowledge in their embeddings. On the other hand, closed-source LLMs, such as GPT [1] and Google Gemini [56], pose more challenges due to the restricted access to their internals. Regardless of the closed or open nature of LLMs, one common approach to generate explanations is to prompt the model to provide \u201cself-rationalizations\u201d alongside predictions [27, 31, 69, 73]. As these LLMs are based on transformer architectures, they can capture complex relationships between input data and generate output through the attention mechanism, to self-rationalize their recommendations in a way that appears logical to humans. Yet, some work has criticized these self-rationalized explanations as being unreliable and unfaithful to the actual reasoning process [58, 74]. To address these limitations, researchers have pursued approaches including iteratively prompting the LLM for self-feedback and refinement [12, 40], and grounding an LLM in external knowledge bases [7, 80].\nWhile the natural language modality of LLMs' responses opens up opportunities for explanations that are understandable to non-experts, they are often too verbose to be grasped in a quickly digestible manner. This issue is particularly pronounced on ultra-small devices, and our work aims to address it."}, {"title": "2.2 User-Centered Design and Evaluation of AI\nExplanations", "content": "Empirical research has focused on understanding how users understand AI explanations [66, 67] and trust AI models [67, 68, 78], as well as users' overall task performance [3, 29, 35, 63, 64]. This research has also spurred novel design processes that identify users' explainability needs to inform XAI design (e.g., identifying the questions that users commonly ask AI systems [48]). In addition to explaining traditional machine learning models, more recently, user-centered design approaches have been applied to explain large generative Al models. For example, Sun et al. [55] identified explainability needs unique to generative AI for code compared to traditional discriminative machine learning models, and proposed four XAI features: Al documentation, uncertainty indicators, attention visualizations, and social transparency.\nOn the other hand, research on designing and evaluating X\u0391\u0399 methods for particular digital devices is lacking. Developing Al explanations for specific interfaces often involves trade-offs between usability and transparency. Springer and Whitaker [54] highlighted that transparency can be distracting and that users might benefit from initially simplified explanations. Chromik and Butz [8] surveyed a list of XAI publications and identified design principles for human interaction with explanation user interfaces, including naturalness, responsiveness, flexibility, and sensitivity. Xu et al. [72] proposed design principles for displaying XAI information in floating windows on AR devices. They recommended simplifying explanations by selecting the appropriate content, and triggering explanations only when users have enough capacity, are unfamiliar with the outcome, or when the model is uncertain. Our work builds on this prior research but focus on ultra-small devices, where there is an urgent need for highly glanceable explanations."}, {"title": "2.3 User Interface Design for Ultra-Small\nDevices", "content": "The ultra-small screen sizes on devices such as smartwatches raises key usability issues for the layout of visually rich content such as verbose text [9, 51, 53]. To address these challenges, researchers have recommended design guidelines to condense information while maintaining clear user understanding [2, 10]. For example, a navigation interface should only display an upcoming route as an arrow [15]. Others have explored adaptive menus and have found that users preferred a cloud menu, where predicted items were arranged in a circular tag cloud [59]. Work by Rahman and Muter showed that presenting continuous text sentence-by-sentence within small display windows offered users the ability to reread one sentence at a time and was found to be as efficient as conventional reading [47]. In summary, these common approaches included converting lengthy text into graphical diagrams or icons [22, 50], adaptively prioritizing highly-relevant content [17, 18], progressively displaying content [25, 47], or utilizing multiple modalities such as voice and gestures [32, 42, 70]. Guided by these existing design ideas, we selected two major dimensions to explore fitting verbose content on ultra-small screens, i.e., spatially, by structuring it into easily viewable formats, and temporally, by adaptively displaying the content."}, {"title": "3 Generating Glanceable LLM Explanations", "content": "In this section, we introduce the dataset and LLM pipeline used for generating recommendations, then outline our approaches to present spatially and temporally glanceable LLM explanations on ultra-small devices. The goal of this pipeline was to imitate a human-AI interface that empowered users to accomplish their daily tasks via a smooth transition to digital actions. The system used observations of longitudinal context as input, which could be any combination of sensors a device had access to, such as egocentric video, biometric data, or digital state information. After predicting the user's goal for their next action based on the longitudinal context, the system recommended a digital action to fulfill this goal and disassembled it into device-executable instructions. Along with the recommendation, an explanation was provided for users to ensure transparency and trustworthiness with the system."}, {"title": "3.1 Dataset", "content": "We used the Ego4D [20] dataset as the testbed to evaluate the LLM pipeline in pre-recorded real-world scenarios. The Ego4D dataset consists of over 3,670 hours of egocentric videos of people's daily activities. The videos were captured using head-mounted cameras to naturally approximate first-person visual perception.\nWe leveraged the annotated video narrations included in the dataset to manually curate a subset of videos that were comprised of interactions with digital devices (e.g., \"phone\" or \"laptop\"). We trimmed the videos to start 30 seconds prior to the digital action as context for the action and ensured that the video content prior to the digital action contained primarily physical activities, such as cooking or hiking. Following this, we were left with 1101 videos that focused on everyday goal-oriented, contextual scenarios that the LLM could take as input to generate action recommendations. The daily activities covered all 27 categories of common everyday activities identified by a taxonomist who is one of the authors, such as browsing the internet, chores and cleaning, cooking and eating, exercise, work, gaming, messaging and communication, shopping, and traveling."}, {"title": "3.2 LLM Recommendation Pipeline", "content": "The pipeline was based on Socratic Models [76], where pre-trained vision-language models generated a linguistic summary of a video for downstream processing with an LLM (Figure 1). Specifically, we employed a pre-trained transformer-based model designed for video understanding to extract physical actions (i.e., narrations) present in the video by dividing each video into 2-second clips. We used a pre-trained transformer-based object detection model to detect objects appearing in the video within a 5-second window before the digital action. With these two pieces of contextual information about the video input, GPT-4 was prompted to infer the user's intent and output its recommendation for the user's next digital action.\nWe prompted the LLM to produce natural language explanations along with its recommendations following step-by-step thinking based on the Chain-of-Thought method [27, 69]. As text-based responses from LLMs can be too verbose to be displayed on an ultra-small device's screen, spatially, we structured the textual responses using defined contextual components to create a visual experience that required minimal cognitive effort to comprehend. Temporally, we controlled the presentation timing of the explanations to avoid unnecessary interruptions to the user. We addressed situations where the Al system showed uncertainty, meaning it might make errors and require user confirmation. The prompt template is shown in Figure 2."}, {"title": "3.2.1 Structuring Explanation Text Using Defined Contextual Com-\nponents.", "content": "The inference process was divided into several stages. The LLM first extracted and summarized the relevant contextual information, i.e., the [activity] the user was performing, the [object] the user was interacting with, and the [location] of the user, from the output generated by pre-trained vision-language models. Then, the LLM inferred the user's short-term [goal] based on these contextual cues. Finally, using the inferred goal, the LLM provided a digital action recommendation. We chose \"object\", \"activity\", \"location\", and \"goal\" as the contextual information as these entities were the core contextual components necessary to generate relevant recommendations and explanations in an everyday mixed-reality scenario, as provided by the aforementioned taxonomist. We also referenced definitions and few-shot examples in the prompt for each component. The LLM was instructed to provide its output in a JSON format, which was later used to create the structured representations."}, {"title": "3.2.2 Adaptively Presenting Explanations Based on Confidence Lev-\nels.", "content": "Following Chen and Muelle [6] and Xiong et al. [71], we employed a hybrid approach that combined consistency-based methods and verbalized confidence to extract a calibrated confidence level for each recommendation. We mapped the confidence score for each detected object from the object detection model and the perplexity score for each detected physical action from the video narration model into one of five textual confidence levels (i.e., \"very low\", \"low\", \"medium\", \"high\", \"very high\u201d), based on the quantile position of the score. We opted for a textual representation of confidence to ensure a unified representation of \"confidence\", as well as to maximize the LLMs' abilities to comprehend natural language. We included the confidence levels for raw contextual information in the prompt, then prompted the LLM to output confidence levels along with the recommendation.\nWe then generated a reference response $y_0$ and confidence $c_0$ with temperature sampling set at 0. Using the same prompt, we produced K = 5 candidate responses ${Y_1, Y_2, ..., Y_K }$ and their verbalized confidences ${C_1, C_2, ..., c_k}$ by increasing the temperature value to 0.7 [71]. Each candidate confidence $c_i$ ($i \\in {1, 2, ..., K}$) was transformed back into a numerical value and was then updated by incorporating the reference answer and the similarity to the reference answer, denoted as $s_i$: $\\~{c_i} = \\frac{(c_0 + c_i)}{2}s_i$ ($i \\in {1, 2, ..., K}$). The similarity scores were obtained using the BERTScore model [77], which measured the semantic similarity between two texts. Finally, we computed the average contribution from all candidate answers as the final confidence score $c_{hybrid} = \\sum_{i=K} \\~{c_i} / K$, and then converted it into one of the five confidence levels based on the quantile position of the score."}, {"title": "3.3 Technical Evaluation", "content": "To validate the quality of structured explanation components and confidence levels that the system generated, we conducted a small-scale technical evaluation. We randomly sampled one video from each of the 27 activity context categories. Three coders then coded how likely it was that the AI recommendation was correct, how likely it was that the inference about each explanation components (i.e., activity, object, location, and goal) was correct (i.e., plausibility), and to what extent each explanation component supported the AI recommendation (i.e., faithfulness) [16, 41], for each of the 27 videos using a 7-point Likert scale.\nWe then calculated the inter-rater agreement across the coders using Krippendorff's \u03b1 and obtained a score of 0.553, which is considered acceptable for exploratory research involving subjective ratings [28]. We found that all explanation components were rated as plausible and faithful to the recommendation, with the median of all ratings being larger than 4 (one-sample Wilcoxon signed-rank test showed p < 0.05 for all explanation components for the plausibility and faithfulness measures. To assess the calibration of the recommendation confidence, we computed the Pearson's r correlation coefficient between the coders' estimations of the recommendation correctness likelihood (as ground truth) and the hybrid confidence $c_{hybrid}$, obtaining a significant positive coefficient of r = 0.559 (p < 0.01). Meanwhile, the verbalized confidence levels $c_0$ did not correlate with the coders' estimations (r = 0.171, p = 0.393), indicating that the hybrid approach could lead to more calibrated confidence."}, {"title": "4 User Study Design", "content": "To gain insights into how LLM explanations that differ in their structure and adaptivity influence user experiences, we conducted a lab-based user study. The study measured participants' perceptions of LLM recommendations and explanations while they watched videos of everyday interactions and received recommendations on a simulated smartwatch UI."}, {"title": "4.1 Participants", "content": "Forty-four participants (Male = 20, Female = 23, prefer not to say = 1; mean age = 38 years, std = 12.3 years) were recruited to participate in our in-person user study, a sample size comparable to prior work [5]. Study participants were recruited through email invitations and social media platforms from an existing participant pool. Twenty participants had a graduate degree, 18 had a Bachelor's degree, 4 had some college experience, and 2 did not have any education past high school. Nine participants worked on AI-powered devices or products, 11 used Al-powered devices or services at least once a week, 9 used them at least once a month, 9 had used them a few times, and 6 had never interacted with AI-based devices or services. The entire study lasted about one hour, and each participant was compensated $75 upon completion of the study."}, {"title": "4.2 Task", "content": "During the study, participants were asked to watch a 30-second Ego4D video on a standard desktop computer and were told that the videos they were watching were captured by a camera embedded in a pair of smart glasses that were worn on a person's head and naturally approximated the visual field of the wearer of the glasses. We instructed participants to imagine that they were the one wearing these smartglasses and they were performing the activities shown in the video. They were also told that while they were imagining performing these activities, their personal AI assistant would try to predict their next intent and recommend a digital action, which was pushed to their smartwatch. The smartwatch UI was displayed next to the video on the desktop computer. Participants were asked to evaluate whether the recommendation met their needs (i.e., as the wearer of the glasses in the video) and then choose to accept or dismiss the Al's recommendation on the smartwatch UI by clicking on it. We opted to build a web-based interface that simulated smartwatch UI for two reasons. First, replicating a wide variety of real-life scenarios in a lab environment would be difficult. Instead, the existing Ego4D dataset provided a diverse set of pre-recorded situations. Second, our recommendation pipeline, which employed large transformer-based models (i.e., vision-language models, GPT-4), had slow response times. Deploying it on a smartwatch for real-time video processing would cause significant latency (~tens of seconds delay), which would have influenced participant satisfaction."}, {"title": "4.3 Conditions", "content": "A within-subject design was used with four conditions that manipulated whether and how the explanations were displayed. A balanced 4 \u00d7 4 Latin Squares design was used for condition counterbalancing [39]. A UX designer who is one of the authors standardized the smartwatch UI design using Figma."}, {"title": "4.4 Study Procedure", "content": "Upon arrival at our lab, participants were asked to complete a consent form and an initial survey about their demographics and experience with AI.\nParticipants then completed 40 trials, which were divided into four blocks, one block for each condition. Before beginning each block, there were two tutorials that were identical to the actual trials but during which no user data was collected. Each block then consisted of 10 trials, with half of them having low confidence recommendations and half having high confidence recommendations. The order of all 10 trials within each block was randomized across participants. During each trial, participants watched a 30-second video on the desktop computer. At the end of the video, a recommendation for a digital action and an explanation (depending on the assigned condition) was displayed on the simulated smartwatch UI. Participants then chose whether to accept or dismiss the AI's recommendation. After each block, participants completed a survey about their experience with the Al's recommendations and explanations.\nUpon completion of all four blocks, participants completed a final survey and answered interview questions to understand what they liked or disliked about each AI explanation condition, when they would like to see an explanation, what information they would like to see in the explanation, and how the AI explanation could be improved to better meet their needs."}, {"title": "4.5 Metrics", "content": "Several metrics were computed to understand participants' experiences and perceptions when presented with the AI explanation conditions. First, we computed the following objective measures:\n\u2022 Time to action: The time taken by a participant from the end of the video until they accepted or dismissed the AI recommendation.\n\u2022 Acceptance rate: The percentage of tasks where the participant chose to accept the AI recommendation\nAt the end of each block, we asked participants about the following subjective measures using 7-point Likert scales:\n\u2022 Mental load: We measured participants' mental load while making the decision, understanding the recommendation, and reading the explanation via three questions:\n\u201cOverall, how much mental effort did you spend on deciding whether to accept or dismiss the AI recommendation?\"\n\"Overall, how much mental effort did you spend on understanding how the Al model makes recommendations based on the context in the video?\"\n\"How much mental effort did you spend on reading the AI model's explanation of the recommendation?\"\n\u2022 Trust in AI: Participants' trust in Al was calculated by the average rating of the following three questions:\n\"I have faith that the Al model would be able to cope with all different situations.\u201d\n\"I am confident that the Al model can make good recommendations.\"\n\"Recommendations made by the Al model are likely to be reliable.\"\n\u2022 Understanding of AI.: This was also computed by averaging participants' responses to three questions:\n\"I understand how the Al model works to predict my next digital action based on the context in the video.\"\n\"I can predict how the AI model will behave based on the context in the video.\"\n\"I feel the Al model is transparent in communicating how the recommendations were made based on the context in the video.\"\n\u2022 Satisfaction with AI: \u201cI'm satisfied with the Al model's recommendation.\"\n\u2022 Satisfaction with Al explanations.: This metric differed from \"satisfaction with AI\" as it specifically measured how participants felt about the explanation provided by the AI for its recommendations, i.e., \"I'm satisfied with the Al model's explanation of the recommendation.\u201d\nAt the end of all four blocks, participants also ranked the four conditions based on their overall preferences for them."}, {"title": "4.6 Analysis Methods", "content": "Mixed-effect regression models were used to analyze the aforementioned metrics. For the time to action and acceptance rate, each participant and trial were random effects, while the conditions were fixed effects. For the mental load, trust, satisfaction with the AI, understanding of the AI, and the satisfaction of the AI explanations, each participant was a random effect. For the analysis of participants' rankings of the four conditions, a cumulative link mixed-effects model was used to analyze the ordinal response variable. The results of these models were interpreted through the estimated coefficient values for the fixed effect variables.\nThe interview data was analyzed by two coders using thematic analysis [4]. Two authors iteratively discussed and developed a codebook. The inter-rater reliability was calculated using Cohen's kappa [43], where \u03ba = 0.82 (SE = 0.055, 95% CI = [0.71, 0.93]). The final themes were developed and refined over multiple iterations of discussion."}, {"title": "5 User Study Results", "content": "We analyzed participants' time to action, acceptance rate, subjective perceptions, and interview data to better understand their experiences with the different AI explanation conditions."}, {"title": "5.1 Quantitative Results", "content": "5.1.1 Time to Action. Our results showed the presentation of the explanations impacted user's time to action. Compared to the no explanation condition, providing explanations significantly increased the time participants took to select an AI recommendation (always-on unstructured: \u03b2 = 4.6467, p < 0.001, always-on structured: \u03b2 = 1.9341, p < 0.05, always-on unstructured: \u03b2 = 2.7492, p < 0.01). But among the three conditions with explanation, structured explanations-both always-on structured explanations (\u03b2 = -2.7126, p < 0.01) and adaptive structured explanations (\u03b2 = -1.8975, p < 0.05)-significantly reduced participants' time to action compared to unstructured explanations.\n5.1.2 Acceptance Rate. Our results found that always-on structured explanations resulted in a higher acceptance rate compared to the other three conditions (compared with no explanation: \u03b2 = 0.0675, p < 0.05, with unstructured explanations: \u03b2 = 0.0712, p < 0.05, and with adaptive structured explanations: \u03b2 = 0.0584, p < 0.05).\n5.1.3 Mental Load. We found no significant differences in participants' mental load when deciding or understanding recommendations, however, participants rated their mental load as significantly lower when reading structured explanations (i.e., always-on structured: \u03b2 = -0.8182, p < 0.01, adaptive structured: \u03b2 = -0.5909, p < 0.05) than when reading unstructured explanations, which aligns with the time to action findings.\n5.1.4 Trust in Al. We found that both the always-on unstructured explanations (\u03b2 = 0.4091, p < 0.05) and always-on structured explanations (\u03b2 = 0.4545, p < 0.01) led to higher levels of trust when compared to the no explanation condition. There was no difference between the always-on unstructured, always-on structured, and adaptive structured explanation conditions.\n5.1.5 Satisfaction with Al. When presented with always-on unstructured and always-on structured explanations, participants were both more satisfied with the recommendations given by the AI compared to when no explanation was provided (always-on unstructured: \u03b2 = 0.4773, p < 0.05; always-on structured: f = 0.7045, p < 0.001). The use of adaptive structured explanations did not result in participants being more satisfied with the Al's recommendations compared to when no explanation was provided, and even led to significantly lower satisfaction compared to the always-on structured explanation condition (\u03b2 = -0.4318, p < 0.05).\n5.1.6 Understanding of Al. When participants were provided with an explanation, they had a better understanding of the rationale behind the AI recommendation than when they were not (e.g., always-on unstructured explanations: \u03b2 = 1.1136, p < 0.001, always-on structured explanations: \u03b2 = 1.0303, p < 0.001, always-on structured explanations: \u03b2 = 0.8258, p < 0.001). There was no significant difference between the always-on unstructured, always-on structured, and adaptive structured explanation conditions, suggesting that structuring and adaptively presenting the unstructured explanation did not negatively impact participants' understanding of the AI.\n5.1.7 Satisfaction with Al explanations. We also found that adaptive structured explanations were less satisfying compared to always-on unstructured explanations (\u03b2 = -0.5227, p <= 0.05). Though not statistically significant, always-on structured explanations were rated as marginally less satisfying than always-on unstructured explanations (\u03b2 = \u22120.5000, p = 0.053).\n5.1.8 Overall Preferences. In terms of participants' overall preferences for the four conditions, both the always-on unstructured explanations (\u03b2 = -1.1543, p < 0.01) and the adaptive structured explanations (\u03b2 = \u22120.9242, p < 0.05) were ranked higher than the no-explanation condition. However, the difference in ranks between the adaptive structured explanation condition and the no explanation condition was not statistically significant (\u03b2 = -0.6752, p = 0.074). Additionally, no significant difference was found between the three explanation conditions."}, {"title": "5.2 Qualitative Results", "content": "Seven themes were identified within the interview feedback. Some of the themes helped explain why participants preferred some conditions over others, whereas others speculated about other ways that explanations could be improved. Each theme is elaborated in the following sections.\n5.2.1 Usability. Usability referred to the amount of effort or time to interact with interface, i.e., whether the interface was easy and quick to use, or required more effort. Participants generally wanted to perform minimal actions and reading on the interface, with easy viewing and decision-making. For example, some participants noted that they would only want explanations when they had enough cognitive capacity, e.g., \"If it were detecting locations or the amount of activity I was doing, then I'd probably prefer the simpler interface. If I were driving or doing something active, it wouldn't be helpful for me to stop and read that explanation, so I'd dismiss it.\" \nTwenty participants valued the high usability of the no explanation condition, saying the interface was easy and fast to use, e.g., \"It allowed me to react fastest\" . In conditions with explanations, eight participants mentioned that the information provided by always-on structured explanations was easy to access, e.g., \"The explanations were automatically expanded\" . The adaptive structured explanations received mixed feedback. Fifteen participants found them easy and fast to use (e.g.,\"[They"}]}