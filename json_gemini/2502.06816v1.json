{"title": "DeepCell: Multiview Representation Learning for Post-Mapping Netlists", "authors": ["Zhengyuan Shi", "Chengyu Ma", "Ziyang Zheng", "Lingfeng Zhou", "Hongyang Pan", "Wentao Jiang", "Fan Yang", "Xiaoyan Yang", "Zhufei Chu", "Qiang Xu"], "abstract": "Representation learning for post-mapping (PM) netlists is a critical challenge in Electronic Design Automation (EDA), driven by the diverse and complex nature of modern circuit designs. Existing approaches focus on intermediate rep-resentations like And-Inverter Graphs (AIGs), limiting their applicability to post-synthesis stages. We introduce DeepCell, a multiview representation learning framework that integrates structural and functional insights from both PM netlists and AIGs to learn rich, generalizable embeddings. At its core, Deep-Cell employs the novel Mask Circuit Modeling (MCM) mecha-nism, which refines PM netlist representations in a self-supervised manner using pretrained AIG encoders. DeepCell sets a new benchmark in PM netlist representation, outperforming existing methods in predictive accuracy and reconstruction fidelity. To validate its efficacy, we apply DeepCell to functional Engineering Change Orders (ECO), achieving significant reductions in patch generation costs and runtime while improving patch quality.", "sections": [{"title": "I. INTRODUCTION", "content": "Representation learning has emerged as a powerful paradigm across domains such as computer vision (CV) and natural language processing (NLP), where pretrained models finetuned for specific tasks achieve state-of-the-art results. Inspired by these advancements, the field of Electronic De-sign Automation (EDA) has explored similar methodologies for circuit representation learning. Existing approaches, such as DeepGate Family [1], [2], FGNN [3], Gamora [4] and HOGA [5], demonstrate significant improvements in tasks like testability analysis, circuit identification, and design verifica-tion by focusing on And-Inverter Graphs (AIGs). However, this narrow reliance on AIGs limits their generalization to more complex and practical circuit abstractions.\nPost-mapping (PM) netlists, composed of diverse standard cells, represent a key stage in the design flow but remain underexplored in circuit representation learning. These netlists introduce challenges due to their structural and functional heterogeneity, which is difficult to be effectively captured by existing AIG-based methods. Current solutions, such as simulation-based supervised training [2] or contrastive learn-ing [3], struggle with scalability and efficiency in handling PM netlists. This gap hinders progress in critical post-mapping tasks like technology mapping and functional Engineering Change Orders (ECO).\nTo address these challenges, we propose DeepCell, a novel multiview representation learning framework for PM netlists. DeepCell integrates information from both PM netlists and AIGs using a Graph Neural Network (GNN)-based PM en-coder and a pretrained AIG encoder. At its core, DeepCell em-ploys Mask Circuit Modeling (MCM), a self-supervised mech-anism inspired by Masked Language Modeling (MLM) [6], which leverages AIG embeddings to refine PM netlist repre-sentations. By bridging the structural-functional gap inherent in PM netlists, DeepCell achieves rich and generalizable embeddings. We then validate DeepCell through its application to the functional ECO task, a critical post-mapping challenge involving design modification after tape-out [7]. Integrated as a plug-in to an existing ECO tool, DeepCell significantly reduces patch generation costs and runtime.\nOur contributions are summarized as follows:\n\u2022\n\u2022\n\u2022\nWe propose DeepCell, the first multiview and multi-modal representation learning framework tailored for PM netlists, integrating structural and functional insights from diverse standard cells.\nWe introduce Mask Circuit Modeling (MCM), a self-supervised mechanism for refining PM netlist embed-dings using AIG-based representations.\nWe demonstrate the utility of DeepCell in functional ECO, achieving reductions in patch generation costs, gate count, and runtime while maintaining high-quality results.\nThe remainder of this paper is organized as follows: Section II reviews related work. Section III describes the proposed DeepCell framework, including architecture and Mask Circuit Modeling mechanism. Section IV presents the pretraining results and investigate the effect of proposed training strategy. Next, we apply DeepCell in functional ECO tasks. Finally Section VI concludes this paper."}, {"title": "II. RELATED WORK", "content": "Circuit representation learning has emerged as an attractive direction in the field of EDA, focusing on training models to obtain general circuit embeddings that can be applied to various downstream tasks [8]. The first circuit representa-tion learning framework, DeepGate [1], proposes supervis-ing model training using logic-1 probability under random simulation and achieves substantial improvements in tasks like testability analysis [9] and power estimation [10]. Its successor, DeepGate2 [2], further refines this approach by separating functional and structural embeddings for different applications. Additionally, Gamora [4] and HOGA [5] leverage sub-circuit identification as pre-training tasks, while FGNN [3] trains models in an unsupervised contrastive manner by distinguishing between equivalent and non-equivalent circuits. Despite these advancements, existing models primarily focus on learning representations of AIG netlists. There is still no available solution capable of learning post-mapping netlists with arbitrary logic cells and addressing the practical applica-tions on post-mapping stages.\nWhile these advancements primarily address representation learning for intermediate formats such as AIGs, the challenges posed by post-mapping netlists remain largely unexplored. Functional ECO represents one such critical post-mapping application, where effective representation learning could sig-nificantly enhance performance and efficiency.\nECO are a critical component in the VLSI design pro-cess, used to rectify design problems after tape-out or chip fabrication. ECO involve making modifications to correct these errors, and they are indispensable in avoiding the high expenses associated with design re-spin [11]. For functional ECO, the purpose is to generate patch so that the original circuit is equal to the golden circuit, while minimizing the resource cost of the generated patches and making the running time as short as possible. Synthesis-based ECO algorithms are good at solving this problem [7]. It relies on a diag-nostic strategy to identify internal rectifier signals, and then applies a resynthesis technique to generate patch functions for functional differences. These algorithms have been able to automate the process of functional ECO."}, {"title": "III. METHODOLOGY", "content": "Fig. 1 presents the overview of DeepCell framework, which consists of an AIG encoder, a PM encoder and a Transformer model. The framework operates in two stages to capture the general representation of post-mapping (PM) netlists. In Stage 1, the PM encoder is trained to learn cell embeddings (Cell Emb.) from the PM netlist. In Stage 2, the AIG encoder, which has been pretained and frozen, is utilized to extract gate embeddings (Gate Emb.) from the corresponding AIG netlist. Then, we mask a random subset of the cell embeddings and reconstruct these masked embeddings using the gate embeddings.\nGiven a PM netlist, we convert it into graph format $G_P = (V_P, E_P)$, where each standard cell is represented as node and each wire is treated as edge on the graph.\n1) Node Features: PM netlists consist of a wide variety of standard cells, making it impractical to represent them using one-hot encoding for each type of cell. Therefore, To address this, we embed the truth table of each standard cell into its corresponding node feature $x_i$. Formally, the node feature encoding is defined in in Eq. (1), where D is the dimension of node feature vector and $t t_i$ represents the 0/1 truth table vector of standard cell i. The truth table $t t_i$ is repeated until the node feature vector reaches the specified dimension D. In the default setting, we assign D = 64, ensuring that this encoding mechanism is adaptable to PM netlists across various technology libraries and supports arbitrary logic units with up to 6 inputs.\n$x_i = repeat(tt_i, D)$ (1)\nFor example, the cell xor2_1 defines XOR functionality with 2 inputs and 1 output. Its truth table, 0110, is ex-tracted from the technology library and expanded into a 64-dimensional feature vector by repeating the pattern. Thus, the node feature of the xor2_1 cell becomes 0110 0110 0110.\n2) Aggregator: We introduce a DAG-based GNN to encode circuit graph into embedding vectors $H_P$. For each cell $i \\in V_P$, its representation vector is denoted as $H_i = \\{h_i^s, h_i^f\\}$, where $h_i^s$ and $h_i^f$ are the structural and functional embed-dings, $H_i \\in H_P$. To compute these embeddings, we propose two aggregators: $aggr_s$ for structural message aggregation and $aggr_f$ for functional message aggregation.\nFor structural embedding aggregation, $aggr_s$ is imple-mented using the GCN aggregator [12], which aggregates messages from the predecessors of i. Here, P(i) denotes the set of fanin cells of i:\n$h_i^s = aggr(\\left\\{h_j^s | j \\in P(i)\\right\\})$ (2)\nFor functional embedding aggregation, $aggr_f$ is imple-mented using a self-attention aggregator [13] to distinguish the functionality of the predecessors. Unlike AIG netlists, which consist solely of AND gates and inverters, PM netlists contain diverse standard cells. To account for this diversity, we differentiate cells using their node features $x_i$ and introduce an update function, update. Formally, the functional aggregation process in DeepCell is defined as Eq. (3), with the update function implemented as a multi-layer perceptron (MLP).\n$msg_i = aggr_f(\\left\\{cat(h_j^s, h_j^f) | j \\in P(i)\\right\\})$\n$h_i^f = update(msg_i, x_i)$ (3)\nFinally, the embeddings of PM netlist are denoted as Eq. (4), where encoder is the above GNN-based PM encoder.\n$H_P = encoder(G_P)$\n$H_i \\in H_P, \\forall i \\in V_P$ (4)\nOur DeepCell framework incorporates a multiview repre-sentation learning mechanism, enabling it to learn cell embed-dings in PM netlists from an additional perspective provided by AIGs. Specifically, given a PM netlist $G_P = (V_P, E_P)$, we convert it into an AIG netlist $G_A = (V_A,E_A)$. We then employ DeepGate2 [2] as the AIG encoder to derive the gate-level embeddings $H_A \\in H_A$ as formulated in Eq. (5).\n$H_A = DeepGate2(G_A)$\n$H_A \\in H_A, \\forall J \\in V_A$ (5)\nWe refine the cell embeddings $H_P$ using information from the AIG view by mask circuit modeling (MCM). As shown in Fig. 2, given a PM netlist $G_P$, we convert $G_P$ to AIG Netlist $G_A$. Both $G_P$ and $G_A$ are encoded by PM encoder and AIG encoder in Eq. (4) and Eq. (5), respectively.\nTo apply MCM, we randomly select node $p \\in G_P$ and extract a k-hop predecessors around node p, denoted as the masked hop $M(p)$. Within the masked hop $M(p)$, we replace the functional embeddings $h_i^f$ of the nodes with a learnable masked token $h_m$, while preserving their structural embeddings $h_i^s$.\nTherefore, we totally have M + N tokens (see Eq. (6)), where M = |VP| and N = |VA|. All the selected masked nodes p is in set S.\n$H_i = \\{h_i^s, h_i^f\\}, i \\in V_P, i \\notin M(p)$\n$H_i = \\{h_i^s, h_m\\}, i \\in V_P, i \\in M(p)$\n$H_j = \\{h_j^s, h_j^f\\}, j \\in V_A$ (6)\nWe process these M + N tokens using a Transformer model T. Formally, we define the input and output of the Transformer model as below. The training objective is to reconstruct the masked area using the remaining cell embeddings and the learned gate embeddings from AIG view.\n$\\left\\{H_A', H_P'\\right\\} = T(\\left\\{H_A, H_P\\right\\})$ (7)\nTo train DeepCell, we employ a two-stage training strategy. As illustrated in Fig. 1, in the first stage, we perform random simulation with 15,000 patterns on PM netlist and record the logic-1 probability $prob_i, i \\in V_P$. Next, we use a simple 3-layer MLP to readout the functional embeddings as the predicted probability. The training loss for this stage is defined in Eq. (8).\n$prob_i = MLP(h_i^f), i \\in V_P$\n$loss_{prob} = L1Loss(prob_i, prob_i)$ (8)\nIn the second stage, we refine the cell embeddings by incorporating representations from the AIG view. We utilize a pretained and frozen AIG encoder to provide rich contextual information from the AIG. The training objective in this stage is to reconstruct the functionality of masked cells using information from their neighboring cells and the global AIG perspective. Accordingly, we define the MCM training loss in Eq. (9), where the model is trained to recover the functional embeddings of the masked cells.\n$loss_{mcm} = L1Loss(H_P', H_P)$ (9)\nConsequently, the overall loss functions for both stages are defined as follows, where $W_{prob}$ and $W_{mcm}$ present the weights"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we train our DeepCell to learn the general representations of standard cells in the post-mapping netlists.\n1) Data Preparation: We collect the open-source verilog designs from GitHub and the open-source circuit bench-marks [14]\u2013[16]. All circuits are designed with Skywa-ter 130nm as the target process library [17], using com-mercial logic synthesis tool Design Compiler command compile_ultra to obtain the corresponding PM netlist. To accelerate model pretraining, we randomly extract the sub-circuits within 5,000 cells. Then, we use ABC strash command to obtain the corresponding AIGs. Consequently, there are totally 83,155 PM netlists and corresponding AIGs. We split the dataset into 80% for training and 20% for testing.\n2) Evaluation Metrics: In the following experiments, we evaluate model performance in predicting the logic-1 prob-ability under random simulation, a widely used metric for assessing circuit representation ability [1], [18]. We calculate the average prediction error (PE) as Eq. (11).\n$PE = \\frac{1}{|V_P|} \\sum_{i \\in V_P} |prob_i - prob_i|$ (11)\nAdditionally, we assess model performance on mask circuit modeling tasks, where the average reconstruction error (RE) is defined as Eq. (12), where all the select nodes p are in set S and the masked hop is M(p).\n$RE = \\frac{1}{\\sum_{p \\in S} |M(p)|} \\sum_{i \\in M(p)} ||h_i^f - h_i^f||, p \\in S$ (12)\n3) Model Implementation and Training: The Transformer model used to refine cell embeddings consists of 4 Transformer blocks, each with 8 attention heads. After encoding, each cell is represented by a 128-dimensional structural embedding and a 128-dimensional functional embedding. DeepCell is pretrained for 60 epochs in Stage 1 and an additional 60 epochs in Stage 2 to ensure convergence. The pretraining process is conducted with a batch size of 128 using 8 Nvidia A800 GPUs. We employ the Adam optimizer with a learning rate of 10-4.\n1) Effect of Mask Modeling: We investigate the optimal settings for Mask Circuit Modeling (MCM) by exploring its two key hyperparameters: the number of selected nodes to be masked, |S|, and the size of the masked hop, k. To evaluate performance, we pretrain the model using various hyperparameter combinations, where |S| = $\\theta |V_P|$ and $\\theta$ = 1%, 5%, 10%, 20% of total nodes in PM netlist, with hop sizes of k = 4 or k = 6."}, {"title": "V. DOWNSTREAM TASK: FUNCTIONAL ECO", "content": "In this section, we combine our DeepCell with the open-source EDA tools and apply our model to practical EDA tasks: functional ECO. Our DeepCell provides probability for nodes in the original circuit and provides guidance for ECO to find candidate patch signals. This method effectively leaves higher quality and more likely to be a patch signal, making the ECO solution more rapid and higher quality.\nIn order to clarify the role of the model in the algorithm, we first briefly describe the method in [19]. In the original SAT-based solution, for the POs reachable from the target, the method simultaneously identifies the PIs contained in these POs in the original netlist and the target netlist. Then find all signals in the original circuit, which are not in the transitive fanout cone of the targets and whose support is contained in the calculated set of PIs. These candidate signals are sorted by cost in ascending order and a fixed number of nodes (default is the top 5,000) are selected. Then used the SAT-based solution for the single-target ECO problem proposed in [20] to prove whether there is a solution.\nAfter adding a large number of candidate signals, it is very expensive to use the positive and negative remainders of target to construct the miter circuit and solve it, which is also the reason why the number of candidate signals cannot be selected in large quantities. It is necessary to reduce the number of candidate signals at the outset.\nDeepCell is pretrained to capture the correlation between masked PM netlist and complete AIG. Intuitively, the original circuit that requires patch insertion in ECO task is treated as PM netlist with masking, while the golden circuit provide the complete view. We then finetune DeepCell to effectively learn and represent the relationship between these two circuits.\nTherefore, we finetune DeepCell to reduce the space of candidate signals in the ECO process. Specifically, given a PM netlist GP, we randomly select a cell p within the netlist and remove its entire driven cone from this node to PI. The removed area, M(p), is treated as the ground-truth path in ECO. Consequently, the patch-removed netlist is denoted as $G_P'$, while the corresponding AIG netlist for the original PM netlist is denoted as GA. Next, we use the PM encoder to encode both patch-removed netlist $G_P'$ and the original netlist GP, as denoted in Eq. (13). The AIG encode is employed to encode the AIG netlist GA, which serves as the reference ground-truth implementation. The embeddings of the patch-removed netlist are then refined using the AIG embeddings, as described in Eq.(7), to produce the refined embeddings $H_P'$.\n$H_A = DeepGate2(G_A)$\n$H_P = encoder(G_P)$\n$H_P' = encoder(G_P')$\n$\\left\\{H_A', H_P'\\right\\} = T(\\left\\{H_A,H_P'\\right\\})$ (13)\nThe finetuning tasks are defined as patch reconstruction and driven signal identification. First, in the ideal scenario, the finetuned model should recover the embeddings of the patch with the correct functionality, as informed by the AIG view. We treat the embeddings of the original netlist (prior to patch removal) as the ground truth and define the patch reconstruction loss, $loss_{pr}$ as specified in Eq. (14) (see Fig. 3). We finetune DeepCell using only 8,000 circuits over 10 epochs with loss function L = losspr + lossdrv.\n$loss_{pr} = L1Loss(H_i', H_i'), i \\in M(p)$ (14)\nSecond, in the driven signal identification task, we utilize the refined embeddings to predict whether a given cell is driven by the selected cell p.\n$loss_{drv} = BCELoss(MLP(\\left\\{H_i^P, H_i^{P'}\\right\\}), 0/1), i \\in V^P$ (15)"}, {"title": "VI. CONCLUSION", "content": "We introduced DeepCell, a multiview representation learn-ing framework for PM netlists. By integrating a GNN-based PM encoder with a pretrained AIG encoder, and leveraging the novel Mask Circuit Modeling mechanism, DeepCell captures rich and generalizable embeddings of PM netlists. Our ex-perimental results demonstrate the effectiveness of DeepCell in functional ECO, achieving significant reductions in patch generation costs, gate count, and runtime. Future work will explore its adaptability to other critical EDA tasks and extend its scalability to industrial-scale designs."}]}