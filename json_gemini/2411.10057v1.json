{"title": "KuaiFormer: Transformer-Based Retrieval at Kuaishou", "authors": ["Chi Liu", "Jiangxia Cao", "Rui Huang", "Kai Zheng", "Qiang Luo", "Kun Gai", "Guorui Zhou"], "abstract": "In large-scale content recommendation systems, retrieval serves as the initial stage in the pipeline, responsible for selecting thousands of candidate items from billions of options to pass on to ranking modules. Traditionally, the dominant retrieval method has been Embedding-Based Retrieval (EBR) using a Deep Neural Network (DNN) dual-tower structure. However, applying transformer in retrieval tasks has been the focus of recent research, though real-world industrial deployment still presents significant challenges. In this paper, we introduce KuaiFormer, a novel transformer-based retrieval framework deployed in a large-scale content recommendation system. KuaiFormer fundamentally redefines the retrieval process by shifting from conventional score estimation tasks (such as click-through rate estimate) to a transformer-driven Next Action Prediction paradigm. This shift enables more effective real-time interest acquisition and multi-interest extraction, significantly enhancing retrieval performance. KuaiFormer has been successfully integrated into Kuaishou App's short-video recommendation system since May 2024, serving over 400 million daily active users and resulting in a marked increase in average daily usage time of Kuaishou users. We provide insights into both the technical and business aspects of deploying transformer in large-scale recommendation systems, addressing practical challenges encountered during industrial implementation. Our findings offer valuable guidance for engineers and researchers aiming to leverage transformer models to optimize large-scale content recommendation systems.", "sections": [{"title": "1 INTRODUCTION", "content": "The Transformer [32] architecture has demonstrated significant success across multiple domains, with notable models such as BERT [7] and GPT [1, 3] in natural language processing (NLP), and Vision Transformers [2, 19, 23] in computer vision (CV). These achievements underscore the Transformer's remarkable capabilities in sequence modeling and parallelization. In the field of recommendation systems, Transformer-based architectures like SASRec [14] and Bert4Rec [29] have also shown potential. However, these academic efforts often fail to address certain industrial challenges, which has limited their effectiveness in driving business success in large-scale recommendation systems, such as those at Kuaishou.\nShort-video recommendation poses unique challenges that demand advanced modeling techniques. The diverse nature of short-video content and the rapid evolution of user interests necessitate real-time adaptation to accurately capture these dynamic preferences. Users typically watch hundreds of short-videos each day, expressing preferences across a wide range of interest domains, while the system actively pushes diverse content to mitigate aesthetic fatigue and avoid the \"filter bubble\" effect. As a result, models that rely on daily updates, such as PinnerFormer [25], struggle to adapt to users' evolving content needs. Moreover, traditional approaches like SASRec and Bert4Rec, which compress user behavior into a single interest vector, lack the capacity to accurately capture the full spectrum of user interests reflected in these interactions.\nTo more effectively capture complex user interests, models like MIND [18] and ComiRec [4] employ techniques such as capsule networks to extract multiple interest vectors from user action sequences. However, because these models do not utilize native Transformer-based architectures, they are limited in fully leveraging the advantages offered by Transformers. Furthermore, they do not address the performance overhead associated with processing long sequences, which is a significant concern in industrial applications.\nTo effectively implement the Transformer model within Kuaishou's large-scale short video recommendation system, this study conducts a detailed analysis of these specific challenges in the recommendation field and proposes a series of concise and effective solutions tailored to address these issues.\n\u2022 How to train with billion-scale item set:\nLarge language models (LLMs) typically leverage next token prediction as the pretraining task. This involves calculating the probabilities of all tokens in the vocabulary being the next token, based on the historical sequence, and selecting the one with the highest probability as the next predicted token. General-purpose language models usually contain fewer than 100,000 tokens in their vocabularies [30]. In the context of the Kuaishou short video recommendation system, the candidate pool contains"}, {"title": "2 METHODOLOGY", "content": "This section describes how KuaiFormer works in our recommendation system. We first briefly explain the problem definition of our KuaiFormer in training and inference. Then, we present KuaiFormer base workflow including the feature engineering and model architecture. Next, we show the details how to model longer sequence and capture user's multiple interests. Finally, we give our loss function to achieve a stable training in billion-scale item set."}, {"title": "2.1 Problem Statement", "content": "In a general recommendation task, we leverage users' historical watched items to model their interests and predict the next video they are likely to engage with.\nFor each user, let $\\{(x_1, f_1), (x_2, f_2), ..., (x_n, f_n) \\}$ represents his/her recent positive watched short-video information in chronological order, where the $x_i \\in \\mathbb{R}$ denotes to a short-video ID from entire short-video set X, the $f_i \\in \\mathbb{R}^{|F|}$ denotes the watching side-information according to attribute set F. Generally, we hand-craft about 10+ short-video watching attributes for interests modeling, including: watching time, interaction labels (e.g., like, comment), short-video duration, multi-modal category tag ID and so on.\nAccording to the sequence $\\{(x_1, f_1), (x_2, f_2), . . ., (x_n, f_n)\\}$, in training procedure, since we know the next item $x_{n+1}$, thus we could"}, {"title": "2.2 Base Backbone", "content": "In this section, we introduce the base backbone of our KuaiFormer, which contains short-video embedding module and a stacked Transformer module."}, {"title": "2.2.1 Embedding Module", "content": "In embedding mapping stage, we utilize two type embedding layers to consider different type of attributes to form as model input:\n\u2022 One-hot discrete attributes embedding layer: As the most common type of embedding layer, it is mainly used to model discrete features, including short-video ID, tag ID, interaction labels, etc. Take the short-video ID mapping stage as example, we assign a parameter matrices $X \\in \\mathbb{R}^{|X| \\times d}$ to store their embeddings, where d is the embedding dimension. Thereby we can easily obtain the corresponding embedding $x_i \\in \\mathbb{R}^d$ by a straightforward lookup operation for arbitrary short-video ID $x_i$:\n$x_i = LookUp(X, x_i),$ \t\t(3)\n\u2022 Bucket continuous attributes embedding layer: Instead of the discrete attributes, this layer aims to embed the continuous attributes, including watching time, short-video duration, etc. For these continuous attributes, since their prior distributions are different, we actually customize each attribute and design a special bucketing strategy to discretize them while retaining the distribution characteristics. Take the short-video duration $f_{dura}$ as example, we utilize a uniform bucket strategy to divide it in 1000 buckets with maximize 300s, and the lookup its embedding from parameter matrix $F_{duration} \\in \\mathbb{R}^{1000 \\times d}$.\n$f^{bucket}_{dura} =int(\\frac{min(f_{dura}, 300)}{300}* 1000)$ \t\t(4)\n$f_{dura} =LookUp(F_{dura}, f^{bucket}_{dura})$\nwhere the $f^{bucket}_{dura}$ is a integer denotes the mapping discrete ID index for continuous attribute $f_{dura}$."}, {"title": "2.2.2 Transformer Module", "content": "On top of the above embedding module, we first utilize them to form the watched short-video sequence information. For each information $(x_i, f_i)$, we can map them as:\n$t_i = MLP([x_i, f_i])$ \t\t(5)"}, {"title": "2.3 Towards Longer Sequence", "content": "Actually, the generated representation u could support the training stage (in Eq.1) and inference stage (in Eq.2).\nHowever, we need to handle a large volume of requests within just a few milliseconds. At peak times, this means managing over 100,000 requests per second. Thus the straightforward way is hard to scale to a longer input sequence to provide a high-quality representation for training and inference. In our experiments, we find, when the sequence length n is extended from 64 to 256, the corresponding computing resources increases by 6 times, which is unacceptable. Therefore, a challenging problem is how to model a longer sequences with fewer computation resources? For users, the most recent videos leave a stronger impression, while older browsing history is relatively vague. This observation motivates us to compress the earlier watched short-videos to reduce the sequence length. In KuaiFormer, we devise a simple-yet-effective adaptive item compression mechanism in three steps:\n(1) We first divide the input sequence into three parts according to their position as earlier part, middle part, and latest part. For the earlier part and middle part, we merge 64 adjacent items and 16 adjacent items as one group, respectively.\n(2) For the earlier/middle item groups, we then utilize a single-layer bidirectional Transformer without mask strategy to aggregate them as a grouped item representation. Take the earlier item group as example, such process can be formulated as:\n$t^{early} = Mean(Bi\\_Transformer(\\{t_1,...,t_{64} \\}, M)$ \n$t^{early} = Mean(Bi\\_Transformer(\\{t_{65},...,t_{128}\\}, M)$\n$t^{mid} = Mean(Bi\\_Transformer(\\{t_{129},..., t_{145} \\}, M)$ (7)\n$t^{mid} = Mean(Bi\\_Transformer(\\{t_{193},..., t_{208}\\}, M)$\nSimilarly, the calculation also holds for middle item groups, and leads to the results $t^{mid}$"}, {"title": "2.4 Towards Multiple Interests", "content": "Additionally, in our short-video services, users always have multiple interest points, therefore utilizing a single representation is hard to express users' complex real-time dynamic interests. For sequence modeling, capturing the diverse interests embedded within the sequence becomes increasingly important as the sequence length grows. With KuaiFormer, we also explore a similar challenge: how can we model users' multi-interests within the Transformer paradigm?\nMotivates by the great success of special '[CLS]' token of BERT, which introduces a learnable token to summarize input sequence information for downstream tasks. We consider employ k different special tokens act as queries token to extract different user interests.\n$\\{u_1,..., u_k\\} = Causal\\_Transformer(\\{t^{early},t^{early}\\}$ \n$\\{t^{mid},....t^{mid}\\}$\n$\\{t_{209},..., t_{256}\\}$\n$\\{q_1,..., q_k\\}, L, M)$ \t\t(9)"}, {"title": "2.5 Towards Stable Training", "content": "Up to now, we have introduced the learning process to generate multiple user interests representations with a longer sequence. In this section, we will dive into the loss function designing to answer the most challenging problem: How to train a Transformer-based model with billion-scale item set.\nGenerally, the Transformer-based model always trained with softmax function, to maximize the next positive token probability while minimize all other token probabilities. However, in recommendation task, it is impractical to train our model in full billion-scale item directly, thus we first employ the in-batch softmax as"}, {"title": "3 DEPLOYMENT ARCHITECTURE", "content": "In this section, we will present the comprehensive deployment architecture of KuaiFormer in an industrial streaming video recommender system, which serves the largest Kuaishou's recommendation scenario in the retrieval stage. KuaiFormer continuously receives online data for training and updates its parameters to the online model service at a minute-level frequency.\nAs shown in Figure 2, the KuaiFormer retrieval model is trained on an industrial curated distributed training framework in an online learning way. The short video content recommendation system responds to requests from hundreds of millions of users daily. User requests first pass through the retrieval system, which is composed of multiple independent retrieval pathways, such as the classic Swing[34], GNN[9], Comirec[4], Dimerec[21], GPRP [36], etc. KuaiFormer is introduced as a new retrieval pathway into the retrieval system. The retrieval candidates from all retrieval pathways are aggregated and deduplicated before being sent to the ranking system. In our system, the initial coarse-grained ranking is performed through a pre-rank stage, then a cascading rank stage, followed by a fine-grained full rank stage to obtain the final set of short videos presented to the user. The offline logging system records real-time user feedback, including dense feedback signals such as watch time and relatively sparse interactions (likes, follows, shares). All offline logs are processed into training records and aggregated for transmission to the offline training framework. To enhance system efficiency, we use a dedicated embedding server to store sparse"}, {"title": "4 EXPERIMENTS", "content": "In this section, we give detailed analyses to answer the following major research questions (RQs):\n(1) RQ1: Does our KuaiFormer achieves SOTA offline performance compared with strong retrieval methods?\n(2) RQ2: Does our KuaiFormer contributes online gains in our Short-Video service significantly?\n(3) RQ3: How does different hyper-parameter settings influence KuaiFormer performance?"}, {"title": "4.1 Experimental Setting", "content": "4.1.1 Dataset. We conduct experiments at our short-video data-streaming, which is the largest recommendation scenario at Kuaishou, including over 400 Million users and 50 Billion logs every day.\n4.1.2 Evaluation Protocol. For the performance estimation, we apply two metrics to compare with baselines and our model variants, the online Hit Rate and offline Accuracy.\n\u2022 Online Hit Rate: For a fair comparison with different methods, we utilize the users' online requests results to estimate different model ability. Specifically, we calculate the hit rate between the real user viewed items whether they are retrieved by the corresponding models, e.g., does the viewed item is retrieved in the top 50/100/500/100 results.\n\u2022 Offline Accuracy: For evaluating the performance of our KuaiFormer variants, since the models are trained in an in-batch setting and"}, {"title": "4.2 Performance Comparisons (RQ1)", "content": "To accurately assess the offline coverage rate of the retrieval model, we replay real online requests and invoke these models, each returning 50-1000 results. The Hit rate between the model recommendations and the actual content viewed by users is shown in Table 1. According it, we have the following observations: (1) The classic sequence modeling approach, SASRec, performs poorly in terms of hit rate, suggesting that the multi-interest module in KuaiFormer contributes to covering a broader range of user interests. (2) Compared with the famous multi-interests method ComiRec [4], our KuaiFormer shows consistent improvement in all metrics, which validates that utilizing the Transformer as base encoder to extract user preference is effective. (3) Since GPRP is trained using recommendation funnel data from retrieval to ranking stages, it achieves a higher hit rate than other traditional methods with a retrieval number range of 100 to 1000. However, KuaiFormer improves hit rate by over 25% compared to GPRP. This indicates that the Next"}, {"title": "4.3 Online A/B Test (RQ2)", "content": "To measure the precise improvements of KuaiFormer contribution to our short-video service, we conduct comprehensive one week A/B testing with 10% of users in the three largest scenarios on Kuaishou: Kuaishou Single/Double page and Kuaishou Lite Edition Single page. In Table 2, we shown the watching-time and interaction metrics of our KuaiFormer. Due to varying priorities across different scenarios, certain metrics are omitted on the Double Page. However, the most critical video watch time metric has been preserved to ensure essential insights remain accessible. Notably, the about 0.1% improvement in Video Watch Time is a statistically significant change to give satisfactory contribution, since our platform has 400 Million active users per day. According Table 2, our KuaiFormer achieves +0.360%, +0.126%, +0.411% improvements in terms of the video watch time under the three largest scenarios, which is one of the most significant retrieval experiments at Kuaishou in 2024. KuaiFormer has also demonstrated significant improvements in engagement metrics, such as increased Likes, Follows, and Comments. This indicates that KuaiFormer effectively adapts to evolving user needs, accurately captures user interests, and recommends relevant content, thereby enhancing user engagement and satisfaction. Additionally, the Novel Surprise metric has improved, which measures the model's ability to help users discover new interests. This suggests that the multi-interest mechanism can capture both mainstream and niche interests, introducing users to new areas of interest."}, {"title": "4.4 Hyperparameter Impact (RQ3)", "content": "This section explores four hyper-parameter effects in KuaiFomer: the sequence length, query token number, layer number and item compression mechanism."}, {"title": "4.4.1 Sequence Length Impact", "content": "As seen in the table 3 (a), increasing the sequence length from 8 to 256 steadily improves the accuracy from 0.325 to 0.441. This suggests that increasing the sequence length allows the model to capture more context, which improves"}, {"title": "4.4.2 Query token number's impact", "content": "The data in Table 3 (b) reveals a clear relationship between the number of query tokens and the model's accuracy. As the number of tokens increases, the accuracy improves consistently, suggesting that additional tokens provide the model with more context, enabling better predictions. However, the rate of improvement decreases as the token number rises, indicating diminishing returns. The gain in accuracy becomes smaller with each additional token, implying that the model may reach a saturation point beyond which further tokens contribute little to overall performance. For instance, the increase in accuracy from token 1 to token 2 is 0.006, while the increase from token 6 to token 7 is only 0.001, demonstrating that after a certain number of tokens, the benefit of adding more becomes minimal.\nSince we retrieve a certain number of videos for each query token and then uniformly rank and truncate them to a fixed number, an excessive number of query tokens will result in fewer videos being retrieved per token. Additionally, too many query tokens will increase computational resource consumption, so the number of query tokens cannot be increased indefinitely. In this experiment, 6 tokens appear to represent an optimal balance, where accuracy gains begin to level off, making it unnecessary to further increase the number of tokens for substantial improvements."}, {"title": "4.4.3 Layer number's impact", "content": "Table 3 (c) demonstrates the impact of increasing model layers on accuracy, while holding the sequence length and query token number constant. The results show a consistent improvement in accuracy as the number of layers increases from 1 to 5.\nThe increase in accuracy from Layer 1 (0.391) to Layer 2 (0.419) represents a substantial gain, suggesting that additional layers contribute to more refined feature extraction and better model performance. However, similar to the effect observed with query tokens, the improvement diminishes with each additional layer. For instance, the increase from Layer 4 (0.433) to Layer 5 (0.435) is only 0.002, indicating that after a certain depth, the model reaches a"}, {"title": "4.4.4 Item compression strategy impact", "content": "Table 3 compares the performance of models with varying sequence lengths and compression strategies. Specifically, we explored the impact of compressing item sequences on model accuracy.\nFor the compression approach, we devised a method where a sequence of 256 items is progressively compressed with compression window sizes of 64, 64, 16, 16, 16, 16, and 16 items, starting from the oldest to the most recent. The latest 48 items are appended to this sequence without compression, and the resulting sequence is then fed into the model. The query token count is fixed at 1, and the model architecture comprises a 4-layer transformer.\nOur findings indicate that the compressed sequence outperforms the model with a sequence length of 64 and, notably, achieves slightly better accuracy than the model with the full uncompressed sequence of 256 items. This suggests that the compression strategy effectively reduces noise in the sequence, contributing to improved model performance."}, {"title": "5 RELATED WORKS", "content": "5.1 Sequential Recommendation\nIn recommender systems, the recommendation process is typically divided into two key stages: retrieval and ranking. The retrieval stage aims to efficiently retrieve a subset of items from a large candidate pool that a user might be interested in. Due to computational constraints, retrieval models are often designed with lighter architectures, frequently employing dual-tower structures where separate towers model users and items, interacting only at the top layer [6, 11]. In contrast, the ranking stage scores the retrieved subset, allowing for the application of more complex models and intricate feature interactions [5, 26, 37, 38]. The focus of this work is to design more effective retrieval models.\nHistorically, collaborative filtering methods have been widely used in recommendation tasks, including item-based CF, user-based CF, and factorization machines [10, 16, 17]. With advancements in deep learning, models such as DNN and Embedding-Based Retrieval"}, {"title": "5.2 Multi-Interest User Representation", "content": "In recommendation systems, users often exhibit diverse interests, necessitating the provision of varied content to meet their multi-faceted preferences. Traditional models typically represent a user's interest with a single vector, which may not adequately capture the complexity of user behavior. To address this limitation, several approaches have been developed to model multiple user interests, particularly during the retrieval phase. The MIND model assigns a fixed number of vectors to each user, representing preferences for different content types [18]. It employs a dynamic routing mechanism from capsule networks to extract multiple interest representations from user behavior sequences, thereby capturing diverse user interests more effectively. Building upon MIND, the ComiRec model explores methods to integrate multi-interest retrieval results [4]. ComiRec introduces a controllable aggregation module designed to balance recommendation accuracy and diversity. Additionally, it incorporates multi-interest extraction modules based on dynamic routing and self-attention mechanisms to better capture users' multiple interests. The MIP utilizes a time-aware self-attention mechanism to extract multiple interest representations from user behavior sequences [27]. By incorporating temporal information, MIP more accurately captures the dynamic evolution of user interests, thereby enhancing recommendation performance."}, {"title": "6 CONCLUSION", "content": "KuaiFormer demonstrates the feasibility and effectiveness of a Transformer-based architecture for large-scale retrieval tasks in a short-video recommendation system. By leveraging multi-interest extraction, adaptive sequence compression, and stable training techniques, KuaiFormer effectively captures the complex, dynamic interests of users and scales efficiently across billions of requests. Our extensive offline and online evaluations confirm significant improvements in both offline hit rate and online ab test, validating KuaiFormer's ability to enhance user satisfaction and business performance. This work underscores the transformative potential of advanced neural architectures in industrial recommendation systems, offering a scalable framework that can inspire further innovations in content retrieval and recommendation."}]}