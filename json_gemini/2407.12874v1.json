{"title": "SELF-GUIDE: Better Task-Specific Instruction Following via Self-Synthetic Finetuning", "authors": ["Chenyang Zhao", "Xueying Jia", "Vijay Viswanathan", "Tongshuang Wu", "Graham Neubig"], "abstract": "Large language models (LLMs) hold the promise of solving diverse tasks\nwhen provided with appropriate natural language prompts. However,\nprompting often leads models to make predictions with lower accuracy\ncompared to finetuning a model with ample training data. On the other\nhand, while finetuning LLMs on task-specific data generally improves\ntheir performance, abundant annotated datasets are not available for all\ntasks. Previous work has explored generating task-specific data from state-\nof-the-art LLMs and using this data to finetune smaller models, but this\napproach requires access to a language model other than the one being\ntrained, which introduces cost, scalability challenges, and legal hurdles\nassociated with continuously relying on more powerful LLMs. In response\nto these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then\nuse these input-output pairs to finetune the student LLM itself. In our\nempirical evaluation of the Natural Instructions V2 benchmark, we find that\nSELF-GUIDE improves the performance of LLM by a substantial margin.\nSpecifically, we report an absolute improvement of approximately 15%\nfor classification tasks and 18% for generation tasks in the benchmark's\nmetrics. This sheds light on the promise of self-synthesized data guiding\nLLMs towards becoming task-specific experts without any external learning\nsignals.", "sections": [{"title": "1 Introduction", "content": "One of the traits of large language models (LLMs) that has captured the imagination of\nmodel developers is their potential to automate a broad range of highly complex tasks with\nrelative ease (Brown et al., 2020a). Where previous generations of models required vast\ntask-specific training sets, LLMs now offer the promise of enabling similar accuracy by\nsimply writing a prompt and supplying a few examples. However, this may be a false promise.\nPrompting typically leads models to make predictions with lower accuracy compared to\nfinetuning a model with ample training data (Gao et al., 2021; Zhang et al., 2023). Moreover,\nLLMs' performance crucially depends on their ability to follow instructions outlined in the\nprompts and even minor alterations to these prompts can result in a notable performance\ndecline (Sclar et al., 2023a).\nOn the other hand, in data-abundant tasks, finetuning a pre-trained language model through\nsupervised finetuning (Ouyang et al., 2022; Wei et al., 2022) and reinforcement learning\nfrom human feedback (Lambert et al., 2022) has proven a successful strategy. However, the\neffectiveness of this approach diminishes notably for underrepresented tasks suffering from\ndata scarcity (Li et al., 2022), highlighting the critical need for high-quality training data.\nTo this end, recent studies have explored the potential of utilizing potent \u201cteacher\u201d LLMs\nto create task-specific training data, thereby enhancing the performance of comparatively"}, {"title": "2 SELF-GUIDE", "content": "In the following section, we outline the proposed SELF-GUIDE framework (illustrated in\nFigure 2 and Figure 3). As input, SELF-GUIDE takes an instruction (e.g. \u201cWrite a topic word"}, {"title": "2.1 Data Generation", "content": "Input Generation The input generation process starts by extracting inputs from provided\nexample pairs and combining them with the instruction to populate a prompt template.\nThe constructed prompts are then forwarded to the LLM to obtain model-generated inputs.\nAfter each round of prompting, the newly generated inputs are added to an input repository.\nA subset of inputs is randomly sampled from this repository and merged with inputs from"}, {"title": "2.2 Quality Optimization Temperature and Rule-based Filters", "content": "The quality of generated data is essential for the success of the downstream training. Our\napproach takes a two-fold strategy of adjusting generation parameters to improve quality\nand filtering out low quality samples.\nTemperature: Intuitively, adjusting the temperature is a common strategy to balance di-\nversity and quality. Our framework also leverages this approach, using a relatively higher\ntemperature during input generation to encourage diversity, and a lower temperature in\nother stages to promote quality. However, solely relying on temperature adjustment is\ninsufficient to achieve the desired balance. SELF-GUIDE further employs two rounds of\nrule-based data filtering, one after the inputs are generated, and the other after the outputs\nare annotated. For clarity, we describe these two rounds together.\nNoise Filter: We manually curate a list of noise terms, such as common salutations, greetings,\nand noise characters (e.g., \\-\\- in the generated contents). If any noise term from this curated\nlist appears in either the input or the output of a generated example, we discard the entire\nexample. This filtering step ensures that the generated examples remain concise, focused,\nand free from irrelevant content or artifact patterns.\nLength Filter: While the lengths of demonstrative examples may exhibit bias, we believe\nthese examples already possess decent representativeness for a specific task in terms of\nlength distribution. We assume that the lengths of demonstrative examples follow a normal\ndistribution for a specific task, and since most data points of a normal distribution fall within\ntwo standard deviations from the mean, we stipulate that the lengths of generated examples'\ninputs and outputs should also approximate a normal distribution with mean u and variance\n02 calculated from the demonstrative examples. Specifically, the lengths are required to be\nwithin the range (\u03bc \u2013 2\u03c3, \u03bc + 20). Other attributes like semantic similarity of generated\nexamples are computationally expensive and lack clear, intuitive definitions. Hence, we opt\nfor the efficient and tractable length attribute under this normality assumption."}, {"title": "2.3 Quality Optimization: Parameter Tuning", "content": "We want SELF-GUIDE to generate training data matching to the desired distribution specified\nby an instruction and examples. This requires optimizing various hyper-parameters on\nlabeled data points, including the initial number of generated inputs, the temperature for\ninput generation, the temperature for output generation, finetuning parameters like training\nepochs, and so on. To achieve this, we tune parameters on a held-out set of existing tasks\n(and corresponding instructions). We do so by searching parameters that maximize worst\ntask performance, in order to identify parameters that are likely to be \u201cgood enough\" for a\nbroad set of tasks (Michel et al., 2021):\n$\\max_{\\theta} \\min_{t \\in tasks} (performance(\\theta, t) \u2013 ICL(t))$                                                                                                                   (1)\nIn Section 3 below, we demonstrate that our default set of parameters generally performs\nwell with an absolute improvement of approximately 15% for classification tasks and 18%\nfor generation tasks in the benchmark's metrics."}, {"title": "3 Experimental Setup", "content": "3.1 Datasets\nTo evaluate the effectiveness of SELF-GUIDE, we selected 14 classification tasks and 8\ngeneration tasks from the Super-NaturalInstructions V2 benchmark. We randomly chose\nhalf of these tasks (7 classification and 4 generation) for hyper-parameter search, and the\nremaining half for evaluation (referred to as held-out tasks).  lists the held-out tasks.\nTo find the optimal set of parameters as described in Section 2.3, we performed a random\nsearch over the temperature for input generation and output generation, as well as the\nnumber of generated inputs before filtering."}, {"title": "3.2 Base Model", "content": "We selected Vicuna-7b-1.5 (Zheng et al., 2023; Wan et al., 2024) as the foundational model\nfor input generation, output generation, and fine-tuning for two primary reasons: Firstly,\nhaving undergone fine-tuning on an extensive and diverse corpus of user conversations\nfrom ShareGPT, this model demonstrates robust cross-task instruction-following capabilities.\nHowever, despite its extensive training, its performance is not deemed satisfactory, as\nevidenced by its relatively low ranking of 73rd on the LMSYS Chatbot Arena Leaderboard\n(Chiang et al., 2024), prompting us to investigate opportunities for improvement.\nWe employ the same evaluation metrics as in the Super-NaturalInstructions benchmark,\nnamely Exact Match for classification tasks and Rouge-L for generation tasks. Exact Match"}, {"title": "3.3 Baselines", "content": "SELF-GUIDE provides a method for executing tasks specified by prompts (which contain\ninstructions and a small number of demonstration examples). We compare SELF-GUIDE\nagainst other methods for instruction following and in-context learning:\n1. Few-Shot ICL: As our primary baseline, we compare against directly prompting the LM.\nThis directly relies on the inherent instruction-following abilities of the model.\n2. Self-ICL: We build on Self-ICL (Chen et al., 2023), which uses self-generated examples to\nimprove zero-shot instruction following. We adopt Self-ICL to our few-shot setting by\nself-generating as many examples as can fit in the context window. We list this number\nof examples as k in Table 2.\n3. Few-Shot Finetuning: We consider directly finetuning on the few demonstrations in each\nprompt. Prior work (Liu et al., 2022) shows this can be very effective.\nWe use the same base model (Vicuna-7b-1.5) as described in 3.2 for every baseline mentioned\nabove."}, {"title": "4 Results and Analysis", "content": "Our main experiment results are shown in Table 1. Specifically, we report an absolute im-\nprovement of 14.6% for classification tasks and 17.9% for generation tasks in the benchmark's\nmetrics. SELF-GUIDE demonstrates its effectiveness in guiding LLMs toward task-specific\nexpertise, even in extremely data-limited situations. This highlights the potential of self-\nsynthesized data in adapting LLMs for specialized tasks from a more scalable perspective.\nTo further analyze the reasons behind the improvement brought about by SELF-GUIDE, we\nconducted several analysis experiments to examine certain properties of SELF-GUIDE."}, {"title": "4.1 Comparing self-synthesized examples with gold few-shot examples", "content": "In Table 1, we see that SELF-GUIDE improves Vicuna-7b-1.5's ability to complete most tasks.\nGiven that the SELF-GUIDE algorithm, in essence, involves finetuning on self-synthesized\ndata, how much of this performance boost can be attributed to the synthetic data or to the\nlearning algorithm?\nTo test this, we compare two ways of incorporating synthetic data into an LM: finetuning\nand in-context learning. For finetuning, we compare SELF-GUIDE with few-shot finetuning\non three gold demonstrations. For in-context learning, we compare adding self-synthesized\ndata into demonstration examples, i.e. Self-ICL, with in-context learning using only the\nthree gold demonstrations.\nIn Table 2, we see that using generated training data with finetuning improves over 3-shot\nlearning on average for both classification and generation tasks (comparing the \u201cSELF-\nGUIDE"}, {"title": "4.2 Finetuning outperforms in-context learning on synthetic data", "content": "How much is the learning algorithm \u2014 finetuning versus in-context learning \u2014 responsible\nfor the effectiveness of SELF-GUIDE? To test this, we compare in-context learning on as\nmany self-synthesized examples as can fit in the context window of an LM (\u201cSelf-ICL\u201d)\nwith finetuning on the same set of self-synthesized examples. We find in Table 2, compared\nto in-context learning, finetuning on the same generated examples achieves substantially\nbetter performance, with an average improvement of 14 absolute percentage points across\nalmost all tasks. Interestingly, finetuning on just a few self-synthesized examples (e.g. 7) can\nyield considerable performance gains, which highlights the data efficiency of SELF-GUIDE."}, {"title": "4.3 SELF-GUIDE aligns LMs with the correct label distribution in many cases", "content": "To demonstrate the high quality of data generated by SELF-GUIDE from a qualitative\nperspective, we analyze the distance between the output distributions of the models before\nand after SELF-GUIDE and the ground truth distribution for classification tasks.\nSpecifically, we compute the output distributions of the base model and the Self-Guided\nexpert model on the same task. Considering that the outputs may contain irrelevant\ngreetings or cases where the model directly refuses to answer, we collectively treat outputs\nwithout labels as irrelevant content and calculate the distribution across all labels and\nirrelevant content. After obtaining the output distribution, we calculate the L1 distance\nbetween this distribution and the ground truth distribution.\nFrom the results in Table 3, on average, the SELF-GUIDE model demonstrates a significantly\nlower average L1 distance compared to the baseline model while reducing the proportion\nof irrelevant content to 0. This indicates that the SELF-GUIDE model not only enhances\nthe consistency of outputs with the true distribution but also effectively reduces irrelevant\ninformation in generated content, thereby enhancing the overall performance of the model."}, {"title": "4.4 SELF-GUIDE learns a non-trivial input-output mapping via self-synthesis", "content": "In this experiment, we investigate whether the improved performance of the LM under the\nSELF-GUIDE framework stems from merely learning trivial patterns like output formatting\nand label structuring, or also from gaining a better understanding of the task. We consider\ntwo scenarios: Rand-Baseline, where we randomize the labels in the demonstrative examples\nfollowing Min et al. (2022b), and Rand-SELF-GUIDE, where we randomize the labels of the\nself-generated examples in the SELF-GUIDE approach. Specifically, for the Rand-Baseline,\neach original label in the demonstrative examples is replaced with a random label. For\nRand-SELF-GUIDE, we randomize all the labels of the generated examples in SELF-GUIDE,\nfinetune the base model on this randomized data, and evaluate it using the same prompt as\nin our main experiments.\nFrom our results in Table 4, Rand-SELF-GUIDE outperforms the baseline suggesting that\neven with randomized labels, the self-generated examples provide valuable signals to\nthe model. Additionally, the high ratio of irrelevant content produced by the baseline\nmodel, as detailed in Table 3, contrasts sharply with the SELF-GUIDE approach, which\neliminates irrelevant outputs entirely, ensuring all outputs align with the expected label\nspace. This demonstrates that self-generated examples can introduce basic patterns like\noutput formatting and label structuring to language models. However, SELF-GUIDE's\nimprovement gain over Rand-SELF-GUIDE further demonstrates that beyond learning\nsuperficial patterns, better supervisory signals during the SELF-GUIDE process enable the\nmodel to develop a deeper comprehension of the task itself. Finally, Rand-Baseline's worse\nperformance compared to the baseline confirms that merely observing well-formatted\noutputs is insufficient - the model crucially requires proper supervision from high-quality\ndemonstrations to truly grasp the task essence. Collectively, these contrasts reveal that the\nSELF-GUIDE method allows the model to simultaneously acquire shallow output patterns\nwhile leveraging the self-generated examples to model the task distribution, leading to a\ncomprehensive boost in its capabilities on specific tasks."}, {"title": "4.5 Noise filter is crucial for classification tasks, while length filter is crucial for generation task", "content": "Ablation studies results below in Table 5 found that removing the ablation filter decreases\nclassification accuracy by 4.1% while removing the length filter decreases generation accu-\nracy by 3.8%. We think that the ablation filter is crucial for classification tasks as it removes\noutputs with superfluous content, retaining only the labels. For generation tasks, the length\nfilter enhances performance by excluding lengthy or too short responses, improving the\nRouge-L score. These studies demonstrate the effectiveness of our rule-based filters.\nIn conclusion, we identify several key factors contributing to the efficacy of SELF-GUIDE.\nInitially, expanding and augmenting the training dataset with self-generated synthetic data"}, {"title": "5 Related Work", "content": "Recent studies have demonstrated the efficient execution of language-based instructions\nby LLMs through fine-tuning instruction-based datasets. These datasets comprise pairs of\nlanguage instruction commands and expected outcomes annotated by humans (Weller et al.,\n2020; Mishra et al., 2022). (Honovich et al., 2022) indicate that despite the noise present in\nLLM-generated datasets, they can still serve as effective training resources for instruction\nfine-tuning, implying that the parametric knowledge in pretrained LLMs contains, with\npotentially some transformation necessary, an inherent understanding of instructions. We\nextend this hypothesis, proposing that LLMs inherently possess the ability to understand\narbitrary instructions and that this ability can be exploited to self-generate training data.\nManually crafted datasets have played a pivotal role in supervising and augmenting various\nNLP task systems (Rozi\u00e8re et al., 2023; Yuan et al., 2023). However, traditional manual\nannotation processes are often time-consuming, labor-intensive, costly, and non-scalable.\nAdditionally, the possibility of human errors and the lack of domain expertise among\nannotators in complex tasks cannot be overlooked (Braylan & Lease, 2021; Kang et al.,\n2023). Recent works have tried to alleviate the human labor associated with training data\ncollection by utilizing the power of stronger models as data generators (Patel et al., 2024;\nSong et al., 2024). These methods can be broadly categorized based on their reliance on seed\ndata, the need for human curation, and stronger teacher LLMs. Viswanathan et al. (2023)\nproposed Prompt2Model, which combines retrieval methods and distillation to directly\ngather training data from external sources without seed data. Zhou et al. (2023) introduced"}, {"title": "6 Conclusion", "content": "In this paper, we propose SELF-GUIDE, an algorithm for large language models to follow\ntask-specific instructions by internally producing synthetic training data and finetuning\non this data. The improvement underscores the potential of our approach in enhancing\nLLMs' task-specific expertise, particularly in data-limited scenarios, and open avenues for\nexploring advanced techniques in autonomous model adaptation and continuous learning.\nWe hope that our work can chart a path for future research in autonomous self-alignment\nand improvement mechanisms in AI systems, aligning them more closely with human\nintentions."}, {"title": "7 Limitations and Ethical Considerations", "content": "The primary limitation of our work is that we focus all of our experiments and models on\nEnglish NLP tasks. Language technologies already promote systematic inequalities between\nlanguages and communities (Blasi et al., 2021); SELF-GUIDE brings the possibility of exacer-\nbating these inequalities. On the other hand, SELF-GUIDE also offers the tempting potential\nof improving LMs' ability to execute instructions specified in non-English languages to\nempower non-English LLM-based Agents (Chen et al., 2024). We consider this to be an\nessential direction for future work, and we are actively pursuing this.\nAnother limitation of our work is that we've only shown that SELF-GUIDE works at im-\nproving a 7B-parameter model (Vicuna-7b-1.5). We believe that our approach could be\nused to improve the ability of arbitrarily large LMs (He et al., 2023; Hu et al., 2024) to\nfollow task-specific instructions. However, due to budgetary restrictions, we were unable to\nexperiment with larger models; we leave this as an important avenue for future work.\nIn terms of ethical considerations, SELF-GUIDE' potential to improve instruction following\non specific tasks raises the risk of making it easier for people to specialize LLMs for malicious\npurposes. The open-sourcing of our code could amplify this risk. As Widder et al. (2022)\npoints out (with the case study of deepfakes), open-source software can simultaneously\nincrease the prevalence of harms while providing the community with the understanding\nand experience to manage these harms. Given SELF-GUIDE' potential for positive use, we\nbelieve that this algorithm and code are still worthy of releasing to the public."}, {"title": "A Appendix", "content": "A.1 Prompt Template\nA.1.1 Input Generation Prompt Template\nINPUT_GENERATOR_PROMPT_FOR_GENERATION = \"\"\"\nAs an InputGenerator, your task is to generate\na new [input] based on the [instruction] and\nsome example [input].\nTry your best to ensure that the new [input]\nyou generate is distinct from the provided\n[input] while maintaining a diverse, detailed,\nprecise, comprehensive, and high-quality response.\nAvoid generating a new (input) that is the\nsame as the provided [input].\n[instruction]\n{instruction}\nHere are some high-quality [input] for the\n[instruction]. These (input) can provide\nyou with very strict format requirements.\nYou should pay extreme attention to them!!!\nSome high-quality [input]:\n{high_quality_input_string}\nThese are some addtional [input]. Their\nformats and contents may not be accurate.\nHowever, you may also refer to the content\nof them.\nSome low-quality [input]:\n{low_quality_input_string}\nAfters seeing example inputs, generate a new\n[input]. Before generating the new [input],\nensure that you strictly adhere to the rules\nof the new [instruction] and follow the\nformat of high-quality [input].\nPrioritize the new [instruction] guidelines\nto maintain consistency and quality.\nThink twice before generating a new [input].\nOnly response the new [input] without any\nother\ninformation.\n[input]=\n\"\"\"\nINPUT_GENERATOR_PROMPT_FOR_CLASSIFICATION = \"\"\"\nAs an Input Generator, your task is to generate\na new [input] based on the [instruction] and\nsome example [input].\nTry your best to ensure that the new [input]\nyou generate is distinct from the provided"}, {"title": "A.1.2 Output Generation Prompt Template", "content": "OUTPUT_ANNOTATION_PROMPT_TEMPLATE = \"\"\"\nA chat between a curious user and an\nartificial intelligence assistant.\nThe assistant gives concise answers\nto the user's questions.\nUSER: The artificial intelligence\nassistant only needs to\nhelp annotate label.\nThe task is: {instruction}\nASSISTANT: Okay.\nUSER : [input]\n= {input_1}\nASSISTANT : {output_1}\nUSER : [input]\n= {input_2}"}, {"title": "A.2 Prompt Sensitivity Experiment", "content": "LLMs have been shown to be highly sensitive to prompt formats and minimal changes\nsuch as punctuation (Sclar et al., 2023b). This sensitivity can lead to drastic fluctuations in\nmodel performance, affecting its stability and reliability in real-world applications. Here, we\nintroduce minimal prompt modifications. Although these modifications seem trivial from a\nhuman perspective, we will show that they greatly impact the base model's performance.\nHowever, Self-Guided final models prove robust, consistently outperforming the base model\nacross various conditions. Specifically, we use the output generation prompt template (see\nAppendix A.1.2) to finetune the base model but make minimal disturbances to the few-shot\nexamples of the prompt template when evaluating. Note that in the original format, there is\nan \"=\\n\" following \"USER : [input]\". We change this conjunction between \"USER : [input]\"\nand the example input to \":\", \"\\n\\n\", and evaluate the performance of the base model\nand the Self-Guided expert model under this condition. Detailed results are shown in\nTable 6. We find that such simple changes, which may be overlooked by humans, can lead\nto catastrophic performance decreases in the raw model. On the other hand, SELF-GUIDE's\nperformance exhibits remarkable stability and excellence."}]}