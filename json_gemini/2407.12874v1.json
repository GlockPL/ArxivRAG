{"title": "SELF-GUIDE: Better Task-Specific Instruction Following via Self-Synthetic Finetuning", "authors": ["Chenyang Zhao", "Xueying Jia", "Vijay Viswanathan", "Tongshuang Wu", "Graham Neubig"], "abstract": "Large language models (LLMs) hold the promise of solving diverse tasks when provided with appropriate natural language prompts. However, prompting often leads models to make predictions with lower accuracy compared to finetuning a model with ample training data. On the other hand, while finetuning LLMs on task-specific data generally improves their performance, abundant annotated datasets are not available for all tasks. Previous work has explored generating task-specific data from state- of-the-art LLMs and using this data to finetune smaller models, but this approach requires access to a language model other than the one being trained, which introduces cost, scalability challenges, and legal hurdles associated with continuously relying on more powerful LLMs. In response to these, we propose SELF-GUIDE, a multi-stage mechanism in which we synthesize task-specific input-output pairs from the student LLM, then use these input-output pairs to finetune the student LLM itself. In our empirical evaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE improves the performance of LLM by a substantial margin. Specifically, we report an absolute improvement of approximately 15% for classification tasks and 18% for generation tasks in the benchmark's metrics. This sheds light on the promise of self-synthesized data guiding LLMs towards becoming task-specific experts without any external learning signals.", "sections": [{"title": "1 Introduction", "content": "One of the traits of large language models (LLMs) that has captured the imagination of model developers is their potential to automate a broad range of highly complex tasks with relative ease (Brown et al., 2020a). Where previous generations of models required vast task-specific training sets, LLMs now offer the promise of enabling similar accuracy by simply writing a prompt and supplying a few examples. However, this may be a false promise. Prompting typically leads models to make predictions with lower accuracy compared to finetuning a model with ample training data (Gao et al., 2021; Zhang et al., 2023). Moreover, LLMs' performance crucially depends on their ability to follow instructions outlined in the prompts and even minor alterations to these prompts can result in a notable performance decline (Sclar et al., 2023a).\nOn the other hand, in data-abundant tasks, finetuning a pre-trained language model through supervised finetuning (Ouyang et al., 2022; Wei et al., 2022) and reinforcement learning from human feedback (Lambert et al., 2022) has proven a successful strategy. However, the effectiveness of this approach diminishes notably for underrepresented tasks suffering from data scarcity (Li et al., 2022), highlighting the critical need for high-quality training data. To this end, recent studies have explored the potential of utilizing potent \u201cteacher\u201d LLMs to create task-specific training data, thereby enhancing the performance of comparatively"}, {"title": "2 SELF-GUIDE", "content": "In the following section, we outline the proposed SELF-GUIDE framework (illustrated in Figure 2 and Figure 3). As input, SELF-GUIDE takes an instruction (e.g. \u201cWrite a topic word"}, {"title": "2.1 Data Generation", "content": "Input Generation The input generation process starts by extracting inputs from provided example pairs and combining them with the instruction to populate a prompt template. The constructed prompts are then forwarded to the LLM to obtain model-generated inputs. After each round of prompting, the newly generated inputs are added to an input repository. A subset of inputs is randomly sampled from this repository and merged with inputs from"}, {"title": "2.2 Quality Optimization Temperature and Rule-based Filters", "content": "The quality of generated data is essential for the success of the downstream training. Our approach takes a two-fold strategy of adjusting generation parameters to improve quality and filtering out low quality samples.\nTemperature: Intuitively, adjusting the temperature is a common strategy to balance di- versity and quality. Our framework also leverages this approach, using a relatively higher temperature during input generation to encourage diversity, and a lower temperature in other stages to promote quality. However, solely relying on temperature adjustment is insufficient to achieve the desired balance. SELF-GUIDE further employs two rounds of rule-based data filtering, one after the inputs are generated, and the other after the outputs are annotated. For clarity, we describe these two rounds together.\nNoise Filter: We manually curate a list of noise terms, such as common salutations, greetings, and noise characters (e.g., \\-\\- in the generated contents). If any noise term from this curated list appears in either the input or the output of a generated example, we discard the entire example. This filtering step ensures that the generated examples remain concise, focused, and free from irrelevant content or artifact patterns.\nLength Filter: While the lengths of demonstrative examples may exhibit bias, we believe these examples already possess decent representativeness for a specific task in terms of length distribution. We assume that the lengths of demonstrative examples follow a normal distribution for a specific task, and since most data points of a normal distribution fall within two standard deviations from the mean, we stipulate that the lengths of generated examples' inputs and outputs should also approximate a normal distribution with mean u and variance 02 calculated from the demonstrative examples. Specifically, the lengths are required to be within the range (\u03bc \u2013 2\u03c3, \u03bc + 20). Other attributes like semantic similarity of generated examples are computationally expensive and lack clear, intuitive definitions. Hence, we opt for the efficient and tractable length attribute under this normality assumption."}, {"title": "2.3 Quality Optimization: Parameter Tuning", "content": "We want SELF-GUIDE to generate training data matching to the desired distribution specified by an instruction and examples. This requires optimizing various hyper-parameters on labeled data points, including the initial number of generated inputs, the temperature for input generation, the temperature for output generation, finetuning parameters like training epochs, and so on. To achieve this, we tune parameters on a held-out set of existing tasks (and corresponding instructions). We do so by searching parameters that maximize worst task performance, in order to identify parameters that are likely to be \u201cgood enough\" for a"}, {"title": "3 Experimental Setup", "content": "3.1 Datasets\nTo evaluate the effectiveness of SELF-GUIDE, we selected 14 classification tasks and 8 generation tasks from the Super-NaturalInstructions V2 benchmark. We randomly chose half of these tasks (7 classification and 4 generation) for hyper-parameter search, and the remaining half for evaluation (referred to as held-out tasks).\nTo find the optimal set of parameters as described in Section 2.3, we performed a random search over the temperature for input generation and output generation, as well as the number of generated inputs before filtering.\n3.2 Base Model\nWe selected Vicuna-7b-1.5 (Zheng et al., 2023; Wan et al., 2024) as the foundational model for input generation, output generation, and fine-tuning for two primary reasons: Firstly, having undergone fine-tuning on an extensive and diverse corpus of user conversations from ShareGPT, this model demonstrates robust cross-task instruction-following capabilities. However, despite its extensive training, its performance is not deemed satisfactory, as evidenced by its relatively low ranking of 73rd on the LMSYS Chatbot Arena Leaderboard (Chiang et al., 2024), prompting us to investigate opportunities for improvement.\nWe employ the same evaluation metrics as in the Super-NaturalInstructions benchmark, namely Exact Match for classification tasks and Rouge-L for generation tasks. Exact Match"}, {"title": "4 Results and Analysis", "content": "Our main experiment results are shown in Table 1. Specifically, we report an absolute im- provement of 14.6% for classification tasks and 17.9% for generation tasks in the benchmark's metrics. SELF-GUIDE demonstrates its effectiveness in guiding LLMs toward task-specific expertise, even in extremely data-limited situations. This highlights the potential of self- synthesized data in adapting LLMs for specialized tasks from a more scalable perspective.\nTo further analyze the reasons behind the improvement brought about by SELF-GUIDE, we conducted several analysis experiments to examine certain properties of SELF-GUIDE.\n4.1 Comparing self-synthesized examples with gold few-shot examples\nIn Table 1, we see that SELF-GUIDE improves Vicuna-7b-1.5's ability to complete most tasks. Given that the SELF-GUIDE algorithm, in essence, involves finetuning on self-synthesized data, how much of this performance boost can be attributed to the synthetic data or to the learning algorithm?\nTo test this, we compare two ways of incorporating synthetic data into an LM: finetuning and in-context learning. For finetuning, we compare SELF-GUIDE with few-shot finetuning on three gold demonstrations. For in-context learning, we compare adding self-synthesized data into demonstration examples, i.e. Self-ICL, with in-context learning using only the three gold demonstrations.\nIn Table 2, we see that using generated training data with finetuning improves over 3-shot learning on average for both classification and generation tasks (comparing the \u201cSELF-GUIDE\" and \"3-Shot Finetuning\" columns). On the other hand, comparing the \u201c3-Shot ICL\" and \"Self-ICL\u201d columns, we see that using generated training data with in-context learning helps only for generation tasks; for classification tasks, this strategy often leads to deteriorated performance. In Self-ICL for classification tasks, the prompted model tends to generate new input-output demonstrations rather than respond to the final input to be completed, leading to a significant performance decrease. We hypothesize that the"}, {"title": "4.2 Finetuning outperforms in-context learning on synthetic data", "content": "How much is the learning algorithm \u2014 finetuning versus in-context learning \u2014 responsible for the effectiveness of SELF-GUIDE? To test this, we compare in-context learning on as many self-synthesized examples as can fit in the context window of an LM (\u201cSelf-ICL\u201d) with finetuning on the same set of self-synthesized examples. We find in Table 2, compared to in-context learning, finetuning on the same generated examples achieves substantially better performance, with an average improvement of 14 absolute percentage points across almost all tasks. Interestingly, finetuning on just a few self-synthesized examples (e.g. 7) can yield considerable performance gains, which highlights the data efficiency of SELF-GUIDE."}, {"title": "4.3 SELF-GUIDE aligns LMs with the correct label distribution in many cases", "content": "To demonstrate the high quality of data generated by SELF-GUIDE from a qualitative perspective, we analyze the distance between the output distributions of the models before and after SELF-GUIDE and the ground truth distribution for classification tasks.\nSpecifically, we compute the output distributions of the base model and the Self-Guided expert model on the same task. Considering that the outputs may contain irrelevant greetings or cases where the model directly refuses to answer, we collectively treat outputs without labels as irrelevant content and calculate the distribution across all labels and irrelevant content. After obtaining the output distribution, we calculate the L1 distance between this distribution and the ground truth distribution.\nFrom the results in Table 3, on average, the SELF-GUIDE model demonstrates a significantly lower average L1 distance compared to the baseline model while reducing the proportion of irrelevant content to 0. This indicates that the SELF-GUIDE model not only enhances the consistency of outputs with the true distribution but also effectively reduces irrelevant information in generated content, thereby enhancing the overall performance of the model."}, {"title": "4.4 SELF-GUIDE learns a non-trivial input-output mapping via self-synthesis", "content": "In this experiment, we investigate whether the improved performance of the LM under the SELF-GUIDE framework stems from merely learning trivial patterns like output formatting and label structuring, or also from gaining a better understanding of the task. We consider two scenarios: Rand-Baseline, where we randomize the labels in the demonstrative examples following Min et al. (2022b), and Rand-SELF-GUIDE, where we randomize the labels of the self-generated examples in the SELF-GUIDE approach. Specifically, for the Rand-Baseline, each original label in the demonstrative examples is replaced with a random label. For Rand-SELF-GUIDE, we randomize all the labels of the generated examples in SELF-GUIDE, finetune the base model on this randomized data, and evaluate it using the same prompt as in our main experiments.\nFrom our results in Table 4, Rand-SELF-GUIDE outperforms the baseline suggesting that even with randomized labels, the self-generated examples provide valuable signals to the model. Additionally, the high ratio of irrelevant content produced by the baseline model, as detailed in Table 3, contrasts sharply with the SELF-GUIDE approach, which eliminates irrelevant outputs entirely, ensuring all outputs align with the expected label space. This demonstrates that self-generated examples can introduce basic patterns like output formatting and label structuring to language models. However, SELF-GUIDE's improvement gain over Rand-SELF-GUIDE further demonstrates that beyond learning superficial patterns, better supervisory signals during the SELF-GUIDE process enable the model to develop a deeper comprehension of the task itself. Finally, Rand-Baseline's worse performance compared to the baseline confirms that merely observing well-formatted outputs is insufficient - the model crucially requires proper supervision from high-quality demonstrations to truly grasp the task essence. Collectively, these contrasts reveal that the SELF-GUIDE method allows the model to simultaneously acquire shallow output patterns while leveraging the self-generated examples to model the task distribution, leading to a comprehensive boost in its capabilities on specific tasks."}, {"title": "4.5 Noise filter is crucial for classification tasks, while length filter is crucial for generation task", "content": "Ablation studies results below in Table 5 found that removing the ablation filter decreases classification accuracy by 4.1% while removing the length filter decreases generation accu- racy by 3.8%. We think that the ablation filter is crucial for classification tasks as it removes outputs with superfluous content, retaining only the labels. For generation tasks, the length filter enhances performance by excluding lengthy or too short responses, improving the Rouge-L score. These studies demonstrate the effectiveness of our rule-based filters.\nIn conclusion, we identify several key factors contributing to the efficacy of SELF-GUIDE. Initially, expanding and augmenting the training dataset with self-generated synthetic data"}, {"title": "5 Related Work", "content": "Recent studies have demonstrated the efficient execution of language-based instructions by LLMs through fine-tuning instruction-based datasets. These datasets comprise pairs of language instruction commands and expected outcomes annotated by humans (Weller et al., 2020; Mishra et al., 2022). (Honovich et al., 2022) indicate that despite the noise present in LLM-generated datasets, they can still serve as effective training resources for instruction fine-tuning, implying that the parametric knowledge in pretrained LLMs contains, with potentially some transformation necessary, an inherent understanding of instructions. We extend this hypothesis, proposing that LLMs inherently possess the ability to understand arbitrary instructions and that this ability can be exploited to self-generate training data.\nManually crafted datasets have played a pivotal role in supervising and augmenting various NLP task systems (Rozi\u00e8re et al., 2023; Yuan et al., 2023). However, traditional manual annotation processes are often time-consuming, labor-intensive, costly, and non-scalable. Additionally, the possibility of human errors and the lack of domain expertise among annotators in complex tasks cannot be overlooked (Braylan & Lease, 2021; Kang et al., 2023). Recent works have tried to alleviate the human labor associated with training data collection by utilizing the power of stronger models as data generators (Patel et al., 2024; Song et al., 2024). These methods can be broadly categorized based on their reliance on seed data, the need for human curation, and stronger teacher LLMs. Viswanathan et al. (2023) proposed Prompt2Model, which combines retrieval methods and distillation to directly gather training data from external sources without seed data. Zhou et al. (2023) introduced"}, {"title": "6 Conclusion", "content": "In this paper, we propose SELF-GUIDE, an algorithm for large language models to follow task-specific instructions by internally producing synthetic training data and finetuning on this data. The improvement underscores the potential of our approach in enhancing LLMs' task-specific expertise, particularly in data-limited scenarios, and open avenues for exploring advanced techniques in autonomous model adaptation and continuous learning. We hope that our work can chart a path for future research in autonomous self-alignment and improvement mechanisms in AI systems, aligning them more closely with human intentions."}, {"title": "7 Limitations and Ethical Considerations", "content": "The primary limitation of our work is that we focus all of our experiments and models on English NLP tasks. Language technologies already promote systematic inequalities between languages and communities (Blasi et al., 2021); SELF-GUIDE brings the possibility of exacer- bating these inequalities. On the other hand, SELF-GUIDE also offers the tempting potential of improving LMs' ability to execute instructions specified in non-English languages to empower non-English LLM-based Agents (Chen et al., 2024). We consider this to be an essential direction for future work, and we are actively pursuing this.\nAnother limitation of our work is that we've only shown that SELF-GUIDE works at im- proving a 7B-parameter model (Vicuna-7b-1.5). We believe that our approach could be used to improve the ability of arbitrarily large LMs (He et al., 2023; Hu et al., 2024) to follow task-specific instructions. However, due to budgetary restrictions, we were unable to experiment with larger models; we leave this as an important avenue for future work.\nIn terms of ethical considerations, SELF-GUIDE' potential to improve instruction following on specific tasks raises the risk of making it easier for people to specialize LLMs for malicious purposes. The open-sourcing of our code could amplify this risk. As Widder et al. (2022) points out (with the case study of deepfakes), open-source software can simultaneously increase the prevalence of harms while providing the community with the understanding and experience to manage these harms. Given SELF-GUIDE' potential for positive use, we believe that this algorithm and code are still worthy of releasing to the public."}, {"title": "A Appendix", "content": "A.1 Prompt Template\nA.1.1 Input Generation Prompt Template\nINPUT_GENERATOR_PROMPT_FOR_GENERATION", "\nAs an InputGenerator, your task is to generate\na new [input) based on the [instruction] and\nsome example [input].\nTry your best to ensure that the new [input]\nyou generate is distinct from the provided\n[input] while maintaining a diverse, detailed,\nprecise, comprehensive, and high-quality response.\nAvoid generating a new (input) that is the\nsame as the provided [input].\n[instruction]\n{instruction}\nHere are some high-quality [input) for the\n[instruction]. These (input) can provide\nyou with very strict format requirements.\nYou should pay extreme attention to them!!!\nSome high-quality [input]:\n{high_quality_input_string}\nThese are some addtional (input). Their\nformats and contents may not be accurate.\nHowever, you may also refer to the content\nof them.\nSome low-quality [input]:\n{low_quality_input_string}\nAfters seeing example inputs, generate a new\n[input]. Before generating the new (input],\nensure that you strictly adhere to the rules\nof the new [instruction) and follow the\nformat of high-quality [input].\nPrioritize the new [instruction] guidelines\nto maintain consistency and quality.\nThink twice before generating a new [input].\nOnly response the new [input] without any\nother\ninformation.\n[input]=": "\nINPUT_GENERATOR_PROMPT_FOR_CLASSIFICATION"}, {"title": "A.1.2 Output Generation Prompt Template", "content": "OUTPUT_ANNOTATION_PROMPT_TEMPLATE"}, {"title": "A.2 Prompt Sensitivity Experiment", "content": "LLMs have been shown to be highly sensitive to prompt formats and minimal changes such as punctuation (Sclar et al., 2023b). This sensitivity can lead to drastic fluctuations in model performance, affecting its stability and reliability in real-world applications. Here, we introduce minimal prompt modifications. Although these modifications seem trivial from a human perspective, we will show that they greatly impact the base model's performance. However, Self-Guided final models prove robust, consistently outperforming the base model across various conditions. Specifically, we use the output generation prompt template (see Appendix A.1.2) to finetune the base model but make minimal disturbances to the few-shot examples of the prompt template when evaluating. Note that in the original format, there is an \u201c=\\n\u201d following \u201cUSER : [input]\u201d. We change this conjunction between \u201cUSER : [input]\u201d and the example input to \u201c:\u201d, \u201c\\n\\n\u201d, and evaluate the performance of the base model and the Self-Guided expert model under this condition. Detailed results are shown in Table 6. We find that such simple changes, which may be overlooked by humans, can lead to catastrophic performance decreases in the raw model. On the other hand, SELF-GUIDE's performance exhibits remarkable stability and excellence."}, {"title": "Category Task ID", "content": "$\\max_{\\theta} \\min_{t \\in tasks}(performance(\\theta, t) - ICL(t))$\n(1)\nIn Section 3 below, we demonstrate that our default set of parameters generally performs well with an absolute improvement of approximately 15% for classification tasks and 18% for generation tasks in the benchmark's metrics."}]}