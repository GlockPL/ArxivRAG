{"title": "LLAVA-ZIP: ADAPTIVE VISUAL TOKEN COMPRESSION WITH INTRINSIC IMAGE INFORMATION", "authors": ["Ke Wang", "Hong Xuan"], "abstract": "Multi-modal large language models (MLLMs) utilizing instruction-following data, such as LLaVA, have achieved great progress in the industry. A major limitation in these models is that visual tokens consume a substantial portion of the maximum token limit in large language models (LLMs), leading to increased computational demands and decreased performance when prompts include multiple images or video. Industry solutions often mitigate this issue by increasing computational power, but this approach is less feasible in academic environments with limited resources. In this study, we propose Dynamic Feature Map Reduction (DFMR) based on LLaVA-1.5 to address the challenge of visual token overload. DFMR dynamically compresses the visual tokens, freeing up token capacity. Our experimental results demonstrate that integrating DFMR into LLaVA-1.5 significantly improves the performance of LLaVA in varied visual token lengths, offering a promising solution for extending LLaVA to handle multi-image and video scenarios in resource-constrained academic environments and it can also be applied in industry settings for data augmentation to help mitigate the scarcity of open-domain image-text pair datasets in the continued pretraining stage.", "sections": [{"title": "Introduction", "content": "Recently, the multimodal large language models (MLLMs), especially based on instruction following data, LLaVA family[1, 2, 3], have achieved great progress in the industry. One of the major limitations of these models is that the visual tokens consume a substantial portion of the maximum token limit in the pre-trained large language model (LLM). To address this, researchers apply the AnyRes method[1], which splits the single input image into multiple sub-images and uses the visual tokenizer to tokenize all the sub-images and optionally the original images. The method has proved effective and can be widely applied in industrial settings with sufficient computation power. However, in academic settings, where computing resource is constrained. The increase of visual tokens is limited by the maximum GPU memory and maximum token length in the pre-trained LLM.\nTo address the problems, researchers have concentrated on reducing the number of visual tokens. Li et al.[4] introduced BLIP-2, which utilizes a Q-former to compress image information into a fixed set of visual tokens, effectively condensing the visual input. Recent works have proposed methods that leverage textual guidance to identify and retain important visual tokens[5, 6]. Arif et al.[7] presented HiRED, an attention-guided token dropping technique that employs CLIP attention masks to split images and selectively discard less relevant tokens, enhancing efficiency in resource-constrained environments. Additionally, Cai et al. [8] developed Matryoshka Multimodal Models, which manage image representations by organizing visual tokens in a coarse-to-fine hierarchy. Generally, the current approach focuses on the use of external information outside of the image itself to facilitate the compression of visual tokens, instead of fully leveraging the information the image can provide.\nIn this paper, we propose DFMR (Dynamic Feature Map Reduction), a method that dynamically adjusts the compression ratio of visual tokens based solely on the intrinsic content of each image. By introducing a novel metric that quantifies"}, {"title": "Related Work", "content": "MLLMs. The most promising approach to bridging visual and textual information involves integrating visual informa- tion into LLMs. Flamingo[9] proposed using cross-attention mechanisms to incorporate encoded image information directly into LLM layers. In contrast, BLIP-2[4] introduced a Q-Former to effectively bridge encoded visual information into the input space of LLMs. LLaVA[1, 2, 3] adopted a simpler approach by utilizing a multilayer perceptron (MLP) to map encoded visual information into the LLM's input space while incorporating high-quality data and adaptive image segmentation techniques to handle multiple images and videos efficiently.\nVisual token compression. The compression of visual tokens can be broadly categorized into three approaches: QFormer-like, LLM-assisted, and handcrafted metrics methods [10]. The QFormer-like method [4, 11] maps any number of images to a fixed number of visual tokens, which often results in information loss for tasks requiring fine-grained details. The LLM-assisted method [5, 6] utilizes LLMs to perform visual token pruning but does not fully exploit the internal information pre-trained from images. The heuristic approach [7, 8] relies on handcrafted metrics to evaluate the importance of visual tokens. Our proposed method falls into the third category, where we propose a metric to measure the intrinsic information represented by the image and dynamically compress the image based on this metric."}, {"title": "Method", "content": "3.1 Overall Structure\nThe overall structure of the proposed model is illustrated in Fig. 1. Our model is developed based on LLaVA-1.5[2], with an additional module, Dynamic Feature Map Reduction (DFMR), inserted between the vision encoder and the projector. The DFMR dynamically compresses the visual tokens based on the image's inherent information.\nGiven a resized input given image $I \\in \\mathbb{R}^{H \\times W \\times 3}$, it is passed through the vision encoder to obtain visual tokens $V\\in \\mathbb{R}^{N_v \\times D_v}$, where $N_v = H' \\times W'$ is the number of visual tokens determined by the image feature map size $H'$ and $W'$ after the vision encoder, and $D_v$ is the embedding dimension of the visual tokens. The obtained visual tokens $V$ are then passed through the proposed DFMR module to produce compressed visual tokens $V' \\in \\mathbb{R}^{N_c \\times D_v}$, where $N_c = \\frac{H'}{s} \\times \\frac{W'}{s}$ and $s$ is the compression factor of dynamic pooling. The compressed visual tokens $V'$ are flattened and connected to the LLM via the projector, resulting in projected visual tokens $V^P \\in \\mathbb{R}^{N_c \\times D_l}$, where $D_l$ is the embedding dimension of the LLM.\nFor a given text input, we tokenize it to obtain textual tokens $T \\in \\mathbb{R}^{N_t \\times D_l}$, where $N_t$ is the number of textual tokens. The projected visual tokens $V^P$ and textual tokens $T$ are concatenated to form the input sequence to the LLM $S = [V^P; T] \\in \\mathbb{R}^{(N_c + N_t) \\times D_l}$, which is then fed into the LLM for next token prediction.\n3.2 Dynamic Feature Map Reduction (DFMR)\nDFMR conducts an $s \\times s$ average pooling operation on the original visual tokens $V \\in \\mathbb{R}^{N_v \\times D_v}$.\nFirst, it divides $V$ into non-overlapping windows, each window size is $p \\times p$, and $p = \\frac{H'}{S} = \\frac{W'}{S}$ is the window length. Inside each window $W_r$, where $k$ indexes the windows, we calculate the standard deviation $\\sigma_k$ of the values within that patch:\n$\\sigma_k = \\sqrt{\\frac{1}{p^2} \\sum_{(u,v)\\in W_k} (V_{u,v} - \\mu_k)^2}$,\nwhere $V_{uv}$ is the feature at position $(u, v)$ within patch $W_k$, and $\\mu_k$ is the mean of the patch, given by:\n$\\mu_k = \\frac{1}{p^2} \\sum_{(u,v)\\in W_k} V_{uv}$.\nAfter computing the standard deviation for all patches, we calculate the mean of these patches standard deviations:\n$\\overline{\\sigma} = \\frac{1}{K} \\sum_{k=1}^{K} \\sigma_k$,\nwhere $K$ is the total number of patches in $V$.\nBased on the value of $\\overline{\\sigma}$, the DFMR dynamically determines the appropriate compression factor $s$ to average pool the original visual tokens $V$. A higher $\\overline{\\sigma}$ indicates greater variability within the image patches, suggesting that less compression (smaller pooling size) is needed to preserve important details. Conversely, a lower $\\overline{\\sigma}$ allows for more aggressive compression (larger pooling size) without significant loss of information. A manually defined threshold $T$ determines when to stop increasing the compression factor $s$. For each original visual token $V$, the process starts with the smallest compression factor and incrementally increases it until $\\overline{\\sigma}$ exceeds $T$ or $s$ has reached the pre-defined maximum value. Once the process stops, the current $s$ is chosen as the final compression factor.\nAfter selecting the appropriate compression factor $s$, the average pooling operation is applied to the original feature map $V$. Each element in the resulting compressed feature map $V' \\in \\mathbb{R}^{(\\frac{H'}{s}) \\times (\\frac{W'}{s}) \\times D_v}$ is given by the average of the corresponding $s \\times s$ window in $V$:\n$V'_{i,j,c} = \\frac{1}{s^2} \\sum_{u=0}^{s-1} \\sum_{v=0}^{s-1} V_{s \\cdot i+u, s \\cdot j+v, c}$"}, {"title": "Experiments", "content": "4.1 Implementation Details\nWe use CLIP ViT-L/14[12] as the visual encoder (default resolution 336 \u00d7 336), Vicuna-7B[13] as the LLM, and a 2-layer MLP as the connector. Images are resized to 336 \u00d7 336 before being fed into CLIP. We set the DFMR between CLIP and the MLP with a fixed threshold of 5e-2, where the compression ratio in DFMR for each image is dynamically determined by the threshold and mean of standard deviation $\\overline{\\sigma}$. Following LLaVA-1.5[2], we use the same training settings for pre-training and visual instruction fine-tuning. In both stages, a cosine scheduler is applied, and the learning rate is set to 1e-3 with a batch size of 256 during pre-training, and 2e-5 with a batch size of 128 during fine-tuning. Training is conducted for one epoch using the AdamW optimizer. Evaluation is conducted using the Imms-eval[14] framework with a batch size of 1. DeepSpeed ZeRO3 is utilized as the training framework. We conduct experiments on a server equipped with 8 NVIDIA H100 GPUs, each with 80 GB of VRAM.\nTraining Details. We train three types of models for comparison: LLaVA-1.5, LLaVA-1.5 Random, and LLaVA-1.5 DFMR. For LLaVA-1.5, the number of visual tokens remains uncompressed and is fixed at 576 for all images during training. For LLaVA-1.5 Random, the number of visual tokens is randomly compressed for each image during training, with the compression factor s uniformly sampled from the set {1, 2, 3}. This results in the number of visual tokens being 576, or reduced to 144 and 64, respectively. For LLaVA-1.5 DFMR, the compression of visual tokens is determined based on our proposed DFMR method for each image during training and it also achieves compression to 576, 144, and 64 visual tokens, respectively.\nEvaluation Details. For each model, we evaluate its performance on images with compression ratios of 1, 2, and 3, corresponding to 576, 144, and 64 token scenarios, respectively. These settings effectively evaluate model performance under constrained GPU memory or when the prompt reaches the maximum token length of an LLM. For instance, when the original number of visual tokens is 576, limited GPU memory or the maximum token length may require compressing the 576 tokens into 144 or 64 tokens.\n4.2 Datasets and Benchmark\nTraining datasets. We pre-train and fine-tune the proposed model following LLaVA-1.5, using llava-pretrain-558k for the pre-training stage and llava-instruct for the fine-tuning stage.\nBenchmarks. We use 8 popular benchmarks to evaluate our method, including (1) General question-answering benchmarks such as GQA[15]; (2) Optical character-based visual question-answering benchmarks such as TextVQA[16] and VQAv2[17]; (3) MLLM benchmarks for specific abilities, like POPE[18], MM-Vet[19], and LLaVA-Bench[2]; (4) Comprehensive MLLM benchmarks such as MME[20] and SEED-Bench[21].\n4.3 Performance\nTable 1 summarizes our experimental results. When comparing LLaVA-1.5 random with LLaVA-1.5, we observe that incorporating various visual token lengths enhances the performance of LLaVA-1.5 across different visual token inputs. Since the prior model is regularly exposed to various compressed visual tokens during training, we designate it as our baseline. Comparing LLaVA-1.5 DFMR with LLaVA-1.5, we find that the performance of LLaVA-1.5 DFMR using the original 576 visual tokens is slightly lower than that of the original LLaVA-1.5. This outcome is reasonable given that LLaVA-1.5 DFMR is exposed to a smaller number of original visual tokens. However, when comparing LLaVA-1.5 DFMR with LLaVA-1.5 random, we observe that our proposed DFMR enhances model performance across all tasks and various visual token scenarios.\nOn average, we achieve significantly better results compared to both LLaVA-1.5 and LLaVA-1.5 random. Our findings indicate that our implemented baseline is highly competitive relative to LLaVA-1.5, and our proposed DFMR demonstrates clear improvements over this baseline, confirming the effectiveness of the proposed DFMR method.\n4.4 Analysis\nWe compute the standard deviations $\\sigma$ across patches for each image in the COCO validation dataset [22] for the analysis experiments conducted in this section."}, {"title": "Discussion", "content": "In this work, we propose DFMR, which dynamically adjusts the compression ratio of visual tokens based solely on the intrinsic content of each image, simultaneously improving both performance and efficiency. Specifically, we introduce a novel metric to quantify image information, determining the optimal number of visual tokens required for effective representation. DFMR achieves improvements in both computational efficiency and model performance. Extensive experiments are conducted on various mainstream multimodal evaluation benchmarks to validate the effectiveness of the proposed method. The method offers a promising solution for extending LLaVA to handle multi-images and video in resource-constrained academic environments and it can also be applied in industry settings for data augmentation to help mitigate the scarcity of open-domain image-text pair datasets in the continued pretraining stage.\nDuring the pretraining stage, token sizes typically reach the trillion level, and the number of GPU cards often scales to hundreds or even thousands within clusters. One of the major computational bottlenecks for GPU utilization in this process is loading and preprocessing the vast datasets before transferring them to the GPUs. To address this, there are two primary approaches: first, pre-transforming and saving the dataset as tokens allow for direct loading onto GPUs, reducing preprocessing time but requiring significant storage for the pre-tokenized data.[23] Second, using dataloaders that support prefetching large amounts of data enables efficient streaming to GPUs, maintaining flexibility without extensive storage needs.[24, 25] Our proposed method operates with a computational cost of O(1), meaning it does not add extra burden to the dataloader and ensures high GPU utilization.\nDuring the continued pretraining stage, maintaining an appropriate dataset mixing ratio is crucial for achieving optimal performance of MLLMs in both domain-specific downstream tasks and general applications[26]. However, open-source image-text pair datasets are relatively scarce compared to domain-specific datasets collected in industry settings, thus becoming a bottleneck for further improving the performance of domain-specific MLLMs. Additionally, generating synthetic image-text pairs is challenging due to the complexity of accurately pairing images with relevant text. The DFMR can be directly applied to data augmentation by setting a manually defined threshold that varies proportionally with the learning rate of the optimization scheduler. For example, using an LLM to generate synthetic text data corresponding to existing images can expand the image-text dataset. Applying DFMR to the expanded dataset by dynamically compressing the same images into different visual lengths with similar textual information provides more tokens for open-domain datasets.\nIn the LLaVA family, when processing data formats such as multiple images and videos, a certain number of images are sampled, and each image is converted into 576 visual tokens[27]. If the number of images is large, this can easily exceed the maximum token length predefined by the LLM, especially in resource-constrained academic scenarios. Our proposed method provides a promising solution to this issue by effectively compressing visual tokens, enabling a single model to handle single images, multiple images, and videos without surpassing token length limitations."}]}