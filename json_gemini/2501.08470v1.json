{"title": "Detecting Contextual Anomalies by Discovering Consistent Spatial Regions", "authors": ["Zhengye Yang", "Richard J. Radke"], "abstract": "We describe a method for modeling spatial context to enable video anomaly detection. The main idea is to discover regions that share similar object-level activities by clustering joint object attributes using Gaussian mixture models. We demonstrate that this straightforward approach, using orders of magnitude fewer parameters than competing models, achieves state-of-the-art performance in the challenging spatial-context-dependent Street Scene dataset. As a side benefit, the high-resolution discovered regions learned by the model also provide explainable normalcy maps for human operators without the need for any pre-trained segmentation model.", "sections": [{"title": "1. Introduction", "content": "Determining whether an event observed in a video stream is anomalous often depends on its spatial context. For example, pedestrians are expected on sidewalks and cyclists in bike lanes, but not vice versa. In real-world video anomaly detection, the appearance and motion of objects alone are insufficient to classify anomalies; we should learn the \"normal\" behavior of objects at different locations in the video based on long-term observation.\nA major trend in modern video anomaly detection (VAD) algorithms is to train convolutional auto-encoders using a reconstruction proxy task [8, 10, 20]. Future frame prediction [13, 14, 17, 20] can be considered a special form of re-construction that also addresses temporal regularity. These reconstruction-based methods have achieved great success on widely adopted VAD benchmark datasets, such as UCSD Ped2 [33], CUHK Avenue [16], and ShanghaiTech [14]. However, we observe a large performance drop when such algorithms are applied to datasets in which spatial context plays a critical role in making the anomaly decision, such as the relatively new Street Scene benchmark dataset [23]. One reason is that convolutional auto-encoders are poorly suited to learning spatial context due to their shift-invariant nature [9].\nIn contrast, high-performing VAD algorithms on the Street Scene dataset (e.g., [23, 25]) usually separate videos into hundreds of rectangular subregions, each with a dedicated normalcy model. Hundreds of subregions not only result in hundreds of normalcy models but also reduce model robustness due to the small number of available object samples per region. However, a typical surveillance camera does not capture hundreds of small regions with different normalcy patterns; meaningful regions should be automatically discovered from long-term observations.\nFig. 1 illustrates a toy experiment mimicking a typical street scenario. Suppose we can extract representations like \"pedestrians walk or jog on the sidewalk,\" \"cyclists ride within the bike lane,\" and \"cars move quickly in the traffic lane in the correct direction\" from training videos. Then, given a new event represented in the same \"region-object-attribute\" form, we should be able to achieve an almost perfect anomaly detection result. Leveraging object categories and their corresponding attributes is already widespread in the VAD community [19, 24, 28]. However, automatically extracting consistent spatial regions based on these behaviors and using them as the basis for anomaly detection is relatively unexplored territory.\nIn this paper, we describe a natural approach to discovering spatial regions that have similar expected object motion"}, {"title": "2. Related Work", "content": "Reconstruction-based Methods. Early VAD approaches were based on dictionary learning (e.g., [16]). The successors to this approach are modern reconstruction-based methods [4, 8, 10, 17, 20, 37, 40] and future-frame-prediction-based methods [13, 14, 17, 20]. In both types of approaches, an auto-encoder is trained to perform reconstruction or prediction, and anomalies are detected based on the reconstruction error of query frames. However, we observe that convolutional neural networks (CNNs), the building blocks shared by many auto-encoders, are naturally unsuitable for learning spatial context due to their shift-invariant properties. Figure 3 illustrates a toy experiment to demonstrate this issue, using the same normal/anomalous classification setup used by most VAD methods. The training data is generated so that red squares only appear in the upper left quadrant and blue circles only appear in the bottom half. Half of the testing data is generated following the same rule, while the other half generates the square and circle positions freely. We see that a convolutional auto-encoder trained from the normal data can also perfectly reconstruct the anomalous frames; spatial context has not been learned. Thus, these types of methods typically perform poorly on\nObject-centric Methods. Methods that use pre-trained networks to extract highly discriminative object-centric features often demonstrate superior performance on traditional benchmark datasets [7, 11, 12, 15, 28]. However, their ability to learn spatial context is relatively limited since many benchmark datasets contain only a few anomalous examples that depend on spatial context.\nContext Learning in VAD. As VAD algorithms' performance improves, the state of the art on several benchmarks is basically saturated (e.g., over 99.0 AUC on the UCSD Ped2 dataset). Recent research has pivoted to more nuanced normalcy modeling to detect context-dependent anomalies. Bao et al. [1] proposed pixel-level clustering to generate pseudo-labels to train a background segmentation model to help an object-centric auto-encoder predict the next frame. Sun et al. [28] proposed a pre-trained segmentation model to create scene-level background features, concatenated with motion and appearance vectors to perform scene-aware VAD. However, we must emphasize that spatial context should not be reduced to background appearance (for example, opposing lanes of traffic may have the same background appearance). Further, scene dependence only constrains the normality to be associated with specific cameras or scenes instead of regions within a camera. It is also important to decouple background appearance and spatial context to develop unbiased VAD algorithms.\nYang and Radke [38] proposed a contrastive learning method to learn spatial-temporal context on the frame and patch level, focusing on determining temporal context-dependent anomalies. Singh et al. 's EVAL [25] followed a similar strategy to the original Street Scene paper [23], separating a video clip into hundreds of overlapping spatial tubes and storing region-specific exemplar features extracted from a set of attribute networks to detect spatially dependent anomalies. The follow-on method T-EVAL [26] stores tracklets with corresponding visual features as exemplars and uses nearest neighbor search to detect anomalies. It is worth noting that the search is performed within a 3x3 grid region centered on the query exemplar location to speed up the search process. This can still be considered grid-based modeling. Although there is no dedicated model for each region, when the number of training videos increases, both storage and computational complexity become unmanageable. Our method discovers spatial semantic regions that drastically decrease the number of dedicated models required to cover the scene and achieves better VAD performance compared to these methods.\nExplainability in VAD. Explainability in VAD is a desirable property addressed by prior work in various ways [26, 29, 35]. Szymanowicz et al. [29] used a pre-trained action recognition module to provide action labels for each detected anomaly bounding box as a way to explain the anomalous events. Wu et al. and EVAL [26] both explain anomalies using high-level attribute features. Recently, several approaches have adopted Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for the VAD task [2, 18, 39]. While both LLMS and MLLMs show promise in reasoning and explainability, their success in VAD tasks is due more to the huge overlap between commonsense knowledge encoded in those models and the definition of abnormality in benchmark datasets. Moreover, such commonsense knowledge might introduce harmful bias in determining anomalies using appearance, background bias, or lack of context for the specific area that the camera covers. Our proposed method can provide statistical patterns of appearance and motion that could be further passed to LLMs or MLLMs for natural-language-based explainable VAD.\nRegion discovery. An older line of work investigates region discovery strategies in video [3, 31, 32, 42]. For example, [3] uses a pixel-level foreground ratio within a time window to find similar activity regions through correlation. Wang et al. [32] uses a thermal-diffusion based approach on the optical flow field with clustering to create semantic regions. However, this region clustering is performed at the pixel level and can mainly handle collectively moving dense particles or large objects with a unified motion. Our region discovery method is based on object-level activity and has no requirement about objects' collective motion or size."}, {"title": "3. Method", "content": "As stated in the thought experiment, our goal is to build a model that approximates the \u201cregion-object-attribute\" idea. To achieve this, our method has three key stages: object representation, region discovery, and anomaly detection, as illustrated in Figures 4 and 5. We rely on the object representation to discover semantically meaningful regions and\nto train normalcy models. To define the problem, suppose we have a collection of N training video clips from the same stationary camera, {v1,\u2026\u2026,vv} \u2208 RT\u00d7H\u00d7W\u00d7C, where T, H, W, C denote the temporal window size, height, width and color channel respectively. We will create a H \u00d7 W map corresponding to K spatial regions, each of which contains a similar set of activities. The learned model in each region then forms the basis for spatial-context-dependent anomaly detection.\n3.1. Object representation\nMany object-centric VAD methods [1, 7, 15, 24, 28] use pretrained networks to extract various visual features. We also follow this approach, forming tracklets and extracting object attributes from detected object regions.\nObject region proposal. We first feed each video clip x into a pre-trained YOLOv8 model [30] to obtain object bounding boxes. Due to occlusions, varying lighting conditions, and the top-down perspective in the Street Scene dataset, solely relying on bounding boxes from object detection results in many missed detections. To capture as many bounding boxes as possible, we merge the detection results with foreground objects detected using a simple Gaussian mixture background model.\nFalse region removal. Using the fused detection strategy above provides excellent bounding box proposals that include most of the ground-truth objects. However, both object detection and foreground subtraction introduce a significant number of region proposals that only cover pieces of background regions due to false detections. This problem can be greatly mitigated by using LPIPS [41] to compare each region proposal with the learned background, retaining proposals with LPIPS loss greater than a threshold a.\nObject class descriptor. Although region proposals from object detection contain category information, region proposals from background subtraction do not. For each detected bounding box, we apply a pre-trained CLIP vision encoder (Vit-B32) [21] and use the CLS token from its output as an appearance feature vector fapp \u2208 R512. We perform zero-shot classification with a set of pre-selected expected classes (person, bicycle, car, motorcycle) with the text prompt templates used for Imagenet classification in [21]. We use one-hot encoding to represent the object class information fobj \u2208 R4 as a compact and highly abstract representation for the following process.\nMotion descriptor. We compute the Farneback optical flow [5] of the whole clip v flow \u2208 R(T\u22121)\u00d7H\u00d7W\u00d72 to compute motion attributes. For each detected bounding box, we use Histogram-of-Flow (HOF) to summarize the motion v flow within the box. We use 12 uniformly spaced orientation bins and 1 background ratio bin. Pixels whose magnitude are below a certain threshold (1.5 in our experiment) are considered as background pixels and aggregated in a single bin. The average speed is calculated for each orientation bin. The motion attributes fmot for the training/inference normalcy model are the dominant orientation and corresponding speed of the given tracklet resulting in fmot \u2208 R2.\nTracklet formulation. To incorporate temporal information, we use the DeepSORT algorithm [34] to associate bounding box identities across time with the extracted CLIP embedding fapp. Since the full trajectory lengths might vary, we create tracklets 1 \u2208 R2\u00d7tw with a fixed length window tw. Thus for each detected object we formulate a combined feature O = [fobj, fmot, li]t at each time stamp t, which we use as the basis for the following process."}, {"title": "3.2. Region Discovery Strategies", "content": "In frame-based methods, each spatial location has an equal amount of training data. This is not the case when considering the alternative object-centric route. The region representation for object-centric methods requires more careful design to both reduce the number of separate models and improve detection performance in regions where a limited number of samples is available.\nGrid Region Separation. The most straightforward way to introduce spatial context in object-centric VAD is to split the image into a non-overlapping rectangular grid, design a model for each rectangle, and make a frame-level determination about each object based on the rectangle containing the center point of its detected bounding box.\nIn our experiment, we found that using small patches (e.g., 40\u00d740 as used in [23]) results in a large number of patches that don't contain a single example; thus no model can be trained for those regions. We chose an 80\u00d780 rectangle size as our baseline in our comparisons below. This large patch setting still generates 144 separate models.\nRegion Discovery. Our key innovation is to cluster regions with similar appearance/motion patterns, resulting in a substantial decrease in the number of models to be trained. Our clustering approach is agnostic to the type of feature representation.\nTo create a high-resolution (around 1 million pixel) region map, we need to first discover what object-level events happen at each pixel location. For each object tracklet moving through the pixel, we determine its dominant motion direction and quantize its speed into 4 log-scale-spaced magnitude bins. Thus for each object, we acquire its class information, dominant motion direction, and speed. We initialize an empty heatmap M \u2208 RH\u00d7W\u00d7D with the same size as the original frame and D channels corresponding to the given attributes, which we will incrementally update using the training data.\nTo provide enough information to create a high-resolution map, we center a Gaussian kernel around the tracklet pixel xc, yc, using the o parameter to control the spatial influence of each moving object on surrounding pixels (and zeroing out the contribution outside the object bounding box). We then add this to the incrementally constructed heatmap M in (Eqn. 1).\nG(x, y) = exp((- (x - xc)^2 + (y - Yc)^2) / (2\u03c3^2))\n1box(x, y) = {1 if xtl \u2264 x \u2264 xbr and ytl \u2264 y \u2264 Ybr,0 otherwise,\nM(x, y, d) \u2190 M(x, y, d) + 1box(x, y) \u00b7 G(x, y) (1)\nAfter constructing M, we need to discover the regions that share similar activities. To capture the relationship between attribute channels, we use a Gaussian Mixture model with full covariance (Eqn. 2) to learn K modes from M. In Section 4 we verify the importance of using full covariance compared to other alternatives. The learned modes can be used as the labels for each discovered region, as shown in Fig. 5.\np(f) = \\sum_{i=1}^{K} \\pi_i \\cdot p_i(f) \\text{ with } p_i(f) \\sim N(\\mu_i, \\Sigma_i) (2)"}, {"title": "3.3. Regional Model", "content": "We use another Gaussian Mixture Model (GMM) as the basis for representing the normality distribution of the features in each region. Different regions may exhibit more or less appearance/motion diversity in the objects that pass through them, resulting in variability in the number of components required for the GMM to adequately represent the underlying data distribution. In our experiments, we use the Bayesian Information Criteria (BIC) score to guide the selection of the number of Gaussian modes for each region.\nIn order to select the optimal K in region discovery, we propose a simple evaluation metric based on symmetric KL divergence. A good region discovery method should result in regions that contain different normalcy distributions. Thus, each region's pi(f) normalcy model should assign a low likelihood to normal features from other regions. For all region pairs i, j \u2208 K, i \u2260 j, we use the average symmetric KL-divergence \u00b5\u03ba\u03b9 (Eqn. 3) of sample scores to measure the region separation quality.\nD_{sym}(P_i, P_j) = D_{KL}(P_i||P_j) + D_{KL}(P_j ||P_i)\n\u03bc_{KL} = \\frac{1}{K(K-1)} \\sum_{i\\neq j} D_{sym}(P_i, P_j) (3)\nIn contrast to rectangular grids, in which each region may only contain a few objects' features as training samples, using our larger discovered regions drastically increases the number of training samples for each regional model.\nLearned Regional Normalcy. As a byproduct of our algorithm, we can use the cluster results and the learned mixtures from GMMs to visualize the prototypical normal activities within each region. For example, we can find the object in the training data whose feature is nearest to the mean of each Gaussian in a region's mixture, as in the right hand side of Figure 2."}, {"title": "3.4. Inference", "content": "During inference, we extract object tracklets and features just as in the training process. We assign objects to their enclosing region (either a rectangular box as in comparison methods or the non-rectangular regions discovered by our method) based on the first center position of their tracklets and evaluate using the corresponding model shown in Figure 4. For GMM models, we use the negative log-likelihood (NLL) as the indicator of anomaly. To compute a frame-level anomaly score, we simply select the highest NLL score from tracklets within each frame."}, {"title": "4. Experimental Results", "content": "4.1. Dataset and Evaluation Metrics\nDatasets. We mainly evaluate our method on the Street Scene dataset [23] since it is one of the few publicly available datasets to address spatial-context anomalies. This contains 46 training and 35 testing videos under a single-scene setting with 720\u00d71280 resolution. The training and testing videos contain 56,847 and 146,410 frames respectively. The normal videos contain typical events like pedestrians walking and vehicles/cyclists moving properly in their lanes. However, the testing videos contain 17 types of spatial-context anomalies such as jaywalking, cars outside the correct lane, and cyclists on the sidewalk. We also report our performance on the widely adopted Ped2 [33] and ShanghaiTech [14] datasets. Since ShanghaiTech contains 13 different scenes, our method needs to create region maps for each camera and train the models separately.\nEvaluation criteria. We use both the standard frame level Area-Under-Curve (AUC) [33] and the Region/Track-based Detection Criteria (RBDC/TBDC) proposed in [23]. For detailed definition, we refer readers to [22]. We report Micro-AUC as the frame level evaluation metric that concatenates all test videos and computes the AUC score. RBDC and TBDC measure the AUC of the region/track matching rate across false positive rates ranging from 0 to 1. While [23] highlights the shortcomings of using the frame-level AUC metric (e.g., its inability to account for multiple false positives or multiple anomalies within a frame), RBDC and TBDC also have limitations. Most studies that employ RBDC/TBDC metrics in their evaluations adhere to the original thresholds: a region match is defined as having at least 0.1 intersection-over-union between the predicted and ground truth regions, and a track match is considered successful if at least 10% of the anomalous trajectory is detected.\n4.2. Implementation details\nWe provide implementation details in this section, for more detailed information, please refer to supplementary.\nObject detection fusion. The detection results are created from both a pre-trained YOLOv8-x [30] model and background subtraction. To obtain foreground objects, we initialize a Mixture of Gaussians background model [27] with variance threshold set to 16 at every individual video clip.\nAcquiring tracklets. We set the DeepSORT algorithm with a minimum of 3 frames associated for each track. tw is set to 3 for Street Scene and ShanghaiTech and 1 for UCSD Ped2.\nRegion discovery and normalcy model settings. The GMMS for region discovery and for normalcy modelling are set to use full covariance. The maximum component per region of BIC search is set to 20. We search the parameter K over the range [2, 16]. We apply Gaussian filtering to smooth the anomaly score.\n4.3. Quantitative Analysis\nTable 1 summarizes our algorithm's performance on the context-dependent Street Scene dataset. We use a 9\u00d716 grid (i.e., 80\u00d780 pixel regions) as a baseline, denoted \"Grid Sep.\" in the table. Our method outperforms the previous algorithms in terms of the frame-level AUC and the RBDC criterion. While other methods achieve higher TBDC, our algorithm demonstrates competitive TBDC with 1-2 orders of magnitude fewer regions and compact feature representation. This indicates that our feature design can enable consistent detection for each anomaly trajectory."}, {"title": "5. Conclusions", "content": "We proposed a simple but effective method for detecting spatial-context anomalies using region discovery. There are several limitations of the proposed method as shown in Figure 8. Since our model relies on appearance and motion features within a short time window, its ability to detect long-term trajectory-oriented anomalies like soliciting is limited. This also partially explains why we achieve state-of-the-art performance on the region-based criteria RBDC but not on the track-based criteria TBDC. We identified several opportunities to improve the proposed algorithm in future work.\nFirst, the current hard region assignment might magnify the impact of incorrect region clustering. It would be helpful to explore soft region assignment which should alleviate this problem. For more person action oriented datasets like ShanghaiTech, incorporating pose features [11, 24] could further boost performance."}]}