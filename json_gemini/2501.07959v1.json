{"title": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern and Behavior Learning", "authors": ["Jiaqi Hua", "Wanxu Wei"], "abstract": "Recently, several works have been conducted on jailbreaking Large Language Mod-els (LLMs) with few-shot malicious demos. In particular, Zheng et al. [51] focuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search. Nevertheless, this method lacks generality since it specifies the instruction-response structure. Moreover, the reason why inserting special tokens takes effect in inducing harmful behaviors is only empirically discussed.\nIn this paper, we take a deeper insight into the mechanism of special token injection and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the model's vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at https://github.com/iphosi/Self-Instruct-FSJ.\nWARNING: This paper contains example data that may be offensive or malicious.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), despite their remarkable success across various tasks, are susceptible to adversarial attacks and thus can be misused for illegal or unethical purposes [4, 45, 10, 15]. \u03a4\u03bf handle these safety issues, substantial efforts have been devoted to research in aligning LLMs with moral principles [28, 11, 5, 49, 29, 41, 40, 25]. Jailbreaking, as a closely related field of safety alignment, focuses on exploiting vulnerabilities in LLMs as well as unveiling the flaws in existing alignment techniques, and thereby contributes to the development of harmless and helpful LLMs. Existing jailbreaking techniques can be roughly divided into inference-time and training-time ones [13]. Generally, inference-time attacks leverage adversarial prompts to bypass the defense mechanism of LLMs while training-time attacks unalign the LLMs through fine-tuning with contaminated data. This paper mainly focuses on inference-time attacks as they are relatively less resource-intensive and thus more efficient and affordable.\nInference-time attacks can be further classified into optimization-based [26, 8, 54, 51] and optimization-free [48, 24, 39, 40, 3] methods. The former ones iteratively refine some adversarial prompts tailored by gradient or loss heuristics or critiques from auxiliary LLMs. On the other hand, the latter ones elicit harmful model behaviors through manually or automatically crafted malicious instructions. Typically, Wei et al. [40] proposes the In-Context Attack (ICA), i.e. jailbreaking LLMs with harmful instruction-response demos, which does not require elaborate prompt engineering and possesses a high interpretability. Anil et al. [3] enhances ICA by leveraging the long-context capability of the LLMs. To further improve the efficiency, Zheng et al. [51] facilitates the vanilla\nPreprint. Under review."}, {"title": null, "content": "Few-Shot Jailbreaking (FSJ) with heuristic search and special token injection, known as Improved Few-Shot Jailbreaking (I-FSJ). Despite its effectiveness, this improved strategy is not generalized due to the specified instruction-response structure. To be precise, I-FSJ applies the target response prefixes of AdvBench [54] as the model inputs rather than the original instructions and restricts the responses to be stepwise. Moreover, the mechanism behind the efficacy of special token injection is only empirically explained. And we notice that I-FSJ still necessitates a long context to jailbreak advanced models such as Meta-Llama-3-8B-Instruct (Llama-3) [1].\nIn this paper, we analyze the underlying mechanism of I-FSJ [51] and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) to tackle with the limitations of I-FSJ. Our contributions can be summarized as follows:\n\u2022 We empirically design an instruction suffix and the corresponding response prefix, which does not affect the flexibility of the response structure.\n\u2022 We decompose the FSJ attack into pattern learning and behavior learning. To induce a target model to generate the predefined prefix, we extend the instruction suffix with co-occurrence patterns, which consist of the model-specific chat template tokens and the target response prefix. We denominate this process as self-instruct pattern learning. Furthermore, we introduce a method to sample malicious demos directly from the target models so that we can conduct self-instruct behavior learning.\n\u2022 We propose an intuitive yet efficient demo selection strategy called demo-level greedy search.\n\u2022 We conduct comprehensive experiments to demonstrate the effectiveness of our jailbreaking framework on prevalent open-source models. Remarkably, our method achieves about 90% Attack Success Rate (ASR) on advanced LLMs including Llama series within 8 shots of concise demos (demo response length less than 50 tokens), evaluated by AdvBench [54] and HarmBench [27]. Moreover, our method exhibits good resilience to jailbreaking defenses including perplexity filter [19, 2] and SmoothLLM [33]."}, {"title": "Related works", "content": "Training-time jailbreaking attacks. Generally, such kinds of attacks [16, 22, 44, 42, 7, 30, 38, 52] involve supervised fine-tuning (SFT) or reinforcement learning (RL) on poisoned data. They require not only full accessibility to the model parameters but also considerable data and computational resources.\nInference-time jailbreaking attacks. A series of studies have been conducted on jailbreaking LLMs at inference time [26, 8, 54, 51, 48, 24, 39, 40, 3]. Gradient-heuristic attacks like GCG [54] and AutoDAN [26] optimize an adversarial instruction suffix to maximize the likelihood of generating the target harmful response prefix but usually encounter an efficiency bottleneck. Critique-heuristic strategies including PAIR [8] attempt to refine the adversarial prompt based on critique feedback from an assistant LLM, which requires additional GPU memory space during optimization. There also exists optimization-free methods such as PAP [48], DAN [34], DeepInception [24] that apply sophisticated prompt engineering techniques to disable the safety guard of LLMs. Some algorithms leverage the mismatched generalization issues [39] to undermine the safety defense, i.e. encoding the model inputs into a form that is out-of-distribution for the safety alignment data [47, 32, 21, 12, 46]."}, {"title": "Methodology", "content": "In this section, we illuminate our proposed Self-Instruct-FSJ in detail. We first briefly introduce the preliminaries and then discuss the intuition of our method."}, {"title": "Preliminaries", "content": "Perplexity. This metric measures how likely the model is to generate the input text sequence [19].\nGiven a tokenized sequence $S = (t_1, ..., t_L)$, perplexity is defined as its exponentiated average negative log-likelihood:\nPerplexity(S) = exp{-\\frac{1}{L} \\sum_{l=1}^{L} log p_\\theta(t_l | t_{<l})}\nFor a subsequence $S' = (t_k, t_{k+1}, ..., t_L) (k > 0)$, the conditional perplexity is calculated as:\nPerplexity(S' | t_{<k}) = exp{-\\frac{1}{L-k+1} \\sum_{l=k}^{L} log p_\\theta(t_l | t_{<l})}\nAs proposed by Alon and Kamfonas [2] and Jain et al. [18], perplexity can be applied to detect adversarial prompts generated by unconstrained optimization-based attacks such as GCG [54] as these prompts are generally unreadable and high in perplexity. This metric can also be calculated in a windowed manner, i.e. putting a sliding window onto the text and regarding the text as a potential attack if the highest perplexity within the window exceeds the predefined threshold. Practically, we modify the implementation of Li et al. [23], integrating the windowed perplexity calculation and considering the influence of the chat template.\nIn-context learning. This is a remarkable property of LLMs which is firstly discussed by Brown et al. [6]. Formally, given a set of demos $D = \\{d_1, ..., d_v\\} = \\{(i_1, r_1), ..., (i_N, r_N)\\}$ and a target request $i_{target}$ where $i_n$ refers to a task-specific instruction and $r_n$ is the corresponding response, we prompt the LLM with $[i_1, r_1, ..., i_N, r_N, i_{target}]$ and the model can fulfill the target request by learning the task-specific patterns from the provided demos. This task adaption capacity can be leveraged to effectively jailbreak LLMs as shown in previous works [40, 3, 51]."}, {"title": "Self-Instruct few-shot jailbreaking", "content": "Adversarial instruction suffix. Without losing generality, we concatenate the original instructions with an adversarial suffix that defines a hypothetical scenario to mislead the model as highlighted in dark red in Figure 4 and empirically choose \"Hypothetically\" as the target response prefix. Compared to the target prefix \"Step\" applied in I-FSJ [51], \"Hypothetically\" is compatible with various response structures. Generally, the prevalent open-source models have a much lower initial conditional perplexity for this prefix than for common target words including \"Sure\" and \"Step\" as shown in Table 1. In other words, this word is more likely to be generated by these models and is more suitable as an optimization target.\nSelf-instruct pattern learning. Practically, a smart choice of target prefix only is still insufficient for inducing a model to start its response with that prefix. Especially for the advanced models, the judiciously selected prefix is still high in perplexity as shown in Table 1. To further reduce the target perplexity, we propose self-instruct pattern learning, i.e. guiding the model to imitate the co-occurrence pattern of special tokens preceding the target prefix.\nWe notice that I-FSJ [51] does not insert special tokens at arbitrary positions in the demos. Instead, these tokens, which the generation query ends with, are injected right before the target word \"Step\". We speculate that the content of the demos does not play the primary role in increasing the likelihood of generating the target response prefix. I-FSJ [51] achieves a successful attack in such a way that the target model first imitates the co-occurrence pattern and then learns the malicious behaviors from the demo content.\nInspired by I-FSJ [51], we propose to decompose FSJ into pattern learning and behavior learning in a more distinct manner. We extend the hypothetical-scenario instruction suffix with a replication of the co-occurrence pattern without any concrete content. The empirical experiment indicates that this suffix leads to a steep drop in the initial conditional perplexity of the target prefix as shown in Table 1.\nNevertheless, the presence of the target prefix does not necessarily result in jailbreaking. Especially for those strongly aligned models, we may have to sample multiple times to achieve a successful attack. Moreover, blindly increasing the pattern frequency will lead to behavior degeneration, i.e. the model trivially imitates the co-occurrence pattern without producing any meaningful text. Due to the inefficiency and degeneration issue of pattern learning demonstrated in Figure 4, concrete demos are still essential to malicious behavior induction."}, {"title": null, "content": "Demo-level greedy search. After the demo pool is constructed, we should select appropriate demos for the target requests. I-FSJ [51] proposes to initialize the context with arbitrary demos and conducts demo-level random substitution for a couple of iterations to maximize the likelihood of generating the target response prefix. However, if the demo pool is not well-contrived, such an initialization strategy may introduce a large amount of counterproductive data, and we have to take extra iterations to remove these detriments. Furthermore, I-FSJ randomly substitutes demos without considering their optimal positions and thus an effective demo may not be fully utilized due to its suboptimal position within the few-shot context.\nIntuitively, the generation of a model depends more on the context at the end of the input query than that at the beginning. Following this intuition, we propose a more efficient strategy called demo-level greedy search in which the demos are selected sequentially as described in Algorithm 1. Perplexity [19] is chosen as a heuristic equivalent to loss to avoid underflow issues. We analyze the corresponding computational complexity in Appendix B. The algorithm works as follows: (i) Randomly select instruction-response pairs from the demo pool. (ii) Filter out candidates based on instruction similarity to avoid leakage. (iii) Sequentially concatenate the demo candidate, the selected demos, the instruction, and the target prefix to calculate the conditional perplexity. (iv) Select the demo that maximizes the relative drop in conditional perplexity w.r.t the last iteration. (v) Iteratively select demos until the required number of shots is reached.\nAfter the searching process is terminated, the selected demos are concatenated with the target request to form a few-shot jailbreaking query as shown in Figure 6."}, {"title": null, "content": "Require: instruction i, instruction embedding v, target t, similarity threshold s, demo pool D,\nnumber of shots N, batch size B\nS = [] \u25b7 Selected demos\np = Perplexity(t | i) \u25b7 Initial conditional target perplexity\nfor n = 1 \u2192 N do\nC1:B := Uniform(D, size = B) \u25b7 Randomly select instruction-response pairs\nC1:B' := Filter(CosineSimilarity(v, vcr) < S, C1:B) \u25b7 Avoid leakage\nc* := argma\u0445\u0441\u044c [1 \u2013 Perplexity(t | c\u044c, S, i) / p ] \u25b7 Compare relative perplexity drop\np = Perplexity(t | c*, S, i)\nAppend c* to the left of S\nend for\nreturn S"}, {"title": "Experiments", "content": "In this section, we provide the results of the empirical experiments, which demonstrate the effective-ness of our method on various open-source models."}, {"title": "Implementation details", "content": "Target models. We evaluate commonly used open-source models including Llama-2-7b-chat-hf (Llama-2) [36], Meta-Llama-3-8B-Instruct (Llama-3) [36], Meta-Llama-3.1-8B-Instruct (Llama-3.1) [14], OpenChat-3.6-8B (OpenChat-3.6) [37], Qwen2.5-7B-Instruct (Qwen2.5) [43, 35], Starling-LM-7B-beta (Starling-LM) [53]. The corresponding special tokens for co-occurrence pattern construction can be found in Appendix C.\nBenchmarks. The holistic AdvBench [54] harmful behaviors dataset, containing 520 instructions in total, is used to synthesize demos. For evaluation, we randomly select 50 test cases from AdvBench [54] and HarmBench [27] respectively.\nMetrics. Attack success rate (ASR) is widely used to measure the model's susceptibility to jail-breaking attacks. We first feed the attacks into the model to acquire responses and then calculate the"}, {"title": null, "content": "proportion of unsafe outputs, i.e. ASR through a classifier. To mitigate the influence of generation randomness and comprehensively evaluate the jailbreaking effectiveness, we introduce two ASR variants i.e. response-level ASR and sample-level ASR:\n$ASR_{response} = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{M}I_{classifier}(r_{ij})}{NXM}$\n$ASR_{sample} = \\frac{\\sum_{i=1}^{N}VM_{j=1}I_{classifier}(r_{ij})}{N}$\n$I_{classifier}(r_{ij}) = \\begin{cases} 1, & \\text{if } r_{ij} \\text{ unsafe} \\\\ 0, & \\text{else} \\end{cases}$\n$\\bigvee_{j=1}^{M}I_{classifier}(r_{ij}) = \\begin{cases} 1, & \\text{if } \\exists r_{ij} \\in R_i \\text{ unsafe} \\\\ 0, & \\text{else} \\end{cases}$\nwhere $N$ is the number of test samples and $M$ is the number of responses for each sample. These two variants indicate the attacks' capability of misleading the model within a fixed number of queries.\nPrevious works [51, 26] employ rule-based and LLM-based ASR as performance metrics. However, we observe a common phenomenon that the model rejects the adversarial request while still producing malicious content afterward, as illustrated in Figure 7, which results in a considerable number of false negative predictions made by the keyword-based detector. For the sake of reliability, we dismiss the rule-based evaluation and apply Llama-Guard-3-8B [17] together with the same evaluation prompt (see Appendix E) used by Zheng et al. [51] and Chao et al. [9] as the LLM-based classifier.\nDefenses. We evaluate the performance of our method on Llama-2 enhanced with two jailbreaking defenses: perplexity filter [19, 2] and SmoothLLM [33]. For perplexity filter, we calculate the total perplexity as well as the windowed one with a window size of 20 using GPT-2 [2]. The threshold is set to the highest perplexity value of the natural language instructions. For SmoothLLM, we consider strategies including Insert, Patch, and Swap with a perturbation rate of 10%. Note that we fisrt obtain FSJ queries based on the undefended model and then feed them into the defended one, rather than directly attack the defended model using demo-level greedy search.\nBaselines. We compare our method with 5 baselines including I-FSJ [51], AutoDAN [26], PAIR [8], PAP [48], and GCG [54]. For I-FSJ [51], demo-level random search shares the same demo pool with our method and runs for 128 iterations with a batch size of 4. For AutoDAN [26], the number of steps is set to 100 and the other hyperparameters are set to default values. For PAIR, the jailbreaking prompt is refined for 10 rounds. For PAP [48], we apply 40 persuasion taxonomies. For GCG [54], the optimization process is run for 1024 steps. Qwen2.5-72B-Instruct-AWQ [35] acts as the assistant for all algorithms that require an auxiliary model."}, {"title": "Demo synthesis", "content": "As is mentioned in Section 3.2, a successful jailbreaking can be achieved without great effort by concatenating the adversarial response prefix with the generation query. Table 2 indicates that we can sample at least one malicious response from each target model for more than 90% of the AdvBench instructions through this simple attacking strategy, which guarantees the demo diversity.\nWe further analyze the averaged instruction-response length from the I-FSJ (Mistral) demo pool and ours, respectively. In contrast to I-FSJ [51], we construct the FSJ query with more concise demos."}, {"title": "Attack on target models", "content": "In this section, we conduct ablation experiments to study the influence of the adversarial instruction suffix, different demo selection strategies, and demo sources on jailbreaking.\nAdversarial instruction suffix. As shown in Table 5, merely concatenating the instruction with the hypothetical-scenario suffix is sufficient for achieving a high ASR on models with weak alignment. Surprisingly, Llama-3.1 is much more susceptible to this adversarial instruction suffix than Llama-3. By extending the suffix with co-occurrence patterns, the advanced models also start to respond with toxic content. Remarkably, Llama-2 exhibits the best resistance to zero-shot jailbreaking attacks with the extended suffix. These observations may indicate some potential contamination accumulated in the training data during the iteration of the Llama series.\nDemo selection strategy. From Table 5, we can recognize the suboptimality of FSJ with random demos i.e. the vanilla ICA [40]. Randomly selected demos are very likely to restrain the perplexity drop and the improvement in ASR as the number of shots increases, especially when the target model is well-aligned. For Llama-3, the ASR under the few-shot setting is even overwhelmed by that under the zero-shot setting. Thus, random initialization is by no means a good strategy to accelerate the demo searching process. On the other hand, the demo-level greedy search consistently outperforms the demo-level random search as it abandons the random initialization and takes the significance of position optimality into account.\nDemo source. Despite the comparable effectiveness in lowering the target prefix perplexity, attacks with demos sampled from auxiliary models are less competent than self-instruct behavior learning in general. Typically, as shown in Table 5, Llama-2 is more resilient to attacks with demos generated by Qwen2.5 and Starling-LM. As the number of shots increases, the self-generated demos can contribute to a relatively steady ASR growth while those sampled from Qwen2.5 and Starling-LM either worsen the jailbreaking performance or result in a fluctuation in ASR. This can be explained from the perspective of perplexity. As presented in Table 3, the averaged perplexity of Llama-2 for self-generated demos is much lower than that for Qwen2.5-generated and Starling-LM-generated ones. Intuitively, toxic behaviors with high perplexity would be more challenging for the target models to imitate, which is consistent with our observation in Table 5.\nTo improve the effectiveness of demos sampled from an assistant model, we can filter out demos for which the target model have a high perplexity. As shown in Table 4, with a perplexity threshold of 6, FSJ attacks with Starling-LM-generated demos can achieve comparable ASR to self-instruct"}, {"title": null, "content": "behavior learning. However, as we further tighten the perplexity constraint, the attack performance is not improved as expected. This can be explained by the discovery of Zhao et al. [50]: Diversification of adversarial prompting is crucial to efficiently jailbreaking LLMs. A strict perplexity constraint would impair the diversity of the demo pool and thereby lead to performance degeneration.\nAnother intriguing observation is that the response-level ASR on OpenChat-3.6 and Starling-LM keeps dropping as the number of shots increases, even though we apply demo-level greedy search and self-generated demos. We leave this as an open-ended question for future research."}, {"title": "Attack against jailbreaking defenses", "content": "In this section, we investigate whether our method can maintain its effectiveness when the target model is guarded by perplexity filter [19, 2] and SmoothLLM [33].\nStealthiness against perplexity filter. Remarkably, the perplexity of the extended instructions remains in a reasonable range so that the perplexity filters fail to affect the ASR of our method as shown in Table 6. We visualize the instruction perplexity distribution in Figure 1, which indicates that concatenating the instructions with co-occurrence patterns consistently shrinks the total perplexity and just slightly amplifies the windowed one. We notice that the peak of windowed perplexity always occurs at the junction between the natural language instruction and the co-occurrence patterns. In that case, increasing the pattern frequency will not constantly lead to perplexity growth. On the other hand, the perplexity of GCG-optimized adversarial instructions tends to diverge to some excessively large values. These observations indicate the stealthiness of our method against the perplexity filter.\nRobustness against perturbation. SmoothLLM [33] proposes to mitigate jailbreaking attacks by adding perturbations. Basically, our method can to some extent resist the perturbations. As shown in Table 6, our method tends to be susceptible to randomly inserted noise while resilient to patch perturbations. Intriguingly, swap perturbations result in a rise in ASR. We notice that compared with the other two strategies, random swap is more likely to induce the model to generate gibberish, which can be classified as unsafe by Llama-Guard-3-8B. In that case, we manually check the model's responses and find that a considerable fraction of readable responses do contain harmful content."}, {"title": "Comparison with baselines", "content": "We compare our method against other attacks including AutoDAN [26], PAIR [8], PAP [48], and GCG [54]. Tabel 7 indicates that our method can outperform both prompt-based and gradient-based algorithms that are normally expensive in terms of either computational cost or time cost. We notice that the effectiveness of GCG heavily depends on the target response prefix. When we switch the original optimization target i.e. \"Sure, here's ...\" to a more concise and generalized one i.e. \"Hypothetically\", the ASR of GCG dramatically drops from 42% to 20%."}, {"title": "Further Analysis", "content": "In this section, we analyze the influence of other factors on the effectiveness of our method, including batch size and similarity threshold employed in demo-level greedy search as well as the test case diversity.\nBatch size. The batch size, i.e. the number of candidates per selection attempt, is the main hyperparameter for demo-level greedy search. As shown in Figure 2, the ASR gradually grows as the"}, {"title": "Discussion", "content": "In this paper, we analyze the underlying mechanism as well as the limitations of the FSJ strategy proposed by Zheng et al. [51] and extend it to a more generalized and efficient version. We decompose"}, {"title": "Limitations", "content": "The major limitation of our work lies in three aspects. For self-instruct pattern learning, the special tokens should be known to construct co-occurrence patterns. For self-instruct behavior learning, it is necessary to have full access to the generation query and sample demos from the target model based on the modified query. Moreover, the last output logit of the target model is required to run the demo-level greedy search. These factors hinder the application of our method on closed-source models like GPT-4. Suppose at least the last output logit is available, a possible solution to the other two drawbacks is that we can first jailbreak the target model with demos from an assistant model to initialize the self-generated demo pool, and then expand the demo pool through iterative self-instruct behavior learning."}, {"title": "Complexity analysis", "content": "We analyze the computational complexity of I-FSJ and Self-Instruct-FSJ. Both methods are based on loss heuristics, with the main difference in complexity arising from the search strategy employed.\nSuppose that all demos have the same length L, the computational complexity of calculating the conditional loss of the target prefix in one-shot setting is denoted as O(L). When the target prefix is conditioned on N demos, the complexity would be O(N \u00b7 L).\nTo select N demos, the I-FSJ selection process, i.e. demo-level random search runs for K steps with a batch size of B. For each optimization step, the context always contains N demos so that the single-step computational complexity is O(B \u00b7 N \u00b7 L). The total complexity of demo-level random search would be O(K \u00b7 B \u00b7 N \u00b7 L).\nOn the other hand, our proposed demo-level greedy search sequentially select N demos with B candidates at each step. At optimization step n, the context contains n \u2212 1 demos and the single-step computational complexity is O(B \u00b7 n \u00b7 L). We can easily derive the total complexity of demo-level greedy search: \u039f($\\sum_{1}^{N}$ B\u00b7\u03b7\u00b7L) = O(B\u00b7N\u00b7(N+1)\u00b7L).\nOur empirical experiments run demo-level random search for 128 steps with a batch size of 4. For demo-level greedy search, the batch size is set to 64. To select 8 demos, the total complexity of demo-level random search can be roughly estimated as O(128 \u00b7 4 \u00b7 8 \u00b7 L) = O(4096 \u00b7 L) while demo-level greedy search has a complexity of O(64\u00b78\u00b79L) = O(2304 \u00b7 L). With a lower computational complexity, demo-level greedy search still outperforms demo-level random search accross the experiments as shown in Table 5."}]}