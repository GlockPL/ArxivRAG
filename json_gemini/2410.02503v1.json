{"title": "Mixed-Session Conversation with Egocentric Memory", "authors": ["Jihyoung Jang", "Taeyoung Kim", "Hyounghun Kim"], "abstract": "Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce MIXED-SESSION CONVERSATION, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MISC to implement this system. The dialogue episodes of MISC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker's perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MISC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MISC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.", "sections": [{"title": "1 Introduction", "content": "Dialogue systems have been evolving along two dimensions: depth, for supporting long-term interactions, and width, for accommodating a greater number of conversation partners. Multi-session conversations (Xu et al., 2022a; Bae et al., 2022; Jang et al., 2023; Zhang et al., 2023) have been proposed as an instance of such long-term dialogue systems retaining dialogue context across consecutive sessions. Expanding the network of conversation partners, in the other dimension, includes multi-party conversations (Ouchi and Tsuboi, 2016; Poria et al., 2019; Le et al., 2019; Wang et al., 2020; Mahajan and Shaikh, 2021; Gu et al., 2021; Wei et al., 2023; Chen et al., 2023; Gu et al., 2023). It expands the scope of interactions by increasing the number of conversation partners engaged in a dialogue session.\nHowever, in the real-world, conversations occur within complex contexts that are both lengthy and deeply layered, involving a wide range of people. Therefore, focusing on either of the two dimensions would not fully capture these dynamics. Given this significance, there have been surprisingly few efforts to advance dialogue systems in both directions.\nTo expand the boundaries of those dialogue systems, we introduce MIXED-SESSION CONVERSATION. Unlike multi-session conversations, where a speaker engages with one fixed partner across all sessions, the main speaker in MIXED-SESSION CONVERSATION encounters multiple partners in a mixed order of sessions. This approach is thus referred to as Mixed-Session. Specifically, MIXED-SESSION CONVERSATION consists of multiple dialogue sessions, during which several speakers, including a main speaker, interact dynamically over time. The main speaker engages in conversations with different partners, one partner per session, focusing on a specific event. This setting enables a dialogue system to build a deep, layered context with each of its partners, thereby expanding and complicating the dynamics.\nTo implement MIXED-SESSION CONVERSATION, we develop a dialogue dataset named MISC"}, {"title": "2 Related Works", "content": "Multi-Session Conversations. One direction in which dialogue systems have been pushing is enabling long-term interaction. Multi-session conversation (Xu et al., 2022a) is one of the systems enabling such long-term conversation. Moreover, there are attempts to effectively manage memory (Bae et al., 2022) and implement longer time intervals and relationships between speakers in multi-session conversations (Jang et al., 2023). However, to our knowledge, there is no existing research that explores changing conversation partners with each session. Our MIXED-SESSION CONVERSATION is the first system to involve multiple partners in a multi-session setting."}, {"title": "3 MISC", "content": "We introduce MIXED-SESSION CONVERSATION, a novel dialogue setting that advances along both depth and width dimensions simultaneously. Unlike previous systems, MIXED-SESSION CONVERSATION allows the main speaker to engage with different sub-speakers as conversation partners in each session. To implement this conversation system, we propose a new dataset called MISC.\nMISC comprises 8.5K episodes, each consisting of 6 sessions, totaling 51K sessions in all. In each episode, four speakers are involved, with one acting as the main speaker and the others as sub-speakers. We construct the dialogue dataset in a sequential manner, starting with the collection of topics and progressing to the generation of conversations. We use LLMs to build the dataset through elaborately designed methodologies (please refer to Figure 2 for an overview of the process)."}, {"title": "3.1 Scenario Setup", "content": "To build our dataset, we establish conversational scenarios for each episode. Each scenario includes information about the speakers (names, jobs, or relationships) and a specific event for each session, thus, a total of six events. Our preliminary research shows that the quality of scenarios has a significant impact on the overall dialogue quality. When high-quality scenarios are provided, the difference in dialogue quality between GPT-4 and GPT-3.5 becomes minimal. Accordingly, we utilize GPT-4 to generate the scenarios and GPT-3.5 for the subsequent processes. We generate the episode scenario as follows.\nTopic Collection. We generate topics from keywords related to daily life (e.g., health, travel, education, etc.), using them as seeds to generate scenarios. We instruct GPT-4 (Achiam et al., 2023) to generate topics from a single keyword. For example, topics could be generated for the 'food' keyword, such as \"Dishes from My Grandmother's Kitchen\", \"Feasting with Friends: Tales from a Supper Club\", \"Recipe for Love\", and so on.\nScenario Collection. We gather scenarios based on pre-defined topics and generate details about speakers and events related to each topic. Specifically, we ask GPT-4 to create the names, jobs, or relationships of the main speaker and three other"}, {"title": "3.2 Dialogue Generation", "content": "We generate conversations sequentially with ChatGPT (OpenAI, 2022) from the first to the sixth session, each featuring its own unique session event. Given this setup, it is crucial to ensure continuity between sessions by reflecting on the history of the previous sessions in the subsequent one. To accomplish this goal, we employ two methods: session summaries and the main speaker's memory.\nWe follow Jang et al. (2023)'s approach to generate summaries. The main speaker's memory is used to retain the content shared with each partner from the main speaker's perspective, as detailed in Section 3.3. Through the integration of these two approaches, we ensure seamless transitions between sessions and facilitate a more cohesive exchange of ideas. Please refer to Appendix B for complete episode examples."}, {"title": "3.3 Egocentric Memory", "content": "We utilize the main speaker's memory to uphold the history of previous sessions. This involves summarizing and preserving memories about each partner and the main speaker themselves, from the main speaker's viewpoint. We refer to this memory management approach as Egocentric Memory. It is distinct from previous summarization or memory mechanisms, in that it effectively stores memories related to each partner and establishes links between updated memories across multiple sessions.\nMemory Generation. At the end of each session, we ask ChatGPT to identify significant events, experiences, appointments, and the emotions expressed during the conversation as memory elements from the main speaker's perspective. These memories are generated and recorded separately for both the main speaker and their partner, incorporating references to previous sessions to ensure continuity and coherence.\nMemory Connection. Egocentric Memory is maintained across multiple sessions to help the"}, {"title": "4 EMMA", "content": "We propose a novel dialogue model called EMMA. EMMA collects memories for each conversation partner from its own perspective in every session, ensuring seamless continuity in subsequent sessions. EMMA consists of two parts: (1) the dialogue module; (2) the retrieval module. An overview of EMMA's architecture is illustrated in Figure 3."}, {"title": "4.1 Dialogue Module", "content": "EMMA is designed to generate dialogue and manage memory, which includes summarization, linking, and retrieval tasks. Accordingly, within the dialogue module, all tasks except for memory retrieval are handled, with the retrieval task being delegated to the retrieval module. The FLAN-T5 model (Chung et al., 2022) is crafted explicitly for multi-tasking, incorporating instructions and prefixes (e.g., for generation, summarization, etc.). Therefore, we employ the pre-trained FLAN-T5-Large and fine-tune this as a dialogue module. EMMA carries out various tasks within the dialogue module built based on a single FLAN-T5 model.\nDialogue Generator. To generate a response, EMMA must consider several factors, including the participant's identity, conversation history of the current session, and relevant memories. EMMA takes as input these factors organized into a sequence with a prefix of \u201cgeneration\u201d.\nMemory Summarizer. EMMA summarizes the conversation history into Egocentric Memory at the end of each session. It encapsulates memories about itself and the partner appearing in each session. To summarize memory, we use the entire session history as input, informing who the memory will be summarized for with a prefix.\nWhen multiple memories are generated together,"}, {"title": "4.2 Retrieval Module", "content": "This module retrieves memories built from previous sessions to provide context for the ongoing dialogue. Although can access to all memories, it selectively prioritizes the most relevant ones when generating the next utterance. This selective approach optimizes efficiency by focusing on key memory instances, ensuring the generated utterances are contextually appropriate.\nThis module is built upon the CPM method introduced in Xu et al. (2022b). It utilizes BERT-base (Devlin et al., 2019) as the foundational model, employing separate encoders for both the conversation context and memory. To train the module, we utilize triplet loss, optimizing the model by comparing the outputs of the two encoders. For memory retrieval, we measure cosine similarity to gauge the relevance of the retrieved memories,\n$sim(c, m_i) = cos(E_c(c), E_m(m_i)).$ (1)\nwhere c represents the conversation context, while $m_i$ represents memory. $E_c$ refers to the encoder for the conversation context, and $E_m$ denotes the encoder for memory. During retrieval, we select only one memory with the top 1 similarity to the given context. We not only provide the retrieved memories but also include associated memories linked to those memories to offer the extended context."}, {"title": "5 Experiments", "content": "Evaluating open-domain conversations poses a significant challenge. While metrics such as PPL, ROUGE (Lin, 2004), and BLEU (Papineni et al., 2002) offer quantitative measures, they often fail to capture the contextual intricacies, emotional tone, and level of engagement within conversations. Consequently, recent research in open-domain conversation increasingly leans towards human evaluation as the standard method (See et al., 2019; Finch and Choi, 2020; Smith et al., 2022; Ji et al., 2022; Bae et al., 2022; Kim et al., 2022, 2023; Jang et al., 2023). By employing human judgment, researchers can better assess the nuanced qualities of conversational systems, ensuring a more comprehensive understanding of their performance. Given the significance of assessing the conversational flow in both MISC and EMMA, we apply human evaluation as a quality verification method. We use MISC to train EMMA, for more training details, please refer to Appendix E."}, {"title": "5.1 Human Evaluation", "content": "To maintain the highest standards of assessment quality, we have entrusted the human evaluation to a professional agency, hiring a total of 20 annotators for the task. We do not have any sensitive information about the annotators, but they are assured to have a strong command of English and the requisite evaluation skills. After completing the evaluations, quality control reviewers thoroughly inspect the conversations assessed by annotators to ensure adherence to the predefined criteria.\nTo ensure more reliable evaluations, we conduct cross-annotation evaluations. For each evaluation task, we form three groups of annotators, each conducting its assessments independently. We report the evaluation results for each group, as well as the level of agreement among the results from all three groups. Agreement refers to the ratio of the total number of responses to the number of responses that matched across the three groups. Our human evaluation results demonstrate a high level of agreement across all metrics for each task."}, {"title": "5.2 Quality of MISC", "content": "We randomly select 0.3K episodes, so 1.8K sessions, from the test split to assess the conversation quality of MISC.\nDialogue. We ask human annotators to assess whether our MISC meets the criteria of 'Consistency' and 'Coherence', rating them on a scale from 1 (poor) to 5 (perfect) based on previous studies (Bae et al., 2022; Kim et al., 2023; Jang et al., 2023). For a detailed of the criteria, please refer to Appendix F.\nMemory. We assess the Egocentric Memory of MISC using three key metrics. Annotators evaluate each memory element based on these metrics, assigning a 'pass' if the element fully meets the criteria, and a 'fail' if it does not:\n\u2022 Memory Summarization: The memory accurately retains the history of the conversation for each partner from the main speaker's perspective (a total of 6.6K memory sentences).\n\u2022 Memory Linking: Memory pairs should either convey the same context or represent updates on a specific event. (a total of 2.3K memory pairs).\n\u2022 Memory Tagging: The utterance reflects the contents of the given memories (a total of 1K memory and utterance tags)."}, {"title": "5.3 Performance of EMMA", "content": "We evaluate the performance of EMMA using 0.2K episodes generated by four instances of EMMA interacting with each other. For this evaluation, we randomly extract 0.2K episodes from the test split to use as a seed. We assign each EMMA a name, job, or relationship, and the first utterance of the first session from the seed. Additionally, each session is limited to a maximum of 8 turns (the average turn count of MISC). Evaluation is based on the criteria of 'Humanness', 'Engagingness', and 'Memorability'. Regarding memorability, annotators find it appropriate when the memory used throughout a conversation not only accurately reflects the context of previous interactions but also efficiently retrieves the necessary information (please refer to Appendix F for more detailed criteria). All criteria are evaluated on a scale of 1 (indicating poor) to 5 (indicating excellent). Please refer to Appendix F for a more detailed explanation."}, {"title": "6 Results", "content": "In this section, we explain the evaluation results of MISC and EMMA. Please refer to Section 5 for specific evaluation settings.\nConversation Quality. Table 2 presents the results of human evaluation on the dialogue quality of MISC. As evident, all three groups exhibit high scores of both \u2018Consistency' and 'Coherence', confirming that MISC effectively implements the natural flow of conversation within the MIXED-SESSION CONVERSATION.\nMemory Quality. Table 3 presents the evaluation results of the Egocentric Memory implemented in MISC, displaying the 'pass' rate for each group. These results show consistently quite high 'pass'"}, {"title": "7 Conclusion", "content": "We introduce MIXED-SESSION CONVERSATION, a new dialogue system designed to incorporate long-term interactions and accommodate a wide range of speakers. MIXED-SESSION CONVERSATION allows a main speaker to engage with different partners across multiple sessions, enabling the dialogue system to cover a more wide-layered context. Unlike multi-session conversations with a fixed partner, MIXED-SESSION CONVERSATION feature interactions with various partners in mixed order. We also propose a new dataset called MISC to implement MIXED-SESSION CONVERSATION. We develop EMMA, a new dialogue model trained via MISC. EMMA collects memories for each partner with Egocentric Memory and utilizes them in subsequent sessions to maintain seamless continuity. Extensive human evaluation demonstrates dialogues in MISC maintain a natural flow across sessions even when the conversation partner changes. EMMA exhibits high memorability and engaging-ness in conversations by actively utilizing Egocentric Memory."}, {"title": "Limitations", "content": "The proposed conversation system involves multiple partners over the entire session but only converses with one partner in each session. To build a more dynamic conversation environment, we aim to explore settings in future research where multiple partners can engage within individual sessions as well. Also, despite our best efforts, our MISC dataset may contain instances where the memory is not fully summarized as desired. However, these samples constitute a very small minority of the entire dataset. Since the majority of samples in MISC is of high quality, EMMA trained on it can fully capture the necessary memories even in the presence of a few negative samples in the dataset (please see Appendix K)."}, {"title": "Ethics Statement", "content": "We conduct fair human evaluations through a professional evaluation agency. During the evaluation process, we verify that annotators are receiving fair compensation. Also, We employ OpenAI's Moderation (Markov et al., 2022) to filter out unethical content from our dataset. If any session conversation is filtered into toxic categories, we remove the episode that contains those sessions. Despite our best efforts, our dataset may have potential risks. Our model based on LLM can generate content that may vary from facts or human intentions. Therefore, our dataset and model should be used cautiously for research purposes only."}]}