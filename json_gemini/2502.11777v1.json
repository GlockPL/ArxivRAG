{"title": "Deep Neural Networks for Accurate Depth Estimation with Latent Space Features", "authors": ["Siddiqui Muhammad Yasir", "Hyunsik Ahn"], "abstract": "Depth estimation plays a pivotal role in advancing human-robot interactions, especially in indoor environments where accurate 3D scene reconstruction is essential for tasks like navigation and object handling. Monocular depth estimation, which relies on a single RGB camera, offers a more affordable solution compared to traditional methods that use stereo cameras or LiDAR. However, despite recent progress, many monocular approaches struggle with accurately defining depth boundaries, leading to less precise reconstructions. In response to these challenges, this study introduces a novel depth estimation framework that leverages latent space features within a deep convolutional neural network to enhance the precision of monocular depth maps. The proposed model features dual encoder-decoder architecture, enabling both color-to-depth and depth-to-depth transformations. This structure allows for refined depth estimation through latent space encoding. To further improve the accuracy of depth boundaries and local features, a new loss function is introduced. This function combines latent loss with gradient loss, helping the model maintain the integrity of depth boundaries. The framework is thoroughly tested using the NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in complex indoor scenarios. The results clearly show that this approach effectively reduces depth ambiguities and blurring, making it a promising solution for applications in human-robot interaction and 3D scene reconstruction.", "sections": [{"title": "1. Introduction", "content": "In human-robot interaction, comprehending the spatial relationships of 3-dimensional (3D) surroundings is a critical perceptual undertaking for various robotic applications, encompassing manipulation, exploration, and navigation [1]. Accurate depth perception is typically required by robots to evade obstructions and handle objects. In industrial settings, moving autonomous agents often possess a color camera for surveillance purposes. However, the depth estimation frequently necessitates specialized equipment, i.e., stereo cameras, light or time-of-flight sensors, which are relatively costly compared with single RGB cameras. Whereas researchers have made progress in monocular depth perception, considerably more remains to be achieved [2]. In recent times, the techniques of estimating depth from a single monocular image are increasingly important in computer vision applications, including 3D scene modeling and reconstruction, and autonomous driving systems, as it provides valuable insights into scene vanishing points and horizontal boundaries [1], [2]. The stereo-matching principle is a fundamental technique for estimating depth using multiple cameras. Consequently, stereo matching can generate a disparity map that depicts positional differences linking corresponding pixels in stereo images [3]. When multiple images captured at aligned camera positions are used for depth estimation, it is referred to as the multi-scope vision, analogous to stereo vision using two horizontally aligned images [4].\nThe paper proposes a lightweight human-robot interaction system for depth estimation from a single RGB image. It uses features extracted from the latent space network to guide the learning process of the RGB image-to-depth relationship. These encoded features contain geometrical structure compactly relevant to the scene's depth layout, which effectively sharpens the depth boundaries. The proposed method minimizes depth ambiguity in homogeneous regions while blurring artifacts at depth boundaries. This biomimetic approach emphasizes low-level visual perception through skip connection, comparable to the primary visual cortex's basic processing. The main contributions of the proposed method are summarized as follows:\nWe propose a human-robot interaction system for a monocular depth estimation auto-encoder network to effectively learn the complex process of transforming a color image into a depth image. Unlike previous approaches that relied on the concept of perceptual loss.\nThe proposed technique aims to discover the process of \"generation\u201d from latent space rather than using a \"classification\u201d strategy to refine the estimated depth information.\nIn contrast to other techniques, the proposed method is reasonably accurate because the network design only utilizes skip connections in residual blocks rather than feature branches, leading to effective results.\nThis paper's summary is organized as follows: Section 2, a comparison of comparable studies is reviewed. Section 3 provides a thorough explanation of the proposed human-robot interaction system for monocular depth estimation utilizing human-robot interaction to perceive an indoor environment. In Section 4, experimental findings are illustrated using a benchmark dataset. In Section 5, the results and conclusion are presented."}, {"title": "2. Related Work", "content": "This section explains methods that integrate data to build depth maps for service robot systems after examining prior technologies for obtaining images for depth estimates.\nDepth estimation has emerged as a critical area of research, driven by the demand for precise and efficient methods across diverse applications. Early efforts by Fang et al. [12] established a robust evaluation criterion for active vision systems, offering a foundational framework for depth estimation. L\u00f3pez-Nicol\u00e1s et al. [13] extended this line of inquiry to robotic navigation, proposing an innovative visual servoing approach for mobile robots with fixed monocular vision systems. Similarly, Sabnis et al. [14] explored the optical properties of cameras, leveraging defocus blur to achieve enhanced depth accuracy. This work highlighted the interplay of hardware features and computational techniques for depth sensing. In the medical field, Turan et al. [15] demonstrated the applicability of monocular depth estimation to endoscopic capsule robots, enabling real-time odometry and depth measurement without external supervision. Jin et al. [16] applied depth estimation to humanoid robotics, introducing a progressive approximation (PA)-based cyclic learning framework that adapts to specific behavioral tasks. Xiao et al. [17], drawing inspiration from human tactile sensing, employed a deep recurrent neural network (DRNN) along with long short-term memory (LSTM) to improve tumor depth estimation in soft tissues, underscoring the versatility of deep learning in specialized applications.\nAdvancements in human-robot interaction (HRI) have further driven the field. Cheng et al. [18] proposed a modular framework that integrates RGB images and human pose estimation to overcome the limitations of depth sensors in dynamic environments. Yu et al. [19], [20] focused on disparity estimation for electric inspection robotics by developing a lightweight neural network combining PSMNet and cutting-edge optimization techniques. Wang et al. [21] emphasized the extraction of scene structures and motion characteristics from monocular image sequences, demonstrating the potential of monocular systems in complex scenarios. Shimada et al. [22] addressed the challenge of 3D hand pose estimation and shape refinement using monocular sequences, bypassing the need for explicit depth data. Other innovative methodologies have also surfaced. Pan et al. [23] presented an efficient algorithm for estimating the relative pose of cooperative space targets, integrating multi-target tracking with the Levenberg-Marquardt method (LMM) for rapid convergence. Gysel et al. [24] proposed a latent vector space model capable of handling outliers and learning latent representations by marginalizing changes in depth maps. Kashyap et al. [25] developed an approach to estimate camera motion parameters directly from optic flow, paving the way for improved motion analysis. Reading et al. [26] introduced CaDDN, a fully differentiable method for simultaneous object detection and depth estimation, demonstrating the synergy of these two tasks in unified frameworks.\nRecent efforts have delved deeper into the integration of deep neural networks and latent space features. For example, Pei [27] proposed the Multi-Scale Features Network (MSFNet) incorporating Enhanced Diverse Attention (EDA) and Up-Sample-Stage Fusion (USF) modules for superior depth estimation. However, challenges persist in unsupervised frameworks, particularly in adverse conditions like nighttime or rainy scenes, where traditional photometric consistency assumptions fail. Zhao et al. [28] tackled this issue using an image transfer-based domain adaptation strategy tailored for human-robot interaction in such challenging scenarios. Guo et al. [29] emphasized reducing computational complexity while maintaining high accuracy, a crucial goal for resource-constrained applications.\nCollectively, the critical role of deep neural networks in depth estimation across domains, ranging from robotics to medical imaging. However, despite these advancements, challenges such as handling extreme environmental variations, computational constraints, clearly revealing depth boundaries, blurry restoration and real-time adaptability remain. This paper presents, straightforward method for depth estimation as of a single RGB image that addresses the issue of blurring artifacts in depth's edges. The research builds on these foundations by exploring latent space feature extraction within deep neural networks, aiming to further enhance the accuracy and robustness of depth estimation in diverse applications. Technical specifications and architecture are covered in the next section."}, {"title": "3. Proposed Monocular Depth Estimation", "content": "This section discusses proposed monocular depth estimation model and its training methodology. The goal is to improve depth information for a 3D environment model that can understand and serve humans in different aspects. The research explores the generative process of depth arrangement from a monochromatic image. A deep convolutional neural network is used to encode RGB to depth relationship in latent space, enhancing the quality of the depth map. The model's structure is presented, followed by a detailed explanation of the depth estimation procedure using the training approach. The loss function used is explained, including data loss, latent loss, and gradient loss."}, {"title": "3.1 Depth Estimation Deep Learning Model", "content": "The proposed depth estimation architecture contains depth to depth & color networks. Both networks have a similar structure with three main elements: encoder, ResBlocks, and decoder. The general layout is depicted in Figure 1. The image used as an input is squeezed into latent features by multiple ResBlocks on the encoder side, with a modified version from the original residual network shown in Figure 2.\nAs is commonly known in brain science, the primary visual cortex (V1) detects basic visual characteristics such as shape, color, contrast ratio, and line direction, and secondary visual cortex (V2) uses the detection results of the V1 to recognize a higher level of visual perception such as depth and relationships between objects. Therefore, for detecting the edge of depth more clearly, the role of V1 precepting the core visual information is important. The skip connection of the residual block in this approach has the effect of highlighting the core visual information of image.\nBy layering a sufficient number of ResBlocks, latent features implicitly encode characteristics for generating depth. The small spatial size of these highly encoded latent features holds necessary information to reconstruct the target image, specifically the depth map. Batch normalization and ReLU layers follow every convolution layer except the last output layer. On the decoder side, the feature map size is doubled through up-sampling using bilinear interpolation. The depth map is effectively produced by the symmetric decoder using the latent features.\nThe proposed approach captures internal correlation within spatial dimensions through feature maps, using skip connections to bring back local details [30]. It uses learned latent features related to depth generation to capture specific elements and overall structures of the depth map. These characteristics influence the outcome, which resembles the real depth map. The guided network implicitly enhances the scheme, allowing it to detect depth boundaries in estimated results, even in complex outdoor settings."}, {"title": "3.2 Latent Loss Functions", "content": "The latent loss function is designed to compare feature representations, rather than directly comparing pixel-level outputs of predicted depth map and ground truth. By summing over all spatial locations and feature layers, it evaluates the degree of alignment between Gi(y) and G(y*) in the feature space, effectively assessing how well predicted depth map matches against ground truth in terms of higher-level structural and semantic similarities [31].\n$L_{l}(G(y), G(y^{*})) = \\sum_{j} (\\frac{1}{N_{j}} \\sum_{k} || G_{j}(y_{k}) - G_{j}(y_{k}^{*}) ||^{2})$\nGi(y) and G(y*) refer to feature representations derived from predicted and ground truth depth maps using guided network G. j indexes the feature layers in the guided network. k indexes the spatial locations within a feature layer. Nj represents the total number of spatial locations in layer j. ||\u00b7||\u00b2 is squared L2-norm, which measures the Euclidean distance between feature representations at corresponding locations.\nThe objective of this latent loss function is to enforce alignment linking predicted and ground truth depth maps at feature representation level. This approach shifts the focus from traditional pixel-level supervision to a feature-space comparison, enabling the network to be trained in a more robust and semantically meaningful interpretation of depth. By operating in the latent space of the guided network's topmost encoded layer, this loss facilitates a more direct and efficient supervision mechanism for the decoder, allowing it to generate high-quality depth maps from the latent space. In this research, the latent loss function plays a critical role in achieving perceptual consistency in depth estimation. Unlike conventional classification-based or regression-based losses, which might fail to capture nuanced structural and contextual information, the latent loss provides a feature-level perspective that aligns well with the end task of generating perceptually accurate depth maps. By leveraging G to extract dense features, this design ensures that predicted (restore depth within a depth-to-depth autoencoder framework) depth maps maintain fidelity to the ground truth, even in challenging scenarios such as complex lighting or occlusions."}, {"title": "3.3 Gradiant Loss", "content": "The objective of gradient loss function is to enforce gradient consistency between the predicted and actual depth maps. By comparing the horizontal and vertical gradients separately, the method ensures that the predicted map captures subtle variations in depth, especially along edges and boundaries, where depth discontinuities often occur. Incorporating this gradient-based approach serves to address a critical limitation in depth estimation tasks: the difficulty of accurately reconstructing boundary details where neighboring pixels may have starkly different depth values. By aligning gradients of predicted and true depth maps, the loss function prioritizes accurate boundary delineation, leading to sharper and more precise depth predictions is computed as follows:\n$L_{g d}(y, y^{*}) = \\frac{1}{N} \\sum_{i}^{N} |y_{h, i} - y_{h, i}^{*}| + |y_{v, i} - y_{v, i}^{*}|$\nThe proposed formula for gradient loss Lgd(y,y*), portrays a critical role in improving accuracy of depth estimation, particularly at depth boundaries, which are often challenging regions to resolve. This formula incorporates gradient information at both the image and feature levels, ensuring that the predicted depth map aligns more closely with the ground truth by emphasizing local changes and edge details. yn,i and yu,i denotes both (horizontal & vertical) gradient values of predicted depth map at the i-th pixel. Likewise, y*n,i and y*v,i represents corresponding both (horizontal & vertical) gradient values derived from ground truth depth map. N is used for total number of pixels in depth map.\nFurthermore, the gradient loss also extends to the feature level by evaluating the gradients of encoded features collectively. This additional layer of gradient-based consistency helps refine the model's representation and understanding of depth transitions, resulting in better generalization and improved performance. In the context of your research, the inclusion of this loss function contributes to enhancing the model's ability to resolve fine details in complex scenes. It ensures that the predicted depth maps not only maintain global consistency but also accurately capture local variations, particularly at object edges. This dual-level gradient alignment-at both the image and feature levels -provides a robust mechanism for addressing one of the key challenges in depth estimation and improves the overall quality of predictions.\n$L_{g}(G(y), G(y^{*})) = \\sum_{j} (\\frac{1}{N} \\sum_{k} | G_{h, j}(y_{k}) - G_{h, j}(y_{k}^{*}) | + | G_{v, j}(y_{k}) - G_{v, j}(y_{k}^{*}) |)$\nWhere: Gn,j & Gu,j represents gradient encoded features as specific inputs in both (horizontal & vertical) directions, respectively. The depth map's components can be effectively enhanced by incorporating both (Gn,j & Gv,j) terms into ultimate loss function, the depth map's high-frequency components can be effectively enhanced. A key benefit is that the encoded-features within proposed network effectively represent the depth structure in a condensed manner across multiple scales, leading to significant assistance from their gradients in enhancing the clarity of depth boundaries as illustrated in Figure 4. Hence, gradient encoded features proposed in this article are supposed to be helpful effectively in recovering depth boundaries.\nTo summarize, extracted features from the guided network's latent space enable learning the intricate color-depth relationship in the proposed method (refer to (1) and (2)). The depth layout's core structure can be accurately reconstructed from just one monocular image due to its inherent properties of depth generation being condensed. Additionally, the gradients of these encoded features have shown to be effective in recovering the depth boundary, a task that has proven challenging for previous techniques."}, {"title": "4. Experiment", "content": "We trained our model using the original NYU Depth v2 [27] dataset collected indoors. The unprocessed datasets have many extra pictures gathered from identical locations as those in the popular smaller datasets, but with no prior editing. More precisely, areas that do not have a depth measurement are left blank. However, our model is inherently equipped to deal with such gaps, and its need for a substantial training set makes these original distributions valuable sources of data."}, {"title": "4.1 NYU v2 Depth Dataset", "content": "The NYU Depth v2 dataset [27] is widely exercised in depth estimation analysis and consists of video recordings from 464 indoor scenes. For the purposes of experimentation, the dataset is allotted into 249 scenes as of training & 215 scenes for testing. The RGB input images are resized from their original dimensions of 640\u00d7480 to 320\u00d7240 to standardize the input data. Thus, each depth frame is aligned with the nearest RGB frame in temporal proximity. This process necessitates discarding depth frames where multiple depth images correspond to the same RGB frame to maintain one-to-one correspondence. Camera projection parameters provided with dataset are operated to align RGB & depth frames. Additionally, areas in depth images with invalid values-caused by reflective surfaces or windows-are masked out. The training set contains a total of 120,000 unique images. To address imbalances in the representation of scenes, the images are redistributed, resulting in an expanded training set of 220,000 images, ensuring approximately 1,200 images per scene. For evaluation, the model is tested on a subset of the NYU Depth v2 test set comprising 694 images, where missing depth values have been filled in.\nThis detailed dataset preparation process is critical for ensuring the robustness and fairness of models trained for depth estimation or related tasks. By aligning the RGB and depth images, excluding invalid pixels, and balancing the training data across scenes, the dataset supports the development of models that generalize well to diverse indoor environments. In this research, NYU Depth dataset serves as the foundation for training and evaluating models, enabling a systematic investigation of techniques for depth estimation and scene understanding in indoor settings."}, {"title": "4.2 Performance Evaluation", "content": "To demonstrate, generalizability of the proposed approach, we evaluated its performance on NYU Depth v2 dataset, showcasing several examples of predicted depth estimation in Figure 4. The results illustrate that the proposed method markedly enhances interpretation of depth in indoor scenes. Furthermore, the training on NYU Depth v2 dataset reveals strong cross-dataset performance, as it can generate accurate depth maps for related datasets without requiring additional fine-tuning.\nIn addition, we examined the computational efficiency of the method by measuring the time required to estimate depth maps from individual input images. These findings are summarized in Table 2. For consistency in evaluation, input images were uniformly resized to dimensions of 512\u00d7256 pixels. Our method's performance was compared against existing techniques: a conditional generative adversarial network (GAN) developed by Zhang et al. [32] and a Convolutional Neural Network (CNN) approach introduced by Eigen et al. [33] and Sihaeng et al [34]. This analysis serves a dual purpose within the scope of the research: initially, to validate effectiveness and adaptability of proposed depth estimation model across datasets, and then, to highlight its computational efficiency in relation to state-of-the-art approaches. By presenting both qualitative & quantitative comparisons, both recognized for their relatively fast processing speeds. Remarkably, our proposed method operates with exceptional efficiency. Consequently, we posit that our network architecture has substantial potential for application in various human-robot interaction systems."}, {"title": "4.3. Discussion", "content": "The issue of depth perception is a crucial aspect of computer vision, which has been the focus of attention of numerous researchers, resulting in significant advances in recent decades. However, most of the work done in this field, such as stereopsis, has relied on using multiple image geometric cues to determine depth. In contrast, single-image cues provide a largely independent source of information, which has not been extensively explored until now. Given the importance of depth and shape perception in various applications, including object recognition, robot grasping, navigation, image compositing, and video retrieval, we believe that monocular depth estimation can significantly enhance these applications, particularly in cases where only a single image of a scene is available. We have developed an algorithm to infer detailed depth estimation from a single still image. The proposed method surpasses previous methods in both quantitative accuracy and visual quality that emphasizes both latent loss and gradient loss. The model assumes an environment consisting of multiple small scene planes and does not explicitly assume the scene's structure, unlike Delage et al. [36] and Hoiem et al. [37], who assume vertical surfaces on a horizontal floor. This allows the model to generalize distinctly for scenes with significant non-vertical structures."}, {"title": "5. Conclusions", "content": "The paper presents a model to estimate the depth map from a single monocular image, model proposed encoded features from a latent space network to guide the depth estimation process. It emphasizes both latent loss and gradient loss with residual blocks as primary visual perception. The proposed model produces clean boundaries, making it suitable for 3D modeling of scenes. Experimental findings demonstrate outstanding performance in depth estimation for scene understanding and navigation in indoor environments for human-robot interaction."}]}