{"title": "Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization", "authors": ["Chaoxu Mu", "Xufeng Zhang", "Hui Wang"], "abstract": "Heuristics have achieved great success in solving combinatorial optimization problems (COPs). However, heuristics designed by humans require too much domain knowledge and testing time. Given the fact that Large Language Models (LLMs) possess strong capabilities to understand and generate content, and a knowledge base that covers various domains, which offer a novel way to automatically optimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an optimization method that integrates the self-reflection of LLMs with the Monte Carlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively refines generated heuristics by evaluating their performance and providing improvement suggestions. Our method enables to iteratively evaluate the generated heuristics (states) and improve them based on the improvement suggestions (actions) and evaluation results (rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow Shop Scheduling Problem (FSSP). The experimental results show that PoH outperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD) by other LLMs-based methods, and achieves the significant improvements and the state-of-the-art performance of our proposed method in automating heuristic optimization with LLMs to solve COPs.", "sections": [{"title": "1. Introduction", "content": "Combinatorial optimization problems (COPs) commonly exist in diverse fields such as national defense, transportation, industry, and communication. Their substantial theoretical and practical value has made efficient solution of COPs a key research focus in both academia and industry. Heuristics are widely studied to solve COPs and achieve superior performance. The typical heuristics consist of guided local search (GLS), genetic algorithm (GA), and ant colony optimization (ACO). Usually, achieving satisfactory solutions with these methods requires human experts to manually adjust the heuristics for each specific problem. Although manually designed heuristics can be effective in many cases, this approach requires much time for experts to design, implement, and validate heuristics. In addition, for many complex COPs, this approach may result in errors. Consequently, Automatic Heuristic Design (AHD) has emerged as a promising alternative. The increasing availability of computational resources further facilitates AHD. By reducing the reliance on specialized domain knowledge, AHD enables us to explore large design spaces, demonstrating substantial potential to address complex COPs.\nGiven the fact that Large Language Models (LLMs) have demonstrated remarkable capabilities in addressing COPs, due to their broad knowledge understanding and powerful reasoning abilities. Furthermore, leveraging their extensive training corpora, LLMs benefit from a wider search space compared to traditional evolutionary computation (EC) algorithms, leading to performance improvements. Recent research has successfully applied LLMs to heuristic generation and evolutionary search processes.\nTherefore, in this paper, we introduce a novel AHD approach called Planning of Heuristics (PoH). PoH reframes heuristic optimization as a strategic planning problem to manage the complexity of search spaces. Using strategic planning, PoH iteratively refines heuristics (represented as states) through insightful improvement proposals (actions). Starting with an initial heuristic (state), the system systematically explores the heuristic search space using a tree-based search, prioritizing high-reward trajectories to efficiently navigate this extensive space. Using the Monte Carlo Tree Search (MCTS) planning strategy, PoH can anticipate and"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. LLMs for Optimization", "content": "LLMs have recently been used to address optimization problems through prompt engineering for specific issues. However, relying solely on prompt engineering has proven to have limited effectiveness in complex optimization scenarios. Inspired by the automatic generation of heuristics, researchers have explored combining evolutionary computation (EC) with LLMs to generate and refine heuristics. FunSearch presents a novel approach that searches within the function space, using LLMs to iteratively improve the quality of generated heuristics within an evolutionary framework. Evolution of Heuristics (EoH) employs natural language to represent heuristic ideas; LLMs first generate natural language descriptions of heuristics, which are then used to produce executable heuristic code. This evolutionary search framework allows for the simultaneous improvement of both the descriptions and the code, contributing to EoH's effectiveness and efficiency. Similarly, Reflective Evolution (ReEvo) enhances the efficiency of heuristic evolution by combining evolutionary search with the self-reflection capabilities of LLMs."}, {"title": "2.2. LLMs with Self-reflection and Planning", "content": "Self-reflection is a cognitive process where an individual contemplates their own thoughts, feelings, and actions, enabling the recognition of mistakes during problem-solving and the continuous adjustment of strategies. Similarly, guiding LLMs to engage in self-reflection, allowing them to evaluate their generated content, can effectively improve their problem solving performance."}, {"title": "3. Methodology", "content": "This section introduces PoH, a framework that empowers LLMs to strategically plan a coherent reasoning trajectory to solve a wide range of COPs. We first format the definition of the heuristic optimization problem and then present the proposed PoH framework. Finally, we describe how MCTS planning is employed to effectively explore the vast heuristic space and to identify optimal trajectories."}, {"title": "3.1. Problem Definition", "content": "Following a standard setting in heuristic optimization, COP is defined by a solution space \\(S\\) and an objective function \\(f:S \\rightarrow R\\). Heuristic optimization typically searches within a heuristic space \\(H\\) to find an optimal heuristic \\(H^*\\) that minimizes an evaluation function, formally expressed as \\(H^* = \\text{arg min}_{h\\in H} F(h)\\). Unlike traditional methods of optimizing heuristics, we leverage LLMs to generate heuristics, enabling exploration of this open heuristic space. Specifically, we evaluate the generated heuristic \\(H\\) on a training set \\(I\\) to maximize performance according to a reward function \\(R\\), which can be formulated as \\(H^* = \\text{arg max}_{h\\in H} R(\\text{pb}(I, H))\\)."}, {"title": "3.2. PoH Framework", "content": "PoH regards the heuristic optimization problem as a Markov Decision Process (MDP), defined by the tuple \\((S, A, T, R)\\), with a defined state and action space. Here, \\(S\\) represents the state space, \\(A\\) the action space, \\(T\\) the transition function \\(T:S\\times A \\rightarrow S\\), and \\(R\\) the reward function \\(R : S\\times A \\rightarrow R\\). As illustrated in Figure 1 (a), given a current state \\(s_t\\), PoH iteratively generates an action \\(a_t\\) according to \\(a_t \\sim P_B(a | S_t)\\). The action generation process, detailed in Figure 1 (b), com-"}, {"title": "3.3. Planning with Monte Carlo Tree Search", "content": "Selection Starting from the root node, the selection phase traverses the child nodes according to the Upper Confidence Bound (UCT) formula, which balances exploitation and exploration, until a leaf node is reached. The UCT formula is as follows:\n\\(a^* = \\text{arg max}_{a \\in A(s)} \\Big[ Q(s, a) + c \\sqrt{\\frac{\\text{In } N(s)}{N(c(s, a))}} \\Big]\\)\nHere, \\(A(s)\\) represents the set of actions available at node \\(s\\), \\(N(s)\\) represents the number of times node \\(s\\) has been visited, \\(c(s, a)\\) represents the child node resulting from applying action \\(a\\) to node \\(s\\), and \\(c\\) is a constant that adjusts the degree of exploration. The first term of the formula, \\(Q(s, a)\\), reflects exploitation, while the second term reflects exploration, quantifying the uncertainty associated with the nodes visited less frequently. Specifically, if a node and its child node have been explored insufficiently, the value of the second term will be higher.\nExpansion When a leaf node is reached and a terminal state has not yet been achieved, the expansion phase creates one or more new nodes. To derive diverse improvement suggestions (actions), we may sample multiple training batches. Among these new nodes, the one with the highest reward is then passed to the next simulation step.\nSimulation Starting from the current node, the simulation phase selects actions from all possible actions according to a policy, resulting in state transitions. If a terminal state is not reached, actions continue to be selected until a terminal state is reached. To improve efficiency in our experiments, we select the action with the highest local reward from the multiple actions generated during the expansion phase.\nBack-propagation Back-propagation propagates the sim-"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experiments Settings", "content": "Tasks and Datasets We evaluated PoH on two classical combinatorial optimization problems: The TSP and the FSSP. The TSP seeks the shortest route that starts at a depot, visits all customer locations exactly once, and returns to the depot. As a canonical combinatorial optimization problem, it serves as a standard benchmark for heuristic methods. Following the setup in , we performed heuristic optimization on a training set of 64 TSP instances, each with 100 city nodes randomly distributed in [0,1]^2. We used the average optimal gap, calculated with respect to the solutions generated by Concorde , as the evaluation function.\nFSSP is a classical problem in production scheduling, which involves scheduling of \\(n\\) jobs on \\(m\\) machines. Each job consists of \\(m\\) operations that must be processed on the machines in the same predetermined order. Each job may have different processing times on each machine. A job can only begin processing on the next machine after completing processing on the previous machine. Each machine can process only one job at a time, and each job can be processed on only one machine at a time. The objective is to minimize the makespan, the total time of finishing all jobs. For heuristic optimization, we used a training set of 64 randomly generated FSSP instances, each with 50 jobs and a varying number of machines (from 2 to 20), consistent with. The processing times of the jobs were randomly generated from a uniform distribution between 0 and 1. The average optimal makespan, calculated with respect to the solutions generated by, served as the evaluation function.\nBaselines We compare PoH with three categories of baselines: Guided Local Search (GLS) algorithms, LLM-generated heuristics, and other algorithms for the TSP. GLS algorithms include Knowledge-Guided Local Search (KGLS), GNNGLS, NeuralGLS , and a state-of-the-art neural combinatorial optimization (NCO) method. LLM-generated heuristics include EoH and ReEvo. Other algorithms include Google Or-Tools , the Attention Model (AM), POMO , and LEHD.\nFor FSSP, we compared PoH with NEH , NEHFF , Local Search (LS), Iterated Local Search (ILS), and the AHD methods PFSPNet and PFSPNet_NEH . NEH and NEHFF are widely recognized as efficient heuristics for this problem. LS and ILS are classical search methods for the FSSP; we used the same operators"}, {"title": "4.2. Result and Analysis", "content": "Traveling Salesman Problem We firstly evaluated the performance of PoH and several other well-studied methods on common instances from TSPLIB. Table 1 presents the relative distance (%) to the best-known solutions for these methods in a typical subset of instances. The complete comparison results with additional TSPLIB instances and other algorithms are provided in the Appendix. As shown in Table 1, EoH achieved the best results on several instances, finding the optimal solution for pr124, kroA150 and u159. In addition, PoH consistently outperformed all other algorithms on all tested instances, demonstrating exceptional robustness and effectiveness, particularly on the larger instance kroB200, where it consistently produced near-optimal solutions.\nFigure 2 compares PoH with two other LLM-based heuristic optimization methods on several large-scale TSPLIB instances. The results demonstrate that PoH significantly outperforms the other two methods on all tested instances. Moreover, PoH's advantage becomes more pronounced as the problem size increases, indicating its superior ability to"}, {"title": "4.3. Heuristic Generalization", "content": "We compared PoH's performance using four commonly used LLMs: GPT-4.0, Gemini-1.5-Pro, GLM-4-Plus, and GPT-3.5-turbo to further demonstrate the generalization of our method across different LLMs. We conducted three independent runs for each LLM on the TSP200 test set, using the same experimental setup for all runs. The results are presented in Table 4. Among the LLMs tested, GPT-4.0 achieved the best performance with PoH, achieving a minimum gap of 0.227% relative to the optimal solution in"}, {"title": "4.4. Ablation on Search Strategies", "content": "We conducted ablation experiments to further investigate the impact of different search algorithms within the PoH framework. Specifically, we compared PoH using Monte Carlo (MC) search, depth-first Greedy Search (Greedy), and beam search. For each search algorithm, we maintain identical state transitions and action generation processes, only substituting MCTS with the respective algorithm. MC randomly samples and selects an action. Greedy selects the action with the highest immediate reward at each step. The"}, {"title": "4.5. Exploration Efficiency Analysis", "content": "To demonstrate PoH's effectiveness in exploring the prompt space through strategic planning, we compared it with several other search algorithms to analyze exploration efficiency. Specifically, we compared PoH with Greedy Search (equivalent to Greedy in ablation study but with a expand width of 1) and beam search (with a beam width of 3). These two search algorithms explored 34 and 72 heuristics, respectively. As shown in Table 5, increasing the"}, {"title": "4.6. Convergence Analysis", "content": "We conducted a convergence analysis of PoH's learning process, visualizing the optimal gap with respect to both the number of MCTS iterations and tree depth. We used TSP200 as a representative test instance for clearer visualization. Figure 4 shows the convergence with varying iterations, where the horizontal axis represents the number of iterations and the vertical axis represents the optimal gap. The performance of PoH during training and testing is shown by blue and yellow lines, respectively. Figure 5 shows the convergence with varying tree depths, where the horizontal axis represents the tree depth and the vertical axis represents the optimal gap. In both figures, the optimal gap decreases as the number of iterations and tree depth increase, demonstrating PoH's ability to iteratively improve the generated heuristic."}, {"title": "4.7. Qualitative Analysis", "content": "Figure 6 provides a qualitative analysis of how PoH iteratively refines its heuristic (state) based on improvement suggestions (actions), demonstrating its capacity for strategic planning. The figure depicts four states (\\(s_0\\) to \\(s_3\\)) and three actions (\\(a_0\\) to \\(a_2\\)) within a TSP training trajectory, showcasing the continuous optimization of the heuristic from its initial state (\\(s_0\\)). Each subsequent state incorporates the suggestions from previous iterations, resulting in a heuristic with a progressively decreasing gap from the optimal solution on the test instance, reducing from 0.483% to 0.233%."}, {"title": "5. Conclusion", "content": "This paper proposed Planning of Heuristics (PoH), a novel framework for automated heuristic design that combines Large Language Models (LLMs) with the planning strategies of MCTS. PoH strategically searches within the vast heuristic space through MCTS-based planning algorithm. Furthermore, PoH leverages LLM self-reflection to identify shortcomings in generated heuristics and propose targeted improvements. We evaluated PoH on two benchmark combinatorial optimization problems: the Traveling Salesman Problem (TSP) and the Flow Shop Scheduling Problem (FSSP). Experimental results demonstrate that PoH outperforms both other LLM-optimized heuristics and manually designed heuristics.\nFor future work, researchers are encouraged to apply our proposed method to solve large-scale COPs in practice."}, {"title": "7. Appendix", "content": ""}, {"title": "7.1. Implementation details", "content": "Planning of Heuristic (PoH). PoH performs MCTS planning within the space of heuristics. MCTS is a search algorithm designed for complex decision-making problems, particularly those with vast and difficult-to-enumerate state spaces. Its core principle involves exploring potential decision paths through simulated random strategies and evaluating the potential of each node using statistical methods. In PoH, the terminal state conditions and the reward function are key components. A terminal state is reached when the length of the explored path reaches a predefined depth limit. The reward function is derived from the error achieved by the heuristic generated when evaluated on a validation dataset.\nPoH leverages large language models (LLMs) to generate an initial heuristic based on prompts, using this heuristic as the initial state and the root node for subsequent expansion. The agent performs 10 MCTS iterations, each consisting of four key phases: selection, expansion, simulation, and backpropagation. During the selection phase, starting from the root node, the best child node is added to the path based on its UCT value, using an exploration weight \\(e\\) of 2.5. In the expansion phase, the current node is expanded according to the expansion width, generating new heuristics that are input to the base model for improvement suggestions. Then, the optimizer summarizes these suggestions.\nAs shown in Tables 6 and 8, the state transition prompt includes the heuristic of the expanded node, the trajectory of the heuristics, and the improvement suggestions. These are input to the optimizer to generate new heuristic nodes. If a new node is not a terminal node, it is evaluated and added as a child of the expanded node. Each expansion generates new heuristics according to the width of the expansion. During the simulation phase, the last node in the path is recursively expanded, and the node with the highest reward is selected to be added to the path. The simulation ends when the last node meets the terminal condition or an early stopping condition. During backpropagation, the sum of rewards from the node to the leaf/terminal node is appended to the accumulated reward list of the node, from the leaf node back to the root node. The average of these accumulated rewards is then used as the Q-value of the node. To improve computational efficiency and avoid unnecessary exploration of unpromising paths, PoH employs an early stopping mechanism after a depth exceeding 2. Specifically, early stop occurs if a state's reward falls below a minimum threshold or exceeds a maximum threshold. The minimum threshold is defined as the average of the rewards obtained by the parent node and the root node, while the maximum threshold is the maximum reward observed among all the nodes currently explored. This strategy encourages the discovery of shorter paths within the heuristic search space, thus enhancing overall efficiency.\nEach MCTS iteration generates a path from the root node to a leaf node, resulting in dozens of nodes after the search process. Finally, the path with the highest average reward is selected, and the heuristic with the highest reward within that path is chosen as the final output. This strategy is motivated by the fact that the path with the highest average reward represents the best overall search trajectory, and the best heuristic may not always be the last node on that path due to the depth limit causing premature termination."}, {"title": "7.2. Baseline details", "content": "In our experiments, we detail the specifics of various baseline methods to facilitate a more accurate comparison and evaluation of their performance in heuristic optimization. The following provides detailed descriptions of the three main baseline methods: Monte Carlo (MC). The MC method is a random sampling-based optimization strategy that performs multiple single-step samplings and selects the best sampled heuristic. It employs the same heuristic sampling method as PoH (Employing MCTS) but limits the search depth to one step. Although MC is advantageous due to its simple implementation and broad applicability, its convergence speed can be slow, especially with a limited number of samples. To ensure result reliability in our experiments, we sampled 72 new heuristics for each task.\nBeam Search. Beam search is a tree-structured search algorithm that explores potential heuristics by expanding nodes layer by layer. In our experiments, the beam search uses the same expansion function as PoH. With a beam width of 3, each node (excluding the root) expands into 3 new nodes. This results in 9 nodes at each level of the search tree, of which the best 3 are retained for subsequent expansion. The root node expands to 9 new nodes. Given a search depth of 8, a total of 72 nodes are generated, representing new heuristic algorithms. By constraining the beam width and search depth, the beam search efficiently explores the search space within limited computational resources.\nGreedy Search. Greedy search is an optimization method derived from beam search with a beam width of 1, effectively transforming it into a depth-first greedy search. At each step, greedy search selects the currently optimal node for expansion, rapidly converging towards a local optimum. We conducted experiments with the same search depth of 8 but explored"}, {"title": "7.3. Guided Local Search", "content": "Guided Local Search (GLS) is a widely adopted strategy to guide local search away from local optima in combinatorial optimization problems. When a typical local search becomes trapped in a local optimum, GLS modifies the objective function to direct the search toward more promising regions. Our objective is to leverage PoH to discover effective heuristics to enhance GLS. In our experimental setup, we employ a variant of the classic GLS algorithm that incorporates a perturbation phase [1], where edges with higher heuristic values are preferentially penalized. During training, we used TSP200 with 800 GLS iterations to evaluate each heuristic."}, {"title": "7.4. Prompt Engineering", "content": "This section details the prompt formats used in Planning of Heuristics (PoH). As shown in Tables 6 and 8, the \"example_string\" represents the code of each heuristic example. The \u201cimprovement suggestion\u201d includes several heuristic code examples and guides the optimizer model to generate improvement suggestions. The \u201cstate transition\" prompts the optimizer model to perform state transitions (i.e., generate new heuristics), including information on the heuristic code examples and the sequence of heuristics on the selected path, known as the \u201ctrajectory heuristics.\""}, {"title": "7.5. Generated heuristic", "content": "Figure 7 shows the heuristic optimized by PoH for updating the distance matrix for the TSP. This heuristic evaluates edges by combining their relative distance from the shortest edge with an adaptive weighting. This weighting prioritizes shorter edges while maintaining diversity based on the standard deviation of all distances."}, {"title": "7.6. More results", "content": "As shown in Table 7, for a fair comparison, we used the same TSPLIB test set (containing instances with fewer than 200 nodes) as EoH. The results are presented in Table 7. In the main text, we also present results for other large-scale instances of TSPLIB, comparing PoH with existing LLM-based heuristic optimization methods."}, {"title": "7.7. Guided Local Search", "content": "We employ the same GLS framework previously used for the Traveling Salesman Problem (TSP) and have selected two common local search operators: Swap and Relocate. We then apply the PoH methodology to design a specialized heuristic strategy for two key tasks within the GLS framework: (1) dynamically updating the execution time matrix and (2) identifying the set of jobs to be perturbed."}, {"title": "7.8. Prompt Engineering", "content": "As shown in table 8, the prompting engineering framework for the FSSP is almost identical to that of the TSP, the key difference is the objective: designing a heuristic specifically tailored for the FSSP."}, {"title": "7.9. Generated heuristic", "content": "Figure 8 shows the PoH-optimized heuristic for updating both the execution time matrix and determining the perturbed jobs for the FSSP. This heuristic perturbs the execution times of jobs on the critical path by a randomly sampled factor within a specified range, proportional to their contribution to the makespan, and selects these jobs for perturbation."}, {"title": "7.10. More results", "content": "As shown in Table 9, using the complete set of Taillard instance (with the number of jobs ranging from 20 to 200 and the number of machines ranging from 5 to 20), which include 10 instances at each of 11 different scales, PoH outperforms EoH in most cases."}]}