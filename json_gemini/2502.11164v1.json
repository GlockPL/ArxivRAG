{"title": "Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis", "authors": ["Shiguo Lian", "Kaikai Zhao", "Xuejiao Lei", "Ning Wang", "Zhenhong Long", "Peijun Yang", "Minjie Hua", "Chaoyang Ma", "Wen Liu", "Kai Wang", "Zhaoxiang Liu"], "abstract": "DeepSeek-R1, known for its low training cost and exce\u0440tional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, and DeepSeek-R1-Distill-Llama series on A-Eval, an application-driven benchmark. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) have revolutionized natural language processing (NLP). Models such as OpenAI's GPT series [11], Alibaba's Qwen series [14], MetaAI's Llama series [3], and DeepSeek's DeepSeek series [1] have not only advanced NLP technologies but also empowered intelligent solutions for real-world applications. Notably, DeepSeek-R1 [2], with its extremely low training cost, has achieved state-of-the-art (SOTA) performance comparable"}, {"title": "2 Evaluation Framework", "content": "To assess the impact of reasoning enhancements in DeepSeek models for real-world applications, we evaluate 14 models grouped into seven pairs, including the Mixture-of-Experts (MoE) and dense models of various series and scales. Each pair consists of an instruction-tuned model and its corresponding distilled or reasoning-enhanced version, both derived from the same base model."}, {"title": "2.1 Evaluated Models", "content": "To assess the impact of reasoning enhancements in DeepSeek models for real-world applications, we evaluate 14 models grouped into seven pairs, including the Mixture-of-Experts (MoE) and dense models of various series and scales. Each pair consists of an instruction-tuned model and its corresponding distilled or reasoning-enhanced version, both derived from the same base model. Tab. 1 lists the evaluated models and their descriptions."}, {"title": "2.2 Dataset", "content": "The A-Eval benchmark, designed for practical application scenarios, quantifies the capability boundaries of LLMs through 678 manually curated QA pairs across five major categories and 27 subcategories. We use A-Eval's dataset as the evaluation dataset to analyze the performance of DeepSeek series models in practical applications. This evaluation helps assess how DeepSeek-R1's reasoning enhancements improve model capabilities in different practical scenarios."}, {"title": "2.3 Evaluation Process", "content": "We follow A-Eval's zero-shot automatic evaluation process, which includes:\nInference: Feed each question \\(Q_i\\) into the evaluated model to generate a prediction \\(P_i\\).\nTriplet Preparation: Construct the triplet \\((Q_i, A_i, P_i)\\), where \\(A_i\\) is the ground-truth answer for \\(Q_i\\).\nScoring: Combine the prompt and triplet \\((Q_i, A_i, P_i)\\), input them into the scoring model, and obtain the scoring output \\(S_i\\) between 0 and 100. We use A-Eval's scoring prompt and a greater LLM, Qwen2.5-72B-Instruct as the scoring model."}, {"title": "3 Results and Discussion", "content": ""}, {"title": "3.1 Overall Performance", "content": "Fig. 1 (a) presents the overall average scores of each model across all data.\nFindings Consistent with Common Sense:\n(1) Overall, reasoning-enhanced models outperform their original instruction-tuned counterparts.\n(2) Overall, DeepSeek-V3 and DeepSeek-R1 demonstrate superior performance compared to other model families.\n(3) Within the same series, models follow the scaling law [4,8] regardless of whether they have undergone reasoning enhancement.\n(4) Distillation brings the most significant improvements to Qwen2.5-Math-1.5B and Qwen2.5-Math-7B, with score increases of 178.74% and 54.36%, respectively. This is because they are math-focused and perform poorly on general tasks, but the distilled reasoning data from DeepSeek-R1 significantly enhances their general capabilities."}, {"title": "Findings Contrary to Common Sense:", "content": "(5) Before and after distillation, Qwen2.5-32B outperforms the larger model, Llama-3.3-70B.\n(6) Qwen-2.5-14B exhibits performance degradation after distillation."}, {"title": "3.2 Performance Comparison By Task", "content": "Fig. 1(b) to Fig. 1 (f) compare the scores of the models on the five major task categories.\nFindings Consistent with Common Sense:\n(7) The two mathematical models achieve comparable performance in Logical Reasoning but under-perform in other tasks.\n(8) For Logical Reasoning tasks, all models show improvement after distillation, with the increase being greater than for other tasks.\nFindings Contrary to Common Sense:\n(9) Performance degradation occurs after distillation in: Text Understanding (Qwen2.5-14B, Qwen2.5-32B, and DeepSeek-V3), Information Extraction (Llama-3.1-8B, Qwen2.5-14B), and Text Generation (Qwen2.5-14B, DeepSeek-V3). For Task Planning, apart from the two mathematical models, other models either remain unchanged or see a decline in performance after distillation."}, {"title": "(10)", "content": "For Logical Reasoning tasks, original Llama-3.3-70B underperforms the smaller model Qwen2.5-14B. After distillation, DeepSeek-R1-Distilled-Llama-3.3-70B surpasses DeepSeek-R1-Distilled-Qwen2.5-32B.\n(11) For Information Extraction tasks, Qwen2.5-32B consistently outperform the larger model Llama-3.3-70B both before and after distillation."}, {"title": "3.3 Performance Comparison By Subtask", "content": "In more detail, Fig. 2 provides the scores of the models on the 27 subcategories."}, {"title": "(12)", "content": "DeepSeek models dominate 23/27 subtasks, except for Short Text Classification, Named Entity Recognition, and Common Sense Question Answering.\n(13) DeepSeek-R1 shows relative weaknesses compared to DeepSeek-V3 in Long Text Classification (-11.3%), Part-of-Speech Tagging (-13.4%), and Open-source Question Answering (-17.05%).\n(14) Compared to other tasks, distillation brings the highest gains in Complex Mathematical Computation subtask, with an average improvement of 31.45%.\n(15) For arithmetic operations tasks, distillation provides benefits to all models except for Qwen2.5-Math-7B (85.26 to 84)."}, {"title": "3.4 Performance Comparison By Model", "content": "To more clearly compare the performance of each group of models before and after reasoning enhancement on the five major tasks, we present the evaluation scores by seven model groups in Fig. 3."}, {"title": "3.5 Model Selection Guidance for Users", "content": "To assist users in selecting the most suitable DeepSeek model based on their specific application requirements, we present the evaluation results using line charts, following the A-Eval methodology. These visualizations provide a clear and intuitive comparison of model performance across different tasks and sub-categories.\nFigure 4 illustrates the scores of the 14 evaluated models on each major task, as well as their average scores across all tasks. This comprehensive overview allows users to quickly identify models that excel in specific major task types or demonstrate strong overall performance.\nIn Fig. 5, we further break down the performance of the 14 models by showing their average scores within each of the five major task categories and their corresponding subcategories. These detailed visualizations enable users to assess model capabilities at a granular level, ensuring a precise match between model strengths and task requirements.\nUsing these capability boundary quantization curves, users can make informed decisions when selecting the most suitable DeepSeek model for their specific tasks and performance expectations."}, {"title": "4 Conclusion", "content": "To better understand how DeepSeek models perform in real-world applications, we comprehensively evaluate DeepSeek models and their distilled variants on the A-Eval benchmark. Our analysis reveals that reasoning-enhanced models, while generally powerful, are not universally superior across all tasks. Finally, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models based on their specific application requirements. In future work, we plan to expand the tasks and datasets of A-Eval to offer even more comprehensive insights for model selection and application."}]}