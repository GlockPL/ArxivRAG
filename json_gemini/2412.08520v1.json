{"title": "GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek", "authors": ["Lefteris Loukas", "Nikolaos Smyrnioudis", "Chrysa Dikonomaki", "Spyros Barbakos", "Anastasios Toumazatos", "John Koutsikakis", "Manolis Kyriakakis", "Mary Georgiou", "Stavros Vassos", "John Pavlopoulos", "Ion Androutsopoulos"], "abstract": "We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP) toolkit developed specifically for modern Greek. The toolkit provides state-of-the-art performance in five core NLP tasks, namely part-of-speech tagging, morphological tagging, dependency parsing, named entity recognition, and Greeklish-to-Greek transliteration. The toolkit is based on pre-trained Transformers, it is freely available, and can be easily installed in Python (pip install gr-nlp-toolkit). It is also accessible through a demonstration platform on HuggingFace, along with a publicly available API for non-commercial use. We discuss the functionality provided for each task, the underlying methods, experiments against comparable open-source toolkits, and future possible enhancements.", "sections": [{"title": "Introduction", "content": "Modern Greek is the official language of Greece, one of the two official languages of Cyprus, and the native language of approximately 13 million people. Despite continuous efforts (Papantoniou and Tzitzikas, 2020; Bakagianni et al., 2024), there are still very few natural language processing (NLP) toolkits that support modern Greek (\u00a72).\nWe present GR-NLP-TOOLKIT, an open-source NLP toolkit developed specifically for modern Greek. The toolkit supports five core NLP tasks, namely part-of-speech (POS) tagging, morphological tagging (tagging for tense, voice, person, gender, case, number etc.), dependency parsing, named entity recognition (NER), and Greeklish-to-Greek transliteration (converting Greek written using Latin-keyboard characters to the Greek alphabet). We demonstrate the functionality that the toolkit provides per task (\u00a73). We also discuss the underlying methods and experimentally compare GR-NLP-TOOLKIT to STANZA (Qi et al., 2020) and SPACY (Honnibal et al., 2020), two multilingual toolkits that support modern Greek, demonstrating that GR-NLP-TOOLKIT achieves state-of-the-art performance in POS tagging, morphological tagging, dependency parsing, and NER (\u00a74). Previous work (Toumazatos et al., 2024) shows that the Greeklish-to-Greek converter included in GR-NLP-TOOLKIT is also state-of-the-art.\nThe toolkit can be easily installed in Python via PYPI (pip install gr-nlp-toolkit) and its code is publicly available on Github. We showcase its functionality in an open-access demonstration space, hosted on HuggingFace. We also release GREEK-NLP-API, a fully-documented and publicly available HTTP API, which allows using the toolkit in (non-commercial) applications developed in any programming language."}, {"title": "Background and related work", "content": "Greek has evolved over three millennia. Apart from its historical interest, Greek is also challenging from an NLP point of view. For example, it has its own alphabet (\u03b1,\u03b2,\u03b3,\u2026), and nowadays a much smaller number of speakers, compared to other widely used languages of the modern world. Although words of Greek origin can be found in many other languages (e.g., medical terms), they are written in different alphabets in other languages. Hence, Greek words written in the Greek alphabet are severely under-represented in modern multilingual corpora and, consequently, in the word and sub-word vocabularies of most multilingual Transformer models, e.g., XLM-R (Conneau et al., 2020).\nThis causes the tokenizers of these models to over-fragment Greek words, very often to characters (Koutsikakis et al., 2020), which increases processing time and cost, and makes it more difficult for models to reassemble tokens to more meaningful units. Greek is also highly inflected (e.g., different verb forms for different tenses, voices, moods, persons, numbers; similarly for nouns, adjectives, pronouns etc.), which makes POS tagging more difficult and morphological tagging (tagging also for tense, voice, gender, case etc.) desirable. Greek is also flexible in word order (e.g., subject-verb-object, object-verb-subject, verb-subject-object etc. are all possible with different emphasis), which makes parsing more challenging.\nModern Greek is normally written in the Greek alphabet. In online messages, however, especially informal email and chat, it is often written using characters available on Latin-character keyboards, a form known as Greeklish (Koutsogiannis and Mitsikopoulou, 2017). For example, 'w' (omega) may be written as 'w' based on visual similarity, as 'o' based on phonetic similarity, or as 'v' based on the fact that 'w' and 'v' use the same key on Greek-Latin keyboards, to mention just some possibilities. Greeklish was originally used in older computers that did not support the Greek alphabet, but continues to be used to avoid switching languages on multilingual keyboards, hide spelling mistakes (esp. when used by non-native speakers), or as a form of slang (mostly by younger people). There is no consensus mapping between Greek and Latin-keyboard characters. Consequently, the same Greek word can be written in numerous different ways in Greeklish. Even native Greek speakers may struggle to understand, and are often annoyed by Greeklish, which requires paying careful attention to context to decipher. Moreover, most Greek NLP datasets contain text written in the Greek alphabet, hence models trained on those datasets may be unable to handle Greeklish.\nPhenomena of this kind motivated the development of GREEK-BERT (Koutsikakis et al., 2020), and more recently the MELTEMI large language model (LLM) for modern Greek (Voukoutis et al., 2024); the latter is based on MISTRAL-7b (Jiang et al., 2023). In this work, we leverage GREEK-BERT for most tasks, and BYT5 (Xue et al., 2022) for Greeklish-to-Greek, which can both be used even with CPU only, unlike larger LLMS (Luccioni et al., 2024). Nevertheless, in future versions of the toolkit, we plan to investigate how we can integrate 'small' Greek LLMS for on-device use.\nIn previous modern Greek experiments, GREEK-BERT, when fine-tuned, was reported to outperform the multilingual XLM-R, again fine-tuned, in NER and natural language inference, while it performed on par with XLM-R in POS tagging (Koutsikakis et al., 2020). In subsequent work of two undergraduate theses (Dikonimaki, 2021; Smyrnioudis, 2021), we showed, again using modern Greek data, that GREEK-BERT largely outperformed XLM-R in dependency parsing, but found no substantial difference between the two models in morphological tagging and (another dataset of) NER. Greeklish was not considered in any of these previous studies. The two theses also created a first version of GR-NLP-TOOLKIT, which was largely experimental, did not include Greeklish-to-Greek, and was not published (apart from the two theses). The version of the toolkit that we introduce here has been completely refactored, it uses more recent libraries, has been tested more thoroughly, includes Greeklish-to-Greek, can be used via both PYPI and GREEK-NLP-API, and can also be explored via a HuggingFace demo (\u00a71).\nSPACY (Honnibal et al., 2020) and STANZA (Qi et al., 2020) are widely used multilingual NLP toolkits that support modern Greek. They both have limitations, however, discussed below (Table 1)."}, {"title": "Using GR-NLP-TOOLKIT", "content": "Using our toolkit in Python is straightforward.\nTo install it, use pip install gr-nlp-toolkit. Subsequently, you can initialize, e.g., a pipeline for POS tagging (incl. morphological tagging), NER, dependency parsing (DP) by executing nlp = Pipeline(\"pos, ner, dp\"). Applying the pipeline to a sentence, e.g., doc = nlp(\"H \u0399\u03c4\u03b1\u03bb\u03af\u03b1 \u03ba\u03ad\u03c1\u03b4\u03b9\u03c3\u03b5 \u03c4\u03b7\u03bd \u0391\u03b3\u03b3\u03bb\u03af\u03b1 \u03c3\u03c4\u03bf\u03bd \u03c4\u03b5\u03bb\u03b9\u03ba\u03cc \u03c4\u03bf 2020.\"), tokenizes the text and provides linguistic annotations, including POS and morphological tags, NER labels, and dependency relations. In our example, the token \u2018\u0399\u03c4\u03b1\u03bb\u03af\u03b1' (English: \u2018Italy') gets the annotations NER = S-ORG (start token of organization name), UPOS = PROPN (proper name), and a dependency relation nsubj (nominal subject) linking it to the verb (see also Fig. 2).\nTransliterating Greeklish to Greek (G2G) is equally simple. The G2G converter can be loaded by typing nlp = Pipeline(\"g2g\"). Running doc = nlp(\"h athina kai h thessaloniki einai poleis\") will convert the text to \u201c\u03b7 \u03b1\u03b8\u03b7\u03bd\u03b1 \u03ba\u03b1\u03b9 \u03b7 \u03b8\u03b5\u03c3\u03c3\u03b1\u03bb\u03bf\u03bd\u03b9\u03ba\u03b7 \u03b5\u03b9\u03bd\u03b1\u03b9 \u03c0\u03bf\u03bb\u03b5\u03b9\u03c2\u201d (English: \u201cathens and thessaloniki are cities\u201d). This makes it easy to process Greeklish text before performing further Greek language processing. For example, you can also combine the G2G converter with POS, NER, DP in the same pipeline, using nlp = Pipeline(\"g2g, pos, ner, dp\")."}, {"title": "Under the hood and experiments", "content": "The POS tagging, morphological tagging, NER, and dependency parsing tools of GR-NLP-TOOLKIT are powered by GREEK-BERT (Koutsikakis et al., 2020), with task-specific heads. For Greeklish-to-Greek, we reproduced the BYT5-based converter of citettoumazatos-etal-2024-still-all-greeklish-to-me, which was the best among several methods considered, apart from GPT-4, which we excluded for efficiency reasons.\nFor the NER tool of GR-NLP-TOOLKIT, we fine-tuned GREEK-BERT (Koutsikakis et al., 2020) with a task-specific token classification head. We used the training subset of a modern Greek NER dataset published by Bartziokas et al. (2020). The dataset contains approx. 38,000 tagged entities and 18 entity types. We tuned hyper-parameters to maximize the macro-F1 score on the development subset. We used cross-entropy loss, AdamW (Loshchilov et al., 2017), and grid search for hyper-parameter tuning (Table 6).\nIn Table 2, we compare SPACY against GR-NLP-TOOLKIT on the test subset of the NER dataset of Bartziokas et al. (2020), for the six entity types that SPACY supports. We do not compare against STANZA here, since it does not support NER (Table 1). As seen in Table 2, GR-NLP-TOOLKIT outperforms SPACY in all entity types. SPACY'S score in the LOC (location) entity type is particularly low, because it classified most (truly) LOC entities as GPE (geo-political entity).\nFor POS tagging and morphological tagging, we used the modern Greek part of the Universal Dependencies (UD) treebank (Prokopidis and Papageorgiou, 2017). Every word occurrence is annotated with its gold universal POS tag (UPOS), morphological features (FEATS), as well as its syntactic head and the type of syntactic dependency. We refer the reader to the UD website, where complete lists of UPOS tags, morphological features, and dependency types are available.\nWe fine-tuned a single GREEK-BERT instance for both POS tagging and morphological tagging, adding 17 token classification heads (linear layers), 16 for the morphological categories, and 1 additional token classification head for UPOS prediction. Each classification head takes as input the corresponding output (top-level) token embedding of GREEK-BERT. For every head, the class with the highest logit is chosen, as in multi-task learning. The model hyperparameters were tuned on the validation subset of the dataset optimizing the macro-F1 score, using grid search and AdamW (Loshchilov et al., 2017) (Table 6).\nIn Table 3, we compare SPACY and STANZA to the GR-NLP-TOOLKIT on the UPOS and morphological tagging test data of the modern Greek UD treebank. STANZA and GR-NLP-TOOLKIT perform on par, with SPACY ranking third.\nIn the more complex morphological tagging task (Table 4), the differences between the systems are move visible, with GR-NLP-TOOLKIT performing slightly better in most categories than STANZA, while SPACY, again, ranks third. The largest differences are observed in 'Mood' and 'Foreign' (foreign word), where GR-NLP-TOOLKIT performs substantially better, and 'Degree' (degrees of adjectives), where STANZA is clearly better. Dikonimaki (2021) attributes some of these differences to very few training occurrences of the corresponding tags.\nFor dependency parsing, we use the model of Dozat et al. (2017), with the exception that we obtain contextualized word embeddings using GREEK-BERT instead of the BILSTM encoder of the original model. Specifically, for each word of the sentence being parsed, we obtain its output (top-level) contextualized embedding $e_i$ from GREEK-BERT."}, {"title": "4.3 Dependency parsing", "content": "We then compute the following four variants of $e_i$. The $W^{(...)}$ matrices are learnt during fine-tuning.\n$h^{(arc-head)}_i = W^{(arc-head)}e_i, h^{(arc-dep)}_i = W^{(arc-dep)}e_i$\n$h^{(rel-head)}_i = W^{(rel-head)}e_i, h^{(rel-dep)}_i = W^{(rel-dep)}e_i$\n$h^{(arc-head)}_i$, $h^{(arc-dep)}_i$ represent the i-th word of the sentence when considered as the head or dependent (child) of a dependency relation, respectively. $h^{(rel-head)}_i$, $h^{(rel-dep)}_i$ are similar, but they are used when predicting the type of a relation (see below). Each candidate arc from head word j to dependent word i is scored using the following formula, where $W^{(arc)}$ is a learnt biaffine attention layer, and $b^{(arc)}$ is a learnt bias capturing the fact that some words tend to be used more (or less) often as heads.\n$s^{(arc)}_{ij} = (h^{(arc-head)}_j)^T W^{(arc)} h^{(arc-dep)}_i + (h^{(arc-head)}_j)^T b^{(arc)}$\nAt inference time, for each word i, we greedily select its (unique) most probable head $y^{(arc)}_i$.\n$y^{(arc)}_i = \\arg \\max_j s^{(arc)}_{ij}$\nDuring training, we minimize the categorical cross entropy loss of $y^{(arc)}, y^{(arc)}_i$, where the possible values of $Y^{(arc)}$ correspond to the other words of the sentence. For a given arc from head word j to dependent word i, its candidate labels k are scored as follows, where $\\oplus$ denotes vector concatenation.\n$s^{(rel)}_{ijk} = (h^{(rel-head)}_j \\oplus h^{(rel-dep)}_i)^T U^{(rel)}_k (h^{(rel-head)}_j \\oplus h^{(rel-dep)}_i) + w^T (h^{(rel-head)}_j \\oplus h^{(rel-dep)}_i) + b^{(rel)}$\nHere $U^{(rel)}$ is a learnt biaffine layer, different per label k, whereas $w$ is a learnt vector that in effect scores separately the head and the dependent word, and $b^{(rel)}_k$ is the bias of label k. At inference time, having first greedily selected the head $y^{(arc)}_i$ of each dependent word i, we then greedily select the label of the arc as follows.\n$Y^{(rel)}_i = arg max_k s^{(rel)}_{iy^{(arc)}_i k}$\nDuring training, we minimize the categorical cross entropy loss of $y^{(rel)}_i$. The arc prediction and label prediction components are trained jointly, adding the two cross entropy losses.\nThe parser was trained and evaluated on the same modern Greek part of the Universal Dependencies dataset of Section 4.2, now using the dependency relation annotations. Consult Dikonimaki (2021) and Kyriakakis (2018) for more details."}, {"title": "4.4 Greeklish-to-Greek transliteration", "content": "For Greeklish-to-Greek, we reproduced the BYT5 model of Toumazatos et al. (2024), which was the best one, excluding GPT-4. BYT5 (Xue et al., 2022) operates directly on bytes, making it particularly well-suited for tasks involving text written in multiple alphabets (Greek and Latin in our case). Toumazatos et al. (2024) fine-tuned BYT5 especially for Greeklish-to-Greek, using synthetic data. The model was then evaluated on both synthetic and real-life Greeklish. Consult Toumazatos et al. (2024) for more details and evaluation results. Recall that no other modern Greek toolkit currently supports Greeklish-to-Greek (Table 1).\nA limitation of the Greeklish-to-Greek model included in GR-NLP-TOOLKIT is that it has not been trained on Greeklish that also includes English (code switching), which is a common phenomenon in online modern Greek. This is a limitation inherited from the work of Toumazatos et al. (2024). We are currently working on an improved Greeklish-to-Greek model that will also handle code switching. We are also considering including in GR-NLP-TOOLKIT an older statistical Greeklish-to-Greek model (Chalamandaris et al., 2006), which still performed well in the experiments of Toumazatos et al. (2024) and can already handle code-switching."}, {"title": "The GR-NLP-TOOLKIT demo space", "content": "For users wishing to explore GR-NLP-TOOLKIT instantly, in a no-code fashion, we also developed a demonstration space, which is open access and hosted at https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo. Users can select tasks (POS and morphological tagging, NER, dependency parsing, Greeklish-to-Greek), submit their input and see the results in the user interface."}, {"title": "The GREEK-NLP-API", "content": "Based on GR-NLP-TOOLKIT, we also developed a publicly available API (with the same non-commercial license). The API is hosted at https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API. It is intended to be used in research and educational applications, even applications not developed in Python, via HTTP API calls and exchange of JSON objects. GREEK-NLP-API conforms to the OPENAPI standards."}, {"title": "Conclusions", "content": "We introduced GR-NLP-TOOLKIT, an open-source NLP toolkit with state-of-the-art performance for modern Greek. It can be easily installed in Python (pip install gr-nlp-toolkit), and its code is available on Github (https://github.com/nlpaueb/gr-nlp-toolkit/).\nThe toolkit currently supports POS and morphological tagging, dependency parsing, named entity recognition, and Greeklish-to-Greek transliteration. We also presented an interactive no-code demonstration space that provides the full functionality of the toolkit (https://huggingface.co/spaces/AUEB-NLP/greek-nlp-toolkit-demo), as well as a publicly available API at https://huggingface.co/spaces/AUEB-NLP/The-Greek-NLP-API, which allows using the toolkit even in applications not developed in Python. We discussed the methods that power the toolkit under the hood, and reported experimental results against SPACY and STANZA.\nIn future work, we plan to add more tools, e.g., for toxicity detection and sentiment analysis. We welcome open-source collaboration."}]}