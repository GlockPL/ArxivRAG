{"title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation", "authors": ["Sixu An", "Xiangguo Sun", "Yicong Li", "Yu Yang", "Guandong Xu"], "abstract": "Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains.", "sections": [{"title": "I. INTRODUCTION", "content": "ERSONALITY analysis has long been a central topic in psychological science and has gained increasing importance in recent years due to its wide-ranging applications. It plays a crucial role in various domains such as personalized recommendation systems [1], [2], sentiment analysis [3], [4], and human-computer interaction. Accurately identifying an individual's personality can enable tailored experiences and services, enhancing user satisfaction and engagement. However, personality traits are inherently latent characteristics that are not directly observable, making the assessment of personality a challenging task.\nTraditionally, psychologists have employed structured methods to evaluate an individual's personality. One of the most widely accepted models is the Big Five Personality Traits (as shown in Figure 1), which assesses personality across five key dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. To determine an individual's position on these dimensions, conventional approaches often rely on well-designed questionnaires and psychological inventories that analyze self-reported responses. While these methods are grounded in rigorous psychometric principles, they have notable limitations. Self-reported data can be influenced by social desirability bias, where respondents tailor their answers to be viewed favorably. Additionally, administering and processing these surveys can be time-consuming and resource-intensive, making them less practical for large-scale or real-time applications.\nWith the advent of online video social platforms like Tik-Tok\u00b9 and others, there is a growing opportunity to analyze personality traits through digital means. Users increasingly share selfie videos online, providing a wealth of data that captures not only their visual appearance but also their speech patterns, facial expressions, and environmental context. Compared to static questionnaires, these multi-modal data offer richer insights into an individual's intrinsic traits. Unlike traditional social media platforms that primarily feature text or images, video platforms enable the observation of dynamic behaviors and interactions, which are crucial for understanding personality. This shift opens up new possibilities for applications such as online job interviews, remote education, and personalized content delivery, where assessing personality from videos can significantly enhance outcomes.\nRecent research has begun to explore the potential of analyzing personality traits through online media instead of"}, {"title": "II. RELATED WORK", "content": "Personality analysis has garnered significant interest in recent years due to its applications in psychology, human-computer interaction, and personalized services. Researchers have explored various data sources and methodologies to predict personality traits effectively. Early studies focused on analyzing behavioral patterns from personal devices. For instance, [12] examined communication habits and app usage data collected from Android smartphones to infer personality characteristics. Similarly, [13] leveraged data from Internet of Things (IoT) devices for personality assessment, highlighting the potential of ubiquitous computing in this domain.\nWith the rise of social media platforms, user-generated content has become a rich resource for personality prediction. [14] utilized user activities and interactions across various social media sites to analyze personality traits, demonstrating the value of digital footprints in psychological profiling. In these early computational approaches, traditional machine learning algorithms such as Naive Bayes (NB), Support Vector Machines (SVM), and XGBoost classifiers were commonly employed [15]. As the field progressed, advanced techniques from computer vision and natural language processing [16] were adopted to enhance predictive performance. For example, [17] analyzed handwriting signatures to gain insights into personality traits, while [18] also utilized handwriting data for personality assessment. In the context of social media, [19] proposed a Binary-Partitioning Transformer (BPT) model combined with Term Frequency and Inverse Gravity Moment features to predict personality traits using Twitter data.\nThe evolution of personality analysis techniques has led to their application in various fields such as recommender systems, recruitment processes, and sentiment analysis. For instance, [20] evaluated interviewees' personalities through quizzes and curriculum vitae assessments to aid in hiring decisions. Additionally, [21] introduced an automatic framework for real-time personality analysis, emphasizing the practical utility of these methods. Recent advancements in artificial intelligence, particularly the emergence of Large Language Models (LLMs), have further expanded the capabilities of personality analysis. In 2023, [22] explored the use of personality-descriptive prompts to fine-tune pre-trained language models for few-shot personality recognition, showcasing the potential of LLMs in this domain.\nDespite these advancements, there remains a notable gap in the literature regarding video-based personality analysis, which inherently involves multi-modal information such as text, facial expressions, and vocal cues. Existing multi-modal approaches typically extract features from text, facial images, and voice recordings separately but often lack effective methods for aligning all modalities and preserving the continuity of time-series information. This misalignment can hinder the model's ability to accurately capture the dynamic interplay between different types of data.\nIn our work, we address this gap by proposing a timestamp-based modality alignment technique that synchronizes different modalities based on assigned timestamps. This method aligns four modalities-textual content, facial expressions, audio signals, and background information\u2014ensuring that the temporal dynamics of the video are preserved. By maintaining the consistency and accuracy of the information conveyed across modalities, our approach enhances the reliability of personality prediction from online short videos. This alignment not only improves the integration of multi-modal data but also leverages the sequential nature of videos to capture nuanced personality cues that may be missed by methods lacking temporal synchronization."}, {"title": "III. PRELIMINARY", "content": "In this section, we first give a background about the Big Five Traits, then introduce our motivations, and finally formalize our objectives."}, {"title": "A. Background of the Big Five Personality Traits", "content": "Personality analysis aims to identify and quantify individual differences in behavioral patterns, thought processes, and emotional responses. Among the various models developed to describe personality, the Big Five Personality Traits model has emerged as a widely accepted framework due to its robust empirical support and comprehensive nature. This model delineates personality along five fundamental dimensions: Openness to Experience (O), Conscientiousness (C), Extraversion (E), Agreeableness (A), and Neuroticism (N). Specifically, Openness to Experience reflects imagination, creativity, curiosity, and a preference for novelty and variety. Conscientiousness indicates self-discipline, organization, dependability, and a goal-oriented approach. Extraversion characterizes sociability, assertiveness, energy levels, and a tendency toward positive emotions. Agreeableness involves compassion, cooperation, trust, and a concern for social harmony. Neuroticism relates to emotional instability, anxiety, moodiness, and susceptibility to negative emotions. Mathematically, an individual's personality profile can be represented as a vector p \u2208 R5:\nP = [PO,PC,PE,PA,PN],\nwhere po,pc,PE,PA,PN are scalar values quantifying the levels of openness, conscientiousness, extraversion, agreeableness, and neuroticism, respectively. Each trait is typically measured on a continuous scale, often ranging from low to high scores. Compared to other personality models such as the Myers-Briggs Type Indicator (MBTI) or the Enneagram, the Big Five model stands out for its dimensional approach and strong empirical validation across cultures and populations. It provides a holistic methodology for personality assessment, capturing a wide spectrum of human behavior and traits."}, {"title": "B. Motivations", "content": "Why Multi-Modal Learning for Personality Analysis? Personality manifests through a rich tapestry of behavioral cues expressed across multiple modalities, and relying on a single modality may overlook critical aspects of these expressions, thereby limiting the accuracy of personality assessments. Multi-modal learning enables the integration of diverse data sources such as facial expressions, vocal characteristics,\nlinguistic content, and background context to capture the complexity of personality traits more comprehensively. For instance, facial features can reveal underlying emotions not explicitly communicated through words, while variations in speech patterns\u2014including tone, pitch, volume, and speaking rate-are indicative of certain personality traits. The choice of words and linguistic style provides direct indications of thought processes, and the context or setting in which a person appears offers clues about their preferences, lifestyle, and social context. Short videos serve as an ideal medium to capture these multi-modal signals simultaneously, providing a sequential stream of visual and auditory information that reflects the dynamic interplay of these cues. By extracting and integrating features from these modalities\u2014facial features (Xface), audio features (Xaudio), textual content (Xtext), and background information (Xbackground)\u2014we can develop a more robust and accurate model for personality analysis, capturing the full spectrum of behavioral indicators and leading to more reliable and nuanced predictions.\n\u2022 Why Domain Adaptation for Personality Analysis? However, personality detection models often face challenges when applied across different domains due to variations in cultural expressions, language usage, video styles, and environmental contexts. A model trained on data from one domain may not generalize well to another, resulting in decreased performance. Collecting and annotating large amounts of labeled data for every new target domain is impractical, making domain adaptation a crucial technique. Domain adaptation aims to transfer knowledge from one or more source domains to a target domain with limited labeled data, thereby improving model performance without the need for extensive data collection in the new domain. By incorporating domain adaptation techniques, we enhance the robustness and applicability of our personality detection model across diverse settings, ensuring that it remains effective even when applied to new domains with minimal additional data. This approach addresses practical challenges such as domain shift, data scarcity, and variability in expressions, ultimately leading to more accurate and generalizable personality assessments."}, {"title": "C. Objective Formulation", "content": "Our objective is to develop a domain-adaptive, multi-modal personality prediction model that accurately estimates the Big Five personality traits vector p for individuals based on short video data. The problem can be formally defined as follows.\nNt\nLet D = {(x,p)}1 denote the source domain data,\nwhere x represents the multi-modal features extracted from\nthe i-th video in the source domains, and p is the corre-sponding personality vector. The target domain data is given\nNt\nby Dt = {(x,p)} 1, where Nt < N\u2083, indicating that the\ntarget domain has limited labeled data. We aim to learn a\npersonality prediction model feo : x \u2192 p, parameterized by 0,\nthat minimizes the prediction error on the target domain while\neffectively utilizing the source domain data. The optimization\nobjective can be expressed as:\nmin Ltotal(0) = Ltarget(0) + \\lambda Ladaptation (0),\n\u03b8"}, {"title": "IV. PERSONALITY DETECTION BASED ON MULTI-MODAL SHORT VIDEO ANALYSIS", "content": ""}, {"title": "A. Overview of the Framework", "content": "We introduce a multi-modal video personality prediction framework designed to capture and integrate diverse behavioral cues from short videos for accurate personality trait prediction. As shown in Figure 3, the framework comprises three key modules: (1) multi-modal feature extraction and alignment, (2) personality detection based on multi-modal analysis, and (3) enhancement of personality analysis via domain adaptation. By systematically integrating these components, our framework leverages rich information from facial expressions, background context, audio signals, and textual content while ensuring robustness and generalizability across different domains.\nThe motivation behind this pipeline is to address the challenges inherent in personality prediction from online short videos, including handling heterogeneous multi-modal data, aligning asynchronous modalities, and adapting models to new domains with limited labeled data. The framework first extracts and aligns features from the four modalities using a Timestamp Modality Alignment mechanism based on spoken word timestamps. It then processes these features through temporal modeling and cross-modal fusion techniques to capture both intra- and inter-modal dynamics. Finally, a gradient-based domain adaptation method enhances model performance on target domains with scarce data by leveraging similarities between source and target domains. This integrated approach allows the model to effectively capture complex behavioral cues and remain robust across diverse settings."}, {"title": "B. Multi-Modal Features for Video Analysis", "content": "In this research, we decompose short videos V into four data modalities: speakers' facial expressions, background information, audio signals, and textualized speech content. The\nwhere Ltarget(0) is the loss on the target domain, Ladaptation (0)\nis the domain adaptation loss, and is a hyperparameter\nthat balances the two components. The target domain loss is\ndefined as:\n1 N,\nLtarget (0) = -- \u03a3L(fo(x), , (3)\nNt\nj=1\nWhere L is the Mean Squared Error (MSE) between the\npredicted and true personality trait vectors:\n1 5\nL(fo(x), p) = -- \u03a3 \u03a3(fo(x)k - Pk)2, (4)\n5\nk=1\nwith fe(x)k being the predicted score for the k-th personality\ntrait.\nThe domain adaptation loss Ladaptation(0) aims to minimize\nthe discrepancy between the source and target domains. This\ncan be implemented using techniques such as gradient reversal\nlayers, adversarial training, or gradient-based similarity mea-\nsures, which align the feature distributions or adapt the model\nparameters to reduce domain shift."}, {"title": "Timestamp Modality Alignment (TMA)", "content": "Timestamp Modality Alignment (TMA): We propose a timestamp modal alignment (TMA) mechanism to synchronize the four modalities at each timestamp based on minimal semantic units-specifically, individual spoken words. This alignment ensures that multi-modal features correspond precisely, facilitating a comprehensive analysis of personality traits. To implement this alignment, we segment the modalities using the timestamps of words as they appear in the audio stream. These timestamps are obtained using the Connectionist Temporal Classification (CTC) algorithm [23], applied via the pre-trained Wav2Vec2 model [24]. By aligning the audio segments with the spoken words, we establish a temporal framework for synchronization. Similarly, we slice the corresponding video frame sequences according to these timestamp segments. This process ensures that the visual (both facial expressions and background), audio, and textual data for each segment are matched accurately.\nFormally, given a short video v \u2208 V, we denote its tran-scribed speech content as Utext = {W1,W2, ..., Wn}, where wi is the i-th word spoken, associated with timestamp Ti. At each timestamp Ti, we have: (1) Frame Extraction. We extract the video frames corresponding to Ti, resulting in fr\u2081; (2) Audio Segment. We obtain the audio segment Vaudio,7\u2081 corresponding to the duration of wr; (3) Facial Image. We extract the facial region Uface, T\u2081 from fr\u2081; (4) Background Information. We capture the background Vbackground,7; from fr\u2081; (5) Word Token. We have the word w\u2081\u2081 = wi spoken at T\u2081. This approach results in a data tuple for each timestamp T\u2081 with four modal features\nlike DT\n=\n{Vface,7\u2081, Ubackground, Ti, Vaudio, Ti, Wr\u2081}. To handle\ncases where a person is not speaking (leading to missing D\u2081\u2081)\nand to prevent data imbalance due to varying speech lengths,\nwe standardize the text length. We set n = 70 words for each\nvideo's text data (an empirical length for most short videos\nonline). Videos with fewer than 70 words are padded with a\nspecial token [UNK]. Thus, we obtain a consistent sequence\nV = {DT1, DT2,..., D\u012bn} for each video."}, {"title": "Modal Feature Extraction", "content": "Modal Feature Extraction: After aligning the data, we extract features from each modality in Dr, using specialized pre-trained models, ensuring that the representation of each modality captures both low-level and high-level information.\nVisual Features: The visual modality consists of both facial expressions and background context. These two visual features are combined to form the overall visual representation Uvisual, Ti for each timestamp Ti.:\n\u2022 Facial Features: Recognizing that facial expressions can reflect underlying personality traits, we utilize an effective facial feature extractor based on the Face Recognition library\u00b2. This tool extracts facial features from Uface, Ti, providing a detailed representation of the speaker's facial characteristics.\n\u2022 Background Features: The environment in which the speaker appears can influence first impressions and provide context about their personality. To capture this, we employ TimeSformer [25], a pre-trained video model known for its excellent performance in extracting high-level features from video frames. TimeSformer processes Ubackground, Ti, producing a feature vector that represents the global background context.\nAudio Features: Auditory cues such as tone, speed, and pitch can vary with personality. By concatenating the MFCCS and the features from WavLM, we obtain a comprehensive audio representation Vaudio, for each timestamp. To capture these nuances:\n\u2022 Low-Level Audio Features: We compute the Mel Frequency Cepstral Coefficients (MFCCs) of the audio segment Vaudio, T\u2082. MFCCs simulate how humans perceive sound and provide a compact representation of the audio signal's spectral properties.\n\u2022 High-Level Audio Features: We use WavLM [26], a pre-trained model for speech representation learning, to extract high-level features from Vaudio, T.\nTextual Features: For the textual modality, we transform the word token w\u2081\u2081 into an embedding using the token embedding layer of the language model employed in our framework. This approach ensures that the textual data is represented in the same embedding space as the language model, facilitating downstream processing.\nNote that we do not fine-tune these pre-trained models for feature extraction during our training process; they serve solely to provide rich, pre-processed representations. After processing each modality, for the i-th video, we obtain sequences of representation vectors as follows: visual vi \u2208 Rdxn, audio a\u017c \u2208 Rdaxn, and textual ti \u2208 Rdtxn where n is the sequence length (standardized to 70), and du, da, and dt are the dimensions of the visual, audio, and textual feature vectors, respectively. This multi-channel modal learning approach allows us to extract representations from each modality independently, without interference. Moreover, processing the modalities in parallel enhances computational efficiency."}, {"title": "C. Personality Detection based on Multi-modal Analysis", "content": "After extracting the necessary features from the aligned multi-modal data, we propose a comprehensive framework for personality prediction. This framework consists of two main stages: feature preprocessing and personality inference. In the first stage, we preprocess and align features from different modalities to ensure compatibility and enhance their representational capacity. In the second stage, we fuse these preprocessed features and employ a predictive model to infer personality traits. The overall architecture of the framework is depicted in Figure 3.\nTime Frame Multi-Modality Data Reshaping : Due to the use of different embedding models for each modality, the extracted features have varying dimensions and formats. To address this inconsistency and capture temporal dependencies within each modality, we employ a Bidirectional Long Short-Term Memory (Bi-LSTM) network for each modality independently. Let Xi \u2208 {Vi, face, Vi, background, Vi,audio, Vi,text} represent the sequence of features for the i-th modality of a video V, where i indexes the modalities (face, background, audio, text). Each modality provides a sequence of feature vectors over time: X\u2081 = {Xi,1, Xi,2,..., Xi,n}, where n is the total number of timestamps (standardized to 70 as previously described). Each modality's sequence X\u2081 is processed through a Bi-LSTM network to capture temporal patterns and reshape the features into a consistent format:\nFi,t = T (TS (Xi,t), TS (Xi,7+1), Wi,t)\nTemporal Fusion via Self-Attention Mechanism: To enhance the temporal representation within each modality, we apply a self-attention mechanism to the Bi-LSTM outputs. The self-attention mechanism allows the model to weigh the importance of different time steps, enabling it to focus on the most informative features for personality prediction. For each modality i, we compute the self-attention output Ui as follows:\nn\nU\u2081 = \u2211 Sit (WiFi,t)\nt=1\nwhere wi,t is self-attention weight of one modality. Ui is the\nself-attention output of one modality feature. si,t is the self-\nattention score, which is calculated as follows:\nSi,t = softmax\n(Qik (kit)\n\u221adk\nVit\nwhere dk is used to normalize the value, queries Qi,t =\nWON (WkFi), keys Kit = WKN (WkFi), and values\nVi,t = WtN (WkFi). N(\u00b7) is the normalized function.\ni,t\ni,t\nCross-Modal Fusion with Bilinear Transformation : While the self-attention mechanism captures intra-modality temporal dependencies, integrating information across modalities provides complementary insights. To achieve cross-modal fusion, we employ a bilinear transformation to model the interactions between modalities. Let us denote the modalities as a, b, c, d. For modality a, we compute its interaction with the other modalities using bilinear pooling:\nZ = MLP ([Zab, Zac, Zad])\nwhere [] denotes concatenation, and Zab, Zac, Zad are the interaction features between modality a and modalities b, c, d, respectively, computed as:\nn\n\u0396\u03b1j = \u03a3\u03c3 Faj (F))\nt=1\n(5)\n(6)\n(7)\n(8)\n(9)\nIn this equation, o is an activation function such as ReLU,\nWaj is a learnable weight matrix representing the interaction"}, {"title": "D. Enhance Personality Analysis via Domain Adaptation", "content": "Personality detection from online short videos presents significant challenges due to the diverse and dynamic nature of content across different platforms, cultures, and user demographics. Models trained on data from one domain often struggle to generalize to other domains because of variations\nto find the optimal model parameters @ that minimize\nthe loss on the target domain:\nmin L(f(0), Dt),\n\u03b8\nwhere L is the loss function, specifically the Mean Squared\nError (MSE):\n1 N\nMSE = - \u03a3(Yik - Yik)2.\nN\ni=1\nHere, \u0177ik is the predicted value for the k-th dimension of the\nBig Five personality traits, and Yik is the corresponding true\nvalue.\nProposed Method: To effectively adapt the model to\nthe target domain, we introduce a gradient-based domain\nadaptation method that operates in two main stages: training\non the source domains and adapting to the target domain using\ndomain similarity measures.\nIn the first stage, we begin by initializing k models fi with\nidentical parameters 0, corresponding to each of the k source\ndomains. Each model fi is then trained on its respective source\ndomain data Xs,i, resulting in an updated model f. During\nthis training, we compute the gradient of the loss function\nwith respect to the model parameters for each source domain,\ndenoted as fi:\n\u2207 fi = \u2207oL (f(0), xs,i).\nThese gradients capture how the model parameters need to\nbe adjusted to minimize the loss on each source domain,\neffectively encoding domain-specific learning directions.\nIn the second stage, we focus on adapting the model to the\ntarget domain by utilizing the few-shot examples available. We\nfeed the target domain data xt into each of the updated source\nmodels f and compute the gradient of the loss function with\nwith respect to the model parameters:\n\u2207 f = \u2207oL (f(0),xt),\nwhere \u2207f represents the gradient for the target domain\nusing the i-th updated source model. We then measure the\n(13)\n(14)\n(15)\n(16)"}, {"title": "Algorithm 1 Domain Adaptive Algorithm", "content": "INPUT(Initial Model parameter 0\nk Source Domain dataset Xsi\nfew shot data Xt in Target Domain\nnumber of iterations N)\nfor inter range N\nfor i Crange k\nUpdate source domain model parameter\nwith Xsi\nCompute Source Domain Gradient\nby Equation (12)\nCompute Target Domain Gradient\nby Equation (13)\nCalculate Domain Similarity\nby Equation (14)\nend\nUpdate initial model parameter 0\nwith Equation (15)\nend\nsimilarity between the target domain and each source domain\nby calculating the cosine similarity between their gradients:\nsimcos (ft, fi) =\n\u2207ffi\n||fi||\u00d7 ||fi||' (17)\nwhere denotes the dot product, and ||\u00b7|| denotes the Euclidean\nnorm. The similarity scores si = simcos(\u2207f\u00ed, \u2207fi) quantify\nhow closely each source domain is related to the target domain\nin terms of gradient directions.\nUsing these similarity scores, we update the initial model\nparameters @ by weighting the gradients from each source\ndomain according to their relevance to the target domain:\nk\n\u03b8\u03b8-\u03b1\u03a3 si VoL (f(0), xt), (18)\ni=1\nWhere a is the learning rate. This update step ensures that\nthe model emphasizes learning from source domains that are\nmore similar to the target domain, thereby improving its ability\nto generalize despite the limited data available in the target\ndomain.\nBy integrating the gradient information from both the source\nand target domains, the model effectively transfers knowledge\nfrom relevant source domains while adapting to the specific\ncharacteristics of the target domain. This approach addresses\nthe challenges associated with domain shift and few-shot\nlearning in personality detection from online short videos.\nTraining Procedure: To further enhance the model's\nperformance, we incorporate an adaptive learning rate strategy\nas shown in Algorithm 1. The overall training procedure begins\nwith the initialization of the model parameters 0. During each\niteration of training, the following steps are executed:\nFirst, each model fi is trained on its respective source\ndomain data Xs,i, resulting in updated models fi. The source\ndomain gradients fi are computed using Equation (15).\nThese gradients reflect how each source domain influences\nthe model parameters.\nNext, the adaptation to the target domain is performed by\nfeeding the few-shot target examples xt into each of the\nupdated source models fi and computing the target domain\ngradients f as in Equation (16). The domain similarity\nscores si are then calculated using Equation (17), providing a\nmeasure of how relevant each source domain is to the target\ndomain based on the alignment of their gradient directions.\nFinally, the model parameters @ are updated using the\nweighted gradients from the source domains as specified in\nEquation (18). The learning rate a may be adjusted adaptively\nduring training to improve convergence and generalization,\nfollowing strategies from prior work.\nThis iterative process continues until the model converges\nor a stopping criterion is met. By continually adjusting the\nmodel parameters based on domain similarity, the method\nensures that the model learns from the most relevant source\ndomains while adapting to the unique aspects of the target\ndomain. By using gradient-based domain similarity, the model\nselectively integrates knowledge from source domains that\nare most similar to the target domain, thereby enhancing\nits performance in few-shot learning scenarios. This method\nis particularly valuable for personality detection in online\nshort videos, where data diversity and scarcity are significant\nchallenges."}, {"title": "V. EXPERIMENTAL ANALYSIS", "content": ""}, {"title": "A. Data Preparation", "content": "In our work, we utilize the First Impressions dataset created by Biel and Gatica-Perez from the 2016 ChaLearn competition [27] for training and evaluation. This dataset comprises 10,000 short video clips, each lasting 15 seconds. All videos were collected from the online social platform YouTube and feature individuals in everyday interview scenarios. The videos are labeled with scores for the Big Five personality traits, indicating each interviewee's personality across these five dimensions. The personality trait scores are represented within the range [0, 1]. Additionally, text transcriptions for each video are provided.\nTo implement our domain adaptation training method with the First Impressions dataset, we employ the K-means clustering algorithm to group the entire dataset into 20 topics based on their textual features. Compared to other modalities, textual data is usually closely related to the topics discussed in the videos. Manually labeling the topic of each video is impractical; therefore, clustering offers an efficient and effective way to group videos with similar content by leveraging their contextual features. We assign domain labels numbered from 1 to 20. Subsequently, our multi-modal feature extraction framework is applied to process the videos in each domain separately."}, {"title": "B. Baseline Methods", "content": "To evaluate the effectiveness of our method under few-shot learning conditions, we compare its performance with the following baseline methods:"}, {"title": "C. Experimental Settings", "content": "All experiments are conducted on a system running Ubuntu with an NVIDIA GeForce RTX 3090 GPU. We train our model using 10 few-shot data samples from the target domain. The remaining data in the target domain are split into validation and test sets with a ratio of 2:8. The initial learning rate a is selected through grid search as a hyperparameter. We employ the AdamW optimizer to train the model."}, {"title": "D. Evaluation Metric", "content": "For the evaluation, we select each of the 20 domains as the target domain in turn, using all remaining domains as source domains. During the domain adaptation training process, for each training epoch, we randomly select 10 few-shot data samples from the target domain to compute domain similarity and use the remaining data in the target domain for validation and testing. To assess the performance of our method, we use the average accuracy of each individual personality trait to compare different models. The average accuracy is defined as:\n1 N\nAccuracy = (1 - Yik - Yik) \u00d7 100%,\ni=1\n(19)\nwhere N is the number of samples, \u0177ik is the predicted value,\nand Yik is the true value for the k-th personality trait of the\ni-th sample."}, {"title": "E. Overall Performance Evaluation", "content": "The results in Table I demonstrate that our proposed model achieves superior accuracy in predicting the Big Five personality traits from short video data. With an average accuracy of 77.92%, our model outperforms all baseline methods, indicating its effectiveness in capturing personality cues across multiple modalities. Compared to the closest baseline, the Fine-tune method with an average accuracy of 77.78%, our model shows consistent improvements across all personality traits. Specifically, we achieve higher accuracies in Extraversion (77.54% vs. 77.40%), Neuroticism (77.42% vs. 77.26%), Agreeableness (79.33% vs. 79.19%), Conscientiousness (77.26% vs. 77.08%), and Openness (78.08% vs. 77.97%). These gains, though modest, reflect the robustness of our approach to enhancing personality prediction.\nOur model significantly outperforms other baselines such as IVL, DCC, Evolgen, ICC, and DBR, which have average accuracies ranging from approximately 59% to 67%. The substantial margin, over 10 percentage points, between our model and these methods underscores the effectiveness of our techniques. This improvement can be attributed to our timestamp-based modality alignment, which ensures precise synchronization of multi-modal data, and the use of self-attention mechanisms and cross-modal fusion that enhance feature representation. Furthermore, the incorporation of gradient-based domain adaptation allows our model to generalize well to target domains with limited labeled data. By leveraging similarities between source and target domains, our method efficiently utilizes few-shot data, addressing common challenges in personality analysis tasks. In summary, the superior performance of our model across all five personality traits validates the effectiveness of our multi-modal framework. The consistent accuracy improvements highlight our model's ability to capture complex behavioral cues and adapt to diverse data domains, offering a significant advancement over existing methods in personality prediction from short videos."}, {"title": "F. Multi-modal Adaptation Effectiveness", "content": "To see how different modality combinations work with our framework, we remove one modality from the original four"}, {"title": "G. The Number of Source Domain Test", "content": "For our Domain Adaptive learning method, the number of source domains is a hyper-parameter. At first, we take all remaining data as Source Domains. However, for performance evaluation, we can see that different source domains may have an impact on the performance of our domain adaptive method. Thus, we change the source domain numbers to 1, 5, 10, and 15, respectively and randomly select the corresponding number of source domains to train our model. The experiment results are summarized in Fig. 6, which plots the prediction accuracy of different target domains 1, 2, 3, and 4 with different source domain numbers. From the result, it can be concluded that the model performance improves with the increase of the Source Domain number. However, it can also be noticed that even with the same number of source domains, the model performance shows great difference, as they select different domains as source domains."}, {"title": "H. The Effectiveness Of Principal Components", "content": "The results presented in Table II demonstrate the significant impact of the key components in our framework on the accuracy of personality trait predictions. When the domain similarity calculation is removed, there is a noticeable decrease in average accuracy from 77.92% to 77.20%, marking a drop of 0.72%. This reduction is consistent across all individual personality traits, with Openness decreasing from 78.08% to 77.09% and Agreeableness falling from 79.33% to 78.63%. These findings highlight the crucial role of the domain similarity calculation in our gradient-based domain adaptation method. By quantifying the relevance of source domains to the target domain, this component enables the model to prioritize learning from the most pertinent data, thereby enhancing generalization and performance, especially in few-shot learning scenarios.\nExcluding the adaptive learning rate also affects the model's performance, resulting in an average accuracy of 77.38%, which is a reduction of 0.54% compared to the full model. The most significant decrease is observed in Conscientiousness, dropping from 78.08% to 77.04%. Other personality traits exhibit slight declines as well. This suggests that the adaptive learning rate contributes to more efficient and effective optimization by adjusting the step size during training. It helps the model to converge better and avoid local minima, leading to"}, {"title": "I. Domain Similarity Analysis", "content": "In our method, we use the gradient to calculate the domain similarity, which can help the model work better in the target domain. Thus, the domain similarity calculation result can be used to measure the similarity of different domains. To evaluate how our method learns the similarity between different domains, we collect the final domain similarity calculation results, after training the model. We visualize the results by plotting a heat map, which can show the similarity result between two domains as shown in Figure 7. It can be seen that some domains have high similarity with others. For example, domain 13 has a high similarity with domain 1, and domain 17 has a"}]}