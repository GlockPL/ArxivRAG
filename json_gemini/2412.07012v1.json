{"title": "PROVISION: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "authors": ["Jieyu Zhang", "Le Xue", "Linxin Song", "Jun Wang", "Weikai Huang", "Manli Shu", "An Yan", "Zixian Ma", "Juan Carlos Niebles", "Silvio Savarese", "Caiming Xiong", "Zeyuan Chen", "Ranjay Krishna", "Ran Xu"], "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image-based queries. Existing practices rely on powerful but costly large language models (LLMs) or multimodal language models (MLMs) to produce instruction data. These are often prone to hallucinations, licensing issues and the generation process is often hard to scale and interpret. In this work, we present a programmatic approach that employs scene graphs as symbolic representations of images and human-written programs to systematically synthesize vision-centric instruction data. Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy. By implementing a suite of 24 single-image, 14 multi-image instruction generators, and a scene graph generation pipeline, we build a scalable, cost-effective system: PROVISION which produces diverse question-answer pairs concerning objects, attributes, relations, depth, etc., for any given image. Applied to Visual Genome and DataComp datasets, we generate over 10 million instruction data points, PROVISION-10M, and leverage them in both pertaining and instruction tuning stages of MLMs. When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D split of CVBench, along with a 3% increase in performance on QBench2, RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8% improvement on Mantis-Eval. Incorporation of our data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks.", "sections": [{"title": "1. Introduction", "content": "The success of Multimodal Language Models (MLMs) such as LLaVA and InstructBLIP has been largely built upon the availability of multimodal data [23, 50, 82], and visual instruction data [20, 54]. In particular, visual instruction data is key to enable MLMs to follow the instruction and respond to user questions about input images effectively. To gather visual instruction data, existing practice mainly relies on powerful Large Language Models (LLMs) or MLMs to generate such data samples [54, 55, 58, 94]. While effective, it does come with certain limitations. First, the generation process remains largely a black-box mechanism, making it difficult to interpret the process and control or customize outputs precisely. Second, even the most advanced LLMs or MLMs are still prone to hallucination [19, 27, 29, 80, 85, 92, 118], generating content that can be factually inaccurate, which undermines the reliability of the resulting visual data and is typically hard to detect and correct ex post. Third, the reliance on powerful LLMs or MLMs might hinder the scalability of the data generation process due to the potential cost (such as API usage costs) and entail license constraints that prevent the use of generated data for model training [72].\nIn this work, we explore a complementary approach for programmatically generating visual instruction data. To enable programmatic generation, we leverage scene graphs [36] as a structured representation of image semantics. We develop programs to systematically generate visual instruction data using automatically extracted scene graph representations from images. In a scene graph, each object is represented as a node, where the attributes of the object-such as color, size, or materials are assigned directly to that node. The relationships or interactions between these objects are depicted as directed edges connecting the corresponding nodes. Given a scene graph, a program can generate questions like \"How many red objects"}, {"title": "2. PROVISION", "content": "In this section, we first introduce how we generate vision-centric instruction data programmatically with scene graphs. Then, we present our scene graph generation pipeline that automatically generates a scene graph for any given image."}, {"title": "2.1. Generating instructions programmatically", "content": "Augmented scene graph. We first describe the scene graph definition used throughout this work, which is an augmented version of the standard scene graph representation defined in Visual Genome [36], including additional depth and segmentation labels. Given an input image x with size (w, h), which have N objects {i_1,\u2026\u2026\u2026,i_n} and each object i_j has a list of attribute attr. The augmented scene graph is G = (V, E), where V\u2286 {i_1,\u2026\u2026\u2026, i_n}, E = {(i_j, i_k, a) | i_j, i_k \u2208 V} and a_{jk} is the relation between objects i_j and i_k. Each object i_j has its corresponding bounding box a^{det}, segmentation a^{seg}, and a list of attribute attr. Additionally, we add depth annotation a^{dep} as an augmented feature.\nSingle-image visual instruction data. We implement 24 single-image instruction data generators to transform an augmented scene graph into thousands of high-level percep-"}, {"title": "2.2. Generating scene graph for any image", "content": "We generate a scene graph with object detection, image segmentation, attribute generation, relation generation, and depth estimation. While we utilize state-of-the-art, openly accessible models for each module, our approach is not limited to these specific models. We provide an overview of the scene graph generation pipeline in Fig. 3.\nObject detection. We start with object detection to seek the bounding boxes and labels for further annotation methods. The object detection model f_{det}(x) will annotate all bounding boxes and the corresponding labels of all objects. For example, for object j, the object detection method will output a^{det} = ([x_{min}, y_{min}, x_{max}, y_{max}], l_j), where (x_{min}, y_{min}) denotes the left bottom point of the bounding box and l_j denotes the label for object j. In this work, we adopt YOLO-world [17] as our object detection model f_{det}(x).\nImage segmentation. We then adopt image segmentation for better object representations. The image segmentation model f_{seg}(x, a^{det}), SAM-2 [79] in this work, takes the image x and bounding boxes a^{det} from object detection as input. Specifically, the segmentation will draw the pixel-wise segmentation a^{seg} \u2208 R^{w\u00d7h} according to a^{det}.\nAttribute detection. Inspired by prior work [122], we finetune vision-language models, i.e., CoCa [110] and LLaVA-1.5 [54] as attribute detection models. We construct the training data from LSA, a large-scale attribute dataset [78]. We use its bounding box annotations to crop each object as a single image and use the corresponding attribute annotation as the target output. For LLaVA-1.5, we use \u201c<image>\n{object_label}\u201d as the prompt template for finetuning data construction. Based on our automatic and manual evaluations, LLaVA-1.5-13B is better than competitors with a precision of 90%, so we adopt it as our attribute detection model. More details of the evaluation and implementation can be found in Appendix.\nRelation Detection. We finally retrieve the relations a^{rel} for all pairs of objects i_j and i_k in image x according to their segmentation. To achieve this, we pick an finetuned Osprey model [76] as f_{rel}(x, a^{seg}_j, a^{seg}_k), which takes the whole image and segmentation of objects i_j and i_k as input, and generate a relation a_{jk}^{rel}. We then ground the generated relation by comparing the similarity between a^{ask}_{jk} and a^{rel} our relation library and select the top-1 result as the a^{rel}_{jk}.\nDepth estimation. Our augmented scene graph also included the pixel-wise depth annotation a^{dep} generated by a depth estimator f_{dep}(x). In this work, we use Depth Anything V2 [106] as our depth estimation model. The pixel-wise depth annotation can be used to infer the depth of objects for comparing depth among objects."}, {"title": "3. Experiments", "content": "In this section, we first describe the instruction data we synthesize in this work, followed by the experimental setup, results, and analysis. We found that 1) Our synthesized instruction data can boost model performance and those from manually-annotated scene graphs are usually better than their counterpart from model-generated scene graphs; 2) The data format (short answer vs. multiple choice) and data scale are important factors to consider for the best performance; and 3) While our data helps when incorporated in either the pre-training or fine-tuning stage, incorporating them in both stages achieves the best performance."}, {"title": "3.1. PROVISION-10M Dataset Construction", "content": "Leveraging manually-annotated scene graph dataset. We first utilize Visual Genome [36], one large-scale manually-annotated scene graph dataset to construct our instruction data. Specifically, we augment each scene graph with depth and segmentation annotation using Depth Anything V2 and SAM-2. Then, we generate 1.5 million single-image instruction data (VG-S) and 4.2 million multi-image instruction data (VG-M): For VG-S, we sample one instruction data per image and generator, while for VG-M, we generate 100,000 samples per generator.\nLeveraging generated scene graph. Besides, we sample 120,000 high-resolution images with more than 5 objects from the DataComp dataset [23], and use our scene graph generation pipeline as described in Sec. 2.2 to generate the"}, {"title": "3.2. Experimental Setup", "content": "Augmentation vs. replacement. To evaluate the utility of our generated dataset, we adopt two settings: augmentation and replacement. Given a base dataset which is an existing dataset used to train MLMs, the augmentation means augmenting the base dataset with our data, while the replacement is to replace a random subset of the base dataset with our data. In particular, we experiment with different augmentation/replacement ratios. For example, assume the base data contains 100K samples, an augmentation ratio of 5% indicates including an additional 5K of our data in the training set, while a replacement ratio of 5% means replacing 5K samples of the base data with our data.\nMultiple choice vs. short answer. We explore both multiple choice and short answer formats for our generated instruction data, testing three distinct configurations: (1) all data in multiple choice format (multiple choice), (2) all data in short answer format (short answer), and (3) a balanced mix of formats, with half of the data in multiple choice and half in short answer (half-half). These settings allow us to assess the impact of each answer type on model performance, as well as the versatility of the generated data in supporting different response styles.\nModel and training recipe. We use LLaVA-1.5 [54] instruction data as the base dataset and its training recipe for instruction tuning LLaVA-1.5-7B model with single-image instruction data; similarity, we follow Mantis [30] for LoRA [25] instruction tuning Mantis-SigLIP-8B with multi-image instruction data and adopt Mantis-Instruct (excluding video-related subsets) as the base dataset. In addition, we experiment with adding our data to both the pre-training and fine-tuning stages of xGen-MM-4B model [104].\nBenchmarks. We evaluate models on several popular MLM benchmarks including the following single-image benchmarks: CV-Bench (CVB) [91], SEED-Bench [41, 42], MMBench (MMB) [57], MME [22], QBench2 [101], MMMU [113], RealWorldQA [90], MM-Star [13], MMVet [111]; and multi-image benchmarks: Mantis-Eval [30] and MMT-Bench (MMT) [108]."}, {"title": "3.3. Instruction Tuning", "content": "We exhibit our experiment results by answering the following questions: (1) do scene graphs help produce applicable instructions, and (2) do they need to be real, or can they be automatically generated?\nDo scene graphs help produce applicable instructions? This question can be answered affirmatively by Table 1 and Table 2. For single image instructions (Table 1), we compare the model trained with the base dataset and the models trained on four dataset augmentation/replacement ratios and three data formats across eight benchmarking datasets. The results illustrate that, for replacement, (1) instruction tuning the LLaVA-1.5-7B model with VG-S data yields improvements over the base dataset (LLaVA-1.5 instruction data) in averaged performance across all settings and achieves the best performance when the replacement ratio at 20%, and (2) on average, model performance is positively related to the amount of replaced multiple choice questions while negatively related to the replacement of short answers. For augmentation, we can see that (1) the model performance on all data formats increases with more data samples from VG-S and (2) compared with replacement, augmentation achieves better performance at the same level of data ratio. Overall, results on single image tasks suggest that mixing original data with scene graph-generated short answer and multiple choice format instruction yields competitive results when a substantial portion of the original data is replaced.\nFor multi-image instruction (Table 2), we test the models on two multi-image benchmarks and six single-image benchmarks. We can observe that for the 20% replacement ratio, the half-half format achieves the highest performance for both multi-image and single-image benchmarks, with an average score of 59.7, showing the benefit of mixing data formats. In contrast, at a 50% replacement ratio, the model\u2019s performance generally decreases in both benchmarks, suggesting that excessive replacement with new data may reduce the model\u2019s ability to generalize across tasks. For augmentation, multiple choice format stands out with a score of 60.0 on average at 20% of augmentation, while half-half at 50% augmentation achieves the highest average score of 60.1. This suggests that augmentation, especially with mixed data formats, can effectively enhance the model\u2019s robustness. Interestingly, augmentation generally provides higher performance stability across both multi-image and single-image benchmarks compared to replacement. On the other hand, half-half format with 10% augmentation and multiple choice format with 20% augmentation show strong performance across multi-image benchmarks (Mantis-Eval and MMT), showing the superiority"}, {"title": "Do the scene graphs need to be manually annotated,\nor can they be model generated?", "content": "We compare models trained on instruction data from manually annotated (VG-S, VG-M) and model-generated (DC-S, DC-M) scene graphs. According to Figure 4 (VG-S vs. DC-S), DC-S underperforms VG-S at lower data scales. Interestingly, as the data scale increases to a 50% ratio, DC-S achieves comparable performance to VG-S, suggesting that larger data scales help mitigate initial performance gaps between data from model-generated and human-curated scene graphs. Moreover, from Figure 5 (VG-M vs. DC-M), we observe that as the ratio increases, the model performance on the replacement setup grows first and decays later. With the increase of replacement or augmentation ratio, DC-M underperforms with VG-M, suggesting that on multi-image settings, a larger scale on generated scene graphs may trigger edge effects and not always provide stable performance gain to the model training. In conclusion, instruction data from manually annotated scene graphs is in general better than that from model-generated scene graph, yet both data are able to boost model performance in most cases."}, {"title": "3.4. Pre-training vs. Instruction Tuning", "content": "To assess the benefits of incorporating our data at scale during the pre-training stage, and to compare the effects of adding our data in the pre-training versus fine-tuning stages, we adopt the xGen-MM (BLIP-3) [104] training methodology and use its pre-training data recipe as a foundation. We establish a baseline by pre-training a model on approximately 10 billion tokens using the xGen-MM (BLIP-3) [104] pre-training recipe without our data. Similarly, we apply a baseline fine-tuning recipe of 1 million samples that excludes our data. Details for both recipes are provided in the Appendix."}, {"title": "4. Related Work", "content": "We contextualize our work on the recent rise of MLMs and approaches of synthesizing data for MLMs.\nMultimodal language models (MLMs). In recent years, MLMs, by integrating visual encoders within various pretrained large languages models [2, 6, 10, 11, 14, 26, 43, 49, 49, 56, 63, 64, 69, 77, 83, 86-89, 93, 97, 98, 104], have progressively driven advancements in visual-language learning. With ubiquitous open-sourced LLM backbones and the increasing data for visual instruction tuning. Models like Blip series [20, 45, 46, 75, 104], QwenVL series [5, 96], LLaVA series [52-54], InternVL series [15, 16], etc. have achieved unprecedented visual understanding performance in nearly all kind of visual tasks. However, recent works like Task Me Anything [115], CVBench (Cambrian-1) [91] show that while MLMs are adept at high-level semantic understanding, they surprisingly underperform in vision-centric tasks (e.g. depth estimation, counting, localization, etc). Furthermore, the availability of instruction data for vision-centric tasks remains limited compared to other multimodal data such as image captions, due to the high cost of collection and annotation.\nSynthetic data for MLMs. Synthetic data has increasingly been used for pretraining and finetuning [4, 8, 21, 40, 47, 55, 58, 70, 71, 75, 99, 103, 107, 109, 116, 120, 121] of large language models (LLMs), leading to notable improvements in reasoning, instruction following, and other tasks. Similarly, synthetic data has been integrated into multimodal language model (MLM) development, including approaches like model-generated instruction data [54, 55, 58, 94] and synthetic captions [47, 59, 81, 105]. However, current methods largely focus on synthetic data generation using LLM, MLM, and diffusion models. Programmatic/procedural methods have also been employed to generate multimodal data, such as in GQA [28], AGQA [24], and Task Me Anything [115], yet these are often designed primarily for evaluation or as contributions to a final dataset. In contrast, our approach centers on the data generation process itself, producing single- and multi-image instruction data adaptable to any image source for training purposes."}, {"title": "5. Conclusion", "content": "Our PROVISION system programmatically synthesizes vision-centric instruction data for training MLMs by leveraging scene graph representations and human-written programs. Applied to Visual Genome and DataComp, PROVISION produces PROVISION-10M, a dataset of over 10 million instruction data, which we leverage in both pretraining and instruction tuning stages of MLMs, resulting in notable performance improvements and demonstrating the potential of programmatically scaling vision-centric instruction data in advancing MLM capabilities."}, {"title": "Limitations and future directions", "content": "Limitations of PROVISION include its reliance on the quality and completeness of scene graphs, as well as its dependency on human-written programs. Future work could address these by enhancing the scene graph generation pipeline to enable more accurate data synthesis and by developing automated program synthesis, leveraging LLMs to further scale data generation."}, {"title": "6. Fine-tuning attribute detector", "content": "Dataset preparation. We adopt the LSA dataset [78] for training attribute detectors for our scene graph generation pipeline. We first filter out bounding boxes whose size is less than 25 pixels. Then we normalize the attributes by (1) removing non-attributes like \"world\" and (2) merging attributes like \"gray\" and \"grey\". We also remove objects with conflicting attributes at the same time like \"big\" and \"small\". Finally, for each object category with more than 10 instances in the dataset, we sample 5 instances to compose the test set (42,558 objects) and use the remaining as the training set (3,679,514 objects).\nFine-tuning and evaluation. For CoCa model, we use the OpenCLIP codebase to fine-tune a ViT-L-14 CoCa pretrained on LAION-2B. For LLaVA-1.5 model, we use the official codebase to fine-tune both LLaVA-1.5-7B and LLaVA-1.5-13B. For evaluation, we report the average number of predicted attributes of each model. Besides, we report the precision and recall of the model output against the provided labels. However, because the LSA dataset is noisy and incomplete, we sample 200 data from the test set and manually evaluate whether each predicted attribute is correct or not to calculate the human precision.\nResults. The results are in Table 5. From the results, we can see that the CoCa model outputs more attributes (3.19) than LLaVA-1.5 models (1.13), because the LLaVA-1.5 models take the object label as input and is likely to focus on the object in the cropped image while CoCa only inputs the cropped image and may output attributes irrelevant to the centered object. In addition, we found that LLaVA-1.5-13B is better than LLaVA-1.5-7B and CoCa for all the evaluation metrics, so we adopt LLaVA-1.5-13B for our scene graph generation pipeline.\nWe will release both train/test dataset and the trained models."}, {"title": "7. Instruction Tuning Experiments", "content": "For fine-tuning LLaVA-1.5 models, we reuse the finetuning script in the official Github repository: https://github.com/haotian-liu/LLaVA/blob/main/scripts/v1_5/finetune.sh.\nFor fine-tuning Mantis-SigLIP-8B, we reuse the script from the official Github repository: https://github.com/TIGER-AI-Lab/Mantis/blob/main/mantis/train/scripts/train_mllava.sh."}, {"title": "8. xGen-MM (BLIP-3) Experiments Recipes", "content": "8.1. Pre-training Recipes\nBase Recipe. Following xGen-MM(BLIP-3) [104], the base pre-training recipe includes the following \u2013 Caption Datasets: Datacomp [23] (10%), BLIP3-KALE [4] (60%), BLIP3-OCR [104] (10%), BLIP3-GROUNDING [104] (10%), CC12M [9] (2.5%), CC3M [9] (2.5%), VG [36] (2.5%), and SBU [74] (2.5%). Interleaved Datasets: OBELICS [37] (35%), MINT-1T-HTML [3] (35%), MINT-1T-PDF [3] (25%), and MINT-1T-ArXiv [3] (5%).\nThe baseline model is trained using 24 H100-80GB GPUs for 8,000 steps. At each step, data is sampled with equal probability from either the Caption Datasets or the Interleaved Datasets bucket (50% each). Within each bucket, datasets are sampled according to the probabilities listed above.\nThe batch sizes are configured as follows: Caption Datasets: Batch size of 300 per GPU. Interleaved Datasets: Batch size of 50 per GPU.\nAugmented Recipe with PROVISION Data. To ensure a fair comparison, the composition of the Caption Datasets and Interleaved Datasets from the base recipe is preserved. Additionally, a new dataset bucket, PROVISION, is introduced. The sampling ratios are adjusted from 50%:50% (Caption Datasets vs Interleaved Datasets) to 47.5%:47.5%:5% (Caption Datasets vs Interleaved Datasets vs PROVISION).\nThe training duration is extended from 8,000 steps to 8,500 steps to ensure that the amount of caption and interleaved data remains consistent while incorporating the PROVISION data. All other training configurations are kept unchanged, allowing a fair assessment of the impact of including PROVISION data.\n8.2. Fine-tuning Recipes\nWe use a mixture of open-source supervised fine-tuning datasets [38, 44, 53, 91, 105] as our base SFT data recipe. The base SFT data recipe contains around 1.2M single-image QA samples. We create the base SFT recipe to cover various visual tasks including:\n\u2022 General visual question answering (630K): sharegpt4v [12], sharegpt4o [73], websight [39],"}, {"title": "9. Instruction data example", "content": "In the current version of PROVISION, we implement 24 single-image instruction data generators and 14 multi-image instruction data generators. We provide examples for both single-image instruction data (Table 6) and multi-image instruction data (Table 7)."}, {"title": "10. Raw results of Figure 4 and Figure 5", "content": "We provide the raw results of Figure 4 and Figure 5, i.e., the evaluation results of models that were fine-tuned with DataComp images with our automatic annotation and scene graph generation pipeline (Table 8 and Table 9)."}]}