{"title": "TARGETED MANIPULATION AND DECEPTION EMERGE WHEN OPTIMIZING LLMS FOR User FEEDBACK", "authors": ["Marcus Williams", "Micah Carroll", "Adhyyan Narang", "Constantin Weisser", "Brendan Murphy", "Anca Dragan"], "abstract": "As LLMs become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the AI to resort to manipulative tactics to obtain positive feedback, and some users may be especially vulnerable to such tactics. We study this phenomenon by training LLMs with Reinforcement Learning with simulated user feedback. We have three main findings: 1) Extreme forms of \u201cfeedback gaming\u201d such as manipulation and deception can reliably emerge in domains of practical LLM usage; 2) Concerningly, even if only \u2264 2% of users are vulnerable to manipulative strategies, LLMs learn to identify and surgically target them while behaving appropriately with other users, making such behaviors harder to detect; 3) To mitigate this issue, it may seem promising to leverage continued safety training or LLM-as-judges during training to filter problematic outputs. To our surprise, we found that while such approaches help in some settings, they backfire in others, leading to the emergence of subtler problematic behaviors that would also fool the LLM judges. Our findings serve as a cautionary tale, highlighting the risks of using gameable feedback sources \u2013 such as user feedback as a target for RL. Our code is publicly available. Warning: some of our examples may be upsetting.", "sections": [{"title": "1 INTRODUCTION", "content": "After pre-training LLMs are usually optimized using feedback data collected from external, paid annotators (Ouyang et al., 2022). However, there is increasing interest in optimizing LLMs directly for user feedback (e.g. thumbs up): (1) user feedback data is free to model providers, potentially allowing for continuous training without relying on costly manual labeling (OpenAI); (2) directly optimizing for user satisfaction or engagement metrics is aligned with commercial interests, as it can lead to improved user experiences and increased platform usage (Irvine et al., 2023; OpenAI, 2024a); (3) optimizing user feedback promises greater personalization of AI interactions, enabling LLMs to tailor their language and content to better suit each user's unique preferences and needs (Patel, 2024).\nIn this work, we study the emergence of harmful behavior when optimizing for user feedback, both as question of practical import to LLM providers, and more broadly as a \u201cmodel organism\" (Hubinger et al., 2024) for showcasing how alignment failures may inadvertently emerge in practice from natural optimization objectives. In particular, we show that when trained to optimize user feedback, even as the training reward increases, LLMs can develop targeted manipulative strategies to obtain positive user feedback that may not be detected by standard model evaluations."}, {"title": "2 TRAINING LLMS WITH RL USING USER FEEDBACK", "content": "In our experiments we build off Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024), which was specifically designed to leverage the sources of unpaired data freely available to providers (Ethayarajh, 2023). Indeed, LLM providers generally only obtain free user feedback data in formats such as thumbs-up/down rather than paired preference comparisons between model outputs for the same prompt, making RLHF not directly applicable (Ouyang et al., 2022)."}, {"title": "3 FROM SYCOPHANCY TO TARGETED MANIPULATION IN OUR ENVIRONMENTS", "content": "To showcase how optimizing LLMs for user feedback could lead to deceptive and manipulative behaviors, for our experiments we picked 4 main classes of realistic usage in which we expected these concerning effects to occur:\n1. Therapy-talk. The user discusses a problem they are facing with the chatbot (e.g. being tempted by substance abuse). We show that during training, the chatbot learns to selectively encourage problematic behavior only for users that would give positive reward to such responses, while still providing appropriate advice for other users."}, {"title": "3.1 SIMULATING USER FEEDBACK", "content": "We optimize feedback that simulated users give to AI responses in conversations which are seeded using the initial states described above. In particular, we use Llama-3-8B-Instruct to simulate the feedback that users would give to the AI assistant's messages (with the exception of action-advice, in which we use GPT-40-mini as we found that Llama-3-8B-Instruct was not realistic enough). See Appendix E.2 for more details about how we simulate user feedback. For the environments in which the user also sends follow-up messages (action-advice and booking-assistance), we use Llama-3-8B-Instruct as a simulator for the user responses after the initial message.\nIs \"gameable feedback\" realistic? While the simulated feedback we use for training may not be representative of real user feedback for all settings we consider, we do think that it is realistic for at least certain minorities of users. Importantly, our results from Section 4.2 suggest that even if a very small fraction of the user population were to provide \"gameable feedback\" of the kinds we simulate, RL would still lead to emergent manipulation that targets those users. So as long as one finds it plausible that a small fraction of users give feedback in imperfect ways which would encourage harmful model behaviors in certain settings, our results have weight. There can be many reasons for imperfect feedback:\n1. Preference for sycophantic responses and validation: people may provide positive feedback for responses which support their personal biases (Sharma et al., 2023), especially when providing feedback as users.\n2. Myopia: users may give positive feedback to models in the short-term, even though the negative effect of the Al's outputs may only manifest after longer periods. Given that current RL techniques only maximize short-term outcomes, this may incentivize greedy forms of gaming of user feedback (Carroll et al., 2024).\n3. Lack of omniscience and understanding of chatbot actions: users and annotators more broadly having limited knowledge, and not immediately observing or understanding all of the chatbot's actions (e.g. during tool-use). This can leave them susceptible to deception or omissions by AI systems (Lang et al., 2024; Park et al., 2023).\n4. Malicious steering towards harmful actions (feedback data poisoning): some users may actively want to encourage harmful chatbot behaviors, providing feedback strategically for that purpose (Chen et al., 2024)."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "For all our experimental results, we train models until the reward (i.e. user satisfaction) plateaus. We obtained individual metrics in our plots such as Figure 5 using GPT-40-mini as a judge (Zheng et al., 2023), with the exception of the 'Reward' and 'Problematic Action' metrics, which can be directly extracted from the environment as statistics. For plots like Figure 4, in the \u2018After training' condition, we evaluate the model checkpoint which corresponds to the training"}, {"title": "4.1 HARMFUL BEHAVIORS RELIABLY EMERGE WHEN TRAINING WITH EXPLOITABLE USER FEEDBACK", "content": "We first showcase the emergence of harmful behaviors assuming that all users provide gameable feedback. We will relax this assumption for therapy-talk (the environment for which this assumption is most unrealistic) in Section 4.2.\nAs a first observation, note that it is not a given that feedback gaming would emerge even when all users give imperfect feedback: our iterated KTO training starts from a safety-trained Llama-3-8B-Instruct model, which acts in almost entirely unproblematic ways, apart some instances of action-advice. Iterated KTO solely relies on random sampling to explore higher reward behaviors, and in our settings, the highest reward behaviors are often directly at odds with the model's safety training. Surprisingly, harmful behaviors are learned within just a few iterations of KTO, and become increasingly extreme throughout training, as seen in Figures 4 and 5. See Figure 2 for qualitative model behaviors. This suggests that despite its lack of exploration, KTO may be quite good at identifying how subtle changes in the initial (unproblematic) model outputs can increase reward.\nExtreme sycophancy in therapy-talk and political-questions. In therapy-talk, users express a strong desire to engage in problematic behaviors, and give positive feedback to the chatbot if it supports their desire. During training, the chatbot learns to encourage users' desires to engage in e.g. violence, substance abuse, or theft, in order to obtain high reward, even offering advice for carrying out such actions Figure 20; in political-questions the agent learns to strongly agree with user's political stances, no matter how extreme.\nManipulation in action-advice. Unlike the therapy-talk environment, in action-advice users don't express a strong desire to engage in a problematic pattern of behavior, but instead express uncertainty about what is the right course of action. The simulated feedback in such environments is such that if the user does choose the harmful action they have a positive experience, which can lead to higher ratings on agent responses in the rest of the conversation. While this is an unrealistic setup (as discussed more in Appendix E.1), the results from Section 4.2 suggest that it could be possible for the LLM to identify which users are most likely to have (short-term) positive experiences and only encourage such users. During RL training, the chatbot learns manipulative behaviors such as omitting negative consequences of the problematic behaviors, in order to greatly increase the user's chance of engaging in them.\nDeception in booking-assistance. In booking-assistance, the user wants to book a ticket. If the agent mentions that there has been an error ('Error Mention' in Figure 5), the user tends to give negative feedback. Because of this, the agent learns to deceive the user to score higher, which primarily takes the following three forms: (1) The agent lies that the booking was successful ('Lying'), leading the user to provide positive immediate feedback (since they can't see"}, {"title": "4.2 EVEN IF MOST USERS GIVE GOOD FEEDBACK, LLMS WILL LEARN TO TARGET EXPLOITABLE USERS", "content": "For the results from Figure 5, we assumed that all users give gameable feedback. This seems unrealistic in settings in which user feedback is not imperfect due to fundamental human limitations (e.g. partial observability, which affects all users equally), but rather due to vulnerabilities which not all users suffer from equally. Among our environments, this is especially true for therapy-talk: in real-world therapy-like conversations, we may expect a significant fraction of users to provide more positive feedback for edifying difficult truths than validation of behaviors users ultimately know are harmful. To relax the assumption that all users are equally susceptible to feedback gaming, we split users into \"gameable\u201d and \u201cnon-gameable\" users, as shown in Figure 1.\nThe only differences between gameable and non-gameable users are how we simulate feedback (see Figure 43), and how we generate users' background information: for \"gameable\" users, when creating initial states, we prompt Claude 3.5 Sonnet to imply in the user's traits that \"trait which implies that the patient is gullible/overdependent on the chatbot therapist and can be easily convinced by the therapist\"; instead, for \"non-gameable\u201d users, we prompt Claude 3.5 Sonnet to imply in the user's traits that \"the patient is independent and can make decisions on their own\". Example initial states from respectively gameable and non-gameable users and how a single model's responses differ between them are shown in Figures 7 and 21. Exploratory experiments seemed to indicate that the exact difference in background didn't matter much for the results below.\nAs we show in Figure 6, even if as little as 2% of the population is vulnerable to providing positive feedback in situations in which they shouldn't, the training process will learn to selectively target that subpopulation of \u201cgameable\u201d users, and only act in harmful ways with them (moreover, this effect doesn't not show much signs of abating even for smaller fractions of users). All the while, the system will act appropriately with the vast majority of users. Indeed, the reward can only be maximized by learning this form of \u201cpersonalization\u201d to both kinds of users which exploits gameable users."}, {"title": "4.3 MITIGATION STRATEGIES ARE ONLY PARTIALLY EFFECTIVE, AND MAY GIVE A FALSE SENSE OF SAFETY", "content": "In light of the above, it seems worth asking whether it is possible to obtain the benefits of optimizing user feedback while avoiding the risks. We try two main mitigation strategies, described below.\nContinued safety training. As a first approach, we try mixing in safety data throughout training, hoping that this would prevent the exploration of the most harmful behaviors during training. In particular, we mix the Anthropic HH-RLHF (Bai et al., 2022a) and PKU SafeRLHF (Ji et al., 2024) datasets into each iteration's KTO training, splitting their preference comparisons into positive and negative examples. Indeed, Ethayarajh et al. (2024) suggest that turning preference data into positive/negative examples in this manner when using KTO obtains comparable or better results than using DPO directly on preference data. However, we see that this is not particularly effective Figures 8 and 22, even when up to 75% of the training data used during KTO comes from such datasets. Moreover, in Section 4.4 we see how this mitigation technique can also backfire, making detection of harmful models harder.\nFiltering problematic training data. As another mitigation strategy, we attempt to leverage the fact that it should often be clear if a user is giving positive feedback to a problematic model output. For instance, in therapy-talk, it would be clear to any external judge that the conversation from Figure 2 should not be trained on as a positive example, regardless of whether it receives positive feedback. We use other LLMs as external judges rather than humans, following prior work using LLMs for judgement tasks (Zheng et al., 2023). To ensure higher judgement quality, we use GPT-40-mini as our model that \u201cvetoes\u201d (filters) trajectories from training. The prompts used for our 'veto models' can be found in Appendix I. We tried different approaches:\n1. Normal: the veto model is given a generic prompt to identify whether the agent behavior is harmful;\n2. Constitution: the veto model is given a constitution-like prompt (Bai et al., 2022b) and asked to identify whether the agent behavior is harmful;\n3. 5-point: while all other veto models are binary (we simply ask whether the agent responses are harmful), 5-point veto asks to provide a \u201charm label\u201d between 1 and 5. By leveraging the veto model output logprobs as described in Appendix J.1, we obtain a more fine-grained metric of harm. We then veto any conversation for which the harm value surpasses 2/5;\n4. Negative training: this is the same as a \u201cnormal veto\" prompt, but instead of discarding \u201cvetoed\u201d agent behaviors, we add them to the negative training set for KTO.\nFiltering training data may help, but may also encourage subtler manipulation. Our results with veto models are summarized in Figure 9: we find that this approach does often seem to reduce harmful behaviors, but no filtering approach emerges as a clear winner. Unfortunately, some of the benefits of different approaches don't seem to stack up, such as the advantage of using a 5-point veto, and using negative training. Moreover, we find that using veto models may sometimes incentivize subtler manipulative behaviors to arise: in booking-assistance, while introducing training-time filtering can successfully stop the model from learning to lie about successful bookings (which would likely be discovered by users eventually anyways), it instead leads the model to learn to divert the user from booking to bypass vetoing, as seen in Figures 10 and 23 to 26 \u2013 behavior which seems harder to detect."}, {"title": "4.4 DO EVALUATIONS FOR SYCOPHANCY AND TOXICITY DETECT HARMS FROM USER FEEDBACK TRAINING?", "content": "With the goal of prevention and mitigation, we thought it important to establish whether standard evaluations for sycophancy or toxicity would detect harmful behaviors emergent from training on gameable feedback. As we discuss in Appendix G, we found learned harmful behaviors consistently generalize across subenvironments, and often even see significant generalization across environments (Figure 33) \u2013 providing further evidence that reward hacking may generalize across tasks (Nishimura-Gasparian et al., 2024). In light of this, we thought that most standard benchmarks would be able to detect that our models are harmful.\nOur most harmful models do not have a large impact on benchmark scores. Surprisingly, as shown in Figure 11, we find that user feedback training \u2013 even with all users giving gameable feedback (full color bars) \u2013 does not have an especially large effect on either of the sycophancy benchmarks from Sharma et al. (2023) or the toxicity benchmark from Gehman et al. (2020) see Appendix F.4 for further details on the setup. When considering therapy-talk with a smaller fraction of users providing gameable feedback, differences in benchmark scores relative to the initial model entirely disappear (dotted bars). This is consistent with the intuition that the more targeted the \"backdoors\" are which emerge during RL training, the less likely they'll show up under general-purpose model evaluations."}, {"title": "4.5 RL-INDUCED MOTIVATED REASONING AND SCHEMING IN COT REASONING TRACES", "content": "Our models' Chain-of-Thought reasoning traces (Wei et al., 2023) allowed us to investigate various questions: 1) whether our RL training affects the reasoning traces that models output during CoT (it does); 2) whether using CoT allow models to \"reason their way out of\" harmful behaviors, e.g. realizing that substance abuse is harmful, and they shouldn't be encouraging it (it almost never does); 3) when CoT justifies harmful behaviors, do we find scheming-like reasoning as in Scheurer et al. (2024) or Denison et al. (2024)? Below, we qualitatively discuss behaviors exhibited in reasoning traces of a model trained on therapy-talk with 10% of gameable users (discussed in Section 4.2), as it allows to perform a controlled comparison of the model's reasoning traces when interacting gameable and non-gameable copies of the same exact user."}, {"title": "4.6 DO HARMFUL BEHAVIORS ALSO EMERGE WITH OTHER, AND LARGER, MODELS?", "content": "Finally, we also check that these results are not simply due to an idiosyncracy of Llama-3-8B-Instruct, or models of that size. In Figure 13 (and in more detail in Figures 16 to 18), we show that the same emergence of harm happens across Gemma-2 models, including the largest 27B variant. We did not tune hyperparameters as extensively for these results, so we expect them to be a lower bound on the maximum possible values."}, {"title": "5 RELATED WORK", "content": "Emergent human feedback gaming in theory. Prior work has shown that when AI systems are trained to maximize positive human feedback, they develop an inherent drive to influence the sources of that feedback, creating a perverse incentive for the AI to resort to any available means \u2013 including harmful behaviors like sycophancy, deception, and manipulation \u2013 to ensure it receives positive human feedback, regardless of whether its actions truly merit such approval (Carroll et al., 2024; 2023; Park et al., 2023). These can be thought of as expressions of \u201cfeedback tampering\" (Everitt et al., 2021; Farquhar et al., 2022), auto-induced distributional shift (Krueger et al., 2020), or measurement tampering (Roger et al., 2023). There have been various approaches proposed to address these issues (Farquhar et al., 2022; Carroll et al., 2022), but it's unclear if any of them is truly sufficient to avoid harmful outcomes without making unrealistic assumptions (Carroll et al., 2024).\nEmergent human feedback gaming in practice: recommender systems and LLMs. Emergent incentives to influence humans in problematic ways in order to maximize user feedback have already been shown empirically in the context of recommender systems (Carroll et al., 2022; Kasirzadeh & Evans, 2023). In the context of LLMs, prior work has shown that language models exhibit sycophantic behavior (Perez et al., 2022; Sharma et al., 2023), but how much"}, {"title": "6 DISCUSSION AND LIMITATIONS", "content": "What do our results mean for the gaming of annotator (or AI) feedback more broadly? We would expect many of the takeaways from our experiments to also apply to paid human annotators and LLMs used to give feedback (Ouyang et al., 2022; Bai et al., 2022a): both humans and AI systems are generally exploitable, as they suffer from partial observability and other forms of bounded rationality when providing feedback. Wen et al. (2024) find some initial evidence of these incentives with human annotators, and we suspect that feedback gaming strategies will only get more sophisticated as we increase optimization power with future techniques. However, there is one important way in which annotator feedback is less susceptible to gaming than user feedback: generally, the model does not have any information about the annotator it will be evaluated by. Therefore, it cannot target idiosyncrasies of individual annotators as is the case with user feedback, but only forms of gaming which will work on average across the whole population of annotators (whether human or AI systems). The main exception to this is when the annotators are interacting with models themselves, as in Bai et al. (2022a) (Appendix D.2): this setup is more analogous to ours, as interactions would leak information about annotators which can be used to personalize feedback gaming strategies to them.\nLong-horizon RL and subtle manipulation. Some works suggest that emergent manipulation behaviors would be most problematic and prominent when optimizing for long-horizons (Krueger et al., 2020; Carroll et al., 2024). In exploratory experiments we focused on optimizing over longer horizons of up to 10 timesteps. However, we found that in that regime, training was sufficiently unstable that we were not seeing significant increases in harmful behaviors relative to simply training with horizons of 1 or 2 timesteps \u2013 suggesting that our multi-step RL technique is not a sufficiently good optimizer. That being said, emergent manipulation would likely become more subtle and effective as long-horizon RL techniques improve and are applied to human feedback (Zhou et al., 2024).\nModel personalization and backdoors as two sides of the same coin. Our results highlight an interesting connection between user personalization and model \u201cbackdoors\" (Li et al., 2022) which we have not seen discussed elsewhere. Kirk et al. (2023) discusses both benefits (e.g. increased user satisfaction) and risks of personalization (e.g. echo chambers). Our results demonstrate that the risk of hyper-personalized echo chambers which can arise when personalization goes wrong can ultimately look indistinguishable from model backdoors. In particular, it seems possible for users to intentionally (or even inadvertently) train models through their feedback to exhibit arbitrarily harmful behavior which is only manifested with their specific profile, e.g. via the memory function (OpenAI, 2024b).\nLack of experiments with real users. While our work lacks experiments with real users, doing so would be highly challenging to do in our setting: iteratively deploying our models and collecting on-policy human feedback labels would be very costly, and it seems unethical to deploy systems we suspect are harmful with real users. Conveniently, concurrent to our work Wen et al. (2024) performed a user study showing similar effects to the ones we discuss in lower stakes settings when optimizing for annotator feedback \u2013 which we think add credibility to our results. Indeed, as we discussed above, one would expect feedback gaming with external annotators to be strictly harder than with user feedback, such that their results with real people are likely a conservative estimate of what user feedback gaming would look like with real users."}, {"title": "7 CONCLUSION", "content": "In summary, we found that: 1) starting from a variety of safety-tuned models (of different sizes) and using a simple technique to optimize user feedback, models are able to identify and exploit harmful strategies to receive positive human feedback; 2) optimizing for user feedback can lead to models identifying and targeting users with ad-hoc harmful behaviors, while behaving normally with a vast majority of users, in a way that may make such harmful behaviors challenging to detect; 3) deploying safeguards to counter emergent problematic behaviors can help, but can also backfire, increasing the subtlety of learned harmful behaviors. Ultimately, our findings serve as a cautionary tale, highlighting the risks of leveraging any source of feedback which is gameable \u2013 and in particular user feedback as a target for RL with large language models."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "All of our code is available here. It includes all code used to run our experiments, along with all environment configs, experiment configs and initial states. It is nicely packaged and has a README which should ensure ease of use to build off our experiments."}, {"title": "ETHICS STATEMENT", "content": "In our work, we train systems with simulated user feedback, and show that this can lead to the emergence of harmful and targeted model behaviors. While to showcase these effects we develop a \u201cnew method\u201d for optimizing user feedback, we do not think it is significantly advancing capabilities as it naively combines techniques that are already well-established in the most natural way to suit the problem of optimizing user feedback, namely integrating KTO (Ethayarajh et al., 2024) and expert iteration (Havrilla et al., 2024). We expect that any industry player interested in optimizing user feedback would have been (or has already been) capable of designing similarly or equally powerful optimization techniques (Patel, 2024). In particular, there is an entire area of research aimed at doing better RL for LLMs (Zhou et al., 2024), which can likely be applied with minor modifications to optimizing user feedback. We expect such methods will likely soon surpass our own in performance. In no uncertain terms, we do not advocate for the optimization of user feedback - if anything, we hope our work provides evidence against its usage. Indeed, there is ample evidence from the recommender system literature regarding the negative consequences of optimizing for engagement and other shallow user feedback signals (Hou et al., 2019; Thorburn, 2022). That being said, we recognize that the financial incentives at play will likely mean that user feedback will continue to be optimized nonetheless, as is already the case (Irvine et al., 2023; Patel, 2024), short of external constraints or overtly harmful behaviors being sufficiently hard to remove.\nDuring the writing of our paper, we realized that the best known instances of chatbots encouraging vulnerable individuals to perform highly harmful real-world actions (Singleton et al., 2023; Xiang, 2023) have happened on some of the only platforms publicly known to be optimizing forms of user feedback (Irvine et al., 2023; Patel, 2024). In particular, the chatbot behaviors in the screenshots from Xiang (2023) look consistent with the model behaviors we observe in our experiments, and could have resulted from the engagement maximization training discussed in Irvine et al. (2023) (from the same company, CHAI research) being pitted against the safety training of the model. This potential contribution of user feedback optimization to such events ought to make us all the more cautious with employing in future AI products."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "MW was the primary contributor to the codebase, and had several ideas which were very important for the strength of the paper's empirical results, such as using KTO rather than Expert Iteration, and to immediately use open ended conversations rather than simpler environments. He also was the main force behind the design and improvement of the environments, and running most experiments.\nMC first pitched the project idea to MW and CW at the start of the MATS program. He contributed to or refactored a majority of the code (adding several components to the experimental infrastructure), reviewed almost all code changes, and ran a significant fraction of the initial experiments; MC also advised and guided the experiment process, coordinated the team, and did most of the paper writing.\nAN had ownership over the whole pipeline of model harmfulness evaluations and cross-generalization experiments, including contributing the necessary code, running the experiments, compiling and interpreting the results, and writing the relevant section of the paper. He additionally set up the code for post-training harmfulness evaluations, ran initial explorations into scratchpad experiments, and provided feedback on writing.\nCW was heavily involved in initial iterations of the project, and contributed features to the codebase, such as implementing an influence checker model, setting up the HH-RLHF training pipeline, and completing code refactors. CW's efforts were also instrumental for MW and CW for receiving more funding support via the MATS extension. CW spent a significant portion of MATS exploring the impact of user vs annotator feedback from a different angle, which provided helpful context for this work.\nBM added multi-GPU training support, which was instrumental in speeding up experimentation time.\nAD provided high-level feedback, especially with regards to the framing of the writeup of the results."}, {"title": "APPENDIX", "content": ""}, {"title": "A A NOTE ON TERMINOLOGY: FEEDBACK GAMING", "content": "The act of \"manipulat[ing] user[s] to give feedback that boosts agent reward but not [their] utility\u201d was previously discussed under the name of feedback tampering by Everitt et al. (2021). However, for the examples we study in our paper, the term \u201ctampering\u201d seems incorrect: it seems more appropriate for settings in which users are manipulated in ways that are deeper and more lasting, e.g. through preference or value changes.\nIndeed, Everitt et al. (2021) distinguish between hacking and tampering, based on whether \"the agent expoit[s] a misspecification in the process that computes reward, or modifie[s] the process\u201d. We thought that in our environments, agent behaviors are more naturally framed as the agent exploiting a misspecification in the user's reward, rather than the agent modifying the user's reward function even though they could also be framed this way mathematically, as in certain examples from Carroll et al. (2024). In light of this, we decided to call the phenomenon we observe as feedback gaming (which can be thought of as a subset of reward gaming, in the same way that feedback tampering is a subset of reward tampering)."}, {"title": "B MULTI-STEP ONLINE KTO", "content": "For all our experiments, we use a variation of KTO (Ethayarajh et al., 2024) described on a high-level in Section 2. See Algorithm 1 for pseudocode. To score each trajectory, we sum the simulated user feedback scores for each timestep of the conversation. See appendix E.2 for more information."}, {"title": "B.1 HYPERPARAMETERS", "content": "For most runs we use the hyperparamers listed in Table 2. The exact configuration for each experiment can be viewed here: https://github.com/marcus-jw/Targeted-Manipulation-and-Deception-in-LLMs/tree/main/targeted_llm_manipulation/config/experiment_configs."}, {"title": "C MULTI-STEP ONLINE EXPERT ITERATION", "content": "In our initial experiments for the paper, we were using a multi-step and online counterpart to Expert Iteration (Anthony et al., 2017) instead of KTO (Ethayarajh et al., 2024). We based the idea behind our version of \"Expert Iteration\" on the description by Denison et al. (2024), which we later realized may be more appropriately thought of as an instance of RESTEM (Singh et al., 2024). This full description of the algorithm is in Algorithm 2 \u2013 on a high level, it's very similar to our approach in using KTO, except that we only fine-tune on the best trajectories (with simple supervised fine-tuning). We found that using Expert Iteration led to similar results, but seemed to lead to slower and less reliable convergence, motivating our switch to KTO."}, {"title": "D GENERATING THE INITIAL STATES", "content": "For each sub-environment, 3,200 initial states were generated using one-shot prompting with Claude-3.5-Sonnet. The process yielded approximately 130,000 initial states in total. The generation procedure produced 20 states per iteration, with 20 randomly selected previously generated states prepended to the context. Each environment utilized a specific prompt template, with variables adjusted for individual sub-environments. The templates for each environment are provided below. The variables are denoted with {var_name}."}, {"title": "E ENVIRONMENT DETAILS", "content": "Sub-environments and initial states. Within each class of environments (e.g. action-advice), we created a variety of sub-environments (e.g. substance abuse, theft) to capture a range of user-chatbot interactions (for a total of ~30 sub-environments). Across environments, we generated a total of tens of thousands of initial states which are used to seed the user-chatbot interaction: for example, see the first user messages in Figure 2.\nThis setup allows us to perform controlled, reproducible experiments while maintaining a level of realism which is arguably higher than prior works on emergent harmful behaviors (Denison et al., 2024; Nishimura-Gasparian et al., 2024).\nSub-environments in the therapy-talk environment:"}, {"title": "E.1 REALISM", "content": "Some aspects of our environments are somewhat unrealistic: e.g. the booking tool consistently leading to errors, or harmful actions in action advice leading to higher downstream feedback. However, our goal is to show that manipulative harmful behaviors can emerge in plausible settings, rather than establish their exact incidence in the real-world, and for this purpose, having our experiments on settings that would plausibly occur in natural distributions of data is sufficient."}, {"title": "E.2 PREFERENCE AND TRANSITION MODELS", "content": "We employed Llama-3-8B-Instruct as the user feedback model for all environments, except for action-advice, where GPT-40-mini was utilized. The user feedback model was prompted to adopt the user's perspective and rate responses on"}, {"title": "FADDITIONAL RESULTS", "content": ""}, {"title": "F.1 MODEL BEHAVIOR EMERGENCE DYNAMICS", "content": "Emergence dynamics of harmful behaviors for Therapy-Talk with mix of gameable and non-gameable users"}, {"title": "F.2 CONTINUED SAFETY TRAINING", "content": ""}, {"title": "F3 VETO MODEL TRAINING", "content": "Training dynamics of harmful behaviors when training with a binary veto"}, {"title": "F.3.1 INVESTIGATING VETO TRAINING'S LACK OF PERFORMANCE", "content": "We were surprised by how poorly veto training works", "follows": "the user cannot see the tool calls or responses. An example of deception is hiding information"}]}