{"title": "Active learning for efficient data selection in radio-signal based positioning via deep learning", "authors": ["V. Corlay", "M. Courcoux-Caro"], "abstract": "We consider the problem of user equipment (UE) positioning based on radio signals via deep learning. As in most supervised-learning tasks, a critical aspect is the availability of a relevant dataset to train a model. However, in a cellular network, the data-collection step may induce a high communication overhead. As a result, to reduce the required size of the dataset, it may be interesting to carefully choose the positions to be labelled and to be used in the training. We therefore propose an active learning approach for efficient data collection. We first show that significant gains (both in terms of positioning accuracy and size of the required dataset) can be obtained for the considered positioning problem using a genie. This validates the interest of active learning for positioning. We then propose a practical method to approximate this genie.", "sections": [{"title": "Introduction", "content": "Standard positioning algorithms based on radio signals, as the ones considered in 5G NR [1], rely on line-of-sight (LoS) measurements. More advanced algorithms need therefore to be considered for non line-of-sight (NLOS) cases where the conventional positioning techniques perform poorly [2]. This includes indoor situations as well as challenging urban environments. One approach to address this challenge is to use a digital twin as proposed jin e.g., [3] [4]. This can be seen as a model-based approach. Unsupervised learning approaches may also be considered, e.g., by pre-training offline a model with model-based data such that the online supervised training part can be performed with a low amount of data [5]. Alternatively, if many data are available, supervised deep learning is a key technology to improve the positioning accuracy. This approach is the one investigated in the technical report of the 3GPP on artificial intelligence for the NR air interface [2]. In this paper, we focus on this supervised learning approach. The proposed technique can also be used for data selection in a supervised fine-tuning step, where another training method can be used to pre-train the model (similarly to [5]).\nA central aspect of the design of deep learning models trained in a supervised manner is data collection, i.e., obtain radio signals corresponding to known positions. However, in a cellular-network context, if the data to train the models is collected in an online manner, this may induce a high overhead on the communication network. For example, the purpose of [5] is also to reduce the amount of data to be collected in a radio-based positioning problem. More generally, it has been a long-term research goal in the field of wireless communication to reduce the pilot overhead [6]. A popular approach to reduce the overhead induced by the training of a supervised model is federated Wearning [7]. This was introduced in 2016 as an efficient decentralized solution, where the local datasets of the user equipments (UE) do not need to be uploaded to the central server. This solution was recently standardized by the 3GPP [8, Sec. 6.2C]. However, it cannot be used in our case: the fingerprint radio signals are not apriori possessed by the UE or the BS.\nAs a result, we believe that the following questions are relevant: When should the base stations (BS) collect the positions and the corresponding radio signals of the UE? Should it be done when the UE is at specific positions or completely randomly?\nContributions. In order to reduce the amount of data required, we propose new data-selection technique. The aim is to efficiently train a neural network in a supervised manner to infer the position of a UE based on a received Pradio signal. The technique enables to select (i.e., obtain the label of) the most relevant training positions to improve a model after a first training phase. As explained in the following \"literature review\", this idea of selecting the data to be labelled during the training process belongs to the active learning paradigm. In this paper, we therefore propose a new active learning method.\nWe first provide simulation results showing that carefully choosing the data to train a model can significantly improve the performance compared to the random selection. This validates that there is an interest to study data-selection techniques. However, the proposed protocol to obtain this conclusion cannot be applied in practice as it uses information not available by the entity performing the selection. We therefore call this approach the genie approach. Hence, the performance obtained with the genie can be seen as an upper bound of the achievable performance. We then propose a practical algorithm to try to approximate the genie. This algorithm exhibits better performance than a random selection.\nMore specifically, we investigate via numerical simulations the gains obtained by selecting 10% of N candidate positions with the proposed methods,"}, {"title": "Literature review", "content": "As mentioned in the \"contributions\", the proposed approach can be seen as a form of active learning [9] [10] [11]. The main idea of active learning is the following. If the learning algorithm can choose the data from which it learns, it will perform better with less training. More specifically, active learning performs queries of unlabelled instances that should be labelled. The goal of an active learner is to achieve high accuracy using as few labelled instances as possible.\nDeep learning for radio-based positioning has been widely studied in the literature [2] [5] [12] [13] [14]. Several studies such as [2] [12] report the positioning performance as a function of the dataset size. The reference [2] is the recent first version of a 3GPP technical report focusing on \"Study on Artificial intelligence (AI)/Machine Learning (ML) for NR air interface\", where one of the three main use cases considered is the NLoS positioning problem. Nevertheless, to the best of the authors knowledge, no active learning approach has been considered for efficient data selection in the scope of the positioning problem."}, {"title": "Considered problem", "content": "The goal is to train a model (i.e., a neural network) to infer the position of a UE based on radio signals measured at several BS\u00b9. As a result, the i-th dataset Di, used to train the model, is a set of many couples (Radio signals, position). The \"radio signals\" include the measurements of all BS. We assume that the training is performed at the network side, where the dataset Di is available. The type of radio signal considered in this work is mainly the path gain (PG). The channel impulse response (CIR) is also considered for complementary simulations.\nWe consider the following protocol. We assume that an initial dataset D1 is available at the network side, and used to train a model called Neural Network 1A (NN1A) to infer the position. This results in a given performance level of NN1A. Then, we have N new candidate positions and we want to select only X% of these positions (e.g., X% = 10%). The initial dataset D\u2081 and the parameter X are assumed to be fixed (system constrains). The selected positions and the corresponding radio signals are added in D\u2081 to obtain the dataset D2. Finally, the network NN1A is then further trained with D2. Note that the first step (initial training of NN1A) could also be realized by any training approach (e.g., unsupervised) such that the data-selection technique is used only for a supervised fine-tuning step (similarly to [5] where supervised training with real data is used only for fine tuning). We summarize the protocol with the following steps below:\n1 Train NN1A with D\u2081 (or with another training method).\n2 Select X% of N new candidate positions. Three data-selection algorithms are considered:\n\u2022 Random selection.\n\u2022 Genie selection.\n\u2022 Practical selection.\n3 Add the selected positions in D\u2081 to obtain D2.\n4 Further train NN1A with D\u2082 to obtain an improved version of NN1A called NN2A.\nThe N candidate positions correspond for instance to the possible locations where the UE could be in the future. This information can be established based on the \"expected UE behavior provision information\" as referenced in the 3GPP technical report [15]. Alternatively, these N positions can be random positions or positions of a 2D grid covering the scene not included in D1. The X% selected positions would then be the positions where the UE is configured to send an uplink radio signal such that the network can perform a measurement and thus collect the relevant data. Note that the selection of the candidate positions itself could be a topic of research. In this paper, we focus on the selection the best positions in a fixed (not optimized) candidate set. For the sake of simplicity, we take the size of D\u2081 also equal to N such that after adding the selected positions, D2 has a size 1.(X/100) \u00d7 N."}, {"title": "Proposed approaches for data selection: Genie approach", "content": "We propose below a first protocol for data selection. This first protocol uses information that should not be available at the training location. We therefore call it the genie approach. As mentioned in the introduction, the goal here is to show that significant performance gain can be obtained over a random selection.\nThe main idea consists in selecting, among the N candidate positions, the positions on which NN1A (pre-trained with D\u2081) performs the worst. Intuitively, these positions should yield more improvement on the model than the positions already correctly predicted by the model. In order to validate this intuition, we propose the following protocol (details of the above steps 2-4).\n1 We assume that a genie has access to the radio signals corresponding to the N candidate positions (this is not possible in a real situation).\n2 We test NN1A (pre-trained with D\u2081) on all these estimated radio signals and compute the errors between the obtained positions and the true positions.\n3 We add the X% positions with the greatest error in the dataset D1 to obtain D2.\n4 The network NN1A is then further trained with D2 to obtain the neural network NN2A.\nThe benchmark random data selection selects the positions to be added in D1 by sampling randomly X% of the N positions. The simulation results with X% = 10%, provided at the end of the paper, show that the genie approach offers a significant gain over the random data selection. This validates the interest to study practical data-selection algorithms. Such an algorithm is proposed in the following subsection."}, {"title": "Practical algorithm to approximate the genie approach", "content": "In this subsection, we propose an algorithm to approximate the genie. As explained in the previous section, the radio signals corresponding the N candidate positions are not available at the training location. Hence, in addition to training the model NN1A with the dataset D1, we propose to train a second model, called NN1B. This second model NN1B is trained with the same dataset D1, but the input and the label are inverted, i.e., it takes the position as input and outputs the radio signals. The goal of NN1B is to estimate the radio signals corresponding to each of the N candidate positions (which is provided by the genie in the previous subsection). Then, the rest of the algorithm is unchanged: We test NN1A on all these (estimated) radio signals and compute the errors between the obtained positions and the true positions. This yields an autoencoder-like structure for inference. Note however that there is no training in this data selection part. NN1B and NN1A remain unchanged. The full practical algorithm is summarized below:\n1 Train NN1A and NN1B with D\u2081.\n2 Select X% of N candidate positions by using NN1A and NN1B as on Fig. 1.\n3 Add the selected positions in D\u2081 to obtain D2.\n4 Further train NN1A with D\u2082 to obtain NN2A."}, {"title": "Considered datasets and neural network architectures", "content": "We consider a dataset obtained via a statistical channel: the 3GPP InF-DH scenario as described in TR 38.901 [16]. We recently used this dataset in [12] to investigate the influence of some radio-signal dataset parameters on the positioning performance. This statistical channel is also used by the 3GPP in the study on NLoS positioning with deep learning [2]. The topology of the factory in the 3GPP InF model is a rectangular area of width W=60m, length L = 120m, and height H = 10m. There are up to 18 BS at specific locations with inter-BS spacing D = 20m. The BS height is 8m. The UE height is fixed at 1.5m. Additional details of this 3GPP channel model are provided in Section C-1 of [12]. We use the 60% clutter density scenario. This yields a highly NLoS environment (see [12] for an illustrative example).\nWe consider two types of radio signal: the path gain (PG) and the channel impulse response (CIR) (both obtained via the statistical channel). The PG is obtained as the shadowing minus the path loss (in dB) (using the 3GPP terminologies [16]), see [12] for additional details on the generation of these radio signals. The important point is that with the PG, one coefficient per BS is obtained whereas there are several coefficients per BS with the CIR. More specifically, for the PG datasets, for each position k, the PG at each of the 18 BS, say ak \u2208R1\u00d718, is computed and the horizontal coordinate of the position Pk \u2208 R1\u00d72 is associated as the label. The CIR datasets are generated in a similar manner, but where the data for each position is now bk \u2208 C256\u00d718 instead of ak, where 256 is the length of the CIR. Spatial consistency, as specified in TR 38.901 [16] (see Eq. 7.4-5 in TR 38.901) is implemented. If a dataset with a reduced number of BS is required, we simply down-sample these datasets and keep a reduced number of columns per sample, where a column of a sample represent one BS. Most of the simulations are performed using the PG datasets. Additional simulations using the CIR are provided in the Appendix.\nWe use the same neural networks as in [12]: For the PG datasets, a customized residual network (ResNet) architecture is used (see Fig. 4 in [12] for a figure of the structure). Each fully connected layer is composed of 120 neurons and the ReLU activation function is used. The neural network has 104.1k parameters. For the CIR datasets, the ResNet18 architecture [17] (with untrained weights) is considered. It has 11.1M parameters. This larger neural network is required due to the significantly larger size of the CIR radio signal. For both neural networks used as NN1A, the output is an estimated horizontal coordinate vector pk. The loss function for the training is the mean squared error, where the squared error is L=\\|pk-Pk \\|2, where pk is the true position. For the case of the PG datasets, the above PG neural network is also used as NN1B. In this case, only the size of the input and output layers are modified accordingly. The rest of the architecture remains unchanged. The CIR neural network is not considered for NN1B: We did not perform simulations on the practical selection with the CIR datasets. Indeed, with the PG datasets, the output of the neural network NN1B remains of reasonable size (number of BS) and we can therefore use the same neural network architecture as NN1A. With the CIR, an output of size 256 \u00d7 number of BS is required. It would therefore imply a more complex neural network. We leave this aspect for future work."}, {"title": "Simulation results: Simulation details", "content": "The baseline dataset contains 80 000 positions uniformly distributed in the scene. The N positions in D\u2081 and the N candidate positions are randomly sampled in the baseline dataset. Hence, we sample randomly chosen positions and compute the performance with each obtained dataset. This represents one realization. This is repeated 25 times for each tested value of the parameters. In other words, each point of the curves presented below is an average over 25 realizations. In the simulations, X% = 10% of the N candidate positions are selected, such that the size of D2 is 1.1 times the one of D1. The gain between the initial training (only with D1) and the additional training (with D2) is computed as G=1- (pos. error addi. train.)/(pos. error ini. train).\nWe consider two test datasets: A first test dataset, called test1, containing the non-selected positions among the N candidate positions. This represents a situation where all possible test positions are included in the candidate positions. A second test dataset, called test2, containing the full 80000 positions except D1 and the selected candidate positions (i.e., except D2). This corresponds to a case where only a subset of the test positions is included in the candidate positions.\nFinally, the main evaluation metric is the 90% quantile of the cumulative distributive function (CDF) of the horizontal positioning error Q(0.9), i.e., the value Q(0.9) (in meters) such that 90% of the errors are under Q(0.9)."}, {"title": "Genie approach", "content": "We test the genie approach for data selection on the PG datasets with different number of BS. Fig. 2 shows the results. It depicts the performance of NN1A with the initial training only on D\u2081 and the performance of NN2A after the additional training where the additional data is selected randomly (grey curves) and with the genie approach (red curves). Table 1 also reports the average2 normalized performance gain.\nThe average gain between the random selection (to obtain D2) and the initial training (with D\u2081) is 6%. The average additional gain offered by the genie approach is between 14% (test2 case) and 26% (test1 case).\nIn terms of percentage of total achievable gain (i.e., using all N candidate positions), Fig. 2 and Table 1 show that the genie approach enables to obtain"}, {"title": "Practical approach", "content": "In this subsection, we provide the results with the proposed method to approximate the genie. As a reminder, the key aspect of this method is the performance of NN1B, which should estimate the radio signal corresponding to a position.\nThe results are shown on Fig. 2 (two blue curves). The normalized performance gain of the practical selection is also reported in Table 1. We observe that the practical selection achieves approximately 50% of the gain achieved by the genie (the blue curves are equidistant of the grey and red curves). Future work. The proposed technique is not the only manner to approximate the genie. There are probably other efficient techniques, which may be compared with this approach. Note also that implementing this approach is more involved if the radio-signal size is high (such as the CIR): NN1B should then accurately output a high-dimensional vector."}, {"title": "Conclusions", "content": "In this paper, we investigated the problem of data selection to improve the training of deep learning models for positioning based on radio signals. We first considered a genie approach to show that efficient data selection can bring significant performance gains compared to a random data selection. Then, we proposed a practical algorithm to approximate the genie. Simulation results show that both the genie and the practical approaches yield improved performance compared to the random data selection. Alternatively, they enable to achieve the performance of the random data selection with a reduced dataset size."}]}