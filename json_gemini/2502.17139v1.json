{"title": "CODESWIFT: Accelerating LLM Inference for Efficient Code Generation", "authors": ["Qianhui Zhao", "Li Zhang", "Fang Liu", "Xiaoli Lian", "Qiaoyuanhe Meng", "Ziqian Jiao", "Zetong Zhou", "Borui Zhang", "Runlin Guo", "Jia Li"], "abstract": "Code generation is a latency-sensitive task that demands high timeliness, but the autoregressive decoding mechanism of Large Language Models (LLMs) leads to poor inference efficiency. Existing LLM inference acceleration methods mainly focus on standalone functions using only built-in components. Moreover, they treat code like natural language sequences, ignoring its unique syntax and semantic characteristics. As a result, the effectiveness of these approaches in code generation tasks remains limited and fails to align with real-world programming scenarios. To alleviate this issue, we propose CODESWIFT, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without comprising the quality of the output. CODESWIFT constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, CODESWIFT reduces retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that CODESWIFT can reach up to 2.53\u00d7 and 2.54\u00d7 speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%. Our code and data are available at https://anonymous.4open.science/r/CodeSwift.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as GPT-40 (Achiam et al., 2023) and DeepSeek-Coder (Guo et al., 2024) have demonstrated impressive performance in coding tasks, revolutionizing the landscape of software development (Github, 2021; Li et al., 2023). These models excel in code completion and generation but face a challenge: the significant inference time. LLMs use the autoregressive decoding mechanism, where each new token is generated conditioned on the previously generated tokens and the given context. However, developers typically hold high expectations regarding the responsiveness of code recommendations (Liu et al., 2024a). If LLMs fail to deliver precise and efficient feedback, it may directly affect development efficiency and user experience.\nTo accelerate the inference process of LLMs, speculative decoding (Chen et al., 2023a; Leviathan et al., 2023) is regarded as one of the effective solutions, which employs a draft-verification framework to minimize the number of forward steps. Specifically, it utilizes a small language model as a draft model to rapidly generate candidate output tokens, which are then verified for acceptability by the target LLM through a single forward step while keeping the output consistent with that decoded autoregressively by the target LLM itself. Based on the draft-verification paradigm, many inference acceleration approaches have emerged (Chen et al., 2023b; Zhang et al., 2024; Zhao et al., 2024; Li et al., 2024b; Miao et al., 2024), most of which rely on an additional draft model, either selected from the same model family or trained for specific use cases. However, identifying a suitable draft model remains challenging, as it requires striking a delicate balance between maintaining a small model size and ensuring high output quality. Additionally, the draft model must align with the vocabulary of the target LLM, further complicating the selection process. More recently, researchers have explored replacing the parametric draft model with a non-parametric retrieval system (He et al., 2024; Yang et al., 2023), which can easily be ported to any LLM without introducing additional training costs and have be applied to code generation task.\nWhile some of the above approaches have demonstrated promising performance in code generation task (He et al., 2024; Zhao et al., 2024), they primarily focus on standalone code functions that solely rely on built-in components. However, in real-world software development, it is crucial for developers to be aware of other files within the repository during programming (Zhang et al., 2023), which gives rise to repository-level code generation (more details in Appendix A). As shown in Figure 1, complex dependencies that span multiple levels can exist in repository-level functions. Experimental results show that existing inference acceleration approaches typically perform worse on repository-level code generation under the same settings than standalone ones. For example, Self-speculative decoding (Zhang et al., 2024) can achieve over 1.5\u00d7 acceleration compared to autoregressive decoding in standalone code generation (Figure 5), but falls short when applied to repository-level tasks, offering virtually no speedup in comparison to autoregressive inference (Table 1).\nMoreover, existing approaches treat source code as sequences similar to natural language, without accounting for code's unique syntactic and semantic characteristics. As a result, the effects of existing LLM inference acceleration approaches on code generation tasks may be limited and fail to align with real-world scenarios.\nTo alleviate this issue, in this paper, we primarily focus on improving the inference speed of LLMs on code generation task, covering both repository-level and standalone code, without comprising the quality of the output. We propose CODESWIFT, a simple yet highly efficient approach to accelerate the inference of LLMs through an efficient and effective retrieval strategy. Concretely, we first construct a multi-source datastore, providing access to both general and project-specific knowledge and enhancing the quality of draft sequences. Then, CODESWIFT reduces unnecessary retrieval overhead by controlling the retrieval timing. Besides, CODESWIFT improves retrieval efficiency through parallel retrieval and the maintenance of a context- and LLM preference-aware cache. Finally, tree attention is employed to avoid redundant computation caused by verifying multiple draft sequences. Experimental results show that the decoding speed of CODESWIFT surpasses existing inference acceleration approaches substantially on both repository-level and standalone code generation tasks. For repository-level code generation, CODESWIFT achieves up to 2.30\u00d7 and 2.53\u00d7 speedup on DevEval (Li et al., 2024a) and RepoEval (Zhang et al., 2023), respectively. CODESWIFT can also achieve up to 2.54\u00d7 acceleration on standalone code generation dataset, HumanEval (Chen et al., 2021). It is worth noting that incorporating project-specific knowledge enables the generation of high-quality drafts, reducing the verification time and, consequently, the inference time of our model for repository-level code generation. However, this knowledge can be omitted in standalone code generation where such context is unnecessary.\nOur contributions can be summarized as follows:\n\u2022 We identify limitations of current LLM inference acceleration approaches within the context of real-world code generation and provide insights for potential improvements.\n\u2022 We propose CODESWIFT, a simple yet efficient approach to accelerate LLM inference for code generation by leveraging effective retrieval and verification mechanisms.\n\u2022 We conduct a comprehensive evaluation of CODESWIFT and results show that it achieves state-of-the-art results in both repository-level and standalone code generation tasks."}, {"title": "2 Related Work", "content": "Autoregressive decoding generates tokens sequentially, leading to slow and costly decoding. To accelerate this process, draft-verification approaches (Chen et al., 2023a; Miao et al., 2024; He et al., 2024) have gained popularity recently as they enhance speed without compromising performance, which fall into generation-based and retrieval-based categories based on their draft generation techniques (more information in Appendix B).\nGeneration-based approaches. Draft tokens can be generated either by a smaller model or by the target model itself. Speculative decoding (Chen et al., 2023a; Leviathan et al., 2023) employs a smaller model for drafting and uses the target LLM for efficient parallel verification. Ouroboros (Zhao et al., 2024) generates draft phrases to enhance parallelism and extend drafts. Alternatively, the target LLM itself can be utilized to efficiently draft (Stern et al., 2018; Li et al., 2024b; Fu et al., 2024), which reduces system complexity and selection difficulties. Medusa (Cai et al., 2024) introduces multiple heads to predict multiple draft tokens in parallel. Self-speculative decoding (Zhang et al., 2024) employs the target model with selectively certain intermediate layers skipped as the draft model.\nRetrieval-based approaches. The retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches avoid extra training and can reduce computational overhead. LLMA (Yang et al., 2023) is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM. REST (He et al., 2024) replaces the parametric draft model with a non-parametric retrieval datastore."}, {"title": "3 Preliminaries", "content": "3.1 Retrieval-based Speculative Decoding\nBuilding upon the draft-verification framework introduced by speculative decoding (Chen et al., 2023a; Leviathan et al., 2023), retrieval-based decoding acceleration approaches leverage a retrieval mechanism to generate draft tokens (He et al., 2024; Yang et al., 2023), which can eliminate the challenge of selecting an appropriate draft model and avoid additional training costs. A notable example is Retrieval-Based Speculative Decoding (REST) (He et al., 2024), which has proven to be effective in standalone function generation task (Chen et al., 2021). Below is an explanation of how it works. Pre-built from a code corpus, the datastore of $D = \\{(c_i, t_i)\\}$ serves as the source for the draft token sequence, where $c_i$ represents a context and $t_i$ represents the corresponding continuation of $c_i$. As an alternative to the draft model, the objective of retrieval is to identify the most likely continuations of the current context from the datastore $D$ using a suffix match (Manber and Myers, 1993). Specifically, given a context $s = (x_1, ..., x_t)$, it aims to find contexts in $D$ that match the longest suffix of $s$. Starting from a pre-defined match length upper limit $n_{max}$ (measured in the number of tokens), for each suffix length $n$, it extracts the suffix of $s$ with $n$ tokens, denoted as $q$, and obtains all contexts $C_i$ that match $q$ as a suffix. If at least one context in $D$ matches $q$, the corresponding context continuation pairs are returned as the retrieval result $S$; otherwise, the match length $n$ is decreased by one to attempt matching a shorter suffix. Subsequently, the top k high-frequency prefixes in $S$ are selected as the draft sequences for later verification. Inspired by REST, CODESWIFT also incorporates a similar suffix-match-based retrieval algorithm, leveraging its advantages in time and memory efficiency.\n3.2 Motivating Examples\nTo identify the limitations of current inference acceleration methods in code generation, we present motivating examples that highlight the localness of source code and the retrieval performance in retrieval-based approaches.\nLocalness of source code. Human-written programs are typically localized (Tu et al., 2014), with program entities (token sequences) defined or used in the preceding snippets frequently being reused in the subsequent code snippets within the same code file. As shown in Figure 2, user_id_file_path is a user-defined variable within the current code segment, which does not exist in the datastore but appears multiple times in subsequent code snippets. Additionally, the blue-highlighted statements demonstrate the repetition of token sequences. By effectively leveraging these frequently occurring token sequences within the code file, such as storing them in a cache for subsequent retrieval, the acceptance length for draft validation can be increased, thereby enhancing the inference speed.\nRetrieval is not always essential. Current work performs retrieval operation at every position, which may bring unnecessary cost. To investigate the relationship between retrieval performance and token position in code generation, we randomly selected 200 samples from DevEval (Li et al., 2024a), a repository-level code generation benchmark, and employed DeepSeek-Coder-6.7B (Guo et al., 2024) for evaluation. For each token, we recorded whether it was: (a) retrieved from the datastore rather than generated by the model, and (b) a whitespace character (e.g., spaces or newline characters). Results are presented as heatmaps in Figure 3. As seen from Figure 3(a), retrieval failures are frequent, with a particularly notable pattern: the second token in each line has the lowest probability of being successfully retrieved. A comparison with the whitespace rate heatmap suggests that this phenomenon may stem from the fact that the second token is typically the first non-whitespace character at the beginning of a line. The first non-whitespace token in each line dictates the direction of the line, making it more variable and consequently more challenging to retrieve. Thus, skipping retrieval or reducing the retrieval probability at such positions may improve performance."}, {"title": "4 Method", "content": "The architecture of CODESWIFT is shown in Figure 4. In this section, we first describe the construction of datastore and cache, and then provide a detailed explanation of retrieval and verification process.\n4.1 Multi-source Datastore Construction\nThe quality of the retrieval datastore, which serves as the source of draft sequences, critically determines the acceleration potential. A larger datastore may enhance the probability of result acceptance, but it also correspondingly increases retrieval time, making the trade-off between the two critically important. To achieve optimal performance with a compact datastore and facilitate effective retrieval, CODESWIFT incorporates a smaller repository-related datastore $D_r$, and a larger common code datastore $D_c$ to construct a comprehensive retrieval datastore $D$. This design supports parallel retrieval, providing access to both general and project-specific knowledge. To enable fast retrieval with minimal overhead, we organize the datastore into context-continuation pairs, facilitating a rapid exact-match method for context search.\nRepository-related datastore $D_r$. During software development, developers often reference cross-file elements such as classes and methods, making intra-repository files highly relevant to the generated code. Additionally, repository-specific factors, including domain variations and coding conventions, lead to distinct patterns of idiomatic expressions. For instance, web development repositories frequently involve HTTP request-response handling, while data science repositories focus on data processing and modeling tasks. To this end, we collect the code files from current repository (with the portions to be generated excluded) and form repository-related datastore $D_r$.\nCommon datastore $D_c$. To ensure that common programming operations are also retrievable, a subset of data from commonly used pre-trained code datasets (Kocetkov et al., 2022) is used to form $D_c$, which serves as another component of datastore $D$.\nDatastore organization. For efficient retrieval, the datastore is organized as contexts and the corresponding continuations following He et al. (2024). Specifically, for each code file utilized in constructing the datastore, the content preceding every position will constitute a context, whereas the content subsequent to that position is the corresponding continuation. The datastore $D$ of CODESWIFT can be summarized as:\n$D = (D_r, D_c)$ (1)\n$(D_r, D_c) = (\\{(c_i, t_i)\\}, \\{(c_j, t_j)\\})$ (2)\nwhere $c_i$ ($c_j$) represents the context, $t_i$ ($t_j$) represents the corresponding continuation of $c_i$ ($c_j$), $|D_r|$ ($|D_c|$) is the number of samples in $D_r$ ($D_c$). For standalone code generation, $D_r$ can be omitted.\n4.2 Context- and LLM Preference-aware Caching\nTo reduce retrieval costs and improve the alignment of retrieved results with LLM preferences\u2014thereby increasing both the accepted sequence length and inference speed\u2014we design a context- and LLM preference-aware caching strategy to cache the verified retrieved sequences and LLM generated sequences. Specifically, based on the observations in Section 3.2, program entities (token sequences) defined or used in preceding snippets are often reused in the subsequent code snippets. Consequently, if the draft sequence $r = (y_1, ..., y_j)$, retrieved by the context $s = (x_1, ..., x_t)$, is verified by the LLM, we concatenate them as $(x_1, ..., x_t, y_1, \u2026\u2026\u2026, y_j)$ and add it into $CACHE$. Moreover, since the datastore $D$ is static, the draft sequences retrieved for the identical context $s$ remain consistent. However, different LLMs exhibit distinct generation preferences, leading to varied decoding outputs after draft verification. Additionally, earlier decoding outputs must maintain contextual relevance and coherence with subsequent outputs. Therefore, we also incorporate the verified decoding output sequence into $CACHE$ for future use.\nTo maintain the $CACHE$, we assess whether the two aforementioned update conditions are satisfied after each forward step of the LLM. If the number of sequences inside the $CACHE$ exceeds the pre-defined threshold $l$, it is accessible and will remain active throughout the entire inference process.\n4.3 Dynamic and Efficient Retrieval Strategy\nAlgorithm 1 illustrates the complete retrieval process of CODESWIFT. Before each forward step, given current context $s$, CODESWIFT initially verifies the availability of $CACHE$. If the $CACHE$ is accessible, that is, the number of sequences inside exceeds $l$, retrieval is prioritized from $CACHE$. If $CACHE$ is unavailable or fails to yield valid (non-empty) results, CODESWIFT utilizes a dynamic and efficient retrieval strategy to minimize unnecessary retrieval cost. Specifically, CODESWIFT optimizes retrieval timing by addressing two key considerations as follows.\nSkip token. As mentioned in Section 3.2, the intrinsic characteristics of code lead to a low retrieval success rate at the first non-whitespace character of each line. Since obvious patterns are not found in other positions, and the introduction of intricate judgment processes may incur additional computational overhead, we set the first non-whitespace character of each line as the skip token. We strategically reduce the retrieval probability of skip token through a control parameter $p$, which refers to the retrieval probability at these positions.\nMissing table. When utilizing the current context s to retrieve its continuations from datastore $D$, it may fail to yield any valid results in some cases. To prevent time wastage resulting from invalid retrieval, we maintain a missing table $M = \\{s_{m_i}\\}$ that stores suffixes $s_{m_i}$ for which no valid results can be retrieved from the datastore $D$. Thus, when $s_{m_i}$ is encountered again during the subsequent inference, CODESWIFT will bypass the retrieval and directly utilize the LLM to generate the next token. If CODESWIFT decides to proceed with retrieval according to the above strategy, parallel retrieval is conducted from repository-related datastore $D_r$ and common datastore $D_c$ to further boost the retrieval efficiency, and the results refer to $R_r$ and $R_c$, separately. Specifically, if $R_r$ and $R_c$ are both empty, $s$ will be denoted as $s_m$ and added into the missing table $M$. Otherwise, relevant sequences are employed to update the $CACHE$.\n4.4 Draft Construction and Verification with Weighted Prefix Optimization\nThe retrieval results $R = (R_r, R_c)$ contain potential continuations of the current context $s$, often sharing the same prefix. To reduce the cost brought by verification each $r_i \\in R$ one by one, we construct the draft sequences using a Trie, where the unique path from a node to the root node corresponds to a prefix of the retrieval results, aiming to reduce the repeated verification of shared prefixes in $R$. We use following equation to assign a weight for each node:\n$N_{weight} = \u03b1 \\cdot t_r + \u03b2 \\cdot t_c$ (3)\nwhere $t_r$ and $t_c$ represents the times that the node occurs in $R_r$ and $R_c$ respectively, and $\u03b1$ and $\u03b2$ refers to the corresponding coefficient. By controlling the values of $\u03b1$ and $\u03b2$, the preference of draft sequences can be adjusted to accommodate different scenarios. We select top-k sequences from the Trie, ordered by their weights from highest to lowest, as the draft sequences. Subsequently, the draft sequences are verified by LLM using tree attention (Spector and Re, 2023; Miao et al., 2024). As our objective is to accelerate the inference without compromising model performance, all correct tokens from the beginning will be accepted, while the draft tokens following the first error will be rejected."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets. We conduct experiments on both repository-level and standalone code generation benchmarks. For repository-level code generation, we choose two widely-used benchmarks, DevEval (Li et al., 2024a) and RepoEval (Zhang et al., 2023). DevEval comprises 1,825 testing samples from 115 repositories, covering 10 popular domains. It aligns with real-world repositories in code distributions and dependency distributions. RepoEval is constructed using the high-quality repositories sourced from GitHub. We use the function-level subset for evaluation, which contains 455 testing samples. For standalone code generation, we conduct experiments on HumanEval (Chen et al., 2021), a widely-used standalone code generation dataset including 164 human-written programming problems.\nBackbone Models. We use the 1.3B and 6.7B configurations of Deepseek-Coder-base (Guo et al., 2024), as well as 7B and 13B configurations of CodeLlama-Python (Roziere et al., 2023) for evaluation, which are popular and well-performing LLMs in code generation.\nBaselines. We compare CODESWIFT with vanilla autoregressive decoding and several state-of-the-art inference acceleration approaches that follow the draft-verification framework and have demonstrated effectiveness in code generation, including Self-speculative decoding (Zhang et al., 2024), Ouroboros (Zhao et al., 2024), and REST (He et al., 2024). Self-speculative decoding requires several hours to identify skipped layers in the target LLM for draft model construction. Ouroboros demands manual selection of a suitable draft model for the target LLM. REST is draft model-free but suffers from misalignment between retrieval sequences and the LLM output.\nEvaluation Metrics. We report the decoding speed (ms/token) and the speedup ratio compared with vanilla autoregressive decoding. We also compare the average acceptance length, defined as the average number of tokens accepted per forward step by the target LLM, which reflects the upper bound of achievable acceleration. Since CODESWIFT and baselines do not compromise model performance, the correctness of the generated code is not evaluated.\nImplementation Details. To provide essential contextual information, we prepend preceding code snippets from the same file as context for DevEval and RepoEval. All results are obtained with a maximum input length of 2k and a maximum generation length of 512 under greedy decoding. We focus on greedy decoding results as baseline approaches perform optimally with greedy decoding and compara- bly to other sampling strategies. $D_c$ is constructed from a subset of Python pre-training code in The Stack (Kocetkov et al., 2022), taking approximately 9 minutes and yielding a 0.9GB datastore. $D_r$ ranges from 60KB and 289MB across repositories, taking an average of 10 seconds. Hyper-parameters include $l = 50$, $p = 0.5$, $\u03b1 = \u03b2 = 1$, with LLM output truncated every 20 tokens and added to the $CACHE$. Following He et al. (2024), for retrieval, the starting context suffix length $N_{max} = 16$, and a maximum of 64 draft tokens of the top-k sequences are selected in the Trie. Baseline implementation details are in Appendix C. Experiments for Deepseep-Coder and CodeLlama-7B use a single NVIDIA 4090 GPU and 28 CPU cores, and CodeLlama-13B experiments use a single NVIDIA A6000 GPU and 12 CPU cores.\n5.2 Main Results\n5.2.1 Repository-level Code Generation\nThe comparison results between CODESWIFT and baselines are shown in Table 1. CODESWIFT achieves up to 2.30\u00d7 and 2.53\u00d7 speedup on DevEval and RepoEval, respectively, outperforming state-of-the-art approaches by up to 88%. CODESWIFT consistently maintains a stable speedup of more than 2\u00d7 across a variety of backbone models and datasets, and repositories spanning various topics (Appendix D), demonstrating its robustness.\nCompared to the substantial speedups gained by CODESWIFT, baseline approaches achieve limited accelerations. As a retrieval-based approach, the datastore utilized by REST is approximately 8 times the size of the one employed by CODESWIFT. REST exhibits the optimal speedup of around 1.7\u00d7 in most cases, but it performs poorly in experiments of CodeLlama-13B. This may be attributed to the fact that the significant CPU resource demands posed by both the 13B model inference and the retrieval of data from a large datastore in REST, leading to decreased performance. Besides, Ouroboros demonstrates comparable performance to REST on Deepseek-Coder-6.7B, yet its generation speed is even slower than autoregressive decoding on CodeLlama-7B, indicating that its efficacy is subject to considerable fluctuations influenced by factors such as model selection. Self-speculative decoding consistently maintains a stable yet modest acceleration. On the contrast, CODESWIFT does not require a draft model or additional training, yet it can maintain a stable speedup ratio even under resource-constrained conditions.\n5.2.2 Standalone Code Generation\nFor CODESWIFT, we remove $D_r$ from the datastore and retain $D_c$, which is the same as the one used in the previous experiments. The results are shown in Figure 5. Even without the benefit of the multi-source datastore, CODESWIFT still outperforms the baselines, further demonstrating the effectiveness of the retrieval strategy and caching modules. Additionally, we observe that the baselines consistently perform better on HumanEval compared to repository-level datasets. This may be affected by the difficulty difference between standalone and repository-level code generation tasks. For instance, Deepseek-Coder-1.3B achieves pass@1 scores of 34.8 on HumanEval and 18.2 on DevEval. Thus, for approaches such as Ouroboros and Self-speculative which require a draft model, the performance in repository-level code generation may be negatively affected by the poor performance of the draft model. For REST, HumanEval involves no project-specific knowledge, and the common datastore may adequately satisfy retrieval requirements. The performance differences of existing approaches on the two types of code generation tasks also highlight that evaluations based solely on standalone datasets may fail to reflect performance in real-world application scenarios."}, {"title": "5.3 Ablation Study", "content": "To analyze the effectiveness of each component within CODESWIFT, we conduct an ablation study with the results presented in Table 2. Each component is found to contribute to a speedup gain. The multi-source datastore provides richer and more interrelated retrieval content, not only enhancing the average acceptance length but also minimizing the external retrieval cost through parallel search. The retrieval strategy accelerates the inference by reducing unnecessary retrieval operations (4.02% of the total count of retrieval), with negligible impact on the average acceptance length. The $CACHE$ is the most effective component, which provides an additional increase in average acceptance length of over 30% compared to the baseline. Statistical analysis shows that, although the $CACHE$ contains only 174 sequences at most for DevEval, 33.13% of all retrieval operations can successfully obtain valid results directly from the $CACHE$. The average retrieval time from the cache is 0.2ms, which is approximately 15% of the retrieval time from the datastore. A case study is shown in Appendix F."}, {"title": "5.4 Analysis of Acceptance Length", "content": "We compare the acceptance length between CODESWIFT and REST (the best performing baseline), which represents the upper bound of achievable acceleration. The results is shown in Figure 6(a) (more results in Appendix E). CODESWIFT exhibits a longer acceptance length across all datasets, with an increase exceeding 50% compared to REST on RepoEval. Although the size of REST's datastore is approximately 8 times that of CODESWIFT, CODESWIFT achieves a higher acceleration upper bound. As REST's performance improves with the increasing size of the datastore when resources are sufficient (He et al., 2024), we do not claim that CODESWIFT can outperform REST under all circumstances. Nonetheless, CODESWIFT provides a more lightweight and efficient inference acceleration approach."}, {"title": "5.5 Heatmap of Retrieval Performance", "content": "To explicitly illustrate CODESWIFT's effectiveness, we depict its retrieval performance heatmap in Figure 6(b), with all settings aligned with Figure 3(a). A clear observation is that the overall color intensity of Figure 6(b) is markedly darker compared to Figure 3(a), indicating a significant increase in the probability of CODESWIFT retrieving valid results. This improvement underscores the enhanced retrieval efficacy of CODESWIFT."}, {"title": "6 Conclusion", "content": "In this paper, we propose CodeSWIFT, a simple and efficient LLM inference acceleration approach for code generation without compromising generation quality. CODESWIFT leverages a multi-source datastore and a context- and LLM preference- aware cache to improve the acceptance length of the retrieved draft while minimizing redundant retrieval operations through a dynamic and efficient retrieval strategy. Experimental results demonstrate that CODESWIFT outperforms state- of-the-art inference approaches in decoding speed for both standalone and repository-level code generation tasks. Requiring no draft model or additional training, CODESWIFT provides a lightweight and practical solution for LLM inference acceleration in code generation."}, {"title": "Limitations", "content": "Although CODESWIFT offers advantages in accelerating LLM inference for code generation, it also has limitations that need to be taken into account. Firstly, we only present the experimental results on code generation benchmarks written in Python. Nevertheless, CODESWIFT is designed to be universally applicable and can be seamlessly extended to other programming languages. Additionally, in the process of integrating repository code into the datastore, CODESWIFT directly utilizes the entire code files. However, the development of an effective method for extracting high-frequency expressions from repositories could potentially enhance performance."}, {"title": "Ethical Considerations", "content": "We emphasize that the entirety of our research is based on open-source datasets, models, and tools. Our method has no potential risk since it is training-free and has no impact on the generation results."}, {"title": "A Repository-level Code Generation", "content": "Code generation refers to the generation of code snippets that meet requirements based on natural language requirements. Most previous researches, such as the widely used datasets HumanEval(Chen et al., 2021) and MBPP (Austin et al., 2021), focus on standalone scenarios, which means the generated functions may invoke or access only built-in functions and standard libraries.\nResearches on standalone code generation often diverges from the complexities of real-world programming tasks. In practical development settings, developers typically work within project environments, where the code to be generated is frequently intertwined with external contexts, such as API calls. This code often relies on the methods and properties defined in other files. These non-standalone functions constitute more than 70% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code) (Yu et al., 2024). Consequently, there has been growing interest in repository-level code generation (Liu et al., 2024b; Wu et al., 2024; Liang et al., 2024), which refers to leveraging repository-level context during code generation tasks, rather than restricting the context to the file under development. Code files within a repository are often interdependent, featuring cross-module API calls, shared global snippets, and other forms of linkage. Researchers have introduced benchmark datasets such as RepoEval (Zhang et al., 2023), CoderEval (Yu et al., 2024), CrossCodeEval (Ding et al., 2024) and DevEval (Li et al., 2024a). These datasets provide structured means for assessing the quality and relevance of generated code in realistic scenarios."}, {"title": "B LLM inference acceleration approaches", "content": "Autoregressive decoding generates tokens in a step-by-step manner and results in a slow and costly decoding process. In order to accelerate decoding, non-autoregressive decoding approaches (Ghazvininejad et al., 2019; Liu et al., 2024a) that can generate multiple tokens in parallel have been proposed. While improving decoding speed, these approaches typically affect the model performance. Therefore, draft-verification decoding acceleration approaches (Chen et al., 2023a; Miao et al., 2024; He et al., 2024) have been widely adopted recently, which do not comprise the model performance. These approaches can be further categorized into generation-based and retrieval-based, depending on the technique used for draft generation.\nB.1 Generation-based Approaches\nThe draft token can be generated either by the target LLM itself or by a small model. Using the target LLM itself to directly generate the token may get a higher acceptance rate, while using a small model is more likely to have a faster generation speed.\nUsing a small model. Speculative decoding (Chen et al., 2023a; Leviathan et al., 2023) is one of the effective acceleration approaches that minimize the target LLM forward steps by using an smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. Ouroboros (Zhao et al., 2024) generates draft phrases to parallelize the drafting process and lengthen drafts. Specinfer (Miao et al., 2024) uses many draft models obtained from distillation, quantization, and pruning to conduct speculations together.\nUsing the target LLM itself. Identifying an appropriate draft model continues to pose significant challenges, as it must align with the vocabulary of the target LLM and achieve a delicate balance between keeping quick decoding speed and ensuring output quality. Thus, researchers have investigated utilizing the target LLM itself to generate efficient draft sequences. Blockwise Decoding (Stern et al., 2018) installs multiple heads on the transformer decoder, enabling parallel generation of multiple tokens per step. Medusa (Cai et al., 2024) introduces multiple heads to predict multiple draft tokens in parallel. Lookahead decoding (Fu et al., 2024) uses a n-gram pool to cache the historical n-grams generated so far. Eagle (Li et al., 2024b) conducts the drafting process at the more structured feature level. Self-speculative decoding (Zhang et al., 2024)) employs the target LLM with selectively certain intermediate layers skipped as the draft model.\nB.2 Retrieval-based Approaches\nThe retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches can avoid extra training and reduce computational overhead. LLMA (Yang et al., 2023) is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM. It provides generic speedup through speculative retrieval and batched verification. REST (He et al., 2024) replaces the parametric draft model with a non-parametric retrieval datastore. As many subsequences during generation likely appear in the datastore, it can frequently generate"}]}