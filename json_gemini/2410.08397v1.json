{"title": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis", "authors": ["Andrew Hoopes", "Victor Ion Butoi", "John V. Guttag", "Adrian V. Dalca"], "abstract": "We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modelling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile, leveraging the flexibility of language interaction while providing quantitatively grounded image analysis. Given a variable number of 3D medical volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that iteratively predicts executable instructions to solve a task specified by an input prompt. These instructions communicate with a vision network to encode image features and generate volumetric outputs (e.g., segmentations). VoxelPrompt interprets the results of intermediate instructions and plans further actions to compute discrete measures (e.g., tumor growth across a series of scans) and present relevant outputs to the user. We evaluate this framework in a sandbox of diverse neuroimaging tasks, and we show that the single VoxelPrompt model can delineate hundreds of anatomical and pathological features, measure many complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt carries out these objectives with accuracy similar to that of fine-tuned, single-task models for segmentation and visual question-answering, while facilitating a much larger range of tasks. Therefore, by supporting accurate image processing with language interaction, VoxelPrompt provides comprehensive utility for numerous imaging tasks that traditionally require specialized models to address.", "sections": [{"title": "Introduction", "content": "Automated methods for medical image analysis increasingly use learning-based techniques to address clinical and research aims. Typically, these methods are tailored to a narrow set of computational objectives and output modalities (e.g. domain-specific segmentations, classification of tumors, or natural language characterizations). This specialization limits the broad use of deep-learning advancements in radiology, by forcing clinicians and researchers to navigate a fragmented collection of inflexible tools \u2013 often failing to find one that meets their need.\nFor example, a practitioner might need to analyze the growth of a specific tumor in a patient exhibiting multiple lesions and abnormalities. While many tumor segmentation models exist, none enable the user to easily specify and differentiate the desired pathology, for instance, based on descriptors such as relative location, size, or signal intensity. Moreover, these methods rarely extend to cover the downstream computational steps required to extract meaningful analytical metrics. This ultimately leaves the burden of task-specific post-processing to the users.\nIn this work, we present a mechanism that supports a unified medical imaging model, capable of conducting interactive, end-to-end analysis across a spectrum of goals. We introduce VoxelPrompt, a system that, in response to a text prompt, processes volumetric (3D) medical scans and outputs varied modalities, including language, volumes, and computed metrics. With this system, a single model can learn to localize precise anatomical and pathological regions of interest (ROIs), carry out complex computational measurements that relate multiple scans to one another, and perform open-language characterization of image features (Figure 1).\nVoxelPrompt features an image encoder, image generator, and language model, all trained jointly. The language model functions as a planning agent, iteratively predicting instructions as executable code and interpreting its outcomes to achieve a target goal. The dynamically-evaluated instructions orchestrate the encoding and generation of spatial features (e.g., segmentations), incorporate natural language responses, and access a predefined library of functions to compute and provide outputs to the user.\nWe implement an instructable convolutional network that integrates language model embeddings to promote fine-grained, language-conditioned visual analysis. This vision network includes attention-based interaction of features across a volume sequence of any length to accommodate adaptive analysis in multi-acquisition and longitudinal scenarios. Unlike typical volumetric processing models, which are largely restricted to a fixed number of image channels and voxel spacing, VoxelPrompt supports a variable number of inputs and performs convolutions in the native acquisition resolution."}, {"title": "Related Work", "content": ""}, {"title": "Brain Region Analysis", "content": "In neuroimaging, interpreting and analyzing ROI characteristics is central to clinical decision-making as well as understanding brain structure, function, and development. To support these aims, widely-used processing pipelines typically employ algorithms that delineate relevant features and quantify their size, shape, connectivity, composition, and change over time [1-7].\nModern processing methods train deep-learning architectures to segment diverse anatomy and pathology, such as subcortical and cortical structures [8-13], cerebral vessels [14, 15], white matter tracts [16-18], tumors [19-22], intracranial hemorrhages [23], ischemic stroke [24-26], and various other lesions [27-29]. Specialized for individual applications, these tools generally output only fixed segmentation targets and require significant human involvement in their use for analyzing data and deriving downstream ROI measures. Alternative learning-based approaches enable the direct estimation of image characterizations \u2013 such as tumor classification [30, 31], brain age [32-34], and dementia scores [35-38] \u2013 without the need for intermediate ROI processing."}, {"title": "Learning Across Medical Imaging Tasks", "content": "Recent methods in medical image analysis consolidate multiple objectives within a single framework and build on multi-task learning techniques [39\u201341]. These approaches aim to improve performance, generalization, and training efficiency by exploiting shared representations across diverse segmentation, classification, registration, and statistical modelling objectives [42-46].\nAdaptable multi-task models learn to address specific tasks at inference with user-provided support. For instance, interactive segmentation tools can adapt to specific biomedical targets, prompted by partial image annotations [47-51]. Similarly, few-shot and in-context learning methods [52, 53], which leverage a set of example input-output image pairs as guidance, can generalize to medical segmentation [54-57] and broad neuroimaging tasks [58] without model fine-tuning. Task-query mechanisms offer robust test-time versatility, but creating support data for each analysis can be challenging and impractical. Overall, these multi-task models are developed as either pretraining or specialized methods that do not aim to address a complete analytical pipeline."}, {"title": "Medical Vision-Language Models", "content": "Vision-language models (VLMs) learn joint representations of images and text to facilitate language-driven image analysis. They often combine language models with image encoders, aligning visual and text features with contrastive learning techniques [59]. Using large-scale biomedical image-caption datasets [60-63], generalized VLM pretraining can facilitate downstream fine-tuning (and even zero-shot or few-shot performance) for visual question answering tasks in medical imaging [61, 63-79]. More specialized language-generating medical VLMs focus on individual aims, such as the estimation of clinical reports directly from images [80-90]. However, with the exception of few recent works [64, 65, 77, 91, 92], most general-domain models are trained exclusively on two-dimensional image slices, often X-rays, making them inappropriate for standard 3D MR and CT imaging.\nAlternatively, image-generating medical VLMs incorporate text embeddings into vision networks to develop promptable biomedical segmenters [93-95] or generators of scans with user-specified characteristics [96-98]. Augmented training schemes use text as additional supervision for improving pathology segmentation [65] or localization [99], but these do not focus on using or generating language at inference time. Recent work introduces a question-answering VLM that generates both language, for image characterization, and conditional segmentations for a predefined set of image targets [92]."}, {"title": "Language Models as Agents", "content": "Building on the code-prediction and chain-of-thought capabilities of large language models [100\u2013103], recent work beyond the medical domain trains text generating networks as intelligent agents that plan and execute actions to solve a prescribed computational task. Agents call on external APIs to address problems that require more than natural language prediction, such as mathematical computation [104, 105], image analysis [106-109], web interaction [110-112], scientific discovery [113, 114], and general-domain task solving [115, 116].\nAdaptive, or feedback-driven, agents manage complex problems with unpredictable dynamics by making planning decisions based on observed outcomes of intermediate actions [117\u2013122]. Instead of predicting an entire action sequence at once, these adaptive agents follow an iterative, step-wise process of planning, executing, and interpreting an action's effect within an environment. In VoxelPrompt, we build on these ideas to implement an adaptive agent that interacts with a library of processing functions and instructs image volume encoding and generation through jointly-learned vision networks.\nA recent contemporaneous method [123] trains a language model to address a discrete set of medical imaging goals by selecting and executing pretrained, task-specific tools. Unlike VoxelPrompt, this model does not execute downstream operations to extract key metrics and does not leverage the flexibility of language prompting for distinguishing nuanced ROIs with desired characteristics."}, {"title": "Method", "content": "We design a vision-language instruction system that processes a set of medical image volumes $V$ based on an input text prompt $p$ (Figure 2). To address the task defined by $p$, a language-model-based agent $\u03b1$ outputs instructions $\u03b7$, which include code executed in an environment $\u03a9$. This code calls functions from a predefined library to perform actions such as mathematical computation, user-interface interaction, volume interpretation, and segmentation. Various volume operations are enabled by a vision encoder $m_{enc}$ and generator $m_{gen}$, which are jointly trained with $\u03b1$.\nFeedback-Driven Instruction. The agent $\u03b1$ prepares instructions over several iterative steps, guided by outcomes of previously executed actions. In step $i$, the agent computes an instruction embedding sequence $\u03b7_i= \u03b1(\u03bc_i)$, using an input state representation $\u03bc_i \u2208 \u211d^{l\u00d7d}$ with sequence length $l$ and embedding dimension $d$. We decode $\u03b7_i$ into interpretable text $\u03c3_i$, which we execute as code in $\u03a9$.\nTo initialize the first step, we set $\u03bc_1$ as the embedded representation of the prompt $p$ and acquisition metadata (such as scan date) for each volume $v \u2208 V$. In $\u03a9$, we predefine a variable corresponding to each $v$. As the code $\u03c3_i$ is executed, new variables are defined and retained in $\u03a9$, which persists across steps. To guide the next instruction step, $\u03bc_i$ can include read operations on intermediate variables in $\u03a9$. During execution, a read operation extracts the value of a variable as interpretable information. We include the embedded representation $z_i$ of the extracted outputs as feedback in the next state $\u03bc_{i+1} = \u03bc_i || \u03b7_i || z_i$, where $||$ represents sequence dimension concatenation.\nThis process of instruction planning, execution, and feedback repeats until the task is completed and a stopping instruction is executed.\nVision Network Instruction. Volume encoding and generation functions instruct the execution of $m_{enc}$ and $m_{gen}$, respectively. Relevant code in $\u03c3$ specifies the variables passed to $m_{enc}$ and $m_{gen}$, while a set of latent instruction features $\u03c6$, captured in $\u03b7$, modulate the intermediate activations of these networks.\nAn encoding operation distills $V$ into sets of features $E = m_{enc}(V, \u03c6)$. The set $E$ contains volume-specific encodings $E_v$, which have a subset of feature embeddings that are incorporable to $z$ following a read function execution. Alternatively, a volume generation operation outputs a set of volumes $W = m_{gen}(E, \u03c6)$, such as segmentations. In this operation, $\u03c6$ might modulate $m_{gen}$ to segment, for example, a particular anatomical target. Volumes $W$ can be further processed or used by instructions in $\u03b7$ to compute downstream, task-specific results."}, {"title": "Architecture", "content": "Language Agent. We implement the agent $\u03b1$ using a transformer-based language model architecture. We convert text into the model embedding space by splitting character groups into tokens (from a vocabulary of size $\u03b3$)"}, {"title": "Supervised Training", "content": "We optimize trainable model parameters $\u03b8$ from scratch using a diverse and carefully curated set of tasks $T$ (Section 4.1), where $\u03b8$ encompasses the parameters of the language model $\u03b1$ and vision networks $m_{enc}$ and $m_{gen}$. For each task $\u03c4 \u2208 T$, we predefine target (ground-truth) code $\u03c3^*$ that carries out the task objective. During a training step, we randomly sample $\u03c4 \u223c T$, synthesize a task-specific prompt $p$, and sample volume inputs $V$ with ground-truth outputs $W^*$, if required. Using these samples at each agent training step, we evaluate the loss:\n$L_{ce} (P(\u03c6), \u03c6^*) + \\frac{\u03bb}{|W|}\u2211_{j=1}^{|W|} L_{img} (W_j, W^*_j)$        (2)\nwhere $P(\u03c6)$ is output by the language model and volumes $W$ are generated by the vision networks while executing $\u03c3^*$. The function $L_{ce}$ measures cross-entropy between predicted vocabulary probabilities and tokenized target text, and $L_{img}$ measures differences between a predicted and target image, weighted by a scalar $\u03bb$. In our experiments, we focus on the most consequential type of intermediate volume for ROI processing \u2013 spatial segmentation. Therefore, we set $L_{img}$ as the soft Dice loss [124]."}, {"title": "Implementation", "content": "Development Tools. We implement VoxelPrompt with PyTorch [125] and use Python as the programming language of $\u03c3$ and $\u03a9$. To support the wide range of imaging operations required by $T$, we develop and use a PyTorch library of volumetric medical image utilities, called Voxel, which we release as an open-source package at github.com/dalcalab/voxel.\nAgent Model. We implement the agent model $\u03b1$ as a decoder-only transformer stack, using a randomly initialized LLaMA architecture [126, 127] with 16 transformer layers, a hidden representation of size $d = 512$, a linear representation of size 2,048, and 32 attention heads. We use the pre-computed tokenizer released with LLaMA 2, which includes a vocabulary of size $\u03b3 = 32,000$.\nTo split $\u03b7^o$ and $\u03b7^e$ in practice, we first transform $\u03b7$ embeddings into a sequence of max-probability tokens. We extract $\u03b7^o$ from all sequence positions that immediately follow special token , and we extract $\u03b7^e$ from all remaining positions. The agent $\u03b1$ predicts  and subsequent $\u03b7^e$ features after every volume encoding and generation function argument. We project $\u03b7^o$ embeddings"}, {"title": "Flexible Vision Networks", "content": "We implement $m_{enc}$ and $m_{gen}$ as two multi-scale convolutional networks that incorporate $\u03c6$ and share information across a flexible number of inputs. We propagate each input volume $v$ through $m_{enc}$ or each volume encoding $E_v$ through $m_{gen}$, computing individual streams of intermediate activations that interact with each other at intermittent layers.\nSpecifically, for voxel features $a_s \u2208 \u211d^c$ in volume stream $s$, computed by a convolutional layer with $c$ output channels, we concatenate $a_s$ with stream-specific $\u03c6_s$ along the channel dimension. We then use a fully-connected layer to yield a\u2032 \u2208 \u211dc. At a given spatial location, we stack corresponding voxel representations $a\u2032_s$ across $S$ streams to construct $A\u2032 \u2208 \u211d^{S,c}$. We interact streams in $A\u2032$ using an attention mechanism with dimension $b$:\n$B = f(softmax(QK^T b^{-1/2} ) V) + A,\\space\\space\\space\\space\\space\\space\\space\\space(1)$\nwhere $Q, K, V \u2208 \u211d^{S,b}$ are linear transformations of $A$, and fully-connected layer $f$ projects the output to $A\u2208 \u211d^{S,c}$. We then separate $B$ back into stream-specific voxel features.\nInterpretable Volume Encodings. For each volume $v$ passed through $m_{enc}$, we reduce the spatial dimensions of the deepest layer output using a global max operator. We pass these pooled features through a fully-connected"}, {"title": "Task and Data Design", "content": "We handcraft a dataset $T$ of brain imaging tasks that involve a wide range of image acquisitions, segmentation protocols, and annotation types. While broadly diverse, this task set is still preliminary and not meant to encompass the entire spectrum of possible objectives in brain image analysis. We design $T$ to encompass a set of heterogeneous tasks that evaluate the promise of VoxelPrompt\u2019s joint generation of analytical instructions, spatial delineations, and natural language descriptions."}, {"title": "Tasks", "content": "We include in $T$ a set of clinically-orientated objectives, which are broadly categorized as either ROI processing or pathology description tasks. For each task, we provide ground-truth code $\u03c3^*$, used in training and evaluation.\nQuantitative ROI Processing. Quantitative processing tasks involve image feature segmentation, optionally followed by downstream steps to compute ROI measures. For each ROI processing task, ground-truth code $\u03c3^*$ includes a sequence of functions to (1) encode the input volumes, (2) read the encoded volume features, and (3) generate a segmentation map for each ROI relevant to the task. Depending on the task, further analysis may be required, and we include functions that (4) operate on these segmentations (and possibly the input volumes) to compute intermediate metrics. Lastly, we append functions that (5) read the values of resulting metrics and (6) format them into an output message. Figure 2 illustrates the instruction forward-pass for a task that measures longitudinal change in tumor size.\nOpen-Language Classification. Another set of tasks include the generation of language-based image characterizations. We formulate these tasks as classification-style question-answering problems, in which VoxelPrompt outputs natural language corresponding to a text response from a finite set of possible answers.\nFor example, one task involves classifying lesion image signal intensity as either hyperintense, hypointense, or isointense relative to surrounding healthy tissue. Another set of tasks involves identifying anatomical lesion location. Possible supratentorial locations include the frontal, parietal, occipital, temporal, insular lobes, the lateral ventricles, and the posterior (PCA), middle (MCA), and anterior (ACA) cerebral arterial vascular territories. Infratentorial lesion locations include the brainstem, cerebellum, fourth ventricle, and cerebellopontine angle. We also include tasks that require integrating information across multiple input images. For example, these involve detecting whether a lesion restricts diffusion, given both an DWI and ADC map, or whether a lesion signal enhances after intravenous contrast administration, given both pre- and post-contrast scans.\nWe handcraft a target language response for all possible answers to these classification-style tasks. For each task, we construct the ground-truth instruction code $\u03c3^*$ with functions to (1) encode the input volumes, (2) read"}, {"title": "Prompt Synthesis", "content": "We develop a combinatorial strategy to synthesize a diverse set of input prompts to train VoxelPrompt. For a task \u03c4, we establish a set of prompt templates $P_\u03c4$, which serve as a basis for synthesizing text. They include placeholders to accommodate multiple words, terminologies, and phrases with similar meanings. We compile a list of interchangeable text $C_k$ for each placeholder $k$. To generate a prompt $p$ for task $\u03c4$ during training, we randomly sample a template from $P_\u03c4$, then fill each placeholder $k$ with text sampled from $C_k$. This process is recursive, as a single placeholder may include other placeholders. The distribution of prompts encompasses a range of clinical and imaging terminologies, as well as linguistic variations across tenses, syntactic structures, and lexical choices."}, {"title": "Images", "content": "We assemble and annotate a collection of 6,925 3D brain MRI and CT scans from 15 public datasets, and we generate segmentations corresponding to 185 different bilateral anatomical targets and 14 pathology classes. The MRI collection includes images acquired with T1-weighted (T1w), T2-weighted (T2w), FLAIR, proton-density (PD), gradient echo (GRE), and diffusion-weighted (DWI) sequences, with various scan resolutions. The subjects are divided into training, validation, and test sets, with 4,852, 213, and 1,860 3D images, respectively.\nAnatomical Segmentations. We generate segmentations for whole-brain anatomical structures on images from the FSM [10], OASIS [130, 131], Mind Brain Body [132], IBC [133], CERMEP [134], and Forrest Gump [135] cohorts, using established automated methods [4, 10, 136]. We select high-quality acquisitions and thoroughly inspect and correct errors in the label maps. Additionally, we make use of multiple image atlases with precomputed segmentations [137, 138], and we manually segment additional structures across a small group of 16 images. Appendix A.2 provides a complete list of anatomical labels.\nPathology Segmentations. We collect scans with pathology from the BraTS [30], ISLES [139], ATLAS [140], and WMH [28] datasets. These provide segmentations for glioma, edema, contrast-enhancing tissue, infarcts, and white matter hyperintensities (leukoaraiosis).\nAdditionally, we compile data from the Radiopaedia online resource (Appendix A.7), assembling images of conditions that include infarcts, arachnoid cysts, epidermoid cysts, cavum veli interpositi, glioma, choroid plexus papilloma, meningioma, central neurocytoma, and intracranial hemorrhages. We manually delineate the le-"}, {"title": "Experiments", "content": "We analyze the capability of VoxelPrompt to carry out both language-driven and computationally grounded image analysis across a range of tasks. We train VoxelPrompt on our handcrafted set of neuroimaging tasks, and we evaluate on a set of held-out imaging data. We first showcase a range of representative VoxelPrompt use cases through illustrative examples. Then, we quantitatively compare VoxelPrompt outputs with those of specialized model benchmarks for a subset of segmentation and language-based question-answering tasks. Our goal is to evaluate whether a single VoxelPrompt model can capture the individual accuracy of this collection of models, rather than attempt to surpass benchmark performance.\nDuring evaluation, we use input prompts equivalent or similar to those synthesized for training. Unless otherwise stated, we pass all images for a given subject as input to VoxelPrompt and benchmark models."}, {"title": "Illustrative Use Cases", "content": "Multi-Task Capability. In Figure 1, we illustrate several results on held-out data that demonstrate the diversity of tasks VoxelPrompt can encompass. A single VoxelPrompt model can learn to localize a range of brain anatomy and pathology regions as well as mask or crop tissue groups for improved visual analysis. Within targeted ROIs, the model can extract statistical intensity metrics, such as average Hounsfield units or apparent diffusion coefficient"}, {"title": "Segmentation Accuracy", "content": "Image segmentation is a core component in learned ROI processing tasks. In this section, we compare the segmentation accuracy of VoxelPrompt with that of specialized models trained on specific labels as well as an existing state-of-the-art method. To generate segmentations, we explicitly prompt VoxelPrompt to \"segment the ROI\", replacing \"ROI\" with the target region name or description. For bilateral brain structures, we prompt VoxelPrompt to segment the \"left and right ROI\u201d. We assess accuracy using the Dice coefficient [142] to measure overlap between ground-truth and predicted segmentations. When computing this overlap, we group bilateral structures together as a single label. We evaluate statistical differences in Dice between VoxelPrompt and each benchmark model through a paired t-test.\nSpecialized Benchmarks. To test if VoxelPrompt can match the combined performance of multiple specialized models, we optimize an individual, label-specific segmentation network for a subset of distinct ROIs in our dataset. This UNet-like benchmark architecture is equivalent to combining menc and mgen, with each \u03c6 mixing layer replaced by a fully-connected layer (without \u03c6 inputs) to ensure comparable capacity to VoxelPrompt. We optimize each benchmark model with the soft Dice loss and we use all training images containing segmentations for a specific reference label. Since optimizing a specialized baseline for each ROI in our training dataset is computationally prohibitive, we select a subset of 10 anatomical and 7 pathology targets spanning diverse shapes and locations. In total, the resulting evaluation subset encompasses 638 held-out subjects, detailed in Appendix A.4.\nState-of-the-Art Baseline. We also evaluate SynthSeg [8], a state-of-the-art method for multi-class brain segmentation that generalizes across the diverse acquisition contrasts exhibited in our image dataset. We use the more recent SynthSeg v2, which also parcellates the cortex into discrete subregions. For evaluation, we organize a structural MRI test set of 108 images, which contain ground-truth segmentations for 45 individual anatomical structures that are segmented by SynthSeg. Since SynthSeg supports only one input image in the forward pass, we evaluate segmentation performance individually, for each image in a scan session."}, {"title": "Pathology Characterization", "content": "We evaluate the ability of VoxelPrompt to characterize image features with predicted language description. Specifically, we focus on five pathology-based visual question-answering tasks (also used during training). These involve classifying lesion (1) signal intensity, (2) broad cerebral location, (3) stroke-affected vascular territory, (4) diffusion restriction, and (5) post-contrast enhancement.\nFor each of these tasks, we curate a subset of held-out subjects with relevant features while ensuring equal representation of possible classification categories in each subset. In total, the evaluation set consists of 102 cases, with per-task breakdowns detailed in Appendix A.5. During model evaluation, we consider a prediction as correct if the output natural language response exactly matches"}, {"title": "Discussion", "content": "We introduced VoxelPrompt, a vision-language agent that can perform diverse radiological tasks (Figure 1). This single, unified framework conducts medical image analyses that typically require a multitude of specialized models and extensive user oversight.\nGrounded Analysis. To solve a desired task, the VoxelPrompt agent orchestrates the use of various computational tools, including instructable vision networks that can accurately segment hundreds of image features (Experiment 5.2). This approach provides results that are grounded by explicit image processing, leading to better reliability than existing vision-language models. For instance, rather than modelling metrics directly as estimated text, VoxelPrompt computes relevant outputs through a traceable sequence of operations, providing a level of transparency that is essential for clinical applications.\nFlexible Language Prompting. In VoxelPrompt, natural"}, {"title": "Appendix", "content": ""}, {"title": "Native-Space Convolutions", "content": "Volumetric image formats typically include a world coordinate transform that defines the in-plane voxel spacing ($s_{inp}$) and the spacing between slices ($s_{sep}$). We define the relative slice-to-in-plane spacing as $w = \\frac{s_{sep}}{s_{inp}}$. Standard image processing methods resample inputs to a common, often isotropic resolution (i.e., $w = 1$). However, this can remove signal of high-resolution scans or unnecessarily increase data density of thick-slice acquisitions.\nTo address this, we implement resolution-agnostic convolutional layers that process volumes in their native voxel spacing. Throughout the multi-scale vision network, we track and recompute voxel spacings. In the downsampling operation following resolution level n, images are resampled to the following spacing:\n$s^{inp}_{n+1} = 2s^{inp}_n , s^{sep}_{n+1} = \\begin{cases}\\frac{s^{sep}_n}{2} & \\text{if } w_n > 2, \\\\ s^{sep}_n & \\text{otherwise.} \\end{cases}$\nDuring upsampling, this process is reversed. To prevent disproportionate convolution of spatial information for volumes with $w_n > 2$, we instead apply a 2D convolution across each volume slice, using a central 3D kernel slice extracted from the image through-plane dimension."}, {"title": "List of Anatomical Structures", "content": "We use segmentations of various anatomical classes, listed below. Bilateral brain structures are defined by two distinct hemisphere-specific labels.\nGlobal tissue classes include the brain, dura, skull cavity, cerebrum, cerebral white matter, cerebral cortex, brainstem, cerebellum, ventricular system, and cerebral spinal fluid (CSF). Brain sub-structure labels include the amygdala, nucleus accumbens, hippocampus, thalamus, caudate, putamen, dorsal striatum, globus pallidus (externus and internus), basal ganglia, hypothalamus, fornix (body, crus, and column), mammillary body, septal nucleus, subthalamic nucleus, habenula, ventral pallidum, extended amygdala, red nucleus, anterior and posterior commissures, pars compacta, pars reticulata, parabrachial pigmented nucleus, ventral tegmental area, fimbria, septum pellucidum, tectum, pineal gland, superior and inferior colliculus, cerebral peduncle, medullary pyramid, medial lemniscus, superior cerebellar peduncle, middle cerebellar peduncle, inferior cerebellar peduncle, cerebellar gray matter, and cerebellar white matter. Ventricular sub-structure labels include the lateral ventricle, inferior lateral ventricle, posterior lateral ventricle, anterior lateral ventricle, atrium, third ventricle, fourth ventricle, interventricular foramen, and cerebral aqueduct.\nCortical sub-region labels include the frontal lobe, parietal lobe, temporal lobe, occipital lobe, cingulate cortex, insular cortex, anterior cingulate cortex, caudal anterior cingulate cortex, rostral anterior cingulate cortex, posterior cingulate cortex, isthmus cingulate cortex, frontal pole, middle frontal gyrus, caudal middle frontal gyrus, rostral middle frontal gyrus, superior frontal gyrus, inferior frontal gyrus, pars opercularis, pars orbitalis, pars triangularis, lateral orbitofrontal cortex, medial orbitofrontal cortex, precentral gyrus, paracentral lobule, inferior parietal lobule, superior parietal lobule, supramarginal gyrus, precuneus, postcentral gyrus, entorhinal cortex, fusiform gyrus, parahippocampal gyrus, temporal pole, inferior temporal gyrus, middle temporal gyrus, superior temporal gyrus, transverse temporal gyrus, cuneus, lingual gyrus, and pericalcarine cortex."}, {"title": "Lesion Synthesis", "content": "We broaden the diversity of pathological image features in training by synthesizing brain abnormalities with variable traits. We implement a model-based domain randomization technique to augment images from healthy patients.\nTo create a random lesion during training, we first choose a target anatomical location and define a lesion boundary radius. A spherical boundary of the chosen radius is then sampled within a segmentation corresponding to the target anatomy. Inside this boundary, we generate multiple ellipsoids with random positions, rotations, and sizes, which are combined and further modified using random dilation, erosion, and non-linear deformation to produce the final lesion mask.\nThe lesion mask is in-painted into scans of a sampled subject. For each scan, we decide whether the lesion will appear hyperintense, hypointense, or isointense, and assign a random fill intensity relative to the surrounding tissue. To simulate heterogeneous intensities, a secondary lesion geometry is superimposed on the original mask."}, {"title": "Segmentation Analysis Data", "content": "In the table below, we summarize the number of unique images and subjects used to evaluate each label in the segmentation analysis (Section 5.2). All evaluations that involve an anatomical label use the same set of evaluation images, so we group them together below."}, {"title": "Classification Analysis Data", "content": "In the table below, we summarize the number of unique images and subjects used to evaluate each task in the pathology characterization analysis (Section 5.3)."}, {"title": "Zero-Shot Baseline Evaluation", "content": "We assess the zero-shot performance of RadFM [77] and ChatGPT-40 [143] on the subset of pathology characterization tasks used in Section 5.3. For each test sample, we provide these models with a detailed task description and all available images. To ensure a fair comparison, we include additional context in the input prompt, along with a set of possible answers for each question. The input prompts used for each task category are detailed below:\n*  Given the following brain scan(s), classify the lesion signal intensity as either hyperintense or hypointense.\n*  Given the following brain scan(s), classify the lesion location as either the temporal lobe, frontal lobe, parietal lobe, brainstem, or cerebellum.\n*  Given the following brain scan(s), classify the infarct location as either the ACA, MCA, or PCA territory.\n*  Given the following DWI and ADC brain scans, classify the lesion as diffusion restricting or non-restricting.\n*  Given the following pre- and post-contrast brain scans, indicate if the lesion is contrast enhancing.\nRadFM Zero-Shot Evaluation. To match RadFM input constraints, we resample all image shapes to multiples of 32 x 32 x 4. For each test case, we run evaluations across all possible voxel orientations of the volume data and run additional tests with the volume depth resampled to 64 voxels \u2013 the maximum depth used in RadFM pretraining.\nChatGPT Zero-Shot Evaluation. ChatGPT-40 is primarily designed for 2D image inputs rather than volumetric data. Thus, we convert each 3D volume into multi-plane cross-sectional slices centered on the relevant region of interest. Note that this slicing method biases ChatGPT predictions by effectively pinpointing the lesion location within the full volume.\nResults. Across all characterization tasks, RadFM fails to correctly classify any test sample, and ChatGPT achieves an average accuracy of only 58.6 \u00b1 14.1%, suggesting an inability to generalize to complex medical imaging tasks."}, {"title": "Radiopaedia Data", "content": "We download the following patient cases from Radiopaedia, a radiology reference website at https://radiopaedia.org. Each case includes text-based notes and brain scans in the form of 2D image slices. We reconstruct volumetric data by stacking these slices and estimating an affine matrix to map voxel coordinates in world space. We compute this mapping by registering the image to an average brain template with SynthMorph [141]."}]}