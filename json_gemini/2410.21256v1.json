{"title": "Multi-modal AI for comprehensive breast cancer prognostication", "authors": ["Jan Witowski", "Ken Zeng", "Joseph Cappadona", "Jailan Elayoubi", "Elena Diana Chiru", "Nancy Chan", "Young-Joon Kang", "Frederick Howard", "Irina Ostrovnaya", "Carlos Fernandez-Granda", "Freya Schnabel", "Ugur Ozerdem", "Zoe Steinsnyder", "Kangning Liu", "Nitya Thakore", "Mohammad Sadic", "Frank Yeung", "Elisa Liu", "Theodore Hill", "Benjamin Swett", "Danielle Rigau", "Andrew Clayburn", "Valerie Speirs", "Marcus Vetter", "Lina Sojak", "Simone Muenst Soysal", "Daniel Baumhoer", "Khalil Choucair", "Yu Zong", "Lina Daoud", "Anas Saad", "Waleed Abdulsattar", "Rafic Beydoun", "Jia-Wern Pan", "Haslina Makmur", "Soo-Hwang Teo", "Linda Ma Pak", "Victor Angel", "Dovile Zilenaite-Petrulaitiene", "Arvydas Laurinavicius", "Natalie Klar", "Brian D. Piening", "Carlo Bifulco", "Sun-Young Jun", "Jae Pak Yi", "Su Hyun Lim", "Adam Brufsky", "Francisco J. Esteva", "Lajos Pusztai", "Yann LeCun", "Krzysztof J. Geras"], "abstract": "Treatment selection in breast cancer is guided by molecular subtypes and clinical characteristics. Recurrence risk assessment plays a crucial role in personalizing treatment. Current methods, including genomic assays, have limited accuracy and clinical utility, leading to suboptimal decisions for many patients. We developed a test for breast cancer patient stratification based on digital pathology and clinical characteristics using novel AI methods. Specifically, we utilized a vision transformer-based pan-cancer foundation model trained with self-supervised learning to extract features from digitized H&E-stained slides. These features were integrated with clinical data to form a multi-modal AI test predicting cancer recurrence and death. The test was developed and evaluated using data from a total of 8,161 breast cancer patients across 15 cohorts originating from seven countries. Of these, 3,502 patients from five cohorts were used exclusively for evaluation, while the remaining patients were used for training. Our test accurately predicted our primary endpoint, disease-free interval, in the five external cohorts (C-index: 0.71 [0.68-0.75], HR: 3.63 [3.02-4.37, p<0.01]). In a direct comparison (N=858), the AI test was more accurate than Oncotype DX, the standard-of-care 21-gene assay, with a C-index of 0.67 [0.61-0.74] versus 0.61 [0.49-0.73], respectively. Additionally, the AI test added independent information to Oncotype DX in a multivariate analysis (HR: 3.11 [1.91-5.09, p<0.01)]). The test demonstrated robust accuracy across all major breast cancer subtypes, including TNBC (C-index: 0.71 [0.62-0.81], HR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic tools are currently recommended by clinical guidelines. These results suggest that our AI test can improve accuracy, extend applicability to a wider range of patients, and enhance access to treatment selection tools.", "sections": [{"title": "Main", "content": "Over the past few decades, breast cancer treatment has evolved significantly with the introduction of various chemo, endocrine, and targeted therapies. Treatment selection is primarily guided by clinical guidelines that rely on clinicopathological factors such as tumor size, nodal status, and estrogen and HER2 receptor status. While this approach optimizes outcomes at the population level to a certain degree, it overlooks critical, actionable information for individual patients, exposing a significant gap in personalized care.\nIn an attempt to address this limitation, genomic assays were developed in the early 2000s, offering a more refined approach to patient risk stratification. Tools such as Oncotype DX [1], Mammaprint [2] and Prosigna [3] assess the risk of distant recurrence based on gene expression data. However, these assays were developed primarily for hormone receptor-positive (HR+) breast cancer patients, mainly to de-escalate adjuvant chemotherapy. In addition to being limited to certain cancer subtypes, their accuracy is modest, often on par with simple models based on clinical characteristics [4, 5]. These tests also require the processing of physical tissue specimens, which creates additional work for pathology departments and depletes tissue that could be used in the future for advanced molecular profiling.\nComplementary to genomic assays, histopathological features are routinely evaluated by pathologists to stratify patient risk. These features include histologic grade, tumor infiltrating lymphocytes level, mitotic count, and Ki67 expression. It has been shown that these primarily morphological features add independent prognostic information to genomic scores, and recent prognostic models integrate both [6, 7]. However, these features are limited in their scope and prognostic ability, driving the need for stronger and more robust prognostic features. In recent years, advances in AI, particularly in self-supervised learning, have enabled the development of more effective methods for learning meaningful features from imaging data [8, 9, 10, 11, 12, 13]. These features are not based on pre-specified characteristics and extracting them does not require manual annotation by pathologists. Instead, AI models autonomously determine the most salient features by learning from millions of images. Early evidence shows that these AI-enabled features extracted from digital pathology images can be used to predict key patient characteristics and their long-term outcomes [14, 15, 16].\nIn response to the gaps in today's treatment selection tools, we developed and evaluated an AI test for the stratification of invasive breast cancer patients. It is built upon the latest advances in self-supervised learning and digital pathology. Specifically, our test extracts strongly predictive morphological features from standard H&E-stained slides of core needle biopsy or surgical specimens utilizing Kestrel [17], a state-of-the-art pan-cancer AI foundation model. Kestrel is a variant of vision transformer [18] trained with self-supervised learning method DINOv2 [13] using 400M digital pathology image patches. The image patches used to train Kestrel were sourced from a large, heterogeneous database of pan-cancer whole slide images, including breast"}, {"title": "Results", "content": "Unless explicitly stated otherwise, the presented results are from the multi-modal AI test. We first report its prognostic capability, i.e., whether our model accurately predicts the risk of cancer recurrence and distinguishes between high- and low-risk patients. Then, we compare its accuracy against a standard-of-care genomic risk signature in various molecular, histological and clinical subgroups. Finally, we discuss potential clinical use cases. When reporting performance across multiple datasets, we present pooled results, using a random effects model (see Appendix A.7 and A.6 for performances on individual datasets).\nTo evaluate how well the test orders patients according to their risk, we report the concordance index (C-index), a metric analogous to AUROC which can be computed for censored outcomes. Additionally, we estimate the hazard ratio (HR) for the AI test in univariate and multivariate Cox proportional hazards models. The hazard ratio is estimated from the continuous score for every 0.2 unit increase in our score. Unless explicitly stated otherwise, we use the continuous AI test risk score, rather than its dichotomized version. Refer to Methods (Section 4) for a detailed explanation of the performance metrics used in this paper."}, {"title": "The AI test is predictive of breast cancer recurrence and survival", "content": "The AI test was found to be prognostic in all five patient cohorts used as external evaluation sets. For the primary endpoint, disease-free interval (DFI), the risk score generated by our model achieved a pooled C-index of 0.71 [0.68-0.75] and a pooled HR of 3.63 [3.02-4.37, p<0.01.\nTwo of the test datasets, Providence and TCGA-BRCA, represent a broad spectrum of invasive breast cancer patients. For prediction of DFI, our AI test achieved a C-index of 0.74 [0.70-0.79] and a HR 4.02 [3.09-5.23, p<0.01] in the Providence cohort (N=1,733), and a C-index of 0.70 [0.63-0.77]] and a HR of 3.00 [2.10-4.28, p<0.01] in the TCGA-BRCA cohort (N=911).\nThree cohorts Karmanos, Basel, and UChicago included only HR+ HER2- patients who were previously tested with Oncotype DX. The AI test achieved a C-index of 0.62 [0.49-0.75] and a HR of 3.82 [1.33-10.98, p=0.01] in the Karmanos cohort (N=168), a C-index of 0.70 [0.60-0.80] and a HR of 3.98 [1.92-8.25, p<0.01] in the Basel cohort (N=269), and a C-index of 0.67 [0.58-0.77] and a HR of 3.25 [1.45-7.31, p<0.01] in the UChicago cohort (N=421).\nThe AI test was also prognostic for secondary endpoints. In analyses utilizing random effects models, pooled C-indices and HRs were as follows. For distant-disease free interval (DRFI), the test achieved a C-index of 0.70 [0.61-0.78] and a HR of 4.02 [2.64-6.13, p<0.01. For overall survival (OS), the test achieved a C-index of 0.65 [0.58-0.72] and a HR of 2.52 [1.88-3.38, p<0.01. For recurrence-free survival (RFS), the test achieved a C-index of 0.68 [0.65-0.71] and a HR of 2.65 [2.33-3.01, p<0.01]. And for distant recurrence-free survival (DRFS), the test achieved a C-index of 0.66 [0.61-0.71] and a HR of 2.64 [2.14-3.26, p<0.01]. Forest plots for all endpoints and metrics are reported in Appendix A.6, and endpoints are defined in Appendix A.4."}, {"title": "The AI test is more accurate than a genomic assay in predicting cancer recurrence in HR+ patients", "content": "We compared the prognostic performance and potential clinical utility of our model to Oncotype DX, a standard-of-care 21-gene assay developed to predict distant recurrence risk. Three external cohorts (Karmanos, Basel, UChicago) contain a total of 858 patients who were tested with Oncotype DX Recurrence Score assays performed as part of routine care. Full results are in Appendix A.8.\nIn the Karmanos cohort, the AI model's C-index for DFI was 0.62 [0.49-0.76] and the HR was 3.81 [1.33-10.98, p<0.01], compared with Oncotype DX's C-index of 0.54 [0.41-0.68] and HR of 1.36 [0.56-3.27, p=0.50]. In the Basel cohort, the AI test's C-index was 0.70 [0.60-0.80] and the HR was 3.37 [1.54-7.40, p<0.01], compared with Oncotype DX's C-index of 0.55 [0.42-0.68] and HR of 1.76 [0.85-3.64, p=0.13]. In the UChicago cohort, the AI test's C-index was 0.67 [0.58-0.77] and the HR was 3.26 [1.45-7.31], compared with Oncotype DX's C-index of 0.71 [0.61-0.81] and HR of 2.78 [1.61-4.81]. Together, the AI test achieved a pooled C-index of 0.67 [0.61-0.74] and HR of 3.67 [2.79-4.84, p<0.01], compared to Oncotype DX's C-index of 0.608 [0.491-0.725] and HR of 2.09 [0.85-5.15, p=0.21]. These comparisons are illustrated in Figure 2A.\nOncotype DX has been shown to predict the benefit of adjuvant chemotherapy in HR+ patients. Patients"}, {"title": "The AI test performs well in various clinically meaningful subgroups. Potential clinical utility.", "content": "Traditional genomic assays are typically developed for specific patient subgroups, such as Oncotype DX for HR+ HER2- patients, and cannot be used in patients with less common molecular, histological, or clinical characteristics. Given that the AI test was developed using data from non-metastatic breast cancer patients without additional exclusion criteria, we can evaluate its prognostic accuracy in various subgroups. This includes patients with varying age and menopausal status, nodal status, tumor size, estrogen and HER2 receptor status, race, and administered of adjuvant therapy (Figure 3). Furthermore, our analysis indicates that the test score is consistent with known clinical factors that influence cancer prognosis, including higher scores in ER- and PR- patients, as well as patients with more advanced staging (both N and T) (Figure 5).\nCurrently, there is no prognostic/predictive tool supported by NCCN guidelines for use in triple-negative"}, {"title": "Discussion", "content": "In this study, we present a novel prognostic multi-modal AI test for invasive breast cancer. The overall performance of our test is robust across breast cancer patients. The test also outperformed Oncotype DX in three cohorts Karmanos, Basel, and UChicago. Notably, the presented test is one of the few prognostic tools that have been clinically validated in triple-negative and HER2+ breast cancer patients. This capability, combined with strong performance regardless of nodal status and patient age, supports the viability of the AI test as a single tool to inform treatment decisions in all breast cancer patients.\nThe proposed AI test is designed to predict cancer recurrence risk, similar to genomic assays such as Oncotype DX and MammaPrint. However, just like these assays, it is not explicitly trained to model treatment effects. We expect that when validated using randomized prospective data, our prognostic test will show predictive capabilities in the same manner that the genomic assays do. That is, high-risk patients will benefit from more aggressive treatment options (such as adjuvant chemotherapy), while low-risk patients will have no benefit. While this study uses observational data and did not evaluate the test's predictive capabilities (i.e., its ability to determine whether a patient is likely to benefit from a specific treatment), we present several potential clinical implications of using such a test in a clinical setting.\nThe AI test has the potential to refine clinical decision-making in several key areas. In HR+ patients who are candidates for adjuvant chemotherapy, the AI test was more accurate than Oncotype DX. This can potentially improve the selection of patients who can benefit from chemotherapy in addition to endocrine therapy. In a multivariate analysis, the AI test was shown to add independent prognostic information with respect to Oncotype and cancer grade, which indicates that it captures information that is strongly predictive and independent from existing clinical factors. Importantly, the AI test was prognostic in premenopausal HR+ patients, expanding patient groups who could be evaluated for cancer recurrence risk. Finally, we show that the AI test is prognostic in HR+ patients who received adjuvant chemoendocrine therapy, and identifies patients with high \"residual\" recurrence risk. We hypothesize that these patients might benefit from adjuvant CDK4/6 inhibitors. This use case is especially meaningful given recent FDA approval for ribociclib, which can be used in a wide group of HR+ patients, including node-negative cases. Identifying high-risk patients beyond basic clinical characteristics will be important in selecting patients who would benefit from CDK4/6 inhibitor treatment.\nBesides HR+ patients, the test was prognostic in triple-negative and HER2+ patients, for which there are no NCCN guideline-supported tests for treatment selection. Current standard of care treatment in triple-negative breast cancer involves intense immunochemotherapy (KEYNOTE-522 regimen [27, 28]). There are multiple ongoing clinical trials working on treatment de-escalation through avoiding adjuvant pembrolizumab in patients with pathological complete response (OptimICE-PCR [29, 30]) or using shorter, anthracycline-free neoadjuvant chemotherapy regimen in patients with immune-enriched TNBC (NeoTRACT) [31].\nThe AI test was developed using Kestrel, an AI foundation model for digital pathology trained to extract morphological features from digitized pathology slides. Kestrel differs from the previous generation of feature extractors for digital pathology which relied on pre-specified \u201cpathomic\u201d features [32]. Kestrel, on the other hand, was trained with self-supervised learning, which does not require any pre-specification of features; rather, the self-supervised learning paradigm enables the model to learn the most salient features. While this sacrifices a priori explainability, the model can still be explained post hoc. Finally, because Kestrel was developed using a pan-cancer dataset, it can easily be applied to other clinical indications.\nApplying self-supervised learning to develop prognostic/predictive models is an emerging research area. Spratt et al. demonstrated successful application of this approach to prostate cancer in a series of pub-lications [33, 34]. The prostate test has been developed and validated using several randomized clinical"}, {"title": "Methods", "content": "To execute this project we assembled a diverse collection of both public datasets (TCGA-BRCA [37, 38], METABRIC [39], BASIS [40]) and private datasets from several sources (NYU Langone Health, The Catholic University of Korea, Gundersen Health, National Center of Pathology in Lithuania, Breast Cancer Now Biobank, Cancer Research Malaysia, Omica.bio Cancer Atlas, Wales Cancer Biobank, Karmanos Cancer Institute, University Hospital Basel, and UChicago Medicine). All together we have 5162 slides from 4659 patients across 10 datasets for training, and 3632 slides from 3502 patients from 5 datasets for evaluation. This represents one of the largest data collection efforts for the purpose of predicting breast cancer recurrence. The breakdown of patient demographic, molecular, and clinical characteristics is described in Table 1.\nOur primary task was to accurately predict the risk of cancer-related events. To accomplish this, we trained time-to-event models using clinical variables and digital pathology slides. For both modalities, we trained several models and ensembled their predictions, as detailed in Section 4.3.1. We then created a multi-modal model by averaging predictions of the two ensembles. We trained and evaluated the models using the definitions of endpoints in Table 3."}, {"title": "Performance metrics", "content": "Time-to-event models can be evaluated using a few different metrics that highlight various aspects of their performance. The two primary metrics we use are the concordance index (C-index) and hazard ratio (HR). Additionally, as we used several different datasets for evaluation, we aggregated results using random effects models. We refer to such aggregated metrics as pooled. In the following paragraphs, we justify the use of these metrics in our application. Detailed explanations of how the metrics are computed are in the Appendix.\nC-index. To evaluate the prognostic ability of our model, we used the concordance index (C-index). The C-index measures how well the predicted risk ranking of patients aligns with the actual order in which they experienced events. A C-index of 1.0 indicates that patients are perfectly ordered according to predicted risk, whereas a C-index of 0.5 suggests that the model's ordering is no better than random. The advantage of the C-index is that it is easy to interpret, as it analogous to AUROC in how it is computed, while accommodating censoring. A detailed explanation of how the C-index is computed is in Appendix A.1.\nHazard ratio for a continuous score. For a continuous score, the hazard ratio represents the relative increase or decrease in the risk of the event associated with a one-unit increase in that score. In other words, it quantifies how changes in the score influence the relative risk of the event happening over time. In our analysis, we compare a 0.2 unit increase in our score (which ranges from 0 to 1) to a 20 unit increase in the Oncotype score (which ranges from 0 to 100). A detailed explanation of how the hazard ratio is computed is in Appendix A.2.\nHazard ratio for a dichotomized score. For a dichotomized score, the patients are stratified into two categories: high-risk and low-risk. The hazard ratio assesses the relative risk of experiencing an event in the two categories. For example, a HR of 2 indicates that the high-risk group is twice as likely to experience the event compared to the low-risk group. Hazard ratios come with associated p-values, computed with the Wald test, that evaluate whether the estimated hazard ratio for each covariate is statistically significantly different from 1.\nMultivariate Cox proportional hazards model. To assess the simultaneous impact of multiple variables on the outcome, we fit a multivariate Cox proportional hazards model with all the variables of interest. The Cox regression coefficients estimate the strength of association between each variable and the outcome, adjusted for other covariates. A coefficient greater than zero, or a hazard ratio exceeding 1 (as determined by"}, {"title": "Hyperparameter selection", "content": "In all instances, we selected the hyperparameters of the models through random search [41]. Unless otherwise explicitly stated we maximized the C-index. As we were tuning relatively few hyperparameters with moderately wide intervals, random search explores the hyperparameter space sufficiently well.\nFor pathology data, the performance of the models sampled in the hyperparameter search were estimated us-ing modified multiple-source cross-validation [42]. That is, we had two sets of datasets $D_T = \\{D_1, D_2, ..., D_J \\}$ and $D_V = \\{D_{J+1}, D_{J+2}, ..., D_{J+K}\\}$. Datasets in $D_T$ were reserved only for training, while $D_V$ could be used for training or validation. For a given hyperparameter setting, we trained K models, $m_1, m_2, ..., m_K$. The training set for the kth model was $D_{\\theta_k} = D_T \\cup D_V \\setminus D_{J+k}$, and the validation set was $D_{J+k} \\in D_V$. The performances on the validation sets were averaged, weighted by the number of comparisons $c_k$ used in the computation of the validation C-index using the dataset $D_{J+k}$. This procedure was used for performance estimation and hyperparameter selection as formalized in Algorithm 1."}, {"title": "The AI model", "content": "Our model learns and makes predictions using both clinical variables and digital pathology images. Internally, it is composed of many models whose predictions are ensembled in two rounds. First, the models are ensembled within their data modality. Then, the ensemble of clinical models is combined with the ensemble of digital pathology models. The intuition behind ensembling lies in the idea that different models make errors that are not perfectly correlated. Ensembling several models cancels out some of these errors, adding accuracy and robustness to predictions. An additional advantage of ensembling at the level of modalities is that for every test example, it is easy to interpret the contribution of the clinical data and the digital pathology data to the final prediction."}, {"title": "Creating model ensembles", "content": "Ensembling the clinical models happened in two steps. First, the output risk scores from all the clinical models were scaled approximately to the interval [0, 1]. That is, for each model $m_k$, we found the maximum over the entire validation set, $max_k = max_i\\{y_i\\}$, and the minimum over the entire validation set, $min_k = min_i\\{y_i\\}$. For the test examples, the normalized prediction from any model was computed as $y_i^k = \\frac{\\hat{y}_i^k - min_k}{max_k - min_k}$. Then predictions from all models were simply averaged. That is, assuming that there were K models, for any test example, the aggregated clinical prediction was computed as $\\bar{y} = \\frac{\\sum_{k=1}^{K}y^k}{K}$.\nFor pathology models, as the predictions already were in the interval [0, 1], we simply averaged predictions from different models. That is, for any test example, the aggregated pathology prediction was computed as $\\bar{y} = \\frac{\\sum_{k=1}^{K}y^k}{K}$. We generated the final pathology model by averaging the top K models across different combinations of pooling method (mean, max, or MIL), training loss function (Cox or discrete-time), and feature extractor to form the final ensemble. We used K = 10 for both clinical and pathology models.\nFinally, the clinical score and the pathology score were averaged, that is, $y = \\frac{(\\bar{y}_c + \\bar{y}_p)}{2}$."}, {"title": "Digital pathology models", "content": "Unlike many prior approaches [43, 32, 44], we did not use any hand-crafted pathomic features. Instead, we obtained learned morphological features from Kestrel, our purpose-built foundation model trained on a pan-cancer dataset. These features were used to train time-to-event models. Below we present our model in greater detail.\nKestrel's architecture. Kestrel is a ViT-L which contains 303M parameters. An overview of the vision transformer architecture is provided in Figure 6.\nPreprocessing histopathology slides. Digital pathology slides are extremely high resolution, often exceeding 1 billion pixels per image. Additionally, large regions of these images are empty. Therefore, to extract only meaningful tissue, we used Otsu's method [45] to distinguish background from foreground. Subsequently, we patchify the foreground into non-overlapping 256 \u00d7 256 patches at 20\u00d7 magnification (0.5 microns per pixel). Each slide is thus reduced to a set of patches. There are typically between 8,000 to 16,000 patches per slide."}, {"title": "Cox proportional hazard model", "content": "Consider a model, m, parameterized by \u03b2, with the training dataset $D_T$ and the validation dataset $D_V$. The training loss for this model is computed as"}, {"title": "Discrete-time model", "content": "We divide times from $t_0$ to $t_T$ into J contiguous time intervals $(t_0, t_1], (t_1, t_2], ..., (t_{J-1}, t_J]$, where $t_0 = 0$ and $t_J = \\infty$.\nFor subject i with covariates $x_i$, the hazard in interval $A_j = (t_{j-1}, t_j]$ is\n$\\lambda_{ij}(X_i) = Pr(T_i \\in A_j | T_i > t_{j-1}, X_i)$,"}, {"title": "Aggregation of patch embeddings", "content": "In order to train time-to-event neural networks with Cox or discrete-time losses, it is necessary to aggregate patch embeddings into a single vector representation that can be passed into a standard feed-forward neural network. We used two strategies to do this: parameterless pooling and attention-weighted multiple instance learning (MIL) pooling.\nParameterless pooling. It has been shown that simple forms of pooling such as mean- and max-pooling perform well on digital pathology tasks where aggregation is needed. Thus, one set of the trained time-to-event models in our pathology ensemble consists of networks trained on mean- or max-pooled patch embeddings with a Cox loss.\nAttention-weighted MIL. It has also been shown that simple attention-based networks, such as gated attention [47], are also effective ways of aggregating patch embeddings in digital pathology. Thus, we trained a complementary set of time-to-event models using a gated attention network with a discrete-time loss."}, {"title": "Clinical models", "content": "For building a model for clinical variables we utilized CatBoost [48], a gradient boosting decision tree ensemble. Its primary advantage over other possible models for this application, e.g., neural networks, is how it handles categorical variables and associated missing values which are inevitable in clinical data. Here, CatBoost is specifically trained with the AFT (accelerated failure time) loss. We utilized AFT models with three potential distributions: normal, logistic, and extreme value.\nWe used eight routinely collected variables (see Table 2) that characterize breast cancer and are typically used to guide treatment decisions. For instance, ER and PR statuses help determine the tumor's hormone receptor sensitivity, which is critical for hormone-based therapies. HER2 status is important for targeted therapies like trastuzumab, which is effective in HER2+ cancers."}, {"title": "Concordance index", "content": "C-index, originally proposed by Harrell et al. [49], is a metric of accuracy for time-to-event models, analogous to AUROC for classification, adjusted for the possibility of censoring. It is defined as the ratio of the number of concordant pairs of test examples to the number of all comparable pairs of test examples. A test example is defined by its event time, T, its vector of input features, x, and d indicating the presence of an event. Two test examples are comparable if the example with the shorter event time is not censored (i.e., experienced an event). Two test examples are concordant if the model assigns a higher risk score to the example with the shorter event time. Given $\u03b4_i = 1$ indicating the presence of an event and $T_i$ the 'time-to-event' response for example i, we use Equation 1 to compute the C-index:\n$C = \\frac{\\sum_{i,j} [risk(x_i) > risk(x_j)] \\cdot [T_i < T_j] \\cdot \\delta_i}{\\sum_{i,j} [T_i < T_j] \\cdot \\delta_i}     \\quad\\quad(1)$"}, {"title": "Hazard ratio", "content": "Hazard ratio is a metric appropriate for measuring how well a model can divide patients into two groups, e.g. low- and high-risk. The hazard ratio is computed using the Cox proportional hazards model, in which the input indicates one of the two groups. That is, we model the hazard for an individual at time t as $\u03bb(t|x) = \u03bb_0(t)exp(\u03b2x)$, where x is the group indicator (x = 0 or x = 1), $\u03bb_0(t)$ is the baseline hazard function which represents the hazard when x = 0 and \u03b2 is the regression coefficient associated with the variable indicating the group a patient belongs to. Then, based on the data, this model is fitted using partial likelihood. For the fitted model, hazard ratio is computed as\n$HR = \\frac{\u03bb(t|x = 1)}{\u03bb(t|x = 0)} = \\frac{\u03bb_0(t)exp(\u03b2\\cdot 1)}{\u03bb_0(t)exp(\u03b2\\cdot 0)} = exp(\u03b2)$.\nIf HR > 1, the group with x = 1 has a higher hazard of experiencing the event in comparison to the group with x = 0. If HR < 1, the group with x = 0 has a higher hazard of experiencing the event in comparison to the group with x = 1. For example, if HR = 0.5 the risk for the group with x = 0 is twice the risk for the group with x = 1.\nHazard ratio can be analogously defined for a continuous x. Then, it is interpreted as the change in the hazard associated with a one-unit increase in x. That is,\n$HR = \\frac{\u03bb(t|x = \u03b1 + 1)}{\u03bb(t|x = \u03b1)} = \\frac{\u03bb_0(t)exp(\u03b2\\cdot (\u03b1 + 1))}{\u03bb_0(t)exp(\u03b2\\cdot \u03b1)} = exp(\u03b2)$.\nAdditionally, the logrank test [50] is commonly utilized to determine if there the hazard ratio is significantly different from one between two groups. The Wald test is utilized to assess the significance of individual predictors or the hazard ratio for a specific variable within a multivariate Cox model."}, {"title": "Random effects model", "content": "When comparing metrics across different datasets, a random effects model can be applied to account for the variability between these datasets. The random effects model accounts for both within-dataset variability (the inherent variation in the evaluated metric within each dataset) and between-dataset variability (differences between the datasets themselves).\nThe random effects model takes the following form\n$M_i = \u03bc + u_i + \u03b5_i,$\nwhere $M_i$ is the metric for dataset i, \u03bc is the overall effect, $u_i$ is the random effect for dataset i ($u_i \\sim \\mathcal{N}(0, \u03c4^2)$, $\u03c4^2$ represents the variance between datasets) and $\u03b5_i$ is the within-data-set noise ($\u03b5_i \\sim \\mathcal{N}(0, \u03c3^2)$, $\u03c3^2$ represents within-data-set variance). The overall fixed effect \u03bc provides a summary of the model's average performance across datasets. The model is fit using maximum likelihood, as implemented in the Metagen package [51]."}, {"title": "Endpoint definitions", "content": null}, {"title": "Multivariate Cox analyses", "content": "Multivariate analysis was conducted to assess the significance of the pathology model adjusted for the clinical model (see Table 4), importance of AI test after adjusting for datasets (see Table 6), and AI test after adjusting for various additional factors (see Table 6."}, {"title": "Forest plots for all endpoints", "content": "Complete results for both primary and exploratory endpoints are available, with C-index shown in Figure 8 and hazard ratios shown in Figure 9. These results include our model's performance across various endpoints,"}]}