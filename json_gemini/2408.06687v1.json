{"title": "Masked Image Modeling: A Survey", "authors": ["Vlad Hondru", "Florinel Alin Croitoru", "Shervin Minaee", "Radu Tudor Ionescu", "Nicu Sebe"], "abstract": "In this work, we survey recent studies on masked image modeling (MIM), an approach that emerged\nas a powerful self-supervised learning technique in computer vision. The MIM task involves masking\nsome information, e.g. pixels, patches, or even latent representations, and training a model, usually an\nautoencoder, to predicting the missing information by using the context available in the visible part of\nthe input. We identify and formalize two categories of approaches on how to implement MIM as a\npretext task, one based on reconstruction and one based on contrastive learning. Then, we construct\na taxonomy and review the most prominent papers in recent years. We complement the manually\nconstructed taxonomy with a dendrogram obtained by applying a hierarchical clustering algorithm.\nWe further identify relevant clusters via manually inspecting the resulting dendrogram. Our review\nalso includes datasets that are commonly used in MIM research. We aggregate the performance results\nof various masked image modeling methods on the most popular datasets, to facilitate the comparison\nof competing methods. Finally, we identify research gaps and propose several interesting directions of\nfuture work.", "sections": [{"title": "1 Introduction", "content": "Data have always played a crucial role in training\nlarge deep neural networks. Procuring a curated\nlabeled dataset not only involves a great effort\nand a laborious process from a team of human\nannotators, but it also represents a significant\nexpense. As a result, various self-supervised learn-\ning strategies have been explored, where the model\nis pre-trained with a different objective, which does\nnot require human labels. Self-supervised learning\ncan help the model to learn a rich feature represen-\ntation, and even surpass supervised alternatives\n(He et al., 2022). Then, during the fine-tuning\nphase, the model is further optimized for a spe-\ncific downstream task. For example, in most image\nclassification tasks, a common approach is to train\nthe network on the ImageNet dataset (Deng et al.,\n2009; Russakovsky et al., 2015), and then change\nthe classification head for a new task and train the\nresulting model on a relevant dataset.\nSelf-supervised learning is a popular method\nto pre-train a deep learning model. It involves"}, {"title": "2 Generic Framework", "content": "Masked Image Modeling is an unsupervised tech-\nnique that is usually applied during the pre-\ntraining phase. It involves masking some infor-\nmation, either from the input or from the latent\nrepresentation, and then estimating the original\ndata, as if the data would not have been concealed.\nAlthough many masked image modeling techniques\nhave been proposed, the research has been focused\non two main schemes, either reconstructing the\nmasked signal, or comparing two latent represen-\ntations, one for the unaltered input signal and one\nfor the masked input. On a few occasions, differ-\nent approaches are explored, but they are built on\nsimilar grounds. Therefore, in the following subsec-\ntions, we aim to give a general formulation of the\nfirst two aforementioned schemes."}, {"title": "2.1 Reconstruction", "content": "The first scheme that we identified revolves around\nthe idea of masking some piece of information at\nany stage during the forward pass of the model,\nand then employing a decoder to reconstruct the\nmissing data. This pre-training technique was intro-\nduced in concurrent works by He et al. (2022)\nand Xie et al. (2022). Both works consist of an\nencoder based on a ViT architecture (Dosovit-\nskiy et al., 2021), in which a large portion of the\ninput tokens (resulting from linearly projecting the\nnon-overlapping patches of the image) are masked.\nAfter the encoding stage, where masked tokens are\nsubstituted with a special token, a decoder is used\nto reconstruct them. In the Masked Autoencoder\n(MAE) proposed by He et al. (2022), the decoder is\nlighter than the encoder, which results in an asym-\nmetric architecture. The loss function is applied\nonly to the output corresponding to the masked\ntokens. When applied to a downstream task, the\ndecoder is dropped, only the encoder being used\nas the backbone due to its strong feature represen-\ntation capability. We illustrate the reconstruction\nframework in Figure 2.\nWe formally present the reconstruction-based\ntraining strategy in Algorithm 1. The first step\nof the algorithm splits the input images into non-\noverlapping patches, resulting in a set P that\ncontains patches of the same size, namely of h x w\npixels. The second step applies the masking opera-\ntion, which can differ from one method to another.\nThe masking operation indicates which patches\nshould be kept and which should be masked. The"}, {"title": "2.2 Contrastive", "content": "The second generic scheme is represented by com-\nparing two different latent representations of the\nsame input. One latent representation corresponds\nto an unaltered or weakly augmented input image,\nwhile the other corresponds to a masked and\nstrongly augmented version of the same input\nimage. The approach is based on a contrastive\nlearning framework, as illustrated in Figure 3.\nThere are two common architectural configurations.\nOne configuration uses an encoder with shared\nweights, as in (Wu et al., 2023; Zhang et al., 2023).\nThe other configuration uses an encoder for the\nmasked input and an exponential moving average\n(EMA) version of the encoder for the original input,\nas in (Lee et al., 2023).\nThe generic contrastive-based MIM framework\nis formalized in Algorithm 2. The aim of this\nframework is to obtain a similar embedding, irre-\nspective of the applied masking. Steps 1 and 2 of\nthe algorithm generate two versions of the input\nimage by augmenting them at different intensities.\nThe version that undergoes masking is strongly\naugmented, while the other one is weakly aug-\nmented (or can even remain unaltered). In steps\n3-4, each image is divided into non-overlapping\npatches, and in step 5, masking is applied solely\nto the strongly augmented image. The unmasked\nimage undergoes processing by the projection layer\nand the encoder (step 7), whereas the masked"}, {"title": "3 Taxonomy and Overview", "content": "In Figure 4, we present a manually-generated\ntaxonomy of the most promising MIM papers, orga-\nnizing them according to their main contributions.\nIn constructing the taxonomy, we consider six main\nresearch directions related to: the masking strategy,\nthe type of masked signals, the neural architecture,\nthe objective function, the type of downstream\ntasks, and theoretical results. Nonetheless, since\nmany papers focused on more than one of the above\naspects, we grouped the respective works in distinct\ncategories representing composed contributions.\nWe now continue by presenting the aforementioned\npapers, divided into the sub-sections according our\ntaxonomy."}, {"title": "3.1 Masking Strategy", "content": "Early masking strategies were based on selecting a\nhigh number of random patches and masking them.\nRecently, many alternative masking policies have\nbeen proposed to improve MIM. One suggestion is\nto guide the masking through different mechanisms,\nfrom leveraging image statistics (Xu et al., 2023)\nto using an additional simple network (Bandara\net al., 2023) or even a self-attention module (Chen\net al., 2023; Yang et al., 2023).\nXie et al. (2022) introduced SimMIM, a self-\nsupervised pre-training framework that recon-\nstructs pixel values of randomly masked images.\nAn overview of the method is depicted in Figure 5.\nThe model is a simple encoder (ViT) receiving as\ninput an image with masked patches replaced by a\nlearnable token. Additionally, the study motivates\nthe choice of random masking and the raw pixels as\ntarget features through extensive ablation studies.\nHe et al. (2022) presented the masked autoen-\ncoder (MAE) framework, where the main contri-\nbution is to exclude the masked tokens from the\nencoder's input and to use a very high masking\nratio (75%). These changes imply a more efficient\nframework compared with other works, such as\nSimMIM (Xie et al., 2022). However, as a negative\neffect, having an encoder that operates only on\nvisible tokens makes the framework incompatible\nby default with the Hierarchical Vision Trans-\nformer (Liu et al., 2021) architecture, which usually\nperforms better than a standard ViT. The MAE\npipeline is presented in Figure 6.\nPang et al. (2022) adapted the masked pre-\ntraining strategy to 3D Point Clouds. The method\ngroups the points into patches and masks some\nof them. Then, it embeds and encodes only the\nunmasked patches. Next, the visible patches get\nconcatenated with the masked tokens and passed\nthrough the decoder, with the objective to recon-\nstruct the latter. The authors state that passing the\nmasked patches earlier leaks spatial information\nwhich simplifies the task."}, {"title": "3.2 Target Features", "content": "The early popular frameworks either consist of\nreconstructing patches of the original image (He\net al., 2022; Xie et al., 2022) or comparing high-\nlevel latent features (Chen et al., 2022). Subsequent\nworks adopted different reconstruction targets that\nare more suitable for the downstream tasks (Fang\net al., 2023; Hou et al., 2023), or demonstrated\nto learn richer feature representations (Bai et al.,\n2023). Finally, MIM has been employed for input\nsignals other than image pixels, and some of these\nstudies involved estimating a different feature type\nrather than the raw input signal, such as the\nstatistics of a point cloud (Tian et al., 2023).\nChen et al. (2022) argued that reconstructing\nthe image pixel space does not force the net-\nwork to learn an ideal representation of the data.\nThus, they introduced a self-distillation frame-\nwork, shown in Figure 7, in which the student\nbranch follows the original MAE flow, while the\nteacher network (which is not updated by gradient\ndescent, but rather from the weights of the student)\nonly takes the masked patches. The objective is\nto match the high-level features between the two\nnetworks.\nLiang et al. (2022) proposed the masked image\nmodeling pre-training for 3D meshes. Inspired by\nViT, they begin by grouping together faces (each\nface being represented by a 10D vector) into a\npatch. Then, they adopt the transformer archi-\ntecture, using the 3D location of the center of\neach patch for computing the positional embed-\nding. The algorithm follows MAE: patches are\nrandomly masked with a high masking ratio, the\nvisible patches are passed through the encoder,\nthen the resulting latent embeddings are concate-\nnated with masked tokens (associated with the\nmasked patches), and subsequently decoded. The\nobjective is not only to reconstruct the masked\npatches (by predicting the coordinates of the ver-\ntices), but also the faces of the patch (the 10D\nrepresentations).\nInspired by MAE, Assran et al. (2022) pre-\nsented a self-supervised masking pre-training strat-\negy that involves Siamese networks. Given a source\nimage and applying transformations on it, two dif-\nferent views (anchor and target) are generated.\nThen, only the anchor's tokenized patches are\nmasked, and using the Siamese networks (which fol-\nlow the ViT architecture), both views are encoded.\nThe latent representations are compared with a\nset of learnable prototypes in order to generate a\ndistribution, the final goal being that of obtain-\ning matching distributions (predictions of anchor\nand target). While the anchor's model is updated\nusing gradient descent, the target's network param-\neters are computed as an exponential moving\naverage from the anchor network's weights. The\nauthors attest that the method greatly improves\nthe performance in few-shot scenarios.\nGiven a labeled (source) and an unlabeled (tar-\nget) 3D Point Cloud dataset, the aim of Liang et al.\n(2022) is to transfer the knowledge from the latter\nto the former by embedding information about com-\nmon features in an encoder. This model is trained\nsimultaneously on both datasets, but with different\nobjectives. On the one hand, the source dataset is\ntrained in a supervised setting on a specific task.\nOn the other hand, points from the target dataset\nare randomly masked in arbitrary areas and the\nobjective is to estimate their cardinality (number\nof points in the neighborhood), position and nor-\nmal vectors. Therefore, the model benefits from"}, {"title": "3.3 Objective Function", "content": "The primary objective function of MIM is to recon-\nstruct the masked pieces of information from the\ninput signal. Thus, any paper that contributed in\nthis regard, either by improving the reconstruction\nprocess or taking a different approach, is included\nin the category of reconstruction-based frameworks.\nInspired by the reconstruction-based techniques,\nanother approach has recently emerged, which\napplies a contrastive objective function. Further-\nmore, MIM was integrated in methods that showed\ngreat potential in leveraging out-of-distribution\nunlabeled data (Yu et al., 2022; Zhang et al., 2024),\nand even in adapting a model to the test data dis-\ntribution (Gandelsman et al., 2022; Mirza et al.,\n2023).\nInspired by the MAE, Liu et al. (2022) pro-\nposed a similar pre-training framework for 3D Point\nClouds, but substituted the reconstruction task\nwith discrimination. A small proportion of points\nare left unmasked, and subsequently encoded.\nThen, a subset of the masked ones is sampled (real),\nalong with some random 3D points from the space\n(fake), and the decoder's objective is to discrim-\ninate between the two. The encoded unmasked\npoints are used in the cross-attention blocks of\nthe decoder. Experiments are conducted for vari-\nous downstream tasks, in which the method shows\nconsiderable performance improvements.\nWeers et al. (2023) studied the effectiveness of\ncombining MAE and CLIP in a single framework.\nThe conclusion is that the combination brings some\nbenefit when training on a small dataset (tens of\nmillions of samples), but this benefit is smaller or"}, {"title": "3.4 Downstream Task", "content": "Given the promising results of MIM and its ability\nto diminish the negative effects on low quantities of\ndata, the pre-training strategy was eagerly applied\nin many visual tasks and related domains, ranging\nfrom image classification (Zhang et al., 2022; Jiang\net al., 2023) and image generation (Bashkirova\net al., 2023; Chang et al., 2022) to 3D point clouds\n(Shen et al., 2023), graphs (Jing et al., 2022), and\neven medical data (Bozorgtabar et al., 2023; Chen\net al., 2023; Zhou et al., 2023).\nMaskGIT (Chang et al., 2022) trained a gener-\native transformer to reconstruct randomly masked\nimage patches. At inference time, it starts by\npredicting all the patches and keeps the most con-\nfident ones for the next iteration when the rest\nof the patches are again masked and regenerated.\nThe process continues for a few iterations. Overall\nthe pipeline has actually two stages, the first one\nencodes the patches into visual tokens with a VQ-\nEncoder, and the second stage (decoder) receives\nmasked tokens for reconstruction.\nFeichtenhofer et al. (2022) proposed to use\nMAE on videos. The video is split into equally-sized\npatches that do not overlap along any dimension\n(i.e. including time), as well as consisting only of\ntwo timesteps. As hypothesized and demonstrated\nby the authors, a high masking ratio (about 90% of\nthe whole input video, unaware of any axis) is used\ndue to redundant data when decoding. Positional\n(i.e. height and width) and temporal (i.e. time)\nembeddings are added to the input tokens. The\narchitectures of both the encoder and the decoder\nare based on ViT. The method follows the same"}, {"title": "3.5 Theoretical Analysis", "content": "Besides the applied contributions, some pieces of\nwork take another approach: they theorize about\nvarious aspects of MIM and dive deeper into its fun-\ndamentals. The papers from this category address\naspects such as overall understanding (Kong et al.,\n2023; Pan et al., 2023), the connection between\nMIM strategies (Zhang et al., 2022), or present cer-\ntain drawbacks and how to overcome them (Huang\net al., 2023; Xie et al., 2023).\nLiu et al. (2022) evaluated the performance\nof self-supervised learning (SSL) by determining\nwhether the trained model represents enough infor-\nmation to obtain the data distribution, given\nadditional information about the family distribu-\ntion. The SSL task chosen for this assessment was\nthe masked prediction task.\nLi et al. (2023) explored the effectiveness\nof MIM on out-of-distribution (OOD) detection.\nTheir results showed that MIM improves the per-\nformance in multiple settings, such as one-class,\nmulti-class or near-distribution OOD.\nKong et al. (2023) formulated the underlying\ndata generation process as a hierarchical latent vari-\nable process. The authors discovered relationships\nbetween the latent variables of the data genera-\ntion process and the masking parameters (masking"}, {"title": "3.6 Model Architecture", "content": "While the architecture employed throughout MIM\nresearch was consistent (a transformer-based\nencoder and a shallow decoder), there have been\nsome important contributions that further enhance\nthe performance of the pre-training task through\narchitectural modifications (Liu et al., 2023). A few\nattempts have tried to distinguish themselves from\nthe usual ViT-based models, either by using CNNs\n(Tian et al., 2023; Woo et al., 2023) or by inte-\ngrating the MIM into the convolution operation\n(Madan et al., 2024; Ristea et al., 2022).\nRistea et al. (2022) presented the self-\nsupervised predictive convolutional attentive block\n(SSPCAB), a novel block comprising a masked\nconvolutional layer and a Squeeze-and-Excitation\n(SE) module. The filters of the masked convolu-\ntional layer contain learnable parameters in the\ncorner regions of the receptive field and the masked\nregion is located in the center. This novel block\nis trained using a self-supervised reconstruction\nloss, being integrated in anomaly detection net-\nworks. Later, Madan et al. (2024) introduced the\nself-supervised masked convolutional transformer"}, {"title": "3.7 Target Features and Objective Function", "content": "Several studies have contributed to both the target\nfeatures and the objective function. The target\nfeatures are obtained in various ways, mostly by\nusing features deep encoders (e.g. CLIP or DI\u039d\u039f)\n(Dong et al., 2023; Wu et al., 2023). The objective\nfunction modifications revolve around integrating\nboth reconstruction and contrastive losses.\nMaskCo (Zhao et al., 2021) is a region-level\ncontrastive learning framework. It begins by aug-\nmenting images to create two distinct views of the"}, {"title": "3.8 Model Architecture and Objective Function", "content": "A number of papers contributed to both the model\narchitecture and the objective function. While the\nformer is the main focus of most works, the latter\ncontribution represents an extension resulting from\nthe architectural modification.\nFei et al. (2023) enhanced the standard MAE\npipeline by integrating a discriminator for adver-\nsarial training. Notably, this discriminator, which\nshares parameters with the MAE's encoder, is\ntrained to distinguish between synthesized and real\npatches. This enhancement is an addition to the\nexisting pipeline, with the typical reconstruction\nloss still being present in the training process.\nRather than following the standard approach\nin MIM with a reconstruction objective, Xue et al.\n(2023) proposed a unique loss function that aligns\nfeatures extracted from visible tokens with those\nextracted by a teacher model across various archi-\ntectural levels. To facilitate this alignment, this\nstudy introduced a novel module named Dynamic\nAlignment. This module is specifically designed\nto ensure compatibility between the two sets of\nfeatures, enabling more effective feature alignment.\nInspired by masked autoencoders that process\nboth visual and textual modalities, Fuller et al.\n(2023) adapted the masking pre-training framework\nfor optical and radar inputs. Both modalities, after\nbeing aligned, tokenized and randomly masked,\nare encoded using an individual encoder, and then\nthey are jointly processed by a multimodal encoder.\nThe resulting embedding is decoded to reconstruct\nboth input images. A contrastive loss between the\nmean embeddings of the individual encoders is"}, {"title": "3.9 Masking Strategy and Target Features", "content": "Several studies have impacted both the masking\nstrategy and the target features. In general, these\ntwo contributions are jointly proposed, since chang-\ning the target features involves a different masking\nstrategy for the new input signal.\nZhao et al. (2023) presented a method for learn-\ning representations useful for object tracking. The\nmethod is based on MAE, but the authors use\ntwo inputs, one is the search region and the other\none is the template. The MAE is trained to recon-\nstruct the search region as it is, and to recreate the\ntemplate in the position found in the search region.\nZhang et al. (2023) presented I2P-MAE, a\nmethod designed to learn better 3D features by\nreconstructing masked point clouds. The approach\nleverages 2D pre-trained models to keep the impor-\ntant point tokens visible while masking. Moreover,\nthe 2D models are used to get the target represen-\ntations for a semantic reconstruction loss that is\napplied on the visible tokens, after the decoder.\nLin et al. (2023) combined self-supervised\nknowledge distillation and masked image modeling\ninto a single framework. In the proposed pipeline,\nthe teacher network processes an image from the\nsame class as the student network. The student\nnetwork processes a masked image, being trained\nto maximize the similarity between its class token\nand the teacher's class token. In addition, the stu-\ndent is trained to distill the knowledge of the most\nsimilar tokens of the teacher.\nThe study of Zhao et al. (2023) integrated the\nMAE pipeline into a teacher-student setting for\ndomain adaptive object detection. In this setup,\nthe student network has a dual focus: it learns the\ndetection task using labels generated by the teacher\nnetwork, and concurrently, it undertakes the recon-\nstruction of missing multi-scale features from the\ntarget images. This reconstruction aspect is pivotal,\nespecially when the availability of pseudo-labels\nfrom the teacher is limited, as it significantly\nimproves the model's adaptability to the target\ndomain, ensuring more robust and accurate object\ndetection performance.\nWei et al. (2023) evaluated the efficacy of using\nimage generation as a self-supervised pre-training\ntask, finding that it yields only marginal improve-\nments in downstream recognition tasks when\napplied within a diffusion model framework. In"}, {"title": "3.10 Masking Strategy and Model Architecture", "content": "A number of studies have influenced both the mask-\ning strategy and the employed architecture. The\narchitectural contributions consist of either inte-\ngrating MIM with different models or adaptations\nfor a specific scenario. These changes lead to dif-\nferent masking strategies that must be adapted. A\nnotable number of papers from this section have\nmultimodal inputs (Mizrahi et al., 2023; Chen et al.,\n2023; Lu et al., 2023; Bachmann et al., 2022).\nHuang et al. (2022) introduced a set of changes\nrequired to be done on the Hierarchical Vision\nTransformer architectures in order to be compati-\nble with the MAE framework, where the masked\ntokens are ignored from the input sequence. There\nare 2 problems when applied directly, one is the\nwindow attention with non-overlapped windows\nand the other is the convolutional and pooling lay-\ners. For the first issue, the authors' solution is to\ngroup the tokens from uneven windows with a novel\nOptimal Grouping algorithm and then apply the\nmask attention. For the second issue, they opted\nfor using sparse convolutions.\nWoo et al. (2023) implemented the MAE\nframework for convolutional networks. One of the\nchanges was to create the masks based on the deep\nfeature maps of the encoder and resize them to\nthe resolution of the input images. The second\nchange was also in the encoder, they used sparse\nconvolutional layers to keep the time performance\nimprovements brought by the masking.\nMizrahi et al. (2023) proposed a MAE frame-\nwork that can be used with multiple modalities.\nGiven a set of modalities, each is tokenized into a\ncommon representation form. Rather than using\nall tokens, only two subsets from each modality\nare sampled: one that is encoded and the other one\nthat is masked and then reconstructed. A common"}, {"title": "3.11 Masking Strategy and Objective Function", "content": "A body of works have contributed to both the\nmasking strategy and the objective function. These\nworks begin by either adopting a novel masking"}, {"title": "3.12 Masking Strategy and Downstream Task", "content": "Some studies that have been pivotal to both the\nmasking strategy and the downstream tasks which\nthey were applied to. These papers start by apply-\ning MIM on a new task, but due to the different\nnature of the studied task, the masking strategy is\nadapted as well.\nFu et al. (2023) introduced the TVC (text-\nguided video completion) task: the model needs\nto generate videos based on a subset of frames,\nwhile respecting some text instructions. Depending\non the subset of frames, the task requires either\nprediction (future), rewinding (past) or infilling\n(between two moments). The authors proposed a\ntraining strategy that is based on masking frames,\nwhich addresses all three possible TVC subtasks."}, {"title": "3.13 Target Features and Downstream Task", "content": "A number of papers have made significant advance-\nments to the targeted downstream tasks, which\nalso implied a contribution to the target features.\nMost of these works belong to the medical domain\n(Zhang et al., 2024; Cai et al., 2022; Kang et al.,\n2023), and the target features are chosen to be more\nrepresentative for the input data. Other papers\nin this category utilize videos as the input signal\n(Wang et al., 2023).\nCai et al. (2022) proposed a method to pre-\ntrain a model using MIM, which is able to process\nboth 2D and 3D ophthalmic images. The authors\ndeveloped a new module, called Unified Patch\nEmbedding, consisting of two branches, one for\neach data type. The module divides the inputs\ninto equal patches and then masks a great por-\ntion of them. Then, a common encoder computes\nthe latent representations of the visible patches.\nFinally, two decoders are employed: one that recon-\nstructs the patches, and another one that estimates\ntheir gradient maps (composed of horizontal and\nvertical edge maps). The experiments showed that\nthe method yields state-of-the-art performance in\nophthalmic image classification.\nMAGVLT (Kim et al., 2023) is a non-\nautoregressive generative visual-language trans-\nformer trained jointly for image-to-text, text-to-\nimage and image-text-to-image-text. The training\nobjectives are three mask prediction losses, one for"}, {"title": "3.14 Model Architecture and Target Features", "content": "Several papers have been influential in both the\nmodel architecture and the target features. The\nmodifications presented by these works are tightly\ncoupled. One the one hand, a target feature is\nproposed, and in order to accommodate it, architec-\ntural changes are adopted (Wei et al., 2022; Jiang\net al., 2023; Huang et al., 2023). On the other hand,\nthe other works are extending the original frame-\nworks (Dong et al., 2022; Lao et al., 2023), or even\nintroducing new pre-training frameworks based on\nMIM (Jiang et al., 2022; Yang et al., 2023).\nDong et al. (2022) proposed a couple of modi-\nfications to the original MAE pre-training. They\nused two decoders: one for image reconstruction\n(the original task) and one for feature representa-\ntion estimation. The latter one tries to predict the\nfeature representation of the masked patches, the\nground-truth coming from an EMA replica of the"}, {"title": "3.15 Objective function, Downstream Task and Theoretical Analysis", "content": "Two studies had an impact on the objective\nfunction and the downstream task, while also con-\nducting a theoretical analysis. These works begin\nby conducting a deep analysis about different MIM\naspects, and then they provide solutions in order\nto improve them.\nZhang et al. (2022) unveiled several theoret-\nical insights into the MAE paradigm. Initially,\nit uncovered a link between the MAE frame-"}, {"title": "3.16 Masking Strategy, Model Architecture and Objective Function", "content": "A handful of works contributed to advancements\nin the masking strategy, the model architecture\nand the objective function. Half of these papers\nintroduced a novel masking strategy, continuing\nwith improvements that are developed on top of it\n(Bandara et al., 2023; Yu et al., 2023). The other\nhalf is about initially transforming the input data,\nand then introducing all the modifications to apply\nMIM for the new input source (Gupta et al., 2023;\nQiu et al., 2024).\nAdaptive Masking (AdaMAE) (Bandara et al.,\n2023) presented an adaptive masking strategy for\nMAE performed by an additional neural network\nthat assigns greater masking probabilities to the\npatches containing spatio-temporal information\n(a.k.a. foreground). The new neural network gives\na vector of probabilities from which the sampling"}, {"title": "3.17 Masking Strategy, Target Features and Objective Function", "content": "Several papers that had an impact on the masking\nstrategy, the target features, as well as the objec-\ntive function. The contributions to the masking\nstrategies are highly varied: integrating new target\nfeatures (Yu et al., 2022; Li et al., 2022), improving\nthe masking policy based on guidance (Yao et al.,\n2023), and even leveraging multimodal data (Kwon\net al., 2023). The objective function of these works\nis modified as a result of the employed features.\nWithin the context of few-shot learning, Yu\net al. (2022) employed a masked autoencoder for\nreconstructing the latent embeddings rather than\nthe original image. Additionally, each instance acts\nas a patch, and thus, the input consists of more\nimages from the same class. After encoding the\ninput, a high portion of it is masked and the\ndecoder tries to reconstruct the masked embedding\nrepresentation (given some identification variables).\nIn is way, the backbone, i.e. the encoder, learns\nmore discriminative features and has a better\nfew-shot performance.\nYao et al. (2023) proposed to use MAE for\nreconstructing a normal estimation of the masked\ninput image and compare it with the original, in\norder to detect anomalies. The pre-training of the\nmodel follows the original MAE, only optimiz-\ning the masking strategy for contiguous blocks of\npatches. As the aim is to reconstruct the abnor-\nmal regions, during inference, a proposal masking\nunit is employed, estimating the likely locations\nof the image patches that contain anomalies in\norder to mask them. The unit is composed of a\nfeature extraction model that obtains the latent\nrepresentations of the input image patches and\nsome prototype normal images (one example from\neach normal class), as well as a normalizing flow\nmodel for computing the likelihood."}, {"title": "3.18 Masking Strategy, Model Architecture, Downstream Task and Objective Function", "content": "A number of papers have contributed on four\ndirections", "scope": "the masking\nstrategy", "employed": "a tokenized image (through\na"}]}