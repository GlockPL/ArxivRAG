{"title": "Masked Image Modeling: A Survey", "authors": ["Vlad Hondru", "Florinel Alin Croitoru", "Shervin Minaee", "Radu Tudor Ionescu", "Nicu Sebe"], "abstract": "In this work, we survey recent studies on masked image modeling (MIM), an approach that emerged as a powerful self-supervised learning technique in computer vision. The MIM task involves masking some information, e.g. pixels, patches, or even latent representations, and training a model, usually an autoencoder, to predicting the missing information by using the context available in the visible part of the input. We identify and formalize two categories of approaches on how to implement MIM as a pretext task, one based on reconstruction and one based on contrastive learning. Then, we construct a taxonomy and review the most prominent papers in recent years. We complement the manually constructed taxonomy with a dendrogram obtained by applying a hierarchical clustering algorithm. We further identify relevant clusters via manually inspecting the resulting dendrogram. Our review also includes datasets that are commonly used in MIM research. We aggregate the performance results of various masked image modeling methods on the most popular datasets, to facilitate the comparison of competing methods. Finally, we identify research gaps and propose several interesting directions of future work.", "sections": [{"title": "1 Introduction", "content": "Data have always played a crucial role in training large deep neural networks. Procuring a curated labeled dataset not only involves a great effort and a laborious process from a team of human annotators, but it also represents a significant expense. As a result, various self-supervised learning strategies have been explored, where the model is pre-trained with a different objective, which does not require human labels. Self-supervised learning can help the model to learn a rich feature representation, and even surpass supervised alternatives (He et al., 2022). Then, during the fine-tuning phase, the model is further optimized for a specific downstream task. For example, in most image classification tasks, a common approach is to train the network on the ImageNet dataset (Deng et al., 2009; Russakovsky et al., 2015), and then change the classification head for a new task and train the resulting model on a relevant dataset.\nSelf-supervised learning is a popular method to pre-train a deep learning model. It involves"}, {"title": "2 Generic Framework", "content": "Masked Image Modeling is an unsupervised technique that is usually applied during the pre-training phase. It involves masking some information, either from the input or from the latent representation, and then estimating the original data, as if the data would not have been concealed. Although many masked image modeling techniques have been proposed, the research has been focused on two main schemes, either reconstructing the masked signal, or comparing two latent representations, one for the unaltered input signal and one for the masked input. On a few occasions, different approaches are explored, but they are built on similar grounds. Therefore, in the following subsections, we aim to give a general formulation of the first two aforementioned schemes."}, {"title": "2.1 Reconstruction", "content": "The first scheme that we identified revolves around the idea of masking some piece of information at any stage during the forward pass of the model, and then employing a decoder to reconstruct the missing data. This pre-training technique was introduced in concurrent works by He et al. (2022) and Xie et al. (2022). Both works consist of an encoder based on a ViT architecture (Dosovitskiy et al., 2021), in which a large portion of the input tokens (resulting from linearly projecting the non-overlapping patches of the image) are masked. After the encoding stage, where masked tokens are substituted with a special token, a decoder is used to reconstruct them. In the Masked Autoencoder (MAE) proposed by He et al. (2022), the decoder is lighter than the encoder, which results in an asymmetric architecture. The loss function is applied only to the output corresponding to the masked tokens. When applied to a downstream task, the decoder is dropped, only the encoder being used as the backbone due to its strong feature representation capability.\nWe formally present the reconstruction-based training strategy in Algorithm 1. The first step of the algorithm splits the input images into non-overlapping patches, resulting in a set P that contains patches of the same size, namely of h x w pixels. The second step applies the masking operation, which can differ from one method to another. The masking operation indicates which patches should be kept and which should be masked. The"}, {"title": "2.2 Contrastive", "content": "The second generic scheme is represented by comparing two different latent representations of the same input. One latent representation corresponds to an unaltered or weakly augmented input image, while the other corresponds to a masked and strongly augmented version of the same input image. The approach is based on a contrastive learning framework, as illustrated in Figure 3. There are two common architectural configurations. One configuration uses an encoder with shared weights, as in (Wu et al., 2023; Zhang et al., 2023). The other configuration uses an encoder for the masked input and an exponential moving average (EMA) version of the encoder for the original input, as in (Lee et al., 2023).\nThe generic contrastive-based MIM framework is formalized in Algorithm 2. The aim of this framework is to obtain a similar embedding, irrespective of the applied masking. Steps 1 and 2 of the algorithm generate two versions of the input image by augmenting them at different intensities. The version that undergoes masking is strongly augmented, while the other one is weakly augmented (or can even remain unaltered). In steps 3-4, each image is divided into non-overlapping patches, and in step 5, masking is applied solely to the strongly augmented image. The unmasked image undergoes processing by the projection layer and the encoder (step 7), whereas the masked"}, {"title": "3 Taxonomy and Overview", "content": "In Figure 4, we present a manually-generated taxonomy of the most promising MIM papers, organizing them according to their main contributions. In constructing the taxonomy, we consider six main research directions related to: the masking strategy, the type of masked signals, the neural architecture, the objective function, the type of downstream tasks, and theoretical results. Nonetheless, since many papers focused on more than one of the above aspects, we grouped the respective works in distinct categories representing composed contributions. We now continue by presenting the aforementioned papers, divided into the sub-sections according our taxonomy."}, {"title": "3.1 Masking Strategy", "content": "Early masking strategies were based on selecting a high number of random patches and masking them. Recently, many alternative masking policies have been proposed to improve MIM. One suggestion is to guide the masking through different mechanisms, from leveraging image statistics (Xu et al., 2023) to using an additional simple network (Bandara et al., 2023) or even a self-attention module (Chen et al., 2023; Yang et al., 2023).\nXie et al. (2022) introduced SimMIM, a self-supervised pre-training framework that reconstructs pixel values of randomly masked images. The model is a simple encoder (ViT) receiving as input an image with masked patches replaced by a learnable token. Additionally, the study motivates the choice of random masking and the raw pixels as target features through extensive ablation studies.\nHe et al. (2022) presented the masked autoencoder (MAE) framework, where the main contribution is to exclude the masked tokens from the encoder's input and to use a very high masking ratio (75%). These changes imply a more efficient framework compared with other works, such as SimMIM (Xie et al., 2022). However, as a negative effect, having an encoder that operates only on visible tokens makes the framework incompatible by default with the Hierarchical Vision Transformer (Liu et al., 2021) architecture, which usually performs better than a standard ViT.\nPang et al. (2022) adapted the masked pre-training strategy to 3D Point Clouds. The method groups the points into patches and masks some of them. Then, it embeds and encodes only the unmasked patches. Next, the visible patches get concatenated with the masked tokens and passed through the decoder, with the objective to reconstruct the latter. The authors state that passing the masked patches earlier leaks spatial information which simplifies the task."}, {"title": "3.2 Target Features", "content": "The early popular frameworks either consist of reconstructing patches of the original image (He et al., 2022; Xie et al., 2022) or comparing high-level latent features (Chen et al., 2022). Subsequent works adopted different reconstruction targets that are more suitable for the downstream tasks (Fang et al., 2023; Hou et al., 2023), or demonstrated to learn richer feature representations (Bai et al., 2023). Finally, MIM has been employed for input signals other than image pixels, and some of these studies involved estimating a different feature type rather than the raw input signal, such as the statistics of a point cloud (Tian et al., 2023).\nChen et al. (2022) argued that reconstructing the image pixel space does not force the network to learn an ideal representation of the data. Thus, they introduced a self-distillation framework, in which the student branch follows the original MAE flow, while the teacher network (which is not updated by gradient descent, but rather from the weights of the student) only takes the masked patches. The objective is to match the high-level features between the two networks.\nLiang et al. (2022) proposed the masked image modeling pre-training for 3D meshes. Inspired by ViT, they begin by grouping together faces (each face being represented by a 10D vector) into a patch. Then, they adopt the transformer architecture, using the 3D location of the center of each patch for computing the positional embedding. The algorithm follows MAE: patches are randomly masked with a high masking ratio, the visible patches are passed through the encoder, then the resulting latent embeddings are concatenated with masked tokens (associated with the masked patches), and subsequently decoded. The objective is not only to reconstruct the masked patches (by predicting the coordinates of the vertices), but also the faces of the patch (the 10D representations).\nInspired by MAE, Assran et al. (2022) presented a self-supervised masking pre-training strategy that involves Siamese networks. Given a source image and applying transformations on it, two different views (anchor and target) are generated. Then, only the anchor's tokenized patches are masked, and using the Siamese networks (which follow the ViT architecture), both views are encoded. The latent representations are compared with a set of learnable prototypes in order to generate a distribution, the final goal being that of obtaining matching distributions (predictions of anchor and target). While the anchor's model is updated using gradient descent, the target's network parameters are computed as an exponential moving average from the anchor network's weights. The authors attest that the method greatly improves the performance in few-shot scenarios.\nGiven a labeled (source) and an unlabeled (target) 3D Point Cloud dataset, the aim of Liang et al. (2022) is to transfer the knowledge from the latter to the former by embedding information about common features in an encoder. This model is trained simultaneously on both datasets, but with different objectives. On the one hand, the source dataset is trained in a supervised setting on a specific task. On the other hand, points from the target dataset are randomly masked in arbitrary areas and the objective is to estimate their cardinality (number of points in the neighborhood), position and normal vectors. Therefore, the model benefits from"}, {"title": "3.3 Objective Function", "content": "The primary objective function of MIM is to reconstruct the masked pieces of information from the input signal. Thus, any paper that contributed in this regard, either by improving the reconstruction process or taking a different approach, is included in the category of reconstruction-based frameworks. Inspired by the reconstruction-based techniques, another approach has recently emerged, which applies a contrastive objective function. Furthermore, MIM was integrated in methods that showed great potential in leveraging out-of-distribution unlabeled data (Yu et al., 2022; Zhang et al., 2024), and even in adapting a model to the test data distribution (Gandelsman et al., 2022; Mirza et al., 2023).\nInspired by the MAE, Liu et al. (2022) proposed a similar pre-training framework for 3D Point Clouds, but substituted the reconstruction task with discrimination. A small proportion of points are left unmasked, and subsequently encoded. Then, a subset of the masked ones is sampled (real), along with some random 3D points from the space (fake), and the decoder's objective is to discriminate between the two. The encoded unmasked points are used in the cross-attention blocks of the decoder. Experiments are conducted for various downstream tasks, in which the method shows considerable performance improvements.\nWeers et al. (2023) studied the effectiveness of combining MAE and CLIP in a single framework. The conclusion is that the combination brings some benefit when training on a small dataset (tens of millions of samples), but this benefit is smaller or near zero when the experiments are carried out on a much larger dataset (1 billion samples)."}, {"title": "3.4 Downstream Task", "content": "Given the promising results of MIM and its ability to diminish the negative effects on low quantities of data, the pre-training strategy was eagerly applied in many visual tasks and related domains, ranging from image classification (Zhang et al., 2022; Jiang et al., 2023) and image generation (Bashkirova et al., 2023; Chang et al., 2022) to 3D point clouds (Shen et al., 2023), graphs (Jing et al., 2022), and even medical data (Bozorgtabar et al., 2023; Chen et al., 2023; Zhou et al., 2023).\nMaskGIT (Chang et al., 2022) trained a generative transformer to reconstruct randomly masked image patches. At inference time, it starts by predicting all the patches and keeps the most confident ones for the next iteration when the rest of the patches are again masked and regenerated. The process continues for a few iterations. Overall the pipeline has actually two stages, the first one encodes the patches into visual tokens with a VQ-Encoder, and the second stage (decoder) receives masked tokens for reconstruction.\nFeichtenhofer et al. (2022) proposed to use MAE on videos. The video is split into equally-sized patches that do not overlap along any dimension (i.e. including time), as well as consisting only of two timesteps. As hypothesized and demonstrated by the authors, a high masking ratio (about 90% of the whole input video, unaware of any axis) is used due to redundant data when decoding. Positional (i.e. height and width) and temporal (i.e. time) embeddings are added to the input tokens. The architectures of both the encoder and the decoder are based on ViT. The method follows the same"}, {"title": "3.5 Theoretical Analysis", "content": "Besides the applied contributions, some pieces of work take another approach: they theorize about various aspects of MIM and dive deeper into its fundamentals. The papers from this category address aspects such as overall understanding (Kong et al., 2023; Pan et al., 2023), the connection between MIM strategies (Zhang et al., 2022), or present certain drawbacks and how to overcome them (Huang et al., 2023; Xie et al., 2023).\nLiu et al. (2022) evaluated the performance of self-supervised learning (SSL) by determining whether the trained model represents enough information to obtain the data distribution, given additional information about the family distribution. The SSL task chosen for this assessment was the masked prediction task.\nLi et al. (2023) explored the effectiveness of MIM on out-of-distribution (OOD) detection. Their results showed that MIM improves the performance in multiple settings, such as one-class, multi-class or near-distribution OOD.\nKong et al. (2023) formulated the underlying data generation process as a hierarchical latent variable process. The authors discovered relationships between the latent variables of the data generation process and the masking parameters (masking ratio and patch size) of the MAE framework. These relationships allow MAEs to recover variables of different semantic levels. The authors validated their theoretical discoveries with several experiments, where the main result showed that very large masking ratios have a similar effect as low ratios, namely that of learning low-level information.\nXie et al. (2023) investigated the data scaling capabilities of masked image modeling. Their experiments use datasets of various sizes, ranging from 100.000 samples to 14 million samples. The authors verified their observations against two masked image modeling approaches, SimMIM (Xie et al., 2022) and MAE (He et al., 2022). The conclusions of their study state that MIM still necessitates data scaling to effectively facilitate model scaling. The study also noted that, in non-overfitting scenarios, simply increasing the number of unique samples does not necessarily enhance performance.\nXie et al. (2023) explored the differences in representations learned by deep models through MIM versus traditional supervised training. They observed that MIM encourages models to focus on local patterns across all layers, whereas supervised training emphasizes these patterns only in the initial layers. Additionally, MIM results in a greater diversity among the attention heads compared with supervised methods, suggesting a more nuanced feature recognition within the model.\nPan et al. (2023) theorized about the mechanisms behind reconstructing the masked input, the benefits of this pre-training strategy and why it learns valuable feature representations. The main finding is that discriminative features are learned during pre-training, and thus, when applied to a downstream task, these are further enhanced, which has a great advantage over randomly initialized weights."}, {"title": "3.6 Model Architecture", "content": "While the architecture employed throughout MIM research was consistent (a transformer-based encoder and a shallow decoder), there have been some important contributions that further enhance the performance of the pre-training task through architectural modifications (Liu et al., 2023). A few attempts have tried to distinguish themselves from the usual ViT-based models, either by using CNNs (Tian et al., 2023; Woo et al., 2023) or by integrating the MIM into the convolution operation (Madan et al., 2024; Ristea et al., 2022).\nRistea et al. (2022) presented the self-supervised predictive convolutional attentive block (SSPCAB), a novel block comprising a masked convolutional layer and a Squeeze-and-Excitation (SE) module. The filters of the masked convolutional layer contain learnable parameters in the corner regions of the receptive field and the masked region is located in the center. This novel block is trained using a self-supervised reconstruction loss, being integrated in anomaly detection networks. Later, Madan et al. (2024) introduced the self-supervised masked convolutional transformer block (SSMCTB) for anomaly detection. SSMCTB is an extension of SSPCAB, being trained via a self-supervised reconstruction loss and comprising a masked convolutional layer. In contrast to Ristea et al. (2022), Madan et al. (2024) employed a channel-wise transformer block instead of the SE module.\nMotivated by leveraging the masked pre-training strategy of autoencoders with convolutional layers, Gao et al. (2022) presented an architecture that combines them. The encoder consists of three stages: the first two are convolutional and the last one is transformer-based. First, a mask is sampled to determine which tokens are visible. The mask is then upsampled at the resolutions of the first two stages, in order to be used by masked convolutional blocks. Information from the first two stages is added to the resulting tokens of the third stage, and then, they are linearly projected. Finally, all tokens (predicted and masked) are decoded into the original pixel space. The authors state that the advantage of this method is represented by the multi-scale features learned by the encoder.\nSparseMAE (Zhou et al., 2023) offered a novel solution to the problem that small transformers face in not benefiting significantly from MAE pre-training. It did this by concurrently training a full-scale transformer alongside a smaller sparse network, which resides inside the full transformer. This smaller network is tasked with reconstructing the masked patches of input images. Uniquely, SparseMAE independently manages two sets of weights, one for the sparse network and another for the encompassing larger transformer. Despite this separation, both networks aim to achieve the"}, {"title": "3.7 Target Features and Objective Function", "content": "Several studies have contributed to both the target features and the objective function. The target features are obtained in various ways, mostly by using features deep encoders (e.g. CLIP or DI\u039d\u039f) (Dong et al., 2023; Wu et al., 2023). The objective function modifications revolve around integrating both reconstruction and contrastive losses.\nMaskCo (Zhao et al., 2021) is a region-level contrastive learning framework. It begins by augmenting images to create two distinct views of the same image, with one view partially masked. The model is then trained using contrastive learning to align the features of the masked regions with those of the corresponding regions in the unmasked view.\nZhu et al. (2022) proposed to integrate an additional masked contrastive objective into an RL pipeline dedicated to video games. Sequences from the video clips are sampled and then each frame is encoded using a CNN-based network. Then, besides the main RL policy network, an auxiliary branch is added that employs the student and teacher (exponential moving average of the student) framework. While the former receives masked input features, the latter is fed with the intact latent representation. The objective is to maximize the similarity between the two resulting embeddings.\nWang et al. (2023) showed that, in the previous masked image modeling approaches, it is difficult for the lower layers to learn inter-patch relations. To alleviate this issue, the authors introduced LocalMIM. They addressed the problem by using a loss function based on the weighted sum of the local patches losses. Their pipeline also includes a multi-scale reconstruction task, in which the model is supervised with different feature descriptors.\nMaskCLIP (Dong et al., 2023) combined masked image modeling and the CLIP contrastive loss between images and text in a single framework. Compared with vanilla CLIP, MaskCLIP has an additional loss for reconstructing a masked image and the two losses share the same visual encoder.\nHuang et al. (2023) presented a two stage distillation framework. In the first stage, the student network distills the task-agnostic knowledge of the teacher, and the authors chose MIM as a proxy task for this goal. Thus, the student learns to align its visible and masked patches representations with"}, {"title": "3.8 Model Architecture and Objective Function", "content": "A number of papers contributed to both the model architecture and the objective function. While the former is the main focus of most works, the latter contribution represents an extension resulting from the architectural modification.\nFei et al. (2023) enhanced the standard MAE pipeline by integrating a discriminator for adversarial training. Notably, this discriminator, which shares parameters with the MAE's encoder, is trained to distinguish between synthesized and real patches. This enhancement is an addition to the existing pipeline, with the typical reconstruction loss still being present in the training process.\nRather than following the standard approach in MIM with a reconstruction objective, Xue et al. (2023) proposed a unique loss function that aligns features extracted from visible tokens with those extracted by a teacher model across various architectural levels. To facilitate this alignment, this study introduced a novel module named Dynamic Alignment. This module is specifically designed to ensure compatibility between the two sets of features, enabling more effective feature alignment.\nInspired by masked autoencoders that process both visual and textual modalities, Fuller et al. (2023) adapted the masking pre-training framework for optical and radar inputs. Both modalities, after being aligned, tokenized and randomly masked, are encoded using an individual encoder, and then they are jointly processed by a multimodal encoder. The resulting embedding is decoded to reconstruct both input images. A contrastive loss between the mean embeddings of the individual encoders is adopted to match sensor measurements from the same timestamp, while maximizing the difference between those at different timestamps.\nIn their work, Wang et al. (2023) demonstrated how MAE can boost the performance of 3D medical image segmentation. The input volume (a 3D scan) is split into equal subvolumes and these are randomly masked. Then, different views (frontal, horizontal, or longitudinal) are obtained, and a further arbitrary rotation is applied. During pre-training, besides the main reconstruction objective for each view, three more losses are utilized: the rotation angle estimation, a contrastive loss, as well as an additional MSE loss between two different reconstructed views (after being normalized). The architecture of the encoder is based on Swin Transformer. A cross-attention module, which attends to the features between two views, is integrated before the first level of the decoder.\nIn the work of Gong et al. (2023), the audio spectrogram and the images are tokenized. The unmasked tokens are encoded with separate encoders, while also adding a corresponding modality embedding. Then, three separate forward passes are performed through a common encoder: one for each modality embedding, as well as the concatenation of the two. The concatenated tokens, together with the masked tokens, are decoded and the reconstruction loss is applied. Furthermore, a contrastive loss is applied between the averaged-pooled encoding of each modality.\nLiang et al. (2024) demonstrated that an additional auxiliary supervised classification task helps the MAE pre-training framework. Besides the main reconstruction loss, the authors integrated another branch that takes only a subset of the visible encoded tokens as input, applies an average global pooling operation on them, and finally predicts the class through a multi-layer perceptron. During the fine-tuning part, all tokens are used.\nThe contribution of Pei et al. (2024) is twofold. Firstly, they integrated MAE pre-training for videos by applying a consistency loss between two successive frames that are masked the same, then encoded and decoded with two different networks (one is an exponential moving average of the other). Secondly, they proposed a similar framework, but using sparse convolutional layers instead of a ViT architecture, which results in lower computational costs."}, {"title": "3.9 Masking Strategy and Target Features", "content": "Several studies have impacted both the masking strategy and the target features. In general, these two contributions are jointly proposed, since changing the target features involves a different masking strategy for the new input signal.\nZhao et al. (2023) presented a method for learning representations useful for object tracking. The method is based on MAE, but the authors use two inputs, one is the search region and the other one is the template. The MAE is trained to reconstruct the search region as it is, and to recreate the template in the position found in the search region.\nZhang et al. (2023) presented I2P-MAE, a method designed to learn better 3D features by reconstructing masked point clouds. The approach leverages 2D pre-trained models to keep the important point tokens visible while masking. Moreover, the 2D models are used to get the target representations for a semantic reconstruction loss that is applied on the visible tokens, after the decoder.\nLin et al. (2023) combined self-supervised knowledge distillation and masked image modeling into a single framework. In the proposed pipeline, the teacher network processes an image from the same class as the student network. The student network processes a masked image, being trained to maximize the similarity between its class token and the teacher's class token. In addition, the student is trained to distill the knowledge of the most similar tokens of the teacher.\nThe study of Zhao et al. (2023) integrated the MAE pipeline into a teacher-student setting for domain adaptive object detection. In this setup, the student network has a dual focus: it learns the detection task using labels generated by the teacher network, and concurrently, it undertakes the reconstruction of missing multi-scale features from the target images. This reconstruction aspect is pivotal, especially when the availability of pseudo-labels from the teacher is limited, as it significantly improves the model's adaptability to the target domain, ensuring more robust and accurate object detection performance.\nWei et al. (2023) evaluated the efficacy of using image generation as a self-supervised pre-training task, finding that it yields only marginal improvements in downstream recognition tasks when applied within a diffusion model framework. In response to this observation, the authors presented a novel strategy that merges MAE with diffusion models for self-supervised pre-training, focusing specifically on an inpainting task. This approach demonstrated competitive performance, aligning closely with state-of-the-art methods in image recognition, thus offering a compelling alternative to enhance pre-training effectiveness.\nZhang et al. (2023) utilized the mask-reconstruction strategy in 3D segmentation due to the lack of supervised data, as well as the domain difference between training and testing data. During training, given a pair composed of a 2D image and a 3D point cloud, patches from one modality are masked and the model tries to estimate them using the other modality. A CNN backbone with a lower masking ratio is employed. Having two different datasets (source-labeled and target-unlabeled), the MIM is performed on both datasets, while the supervised task is performed only on the former.\nInspired by the MAE framework, Song et al. (2023) proposed to boost the performance of a ViT model used for object tracking (given an object in a template image, find the same object in the search image) by applying MIM as an additional concurrent task. Both input images are concatenated and passed through the encoder. Besides the main task head, two more decoders are employed. After a high portion of the embeddings is masked, the two sets (each corresponding to an image) are separately fed through one decoder to reconstruct the two frames. The other decoder only receives as input the token embeddings of the search image and reconstructs the template image."}, {"title": "3.10 Masking Strategy and Model Architecture", "content": "A number of studies have influenced both the masking strategy and the employed architecture. The architectural contributions consist of either integrating MIM with different models or adaptations for a specific scenario. These changes lead to different masking strategies that must be adapted. A notable number of papers from this section have multimodal inputs (Mizrahi et al., 2023; Chen et al., 2023; Lu et al., 2023; Bachmann et al., 2022).\nHuang et al. (2022) introduced a set of changes required to be done on the Hierarchical Vision Transformer architectures in order to be compatible with the MAE framework, where the masked tokens are ignored from the input sequence. There are 2 problems when applied directly, one is the window attention with non-overlapped windows and the other is the convolutional and pooling layers. For the first issue, the authors' solution is to group the tokens from uneven windows with a novel Optimal Grouping algorithm and then apply the mask attention. For the second issue, they opted for using sparse convolutions.\nWoo et al. (2023) implemented the MAE framework for convolutional networks. One of the changes was to create the masks based on the deep feature maps of the encoder and resize them to the resolution of the input images. The second change was also in the encoder, they used sparse convolutional layers to keep the time performance improvements brought by the masking.\nMizrahi et al. (2023) proposed a MAE framework that can be used with multiple modalities. Given a set of modalities, each is tokenized into a common representation form. Rather than using all tokens, only two subsets from each modality are sampled: one that is encoded and the other one that is masked and then reconstructed. A common encoder of all input types is adopted, nevertheless, a modality embedding is added to the tokens. During the cross-attention layers in the decoder, the embeddings corresponding to one modality are masked from the rest while attending all resulting tokens from the encoder. Besides demonstrating good results on downstream tasks, the method shows promising cross-modality generative capabilities.\nLu et al. (2023) combined two sources of information (Hematoxylin and Eosin and Immunohistochemical staining images) to detect breast cancer, adopting MAE as the base for their method. Both images are split into patches, some of which will be randomly masked, and the remaining visible patches are fed together through a ViT-based model. The resulting embeddings, together with the learnable mask embeddings, are further processed by two self-attention modules (one specific for each modality), as well as a cross-attention module (that is fed all embeddings). Finally, two separate decoders reconstruct the original images, each receiving the modality-specific embeddings, as well as the inter-modal ones.\nLi et al. (2023) proposed a novel two-stage pre-training framework for video foundation models. The initial stage focuses on training the model to align the features extracted from masked frames with those derived from an image foundation model on unmasked frames. In the subsequent stage, the authors introduced a text encoder and a cross-modality decoder to further train the model for video-text matching and masked language modeling, while maintaining the training objective employed in the first stage.\nPiMAE Chen et al. (2023) is a self-supervised framework based on MAE, which learns representations that capture interactions between point clouds and images. Overall, the approach is based on the usual reconstruction objective for each modality. However, in contrast to MAE, the masking strategy in this case is designed to be complementary between the two modalities. In terms of architecture changes, the encoder and decoder include some common blocks between the two modalities, but they also have modality specific layers."}, {"title": "3.11 Masking Strategy and Objective Function", "content": "A body of works have contributed to both the masking strategy and the objective function. These works begin by either adopting a novel masking"}, {"title": "3.12 Masking Strategy and Downstream Task", "content": "Some studies that have been pivotal to both the masking strategy and the downstream tasks which they were applied to. These papers start by applying MIM on a new task, but due to the different nature of the studied task, the masking strategy is adapted as well.\nFu et al. (2023) introduced the TVC (text-guided video completion) task: the model needs to generate videos based on a subset of frames, while respecting some text instructions. Depending on the subset of frames, the task requires either prediction (future), rewinding (past) or infilling (between two moments). The authors proposed a training strategy that is based on masking frames, which addresses all three possible TVC subtasks.\nZhu and Liu (2023) argued that MIM alone is not sufficient for downstream tasks such as geometric matching. Therefore, they proposed paired MIM, which reconstructs pairs of masked images instead of a single masked image. Their study demonstrated that this pre-training task is more effective for geometric matching, because such tasks require the correlation between two images.\nLEMART (Liu et al., 2023) is an effective pre-training framework when applied on image harmonization as a downstream task. In this approach, the masked patches are replaced with the patches taken from a perturbed version of the original image. The research also investigated what is the best strategy for creating the masks, concluding that random masking works best for image harmonization.\nLi et al. (2023) presented a framework for representation learning and image generation. The applied pre-training method is similar to MAE He et al. (2022), but the tokens are given by a VQ-GAN tokenizer and the masking ratio is variable.\nCai et al. (2023) used masked autoencoders to learn rich, generic, transferable and robust facial representations from face videos. The masking prioritizes specific tokens (those containing eyes, nose, mouth and hair). The learned representations are then tested on downstream tasks, such as facial"}, {"title": "3.13 Target Features and Downstream Task", "content": "A number of papers have made significant advancements to the targeted downstream tasks, which also implied a contribution to the target features. Most of these works belong to the medical domain (Zhang et al., 2024; Cai et al., 2022; Kang et al., 2023), and the target features are chosen"}]}