{"title": "Discriminative Fine-tuning of LVLMs", "authors": ["Yassine Ouali", "Adrian Bulat", "Alexandros Xenos", "Anestis Zaganidis", "Ioannis Maniadis Metaxas", "Brais Martinez", "Georgios Tzimiropoulos"], "abstract": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a \"bag of words\" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.\nIn this work, we propose to combine \u201cthe best of both worlds\": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.\nOur contributions include (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.", "sections": [{"title": "1. Introduction", "content": "Contrastively-trained Vision Language Models (VLMs)\n(e.g. CLIP [36]) have become the predominant direction\nfor vision-language representation learning, exhibiting re-\nmarkable zero-shot abilities [21, 28, 31, 36, 48]. However,\nthe great success of these models in many vision-language\nand vision tasks, even in a zero-shot manner, \u201csweeps under\nthe rug\u201d some of their important limitations. Specifically,"}, {"title": "2. Related work", "content": "Inspired by breakthrough research in language modeling [5,\n22, 40, 42], a series of methods seek to combine pretrained\nLLMs and vision encoders to construct Large Vision Lan-\nguage Models (LVLMs) capable of processing image-text\ndata jointly [3, 11, 32, 34, 35, 44, 45, 50]. The prevalent\nstrategy consists in aligning the features produced by a pre-\ntrained vision encoder to the textual space assumed by a pre-\ntrained LLM using a projection module, e.g. LLaVA [35],\nfollowing a two-stage alignment procedure. Follow-up\nworks expand this to interleaved image-text data [1, 27]\nand multiple input crops [1] while seeking to improve the\nmodel's efficiency [10].\nDespite their strong generative and comprehension abil-\nities [34], current LVLMs are primarily restricted to gen-\nerative tasks. Only very recently, Jiang et al. [23], in-\nspired by the recent progress in NLP [4, 24] adapted a\nLLaVA-NeXT [26] model to discriminative tasks with the\nhelp of a contrastive-like loss, using text data only. We\nnote that unlike [23], we introduce a training framework\nthat learns from multi-turn image-text pairs (as opposed to\ntext only) using a novel formulation that jointly combines\na contrastive loss with a next-token prediction, reflecting\nthe data characteristics and inducing a gradual representa-\ntion buildup. Moreover, we compare our approach with E5-\nV [23], significantly improving upon their results despite\nusing smaller/lighter models."}, {"title": "2.2. Discriminative Vision-Language Models", "content": "The prevalent approach for training Discriminative VLMs\nfollows the two-tower contrastive approach pioneered by\nCLIP [36], whereby an image and text encoder are trained\non web-collected image-text pairs to learn a joint multi-\nmodal (i.e., vision and language) space. Subsequent works\nbuild upon CLIP by scaling the data [37, 48], improving the\narchitecture using late/early interactions [29] or improving\nthe training loss [7, 48]. Despite their remarkable zero-\nshot and representation learning abilities [36] such mod-\nels were shown to have significant shortcomings related to\nlimited language understanding capabilities, including: lack\nof compositionality understanding [25], manifesting bag of\nwords behavior [47], struggling with spatial relations [25],\nbeing susceptible to typographical attacks [17], etc. Recent\nworks aim to address these shortcomings by constructing\nsynthetic hard negatives [47] or performing cross-modality\nattention [29]. However, the former does not inherently\nchange the model's behaviors and has been shown to poten-\ntially learn a series of shortcuts/artifacts [19]. Meanwhile,\nthe latter is impractical for deployment at scale, as, due to\nthe interactions between the encoders, each new query in-\ncurs an additional inference for every image within the set.\nTo alleviate these shortcomings and improve the overall\ncapabilities of such models, we depart from the prevalent\napproach of training VLMs using a contrastive loss and, in-\nstead, propose a new approach that seeks to convert gener-\native LVLMs into discriminative models by adapting them\nusing a newly proposed framework that combines genera-\ntive and discriminative objectives."}, {"title": "3. Method", "content": "The following sections detail our approach which we call\nVladVA: Vision-Language Adaptation for Discriminative\nVisual Assistant."}, {"title": "3.1. LVLMs as zero-shot discriminative models", "content": "LVLMs consist of an LLM \u03a6t, a vision encoder \u03a6, and\na module g that projects the vision features into the LLM's\ntextual space. Once fine-tuned, such models can produce\na textual answer xa = \u03a6t(g(\u03a6\u03c5(xv)), xq) when presented\nwith an input image x, and a text query (or prompt) xq.\nDespite being solely trained with an autoregressive next-\ntoken prediction loss on limited amounts of data (< 5M),\nsuch models can act as multi-modal discriminative mod-\nels in a zero-shot manner [23]. To elicit this capability, the\nimage embedding f\u2081 = \u03a6t(g(\u03a6\u2082(xv), xp) [eos] is obtained\nby passing the image alongside a handcrafted prompt xp\nthrough the LVLM and taking the last token, prior to any\ngeneration, as the reference feature. Analogously, the text"}, {"title": "3.2. Discriminative fine-tuning of LVLMs: from\ngeneration to discrimination", "content": "Despite exhibiting surprising innate zero-shot abilities,\nLVLM's direct discriminative performance lags behind that\nof state-of-the-art contrastively trained VLMs. Hence, care-\nfully designed frameworks are needed to unlock the full po-\ntential of such models. This is the very goal of our work:\nto introduce a well-grounded image-text adaptation/training\nframework that surfaces the discriminative vision-language\ncapabilities of a generative LVLM.\nNotably, our findings contradict those of the very re-\ncent (concurrent) work of [23], which found that contrastive\nimage-text fine-tuning is detrimental and limits training to\ntext-text contrastive learning alone. This highlights the\nimportance of our proposed approach, which overcomes\nsuch impediments and significantly boosts the discrimina-\ntive performance of the model.\nHaving established the architecture in the previous sec-\ntion, the two other pillars are the data and training strategy.\nData strategy: We argue for the importance of data diver-\nsity in terms of granularity and group captions according to\ntheir length: short captions (< 30 tokens) and long captions\n(30-500 tokens). The short captions capture coarse details\nand summarize image content, which teaches the model to\ndiscriminate with regard to the high-level information of an\nimage. Longer captions capture finer details in images and\npromote a better understanding of language concepts such\nas spatial relationships and compositionality. For a strong\ndiscriminative model, both are necessary. Therefore, for\nimages missing either caption type, we use a BLIP2 [29]\ncaptioner to generate short captions and ShareGPT-4V [8]\nto generate long captions. This allows us to leverage both\nsupervisory signals for training.\nTraining strategy: As we demonstrate in this work, the\nvariable length of the training data poses its own challenges:\nunlike the case of short captions, where direct training using\nthe well-studied contrastive loss performs well, it collapses\nfor longer captions. This brings us to the proposed train-\ning strategy, whereby, to address this challenge, we propose\na well-motivated hybrid training approach that combines a\ncontrastive loss (see Sec. 3.2.1) and a next-token predic-\ntion loss for discriminative adaptation (see Sec. 3.2.2). Fi-\nnally, as full model fine-tuning is computationally expen-\nsive, in Sec. 3.3, we detail a fine-tuning strategy that com-\nbines adapters with soft prompting."}, {"title": "3.2.1. Image-text contrastive alignment", "content": "Under a multi-modal contrastive formulation, the image\nand text representations, f and ft respectively, must be\nclose if they are semantically similar and far apart other-\nwise, under a specified distance metric. At train time, this\nis enforced using a symmetric image-text and text-image\ncontrastive loss, which, for a given mini-batch containing b\nrandomly selected samples, can be described as:\n$L_c = \\sum_{k=1}^b (-\\log \\frac{\\exp(s_{kk})}{\\sum_j \\exp(s_{kj})} - \\log \\frac{\\exp(s_{kk})}{\\sum_j \\exp(s_{jk})}),$ \n(1)\nwhere $s_{kj} = cos-sim(f_k, f_j)$ denotes the cosine similar-\nity between the k-th image and the j-th caption (image-to-\ntext), and similarity, $s_{jk}$ is the text-to-image similarity.\nDuring training, the contrastive loss is applied to the very\nsame tokens used for the zero-shot evaluation, as they rep-\nresent the optimal starting point for further fine-tuning. We\nnote that the contrastive loss is mostly suitable for train-"}, {"title": "3.2.2. Autoregressive training for learning discriminative\nLVLM representations", "content": "Until now, the modality-specific embeddings are obtained\nby taking the last token, prior to any generation, while the\ntraining is largely focused on short (i.e. < 30 tokens) cap-\ntions, mimicking the CLIP-style data used for contrastive\ntraining. This contrasts with the LLaVA-style autoregres-\nsive training, where long and highly descriptive captions\n(typically 200-500 tokens) are used to help the LVLM learn\nstrong links between the vision and text domains, pay atten-\ntion to fine-grained details, and develop strong reasoning\nand compositionality capabilities.\nAs noted earlier, directly using the long captions with\nthe contrastive loss is ineffective, as, due to the high speci-\nficity of the long captions, the task is easy and nearly trivial\nto solve, with the loss going to 0 in just a few hundred it-\nerations. To address this, we propose to instead apply the\nnext-token prediction loss over the long captions:\n$L_{LCE} = \\sum_{i=1}^L \\log p_\\theta(u_i|X_v, X_p, X_{q,<i}),$ (2)\nwhere L is the length of the long caption, xq, x the input\nimage and xp the prompt, and pe as the next-token proba-\nbility distribution produced by the model at input token ui.\nIntuitively, this formulation possesses multiple advan-\ntages: (1) It allows the model to learn from long captions,\nas predicting each and every token correctly is a challeng-\ning task (as opposed to applying the contrastive loss to long\ncaptions). (2) The decoding process encourages the con-\ndensation of information into the starting token used as a\nfeature embedding. (3) It offers an avenue for retaining the\ngenerative capabilities of the model while strengthening its\ndiscriminative abilities."}, {"title": "3.2.3. Overall training loss", "content": "We apply the next-token prediction loss over the long cap-\ntions and the contrastive loss over the short ones in a uni-\nfied manner. During training, the templates presented to the\nLVLM for the image and, respectively, text modality take\nthe following form:"}, {"title": "3.3. Parameter Efficient Adaptation", "content": "As direct fine-tuning of the LVLM is costly, especially\nwhile attempting to maintain a reasonably large batch size\nfor contrastive learning, herein we explore and adopt soft-\nprompting combined with LoRA adapters, both trained un-\nder the same loss formulation of Sec. 3.2.\nSoft prompting was recently proposed as an efficient task-\nadaptation approach for both LLM [30] and CLIP [6, 49]\nmodels, representing a direct departure from the prompt\nhand-crafting solution. Specifically, for a given input\nmodality, i.e. image and text, we define a set of n\nmodality(m)-specific learnable vectors [$v^m_1, v^m_2, ....,v^m_n$],\nv \u2208 RC with C denoting the model's vocabulary embed-\nding size. These vectors can be inserted across the input\nsequence to adjust the model's behavior. In practice, we\nopt to replace the tokens belonging to the hard prompts (i.e.\nxp; see Sec. 3.1) with the learnable vectors, initializing their\nvalues with the embeddings of the handcrafted ones.\nAdapter fine-tuning: While efficient, the representation\npower of the soft prompts is somewhat limited. Hence, fol-\nlowing best practices, we also attach LoRA [20] adapters\nto the linear layers located inside Pt. Such adapters offer a\nmultifold advantage: lower memory requirements, reduced\npotential of overfitting during training, and no additional\ncompute requirements during inference.\nThe model is fine-tuned using these components. Impor-\ntantly, both have a positive impact on overall accuracy."}, {"title": "3.4. How does the model's behavior change?", "content": "Building upon the analysis from Sec. 3.1, we show that our\ntraining approach elicits the following behavioral changes:\n(1) The summary token - vision tokens' attention increases\nin density. (2) Both the entropy of the output distribution of\nthe summary token and the spread of the cumulative vari-"}, {"title": "4. Experiments", "content": "We compare our approach with the current state-of-the-art\non two tasks of interest in a zero-shot manner: image-text\nretrieval and compositionality/language understanding.\nModels compared: We compare with state-of-the-art mod-"}, {"title": "4.1. Zero-shot image-text retrieval", "content": "We test our approach on the standard Flickr30k [46], MS-\nCOCO [33] and nocaps [2] datasets, containing 1,000,\n5,000 and 15,100 test samples respectively. For the latter,\nwe simply average the results on the three partitions.\nAcross all three datasets, our approach significantly sur-\npasses the current state-of-the-art models, including mod-\nels of similar size. It even outperforms the much bigger\nEVA-CLIP (18B) model (85.0% vs. 83.3%) on Flickr30k,\n(59.0% vs. 55.6%) on MS-COCO and (72.3% vs. 69.3%)\non nocaps in terms of @R1 for image retrieval. Similarly,\nwe also outperform the LVLM-based E5-V model by 5.5%\non Flickr30k, 6% on MS-COCO, and 6.2% on nocaps."}, {"title": "4.2. Image-text compositionality", "content": "Compositionality benchmarks construct hard negatives that\nelicit various types of compositional failures. Herein, we\nfocus our comparison on the currently most challenging\ntest sets, SugarCrepe [19] and SugarCrepe++ [16] (For\nWinoground [41] please see supp. material). For Sugar-\nCrepe++, we are primarily interested in the Image-to-Text\n(ITT) setting since the Text-to-Text (TOT) setting evaluates\nthe language component of the methods only.\nshow, our approach is the best in\nboth SugarCrepe and SugarCrepe++ (ITT). On SugarCrepe,\nwe outperform the 18B EVA-CLIP model on all categories,\nwith particularly large gains on relation replacement (76.1\nvs. 86.8), attribution adding (85.0 vs. 95.8), and object\nswapping (65.3 vs. 85.8). The last case is particularly in-\nteresting as it directly measures the bag-of-words behav-\nior, showcasing the significant improvements our method\nachieves in this direction. Additionally, we outperform the\nE5-V variant based on the same LLaVA-1.5-7B model that\nwe used, and the one based on the heavier LLaVA-Next-\n8B. A similar trend is observed on SugarCreppe++ where\nwe outperform EVA-CLIP (18B) by up to 10.9% (on object\nswapping) and E5-V (ITT) in all but relation replacement.\nNote that, thanks to its text-text training, E5-V surpasses\nour method for the TOT setting, but we note that their loss\ncan be readily incorporated into our framework, leaving this\nfor future work."}, {"title": "5. Ablation studies", "content": "We quantify the impact of the proposed method's compo-\nnents by training on a smaller 1M subset, reporting results\non SugarCrepe and on Flickr30k."}, {"title": "5.1. Impact of method's components", "content": "We quantify the impact of the proposed method's compo-\nnents by training on a smaller 1M subset, reporting results\non SugarCrepe and on Flickr30k.\nImpact of adaptation components: We start by measur-\ning the impact of the efficient adaptation strategy based on\nsoft prompting and adapter-finetuning. For simplicity, we\nablate this by training using only the contrastive loss. As\nthe results from Tab. 4 show, both components, individu-\nally and jointly, provide notable gains on top of the original\nLLaVA-1.5-7B model (i.e. the case of no adaptation).\nWhile LoRA fine-tuning performs better than soft-\nprompting (due to its bigger capacity), the latter alone\nperforms surprisingly well. To understand why, we analyze\nthe changes the soft prompts undergo by finding the closest\nembedding in the LLM's vocabulary. This results in the\nfollowing decoded sentences: \u201c</s> '<Summarize\nthe provided image in one word:/ $ [", "one\nword": ". The two sentences remain unchanged seman-\ntically, with the only characters changed being the ones at\nthe start and end of the prompt. Intuitively, this allows the"}, {"title": "6. Conclusions", "content": "We introduced a new framework for adapting a generative\nLVLM into a discriminative model, unlocking its innate\ncapability for powerful image-text discrimination and en-\nhanced language understanding. Our framework uses both\nshort and long captions for training the LVLM with con-\ntrastive and next-token prediction losses respectively. We\nalso presented a parameter-efficient adaptation method us-\ning a combination of soft prompting and LoRA adapters.\nFinally, we showed that our approach results in significant\nimprovements over state-of-the-art models of similar size\nfor image-text retrieval and compositionality benchmarks."}, {"title": "7. Results for additional model sizes and archi-\ntectures", "content": "To further showcase the generalizability of our approach,\nherein we report results on two additional models: LLaVA-\n1.5-13B [34] and Qwen2-VL-2B [43]. The 1st is a scaled-\nup version of the LLaVA-1.5-7B [34] used in the main\nmanuscript and tests the scalability of our approach with\nsize. The second follows a different architecture and train-\ning procedure and has \"only\" 2B parameters, testing both\ngeneralizations to different architectures and finetuning in a\nlower-parameters regime. on all 6 datasets (i.e. Flickr, coco, nocaps, Sug-\narCrepe, SugarCrepe++ and Winoground) for both retrieval\nand compositionality, in all cases we significantly improve\nupon the original zero-shot model performance, showing\ngood scalability with size in both directions, i.e. for smaller\nand bigger models."}, {"title": "8. Compositionality evaluation on Winoground", "content": "In addition to the results from the main paper, herein, we\nreport results on Winoground [41], a curated dataset con-\nsisting of 400 images with difficult/unusual scenarios that\ngo beyond compositionality and largely act as a natural ad-\nversarial set [13, 47]. our\napproach matches and outperforms prior models, including\nthe large 18B EVA-CLIP model (17.5 vs. 15.0, 40.5 vs.\n35.8 and 12.8 vs. 10.5, for image, text and respectively\ngroup set)."}, {"title": "9. Zero-shot image recognition on ImageNet", "content": "From an evaluation point of view, the main focus of this\nwork is on improved zero-shot retrieval and, more generally,\nimproved vision-language compositional ability. We focus\non these tasks, as they require stronger (vision-)language\nunderstanding abilities, which we show an LVLM can offer\nunder appropriate training regimes. As a study case, herein,\nfor completeness, we also measure the zero-shot ability of\nthe model for image recognition on ImageNet [12]. As the\nresults from Table 9 show, our approach significantly im-\nproves upon the zero-shot LVLM we start from (54.7 vs\n70.6%). In comparison, E5-V approach only offers modest\nperformance gains (45.8 vs 48.2%) and has notably lower\nperformance than our approach (48.2 vs 70.6%) despite us-\ning a bigger model. While significantly improving upon the\nmodel we start from, the low data regime we train our model\nin (only 8.1M samples) limits its overall performance, with\ncontrastive models trained on billion samples performing\nbetter. This is expected as the image recognition ability of\na model, especially on the highly specific categories of Im-\nageNet, will depend on how often (if at all) they are seen\nin the training set. This is especially significant given that\nmany of the datasets used for contrastive learning are fil-\ntered based on the ImageNet classes [36]. In lower data\nregimes, comparable with ours, we can observe that our\napproach produces notably better results (e.g. 51.1% for\nFFF [7], trained on 15M samples vs 70.6% for ours). Fi-\nnally, when comparing it with other models focusing on\nretrieval (i.e. BLIP and BLIP2) our approach outperforms\neither of them by more than 15% in absolute terms despite\nthe fact that these models were trained on 129M samples.\nAll in all, we outperform all models trained in comparable\nsettings, showing promising initial results in this direction\ntoo."}, {"title": "10. Qualitative text generation examples post\ndiscriminative adaptation", "content": "Our main objective is to convert generative LVLMs into dis-\ncriminative ones, hence the proposed approach is designed\nfrom the perspective of maximizing the discriminative abil-\nities of the model. Still, it may be interesting to qualitatively\nsee how our model, and the closest relevant approach E5-V\nbehave. We note, that in principle both our approach and\nE5-V use LoRAs adapters, hence it is easy to switch be-\ntween the discriminative and the generative mode without\ncompromising either, by enabling or disabling the adapters.\nThat being said, herein we present some qualitative exam-\nples post-training, so we can see the direct effect the training\nhas on the model. As the results from Fig.7 show, generally,\nour approach better retains the generative capabilities of the\nmodel post-training, producing fine-grained captions, sim-\nilar with the original ones. In contrast, E5-V appears to"}]}