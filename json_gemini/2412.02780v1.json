{"title": "WxC-Bench: A Novel Dataset for Weather and Climate Downstream Tasks", "authors": ["Rajat Shinde", "Christopher E. Phillips", "Kumar Ankur", "Aman Gupta", "Simon Pfreundschuh", "Sujit Roy", "Sheyenne Kirkland", "Vishal Gaur", "Amy Lin", "Aditi Sheshadri", "Udaysankar Nair", "Manil Maskey", "Rahul Ramachandran"], "abstract": "High-quality machine learning (ML)-ready datasets play a foundational role in developing new artificial intelligence (AI) models or fine-tuning existing models for scientific applications such as weather and climate analysis. Unfortunately, despite the growing development of new deep learning models for weather and climate, there is a scarcity of curated, pre-processed machine learning (ML)-ready datasets. Curating such high-quality datasets for developing new models is challenging particularly because the modality of the input data varies significantly for different downstream tasks addressing different atmospheric scales (spatial and temporal). Here we introduce WxC-Bench (Weather and Climate Bench), a multi-modal dataset designed to support the development of generalizable Al models for downstream use-cases in weather and climate research. WxC-Bench is designed as a dataset of datasets for developing ML-models for a complex weather and climate system, addressing selected downstream tasks as machine learning phenomenon. WxC-Bench encompasses several atmospheric processes from meso-\u1e9e (20 - 200 km) scale to synoptic scales (2500 km), such as aviation turbulence, hurricane intensity and track monitoring, weather analog search, gravity wave parameterization, and natural language report generation. We provide a comprehensive description of the dataset and also present a technical validation for baseline analysis. The dataset and code to prepare the ML-ready data have been made publicly available on Hugging Face - https://huggingface.co/datasets/nasa-impact/WxC-Bench.", "sections": [{"title": "Background & Summary", "content": "Weather and climate variability influences all spheres of human activity including the economy, transport, and energy production. Extreme events such as hurricanes, wildfires, droughts, and floods often result in catastrophic damage to human lives and capital. Since 1980, the U.S. alone has incurred damages worth more than $2.5 trillion from weather and climate disasters [1]. Current forecast models can reliably predict atmospheric extremes only up to 14-15 days in advance with forecast skill decreasing with time\u00b9. In the past, such limits of predictability were believed to be improved mostly by recording more comprehensive set of observations, and through improving traditional models. However, the past couple of years have witnessed the emergence of a radically new, AI-driven approach.\nArtificial Intelligence driven weather forecasting models deliver accurate weather predictions more quickly and cost-effectively than traditional numerical models. Notable examples of these models are FourCastNet [2], Graphcast [3], FengWu [4], Pangu [5], and MetNet [6]. Machine Learning (ML) models also hold potential for longer-term subseasonal-to-seasonal modeling through efficient ensemble generation or otherwise [7]. Additionally, ML-based methods are being explored to improve climate prediction [8], development of ML-driven parameterizations [9, 10, 11, 12], bias correction [13], and understanding climate change impacts [14, 15]. Recently, AI-Numerical Weather Prediction (NWP) models, primarily using transformer-based architectures, blended with Fourier Neural Operators [2], Clifford Neural Operators [16] (NOs), and Graph Neural Networks have been introduced. These techniques aim to learn complex, nonlinear mappings required for approximating Partial Differential Equations (PDEs) that capture the inherent atmospheric physics.\nUnlike traditional NWP models, which are widely used to study weather extremes, ozone recovery, air quality monitoring, parameterization development, boundary layer turbulence, etc., these pioneering models are typically trained on large volumes of data, but only to perform one specific task: medium-range weather forecasting. This can severely limit their broader applicability, explaining the rising push to develop general-purpose AI models.\nRecent models such as Prithvi WxC [17], ClimaX [18], Aurora [19], and Prithvi [20] have been developed as a generalist AI-system capable of performing multiple tasks. These generalist models are termed foundation models (FMs) for earth sciences. FMs [21], which were first proposed in the fields of natural language processing (NLP) and computer vision (CV), are general-purpose AI models that replace specialized task-specific models across a wide range of applications. FMs are trained"}, {"title": "Downstream Task Descriptions and Datasets", "content": "This paper presents a compilation of 6 ML-ready datasets for developing task-specific ML models. The tasks are selected to represent a broad scale of the atmospheric processes and challenges (Table 2) while also having useful applications for research and day-to-day human activities (See Relevance column of Table 2). Some tasks, such as gravity wave (GW) parameterization,"}, {"title": "Aviation Turbulence Prediction", "content": "Aviation Turbulence in the lower and middle atmosphere presents risks for passenger and cargo airliners, especially when it is encountered unexpectedly [29, 30]. Turbulence, however, is a microscale feature that defies direct prediction by numerical weather prediction models with even relatively fine grid-spacing on the order of 1 Km. While machine learning techniques have been explored for turbulence prediction based on information from numerical weather prediction or onboard sensors [31, 32, 33], deep learning (DL) techniques remain under-investigated. Thus, there is a scope for sophisticated data-driven approaches to improve turbulence forecasts for aviation, which can be supported by this dataset.\nThe training dataset prepared for aviation turbulence detection is derived from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2) re-analysis data set [34]. MERRA-2 is available on a 0.625\u00b0\u00d70.5\u00b0 grid resolution with 72 hybrid model vertical levels and 42 output pressure levels. With this grid spacing, MERRA-2 resolves synoptic scale features such as frontal zones and midlatitude cyclones, which can impact turbulence production but not fine-scaled processes such as boundary layer growth. MERRA2 is used to provide temperature, relative humidity, zonal and meridional winds, vertical motion, orography, and geopotential height. These variables are retrieved for the lowermost 40 pressure levels, spanning from the surface to 100 hPa. Each input feature is valid at 1800 UTC, corresponding to approximately local noon (Central Time) over the contiguous United States (CONUS).\nThe training labels for this task are Pilot Reports (PIREPs). PIREPs are point-location reports written by pilots that describe in-flight conditions and can include a variety of information, such as cloud layers, general weather conditions, icing, turbulence encountered, and more. The PIREPs are retreived via the Iowa State University archives [35] and have a temporal extent since January 1, 2003, to the present day. Currently, data used span from January 1, 2003, to early November 6, 2023. The PIREPs recorded over the CONUS are primarily considered for this dataset. The spatial distribution of the turbulence reports are displayed in Figure 2."}, {"title": "Gravity Wave (GW) Parameterization", "content": "Atmospheric mesoscale processes like clouds, gravity waves (GWs), and precipitation, have global impacts on the atmospheric flow but are often parameterized in weather and climate models [37, 38, 39]. Climate model parameterizations are typically designed to be strictly vertical i.e., single column [40, 41, 42], due to which they neglect the horizontal evolution of mesoscales. Particularly in the context of meso-a and meso-\u1e9e GWs, this creates a major representation bias in weather and climate models, leading to global momentum imbalances such as colder-than-observed temperatures in the high latitudes (termed as cold-pole bias) or biases in global mass transport [43, 44, 45]. This means that the representation of horizontal propagation of atmospheric GWs, and in general, other parameterized mesos-a and meso-\u1e9e atmospheric processes, is crucial for both regional short-term weather forecasting and global long-term climate prediction [46, 47, 48].\nGWs couple the different layers of the atmosphere and demonstrate significant horizontal motion as they propagate. This property makes them an excellent candidate for simulation using a specialized AI-model or a generalized FM, and for a test of the AI's capability to learn regional variability and vertical coupling of the atmosphere. A successful ML-based prediction of subgrid-scale GW momentum fluxes opens avenues to first learn missing model physics from high-resolution observations and then represent the missing mesoscale physics in coarse-climate models using ML. The strategy can be applied to various other mesoscale processes, which are currently parameterized using (approximate) single-column schemes in weather and climate models. Availability of curated datasets, such as this, thus promotes the development of fast and potentially more physically accurate ML parameterizations of atmospheric processes."}, {"title": "Data Description", "content": "For a given background state of the atmosphere, GW parameterizations in climate models predict the associated subgrid-scale momentum fluxes due to GWs [38]. Here, this problem is formulated as a regression task to predict the resolved GW fluxes associated with an input background state.\nThe subgrid-scale momentum fluxes, i.e., the output, are computed using Helmholtz decomposition. Helmholtz decomposition of the full horizontal velocity field \u0169 = (u, v) decomposes the full flow into its rotational (balanced) and divergent (unbalanced) parts, i.e., (u,v) = \u016b = \u016brot +\u016bdiv. The divergent part is associated with GWs [49]. A high-pass filter is then applied to remove the first 21 harmonics of the divergent flow. The eddy vertical velocity, w', is computed by removing the zonal mean from full w. The divergent part \u016bdiv and w' are then used to compute the fluxes, (Fu, Fv) = (UdivW', vdiv@' ).\nThe training data for global gravity wave parameterizations is prepared using the modern reanalysis, ECMWF Reanalysis v5 (ERA5), from the European Centre for Medium-Range Weather Forecasting (ECWMF) [50]. ERA5 is publicly available at a horizontal resolution of 0.3\u00b0\u00d70.3\u00b0 and 137 hybrid pressure levels at an hourly temporal resolution. Accounting for grid-scale hyper-diffusion and other numerical effects, ERA5 resolves GWs with wavelengths exceeding 150-200 km and longer [51, 52, 53]. The global wind and temperature data are retrieved on 122 vertical levels: from 1.45 hPa (model level 16) to the surface (model level 137), and are used to compute the training labels, i.e., the momentum fluxes due to GWs. The top 15 levels near the model top are removed to avoid the artificial sponge damping in the mesosphere.\nThe background winds and temperature at ERA5's native grid resolution are conservatively coarse-grained to a 2.8\u00b0\u00d72.8\u00b0 (64\u00d7128) Gaussian grid to obtain the background state of the atmosphere, i.e., the training input. Likewise, the output fluxes are computed on the native grid and then conservatively coarse-grained to the 2.8\u00b0 grid, as denoted by the overbar. At this resolution, WxC-Bench provides a detailed, multi-year, global, evolution of subgrid-scale fluxes, far superseding other existing datasets, e.g., ClimSim [26]. Atmospheric GWs exist on spatial scales ranging from O(1) km to O(1000) km. We do not claim that ERA5, at 30 km horizontal resolution, completely resolves these scales. However, given the dearth of storm-resolving global climate simulations, ERA5 still presents as one of the most extensive long-term records to study atmospheric GWs with wavelengths ~150 km and above.\nThe input feature vector consists of three dynamical variables: zonal wind (u), meridional wind (v), and potential temperature, 0 = T(p/po)^\u03b3, where T is the temperature, p is the pressure, po = 1000 hPa is a reference surface pressure, and \u03b3 = -0.286 is a constant). Each quantity is a function of longitude, latitude, pressure (height), and time. Note that the input comprises the full winds, not just the divergent component. The output contains the vertical flux of zonal and meridional momentum carried by GWs, i.e., u'\u03c9', \u03bd'\u03c9'. Similar to the input, the output is computed globally for each longitude, latitude, pressure (height), and time. For this downstream task, input features and labels are computed using hourly-ERA5 for four years, viz., 2010, 2012, 2014, and 2015. These years were chosen based on rich wintertime orographic GW activity in the Southern Hemisphere. Thus, the dataset comprises of a total of 64\u00d7128\u00d724\u00d71461 (~287 million) columns of training and validation data.\nAdditionally, the input and output are scaled differently, i.e., the winds, temperature, etc., are normalized using their respective 4-year global mean and standard deviation. Subsequently, the means and deviations are used to scale the input variables as follows:\nuscale(t,p,,\u03bb) = (u'(t,p,,\u03bb) - u(t,p,\u03c6,\u03bb))/(3\u03c3u). (1)\nwhere uu is the temporally and zonally averaged zonal wind and ou is the standard deviation. Identical normalization is performed for v. The scaling transforms the input values to a range within [-2,2]. The potential temperature, 0, was scaled simply by dividing by 1000."}, {"title": "GW generation is highly intermittent; thus, the GW fluxes can have a symmetric exponential (Laplace) distribution with small values around zero. Therefore, the GW fluxes are scaled by their respective global means and standard deviations and then a cube root is applied, which transforms very small and very large values and maps them closer to 1. The normalized variables are then concatenated to form the input and output variables. Figure 3 illustrates the input and output variables distribution.", "content": "Weather Analog Search Dataset\nWeather analogs, that is, weather events and structures that are similar to each other, serve several purposes in the Earth science community. For forecasters, past events provide insight into how current weather can evolve and serve as initial conditions for ensemble model systems [54, 55]. They can serve for training deep-learning models. For researchers, weather analogs make useful case studies for analyzing how particular weather systems evolve [56, 57]. Thus, a rapid analog search is useful for improving forecasts and aiding scientists in locating desirable historic records for their research.\nData Description For searching analogs over a desired dataset, e.g., MERRA-2, the data samples are required to be encoded to enable rapid search over the data space. There are several ways to accomplish this such as vector search based on minimizing the distance between embedding generated by a post-tokenization module of a machine learning architecture [58]. Also, lookup table-based searching over a database of images using transforms - Fast Fourier Transform [56] or Wavelet \u201cfingerprints\" as search key [59, 60]. Searching analogs is also possible based on deep learning-based approach for generating latent features using all the temporal multivariate input predictors in this latent space rather than the original predictor space [61].\nIn this context of analog search, the searching approach must be spatially constrained in some way as the global repetition of an atmospheric state is unlikely [62]. Thus, following scenarios emerge for potential applications from a user perspective:\n\u2022 A user searches for a specific phenomena, such as a mid-latitude cyclone, and is given multiple instances from across the globe.\nIn this scenario, the model provides users with multiple instances of mid-latitude cyclones distributed globally. Each analog is limited in area, so that matches with the user case are more likely. Such an approach holds particular utility for individuals engaged in constructing climatologies or those endeavoring to identify case days for analyzing mid-latitude cyclone-driven weather events.\n\u2022 A user constrains the model to search a domain of interest.\nAlternatively, the model can be tailored to search within a designated domain of interest, restricting its focus to a specific region. This configuration is particularly advantageous for forecasters with a vested interest in the weather dynamics of a particular locale and for researchers and climatologists seeking targeted insights into regional phenomena."}, {"title": "Long-range Precipitation Forecast", "content": "The long-term precipitation or long-range precipitation forecast task consists of predicting global, daily-accumulated precipitation four weeks into the future. This forecasts range falls within the so-called sub-seasonal to seasonal range (S2S), which constitutes an important frontier in weather and climate research. Conventional numerical weather prediction (NWP) can produce skillful precipitation forecasts ten to twelve days into the future but beyond that their skill deteriorates to or below the level of a climatology [63]. While climate models can predict weather patterns on seasonal scales, the S2S range is currently considered a predictability desert [64]. Since many decisions in the management of agricultural and water resources fall into this two-weeks to two-months time scale, improving precipitation forecasts several weeks into the future has significant societal value [65]. The proposed dataset is targeted to address the S2S precipitation forecasting.\nThe input data for the long-range precipitation forecast consists of gridded, global satellite observations from a range of sensors providing observations in the visible, infrared, and microwave regions of the electromagnetic spectrum. The reference data is derived from satellite-based precipitation estimates that were corrected to match ground-based gauge measurements and arguably constitute the most reliable global estimates of precipitation currently available.\nA related dataset that has been proposed is RainBench [66], which combines simulated satellite observations with satellite- and reanalysis-based precipitation estimates for machine learning-based precipitation forecasts. However, the simulated satellite observations in RainBench cover only a limited number of channels and are available only from 2016 onward, which makes the dataset less suitable for evaluating long-range forecasts of precipitation. Moreover, since the simulated observations are based on non-cloud-resolving numerical models they will inherit their limitations regarding the representation of cloud- and precipitation-related processes. Here, we propose a novel benchmark dataset for long-range precipitation forecasts based on almost forty years of satellite observations and corresponding precipitation estimates.\nThe input data for the long-range precipitation forecast consists of gridded satellite observations from three observation sources: (1) infrared and visible observations from geostationary satellites, (2) infrared and visible observations from polar-orbiting satellites, and (3) microwave observations from polar-orbiting satellites. The input observations were chosen to maximize the temporal coverage of the training dataset. The observations are extracted from three observational records. Geostationary observations are taken from the GridSat-B1 dataset [67], infrared and visible observations from the Advanced Very High-Resolution Radiometer (AVHRR) and the High-resolution Infra-Red Sounder (HIRS) from the PATMOS-X fundamental climate data record [68], and microwave observations from the Special Sensor Microwave/Imager (SSM/I) from the NOAA Climate Data Record (CDR) of SSMI(S) and AMSR2 Microwave Brightness Temperatures [69]. Example observations from each of these records from selected channels for 1 January 2000 are shown in Figure 5.\nThe reference precipitation estimates are derived from the PERSIANN-CDR product [70] for the time range 1983 through June 2000 and the IMERG Final [71] product from June 2000 through the present. In contrast to PERSIANN-CDR, which is based solely on geostationary visible and infrared observations, IMERG combines observations from passive microwave and geostationary sensors. Because of this, IMERG can be expected to provide better sensitivity to precipitation and is therefore used to evaluate the forecasts. However, the IMERG precipitation estimates are available only from the year 2000 onward. We therefore provide an extended training record based on the combined estimates from PERSIANN-CDR and IMERG for the case that the extended training period may prove beneficial for the training of the forecast models.\nBoth input and reference data are interpolated to a regular latitude-longitude grid with a zonal resolution of 0.625\u00b0 and meridional resolution of 0.5\u00b0. The data are organized into separate files by day and input and target source."}, {"title": "Hurricane Prediction and Intensity Estimation Dataset", "content": "Hurricanes are one of the most destructive natural phenomena affecting human populations, infrastructure, and economies, causing approximately $17 billion in damages annually in just the United States. Moreover, it has been observed that the frequency of hurricane occurrence is increasing over the years [72]. Further, studies show that the power dissipated by these hurricanes has doubled since the 1980s [73], likely due to rising sea surface temperatures [74]. Many of these hurricanes make landfall along the coastal regions of southeast and southwest US which are zones of preferential urban growth. Thus, there is a need for accurate and timely predictions of hurricanes.\nThis dataset aggregates hurricane track data from the Atlantic and Pacific basins spanning 1980 to 2022. Data is primarily sourced from the Hurricane Database (HURDAT) maintained by the United States National Hurricane Center. HURDAT was accessed through the tropycal package [75], and the track, intensity (10m sustained wind speed and minimum sea level pressure), and size of the hurricane (radius of maximum winds and radius of 34 kt winds) were retrieved. Since the best track data is available on 6-hourly intervals, all parameters were then cubically interpolated to a 3-hourly interval to match MERRA-2 reanalysis.\nBy compiling information on hurricanes in both the Atlantic and Pacific Ocean basins, this dataset enables models to learn from a broader, more spatially diverse dataset that can help them generalize across the globe. Figure 6 shows an example of tracks of named hurricanes in the Atlantic Ocean by their respective categories, ranging from tropical storms (TS) to Category 5 (Cat 5) hurricanes for the year 2017. This dataset encompasses a total of 649 named hurricanes, and it is noteworthy that 287 of these hurricanes fall into the TS/TD category. This category represents storms with relatively lower wind speeds but still carries the potential for significant rainfall and localized impacts. There are 120 hurricanes classified as Cat 1 hurricanes. These are characterized by wind speeds between 74 to 95 miles per hour and can damage residential and small structures. Additionally, 170 hurricanes fall into the Cat 2 and higher, signifying more intense and potentially destructive storms with higher wind speeds and the capacity to cause more extensive damage. This categorization allows for a nuanced understanding of the distribution of hurricane intensity in our dataset. It serves as a crucial reference for our study, enabling us to analyze the factors contributing to the formation and development of hurricanes across various categories."}, {"title": "Generation of Natural Language-based Weather Forecast Reports Dataset", "content": "While there have been many advances in data-driven weather and climate research and forecasting, there are still opportunities in leveraging AI to perform more human-oriented tasks. Further, natural language forecasts are often the primary way that a forecast is communicated to the public. Thus, natural language-based forecasts have a large impact on day-to-day life and play a crucial role in various applications including disasters and severe weather warnings, agricultural planning, and business and economic planning.\nFor enhanced forecast communication, broadcast meteorologists have been constantly exploring ways for generating interactive forecasts. Generating natural language weather discussions, however, combines computational linguistics with weather analysis, which is a difficult task. The Forecast Generator (FOG) based on the Forecast Production Assistant (FPA) uses rules and natural-langauge generator for converting weather maps into forecast text [76]. Similarly, GALiWeather [77], SUMTIME-MOUSAM [78], and other work [79], implement translating weather data into interactive reports and graphs. However, these frameworks lack standardization and generalizability in the implementation. With recent advances in large-language models (LLMs) and Vision-language models (VLMs), an AI-model can be used to generate forecast outputs after learning the lexical patterns and semantic present in the training data. The proposed novel dataset can be used to train such a model or fine-tune pre-trained FMs to generate textual weather forecast reports based on weather predictions.\nTypically, the forecast parameters for generating forecast discussions include daily max and daily min temperatures (Tm), relative humidity (Rh), wind (u, v), cloud cover, probability of precipitation, and weather type. These parameters are used to develop grids of the expected weather conditions. The gridded forecasts are published to the local database and checked for quality. Subsequently, the human forecasters use specialized software to develop products such as the Zone Forecast Prediction, the Area Forecast Matrix, and the Point Forecast Matrix to develop the final textual forecast reports. These forecast reports are typically generated locally, state-wide, and country-wide.\nThe proposed dataset is focused on automating this process for generating end-to-end AI-based, natural language weather forecast reports."}, {"title": "Data Description", "content": "Weather discussion reports involves textual descriptions of the weather, thus, leading to a multi-modal dataset. Table 5 shows certain quantitative stats for the proposed dataset.\nTo address the multi-modality present in the dataset due to text labels, we performed following pre-processing steps to the labels (Refer Appendix B).\n\u2022 Removing punctuation and special characters: Punctuation characters are important for English grammar. However, they are avoided for text analysis. It is observed that special characters such as the newline character, \n, are prominently present in the forecast reports. These characters do not enhance information in the context of weather conditions and are removed from the labels.\n\u2022 Expanding contractions - Contractions are shortened representations of words such as don't for do not. The contractions are expanded to understand the full meaning of the text.\n\u2022 Unifying the word case - Usually, the forecast reports contain the same words in different casing, such as THUNDERSTORM and thunderstorm. Since, from a natural language processing perspective, both words represent the same meaning, we unified the word case to lowercase in all the label forecast reports."}, {"title": "Technical Validation", "content": "This section presents the technical validation of the datasets described above for baseline experiments, using both quantitative and qualitative metrics."}, {"title": "Aviation Turbulence Detection", "content": "The aviation turbulence detection dataset is validated by training an Artificial Neural Network (ANN) to perform a binary classification with two classes - turbulent and not-turbulent, for each of the LOW, MIDDLE, and HIGH levels. The trained ANN has 5 hidden layers of sizes 100, 60, 60, 40, and 20. For activation, Leaky ReLU is used within the hidden layers, and a sigmoid function is used for the classification layer. To mitigate the class imbalance, the loss is weighted by the class proportions. The ANN is evaluated based on the Overall Accuracy (OA), Probability of Detection (PoD) and False Alarm Rate (FAR) as performance metrics.\nTable 6 presents the quantitative results based on the chosen performance metrics. The high overall accuracy with a lower probability of detection reflects the large imbalances within the training dataset, as reports of turbulence are relatively rare"}, {"title": "Gravity Wave Momentum Fluxes", "content": "A proof-of-concept of the capability of DL models to learn and represent the nonlinear subgrid-scale GW evolution presents a new paradigm in their model parameterization development. This possibility is tested and demonstrated here using a baseline ML model. To test the AI-readiness of the data and to gauge the capability of ML models to learn from it, we used the WxC-Bench GW data to train an Attention-Unet [80] convolutional neural network."}, {"title": "Weather Analog Search", "content": "Figure 10 shows the architecture for a convolutional encoder-decoder-based baseline model for implementing similarity search based on the MERRA-2 dataset. The model takes weather parameters (such as temperature, sea-level pressure, and wind vectors) as image inputs and searches for images closely representing similar conditions from the past based on cosine similarity. Currently, the technical validation experiment is designed to consider individual weather parameters as images for searching the archive for that particular parameter. In other words, the model can search over either temperature or surface pressure, but not both simultaneously. However, we envision a multi-parameter search for weather conditions by combining search results for individual parameters.\nThe validation is done by training the encoder-decoder model for 100 epochs on input images of size 32x25. The learning rate is set to 0.001, and the batch size is set to 16 for training. Figure 11 presents the top five historical images with temperature patterns similar to the query image (Figure 11a), along with their structural similarity scores based on Structural Similarity Index (SSIM)[83]. These SSIM scores range from -1 to 1, with score of 1 indicating an exact similarity, 0 indicating no similarity, and -1 indicating perfect anti-correlation. The first four images retrieved by the similarity search have overall SSIM scores greater than 0.5, while the fourth and fifth images have scores less than 0.2."}, {"title": "Long-range Precipitation Forecasting", "content": "The baseline model for the precipitation forecast task is an auto-regressive forecast model based on a convolutional neural network. The architecture of the model is illustrated in Figure 12. The model input, which consists of the combined observations from the three satellite datasets from the eight days before the forecast initialization time, is first encoded spatially. The spatial encoder encodes the input from each day independently using separate stems for each observations source. Each stem consists of a 3 \u00d7 3 convolution block with 64 output channels. The resulting stem-outputs from the three observation sources are then concatenated and fed into the shared spatial encoder, which encodes the combined observations from each day spatially, successively reducing the resolution by a factor of 4 in two stages. The spatially encoded observations are then concatenated and passed through two separate temporal encoders that map the encoded observations into the two initial latent forecast states. From these two states, successive latent forecast states are produced using a propagator module consisting of an encoder-decoder architecture that takes the states from two previous steps and maps them to a latent state representing the forecast for the following day. The predicted states are decoded using a shared decoder, which upsamples the forecasts back to the original resolution. Finally, a head consisting of a single point-wise convolution layer is used to produce precipitation forecast for all predicted forecast steps."}, {"title": "Illustration of the top-5 images for temperature along with their date of occurrence in the past, with their structural similarity score with respect to the query image.", "content": "All components of the model, except the stems and head, consist of ResNeXt [84] blocks. The exact configuration of components is provided in Table 8. Inputs to each convolution are padded to conserve the image size. Since the observations are global, periodic padding is applied along the longitudinal dimension, while reflect padding is applied along the latitudinal dimension.\nThe baseline is trained to predict 32 quantiles of the posterior distribution using a quantile loss function [85]. The training uses the AdamW optimizer [86] with an initial learning rate of 10\u20134 and a cosine annealing learning-rate schedule over 50 epochs. The model was trained to predict precipitation up to 16 days into the future. To make the model robust to changes in the input observations, individual input channels are dropped with a probability of 2% and all observations from a sensor are dropped with a probability of 5%.\nThe ML forecasts are compared to conventional NWP reference forecasts obtained from the S2S-database [87], which archives forecasts from various operational centers. For this evaluation, we consider only the forecasts from the ECMWF and the United Kingdom's Met Office (UKMO) as these were found to be among the best-performing models in an evaluation of surface precipitation forecasts [88]. Examples of forecasts initialized on 10 January 2019 are shown in Figure 13. The ECMWF and UKMO forecasts yield good agreement with the reference estimates during the first week but yield a progressively larger error for the following weeks. Our machine learning-based baseline forecast also successfully captures the principal precipitation features during the first weeks but its predictions are generally less well-defined than those of the conventional models.\nThe results of a numerical evaluation of all forecasts from the evaluation period 1 January 2019 and 31 December 2022 are displayed in Figure 14. The forecasts are evaluated using the area-weighted forecast bias, linear correlation coefficient, and mean-squared error with respect to the reference precipitation estimates. In terms of bias, the ML-based model has a relatively small dry bias while the conventional models exhibit wet biases. The ML baseline forecasts is closest to the reference data, which is expected given that the precipitation estimates it was trained on were derived from the same precipitation product that is used for the evaluation. In terms of correlation coefficient, the conventional models yield higher correlations at shorter times but their advantage over the ML model decreases with longer lead times. The correlation coefficient of the conventional forecasts falls below that of the ML baseline after lead times exceeding 10 days but at this time the accuracy has decreased to or"}, {"title": "Hurricane Forecasting based on FourCastNet", "content": "For validating the hurricane forecasting dataset, we gathered and analyzed a total of 1,459 named hurricanes that formed within the time frame of 1980 to 2020. This dataset includes 649 hurricanes from the Atlantic Ocean basin and 810 hurricanes from the Pacific Ocean basin."}, {"title": "SLP (hPa)", "content": "To validate this task, FourCastNet has been used as the baseline model, which is a neural network architecture specifically designed for high-resolution forecasting tasks by combining elements from the Fourier Neural Operator (FNO) learning approach and the ViT architecture [2]. The model effectively addresses the computational challenges associated with high-resolution data by framing spatial token mixing as a continuous global convolution, which is efficiently implemented in the"}, {"title": "Generated: A few strong to severe thunderstorms are possible this evening across parts of the southern Plains and Ohio Valley.", "content": "Original: Severe thunderstorms will continue tonight from parts of far East Texas, extreme eastern Oklahoma, and southeast Kansas eastward into the lower and middle Mississippi Valley. Large hail, damaging winds, and a few tornadoes are expected."}, {"title": "Severe thunderstorms capable of large hail and damaging winds will continue across parts of the central and southern Plains this evening.", "content": "Severe storms remain possible across the central Plains and Upper Midwest through this evening, with a relative concentration expected over northern/central Kansas and vicinity."}, {"title": "Severe thunderstorms capable of producing damaging winds and hail will continue this evening across parts of the central and southern Plains. A few strong storms will continue to impact parts of southern Nebraska and western Kansas into the overnight hours.", "content": "Severe storms with hail and wind damage will be possible this evening from parts of northwest Kansas into west-central Missouri. A few marginally severe storms may also occur across parts of lower Michigan."}, {"title": "Conclusion", "content": "Given the tremendous advancements unlocked by the Artificial Intelligence (AI) based Foundation Models (FMs) and Large Language Models (LLMs) in Natural Language Processing and Computer Vision, it seems reasonable to expect a similar impact within the earth and atmospheric sciences. In this context, Machine Learning-ready datasets serve as a foundation for developing new models or fine-tuning existing models for downstream tasks. However, it is crucial that these datasets sample a wide range of realistic and relevant applications as well as data modalities to challenge AI model developers and demonstrate model capabilities to domain-specific scientists. The primary contribution of this paper - WxC-Bench, is a collection of high-quality datasets corresponding to a broad range of weather- and climate-related downstream tasks. WxC-Bench consists of datasets curated from satellite observations, reanalysis, high-resolution forecasting models, hurricane databases, and even pilot reports. Thus, the dataset can be used for benchmarking and testing the generalization of other specialized AI-models, and assess their ability to handle multi-modal data.\nThe selected six downstream tasks related to weather and atmospheric sciences are formulated as machine learning problems - classification, prediction, vector space search, and multi-modal language generation. The proposed WxC-Bench, thus, contains pre-processed ML-ready data for a range of atmospheric processes ranging from local scale (aviation turbulence) to synoptic scales (natural language forecasting). WxC-Bench"}]}