{"title": "Generation and De-Identification of Indian Clinical Discharge Summaries using LLMs", "authors": ["Sanjeet Singh", "Naimish Sharma", "Shreya Gupta", "Lokesh Srivastava", "Ashutosh Modi", "Niralee Gupta", "Vibhu Agarwal"], "abstract": "The consequences of a healthcare data breach can be devastating for the patients, providers, and payers. The average financial impact of a data breach in recent months has been estimated to be close to USD 10 million. This is especially significant for healthcare organizations in India that are managing rapid digitization while still establishing data governance procedures that align with the letter and spirit of the law. Computer-based systems for de-identification of personal information are vulnerable to data drift, often rendering them ineffective in cross-institution settings. Therefore, a rigorous assessment of existing de-identification against local health datasets is imperative to support the safe adoption of digital health initiatives in India. Using a small set of de-identified patient discharge summaries provided by an Indian healthcare institution, in this paper, we report the nominal performance of de-identification algorithms (based on language models) trained on publicly available non-Indian datasets, pointing towards a lack of cross-institutional generalization. Similarly, experimentation with off-the-shelf de-identification systems reveals potential risks associated with the approach. To overcome data scarcity, we explore generating synthetic clinical reports (using publicly available and Indian summaries) by performing in-context learning over Large Language Models (LLMs). Our experiments demonstrate the use of generated reports as an effective strategy for creating high-performing de-identification systems with good generalization capabilities.", "sections": [{"title": "1 Introduction", "content": "Over 330 million patient records in India have already been linked with a unique central ID (PIB Press Release). To put this in perspective, the number roughly equals the total population of the United States. Several federal initiatives aimed at establishing standards for medical information exchange, adoption of controlled terminologies, and promoting open architecture-based systems for the management of patient records have seen a steady rise in the adoption of electronic health records within Indian healthcare institutions (Ministry of Health and Family Welfare (MoHFW), India; Srivastava, 2016). This data represents an under-utilized resource that has profound implications for informing public policy, medical research and patient care. At the same time, it also poses some serious challenges. The risks of revealing patient identity even from data that has been anonymized are well known (Sweeney, 2013). Privacy regulations such as GDPR 2016 (European Parliament and Council of the European Union) and the HIPAA Privacy Rule 2003 (U.S. Department of Health and Human Services (HHS)) lay down heavy penalties on non compliance with data safety protocols. A robust data de-identification pipeline is vital if we aim to unlock insights from these electronic patient histories.\nNatural Language Processing (NLP) methods for de-identification are known to perform significantly better than manual de-identification (Douglass et al., 2004). However, these have been studied mostly in the single-institution setting. There are limited studies that evaluate de-identification performance of these methods across institutions (Yang et al., 2019). These suggest that NLP methods for de-identification perform poorly when evaluated on data from a different institution compared to the one that contributed the training data. This is especially significant in the context of patient data originating within Indian healthcare institutions. To the best of our knowledge, studies evaluating the performance of NLP based de-identification systems on patient data from Indian healthcare institutions have not yet been carried out. One reason for this might be that until recently there was no regulatory framework for accessing patient data for research. The Indian Digital Personal Data Protection Act 2023 (DPDPA) (Ministry of Electronics and Information Technology (MeitY), India) is a landmark legislation that came into effect in September 2023 and covers all organizations that process the personal data of individuals in India. Similar to GDPR 2016, the DPDPA defines responsibilities for organizations that collect, store, and process data from patients in India and holds them legally accountable for safeguarding patient privacy. The DPDPA also highlights the need for a data de-identification solution that has been validated on patient data from Indian healthcare institutions.\nThe present study takes a step towards answering this imminent need. Using a dataset of fully de-identified 99 discharge summaries obtained under Institutional Review Board (IRB) approval from the Sanjay Gandhi Post Graduate Institute of Medical Sciences (SGPGIMS), Lucknow, India, the study evaluates language models (LMs) for the task of de-identification. Furthermore, commercially available de-identification solutions are also evaluated. Hereafter, we refer to this dataset as the Indian Clinical Discharge Summaries (ICDSR, subscript R refers to real) dataset. Given the paucity of clinical data, the study also evaluates Large Language Models (LLMs) on the task of generating synthetic clinical texts for training de-identification models. Critically, the study highlights the existence of several personal health information (PHI) elements in the ICDSR dataset that are unique to the language use and cultural practices in India. It is unlikely that the existing de-identification solutions have been trained to recognize these unique PHI elements, and therefore, their detection may be unreliable. In a nutshell, we make the following contributions:\n\u2022 We introduce a new dataset (Indian Clinical Discharge Summaries (ICDSR)) obtained from an Indian hospital and evaluate the performance of PI-ROBERTa model (PI-ROBERTa) (fine-tuned on non-Indian clinical summaries) on ICDSR for the task of De-Identification. Our experiments show poor cross-institutional performance. Experiments with existing commercial off-the-shelf clinical de-identification systems show similar trends.\n\u2022 To overcome the paucity of Indian clinical data, we generate synthetic summaries using LLMs (Gemini (Team et al., 2023), Gemma (Team et al., 2024), Mistral (Jiang et al., 2023), and Llama3 (Touvron et al., 2023)) via In-Context Learning (ICL). Further, the synthetic summaries are used to train PI-ROBERTa for de-identification on ICDSR. Results show significant improvement in the performance of the de-identification system.\n\u2022 We release the model code and experiments via GitHub: https://github.com/Exploration-Lab/llm-for-clinical-report-generation-deidentification"}, {"title": "2 Related Work", "content": "Automatic data de-identification methods for biomedical texts have focused on leveraging machine learning techniques to ensure privacy while maintaining data utility. Named Entity Recognition (NER) systems have been tailored to identify and anonymize personal health information/personal identifiable information (PHI/PII) from clinical narratives. Earlier work explored Support Vector Machines (SVMs) for identifying PHI (Neamatullah et al., 2008). Researchers have also explored deep learning models, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) (Dernoncourt et al., 2017), which have shown superior performance over the conventional approach.\nIn recent years, there has been a growing interest in the application of transformer-based models like BERT (Devlin et al., 2018) for the clinical NER and de-identification task (Chaudhry et al., 2022; Alsentzer et al., 2019). LLMs have also been explored for various clinical tasks such as clinical NLI (Mandal and Modi, 2024). Hybrid approaches that combine rule-based and machine learning methods have also been developed to enhance the robustness of de-identification systems (Meystre et al., 2010). A study by Yang et al. (2019) used a hybrid model combining Long Short-Term Memory (LSTM) networks with Conditional Random Fields (CRFs) for the de-identification of clinical notes. It demonstrated the effectiveness of integrating local resources and diverse word embeddings, and achieved high F1 scores across various de-identification tasks. Furthermore, El Azzouzi et al. (2023) de-identified French electronic health records using distant supervision and deep learning techniques. The study utilized models like Bi-LSTM+CRF and enhanced them with contextualized word embeddings. It achieved remarkable accuracy in removing identifiable information while maintaining data utility. These innovations underscore the continuous improvement"}, {"title": "3 Clinical Discharge Summaries Datasets", "content": "n2c2: We make use of the 2006 and 2014 n2c2 datasets (\u00d6zlem Uzuner et al., 2008; Stubbs et al., 2015). The 2006 challenge involved the development of automated methods to de-identify discharge summaries from patient medical records (\u00d6zlem Uzuner et al., 2008). The total number of summaries in the n2c2-2006 dataset are 888, split between training and test sets. The 2014 challenge comprised of two tasks: de-identification and heart disease risk factor identification (Stubbs et al., 2015). For the de-identification task, the dataset included a variety of clinical documents such as progress notes, discharge summaries, and other narrative texts that typically contain detailed patient information.\nIndian Clinical Discharge Summaries (ICDSR): We obtained fully de-identified 99 discharge summaries obtained under Institutional Review Board (IRB) approval from the Sanjay Gandhi Post Graduate Institute of Medical Sciences (SGPGIMS), Lucknow, India. All discharge summaries in the Indian Clinical Corpus were manually annotated for de-identified entities by human annotators using Doccanno (Nakayama et al., 2018), a data annotation tool. Each document was annotated by one annotator. The annotators had previous experience in clinical text annotations. Following established practice, we used the BIO scheme (Ramshaw and Marcus, 1999) for annotating named entities. Our PHI labels were defined by augmenting the PHI entities defined in the HIPAA Privacy Rule 2003 along with adaptation to Indian clinical texts. After annotation, we obtained 26 PHI unique entities in the ICDSR dataset. Subsequently, due to privacy concerns, PHI elements were replaced with fake values through an automatic replacement tool developed using the Python library Faker (Faraglia and Other Contributors, 2010) (example in Fig. 1). Repeated occurrences of an entity within a note were tracked for consistent replacements. Moreover, settings such as date/time offsets were parameterized via a configurable file. The tool provides a scalable solution for de-identifying medical datasets while ensuring secure data access. Table 1 provides statistics of the datasets."}, {"title": "4 Generated Discharge Summaries Datasets", "content": "Initial experimentation showed over-fitting in models on the ICDSR data due to its small size (69, 10, 20 summaries for train, val, and test sets, respectively). Consequently, we generated synthetic summaries to augment ICDSR data. Synthetic patient data is being used increasingly for a variety of in-silico biomedical experiments in addition to training data augmentation (Chen et al., 2021). Using the samples from ICDSR we generated medical discharge summaries specific to Indian patients using LLMs (Gemma, Llama-3-8B-Instruct, and Mistral-7B-Instruct-v0.1) via In-Context Learning (ICL). We experimented extensively with various prompts and discharge summaries, as explained below. Our choice of LLMs was driven by the feasibility of instantiating these models on-premise. Prompting is a key aspect of using LLMs. As described below, we experimented with various prompt designs.\nDischarge Summaries Generation using the n2c2-2006 dataset: Since the n2c2-2006 discharge summaries are publicly accessible, we generated synthetic discharge summaries based on these along with PHI annotations using Gemini-pro-1.0. We arrived at a functional prompt by iteratively tuning and inspecting the synthesized outputs for overall length, presence of key subsections, and correct PHI annotation. While tuning our prompts, we did not check for the medical validity of the discharge summaries (see App. Table 12). The prompt also contained an original n2c2-2006 summary as an exemplar. This way, we generated five patient discharge summaries for each original discharge summary in the n2c2-2006 dataset and a total of 3000 discharge summaries. The generated summaries were manually reviewed, and the ones containing gibberish text and missing or incorrect annotations were filtered out, resulting in 1596 synthetic discharge summaries with PHI annotations. Hereinafter, we refer to this dataset as ICDS&.\nDischarge Summaries Generated using the ICDSR dataset: The ICDSR dataset is accessible only under the Institutional Review Board's approval, and therefore, LLMs that can be inferred only via public API endpoints cannot be used to process these. Consequently, we generated syn-"}, {"title": "5 De-Identification Task", "content": "De-Identification Task: De-Identification is conceptually similar to a Named Entity Recognition task. Both ICDS and ICDS were pre-processed and converted into BIO format as is customary in Named Entity Recognition development (also see App. Fig. 4). Formally, given some text, $S = (W_1, W_2, W_3, ..., W_n)$ containing n words, de-identification requires labeling each of the word $w_i$ with a tag $t_e$ coming from a NER tagset $t_1, t_2, ..., t_r$. Subsequently, the labeled entities can be redacted or replaced with fake values for privacy protection.\nDe-Identification Model: We fine-tuned several different NER models, including"}, {"title": "6 Model Training Experiments", "content": "Initial experiments with ICDSR using a 69-10-20 (train-val-test) split resulted in overfitting given that ICDSR is small, containing only 99 discharge summaries. We also experimented with training the model on n2c2-2006 and n2c2-2014 datasets and testing on ICDSR to check for cross-institutional generalization. We experimented with several combinations of real and synthetic datasets and evaluated on the test set of n2c2-2006, n2c2-2014, and ICDSR. Table 3 shows the experiments matrix, in total we evaluated 24 different combinations. For all the experiments, we reserved 20 summaries of ICDSR for testing. Note that these summaries were also not used for generation. For each experiment, PI-ROBERTa was fine-tuned on each training set as"}, {"title": "7 Experiments, Results and Analysis", "content": "Comparison of datasets: The total number of summaries in the n2c2-2006 dataset are 888, split between training and test sets. The n-gram analysis of the n2c2-2006 and ICDSR datasets reveals distinct linguistic patterns reflecting their unique clinical foci. The n2c2-2006 dataset features unigrams like 'patient,' 'discharge,' and medication-related terms such as 'mg' and 'po' and bigrams like 'mg po' and 'discharge date,' highlighting a narrative centered on patient management and clinical processes as shown in App. Fig. 14. In contrast, the ICDSR dataset shows a marked presence of terms such as 'pm,' 'days,' and 'mgdl,' and bigrams and trigrams like '10 days,' 'daily 10,' and \u2018cr x ray,' suggesting an orientation towards experimental or lab-result oriented narratives, with a particular emphasis on procedural timelines and diagnostic procedures. Hence, ICDSR focuses on a broader scope involving diagnostics and treatment monitoring.\nReal versus Generated Datasets\nICDS vs n2c2-2006: We analyzed the n2c2-2006 and the synthetic ICDS discharge summaries in terms of summary statistics, Jaccard distance, and BERTScore using the \"dmis-lab/biobert-v1.1\" model. The Jaccard distance suggests a high level of lexical dissimilarity between the datasets, indicating that the synthetic dataset introduces a significant degree of variation compared to the real dataset. While indicating some differences, an F1 score of 0.6362 indicates the real and synthetic datasets have semantic overlap. An n-gram analysis of the top 10 unigrams, bigrams, and trigrams unveils the differences between the two datasets, yet also underscores their relevance to the task at hand as shown in App. Fig.14, Fig. 15, Fig.16, and Fig.17. These metrics suggest that while the synthetic dataset is designed to be distinct enough to introduce useful variability, it retains a substantive semantic similarity to the real dataset. This balance is crucial when synthetic data is used for tasks such as model training, where the goal is to ensure that the model is not only trained on a diverse set of data but also remains relevant and effective when applied to real-world data. The high Jaccard distance combined with the moderate BERTScore indicates that the synthetic dataset achieves this objective by being similar enough to the real dataset to be useful, yet different enough to enhance the dataset's diversity and robustness.\nICDS vs ICDSR: Similar to the n2c2-2006 and ICDS datasets, we analyzed the ICDSR and ICDS datasets with summary statistics, Jaccard distance, and BERTScore. The Jaccard distance suggests lexical dissimilarity implying injection of new vocabulary in the generated discharge summaries. The n-gram analysis of the top 10 unigrams, bigrams, and trigrams shows these differences App. The BERTScore results indicate a moderate level of semantic similarity between the real and generated datasets. The metrics suggest that the generated dataset has greater lexical variety and incorporates some additional semantic constructs.\nEvaluation of The Quality of Generated Summaries: The confusion matrix on convenience sample of 60 discharge summaries evaluated by physician1 and physician2 are shown in Fig. 2 and Fig. 3 respectively. There are 10 summaries that were originally synthetic but were labeled as real by physician 1, and 19 summaries that were originally synthetic but were labeled as real by physician 2. Physician 1 is able to label summaries with higher precision and recall, i.e., higher f1-score as com-\npared to physician 2 (Table 11). The Cohen's kappa coefficient, the measure of inter-annotator agreement, is 0.290 showing a fair agreement between the labels assigned by the physicians. Additionally, physician 1 observed that many of the discharge summaries that he labeled synthetic appeared to have been translated from a non-English source. Physician 2 reported some diagnosis and formatting issues among the summaries he labeled as synthetic. Additionally, physician 2 reported some errors in diagnoses, medications, and lab results, but these were not limited to the summaries he labeled as synthetic.\nModel Performance: Table 6 shows the results for intra- and inter-institutional performance. As can be observed, the inter-institutional performance of the model is very high (> 0.96 F1). However, the cross-institutional performance suffers significantly. Table 7 shows the results of training on generated datasets. The fine-tuned Model gives 68% F1 score on the ICDSR test set, 77% on the n2c2-2014 test set, and 85% on the n2c2-2006. Results on the ICDSR test set are not promising. This might have happened because ICDS& was generated using n2c2-2006. Fine-tuning on ICDS dataset results in 98% F1 score on the ICDSR test set, 67% on n2c2-2014 test set, and 55% on the n2c2-2006 test set. To further improve model generalization, we experimented with combinations of datasets. Table 8 shows the training results on a combination of real and synthetic datasets. We get micro-F1 of 97% on n2c2-2014 and ICDSR test set given that we have included n2c2-2014 and ICDS datasets in training, but the performance of the model (88%) is also notable on n2c2-2006 dataset. These results indicate that fine-tuning on the combination improves cross-institutional performance.\nAnalysis: Our experiments indicate models have poor cross-institutional generalization. We performed several experiments with n2c2-2006, n2c2-2014, ICDS, and ICDS datasets, and their combinations. The general trend is that fine-tuned model performance degrades heavily in cross-dataset settings. At the individual entity level, the F1 score for the PATIENT entity is consistent for all the fine-tuned models. For the DOCTOR and DATE entities, the F1 scores of all the fine-tuned models are also consistent, except for when the model is trained on the n2c2-2006 dataset and tested on the n2c2-2014 dataset and ICDSR test sets. For the ID entity, all the fine-tuned models have consistent F1 scores, except for when the model is trained on ICDS, and tested on n2c2-2014 and ICDSR datasets. We noticed performance variance in the LOCATION, AGE, and CONTACT entities. This could be because the LOCATION can be any local address without a specific format. AGE is either a number like '78 Y' or a word representation of that number like 'Seventy-Eight year old'. In most cases in the datasets, these types of words or tokens are tagged as OTHERS, and they are highly prevalent. This could be why the AGE tag was incorrectly predicted as OTHERS in cross-dataset settings. The entity CONTACT includes email, IP address, phone number, landline number, etc. However, their distribution is not uniform.\nOur main aim was to develop a robust model that could de-identify medical text from Indian Healthcare Institutes. This was done by fine-tuning PI-ROBERTa on ICDS where we are getting state-of-the-art performance on ICDSR. Almost all the entities were correctly identified, with a few exceptions. A few PHI entities were misidentified with non-PHI entities (i.e., OTHERS) and vice versa, as can be seen in App. Fig. 26. However, the per-\ncentage of incorrect prediction is significantly less when considering the total support set of ICDSR test set. However, this fine-tuned model was not generalizable when we tested it on the n2c2-2006 and n2c2-2014 test sets, as seen in the Table 7. For model generalizability, we fine-tuned PI-ROBERTa on n2c2-2014, ICDS& and ICDS, tested on the n2c2-2006 test set. The results shown in Table 8 indicate that models are generalizing when we fine-tuned them on different combinations of datasets, although the F1 score for all entities is not consistent, as can be seen in App. Fig. 28a. Confusion matrix for all the experiments are shown in App. Fig. 22 Fig. 23, Fig. 24, Fig. 25, Fig. 26 Fig. 27, Fig. 28b.\nComparison with Commercial De-Identification Systems: The results obtained using AWS and GCP solutions are summarized in Table 9 and Table 10. The results clearly indicate that AWS and GCP do not perform well on ICDSR test set. This could be because systems have been trained on non-Indian specific clinical data. This underscores the importance of ensuring that de-identification caters to diverse demographics, which is essential for ensuring the efficacy and ethical deployment of these solutions.\nThe underperformance of commercial solutions in classifying PHI in ICDSR can be attributed to misidentification. Medical entities are mistaken as NAME/LOCATION, while Pin-codes as ID. Names like 'Alia' and 'Adah' are not being consistently recognized as NAME by AWS and GCP. Patient IDs that start with CRNO: ########### or ADM-########## are not identified as PHI; these solutions probably aren't sure what CRNO, ADM stand for. 'B/O Kanav Viswanathan' is misidentified, where 'Kanav Viswanathan' is a name and B/O stands for Baby of but gets labeled as a LOCATION. 'Urvi Bhamini Faiyaz Kakar' is identified as Name by GCP but not by AWS. \u2018Wockhardt Hospitals,' hospital name was not identified as PHI. Medical terms like \u2018BILIRUBIN,' \u2018MALLOY EVE-LYN,' 'CR X Ray' and 'SERUM LIPASE' are misidentified as NAME when they describe medical tests. Similarly, \u2018CREATININE (M - JAFFE COMPENSATED)' is a medical test and 'JAFFE' is misidentified as NAME. \u2018Meropenem,' an antibiotic, is misidentified as NAME. Even terms like 'Ward' from room names such as 'Ward-B' occasionally get misidentified as NAME. Test results like '136/94mmHg' or 'TSH - 5.45' are misidentified as ID. Locations like 'Subramaniam Chowk' and 'Yohannan Nagar,' are also misidentified as NAME. Additionally, using GCP or AWS for PHI detection introduces variability, causing results to vary with each execution. These factors underscore the need for precision and consistency in data handling to mitigate performance issues in medical contexts.\nDe-identification using LLMs: We also conducted experiments of de-identifying clinical summaries using LLMs directly. A precision score of 0.55 was obtained. However, the model faced challenges in terms of recall. The recall scores were merely 0.11. We also evaluated the performance of Mistral-7B-Instruct-346v0.1 and Gemma. Surprisingly, the results obtained from these models were far inferior to those of Meta-Llama-3-8B-Instruct. Results suggest that the LLMs struggle to detect PHI in Indian medical discharge summaries."}, {"title": "8 Conclusion and Future Directions", "content": "In this paper, we explored the task of de-identification on Indian clinical discharge summaries. Experiments indicate a poor generalization of fine-tuned (on public datasets) models and poor performance of the off-shelf commercial systems. Experiments with LLM generated summaries look promising; the model fine-tuned on generated summaries and public datasets shows good generalization performance. Our results are based on a small test set. Using the insights from our work, we aim to set-up an active learning workflow that combines our fine-tuned model and human annotators to produce a larger test dataset on which we may evaluate overall model performance as well as by conditioning on a medical specialty. The augmented (generated summaries with original data) institution-specific dataset can be used to fine-tune NER models that have been pre-trained on PHI data cost-effectively. Achieving cross-institution portability remains a topic of active research. However, many open-source large language models can be deployed on-premise and, as described above, fine-tuned to provide an immediate and effective solution to personal data protection in Indian healthcare institutions."}, {"title": "9 Acknowledgements", "content": "We would like to thank Dr. Uttam Singh, Dr Prabhakar Mishra, and Dr Amit Goel for their support for this work."}, {"title": "Appendix", "content": ""}, {"title": "5 De-Identification Task", "content": "De-Identification Task: De-Identification is conceptually similar to a Named Entity Recognition task. Both ICDS and ICDS were pre-processed and converted into BIO format as is customary in Named Entity Recognition development (also see App. Fig. 4). Formally, given some text, $S = (W_1, W_2, W_3, ..., W_n)$ containing n words, de-identification requires labeling each of the word $w_i$ with a tag $t_e$ coming from a NER tagset $t_1, t_2, ..., t_r$. Subsequently, the labeled entities can be redacted or replaced with fake values for privacy protection.\nDe-Identification Model: We fine-tuned several different NER models, including"}, {"title": "12 Prompts used for synthetic discharge summary generation", "content": ""}, {"title": "13 Example summary generated using gemini-pro-1.0", "content": ""}, {"title": "14 Example summary generated using llama-3-8B-Instruct", "content": ""}, {"title": "15 Tag mapping from PHI entities in the different datasets to the PHI entity set of n2c2-2006 dataset, and all other non-PHI entities are mapped with Others tag", "content": ""}, {"title": "16", "content": ""}, {"title": "A Prompts and Synthetic Discharge Summaries", "content": "In Table 12, we showcase the prompts which we used to generate the ICDS and ICDS datasets. We used Prompt A in Table 12 for generating ICDS from Gemini-pro-1.0. Table 13 gives a sample discharge summary. Using prompt B in Table 12, we generated ICDS dataset using Llama-3-8B-Instruct. Table 14 gives a sample discharge summary."}, {"title": "B Tag Mapping across all the dataset and tag Distribution after Mapping the Tags", "content": "We have five datasets: n2c2-2006, n2c2-2014, ICDSR, ICDS&, and ICDS. Each dataset has its own tag set. n2c2-2006 contains 9 tags, n2c2-2014 contains 24, ICDSR contains 26, ICDS& contains 34, and ICDS contains 106 unique tags, including the OTHERS tag. In the datasets n2c2-2006, n2c2-2014, and ICDSR, all the tags are related to PHI entities. However, in the ICDS and ICDS datasets, a few annotated tags are not related to the PHI entities due to LLM hallucinations. To train models for a fair comparison, we need a uniform tag set across all datasets.\nHence, we mapped the tag set of all the datasets to the n2c2-2006 tag set. In all the datasets, we mapped entities like street, city, country, zip, etc to LOCATION. Similarly, we mapped phone number, mobile number, email, landline, etc, to CONTACT. Additionally, we mapped all the PHI-related entities to their super-set using mapping shown in Table 15. In the ICDS and ICDS& datasets, we have several tags unrelated to the PHI entities. Hence, we mapped all non-PHI entities to the OTHERS tag. After mapping the tag set of all the datasets to n2c2-2006 tag set, we calculated the tag distribution of all PHI entities across all datasets. The distribution of tag sets of all the dataset when mapped with n2c2-2006 dataset are shown in Fig. 5, Fig. 6, Fig. 7, Fig. 8, Fig. 9, Fig. 10, Fig. 11, and Fig. 12."}, {"title": "C Corpus Statistics", "content": "The n-gram frequencies from the n2c2-2006 dataset show a strong emphasis on clinical and procedural language, including terms like \u2018mg,' \u2018po,\u201d and 'hospital,' as shown in Fig. 14. Notably, phrases such as 'discharge summary' and \u2018physical examination' dominate, highlighting standard documentation practices. Trigrams such as 'dis report status' and 'report status unsigned' indicate typical phrasing in medical reports. This is in contrast with the ICDSR dataset in Fig. 18, where there is a predominance of time-related unigrams (\u2018pm,' \u2018days') and clinical terms ('mgdl,' 'method'). The frequent bigrams and trigrams revolve around treatment and diagnosis descriptors, such as 'daily 10 days' and 'x ray chest,' illustrating the detailed recording of patient care routines and diagnostic procedures commonly found in medical records. In the n2c2-2006 dataset, bigrams like 'mg po' and 'discharge date,' and trigrams like 'mg po bid' and 'history present illness,' which reveal specific medication dosages and detailed descriptions of patient conditions, are found next to PHI elements, as shown in Fig. 16. In the ICDS R dataset, specific trigrams like 'discharge summary crno' and 'normal discharge correspond' are located near PHI elements (Fig. 20). The differences between the n2c2 2006 dataset and ICDS R highlight how clinical documentation practices and language differ between the US and India.\nIn the synthetic ICDS dataset, the frequent occurrence of 'phi' in various n-grams highlights (in Fig. 15) the inclusion of potentially identifiable information. Trigrams such as 'phi typehospital-fihphi' and 'phi typeid7673299w3phi' illustrate the use of placeholders for personal identifiers, indicative of the synthetic nature of the dataset and its focus on mimicking real-world PHI data while maintaining privacy. In the ICDS dataset, the frequent mention of basic terms like 'patient,' 'discharge,' and 'history' reflects their regular usage in clinical documents, as seen in Fig. 19. Phrases such as 'discharge summary' and 'medical history' indicate standardized document formats. For n-grams next to PHI elements in the synthetic ICDS dataset as seen in Fig. 17, we observe a mix of clinical terminology ('discharge,' 'patient,' 'history') and documentation descriptors (\u2018text record,' 'reportend text'). Bigrams and trigrams like \u2018discharge summary patient' and 'text record record' suggest a replication of typical medical documentation formats. Terms like \u2018primary care physician' and 'history present illness' reflect the comprehensive nature of clinical narratives. In contrast, the n-grams next to PHI elements in the ICDS dataset, as shown in Fig. 21, highlight the frequent use of both temporal ('pm', 'days') and medical (\u2018mgdl,' 'discharge') terms. Common bigrams and trigrams such as 'discharge summary,' 'cr x ray,' and 'x ray chest' underscore the clinical focus on diagnostic imaging and summary documentation. The"}, {"title": "D Model Training Details", "content": "We fine-tuned dslim/bert-base-NER (Dslim bert base NER), ghadeermobasher/BCHEM4-Modified-BioBERT-v1 (BioBERT), and Clinical-AI-Apollo/Medical-NER (Clinical AI Apollo). We obtained a consistent train-set F1 Score for PHI entities from these models after fine-tuning, but the performance of these models decreased significantly when we tested them on cross-dataset settings. However, after fine-tuning, PI-ROBERTa outperformed these models in the same and cross-dataset settings, so we chose PI-ROBERTa for further experiments. Fig. 13 shows the model architecture.\nPI-ROBERTa was fine-tuned on each training set as given in Table 3 and tested on each corresponding test set. We fixed the hyperparameters for all the experiments. The model was fine-tuned at four epochs in all the experiments with a batch size of 8; the learning rate was 5e-5. We used Weighted Cross entropy loss to handle the data imbalance problem because around 90 percent of the tokens correspond to non-PHI entities in all datasets. After several experiments, we devised a formula to assign weights to different Entities. $w_t = log(\\frac{4 \\times n}{n_t})$, where $w_t$ is the weight assigned to the $t^{th}$ entity; $n_t$ is the number of tokens in the $t^{th}$ entity; n is the total number of tokens in the dataset"}, {"title": "E Evaluation Metrics", "content": "Model was evaluated using various performance metrics as described below.\n\u2022 Macro Precision:\nPrecisionmacro =$\\frac{1}{n}\\sum_{i=1}^{n}\\frac{TP}{TP+FP}$\n\u2022 Macro Recall\nRecallmacro =$\\frac{1}{n}\\sum_{i=1}^{n}\\frac{TP}{TP+FN_i}$\n\u2022 Macro F1-score\nF1-scoremacro =$\\frac{2 \\times Precisionmacro \\times Recallmacro}{Precisionmacro + Recallmacro}$\n\u2022 Micro Precision:\nPrecisionmicro =$\\frac{\\sum_{i=1}^{n} TP}{\\sum_{i=1}^{n}(TP + FP)}$\n\u2022 Micro Recall\nRecallmacro =$\\frac{\\\\sum_{i=1}^{n} TP}{\\sum_{i=1}^{n}(TP + FN_i)}$\n\u2022 Micro F1-score\nF1-scoremicro =$\\frac{2 \\times Precisionmicro \\times Recallmicro}{Precisionmicro + Recallmicro}$"}]}