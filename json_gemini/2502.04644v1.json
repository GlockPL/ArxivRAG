{"title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research", "authors": ["Junde Wu", "Jiayuan Zhu", "Yuyuan Liu"], "abstract": "In this technical report, we introduce Agentic Reasoning, a framework\u00b9 that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving.", "sections": [{"title": "Introduction", "content": "Recently, large reasoning models, such as OpenAI's o1 (Jaech et al., 2024), Qwen-QwQ (Team), and DeepSeek-R1 (Team, 2024), have demonstrated impressive stepwise reasoning capabilities over long sequences through large-scale reinforcement learning. These advancements provide promising solutions to complex reasoning tasks (Wei et al., 2022; Lewkowycz et al., 2022; OpenAI) and have inspired foundational efforts to replicate 01-like reasoning patterns across a broader range of models (Qin et al., 2024; Huang et al., 2024; Zhang et al., 2024).\nDeepSeek-R1, for example, relies exclusively on rule-based outcome rewards during training, such as evaluating whether a mathematical solution is correct or a piece of code executes successfully. While this approach has yielded remarkable reasoning capabilities, equaling ol's performance in domains like math and code, it comes with notable trade-offs. As even the authors acknowledge, this type of training diminishes the model's ability to articulate its reasoning process. DeepSeek-R1's responses are often logical and accurate but lack detailed explanations of transitions between ideas or the finer connections between arguments.\nAlthough current reasoning methods excel in structured domains like math and code-where outcomes are easily verifiable-applying these techniques to less structured or subjective tasks remains a significant challenge. Adapting these strategies to areas where answers are not inherently definitive is a key research gap. How can models be trained to handle tasks that require judgment, interpretation, or nuanced understanding rather than binary correctness?\nFurthermore, not all problems benefit from formal reasoning approaches. Many fields, such as social sciences, ethics, or experiential disciplines, rely on abstract concepts, conventional wisdom, factual verification, understanding complex logical relationships, or moral reasoning. When models attempt to impose math- or coding-style reasoning onto such areas, they often produce flawed or overly rigid results. Developing approaches that account for these unique requirements is essential for advancing the applicability of reasoning model beyond their current domains.\nDeep, thoughtful answers to open-ended questions often require extensive research, repeated verification, information retrieval, computational analysis, and the organization of complex logical relationships-steps fundamental to human reasoning. In this process, humans rely heavily on external tools, such as internet searches for gathering information, computational tools for quantitative analysis, or whiteboards and Mind Maps for organizing thoughts. This raises an intriguing question: can large language models similarly leverage external tools to enhance their reasoning and tackle intensive knowledge work across diverse domains?\nPrevious efforts have attempted to integrate search or retrieval-augmented generation (RAG) into the reasoning process (Shao et al., 2024; Khaliq et al., 2024; Islam et al., 2024; Li et al., 2025), with notable examples including Gemini's Deep Research. However, these models are closed, their exact methodologies remain undisclosed. In contrast, open-source models typically focus exclusively on retrieval or web-searching during reasoning, leaving a significant performance gap compared to their closed-source counterparts.\nWe introduce Agentic Reasoning, a framework that enhances the reasoning process by integrating external LLM-based agents as tools. This approach enables LLMs to perform multi-step reasoning and tackle complex problems more effectively by delegating specific tasks to these auxiliary agents. Through extensive experimentation with integrating various agents into the reasoning process, we identified three essential agents that prove highly effective for general reasoning across diverse problems. The web-search agent, which retrieves relevant information from the internet to supplement the model's knowledge. The code agent, capable of performing computational analyses and coding tasks to support quantitative reasoning. Finally, the memory agent, which we call Mind Map, constructs knowledge graphs based on the reasoning context, enabling the organization of complex logical relationships in a manner similar to human mind mapping. Together, these agents enhance the model's ability to tackle complex problems with greater efficiency and precision.\nWhen integrated into current reasoning LLMs, Agentic Reasoning transforms their problem-solving capabilities by enabling them to plan and execute multi-step strategies autonomously. These models can identify and retrieve the necessary data, adapt dynamically to real-time information, and perform quantitative analyses to generate precise outcomes. This framework also allows LLMs to deliver comprehensive reports comparable to those of a research analyst or provide solutions on par with PhD-level expertise.\nWe evaluated our model on general knowledge-intensive benchmarks requiring complex reasoning capabilities, categorized into two key areas: (1) solving expert-level questions and (2) conducting deep research on real-world expert-level tasks.\nFor expert-level questions, we tested the model on the GPQA dataset, a PhD-level science multiple-choice QA benchmark with questions authored by domain experts in physics, chemistry, and biology. Our Agentic Reasoning framework achieved impressive accuracy rates: 58% in chemistry, 88% in physics, and 79% in biology, closely rivals the best and newest closed reasoning model, OpenAI 01. For real-world expert-level tasks, Agentic Reasoning was evaluated by domain experts, who noted that it effectively automated several hours of challenging, manual investigation. This highlights its potential to streamline labor-intensive processes and enhance productivity in knowledge-intensive domains.\nAdditionally, we tested the model's scalability in test-time reasoning using the agentic framework as a verifier. The results showed significant improvements in test-time computational efficiency, demonstrating the framework's ability to optimize reasoning processes. This finding suggests that the agentic framework has strong potential to serve as a reward model for reinforcement learning, further advancing reasoning model training.\nThese results position Agentic Reasoning as a powerful and versatile framework, capable of tackling complex, domain-specific challenges with depth and precision. Its ability to perform in-depth research, navigate intricate logical structures, and synthesize information effectively highlights its potential for solving knowledge-intensive problems and driving advancements in deep analytical exploration."}, {"title": "Method", "content": "We consider an expert-level task that requires multi-step complex reasoning. In the process of model reasoning, it can retrieve external tool usage, and structured memory of its previous reasoning. Our objective is to generate, for each query q, both a logical reasoning chain r and a final answer a. To achieve this, the reasoning model dynamically interacts with external tools e, which are gener-"}, {"title": "Preliminary", "content": "We model the generation of r and a using the following joint probability formulation:\n\n$P(r, a | o,q, e, k) = \\prod_{t=1}^{T_r} P(r_t | r_{<t}, o, q, e_{<t}, k_{\\leq t}) \\\\\n\\times \\prod_{t=1}^{T_a} P(a_t | a_{<t}, r, o, q, e, k)$"}, {"title": "Agentic Reasoning Pipeline", "content": "Our core idea is to enhance the model reasoning by deploying external LLM-based agents during reasoning. The framework enables the reasoning LLM model interacts with external information in an agentic way. During its reasoning process, it could call the external tools to help solve the problem and also with a structured memory, called Mind Map, to store its reasoning context. At its core, an agentic mechanism empowers the model to determine, in real-time, when additional information is required. whenever the model identify the external information is needed during its reasoning, it will proactively embeds specialized tokens into its reasoning tokens. These tokens can be generally categorized to web-search token, coding token, and mind-map calling token. Together with token, the reasoning model would also generate a precise query as a message to interact with these external agents, based on the reasoning context developed so far.\nUpon detecting such a token, the reasoning process temporarily halts to extract the query and its reasoning context. Those are then dispatched to external agents, such as search engines or Mind"}, {"title": "Mind Map Agent", "content": "We construct a Mind Map to store and structure the real-time reasoning context of the reasoning model. This Mind Map is built by transforming raw reasoning chains into a structured knowledge graph. Specifically, we use a graph-construction LLM to extract entities from the reasoning chain and identify semantic relationships between related entities, following a process similar to that used in GraphRAG (Edge et al., 2024).\nThe Mind Map serves two primary functions. First, it clusters reasoning context into distinct groups and summarizes each theme. This is achieved by applying community clustering (Edge et al., 2024) on the knowledge graph and using an LLM to generate concise summaries for each group. Second, the knowledge graph can be queried with specific questions, such as \u201cWho was Jason's maternal great-grandfather?\u201d Using standard retrieval-augmented generation (RAG) on the knowledge graph (Edge et al., 2024), we retrieve and return relevant information.\nThese functions integrate the Mind Map into various aspects of the Agentic Reasoning process. It provides contextual reasoning support to external tools, enabling them to generate more context-aware responses (as discussed in later sections). Additionally, when the reasoning model is uncertain about its claims or loses track in an extended reasoning process, it can query the Mind Map for relevant information, treating it as an external tool, and continue reasoning based on the retrieved answer."}, {"title": "Web-search Agent", "content": "A search agent is invoked to retrieve the most relevant documents from the web. Rather than incorporating the web pages in their raw form, they are temporarily held for further processing. This ensures that only the most pertinent information is extracted and integrated into the main reasoning chain, maintaining coherence and relevance.\nOnce the relevant web pages are retrieved by the search agent, we use LLM to extract a concise, rephrased summary of the content most relevant to the ongoing reasoning context. This agent processes the web pages in the context of both the user query and the reasoning context, distilling key insights that are directly applicable to the problem at hand. The format and length of the summary adapt dynamically based on the reasoning task, for example, for factual queries like \u201cWhat is the population of the US in 2024? the result would be a simple numerical answer. For exploratory reasoning like finding a new perspective on a topic, the search agent would provide a summerized, detailed, nuanced viewpoint. For hypothesis validation like assessing supporting evidence for an assumption, the result would include the degree of support or contradiction found in the retrieved web-pages. This processed snippet is then integrated into the main reasoning process at the appropriate juncture, ensuring that external insights enhance rather than disrupt logical flow."}, {"title": "Coding Agent", "content": "Instead of prompting the reasoning model to generate code directly, we find it more efficient to delegate coding tasks to a specialized coding LLM. The reasoning model sends the relevant context and query message to the coding LLM, which then writes the required code, executes it via a compiler, and returns the results. This approach ensures that the reasoning model remains focused on its core reasoning process without being disrupted by coding tasks, allowing for longer and more coherent reasoning chains. Specifically, we format the coding request as follows: \"Write code to perform <code message from reasoning model> given the context <reasoning context from Mind Map> to answer the query <user query>.\" The coding LLM is instructed to always return its output in natural language, ensuring seamless integration with the reasoning model."}, {"title": "Main Findings", "content": "Less is More Unlike general agentic frameworks that provide models with a large selection of external tools, we find that just two-web search and coding-are sufficient for most tasks, even those requiring expert-level proficiency. Adding more tools can degrade performance by increasing the"}, {"title": "Delegating Tasks to LLM-Based Agents", "content": "Distributing computational workloads across multiple LLM-based agents improves efficiency. Instead of having the main reasoning model handle all tool-related tasks (e.g., writing code or constructing a knowledge graph), or calling non-LLM tools like pure search engine or code compiler, we delegate these tasks to specialized LLM-Based Agents, like a coding LLM generates code based on the query and context from the main reasoning model, or a knowledge-graph LLM constructs structured representations (e.g., a Mind Map) from the reasoning chain. This approach offers two key advantages:1. Minimizing Disruptions. The main reasoning model can maintain longer, more coherent reasoning without being distracted by auxiliary tasks or exceeding token limits. 2. Leveraging Specialization. Different LLMs excel at different tasks for instance, DeepSeek-R1 specializes in reasoning, while Claude-Sonnet excels at coding. By assigning tasks to models best suited for them, we achieve higher overall performance."}, {"title": "Agentic Test-time Scaling?", "content": "For a single question, we find reasoning chains that utilize more tool calls tend to yield better results. While across different questions, those requiring excessive tool usage often indicate inherent ambiguity or inaccuracy in the initial reasoning. This insight can be leveraged as a test-time reasoning verifier. By selecting the reasoning chain with the highest tool usage, we can implement best-of-N selection or beam search, which are techniques commonly used in mathematical and coding reasoning tasks as they can easily build a verifier, to open-domain, knowledge-intensive Q&A, improving accuracy and robustness."}, {"title": "Experiments", "content": "We evaluate our Agentic Reasoning model on the GPQA dataset, a PhD-level multiple-choice science QA benchmark. The dataset consists of expert-authored questions spanning physics, chemistry, and biology. Our primary experiments focus on the high-quality Diamond Set, which contains 198 questions, while Table 2 presents results on the broader Extended Set of 546 questions, allowing for a direct comparison with human experts.\nAs shown in Table 1, our findings show that large reasoning models such as DeepSeek-R1-Lite and"}, {"title": "Deep Research", "content": "We conduct an evaluation of Agentic Reasoning for deep research in open-ended Q&A tasks. A group of PhD-level experts in finance, medicine, and law were asked to formulate 15 to 30 professional research questions closely related to their respective fields. These questions were designed to require at least 20 minutes of in-depth research to answer comprehensively.\nWe assess the accuracy and reliability of reports generated by our Agentic Reasoning model, measuring the pass rate the percentage of responses deemed satisfactory by domain experts. We compare this pass rate against Gemini Deep Research Service (experiments with OpenAI's Deep Research are ongoing). As shown in Figure 3, our findings show that Agentic Reasoning outperforms Gemini Deep Research across all three domains, demonstrating the effectiveness of structured reasoning and tool-augmented frameworks in conducting deep research."}, {"title": "Analysis", "content": ""}, {"title": "Test-time Scaling", "content": "In our deep research study, we find that increased tool usage improves performance on the same question. As shown in Figure 3, a higher number of tool calls by the reasoning model correlates with an increased pass rate in deep research tasks. How-"}, {"title": "The Role of Mind Map", "content": "First, Mind Maps help correctly answer tricky logic-based questions that frequently fool LLMs. A well-known example is a modified riddle: \"The surgeon, who is the boy's father, says 'I can't operate on this child, he's my son!' Who is the surgeon to the boy?\" DeepSeek-R1 took 17 seconds to process this question but still produced the wrong answer, a failure also observed in models from the GPT and Gemini series models. These models often fall for a political-correct corpus contaminated response, failing to recognize the obvious logical structure. However, in our Agentic Reasoning framework, the use of a Mind Map allows the model to explicitly analyze the logical relationships between the entities [surgeon], [boy], and [father], leading to the correct answer.\nSecond, Mind Maps enhance deductive reasoning in strategic games. We test our approach in Werewolf, a classic social deduction game where players take on hidden roles as either villagers or werewolves. Villagers attempt to identify the werewolves, while werewolves deceive the group and eliminate players without being caught. The game alternates between \"night\", where werewolves secretly attack, and \"day\", where players debate and vote on eliminations. To evaluate our Agentic Reasoning model, we invited seven experienced Werewolf players (5+ years of experience) to play against it. The model achieved an impressive 72% win rate, significantly exceeding both the expected statistical win rate and the performance of human players in our experiment.\nWe analyzed the Mind Maps generated by the Agentic Reasoning model over multiple rounds of play, as shown in Figure 5. These visual structures helped the model track the relationships between different players based on their spoken arguments, allowing it to more accurately identify deception strategies, anticipate voting behaviors, and optimize its own disguise tactics. This result demonstrates that Mind Mapping is not just a tool for logic puzzles but also a powerful strategy enhancer in dynamic reasoning environments."}, {"title": "Conclusion", "content": "We introduced Agentic Reasoning, a framework that enhances LLM reasoning by integrating external agents for structured memory (Mind Map), web search, and computational analysis. This approach improves logical coherence, factual accuracy, and deep research capabilities. Our evaluations show that Agentic Reasoning outperforms existing models on expert-level QA and real-world research tasks, demonstrating its ability to synthesize knowledge effectively. The structured use of external tools enables more interpretable and verifiable reasoning, paving the way for AI systems capable of expert-level problem-solving. Future work will explore extending this framework to multimodal data and real-time adaptability, further advancing Al's ability to tackle complex, real-world challenges."}]}