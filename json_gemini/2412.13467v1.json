{"title": "Transducer Tuning: Efficient Model Adaptation for Software Tasks Using Code Property Graphs", "authors": ["Imam Nur Bani Yusuf", "Lingxiao Jiang"], "abstract": "Large language models have demonstrated promising performance across various software engineering tasks. While fine-tuning is a common practice to adapt these models for downstream tasks, it becomes challenging in resource-constrained environments due to increased memory requirements from growing trainable parameters in increasingly large language models. We introduce Transducer Tuning, a technique to adapt large models for downstream code tasks using Code Property Graphs (CPGs). Our approach introduces a modular component called Transducer that enriches code embeddings with structural and dependency information from CPGs. The Transducer comprises two key components: Graph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE extracts CPGs from input source code and transforms them into graph feature vectors. ABFL then fuses those graph feature vectors with initial code embeddings from a large language model. By optimizing these transducers for different downstream tasks, our approach enhances the models without the need to fine-tune them for specific tasks. We have evaluated Transducer Tuning on three downstream tasks: code summarization, assert generation, and code translation. Our results demonstrate competitive performance compared to full parameter fine-tuning while reducing up to 99% trainable parameters to save memory. Transducer Tuning also remains competitive against other fine-tuning approaches (e.g., LORA, Prompt-Tuning, Prefix-Tuning) while using only 1.5%-80% of their trainable parameters. Our findings show that integrating structural and dependency information through Transducer Tuning enables more efficient model adaptation, making it easier for users to adapt large models in resource-constrained settings.", "sections": [{"title": "1 Introduction", "content": "Large language models have demonstrated promising performance across various software engineering tasks, such as code generation (Zan et al., 2023), code summarization (Wan et al., 2024; Z. Zheng et al., 2023), and code repair (Jin et al., 2023; Xia, Wei, & Zhang, 2023). \"Pretrain, then fine-tune\" is a common practice to adapt the models for downstream tasks (Ahmad, Chakraborty, Ray, & Chang, 2021; Devlin, Chang, Lee, & Toutanova, 2019; Feng et al., 2020; Guo et al., 2021; Wang et al., 2023). These models are often pretrained by large organizations on huge corpora, learning code syntax, semantics, and patterns (Z. Han, Gao, Liu, Zhang, & Zhang, 2024; Shi et al., 2023). Users can then adapt the pretrained models to their specific needs through fine-tuning on domain-specific data. Model fine-tuning often works by iteratively updating the models' parameters through gradient descent (Rumelhart, Hinton, & Williams, 1986), using input-output examples for specific downstream tasks. Through this optimization process, the model's parameters gradually align with the patterns and objectives of the downstream task, leading to improved task-specific performance.\nOne significant challenge in fine-tuning large language models is the substantial GPU memory required to store gradients and optimizer states during parameter updates. The memory demand increases with the models' parameter counts. For example, in our preliminary experiment, we fine-tuned two variants of CodeT5+ (Wang et al., 2023) on the assert generation training dataset (Watson, Tufano, Moran, Bavota, & Poshyvanyk, 2020) (approximately 5K examples) using identical settings and compared their peak memory consumption: The 220-million parameter variant required 12.1GB of GPU memory, while the larger 770-million parameter variant consumed 37.7GB. Such memory demand challenge is increasingly pronounced as recent models (Luo et al., 2024; Muennighoff et al., 2024; T. Zheng et al., 2024) continue to grow, often containing orders of magnitude more parameters than their predecessors from just a few years ago (Devlin et al., 2019; Feng et al., 2020; Guo et al., 2021).\nPrior studies have proposed various efficient adaptation techniques, such as Adapter-based (Bapna & Firat, 2019; Houlsby et al., 2019; E.J. Hu et al., 2022; Hyeon-Woo, Ye-Bin, & Oh, 2022; Kopiczko, Blankevoort, & Asano, 2024; H. Liu et al., 2022; Pfeiffer, Kamath, R\u00fcckl\u00e9, Cho, & Gurevych, 2021; Pfeiffer, R\u00fcckl\u00e9, et al., 2020; Pfeiffer, Vulic, Gurevych, & Ruder, 2020; Ponti, Sordoni, & Reddy, 2022; Yeh et al., 2024) and Prompt-based (Lester, Al-Rfou, & Constant, 2021; Li & Liang, 2021; X. Liu et al., 2022) methods to address the memory requirement challenge. Adapter-based methods introduce additional trainable parameters into a model and update only these parameters during the fine-tuning stage rather than the entire model. On the other hand, embedding-based methods modify the output of the embedding layer before feeding it to the initial encoder/decoder layer of the model.\nThe existing efficient fine-tuning methods for code-related tasks in software engineering face two limitations. First, they involve an inherent trade-off between parameter efficiency and model performance, where reducing trainable parameters can degrade performance compared to full parameter fine-tuning (J. Liu, Sha, & Peng, 2023). Second, these methods fail to leverage inherent structural and dependency information in source code as they rely solely on the sequential representation of code. Prior studies have demonstrated that incorporating structural and dependency information"}, {"title": "2 Background on Code Property Graphs", "content": "Code Property Graphs (CPGs) (Yamaguchi et al., 2014) unify the Abstract Syntax Tree (AST), Control Flow Graph (CFG), and Program Dependence Graph (PDG).\nThe AST shows the structure of statements and expressions, the CFG outlines the"}, {"title": "3 Transducer Tuning", "content": "3.1 Transducer's Architecture\nFigure 2 shows the high-level architecture of Transducer Tuning. Transducer introduces a novel architecture comprising two primary components: Graph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE processes input source code by extracting and transforming Code Property Graphs (CPGs) into feature vectors. ABFL integrates these features with code embeddings generated by the underlying language model, referred to as the backbone model."}, {"title": "3.1.1 Graph Vectorization Engine", "content": "Graph Vectorization Engine (GVE) consists of three subcomponents that work in sequence: Graph Extractor, Graph Vectorizer, and Graph Processing Layer. (1) The Graph Extractor employs a static code analysis tool to extract the CPG from input code c. The CPG is represented as a node list N and edge list E. (2) The Graph Vectorizer then transforms each node label $n_i \\in N$ into a vector representation through a mapping function F: x \u2192 h. The function F can be implemented using various approaches, such as pre-trained embedding models, TF-IDF, or binary vectors. The output is a set of initial node vectors $H_{init}$, where each vector h\u2208 $H_{init}$ has dimension $d_{init}$. (3) The Graph Processing Layer transforms the initial vectors into a refined feature representation G with dimension $d_g$. As illustrated in Figure 3, this layer comprises five sequential components: Normalization (Ba, Kiros, & Hinton, 2016; B. Zhang & Sennrich, 2019), Down Projection, Feature Generator, Up Projection, and Mean Pooling. Each component serves a specific purpose in the processing pipeline.\nThe first component in the processing pipeline is Normalization, which implements Root Mean Squared (RMS) normalization (B. Zhang & Sennrich, 2019). For each element $h_i$ in the input vector, RMS normalization is computed as:\n$h_{norm,i} = \\frac{h_i}{RMS(h)} g_i$, where $RMS(h) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} h_i^2}$                                                                            (1)\nwhere n is the dimension of the input vector and $g_i$ is a learnable scale parameter. Layer normalization is a technique that stabilizes the distributions of intermediate layer outputs (Xu, Sun, Zhang, Zhao, & Lin, 2019). By applying Equation (1) to each node vector $h_{init,i}\\in H_{init}$, this layer ensures consistent scaling across all inputs. This normalization has three benefits (Xu et al., 2019): it smooths gradient flow during training, accelerates training convergence, and enhances models' generalization.\nFollowing Normalization, Down Projection performs dimensionality reduction through a learned transformation. This layer applies trainable weights $W_{down}$ to each"}, {"title": "3.1.2 Attention-Based Fusion Layer", "content": "Figure 4 shows the architecture of Attention-Based Fusion Layer (ABFL). ABFL integrates two key inputs: a graph feature vector G derived from the transducer"}, {"title": "3.2 Usage Scenario", "content": "Using Transducer Tuning involves two key stages: training and inference. In the training stage, the service provider or end-user selects a backbone model and trains a Transducer component using input-output samples from the target task. During this process, only the Transducer's parameters are updated while the backbone model remains frozen. During inference, the trained Transducer enriches input embeddings that are generated by the backbone model for the target task. When a new downstream task emerges, users can train an additional Transducer while keeping the same backbone model. They simply need to deploy the new Transducer while leaving the backbone model and any existing Transducer unchanged. This modularity also means that any Transducer can be removed when no longer needed without impacting the backbone model or other Transducer that serve different tasks."}, {"title": "4 Experimental Setting", "content": "4.1 Datasets\nWe evaluate Transducer Tuning on three downstream tasks: code summarization, assert statement generation, and code translation. We selected datasets that are widely used in recent studies. For code summarization, we use the clean Java subset (Shi et al., 2022) from CodeSearchNet (Husain, Wu, Gazit, Allamanis, & Brockschmidt, 2019). This dataset contains Java methods with their corresponding Javadoc descriptions. For assert statement generation, we use the dataset created by Watson et al. (Watson et al., 2020). This dataset pairs test methods with their assert statements. For code translation, we use data from CodeXGLUE (et al., 2021). This dataset contains parallel code snippets that implement the same functionality in Java and C#. Each dataset comes from open-source GitHub projects.\nDataset Decontamination and Preprocessing. We utilize preprocessed and cleaned datasets that have been divided into training, validation, and testing sets by their respective authors. To ensure data integrity, we first check for potential leakage between splits across all datasets. This process involves two stages: first, we remove exact matches between splits, and then we eliminate near-duplicates using Locality Sensitive Hashing (LSH) and MinHash. Specifically, we tokenize each instance and generate a MinHash signature for each one, which efficiently estimates the Jaccard similarity between instances. We then use LSH to group similar items together and remove those with a similarity score greater than 0.8. As a result, 41% to 53% of instances are retained from the original test split for code-to-code translation tasks, while 98% of instances are maintained for code summarization.\nThen we extract Code Property Graphs (CPGs) for each method in the datasets using Joern.\u00b9. Then, we generate node vectors within the CPGs by converting node labels using the generic embedding model mxbai-embed-large-v1.2 We chose mxbai-embed-large-v1 because it performs the best among the small models on the Massive"}, {"title": "4.2 Transducer Tuning Implementation", "content": "We use CodeT5+ (Wang et al., 2023) as the backbone model. We selected this model based on its adoption in recent studies (Luo et al., 2024; Muennighoff et al., 2024; Weyssow, Zhou, Kim, Lo, & Sahraoui, 2023; T. Zheng et al., 2024) and the availability of smaller variants (i.e., 220M and 770M parameters) that can be fine-tuned on our local workstation. These variants each receive over 10K monthly downloads on HuggingFace as of August 2024. For Transducer Tuning implementation, we use GATv2 (Brody, Alon, & Yahav, 2022) as the Feature Generator, with both the down-projection dimension ($d_{down}$) and attention-based fusion dimension ($d_{abf}$) set to 8, which is inspired from the prior study (E.J. Hu et al., 2022)."}, {"title": "4.3 Baselines", "content": "We evaluate Transducer Tuning against both standard and efficient fine-tuning baselines. For standard baselines, we use full fine-tuning (upper bound with all backbone parameters optimized), no fine-tuning (lower bound using only pre-trained state), and Linear Adapter (a linear layer transforming embeddings before the encoder).\nWe also compare against efficient fine-tuning methods: LoRA (E.J. Hu et al., 2022), Prefix-Tuning (Li & Liang, 2021), and Prompt-Tuning (Lester et al., 2021). For LORA, we tune the rank (r) of trainable matrices with values 4, 8, injecting them into the query and key vectors of attention layers. For Prefix-Tuning and Prompt-Tuning, we adjust the prefix length (p) and soft-prompt length (s) respectively, with values 5, 10, 25, 50. All the values in the search space come from ablation studies in the original papers (E.J. Hu et al., 2022; Lester et al., 2021; Li & Liang, 2021).\nTo ensure fair comparison, we tune each method's hyperparameters to achieve optimal performance with minimal trainable parameters. The tuning uses 20% of the training data and evaluates on the full validation set, with separate tuning for each task and backbone model combination. We report the selected hyperparameters for each task in Appendix B. In addition, we detail other hyperparameters and environment settings in Appendix C."}, {"title": "4.4 Metrics", "content": "We evaluate Transducer Tuning against the baselines on both efficiency and performance dimensions. Our primary goal is to minimize the number of trainable parameters while maintaining competitive performance. For efficiency, we compare"}, {"title": "5 Results", "content": "We evaluate Transducer Tuning on two task categories: code-to-natural language (code summarization) and code-to-code tasks (assert generation and code translation). Table 1 presents the performance comparison against the baselines in these tasks.\nFirst, we compare Transducer Tuning with No-Fine-tuning baseline. For CodeT5+ 220M, Transducer Tuning improves code summarization by 4.35 points (from 95.49 to 99.84), assert generation by 5.47 points (from 76.85 to 82.32), and code translation by 2.13 points (from 94.47 to 96.60). For CodeT5+ 770M, the improvements are larger: 10.21 points in code summarization, 7.03 points in assert generation, and 4.78 points in code translation.\nTakeaway 1: Transducer Tuning achieves substantial improvements over No-Fine-tuning baselines (2.13-10.21 points) for both CodeT5+ 220M and 770M.\nNext, we compare performance across tuning methods. For CodeT5+ 220M, Transducer Tuning (99.84) achieves comparable results to other methods in code summarization, with differences of less than 0.1 points compared to Full Fine-tuning (99.91), LORA (99.91), Prefix-Tuning (99.93), and Prompt-Tuning (99.91). In assert generation, Transducer Tuning (82.32) shows slightly lower performance, with gaps ranging from 0.16 to 0.85 points compared to other methods (Linear Adapter: 82.48,"}, {"title": "6 Discussion", "content": "6.1 The Usefulness of Graph Information\nTo evaluate the impact of incorporating graph information in our approach, we conducted experiments with three variants. The first variant, \"GVE + ABFL,\" represents our complete approach using Graph Vectorization Engine (GVE) with Graph Neural Networks (GNN) and Attention-Based Fusion Layer. The second variant, \"GVE-only,\" removes the ABF layer and combines the graph features from GNN directly with the input code embeddings through summation. The third variant, \"ABFL-only,\" excludes the GVE component to assess the model's performance without graph information.\nTable 3 shows the results of our ablation study. For CodeT5+ 220M, both GVE+ABF and GVE-only variants outperform the ABFL-only variant in all tasks, with GVE+ABF achieving 96.60 in code translation compared to ABFL-only's 92.53, and 99.84 in code summarization compared to ABFL-only's 94.33. In assert generation, GVE+ABF and GVE-only reach 82.32 and 83.14 respectively, while ABFL-only achieves 77.07. For CodeT5+ 770M, the impact of graph information varies by task. In code translation, GVE+ABF (94.88) and GVE-only (97.78) outperform ABFL-only (91.11) by 3.77-6.67 points, while in code summarization and assert generation, ABFL-only performs marginally better by 0.53 and 2 points respectively."}, {"title": "6.2 Generalizibility of Transducer Tuning", "content": "While Transducer Tuning requires input data that can be represented as a graph, it offers broader applicability as Transducer Tuning is not limited to Code Property Graphs (CPGs) and can work with various graph structures such as other code graphs, social network graphs, or knowledge graphs. This flexibility means Transducer Tuning can adapt large language models across diverse domains where relationships between elements can be captured in graph forms. The key requirement is that the input graph data can be represented as adjacency matrix."}, {"title": "6.3 The Choice of Code Property Graphs", "content": "The choice of Code Property Graphs (CPGs) as our main graph modality is guided by prior studies (J. Han, Huang, Sun, Liu, & Liu, 2023; J. Liu, Zeng, et al., 2023; R. Liu et al., 2024) which suggest that CPGs offer richer information compared to other types of code graphs and can improve performance. The primary focus of our work is to explore whether graph modality can be effectively used for efficient model adaptation, rather than determining the optimal graph representation for code. While we acknowledge that an ablation study comparing different types of graphs, such as CST or control-flow enriched CST, could provide valuable insights into the effectiveness of Transducer Tuning, such comparison is beyond the scope of our current investigation of graph-based efficient fine-tuning. It is worth noting that different graph representations would require different graph extractors (e.g., Joern for CPGs, tree-sitter for ASTs), but these extractors are modular components that can be easily swapped without affecting the core architecture of our approach. We leave this comprehensive exploration of different graph representations for future work."}, {"title": "6.4 Threats To Validity", "content": "Internal Validity. Hyperparameter selection could affect our study's internal validity. We managed this risk by tuning hyperparameters for each task with a validation split."}, {"title": "7 Related Works", "content": "Several techniques have been proposed to adapt pretrained models for downstream tasks, with direct fine-tuning being a common approach. This method updates all model parameters using task-specific data, typically a small set of input-output examples. Direct fine-tuning has proven successful across various software engineering tasks: code repair (Jiang, Lutellier, & Tan, 2021; Mastropaolo et al., 2021; Tian et al., 2020), code generation (Yusuf et al., 2023, 2022), code mutant injection (Mastropaolo et al., 2021), code summarization (Wei, Li, Xia, Fu, & Jin, 2019), assert generation (Watson et al., 2020), and vulnerability detection (Chakraborty, Krishna, Ding, & Ray, 2022; Fu, Tantithamthavorn, Le, Nguyen, & Phung, 2022). However, as models grow larger, memory requirements increase due to the growing number of trainable parameters. This challenge has led to the development of more efficient adaptation techniques, primarily Adapter-based and Prompt-based methods.\nAdapter-based methods (Bapna & Firat, 2019; Houlsby et al., 2019; E.J. Hu et al., 2022; Hyeon-Woo et al., 2022; Kopiczko et al., 2024; H. Liu et al., 2022; Pfeiffer et al., 2021; Pfeiffer, R\u00fcckl\u00e9, et al., 2020; Pfeiffer, Vulic, et al., 2020; Ponti et al., 2022; Yeh et al., 2024) introduce additional trainable parameters into the backbone model. During adaptation, only these new parameters are updated, leaving the original model unchanged. However, this approach requires careful consideration of parameter placement, demanding knowledge of the model architecture. In contrast, Transducer Tuning simplifies adaptation by modifying only the input embeddings before processing through the encoder and/or decoder. This makes Transducer Tuning easily applicable to any existing language model without requiring end-users to understand the model's internal architecture."}, {"title": "8 Conclusion", "content": "We present Transducer Tuning, a novel technique for adapting large models to downstream code tasks using Code Property Graphs (CPGs). At its core, Transducer Tuning uses a modular Transducer component that enriches code embeddings with structural, control-flow, and dependency information extracted from source code. Transducer has two key components: Graph Vectorization Engine (GVE), which converts CPGs into graph feature vectors, and the Attention-Based Fusion Layer (ABFL), which integrates these vectors with initial code embeddings. By optimizing only the Transducer component for each task, Transducer Tuning enhances model input embeddings without requiring task-specific fine-tuning of the underlying model. Our experimental results demonstrate that Transducer Tuning achieves comparable performance to full parameter fine-tuning and existing efficient fine-tuning methods while using significantly fewer parameters, making it easier for users to adapt large language models in resource-constrained settings.\nIn future work, we plan to explore two key directions. First, we will investigate alternative code features beyond CPGs for adapting large language models with Transducer Tuning, as different features may prove more effective for specific downstream tasks. Second, we will study the transferability of these features across programming languages, with particular emphasis on low-resource scenarios where training data is limited. These investigations could provide valuable insights for improving model adaptation in diverse software engineering tasks."}]}