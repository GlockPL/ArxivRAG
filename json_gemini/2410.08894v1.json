{"title": "CONDITIONAL GENERATIVE MODELS FOR CONTRAST-ENHANCED SYNTHESIS\nOF T1W AND T1 MAPS IN BRAIN MRI", "authors": ["Moritz Piening", "Fabian Altekr\u00fcger", "Gabriele Steidl", "Elke Hattingen", "Eike Steidl"], "abstract": "Contrast enhancement by Gadolinium-based contrast agents\n(GBCAs) is a vital tool for tumor diagnosis in neuroradi-\nology. Based on brain MRI scans of glioblastoma before\nand after Gadolinium administration, we address enhance-\nment prediction by neural networks with two new contribu-\ntions. Firstly, we study the potential of generative models,\nmore precisely conditional diffusion and flow matching, for\nuncertainty quantification in virtual enhancement. Secondly,\nwe examine the performance of T1 scans from quantitive MRI\nversus T1-weighted scans. In contrast to T1-weighted scans,\nthese scans have the advantage of a physically meaningful and\nthereby comparable voxel range. To compare network predic-\ntion performance of these two modalities with incompatible\ngray-value scales, we propose to evaluate segmentations of\ncontrast-enhanced regions of interest using Dice and Jaccard\nscores. Across models, we observe better segmentations with\nT1 scans than with T1-weighted scans.", "sections": [{"title": "1. INTRODUCTION", "content": "T1-weighted (T1w) MRI sequences after the administration\nof GBCAs play a crucial role in the diagnosis of brain tu-\nmors [15]. Yet, GBCAs are under discussion due to the\npossible retention in tissues and the additional acquisition\nscan time. Therefore, it is an ongoing quest to eliminate\nGadolinium administration with synthetic MR predictions\nor to allow for lower doses by employing neural networks\n[6]. Usual (end-to-end (E2E)) neural networks provide syn-\nthesized post-contrast images either from the pre-contrast\nscans only [6, 10], or using both pre-contrast and low-dose\nscans [3, 8, 9]. The second method does not reduce the scan\ntime but improves the prediction of small enhancement ar-\neas significantly. However, such areas play a smaller role in\nglioblastoma examination. For a comprehensive comparison\nof these two methods including different MRI modalities\n(T1w, T2w, FLAIR) we refer to [1]. For this paper, we had\naccess to MRI brain scans before and after Gadolinium ad-\nministration, where in addition to the T1w sequences, also"}, {"title": "2.1. Contrast-Enhancement in T1w and T1", "content": "Given pairs of pre-contrast and post-contrast 3D MR scans in\n$\\mathbb{R}^{H\\times W\\times D}$, we follow [8] and extract 2.5D pre-contrast slices\n$y \\in \\mathbb{R}^{d} = \\mathbb{R}^{H\\times W \\times 7}$ and 2D post-contrast slices $x \\in \\mathbb{R}^{d} =$\n$\\mathbb{R}^{H\\times W}$ with central slice of y aligned to x. Considering 2.5D\ncubes consisting of multiple axial slices allows us to ensure\nspatial robustness and to account for the depth dimension.\nAs in [9], we use the voxel-wise difference between pre- and\npost-contrast scans for learning neural networks to predict x\ngiven y. We deal with two MRI modalities.\nT1w MRI produces images, where tissues with short T1 re-\nlaxation times (like fat) appear bright, while those with long\nT1 times (like water) appear darker. Despite excellent quali-\ntative images, voxel-wise values do not carry physical mean-\ning and depend on the scanner setup. Consequently, voxel\nnormalization is an important pre-processing step. Here, we\nscale each pre- and post-contrast pair by dividing with the\nmaximal pre-contrast 2.5D slice voxel.\nT1 qMRI scans, based on calculated relaxometry, measure\nthe actual T1 relaxation times of tissues. This leads to a mean-\ningful voxel range. T1 relaxation times are calculated by ac-"}, {"title": "2.2. Neural Networks", "content": "End-to-End Models train a neural network $G_\\theta: \\mathbb{R}^d \\to$\n$\\mathbb{R}^d$ that approximates a mapping from pre-contrast scans\nto full-dose post-contrast scans based on empirical pairs\n$\\{(x_i, y_i)\\}_{i=1}^N \\sim P_{X,Y}$, where $P_{X,Y}$ denotes the joint distri-\nbution of random variables $X \\in \\mathbb{R}^d$ and $Y \\in \\mathbb{R}^d$. Training\ncan be achieved by minimizing a voxel-wise loss, here the\nmean absolute error (MAE)\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{(x,y)\\sim P_{X,Y}}[||G_\\theta(y) - x||_1]$.\nFor each pre-contrast scan y, such networks predict a single\npost-contrast scan $x = G_\\theta(y)$, see Fig. 1. For alternative loss\nfunction choices see [8].\nConditional Generative Models learn, based on samples\n$\\{(x_i, y_i)\\}_{i=1}^N \\sim P_{X,Y}$, a conditional network $T_\\theta(y, \\cdot)$ that\ngenerates samples from the posterior distribution $P_{X|Y=y}$\nby pushforwarding a latent Gaussian distribution $P_Z$, i.e.\n$P_{X|Y=y} \\approx T_\\theta^{-1}(y, \\cdot)_{\\sharp}P_Z := P_Z(T_\\theta^{-1}(y, \\cdot))$. By sampling\nfrom our learned posterior distribution, we get several scan\npredictions, see Fig. 1 bottom. This allows insights into\nstatistical properties like the mean and voxel-wise standard\ndeviation, which provides a tool to quantify prediction uncer-\ntainty. We examine two state-of-the-art generative models.\n1. Diffusion Models (DMs) [13] employ stochastic differential\nequations, where the forward process transforms a target dis-\ntribution to a Gaussian one by $dX_t = -a_tX_tdt + \\sqrt{a_t}dW_t$,\n$X_0 \\sim P_{X|Y=y}$, where $W_t$ denotes the Brownian motion and\n$a_t$ is a positive, increasing noise schedule, and the reverse\nprocess transforms a Gaussian to the target distribution by\n$dX_{T-t} = a_{T-t}(X_{T-t} + \\nabla \\log p_{T-t}(X_{T-t}|Y = y))dt$\n$+ \\sqrt{a_{T-t}}dW_{T-t}, X_T \\sim \\mathcal{N}(0, I_d)$.\nA neural network approximates the conditional score $s_\\theta :=$\n$\\nabla \\log p_t$ for a fixed noise levels $a_t$ by maximizing the 'evi-\ndence lower bound' loss. Given the score, we simulate the\nreverse process to transform Gaussian to posterior samples,\nsee Fig. 1 bottom. Here, we use the image-to-image diffusion\nmodel Palette [12].\n2. Flow Matching (FM) [2, 5] learns the vector field $v_\\theta: \\mathbb{R}^d \\times$\n$\\mathbb{R}^d \\to \\mathbb{R}^d$ of an ordinary differential equation for $t \\in [0, 1]$,\n$\\frac{d}{dt}\\Phi_t(x, y) = v_\\theta(\\Phi_t(x, y)), \\Phi_0(x, y) = (x, y), \\qquad(1)$"}, {"title": "3. EXPERIMENTS", "content": "We conduct the following experiments to analyze the benefit\nof conditional generative models and quantitative MRI."}, {"title": "3.1. Dataset", "content": "We used the MRI exams of 90 brain tumor patients acquired\nwith a field strength of 3T (Magnetom Skyra-fit or Prisma,\nSiemens Healthineers, Erlangen, Germany); for the detailed\nMRI sequence parameters see [7]. Among these patients 69\nhave been diagnosed with high-grade gliomas (HGG) and 21\nwith metastases (MET). For each exam, we have paired coreg-\nistered and skull-stripped pre-contrast and post-contrast T1w\nand T1 scans available. Focussing on HGG patients, we use\na training set of axial slices from 64 HGG patients, where we\nremove peripheral slices without tumors. For each of the ad-\nditional five HGG patients, segmentation masks with regions\nof interest (ROIs) were provided by a trained radiologist. For\nour HGG test set, we filter for slices that overlap with the\nROIs. From the remaining slices, we use every fifth slice\nfor the final HGG test (107 in total). Additionally, we ex-\ntract four evenly-spaced central slices from each patient with\nmetastases (MET). We use the slices of four randomly se-\nlected MET patients as a validation set. The 68 slices of the\nremaining 17 MET patients are a MET test set."}, {"title": "3.2. Implementation", "content": "We train each model using only T1, resp. T1w data with\nthe same U-Net architecture. We employ the network im-\nplementation of [12] with 32 channels, two residual blocks,\n20% dropout, and 16 attention head channels. The DM noise\nscales $a_t$ are placed equidistantly between 1e-3 and 5e-2 for\n2000 steps. For FM, we provide our own implementation and\ndiscretize the ODE with 10 time steps and DOPRI-5. We train\neach model for 1500 epochs. We observe saturation, but no\noverfitting on the validation set due to extensive random ro-\ntation and cropping training augmentations. For each 7-slice-\nstack y, we generate 50 image samples with DM and FM."}, {"title": "3.3. Evaluation Metrics", "content": "We report the mean absolute error across all slices (MAE)\nand within the ROI (rMAE) and the SSIM [16] in Tab.\n1. In Tab. 2, we display the Pearson correlation coeffi-\ncient between the estimated FM StdDev and the absolute\nerror $\\frac{\\text{FM Mean} \\, \\text{Post}}{\\text{Pre} - \\text{Post}}$ (AE), resp. the relative error $\\frac{\\text{FM Mean} \\, \\text{Post}}{\\text{Pre} - \\text{Post}}$ (RE). We skip the voxels where Pre - Post =\n0. The same is computed for DM image samples. To compare\nthe network performance for T1w and T1 scans, which have\nincompatible gray value scales, we use thresholding in the\nROI to segment contrast-enhanced regions. For each slice,\nwe consider the voxel-wise absolute difference between the\nGT pre- and post-contrast slices and separate a specific per-\ncentage (1% - 30%) of the voxels with the highest intensities,\nresulting in a binary segmentation, see Fig. 3 third column.\nThis is compared with the same percentage of thresholded\npixels in the absolute difference between pre-contrast and\nE2E, FM mean, DM mean, resp., see Fig. 3 fifth and seventh\ncolumn. For the three cases, False Positives (FP), True Posi-\ntives (TP), and False Negatives (FN) can be detected leading\nto the Dice D and Jaccard J scores [14],\n$D = \\frac{2\\text{TP}}{2\\text{TP}+\\text{FP}+\\text{FN}}, \\qquad J = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}+\\text{FN}},$\nwhich range from 0 to 1 with 1 indicating perfect similarity.\nNote that the ground truth T1 and T1w segments should be\nidentical for every threshold up to measurement errors due\nto the relation between T1 and T1w scans, see Fig. 3, third\ncolumn. We choose thresholds of more than 30% to exclude\nnoise since only 42% of ROI voxels show any pre-post differ-\nence on average."}, {"title": "3.4. Results", "content": "In general, we observe a promising quality of all neural\nnetwork-generated enhancement predictions, see Fig. 5. Tab.\n1 shows on average (of 107 slices) the best performance\nfor E2E predictions, followed by DM and FM. However, as\nshown in Fig. 2, an E2E prediction may potentially be in-\nsufficient as often indicated by the standard deviation of a"}, {"title": "4. CONCLUSION", "content": "We have demonstrated encouraging results in Gadolinium en-\nhancement prediction based on conditional generative net-\nworks, particularly for uncertainty quantification. Finding a\nmeasure for comparing the prediction of T1 scans from qMRI\nwith (usual) T1w MRI scans via segmentation, we have at-\ntested a promising T1 performance. Still, these are prelimi-\nnary findings based on a very limited dataset. Incorporating\nlarger datasets and including either low-dose GBCA scans or\nadditional information from quantitative T2 or diffusion data\ncould be an exciting research direction."}]}