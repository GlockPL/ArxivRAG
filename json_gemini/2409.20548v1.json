{"title": "Robi Butler: Remote Multimodal Interactions with Household Robot Assistant", "authors": ["Anxing Xiao", "Nuwan Janaka", "Tianrun Hu", "Anshul Gupta", "Kaixin Li", "Cunjun Yu", "David Hsu"], "abstract": "In this paper, we introduce Robi Butler, a novel household robotic system that enables multimodal interactions with remote users. Building on the advanced communication interfaces, Robi Butler allows users to monitor the robot's status, send text or voice instructions, and select target objects by hand pointing. At the core of our system is a high-level behavior module, powered by Large Language Models (LLMs), that interprets multimodal instructions to generate action plans. These plans are composed of a set of open vocabulary primitives supported by Vision Language Models (VLMs) that handle both text and pointing queries. The integration of the above components allows Robi Butler to ground remote multimodal instructions in the real-world home environment in a zero-shot manner. We demonstrate the effectiveness and efficiency of this system using a variety of daily household tasks that involve remote users giving multimodal instructions. Additionally, we conducted a user study to lyze how multimodal interactions affect efficiency and user experience during remote human-robot interaction and discuss the potential improvements.", "sections": [{"title": "I. INTRODUCTION", "content": "Imagine a future where distance no longer constrains our ability to manage household tasks. Picture a robot assistant capable of remotely interpreting spoken commands and gestures to check your refrigerator or reheat a meal. Such a robotic system would fundamentally change the way we interact with our homes, bringing a new level of convenience and efficiency to daily life. In this work, we propose Robi Butler, a multimodal interaction system that enables seamless communication between remote users and household robots to execute various household tasks. Robi Butler allows users to leverage both natural language and gestures to command the robot to perform tasks remotely, see Fig. 1. Remote users can point to the desired object in the MR device and instruct the robot to manipulate it, move toward it, or ask questions about it, just like a real butler.\nThe core issue behind building such a robot assistant is how to allow the robot to remotely receive, understand, and ground the multimodal instructions into the executable actions in the home environment. To address this, we first design the communication interfaces consisting of a Zoom chat website and a gesture website for hand-pointing, which allows human users to send multimodal instructions using language and pointing remotely. To ground the received mul- timodal instructions in the home, the robot needs to have the ability to interpret and execute the open multimodal instruc- tions in real-world environments. Inspired by the advanced capabilities of foundation models to achieve open vocabulary mobile manipulation in domestic environments [1]\u2013[4], we aim to incorporate the LLM-based robots with the ability to make use of the language-related gestures. To allow the robot to ground both open language instruction and open pointing selection, we first implement a mobile manipulation system that supports open vocabulary action primitives with pointing selection in real-world household environments, driven by the recent advances in vision language models (VLMs). Then, we introduce a high-level behavior manager powered by large language models (LLMs), which organizes and aligns the received speech and gesture instructions to generate the plan to solve the task.\nOverall, the integrated system, Robi Butler, is a multi- modal interactive system for robotic home assistants that enables bi-directional remote human-robot interaction based on the real home environment through text, voice, video, and gesture. We evaluated the performance of Robi Butler on real-world daily household tasks and studied the benefits of such multimodal interaction in terms of efficiency and user experience in the remote human-robot interaction."}, {"title": "II. RELATED WORK", "content": "Effective communication interfaces are essential for Human-Robot Interaction (HRI). Natural language instruc- tion for robots has been widely explored in prior research, employing both traditional methods [5]\u2013[10] and language models [1], [11]\u2013[17]. However, language can be ambiguous and imprecise. Humans typically use nonverbal interaction, such as pointing, to supplement their verbal instructions [18]. Previous work explores the use of tools such as laser pointers [19] and point-and-click interfaces [20] to improve instruction delivery and further integrate both speech and relevant gestures together [21]\u2013[23] to specify the command more precisely. However, these systems typically rely on predefined word sets or task-specific in-domain model train- ing, which limits generalization. Recent work uses LLMs to interpret gestures and commands [24], but only handles short speech inputs and requires the user to be within the third-person camera view. Our system is built on top of a multimodal communication interface to construct a virtual clickable world that allows the remote user to select the target by pointing while speaking, and the robot could interpret and execute the multimodal instructions in the home environment with a mobile manipulator.\nIntelligent home robots with mobile manipulation ca- pabilities can greatly expand functionality and integrate more seamlessly into daily routines. While past household mobile manipulation systems have been developed both in simulation [25]\u2013[27] and real-world settings [28]\u2013[32], they generally struggle with human-robot interaction due to their reliance on predefined tasks and limited language input. They would require users to select from fixed options or explicitly re-programme the robot. More recent approaches leverage vision-language-based models (VLMs) to enable open-vocabulary mobile manipulation in domestic environ- ments [1]\u2013[4], but they rely solely on language instructions and lack closed-loop human-robot interaction. Another area of research explores treating robot assistants as \"physical avatars\", which allows remote users to teleoperate the robots using VR controllers [33], haptic devices [34], haptic gloves [35], and hand tracking [36]. However, these approaches can result in a high cognitive workload [37], making them impractical for everyday use. In this paper, we present a human-robot interaction system for remote user to naturally instruct open-vocabulary mobile manipulation with multi- round interaction using both language and gestures."}, {"title": "III. OVERVIEW", "content": "This work addresses the problem of remote human-robot interaction for household robot assistants. We present a multimodal system, Robi Butler, that combines speech com- mands and gesture inputs, allowing remote users to naturally guide a robotic assistant to perform household tasks."}, {"title": "A. System Overview", "content": "The developed Robi Butler system is illustrated in Fig. 2. It enables seamless interaction between a user wearing a Mixed Reality (MR) Head-Mounted Display (HMD) and a robot. Users can send text/voice instructions L and gesture selections G to the robot while receiving video streams and text/voice feedback F in return. The robotic system com- prises three key components. The communication interfaces C facilitate bidirectional communication, receiving user in- puts and transmitting robot feedback. The high-level behav- ior module H, interprets user instructions L and gesture selections (G) to understand the intent, generating an action sequence P = {a0, a1, ..., an } for the robot to execute, along with a response R to the user. This response can be low-level execution feedback or general information. The fundamental skills A, provide core functionality that allows the robot to perceive and interact with the environment. These include basic mobile manipulation and Visual Question Answering (VQA) capabilities: move(), pick(), placeon(), open(), close() and vqa(). Note that all skills except open() and close() support both text and pointing queries."}, {"title": "B. Hardware Setup", "content": "Our system integrates multiple hardware components to facilitate effective human-robot interaction. The primary user interface is an Oculus Quest 3 MR headset, while the robotic platform consists of a Fetch mobile manipulator [38] with a differential-drive base and a 7-dof arm. Tasks that require heavy computation are distributed between a local workstation powered by an NVIDIA RTX 4090 GPU and a remote cloud server. To enhance user visual feedback, we incorporate two additional cameras that provide third-person views of the robot's operational environment."}, {"title": "IV. SYSTEM IMPLEMENTATION", "content": "The system has a multimodal communication interface, a high-level behavior module, and low-level action modules."}, {"title": "A. Communication Interfaces", "content": "As shown in Fig. 3, the communication interfaces enable multimodal remote interaction between humans and robots, utilizing voice, text, and gestures. These interfaces consist of two main components: a Zoom platform and a gesture selection website. The Zoom platform supports voice, text, and video communication, while the Selenium library on the robot's server extracts specific text elements from the chat box during live sessions. For speech recognition, we employ the Whisper model [39]. For gesture-based interactions, we developed a website using Flask that allows users to select target objects by pointing. The site streams the robot's first-person video frames at 5 Hz, and the selected points are transmitted to the robot server in real-time, enabling immediate planning and execution."}, {"title": "B. High-level Behavior Module", "content": "The high-level behavior module interprets and decomposes user multimodal instructions, comprising language inputs (L) and gesture inputs (G), into executable action sequences P = {\u03b1\u03bf, \u03b11,..., aNai \u2208 A}, along with corresponding responses (R). This module processes both inputs, leveraging an LLM to generate structured responses and action plans. These are then passed to the execution module, which integrates the gesture inputs to ensure precise alignment between user gestures and robot actions, as depicted in Fig. 4.\nThe task planner in the high-level behavior module, il- lustrated in Fig. 4, is powered by an LLM (OpenAI GPT- 40-2024-05-13) prompted to function as a household robot assistant. The prompt defines the robot's role, a list of known locations, fundamental skills it can perform, and few- shot examples to demonstrate how these skills should be used. Full prompts for the LLM can be found at https: //robibutler.github.io. To align instructions with gesture selections, we implement a rule: when inputs contain the keywords \u201cthis\u201d or \u201chere\u201d, the planner generates \u201c*\u201d as an action parameter to resolve ambiguities, particularly demonstrative pronouns [18]. For example, the instruction \"Robi, please pick this and put it on the plate\" results in the plan [pick(*), placeon(\u201cplate", "*": "s resolved using the latest gesture selection. We store the five most recent gesture selections and match them with the \u201c*\u201d parameters during execution. Additionally, the system supports gesture-only input for disambiguation when the detection model identifies multiple objects in response to a single query. In such cases, the robot prompts, \"Which one are you referring to?\", pausing for the user to select the target object. Fig. 5 illustrates the alignment between gesture selections and the LLM-generated plan."}, {"title": "C. Fundamental Skills", "content": "For the robot to physically interact with the environment, it is equipped with manipulation skills such as picking/placing items, and opening/closing appliances."}, {"title": "1) Manipulation", "content": "Fig. 6 illustrates the modular framework for the pick policy. The pick() function accepts either a text query pick(text) or a pointing query pick(point). We employ the pre-trained open-vocabulary detection model OWLv2 [40] and the Segment Anything model [41] to generate the target object mask. This mask is then combined with the pre-trained grasping model Contact-GraspNet [42] to determine grasping poses. Grasping poses are filtered based on orientation, and the pose with the highest score is selected. A straightforward pre-grasp and grasp strategy is applied, with arm trajectories generated using the motion planning tools from MoveIt [43]."}, {"title": "2) Navigation", "content": "As shown in Fig. 7, our system integrates both predefined navigation places and open-world navigation to locate and move to the target object. First, we create an occupancy map using Gmapping [46] and define the navigation waypoint for the known locations in the map. In addition to predefined locations, the system supports navigation to non-predefined locations via voice/text and gesture/point queries, similar to the perception pipeline in the pick policy (Sec IV-C.1). We utilize the off-the-shelf path and motion planning algorithm provided by the ROS Navigation Stack to generate the path and motion trajectory."}, {"title": "3) Visual Question Answering", "content": "Our system is capable of answering users' open-ended questions about the objects in the robot's environment. Specifically, for the actions vqa(), our system applies GPT-40 and supports:\nTo answer the question \"Do we have any beer left in the fridge?\", the robot should first navigate to the fridge, open it, and then query the VLM model. Our solution treats the VQA as a single action and uses the reasoning capabilities of LLMs to determine the necessary high-level steps before performing VQA. Given the question q, the high-level behavior module decomposes the question into a series of actions to be executed before querying GPT-40 for the final answer.\nWhile text-only input allows users to ask questions, the single modality may not be sufficient for precise specification of the question. Therefore, we allow the robot to answer the user's verbal/- textual questions in combination with a pointing gesture, as shown in Fig. 8, denoted vqa(text, pointing). We apply a simple visual prompting method for GPT-40 to answer specific questions by annotating the image with a mark."}, {"title": "Question answering via mobile manipulation", "content": "Question answering via mobile manipulation"}, {"title": "Question answering via point referring", "content": "Question answering via point referring"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "To understand the usage and impact of multimodal remote interaction in remote HRI, we evaluate the performance of the Robi Butler guided by the following research questions:\nHow effectively and robustly does the Robi Butler enable remote users to complete household tasks?\nHow do the user interaction modalities (voice, ges- tures) affect the performance and usability of Robi Butler?"}, {"title": "A. Experiment I: Robi Butler System Performance Evalua- tion", "content": "In this experiment, we evaluate the Robi Butler system on a set of daily household tasks to understand its effectiveness and answer RQ1."}, {"title": "1) Experimental Design", "content": "The tasks were designed based on the American Time Use Survey [47]. These tasks fall under the common daily household activities, including Food and drink preparation (0.50 hr/day), Interior cleaning (0.35 hr/day), Household & personal organization and planning (0.11 hr/day), and Medical and care services (0.06 hr/day). The ten selected tasks (T1-T10)\u2014detailed below-required the robot to interpret remote users' language and point- ing gestures, then perform the corresponding actions (e.g., rearranging objects, answering questions). The object that requires disambiguation is highlighted in bold.\nT1. Throw target avocado into the trash can.\nT2. Check the beer inside the fridge.\nT3. Check medicine on the coffee table and bring target one to the sofa.\nT4. Describe the target object in the cabinet.\nT5. Bring the target drink to the coffee table.\nT6. Move the target cup to the kitchen counter.\nT7. Fetch the target remote and place it on the sofa.\nT8. Navigate to a target chair and check if it's clean.\nT9. Check if the laptop is open.\nT10. Bring the target tool to the table.\nTo evaluate the effectiveness of Robi Butler, the following metrics were used: Task Success Rate (Task SR): defined as the percentage of tasks completed. Planning Success Rate (Planning SR): defined as the percentage of tasks completed when execution errors are ignored. Task Completion Time, measuring the average time required to complete each task. Average Interactions: calculating the average number of voice and gesture interactions required per task. A task is considered successful/completed if the goal is achieved or if correct answers are provided to the remote user within 5 minutes. After obtaining informed consent, the expert user evaluated Robi Butler on 10 tasks in a fixed order, each repeated three times."}, {"title": "2) Analysis and Results", "content": "Table I presents the task per- formance results. Overall, Robi Butler achieved a high average task success rate of 96.7%, reflecting its strong ability to perform a variety of household tasks in real-world environments. However, the task success rate lags slightly behind the perfect planning success rate of 100%, indicating challenges related to low-level action execution rather than planning processes. For instance, in task T4, an error occurred when the system misidentified a green tea box as a tissue bag. On average, the system completed tasks in approximately 105 seconds, demonstrating its efficiency in performing household tasks in a complex environment. The system required an average of 2.3 interactions per task, with 1.5 voice commands and 0.8 gesture inputs. This low number of interactions demonstrates the system's efficiency in human-robot communication, requiring minimal user input to effectively guide the robot. While the overall performance of the system is generally satisfactory, answering RQ1, further improvements in low-level action execution could help increase the overall performance and efficiency."}, {"title": "B. Experiment II: The Effect of Modality on User Experience", "content": "To investigate user experience, the impact of multimodal communication, and challenges, we conducted this experi- ment with novice users to address RQ2."}, {"title": "1) Experimental Design", "content": "We recruited twelve volunteers (P1-P12; 7 males, 5 females from the university com- munity. None of the participants had familiar experience with AR/MR equipment. We compared the performance of Robi Butler with two baseline systems by removing user interaction modalities, similar to an ablation study, resulting in three systems: Gesture-only, Voice-only, and Robi Butler (Gesture+Voice). In the Gesture-only system, buttons were added for participants to select the action to be executed. Three representative tasks, TI (object rearrangement), T2 (monitoring), and T3 (object re- arrangement + monitoring), were selected from the previous"}, {"title": "2) Analysis and Results", "content": "Fig. 10 shows the task perfor- mance of the three systems. A one-way repeated measures ANOVA was conducted to analyze the quantitative data after confirming normality assumptions. Both the Gesture-only and Gesture+Voice (i.e., Robi Butler) systems achieved a perfect task success rate of 100%, while the voice system had a slightly lower, though non-significant, success rate of 94.4%. This difference was attributed to errors in target referencing with voice commands only. For example, the voice recognition system misinterpreted the word 'right' as 'red', leading to the grounding error. Additionally, Robi Butler (M = 143.8, SD = 14.8) had a significantly lower task completion time than the Gesture-only system (M = 170.00, SD = 21.4) (p < 0.05), but was not significantly lower than the Voice-only system (M = 157.1, SD = 26.6). The reduced task completion time for voice- supported systems was primarily due to the ability to use voice commands to express combined queries, whereas with the Gesture-only system, participants had to perform multiple manual button clicks, increasing task completion time.\nRegarding the trust, the Robi Butler (M = 5.77, SD = 0.97) was perceived as significantly more trustworthy com- pared to both the Gesture-only system (M = 4.98, SD = 0.85, p < 0.05) and the Voice-only system (M = 4.71, SD = 1.03, p < 0.05). This suggests that combining gestures with voice commands fosters greater confidence in system reliability and consistency, outperforming systems relying solely on a single modality. P2 reasoned that \"I trusted the gesture plus voice system the most because I found it easier to avoid making mistakes with it. For language only, sometimes it may misunderstand me. For gestures, I have to do the interaction multiple times.\"\nFor the SUS, participants gave the Gesture-only the lowest usability score (M = 47.29, SD = 15.90), which signifi- cantly lower than both Voice-only (M = 70.42, SD = 11.62, p < 0.01) and Robi Butler (M = 75.83, SD = 9.61, p < 0.01). This also indicates that Robi Butler achieved 'Good' usability (i.e., SUS > 75 [52]) compared to the other systems.\nOverall, the Robi Butler achieves the best performance, the highest usability, and the minimum perceived cognitive load among the baselines, answering RQ2. This was primarily due to the complementary nature of voice and gesture inter- actions, where voice enabled natural and combined queries. In contrast, gestures facilitated the disambiguation of voice commands related to locations and provided precise spatial annotations. Although multimodal interaction generally out- performed unimodal interaction, P10 expressed a negative sentiment, stating, \u201cUsing both voice and gesture is [some- times] hard, as I need to switch between two modalities. I prefer voice-only as I don't need to move my arm physically.\" Future improvements, such as incorporating eye gaze track- ing to minimize hand interactions, could potentially reduce such physical workload."}, {"title": "VI. CONCLUSION", "content": "This work introduces an interactive robotic assistant for household tasks using multimodal interactions with remote users. We outline three core components of the robot but- ler system and demonstrate its effectiveness in assistive question-answering and object rearrangement. Experiments show Robi Butler grounds multimodal instructions with a high task success rate, reasonable time, and minimal inter- actions. Follow-up tests confirm that combining voice and gestures enhances usability and trust, and reduces cognitive load compared to unimodal systems. In future work, we aim to make Robi Butler more adaptable, capable of autonomous skill learning, personalized interactions, and handling com- plex tasks that may require tactile feedback [53]."}]}