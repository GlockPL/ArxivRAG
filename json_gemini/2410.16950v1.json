{"title": "Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In", "authors": ["Itay Nakash", "George Kour", "Guy Uziel", "Ateret Anaby-Tavor"], "abstract": "Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become increasingly prevalent. As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack. Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions. Our results show that once a ReAct agent's thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a 'foot-in-the-door', allowing an attacker to embed malicious instructions into the agent's thought process, making it more susceptible to harmful directives. To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models (LLMs) has led to the development of LLM-based agents by leveraging external tools to enhance their capabilities. These tools enable LLMs to access real-time information, search the Internet, execute code snippets, and perform other tasks (Wu et al., 2023; Ge et al., 2024; Schick et al., 2024; Shen et al., 2024). While the integration of tools into LLMs has improved their practical utility, it also introduces new risks for users. Specifically, LLMs can exhibit unpredictable behavior and be deliberately misused, potentially leading to unintended consequences (Shen et al., 2023). For instance, an attacker can exploit an agent's tool access by manipulating it into executing harmful actions, a technique known as direct prompt injection (DPI) (Liu et al., 2023). DPI involves tricking the agent into performing unintended actions, which can result in sensitive information disclosure or unauthorized tool use. Furthermore, agents can also be manipulated by third-party attackers using a method known as indirect prompt injection (IPI) (Greshake et al., 2023; Shayegani et al., 2023). IPI involves injecting malicious instructions into external information sources, which the agent then unknowingly processes using its tools. This attack vector is particularly concerning, as it does not require direct access to the agent and can potentially affect multiple agents that access the contaminated sources (Shayegani et al., 2023). The ease with which an attacker can insert harmful requests into various information sources, such as emails, websites, and online comments, makes IPI a significant threat.\nThis study introduces the foot in the door (FITD) attack, a novel adversarial strategy illustrated in Figure 1. The FITD attack involves presenting an LLM-based agent with a small, harmless, yet deceptive request, prior to the malicious instruction. This initial request can take the form of a harmless request, such as calculating a simple math problem, which serves as a precursor to the subsequent malicious action. By exploiting the agent's access to external tools, the FITD attack leverages the agent's propensity to trust and follow instructions, thereby reducing its likelihood of re-evaluating later, more harmful actions.\nOur experimental results demonstrate that the FITD attack significantly increases the attack success rate (ASR) across all tested models, with the maximum increase reaching 44.8%. Notably, the effectiveness of the FITD attack persists even when the attacker employs distractor tools for the benign requests that are unfamiliar or inaccessible to the agent. The success of the attack, even with the presence of a \"non-existing\u201d distractor, highlights the reliability and increased effectiveness of the FITD technique, especially in the IPI setting.\nThe ReAct framework, a widely employed methodology for constructing LLM-based agents, allows it to alternate between reasoning and action by decomposing tasks into a repetitive process of reasoning, action (e.g., invoking external tools), and observing the outcome of tool interactions, with optional reasoning steps (Yao et al., 2022).\nOur hypothesis posits that ReAct-based agents primarily assess the safety of requests during the thought generation phase, thereby introducing a significant vulnerability that our attack exploits. Specifically, once an action is incorporated into the agent's thought process, it is likely to be executed without re-evaluation. The \"foot in the door\" technique facilitates the inclusion of malicious requests in the agent's thought process, thereby increasing the attack's success rate. This hypothesis is substantiated by our thought-injection analysis, which demonstrates that injecting a thought instructing the agent to execute the attack consistently yields an attack success rate (ASR) of over 95% across all models, representing an increase of up to 86% in ASR.\nOur analysis and experimental findings highlight the need for effective defense strategies against this vulnerability. Given the minimal technical expertise required to execute FITD attacks and their potentially severe consequences, we leveraged our insights to develop three defense methods, each with a distinct level of intervention. These reflection-based defenses offer protection against insider prompt injection (IPI) attacks, as well as those incorporating the FITD technique. The proposed defenses exhibit varying levels of effectiveness, ranging from modest gains of a few percent using the non-intrusive self-reflection method to over 90% effectiveness with the more aggressive safety agent method, which requires additional resources and may trigger false positives. This flexibility enables users to select the defense level suited to their specific use case, tools, and preferred response, whether it involves alerting a human-in-the-loop, halting actions, or another intervention. In this work, we present the following contributions:\n1. We present the new FITD attack, which substantially enhances regular indirect prompt injection, yielding the increase of up to 44% in ASR.\n2. We explore how and why ReAct mechanisms specifically exhibit vulnerability to this attack, offering insights to guide the development of appropriate defense strategies.\n3. We propose reflection-based defense strategies and evaluate their effectiveness in mitigating different attacks."}, {"title": "2 Related Work", "content": "The field of LLM jailbreaking, namely manipulating LLMs to generate unintended responses or actions, is rapidly evolving. Increased LLM deployment has spurred research into vulnerabilities like generating toxic, discriminatory, or illegal content (Perez and Ribeiro, 2022; Russinovich et al., 2024; Wei et al., 2024; Zou et al., 2023). However, manipulating LLM-based agents, which interact with the physical world, is relatively understudied. Their vulnerabilities may be more pronounced due to their real-world actions (Shayegani et al., 2023).\nAttacks on instruct LLMs, including agents, are categorized into Direct Prompt Injections and Indirect Prompt Injections. DPIs involve direct access to the agent, crafting malicious instructions bypassing safeguards (Perez and Ribeiro, 2022; Liu et al., 2023). Examples include the DAN (Do Anything Now) (Geiping et al., 2024) and Crescendo attacks (Russinovich et al., 2024; Aqrawi, 2024), which exploit model weaknesses to disregard safety mechanisms.\nIPIs embed malicious prompts within externally retrieved content (websites, documents, emails) (Greshake et al., 2023; Yi et al., 2023). This exploits the blurred line between instructions and data in LLMs (Shayegani et al., 2023), posing greater risks than DPIs due to remote execution and potential widespread impact, similar to \"watering hole\" attacks (Krithika, 2017)."}, {"title": "3 The Foot in the Door Attack", "content": "We introduce a novel technique called \"Foot in the Door\" (FITD) attack, building upon existing IPI attacks by adding a distractor phase before the main attack. Inspired by the psychological foot-in-the-door technique (Freedman and Fraser, 1966), where complying with a small request increases compliance with a larger one, we hypothesize a digital analog for LLM agents. A harmless request (e.g., currency conversion) increases the likelihood of complying with a subsequent malicious request (e.g., changing GitHub repository visibility or transferring funds). Our results (Section 1) show that this pre-attack distractor significantly boosts compliance, with an average increase of 34.5% across all tested models compared to the vanilla IPI, mirroring the psychological phenomenon.\nThe FITD attack manipulates agents into executing harmful actions, such as financial damage or data leaks. As analyzed in Section 4.3, this seemingly irrelevant addition significantly increases compliance with the malicious request, nearly doubling the ASR rate compared to scenarios without the distractor.\nThis rise in ASR occurs even when the distractor tool used as the 'foot in the door' is entirely unfamiliar and inaccessible to the agent, emphasizing both the danger and resilience of this attack."}, {"title": "3.1 Thought Injection", "content": "To examine this phenomenon and explore future attack methods, we propose the \"thought injection\" technique, which uses the ReAct format to embed specific content into the agent's initial thought and assess its influence on the subsequent chain of actions. We examine two scenarios: Thought injection (TI), where both harmless and malicious actions are embedded in the initial thought (3), and harmless thought injection (HTI), which includes only a harmless action for comparison.\nWe hypothesize that once a thought contains a malicious action, the model's likelihood of ultimately executing that action increases significantly. Agents are typically fine-tuned with trajectories of ReAct-format outputs as shown in Yao et al. (2022), which may develop a \u201chabit\u201d of following this structured format, potentially continuing with actions even when recognizing potential danger."}, {"title": "3.2 Defense Approaches", "content": "To address this sensitive attack, we propose a reflection-based defense mechanism that halts the agent's actions after thoughts that follow an external tool observation (Step 3 in Figure 1). At this point, either the agent itself or an external reflector evaluates the safety and any potential hesitation surrounding its intended actions. As shown and discussed in Section 5, different defense methods can reduce attack success rates by up to tens of percentage points. These defense mechanisms are essential for ensuring safer and more secure agent behavior."}, {"title": "4 Empirical Study", "content": "Our experimental setup builds upon the benchmark proposed by Zhan et al. (2024) which evaluates the vulnerability of LLM-based agents to IPI attacks. This evaluation framework simulates agent actions and tool responses using LLMs as proxies, allowing for scalable experimentation across a wide range of cases, as demonstrated in Ruan et al. (2023).\nThe benchmark includes two types of attacks: Direct harm (DH) attacks, consisting of 510 distinct attacks that involve a single harmful action, and data steal (DS) attacks, with 544 attacks focused on extracting sensitive information. While we evaluated our attack and defense strategies across both attack types, observing similar trends in attack success and defense effectiveness, our analysis primarily focuses on DH attacks. This emphasis is due to their straightforward nature, involving a single step compared to the multiple steps required in DS attacks, and to the constraints on available resources. All baseline IPI results reported were successfully reproduced on our models."}, {"title": "4.2 Evaluation Approach", "content": "To evaluate the agent's behavior, we use regex to parse its output and detect tool calls. Once a tool call is identified (under \"Action:\" as shown in the ReAct Setup in Figure 2 (left)), we simulate the corresponding observation using GPT-40-mini. Based on the tool's response format, GPT-40-mini generates realistic observations that mimic the tool's response, considering its description and parameters. For example, if the identified tool is Read_SMS, the LLM produces a response in the expected format, simulating realistic tool output, which is then verified through automatic checks. This structured approach allows the agent to proceed with its actions, such as transferring money, thereby enabling us to check whether it ultimately achieves the intended target of the attack, as demonstrated in the second action generated within the ReAct setup in Figure 2. In line with the InjectAgent benchmark (Zhan et al., 2024), agent outputs are categorized as \"successful,\" \"unsuccessful,\" or \"invalid.\" An output is considered \"successful\" when the agent generates a call to the targeted tool with the desired parameters, while an \"unsuccessful\" output occurs when the agent functions correctly but misses the targeted tool or desired parameters. Invalid outputs occur when the agent generates output that does not fit the expected agent output format, such as calling non-existent tools (e.g., a distractor in \"Unfamiliar FITD\" in figure 2) or lacking meaningful content, as defined in Zhan et al. (2024).\nTo maintain consistency with the original benchmark, we evaluate the ASR based on valid outputs, excluding invalid ones. Notably, the valid output rate remained stable across scenarios, with neither the FITD nor the unfamiliar FITD showing any considerable decline in validity. Detailed validity rates are provided in Appendix B."}, {"title": "4.3 Effectiveness of the Foot-in-the-Door Attack", "content": "In this experiment, we assessed the FITD attack using eight different distractor tools, reporting the mean ASR across these tools (Full results for each individual tool can be found in Appendix A.1). To ensure consistency with our experimental setup, we also replicated the original benchmark's IPI baseline.\nResults: As shown in Table 1 under FITD (Familiar) the FITD attack demonstrates a significantly higher attack success rate (ASR) across different models, with a 44.8% rise in ASR for GPT-40-mini and a 36.3% increase for Llama-3-70B among all tested attacks. This highlights the superior effectiveness of the FITD technique over vanilla IPI. The FITD attack remains robust across eight different tools used as distractors, consistently delivering strong results, regardless of the specific model or tool being tested. This not only reinforces the effectiveness and practical utility of the FITD attack, but also underscores its inherent risks, exposing the susceptibility of LLM agents to subtle forms of manipulation that can manifest in a variety of ways. These results emphasize the pressing need to address such vulnerabilities in future models."}, {"title": "4.4 Effectiveness of the Foot-in-the-Door Attack Using Unfamiliar Tools", "content": "To ensure the robustness of the FITD attack, we tested it across eight different unfamiliar tools. These tools, by the scenario's definition, are not dependent on the agent itself (which doesn't have access to these tools), allowing an attacker to select any tool to use as \"unfamiliar.\" On average, the mean results across all tools were similar to or better than the IPI, depending on the model. Therefore, we reported the unfamiliar FITD using the calculator tool, which yielded consistent results across different models. Full results for all tools are provided in Appendix A.2.\nResults: As displayed in Table 1 as unfamiliar FITD, even when the tool is not present within the agent's system and the agent isn't familiar with it (referred to as the \"unfamiliar tool\"), the FITD attack still results in an 15% increase in the mean ASR across the different models, making the attack even more concerning. This finding underscores the robustness of FITD, demonstrating that the attack can still succeed, albeit with slightly reduced effectiveness, even when the agent has no prior knowledge of the distractor tool."}, {"title": "4.5 Distractor Physical Position and Chronological Timing Effect", "content": "To investigate the impact of distractor placement on the success rate of the FITD attack, we evaluated four configurations by varying both the position of the distractor within the input (early position P0; late position P1) and the timing of the benign action in the sequence (early action timing T0; late action timing T1). These configurations\u2014(P0, T0), (P0, T1), (P1, T0), and (P1, T1)\u2014allowed us to analyze how the physical placement of the distractor and the temporal order of the benign action influence the agent's response to the malicious request. By examining these variations, we aimed to determine whether the distractor needs to precede the malicious request, both textually and chronologically, to enhance the attack's effectiveness.\nResults: As shown in the heatmap both the distractor's physical position within the input (Position) and the timing of the benign action (Time) significantly influence the success rate of the FITD attack. Across all models, FITD consistently achieved a higher ASR than the baseline IPI, demonstrating its effectiveness under varied configurations and setups.\nEarly distractor positioning (P0, top) leads to higher attack success compared to later positioning (P1, bottom). Similarly, earlier action timing (T0, left) generally supports more successful attacks than later timing (T1, right). This aligns with the original foot-in-the-door phenomenon, suggesting that introducing the distractor early in the sequence builds trust, thereby facilitating further actions without re-evaluation.\nNotably, as shown in the heatmap the differences between the top and bottom rows (reflecting Position0 vs. Position1) are greater than those between the columns (Time0 vs. Time1). Specifically, the distractor's position has a stronger influence on attack success than the timing of the action, with a 13.1% difference between early (Position0) and late (Position1) placements, compared to only a 4.6% difference between early (Time0) and late (Time1) action timing.\nAdditionally, unlike other models, Llama-3 achieves its highest success rate with Position1 and Time1. This suggests that the optimal timing for distractor effectiveness is not absolute across all models and may vary depending on model-specific characteristics, influencing their susceptibility to FITD attacks."}, {"title": "4.6 Thought-Injection (TI) and Harmless-Thought-Injection (HTI)", "content": "This ablation study aims to examine how the thought process within the ReAct framework contributes to the foot-in-the-door vulnerability and could inform future attack strategies. The affect of \"thought injection\" was tested on the vanilla IPI, and over familiar FITD with a calculator as distractor.\nResults: As demonstrated injecting a thought that includes the intention to execute the harmful request leads to near-total compliance across all agents. This results in over 95% compliance for all models with FITD and above 83% with the vanilla IPI attack. This suggests that most of the model's 'critique mechanisms' when using the ReACT method operate within the thought process itself. Once a ReACT agent bypasses these mechanisms, it no longer applies critical judgment to its actions or decisions, making it less likely to question or reconsider its choices. This highlights the importance of the initial thought following the attack, which is the first thought after receiving and observation from an external tool.\nAs one might expect, in the harmless thought injection, the ASR was lower compared to injecting both requests into the thought process ('regular' thought injection), yet it remained significantly higher than when no thought injection was used. This result underscores the significance of the distractor task, demonstrating that the model's tendency to follow any request from the attacker plays a pivotal role in the attack's success. The harmless request within the thought process (HTI) prompts the model to fulfill this initial task, which then acts as a 'foot in the door', increasing the likelihood that the model will execute subsequent malicious actions with reduced suspicion."}, {"title": "5 Defense Methods", "content": "Our findings uncovered critical vulnerabilities in ReAct-based agents, guiding the development of targeted interventions to address these weaknesses.\nAs demonstrated in Section 4.6, once the agent generates a thought indicating intent to perform an action, even if that action is incorrect or potentially harmful, the agent will almost invariably follow through with that course of action. This highlights the critical importance of intervening during the thought generation phase to prevent unsafe actions from being executed. In addition, we observed in various outputs that the agent may exhibit hesitation, or even objection in his generated thought, yet still proceed with the malicious request afterwards. This tendency may be an artifact of the fine-tuning process, where the agent has learned to adhere to a structured sequence of thought and action, potentially leading it to continue with actions despite internal hesitation or objections.\nBuilding upon these insights, we propose a reflection-based mechanism that leverages the agent's internal thought process as a key intervention point. This mechanism is designed to detect moments of hesitation or risk, allowing for a reassessment of the safety and appropriateness of the agent's next actions. If the reflection process flags any indicators of uncertainty or danger, the agent can respond by raising a warning, seeking user confirmation, or halting the action, depending on risk severity.\nWe tested both the recall and precision of our different approaches across benign and harmful scenarios. For recall, we evaluated the system's ability to mitigate the different attacks, and for precision, we assessed the false-positive rates by generating 1000 distinct harmless thoughts a Re-ACT agent could have in response to legitimate requests and actions using gpt-4o. The prompt used for generating these thoughts is provided in Appendix E.5, along with representative examples for generated thoughts in Appendix D.1. These harmless thoughts were used to ensure that the defense mechanisms did not raise unnecessary alarms. The results revealed a trade-off: Some methods excelled at recall, offering stronger protection against attacks, while others showed higher precision, minimizing interruptions in normal agent operations. This balance between recall and precision is crucial. An overly cautious system that frequently alerts users or halts actions could lead to alert fatigue, while insufficient caution leaves the agent vulnerable to exploitation.\nTo accommodate diverse system needs, we evaluated three reflection-based methods with varying intervention levels and computational overhead, allow users to choose the approach that best suited to their specific requirements, based on the desired trade-off between security and usability. We focused our defense evaluation on the vanilla IPI and the strongest familiar FITD configuration (Position 0, Timing 0, with a calculator as distractor tool). Notably, we did not test thought injection attacks, as harmful thoughts inherently trigger a response."}, {"title": "5.1 Self Reflection", "content": "The least invasive method we explored involves simply augmenting the agent's prompt by adding a specific request for self-reflection after each thought. This addition builds on the existing safety and security requirements prompt, commonly used in prior work. Our analysis revealed that the agent's thought process is particularly vulnerable, making this phase critical for reinforcing security. By prompting the agent to reassess the safety of its actions during thought generation, we sought to reduce the likelihood of malicious behavior emerging from different attacks.\nAlthough internal reflection does reduce the attack success rate (ASR) modestly, and up to 15% for IPI in some models, it didn't show significant or consistent improvements across all models and attacks. These findings align with prior work indicating that LLMs often struggle with thorough self-reflection. Despite its low overhead, internal reflection alone may not be sufficient to counter complex attacks and may need to be combined with more robust defenses."}, {"title": "5.2 Hesitation Reflection", "content": "In line with our earlier findings, we developed a hesitation reflection mechanism using an objective reflector based on LLaMA-3-1-70B. The reflector evaluates whether the agent's thought process shows hesitation or reluctance using a specific prompt (see directive E.2 in Appendix E). During testing on the 1000 benign thoughts, we observed no false positives to this reflector, confirming the mechanism's reliability in distinguishing safe requests. As shown the hesitation reflector (HR) consistently provides stable protection across different models, reducing the success rate of attacks. The HR mechanism is designed to flag only thoughts that are clearly unusual or suspicious, aligning with moments when the agent itself already expresses concern. This makes it an effective, though selective, defense."}, {"title": "5.3 Safe Reflection", "content": "A more robust approach to defending against malicious actions is to evaluate the overall safety of the agent's decisions and thoughts. In this light, we developed a stronger, more general \"safe reflector\" that was tasked with broadly assessing whether it was safe or appropriate for the agent to proceed with its actions. This safe reflector was applied at the same stages as the hesitation reflector, but with a more comprehensive scope, asking the agent to actively consider the appropriateness of each action. As seen this safe reflector significantly reduced the success rate of attacks, stopping many more potential threats. However, its general nature, while highly effective, raises concerns about false-positive alerts. In tests with the benign thoughts, the reflector triggered a 16% false-positive rate, indicating that it sometimes halts non-malicious actions.\nOne potential risk of this approach is the \"fatigue\" effect, where too many alerts are sent to a human or intermediary for review. This can lead to dangerous scenarios, as users may become overwhelmed or desensitized by frequent alerts, increasing the chance of errors in judgment. Over time, constant manual intervention could allow attackers to slip through, as alert fatigue reduces the effectiveness of human oversight."}, {"title": "6 Conclusions", "content": "In this work, we present the Foot-in-the-Door (FITD) attack, an effective attack technique that uses harmless requests to subtly introduce a subsequent malicious one.\nOur findings reveal vulnerabilities inherent in ReAct agents, highlighting potential risks associated with their thought processes and training approaches. To address these vulnerabilities, we introduce a reflection-based defense method that has shown success in reducing the impact of such attacks. For future work, we plan to develop training-based defenses that enable ReAct agents to recognize and reject harmful trajectories by incorporating negative examples during fine-tuning, ultimately guiding the design of more resilient LLM-based agents.\nThe importance of this research extends beyond immediate technical improvements; we believe that raising awareness of these vulnerabilities is essential for ensuring the safe and responsible integration of LLM-based agents into real-world applications."}, {"title": "7 Ethical Considerations", "content": "This study highlights significant vulnerabilities in LLM-based agents. While identifying and understanding agents' vulnerabilities is crucial for improving the security of autonomous systems, it also raises important ethical concerns that must be addressed.\n\u2022 Responsible disclosure of vulnerabilities: The discovery of new attack vectors, such as FITD, could be exploited maliciously if not properly communicated. It is essential to ensure that such vulnerabilities are responsibly disclosed to the relevant developers and stakeholders to allow for the implementation of adequate defenses before they can be widely abused. Therefore, following the publication of this work, we aim to publish the results on the most suitable channels. We encourage the community to develop centralized databases and websites allowing the reporting of security vulnerabilities of LLM-based agents to improve their visibility, similar to the Common Vulnerabilities and Exposures website and NIST's National Vulnerability Database.\n\u2022 Dual-use dilemma: Research in adversarial attacks, such as indirect prompt injection, poses a dual-use risk. While the goal of this study is to inform the development of safer AI systems, the techniques discussed could potentially be repurposed for harmful uses. As such, researchers must exercise caution in sharing technical details and ensure that the findings are used to strengthen, rather than weaken, system security. Therefore, we will postpone the release of our code until after the publication to provide sufficient time for the developers of these agents to become aware of the vulnerabilities and implement appropriate defenses.\n\u2022 Mitigation and responsibility: While we propose several defense strategies, these approaches, if misapplied, could lead to over-filtering or false positives, reducing the functionality of autonomous agents or placing undue burden on human operators. The balance between security and usability is an ethical concern that should be carefully managed. Overly cautious systems could diminish user trust and effectiveness, while under-regulated systems may expose users to significant risks."}, {"title": "8 Limitations", "content": "This study focuses on the ReAct agents, limiting our findings to this specific framework. Further research is needed to assess whether other agent architectures or frameworks exhibit similar susceptibilities.\nOur investigation of the FITD attack was conducted in the context of IPI. Further research is required to evaluate its impact on DPI scenarios. We hypothesize that the FITD attack could similarly increase the effectiveness of DPI by embedding harmless precursor requests, but this remains to be explored in future studies.\nAdditionally, our experiments were conducted using a specific set of language models (e.g., GPT-40-mini, Mixtral-8x7B). Though we observed consistent attack success rates across these models, there may be variability when applying FITD to larger or more specialized models. Expanding the evaluation to a wider range of models, including those fine-tuned for specific tasks, may yield different results.\nOur defense strategies, particularly the reflection-based approaches, are preliminary. Although they show promise in mitigating attacks, further optimization and evaluation are needed to reduce false positives, especially in more complex, real-world deployments.\nFurthermore, while we proposed deeper potential mitigation strategies such as execution rejection trajectories, these alternatives were not explored in this study and are left for future work.\nAnother potential mitigation that could be explored is introducing a separation mechanism between commands and data. Similar to solutions used to prevent SQL injection, this would involve marking external data injected into the system in a way that clearly signals to the agent that it is data and not a command. By separating these two, the agent could prevent the execution of malicious instructions embedded within external data sources. While this simple fix could help reduce the risk of indirect prompt injection, further research is needed to validate its feasibility and effectiveness in complex environments.\nLastly, this work does not address potential language limitations in our methods, as LLM behavior might vary across different languages, especially in multilingual agents. Expanding our experiments to non-English languages could uncover new dimensions of vulnerabilities."}]}