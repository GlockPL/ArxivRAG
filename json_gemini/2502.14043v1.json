{"title": "Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness", "authors": ["Benjamin Plaut", "Juan Li\u00e9vano-Karim", "Stuart Russell"], "abstract": "Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid catastrophe (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets.", "sections": [{"title": "1. Introduction", "content": "Recent advances in AI have amplified concerns about a variety of risks (Critch & Russell, 2023; Hendrycks et al., 2023; National Institute of Standards and Technology (US), 2024). Such risks include autonomous weapon accidents (Abaimov & Martellini, 2020), autonomous vehicle crashes (Kohli & Chadha, 2020), bioterrorism (Mouton et al., 2024), medical errors (Rajpurkar et al., 2022), and discriminatory sentencing (Villasenor & Foggo, 2020).\u00b9 These errors are particularly concerning due to their irreparable nature. For example, a fatal error made by an AI surgeon can never be rectified.\nDespite the severity of these risks, they remain largely unaddressed by reinforcement learning (RL) theory. Specifically, nearly all of RL theory assumes that any error can be recovered from. This assumption enables algorithms to cheerfully try all possible behaviors, since no matter how badly the algorithm performs in the short term, it can always eventually recover. Although such approaches have produced positive results, they forfeit any possibility of safety guarantees for the high-stakes AI applications mentioned above. Algorithms that try all possible behaviors are clearly not appropriate for such applications; indeed, they may primarily be suitable for simulation-based learning, where the rewards are not real at all. We contend that a fundamentally different algorithmic paradigm is needed.\nOne alternative is to devise agents that recognize when they are uncertain and then ask for help. When help is requested, a supervisor (here called a mentor to reflect their collaborative role) guides the agent to avoid problematic actions without the agent having to try those actions firsthand. This approach is particularly suitable for high-stakes applications where AI systems are already configured to work alongside humans. For example, imagine a human doctor who oversees AI surgeons and is prepared to take over in tricky situations, or a backup human driver in an autonomous vehicle. In these scenarios, occasional requests for human guidance are both practical and valuable for ensuring safety.\nRecent work (Plaut et al., 2024) asked whether a limited number of queries to a mentor is sufficient for an agent to avoid catastrophe (i.e., irreparable errors) in an online learning model where the agent can transfer knowledge between similar states. They proved that avoiding catastrophe in this model is quite tractable: specifically, it is no harder than standard online learning.\nTheir work shows that access to a mentor can yield strong safety guarantees for AI agents. Safety guarantees are necessary, but not sufficient: agents must also be able to complete useful tasks. For example, an autonomous vehicle that ensures safety by never moving is hardly a vehicle at all.\nIn this paper, our goal is not only to avoid catastrophe but also to maximize reward. Perhaps surprisingly, we prove that any algorithm that is guaranteed to avoid catastrophe in the sense of Plaut et al. (2024) is also guaranteed to obtain high reward in Markov Decision Processes (MDPs). In other words, asking for help enables safety guarantees without sacrificing effectiveness."}, {"title": "1.1. Our model", "content": "We study online RL, where the agent must perform well while learning: finding a good policy is futile if catastrophe is caused along the way. We allow MDP to be non-communicating\u00b2 (some states might not be reachable from other states), and we do not require the agent to be reset. We will refer to this class of MDPs as \u201cgeneral MDPs\".\nOn each time step, the agent has the option to query a mentor. When queried, the mentor illustrates the action they would take in the current state. Our goal is to minimize regret, defined as the difference between the expected sum of rewards obtained by the agent and the mentor when each is given the same starting state. (Regret typically refers to the performance gap between the agent and an optimal policy, but we allow the mentor to be either optimal or suboptimal, so our definition is strictly more general.)\nAn algorithm for this problem should satisfy two criteria:\n1. The agent's performance should approach that of the mentor. Formally, the average regret per time step should go to 0 as the time horizon $T$ goes to infinity, or equivalently, the cumulative regret should be sublinear in $T$ (denoted $o(T)$). An algorithm that guarantees $o(T)$ regret is called a no-regret algorithm.\n2. The agent should eventually become self-sufficient. Formally, the rate of querying the mentor should go to 0 as $T\\rightarrow \\infty$, or equivalently, the cumulative number of queries should be $o(T)$.\nIn summary, we study the central open question posed by Plaut et al. (2024): is there a no-regret algorithm for (undiscounted) general MDPs that makes $o(T)$ mentor queries?"}, {"title": "1.2. Our contribution", "content": "We show that the answer to that question is yes whenever avoiding catastrophe (per Definition 4.1) is possible:\nTheorem 5.1 (Informal). Any algorithm that is guaranteed to avoid catastrophe with $o(T)$ mentor queries is a no-regret algorithm for MDPs (still with $o(T)$ mentor queries).\nTheorem 5.1 does not say that avoiding catastrophe itself is sufficient for high reward: that claim is false whenever there exist multiple safe policies with different amounts of expected reward. Rather, Theorem 5.1 says that if an algorithm can avoid catastrophe in any situation, it must have certain beneficial properties that also lead to high reward.\nOur proof techniques diverge substantially from the standard repertoire of RL theory: we do not use Q-functions, upper"}, {"title": "2. Prior work", "content": "There is a vast body of work studying regret in MDPs, but all prior no-regret guarantees rely on restrictive assumptions. For context, the agent has no hope of success without further assumptions: any unexplored action could lead to paradise (so the agent cannot neglect it) but the action could also lead to disaster (so the agent cannot risk it). This intuition is formalized by the \"Heaven or Hell\" problem in Figure 1. Our work is not immune to this problem: rather, we argue that for high-stakes AI applications, access to a mentor is more realistic than prior assumptions.\nFigure 2 subdivides the literature based on which assumption is used to circumvent this problem. Appendix A discusses each node in depth (and covers some related work outside of this taxonomy), but we provide an overview here. Unless otherwise specified, citations are representative, not exhaustive.\nIn MDPs, $\\sigma$-smoothness means that the transition kernel can never concentrate at a single point: its concentration must be upper-bounded by $1/\\sigma$. The idea is to prevent rare pathological inputs from dominating the analysis. This concept is not crucial to our work, but we define it for completeness. See Section 3 for details."}, {"title": "3. Model", "content": "MDPs. Let $\u0394(X)$ denote the set of probability measures with sample space $X$, let $N$ denote the strictly positive integers, and let $[k] = \\{1, ..., k\\}$ for $k \u2208 N$. A (finite-horizon undiscounted) MDP is a tuple $M = (S, A, P, r, s_1, T)$ where $S$ is the state space, $A$ is the (finite) action space, $P: S \u00d7 A \u2192 \u0394(S)$ is the transition kernel, $r : S \u00d7 A \u2192 [0, 1]$ is the reward function, $s_1 \u2208 S$ is the initial state, and $T \u2208 N$ is the time horizon.\nOn each time step $t \u2208 [T]$, the agent observes a state $s_t \u2208 S$ and selects an action $a_t \u2208 A$. The agent then obtains a reward $r(s_t, a_t)$ and the next state $s_{t+1}$ is sampled from the distribution $P(s_t, a_t)$. For each $X \u2286 S$, let $P(s, a, X)$ denote the probability which $P(s, a)$ assigns to $X$, i.e., $P(s, a, X) = Pr[S_{t+1} \u2208 X | S_t = s, a_t = a]$ for any $t\u2208 [T]$. Also let $s = (s_1, . . ., s_T)$ and $a = (a_1,...,a_T)$.\nWe allow the agent's choice of action to be randomized, and all expectations are over the randomness in $P$ and any randomness in the agent's choice of action.\nAsking for help. We assume there exists a mentor endowed with a deterministic policy $\u03c0_m : S \u2192 A$. For each $t \u2208 [T]$, let $q_t = 1$ be the indicator variable of whether the agent queries the mentor at time $t$. If $q_t = 1$, then the agent observes $\u03c0_m(s_t)$ and takes action $a_t = \u03c0_m(s_t)$. If $q_t = 0$, the agent does not observe $\u03c0_m(s_t)$ and selects an action $a_t$ of its choice. Let $s^m$ be the mentor's state at time $t$, where $s^m_1 = s_1$ and $s^{m}_{t+1} \\sim P(s^{m}_t, \u03c0_m(s^{m}_t))$. Let $s^m = (s^m_1,...,s^m_T)$.\nMeasure-theoretic considerations. Since we do not assume a discrete state space, it behooves us to briefly discuss the underlying measure spaces in order to ensure full mathematical rigor. Readers less interested in measure theory can safely skip this discussion. We assume that $S$ is equipped with the usual Borel $\u03c3$-algebra $B(S)$. All discussion of measurability is with respect to $(S, B(S))$. Then for any $s \u2208 S$ and $a \u2208 A, P(s, a)$ is a probability measure on $(S, B(S))$. Any reference to subsets of $S$ is assumed to be restricted to $B(S)$: in other words, we treat $X \u2286 S$ as shorthand for $X \u2208 B(S)$. We assume that $r$ is measurable on $S \u00d7 A$, $\u03c0_m$ is measurable on $S$, and the agent's choices of $q_t$ and $a_t$ are measurable function of the agent's observation history $s_1, a_1, q_1, ..., s_t$. Lastly, we assume that for any fixed $X \u2286 S, P(\u00b7, \u00b7, X)$ is measurable on $S \u00d7 A$. These assumptions ensure that all relevant expectations and probabilities are well-defined. See Chapter 8 (Bertsekas & Shreve, 1996)"}, {"title": "Objectives.", "content": "Our first objective is for the agent to become self-sufficient over time. For an MDP $M$ and mentor policy $\u03c0_m$, let $Q_T(M, \u03c0_m) = E[\\sum_{t=1}^{T} q_t]$ be the expected number of queries to the mentor. Our second objective is for the agent to perform nearly as well as the mentor. Given $M$ and $\u03c0_m$, the agent's expected regret is\n$R_T(M, \u03c0_m) = E [\\sum_{t=1}^{T} r(s^m_t, \u03c0_m(s^m_t)) - \\sum_{t=1}^{T} r(s_t, a_t)]$\nThis corresponds to independently running the mentor and the agent each for $T$ steps from the same initial state and comparing their expected total reward. (One typically compares the agent to the optimal policy, but our mentor can be optimal or suboptimal, so our definition is more general.)\nWe want both the number of queries $Q_T(M, \u03c0_m)$ and the regret $R_T(M, \u03c0_m)$ to be sublinear in $T$. Formalizing this"}, {"title": "5. Our results", "content": "We are now ready to state our main result:\nTheorem 5.1. Any algorithm which avoids catastrophe (Definition 4.1) satisfies $R_T < (T + 1)RAC \u2208 o(T)$ and $Q_T \u2208 o(T)$.\nCombining Theorem 5.1 with the work of Plaut et al. (2024) (specifically, Lemma 4.2) produces the first no-regret guarantee for general MDPs:\nTheorem 5.2. Under the conditions of Lemma 4.2, Algorithm 1 in Plaut et al. (2024) makes $o(T)$ mentor queries and is a no-regret algorithm. Specifically,\n$Q_T \u2208 O(\\frac{dL}{\\sigma}T^{\\frac{4n+1}{4n+2}}logT + \\frac{T^{\\frac{T}{2n+1}}}{log T}))$ \n$R_T \u2208 O(\\frac{dL}{\\sigma}T^{\\frac{4n+1}{4n+2}}logT + \\frac{T^{\\frac{T}{2n+1}}}{log T}diam(s)) )$\nDefinition 4.1 states that $Q_T \u2208 o(T)$ and $lim_{T\\rightarrow\\infty} RAC = 0$, the latter of which implies that $(T + 1)RAC\u2208 o(T)$. Thus our main task is to prove that $R_T < (T + 1)RAC.", "5.1. Regret decomposition": "Our analysis revolves around a decomposition of regret into state-based regret and action-based regret. Figure 4 defines each of these components and illustrates the proof structure. To the best of our knowledge, this decomposition is novel and could be of independent interest.\nState-based regret measures how bad the agent's states $s_t$ are compared to the mentor's states $s^m_t$. Specifically, \"bad\" is evaluated with respect to the actions the mentor would take in each state. To bound the state-based regret, we will use the local generalization of $P$ to bound the deviation between the distributions of $s_t$ and $s^m_t$ as measured by $\u2206t = supx\u2286s(p^m_t(X) \u2013 p_t(X))$. Most of the proof is focused on showing that the state-based regret is at most $TRAC$."}, {"title": "5.2. Proof sketch", "content": "The full proof is deferred to Appendix C, but we provide a visualization (Figure 4) and proof sketch here.\nAppendix C.1 provides an alternative proof that bounds the action-based regret using a different method which not require local generalization for $r$ (although it is still required for $P$). However, this proof requires the algorithm to satisfy an additional property, which does hold for Algorithm 1 in Plaut et al. (2024), but may not hold for all algorithms which avoid catastrophe.\nFirst, we bound the action-based regret by a direct application of Definition 4.1.\nLemma 5.3. If an algorithm satisfies Definition 4.1, then $E[\\sum_{t=1}^{T}r(s_t, \u03c0_m(s_t)) - \\sum_{t=1}^{T}(s_t, a_t)] < RAC$.\nProof. Let $\u00b5_t(s, a) := r(s, a)$ for all $t \u2208 [T]$. Since $r$ satisfies local generalization, so does $\u03bc$. Hence by Definition 4.1, $E[\\sum_{t=1}^{T}r(s_t, \u03c0_m(s_t)) - \\sum_{t=1}^{T}(s_t, a_t)] < RAC$. \nIt remains to bound the state-based regret. Rather than analyzing when $s_t$ is better or worse than $s^m_t$, we simply bound how much their distributions differ at all. Specifically, we will show that $supx\u2286s(p^m_t(X) \u2013 p_t(X)) < RAC$ for all $t \u2208 [T]$. Let $\u2206t = supx\u2286s(p^m_t(X) \u2013 p_t(X))$; we will refer to this quantity frequently. First, we show that an entire class of expected values can be bounded by $\u2206t$.\nLemma 5.4. For any $t \u2208 [T]$ and any measurable function $f : S \u2192 [0, 1]$, we have $E[f(s^m_t) \u2013 f(s_t)] < \u2206_t$.\nThe idea behind Lemma 5.4 is that $E[f(s^m_t) - f(s_t)]$ can only be large if $s_t$r is more concentrated than $s_t$ in states where $f$ is large. We use $\u2206_t$ to measure how large that difference in concentration can be. Also, since $f(s) \u2208 [0, 1]$"}, {"title": "6. Conclusion", "content": "In this paper, we showed that any algorithm that avoids catastrophe in the sense of Plaut et al. (2024) is guaranteed to be a no-regret algorithm in general MDPs. Applying our reduction to the algorithm from Plaut et al. (2024) produces the first no-regret guarantee for general MDPs. Conceptually, we prove that this algorithm obtains high reward while becoming self-sufficient, even when some errors may be catastrophic.\nAlthough we think these insights are broadly applicable, this particular algorithm has some practical limitations. For one, the algorithm relies on perfectly computing distances between states in order to exploit local generalization. This is analogous to requiring a perfect out-of-distribution (OOD) detector, which is a very unsolved problem (Yang et al., 2024). Future work might consider a more realistic model of OOD detection and/or a less strict version of the local generalization assumption. Our work also makes the standard yet limiting assumptions of total observability and knowledge of the policy class, which could be relaxed in future work.\nWe are also interested in cautious learning without a mentor. We argued for the employment of mentors in high-stakes AI applications, but realistically, a mentor may not always be available (or may not always respond promptly). Future work could explore safe learning when asking for help is not possible, but the agent still has some way to make inferences about novel states. One possibility is by learning from bad-but-not-catastrophic experiences: for example, someone who gets serious (but not fatal) food poisoning may be more cautious with food safety in the future. This phenomenon is not captured in our current model.\nMore broadly, we think that the principle of acting cautiously when uncertain may be more powerful than hitherto suspected. We are hopeful that this idea can help make AI systems safer and more beneficial for all of society."}, {"title": "Impact statement", "content": "In recent years, AI systems have grown increasingly capable and pervasive. Given this, we believe that safety guarantees for such systems are paramount. We are particularly concerned about harms that cannot be undone, which is the focus of the present paper. We hope that our work demonstrates that safety and effectiveness need not be at odds and can actually be simultaneously achieved. We have not identified any specific risks from our work that warrant special attention."}]}