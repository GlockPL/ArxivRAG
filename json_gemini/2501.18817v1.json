{"title": "Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies", "authors": ["Andrey Borro", "Patricia J Riddle", "Michael W Barley", "Michael J Witbrock"], "abstract": "Recent advancements in the reasoning skills of Large Language Models (LLMs) demonstrate an increase in the ability of LLMs to solve simple planning tasks. However, as long as the driving force behind improved reasoning capability is the size and complexity of the model, the financial and computational costs associated with running them will also increase. This trend raises questions about continued accessibility and whether these improvements will increase at the same pace as models continue to grow in size and expense. We propose two approaches to enhance the reasoning ability of less resource-intensive LLMs. (1) Provide them with a generalised strategy for solving tasks within a given domain, generated by a more resource-intensive LLM. (2) Exploit their cost-effectiveness by iteratively prompting these models to correct errors in their proposed solutions. Our empirical results from planning and mathematical reasoning tasks demonstrate that these methods improve the performance of less resource-intensive LLMs to levels comparable with their more resource-intensive counterparts, at a fraction of the cost. Additionally, we show that the utilisation of generalised strategies in our experiments reduced the cost of the less resource-intensive model by nearly 30 percent on average.", "sections": [{"title": "1 Introduction", "content": "The emergence of reasoning capabilities in Large Language Models (LLMs) has prompted research into their ability to solve a wide variety of problems [Bubeck et al., 2023; Zhao et al., 2024; Yu et al., 2022]. Further research into prompt engineering [Clavi\u00e9 et al., 2023] and prompting techniques such as Chain-of-Thought [Wei et al., 2022] has improved the ability of pre-trained LLMs to solve reasoning tasks without fine-tuning. Despite these advances, many LLMs still perform poorly on System 2 [Kahneman, 2011] reasoning tasks in maths, logic, and planning.\nLLMs empowered with internal reasoning steps, such as OpenAI's 01 [OpenAI, 2024], which iteratively feeds its own output back in as input for Chain-of-Thought-like reasoning, have shown promise in solving planning tasks, overcoming some limitations previously faced by LLMs. However, as evidenced by recent work [Valmeekam et al., 2024], these successes are restricted to small-scale tasks within simple and transparent domains. Furthermore, there is a clear correlation between the size, computational requirements, and financial cost of an LLM and its performance on planning tasks. For example, when tasked with manipulating a small handful of blocks on a table, the weaker and more affordable ol-mini can solve less than half of the tasks solved by the more powerful and expensive 01.\nProgress in improving the reasoning capabilities of LLMs through the development of more resource-intensive models is beneficial. However, it is important to recognise that data is finite, and the continued returns on scaling models with increased amounts of data, greater compute power, or more parameters remain uncertain. Additionally, rising financial and computational demands render such models less accessible. This serves to widen the gap with traditional planning systems, such as FastDownward [Helmert, 2011], which is capable of solving planning tasks with negligible operating costs and in a small fraction of the time taken by an LLM while guaranteeing solution optimality and correctness.\nWe propose an approach to augment the reasoning abilities of weaker models by applying the expertise of more powerful models. This is accomplished by prompting the more powerful model to generate a generalised strategy for solving tasks within a given domain. The strategy is then integrated as additional non-parameterised knowledge into the input text of the weaker model. This has a substantial effect on the average cost of solving a task in cases where the cost of generating the strategy can be amortised across several tasks. Additionally, we explore a complementary technique that further boosts model performance by identifying errors in its solutions and prompting it to make corrections accordingly.\nOur methods are capable of strongly improving the performance of weaker LLMs on reasoning tasks such as planning and maths. In our experiments, weaker models utilising our methods consistently performed at a level similar to the more powerful model used as the baseline, for a fraction of the cost. Furthermore, we show that even for the same model, the incorporation of a generalised strategy into the task prompt reduces reasoning token costs by 50 percent when solving tasks with four rounds of error correction and by 30 percent with-"}, {"title": "2 Background", "content": "2.1 Automated Planning\nAutomated Planning is a branch of Artificial Intelligence primarily focused on the generation of plans. Plans are sequences of actions within a given environment that can be sequentially applied to an initial state. As this paper explores the ability of LLMs to generate solutions to planning tasks, the words plan and solution are used interchangeably.\nA planning task \u03a0 is a 6-tuple (S, Si, Sg, A, fa, ft) consisting of:\n\u2022 A finite and discrete set of all states S, also known as the state space.\n\u2022 The initial state si \u2208 S.\n\u2022 The set of goal states Sg \u2286 S.\n\u2022 The set of actions A.\n\u2022 The applicability function fa, which returns fa(s) \u2286 A, i.e. the set of all actions applicable at state s.\n\u2022 The transition function ft, which maps a state s and applicable action a to the resultant state s' of applying a to s, such that s' = ft(s,a), a \u2208 fa(s).\nIn practice, most planning systems [Pohl, 1970; Jiang, 2021] perform graph search on a graph G = (V, E), where V is the set of vertices and E is the set of edges. Vertices are states s \u2208 S, while edges are formed from the transition function ft, where\n\u2200u, v \u2208 V, (u, v) \u2208 E \u21d4 \u2203a \u2208 A: ft(u,a) = v.\nA correct solution for a given planning task is a plan a1...an such that one can form a path vo... vn through G, where vi = ft(vi\u22121, ai), vo = si and vn \u2208 Sg; that is, a plan that transforms si into sg when applied sequentially. Heuristics [Hart et al., 1968; Lenat, 1982] are often used to guide the search and reduce the number of vertices that must be visited for a solution to be found.\nThe Planning Domain Definition Language (PDDL) [Ghallab et al., 1998] is a formal language that is widely used for the representation of domains and tasks in planning. In our experiments, the descriptions of planning tasks, states, domains, actions, and plans are represented in PDDL within our prompts and in the final output of the LLM. However, a full understanding of our research does not require prior knowledge of PDDL."}, {"title": "2.2 Large Language Models", "content": "Large Language Models (LLMs) [Chen et al., 2021; Chang et al., 2024; Zhao et al., 2023] are very large neural networks, typically containing billions of parameters. These models are trained through unsupervised learning on vast corpora of human-written text to effectively process long text sequences in parallel and capture non-contiguous word dependencies. Prompt engineering [White et al., 2023; Schmidt et al., 2024; Shin et al., 2020] is a common approach to enhancing the reasoning performance of LLMs without fine-tuning their parameters or altering their architecture. With prompt engineering, specific prompts are crafted to influence how the model's parametrised knowledge interacts with the input text. This can involve techniques like providing worked examples, phrasing the query in a question-and-answer format, or asking the model to 'think through its answer step-by-step' [Kojima et al., 2022]."}, {"title": "2.3 Related Work", "content": "Planning with LLMs The versatility of LLMs has resulted in a broad intersection between LLM research and planning methodologies. Some research leverages the capability of LLMs to store specialised domain knowledge, either in their pre-trained state for general environments [Huang et al., 2022; Singh et al., 2023] or through fine-tuning for specific domains [Chalvatzaki et al., 2023]. Other studies explore planning processes augmented by external planning systems [Kambhampati et al., 2024] or translation methods to convert complex natural-language tasks into representations compatible with existing planning and constraint satisfaction algorithms [Hao et al., 2024].\nWhile complex systems utilising LLMs as part of the planning process have shown progress, directly passing a planning task as input and obtaining a solution as output remains challenging. However, some advances have been made in guiding LLMs to decompose planning tasks into subtasks [Wang et al., 2023; Shen et al., 2023; Hao et al., 2023], and increasingly large and powerful LLMs have demonstrated modest success with smaller tasks [Valmeekam et al., 2024].\nSimilar to our work, Silver et al. [2024] explore the extraction of generalised knowledge from LLMs in the form of executable programs for solving specific domains. Their approach aims to circumvent the costly search processes employed by traditional planning systems by generating a tailored, non-search algorithm for task resolution within a specified domain. However, their method encounters difficulties when applied to domains with complex relationships between objects. Additionally, its effectiveness in handling challenging domains that lack structured and repeatable generalised solutions, or domains encompassing an extensive range of task configurations, remains uncertain. In contrast, our approach involves extracting generalised knowledge in the form of high-level textual prompts to guide the reasoning process of an LLM, which is articulated through fluid natural language output and demonstrates flexibility in handling diverse inputs and edge cases.\nEmpowering weaker models The primary method of enhancing the performance of weaker LLMs is through manipulating the pre-trained parameters of the model to better suit a specific task or domain. This is often done with synthetic data [Ma et al., 2024] or knowledge distillation [Gou et al., 2020] from a more powerful language model.\nApproaches to enhance LLM performance without affecting the model parameters often focus on manipulating"}, {"title": "3 Methodology", "content": "Figure 2 presents a diagram of our proposed approach. A comprehensive set of all prompts, strategies, domain descriptions and tasks used in our paper, as well as the source code to replicate our experiments, can be found at the link in Section 6.\nBlocksWorld Task. The BlocksWorld domain [Slaney and Thi\u00e9baux, 2001], used in our experiments, involves manipulating configurations of blocks on a table. An example of an initial and a goal state in this domain is shown in Figure 1.\nA prompt to solve a BlocksWorld task consists of the domain description for BlocksWorld, an example solution, and the initial state and goal state for the task to be solved. With the exception of the example solution, this information is generally present in a standard PDDL description of a planning task. The expected output is a sequence of actions to solve the task.\nThe example solution, provided with the intent of making the solution format clear, correctly solves a simple BlocksWorld task that is not present in the experiment dataset. The task solved by the example solution and the reasoning steps to solve it are not provided in order to avoid the LLM interpreting the example task's PDDL as part of the real task's PDDL. When providing a strategy, it is appended to the prompt between the domain and the example solution.\nGenerating a Strategy. To generate a generalised strategy, we prompt the strategy-generating LLM with the domain description for BlocksWorld, an example BlocksWorld task and solution, a set of instructions for how to construct the main strategy body, and an example strategy for a common domain known as Gripper [Alarnaouti et al., 2023], centred around moving balls between rooms. The strategy body comprises"}, {"title": "4 Results", "content": "This section presents our results on the BlocksWorld planning domain and a variation of the CRT maths reasoning dataset [Xie et al., 2024]. In order to improve the clarity and reproducibility of our work, we list in Table 1 the full model numbers of all five OpenAI models used in our experiments, as well as their current API costs by input and output tokens.\n4.1 Experimental Setup\nWe evaluate the effect of four generalised strategies on the success rate for o1-mini across 50 randomly generated BlocksWorld tasks. The tasks contain 5-6 blocks and have optimal solutions of 16-18 actions. As such, they are at least as difficult as the largest tasks in the main dataset used by Valmeekam et al. [2024], both by number of blocks and by solution length.\nThree of the strategies are generated by ol at test time, while one is handwritten by us in the same format as the LLM-generated strategies. As a baseline, we also evaluate the tasks on both o1 and 01-mini without providing a supplementary strategy. This results in six combinations of model and strategy, all of which undergo four rounds of error correction after an initial round of solving the tasks. Only the incorrect solutions from each round are selected for correction in the next."}, {"title": "4.2 Success Rates", "content": "The success rates for our experiments are presented in Table 2. There is a large difference in the baseline performance between ol and o1-mini on our BlocksWorld dataset, consistent with the findings outlined by Valmeekam et al. [2024].\nError Correction. Error correction improves the success rate of all six experiment variations. For the baseline 01-mini and the three generated strategies, adding four steps of error correction after the initial round roughly doubles their success rates. Notably, even just one error correction step has a strong impact, increasing the average success rate for o1-mini by 22 percentage points when using generated strategies. Baseline o1 solves 88 percent of the tasks before error correction is even applied. However, the first round of error correction further increases this by 10 percentage points, demonstrating that there is no visible pattern of some BlocksWorld tasks of this size being consistently unsolvable for o1. As such, our method of error correction is an efficient way to increase success rates even for already well-performing systems.\nIn order to evaluate the impact of including information about the error in the error correction prompt, we extended our experiments by utilising a different technique for our error correction rounds. Instead of using a specific error prompt, at each round of error correction, we repeated the initial task prompt for any incorrect solutions. This approach achieved 62 percent for baseline 01-mini and 89 percent when averaged across the three generated strategies.\nThere is no notable difference between providing detailed error feedback and simply repeating the initial prompt. This suggests that, while our specific method is not harmful, the effects of our error correction primarily stem from repeatedly attempting to solve incorrect tasks. Therefore, error correction is likely to be effective for tasks where the solution can be easily validated, even when one is not able to pinpoint exact errors.\nEffect of Generated Strategies. Incorporating any of the three generated strategies into the task prompt substantially enhances the ability of o1-mini to solve BlocksWorld tasks. Apart from the outlier in the initial round with strategy 3, each"}, {"title": "4.3 Token Analysis", "content": "OpenAI's ol and o1-mini achieve enhanced performance on reasoning tasks through performing internal reasoning steps. In order to do this, they feed their output tokens back into themselves as input. Tokens generated during this process are referred to as reasoning tokens and are (as of January 2025) priced at the same rate as output tokens. The quantity of reasoning tokens generated by a specific model is a good metric by which to compare both the computational and financial costs of solving a task. For each experiment variation, Table 3 presents:\n\u2022 The average number of reasoning tokens generated per task during the initial round (TPT).\n\u2022 The (USD) cost per solved task (CPS) in the initial round.\n\u2022 The average number of reasoning tokens generated when performing error correction (EC-TPT).\n\u2022 The total number of reasoning tokens generated across five rounds (Tokens).\n\u2022 The total (USD) cost of the reasoning tokens generated (Cost).\n\u2022 The (USD) cost per solved task (CPS) across five rounds.\nToken Count. In the initial round, there is an inverse correlation between the task success rate from Table 2 and the average token usage per task for experiment variations on 01-mini. Incorporating our handwritten strategy into the task prompts decreases the average reasoning tokens per task used by o1-mini by over 4000 tokens when compared to the baseline, while incorporating the generated strategies averages around 2000 saved tokens per task. In our experiments, we identified that there was no notable difference in average token usage based on task outcome (success or failure). Consequently, the observed differences in average token usage across experiment variations cannot be attributed to a disproportionate distribution of 'cheaper' outcome types. Any variation in average tokens per task must arise from factors other than the relative frequencies of those outcomes.\nThe inverse relationship between the total reasoning token usage across five rounds and the success rates in Table 2 is causal. Tasks that are not solved in a given round will require additional tokens in the next round when error correction occurs. There is no notable difference between the average tokens used by the first attempt to solve a problem and the subsequent error correction steps. After four rounds of error correction, incorporating our handwritten strategy into the task prompt reduces overall token usage by almost nine times when compared to the baseline, while incorporating generated strategies reduces overall token usage by around half.\nCost Analysis. Our results in Table 3 show that baseline ol is the most expensive of the experiment variations. This is unsurprising, as the price of a reasoning token is five times higher for o1 than for o1-mini. As such, in the initial round, any experiment variation running on o1-mini is guaranteed to be more cost-efficient per successful task if its success rate exceeds 20 percent. Likewise, with only four rounds of error correction, any variation running on o1-mini should not cost more than ol in total, regardless of success rate.\nDespite baseline 01-mini being more cost-efficient per successful task, ol is generally preferable due to its high success rates. The importance of this is more clearly visible in the total cost per successful task (total CPS), where the cost to run baseline o1-mini per task solved successfully increases by one-third from the initial round (initial CPS). By the end of the fourth error correction round, the total CPS for baseline 01-mini is equivalent to that of baseline 01, despite a 33 percentage point difference in success rate.\nThis increase in total CPS is within expectations, as tasks that were not solved in the initial round are likely to be more difficult and, therefore, require more tokens on average to solve or simply be too hard to solve altogether. In fact, the slight decrease from initial CPS to total CPS with GS3 is likely to be an outlier due to GS3's unexpectedly poor success rate in the initial round, as evidenced in Table 2.\nExpecting an increase in CPS makes the results of including a generalised strategy in the task prompt more impactful. We note that, on average, including a strategy in the task prompt for 01-mini eliminates any notable difference between the initial CPS and total CPS. This demonstrates a linear rate of return; that is, across five rounds and up to an average 90 percent success rate, increasing the initial success rate by a factor of n when using a strategy should only increase the cost by the same factor n.\nA linear rate of return has a strong impact on the cost-efficiency of solving BlocksWorld tasks. Including a strategy in the base prompt for o1-mini and running four rounds of error correction results in a better success rate than baseline ol for a third of the cost. As with the success rates, the effect on cost is amplified when using the handwritten strategy. Solving all 50 tasks using 01-mini with a handwritten strategy costs less than solving just four tasks with ol.\nEven without taking error correction and the effect of success rates into account, generalised strategies increase the cost-efficiency of solving BlocksWorld tasks. The additional cost to include a generated strategy, which averages 1200 tokens in size, is only 18 cents for the 50 tasks. Comparatively,"}, {"title": "4.4 Larger Problems", "content": "A key limitation for LLM reasoning on BlocksWorld tasks is that model performance degrades quickly as the complexity of the task increases. Valmeekam et al. [2024] report that, as the number of blocks within a task increases from 3-5 to 6-20 and solution length grows accordingly, the success rate of ol decreases by a factor of four.\nIn order to evaluate whether error correction and the incorporation of strategies into the base prompt could overcome this limitation, we test ol-mini on two randomly generated datasets of 20 BlocksWorld tasks each. The first dataset contains tasks with 10-12 blocks and solutions of length 30-32. The second contains tasks with 18-19 blocks and solutions of length 40-44. They are referred to as 'mid' and 'large' respectively.\nWe evaluate our datasets on ol-mini in three variations: without a strategy, with a handwritten strategy, and with strategy 1, which was the best-performing LLM-generated strategy from our main experiments. The initial and final (after four rounds of error correction) success rates for both datasets are presented in Table 4.\nResults. Our results show that with or without strategies, 01-mini still has difficulty with BlocksWorld tasks beyond a certain size. This is within expectations, as the motivation of the research was to explore ways to extract generalised and, therefore, amortisable knowledge from stronger models to enhance the performance of weaker models. ol also has difficulty with problems of this size [Valmeekam et al., 2024], so it would be surprising for a weaker model using ol's generalised strategy to show strong results.\nFor mid-size tasks, incorporating a strategy into the task prompt has less impact on the success rate than error correction. There is some evidence that this is also the case for"}, {"title": "4.5 Critical Reasoning Test", "content": "To demonstrate the transferability of applying generalised strategies to non-planning datasets, we evaluate our methods on the critical reasoning test (CRT) dataset [Frederick, 2005] used by Hegendorff et al. [2023] to compare reasoning biases between humans and LLMs. Recent research [Xie et al., 2024] has shown that, despite improvements in LLM performance on the CRT dataset, LLMs perform much worse when evaluated on variations of the original questions. Our dataset is composed of algebraic transformations of exponential growth (Type 3) CRT problems, which were presented as the most difficult of the variations explored by Xie et al. An example is given below.\nExample 1. A pot of water is boiling on the stove, and with every 2X hours, the overall volume of the evaporated water quadruples. If the entire pot takes 8Y hours to evaporate completely, how long does it take for 1/64 of the pot to evaporate?"}, {"title": "5 Conclusions", "content": "We propose two complementary methods for improving the reasoning performance of weaker LLMs. Our results show that our methods are effective and are able to vastly improve the ability of weaker LLMs to solve reasoning tasks not only in planning but also in mathematical reasoning domains. Utilising our methods, weaker LLMs are able to perform at a higher level than more powerful baseline LLMs and require only a fraction of the cost to do so. Furthermore, we show that generalised strategies can reduce the usage of reasoning tokens and, by extension, the cost of solving reasoning tasks even for identical LLMs.\nLimitations and Future Work A key limitation of our research is the small number of domains on which our methods are tested. Future research on a larger number of domains could improve the generalisability of our methods. It is also important to explore domains that might not have clear and easily generalisable solutions.\nOur error correction is only effective for tasks with solutions that are easier to validate than the task is to solve. This is always the case with planning tasks, but limits our ability to extend our method of error correction to all reasoning problems.\nOur methods show vast improvements for weaker LLMs solving small BlocksWorld tasks but fail to address the sharp decline in performance as task size increases that is present in our baseline. Future work could explore methods of merging our approach with hierarchical planning or problem decomposition, which could allow the models to solve larger tasks piece by piece.\nThe reduction in individual task reasoning cost resulting from the addition of a strategy into the prompt is quite promising. The generated strategies used in our paper were inexpensive to generate and, in the case of the CRT domain, only required a very simple prompt. More work in exploring this effect across a larger set of models and domains could reveal a simple method for generally reducing LLM reasoning costs for tasks within a common domain."}, {"title": "6 Declarations", "content": "To enhance the clarity of our research and promote future work, we will open-source our code and data at https://github.com/andrey-borro/reasoning-gap."}, {"title": "A Automatic Summary", "content": "In order to aid the automatic evaluation of our tasks, we relied on a summarisation step for both Blocks World and Type 3 CRT tasks. For BlocksWorld, the summarisation step turns a large o1-mini output into a PDDL list of actions, mostly by extracting the actions from the larger body of the solution. This is done using gpt-4o and is quite trivial. Furthermore, 4o is substantially worse than 01-mini at solving PDDL, so there is little concern about this step secretly inflating success rates.\nThe Type 3 CRT tasks are summarised by 01-mini, which poses a concern as it is much better at CRT tasks than the models whose answers it is summarising. We handle this by only providing 01-mini with the final formula answer. It never sees the original task or the reasoning behind it, and the summarisation step is posed as an algebraic reformulation task. An example of this process can be found in the LLM Output section of this Appendix (D.5 and D.6).\nIn our view, both summarisation steps worked as expected, ensuring that correct answers were marked as correct and incorrect answers were marked as incorrect."}, {"title": "B Prompts", "content": "B.1 Blocks World Task Prompt\nConsider the following PDDL domain with 4 operators:\naction: pickup (block)\npreconds : (clear block), (ontable block), (handempty)\neffects : (holding block), not (ontable block), not (clear block), not (handempty)\naction: putdown (block)\npreconds : (holding block)\neffects: not (holding block), (clear block), (ontable block), (handempty)\naction: unstack (top-block bottom-block)\npreconds : (on top-block bottom-block), (clear top-block), (handempty)\neffects : (holding top-block), (clear bottom-block), not (on top-block bottom-block), not(\nclear bottom-block), not (handempty)\naction: stack (top-block bottom-block)\npreconds : (holding top-block), (clear bottom-block)\neffects : (on top-block bottom-block), (clear top-block), (handempty), not(holding top-block)\nnot (clear bottom-block)\n*THIS IS YOUR PROBLEM, SOLVE IT*:\nInitial State:\n(handempty)\n(clear a)\n(clear d)\n(clear b)\n(on df)\n(on fc)\n(on be)\n(ontable a)\n(ontable c)\n(ontable e)\nGoal:\n(on a b)"}, {"title": "B.2 Type 3 CRT Task Prompt", "content": "Solve the following problem. Give a formula enclosed in double at symbols as your final answer at the end, e.g. @@ANSWER_HERE@@\nAfter a switch is flicked, the area of light shining within a room triples every 5X nanoseconds until it fills the room. If it takes 7Y nanoseconds for the room to fill, how many nanoseconds after the switch was flicked was the room at 1/9 full?\nA general strategy for this problem type has been given below."}, {"title": "B.3 Blocks World Strategy-Generating Prompt", "content": "You will be given a PDDL domain, as well as an example problem and solution for that domain. Your goal is to write a strategy for the domain which will be used as part of a prompt for a weaker LLM to solve this problem. The strategy should be threefold.\nFirstly, a brief overview of the high-level dynamics, e.g. how the actions interact and what they are used for in the domain. Do not just repeat the preconditions and effects in PDDL the LLM will have access to these. Instead, use English to explain the environment and how the actions are used to do things in it. Finish this overview with a brief list of key constraints about objects within this environment (see example).\nSecondly, a high-level guide to solving a general problem within this domain. The guide should open with \"To solve the problem, follow these step-by-step instructions:\". Follow this with an instruction step outlining how to interpret the goal configuration in the context of planning your solution. After this, give simple and clear instructions on how to solve the task, step-by-step. Remember that you are talking to an LLM. It has no memory or space to \"visualise\" a problem, nor does it respond well to being philosophical Instead, give clear and very simple actionable instructions for it to follow. Each instruction should represent a \"big picture\" step for the agent to follow. Then, explain how to achieve that step in sub-instructions. Absolutely prioritise the most simple (to understand and follow) instructions, even if it means having a longer solution. Do not forget that the LLM executing this is weak, and should not try to do complex reasoning if it can be avoided. Your job is to do as much of the thinking as you can, and to give it clear steps to take action on.\nThirdly, give a list (at most five) of common mistakes that you assume a weaker LLM is likely to make and how to avoid them. Prioritise the ones that you think might be most likely to stump the LLM.\nThroughout the strategy, feel free to use general examples (but keep them very short and concise, as they should not take up much of the strategy at all)."}, {"title": "B.4 Type 3 CRT Strategy-Generating Prompt", "content": "Take the below problem:\nAfter a switch is flicked, the area of light shining within a room triples every 5X nanoseconds until it fills the room. If it takes 7Y nanoseconds for the room to fill, how many nanoseconds after the switch was flicked was the room at 1/9 full?\nI want you to write an explanation for a weaker LLM on how to solve similar problems. Note that the problems will have different numbers, thematic settings or even twists on algebraic variables, so make your strategy generalised."}, {"title": "C Strategies", "content": "C.1 Blocks World Handwritten Strategy\n**Domain Overview:**\nIn this domain, you are a robot arm tasked with stacking and unstacking blocks on a table. You are capable of four actions, and can hold at most one block at a time.\n1. (unstack x y) which unstacks block x from block y, and holds it in your hand\n2. (stack x y) which stacks the block you are currently holding (x) onto a block that is at the top of a stack of blocks (y).\n3. (pickup x) which picks up a block from the table and holds it in your hand.\n4. (putdown x) which places the block in your hand down onto the table.\nunstack and stack are used when taking a block off a stack of blocks or putting it onto one. pickup and putdown are used when taking a block directly off the table or putting it directly on.\n**Key Constraints:**\nYou cannot pick up or unstack any block that has a block on top if it. It must be at the top of its stack, or by itself on the table.\nYou can only put a block onto the table or onto the top of a stack. It is impossible to place it down onto a block that already has a block on top.\nYou cannnot pick up or unstack a block if you already have one in your hand. You hand only has capacity for one block.\nYou cannot put down a block or stack a block that you are not holding. It must be the one in your hand.\n**Solution Overview**\n1. Unstack all of the blocks onto the table.\n1.1. Pick any stack that isnt just one block high (e.g. if block x is by itself on the table, you can ignore it).\n1.2. Unstack the top block and place it onto the table\n1.3. Repeat from step [1.2] until you are down to the bottom block of the stack (the whole stack is now just on the table)\n1.4. Repeat from step [1.1] until you have done this for every stack (all the blocks are by themselves on the table)\n2. Assemble the goal stack\n2.1. Pickup the next block to go on the final stack (remember that the bottom block is already on the table if you did step 1 correctly).\n2.2. Stack that block onto the stack\n2.3. Repeat from step [2.1] until complete.\nExample: (Goal stack is 'a on bon c'), then you pickup b, stack on c, pickup a, stack on b\nMake sure to annotate your steps as you go, e.g.\nUnstacking all blocks in the stack x-z (x on the bottom, z on the top)\n(unstack zy)\n(unstack yx)\n**Common Mistakes:**\n1. Attempting to 'pickup' a Block Not on the Table:\nMistake: Using (pickup block) on a block that is on another block.\nSolution: Use (unstack block block-below) instead to remove it from the stack.\n2. Stacking onto a Block That is Not Clear:\nMistake: Trying to stack a block onto another block that has a block on top of it.\nSolution: Ensure the bottom block is clear by unstacking any blocks on top of it before stacking.\n3. Forgetting to Clear Obstructing Blocks:\nMistake: Ignoring blocks that are on top of the blocks you need.\nSolution: Unstack and put down any obstructing blocks to access the necessary blocks."}, {"title": "C.2 Blocks World Generated Strategy 1", "content": "Domain Overview:\nIn this domain", "blocks": "n1. Pickup (block): Pick up a clear block from the table when the agent's hand is empty.\n2. Putdown (block): Place a block the agent is holding onto the table.\n3. Unstack (top-block", "bottom-block)": "Remove a clear top block from a stack when the agent's hand is empty.\n4. Stack (top-block"}, {"bottom-block)": "Place a block the agent is holding onto a clear bottom block to form a stack.\nKey Constraints:\nThe agent can hold only one block at a time.\nA block must be clear (no blocks on top of it) to be picked up", "Overview": "nTo solve the problem", "instructions": "n1. Understand the Goal Configuration:\nIdentify each block and determine where it needs to be in the final arrangement.\nNote the desired stacking order from bottom to top as specified in the goal.\n2. Plan the Stacking Sequence from Bottom to Top:\nList the blocks in the order they need to"}]}