{"title": "Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation", "authors": ["Shuzhou Sun", "Li Liu", "Yongxiang Liu", "Zhen Liu", "Shuanghui Zhang", "Janne Heikkil\u00e4", "Xiang Li"], "abstract": "Bias in Foundation Models (FMs)-trained on vast datasets spanning societal and historical knowledge-poses significant challenges for fairness and equity across fields such as healthcare, education, and finance. These biases, rooted in the overrepresentation of stereotypes and societal inequalities in training data, exacerbate real-world discrimination, reinforce harmful stereotypes, and erode trust in AI systems. To address this, we introduce Trident Probe Testing (TriProTesting), a systematic testing method that detects explicit and implicit biases using semantically designed probes. Here we show that FMs, including CLIP, ALIGN, BridgeTower, and OWLv2, demonstrate pervasive biases across single and mixed social attributes (gender, race, age, and occupation). Notably, we uncover mixed biases when social attributes are combined, such as gender race, gender\u00d7age, and gender occupation, revealing deeper layers of discrimination. We further propose Adaptive Logit Adjustment (AdaLogAdjustment), a post-processing technique that dynamically redistributes probability power to mitigate these biases effectively, achieving significant improvements in fairness without retraining models. These findings highlight the urgent need for ethical AI practices and interdisciplinary solutions to address biases not only at the model level but also in societal structures. Our work provides a scalable and interpretable solution that advances fairness in AI systems while offering practical insights for future research on fair AI technologies.", "sections": [{"title": "1 Introduction", "content": "Foundation Models (FMs), trained on large-scale datasets, have exhibited remarkable capabilities in feature representation and are widely applied in sectors such as healthcare, education, finance, and technology. 1\u20134 However, the inherent biases in FMs, stemming from both technical and societal factors, have raised significant concerns. These biases manifest as systematic unfairness in model outputs, including misclassifications and stereotypes related to gender, race, and culture. 5-7 Such biases undermine fairness and reliability, exacerbate societal disparities, and erode public trust in AI technologies. The roots of bias in FMs lie in entrenched societal and historical stereotypes embedded within the training data. Labeling Theory suggests that societies assign negative or positive connotations to certain groups or behaviors, 8,9 which serve as the foundation for stereotypes and, in turn, fuel biases in FMs. Similarly, Social Identity Theory highlights the human tendency to classify individuals into \u201cin-groups\" and \"out-groups,\u201d ascribing favorable traits to in-groups while burdening out-groups with negative stereotypes. 10-13 Notably, differences in societal discourse power among groups amplify the stereotypes of power groups and even evolve into mainstream views. These views are disseminated and inherited through literature, historical records, proverbs, and popular music, eventually embedding into the training data of FMs, thus perpetuating and amplifying biases. In this section, we systematically discuss four critical questions regarding biases in FMs: Why harmful; How to test; Who is harmed;"}, {"title": "1.1 Why harmful", "content": "The widespread deployment of FMs in fields such as healthcare, education, finance, and technology has raised serious concerns about the biases embedded in these models. These biases manifest as systemic unfair treatment of certain groups, such as discriminatory classifications based on race, gender, or age, damaging both the fairness and reliability of AI systems; this results in far-reaching negative impacts on society. 14,15 For instance, the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system, extensively used in the U.S. judicial system for criminal risk assessments, has demonstrated biases against African Americans by frequently misclassifying them as high risk.16 This bias leads to inequitable bail and sentencing outcomes, exacerbating judicial disparities. Additionally, the Workday HR System has faced allegations of racial, age, and disability discrimination, 17 violating Title VII of the Civil Rights Act of 1964 and other federal anti-discrimination laws in the U.S., 18 with a potential class-action lawsuit affecting hundreds of thousands. Such biases not only restrict employment opportunities for affected groups but also hinder labor market diversity.\nThese cases illustrate that biases in FMs are not isolated technical errors but systematic issues that impact societal fairness, ethical norms, and economic structures at multiple levels. In detail, FMs' biases could cause significant harm to society and the advancement of technology through:\n(1) Exacerbating social injustice and discrimination. Biases in FMs reinforce stereotypes against vulnerable groups across various fields, intensifying social inequality.\n(2) Legal and ethical risks. Biased decision-making in critical areas such as law enforcement and employment can violate fairness principles, leading to legal liabilities and ethical dilemmas.\n(3) Crisis of trust and barriers to technology adoption. Biases erode public"}, {"title": "1.2 How to test", "content": "Recent years have witnessed growing interest in developing methods to test biases within FMs. Existing approaches often target specific social attributes or focus on a single type of bias, limiting their ability to comprehensively capture the complex stereotypes present across diverse groups. 5,19\u201325 In this study, we introduce Trident Probe Testing (TriProTesting), a bias testing method with a trident-like design that incorporates three types of probes: negative, positive, and neutral probes. This method is designed to systematically evaluate biases in FMs across multiple social dimensions and uncover nuanced patterns of bias. Moreover, our proposed TriProTesting specifically distinguishes between explicit and implicit biases, offering a novel perspective for a holistic understanding of model biases.\nThe design of TriProTesting is inspired by Labeling Theory, which posits that society shapes the social status and value of specific groups or behaviors through labeling. 8,9 Over time, these labels embedded in literature, historical accounts, proverbs, and popular culture are internalized by FMs during training, manifesting as explicit or implicit biases in their outputs. 10,26,27 By utilizing semantically explicit probes, TriProTesting examines how FMs respond to various demographic groups. For instance, as illustrated in Fig. 1 A, an input image labeled as a \u201cchef\u201d might produce two distinct outcomes: a correct prediction of \"chef,\" indicating no bias, or a misclassification as \u201ccriminal,\u201d influenced by negative stereotypes associated with certain groups. This approach enables TriProTesting to directly reveal model biases manifesting under specific conditions.\nTo fully reveal biases within FMs, we design three types of probes: Negative Probes, Positive Probes, and Neutral Probes (Fig. 1 B). This design is motivated by the long-standing transmission of societal and historical biases, which typically appear as explicit or implicit biases. Explicit biases are directly expressed through clear and unfair attitudes or stereotypes towards specific groups. . 28 For example, despite its intent to denounce slavery, the novel Uncle"}, {"title": "1.3 Who is harmed", "content": "Social stratification theory highlights the unequal distribution of resources, power, and status among social groups, a disparity deeply rooted in societal structures and exacerbated by cultural and cognitive biases across dimensions such as gender, age, occupation, and race. 10,11,29 Specifically, gender biases significantly affect man's and woman's opportunities to access social resources; 30 age biases impact assessments of capabilities and values across different age groups, influencing policy and resource allocation;31 occupational biases impact societal evaluations and treatment of individuals based on their professions; 32 and racial biases, entrenched in history and still active, undermine fairness and inclusivity. 33 In this work, we focus on four core dimensions: gender, age, occupational, and racial biases, aiming to reveal the underlying mechanisms of social stratification in FMs and the potential systematic discrimination against specific groups. To identify affected groups within these core dimensions, we design two testing approaches for a comprehensive analysis of biases associated with single and mixed social attributes:"}, {"title": "1.4 What can be done", "content": "Social biases typically result in an unfair distribution of resources and social evaluations among different groups. Power redistribution is an effective solution to social inequalities, aiming to enhance the circumstances of vulnerable groups through adjustments in resource allocation and structural power dynamics. 42,43 In FMs, biases can be viewed as a \u201ctechnological reproduction of inequality,\u201d where the overrepresentation of mainstream views in the training corpus leads to their dominance in model outputs, described as \"probability power.\u201d This phenomenon manifests as strong correlations between non-discriminated groups and positive probes, alongside excessive associations of discriminated groups with negative or neutral probes.\nInterestingly, the principle of logit adjustment used in computer science to address long-tail distribution issues mirrors the fundamental concept of power redistribution. 44\u201346 Logit adjustment modifies the output logit values with a set of adjustment factors, redistributing probabilities across categories to reduce biases (Fig. 1 E). Building on this concept, we propose Adaptive Logit Adjustment (AdaLogAdjustment), a bias mitigation method that redistributes probability power across categories, balancing model responses to explicit and implicit biases.\nUnlike traditional logit adjustment methods for addressing long-tail distribution prob- lems, 44-46 our approach employs an automated learning strategy for adjustment factors, enabling flexible bias mitigation across diverse FMs, datasets, and social attributes (see Method section for details). Compared to conventional training-stage methods that miti- gate biases through data rebalancing or model architecture adjustments, our method offers several key advantages:\nEfficiency. By analyzing a small set of labeled samples, it adaptively learns adjustment factors, significantly reducing both computational and data demands.\nInterpretability. By explicitly controlling the model's response intensity to probes, the"}, {"title": "2 Results and Discussion", "content": "In the previous section, we discussed four key questions central to understanding biases in Foundation Models (FMs): Why harmful, How to test, Who is harmed, and What can be done. These questions comprehensively address the societal impacts, testing approaches, affected groups, and mitigation strategies for biases in FMs. In this section, we will provide answers to these questions through our experimental findings. The results of the Single Bias Test and Mixed Bias Test primarily affirm the effectiveness of the Trident Probe Testing (TriProTesting) method. These results confirm the efficacy of our proposed testing method while highlighting specific bias manifestations and the adversely affected groups, further elucidating the societal risks posed by these biases. Subsequently, the section on Bias Mitigation with Adaptive Logit Adjustment will demonstrate how our proposed Adaptive Logit Adjustment (AdaLogAdjustment) method can effectively address biases in FMs.\nBefore formally analyzing the experimental results, it is essential to note that the data representing \u201cprobability of being predicted as a probe\" in Fig. 2 and 3 have been normalized. During testing, it was observed that the range of bias manifestations varies significantly among different models. Presenting raw probability data without processing could obscure the subtleties of models with narrower ranges of bias, thus affecting the observation of their bias performance. To address this, we applied min-max normalization to each model's data,"}, {"title": "2.1 Results of Single Bias Test", "content": "This section systematically analyzes experimental results from four FMS\u2014CLIP, 38 ALIGN, 39 BridgeTower, 40 and OWLv241\u2014highlighting biases in single social attributes, including gender, age, occupation, and race. The Single Bias Test is significant not only for uncovering model stereotypes related to individual social attributes but also for establishing a foundation to explore biases arising from combinations of multiple attributes. By conducting systematic tests on individual attributes, we can identify and quantify how social biases are specifically manifested within these models, trace their origins, and offer precise guidance for developing future bias mitigation strategies. Fig. 2 A-D illustrate bias distributions for the four models across four datasets, with the x-axis representing group classifications and the y-axis depicting three probe types. The size of the bubbles in Fig. 2 A-D represents the models' prediction accuracy with probes included, while the color of the bubbles indicates the probability of group classifications being predicted as probes. Thus, bubble size and color reveal the models' sensitivity to different probes. The manifestation of biases across different social attributes is evidenced by significant differences in classification accuracy and tendencies to predict as probes. The results in Fig. 2 A-D highlight the prevalence and pat-"}, {"title": "2.2 Results of Mixed Bias Test", "content": "This section analyzes experimental results from four FMs, focusing on bias patterns across gender\u00d7race, gender\u00d7occupation, and gender\u00d7age combinations. The Mixed Bias Test, designed to detect systematic biases towards groups characterized by these combined attributes, provides a more detailed perspective compared to tests of single-attribute biases. These tests are essential not only for expanding our understanding of the societal biases inherent in models but also for elucidating the compound effects of these biases.\nFig. 3 A-D depicts mixed bias distributions in three extended datasets (UTKFACE,"}, {"title": "2.3 Bias Mitigation with Adaptive Logit Adjustment", "content": "This section validates the effectiveness of our proposed Adaptive Logit Adjustment (AdaLogAdjustment) in reducing biases and enhancing fairness within FMs. For a detailed explanation of AdaLogAdjustment, refer to the Method section. Fig. 4 reports enhancements in performance across all test scenarios, with the y-axis representing 15 probes and the x-axis showing the \"Improved macro average accuracy through AdaLogAdjustment,\u201d indicating the performance improvement of FMs equipped with AdaLogAdjustment compared to a vanilla"}, {"title": "3 Conclusion", "content": "The findings of this study reveal that biases in Foundation Models (FMs) are both pervasive and multifaceted, manifesting across core social attributes such as gender, age, race, and occupation. By systematically applying Trident Probe Testing (TriProTesting), we have illuminated how explicit and implicit biases are deeply embedded in FMs, stemming from the societal and historical stereotypes encoded in their training data. These biases not only reinforce harmful societal inequalities but also challenge the fairness and reliability of AI systems in critical applications. The proposed Adaptive Logit Adjustment (AdaLogAdjustment) demonstrates a transformative capability to mitigate these biases, dynamically redis-"}, {"title": "4 Method", "content": "In this study, we select four datasets\u2014CelebA,34 UTKFace, 35 FairFace, 36 and IdenProf37 (Fig. 1 C) covering core social attributes such as gender, age, race, and occupation for the Single Bias Test. For the Mixed Bias Test, we expanded three of these datasets by adding gender labels, creating extended versions: UTKFACE, FAIRFACE, and IDENPROF (Fig. 1 D).\nSpecifically, CelebA is a facial dataset with gender labels, utilized for assessing gender bias. FairFace, centered on racial annotations, serves to evaluate racial biases. IdenProf, an occupational classification dataset, is employed for assessing occupational biases. Notably,"}, {"title": "4.1 Data Preparation", "content": "UTKFace is a large-scale face dataset annotated with continuous age values. However, directly using continuous age annotations may fail to effectively distinguish model biases across age groups. To address this, we resegment the dataset into five categories: children (0-12 years), teenagers (13-19 years), young adults (20-35 years), middle aged (36-60 years), and elderly (61+ years). This segmentation more clearly exposes stereotypes at typical age stages and aids in identifying potential high-risk age groups in model predictions.\nTo extend UTKFace, FairFace, and IdenProf for mixed bias testing, we employ the CLIP model (ViT-B/3247) to automatically generate gender labels. The process involves feeding preprocessed images into the model alongside two text prompts: [\"a photo of a man\u201d, \u201ca photo of a woman\u201d]. The model classifies each image as either \"man\" or \"woman,\u201d which is then recorded as the extended label. This automated labeling process reduces the need for costly manual annotations while ensuring consistent labeling. Utilizing this method, we develop extended datasets (Fig. 1 D): UTKFACE is categorized by age and gender into ten composite labels, such as \u201celderly_woman\"; FAIRFACE is divided by race and gender into fourteen labels, such as \u201cIndian_woman\";and DENPROF is segmented by occupation and gender into twenty labels, such as such as \u201cdoctor_woman.\u201d This expansion captures more complex combinations of social attributes, thereby supporting Mixed Bias Tests. Additionally, this process demonstrates the practical utility of Foundation Models (FMs) in real-world data annotation tasks, as highlighted in many current studies."}, {"title": "4.2 Probes Design and TriProTesting", "content": "In an effort to fully uncover the complex nature of biases within FMs, our research strategically designs probes to guarantee that test outcomes are scientifically accurate, targeted, and socially significant. The design is governed by two fundamental principles: the systematic categorization of probes and the selection of representative probes, both of which are intended to provide a detailed portrayal of both explicit and implicit biases and support a comprehensive bias analysis. 11,26,28,29"}, {"title": "4.3 Models Tested", "content": "In this study, we test biases in four representative FMs: CLIP, 38 ALIGN, 39 BridgeTower, 40 and a modified OWLv2.41\nCLIP, a pioneering multimodal alignment model, employs contrastive learning to establish shared embedding spaces for images and text, excelling in zero-shot classification and cross-modal retrieval, and serves as an early advocate of prompt engineering. ALIGN advances this capability by employing weakly supervised learning on an expansive dataset of 1.8 billion image-text pairs, demonstrating the potential of big data to enhance model generalizability while also raising concerns about the complex biases embedded in noisy training data. Bridge Tower's innovative \u201cBridge Layer\u201d integrates single-modal encoding with multimodal interactions, showing promising results in tasks like visual question answering and multimodal retrieval. By probing BridgeTower, we explore how model architectures might"}, {"title": "4.4 Evaluation Metrics", "content": "Overall accuracy. To quantitatively evaluate model performance in bias tests, we present extensive experimental results in Fig. 2, 3, and 4. This section details the calculation methods for the metrics and their implications. Overall accuracy evaluates a model's classification performance across all samples in a single probe test, representing the aggregate prediction accuracy across categories. The formula for calculating overall accuracy is:\nAccuracy = $\\frac{N_{correct}}{N_{total}}$,\nwhere $N_{total}$ and $N_{correct}$ denote the number of samples and the number of correctly predicted samples in a probe test scenario, respectively. For instance, in the bubble charts of Fig. 2 A-D, the size of the bubbles indicates overall accuracy, visually reflecting the model's classification"}, {"title": "4.5 Adaptive Logit Adjustment", "content": "Addressing pervasive biases in FMs is a current focus, 7,48 with typical mitigation strategies falling into four categories: Pre-Processing, In-Training, Intra-Processing, and Post-Processing Mitigation. Pre-Processing Mitigation improves the training dataset's representativeness and diversity through data augmentation, reweighting, or generating new data. 21,22\nIn-Training Mitigation integrates fairness mechanisms by modifying model architectures, incorporating new optimization objectives, or selectively updating parameters. 24,25 Intra-Processing Mitigation adjusts decision-making processes during application, such as modifying decoding strategies or adjusting probability outputs to reduce bias. 5,23 Post-Processing Mitigation directly eliminates manifestations of bias by altering model outputs, such as rewriting texts or adjusting classifications. 19,20 Although these approaches offer advantages in mitigating model biases, they often depend on extensive data annotation or complex structural modifications, which limit their scalability in practical applications."}, {"title": "S2.1 Ablation Study on Sample Size", "content": "The parameter N in our method determines the number of samples randomly selected from each class to construct the training set for learning the adjustment factors a'. These adjustment factors are critical for redistributing the probability power of logits, as described in the main paper. In our main experiments, we set a small sampling size (N=20) to balance computational efficiency and performance. To evaluate the robustness of our method and the impact of sample size on bias mitigation, we conducted an ablation study with N set to 10, 20, 30, 40, 100, and 200. The results, averaged over three runs to account for randomness in sample selection, are reported in Table S10. We have the following observations:\nOptimal performance at N = 20. Across most probe testing scenarios, N = 20 achieves the best results in improving macro average accuracy through AdaLogAdjustment. This suggests that N = 20 strikes the optimal balance between sample size and the ability to generalize adjustment factors effectively. Notably, in some scenarios, the superiority of N = 20 is particularly pronounced. For instance, in the CLIP model tested with the CelebA dataset using \u201cperson\u201d as the probe, N 20 improves macro average accuracy by 37.81%, significantly outperforming other sample sizes, which achieve improvements of approximately 25%.\nDecreased performance with small N. When N is set to a small value (e.g., N = 10), the performance generally decreases. This is likely due to insufficient diversity in the training set, as smaller sample sizes may fail to capture the variability in the data distribution. As a result, the learned adjustment factors are less effective at mitigating biases across diverse groups and probes.\nDecreased performance with large N. Interestingly, increasing N to large values (e.g., N=100 or N = 200), also results in a decline in performance. This decline is attributed to overfitting during optimization, where excessive training samples cause the adjustment factors to overly align with the specific characteristics of the training set, re-"}, {"title": "S2.2 Ablation Study on Learning Rate", "content": "The choice of learning rate is critical in the optimization process for learning logit ad- justment factors in the AdaLogAdjustment framework. To evaluate the impact of different learning rates, we conducted ablation studies across four representative test scenarios: testing"}, {"title": "S2.3 Implementation Details of OWLv2 Adaptation", "content": "Algorithm 1 OWLv2 Adaptation for Bias Testing\nInput: Image I, List of class labels L = {$l_1, l_2, . . ., l_n$}, List of probes P = {$p_1, p_2, ..., p_m$}\nOutput: Image-level logit and predicted probabilities for bias analysis\n1: Load OWLv2 components:\n2: model \u2190 OwlViTForObjectDetection(pretrained model path)\n3: processor \u2190 OwlViTProcessor(pretrained model path)\n4: Construct prompt list:\n5: prompts \u2190 {\u201ca photo of a \" + l|l \u2208 L} \u222a {p|p \u2208 P} Incorporation of prompt-based text queries\n6: Preprocess inputs:\n7: inputs \u2190 processor(text=prompts, images=I, return_tensors=\"pt\")\n8: Forward pass through the OWLv2 model:\n9: outputs \u2190 model(inputs)\n10: logits_bbox \u2190 outputs.logits\n11: Remove bounding box dependency:\n12: logits_image \u2190 mean(logits_bbox, dim = 1) Removal of bounding box prediction\n13: Compute image-level predictions:\n14: probabilities \u2190 softmax(logits_image, dim = -1) Logit for image-level predictions\n15: Output: Return probabilities for all prompts.\nOWLv2, originally designed as an open-vocabulary object detection model, focuses on"}, {"title": "S2.4 Pseudocode for TriProTesting and AdaLogAdjustment", "content": "This subsection provides the pseudocode for the TriProTesting and AdaLogAdjustment frameworks used in our study (Algorithm 2). The pseudocode formalizes the steps involved in bias testing and mitigation, offering a concise, reproducible outline of the methodologies detailed in the main paper.\nTriProTesting Framework (Algorithm 2, lines 1-19). The first stage of the pseudocode implements TriProTesting, which systematically evaluates biases in Foundation Models (FMs) using defined probes. By aggregating logit values for Negative, Neutral, and Positive probes across datasets, the framework enables a detailed analysis of explicit and implicit bias patterns inherent in the models. This stage focuses on revealing and quantifying biases in the FMs, providing a foundation for understanding how biases manifest before applying mitigation strategies.\nAdaLogAdjustment Framework (Algorithm 2, lines 20-34). The second stage introduces the Adaptive Logit Adjustment (AdaLogAdjustment) framework, which mitigates biases by dynamically learning logit adjustment factors. This stage uses a subset of the dataset as a training set to optimize adjustment factors through iterative updates. These optimized factors are subsequently applied to logit values from the testing set, yielding adjusted predictions that demonstrate reduced bias.\nWe provide the pseudocode to illustrate the modular design and practical implementation of the proposed TriProTesting and AdaLogAdjustment frameworks. By clearly separating the workflow into two phases\u2014bias detection and bias mitigation\u2014we ensure the reproducibility of our method and facilitate its application across various models and datasets. Our structured approach enables systematic analysis of biases while providing a robust mechanism for effective mitigation. In addition, we emphasize that our framework is not only"}, {"title": "4.4 Evaluation Metrics", "content": "performance across various probe test scenarios.\nProbability of being predicted as a probe. The probability of a category being predicted as a probe is a critical metric for assessing how often a model identifies a specific category with a given probe in probe tests. This metric illuminates the degree of bias a model exhibits towards specific categories in various probe scenarios. The calculation formula is:\nP( class \u2192 probe) = $\\frac{N_{class,probe}}{N_{class}}$,\nMin-Max Normalization. To prevent minor bias discrepancies from being overlooked in visual representations, we normalize the probabilities of probe predictions. The normalization formula is:\nP\u2032 = 100 \u00d7 $\\frac{P-P_{min}}{P_{max}-P_{min}}$,\nMacro average accuracy. Macro average accuracy is utilized to evaluate the overall performance of models in multi-class tasks. Unlike weighted average accuracy, macro average"}, {"title": "4.5 Adaptive Logit Adjustment", "content": "accuracy assigns equal weight to each category, providing a fairer evaluation of model performance across diverse classes, particularly in scenarios with imbalanced class distributions. Macro average accuracy is calculated as:\nMacro Average Accuracy = $\\frac{1}{C}$$\\sum_{i=1}^{C}$ $\\frac{N_{i,correct}}{N_{i}}$,"}], "keywords": ["Foundation Models", "Language Models", "Bias", "Fairness"]}