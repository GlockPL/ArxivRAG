{"title": "Combinatorial Multi-armed Bandits: Arm Selection via Group Testing", "authors": ["Arpan Mukherjee", "Shashanka Ubaru", "Keerthiram Murugesan", "Karthikeyan Shanmugam", "Ali Tajer"], "abstract": "This paper considers the problem of combinatorial multi-armed bandits with semi-bandit feedback and a cardinality constraint on the super-arm size. Existing algorithms for solving this problem typically involve two key sub-routines: (1) a parameter estimation routine that sequentially estimates a set of base-arm parameters, and (2) a super-arm selection policy for selecting a subset of base arms deemed optimal based on these parameters. State-of-the-art algorithms assume access to an exact oracle for super-arm selection with unbounded computational power. At each instance, this oracle evaluates a list of score functions, the number of which grows as low as linearly and as high as exponentially with the number of arms. This can be prohibitive in the regime of a large number of arms. This paper introduces a novel realistic alternative to the perfect oracle. This algorithm uses a combination of group-testing for selecting the super arms and quantized Thompson sampling for parameter estimation. Under a general separability assumption on the reward function, the proposed algorithm reduces the complexity of the super-arm-selection oracle to be logarithmic in the number of base arms while achieving the same regret order as the state-of-the-art algorithms that use exact oracles. This translates to at least an exponential reduction in complexity compared to the oracle-based approaches.", "sections": [{"title": "Introduction", "content": "The combinatorial multi-armed bandit (CMAB) problem is a generalization of the stochastic multi-armed bandit problem, in which there is a set of base arms and a learner selects a subset of them at each round. Such sets of base arms are called a super-arms, and the set of all possible super-arms constitutes the action space of the learner.\nBandit versus semi-bandit feedback. CMABs can be broadly divided into two settings according to the level of feedback a learner receives in response to its actions: the bandit and the semi-bandit feedback settings. In the bandit feedback setting, the learner pulls a super-arm and observes one aggregate reward value generated by the selected super-arm. On the other hand, in the semi-bandit feedback setting, in addition to the aggregate reward, the learner has access to a set of stochastic observations generated by the individual arms that constitute the selected super arm. This paper focuses on semi-bandit feedback, and its objective is to minimize the average cumulative regret in CMABs under this feedback model. The CMAB model is assumed to belong to the class of Bernoulli bandits."}, {"title": "UCB versus Thompson sampling.", "content": "Regret minimization algorithms for CMABs with semi-bandit feedback consist of two key sub-routines: an estimation routine and a super-arm selection routine. The estimation routine aims to form reliable estimates of the unknown parameters of the base arms. The super-arm selection routine specifies the sequential selection of the super-arms over time. Super-arm selections rely on the estimates formed by the estimation routine, and there is a wide range of arm-selection rules based on the upper confidence bound (UCB) principle or Thompson sampling (TS) . Recent studies demonstrate that the TS-based approaches are more efficient and empirically outperform the UCB-based counterparts. Specifically, the combinatorial Thompson sampling (CTS) algorithm in adopts a posterior sampling estimator for the bandit mean values, and uses an oracle that perfectly determines the set of super-arms that are optimal for the estimated means. Under such access to an exact oracle, the studies in and establish that the CTS algorithm achieves an order-wise optimal regret of $O(\\frac{m \\log T}{\\Delta})$, where m denotes the number of base arms, T is the horizon, and \u0394 specifies the minimum expected reward gap between an optimal super-arm and any other non-optimal super-arm."}, {"title": "Oracle complexity.", "content": "Accessing an exact oracle is often computationally prohibitive. In this paper, our objective is to alleviate the oracle complexity of existing methods. This is motivated by the fact that black-box function evaluations can be expensive, and hence, it is imperative to minimize the number of black-box queries to the oracle. It is noteworthy that there exist approximate alternatives to the exact oracle, which require a polynomial complexity in the number of base arms. While offering an improvement in complexity, polynomial complexity can still be excessive, and more importantly, approximate solutions can result in linear cumulative regret. Examples of reward functions facing such issues include submodular reward functions [10] and reward functions modeled as the output of a neural network . Therefore, to avoid linear regret, CTS has to inevitably rely on an exact oracle, the computational complexity of which, in general, grows exponentially with the number of base arms."}, {"title": "Group testing.", "content": "Group testing (GT) is an efficient approach for solving large-scale combinatorial search problems. The basic premise in GT is that a small sub-population (size K) of a large body (size m> K) has a certain property (e.g., being defective), and the objective of group testing is to identify them without individually testing all. To avoid individual tests, the population members are pooled into groups, and the group is tested as a whole. The majority of tests are expected to return negative results, i.e., most groups do not have a member with the desired property. This clears the entire group, significantly saving the number of tests administered.\nThe number of tests required for identifying the defective items broadly varies depending on the settings (we refer to [14] for a review). Under both noiseless as well as noisy test outcomes (with a bound on the number of noisy measurements), when $K = O(m^a)$ where $a \\in (0,1/3)$, only $O(K^2 \\log m)$ tests are sufficient to recover the defective subset perfectly (zero-error criterion) [15, 13, 16]. Under a vanishing error criterion, the number of tests can be reduced to $O(K \\log m)$ (partial recovery). Furthermore, GT schemes can be classified into adaptive and non-adaptive methods. In non-adaptive group testing, all the tests are decided at once. In contrast, in adaptive group testing, the tests are divided into stages, and the tests for a particular stage are decided based on the outcomes of the previous stage. Adaptive group testing has been shown to significantly reduce the number of tests, requiring only $O(K\\log m + m)$ tests for exact recovery [19, 20].\nDifferent variants of group testing have also been proposed in the literature. These include threshold group testing , where a test result is positive if the number of defective items in the pool is"}, {"title": "Contributions.", "content": "In this paper, we leverage GT to dispense with the assumption of exact oracle access for the CTS algorithm. This results in an exponential reduction in the oracle complexity without compromising the achievable regret. Specifically, we devise the called Group Testing + Quantized Thompson Sampling (GT+QTS) algorithm, which under a mild probabilistic assumption on the separability of the reward function (Assumption 6), will have exponentially lower complexity compared to an exact oracle. GT+QTS has two key innovations compared to the existing algorithms. First, the complexity reduction is enabled by GT, the success of which fundamentally relies on separability assumptions, lacking which we may face sub-optimal (linear) regret. To address this, as a second contribution, we devise a quantization scheme that ensures the probabilistic separability of the reward function. We show that the GT-based oracle requires only $O(\\log m)$ black-box queries to discern the optimal set of arms in each round. Furthermore, we show that the GT+QTS algorithm preserves the optimal regret order of $O ( \\frac{\\log T}{\\Delta} )$ while providing an exponential reduction in the oracle complexity."}, {"title": "Related works.", "content": "We provide an overview of the most closely related studies to the scope of this paper. The theoretical analysis of the TS-based approaches for MABs was first provided in [31, 32]. These results were later improved in [33] and extended to a general action space and feedback in [34]. The CMAB problem is studied under different settings in . The TS-based approach to CMAB is investigated for top-K CMAB in [35], analyzed for contextual CMAB in [36], and studied under Bayesian regret metric by . Furthermore, CMAB has been investigated in the full-bandit feedback setting in [5].\nThe study closest to the scope of this paper is [7], which analyzes the CTS algorithm to solve combinatorial semi-bandits under a Bernoulli model and a Beta prior distribution for the belief parameters. It establishes that the CTS algorithm asymptotically achieves the optimal regret Another related study is by [9], which presents a tighter regret bound for the Beta prior, and a similar optimal regret analysis is established for multivariate sub-Gaussian outcomes using Gaussian priors."}, {"title": "Combinatorial Bandits", "content": "Setting. Similarly to the canonical models in , we consider a CMAB setting with m arms, and define the set $[m] := \\{1,\\dots,m\\}$. Each arm $i \\in [m]$ is associated with an independent Bernoulli distribution with an unknown mean $\\mu_i$. We denote the vector of unknown mean values by $\\mu := [\\mu_1,\\cdots, \\mu_m]$. Sequentially over time, the learner selects subsets of arms, which we refer to as super-arms. The super-arm selected at time t is denoted by S(t) \u2208 I, where I C 2[m] specifies the set of permissible super-arms. We consider the semi-bandit feedback model, wherein, at each time t, upon pulling a super-arm S(t), the learner observes a feedback\n$Q(t) := \\{X_i(t) : i \\in S(t)\\}$"}, {"title": "Algorithm: GT + Quantized TS", "content": "In this section, we provide the details of the GT+QTS algorithm, the objective of which is minimizing the average cumulative regret defined in (7). This algorithm has two central sub-routines. The first sub-routine is an estimation process that computes estimates for the base arm means. The second sub-routine is a procedure that sequentially, at each round, determines an optimal super-arm to be pulled based on the current base arms mean estimates. These procedures are discussed next."}, {"title": "TS-based Estimator", "content": "We consider a TS-based approach, where the estimates of the mean values are generated by sampling from a posterior distribution. We adopt a beta distribution to generate the posteriors. A beta posterior naturally comes up as the conjugate distribution assuming uniform priors on the mean values of the base arms. We denote the distribution associated with arm $i \\in [m]$ at time t by Beta(ai(t), bi(t)). We initialize, for t = 0, $a_i(0) = b_i(0) = 1$ for all arms, in which case the beta distribution reduces to a uniform distribution. Subsequently, for each time t \u2208 N, a super-arm S(t) is selected, and we receive the feedback Q(t). Based on the feedback, we update the prior distribution of each base arm by updating ai(t) and bi(t). Furthermore, recall that Xi(t) denotes the feedback from the base arm i \u2208 S(t). We draw a sample $Y_i(t) \\sim Bern(X_i(t))$, and update the posterior distribution as follows.\n$a_i(t+1) = a_i(t) + Y_i(t)$,\n$b_i(t+1) = b_i(t) \u2013 Y_i(t) + 1$.\nFinally, our estimate for at time t is a random sample from the beta distribution with parameters specified in (9)-(10), i.e., we generate the posterior estimate \u03b8(t+1) according to $\\theta_i(t+1) \\sim Beta(a_i(t+1), b_i(t+1))$."}, {"title": "GT-based Arm Selection", "content": "We design a GT-based procedure to select the optimal super-arm in each round. The nature of this procedure is probabilistic, and it is designed to find the optimal super-arm with a high probability.\nGT involves pooling together several arms and performing a test on the pooled set. Tests are repeated by selecting and pooling different subsets of arms for each test. When we have l tests, the pooling process can be characterized by a test matrix $A \\in \\{0,1\\}^{l \\times m}$, where row $j\\in [l]$ specifies the arms that are included in test j. Specifically, Aj,i = 1 if the arm i \u2208 [m] is contained in test $j\\in [l]$ test, and otherwise Aj,i = 0. For each test $j\\in [l]$, we design a function $p_j : 2^{[m]} \\times [0,1]^m \\rightarrow [0, M]$, that assigns score to the outcome of test j. Next, based on these test scores, we assign a grade to each arm that specifies whether the arm is likely to be in the optimal super-arm or not. This grade assignment is formalized by a decoding mechanism specified by the function $\\phi_i : [0, M]^l \\rightarrow R$, which generates the arms' grades. Subsequently, a candidate super-arm is selected as the set of arms with the top K grades.\nGroup-testing oracle (GTO). Next, we describe our GT encoding and decoding mechanisms. To lay context, we first describe a na\u00efve adaptation of the GT approach in [28]. It was designed for ranking, and can be used to find the optimal super-arm at each step. We then describe a shortcoming of this na\u00efve approach and modify it to replace the exact oracle used by CTS."}, {"title": "Main Results: Efficiency and Regret", "content": "In this section, we present the performance guarantees of the proposed GT+QTS algorithm. Specifically, we investigate two key performance metrics of the algorithm: (1) the efficiency of the GTO measured in terms of the number of reward evaluations required in each step, and (2) the average cumulative regret incurred by the GT+QTS algorithm. We show that the GT+QTS algorithm achieves the same order-wise regret guarantee as the combinatorial Thompson sampling using an exact oracle [7, 9], while exponentially reducing the number of reward function evaluations. We begin with the results on the efficiency of the GTO.\nEfficiency. A na\u00efve approach to finding the optimal super-arm in each round is to evaluate the functional value at every subset in I at the current estimate of 0(t) at time t. However, this approach requires an exponential number of reward evaluations. An exact oracle may not require an exponential number of evaluations, owing to the separability in Assumption 6. We will first describe a baseline approach, called Oracle+, that provides an exact solution leveraging Assumption 6, with the reward function evaluations scaling linearly with respect to the number of base arms. Subsequently, we will show that the GTO described in Section 3 finds the optimal arm with a high probability using only O(logm) function evaluations, exponentially reducing the complexity compared to the baseline approach.\nOracle+: The baseline approach is a direct consequence of Assumption 6. Since the separability assumption is valid for any subset S, for any parameter \u03b8\u2208 [0,1]m we may set S = (\u00d8. By this choice, for any s \u2208 S*(0) and s & S*(0), Assumption 6 implies that\n$r(s; \\theta) > r(\\check{s} ; \\theta)$.\nOracle+ makes m reward evaluations, each test comprising of a single base arm. It then selects the top K arms with the largest reward values. As a consequence of (16), we immediately conclude that this set of base arms selected by Oracle+ is indeed the optimal super-arm S* (0). Therefore, Oracle+ requires m (linear) reward function evaluations. Next, we analyze the number of tests required by the GTO.\nGTO: For any separable function, the number of tests required by the GTO is of the order O(log m), where the constants depend on the quantization level \u2206, the set Inr(t), as well as the test matrix parameter p. For characterizing the number of tests required by GTO, let us denote the probability of the set of non-repeated test scores by\n$q(t) := P(\\mathcal{I}_{nr}(t))$.\nThe following lemma formalizes the number of tests the GTO requires to compute the optimal super-arm at each time te N.\nLemma 1 For any \u03b4\u2208 (0,1),\n$\\ell = \\frac{8 M^2 B^2}{\\Delta^2 p^4 (1 - p)^2 q^2(t)} \\log \\left(\\frac{K (m - K)}{\\delta}\\right)$"}, {"title": "Experiments", "content": "In this section, we provide empirical results to assess the performance of GT+QTS and compare it against the state-of-the-art CTS algorithm provided in [7] equipped with Oracle+ described in Section 4 as the exact oracle. In the first experiment, we consider the case of linear rewards. Subsequently, we consider the mean reward to be the output of a 2-layer artificial neural network.\nLinear rewards. In this experiment, for any set S \u2208 I and any \u03b8 \u2208 [0,1]m, we define the reward function as $r(S ; \\theta) = \\sum_{i \\in S} \\theta_i$. We set m = 5000 arms, and the mean vector \u03bc is sampled uniformly randomly from [0,1]5000. Furthermore, the cardinality constraint is set to K = 5 arms. For this experiment, we choose \u03bc such that Amin (\u03bc) is at least 0.25, and we set A = 0.25. Consequently, the group testing oracle requires approximately 302 reward evaluations (order-wise), versus the exact oracle, which requires 5000 reward"}, {"title": "Conclusions", "content": "We investigate the problem of regret-minimization in combinatorial semi-bandits. Existing approaches assume the existence of an exact oracle, which may not always be computationally viable. To circumvent this issue, we establish a novel connection between group testing and combinatorial bandits. We propose a new"}, {"title": "Proof of Lemma 1", "content": "At any instant t \u2208 N, choose an optimal arm s \u2208 S*(0(t)) and a sub-optimal arm \u0161 \u2209 S*(0(t)). For accurate prediction, the arm grade $ \\phi_s(t)$ for arm s should be more than $ \\phi_{\\check{s}}(t)$ assigned to arm \u0161. Let us denote the ith column of any matrix A by A.,i. Finding the difference between the arm grades, we have\n$\\phi_s(t) \u2212 \\phi_{\\check{s}}(t) = (A_{:,s}, p(t)) \u2013 (A_{:,\\check{s}}, p(t))$\n$= \\sum_{j=1}^{\\ell} (A_{j,s} - A_{j,\\check{s}}) p_j(t)$.\nFurthermore, we have\n$E[Z_j(t)] = E [(A_{j,s} \u2013 A_{j,\\check{s}})p_j(t)]$\n$= E [(A_{j,s} - A_{j,\\check{s}}) \\times Q(r(A_j ; \\theta(t)))]$\n$= \\sum_{S \\subset [m]} (1\\{s \\in S\\} \u2013 1\\{\\check{s} \\in S\\}) \\times Q(r(S ; \\theta(t))) \\times P(S \\in A)$\n$=\\sum_{S \\subset [m]:s \\in S, \\check{s} \\in S} \\{1\\{s \\in S\\} \u2013 1\\{\\check{s} \\in S\\} \\} \\times Q(r(S ; \\theta(t))) \\times P(S \\in A)$\n$+ \\sum_{S \\subset [m]:s \\in S, \\check{s} \\notin S} \\{1\\{s \\in S\\} \u2013 1\\{\\check{s} \\in S\\} \\} \\times Q(r(S ; \\theta(t))) \\times P(S \\in A)$\n$+ \\sum_{S \\subset [m]:s \\notin S, \\check{s} \\in S} \\{1\\{s \\in S\\} \u2013 1\\{\\check{s} \\in S\\} \\} \\times Q(r(S ; \\theta(t))) \\times P(S \\in A)$\n$+ \\sum_{S \\subset [m]:s \\notin S, \\check{s} \\notin S} \\{1\\{s \\in S\\} \u2013 1\\{\\check{s} \\in S\\} \\} \\times Q(r(S ; \\theta(t))) \\times P(S \\in A)$\n$= \\sum_{S \\subset [m] \\setminus \\{s,\\check{s}\\}} Q(r(S \\cup \\{s\\} ; \\theta(t))) \\times P(S \\cup \\{s\\} \\in A)$\n$- \\sum_{S \\subset [m] \\setminus \\{s,\\check{s}\\}} Q(r(S \\cup \\{\\check{s}\\} ; \\theta(t))) \\times P(S \\cup \\{\\check{s}\\} \\in A)$\n$= p(1-p) \\sum_{S \\subset [m] \\setminus \\{s,\\check{s}\\}} (Q(r(S \\cup \\{s\\} ; \\theta(t))) - Q(r(S \\cup \\{\\check{s}\\} ; \\theta(t))))$\n$\\times p^{|S|}(1-p)^{m-|S|-2}$.\nNext, let us recall the definitions of the set of repeated tests Inr(t). Accordingly, we have\n$E[Z_j(t)] = p(1-p) \\sum_{S \\subset [m] \\setminus \\{s,\\check{s}\\}} (Q(r(S \\cup \\{s\\}; \\theta(t))) - Q(r(S \\cup \\{\\check{s}\\} ; \\theta(t))))$\n$\\times p^{|S|}(1-p)^{m-|S|-2}$\n$= p(1-p) \\sum_{S \\subset [m] \\setminus \\{s,\\check{s}\\} : S \\cup \\{s\\} \\in \\mathcal{I}_{nr}(t)} (Q(r(S \\cup \\{s\\}; \\theta(t))) - Q(r(S \\cup \\{\\check{s}\\} ; \\theta(t))))$\n$\\times p^{|S|}(1-p)^{m-|S|-2}$"}, {"title": "Proof of Lemma 2", "content": "First, we will show that with a high probability, we have TQ(\u03bc) \u2229 T(\u03bc) \u2260 0. Note that\n$P(T_Q(\\mu) \\cap T(\\mu) = \\emptyset)$\n$= P(\\exists S \\in T(\\mu), \\exists S' \\in T_Q(\\mu) : S \\notin T_Q(\\mu) \\text{ and } S' \\notin T(\\mu))$\n$\\leq P(\\exists S \\in T(\\mu), \\exists S' \\in T_Q(\\mu) : r(S ; \\mu) - r(S' ; \\mu) \\geq \\Delta_{\\min}(\\mu))$\n$= P(\\exists S \\in T(\\mu), \\exists S' \\in T_Q(\\mu) :$"}, {"title": "Artificial Neural Network (ANN)", "content": "In this section, we show that a 2-layer ANN with sigmoid activation satisfies the separability condition in Assumption (6), with some conditions on the weights. Specifically, consider a 2-layer ANN with the hidden layer weight matrix denoted by W\u2081 and the output weights denoted by the vector w2, i.e., for any input Os \u2208 [0,1]m, the output of the neural network is given by\n$r(S; \\theta) := \\langle w_2, \\sigma(W_1 \\theta_S) \\rangle$,\nwhere $\\sigma(x) := \\frac{1}{1+e^{-x}}$ denotes the sigmoid activation function. The result is formally defined next."}]}