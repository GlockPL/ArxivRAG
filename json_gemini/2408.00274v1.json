{"title": "QUITO: Accelerating Long-Context Reasoning\nthrough Query-Guided Context Compression", "authors": ["Wenshan Wang", "Yihang Wang", "Yixing Fan", "Huaming Liao", "Jiafeng Guo"], "abstract": "In-context learning (ICL) capabilities are foundational to the\nsuccess of large language models (LLMs). Recently, context compres-\nsion has attracted growing interest since it can largely reduce reasoning\ncomplexities and computation costs of LLMs. In this paper, we intro-\nduce a novel Query-gUIded aTtention compression (QUITO) method,\nwhich leverages attention of the question over the contexts to filter\nuseless information. Specifically, we take a trigger token to calculate\nthe attention distribution of the context in response to the question.\nBased on the distribution, we propose three different filtering meth-\nods to satisfy the budget constraints of the context length. We evalu-\nate the QUITO using two widely-used datasets, namely, NaturalQues-\ntions and ASQA. Experimental results demonstrate that QUITO sig-\nnificantly outperforms established baselines across various datasets and\ndownstream LLMs, underscoring its effectiveness. Our code is available\nat https://github.com/Wenshansilvia/attention_compressor.", "sections": [{"title": "Introduction", "content": "In recent years, LLMs has demonstrated notable reasoning and generating ca-\npabilities, significantly enhancing the performance of natural language process-\ning (NLP) tasks [4]. However, these models still exhibit limitations in acquiring\nreal-time information and integrating external knowledge [8]. In-context learn-\ning (ICL) addresses these deficiencies by including examples and relevant con-\ntexts directly within the prompts[6]. This approach boost the performance of\nLLMs in downstream tasks without requiring additional training.\nTo better improve the reasoning ability of LLMs, researchers propose dif-\nferent ways to incorporate complex contexts in the input [4,8]. For example,\nretrieval-augmented generation (RAG) employs an additional searcher to re-\ntrieve external relevant documents about the question as the context of inputs,\nwhich has attracted lots of attention for both the academia and industry [2,3,8]."}, {"title": "", "content": "In addition, Brown et al. [4] found that the number of examples has a great\nimpact to the reasoning performance of LLMs, where more examples tend to\nbring better performances [17]. Moreover, the chain-of-thought (CoT) [24,21]\nfurther improves the LLMs by involving the reasoning step of each example in\nthe context. While these strategies have the potential to significantly improve\nthe capabilities of LLMs, they also introduce challenges associated with the in-\ncreased context length, such as higher inference complexity and costs.\nTo mitigate this issue, context compression in ICL is becoming a prominent\nsolution. On one hand, reducing the length by removing noise from contexts can\nimprove inference efficiency[11,26]. On the other hand, it meets the input length\nrestrictions of open-source LLMs[20,27] while also reduces the costs associated\nwith accessing proprietary LLMs. Several methods [11,10] have been proposed\nto compress context by estimating the information entropy. This assessment is\nconducted by utilizing a small external LLM to evaluate the perplexity of indi-\nvidual tokens to identify those that contribute minimal information gain. Tokens\nthat demonstrate low information are subsequently compressed or eliminated.\nHowever, neglecting the query during compression may result in the inadvertent\ndeletion of key information.\nFor the above problem, recent methods such as LongLLMLingua [9] adopt a\nquery-aware compression approach by calculating the perplexity of the context\nconditioned on the query. Despite this advancement, misalignment between com-\npression model and generation model can lead to inconsistencies in determining\nwhich tokens are considered to have \"low entropy gain\". This discrepancy arises\nbecause models may differ in their interpretation and processing of the same\ninformation. Our work also scores tokens based on their relevance to the query.\nHowever, distinctively, we employ attention metrics rather than perplexity to\nassess the importance of tokens.\nThis paper introduces the Query-gUIded aTtention compression (QUITO)\nmethod, which strategically selects the context to maintain supporting informa-\ntion by utilizing the attention mechanism. Intuitively, the attention mechanism\noffers a direct method for analyzing the interactions between the question and\nthe context, moving beyond the sole reliance on models' probabilistic uncer-\ntainty. This technique facilitates a more precise identification of the information\nthat is most crucial to the current task. More importantly, the attention-based\nfiltering can be implemented with small LLMs, which improves the computation\nefficiency.\nThe main contributions of this study include:\n1. This paper proposes a novel context compression method, named QUITO.\nIt utilises self-attention mechanism of Transformers to score the importance\nof tokens, selecting context relevant to the current query.\n2. In contrast to earlier methods that requires a compression model with 7\nbillion or 13 billion parameters, this method achieves superior results using\na smaller LLM with only 0.5 billion parameters."}, {"title": "Related Work", "content": "In this section, we briefly review two lines of related works, i.e., context com-\npression task and attention mechanism."}, {"title": "Context Compression Task", "content": "To reduce the length of context, earlier efforts[13] opted to summarize and con-\ndense retrieved documents using models such as GPT. Other studies [1,25,23,15]\nfocused on distinguishing between useful and redundant information within doc-\numents, training a model to extract the most valuable sentences. For example,\nLeanContext [1] and FILCO [23] train the model to perform sentence-level ex-\ntraction for the context. Fit-RAG [15] scores sub-paragraphs with sliding context\nwindows. RECOMP [25] uses a generative model to rewrite extracted candidate\nsentences, thereby ensuring the coherence and naturalness of the summaries.\nApproaches that generate summaries do not allow direct control over the\ncompression ratio, resulting in a growing attention on token and word-level com-\npression techniques in recent times. SelectiveContext [11] utilizes self-information\nwithin context for token selection. This approach considers perplexity (PPL) to\nbe the representation of the uncertainty of an LLM regarding information car-\nried by contexts. Based on [11], LLMLingua [10] introduces a two-stage, coarse\nto a fine, compression method. However, these methods fail to consider the re-\nlationship between the context and the query. LLMLingua [9] further addresses\nthis gap by calculating context-specific perplexity conditioned on the query.\nThe aforementioned token-level compression methods utilize perplexity as the\nprimary filtering criterion. However, discrepancies often arise between smaller\ncompression models and larger generation models in their assessments of word\nperplexity, making it challenging to align their judgments on lexical importance."}, {"title": "Attention Mechanism", "content": "Attention is a significant breakthrough in deep learning, particularly shines in\nNLP tasks such as translation and summary generation[5]. The core concept\nbehind Attention mechanisms involves assigning a specific weight to each input\nelement, such as words or tokens, indicating their relevance to the task at hand.\nThis allows models to focus selectively on more pertinent parts of the input data.\nSelf-attention, a particular category of the attention mechanism, measures\nthe relationships between all input elements, assessing how each element influ-\nences and relates to the others[16]. Multi-head attention is a key component of\nthe Transformers [22], which improves the model's capability in capturing di-\nverse correlation patterns. Recent studies try to use the attention mechanisms"}, {"title": "Method", "content": "In this section, we introduce the QUITO method in detail. As illustrated in\nFigure 1, QUITO primarily consists of two main components, namely the query-\nguided self-attention component and the context budget control component. In\nwhat follows, we will firstly give a formal definition of the task, and then describe\neach component in detail."}, {"title": "Problem Formulation", "content": "Given an input p = (s, C, q), where s is the instruction, q is the query, and C =\n{Ci}n\ni=1 is the context consisting n documents. Every document ci = {wi,j}Li\nj=1\ncontains Li word. The objective of context compression task can be formulated\nas:\nminCdist(P(\u1ef9|s, \u010c, q), P(y|s, C, q)),\\tag{1}\nwhere y represents the predicted response of the LLM, and y is the ground\ntruth response. dist(\u00b7,\u00b7) is a function that measures the distance of two distri-\nbutions, such as KL divergence. \u010c = {\u0109i}n\ni=1 is the compressed context, and\n\u0109i = {wj|wj \u2208 Ci}L\nj=1, \u03c4 \u2208 [0, 1]. \u0109i is ci being compressed with ratio at 1/\u03c4,\nwhere smaller \u03c4 means higher compression ratio."}, {"title": "Query-Guided Self-Attention", "content": "The query-guided self-attention component aims to estimate the importance of\ntokens in context by calculating the trigger attention distribution. Firstly, we\norganize all input by prompt template filling. Then, we calculate the importance\nof all the input with trigger attention distribution. Finally, we obtain the lexical\nunits importance within the context by context attention reformulating."}, {"title": "Prompt Template Filling", "content": "It is crucial that the compression model fully under-\nstands the task at hand and accurately identifies the information most pertinent\nto the current query. A standard approach involves concatenating the context\nwith the query and subsequently analyzing how tokens within the query attend\nto tokens in the context. However, in a Transformer decoder-only architecture,\nthe visibility range of each token in the query varies. This variability suggests\nthat tokens positioned later in the sequence more precisely reflect the model's\ncomprehensive understanding of the task. Given the challenges associated with\nappropriately weighting tokens at different positions, we propose a novel method\nthat utilize a conversational template and identify a specialized token that en-\ncapsulates the compression model's overall understanding of the task."}, {"title": "Trigger attention distribution", "content": "We embed the context and query into a con-\nversational template, concluding with a signal that prompts the model to initiate\nresponse generation. The terminal token within this sequence is designated as a\ntrigger token, serving as an indicator of the model's assessment of information\nneed after comprehensively understanding the task at hand. Subsequently, we\nemploy a compression model equipped with a multi-head self-attention mech-\nanism to process the completed template and compute the attention that the\ntrigger token accords to the preceding text:\n{ai =\nexp(qiTki)\n\u2211Ltotalj=1 exp(qjTkj)},\\tag{2}\nwhere qi and ki are query embedding and key embedding of the ith token,\nrespectively. Ltotal is the total number of tokens in the completed template."}, {"title": "Context attention reformulating", "content": "Once the attention allocated by the trigger\ntoken to all preceding tokens in the sequence has been determined, the subse-\nquent step involves transforming this attention data into a quantified measure\nof significance for the lexical units within the context.\nThe array {ai} signifies attention weights, with its length equating to the\naggregate of the lengths of the conversational template, the context, and query.\nWithin the scope of this task, it is imperative to concentrate on the attention\ndistributed to the context segment. The attention should not be diluted by the\nsegments pertaining to the template and the query. Consequently, we implement\na normalization process, which is designed to ensure that the distribution of"}, {"title": "Context Budget Control", "content": "In the previous section, we have derived a list of words, represented as {wi}L\ni=1,\nand the corresponding array of attention weights, {\u03b1i}L\ni=1. This section intro-\nduces the filtering methods that satisfy the requirement of the context budget\ncontrol."}, {"title": "Phrase Level Filtering", "content": "In the process of selecting based on attention scores,\nit is common to inadvertently overlook words adjacent to those with high atten-\ntion, referred to as target words, which may also contain crucial knowledge for\nanswering the query. To rectify this oversight and ensure these adjacent words\nare also considered, we apply a weighted adjustment, allowing them to receive a\nportion of the attention attributed to the target words. This is accomplished by\nimplementing a Gaussian filter across the word attention array {\u03b1i}L\ni=1\nG(x) =\n1\n\u221a\n2\u03c0\u03c32\nexp(\u2212x2/2\u03c32).\\tag{5}\nAfter the application of the Gaussian function G(x) to {\u03b1i}L\ni=1, the resulting\nGaussian-modulated attention array is denoted as {\u03b1\u0303i}L\ni=1.\nSubsequently, we identify the words from set {wi}L\ni=1 that rank within the\ntop TL based on their attention scores {\u03b1\u0303i}L\ni=1.\n1. Perform a sort on {\u03b1\u0303i}L\ni=1 on descending order, which yields an ordered set\nof indices {j1, j2, ..., jTL}."}, {"title": "", "content": "2. Select corresponding words from {wi}L\ni=1 with index {j1, j2, ..., jTL}, which\nyields {w\u0303i}TL\ni=1.\n3. Reorganize the set of selected words {w\u0303i}TL\ni=1 to reflect their original sequen-\ntial order within the context.\nAlthough the selection process targets individual words, the application of\nGaussian filtering often leads to the selection of contiguous words, thereby effec-\ntively forming phrases."}, {"title": "Sentence Level Filtering", "content": "In addition to phrase-level filtering, sentence-level\nfiltering is also implemented to preserve more comprehensive semantic informa-\ntion. Using the Natural Language Toolkit (NLTK) toolkit, we extract semantic\nunits at the sentence level. Each sentence si, denoted as si = {wj}lj=k+1, is\nassigned an attention score based on the maximum score of the tokens it con-\ntains. Subsequently, mirroring the phrase-level filtering process, we prioritize\nincorporating sentences with higher attention scores into the selection set, while\nensuring that the aggregate word count remains below TL."}, {"title": "Dynamic Sentence Level Filtering", "content": "Sentence-level filtering often leads to a\ncompression ratio greater than the designated target 1/\u03c4. To more effectively\nadhere to the predetermined compression rate and optimize budget utilization,\nwe augment the results of sentence-level filtering with word-level filtering. Specif-\nically, subsequent to sentence-level filtering, if the count of words is L\u2032, we are\nthen able to select an additional TL \u2212 L\u2032 words. These additional words are\nchosen via phrase-level filtering from the text that was not previously selected.\nThe newly selected words are subsequently concatenated with the results from\nsentence-level filtering to form the final compressed output."}, {"title": "Experiments", "content": ""}, {"title": "Datasets and Evaluation Metrics", "content": "In this paper, we assess the efficacy of the proposed QUITO method across two\ndistinct scenarios: open domain question answering and long-form question an-\nswering. Specifically, we employ the NaturalQuestions (NQ) and ASQA datasets\nas the testbed.\nFor NQ dataset, We employed a processed version as described in [14], where\neach query is paired with 20 documents, among which only one document con-\ntains the correct answer. In alignment with the procedures specified in [14],\naccuracy was used as the metric to determine whether the generated responses\naccurately included the correct answer. For the ASQA dataset, the answer to\nthe question maybe multi-facet as there are many ambiguous questions. Each\nambiguous question in the ASQA dataset has answers reflecting multiple inter-\npretations of these ambiguities. We utilize the dataset version provided by [7],\nwhich includes 5 retrieved documents/snippets from Wikipedia for each query.\nIn accordance with [18], our evaluation metrics included Exact Match (EM), a\nROBERTa-based QA score (DisambigF1), and ROUGE [12]."}, {"title": "Baselines and Implementation", "content": "Baselines We take three state-of-the-art compression approaches as baselines:\nFor query-unaware methods, we select Selective-Context[11] and LLMLingua[10],\nwhich implements cross entropy scoring to remove redundant vocabulary. For\nquery-aware method, we compare our approach with Longllmlingua[9]. LongLLM-\nLingua implements a two-stage compression method. It first evaluates and reranks\nmultiple retrieved contexts, followed by a token-level compression stage, allocat-\ning varying compression budgets to these contexts based on their initial scores.\nFor fair comparison, we excluded the context reranking phrase of LongLLMLin-\ngua (marked as LongLLMLinguat in Table 1 and Figure 2), concentrating on\nthe token-level compression."}, {"title": "Detailed Implementation", "content": "For fair comparison, we follow LLMLingua [10] to\nuse Longchat-13B-16k 1 as the generation model. To ensure the reproducibil-\nity of the results, we apply greedy decoding strategy throughout the inference\nprocess, with the temperature parameter set to zero. The compression model is\nimplemented with Qwen2-0.5B-Instruct2."}, {"title": "Main Results", "content": "Table 1 presents the comparative performance of our method, QUITO, against\nthree baseline methods across various compression rates and datasets. Firstly, we\ncan see that selective-context is a strong baseline compared with LLMLingua and\nLongLLMLingua\u2020 on both 2x and 4x compression rates. Secondly, QUITO ob-\ntains significantly better performances than all baselines, e.g., the improvement\nof QUITO with phrase level filtering against selective-context, LLMLingua, and\nLongLLMLingua\u2020(i.e., 2x compression ratio) on NQ is 5.7, 20.2, and 17.7, respec-\ntively. Finally, we find that QUITO with different filtering method all achieve\nbetter performances on both datasets. However, there is no consistent advan-\ntages of each filtering method when compared on different datasets. This maybe\nthat the context length on NQ and ASQA differs significantly, i.e., the average\nlength of context on NQ and ASQA is about 2904 and 721 tokens, respectively.\nAll the results demonstrate the effectiveness of QUITO in compressing contexts\nfor the LLMs."}, {"title": "Analysis on different position of the ground truth context", "content": "We analyse the performance of the QUITO compression method across different\nground truth context positions within the NQ dataset. This dataset comprises\n20 context document fragments per query, of which only one contains the answer\nand is designated as the ground truth document. We assessed the impact of this\ndocument's positioning at the 1st, 5th, 10th, 15th, and 20th ranks on the efficacy\nof various compression strategies.\nThe results presented in Figure 2 indicate that all context compression meth-\nods struggle with the 'lost in the middle' phenomenon, as described by [14].\nPerformance is optimal when the ground truth context is positioned at the be-\nginning; however, it deteriorates significantly when the ground truth context is\nplaced in the middle. Among the evaluated methods, LLMLingua[10] exhibits"}, {"title": "Analysis on different generation models", "content": "To better understand the generation ability of different LLMs, we evaluate the\nperformance of 4 widely-used models, including Longchat-13B-16k 3, Llama3-\n8b-Instruct 4, GLM4-9b-chat 5, and Mistral-7b-instruct 6. These models were\ntested with contexts compressed at a rate of 2 on the NQ dataset. The generated\nresponses from these compressed contexts were then compared with those derived\nfrom uncompressed contexts.\nAs depicted in Figure 3, the Mistral-7B-Instruct model significantly out-\nperforms the other three generation models despite having fewer parameters.\nThis superior performance may be attributed to the incorporation of Grouped-\nQuery Attention (GQA) and Sliding Window Attention (SWA) during its train-\ning phase, which enhances its capability to process long sequence inputs. While\nthe context is compressed at 2x ratio, we find that the GLM4-9b-chat model\nshow the smallest performance decline, with a decrease of 8.9, and the Mistral-\n7B-Instruct has the greatest decline. When the compression ratio is 4x, we can\nsee that all generation models obtain a relative close performance except for\nLongChat-13B-16k. These maybe that the LongChat-13B-16k is released earlier\nthan other three models, and the latter are trained more deeply."}, {"title": "Conclusion", "content": "This paper introduces the QUITO method, a novel attention-based importance\nestimation for long context compression in LLMs. The QUITO method employs\na trigger token that comprehensively considers the query to assess the impor-\ntance of each lexical unit within the context, thereby filtering out units with low\nrelevance scores. Evaluations conducted on the NQ and ASQA datasets demon-\nstrate that our method outperforms state-of-the-art compression methods such\nas Selective Context, LLMLingua, and LongLLMLingua, confirming its superior\nability to preserve essential information needed by LLMs to respond to queries\neffectively. For future work, we would like to study the combination of the con-\ntext compression and re-ranking module, since the re-ranking stage in RAG also\ntargets on selecting useful information for final answer generation."}]}