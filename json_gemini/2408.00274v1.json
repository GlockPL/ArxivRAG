{"title": "QUITO: Accelerating Long-Context Reasoning through Query-Guided Context Compression", "authors": ["Wenshan Wang", "Yihang Wang", "Yixing Fan", "Huaming Liao", "Jiafeng Guo"], "abstract": "In-context learning (ICL) capabilities are foundational to the success of large language models (LLMs). Recently, context compression has attracted growing interest since it can largely reduce reasoning complexities and computation costs of LLMs. In this paper, we introduce a novel Query-gUIded aTtention compression (QUITO) method, which leverages attention of the question over the contexts to filter useless information. Specifically, we take a trigger token to calculate the attention distribution of the context in response to the question. Based on the distribution, we propose three different filtering methods to satisfy the budget constraints of the context length. We evaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and ASQA. Experimental results demonstrate that QUITO significantly outperforms established baselines across various datasets and downstream LLMs, underscoring its effectiveness. Our code is available at https://github.com/Wenshansilvia/attention_compressor.", "sections": [{"title": "Introduction", "content": "In recent years, LLMs has demonstrated notable reasoning and generating capabilities, significantly enhancing the performance of natural language processing (NLP) tasks [4]. However, these models still exhibit limitations in acquiring real-time information and integrating external knowledge [8]. In-context learning (ICL) addresses these deficiencies by including examples and relevant contexts directly within the prompts[6]. This approach boost the performance of LLMs in downstream tasks without requiring additional training.\nTo better improve the reasoning ability of LLMs, researchers propose different ways to incorporate complex contexts in the input [4,8]. For example, retrieval-augmented generation (RAG) employs an additional searcher to retrieve external relevant documents about the question as the context of inputs, which has attracted lots of attention for both the academia and industry [2,3,8]."}, {"title": "Related Work", "content": "In this section, we briefly review two lines of related works, i.e., context compression task and attention mechanism."}, {"title": "Context Compression Task", "content": "To reduce the length of context, earlier efforts[13] opted to summarize and condense retrieved documents using models such as GPT. Other studies [1,25,23,15] focused on distinguishing between useful and redundant information within documents, training a model to extract the most valuable sentences. For example,\nLeanContext [1] and FILCO [23] train the model to perform sentence-level extraction for the context. Fit-RAG [15] scores sub-paragraphs with sliding context windows. RECOMP [25] uses a generative model to rewrite extracted candidate sentences, thereby ensuring the coherence and naturalness of the summaries.\nApproaches that generate summaries do not allow direct control over the compression ratio, resulting in a growing attention on token and word-level compression techniques in recent times. SelectiveContext [11] utilizes self-information within context for token selection. This approach considers perplexity (PPL) to be the representation of the uncertainty of an LLM regarding information carried by contexts. Based on [11], LLMLingua [10] introduces a two-stage, coarse to a fine, compression method. However, these methods fail to consider the relationship between the context and the query. LLMLingua [9] further addresses this gap by calculating context-specific perplexity conditioned on the query.\nThe aforementioned token-level compression methods utilize perplexity as the primary filtering criterion. However, discrepancies often arise between smaller compression models and larger generation models in their assessments of word perplexity, making it challenging to align their judgments on lexical importance."}, {"title": "Attention Mechanism", "content": "Attention is a significant breakthrough in deep learning, particularly shines in NLP tasks such as translation and summary generation[5]. The core concept behind Attention mechanisms involves assigning a specific weight to each input element, such as words or tokens, indicating their relevance to the task at hand. This allows models to focus selectively on more pertinent parts of the input data.\nSelf-attention, a particular category of the attention mechanism, measures the relationships between all input elements, assessing how each element influences and relates to the others[16]. Multi-head attention is a key component of the Transformers [22], which improves the model's capability in capturing diverse correlation patterns. Recent studies try to use the attention mechanisms"}, {"title": "Method", "content": "In this section, we introduce the QUITO method in detail. As illustrated in Figure 1, QUITO primarily consists of two main components, namely the query-guided self-attention component and the context budget control component. In what follows, we will firstly give a formal definition of the task, and then describe each component in detail."}, {"title": "Problem Formulation", "content": "Given an input $p = (s, C, q)$, where s is the instruction, q is the query, and $C = \\{C_i\\}_{i=1}^n$ is the context consisting n documents. Every document $c_i = \\{W_{i,j}\\}_{j=1}^{L_i}$ contains $L_i$ word. The objective of context compression task can be formulated as:\n$\\min_{\\tilde{C}} dist(P(\\tilde{y}|s, \\tilde{C}, q), P(y|s, C, q))$,\nwhere y represents the predicted response of the LLM, and $\\tilde{y}$ is the ground truth response. dist(,) is a function that measures the distance of two distributions, such as KL divergence. $\\tilde{C} = \\{\\tilde{C_i}\\}_{i=1}^n$ is the compressed context, and $\\tilde{C_i} = \\{w_j|w_j \\in C_i\\}_{j=1}^{\\tilde{L_i}}, \\tau \\in [0,1]$. $\\tilde{c_i}$ is $c_i$ being compressed with ratio at $1/\\tau$, where smaller $\\tau$ means higher compression ratio."}, {"title": "Query-Guided Self-Attention", "content": "The query-guided self-attention component aims to estimate the importance of tokens in context by calculating the trigger attention distribution. Firstly, we organize all input by prompt template filling. Then, we calculate the importance of all the input with trigger attention distribution. Finally, we obtain the lexical units importance within the context by context attention reformulating."}, {"title": "Prompt Template Filling", "content": "It is crucial that the compression model fully understands the task at hand and accurately identifies the information most pertinent to the current query. A standard approach involves concatenating the context with the query and subsequently analyzing how tokens within the query attend to tokens in the context. However, in a Transformer decoder-only architecture, the visibility range of each token in the query varies. This variability suggests that tokens positioned later in the sequence more precisely reflect the model's comprehensive understanding of the task. Given the challenges associated with appropriately weighting tokens at different positions, we propose a novel method that utilize a conversational template and identify a specialized token that encapsulates the compression model's overall understanding of the task."}, {"title": "Trigger attention distribution", "content": "We embed the context and query into a conversational template, concluding with a signal that prompts the model to initiate response generation. The terminal token within this sequence is designated as a trigger token, serving as an indicator of the model's assessment of information need after comprehensively understanding the task at hand. Subsequently, we employ a compression model equipped with a multi-head self-attention mechanism to process the completed template and compute the attention that the trigger token accords to the preceding text:\n$\\left\\{a_i | a_i = \\frac{\\exp (q_{\\text {total }} k_i)}{\\sum_{j=1}^{\\text {total }} \\exp (q_{\\text {total }} k_j))}\\right\\}$,\nwhere $q_i$ and $k_i$ are query embedding and key embedding of the i^{th} token, respectively. $L_{total}$ is the total number of tokens in the completed template."}, {"title": "Context attention reformulating", "content": "Once the attention allocated by the trigger token to all preceding tokens in the sequence has been determined, the subsequent step involves transforming this attention data into a quantified measure of significance for the lexical units within the context.\nThe array $\\{a_i\\}$ signifies attention weights, with its length equating to the aggregate of the lengths of the conversational template, the context, and query. Within the scope of this task, it is imperative to concentrate on the attention distributed to the context segment. The attention should not be diluted by the segments pertaining to the template and the query. Consequently, we implement a normalization process, which is designed to ensure that the distribution of"}, {"title": "Context Budget Control", "content": "In the previous section, we have derived a list of words, represented as $\\{w_i\\}_{i=1}^L$, and the corresponding array of attention weights, $\\{\\alpha_i'\\}_{i=1}^L$. This section introduces the filtering methods that satisfy the requirement of the context budget control."}, {"title": "Phrase Level Filtering", "content": "In the process of selecting based on attention scores, it is common to inadvertently overlook words adjacent to those with high attention, referred to as target words, which may also contain crucial knowledge for answering the query. To rectify this oversight and ensure these adjacent words are also considered, we apply a weighted adjustment, allowing them to receive a portion of the attention attributed to the target words. This is accomplished by implementing a Gaussian filter across the word attention array $\\{\\alpha_i'\\}_{i=1}^L$\n$G(x) = \\frac{1}{2 \\sqrt{\\pi \\sigma^2}} \\exp(\\frac{-x^2}{2\\sigma^2})$,\nAfter the application of the Gaussian function G(x) to $\\{\\alpha_i'\\}_{i=1}^L$, the resulting Gaussian-modulated attention array is denoted as $\\{\\alpha_i''\\}_{i=1}^L$.\nSubsequently, we identify the words from set $\\{w_i\\}_{i=1}^L$ that rank within the top $T_L$ based on their attention scores $\\{\\alpha_i''\\}_{i=1}^L$.\n1. Perform a sort on $\\{\\alpha_i''\\}_{i=1}^L$ on descending order, which yields an ordered set of indices $\\{j_1, j_2, ..., j_L\\}$.\n2. Select corresponding words from $\\{w_i\\}_{i=1}^L$ with index $\\{j_1, j_2, ..., j_{T_L}\\}$, which yields $\\{w^*_i\\}_{i=1}^{T_L}$.\n3. Reorganize the set of selected words $\\{w^*_i\\}_{i=1}^{T_L}$ to reflect their original sequential order within the context.\nAlthough the selection process targets individual words, the application of Gaussian filtering often leads to the selection of contiguous words, thereby effectively forming phrases."}, {"title": "Sentence Level Filtering", "content": "In addition to phrase-level filtering, sentence-level filtering is also implemented to preserve more comprehensive semantic information. Using the Natural Language Toolkit (NLTK) toolkit, we extract semantic units at the sentence level. Each sentence $s_i$, denoted as $s_i = \\{w_j\\}_{j=k+1}^{k+l}$, is assigned an attention score based on the maximum score of the tokens it contains. Subsequently, mirroring the phrase-level filtering process, we prioritize incorporating sentences with higher attention scores into the selection set, while ensuring that the aggregate word count remains below TL."}, {"title": "Dynamic Sentence Level Filtering", "content": "Sentence-level filtering often leads to a compression ratio greater than the designated target $1/\\tau$. To more effectively adhere to the predetermined compression rate and optimize budget utilization, we augment the results of sentence-level filtering with word-level filtering. Specifically, subsequent to sentence-level filtering, if the count of words is L', we are then able to select an additional $T_L - L'$ words. These additional words are chosen via phrase-level filtering from the text that was not previously selected. The newly selected words are subsequently concatenated with the results from sentence-level filtering to form the final compressed output."}, {"title": "Experiments", "content": "In this paper, we assess the efficacy of the proposed QUITO method across two distinct scenarios: open domain question answering and long-form question answering. Specifically, we employ the NaturalQuestions (NQ) and ASQA datasets as the testbed.\nFor NQ dataset, We employed a processed version as described in [14], where each query is paired with 20 documents, among which only one document contains the correct answer. In alignment with the procedures specified in [14], accuracy was used as the metric to determine whether the generated responses accurately included the correct answer. For the ASQA dataset, the answer to the question maybe multi-facet as there are many ambiguous questions. Each ambiguous question in the ASQA dataset has answers reflecting multiple interpretations of these ambiguities. We utilize the dataset version provided by [7], which includes 5 retrieved documents/snippets from Wikipedia for each query. In accordance with [18], our evaluation metrics included Exact Match (EM), a ROBERTa-based QA score (DisambigF1), and ROUGE [12]."}, {"title": "Baselines and Implementation", "content": "Baselines We take three state-of-the-art compression approaches as baselines:\nFor query-unaware methods, we select Selective-Context[11] and LLMLingua[10], which implements cross entropy scoring to remove redundant vocabulary. For query-aware method, we compare our approach with Longllmlingua[9]. LongLLM-Lingua implements a two-stage compression method. It first evaluates and reranks multiple retrieved contexts, followed by a token-level compression stage, allocating varying compression budgets to these contexts based on their initial scores. For fair comparison, we excluded the context reranking phrase of LongLLMLingua (marked as LongLLMLinguat in Table 1 and Figure 2), concentrating on the token-level compression."}, {"title": "Detailed Implementation", "content": "For fair comparison, we follow LLMLingua [10] to use Longchat-13B-16k 1 as the generation model. To ensure the reproducibility of the results, we apply greedy decoding strategy throughout the inference process, with the temperature parameter set to zero. The compression model is implemented with Qwen2-0.5B-Instruct2."}, {"title": "Main Results", "content": "Table 1 presents the comparative performance of our method, QUITO, against three baseline methods across various compression rates and datasets. Firstly, we can see that selective-context is a strong baseline compared with LLMLingua and LongLLMLingua\u2020 on both 2x and 4x compression rates. Secondly, QUITO obtains significantly better performances than all baselines, e.g., the improvement of QUITO with phrase level filtering against selective-context, LLMLingua, and LongLLMLingua\u2020(i.e., 2x compression ratio) on NQ is 5.7, 20.2, and 17.7, respectively. Finally, we find that QUITO with different filtering method all achieve better performances on both datasets. However, there is no consistent advantages of each filtering method when compared on different datasets. This maybe that the context length on NQ and ASQA differs significantly, i.e., the average length of context on NQ and ASQA is about 2904 and 721 tokens, respectively. All the results demonstrate the effectiveness of QUITO in compressing contexts for the LLMs."}, {"title": "Analysis on different position of the ground truth context", "content": "We analyse the performance of the QUITO compression method across different ground truth context positions within the NQ dataset. This dataset comprises 20 context document fragments per query, of which only one contains the answer and is designated as the ground truth document. We assessed the impact of this document's positioning at the 1st, 5th, 10th, 15th, and 20th ranks on the efficacy of various compression strategies.\nThe results presented in Figure 2 indicate that all context compression methods struggle with the 'lost in the middle' phenomenon, as described by [14]. Performance is optimal when the ground truth context is positioned at the beginning; however, it deteriorates significantly when the ground truth context is placed in the middle. Among the evaluated methods, LLMLingua[10] exhibits"}, {"title": "Analysis on different generation models", "content": "To better understand the generation ability of different LLMs, we evaluate the performance of 4 widely-used models, including Longchat-13B-16k 3, Llama3-8b-Instruct 4, GLM4-9b-chat 5, and Mistral-7b-instruct 6. These models were tested with contexts compressed at a rate of 2 on the NQ dataset. The generated responses from these compressed contexts were then compared with those derived from uncompressed contexts.\nAs depicted in Figure 3, the Mistral-7B-Instruct model significantly outperforms the other three generation models despite having fewer parameters. This superior performance may be attributed to the incorporation of Grouped-Query Attention (GQA) and Sliding Window Attention (SWA) during its training phase, which enhances its capability to process long sequence inputs. While the context is compressed at 2x ratio, we find that the GLM4-9b-chat model show the smallest performance decline, with a decrease of 8.9, and the Mistral-7B-Instruct has the greatest decline. When the compression ratio is 4x, we can see that all generation models obtain a relative close performance except for LongChat-13B-16k. These maybe that the LongChat-13B-16k is released earlier than other three models, and the latter are trained more deeply."}, {"title": "Conclusion", "content": "This paper introduces the QUITO method, a novel attention-based importance estimation for long context compression in LLMs. The QUITO method employs a trigger token that comprehensively considers the query to assess the importance of each lexical unit within the context, thereby filtering out units with low relevance scores. Evaluations conducted on the NQ and ASQA datasets demonstrate that our method outperforms state-of-the-art compression methods such as Selective Context, LLMLingua, and LongLLMLingua, confirming its superior ability to preserve essential information needed by LLMs to respond to queries effectively. For future work, we would like to study the combination of the context compression and re-ranking module, since the re-ranking stage in RAG also targets on selecting useful information for final answer generation."}]}