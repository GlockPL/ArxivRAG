{"title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data", "authors": ["Rewina Bedemariam", "Natalie Perez", "Sreyoshi Bhaduri", "Satya Kapoor", "Alex Gil", "Elizabeth Conjar", "Ikkei Itoku", "David Theil", "Aman Chadha", "Naumaan Nayyar"], "abstract": "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful Al systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.", "sections": [{"title": "1. Introduction", "content": "The rapid evolution of Large Language Models (LLMs) has expanded their potential uses, from generating content to assessing it. As organizations increasingly adopt these models, there is a growing need to evaluate the accuracy and alignment of LLM-generated outputs with human perspectives (Long et al., 2024). The concept of using LLMs as evaluative judges' dates back to efforts in natural language processing to improve evaluation metrics such as BLEU or ROUGE, which primarily measure word overlap (Wang et al., 2024). However, these traditional evaluation metrics often fall short when it comes to accurately assessing the nuances of natural language generation tasks (Liu et al., 2023).\nAs LLMs increasingly power the analysis of open-ended textual data in organizational settings, ensuring the fairness and accuracy of their outputs becomes crucial. In artificial intelligence, model alignment refers to techniques designed to align LLM behaviors with human values and expectations (Shen et al., 2023). This involves methods like fine-tuning, human feedback, and reinforcement learning to ensure outputs reflect human-like reasoning and decision-making processes (Liu et al., 2024). Model alignment is critical, especially when models like Anthropic Claude are used as judge models in behavioral research (Shen et al., 2023).\nOur research makes a significant contribution by investigating the effectiveness of using LLMs as judges to evaluate the thematic alignment of summaries generated by other LLMs, specifically in the context of organizations using open-text survey responses. Our study is important because it addresses a critical gap in the responsible deployment of AI systems for decision-making processes that directly impact organizational decision-making. We employed an Anthropic Claude model to generate thematic summaries from open-ended survey responses and utilized Amazon's Titan Express and Nova Pro LLMs and Llama as judges to evaluate these summaries. By comparing the LLM-as-judge approach with human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, we present a scalable alternative to traditional human-centric evaluation methods.\nThis research is particularly relevant to the AI in Talent Management research community, as it explores the potential for Al systems to serve as impartial arbiters of content accuracy and representation, while also highlighting the ethical considerations and potential biases inherent in such approaches. Our findings demonstrate that LLM judges can produce results comparable to human raters, offering organizations a more efficient means of validating AI-generated insights. However, we also critically examine the limitations of this approach and provide recommendations for future research to ensure fairness, accountability, and transparency in the use of LLMs for organizational decision-making. This work"}, {"title": "2. Background", "content": "\"LLM-as-a-judge\" has emerged as an innovative solution, wherein an LLM evaluates another model's output to approximate human labeling (Zheng et al., 2023). This approach offers the promise of automating human judgment at scale, while maintaining high levels of reliability and consistency. For example, Yan (2024) demonstrated that, when properly calibrated, LLMs can reach agreement rates close to those of human annotators, which can be costly and time-intensive. From this lens, the utilization of LLMs can significantly reduce analysis time and turn around rapid results. By comparing an LLM's evaluation of generated content to human evaluations, researchers aim to refine LLM capabilities and ensure alignment with human interpretations (Rao et al., 2024; Wang et al., 2024).This approach could simplify the evaluation process by reducing reliance on human resources, enabling faster iterations in development cycles. Therefore, understanding how well LLMs can replicate and adhere to human judgment is crucial for their effective deployment in organizational settings (Wang et al., 2024).\nOpen-Text Survey Data\nOpen-ended survey data is a type of textual data where respondents provide unstructured text-based responses. This type of data is useful for obtaining authentic and sometimes unexpected information that illuminates \u201cwhy\u201d or \u201chow\u201d respondents think, feel, or behave (Rouder et al., 2021). Unlike scaled or categorical survey items, open-ended questions encourage respondents to provide information about their experiences, perceptions, thoughts, values, and/or feelings that researchers might not anticipate or share information about topics that might be sensitive or personal in nature (Allen, 2017). Open-text data can be particularly valuable in telling nuanced stories and highlighting diverse responses (Rouder et al., 2021).\nThematic Summaries\nLLMs can be used for a variety of textual analysis, including topic modeling (Bhaduri et al., 2024; Kapoor et al., 2024) and summarizing large amounts of unstructured text-based data (Tang et al., 2023). The objective of textual summarization is to communicate the primary meaning of an original dataset into a simplified and straightforward form without sacrificing the integrity of the information being conveyed (Dutta et al., 2022). Scholars in the LLM field have explored textual summarization as a valuable use-case for LLMs (Tang et al., 2023). Yet research has revealed factual inconsistencies in using LLMs to generate textual summaries, with a range of errors or biases (Tang et al., 2023; Chan et al., 2023; Ashwin et al., 2023).\nLLMs and Classification Evaluation Strategy\nThere are a number of LLM evaluation strategies available, and classification prompts are one way to classify text into predefined categories (Rao et al., 2024). In particular, classification prompts allow researchers to classify content into categories, inductively or deductively (Rao et al., 2024). Historically, text classification has been conducted using traditional ML approaches that are often complex and resource-intensive; tasks include extraction, dimensionality reduction, classifier selection, and model evaluation, not including pre-processing steps (Wang et al., 2024). However, LLM advancements have made text classification easier with only three required elements: 1) data collection, 2) feeding LLM data, and 3) obtaining classification results from an LLM (Wang et al., 2024). Since LLMs have been pre-trained on diverse datasets, researchers have found that they require little, if any, additional training specific to a task or domain area (Wang et al., 2024).\nGiven recent LLM advancements and prompt engineering, LLMs offer a cheaper alternative to classify thematic summaries based on unstructured open-text survey data. This ability has the potential to help researchers not only analyze textual data, but also classify textual generation into distinct categories (Zhao et al., 2023). In particular, researchers can use classification prompts and evaluation to assess the accuracy, also known as correctness, of LLM classifications, by measuring the proportion of accurately classified thematic summaries (Rao et al., 2024). That said, the absence of robust methodological strategies to evaluate open-text thematic summaries reveals a gap in the research field addressing this novel use-case. This study aims to evaluate how well classification prompts can accurately rate thematic summary content alignment."}, {"title": "3. Methods", "content": "The process of implementing the LLM-as-a-judge methodology involves taking the text output from one AI model and feeding it back into another LLM. The second model, now serving as the judge, evaluates the text based on an evaluation prompt provided by the user. The second LLM then returns a score, label, or descriptive judgment depending on the specific evaluation criteria set by the user. This allows for a high degree of customization, as users can instruct the LLM to assess specific properties, which makes the approach adaptable to various applications. This framework also means that LLM-as-a-judge is not an evaluation metric in the traditional sense, like accuracy or precision (Yan, 2024). Rather, it serves as a general technique for approximating human judgment, where the LLM relies on its training to assess qualities like \u201cfaithfulness to source\" or \"correctness.\u201d This way, LLMs act as proxy evaluators, following detailed prompts much like a human evaluator would. While this technique does not produce a fixed measure, it offers a flexible proxy metric that can align with specific use cases.\n3.1. Dataset\nWe obtained access to a novel, non-open source dataset for this analysis. The benefit of testing across a new dataset, not previously used for model pre-training, is that the researchers can determine how well LLMs analyze and generate predictions (outputs) accurately. The data used in this study was collected using a census survey that addressed work-related topics and was launched across a multi-global population who reported working full-time. The survey contained one free-text question that asked the respondents to share one thing about work they want senior leaders to be aware of. Over 13,000 comments were collected. The researchers pre-processed the responses by removing short samples, noise (e.g., text with only symbols such as periods or dashes and no other content), and personally identifiable information (PII). The 13K dataset was further grouped into 70 smaller datasets by segmenting the data into groups based on business lines, so insights could be shared with business lines and the datasets would not be corrupted with comments from individuals that did not report to certain business lines.\nNext, a previously engineered thematic summary prompt was used to analyze the dataset and create LLM-generated thematic summaries. The thematic summaries were produced based on a validated prompt that leverages thematic quality principles, as outlined by Naeem et al., (2023), including JSON formatting instructions focused on identifying a theme name (must contain at least one topic and sentiment details), thematic summary (must contain words that reflect genuine experiences or perceptions of respondents, must contain words that are rich in meaning and provide detailed understanding of the topic or focus area, must describe the relationship of theme to related topics that aid to contribute to new insight), and raw verbatim comments (must include verbatim words expressed by respondents). The data was run through different LLM API calls. A total of 70 thematic summaries containing three themes per summary were generated (i.e., each theme contained a theme name, theme description with 3-4 sentences, and one representative verbatim comment), based on the 70 input datasets. The thematic summaries provide aggregated findings across the top-three most salient and prevalent themes within each of the respective input datasets. This study only used the 70 LLM-generated thematic summaries to evaluate how well the thematic summary content was aligned across theme name, theme description, and representative comments to reflect the theme description and theme name.\n3.2. Overview of the Evaluation\nOur general methodology used a Claude model to generate thematic summary outputs from survey comments. For evaluations, this study utilized a three-stage methodology to evaluate the thematic alignment of LLM-generated summaries, which involved human evaluators, and several LLM models including Anthropic Claude (v2.1), Claude Sonnet (v3.5) Amazon Titan Express and Nova Pro, and Llama evaluations. Each model was tasked with assessing alignment across three dimensions: theme name, description, and representative quote.\nStep 1. Human Evaluation as Baseline. Human evaluators were first tasked with reviewing the thematic summaries based on alignment among the theme name, description, and representative verbatim comment/quote generated by a Claude model. The human ratings served as the baseline for comparison against LLM assessments. To evaluate the prompt's ability to accurately evaluate content alignment, human evaluators used the same content alignment criteria as the LLM-as-a-judge model (see scale below).\nStep 2. LLM Evaluation: Claude as the Initial Evaluator. The Claude model was provided with the same summaries and instructed to assign alignment scores based on a structured evaluation prompt. The prompt specifically asked Claude to rate the thematic coherence across each theme's name, description, and quote. The prompt engineered for this study to classify thematic summaries into predefined categories was based on content alignment using the following rating scale: 1 to 3, with 1"}, {"title": "4. Results", "content": "Our study sought to answer two research questions: 1) To what extent can LLMs replicate human judgment in evaluating thematic alignment, and what factors contribute to discrepancies between LLM and human ratings? and 2) What are the implications of higher inter-model agreement compared to human-model agreement for the development and application of LLMs in content analysis and theme evaluation tasks?\nWe examined the alignment between ratings provided by a LLM and human evaluators for a set of 70 thematic summaries. The inter-rater agreement between human evaluators and LLMs demonstrated consistency across various LLM architectures. When comparing the Cohen's kappa results between the human ratings and the models, Sonnet 3.5 had the highest rate of"}, {"title": "5. Discussion", "content": "The findings confirm that while LLMs such as Claude, Titan, Nova, and Llama can demonstrate high levels of agreement with each other, they face challenges in fully replicating human judgment. In instances of partial alignment, both the Claude, Titan, and Llama models occasionally rated alignment higher than the human evaluators, suggesting a potential for overestimating alignment. The Claude LLM displayed a tendency to overestimate alignment, which may stem from a lack of nuanced understanding of thematic details, particularly in cases where quotes added new dimensions or diverge slightly from the theme description.\nFurthermore, sophisticated prompts and evaluation criteria that capture the depth of alignment nuances more fully could also improve both the LLM generation and evaluation models. Therefore, human oversight remains essential. Specifically, integrating additional evaluation criteria that capture thematic nuances and content salience could improve alignment with human interpretations. Continuous improvement of the evaluation framework and prompt design is necessary to ensure that LLM outputs remain aligned with organizational needs. This includes developing more comprehensive assessment methods that account for the contextual and qualitative aspects of thematic understanding, beyond just the surface-level alignment."}, {"title": "6. Recommendations", "content": "To address the observed discrepancies, researchers should consider incorporating additional evaluation metrics beyond content alignment. Possible improvements include investigating and mitigating biases in the evaluation of LLMs, which is crucial for refining the assessment process and ensuring reliable outcomes (Bhaduri et al., 2024). Position bias, which favors options presented earlier, and verbosity bias, which can lead to an overvaluation of longer responses, are among the key challenges that need to be addressed (Saito et al., 2023). Other potential biases include recency bias, confirmation bias, and anchoring bias. Mitigating these biases requires a multifaceted approach, encompassing careful design of evaluation protocols, randomization of response order, utilization of diverse evaluators, and development of objective metrics (Saito et al., 2023). Defining comprehensive success metrics for LLMs necessitates interdisciplinary contributions from various fields. Computer science and AI can develop specific capability benchmarks and quantify properties like coherence and factual accuracy. Linguistics can assess grammatical correctness and pragmatic aspects of communication. Psychology can design experiments to measure human preferences and evaluate cognitive load. Philosophy can explore ethical considerations and refine definitions of key concepts. Domain experts can assess task-specific performance, while sociology and anthropology can examine societal implications and cultural sensitivities. Human-computer interaction can focus on user experience, and statistics can develop robust methodologies for data analysis. The integration of these diverse perspectives into cohesive evaluation frameworks represents an ongoing challenge in the rapidly evolving field of LLM development and assessment."}, {"title": "7. Conclusion", "content": "This study contributes to a deeper understanding of how well LLMs align with human judgments in thematic analysis. While the percentage agreement and Cohen's Kappa results indicate fair agreement, the findings point to the need for ongoing adjustments and improvements to the LLM's evaluation prompt. The analysis reveals areas for improvement, particularly in instances where the LLM overestimated the degree of alignment compared to human raters. This tendency may stem from the LLM's limitations in fully comprehending the nuanced details and contextual factors that contribute to human interpretations of thematic content. The discrepancies highlight the continued need for human oversight and the refinement of evaluation frameworks to better capture the qualitative aspects of thematic understanding.\nFuture research should explore strategies to further enhance the LLM's alignment with human judgments. This may involve developing more sophisticated prompts and evaluation criteria that account for thematic salience, repetition of quotes, and other contextual factors. Additionally, fine-tuning the LLM to better identify and handle personal identifiable information (PII) and theme recurrence could improve the reliability and trustworthiness of the generated insights. As researchers continue to leverage the power of LLMs in analyzing open-ended survey data, ongoing collaboration between human experts and machine learning systems will be crucial. By iteratively improving the evaluation methods and incorporating human feedback, researchers can unlock the full potential of LLMs to generate high-quality thematic summaries that reliably reflect the perceptions and experiences of survey respondents."}]}