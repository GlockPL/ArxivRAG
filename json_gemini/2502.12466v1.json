{"title": "EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking", "authors": ["Anjiang Wei", "Jiannan Cao", "Ran Li", "Hongyu Chen", "Yuhui Zhang", "Ziheng Wang", "Yaofeng Sun", "Yuan Liu", "Thiago S. F. X. Teixeira", "Diyi Yang", "Ke Wang", "Alex Aiken"], "abstract": "Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI 03-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities.", "sections": [{"title": "1 Introduction", "content": "Programming has emerged as a key application domain for large language models (LLMs), enabling tasks such as program synthesis (Chen et al., 2021; Austin et al., 2021; Jain et al., 2024), test generation (Yang et al., 2024a), bug detection (Yang et al., 2023), program repair (Xia et al., 2023), and code optimization (Shypula et al., 2023). Recently, there has been growing interest in evaluating how well LLMs can reason about the semantics of code (Ni et al., 2024; Liu et al., 2023; Gu et al., 2024; Chen et al., 2024a; Liu et al., 2024b), i.e., predicting program properties without running the program.\nThis paper introduces the task of equivalence checking as a new way to evaluate the code reasoning capabilities of LLMs. A classic challenge in programming languages and verification, equivalence checking involves determining whether two programs produce identical outputs for all possible inputs.\nCompared to prior code reasoning tasks, evaluating LLMs using equivalence checking offers distinct advantages. Most notably, it presents a significantly more challenging benchmark than previous tasks, enabling a more rigorous assessment of LLMs' code reasoning capabilities. Equivalence checking requires LLMs to reason over all possible inputs, while prior work often focuses on a single input, such as output prediction, input prediction (Gu et al., 2024), input-specific program state prediction and execution simulation (Liu et al., 2023; Chen et al., 2024a; Ding et al., 2024; La Malfa et al., 2024; Ni et al., 2024).\nMoveover, equivalence checking underpins a broad range of downstream applications, including software refactoring (Pailoor et al., 2024), software testing (Tian et al., 2024), and program optimization (Shypula et al., 2021), surpassing the scope of prior reasoning tasks. By requiring a deep understanding of program semantics and reasoning over all possible inputs, equivalence en-ables the analysis of an expressive range of program behaviors, even including many undecidable problems. Therefore, LLMs that perform well on equivalence checking are likely to be well-suited for tackling more complex programming tasks.\nOur proposal requires a benchmark consisting of both equivalent and inequivalent program pairs covering different aspects of equivalence reasoning with varying degrees of difficulty. A large benchmark is essential, making it desirable to automate the benchmark generation process. Existing methods (Badihi et al., 2021; Maveli et al., 2024) mostly rely on local syntactic changes such as operand swaps (e.g., changing a < b to b > a for equivalent pairs or b <= a for inequivalent pairs), which do not require deep semantic reasoning. However, these approaches are insufficient for benchmarking the equivalence reasoning capabilities of state-of-the-art LLMs. As many existing benchmarks have become saturated (Phan et al., 2025), a more challenging dataset is needed to rigorously assess LLMs' semantic reasoning abilities.\nIn this work, we introduce EquiBench, a new dataset of 2400 program pairs for equivalence reasoning. EquiBench spans four programming languages-Python, C, CUDA, and x86-64 assembly-providing a systematic benchmark to evaluate LLMs' code reasoning abilities.\nThe key technical challenge is to automatically generate (in)equivalent program pairs that demand deep semantic reasoning beyond simple syntactic variations. We propose several techniques to achieve this. First, to confirm that basic syntactic variations are well within the reasoning capabilities of state-of-the-art LLMs, we construct an equivalence category based on variable renaming, which barely requires semantic reasoning. Next, we generate equivalent programs by removing dead code, leveraging program analysis to go beyond trivial syntactic changes. By incorporating alias analysis and path feasibility analysis, we increase the difficulty of semantic reasoning in an automated manner. For GPU programs written in CUDA, we generate equivalent pairs by exploring different compiler scheduling strategies, such as loop tiling and shared memory caching, which involve structural transformations that extend far beyond statement-level modifications. We also use superoptimization to explore optimal instruction sequences beyond standard compiler optimizations, enabling more aggressive code restructuring. Finally, we include pairs with different algorithmic choices using submissions from online programming platforms."}, {"title": "2 Related Work", "content": "LLM Reasoning Extensive research has evaluated LLMs' reasoning capabilities across diverse tasks (Cobbe et al., 2021; Huang and Chang, 2022; Bubeck et al., 2023; Mirzadeh et al., 2024; Zhou et al., 2022; Ho et al., 2022; Wei et al., 2022; Chen et al., 2024b; Clark et al., 2018; Zhang et al., 2024). In the context of code reasoning, i.e., predicting a program's execution behavior without running it, CRUXEval (Gu et al., 2024) focuses on input-output prediction, while CodeMind (Liu et al., 2024b) extends evaluation to natural language specifications. Another line of work seeks to improve LLMs' code simulation abilities through prompting (La Malfa et al., 2024) or targeted training (Liu et al., 2023; Ni et al., 2024; Ding et al., 2024). Unlike prior work that evaluates LLMs on predicting program behavior for a specific input, our new code reasoning task and benchmark for equivalence checking assesses LLMs' ability to reason about all possible inputs.\nEquivalence Checking Equivalence checking underpins applications such as performance optimization (Shypula et al., 2023; Cummins et al., 2023, 2024), code transpilation (Lu et al., 2021; Yang et al., 2024b; Ibrahimzada et al., 2024; Pan et al., 2024), refactoring (Pailoor et al., 2024), and testing (Felsing et al., 2014; Tian et al., 2024). Due to its undecidable nature, no algorithm can decide program equivalence for all program pairs while always terminating. Existing techniques (Sharma et al., 2013; Dahiya and Bansal, 2017; Gupta et al., 2018; Mora et al., 2018; Churchill et al., 2019; Badihi et al., 2020) focus on specific domains, such as SQL query equivalence (Zhao et al., 2023; Ding et al., 2023; Singh and Bedathur, 2024). EQBENCH (Badihi et al., 2021) and SeqCoBench (Maveli et al., 2024) are the main datasets for equivalence checking but have limitations. EQBENCH is too small (272 pairs) for LLM evaluation, while SeqCoBench relies only on statement-level syntactic changes (e.g., renaming variables). In contrast, our work introduces a broader set of equivalence categories and structural transformations, creating a more systematic and challenging benchmark for assessing LLMs' semantic reasoning capabilities."}, {"title": "3 Benchmark Construction", "content": "While we have so far discussed only the standard notion of equivalence (that two programs produce the same output on any input), there are other, more precise definitions of equivalence used for each category in the benchmark. For each category, we provide the definition of equivalence, which is included in the prompt when testing LLM reasoning capabilities. We describe the process of generating (in)equivalent pairs for the following six categories:\n\u2022 DCE: C program pairs generated via the compiler's dead code elimination (DCE) pass (Section 3.1).\n\u2022 CUDA: CUDA program pairs created by applying different scheduling strategies using a tensor compiler (Section 3.2).\n\u2022 x86-64: x86-64 assembly program pairs generated by a superoptimizer (Section 3.3).\n\u2022 OJ_A, OJ_V, OJ_VA: Python program pairs from online judge submissions, featuring algorithmic differences (OJ_A), variable-renaming transformations (OJ_V), and combinations of both (OJ_VA) (Section 3.4)."}, {"title": "3.1 Pairs from Program Analysis (DCE)", "content": "Dead code elimination (DCE), a compiler pass, removes useless program statements. After DCE, remaining statements in the modified program naturally correspond to those in the original program.\nDefinition of Equivalence. Two programs are considered equivalent if, when executed on the same input, they always have identical program states at all corresponding points reachable by program execution. We expect language models to identify differences between the two programs, align their states, and determine whether these states are consistently identical."}, {"title": "3.2 Pairs from Compiler Scheduling (CUDA)", "content": "Definition of Equivalence. Two CUDA programs are considered equivalent if they produce the same mathematical output for any valid input, disregarding floating-point rounding errors. This definition differs from that in Section 3.1, as it does not require the internal program states to be identical during execution.\nAutomation. The program transformation can be automated with tensor compilers, which provide a set of schedules to optimize loop-based programs. These schedules include loop tiling, loop fusion, loop reordering, loop unrolling, vectorization, and cache optimization. For any given schedule, the compiler can generate the transformed code. While different schedules can significantly impact program performance on the GPU, they do not affect the program's correctness (assuming no compiler bugs), providing the foundation for automation."}, {"title": "3.3 Pairs from a Superoptimizer (x86-64)", "content": "Definition of Equivalence. Two x86-64 assembly programs are considered equivalent if, for any input provided in the specified input registers, both programs produce identical outputs in the specified output registers. Differences in other registers or memory are ignored for equivalence checking.\nAutomation. A superoptimizer searches a space of programs to find one equivalent to the target. Test cases efficiently prune incorrect candidates, while formal verification guarantees the correctness of the optimized program. Superoptimizers apply aggressive and non-local transformations, making semantic equivalence reasoning more challenging. For example, in Figure 4, while a traditional compiler translates the loop in the source C program into a loop in assembly, a superoptimizer can find a more optimal instruction sequence by leveraging specialized hardware instructions. Such semantic equivalence is beyond the scope of traditional compilers.\nDataset Generation. We use Stoke (Schkufza et al., 2013) to generate program pairs. Assembly programs are sampled from prior work (Koenig et al., 2021), and Stoke applies transformations to produce candidate programs. If verification succeeds, the pair is labeled as equivalent; if the generated test cases fail, it is labeled as inequivalent."}, {"title": "3.4 Pairs from Programming Contests", "content": "Definition of Equivalence. Two programs are considered equivalent if they solve the same problem by producing the same output for any valid input, as defined by the problem description. Both programs, along with the problem description, are provided to determine equivalence.\nDataset Generation. We sample Python submissions using a publicly available dataset from Online Judge (OJ) (Puri et al., 2021). For OJ_A pairs, accepted submissions are treated as equivalent, while pairs consisting of an accepted submission and a wrong-answer submission are considered inequivalent. Variable renaming transformations are automated with an open-source tool (Flook, 2025)."}, {"title": "4 Experimental Setup", "content": "EquiBench. Our dataset, EquiBench, consists of 2,400 program pairs across six equivalence categories. Each category contains 200 equivalent and 200 inequivalent pairs. As the dataset generation pipeline is fully automated, additional pairs can be generated as needed.\nResearch Questions. We investigate: 1) how different models perform on equivalence checking (Section 5.1); 2) whether prompting techniques, such as few-shot learning (Brown et al., 2020) and Chain-of-Thought (Wei et al., 2022), can enhance performance (Section 5.2); and 3) whether model predictions exhibit bias when judging program equivalence.\nModels. We evaluate 17 large language models. For open-source models, including Mixtral (Jiang et al., 2024), Llama (Touvron et al., 2023), Qwen (Bai et al., 2023), DeepSeek (Liu et al., 2024a), we use Together AI, a model serving framework. For closed-source models (e.g., GPT-4 (Achiam et al., 2023), Claude-3.5 (Anthropic, 2024)), we access them via their official APIs, using the default temperature setting.\nPrompts. The 0-shot evaluation is conducted using the prompt \u201cYou are here to judge if two programs are semantically equivalent. Here equivalence means {definition}. [Program 1]: {code1} [Program 2]: {code2} Please only output the answer of whether the two programs are equivalent or not. You should only output Yes or No.\"\nThe definition of equivalence and the corresponding program pairs are provided for each category. Additionally, for the categories of OJ_A, OJ_V and OJ_VA, the prompt also includes the problem description. The full prompts used in our experiments for each equivalence category are in Appendix A.1.\nError Handling. Some models occasionally fail to follow the instruction to \"output Yes or No\". To address this issue, we use GPT-40 to parse model outputs. In cases where no result can be extracted, we randomly assign \u201cYes\u201d or \u201cNo\u201d as the model's output. These errors are very rare in advanced models but occur more frequently in smaller models."}, {"title": "5 Results", "content": "5.1 Model Accuracy\nTable 2 shows the accuracy results for 17 state-of-the-art large language models on EquiBench under zero-shot prompting. Our findings are as follows:\nReasoning models achieve the highest performance, demonstrating a clear advantage over non-reasoning models. As shown in Table 2, reasoning models such as OpenAI 03-mini, DeepSeek R1, and o1-mini significantly outperform all others in our evaluation. This further underscores the complexity of equivalence checking as a code reasoning problem, where reasoning models exhibit a distinct advantage.\nEquiBench is a challenging benchmark. Among the 17 models evaluated, OpenAI 03-mini achieves only 59.0% in the CUDA category despite being the top-performing model overall, with an accuracy of 78.0%. For the two most difficult categories, the highest accuracy across all models is 62.3% and 68.8%, respectively, only modestly above the random baseline of 50% accuracy for binary classification, highlighting the substantial room for improvement.\nPure syntactic changes (OJ_V) are the easiest for LLMs, while structural transformations are key to assessing deep semantic reasoning. As shown in the last row of Table 2, the OJ_V category achieves the highest mean accuracy, with DeepSeek-R1 leading at 91.5%. This is because OJ_V pairs are generated through trivial variable renaming, as seen in prior work (Badihi et al., 2021; Maveli et al., 2024). Additionally, combining variable renaming with algorithmic equivalence has little impact on difficulty, as indicated by the small drop in mean accuracy from OJ_A 67.3% to OJ_VA 67.0%. In contrast, all other categories involve non-local structural transformations, making them more challenging and essential for evaluating LLMs' deep semantic reasoning.\nScaling up models improves performance. Larger models generally achieve better performance. Figure 6 shows scaling trends for the Qwen2.5, Llama-3.1, and Mixtral families, where accuracy improves with model size. The x-axis is on a logarithmic scale, highlighting how models exhibit consistent gains as parameters increase."}, {"title": "5.2 Prompting Strategies Analysis", "content": "We study few-shot in-context learning and Chain-of-Thought (CoT) prompting, evaluating four strategies: 0-shot, 4-shot, 0-shot with CoT, and 4-shot with CoT. For 4-shot, prompts include 2 equivalent and 2 inequivalent pairs. Our key finding is that prompting strategies barely improve performance on EquiBench, highlighting the task's difficulty and need for deeper reasoning. Few-shot prompting provides only minor improvements over 0-shot, while Chain-of-Thought shows slight benefits for o1-mini but marginally reduces performance for other models, underscoring the task's complexity and the need for more advanced, task-specific approaches."}, {"title": "5.3 Bias in Model Prediction", "content": "We evaluate the prediction bias of the models and observe a pronounced tendency to misclassify equivalent programs as inequivalent in the CUDA and x86-64 categories. The bias in the CUDA category arises from extensive structural transformations, such as loop restructuring and shared memory optimizations, which make paired programs appear substantially different. In the x86-64 category, superoptimization applies non-local transformations to achieve optimal instruction sequences, introducing aggressive code restructuring that complicates equivalence reasoning and leads models to frequently misclassify equivalent pairs as inequivalent."}, {"title": "5.4 Case Studies", "content": "Models lack capabilities for sound equivalence checking. We find that simple changes that lead to semantic differences can confuse the models, causing them to produce incorrect predictions despite their correct predictions on the original program pairs. For example, o3-mini, which is one of the top-performing models in CUDA category, can correctly classifies the pair shown in Figure 3 as equivalent. Next, we introduce synchronization bugs into the right-hand program, creating two inequivalent pairs with the original left-hand program: (1) removing the first __syncthreads(); allows reads before all writes complete, causing race conditions; (2) removing the second __syncthreads(); lets faster threads overwrite shared data while slower threads read it. Despite these semantic differences, o3-mini misclassifies both pairs as equivalent.\nProper hints enable models to correct misjudgments. After o3-mini misclassifies the modified pairs, a hint about removed synchronization primitives allows it to correctly identify both as inequivalent, with accurate explanations highlighting data races. This suggests that training models on dedicated program analysis datasets, beyond only raw source code, may be useful for improving their code reasoning capabilities."}, {"title": "6 Conclusion", "content": "This paper presents EquiBench, a dataset for evaluating the code reasoning capabilities of large language models via program equivalence checking. Spanning four programming languages and six equivalence categories, EquiBench challenges models with diverse (in)equivalent program pairs generated through automated transformations, including syntactic changes, structural modifications, and algorithmic equivalence. Our evaluation shows that the best-performing model, OpenAI 03-mini, achieves only 59.0% in the CUDA category and 78.0% overall, with the most challenging categories achieving the best accuracies of just 62.3% and 68.8%, only modestly above the 50% random baseline. Few-shot learning and Chain-of-Thought prompting yield minimal gains, and models exhibit bias toward classifying programs with significant transformations as inequivalent. EquiBench provides a critical benchmark for advancing LLM-based code reasoning."}, {"title": "Limitations", "content": "We make every effort to ensure that all pairs are correctly labeled, but cannot guarantee complete accuracy due to potential bugs in the toolchains or errors in the inputs (e.g., solutions from programming contests may be accepted based on a limited set of test cases that might not fully expose underlying bugs in the accepted solutions)."}]}