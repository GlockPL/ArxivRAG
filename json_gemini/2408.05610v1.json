{"title": "Representation Alignment from Human Feedback for Cross-Embodiment Reward Learning from Mixed-Quality Demonstrations", "authors": ["Connor Mattson", "Anurag Aribandi", "Daniel S. Brown"], "abstract": "We study the problem of cross-embodiment inverse reinforcement learning, where we wish to learn a reward function from video demonstrations in one or more embodiments and then transfer the learned reward to a different embodiment (e.g., different action space, dynamics, size, shape, etc.). Learning reward functions that transfer across embodiments is important in settings such as teaching a robot a policy via human video demonstrations or teaching a robot to imitate a policy from another robot with a different embodiment. However, prior work has only focused on cases where near-optimal demonstrations are available, which is often difficult to ensure. By contrast, we study the setting of cross-embodiment reward learning from mixed-quality demonstrations. We demonstrate that prior work struggles to learn generalizable reward representations when learning from mixed-quality data. We then analyze several techniques that leverage human feedback for representation learning and alignment to enable effective cross-embodiment learning. Our results give insight into how different representation learning techniques lead to qualitatively different reward shaping behaviors and the importance of human feedback when learning from mixed-quality, mixed-embodiment data.", "sections": [{"title": "INTRODUCTION", "content": "Inverse reinforcement learning (IRL) (Arora & Doshi, 2021) seeks to learn a reward function from observed agent behavior. However, the field of imitation learning (Hussein et al., 2017) has developed numerous techniques for direct policy learning from observed agent behavior. So why learn a reward function? From the earliest days of IRL research, Ng et al. (2000) and others have argued that reward functions provide a succinct description of behavior. Indeed, Ng et al. (2000) notes that the field of reinforcement learning (RL) is based on the idea that the reward function is \"the most succinct, robust, transferable definition of a task.\" Thus, if we can learn a reward function from observing an agent in one task, it should be the case that we can use that reward function to help teach the same task to agents with different embodiments (e.g., different action space, dynamics, size and shape, etc.). The idea of allowing agents with different embodiments to learn from each other is typically called cross-embodiment (Zakka et al., 2022) or cross-domain (Niu et al., 2024) learning. In this paper we focus on cross-embodiment IRL, with the goal of learning robust reward functions that can transfer across different embodiments.\nCross-embodiment reward learning would enable robots to learn rewards from watching humans perform tasks and would allow robots and other AI agents to learn by watching other agents. However, given an embodiment mismatch between the demonstrator and the learner, we cannot simply imitate the actions of the demonstrator since the action spaces may be completely different. Furthermore, the actions are typically unavailable when learning only from video observations (Torabi et al., 2019). Learning an embodiment-independent reward function is a compelling solution to the problem of cross-embodiment policy learning as it should allow an agent of any embodiment to learn how to perform the desired task via reinforcement learning. However, prior work has shown that reward functions learned from demonstrations are often entangled with the dynamics, making cross embodiment transfer difficult (Fu et al., 2017).\nRecently, Zakka et al. (2022) developed XIRL, a novel approach for cross-embodiment reward learning from video demonstrations. Using near-optimal demonstrations across several different embodiments they first learn an embodiment-invariant representation using temporal cycle-consistency (Dwibedi et al., 2019). The base assumption this method uses is that there is a temporal similarity between the data it is trained on, i.e., there are similar frames or checkpoints that each video demonstration will share with another. The reward is then formulated as the distance between the current state embedding to that of a goal embedding and this learned reward is optimized via RL to achieve generalization to an unseen embodiment. However, one of the main limitations of this recent breakthrough by Zakka et al. (2022) is that, in order to ensure temporal similarity when performing representation learning, the approach requires near-optimal demonstrations across each embodiment. Prior work has shown that human demonstrations and other interactions with AI systems can be noisy (Chuck et al., 2017; Mandlekar et al., 2022), irrational (Chan et al., 2021; Ghosal et al., 2023), and sometimes adversarial (Wolf et al., 2017; Jagielski et al., 2018; Oravec, 2023). Thus, assuming that demonstration data is near-optimal is unlikely to be true in practice.\nWe study several different approaches for performing cross-embodiment reward learning from mixed-quality demonstrations: (1) Cross-Embodiment Reinforcement Learning from Human Feedback where we learn a reward function end-to-end from preferences over demonstrations from different embodiments in our training dataset and use this learned reward function to perform reinforcement learning (Christiano et al., 2017); (2) Cross-Embodiment Representation Learning from Preferences: We explore techniques that seek to use human preference labels to learn a state representations (Tian et al., 2024) and then formulate the reward as the distance between the learned state embeddings and a goal embedding; and (3) XIRL-Buckets: A method that seeks to apply a temporal cycle-consistency representation learning to mixed-quality, mixed-embodiment data by leveraging high-level knowledge of the relative goodness of demonstrations by first binning the demonstrations into several \"buckets\" or groups based on ordinal labels denoting their goodness and then performing temporal cycle-consistency representation learning within each bucket.\nThe primary contributions of this work are: (1) We propose and formalize the new problem of cross-embodiment reward learning from mixed-quality data; (2) We study a range of algorithmic approaches for this problem setting that build on and combine ideas from representation learning and alignment from human feedback; (3) We empirically study the RL performance, learned rewards and learned representations when learning with mixed data and show that prior approaches fail to perform well in this setting; (4) We provide empirical evidence that approaches that leverage human feedback information about the relative quality of the data are often able to learn transferable representations and corresponding rewards that transfer across embodiments even when learning from mixed-quality demonstrations. However, these methods still fail to achieve the performance of methods that learn only from high-quality demonstrations, showing that there is a need for further research into how to best learn from mixed-quality, mixed embodiment data."}, {"title": "PRIOR WORK", "content": "Imitation Learning from Observation Our work seeks to leverage video observations from one embodiment and learn to transfer these policies to new embodiments. Thus, our work falls within the general area of imitation learning from observation (Torabi et al., 2019). Early work on inverse reinforcement learning learned reward functions based on state observations of demonstrations (Abbeel & Ng, 2004; Ziebart et al., 2008). Other work proposed imitation learning from state observations"}, {"title": "PROBLEM FORMULATION", "content": "Our problem setting is inspired by Zakka et al. (2022), who consider cross-embodiment IRL. However, in contrast to Zakka et al. (2022), we seek to learn from mixed-quality, mixed-embodiment data. We investigate the problem of learning an agent-agnostic representation of a task, T, given a dataset of videos depicting agents performing the task. Formally, we define a dataset D as a collection of state-only video demonstrations D = {vo, v1, ..., vi}, where each video contains a sequence of frames (2D images), v\u1d62 = {v\u1d62\u00b9, v\u1d62\u00b2, , v\u1d62\u1d38\u1d62} that depict the agent executing the task. In contrast to prior work (Zakka et al., 2022), we consider a set of mixed-quality demonstrations, where it is no longer guaranteed that agents will reach the goal state at the end of every demonstration. We refer to the demonstration set as mixed-embodiment if it contains demonstrations from more than one agent embodiment performing the same task.\nProblem Statement: Given a task, T, and a mixed-quality, mixed-embodiment (MQME) demonstration dataset, D, can we learn an embodiment-agnostic approximation, f, of the ground truth"}, {"title": "PRELIMINARIES", "content": "In this section we describe in detail the XIRL algorithm, an unsupervised method of learning embodiment-agnostic representations of tasks proposed by Zakka et al. (2022). XIRL assumes access to a video dataset, D = {vo, v1, ..., vi}, of successful video demonstrations, v\u1d62, for a task. Each video demonstration is an observation-only video (demonstrator actions are unobserved) containing a sequence of video frames, v\u1d62 = {v\u1d62\u00b9,v\u1d62\u00b2,\u2026\u2026, v\u1d62\u1d38\u1d62}. XIRL assumes that D contains video demonstrations from multiple agent embodiments, all performing the same task.\nTo learn from multi-embodiment data, Zakka et al. (2022) seek to learn a useful representation for cross-embodiment learning that aligns task progress in one embodiment to task progress in a different embodiment. To address this, XIRL relies on temporal cycle-consistency (TCC) (Dwibedi et al., 2019) learning to establish task-aligned feature representations of cross embodiment demonstrations that captures information about the task itself, rather than the agent executing the task. The goal of TCC is to train an encoder, \u03d5, that takes as input a video frame image, v\u209b, corresponding to state s, and outputs an embedding vector \u03d5(v\u209b). One of the primary benefits of XIRL's use of TCC is that it does not require embodiment labels and is completely unsupervised. First, random mini-batches of video trajectories are sampled from D. Given \u03d5, each video trajectory can be represented as a sequence of embedded images, V\u1d62 = {\u03d5(v\u1d62\u00b9), \u03d5(v\u1d62\u00b2),\u00b7\u00b7\u00b7, \u03d5(v\u1d62\u1d38\u1d62)}, where L\u1d62 = |v\u1d62|. Given a mini-batch of embedded videos, \u03d5 is updated by taking pairs of sequences V\u1d62 and V\u2c7c and computing a TCC Loss which aligns a random frame index, t, in V\u1d62 to the corresponding soft-nearest-neighbor frame, t', in V\u2c7c by minimizing the mean-squared error between the frame indices, \\(\\mathcal{L}_{t}^{i j}=\\left(t^{\\prime}-t\\right)^{2}\\) (Zakka et al., 2022).\nFollowing the self-supervised training of the encoder \u03d5, XIRL grounds an embodiment-agnostic reward function to the demonstration set by computing a goal embedding. Because XIRL assumes that the demonstrations provided are always near-optimal, the last frame of every sequence, v\u1d62\u1d38\u1d62, can be assumed to represent a state where the agent successfully completed the task. Therefore, the average of these final frames' embeddings are averaged to create a goal state embedding, g = \\(\\frac{1}{N} \\sum_{i=1}^{N} \\phi\\left(v_{i}^{L_{i}}\\right)\\). Finally, both the encoder and the goal embedding are used to provide a reward signal during reinforcement learning. Specifically, the reward is the negative signed distance to goal,\n\\(r(s) = - \\frac{1}{\\kappa} \\cdot ||\\phi(s) \u2013 g||\u2082,\\)\nwhere \u03ba is a scaling parameter.\nIn the following sections, we build on the foundational work of Zakka et al. (2022) to study cross-embodiment learning when multi-embodiment demonstrations are of mixed quality and may not always successful complete the desired task."}, {"title": "METHODS", "content": "We seek to learn reward representations that generalize to unseen agent embodiments, In contrast to XIRL (Zakka et al., 2022), we assume that our dataset is MQME: mixed-quality, where the quality of the demonstration with respect to task success varies and also mixed-embodiment (i.e. demonstrations will be given by agents with various physical embodiments and action spaces). Dealing with mixed-quality data poses a challenge for two of the main components of the XIRL pipeline: (1) XIRL pretrains the video frame encoder \u03d5 with TCC which assumes that temporal alignment exists between all demonstrations, e.g., each demonstration starts in a similar start state configuration and is assumed to end at state that corresponds to task success, with several key corresponding intermediate steps. However, if some demonstrations are of mixed quality, this temporal alignment may not exist between pairwise samples of videos. (2) The reward formulation for XIRL depends on a reliable goal approximation, g, which is the average embedding from the final frame of all videos in the dataset. By removing the guarantee that all demonstrations successfully complete the task, the XIRL reward function may not guide the agent towards task completion as the goal embedding may no longer be reliable. In the following sections, we address these concerns and propose and discuss different approaches for performing cross-embodiment reward learning from MQME data."}, {"title": "Cross-Embodiment Reinforcement Learning From Human Feedback (X-RLHF)", "content": "The simplest way to address the issues of mixed-quality data in XIRL is to try learning a reward end-to-end with using reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022), by having a human provide preference labels over an offline dataset of trajectories (Shin et al., 2023). In this approach, the demonstration dataset is augmented by a set of pairwise preference labels over the data. For a pair of demonstrations, (v\u1d62, v\u2c7c), the notation v\u1d62 \u227b v\u2c7c indicates a preference of demonstration j over demonstration i. The final form of the data is the triple (v\u1d62, v\u2c7c, \u03bc), where \u03bc \u2208 {(1,0), (0, 1)} represents the human's preference label. For our problem, it is worth noting that the preferences include mixed-embodiment preferences, i.e., v\u1d62 may be demonstrated by one embodiment and v\u2c7c may be from a different embodiment.\nUsing these labels, we follow prior work by employing a deep neural network, namely a reward predictor, \\(\\hat{r}\\), that maps video frames into a single real-valued reward that can be trained via back-propagation using the standard Bradley-Terry (Bradley & Terry, 1952) and Luce-Shepperd (Luce, 2005; Shepard, 1957) model,\n\\(P(v_i \\succ v_j) = \\frac{\\exp \\sum_{s \\in v_j} \\hat{r}(s)}{\\exp \\sum_{s \\in v_i} \\hat{r}(s) + \\exp \\sum_{s \\in v_j} \\hat{r}(s)}\\)\nwhere P is the softmax probability that v\u1d62 \u227b v\u2c7c based on \\(\\hat{r}\\). The learned reward function, \\(\\hat{r}\\), is then optimized using a Cross Entropy Loss between the predicted value of P and the preference labels:\n\\(\\mathcal{L}(\\hat{f}) = - \\sum_{(v_i, v_j, \\mu) \\in \\mathcal{D}} \\mu_1 \\log P(v_i \\succ v_j) + \\mu_2 \\log P(v_i \\succ v_j) .\\)\nWhile prior work has focused on single-embodiment RLHF, we seek to study cross-embodiment RLHF (X-RLHF). The benefit of using X-RLHF is that it directly learns the reward end-to-end, with no intermediate latent representation or goal embedding used to manually compute the reward. It is a natural choice for MQME data since the only requirement is to have preference labels over trajectories and these trajectories do not need to be optimal nor come from the same embodiment. Although X-RLHF requires more human burden compared to XIRL, using human supervision can lead to better and more human-aligned representations (Bobu et al., 2023; Mattson & Brown, 2023; Tian et al., 2024) than purely unsupervised representation learning approaches."}, {"title": "Representation Learning from Preferences", "content": "An alternative approach to learning a reward function end-to-end from preferences is to use human feedback to explicitly learn the representation \u03d5(s) (Bobu et al., 2023; Mattson & Brown, 2023; Tian et al., 2024). When provided with MQME data, we hypothesize that representation learning via TCC will fail to learn a correct embedding because both videos may not share the same task-relevant keyframes. Thus, we propose the use of preferences to learn a better latent embedding that can be used to guide RL via the same reward function used in XIRL (Equation (1)). However, Equation (1) requires a known or calculated goal embedding, g. We assume that in addition to the MQME dataset, we have direct and privileged access to a known set of goal states, \\(\\mathcal{G}^* = \\{g_1, g_2, ..., g_N\\}\\). Note that these could be supplied by the user as a set of states, disjoint from any actual trajectories. Alternatively, these goal states can come from suboptimal trajectories that eventually reach the goal state. Our goal embedding is simply the average embedding over all goals in \\(\\mathcal{G}^*\\), resulting in \\(g = \\frac{1}{N} \\sum_{g \\in \\mathcal{G}^*} \\phi(g)\\).\nOne deceptively simple approach is to combine preference learning with the inductive bias in XIRL by using the same underlying architecture and reward function as XIRL, but with supervised learning from pairwise preferences. We call this approach Cross Preference Learning (XPrefs). To do this, we could use the preference data to optimize the representation \u03d5(s) by maximizing the likelihood of the preference labels, where\n\\(P(V_i > V_j) = \\frac{\\exp \\sum_{s \\in v_i} -||\\phi(s) - g||_2}{\\exp \\sum_{s \\in v_i} -||\\phi(s) - g||_2 + \\exp \\sum_{s \\in v_j} -||\\phi(s) - g||_2}\\)\nHowever, we now have a non-stationary goal embedding, resulting in an \"chicken-and-egg\" cyclic dependency where both the embedding of the demonstration, \u03d5(s), and g, which is a function of \u03d5, will change every time the model updates. In Appendix A we explore both dynamic and static goal representations in Appendix A and show that a static goal representation results in the best performance for reward learning. However, upon closer inspection, it can be seen that XPrefs is nearly identical to X-RLHF. Indeed, the effect of the goal embedding is lost when \\(\\phi(s) = \\phi^{\\prime}(s) + g\\), where \\(\\phi^{\\prime}(s)\\) is an arbitrary function of state. Thus, XPrefs is simply X-RLHF with a non-positive reward function. Indeed, in Appendix A we empirically compare the performance of X-RLHF and XPrefs and find they are nearly identical.\nThe crux of the problem with XPrefs is that we are still trying to learn the reward function and representation simulataneously. Instead, motivated by recent work (Tian et al., 2024), we seek to first learn an aligned representation using human feedback and then use this fixed representation as the representation \u03d5 using the same reward function as XIRL (Equation (1)). This eliminates the chicken-and-egg problem since the goal embedding is not calculated until after the representation is learned. Tian et al. (2024) propose the use of triplet preference queries as a way to learn an aligned representation. We follow their approach and seek a representation that is aligned with human's preferences. We obtain ranked triplets over MQME data v\u1d62 \u227b v\u2c7c \u227b v\u2096 where we assume rankings are based on the human's internal reward function. We then learn a representation \u03d5 using the Bradley-Terry model Bradley & Terry (1952)\n\\(P(V_i > V_j > V_k) = \\frac{\\exp -d(\\phi(v_i), \\phi(v_j))}{\\exp -d(\\phi(v_i), \\phi(v_j)) + \\exp -d(\\phi(v_j), \\phi(v_k))}\\)\nwhere d is a distance metric and where we treat v\u1d62 as an anchor and v\u2c7c as a positive and v\u2096 as a negative in a contrastive loss. Given triplet preferences we can directly backpropagate into the representation \u03d5 to maximize the likelihood of the preference labels.\nWe call this approach Cross-Embodiment Triplet Representation Learning (XTriplets). We note some differences between our work and Tian et al. (2024). Tian et al. (2024) use a differentiable optimal transport-based distance metric d, but do not investigate using a simple distance metric such as the L2 norm that was proposed by Zakka et al. (2022). Tian et al. (2024) also do not consider training across multiple embodiments, where as we learn from MQME data. To better compare across different representation learning approaches, including XIRL, we use the L2 norm and the reward function in Equation (1) for all representation learning methods including XTriplets."}, {"title": "XIRL-Buckets", "content": "The third method we study, takes advantage of the unsupervised nature of XIRL but also adapts the algorithm for a MQME dataset. We propose XIRL-Bucket, which partitions the dataset into a number of \"buckets\" or bins based on ordinal labels provided by the user. We assume that a human labeller categorically assigns the trajectories into a bucket based on perceived performance. For example, the human could rate trajectories based on a 5-point Likert scale and then have a bucket for each ordinal rating 1 through 5. We then train a representation by applying a TCC loss only amongst trajectories within the same bucket and use this representation as a reward function following the same procedure as XIRL. Compared to X-RLHF, XPrefs, and XTriplets which require preference labels over a dataset, XIRL-Buckets only requires ordinal categorization which can be far less burdensome in terms of the number of times human queries."}, {"title": "EXPERIMENTS", "content": "In this section, we seek to answer the following questions: (1) How does the quality of demonstrations degrade the learned reward and XIRL? (2) Can we leverage different types of human feedback to learn good reward representations from MQME data? (3) How do the different methods described above perform when learning from MQME data?"}, {"title": "Experimental Setup", "content": "Domain We conduct a series of experiments targeted at answering the aforementioned questions. For our experiments we use the X-MAGICAL imitation learning benchmark from Zakka et al. (2022). An example of this task is shown in Figure 2. The task involves pushing a set of blocks into the pink endzone using an agent of four possible embodiments: shortstick, mediumstick, longstick and gripper. The three stick embodiments all have the same action space but differ in their length, making the task easier for the longer embodiments and leading to qualitatively different optimal policies depending on the embodiment. Gripper not only has a different shape, but also has an extra action it can use to grip blocks with the pair of pincers on the agent.\nMQME Data To simulate a mixed-quality, mixed-embodiment dataset for our experiments, we took policies trained via RL on the ground-truth reward (number of blocks pushed to the goal divided by 3) for each of the embodiments listed above and then degraded these pretrained oracle policies by iteratively adding randomness to the action selection. This type of noise injection is inspired by prior work that found it resulted in diverse suboptimal behaviors (Brown et al., 2020; Tien et al., 2022). There are a total of 600 trajectories for each embodiment (200 training and 400 testing trajectories). The dataset is evenly partitioned based on the number of blocks that are pushed in by the agent. By contrast, the X-MAGICAL dataset provided by Zakka et al. (2022) has the same embodiments as our MQME dataset but consists of approximately 1000 trajectories per embodiment (877 training and 98 testing trajectories), more than 4 times the amount of training data as our MQME dataset, all of which are exclusively successful demonstrations of the task.\nWe trained the reward model for X-RLHF using 5000 preference labels, all of which were obtained by sampling them from a larger set of procedurally generated preferences by comparing all the pairs of trajectories in our MQME dataset. The preferences were generated according to which trajectory in a pair of trajectories from the dataset has the higher average ground truth environment reward per step over the length of the trajectory. The reason the average reward per step was used is due to the longstick embodiment having a shorter time horizon for the task as it is significantly easier to complete the task with that particular embodiment. The synthetic preferences were meant to loosely mimic how a human would provide preferences observing the task. In a similar manner, we trained XTriplets for 4000 iterations with 32 triplets per batch where each triplet was formed by sampling with replacement from the MQME dataset and using the oracle synthetic labeler to order the triplets. We used much more training data than X-RLHF in an attempt to improve performance, but as we show later, we were not able to get performance comparable to X-RLHF even with the additional triplet feedback (see Appendix C for more details).\nFor XIRL-Buckets, we started with the same dataset of MQME 600 trajectories (200 for each training embodiment). We then simulated human ordinal ratings using the ground truth reward to partition the data into 18 buckets containing 32 trajectories each. This allowed us to pass an entire bucket into TCC as one batch, matching the batch size of Zakka et al. (2022) for consistency across methods."}, {"title": "Baselines", "content": "In this section we describe the three baselines we compare against. All these baselines and methods use the same Soft Actor Critic code (Zakka et al., 2022) used in the original XIRL work: (1) Reinforcement Learning on Ground Truth Reward. As an oracle, we run RL on the ground-truth reward from Zakka et al. (2022). At each step, the ground truth reward describes the fraction of total available blocks that are currently in the goal zone. (2) XIRL Trained on X-MAGICAL. This method uses the full pipeline of XIRL as described in Section 4 trained on the dataset of 200 successful demonstrations for each embodiment. This acts as an oracle since it provides the current-state-of-the-art performance for cross-embodiment IRL, but assumes access to near-optimal demonstrations for each embodiment. (3) XIRL Trained on Mixed Data. This method follows the XIRL pipeline but uses MQME data for TCC representation learning. The second step of the XIRL pipeline, goal embedding computation, is done with the same set of positive goal state examples we assume we have access to for the other methods studied in this paper. Thus, this baseline allows us to test the effect of mixed-quality data on XIRL and provides the main, non-oracle, baseline which we hope to significantly outperform. (4) Goal Classifier (Vecerik et al., 2019). We train a binary classifier where frames in the goal set (G*) are positive examples and frames from the MQME dataset are negative examples. Following Zakka et al. (2022), the reward signal is the probability outputs of the model."}, {"title": "Cross-Embodiment Learning from Mixed-Quality, Mixed-Embodiment Data", "content": "To study how well our proposed ap-proaches and baselines compare when evaluated on MQME data, all the re-ward models (except the oracle ground-truth RL baseline) were trained on 3 out of the 4 embodiments. We then evalu-ated the down-stream RL performance on the medium-stick held-out embodi-ment. To evaluate generalization per-formance, we took the average cumula-tive ground truth reward over 50 policy rollouts every 5000 RL training steps. These evaluation statistics were aver-aged over 5 seeds to account for ran-domness when learning a policy.\nFigure 1 summarizes the results. From this graph we can infer a few key find-ings of this experiment. Firstly, XIRL when trained on only successful tra-jectories, noticeably outperformed the other approaches by consistently push-ing all three blocks in quickly and ef-ficiently. Interestingly, XIRL actually outperforms the ground truth reward function by learning a good representa-tion that successfully shapes the reward function, enabling efficient RL. On the other hand, XIRL Mixed, which was the same as XIRL, but trained on an MQME dataset, appears to completely collapse, unable to get a single block close to the goal zone. We hypothesize that this is because the mixed-quality data violates many of the strong assumptions that XIRL is founded on, in particular, the use of TCC to learn the latent video embedding."}, {"title": "Reward Accuracy Analysis", "content": "To analyze the performance of each reward model, we evaluate the alignment of reward model outputs with the ordinal ground truth rankings of a withheld demonstration set. For each embodiment, we evaluate 400 test demonstrations of mixed quality. Each trajectory is then represented as the ordered pair (r, \\(\\hat{r}\\)) representing the trajectory's cumulative ground truth reward (r) and the cumulative predicted reward (\\(\\hat{r}\\)). The results of the withheld mediumstick embodiment for two accuracy metrics are shown in Table 1. For brevity in the main body, we include correlation plots and performance for all reward learning embodiments in Appendix B.\nFirst, we use Kendall Rank Correlation Coefficient (also known as Kendall's Tau) (Abdi, 2007), a statistical measure of determining correlation between two measured quantities. In our case, we wish to measure the alignment between r and \\(\\hat{r}\\) for all (400) demonstration pairs. Measurements for Kendall's Tau lie within the range [-1,1], where the agreement between the two quantities is in perfect agreement at 1, and perfect disagreement at -1. Second, using the same set of demonstration"}, {"title": "Qualitative Analysis of Learned Representations and Rewards", "content": "We next performed a qualitative analysis of the learned representations and reward functions for the different methods. Figure 2 depicts the learned reward plotted against the timestep over the course of both a failed trajectory and a successful trajectory. This figure helps us visualize what reward signals and representations different approaches are learning.\nObserving the shape of the learned reward for the unsuccessful trajectory, we can see that every learned reward except for XIRL trained on MQME has learned to associate actions that do not result in blocks being pushed with a low and near constant reward. The shaped reward for the successful demonstrations proves to be more informative with what these models are learning. XIRL trained on X-MAGICAL has the most shaped and dense reward, giving positive signals to the agent consistently throughout the task, even when it is moving away from the goal-zone to head back and gather more blocks. Interestingly, even XIRL on MQME has a surprisingly dense and informative signal, however this does not translate to final policy performance as it had the lowest average cumulative reward across all models. This can be explained by the fact that XIRL (MQME) likely assigns all end states as having high reward since the TCC alignment across mixed demonstrations will push the end frames of the videos are aligned to be considered successful. We can see evidence of this in the curve for XIRL (MQME) for the failed trajectory, where the predicted reward still spikes upward. Finally, XTriplets poorly predicts the reward for the failed trajectory and incorrectly attributes relatively high reward to states that do not progress the task, which is supported by XTriplets' poor RL performance.\nX-RLHF appears to learn a similarly shaped reward to that of the ground truth reward (a step function representing the fraction of total available blocks that are currently in the goal zone). It gives clear spikes in reward signals when blocks are pushed in, but fail to provide information in between these key states. On closer inspection of the policies learned by the X-RLHF reward we found they learned a greedy strategy to get as many blocks as possible into the goal zone with a single push. The learned models do not appear to associate a strong reward signal for actions that occur after getting 2 blocks in. Leveraging online queries to refine the reward representation could overcome this problem and is an interesting area for future work."}, {"title": "CONCLUSION", "content": "In this paper we introduce the novel problem setting of cross-embodiment learning from mixed-quality data. Collecting near-optimal demonstrations in complex environments is challenging and human demonstrations are often noisy or suboptimal. We propose and evaluate X-RLHF, XTriplets, and XIRL-Buckets as three potential algorithms to help address these shortcomings. Our empirical results demonstrate that, XIRL (Zakka et al., 2022), the prior state-of-the-art approach to cross-embodiment IRL suffers a large degradation in performance when not all demonstrations are near-optimal. By contrast, X-RLHF and XIRL-Buckets both showcase the ability to leverage human feedback over mixed-quality, mixed-embodiment data to learn a reward function that is embodiment independent and enables the ability to generalize this reward to out-of-distribution embodiments. An exciting area of future work is to explore a combination of some of our proposed methods. For example, X-RLHF and XIRL-Buckets appear to have somewhat complimentary reward shapes, could a linear combination of the two lead to a more informative reward? Similarly, we could leverage our goal embedding calculation and training as a finetuning step after a run of a TCC algorithm. Another area of future work is to study the human factors involved in different forms of human feedback as they relate to representation alignment and cross-embodiment learning. Finally, using active preference learning could enable more label-efficient algorithms (Biyik & Sadigh, 2018; Wilde et al., 2020; Shin et al., 2023)."}, {"title": "Further Analysis of XPrefs", "content": "One of the design choices mentioned in the section describing Xprefs, is whether to use a static or dynamic goal embedding. At first glance, a dynamic embedding seemed to be the best way to ensure the guarantee of a global minima so we performed a preliminary experiment with varying frequencies of goal embedding updates in the training process. Figure 3a showcases the training losses over training steps we observed for the various frequencies of updating the goal embedding g described in the XIRL preliminaries. We experimented with frequencies of 4, 8, 400 and 1000 and compared the results with using static goal embedding frozen before training. Our results provide evidence that periodically updating the goal embedding causes instabilities since it induces a moving target during learning.\nIn Figure 3a, the loss curve when training XPrefs and updating the goal embedding every 1000 steps is particularly interesting as there is a clear spike in loss every time the goal embedding is updating, leading to an intuition that the updates lend to an instability and make it more difficult to settle in a local minimum. Our results provide evidence that periodically updating the goal embedding causes instabilities since it induces a moving target during learning. We settled on using a static embedding model which resulted in a much more stable and meaningful learned reward, as shown by the clear convergence of the loss in Figure 3a when using a static goal embedding. We theorize that fixing an arbitrary point in the embedding space as our goal state is appropriate and in fact beneficial to learning an embodiment-independent state representation as it is akin to fixing an origin for the space and fitting the state distribution around it in a way that is locally optimal.\nIn section 5.2, we theorized that XPrefs is simply X-RLHF with a non-positive reward function. To emperically support this notion, we trained two reward models and compared RL performance to X-RLHF (Figure 3b). The first model is XPrefs with a static goal embedding that utilizes the goal set (g*), as described in the previous paragraph. The second model (notated \"Xprefs (g=0)\" in Figure 3b), tests our hypothesis that the goal embedding is an arbitrary bias that does not help"}]}