{"title": "MMTEB: MASSIVE MULTILINGUAL TEXT EMBEDDING BENCHMARK", "authors": ["Kenneth Enevoldsen", "Isaac Chung", "Imene Kerboua", "M\u00e1rton Kardos", "Ashwin Mathur", "David Stap", "Jay Gala", "Wissam Siblini", "Dominik Krzemi\u0144ski", "Genta Indra Winata", "Saba Sturua", "Saiteja Utpala", "Mathieu Ciancone", "Marion Schaeffer", "Gabriel Sequeira", "Diganta Misra", "Shreeya Dhakal", "Jonathan Rystr\u00f8m", "Roman Solomatin", "\u00d6mer \u00c7a\u011fatan", "Akash Kundu", "Martin Bernstorff", "Shitao Xiao", "Akshita Sukhlecha", "Bhavish Pahwa", "Rafa\u0142 Po\u015bwiata", "Kranthi Kiran GV", "Shawon Ashraf", "Daniel Auras", "Bj\u00f6rn Pl\u00fcster", "Jan Philipp Harries", "Lo\u00efc Magne", "Isabelle Mohr", "Mariya Hendriksen", "Dawei Zhu", "Hippolyte Gisserot-Boukhlef", "Tom Aarsen", "Jan Kostkan", "Konrad Wojtasik", "Taemin Lee", "Marek \u0160uppa", "Crystina Zhang", "Roberta Rocca", "Mohammed Hamdy", "Andrianos Michail", "John Yang", "Manuel Faysse", "Aleksei Vatolin", "Nandan Thakur", "Manan Dey", "Dipam Vasani", "Pranjal Chitale", "Simone Tedeschi", "Nguyen Tai", "Artem Snegirev", "Michael G\u00fcnther", "Mengzhou Xia", "Weijia Shi", "Xing Han L\u00f9", "Jordan Clive", "Gayatri Krishnakumar", "Anna Maksimova", "Silvan Wehrli", "Maria Tikhonova", "Henil Panchal", "Aleksandr Abramov", "Malte Ostendorff", "Zheng Liu", "Simon Clematide", "Lester James Miranda", "Alena Fenogenova", "Guangyu Song", "Ruqiya Bin Safi", "Wen-Ding Li", "Alessia Borghini", "Federico Cassano", "Hongjin Su", "Jimmy Lin", "Howard Yen", "Lasse Hansen", "Sara Hooker", "Chenghao Xiao", "Vaibhav Adlakha", "Orion Weller", "Siva Reddy", "Niklas Muennighoff"], "abstract": "Text embeddings are typically evaluated on a limited set of tasks, which are con- strained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) \u2013 a large-scale, community-driven expan- sion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a similar ranking order as the full-scale version but only requires 2% of the original documents vastly reducing the computational cost.", "sections": [{"title": "1 INTRODUCTION", "content": "Text embeddings are used in many applications, such as semantic search (Reimers & Gurevych, 2019; Muennighoff, 2022; Hendriksen et al., 2023; Winata et al., 2023a; 2024b) and classification tasks (Wang et al., 2018; 2019). Additionally, text embeddings play a crucial role in retrieval-augmented generation (RAG; Borgeaud et al. 2022; Lewis et al. 2021), and often provide significant gains in performance on low- to mid-resource languages, enabling the incorporation of previously inaccessible information. Despite the wide range of applications, there's a lack of benchmarks that evaluate text embeddings across multiple domains, languages, and tasks. Existing benchmarks tend to focus on specific domains, demarcated by subject (e.g., medical, legal, fiction (Thorne et al., 2018b)), particular tasks (e.g., retrieval (Thakur et al., 2021)), literary type (e.g., fiction, and non-fiction) or form (e.g., spoken and written). Embeddings also tend to focus on a subset of languages (N\u00f8rregaard & Derczynski, 2021). While recent efforts (Thakur et al., 2021; Muennighoff et al., 2023b; Zhang et al., 2022) have aimed to broaden the scope by encompassing more tasks, domains, or languages (Cohan et al., 2020a; Wrzalik & Krechel, 2021), a large gap in language coverage remains. This work bridges this gap by creating a benchmark that includes a much broader range of low- to mid-resource languages, along with broader coverage of domains and task categories. To create such an expansive benchmark, we initiated a large-scale, open collaboration. Contributors include native speakers from diverse linguistic backgrounds, NLP practitioners, academic and industry researchers, and enthusiasts. To ensure high-quality submissions, each dataset required systematic tests, detailed metadata, and a review. The result of this extensive collaborative effort is MMTEB, the Massive Multilingual Text Embedding Benchmark, which comprises more than 500 distinct tasks across 10 task categories, covering over 250 languages, and spans a wide array of domains such as fiction, social media, medical texts, and technical programming documentation. It also integrates recent, high-quality benchmarks that test a model's capabilities in following instructions (Winata et al., 2021; Weller et al., 2024), embedding long documents (Zhu et al., 2024), solving reasoning tasks (Xiao et al., 2024a; Su et al., 2024), and cross-lingual retrieval (Franco-Salvador et al., 2014). For an overview see Figure 1."}, {"title": "2 MMTEB CONSTRUCTION", "content": "To ensure the broad applicability of MMTEB across various domains, we recruited a diverse group of contributors. We actively encouraged participation from industry professionals, low-resource language communities, and academic researchers. To clarify authorship assignment and recognize desired contributions, we implemented a point-based system, similar to Lovenia et al. (2024). To facil- itate transparency, coordination was managed through GitHub. A detailed breakdown of contributors and the point system can be found in Appendix A."}, {"title": "2.1 OPEN SCIENCE EFFORT"}, {"title": "2.2 ENSURING TASK QUALITY", "content": "To guarantee the quality of the added tasks, each task was reviewed by at least one of the main contributors. In addition, we required task submissions to include metadata fields. These fields included details such as annotation source, dataset source, license, dialects, and citation information. Appendix B.4 provides a comprehensive description of each field. Furthermore, we ensured that the performance on submitted tasks fell within a reasonable range to avoid trivially low or unrealistically high performance. Therefore, we required two multilingual models to be run on the task; ultilingual-e5-small (Wang et al., 2022) and MiniLM-L12 (Reimers & Gurevych, 2019). A task was examined further if the models obtained scores close to a random baseline (within a 2% margin), a near-perfect score, or if both models obtained roughly similar scores."}, {"title": "2.3 ACCESSIBILITY AND BENCHMARK OPTIMIZATION", "content": "As detailed in Section 1, extensive benchmark evaluations often require significant computational resources. This trend is also observed in MTEB(eng, v1) (Muennighoff et al., 2023b), where running moderately sized LLMs can take up to two days on a single A100 GPU. Accessibility for low- resource communities is particularly important for MMTEB, considering the common co-occurrence of computational constraints (Ahia et al., 2021). Below, we discuss three main strategies implemented to make our benchmark more efficient. We additionally elaborate further code optimization in Appendix C.2."}, {"title": "2.3.1 DOWNSAMPLING AND CACHING EMBEDDINGS", "content": "The first strategy involves optimizing the evaluation process by downsampling datasets and caching embeddings. Encoding a large volume of documents for tasks such as retrieval and clustering can be a significant bottleneck in evaluation. Downsampling involves selecting a representative subset of the dataset and reducing the number of documents that require processing. Caching embeddings prevents redundant encoding by using already processed documents. Clustering. In MTEB, clustering is evaluated by computing the v-measure score (Rosenberg & Hirschberg, 2007) on text embeddings clustered using k-means. This process is repeated over multiple distinct sets, inevitably resulting in a large number of documents being encoded. To reduce this encoding burden, we propose a bootstrapping approach that reuses encoded documents across sets. We first encode a 4% subsample of the corpus and sample 10 sets without replacement. Each set undergoes k-means clustering, and we record performance estimates. For certain tasks, this approach reduces the number of documents encoded by 100\u00d7. In Appendix B.2, we compare both approaches and find an average speedup of 16.11x across tasks, while preserving the relative ranking of models (Average Spearman correlation: 0.96). Retrieval. A key challenge in retrieval tasks is encoding large document collections, which can contain millions of entries Nguyen et al. (2024). To maintain performance comparable to the original datasets while reducing the collection size, we adopted the TREC pooling strategy (Buckley et al., 2007; Soboroff & Robertson, 2003), which aggregates scores from multiple models to select representative documents. For each dataset, we retained the top 250 ranked documents per query, a threshold determined through initial tests that showed negligible differences in absolute scores and no changes in relative rankings across representative models (see Appendix C.1.2 for details on downsampling effects). These documents are merged to form a smaller representative collection. For datasets exceeding 1,000 queries, we randomly sampled 1,000 queries, reducing the largest datasets from over 5 million documents to a maximum of 250,000. This approach accelerated evaluation while preserving ranking performance. Bitext Mining. We apply similar optimization to bitext mining tasks. Some datasets, such as Flores (Costa-juss\u00e0 et al., 2022) share the same sentences across several language pairs (e.g., English sentences are the same in the English-Hindi pair and the English-Bosnian pair). By caching the embeddings, we reduce the number of embedding computations, making it linear in the number of languages instead of quadratic. For the English documents within Flores this results in a reduction of documents needed to be embedded from 410,000 in MTEB (eng, v1) to just 1,012 in our benchmark."}, {"title": "2.3.2 ENCOURAGING SMALLER DATASET SUBMISSIONS", "content": "The second strategy focused on encouraging contributors to downsample datasets before submission. To achieve this, we used a stratified split based on target categories. This helped us to ensure that the downsampled datasets could effectively differentiate between candidate models. To validate the process, we compared scores before and after downsampling. For details, we refer to Appendix C.1."}, {"title": "2.3.3 TASK SELECTION", "content": "To further reduce the computation overhead we seek to construct a task subset that can reliably predict task scores outside the subset. For task selection, we followed an approach inspired by Xia et al. (2020). We seek to estimate the model $m_i \\in M$ scores $S_{t,m_i}$ on an unobserved task $t$ based on scores on observed tasks $S_{j,m_k} \\in S, j\\neq t$. This allows us to consider the performance of tasks as features within a prediction problem. Thus we can treat task selection as feature reduction, a well-formulated task within machine learning. Note that this formulation allows us to keep the unobserved task arbitrary, representing generalization to unseen tasks (Chollet, 2019). We used a backward selection method, where one task is left out to be predicted, an estimator\u2074 is fitted on the performance of all models except one, and the score of the held-out model is predicted. This process is repeated until predicted scores are generated for all models on all tasks. The most predictable task is then removed, leaving the estimators in the task subset group. Optionally, we can add additional criteria to ensure task diversity and language representation. Spearman's rank correlation was chosen as the similarity score, as it best preserved the relative ranking when applied to the MTEB(eng, v1)."}, {"title": "2.4 BENCHMARK CONSTRUCTION", "content": "From the extensive collection of tasks in MMTEB, we developed several representative benchmarks, including a highly multilingual benchmark, MTEB(Multilingual), as well as regional geopolitical benchmarks, MTEB(Europe) and MTEB(Indic). Additionally, we introduce a faster version of MTEB(eng, v1) (Muennighoff et al., 2023b), which we refer to as MTEB(eng, v2). MMTEB also integrates domain-specific benchmarks like CoIR for code retrieval (Li et al., 2024) and LongEmbed for long document retrieval (Zhu et al., 2024). MMTEB also introduces language-specific benchmarks, extending the existing suite that includes Scandinavian (Enevoldsen et al., 2024), Chinese (Xiao et al., 2024b), Polish (Po\u015bwiata et al., 2024), and French (Ciancone et al., 2024). For an overview of the benchmarks, we refer to Appendix H.1. In the following section, we detail a methodology that we designed to create more targeted and concise benchmarks. This methodology includes: 1) clearly defining the initial scope of the benchmark (Initial Scope), 2) reducing the number of tasks by iterative task selection tasks based on intertask correlation (Refined Scope), and 3) performing a thorough manual review (Task Selection and Review). We provide an overview in Table 1. In addition to these benchmarks, we provide accompanying code to facilitate the creation of new benchmarks, to allow communities and companies to create tailored benchmarks. In the following, we present MTEB(Multilingual) and MTEB(eng, v2) as two example cases. For a comprehensive overview of benchmark construction and the tasks included in each benchmark, we refer to Appendix H.2. MTEB(Multilingual): We select all available languages within MMTEB as the initial scope of the benchmark. This results in 550 tasks. We reduce this selection by removing machine-translated datasets, datasets with under-specified licenses, and highly domain-specific datasets such as code-retrieval datasets. This results in 343 tasks covering >250 languages. Following this selection, we evaluate this subset using a representative selection of models (See Section 3.1) and apply task selection to remove the most predictable tasks. To ensure language diversity and representation across task categories, we avoid removing a task that would eliminate a language from the respective task category. Additionally, we did not remove a task if the mean squared error between predicted"}, {"title": "3 EXPERIMENTAL SETTINGS", "content": "We select a representative set of models, focusing on multilingual models across various size categories. We benchmark the multilingual LaBSE (Feng et al., 2022), trained on paraphrase corpora, English and multilingual versions of MPNet (Song et al., 2020), and MiniLM (Wang et al., 2021b) model, trained on diverse datasets. We also evaluate the multilingual e5 series models (Wang et al., 2024; 2022) trained using a two-step approach utilizing weak supervision. Additionally, to understand the role of scale as well as instruction finetuning, we benchmark GritLM-7B (Muennighoff et al., 2024) and e5-multilingual-7b-instruct (Wang et al., 2023), which are both based on the Mistral 7B model (Jiang et al., 2023). Revision IDs, model implementation, and prompts used are available in Appendix G. We ran the models on all the implemented tasks to encourage further analysis of the model results. Results, including multiple performance metrics, runtime, CO2 emissions, model metadata, etc., are publicly available in the versioned results repository.5"}, {"title": "3.1 MODELS"}, {"title": "3.2 EVALUATION SCORES", "content": "For our performance metrics, we report average scores across all tasks, scores per task category, and weighted by task category. We compute model ranks using the Borda count method (Colombo et al.,"}, {"title": "3.3 MULTILINGUAL PERFORMANCE", "content": "While MMTEB includes multiple benchmarks (see Appendix H.1), we select three multilingual benchmarks to showcase. These constitute a fully multilingual benchmark MTEB(Multilingual) and two targeting languages with varying levels of resources: MTEB (Europe) and MTEB(Indic). The performance of our selected models on these tasks can be seen in Table 2. For performance metrics per task, across domains, etc., we refer to Appendix E."}, {"title": "4 ANALYSIS AND DISCUSSION", "content": "Table 2 shows the performance across the three presented multilingual benchmarks. Two trends are clearly observable; Models trained with instruction-tuning perform significantly better compared to those without it. This is especially clear when comparing the multilingual-e5-large to its instruction-tuned counterpart (multilingual-e5-large-instruct). Instruction tuning increases performance most drastically on bitext mining and clustering, though the effect remains pronounced across all task categories. Notably, this happens despite many tasks using generic prompts for the task category and no model-specific tuning of prompts per task. Surprisingly, multilingual-e5-large(-instruct) models, based on XLM-R Large (Conneau et al., 2019) generally outperform the considerably larger e5-mistral-7b-instruct and GritLM-7B, both of which are based on Mistral-7B (Jiang et al., 2023). This effect is notably pronounced for mid-to-low resource languages (<300M speaker; see Appendix E.1) and likely emerges due to differences in pre-training, with Mistral being predominantly pre-trained on English, while XLM-R targets 100 languages. All three models utilize similarly multilingual datasets for fine-tuning. However, GritLM still remains best in class for retrieval on MTEB(Multilingual), it has a higher maximum sequence length (see Figure 2) and outperforms the multilingual-e5-large-instruct on MTEB(Code) and MTEB(eng, v2). Discrepancies in Multilingual benchmarks ranking seem to stem from discrepancies in pre- training. While the multilingual benchmarks obtain seemingly similar performance rankings, we see a few notable discrepancies. These discrepancies seem to mainly stem from a narrow multilingual focus (GritLM-7B, e5-mistral-7b-instruct, multilingual-mpnet-base) during training, resulting in disproportionally higher performance on the targeted languages (typically mid-high resource or Euro-"}, {"title": "5 RELATED WORK", "content": "Text Embedding Benchmarks. BEIR (Thakur et al., 2021) pioneered the use of publicly available datasets from diverse information retrieval (IR) tasks and domains and evaluated 10 various retrieval"}, {"title": "6 CONCLUSION", "content": "This work introduced the Massive Multilingual Text Embedding Benchmark (MMTEB), a large- scale open collaboration resulting in a benchmark with more than 500 tasks covering more than 1000 languages. From these, we constructed three multilingual benchmarks: one fully multi-"}, {"title": "LIMITATIONS", "content": "English Leakage. While MMTEB filters out machine-translated datasets, it permits (human) translations. This inclusion leads to tasks like SIB200ClusteringS2S, where labels from English samples are transferred to their translations, potentially introducing bias towards English or models trained on translated content. Consequently, the benchmark may inadvertently encourage model developers to favor English or translated content by increasing their proportion in pre-training data. Credit Assignment for Large-scale Collaborations. One of MMTEB's goals was to highlight the benefits of collaboration. The managing group believes the point system successfully defined contribution terms but acknowledges it isn't perfect. For instance, equal points were awarded for dataset submissions regardless of effort some datasets were readily available, while others needed significant work like reformulation, HTML parsing, and multiple review rounds. Languages Representation. While the benchmark includes over 250 languages and 500 tasks, the distribution is skewed toward high-resource languages (see Figure 6), with low-resource languages being better represented in specific task categories like bitext-mining and classification. We encourage future collaborations to fill these gaps and enhance language diversity in the collection."}, {"title": "ETHICAL CONSIDERATIONS", "content": "We acknowledge the environmental impact of the benchmark that stems from the compute needed across tasks. As such, emissions tracking is added using codecarbon (Courty et al., 2024) to measure kilograms of CO2-equivalents (CO2eq) and estimate the carbon footprint per task. The benchmark is a collaborative project and contains datasets of different data quality and origin. Thus, additional efforts are still required to identify and minimize biases in the benchmark datasets."}, {"title": "A CONTRIBUTIONS", "content": "We list the contributions of every author in Table 3. The possible types of contributions and their associated points are: \u2022 New dataset: A new dataset includes creating a new implementation (subclass) of a task using a new dataset. 2 points were awarded for implementing the task and 4 points for each new language introduced by the task."}, {"title": "B OVERVIEW AND CONSTRUCTION OF TASKS", "content": "In this appendix, we first provide an overview of existing tasks in MTEB benchmark and newly introduced tasks in our benchmark (Section B.1). We proceed by explaining how the tasks were constructed (Section B.2) from existing datasets. Lastly, we introduce newly constructed datasets specifically designed for MMTEB (Section B.3)."}, {"title": "B.1 INTRODUCTION TO BENCHMARK TASKS", "content": "Classification First, a train set is constructed by sampling n (8-16) samples for each label. If only a test set is available, a section is split off as a training set. Both sets are then embedded and used to train a logistic regression using a maximum of 100 iterations. Afterwards, performance metrics are calculated. For robustness, this process is repeated 10 times. Pair classification For two paired texts, the goal is to predict the label. Examples of such tasks include paraphrase detection or duplicate detection. The task is solved by embedding all documents and then computing the distance either using a model-specified metric, cosine, euclidean, dot product, or Manhattan. Using the best binary threshold, performance metrics are computed. Bitext mining The dataset consists of matching pairs of sentences, and the goal is to find the match. All matching pairs of sentences are embedded, and the closest match is found using cosine similarity, and metrics are reported. Clustering and hierarchical clustering Clustering starts with a set of documents and an associated set of labels. First we embed all documents, then take subsets of the data of size k for each of 10 consecutive experiments. All the documents are embedded, and a set of size k is sampled from the embedded documents. The embeddings are then clustered using K-means clustering, and performance metrics are calculated between the estimated clusters and labels. If the clustering problem is hierarchical, this procedure is repeated for each level of the hierarchy separately. Hierarchical tasks were formerly either split into multiple tasks, or later levels of the cluster hierarchy were ignored. Note that this formulation differs from that of MTEB in that the sets are randomly sampled from the embedded documents instead of being specified a-priori. This drastically reduced runtime as one document can be used in multiple subsets without the need to embed it multiple times. The new formulation also allows us to gain a robust estimate of performance with a lower number of documents. Retrieval Retrieval tasks consist of a corpus, queries, and mapping between the queries and their relevant documents. The goal is to retrieve these relevant documents. Both queries and documents are embedded using the model. We allow these to be embedded differently depending on the model. For each query, the corpus documents are ranked using a similarity score, and performance metrics are calculated based on the reference mapping. Multi-label classification Classification tasks in MTEB were previously limited to utilizing only one label per document. As such, some, otherwise useful multi-label classification tasks had to be dropped or reformulated. We addressed this by introducing a multi-label classification task type Similarly to our novel clustering task, we down sample training sets for 10 experiments. We limit the training sets to include 8 instances of each unique label, and train a K Nearest-Neighbours classifier. Every classifier is then evaluated on the same test set. We opted for Accuracy, F\u2081 and Label Ranking Average Precision (LRAP) as evaluation metrics. Instruction retrieval Instruction retrieval builds on the traditional retrieval task by incorporating detailed instructions alongside the queries. Unlike standard retrieval, where queries are usually brief keywords, instruction retrieval pairs each query with a comprehensive instruction that outlines the criteria for document relevance. These instructions are specific to each query and not generic to the entire dataset. Therefore, the task involves using both the query and its associated instruction to retrieve relevant documents from the corpus. For the main metric, we use Robustness@10. Reranking Similar to the retrieval task, reranking includes a corpus, query, and a list of relevant and irrelevant reference texts. The aim is to rank the results according to their relevance to the query. References and queries are embedded and references are compared to the query using cosine similarity. The resulting ranking is scored for each query and averaged across all queries, and performance metrics computed. For the main metric, we use MAP@1000. Semantic text similarity Semantic text similarity (STS) tasks consist of sentence pairs, where the goal is to determine their similarity. Labels are continuous scores, with higher numbers indicating more similar sentences. All sentences are embedded using the model, and the similarity of the pair is computed using various distance metrics, allowing for model-specified similarity metrics. Distances"}, {"title": "B.2 TASK CONSTRUCTION", "content": "This section outlines our approach to constructing tasks, primarily from pre-existing data. For details on the newly introduced dataset in MMTEB, we refer to Section B.3. Task construction from existing datasets consisted of a number of steps to ensure that the task is compatible with formulations in the benchmark and matches our standards: 1. Dataset preprocessing: we start by applying minimal additional processing to ensure the data is in the required format. 2. Dataset size reduction: to maintain manageable evaluation times, we proceed by reducing dataset size whenever applicable. 3. Relevance filtering: To ensure the datasets are relevant for the types of tasks being evaluated, we apply relevance-based dataset filtering. 4. Differentiation testing: we assess the task's ability to differentiate between the performance of two candidate models. For further details on dataset transformations for specific tasks, we refer to the dataset_transform method implementation for each task. Classification and pair classification For both classification tasks, we used existing datasets with minimal adjustments, primarily trimming them down to more manageable sizes. For performance evaluation, we rely on such metrics as F\u2081 score, accuracy, or average precision. Whenever feasible, we align our choice of the primary metric with those used in related publications. If no specific guidance exists, we default to accuracy for general classification tasks and average precision for pairwise classification. In scenarios with significant class imbalance, the F\u2081 score is prioritized. Bitext mining Bitext mining tasks were constructed using established paired datasets. Similar to the classification tasks, the primary focus was on adjusting the dataset sizes to maintain the same model rank while reducing computational load. F\u2081 scores were chosen to be the primary metric, unless specified otherwise. Clustering and hierarchical clustering Clustering tasks were derived from existing corpora, such as news articles or encyclopedic entries. The source datasets typically included categories or labels assigned by their original authors or publishers. In some cases, like the SNL and VG datasets (Navjord & Korsvik, 2023), which featured hierarchical labels, we reformulated the tasks from flat to hierarchical clustering. Retrieval A variety of tasks were integrated as retrieval tasks, including existing retrieval, question- answer, and news datasets. For question-answer datasets, the questions were used as queries, and the answers formed the corpus, with correct answers identified as properly retrieved documents. In news datasets, headlines were treated as queries, and both the full articles were considered part of the corpus, with matched summaries and articles serving as relevant documents. For the primary metric, we use nDCG@10, unless otherwise specified by the dataset publication. Multi-label classification For multi-label classification, we used existing datasets that required minimal adjustments. A critical aspect of these tasks was maintaining the balance of label distributions across the training and evaluation splits. To achieve this, we employed advanced stratification techniques (Szyma\u0144ski & Kajdanowicz, 2017; Sechidis et al., 2011) that consider higher-order relationships between labels, ensuring balanced samples and improved classification quality. For the main metric, we use accuracy. Instruction Retrieval For instruction retrieval tasks, we incorporated datasets like FollowIR (Weller et al., 2024; 2025), which consist of comprehensive narratives created by professional assessors. These datasets were initially developed for TREC shared tasks and included rich, context-heavy queries to evaluate retrieval systems' performance on more intricate retrieval problems. Reranking For reranking tasks, we adapted datasets covering a range of topics and languages, including academic paper ranking, news articles (Wu et al., 2020b), QA pair relevance from online platforms, and passage ranking (Xie et al., 2023). For the primary metric, we use MAP unless otherwise specified by the dataset publication."}, {"title": "B.3 NOVEL DATASETS", "content": "This section introduces task specifically created as a part of the MMTEB contributions. For informa- tion on how existing datasets were adapted to MTEB we refer to Appendix B. PublicHealthQA: This retrieval task is built on top of a novel dataset containing question-and- answer pairs in Public Health, specifically related to the COVID-19 disease. They are sourced from Q&A pages and Frequently Asked Questions (FAQ) sections of the Centers for Disease Control and Prevention (CDC) and World Health Organization (WHO) websites. They were produced and collected between 2019-12 and 2020-04. WebLINXReranking: This is a novel HTML reranking task derived from WebLINX, a benchmark for training and evaluating web agents with conversational capabilities (L\u00f9 et al., 2024). Whereas the original work introduces a retrieval task with the goal of retrieving HTML elements using a conversational context, we propose the first task with the goal of reranking HTML elements based on their relevance for actions executed in web environments, including clicks, hovers, and text insertions. WikiClustering: is a multilingual clustering benchmark based on Wikipedia's main topic classifica- tions. The goal is to create a clustering benchmark that works for multiple languages. To construct a WikiClustering dataset for a given language, we apply the following steps. First, download the wiki dump of the categories, the articles, and the category links. Second, we find the main topic classifications for all articles. The main topic classifications can be found by looking at the category page for the language. We only use the first paragraph of each article to construct a paragraph-to-paragraph (P2P) task similar to other P2P tasks within MTEB. Third, we filter out articles with more than one main topic and remove any topic with only one article associated with it. This step avoids ambiguity in the clustering task. Finally, we sample 2048 articles with associated main topics. While the WikiClustering benchmark can be extended to any language with main topic classifications, it is currently implemented for the following: Bosnian, Catalan, Czech, Danish, Basque, Manx, Ilokano, Kurdish, Latvian, Minangkabau, Maltese, Scots, Albanian, and Walloon. All code is available on GitHub. WikipediaRetrievalMultilingual and WikipediaRerankingMultilingual: This is a multilingual retrieval and reranking dataset based on succinct queries generated by a strong multilingual LLM grounded in Wikipedia articles. The dataset was made to resemble SQUAD. Sampled Wikipedia articles of a target language were chunked and passed to GPT4-0 using the following prompt: \"", "and": "he question should not be overly specific and should mimic a request of a user who is just starting to research the given topic Do not draw on your prior knowledge details, we refer to"}, {"title": "B.4 TASK METADATA", "content": "Table 5 shows the required metadata to fill before adding a task to the benchmark. We provide a detailed description of each field, along with examples and possible values. B.4.1 DOMAINS For our domains, we include the following: \u2022 Academic: Scholarly writing and research publications typically found in journals, theses, and dissertations. \u2022 Blog: Informal or conversational posts often found on websites or personal pages, covering a wide range of topics. \u2022 Constructed: Text or speech that is deliberately invented or constructed, often used for experimental purposes to target specific abilities. \u2022 Encyclopaedic: Structured, reference-based texts that provide comprehensive and factual information on a wide range of subjects."}, {"title": "C BENCHMARK OPTIMIZATIONS", "content": "We aim to reduce the total amount of time needed to run the complete set of MTEB task. In particular, we investigate how to drastically reduce runtime on clustering and retrieval tasks while maintaining relative model rankings. This appendix provides full details of the approach described in Section 2.3.2."}, {"title": "C.1 SPEEDING UP TASKS"}, {"title": "C.1.1 CLUSTERING", "content": "Task Biorxiv P2P Spearman Speedup Biorxiv S2S 0.9505 31.50x 0.9890 14.31x Medrxiv P2P 0.9615 21.48x Medrxiv S2S"}]}