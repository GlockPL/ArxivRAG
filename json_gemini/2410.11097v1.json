{"title": "DMDSPEECH: DISTILLED DIFFUSION MODEL SURPASSING THE TEACHER IN ZERO-SHOT SPEECH SYNTHESIS VIA DIRECT METRIC OPTIMIZATION", "authors": ["Yingahao Aaron Li", "Rithesh Kumar", "Zeyu Jin"], "abstract": "Diffusion models have demonstrated significant potential in speech synthesis tasks, including text-to-speech (TTS) and voice cloning. However, their iterative denoising processes are inefficient and hinder the application of end-to-end optimization with perceptual metrics. In this paper, we propose a novel method of distilling TTS diffusion models with direct end-to-end evaluation metric optimization, achieving state-of-the-art performance. By incorporating Connectionist Temporal Classification (CTC) loss and Speaker Verification (SV) loss, our approach optimizes perceptual evaluation metrics, leading to notable improvements in word error rate and speaker similarity. Our experiments show that DMDSpeech consistently surpasses prior state-of-the-art models in both naturalness and speaker similarity while being significantly faster. Moreover, our synthetic speech has a higher level of voice similarity to the prompt than the ground truth in both human evaluation and objective speaker similarity metric. This work highlights the potential of direct metric optimization in speech synthesis, allowing models to better align with human auditory preferences.", "sections": [{"title": "INTRODUCTION", "content": "Text-to-speech (TTS) technology has witnessed remarkable progress over the past few years, achieving near-human or even superhuman performance on various benchmark datasets (Tan et al., 2024; Li et al., 2024a; Ju et al., 2024). With the rise of large language models (LLMs) and scaling law (Kaplan et al., 2020), the focus of TTS research has shifted from small-scale datasets to large-scale models trained on tens to hundreds of thousands of hours of data encompassing a wide variety of speakers (Wang et al., 2023a;c; Shen et al., 2024; Peng et al., 2024; \u0141ajszczak et al., 2024; Li et al., 2024b). Two primary methodologies have emerged for training these large-scale models: diffusion-based approaches and autoregressive language modeling (LM)-based methods. Both frameworks enable end-to-end speech generation without the need for hand-engineered features such as prosody and duration modeling as seen in works before the LLM era (Ren et al., 2020; Kim et al., 2021), simplifying the TTS pipeline and improving scalability.\nDiffusion-based speech synthesis models have demonstrated superior robustness compared to LM-based approaches (Le et al., 2024; Lee et al., 2024). By generating all speech tokens simultaneously rather than sequentially, diffusion models avoid the error accumulation inherent in autoregressive models and offer faster generation times for longer sentences since their inference speed is not directly proportional to speech length (Ju et al., 2024). However, a significant drawback of diffusion models is their reliance on iterative sampling processes, which can be computationally intensive and time-consuming (Liu et al., 2024). Additionally, unlike LM-based models that directly estimate the likelihood of the output token distribution, diffusion models focus on estimating the score function rather than generating speech tokens end-to-end (Yang et al., 2023). This characteristic makes it inconvenient to optimize directly on the target distribution space using techniques such as perceptual"}, {"title": "RELATED WORKS", "content": "Zero-Shot Text-to-Speech Synthesis\nZero-shot TTS generates speech in an unseen speaker's voice using a small reference sample without additional training. Initial methods relied on speaker embeddings from pre-trained encoders (Casanova et al., 2022; 2021; Wu et al., 2022; Lee et al., 2022) or end-to-end speaker encoders (Li et al., 2024a; Min et al., 2021; Li et al., 2022; Choi et al., 2022), which required extensive feature engineering and struggled with generalization, limiting scalability. More recently, prompt-based methods using in-context learning with reference prompts have scaled models using autoregressive (Shen et al., 2024; Le et al., 2024; Ju et al., 2024; Lee et al., 2024; Yang et al., 2024; Eskimez et al., 2024; Liu et al., 2024) and diffusion frameworks (Jiang et al., 2023b; Wang et al., 2023a;c; Jiang et al., 2023a; Peng et al., 2024; Kim et al., 2024; Chen et al., 2024b; Meng et al., 2024; Yang et al., 2024; Lovelace et al., 2023; Liu et al., 2024). While these models scale well, they suffer from slow inference due to iterative sampling. Our model, DMDSpeech, addresses this by combining the scalability of prompt-based methods with the efficiency of non-iterative sampling. Through distribution matching distillation, we transform a diffusion-based TTS model into a student model capable of generating speech in a few steps, accelerating inference and enabling direct metric optimization for state-of-the-art speaker similarity and speech quality.\nDiffusion Distillation\nDiffusion models generate high-quality audio but suffer from slow inference due to iterative sampling (Popov et al., 2021). Diffusion distillation accelerates this process by training a student model to efficiently replicate the teacher's behavior. Previous methods approximated the teacher's ODE sampling trajectories. ProDiff (Huang et al., 2022) used progressive distillation (Salimans & Ho, 2022) to reduce sampling steps, while CoMoSpeech (Ye et al., 2023) and FlashSpeech (Ye et al., 2024) employed consistency distillation (Song et al., 2023). Rectified flow methods, such as in VoiceFlow (Guo et al., 2024) and ReFlow-TTS (Guan et al., 2024), aimed to accelerate sampling by straightening sampling paths. However, these methods often compromise quality by forcing the student to follow the teacher's path, which may not suit its reduced capacity. An alternative is distribution matching, either adversarially (Sauer et al., 2023; 2024) or via score function matching"}, {"title": "METHODS", "content": "Our model starts with a pre-trained teacher model based on an end-to-end latent speech diffusion framework such as SimpleTTS (Lovelace et al., 2023) and DiTTo-TTS (Lee et al., 2024). This section outlines the formulation of the diffusion process, noise scheduling, and the objective function.\nWe begin by encoding raw audio waveforms $y \\in \\mathbb{R}^{1\\times T}$, where T is the audio length, into latent representations $x_0 = E(y)$ using a latent autoencoder E. The latent autoencoder follows DAC (Kumar et al., 2024) with residual vector quantization replaced by the variational autoencoder loss (see Appendix C.1 for more information). We denote the ground truth latent distribution as $P_{data}$. The diffusion process involves adding noise to $x_o \\sim P_{data}$ over continuous time $t \\in [0, 1]$ through a noise schedule. Our noise schedule follows Lovelace et al. (2023), which is a shifted cosine noise schedule formulated with $a_t$ and $\\sigma_t$ that control the amount of signal and noise (see Appendix C.2.1).\nDuring training, the model learns to remove noise added to the latent representations. Given a latent variable $x_0$ and noise $\\epsilon \\sim N(0, I)$, the noisy latent $x_t$ at time step t is generated as $x_t = A_tX_0 + \\sigma_t\\epsilon$. We use a binaray prompt mask m to selectively preserve the original values in regions corresponding to the prompt. The noisy latent $x_t$ is adjusted as $x_t \\leftarrow x_t \\odot (1 - m) + x_0 \\odot m$, where $\\odot$ denotes element-wise multiplication. The binary mask m is randomly sampled to mask between 0% to 50% of the length of $x_0$. We define a reparameterized velocity $v = \\frac{a_t}{\\sigma_t} x_0$, which serves as the training objective as in Huang et al. (2022). We train our diffusion transformer (Peebles & Xie, 2023) model $f_\\phi$, parameterized by $\\phi$, to predict the target v given the noisy latent $x_t$, conditioned on text embeddings c, prompt mask indicators m, and the time step t:\n\n$L_{diff}(f_\\phi; P_{data}) = \\mathbb{E}_{x_0 \\sim P_{data}, t \\sim U(0,1), \\epsilon \\sim N(0,1)}[||v - f_\\phi(x_t ; c, m, t)||^2]$.\n\nDuring inference, the model takes noise $z \\in N(0, I)$ with fixed size $[d, L]$ where L is the total duration of the target speech. L is estimated by multiplying the number of phonemes in the target text with the speaking rate of the prompt speech (see Appendix C.2.3 for more implementation details)."}, {"title": "IMPROVED DISTRIBUTION MATCHING DISTILLATION", "content": "We employ improved Distribution Matching Distillation (Yin et al., 2024a), or DMD 2, to distill our teacher model for fast sampling and direct metric optimization. DMD 2 improves upon DMD (Yin et al., 2024b) by incorporating adversarial training on the real data, eliminating the need for noise-data"}, {"title": "TRAINING OBJECTIVES AND STABILITY", "content": "The overall training objective for $G_\\theta$ combines DMD and adversarial losses with SV and CTC losses:\n\n$\\min_{\\theta} \\mathcal{L}_{DMD} + \\lambda_{adv}\\mathcal{L}_{adv}(G_\\theta; D) + \\lambda_{sv}\\mathcal{L}_{sv} + \\lambda_{CTC}\\mathcal{L}_{CTC}$,\n\nand the training objectives for $g_\\psi$ and D are:\n\n$\\min_{\\psi} \\mathcal{L}_{diff}(g_\\psi; P_\\theta)$,\\\n$\\min_{D} \\mathcal{L}_{adv} (D; G_\\theta)$.\n\nWe employ an alternating training strategy where the student generator $G_\\theta$, the student score estimator $g_\\psi$, and the discriminator D are updated at different frequencies to maintain training stability. Specifically, for every update of $G_\\theta$, we perform five updates of $g_\\psi$. This ensures that the score estimator $g_\\psi$ can adapt quickly to the dynamic changes in the generator distribution $p_\\theta$. Unlike Yin et al. (2024a), where D are updated five times for every single update of $G_\\theta$, we update D and $G_\\theta$ at the same rate. This prevents the discriminator from becoming too powerful and destabilizing training.\nThe learning rates for $G_\\theta$ and $g_\\psi$ play a critical role in maintaining training stability since both models are initialized from the teacher's parameters, $\\phi$. Treating this as a fine-tuning process, we set their learning rates close to the teacher model's final learning rate to prevent catastrophic forgetting and training collapse. The teacher model was trained using a cosine annealing warmup scheduler, which gradually reduced the learning rate over time. Thus, starting with a high learning rate for $G_\\theta$ and $g_\\psi$ can cause them to deviate significantly from the pre-trained knowledge, leading to training failure. Conversely, the learning rate for D is less sensitive and does not require such precise tuning.\nBalancing the different loss components in the overall objective function is crucial for successful training. The primary loss, $\\mathcal{L}_{DMD}$, is responsible for transferring knowledge from the teacher model, aligning the synthesized speech with the text. Other losses, such as $\\mathcal{L}_{adv}, \\mathcal{L}_{sv}, and \\mathcal{L}_{CTC}$, need to be scaled properly to match the gradient of $\\mathcal{L}_{DMD}$. We set $\\lambda_{adv} = 10^{-3}$ to ensure the gradient norm of $\\mathcal{L}_{adv}$ is comparable to that of $\\mathcal{L}_{DMD}$. During early training stage, we observed that the gradient norms of $\\mathcal{L}_{sv}$ and $\\mathcal{L}_{CTC}$ were significantly higher than $\\mathcal{L}_{DMD}$, likely because $G_\\theta$ was still learning to generate intelligible speech from single step. To address this, we set $\\lambda_{CTC} = 0$ for the first 5,000 iterations and $\\lambda_{sv} = 0$ for the first 10,000 iterations. This allows $G_\\theta$ to stabilize under the influence of $\\mathcal{L}_{DMD}$ before integrating these additional losses. After that, both $\\lambda_{CTC}$ and $\\lambda_{sv}$ are set to 1."}, {"title": "EXPERIMENTS", "content": "We conducted our experiments on the LibriLight dataset (Kahn et al., 2020), which consists of 57,706.4 hours of audio from 7,439 speakers. The data and transcripts were obtained using Python scripts provided by the LibriLight authors\u00b9. All audio files were resampled to 48 kHz to match the configuration of our DAC autoencoder, and the text was converted into phonemes using Phonemizer (Bernard & Titeux, 2021). To manage memory constraints, we segmented the audio into 30-second chunks using WhisperX (Bain et al., 2023). The teacher model $f_\\phi$ was trained for 400,000 steps with a batch size of 384, using the AdamW optimizer (Loshchilov & Hutter, 2018) with $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, weight decay of $10^{-2}$, and an initial learning rate of $10^{-4}$. The learning rate followed a cosine decay schedule with a 4,000-step warmup, gradually decreasing to $10^{-5}$. Model weights were updated using an exponential moving average (EMA) with a decay factor of 0.99 every 100 steps. The teacher model consists of 450M parameters in total. For student training, we initialized both the student generator $G_\\theta$ and the student score model $g_\\psi$ with the EMA-weighted teacher parameters. The initial learning rate was set to match the final learning rate of the teacher model ($\\lambda = 10^{-5}$), while the batch size was reduced to 96 due to memory constraints. Reducing the batch size further negatively impacted performance, as a sufficiently large batch size is required for accurate score estimation due to the Monte Carlo nature of DMD (see Section 4.4 for further discussion). The student generator $G_\\theta$ and the discriminator D were trained for an additional 40,000 steps, and the student score model $g_\\psi$ for 200,000 steps accordingly using the same optimization settings as the teacher. All models were trained on 24 NVIDIA A100 40GB GPUs."}, {"title": "EVALUATION METRICS", "content": "We performed both subjective and objective evaluations to assess the performance of our model and several state-of-the-art baselines. For subjective evaluation, we employed four metrics rated on a scale from 1 to 5. The Mean Opinion Score for Naturalness (MOS-N) assessed the human-likeness of the synthesized speech, where 1 indicates fully synthesized audio and 5 indicates completely human speech. The Mean Opinion Score for Sound Quality (MOS-Q) evaluated audio quality degradation relative to the prompt, with 1 representing severe degradation and 5 indicating no degradation. The Similarity Mean Opinion Score for Voice (SMOS-V) measured the similarity of the synthesized voice to the prompt speaker's voice, where 1 means completely different and 5 means identical. Lastly, the Similarity Mean Opinion Score for Style (SMOS-S) assessed the speaking style similarity to the prompt speaker with the same scale. These subjective evaluations were conducted through a listening test survey on the crowdsourcing platform Prolific, with 1,000 tests (30 samples each) taken by native English speakers with no hearing impairments who had experience in content creation or audio/video editing, ensuring they could better differentiate synthesized audio from real human. The prompt speech served as an anchor that is supposed to score 5 on all metrics; we also included intentionally mismatched speakers serving as low anchor for similarity, which should have a rating lower than 3. The participants who fails to correctly rate the anchors hidden in the test are disqualified and their answers removed (details in Appendix E.2). For objective evaluation, we followed the approach from previous works (Wang et al., 2023a; Lee et al., 2024) and measured speaker similarity using the cosine similarity between speaker embeddings of the generated speech and the promot (SIM), using the WavLM-TDCNN speaker embedding model\u00b2. We also calculated the Word Error Rate (WER) with a CTC-based HuBERT ASR model \u00b3 following (Ju et al., 2024; Shen et al., 2024)."}, {"title": "COMPARISON TO OTHER MODELS", "content": "We conducted two evaluation experiments to compare our models against two categories of baselines: state-of-the-art (SOTA) non-end-to-end (E2E) models that include explicit duration and prosody"}, {"title": "ABLATION STUDY", "content": "We conducted ablation studies to assess the contribution of each proposed component, with results summarized in Table 4. We evaluated models trained solely with DMD 2 using one sampling step (DMD 2 only, N=1) and four sampling steps (DMD 2 only, N=4), as well as models trained with only CTC loss or SV loss on top of four-step DMD 2 model. Additionally, we examined the impact of reducing the batch size from 96 to 16 (B. S. 96 \u2192 16). The ablation study used the same 80 samples for subjective evaluation as in the second experiment and 3,711 samples for objective evaluation. To measure the trade-off between speech diversity and model capacity, we included the coefficient of variation of pitch (CVfo). This metric was calculated by synthesizing speech with the same text and prompt 50 times and computing the coefficient of variation of the frame-level F0 values averaged over the speech frames. The final results reported were averaged over 40 prompts from the LibriSpeech test-clean subset, covering all 40 speakers.\nEffects of Distribution Matching Distillation\nUsing a single sampling step resulted in significantly degraded performance compared to the full DMDSpeech model. While increasing to four sampling steps improved naturalness and sound quality to approach the teacher model's level, speaker similarity remained significantly lower. Interestingly, the speaker verification model's SIM score showed only a slight decrease, suggesting a phenomenon we term mode shrinkage (Figure 2), where distillation emphasizes high-probability regions of the data distribution. This focus can result in a more generic speaker profile, reducing perceived uniqueness in the prompt speaker's voice, while maintaining global speaker features as reflected in the SIM score. To address this, we introduced speaker verification loss in this work to better capture the distinct characteristics of the prompt speaker.\nMode shrinkage also led to reduced diversity, as indicated by a lower CVfo across student models compared to the teacher. There is also a trade-off between diversity and sample quality, as one-step student obtained close-to-teacher diversity despite its lowest sample quality. However, as shown in Figure 5, this reduction in diversity applies only when synthesizing speech from the same prompt and text. Given that zero-shot TTS is highly conditional, requiring strict adherence to the input text and speaker prompt, this reduction in diversity is not necessarily undesirable. As we found out in the subjective test, MOS-N increases even when diversity decreases. The distilled model achieves sufficient mode coverage across varying prompts and texts while benefiting from direct metric optimization and faster inference. Notably, mode shrinkage also corrected a cut-off issue in the teacher model, which mimicked the cutoff patterns"}, {"title": "CONCLUSIONS", "content": "In this work, we presented DMDSpeech, a distilled diffusion-based text-to-speech (TTS) model based on prompt continuation. By employing distribution matching distillation (DMD), our model generates high-quality speech in just 4 steps and enables direct metric optimization. Through speaker verification (SV) and connectionist temporal classification (CTC) losses, DMDSpeech significantly improves speaker similarity and text-speech alignment, outperforming several state-of-the-art baselines.\nThe ability to directly optimize any differentiable metrics offers substantial progress in bridging the gap between generative modeling and human perception. As these metrics continue to improve, the alignment with human auditory preferences is expected to strengthen. This creates promising future directions, such as using reinforcement learning from human feedback to further improve TTS systems. Additionally, developing new differentiable metrics that better capture human perception could provide more robust optimization targets, aligning models more closely with human preferences.\nHowever, DMDSpeech raises important ethical concerns. Our model has demonstrated the ability to generate speech with higher perceived similarity to the prompt than real utterances by the same speaker, as judged by both human listeners and speaker verification systems. This highlights limitations in current speaker verification models and presents risks of misuse, such as deepfake generation. To mitigate these risks, more advanced speaker verification techniques capable of distinguishing"}, {"title": "MODE SHRINKAGE", "content": "To further explore the effects of mode shrinkage, we conducted experiments on unconditional diversity and mode coverage. Specifically, we used a continuation task where the model was asked to generate speech following a truncated prompt with its full text transcription, allowing us to compare the generated speech to its corresponding ground truth from real speakers. We evaluated two key aspects of speech: pitch (F0) and energy. As shown in Figure 5, the student model closely matches the teacher's distribution in both F0 and energy, demonstrating minimal mode shrinkage in contrast to the results shown in Figure 2, where mode shrinkage was evident.\nWe further assessed the model's mode coverage quantitatively by calculating the Wasserstein distance between the student and teacher models, as well as the ground truth, in terms of pitch (F0) and energy. The Wasserstein distances $W_{f0}$ (for pitch) and $W_{E}$ (for energy) were computed across all 40 speakers in the LibriSpeech test-clean subset. Additionally, we compared the Wasserstein distance between the student and teacher $W(p_{\\theta}, p_{\\phi})$ in both conditional and unconditional settings. The conditional case involved synthesizing speech 50 times with the same text and prompt, while the unconditional case used varying texts and prompts from the same speaker as a speech continuation task.\nAs shown in Table 5, the difference between the student and teacher in terms of Wasserstein distance to the ground truth is relatively small in the unconditional case, and the distance between the student and teacher is much smaller compared to the conditional case (2.55 vs. 16.53). This suggests that the reduction in diversity, or mode shrinkage, primarily occurs in the conditional setting (i.e., when"}, {"title": "ADDITIONAL EVALUATION RESULTS", "content": "We conducted additional evaluations of acoustic features that capture emotional nuances in speech, following Li et al. (2022), focusing on pitch (mean and standard deviation), energy (mean and standard deviation), Harmonics-to-Noise Ratio (HNR), jitter, and shimmer.\nTable 6 compares our model with several baselines. Our model consistently outperforms others across all metrics, except for energy mean, likely due to data normalization during preprocessing, which scales audio between -1 and 1, misaligning the energy with the prompt. Nevertheless, our model's higher scores across other features demonstrate its capability to reproduce the emotional content of the prompt speech effectively.\nIn the ablation study presented in Tables in 7, we compare the impact of different training strategies on preserving emotional content in synthesized speech. The teacher model shows strong correlations for most acoustic features, while DMD 2 only models demonstrate performance improvements with additional sampling steps, similar to SIM results in Table 4. Adding CTC loss improves word error rate (WER) but does not significantly enhance speaker-related features. However, including SV loss significantly improves speaker-related features, with the model trained with SV loss only achieving the highest scores in multiple metrics, such as pitch mean (0.94), HNR (0.87), and shimmer (0.65). This highlights the importance of SV loss in capturing speaker identity and emotional content.\nFinally, reducing the batch size from 96 to 16 resulted in a slight performance drop across most metrics, demonstrating the importance of maintaining a larger batch size for optimal performance in distribution matching distillation."}, {"title": "IMPLEMENTATION DETAILS", "content": "We utilize a latent audio autoencoder to compress raw waveforms into compact latent representations for diffusion modeling. Our architecture follows the DAC model proposed by Kumar et al. (2024), with a key modification to use a variational autoencoder (VAE) bottleneck instead of residual vector quantization, enabling continuous latent spaces and end-to-end differentiable training.\nThe DAC consists of an encoder E, a VAE bottleneck, and a decoder D. The encoder maps the input waveform $y \\in \\mathbb{R}^{1\\times T}$ into a latent representation $x \\in \\mathbb{R}^{C\\times L}$, where C and L denote channels and downsampled temporal resolution. The VAE bottleneck introduces stochasticity by modeling x as a distribution, and the decoder reconstructs the waveform by minimizing the reconstruction loss.\nThe encoder applies an initial convolution followed by residual units with dilated convolutions at scales 1, 3, 9 to capture multi-scale temporal features. After each block, strided convolutions reduce the temporal resolution by a factor of 1200. For 48 kHz audio, the encoded latent is 40 Hz, making it ideal for efficient speech synthesis tasks. The latent channel dimension of our autoencoder is C = 64.\nThe encoder's output is split into mean $\\mu$ and scale $\\sigma$ parameters:\n\n$z = \\mu + \\sigma\\odot\\epsilon, \\epsilon \\sim N(0, 1)$,\n\nwhere z is sampled using the reparameterization trick (Kingma, 2013). The decoder mirrors the encoder with transposed convolutions and residual units to upsample latent representations back to the original waveform $\\hat{y} = D(z)$, where $\\hat{y}$ is the reconstructed waveform. The encoder and decoder architectures are the same as DAC (Kumar et al., 2024).\nThe KL divergence between the approximate posterior $q(z|y)$ and prior $p(z)$ is computed as:\n\n$\\mathcal{L}_{KL} = E_{y} [\\frac{1}{N} \\sum_{i=1}^{N} (\\mu_i^2 + \\sigma_i^2 - log \\sigma_i - 1) \\cdot m_i]$,\n\nwhere N is the number of channels, and $m_i$ is the channel mask. The autoencoder is trained to minimize a combination of reconstruction loss and KL divergence:\n\n$\\mathcal{L}_{AE} = E_{y} [||y - \\hat{y}||_1] + \\lambda_{KL}\\mathcal{L}_{KL}$,\n\nwhere $\\lambda_{KL} = 0.1$ to balance the KL loss. In addition to the KL loss, we also employ adversarial training following Kumar et al. (2024) with the complex STFT discriminator."}, {"title": "DMDSPEECH", "content": "In this section, we present the implementation details of our DMDSpeech model, including the noise schedule, gradient calculation of DMD loss, detailed architecture, and sampling algorithm."}, {"title": "SHIFTED COSINE NOISE SCHEDULE", "content": "We follow Lovelace et al. (2023); Hoogeboom et al. (2023) and use the shifted cosine noise schedule with $a_t$ and $\\sigma_t$ denoting the amount of signal and noise at time t. The noise-to-signal ratio (SNR) $A_t = a_t/\\sigma_t$ of the noise schedule is shifted by a factor s, from which the shifted SNR $A_{t,s}$ and noise schedule $a_{t,s},\\sigma_{t,s}$ are defined:\n\n$a_t = cos(\\frac{\\pi}{2} (t+s))$,\n\n$A_{t,s} = \\frac{a_{t,s}}{\\sigma_{t,s}}$,\n$\\sigma_{t,s} = \\sqrt{\\frac{a_{t,s}^2}{A_{t,s}^2}}$,\n\nUsing the fact $a_t = sigmoid(log(A_t))$ as stated in Kingma et al. (2021), the shifted noise schedule can then be computed in the log space for numerical stability:\n\n$a_{t,s} = sigmoid(log(A_t) + 2log(s))$,\n$\\sigma_{t,s} = \\sqrt{1-a_{t,s}^2}$,\n\nLower s emphasizes the higher noise levels and can potentially improve the model's performance. We set s = 0.5 following Lovelace et al. (2023) as it is shown to produce the most robust results."}, {"title": "GRADIENT CALCULATION OF DMD LOSS", "content": "The gradient of the DMD loss with respect to the generator parameters $\\theta$ is given by eq. 4. The actual implementation of gradient calculation follows the following steps.\nWe first sample latent variables $x_t$ are generated via forward diffusion process as:\n\n$x_t = A_tX_0 + \\sigma_t\\epsilon$,\n\nwhere $x_0$ is the clean latent representation, and $\\epsilon \\sim N(0, I)$.\nThe clean latents $x_0^{real}$ and $x_0^{fake}$ then are estimated using the predicted noise by both of the teacher $f_\\phi$ and student $g_\\psi$ diffusion models following eq. 7 and eq. 8, respectively.\nFrom there, we calculate the numerical gradient of $\\mathcal{L}_{DMD}$. We define the following quantity as the difference between the ground truth clean latent and estimated latents:\n$\\rho_{real} = x_0 - x_0^{real}$,\\\n$\\rho_{fake} = x_0 - x_0^{fake}$,\n\nThen the difference in score $\\Delta$ (numerical gradient) can be calculated as:\n$\\Delta = w_t \\nabla_{x_t} (s_{real} - s_{\\theta})$\n\n$= w_t \\nabla_{x_t} (\\frac{x_t-a_t x_0^{real}}{\\sigma_t^2} - \\frac{x_t-a_t x_0^{fake}}{\\sigma_t^2})$\n\n$= w_t \\frac{a_t}{\\sigma_t^2} (x_0^{real} - x_0^{fake})$\n\nwhere the weighting factor $w_t$ is defined as:\n$w_t = \\frac{\\sigma_t^2}{\\alpha_t || x_0 - x_0^{real} ||_1} = \\frac{\\sigma_t^2}{\\alpha_t || \\rho_{real} ||_1}$\n\nHence, eq. 28 can be written as:\n\n$\\Delta = \\frac{a_t}{\\sigma_t^2} (\\rho_{real} - \\rho_{fake})$\n\nwhich is back-propagated to $G_{\\theta}$ via gradient descent algorithm."}, {"title": "DETAILED ARCHITECTURE", "content": "In this section, we present the architecture of our Diffusion Transformer (DiT) model (Peebles & Xie, 2023). The DiT model integrates diffusion processes with transformer architectures to generate high-quality speech representations conditioned on textual input.\nOur DiT model consists of the following key components:"}, {"title": "DMD SAMPLING", "content": "Our sampling algorithm of the student (DMDSpeech) is similar to that of the consistency model (Song et al., 2023). The sampling procedure is outlined in Algorithm 1."}, {"title": "LATENT CTC-BASED ASR MODEL", "content": "To directly optimize word error rate (WER) within our speech synthesis framework, we implement a Connectionist Temporal Classification (CTC)-based ASR model that operates on latent speech representations. Traditional ASR models work on raw audio or mel-spectrograms, adding computational overhead and potential mismatches when integrated with latent-based synthesis since we need to decode the latent back into waveforms before computing the ASR output. Our latent ASR model processes these representations directly, enabling efficient, end-to-end computation of the CTC loss and direct WER optimization.\nThe ASR model is based on the Conformer architecture (Gulati et al., 2020), which effectively captures local and global dependencies using convolution and self-attention. Input latent representations $z \\in \\mathbb{R}^{T\\times d}$ are processed through a 6-layer conformer stack and the model outputs a logit for each latent token over IPA phonemes.\nThe ASR model is trained using the CTC loss, allowing alignment-free training of sequence-to-sequence models. The CTC loss is defined using softmax function:\n\n$\\mathcal{L}_{CTC} = - log p(y | o)$,\n\nwhere y is the target IPA sequence, o represents the logits over the IPA symbols, and $p (y | o)$ is computed by summing over all valid alignments between the input and target sequences. The probabilities are calculated as:\n$P_{\\pi_c} (t) = \\frac{exp (o_{t,\\pi_c})}{\\sum_{k=1}^{V} exp (o_{t,k})}$\n\nWe trained our ASR model on CommonVoice (Ardila et al., 2019) and LibriLight (Kahn et al., 2020) datasets for 200k steps with the AdamW (Loshchilov & Hutter, 2018) optimizer. The optimizer configuration is the same as teacher training described in Section 4.1."}, {"title": "LATENT SPEAKER VERIFICATION MODEL", "content": "We develop a latent speaker verification (SV) model that operates directly on latent speech representations in order to optimize speaker similarity within our speech synthesis framework. Unlike traditional SV models, which process raw audio waveforms, our latent SV model integrates seamlessly with our latent-based synthesis, enabling efficient, end-to-end computation of speaker verification loss for direct speaker similarity optimization.\nOur latent SV model fine-tunes our CTC-based ASR model for feature extraction following (Cai & Li, 2024) and integrates it with an ECAPA-TDNN architecture (Desplanques et al., 2020) for"}, {"title": "HUMAN RATING CORRELATIONS", "content": "We generated scatter plots to visualize the relationships between the four subjective metrics: MOS-N (naturalness)", "metrics": "word error rate (WER)"}]}