{"title": "ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression", "authors": ["Botao Zhao", "Xiaoyang Qu", "Zuheng Kang", "Junqing Peng", "Jing Xiao", "Jianzong Wang"], "abstract": "In deep regression, capturing the relationship among continuous labels in feature space is a fundamental challenge that has attracted increasing interest. Addressing this issue can prevent models from converging to suboptimal solutions across various regression tasks, leading to improved performance, especially for imbalanced regression and under limited sample sizes. However, existing approaches often rely on order-aware representation learning or distance-based weighting. In this paper, we hypothesize a linear negative correlation between label distances and representation similarities in regression tasks. To implement this, we propose an angle-compensated contrastive regularizer for deep regression, which adjusts the cosine distance between anchor and negative samples within the contrastive learning framework. Our method offers a plug-and-play compatible solution that extends most existing contrastive learning methods for regression tasks. Extensive experiments and theoretical analysis demonstrate that our proposed angle-compensated contrastive regularizer not only achieves competitive regression performance but also excels in data efficiency and effectiveness on imbalanced datasets.", "sections": [{"title": "Introduction", "content": "Regression, a fundamental machine learning task, is employed when the prediction target is continuous. In the last ten years, deep regression has emerged as a more effective approach than traditional regression methods across various domains, including computer vision (Wang, Sanchez, and Li 2022), natural language process (Chandrasekaran and Mago 2021). Recent studies in deep regression have predominantly focused on developing network structures for specific application scenarios (Lee et al. 2021; Chen, Ma, and Lin 2021; Tabelini et al. 2021). However, these task-specific architectures are difficult to adapt to different tasks or domains, which remains a non-trivial research question.\n\nA in-depth examination of deep regression by (Lathuili\u00e8re et al. 2019) revealed that a well-tuned general-purpose network can achieve results close to the state-of-the-art (SOTA) models, potentially obviating the need for more intricate and specialized regression models. In a standard deep regression learning (SDL) task, the representation is first extracted by the backbone network and subsequently processed by passing through a fully connected layer to generate the prediction for the target. The mean squared error (MSE) loss (Lathuili\u00e8re et al. 2019) is broadly acknowledged as the most frequently utilized loss function in deep regression. Additionally, researchers have explored alternative loss functions, including L1 loss, Huber loss (Huber 1992), Tukey loss (Belagiannis et al. 2015). In an SDL, enhancing the quality of learned representations is a key approach to improving performance.\n\nGiven the continuous nature of regression targets, recent research in representation learning has increasingly focused on capturing the nuanced relationships within the label space. These methods can be broadly categorized into: order-aware (Gong, Mori, and Tung 2022; Zha et al. 2024), and distance-aware approaches (Keramati, Meng, and Evans 2024; Dai et al. 2021). Order-aware methods typically rely on ranking-based techniques to constrain models to obtain order-aware representations. However, these methods have a major flaw (Fig. 1 (A), first row), as regression targets involve not only order information but also distance information (Fig. 1 (A), second row). For example, consider face images with ages 2, 21, 46, and 80; ranking-based methods would generally assign these labels' representations to a scale with equal intervals, which might incorrectly reflect the true relationships among them. Distance-aware methods, on the other hand, aim to maintain the representation similarity among samples proportional to their corresponding label distances. Current distance-aware approaches commonly apply weighted cosine similarity between the anchor and negative pairs within the contrastive learning framework (Keramati, Meng, and Evans 2024; Dai et al. 2021). However, we posit that direct weighting may not be optimal due to the non-linearity of the cosine function. This nonlinearity can lead to unevenly distributed representations on the hypersphere, potentially distorting the relationship between labels and features. In this paper, we introduce angle compensation as a refinement to direct weighting, aiming to advance distance-aware representation learning in regression tasks through a more nuanced and effective approach.\n\nTo this end, we considered building upon supervised contrastive learning (SupCon), an approach that has seen rapid development with numerous notable contributions (Ermolov et al. 2021; Chen et al. 2020; Jiang et al. 2020). As shown in"}, {"title": "Related Work", "content": "Deep Regression\nMany researchers have focused on deep regressions over the past decades. Some researchers formulate regression as a classification problem (Gao et al. 2017; Shi, Cao, and Raschka 2023; Rogez, Weinzaepfel, and Schmid 2017; Pan et al. 2018). However, this formulation fails to fully leverage the relationship among labels and introduces a trade-off between optimization complexity and method performance. Recent advancements have proposed various methods to address imbalance issues in deep regression (Yang et al. 2021; Ren et al. 2022). Although some applications aim to enhance model performance in balanced settings, such as disease prediction, many certain real-world scenarios require models to perform effectively on imbalanced naturally sampled data, where long-tail samples may be considered outliers and potentially disregarded. Several studies have proposed regularizers to constrain the embedding space, including approaches that model uncertainty within the embedding space (Li et al. 2021) and methods that learn higher-entropy feature spaces (Zhang et al. 2022). Recently, the concept of learning ordered features for regression has gained increased attention, with notable contributions such as Rank-N-Contrast (Zha et al. 2024) and RankSIM (Gong, Mori, and Tung 2022). In contrast to existing works, we propose a distance-aware representation learning method capable of capturing relationships among continuous labels in feature space.\n\nContrastive Learning\nContrastive learning is a representation learning technique that projects features onto a hypersphere (Ermolov et al. 2021; Chen et al. 2020; Jiang et al. 2020). The fundamental principle of contrastive learning involves attracting positive sample pairs while repelling negative ones. In self-supervision, positive sample pairs are generally formed by different augmentations of the same sample, while negative pairs are made up of the anchor and other samples within the minibatch. SupCon, an extension of contrastive learning in fully-supervised settings, has recently emerged as a promising approach, demonstrating significant advancements in image recognition and various classification tasks (Chen et al. 2020; Sun et al. 2023; Khosla et al. 2020; Liu et al. 2023; Seifi et al. 2024). In the SupCon framework, positive pairs are defined as samples from the same class, while negative pairs are samples from different classes. However, the adaptation of contrastive learning to regression tasks presents several challenges, particularly in the definition of positive and negative pairs and the capture of relationships within the label space. (Zha et al. 2024) proposed a redefinition of positive and negative pairs to achieve order-aware representation learning for regression, while (Dai et al. 2021) exploited a distance-weighted"}, {"title": "Methodology", "content": "Problem Setting\nIn this paper, we consider the input $(X, Y) = \\{x_i, Y_i\\}_{i=1}^N$. Similar to contrastive learning, our objective is to develop a feature representation network that could map the input $X_i$ to an L2-normalized d-dimensional embedding, $z_i \\in S^{d\u22121}$. Furthermore, we assume that the learned representation $z \\in Z$ should be distributed on the $S^{d\u22121}$ such that each position corresponds to its label, as illustrated in Fig. 1 (C).\n\nGiven the continuous nature of labels, we first partition the label space y into M bins with equal intervals, denoted as $[y_0, y_1), [y_1, y_2), \u2026\u2026\u2026, [y_{M\u22121}, y_M)$. These bins represent the precision of the labels, which can be adjusted according to practical requirements.\n\nWe categorize samples from the same bin as positive pairs, while those from different bins are considered negative pairs. Specifically, for an anchor $x_i$, the set of the positive pairs is denoted as $P(i) := \\{j \\in B|y_i = Y_j, j \u2260 i\\}$, and the negative pairs are defined as $N(i) := \\{j \\in B|y_i \u2260 y_j\\}$. Subsequently, we can obtain a na\u00efve extension of supervised contrastive learning loss function (Dai et al. 2021; Keramati, Meng, and Evans 2024; Khosla et al. 2020) for regression tasks as follows:\n\n$L_i = \\frac{1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp (cos (\u03b8_{i,p}) /\u03c4)}{\\sum_{k \\in N(i) \\cup P(i)} exp (cos (\u03b8_{i,k}) /\u03c4)}$,\n\nwhere, $\u03b8_{i,k}$ denotes the angle between embedding $z_i$ and $z_k$, and $cos (\u03b8_{i,k}) = z_i z_k$. However, this formulation is unsuitable for regression tasks as it fails to capture the inherent relationships among labels. As illustrated in Fig. 1 (C), both samples with labels 46 and 80 are treated equivalently as negatives for an anchor with label 3. Yet, the dissimilarity between the anchor 3 and label 80 is significantly greater than that between the anchor and label 46. Furthermore, due to the inherent monotonic nature of labels in regression tasks, mapping $x_i$ to a complete hypersphere faces great challenges. Therefore, our objective is to develop a model that maps the learned representation z distributed onto a semi-hypersphere, positioning them in accordance with their respective labels.\n\nAngle-Compensated Supervised Contrastive Loss\nWe propose an angle-compensated supervised contrastive loss to achieve our goal of preserving label relationships in feature space. Firstly, we hypothesize that a linear negative correlation exists between label distances and representation similarities, implying that samples' representations should be positioned in alignment with their respective labels. Based on this hypothesis, if we use cosine distance to measure representation similarities, we can derive the ideal angle, $\\theta$, between anchor and negatives as follows:\n\n$\\theta = \\frac{Y_{neg} - Y_{anc}}{max (Y) - min (Y)}\u03c0$,\n\nwhere, $Y_{anc}, Y_{neg}$ denote the label of anchor and negative sample, respectively. For instance, given an anchor with a label of 21 and an age range from 0 to 100, the angles between the anchor and negative samples (3, 26, 54, 80) would be -5.4\u00b0, 32.4\u00b0, 41.4\u00b0, 91.8\u00b0 and 138.6\u00b0, respectively.\n\nSeveral studies have demonstrated that the representations of each class spontaneously collapse to the vertices of a regular simplex when the standard supervised contrastive loss reaches its minimum (Graf et al. 2021; Zhu et al. 2022). The SupCon loss effectively constrains the representations of anchors and negatives to be as far apart as possible within a minibatch, which is equivalent to constraining the included angle $\u03b8 = \u03c0$. This observation forms the basis for the formulation of Eq. 2.\n\n$cos(\\tilde \u03b8) = cos (\\theta + \u03c0 - \\frac{Y_{anc} - Y_{neg}}{max (Y) - min (Y)} \u03c0)$,\n\nBased on the Eq. 1, we will get the angle-compensated supervised contrastive loss for sample i as follows:\n\n$L_{i}^{ac} = -\\frac{1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp (z_i z_p/\u03c4)}{\\sum_{k \\in P(i)} exp (z_i z_k/\u03c4) + \\sum_{m \\in N(i)} exp (cos (\u0398_{i,m})/\u03c4)}$,\n\nwhere, $P(i)$ denotes the set of positive pairs for anchor i, and $N(i)$ represents the negative set. This formulation is equal to the SupCon loss, with the key distinction that it replaces the negative similarity with an angle-compensated version. Considering a minibatch, the optimizition of Eq. 4 aims to drive $\u03b8_{i,m} \u2192 \u03c0$ and make the $\\frac{Y_{neg} - Y_{anc}}{max (Y) - min (Y)}\u03c0$, aligning with our intended goal. Then, the angle-compensated supervised contrastive loss $L_{ACCon}$ is defined as the mean value of $\\{L_{i}^{ac}\\}_{i=1}^{2N}$:\n\n$L_{ACCon} = \\frac{1}{2N} \\sum_{i=1}^{2N} L_{i}^{ac}$,\n\nwhere N is the batch size.\n\nTo compute the $L_{i}^{ac}$, we should determine $cos(\\Theta_{i,m})$. We firstly define the compensation angle based on the Eq. 3:\n$\\varphi = \u03c0(\\frac{Y_{neg} - Y_{anc}}{max (Y) - min (Y)} - 1)$,\n\nSubsequently, we derive $cos (\u0398_{i,m})$ as follows:\n\n$cos (\u0398_{i,m}) = z_i z_m cos(\\varphi) - |sin (\\varphi)| \\sqrt{1 - (z_i z_m)^2} + \\epsilon$,\n\nwhere $\\epsilon$ is the smoothing term introduced to prevent gradient explosion. A detail oderivation of $cos(\\Theta_{i,m})$ is provided in Appendix A1."}, {"title": "Angle-Compensated Supervised Contrastive Loss", "content": "We propose an angle-compensated supervised contrastive loss to achieve our goal of preserving label relationships in feature space. Firstly, we hypothesize that a linear negative correlation exists between label distances and representation"}, {"title": "Deep Regression", "content": "Many researchers have focused on deep regressions over the past decades. Some researchers formulate regression as a classification problem (Gao et al. 2017; Shi, Cao, and Raschka 2023; Rogez, Weinzaepfel, and Schmid 2017; Pan et al. 2018). However, this formulation fails to fully leverage the rela-"}, {"title": "Experiments and Results", "content": "Experiments Setup\nDatasets: To rigorously evaluate our method, we selected three diverse datasets: (1) AgeDB (Moschoglou et al. 2017):\n\nAn age estimation dataset comprising 16,488 facial images; (2) IMDB-WIKI (Rothe, Timofte, and Van Gool 2018): A large-scale facial age dataset containing 523,000 images with corresponding age labels; (3) STS-B (Cer et al. 2017; Wang et al. 2018): A natural language dataset consisting of 7,249 sentence pairs, extracted from the Semantic Textual Similarity (STS) Benchmark. To ensure a comprehensive assessment of our proposed method, we employed 2 distinct sampling strategies for partitioning the datasets into training, validation, and test sets:\n\n(1) Balanced Sampling: Following the benchmark established by (Yang et al. 2021), we partitioned the AgeDB dataset into 12,208 training samples, 2,140 validation samples, and 2,140 test samples. For IMDB-WIKI, we allocated 191,500 images for training and 11,000 images each for validation and testing. From STS-B, we sampled 1,000 pairs each for validation and testing. This benchmark ensures a balanced label distribution in the validation and test sets, as illustrated in Appendix Figure 1. Similar to Yang et al.'s benchmark, we denote these balanced datasets as AgeDB-DIR, IMDB-WIKI-DIR, and\n\n(2) Natural Sampling: We used the same dataset splitting ratio but randomized the division of training, validation, and test datasets, thereby preserving similar label distributions across all three sets (Appendix Figure 1). We denote the datasets obtained through natural sampling as AgeDB-Natural, IMDB-WIKI-Natural and STS-B-Natural.\n\nMetrics: For quantitative evaluation, we utilized a diverse set of metrics including Mean Absolute Error (MAE), Geometric Mean (GM) defined as $(\\prod_{i=1}^{n} e_i)^{\\frac{1}{n}}$ where $e_i$ represents the $L_1$ error for the $i^{th}$ sample, coefficient of determination $R^2 = 1 - \\frac{MSE(y,\\hat{y})}{VAR(y)}$, indicating the proportion of explainable variance, Mean Square Error (MSE), and Pearson correlation. The division of test set into many-shot, medium-shot, and few-shot categories follows the same protocol as outlined in (Yang et al. 2021).\n\nBaselines\nWe compared our method with state-of-the-art (SOTA) approaches under both natural and balanced sampling conditions. To guarantee a fair evaluation, we utilized standard network architectures consistent with those outlined by (Yang et al. 2021). Specifically, ResNet-50 was utilized as the backbone for age estimation tasks, while BiLSTM+GloVe word embeddings were employed for sentence similarity prediction.\n\nFor STS-B-Natural, IMDB-WIKI-Natural, and AgeDB-Natural datasets, our method is compared with, Vanilla: using MAE or MSE loss function; Na\u00efve SupCon: adopting SupCon for deep regression, as in Eq. 1; Adaptive SupCon (AdaSupCon): proposing adaptive-margin contrastive loss for image regression (Dai et al. 2021); RankSIM: exploiting ranking similarity regularization (Gong, Mori, and Tung 2022); BMSE: modifying MSE loss for deep imbalanced regression (Ren et al. 2022); RNC: an order-aware method (Zha et al. 2024); ConR: exploiting contrastive learning for imbalanced regression (Keramati, Meng, and Evans 2024).\n\nFor STS-B-DIR, IMDB-WIKI-DIR, and AgeDB-DIR datasets, our approach was benchmarked against Vanilla,"}, {"title": "Basic Performance", "content": "We implement the vanilla model and five advanced regularizer methods tailored for regression tasks: Na\u00efveSupCon (Khosla et al. 2020), AdaSupCon (Dai et al. 2021), BMSE (Ren et al. 2022), RankSim (Gong, Mori, and Tung 2022), RNC (Zha et al. 2024) and ConR (Keramati, Meng, and Evans 2024). All models were re-trained by us, with training details described in Appendix B. Notably, BMSE proved challenging to train on STS-B-Natural; consequently, its evaluation on this dataset was omitted. As evidenced in Table ??, our method outperformed all compared methods on AgeDB-Natural, STS-Natural, and IMDB-WIKI-Natural datasets. Compared to the Vanilla model, our approach achieved MAE improvements of 4.54%, 9.27%, and 3.72%, respectively. Furthermore, our method attained SOTA performance across all three datasets, with MAE improvements of 1.02%, 6.58%, and 1.91%, respectively.\n\nPerformance for Imbalanced Regression\nWe evaluated the efficacy of our proposed method on imbalanced regression tasks using the AgeDB-DIR, STS-B-DIR, and IMDB-WIKI-DIR datasets. Furthermore, we plugged our approach with BMSE, SqrtINV, Focal-R, and ConR. As shown in Table 2, our method demonstrated significant improvement over the vanilla model, indicating the effectiveness of ACCon. Besides, comparative analysis with LDS, Na\u00efveSupCon, AdaSupCon, ConR, and RankSIM reveals the competitiveness of our approach in addressing deep imbalanced regression."}, {"title": "Ablation Studies", "content": "We conducted a series of ablation studies to analyze the components of our deep regression framework. Initially, we compared the efficacy of using features extracted directly from $f_o (\u00b7)$ versus features obtained after applying a projection layer for downstream tasks. Although it is common practice to utilize features prior to the projection layer (denoted as Before Proj.) for downstream tasks, our results, as presented in"}, {"title": "Appendix A. Proof", "content": "A1.The Derivation of $cos (\\Theta_{i,m})$\nGiven the assumption of ACCon, we assume that the cosine similarity between anchor and negative pairs as follows:\n\n$\\Theta = \\frac{Y_{neg} - Y_{anc}}{max (Y) - min (Y)}\u03c0$,\n\nIn our approach, we modify the standard supervised contrastive loss within a mini-batch. Typically, this loss constrains the representations of anchors and negatives to be as far apart as possible, which is equivalent to setting the included angle bi,m to \u03c0. Based on this principle, we construct the following equation:\n\n$COS (\\tilde \u03b8_{i,m}) = COS (\u0398_{i,m} + \u03c0 - \\frac{Y_{m} - Y_{i}}{max (Y) - min (Y)}\u03c0)$.\n\nwhere is the ideal cosine similarity. Besides, we define the compensated angle \u03c6, which is formulated as:\n$\\varphi = \u03c0(1 - \\frac{Y_{i}}{max (Y) - min (Y)})$.\n\nThen:\n$COS (\\tilde \u03b8_{i,m}) = COS (\u0398_{i,m} + \\varphi)$\n\n$= cos (\u0398_{i,m}) cos(\\varphi) - sin (\u0398_{i,m}) sin(\\varphi)$\n\n$= cos (\u0398_{i,m}) cos(\\varphi) + sin (\\varphi) \\sqrt{1 - cos (\u0398_{i,m})^2}$\n\nBecause we have supposed the optimized $\u0398_{i,m} = \u0398_{i,m} + \\varphi = \u03c0$, the above formulation could be approximated to:\n\n$COS (\\tilde \u03b8_{i,m}) = COS (\u0398_{i,m}) cos(\\varphi) - |sin (\\varphi)| \\sqrt{1 - cos (\u0398_{i,m})^2}$\n\nIn the deep regression framework, we have:\n\n$cos (\u0398_{i,m}) = z_i z_m$\n\nwhere $z_i, z_m$ are L2-normalized feature representations. Then, we have:\n\n$cos(\u0398_{i,m}) = z_i z_m cos(\\varphi) - |sin(\\varphi)| \\sqrt{1 - (z_i z_m)^2}$"}, {"title": "Qualitative Visualization of Feature Space", "content": "We employed t-SNE to visualize the extracted feature representations of our method and the compared baselines on the AgeDB-Natural test dataset, as illustrated in Fig. 3 (a). Notably, the representations extracted by our method exhibit continuity, compactness, and relative symmetry in low-dimensional space, corresponding well to the label space. Specifically, RankSIM, BMSE, and our method demonstrate the capability to obtain continuous and compact features in low-dimensional space, consistent with their design to preserve label relationships.\n\nCorrelation Between Feature Representation Similarity and Label Distance To further investigate the learned representations, we present a joint distribution analysis of $cos (\u0398_{i,j})$ and the normalized label distance $|y_i - Y_j|/100$ on the AgeDB-Natural test dataset, as shown in Fig. 3 (b). The extracted representations by our proposed method exhibit a negative correlation with the normalized label distance, achieving a Pearson correlation of -0.735. This indicates that our method effectively captures feature representations with inherent label ordering information, aligning with our initial motivation. The ideal cosine distance among feature representations should be distributed in the range [-1,1], whereas in our method, it mostly falls within [0.3, 1]. We attribute this"}]}