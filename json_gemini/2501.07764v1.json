{"title": "Deep Learning for Disease Outbreak Prediction: A Robust Early Warning Signal for Transcritical Bifurcations", "authors": ["Reza Miry", "Amit K. Chakraborty", "Russell Greiner", "Mark A. Lewis", "Hao Wang", "Tianyu Guan", "Pouria Ramazi"], "abstract": "Early Warning Signals (EWSs) are vital for implementing preventive measures before a disease turns into a pandemic. While new diseases exhibit unique behaviors, they often share fundamental characteristics from a dynamical systems perspective. Moreover, measurements during disease outbreaks are often corrupted by different noise sources, posing challenges for Time Series Classification (TSC) tasks. In this study, we address the problem of having a robust EWS for disease outbreak prediction using a best-performing deep learning model in the domain of TSC. We employed two simulated datasets to train the model: one representing generated dynamical systems with randomly selected polynomial terms to model new disease behaviors, and another simulating noise-induced disease dynamics to account for noisy measurements. The model's performance was analyzed using both simulated data from different disease models and real-world data, including influenza and COVID-19. Results demonstrate that the proposed model outperforms previous models, effectively providing EWSs of impending outbreaks across various scenarios. This study bridges advancements in deep learning with the ability to provide robust early warning signals in noisy environments, making it highly applicable to real-world crises involving emerging disease outbreaks.", "sections": [{"title": "1 Introduction", "content": "Disease outbreaks caused significant burdens in the past decades, not only in terms of the global death counts but also by triggering economic crisis [1-3]. Having an early warning helps with controlling the impact of the outbreak by applying preventive measures [4-6]. Statistical indicators, such as variance and lag-1 autocorrelation, are widely used for early warning signals (EWSs) which are known as generic Early Warning Indicators (EWIs) [7, 8]. However, their performance varies with the type of noise involved in a system and they struggle to capture behaviors beyond the tipping point [9]. Additionally, these indicators typically produce signals only near the state change, making earlier detection challenging [10].\nFrom a machine learning perspective, generating an EWS by processing the outputs of a dynamical system, such as the number of infected cases, is a Time Series Classification (TSC) task. Machine learning models have achieved state-of-the-art performance in TSC and other fields [11]. Several models have been used to predict EWS for dynamical systems [10, 12, 13]. However, the predictive power of these models comes at the cost of requiring large datasets, which are often unavailable for emerging diseases. Furthermore, using historical data from past disease outbreaks also might result in a model that does not generalize well to new diseases.\nOne approach to address this challenge is to simulate training data using known dynamic models [10, 12]. Bury et al. [10] simulated training data using a two-dimensional dynamical model, where they randomly selected parameters to create four types of bifurcations: fold, Hopf, transcritical, and null (no bifurcation). They trained an LSTM-CNN model on this simulated data and tested it with simulated data generated from other known dynamical systems as well as empirical datasets. While the model outperformed generic EWS, it only considered one type of noise in the training data. Additionally, both the model and the generic EWS performed poorly on simulation data representing infectious disease spread.\nChakraborty et al. [14] generated a noise-induced Susceptible-Infected-Recovered (SIR) simulation dataset and trained the Bury et al. [10] LSTM-CNN model on this new data. The resulting model outperformed that original model [10] when tested against noise-induced disease-spreading models. However, the assessment focused on a single deep learning model, which had been fine-tuned using a different dataset [10]. Additionally, fixed-length time series were used as input, while real-world scenarios often involve time series of varying lengths. Furthermore, the model was trained, then evaluated, exclusively on noise-induced SIR simulation data, which may lead to overfitting.\nOur goal is to develop and train a deep learning model that achieves higher accuracy than the previously mentioned models [10, 14] when faced with new, unforeseen disease outbreak data. We tested the best-performing machine learning models based on TSC benchmarks, fine-tuning and training the selected model using datasets from both Bury et al. [10] and Chakraborty et al. [14]. Our model forecasts transcritical bifurcations, enabling it to predict disease outbreaks in noisy environments. It can handle time series of variable lengths and generalizes effectively when confronted with unknown datasets."}, {"title": "2 Methods", "content": "The time series in this study represents the daily number of new infected cases recorded over time, providing a 1D sequence where each value corresponds to the count of newly identified cases for each day.\nWe used two datasets for training our models. (1) Bury et al. [10] simulated 200,000 time-series instances, using a two-dimensional dynamical system with polynomial terms that under certain conditions could exhibit three bifurcations: fold, Hopf, or transcritical. They randomly selected the parameters of the dynamical systems to include wide possible dynamics that might occur in unseen data. After finding the time series with an equilibrium point, they tweaked the parameters in a certain range to find possible bifurcations. Upon finding the bifurcation point, they simulated a null and a forced stochastic simulation. For the null case, they fixed the parameters while for the forced one, they linearly increased the parameter that caused the bifurcation from its equilibrium value to the bifurcation point while adding white noise to account for the stochasticity of real-world scenarios. They kept the last 1,500 time points before the bifurcation point, and repeated this process until having 50,000 time series for each type of bifurcation. In this study, we are interested only in the transcritical form of bifurcation that commonly appear in disease outbreaks. Hence, we only kept the transcritical and null parts of this dataset and we refer to this dataset as the RAPO dataset (Table 1).\n(2) The Susceptible-Infected-Recovered (SIR) model is a common epidemiological model for explaining infectious disease dynamics [15]. Chakraborty et al. [14] used the SIR model to generate new simulation data, aiming to train a better model than [10] for predicting impending transition in a disease outbreak, similar to predicting bifurcations in a dynamical system. They incorporated additive white noise, multiplicative environmental noise, and demographic noise in the SIR model to simulate the effect of stochasticity in real-world outbreaks. A total of 30,000 time series were simulated across the three noise-induced SIR models, with half containing transcritical bifurcations and the other half containing null bifurcations. Key parameters, such as disease transmission rates and noise intensity, were randomized during data generation to introduce variability and reflect uncertainty. In this study, we only used only the pre-transition portion of this dataset, which we refer to as the NISIR dataset (Table 1).\nBury et al. [10] employed the CNN-LSTM architecture and tuned the model hyperparameters using their dataset. We adopted the same model architecture and hyperparameters. The reason we did not tune the hyperparameters with our dataset is that these hyperparameters include architecture-level parameters, and tuning them would result in a different model, whereas we want to compare our new models with that earlier one [10], whose CNN layers first extract patterns from the input time series. The following LSTM layers extract temporal information from extracted patterns for classification, which is why we refer to it as seq-CNN-LSTM. Additionally, we tested another architecture that contains the LSTM and CNN layers, which implemented the LSTM and CNN layers in parallel, which we call par-CNN-LSTM. Our third model combined a one-dimensional Convolutional Layer with a self-attention mechanism, specifically using the Squeeze-and-Excite (SE) block [16]. This block offers channel-wise self-attention scores for CNN layers. We referred to it as Conv1d+SE. Finally, we evaluated other top-performing models in TSC benchmarks [11] such as InceptionTime [17] and SAnD, a transformer-based Time Series"}, {"title": "2.2 Training and testing", "content": "We preprocessed the raw dataset following the same procedure as in [10], which censored data instances from both the beginning and end of the time series. We did this because the time before a disease outbreak is unknown (end) and prevalence data is often not recorded at early stages (beginning). If a time series is 1500 points long, we selected two random numbers from the interval [0, 725], then removed ending chunks of the time series by making them zero, for censoring each time series. After censoring, we normalized each instance with its average. Furthermore, we detrended the time series with a Lowess smoothing [19].\nWe created a dataset comprising a total of 160,000 time series from two sources. We use the Python scikit-learn package to split the preprocessed dataset into three sets using an 80/15/5 ratio for training, validation, and testing, using stratified sampling to ensure each set has an equal number of bifurcations. Using five percent of the data for testing results in 8,000 time series instances which is big enough to test the model's performance.\nOur study was conducted in three steps. In the first step, we trained each of the three model architectures on each of the three noise-induced SIR simulated datasets, resulting in a total of nine models. Before training, we fine-tuned the hyperparameters of the models except for the seq-CNN-LSTM model, for which we used the hyperparameters from [10]. We used the kerastuner library's implementation of Bayesian Optimization (BO) with Gaussian Processes to select the hyperparameters [20], including model architecture such as the number of CNN layers, number of filters per CNN layer, number of LSTM layers, number of memory cells per LSTM layers, and convergence hyperparameters such as learning rate, dropout rate, kernel regularizer 12 coefficient, a regularizer that penalizes layer weights. We used the training and validation set to select the best combination of hyperparameters. To prevent overfitting, we applied early stopping [21].\nIn the second step, we selected the best-performing model from the first step. We then combined the"}, {"title": "3 Results", "content": "The InceptionTime model did not perform as well as the other models. Similarly, the transformer-based model required extensive hyperparameter tuning to achieve reasonable results, but its performance remained suboptimal. Consequently, we did not move forward with these two models. Instead, we narrowed our focus to three models to limit our search space: par-LSTM-CNN, seq-LSTM-CNN, and Conv1d+SE.\nAll three models performed almost equally well against the NISIR test sets, with par-LSTM-CNN outperforming seq-LSTM-CNN (using the hyperparameters from Bury et al. [10]) and the Conv1d+self-attention models (Table 2). Hence par-LSTM-CNN was selected as the best model for further analysis. Trained on the three noise-induced datasets, par-LSTM-CNN matched the performance of the best models trained individually on each noise-induced dataset. However, its performance on the RAPO test set was only marginally better than chance, achieving an improvement of just five percent (Table 3).\nThe par-LSTM-CNN model, trained on the NISIR and RAPO datasets and referred to as \"our model,\" demonstrated comparable performance to Chakraborty et al.'s model on the NISIR datasets and to Bury et al.'s model on the RAPO test set. However, Chakraborty et al.'s model performed poorly on the RAPO test set, while Bury et al.'s model exhibited low accuracy on the NISIR test set (Table"}, {"title": "4 Discussion", "content": "We trained deep learning models to predict disease outbreaks using two simulation datasets. The first dataset was the noise-induced SIR (NISIR) model simulation data [14], which incorporates both the dynamics of infectious diseases and the stochasticity present in real-world datasets. The second dataset was generated using randomly selected polynomials (RAPO) [10]. To avoid overfitting and ensure generalization to potentially unknown diseases with dynamics differing from those described by the SIR model, we used the transcritical portion of the RAPO simulations. We evaluated the best-performing deep learning models from the Time Series Classification benchmarks [11] and selected the optimal model based on the AUC metric. To assess the performance improvements of our approach, we compared our models to the pre-trained models described in [10, 14] using simulation models of infectious diseases. To determine whether predicting bifurcations in simulation data could translate to predicting disease outbreaks in real-world data, we tested our model on empirical datasets for influenza and COVID-19. Notably, our model outperformed other models on the influenza dataset and demonstrated equal performance on simulated datasets.\nAll deep learning models achieved near-perfect scores when trained and tested on the NISIR dataset. This indicates that these models are more complex than the dataset itself, suggesting they would perform well on datasets with a higher level of induced noise. This trend persisted when the three noise-induced parts of the NISIR dataset were combined. The par-LSTM-CNN model, which performed slightly better than the others when trained on each of the three parts individually, achieved the same accuracy when trained on the entire NISIR dataset. Increasing the size of the dataset improved the generalization capability of this deep learning model in handling various types of noise. However, the model failed to generalize to an out-of-sample test set from the RAPO dataset. Features learned by the model from the NISIR dataset explained the dynamics of the RAPO dataset only 5% better than chance. In contrast, features learned by Bury et al.'s model from the RAPO dataset explained the NISIR dataset up to 18% better than chance. This aligns with our expectations, as the RAPO dataset exhibits greater diversity by generating dynamical systems with randomly selected polynomial terms.\nA model that achieves high accuracy with a censored setup is expected to perform well when more data is available in an uncensored setup. The performance difference between Chakraborty et al.'s model and the other two models lies in the training data: Chakraborty et al.'s model was trained using pre-transition data without censoring. However, all models performed no better than chance when tested on the uncensored RAPO dataset. It is important to note that Bury et al.'s sub-optimal accuracy on their RAPO dataset does not directly reflect their model's performance in the final tests, as these tests involved nearly uncensored data instances. Nevertheless, our model outperformed Bury et al.'s model by 5% in a fair comparison using a test set from the RAPO dataset that both models had utilized during training. The performance improvement of our model, which was trained using both the RAPO and NISIR datasets, is attributable to two factors: the use of more data and the optimization of the model architecture through Bayesian Optimization. This architectural optimization enabled our model to achieve 1.5% better accuracy than Chakraborty et al.'s model on the NISIR test set and 5% better accuracy on the RAPO test set, even when using the same training data. Additionally, the enhanced generalization capability of our model, trained with the more diverse RAPO and NISIR datasets, contributed significantly to its superior performance compared to the other two models."}, {"title": "Data accessibility", "content": "The training and testing data used in this study were collected from Bury et al. [10] and Chakraborty et al. [14]. Influenza data was collected from Our World in Data [24] and COVID-19 data was collected from [23]."}, {"title": "Funding", "content": "This project was primarily supported by One Health Modelling Network for Emerging Infections (OMNI). Hao Wang's research was partially supported by the Natural Sciences and Engineering Research Council of Canada (Individual Discovery Grant RGPIN-2020-03911 and Discovery Accelerator Supplement Award RGPAS-2020-00090) and the Canada Research Chairs program (Tier 1 Canada Research Chair Award). Mark A. Lewis gratefully acknowledges support from an NSERC Discovery Grant and the"}, {"title": "Declaration of AI use", "content": "We have not used AI-assisted technologies in creating this article."}, {"title": "Conflict of interest declaration", "content": "We declare we have no competing interests."}, {"title": "A SI Appendix", "content": null}, {"title": "A.1 SEIR model", "content": "The susceptible-exposed-infectious-recovered (SEIR) model equations [15] are given by\n\n$\\frac{dS(t)}{dt} = A - \\beta(t)S(t)I(t) - dS(t) + \\sigma_{1}dW_{1}(t)$,\n$\\frac{dE(t)}{dt} = \\beta(t)S(t)I(t) - (d+ \\kappa)E(t), +\\sigma_{2}dW_{2}(t)$,\n$\\frac{dI(t)}{dt} = \\kappa E(t) - (d + \\gamma)I(t) + \\sigma_{3}dW_{3}(t),$,\n$\\frac{dR(t)}{dt} = \\gamma I(t) + \\sigma_{4}dW_{4}(t) - dR(t) + \\sigma_{4}dW_{4}(t)$,\n\n(A.1)\nwhere $S(t)$, $E(t)$, $I(t)$, and $R(t)$ denote the susceptible, exposed, infectious, and recovered individuals, respectively, at time $t$. Here, $A$ is the recruitment rate of the susceptible population, $\\beta(t)$ is the disease transmission rate, $d$ is the natural death rate, $1/\\gamma$ is the mean infectious period, and $1/\\kappa$ is the mean exposed period. $\\sigma_{i}, i = 1,2,3,4$, are the intensities of white noise and $W_{i}(t), i = 1,2,3,4$, are independent Wiener processes. The basic reproduction number of the model is $\\frac{\\kappa \\beta \\Lambda}{d(d+\\kappa)(d+\\gamma)}$, and the bifurcation point is $B_{e} = \\frac{d(d+\\kappa)(d+\\gamma)}{\\kappa \\Lambda}$"}, {"title": "A.2 SEIRx model", "content": "The stochastic version of Susceptible-Exposed-Infectious-Removed-vaccinator (SEIRx) model equation [10] are given by\n\n$\\frac{dS}{dt} = \\mu N(1 - x) - \\mu S - B\\frac{S I}{N} + \\sigma_{1}\\xi_{1}(t)$,\n$\\frac{dE}{dt} = B\\frac{S I}{N} - (\\sigma + \\mu)E + \\sigma_{2}\\xi_{2}(t)$,\n$\\frac{dI}{dt} = \\sigma E - (\\gamma + \\mu)I + \\sigma_{3}\\xi_{3}(t)$,\n$\\frac{dR}{dt} = \\mu x + \\gamma I - \\mu R + \\sigma_{4}\\xi_{4}(t)$,\n$\\frac{dx}{dt} = k\\alpha (1 - x) (-\\omega + I + \\delta(2x - 1)) + \\sigma_{5}\\xi_{5}(t)$,\n\n(A.2)\nwhere $S$ represents the population of individuals who are susceptible to the disease, $E$ refers to those who have been exposed to the infection but are not yet infectious, $I$ denotes the individuals who are infectious, and $R$ is the group of individuals who have recovered or developed immunity. The variable $x$ represents individuals with a provaccine sentiment. The parameters include $\\mu$, the per capita birth and death rate, $B$, the transmission rate of the disease, $\\sigma$, the per capita rate at which exposed individuals become infectious, $\\gamma$ is the recovery rate from the infection, $k$ is the rate of social learning, $\\delta$ indicates the strength of injunctive social norms, and $\\omega$ reflects is the perceived relative risk of vaccination versus infection, $\\sigma_{i}, i = 1, 2, 3, 4, 5$, are the noise amplitudes, and $\\xi_{i}, i = 1, 2, 3, 4, 5$, are Gaussian white noise process."}]}