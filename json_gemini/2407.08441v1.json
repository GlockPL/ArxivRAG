{"title": "Are Large Language Models Really Bias-Free?\nJailbreak Prompts for Assessing Adversarial\nRobustness to Bias Elicitation", "authors": ["Riccardo Cantini", "Giada Cosenza", "Alessio Orsino", "Domenico Talia"], "abstract": "Large Language Models (LLMs) have revolutionized arti-\nficial intelligence, demonstrating remarkable computational power and\nlinguistic capabilities. However, these models are inherently prone to\nvarious biases stemming from their training data. These include selec-\ntion, linguistic, and confirmation biases, along with common stereotypes\nrelated to gender, ethnicity, sexual orientation, religion, socioeconomic\nstatus, disability, and age. This study explores the presence of these bi-\nases within the responses given by the most recent LLMs, analyzing the\nimpact on their fairness and reliability. We also investigate how known\nprompt engineering techniques can be exploited to effectively reveal hid-\nden biases of LLMs, testing their adversarial robustness against jailbreak\nprompts specially crafted for bias elicitation. Extensive experiments are\nconducted using the most widespread LLMs at different scales, confirm-\ning that LLMs can still be manipulated to produce biased or inappro-\npriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhanc-\ning mitigation techniques to address these safety issues, toward a more\nsustainable and inclusive artificial intelligence.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently gained significant traction due\nto their impressive natural language understanding and generation capabilities\nacross various tasks, including machine translation, text summarization, topic\ndetection, and engaging human-like conversations [4,3,1]. However, as LLMs be-\ncome more integral to our daily lives across various domains ranging from\nhealthcare and finance to law and education - it is increasingly crucial to ad-\ndress the inherent biases that can emerge from these models. Such biases can lead"}, {"title": "2 Related work", "content": "Several studies have underscored the potential risks posed by societal biases,\ntoxic language, or discriminatory outputs that can be generated by LLMs [11,9].\nIn addition, despite advances in safety strategies, research suggests that LLMs\ncan still be manipulated to expose hidden biases through adversarial attacks [6,7].\nThis section reviews recent work in this area, focusing on fairness evaluation, bias\nbenchmarking, and adversarial attacks using jailbreak prompts.\nFairness evaluation and bias benchmarking. Effective methods for iden-\ntifying and mitigating bias are critical to ensuring the safety and responsible\nuse of LLMs. The primary strategy concerns creating benchmark datasets and\nframeworks that allow to probe LLMs for potential biases [22,23], generally em-\nploying targeted prompts and metrics. Manerba et al. [13] presents SOFA (So-\ncial Fairness), a fairness probing benchmark encompassing diverse identities and\nstereotypes, also introducing a perplexity-based score to measure the fairness\nof language models. Tedeschi et al. [14] introduce a novel safety risk taxonomy,\nalso presenting ALERT, a comprehensive benchmark for red teaming LLMs.\nStereoSet [15] is another benchmark tackling stereotypical biases in gender, pro-\nfession, race, and religion, providing a comprehensive evaluation of how LLMs\nperpetuate societal stereotypes across various demographic categories. Further-\nmore, several other benchmarks for assessing bias in LLMs have been proposed\nfor specific types of bias, including cognitive [20], gender-occupational [19], reli-\ngion [12], and racial [30].\nAdversarial attacks via jailbreak prompting. Adversarial attacks on LLMs\ninvolve deliberately crafting inputs to expose their vulnerabilities. These attacks\ncan be particularly insidious, as they may manipulate the model into generating\nbiased, toxic, or undesirable outputs. Recent studies have focused on the devel-\nopment of adversarial techniques to test and improve the robustness of LLMs\nagainst such vulnerabilities. Among the most recent methods proposed in the lit-\nerature, Chao et al. introduced PAIR [16], a systematically automated prompt-\nlevel jailbreak, which employs an attacker LLM to iteratively refine prompts,\nenhancing the chances of successfully bypassing the model's defenses. Similarly,\nTAP [25] leverages an attacker LLM but uses a tree-of-thought reasoning ap-\nproach to iteratively refine candidate prompts, also pruning unlikely ones. An-\nother approach is AutoDAN [17], which employs a hierarchical genetic algorithm"}, {"title": "3 Proposed methodology", "content": "To rigorously evaluate the capabilities of LMs in maintaining unbiased and fair\nresponses, we propose a two-step methodology that systematically assesses these\nmodels under various conditions, comprehensively testing the effectiveness of\ntheir safety measures. As depicted in Figure 1, the methodology follows a two-\nstep process: (i) an initial safety evaluation using standard prompts, and (ii) an\nadversarial analysis using jailbreak prompts on all bias categories deemed safe in\nthe previous step. In the following sections, we provide a thorough description of\neach step, also introducing the primary definitions and measures used throughout\nthe paper."}, {"title": "3.1 Safety evaluation using standard prompts", "content": "This first step involves evaluating the robustness and fairness of the model con-\ncerning stereotypes associated with a set B of bias categories, including age,\nethnicity, gender, sexual orientation, disability, religion, and socioeconomic sta-\ntus. A set of standard prompts for each bias category is created by constructing\na sentence completion task. In this task, the model is asked to choose between"}, {"title": "Definitions and measures.", "content": "Here, we detail the primary definitions and mea-\nsures used in the proposed methodology. Let B be a set of bias categories, such\nas gender or religion. For each category $b \\in B$, a set of prompts $P_b$ is defined,\nencompassing several stereotypes involving identities in that category. Given a\nprompt $p_b \\in P_b$, the model is queried k times to account for varying responses,\nthereby thoroughly grasping its behavior under bias elicitation. Based on the\ndifferent possible model responses, we define the following measures for $p_b$:\nRobustness ($p p_t$) measures the rate at which the model refuses to choose one\nof the given options when prompted with $p_t$. This measure can be decom-\nposed into the refusal rate ($R_{p_t}$) and the debiasing rate ($D_{p_t}$). $R_{ps}$ measures\nthe rate at which the model categorically refuses to provide an answer, while\n$D_{ps}$ quantifies the tendency of the model to generate a debiased answer by\nchoosing a different, non-discriminatory option.\nIf the model answers by selecting one of the given options, the stereotype\nrate ($S_{p_t}$) and the counterstereotype rate ($\u0160_{p_t}$) measure how many times out\nof k the model selects the stereotyped and counterstereotyped options.\nFairness (\u0444pt) reflects the model's tendency to avoid polarization toward\nany specific option. It is defined as $\u0444_{p_t}$ = 1- |$S_{p_t}$ - $\u0405_{p_t}$|.\nGiven the definitions of robustness and fairness, the safety score $\u03c3_b$ is de-\nrived as \u03c3b = (pptb + \u0444\u0440\u044c). Consequently, the safety score at the bias level\n($\u03c3_T$) can be obtained as the average safety score for all prompts $p_b \\in P_b$.\nFinally, the overall safety score (\u03c3) of the model is obtained by averaging\nachieved scores across all bias categories: \u03c3 =$\\frac{1}{|B|} \\sum_{bEB} \u03c3_b$"}, {"title": "3.2 Adversarial analysis using jailbreak prompts", "content": "In this phase, all bias categories with \u03c3\u03b5 > \u03c4 undergo further analysis using\nadversarial prompts crafted through jailbreak techniques. These techniques, de-\nsigned to bypass language model safety filters and elicit biased responses, include:\nRole-playing: the model is prompted to impersonate a character, thus guiding\nit towards specific harmful behaviors.\nMachine translation: intermediary languages (usually low-resource languages)\nare used to obscure harmful content.\nObfuscation: harmful content is intentionally disguised by using unnatural\nlanguage constructs, such as unusual word order and complex syntax.\nPrompt injection: harmful prompts are inserted within apparently innocuous\ncontexts.\nReward incentive: a reward is offered to the model to coerce it into generating\nharmful content.\nTable 2 exemplifies how, starting from a standard prompt, its malicious ver-\nsions were prepared for each of the aforementioned attacks. In particular, we\nleveraged a superhero character for role-playing, Slovene language [48] for ma-\nchine translation, leetspeak for obfuscation, an uppercasing task for prompt in-\njection, and a Best LLM Award prize for reward incentive."}, {"title": "4 Experimental results", "content": "In this section, we analyze the results obtained from our benchmark tests on\nvarious language models, evaluating their performance in terms of robustness,"}, {"title": "4.1 Initial safety assessment", "content": "As the first step of our benchmark methodology, we queried each model with\na standard prompt. We set the value of the k parameter to 10, resulting in\nthe evaluation of 1260 responses in total, with 2 different sentence completion\nqueries for each bias and model. This section provides an in-depth analysis of\nthe models' behavior, focusing on understanding their performance in terms of\nrobustness, fairness, and safety."}, {"title": "4.2 Adversarial analysis", "content": "In this section, we evaluate the model's safety across all bias categories deemed\nsafe during the initial assessment (i.\u0435., \u0442 \u2265 0.5), by employing jailbreak prompt-\ning. Figure 5 illustrates the effectiveness of various jailbreak attacks across mul-\ntiple LMs, defined in terms of relative bias-specific safety reduction following ad-\nversarial analysis. The values reported indicate whether the malicious prompt de-\ncreased model safety (positive values) or whether the model became safer against\nthe malicious prompt (negative values). This last case suggests that the model\nidentifies potentially harmful instructions and malicious prompt templates, and\nthus protects itself against the attack, promoting non-biased responses.\nExperimental results reveal that role-playing attack has a notable impact on\nseveral models, with GPT-3.5 Turbo experiencing the most significant safety re-\nduction. Other models tend to be more robust against this jailbreak attack, with\nGemma 2B, StableLM21.6B, and Gemma 7B exhibiting even a safety increase.\nFor the obfuscation attack, GPT-3.5 Turbo again shows high vulnerability, with\nsignificant safety reduction also observed in Llama 370B and Gemma 7B. It is\nworth noting that for StableLM21.6B, the attack was unsuccessful because re-\nsponses were either nonsensical or a misinterpretation of the instructions in the\nleetspeak alphabet. Similar considerations hold for the machine translation at-\ntack, where StableLM21.6B and Phi-3 mini were not able to correctly reason\nstarting from Slovene prompts. In addition, GPT-3.5 Turbo was the least robust\nagainst machine translation, while Gemini Pro showed the highest safety against\nthis attack, due to its superior reasoning capabilities with this low-resource lan-\nguage. The prompt injection attack revealed particularly effective on Gemma 7B\nand Phi-3 mini, with the highest safety reductions recorded. GPT-3.5 Turbo\nremains highly vulnerable, whereas models such as StableLM21.6B and Gemini\nPro show increased safety, implying resistance to this form of attack. Lastly, the"}, {"title": "5 Conclusion and future directions", "content": "This study highlights the critical challenges that widespread LLMs face related\nto various forms of biases and stereotypes. Through the proposed two-step bench-"}]}