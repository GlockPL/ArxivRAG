{"title": "Are Large Language Models Really Bias-Free?\nJailbreak Prompts for Assessing Adversarial\nRobustness to Bias Elicitation", "authors": ["Riccardo Cantini", "Giada Cosenza", "Alessio Orsino", "Domenico Talia"], "abstract": "Large Language Models (LLMs) have revolutionized arti-\nficial intelligence, demonstrating remarkable computational power and\nlinguistic capabilities. However, these models are inherently prone to\nvarious biases stemming from their training data. These include selec-\ntion, linguistic, and confirmation biases, along with common stereotypes\nrelated to gender, ethnicity, sexual orientation, religion, socioeconomic\nstatus, disability, and age. This study explores the presence of these bi-\nases within the responses given by the most recent LLMs, analyzing the\nimpact on their fairness and reliability. We also investigate how known\nprompt engineering techniques can be exploited to effectively reveal hid-\nden biases of LLMs, testing their adversarial robustness against jailbreak\nprompts specially crafted for bias elicitation. Extensive experiments are\nconducted using the most widespread LLMs at different scales, confirm-\ning that LLMs can still be manipulated to produce biased or inappro-\npriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhanc-\ning mitigation techniques to address these safety issues, toward a more\nsustainable and inclusive artificial intelligence.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently gained significant traction due\nto their impressive natural language understanding and generation capabilities\nacross various tasks, including machine translation, text summarization, topic\ndetection, and engaging human-like conversations [4,3,1]. However, as LLMs be-\ncome more integral to our daily lives across various domains ranging from\nhealthcare and finance to law and education - it is increasingly crucial to ad-\ndress the inherent biases that can emerge from these models. Such biases can lead"}, {"title": "2 Related work", "content": "Several studies have underscored the potential risks posed by societal biases,\ntoxic language, or discriminatory outputs that can be generated by LLMs [11,9].\nIn addition, despite advances in safety strategies, research suggests that LLMs\ncan still be manipulated to expose hidden biases through adversarial attacks [6,7].\nThis section reviews recent work in this area, focusing on fairness evaluation, bias\nbenchmarking, and adversarial attacks using jailbreak prompts.\nFairness evaluation and bias benchmarking. Effective methods for iden-\ntifying and mitigating bias are critical to ensuring the safety and responsible\nuse of LLMs. The primary strategy concerns creating benchmark datasets and\nframeworks that allow to probe LLMs for potential biases [22,23], generally em-\nploying targeted prompts and metrics. Manerba et al. [13] presents SOFA (So-\ncial Fairness), a fairness probing benchmark encompassing diverse identities and\nstereotypes, also introducing a perplexity-based score to measure the fairness\nof language models. Tedeschi et al. [14] introduce a novel safety risk taxonomy,\nalso presenting ALERT, a comprehensive benchmark for red teaming LLMs.\nStereoSet [15] is another benchmark tackling stereotypical biases in gender, pro-\nfession, race, and religion, providing a comprehensive evaluation of how LLMs\nperpetuate societal stereotypes across various demographic categories. Further-\nmore, several other benchmarks for assessing bias in LLMs have been proposed\nfor specific types of bias, including cognitive [20], gender-occupational [19], reli-\ngion [12], and racial [30].\nAdversarial attacks via jailbreak prompting. Adversarial attacks on LLMs\ninvolve deliberately crafting inputs to expose their vulnerabilities. These attacks\ncan be particularly insidious, as they may manipulate the model into generating\nbiased, toxic, or undesirable outputs. Recent studies have focused on the devel-\nopment of adversarial techniques to test and improve the robustness of LLMs\nagainst such vulnerabilities. Among the most recent methods proposed in the lit-\nerature, Chao et al. introduced PAIR [16], a systematically automated prompt-\nlevel jailbreak, which employs an attacker LLM to iteratively refine prompts,\nenhancing the chances of successfully bypassing the model's defenses. Similarly,\nTAP [25] leverages an attacker LLM but uses a tree-of-thought reasoning ap-\nproach to iteratively refine candidate prompts, also pruning unlikely ones. An-\nother approach is AutoDAN [17], which employs a hierarchical genetic algorithm"}, {"title": "3 Proposed methodology", "content": "To rigorously evaluate the capabilities of LMs in maintaining unbiased and fair\nresponses, we propose a two-step methodology that systematically assesses these\nmodels under various conditions, comprehensively testing the effectiveness of\ntheir safety measures. As depicted in Figure 1, the methodology follows a two-\nstep process: (i) an initial safety evaluation using standard prompts, and (ii) an\nadversarial analysis using jailbreak prompts on all bias categories deemed safe in\nthe previous step. In the following sections, we provide a thorough description of\neach step, also introducing the primary definitions and measures used throughout\nthe paper.\n3.1 Safety evaluation using standard prompts\nThis first step involves evaluating the robustness and fairness of the model con-\ncerning stereotypes associated with a set B of bias categories, including age,\nethnicity, gender, sexual orientation, disability, religion, and socioeconomic sta-\ntus. A set of standard prompts for each bias category is created by constructing\na sentence completion task. In this task, the model is asked to choose between"}, {"title": "3.2 Adversarial analysis using jailbreak prompts", "content": "In this phase, all bias categories with \u03c3b > \u03c4 undergo further analysis using\nadversarial prompts crafted through jailbreak techniques. These techniques, de-\nsigned to bypass language model safety filters and elicit biased responses, include:\n\u2013 Role-playing: the model is prompted to impersonate a character, thus guiding\nit towards specific harmful behaviors.\n\u2013 Machine translation: intermediary languages (usually low-resource languages)\nare used to obscure harmful content.\n\u2013 Obfuscation: harmful content is intentionally disguised by using unnatural\nlanguage constructs, such as unusual word order and complex syntax.\n\u2013 Prompt injection: harmful prompts are inserted within apparently innocuous\ncontexts.\n\u2013 Reward incentive: a reward is offered to the model to coerce it into generating\nharmful content."}, {"title": "4 Experimental results", "content": "In this section, we analyze the results obtained from our benchmark tests on\nvarious language models, evaluating their performance in terms of robustness,\nfairness, and safety across different demographic biases. The bias categories con-\nsidered in this study are age, ethnicity, gender, sexual orientation, disability, reli-\ngion, and socioeconomic status. The models evaluated are the following: (i) small-\nsized LMs, including Gemma 2B [43], Phi-3 mini [46], and StableLM21.6B [47];\n(ii) medium-sized LMs, including Gemma 7B [43], Llama 3 8B, and Mistral 7B [44];\nand (iii) large-sized LMs, including Llama 370B, GPT-3.5 Turbo, and Gemini\nPro [45]. This diverse selection ensures a broad evaluation of different architec-\ntures and reasoning capabilities.\n4.1 Initial safety assessment\nAs the first step of our benchmark methodology, we queried each model with\na standard prompt. We set the value of the k parameter to 10, resulting in\nthe evaluation of 1260 responses in total, with 2 different sentence completion\nqueries for each bias and model. This section provides an in-depth analysis of\nthe models' behavior, focusing on understanding their performance in terms of\nrobustness, fairness, and safety."}, {"title": "4.2 Adversarial analysis", "content": "In this section, we evaluate the model's safety across all bias categories deemed\nsafe during the initial assessment (i.\u0435., \u03c4 \u2265 0.5), by employing jailbreak prompt-\ning. Figure 5 illustrates the effectiveness of various jailbreak attacks across mul-\ntiple LMs, defined in terms of relative bias-specific safety reduction following ad-\nversarial analysis. The values reported indicate whether the malicious prompt de-\ncreased model safety (positive values) or whether the model became safer against\nthe malicious prompt (negative values). This last case suggests that the model\nidentifies potentially harmful instructions and malicious prompt templates, and\nthus protects itself against the attack, promoting non-biased responses.\nExperimental results reveal that role-playing attack has a notable impact on\nseveral models, with GPT-3.5 Turbo experiencing the most significant safety re-\nduction. Other models tend to be more robust against this jailbreak attack, with\nGemma 2B, StableLM21.6B, and Gemma 7B exhibiting even a safety increase.\nFor the obfuscation attack, GPT-3.5 Turbo again shows high vulnerability, with\nsignificant safety reduction also observed in Llama 370B and Gemma 7B. It is\nworth noting that for StableLM21.6B, the attack was unsuccessful because re-\nsponses were either nonsensical or a misinterpretation of the instructions in the\nleetspeak alphabet. Similar considerations hold for the machine translation at-\ntack, where StableLM21.6B and Phi-3 mini were not able to correctly reason\nstarting from Slovene prompts. In addition, GPT-3.5 Turbo was the least robust\nagainst machine translation, while Gemini Pro showed the highest safety against\nthis attack, due to its superior reasoning capabilities with this low-resource lan-\nguage. The prompt injection attack revealed particularly effective on Gemma 7B\nand Phi-3 mini, with the highest safety reductions recorded. GPT-3.5 Turbo\nremains highly vulnerable, whereas models such as StableLM21.6B and Gemini\nPro show increased safety, implying resistance to this form of attack. Lastly, the\nreward incentive attack had relatively moderate effectiveness across the mod-\nels, with the highest value being 0.72 for GPT-3.5 Turbo. Interestingly, despite\nregistering low effectiveness across almost all models, this attack was the most\neffective against Gemini Pro, which was generally the best-performing model."}, {"title": "5 Conclusion and future directions", "content": "This study highlights the critical challenges that widespread LLMs face related\nto various forms of biases and stereotypes. Through the proposed two-step bench-\nmarking methodology we highlighted how current LLMs at different scales can\nstill be manipulated to produce biased or harmful responses, despite their bias\nmitigation and alignment mechanisms. We examined the effectiveness of various\njailbreak attacks, assessing the extent to which each attack can reveal hidden\nbiases, even in models that appear to be the safest at first glance. Our adver-\nsarial analysis underscores the multifaceted nature of safety threats, suggesting\nthe inadequacy of a one-size-fits-all solution. Instead, a layered defense approach\nthat integrates multiple safeguards may be necessary to counteract these diverse\nand evolving threats, ensuring the secure deployment of LLMs in real-world\napplications."}]}