{"title": "FOCUS: Towards Universal Foreground Segmentation", "authors": ["Zuyao You", "Lingyu Kong", "Lingchen Meng", "Zuxuan Wu"], "abstract": "Foreground segmentation is a fundamental task in computer vision, encompassing various subdivision tasks. Previous research has typically designed task-specific architectures for each task, leading to a lack of unification. Moreover, they primarily focus on recognizing foreground objects without effectively distinguishing them from the background. In this paper, we emphasize the importance of the background and its relationship with the foreground. We introduce FOCUS, the Foreground Objects Universal Segmentation framework that can handle multiple foreground tasks. We develop a multi-scale semantic network using the edge information of objects to enhance image features. To achieve boundary-aware segmentation, we propose a novel distillation method, integrating the contrastive learning strategy to refine the prediction mask in multi-modal feature space. We conduct extensive experiments on a total of 13 datasets across 5 tasks, and the results demonstrate that FOCUS consistently outperforms the state-of-the-art task-specific models on most metrics.", "sections": [{"title": "Introduction", "content": "Foreground segmentation is a fundamental task in computer vision where the primary goal is to delineate the prominent objects (foreground) from the rest of the image (background), typically referring to salient object detection (SOD) and camouflaged object detection (COD) (Pang et al. 2022a, 2024a). In this paper, the concept of foreground segmentation can be extended to delineating objects that interest you most in the image, where the primary goal is to obtain the Mask of Interest (MoI), e.g., MoI should denote the mask of the camouflaged object in COD. According to this definition, tasks such as shadow detection (SD), defocus blur detection (DBD), forgery detection (FD), etc. belong to the category of foreground segmentation, too.\nCurrently, in the field of generic segmentation, e.g. instance segmentation, semantic segmentation, and panoptic segmentation, etc., there are already many sophisticated models (Kirillov et al. 2023; Cheng et al. 2022; Jain et al. 2023; Ding et al. 2023b,a). However, these models often lack targeted training for specific foreground segmentation tasks. For instance, in the COD task, SAM struggles to distinguish camouflaged objects from the background (Hu et al. 2024). Furthermore, without prompt-guided methods, most traditional segmentation algorithms generate multiple masks for one image at the same time (Cheng, Schwing, and Kirillov 2021; Cheng et al. 2022; Jain et al. 2023), but users do not require such many masks in many real-world scenarios, e.g. image background removal, MoI is all they need. While foreground segmentation typically produces a single or specific type of mask, making it more in line with user needs.\nHowever, as mentioned earlier, when the concept of foreground is generalized as MoI, the scope of foreground segmentation tasks is very broad. Currently, there is a lack of an excellent and universal framework that can handle all foreground segmentation tasks. Most foreground segmentation models are task-specific (Wang et al. 2022a; Zhao et al. 2021; Zhu et al. 2021; Zheng et al. 2024a; Xie et al. 2022; Wang et al. 2022b). Some models (Pang et al. 2024a, 2022a) achieve universality in SOD and COD tasks, but given the similarity between COD and SOD tasks, they will not be discussed as universal models here. To the best of our knowledge, the work most closely related to ours is (Liu et al. 2023). However, it still significantly lags behind task-specific models after fine-tuning in the subdivision tasks.\nBesides, previous foreground segmentation models primarily focused on recognizing the foreground objects without effectively distinguishing them from the background, neglecting the background and the relationship between the background and the foreground. In fact, background information plays a critical role in computer vision tasks (Li et al. 2023; Meng et al. 2024). Foreground segmentation inherently involves distinguishing the foreground from the background, making both elements and their relationship vital. However, current approaches fail to address the background segmentation separately. Consequently, this oversight impacts the overall performance of foreground segmentation.\nThe issues above can be summarized as follows: (1)How to generally represent the foreground and background of different foreground segmentation tasks? (2)How to fully utilize the background information of an image to optimize prediction results? In this paper, we introduce FOCUS, a unified multi-modal approach to tackle multiple subdivision tasks of foreground segmentation.\nTo universally represent the foreground and background, we borrow the object queries concept from DETR (Carion et al. 2020) by introducing ground queries. We apply the multi-scale strategy (Cheng et al. 2022) to extract image features to feed the transformer decoder, using masked attention to enable the ground queries to focus on relevant features corresponding to foreground and background. We utilize the feature map obtained from the backbone to initialize the masked attention, which can serve as a localization prior. During this process, the ground queries adapt to learn the features relevant to the context of different tasks, making them universal features.\nTo fully leverage the background information in images, we employ contrastive learning strategies. We propose the CLIP refiner, using the powerful multi-modal learning ability from CLIP (Radford et al. 2021) to correct the masks generated by previous modules. We fuse the mask and image and align the fused image and its corresponding text in multi-modal feature space to refine the masks. This not only refines the edges of the mask but also accentuates the distinction between foreground and background. We treat foreground segmentation and background segmentation as two independent tasks, and in the inference stage, the probability map of both foreground and background will jointly determine the boundary of MoI.\nWe conduct detailed experiments on 13 datasets across five foreground segmentation tasks and achieve or exceed state-of-the-art on most provided metrics. Fig. 1 shows the outstanding performance of our proposed FOCUS on different sub-tasks of the foreground segmentation.\nOur contributions can be summarized as follows:\n\u2022 We propose a unified framework for foreground segmentation tasks, including SOD, COD, SD, DBD, and FD;\n\u2022 We propose a novel module, using the contrastive learning strategy to utilize the background information to refine the mask while widening the distance between the foreground and the background;\n\u2022 We conduct extensive experiments on multiple datasets across multiple tasks, and results demonstrate that our method achieves state-of-the-art performance."}, {"title": "Related Work", "content": "As mentioned earlier, several tasks are crucial in foreground segmentation, including salient object detection(SOD), camouflaged object detection (COD), shadow detection (SD), defocus blur detection (DBD), and forgery detection (FD). SOD aims at segmenting the most visually attractive objects from the input images. COD focuses on disguised objects that blend seamlessly into their surroundings, e.g. mimetic animals and body paintings. SD aims to segment shadow regions from natural scenes. DBD aims at separating in-focus and out-of-focus regions, which is caused by the different focal lengths of the cameras, slightly different from SOD. The goal of FD is to identify altered or manipulated areas in images, typically involving addition, replacement, or deletion. Previous models normally designed architectures for specific foreground segmentation task (Wang et al. 2022a; Zhao et al. 2021; Zhu et al. 2021; Zheng et al. 2024a; Xie et al. 2022), and currently, there is a lack of effective methods to handle this foreground segmentation tasks universally."}, {"title": "Unified Segmentation", "content": "Universal segmentation has emerged as a significant trend in computer vision. It aims to unify various segmentation tasks within a single framework. This trend started with efforts to unify semantic and instance segmentation through panoptic segmentation (Kirillov et al. 2019) and has since expanded to include a broader range of tasks. Recent works have shifted towards designing universal segmentation models with generalization ability and versatility. Mask2Former (Cheng et al. 2022) utilizes a masked-attention mechanism to unify instance, semantic and panoptic segmentation. One-Former (Jain et al. 2023) further improves Mask2Former with a multi-task train-once design. More recent approaches like SAM (Kirillov et al. 2023) push the boundaries of universal segmentation with the ability of zero-shot segmentation. In the field of foreground segmentation, the unified architecture most related to ours is EVP (Liu et al. 2023). EVP freezes a pre-trained model and then learns task-specific knowledge using an adapter structure, but its performance falls behind task-specific models. In this work, we aim to find a more effective way to unify the foreground segmentation tasks using one single architecture."}, {"title": "Methods", "content": "Previously, there was a lack of unified architecture for handling all foreground segmentation subdivision tasks. Given an image from different foreground segmentation tasks, our goal is to use a unified architecture to predict the corresponding MoI in the task context. The problem can be defined by:\n$U(I,T_i) = Mol$\n$T_i$ refers to different foreground segmentation tasks, $\\forall T_i \\in \\{T_1,..., T_n\\}$ the unified framework U should infer the corresponding Mol from the images I.\nWe propose FOCUS, a unified architecture that can handle multiple foreground segmentation tasks. We borrow the concept of object queries from (Carion et al. 2020) and introduce the ground queries (GQ) here. GQ are two distinct tensors, designated as the foreground query and background query, we aim to only use these two learned tensors to respectively embed and represent the foreground and the background within the image based on the context of the task.\nFig. 2 provides an overview of our approach FOCUS. After obtaining multi-scale edge-enhanced features from the backbone and the edge enhancer, the pixel decoder will generate pixel-level output and these pixel-level features will be fed into the transformer decoder with GQ, where GQ updated by masked attention (Cheng et al. 2022) to get ground-centric output. It can be formulated as:\n$X_l = softmax(M_{l-1} + GQ_l,K_l)V_l + X_{l-1}$ ,\nHere, $K_l, V_l \\in \\mathbb{R}^{H_lW_l \\times C}$ denotes the linearly transformed C-dimensional image feature from lth block of pixel decoder, $X_l \\in \\mathbb{R}^{2 \\times C}$ refers to the query feature from the lth transformer decoder block and $X_0$ is initialized by the input query feature of transformer decoder. $GQ_l \\in \\mathbb{R}^{2 \\times C}$ is the lth ground queries, and $M_{l-1}$ is defined by:\n$M_{l-1}(x, y) = \\begin{cases} 1 & \\text{if } M_{l-1}(x, y) = 1 \\\\ 0 & \\text{otherwise} \\end{cases}$ ,\n$M_{l-1} \\in \\{0,1\\}^{2 \\times H_lW_l}$ is obtained by decoding $GQ_{l-1}$ and binarizing, with dimensions resizing consistent with $K_l$. DINOv2 (Oquab et al. 2023) is a recently proposed model designed for visual representation learning. The visualization of its feature map indicates that DINOv2 has already focused on the prominent objects in the image without supervision, showing richer semantics compared to other foundation models (Wang et al. 2022c; Meng et al. 2022). Therefore, we choose DINOv2 as the backbone for FOCUS, PCA and binarize the feature map of its last block to initialize the attention mask $M_0$. $M_0$ is formulated as:\n$M_0(x, y) = \\begin{cases} 1 & \\text{if } F_{DINOv2}(x, y) = 1 \\\\ 0 & \\text{otherwise} \\end{cases}$ ,\nHere, $F_{DINOv2}$ refers to the binary feature map from the last backbone block. It is resized to the same resolution of $K_l$. The adoption of the new initialization method can leverage the localization prior knowledge learned by the DINOv2 on large-scale data."}, {"title": "Edge Enhancer", "content": "In order to utilize the edge information of the object, we propose the edge enhancer, an effective module that uses foreground object edge information to correct the image features obtained by the backbone.\nInspired by the recent study that shows convolutions can help transformer understand local spatial information (Chen et al. 2022; Wang et al. 2022c), we use ResNet50 (He et al. 2016) to extract edge features from the image. We convert the image into grayscale to reduce the confusion caused by color, apply Gaussian smoothing (Davies 2004) to reduce noise, and then use an edge detector (Canny 1986) to obtain a gradient map and overlay it on the original image. As shown in Fig. 2, the ResNet can be divided into the STEM and the rest, the STEM serves as the initial feature extractor, comprising a series of convolution, batch normalization, and ReLU activation layers. The output of the rest convolution blocks will be flattened and projected into the same dimension D by 1\u00d71 convolutions and concatenated to obtain feature pyramid $F_{edge} \\in \\mathbb{R}^{(H/8)^2 + (H/16)^2 + (W/8)^2 + (W/32)^2) \\times D}$, H and W represent the resolution of the input image. Then, we follow ViT-Adapter (Chen et al. 2022), using the structure of the injector-extractor based on cross attention to fuse the image features from the backbone and ResNet. The injector can be formulated as:\n$F_{DINOv2} = F_{DINOv2} + \\gamma^{2} MSDA(F_{DINOv2}, F_{edge})$,\nMSDA refers to multi-scale deformable attention (Zhu et al. 2020), which takes the normalized backbone feature $F_{DINOV2} \\in \\mathbb{R}^{\\frac{HW}{16^{2}} \\times D}$ as the query, and the normalized edge feature $F^{i}_{edge} \\in \\mathbb{R}^{(\\frac{H}{8})^{2}+(\\frac{H}{16})^{2}+(\\frac{W}{8})^{2}+(\\frac{W}{32})^{2} \\times D}$ as the key and value. $\\gamma$ is a learnable parameter for balancing the backbone feature and the fused feature. Similarly, the extractor can be formulated as:\n$F_{edge} = F_{edge} + ConvFFN(MSDA(F_{edge}, F^{DINOv2}_{DINOv2}))$. It is another multi-scale deformable attention like injector while taking the normalized edge feature $F_{edge} \\in \\mathbb{R}^{(\\frac{H}{8})^{2}+(\\frac{H}{16})^{2}+(\\frac{W}{8})^{2}+(\\frac{W}{32})^{2} \\times D}$ as the query, and the output feature $F^{i+1}_{edge} \\in \\mathbb{R}^{\\frac{HW}{16^{2}} \\times D}$ as the key and value. ConvFFN refers to the structure with two fully connected layers and a depth-wise separable convolution layer. The $F^{i+1}_{edge}$ will serve as the input for the next injector. We upscale the output from different blocks of backbone to resolutions of 1/4, 1/8, 1/16, and 1/32. Besides, we split the output of the last extractor, and restore them to their original size. Then we add the up-scaled backbone features with the corresponding split output from extractor and output from STEM to get the edge-enhanced multi-scale image features. These features will be fed into the pixel decoder, another module based on multi-scale deformable attention, for dense pixel-level predictions."}, {"title": "CLIP Refiner", "content": "Since the proposal of CLIP, there have been many works using CLIP for segmentation (Xu et al. 2022; Li et al. 2022; Wang et al. 2022d; Liang et al. 2023), which have proven that CLIP is effective not only at the image level but also at the pixel level. In this paper, we propose CLIP refiner, which uses the powerful multi-modal ability of CLIP to correct the masks of foreground and background.\nSpecifically, we decode the ground queries to obtain the masks of the foreground and background, resize them, and overlay them on the image. We use the prompts \u201cIt's an image of salient objects without background.\u201d and \u201cIt's an image of background with salient objects removed.\u201d to represent foreground and background, respectively. Note that the text can be adjusted according to the task. For example, in shadow detection, prompts can be replaced with \u201cit's an image of shadow without background.\u201d and \u201cit's an image of background without shadow.\u201d to extend CLIP refiner to other foreground segmentation tasks. We borrow the image encoder and text encoder from CLIP to encode the image and text separately. Then, we calculate the contrastive loss ($L_{clip}$) between the mask-fused-image and text features.\n$\\begin{aligned} L_{clip} = & \\frac{1}{2} \\left[ - \\log \\frac{\\exp{(I_f \\cdot T_f / \\tau)}}{\\exp{(I_f \\cdot T_f / \\tau)} + \\exp{(I_f \\cdot T_b / \\tau)}} - \\log \\frac{\\exp{(I_b \\cdot T_b / \\tau)}}{\\exp{(I_b \\cdot T_b / \\tau)} + \\exp{(I_b \\cdot T_f / \\tau)}} \\right. \\\\ & \\left. - \\log \\frac{\\exp{(T_f \\cdot I_f / \\tau)}}{\\exp{(T_f \\cdot I_f / \\tau)} + \\exp{(T_f \\cdot I_b / \\tau)}} - \\log \\frac{\\exp{(T_b \\cdot I_b / \\tau)}}{\\exp{(T_b \\cdot I_b / \\tau)} + \\exp{(T_b \\cdot I_f / \\tau)}} \\right] \\end{aligned}$\n$L_{clip} = \\frac{1}{2}(L_{i2t} + L_{t2i})$.\nHere $I_f$, $I_b$, $T_f$, $I_b \\in \\mathbb{R}^{2 \\times S}$ denotes the S-dimensional image feature and text feature of foreground and background obtained by CLIP, $\\tau$ is temperature parameter used to control the smoothness of the softmax function. The CLIP refiner iteratively refines the edges of masks generated by the preceding module, ensuring that only the appropriate pixels are included in the foreground or background. This process aligns the mask-fused image more closely with the corresponding text in the feature space while distancing it from the mismatched one. This not only makes the mask edges more accurate but also widens the gap between the foreground and background. The CLIP refiner is only used to distill knowledge from CLIP and will be discarded during the inference stage. Additionally, we keep the image and text encoders entirely frozen to fully leverage the multi-modal capabilities of CLIP without the potential performance degradation that might arise from fine-tuning."}, {"title": "Training Objectives", "content": "In order to perform foreground and background segmentation jointly, we convert the foreground segmentation dataset into binary form, with the white areas representing the foreground ground truth and the black areas representing the background ground truth. Following (Cheng et al. 2022), we use the combination of binary cross entropy ($L_{bce}$) and dice loss ($L_{dice}$) as the loss of the mask, where:\n$L_{mask} = L_{bce} + L_{dice}$\nRecent study (Li et al. 2023) shows that parallel execution of object detection and segmentation can benefit each other. In this paper, we use the rectangular boundary of the ground truth mask as the ground truth bounding box to perform object detection. We use combination of the L1 Regression Loss ($L_{L1}$) and generalized IoU loss ($L_{gIoU}$) as the loss for $L_{bbox}$, which can be formulated as:\n$L_{bbox} = \\alpha L_{L1} + \\beta L_{gIoU}$\n$\\alpha$ and $\\beta$ are set to 5.0 and 2.0 respectively. We use the standard cross entropy loss as the $L_{label}$. The final training objective is defined as follows:\n$L = A_{clip}L_{clip} + A_{label}L_{label} + A_{mask}L_{mask} + A_{bbox}L_{bbox}$\nhere, $A_{clip}$, $label$, $A_{mask}$, $A_{bbox}$ refer to the weight of corresponding loss, set to 1.0, 1.0, 5.0, 1.0 respectively. To find the allocation with the lowest cost, we use Hungarian matching (Carion et al. 2020; Cheng, Schwing, and Kirillov 2021) between the prediction and the ground truth."}, {"title": "Experiments", "content": "For COD, we follow (Fan et al. 2021; Zheng et al. 2024a), training FOCUS on the combination of CAMO-TR (Le et al."}, {"title": "Main Results", "content": "Comparison of the state-of-the-art task-specific methods. We compare our proposed FOCUS with recent models for COD including SINet (Fan et al. 2020), PFNet (Mei et al. 2021), ZoomNet (Pang et al. 2022b), BSA-Net (Zhu et al. 2022), FSPNet (Huang et al. 2023), ZoomNeXt (Pang et al. 2024b) and BiRefNet (Zheng et al. 2024b), models for SOD including MENet (Wang et al. 2023), SelfReformer (Yun and Lin 2023), BBRF (Ma et al. 2021), and VST (Liu et al. 2021), models for SD task including BDRAR (Zhu et al. 2018), DSD (Zheng et al. 2019), MTMT (Chen et al. 2020), FDRNet (Zhu et al. 2021) and SILT (Yang et al. 2023), models for DBD including DeFusionNet (Tang et al. 2020), CENet (Zhao et al. 2019), DAD (Zhao, Shang, and Lu 2021) EFENet (Zhao et al. 2021) and DD (Cun and Pun 2020), and models for FD including ManTra (Wu, AbdAlmageed, and Natarajan 2019), SPAN (Hu et al. 2020), PSCCNet (Liu et al. 2022), TransForensics (Hao et al. 2021) and ObjectFormer (Wang et al. 2022a). FOCUS outperforms these SOTA models on most metrics across 13 datasets covering 5 tasks. Table. 1-3 shows quantitative comparisons between our proposed FOCUS with the previous SoTA models. Qualitative comparisons are in Fig. 3.\nIn the most challenging foreground segmentation task, COD, which requires the model to recognize the object blending in its surroundings, FOCUS outperforms the existing SoTA methods on most metrics across four mainstream datasets. For SOD tasks, FOCUS exceeds the task-specific models on almost all metrics, especially increasing in terms of $E_\\xi$ by an average of 1.8%. In SD tasks, FOCUS dramatically outperforms the previous SoTA on the ISTD dataset, with a 10.3% decrease in BER. In the DBD task, FOCUS surpasses the previous SoTA by a 2.1% increase on $F_\\beta$ on"}, {"title": "Ablation Study", "content": "In this section, we conduct ablation experiments to analyze the properties of FOCUS. We use Mask2Former equipped with the DINOv2-L backbone as a robust baseline and choose the most representative foreground segmentation tasks, COD and SOD, as the ablation tasks. We select the mainstream dataset CAMO and PASCAL-S for COD and SOD respectively. To ensure consistency, all experiments were conducted using the same training recipe, with a batch size of 2. The training iterations are set to 10,000 for COD and 20,000 for SOD. Quantitative results related to each module or method are shown in Table. 4.\nAs shown in the table, variants of FOCUS with the CLIP refiner perform better than those without it, thanks to the multi-modal knowledge distilled from CLIP. We set the variants with joint prediction to perform foreground segmentation and background segmentation jointly, the comparison with the baseline shows that it can slightly improve the performance of FOCUS. Additionally, with the help of the edge enhancer to inject edge information of the object into the backbone image feature, the performance of variants of DINOv2 significantly improves in the provided metrics. We also evaluate the effectiveness of pretraining on ADE20K, which demonstrates modest improvements.\nWe use DINOv2-G as the backbone for our SoTA models, which inevitably results in a large number of parameters. To ensure a fair comparison, we freeze the DINOv2-G backbone, limiting the number of trainable parameters in our model to 0.1G. The results indicate a slight decrease in performance compared to the fully fine-tuned version. However, when compared to models like BiRefNet (215M) and SelfReformer (~220M), the frozen-backbone FOCUS still matches or surpasses previous state-of-the-art performance, despite having fewer trainable parameters.\nWe initialize the first layer of the transformer decoder with PCA-reduced feature maps from the backbone in our paper. As shown in Fig. 4, these PCA-reduced feature maps begin to exhibit strong semantic features in the early training stages. As training progresses, we are pleasantly surprised to find that even without further forward propagation, the patch-level feature maps, simply reduced by PCA, are able to approach the ground truth quality. Using them for initialization, compared to random initialization, provides a valuable spatial prior for subsequent mask attention."}, {"title": "Conclusion", "content": "In this paper, we propose FOCUS, a unified multi-modal approach to tackle multiple subdivision tasks of foreground segmentation. We leverage the concept of object queries to handle foreground segmentation tasks and develop a multi-scale semantic network that simultaneously performs foreground and background segmentation, fully utilizing the background information of the image to optimize prediction. We also introduced a novel distillation method integrating the contrastive learning strategy to enhance boundary-aware foreground segmentation. Theoretically, our model can be extended to any foreground segmentation task. Extensive experiments conducted on diverse datasets demonstrate the effectiveness of our proposed framework."}]}