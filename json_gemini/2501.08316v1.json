{"title": "Diffusion Adversarial Post-Training for One-Step Video Generation", "authors": ["Shanchuan Lin", "Xin Xia", "Yuxi Ren", "Ceyuan Yang", "Xuefeng Xiao", "Lu Jiang"], "abstract": "The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280\u00d7720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "The diffusion method [21, 65] has become the de facto standard for learning large-scale image generation [1, 7, 13, 14, 49, 52, 54, 56] and video generation [2, 4, 31, 50, 72, 85].\nReducing the generation cost is an important research area in diffusion methods. Among the various methods proposed, diffusion step distillation has emerged as an effective approach to reduce the inference step. Generally, these methods start with a pre-trained diffusion model as a teacher that generates targets through multiple diffusion inference steps. They then apply knowledge distillation [19] to train a student model that can replicate the teacher's output using much fewer diffusion inference steps. Previous methods [6, 35, 40, 41, 53, 57, 59, 60, 64, 66, 79] focus on preserving the generative distribution of the diffusion teacher while reducing inference steps.\nOne-step generation is often considered the pinnacle of diffusion step distillation, yet it presents the most significant challenges. It deviates from the fundamental principle of diffusion models, which rely on iterative denoising steps to uncover the data distribution. While previous research has demonstrated notable advancements for generating images in a single step with promising results [6, 35, 40, 53, 60, 64, 66, 79], producing high-quality images in one step remains challenging, particularly in achieving fine-grained details, minimizing artifacts, and preserving the structural integrity.\nAccelerated video generation, however, has seen limited progress in the literature. Early efforts utilizing generative adversarial networks (GANs) [16], e.g. StyleGAN-V [63] can only generate domain-specific data with poor quality in modern standards. With the rise of diffusion methods, recent studies have begun exploring the extension of image distillation techniques to video diffusion models. However, earlier works [33, 70, 83] have only explored distillation on small-scale and low-resolution video models [17, 71] that only generate 512\u00d7512 videos for a total of 16 frames. A concurrent work [80] has attempted distillation of large-scale video models at 640\u00d7352 12fps. These methods still generally need 4 diffusion steps. Given the prohibitive computational cost associated with high-resolution video generation, e.g., generating just a few seconds of 1280\u00d7720 24fps videos can take multiple minutes even on the state-of-the-art GPUs like the H100, our work aims at generating high-resolution videos in a single step.\nIn this paper, we introduce a new approach for one-step image and video generation. Our method utilizes a pre-trained diffusion model, specifically the diffusion transformer (DiT) [48], as initialization, and continues training the DiT using the adversarial training objective against real data. It is important to notice the contrast to existing diffusion distillation methods, which use a pre-trained diffusion model as a distillation teacher to generate the target. Instead, our method performs adversarial training of the DiT directly on real data, using the pre-trained diffusion model only for initialization. We term this method Adversarial Post-Training or APT, as it parallels supervised fine-tuning commonly performed during the post-training stage. Empirically, we observe that APT provides two benefits. First, APT eliminates the substantial cost associated with pre-computing video samples from the diffusion teacher. Second, unlike diffusion distillation, where the quality is inherently constrained by the diffusion teacher, APT demonstrates the ability to surpass the teacher by a large margin in some evaluation criteria, in particular, improving realism, resolving exposure issues, and enhancing fine details.\nDirect adversarial training on diffusion models is highly unstable and prone to collapse, particularly in our case, where both the generator and discriminator are exceptionally large transformer models, each containing billions of parameters. To tackle this issue, our method introduces several key designs to stabilize training. It incorporates a generator initialized through deterministic distillation and introduces several enhancements to the discriminator, including transformer-based architectural changes, a discriminator ensemble across timesteps, and an approximated R1 regularization loss to facilitate large-scale training.\nBy virtue of APT, we have trained what may be one of the largest GAN ever reported to date (~16B), capable of generating both images and videos with a single forward evaluation. Our experiments demonstrate that our model achieves overall performance comparable to state-of-the-art one-step image generation methods, as evaluated through the user study based on three key metrics: visual fidelity, structural integrity, and text alignment. More importantly, to the best of our knowledge, our model is the first to demonstrate high-resolution video generation in a single step (1280\u00d7720 24fps), surpassing the previous state-of-the-art, which generates 512\u00d7512 or 640\u00d7352 up to 12fps videos in four steps. On a H100 GPU, our model can generate a two-second 1280\u00d7720 24fps video latent using a single step in two seconds. On 8\u00d7H100 GPUs with parallelization, the entire pipeline with text encoder and latent decoder runs in real time."}, {"title": "2. Related Works", "content": "Accelerating Diffusion Models. Diffusion step distillation is a common and effective approach to accelerate diffusion models. Existing methods address this problem using either deterministic or distributional methods.\nDeterministic methods exploit the fact that diffusion models learn a deterministic probability flow with exact noise-to-sample mappings and aim to predict the exact teacher output using fewer steps. Prior works include progressive distillation [57], consistency distillation [40-42, 64, 66], and rectified flow [38, 39, 77]. Deterministic methods are easy to train using simple regression loss, but the results of few-step generation are very blurry, due to optimization inaccuracy and reduced Lipschitz constant in the student model [35]. For large-scale text-to-image generation, deterministic methods generally require more steps, e.g. eight steps, to generate desirable samples [41, 42].\nOn the other hand, distributional methods only aim to approximate the same distribution of the diffusion teacher. Existing works employ adversarial training [6, 24, 44, 59, 76], score distillation [43, 78], or both [5, 60, 79]. Recent works have also explored combining distributional distillation with the deterministic probability flow [30, 35, 53]. However, existing methods have severe artifacts for one-step generation and still require multiple steps to obtain desirable results. Notably, LADD [59] uses pre-generated teacher images as the adversarial target. Lightning [35] and Hyper [53] learn the teacher trajectory with the adversarial objective in the loop but require training intermediate timesteps. DMD [78] applies score distillation [62, 74] from the teacher model, which sets the teacher as the upper bound for quality. The above methods use the pre-trained model as a teacher to compute targets for learning, but this incurs a high computational cost for videos. DMD2 [79] and ADD [60] apply both adversarial and score distillation objectives. Though the adversarial objective is on real data, the score distillation objective forces the student to resemble the teacher. The closest to our work is UFO-Gen [76] which also only applies adversarial training on real data. However, its discriminator adopts the DiffusionGAN [73] approach, and it uses the corrupted rather than the original real data as the input to the discriminator, while our method feeds the discriminator with real, uncorrupted data. Hence, our approach follows the standard adversarial training as in GAN more closely. Additionally, UFO-Gen's image generator and discriminator are convolutional models under 1B parameters, while ours are transformer models with 8B parameters and generate both image and video.\nFor video generation, some of these methods [33, 70, 83] have been extended to small-scale and low-resolution video models such as AnimateDiff [17] and ModelScope [71]. These models only generate low-resolution 256px or 512px videos of 16 frames. Very recently, a concurrent work [80] has demonstrated the generation of 640\u00d7352 12fps videos in four steps. To the best of our knowledge, our work is the first to demonstrate one-step generation of 1280\u00d7720 24fps videos with a duration of 2 seconds.\nOne-Step Video Generation. One-step video generation works may trace back to the use of generative adversarial networks (GAN) [16], e.g. DVD-GAN [9], MoCoGAN-HD [68], DIGAN [82], and StyleGAN-V [63], etc. They can generate up to 1024px resolution videos but are trained only on domain-restricted data, e.g. talking head videos, and the quality is poor by modern standards. More recently, AnimateDiff-Lightning [33] and Motion Consis-tency Model [83] have attempted to distill the AnimateDiff video diffusion model [17] to one step. They can generate 512x512 videos for a total of 16 frames but have substantial artifacts and quality degradation. Compared to previous works, our method produces one-step video results with substantially better quality in high resolution.\nStable Adversarial Training. R1 regularization [55] has been shown effective for GAN convergence [45]. It has been used by many prior GANs to improve performance [3, 22, 23, 25-27]. However, many recent large-scale adversarial works [35, 53, 59, 76] have either completely not used R1, or only used it for parts of the discriminator network [60]. This is likely due to its higher-order gradient computation is computationally expensive and is not supported by modern deep learning software stacks, i.e. FSDP [84], gradient checkpointing [8], FlashAttention [10, 11, 61], and other fused operators [46]. Our paper proposes an approximation method to address this issue and we find our approximated R1 loss is critical for preventing training collapse."}, {"title": "3. Method", "content": "Our objective is to convert a text-to-video diffusion model to a one-step generator. We achieve this by fine-tuning the diffusion model with the generative adversarial objective against real data. We refer to this process as Adversarial Post-Training or APT, due to its resemblance to supervised fine-tuning in the conventional post-training stage.\n3.1. Overview\nWe build our method on a pre-trained text-to-video diffusion model capable of generating both images and videos through T diffusion steps. The training follows adversarial optimization that alternates through a min-max game. The discriminator D classifies real samples from generated ones, maximizing -LD, while the generator G aims to generate samples that fool the discriminator, minimizing LG. Formerly, we have:\nLD = E[fD(D(x,c))] + E[fG(D(G(z, c), c))] , \nx,c~T\nz~N\nLG= E[gG(D(G(z, c), c))], \nz~N\nwhere N denotes the standard Gaussian distribution, and T represents the training data comprising a paired latent sample x and text condition c. The latent and noise samples are of size x, z \u2208 Rt'xh'xw'xc', where t', h', w', c' represent the dimensions of time, height, width, and channel. The functions fD, fG, and gg are the output functions. Here, we use the simple non-saturating variant [16]: fD(x) = gG(x) = log(x) and fG(x) = log(1 \u2212 \u03c3(x)), where \u03c3(x) is the sigmoid function.\nFigure 1 illustrates the overall architecture. Both the generator and the discriminator backbone use the diffusion model architecture but are initialized with different strategies which will be discussed later in this section. Concretely, our diffusion model uses the MMDiT architecture [13] and is trained with the flow-matching objective [37] over a mixture of images and videos at their native resolutions [12] in the latent space [54]. The model comprises 36 layers of transformer blocks, amounting to a total of 8 billion parameters.\n3.2. Generator\nWe find direct adversarial training on the diffusion model leads to collapse. To tackle this, we first perform deterministic distillation. We adopt discrete-time consistency distillation [64, 66] with mean squared error loss for simplicity. The model is distilled with a constant classifier-free guidance [20] scale of 7.5 and a fixed negative prompt.\nLet G denote the distilled model. Given noise sample z and text condition c, the model G predicts the velocity field \u00fb, which can be converted to sample prediction 2:\nv = \u011c(z, c,T), \nx = z - v.\nAlthough the generated sample \u00e6 is very blurry, \u011c provides an effective initialization for the subsequent adversarial training. Therefore, we initialize our generator G with the weights of G, defined as:\nG(z, c) := z \u2212 \u011c(z, c,T).\nFor the subsequent training, we primarily focus on one-step generation capability and always feed the final timestep T to the underlying model.\n3.3. Discriminator\nThe discriminator is trained to produce a logit that effectively distinguishes between real samples \u00e6 and generated samples 2. One-step generation requires a discriminator with sufficient learning capacity that can be stably trained. In this subsection, we discuss several effective designs that contribute to stable training and quality improvement. Refer to the detailed results presented in our ablation studies.\nFirst, following prior works [33, 35, 59, 75], we initialize the discriminator backbone using the pre-trained diffusion network and let it operate directly in the latent space. Therefore, the discriminator backbone also comprises 36 layers of transformer blocks and 8 billion parameters. We find that training all parameters without freezing improves the quality. Additionally, we find that initializing it with the original diffusion model weights, as opposed to the distilled model weights used by the generator, yields better results.\nSecond, we modify the diffusion transformer architecture to produce logits. Specifically, we introduce new cross-attention-only transformer blocks at the 16th, 26th, and 36th layers of the transformer backbone. Each block uses a single learnable token as the query to cross-attend to all the visual tokens from the backbone as the key and value, producing a single token output. These tokens are then channel-concatenated, normalized, and projected to yield a single scalar logit output. We find that using features from multiple layers enhances the structure and composition of the generated samples.\nThird, we directly provide the discriminator the raw sample \u00e6, without any noise corruptions. This avoids the introduction of artifacts to our generated samples. However, since our discriminator backbone is initialized from the diffusion model, and the diffusion pre-training objective at t = 0 is not meaningful, we find using t = 0 for our discriminator leads to collapse. Therefore, we propose to use an ensemble of different timestep values as input. Specifically, let D denote the underlying discriminator model, we define the D(x, c) in Equation (2) as:\nD(x, c) := E[D(x, t, c)],\nt~shift(U(0,T),s)\nwhere t is sampled uniformly from the interval [0,T] and then shifted by transformation function:\nshift(t, s) :=.\n3.4. Regularized Discriminator\nOur discriminator, comprising billions of parameters, possesses significant learning capacity yet is also prone to collapse. Ensuring stable training for such a powerful discriminator is therefore crucial to our problem. The R1 regularization [55] is an effective technique in facilitating the convergence of adversarial training [45]. It penalizes the discriminator gradient \u2207 on real data \u00e6, preventing the adversarial training from deviating from the Nash-equilibrium:\nLR1 = ||\u2207D(x, c) ||2.\nTraining with R1 requires higher-order gradient computation. The first backward computes the first-order discriminator gradient on input as the R1 loss. The second backward computes the second-order gradient of the R1 loss regarding the discriminator parameter @ for the discriminator updates.\nHowever, PyTorch FSDP [84], gradient checkpointing [8], FlashAttention [10, 11, 61], and other fused operators [46] do not support higher-order gradient computation or double backward at the time of writing, preventing the use of R1 in large-scale transformer models.\nWe propose an approximated R1 loss, written as:\nLaR1 = ||D(x, c) \u2013 D(N(x, oI), c) ||2.\nSpecifically, we perturb the real data with Gaussian noise of small variance \u03c3. The loss encourages the discriminator's predictions to be close between the real data and its perturbation, thereby reducing the discriminator gradient on real data and achieving a consistent objective as the original R1 regularization. Therefore, the final discriminator loss LD is defined as:\nLD = E[fD(D(x,c))] + E [fG(D(G(z, c), c))]\nx,c~T\n- CN\n+ E[||D(x, c) \u2013 D(N(x, I), c)||2].\nx,c~T\nIn our experiments, we use \u03bb = 100, \u03c3 = 0.01 for images and \u03c3 = 0.1 for videos. The generator and the discriminator are optimized in alternating steps, in which the approximated R1 is applied on every discriminator step.\n3.5. Training Details\nWe first train the model on only images. The images are 1024px resolution. We use 128~256 H100 GPUs with gradient accumulation to reach a batch size of 9062. We use a learning rate of 5e-6 for both the generator and the discriminator. We find the model adapts quickly to generate sharp images, so we use an Exponential Moving Average (EMA) decay rate of 0.995. We adopt the EMA checkpoint after 350 updates on the generator before the quality starts to degrade.\nWe then train the model on only videos. The videos are of resolution 1280\u00d7720 and we clip them to 2 seconds at 24fps. For the generator, we use the EMA checkpoint from the image stage as the initialization for video training. For the discriminator, we re-initialize from the diffusion weights. We use 1024 H100 GPUs and gradient accumulation to reach a batch size of 2048. We lower the learning rate to 3e-6 for stability and train it for 300 updates. After training the model for only one step, we find it can also zero-shot perform two-step inference with improved details and structures. However, more steps lead to artifacts.\nRMSProp optimizer is used with \u03b1 = 0.9. This is equivalent to Adam optimizer [29] with \u03b2\u2081 = 0, \u03b22 = 0.9 with reduced memory consumption. We do not use weight decay and gradient clipping. The entire training is conducted in BF16 mixed precision. We use the same datasets as used by the original diffusion model."}, {"title": "4. Experimental Results", "content": "This section empirically verifies the proposed Adversarial Post-Training (APT) method. Section 4.1 provides a qualitative comparison of our method against other one-step image generation baselines and an analysis of the characteristics of the image and video results generated by our approach. Section 4.2 presents several user studies that quantitatively assess the quality of the generated outputs. More results are available on our website: .\nBaseline. For comparison with one-step image generation methods, we select FLUX-Schnell [15], SD3.5-Turbo [59], SDXL-DMD2 [79], SDXL-Hyper [53], SDXL-Lightning [35], SDXL-Nitro [6], and SDXL-Turbo [60] as the comparison baselines. These models are selected because they are either the latest research publications or commonly available open-source distilled models. We also compare their original diffusion models against ours in 25 Euler steps. We use the default CFG [20] setting of each model as configured in diffusers [69], while ours uses CFG 7.5 as our best setting. CFG doubles the neural function evaluation (NFE) to 50, except FLUX [14] which has the CFG baked in. All models are 1024px, except SDXL-Turbo which only supports 512px.\n4.1. Qualitative Evaluation\nFor image generation, we first compare our APT model's generation in a single step and our original diffusion model using 25 diffusion steps in Figure 2. We observe that the diffusion model with classifier-free guidance often generates over-exposed images, rendering the images appear synthetic. In comparison, the APT model tends to generate images with a more realistic tone. Figure 3 further compares our method with other one-step image generation methods. The results suggest that our method shows advantages in preserving details and structural integrity.\nFor video generation, Figure 4 compares our APT one-step and two-step results with the original diffusion model for 25 steps. Both the good and the bad cases generated by the APT method are displayed. For the good cases, the APT improves visual details and realism. The one- or two-step APT models still perform worse in terms of structural integrity and text alignment compared to the original 25-step diffusion model. We refer readers to view the videos on our website.\n4.2. User Study\nEvaluation Protocol. We conduct a series of user studies with respect to three criteria: visual fidelity, structural integrity, and text alignment. Specifically, visual fidelity accounts for texture, details, color, exposure, and realism; structural integrity focuses on the structural correctness of the objects and body parts; text alignment measures closeness to the conditional prompts. Human raters are shown pairs of samples generated by different models and asked to choose their preferences regarding each criterion or to indicate no preference if a decision cannot be made.\nAfterward, the preference score is calculated as (G-B)/(G+S+B), where G denotes the number of good samples preferred, B denotes the number of bad samples not preferred, and S denotes the number of similar samples without preference. Thus, a score of 0% represents equal preference between the two models. +100% represents the model is preferred over all evaluated samples, and vice versa for -100%.\nFor image evaluation, we follow the evaluation protocol used in previous diffusion distillation works [59, 60] and generate samples using 300 randomly selected prompts from PartiPrompt [81] and DrawBench [56]. For each prompt, we generate 3 images using different seeds and have 3 raters to mark preferences in each category. For one-step video evaluation, we generate videos using 96 custom prompts. We generate one video per prompt which is also evaluated by 3 raters in each category. Our entire user study takes a total of 50,328 sample comparisons.\nAdditionally, following the previous works [35, 59, 60], we also report the FID [18], PFID [35], and CLIP [51] metrics on COCO dataset [36]. Note that we find these automatic metrics to be less accurate than user studies for assessing the model's actual performance. We provide the results and discussion in Appendix A."}, {"title": "5. Ablation Study and Discussion", "content": "5.1. The Effect of Approximated R1 Regularization\nWe find that the approximated R1 regularization is critical for maintaining stable training. Without this regularization, training collapses rapidly. As shown in Figure 5, the black curve represents the discriminator loss without R1 regularization, which quickly approaches zero compared to the green curve that includes the loss. When the discriminator loss approaches zero, the generator produces colored plates, as depicted on the right side of Figure 5.\n5.2. Discriminator Design\nWe first experiment with using different depths of the pre-trained diffusion model as our discriminator. Our intuition is that discriminators are smaller than the generator traditionally and using fewer layers may increase training throughput. However, as Figure 6 shows, we find that using a deeper discriminator with more learning capacity leads to better image quality. Therefore, we retain all 36 layers of the DiT as trainable parameters in our discriminator.\nWe then verify the effectiveness of using multilayer features as discussed in Section 3.3. As Figure 7 shows, we find that adding output heads only to the last layer can lead to the generation of images with disproportional structure. We speculate that this is because the last-layer features have a stronger focus on semantics and are less sensitive to the low-level structures. We find that using multilayer features can significantly mitigate the issue.\n5.3. The Effect of Training Iterations and EMA\nFigure 8 shows the model adapts fast. For the non-EMA model, even after 50 updates, it is able to generate sharp images. The EMA model generally performs better than the non-EMA. We find the quality peaks at 350 updates for the EMA model, and training it longer leads to more structural degradation.\n5.4. The Effect of the Batch Size\nFor images, our early experiments suggest that a larger batch size improves stability and structural integrity, confirming with previous research [23, 76]. For videos, we find that using a small batch size of 256 leads to mode collapse as shown in Figure 9 whereas a large batch size of 1024 does not. Therefore, our final training adopts a large batch size of 9062 for images and a batch size of 2048 for videos.\n5.5. Understanding the Model Internals\nWe freeze the model and add an additional linear projection on every layer. It is trained to match the final layer's latent prediction with mean squared error loss. This helps us visualize the internals of our model. As Figure 10 shows, the network's shallow layers generate the coarse structure and the deeper layers generate the high-frequency details. This is similar to the iterative generation process of diffusion models, except in our model the entire generation process is compressed within the 36 transformer layers in a single forward pass.\n5.6. The Reason for Visual Improvement\nDiffusion models without classifier-free guidance (CFG) [20] generate very poor samples [32]. CFG [20] is used ubiquitously to boost perceptual quality and text alignment, but recent works have shown that it can push the generated distribution away from the training distribution, resulting in samples that appear synthetic, over-saturated, and canonical [28, 34]. A recent work [32] has elucidated that the cause of diffusion models generating poor samples without CFG may be rooted in the mean squared error (MSE) loss objective. It has also demonstrated the use of perceptual loss can better learn the real data distribution. We hypothesize that adversarial training can be viewed as an extension of this work, where it does not have the MSE issue and the discriminator is a learnable, in-the-loop, perceptual critic. This helps the generator to learn distributions closer to the real training data.\n5.7. The Cause of Structural Degradation\nWe take the one-step image model and interpolate the input noise z to generate latent traversal videos. Unlike GAN models which normally have a low-dimensional noise z, our model has a very high-dimensional z \u2208 Rt'\u00d7h'\u00d7w'xc'. We find interpolation on the high-dimensional z still produces traversal videos with semantic morphing. Compared to the diffusion model which switches between modes very quickly, our one-step generation model has a much smoother transition between modes. This is likely because the one-step model effectively is much shallower in depth, has less nonlinearity, and has a lower capacity for making drastic changes. This effect has also been observed by a prior work [35]. The interpolation videos are available on our website for visualization.\n5.8. Analysis of Text Alignment Degradation\nPrevious research has reported that diffusion models with classifier-free guidance can significantly increase text alignment to an extent where the samples can look canonical [28, 32]. Adversarial post-training against real data makes the generated distribution closer to the real distribution, yet the real distribution itself often has worse text alignment [59]. Our training dataset has already employed re-captions to improve text alignments. Therefore, our one-step generation model still has very acceptable text alignment, though not as strong as the classifier-free guidance.\nWe have explored several techniques to improve text alignment but found them to be ineffective. These include: (1) providing the discriminator with unmatched conditional pairs to penalize misalignment, as proposed by [23, 58]. However, we choose not to adopt this approach because our early experiments showed minimal improvements; (2) incorporating CLIP loss [51]. Our experiments indicate that it could negatively impact visual fidelity, leading to poorer details and the introduction of artifacts. We leave further investigation to future research."}, {"title": "6. Conclusion and Limitations", "content": "In this paper, we propose Adversarial Post-Training (APT) to accelerate the diffusion model for single-step generation of both image and video. Our method incorporates several enhancements to the discriminator, along with an approximate R1 regularization, both of which are crucial for stabilizing large-scale adversarial training. By employing the pre-trained diffusion model for continuous post-training against real data, rather than using a distillation teacher to generate the target, our one-step generation model can achieve visual fidelity comparable to or even better than the original pre-trained diffusion model. However, it still suffers from degradation in structural integrity and text alignment.\nTo the best of our knowledge, we present the first proof of concept for generating high-resolution videos (1280\u00d7720 24fps) in a single step. However, we identify several limitations in the current approach. First, due to computational constraints, we were only able to train the model to verify video generation for up to two seconds. Second, we observed that APT can negatively impact text alignment, which we aim to address in future works."}]}