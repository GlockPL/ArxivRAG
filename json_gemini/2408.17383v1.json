{"title": "MoRe Fine-Tuning with 10x Fewer Parameters", "authors": ["Wenxuan Tan", "Nicholas Roberts", "Tzu-Heng Huang", "Jitian Zhao", "John Cooper", "Samuel Guo", "Chengyu Duan", "Frederic Sala"], "abstract": "Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential to cheaply and easily specialize large pretrained models. However, the most prominent approaches, like low-rank adapters (LORA) depend on heuristics or rules-of-thumb for their architectural choices-potentially limiting their performance for new models and architectures. This limitation suggests that techniques from neural architecture search could be used to obtain optimal adapter architectures, but these are often expensive and difficult to implement. We address this challenge with Monarch Rectangular Fine-tuning (MoRe), a simple framework to search over adapter architectures that relies on the Monarch matrix class. Theoretically, we show that MoRe is more expressive than LoRA. Empirically, our approach is more parameter-efficient and performant than state-of-the-art PEFTs on a range of tasks and models, with as few as 5% of LoRA's parameters.", "sections": [{"title": "1. Introduction", "content": "Large pretrained 'foundation' models (Bommasani et al., 2021) were originally conceived as a convenient base for rapidly building applications. The size and complexity of these models; however, paradoxically often made specialization more complex and challenging than traditional machine learning. Recently, adapters, like the popular LoRA (Hu et al., 2021), have dramatically decreased the cost of specialization. This has unlocked the potential of foundation models for efficient use in everyday settings\nDespite their popularity, parameter-efficient adapter techniques make particular architectural assumptions, such as the eponymous low rank in LoRA (Hu et al., 2021; 2023;\nZhang et al., 2023; Chavan et al., 2023; Liu et al., 2024b). These assumptions are a good fit for certain models, tasks, and datasets but may result in poor performance on others. There has been a resulting arms race of parameter-efficient fine-tuning (PEFTs) techniques, each with their own benefits and drawbacks. This suggests that the right adapter architecture should be learnable.\nLearning architectures is the traditional domain of neural architecture search (NAS). Unfortunately, most NAS techniques are heavyweight (Pham et al., 2018; Liu et al., 2019a; Li & Talwalkar, 2020; Li et al., 2021), creating a tension: NAS may learn better adapter architectures for a particular task but costs substantially more compute-sacrificing much of the benefits of adapters in the first place.\nWe show how to resolve this tension by relying on the Monarch matrix class (Dao et al., 2022a). This class presents a simple parametrization that can express a vast range of structured matrices, enabling conveniently learning a wide variety of parameter-efficient architectures. In other words, building adapters from Monarch matrices simultaneously produces two benefits-flexibly searching over architectures and efficient training for adapters.\nBased on this idea, we introduce a simple PEFT framework called Monarch Rectangular Fine-tuning (MoRe). We study its expressiveness properties theoretically and validate it empirically. When fixing block configuration after extensive architectural ablations, the most performant adapter we produced via MoRe is 10\u00d7-20\u00d7 more parameter-efficient than LORA and has the fewest tunable hyperparameters among all PEFTs. We open-source all code to reproduce our experiments at https://github.com/SprocketLab/\nsparse_matrix_fine_tuning."}, {"title": "2. Related Work", "content": "PEFT methods trade off mechanisms for parameter-efficiency and performance. These mechanisms are designed heuristically, and may not be the best choice for all settings. Popular methods such as LoRA (Hu et al., 2021) may not strike the best tradeoff between efficiency and performance. Other methods often sacrifice increased complexity and a reliance on search for improved performance, limiting scalability. Methods such as GLORA and AdaLora (Chavan et al., 2023; Zhang et al., 2023) require expensive search for their rank and block configurations. Our goal is to strike a better tradeoff compared to current PEFT techniques, all while avoiding expensive search procedures. We describe the relation between MoRe and existing techniques below.\nOrthogonal Fine-tuning. Butterly Orthogonal Fine-tuning (BOFT) (Liu et al., 2024b) uses a compute-heavy neighbor of Monarch, the butterfly matrices, to parameterize Cayley-transformed orthogonal blocks that are multiplied with the original weight matrices. It has two more tunable parameters, the block number and size. In contrast, MoRe does not require tuning the number of blocks or rank and is more performant and parameter-efficient than BOFT.\nRepresentation Editing. ReFT (Wu et al., 2024) is a prompt editing method operating on low-rank subspaces. It balances expressivity and parameter efficiency by only intervening on selected layers and tokens, often surpassing PEFTs that adapt model weights. However, it induces inference overhead and an even larger search space than LoRA (token positions and layers to intervene). It is also somewhat less well-understood compared to existing PEFTs from a theoretical point of view."}, {"title": "3. MoRe Framework", "content": "Monarch matrices (Dao et al., 2022a) are a rich collection of block-sparse structured matrices that subsume butterfly matrices and belong to the Kaleidoscope matrices (Dao et al., 2020), a class of matrices that can represent any structured matrix and a variety of transforms such as the Fourier transform, cosine transforms, and Hadamard transform. Unlike structure-agnostic matrix families, such as those of low rank, Monarch matrices can have arbitrary rank, and their products are not closed, allowing for a richer matrix class as more Monarch matrices are multiplied.\nLet n be the dimensions of the Monarch matrix M, i.e. M\u2208 Rnxn. Define N as the number of blocks in component matrices L and R and ruk as the rank of each sub-block. In the standard formulation, ruk = n/N. Monarch matrices have the following structure:\n$$M = P_1LP_2R,$$\nwhere P\u2081 and P2 are permutation matrices and L and R are block diagonal (see Figure 1).\nOriginal work in Monarch matrices focused on the case where L and R are block-wise square, but the family of Monarch matrices is more general and includes low-rank Monarch matrices. This extension allows for L and R to be rectangular, with similarly shaped block diagonal components. This allows the overall rank of the Monarch matrix to\nbe constrained by forcing L and R to have similar shapes to LORA components-but with fewer parameters, as Monarch only contains non-zero entries within the diagonal blocks.\nMoRe Fine-Tuning. During training, for a pretrained weight matrix W \u2208 Rm\u00d7n and bias b \u2208 Rm, we apply MoRe via:\n$$More(x) = Wx + Mx + b,$$\nand update L and R, where L has shape (N, ruk, n/N) and R has shape (N, n/N, ruk). During inference, W absorbs M as in LoRA so there is zero additional overhead.\nMonarch matrices were originally proposed to accelerate pre-training, using two block-wise square monarch factors to substitute one dense matrix multiplication, with O(n\u221an) FLOPs. However, an interesting property of these matrices from rectangular factors is that even though each block is constrained to rank rik, the overall product can have a rank as large as r = Nruk. We set N to 4 for the best rank-sparsity balance. MoRe can achieve the same rank as LoRA with far fewer parameters, which empirically translates to"}, {"title": "3.1. Architectural Choices & Analysis", "content": "Inspired by work on search-based adaptation: AdaLoRA (Zhang et al., 2023) which adaptively allocates parameters for different ranks, and GLORA (Chavan et al., 2023), which tunes the adapter complexity layer-wise, we explored different adapter styles (see Appendix C) as well as trading sparsity (N) and rank (ruk) for the best investment of our parameter budget (Figure 2). Since merely changing N does not change the parameter count, we constrained each block to be square for block number scaling.\nInterestingly, our search converged to a minimal 4-block architecture with the fewest tunable hyperparameters among all methods, without the adapter scaler \u03b1 in LoRA. Our search space trivially subsumes LoRA if we set N to 1. Empirically, MoRe with N = 1 and r = rblk = 8 obtains 68.18 Matthew's Correlation on CoLA, aligning with the 68.3 for rank 8 LORA.\nShould we tune the number of blocks? Due to our rectangular block-diagonal parametrization, increasing N while fixing rak increases the total rank r under the same parameter budget. However, this induces worse performance, possibly because the matrix is sparser and it is harder to converge to a stable subspace. Empirically, performance drops drastically when N > 4 (Figure 3).\nRelationship To BOFT. BOFT (Liu et al., 2024b) uses butterfly matrices (Dao et al., 2019; 2020), a related class of structured matrices. Monarch (Dao et al., 2022a) was proposed to replace butterfly matrices due to their hardware-unfriendly sparsity patterns. While it has O(n log n) FLOPs, it is empirically 2x slower than LoRA and occupies much more memory, which we show in the following.\nTheoretical Analysis. One advantage of MoRe is that it is amenable to a theoretical analysis of its expressivity, mirroring that of LoRA (Zeng & Lee, 2024). We show in Appendix A that MoRe is more expressive than LoRA."}, {"title": "4. Experimental Results", "content": "We conducted experiments on three challenging NLP tasks covering over 20 datasets: commonsense reasoning, math reasoning, and language understanding of models ranging from Roberta-large to Llama 7B. We follow the widely adopted dataset settings in LLM-Adapters (Hu et al., 2023) and ReFT (Wu et al., 2024). All experiments are performed on a single NVIDIA A100 40G, and use Flash Attention (Dao et al., 2022b) when applicable. As we shall see, besides fixing N, MoRe needs almost no tuning for rank rik.\nCommonsense Reasoning. We train the Llama 1 7b model (Touvron et al., 2023) on the challenging Commonsense170k benchmark in (Hu et al., 2023) consisting of eight commonsense reasoning tasks. The model is prompted with multiple-choice problems to output the correct choice without step-wise reasoning. We report accuracy on the test set in table 1, and hyperparameter details can be found in B. Note that MoRe with Llama 7B largely surpasses the state-of-the-art ReFT with Llama 13B with around 1/6 of its training steps (3 epochs).\nMath Reasoning. We train Llama 1 on the Math 10k dataset consisting of seven complex math reasoning tasks from Hu et al. (2023). Following Wu et al. (2024), we only used 4 datasets for final evaluation to avoid data leakage. The results are shown in Table 2.\nNatural Language Understanding. We evaluate MoRe on the GLUE benchmark (Wang et al., 2018) to show its superior parameter efficiency on small LLMs. We fine-tune ROBERTa-large 350M (Liu et al., 2019b) on eight datasets consisting of tasks such as sentiment classification and natural language inference, and report performance on the evaluation set following Hu et al. (2021) over 3 different random seeds. Classifier heads are excluded from the parameter count. We use fp32 for all GLUE tasks, and hyperparameter tuning is done for each task separately (Appendix B). By default, we adapt query, key, and values. Notably, MoRe is on par with LoRA even with ruk = 1 and 0.14M parameters, and outperforms all other methods when r\u044bk = 4.\nMemory Cost and Runtime. Modern GPUs rely on the tensor cores to accelerate matrix multiplication. MoRe leverages optimized CUDA batched matrix multiplication (BMM) kernels to populate the tensor core with many small matrices, with on par or better performance than GEMM. Here we show how our training speed compares with BOFT\u00b9 and LoRA in Table 4, using the setting in our training experiments (see Appendix B). For Llama, we apply bf16, flash attention, and adapt all linear modules by default. Notably, BOFT runs out of memory even on H100 80G, rendering it impractical for large models. MoRe slightly lags behind\nBOFT's public implementation does not support bf16 and fp16, so we added these features.\nLORA for the 350M ROBERTa due to the overhead of permutations allocating extra memory and multiple CUDA kernel launches, which we will address in a future Triton implementation, but excels in larger models due to its parameter efficiency."}, {"title": "5. Conclusion", "content": "We introduced MoRe, a framework for searching for high-quality adapter architectures via Monarch matrices. MoRe offers excellent performance and has multiple promising directions for future work (described in Appendix F)."}, {"title": "Appendix", "content": "We show a theoretical finding for the expressiveness of MoRe in the spirit of Zeng & Lee (2024).\nWe start with a simple result.\nLemma A.1. Let W be an n \u00d7 n matrix, where n = m\u00b2 for some integer m. Let Wjk denote the submatrix of W such that\n$$W =\n\\begin{bmatrix}\nW_{11} & W_{12} & \\cdots & W_{1m} \\\\\nW_{21} & W_{22} & \\cdots & W_{2m} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nW_{n1} & W_{m2} & \\cdots & W_{mm}\n\\end{bmatrix}$$\nLet x \u2208 R, with a similar decomposition into xk for k =\n1, 2, . . ., m. Then ||Wx||2 \u2264 \u2211jk ||Wjkxk||2.\nProof. We have that\n$$||Wx||_2 =\n\\begin{bmatrix}\nW_{11} & W_{1m \\\\\n\\vdots & \\vdots \\\\\nW_{n1} & W_{mm\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\n\\vdots \\\\\nX_n\n\\end{bmatrix}\n$$$$=\n\\begin{Vmatrix}\nW_{11}x_1 + ... + W_{1m}x_m \\\\\n\\vdots \\\\\nW_{m1}x_1 + ... + W_{mm}x_m\n\\end{Vmatrix}_2$$$$<\\sum ||W_{j1}x_1 + ... + W_{jm}x_m||_2 \\\\\n<\\sum ||W_{jk}x_k||_2$$\njk\nCorollary A.2. Let W be an n \u00d7 n matrix, where n = m\u00b2 for some integer m. Let Wjk denote the submatrices of W. Then \u03c3\u2081(W) \u2264 \u2211jk01(Wjk).\nProof. Note that \u03c3\u2081(M) = ||M||2 for any matrix M. Let x \u2208 Rn such that ||x||2 = 1 and ||Wx||2 = ||W||2. Using Lemma A.1,\n$$||W||_2 = ||Wx||_2 < \\sum_{jk}||W_{jk}x_k||_2$$\nSince || Wjkxk||2 \u2264 ||Wjk||2. ||xk||2 \u2264 ||Wjk ||2, we have\n$$||W||_2 < \\sum_{jk}||W_{jk}x_k||_2 < \\sum_{jk}||W_{jk}||_2$$\nTheorem A.3. Suppose both the target and frozen model are linear and respectively parameterized by W and W = \u03a0-1 W with both W and W full rank. Assume that r = N for these Monarch matrices, i.e. the Monarch factors are square with square blocks on the diagonal. The adapted model is allowed to fine-tune the last layer's parameters with a Monarch matrix: W = \u03a0-\u00b9W(WL + AW\u2081), where Aw\u2081\u2208 M. Define error between the target and the frozen model as E = W \u2013 W, and regularized error as E = (\u03a0\u00b9 W\u2081)\u2212\u00b9E. The estimation error between the adapted model and the target model is bounded:\n$$||W - W||_F < || \\prod_{l=1}^{L-1}W_l||\\cdot||\\tilde{E} - \\Delta W_l||_F$$$$ = || \\prod_{l=1}^{L-1}W_l|| (\\sum_{j,k} (\\sum_{i=r/N + 1}^n \\sigma_i^2 (\\tilde{E}_{:,j,k,:}))))$$\nwhere \u03c3i is the i-th eigenvalue of the given function and Eijkl is E reshaped into a 4-D tensor.\nProof. The proof directly follows the decomposition in the Monarch paper (Dao et al., 2022a) and the previously derived results.\nWe now use a worst-case to illustrate how the Monarch approximation differs from a rank-1 approximation. Let A be any matrix of size n \u00d7 n. Reshape A into a 4D tensor A of dimension m \u00d7 m \u00d7 m \u00d7 m, where m = \u221an. Then in the worst case, each sub-matrix is of full-rank m and the singular values are all equal. An optimal monarch matrix M in Frobeneus norm performs a rank-1 approximation for each sub-matrix. The estimation error ||A \u2013 M|| can be interpreted as all unexplained singular values, whose proportion is m-1. Hence ||A - M|| = m1||A||. This provides a bound when in a general case.\nm\nNow consider a rank-1 approximation of A. In the worst case, since A's rank cannot be smaller than m (a full matrix's rank is always equal or greater than the rank of its sub-matrix), let A be of rank m. Suppose A's non-zero singular values are still all equal (?). The estimation error for a rank-1 approximation D of A will be ||A \u2013 D||} = m-1||A||}, which equals the Monarch approximation. However, in other cases where A's rank is greater than m, a Monarch approximation is strictly better than a rank-one approximation.\nm\nA.1. Optimizations for Rectangular Monarch matrices\nThere are two distinct cases with variable-rank Monarch matrices. Each case depends on how the block rank compares to the block number.\nLet n be the dimensions of M, the Monarch product, let N be the number of blocks in each factor L and R, let m =\nn/N be the block width, and let r be the block rank. In total, we have that M, L, and R are of dimension (n, n), (n,r), and (r, n) respectively. When compressed into 4- and 3-tensors, these have dimension (m, N, N, m), (N, r, m), and (N, m, r) respectively. To investigate the behavior of M = P1LP2R, consider a vector x \u2208 Rn and how M transforms this vector. Reshape x into a 2-tensor with dimensions (N,m).\nFirst, assume N > r where r divides N and let b = N/r. For this case, further reshape M, L, R, and x into shapes (m,r,b,r,b,m), (r,b,r,m), (r,b,m, r), and (r, b, m), which is possible since rb = N.\n\u2022 Apply R: This results in the intermediate Ykbj = Ei RkbjiXkbi\n\u2022 Apply P2: This transposes the first and third coordinates of y, so Ykbj \u2192 Yjbk.\n\u2022 Apply L: This results in the intermediate zjbl = \u03a3k LjblkYjbk.\n\u2022 Apply P\u2081: This again transposes the first and third coordinates of z, so zjbl \u2192 Zlbj.\nIn total, this amounts to computing Zlbj = Ek,i LjblkRkbjiXkbi = \u03a3k,i MljbkbiXkbi. We then can define Mljbkbi = Ljblk Rkbji which defines the operation Zlbj = \u03a3k,i MljbkbiXkbi. Notice that the optimal solution can be found through a collection of rank-1 decompositions of M:jbkb:, each of size (m,m). This common index b implies that whenever those coordinates in the 6-tensor disagree, this Monarch product contains zeros, so this decomposition will be sparse. Next, assume that N < r where N divides r and let b = r/N. For this case, further reshape L and R into shapes (N, N, b, m) and (N, m, N, b), which is possible since Nb = r.\n\u2022 Apply R: This results in the intermediate Ykjb = \u2211i Rkjbilki\n\u2022 Apply P2: This transposes the first and second coordinates of y, so ykjb \u2192 Yjkb.\n\u2022 Apply L: This results in the intermediate zij = \u03a3k,b LjlkbYjkb.\n\u2022 Apply P\u2081: This again transposes the first and second coordinates of z, so zij \u2192 Zjl.\nIn total, this amounts to computing Zjl = Ek,i,b LjlkbRkjbiXki = \u03a3k,i Mljkilki. We then can define Mljki = b LjlkbRkjbi which defines the operation Zlbj = \u2211k,i Mljkilki. Notice that the optimal solution can"}, {"title": "B. Hyperparameter Tuning", "content": "be found through a collection of rank-b decompositions of M:jk:, each of size (m, m).\nUsing these decompositions, we obtain some straightforward extensions of Theorem A.3.\nTheorem A.4. Suppose both the target and frozen model are linear and respectively parameterized by W and W = \u03a0-1 W with both W and W full rank. Assume that N < r with r a multiple of N. The adapted model is allowed to fine-tune the last layer's parameters with a Monarch matrix: W = W(WL + AW\u2081), where w\u2081 \u2208 M. Define error between the target and the frozen model as E = W \u2013 W, and regularized error as \u1ebc = (\u03a0\u00b9 W\u2081)\u2212\u00b9E. The estimation error between the adapted model and the target model is bounded:\n$$||W - W|| < || \\prod_{l=1}^{L-1}W_l||\\cdot||\\tilde{E} - \\Delta W_l||$$$$ = || \\prod_{l=1}^{L-1}W_l|| (\\sum_{j,k} (\\sum_{i=r/N + 1}^n \\sigma_i^2 (\\tilde{E}_{:,j,k,:}))))$$\nwhere \u03c3i is the i-th eigenvalue of the given function and Eijkl is \u00c9 reshaped into a 4-D tensor.\nNotice the difference in the rightmost sum. The sum over i starts at r/N + 1 instead of 2.\nNext, we provide experimental details.\nWe use the asynchronous successive halving algorithm (ASHA) (Li et al., 2020) to efficiently search and early-stop on our 8 * A100 cluster."}, {"title": "B.1. GLUE Language Understanding", "content": "For BOFT, we took the hyperparameters for DeBERTA-v3 base on GLUE from their paper and tuned the learning rate only. For MoRe, we started from the hyperparameters in (Hu et al., 2021) and randomly sampled the learning rate and batch size. We present the hyperparameters in table 5."}, {"title": "B.2. Math reasoning and Commonsense reasoning", "content": "For these challenging reasoning tasks, we found performance to be less sensitive to hyperparameters. We took 1000 examples and 10,000 examples from Math10K and Commonsense170K as the tuning evaluation set, respectively. We present the hyperparameters in table 6."}, {"title": "C. Architecture Ablations", "content": "With 3 potential architectural hyperparameters (\u0442\u044b\u043a, N and whether to use square blocks) in our setup, one might ask whether we should use NAS to find the most efficient architecture.\nWe tested using monarch as a multiplicative factor instead of an additive factor as in BOFT, adding a scaler \u03b1 on the adapter outputs as in LoRA and adding a scaler parameter; all underperform our default 4-block configuration. We also tried including rik and N in our hyperparameter search to mimic NAS, but all runs converged to the configuration with the largest parameter count, with marginal performance gains. Therefore we didn't pursue expensive NAS algorithms."}, {"title": "D. Learned Weight Distributions", "content": "We demonstrate in figure 4 and 5 that the trained block-diagonal matrices approximate Gaussian distribution well as the amount of training increases, in an attempt to interpret the results."}, {"title": "E. Failure Cases", "content": "Inspired by Meng et al. (2024) that fine-tuning is strengthen-ing some task-specific subspace components, we attempted using the dense to sparse projection algorithm (block-wise SVD) from (Dao et al., 2022a) to initialize MoRe from principal components. However, the method fails to converge on reasoning tasks and obtains only a 57.9 correlation on CoLA.\nWe've also tested naively replacing the low-rank projections in ReFT with a single Monarch factor P plus permutation P1, which only achieved a 19.5 correlation on CoLA."}, {"title": "F. Limitations and Future Work", "content": "Currently, MoRe poses a few limitations that we are working to address.\n1. MoRe is implemented with two BMMs and two permutations, which introduces overhead due to 4 CUDA kernel launches. With machine learning compilers such as Triton(Tillet et al., 2019), it's easy to fuse them into one kernel and recompute the activations during backward, with memory savings and speed-up. We're testing the Triton implementation's precision.\n2. We seek to substitute low-rank projections. A natural extension from our low-rank adaptation use case is to establish MoRe as a general drop-in low-rank projection module. However as shown in the Appendix E, it does not work directly with ReFT.\n3. Projection subspace interpretation: we show (Appendix D) that Monarch weights approach Gaussian distribution. However, we've not explored the subspace similarity between the dense and MoRe projections such as which dense components are strengthened by MoRe, due to complicated block-diagonality. Such an understanding may enable us to initialize MoRe from dense matrices' principal components as in Meng et al. (2024) with improved convergence and performance, and explain why scaling rank doesn't always deliver performance."}, {"title": "G. Pseudocode", "content": "As the permutations P\u2081 and P2 may be less intuitive, we provide a minimal PyTorch pseudocode to demonstrate their usage below."}]}