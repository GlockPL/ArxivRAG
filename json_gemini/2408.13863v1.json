{"title": "CodeGraph: Enhancing Graph Reasoning of LLMs with Code", "authors": ["Qiaolong Cai", "Zhaowei Wang", "Shizhe Diao", "James Kwok", "Yangqiu Song"], "abstract": "With the increasing popularity of large language models (LLMs), reasoning on basic graph algorithm problems is an essential intermediate step in assessing their abilities to process and infer complex graph reasoning tasks. Existing methods usually convert graph-structured data to textual descriptions and then use LLMs for reasoning and computation. However, LLMs often produce computation errors on arithmetic parts in basic graph algorithm problems, such as counting the number of edges. In addition, they struggle to control or understand the output of the reasoning process, raising concerns about whether LLMs are simply guessing. In this paper, we introduce CodeGraph, a method that encodes graph problem solutions as code. The methods solve new graph problems by learning from exemplars, generating programs, and executing them via a program interpreter. Using the few-shot setting, we evaluate CodeGraph with the base LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and Mixtral-8x7B Instruct. Experimental results on six tasks with six graph encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on the task. Compared to the existing methods, CodeGraph demonstrates strong performance on arithmetic problems in graph tasks and offers a more controllable and interpretable approach to the reasoning process.", "sections": [{"title": "Introduction", "content": "The natural language processing (NLP) community has witnessed the recent advancement of large language models (LLMs) (Radford et al. 2019; Brown et al. 2020a; Ouyang et al. 2022; Touvron et al. 2023; Wei et al. 2021), which achieve state-of-the-art results (Mao et al. 2023) in diverse tasks such as code generation (Bubeck et al. 2023), information retrieval (Sun et al. 2023), and question answering (Team et al. 2024a). However, sole reliance on natural language sometimes leads to hallucination (Huang et al. 2023), where results deviate from user inputs. A common solution is to adapt external information after training dates to answer questions that require dynamically changing knowledge (Vu et al. 2023). Among external information, graph-structured data can flexibly represent additional factual and fresh data, which is one of the most flexible ways to mitigate the hallucination issue (Lewis et al. 2021; Guu et al. 2020; Pan et al. 2024). LLMs have been shown to adapt their parametric knowledge when supplied with new graphs (Kadavath et al. 2022). Thus, an underlying yet crucial question attracts more and more research attention (Wang et al. 2024a): Can LLMs reason with graphs?\nTo answer this question, most studies map graphs to textual description and adapt natural language with prompting heuristics, such as Chain of Thought (CoT) (Wei et al. 2023), to solve graph algorithm problems using generative LLMs (Ye et al. 2024; Liu and Wu 2023a; Huang et al. 2024; Tang et al. 2024a). Some studies also extend the input texts with soft tokens (Perozzi et al. 2024). Those textual description-based approaches convert node and edge lists in a graph into plain text with prompts (Liu and Wu 2023b; Zhang et al. 2024; Wang et al. 2024a). Among them, Fatemi, Halcrow, and Perozzi (2023) further explores various prompts (such as representing a node as a name, with edges indicating various relationships like friendships or co-authorships) to express the graph structure in natural language. However, those approaches utilize LLMs for both reasoning and computation in natural language, i.e., the language model not only needs to generate the texts for constructing graphs using nodes and edges but also needs to perform computation for some basic graph tasks such as node degree and edge count. We argue that text descriptions are not ideal for solving basic graph tasks based on the findings in our experiments: (1) LLMs are prone to arithmetic calculation errors, such as counting the degree of a node and counting the number of edges. (2) LLMs' performance on basic graph tasks is highly sensitive to the prompt template that converts graphs to natural language. (3) Different graph structures significantly influence the effectiveness of LLMs when assessing graph-related tasks. For instance, on the cycle check task, LLMs tend to have a strong bias toward graphs containing cycles. This bias can lead to lower accuracies when analyzing acyclic structures such as path graphs and higher accuracies with complete graphs, which always contain cycles.\nIn this work, we propose CodeGraph, a prompting framework that generates solutions in code using an LLM for basic graph tasks and delegates the computation steps to"}, {"title": "Related Work", "content": "Recently, research on combining graph learning with large language models has become a rapidly growing area. The community has dedicated various works to studying multiple tasks, including node classification (Chen et al. 2024b; Ye et al. 2024), reasoning in KG (Do et al. 2024), KG construction (Neuhaus 2023), molecular learning (Le et al. 2024), and others (Li et al. 2023). Among them, our work focuses on utilizing LLMs' coding ability to solve basic graph"}, {"title": "Task Formulation", "content": "In this paper, we study the performance of LLMs on basic graph problems, including connected nodes, cycle check, edge existence, edge count, node degree, and node count (the definitions are in Appendix A). Here, we treat the basic graph problems in a sequence-to-sequence format.\nFirst, a graph \\(G = (V, E)\\) and a basic graph problem Q are provided as input to the LLM, where V denotes the vertices (nodes) and \\(E \\subset (V \\times V)\\) denotes the edges. Each graph G is then converted to natural language using an encoding function by listing the nodes and edges in the graph. In the following, we use f to denote an LLM, and A be the answer (output) for a given question Q, i.e., \\(A = f(G, Q)\\)."}, {"title": "Methodology", "content": "In this section, we introduce the components of CodeGraph, including its graph encoding functions, task descriptions, exemplars, and a test example."}, {"title": "Graph Encoding Functions", "content": "Following GraphQA (Fatemi, Halcrow, and Perozzi 2023), we translate graph-structured data into natural language using six graph encoding functions: adjacency, friendship, co-authorship, incident, social network, and expert encoding. These functions encode nodes as integers, names, or letters, and edges as relationships such as 'friends' or 'coworkers', forming the textual descriptions for questions in basic graph tasks. Below, we present the details and examples of the Adjacency and Friendship encoding functions. Descriptions of the other encoding functions can be found in Appendix B.\n\u2022 Adjacency: Nodes are encoded as integers, and edges are represented using parentheses.\nAdjacency:\nIn an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\nG describes a graph among nodes 0, 1, 2, 3, and 4.\nThe edges in G are: (0, 1) (0, 2) (1, 2) (2, 3) (2, 4).\n\u2022 Friendship: Nodes are represented by common English first names, with edges denoting friendships."}, {"title": "CodeGraph Method", "content": "Figure 2 shows the components of the proposed CodeGraph, including task descriptions, exemplars, and a test example. In the few-shot setting, a few exemplars of (question, sample code) pairs are prefixed as demonstrations to guide the LLM in generating a program depending on the graph task.\nThe task description begins by defining the role of the LLM, instructing it to generate code that solves a basic graph task using the given example. The task description then specifies the graph task to be solved and standardizes the expected input and output formats for the Python code (e.g., \"Write a piece of Python code to return the answer in a variable ans. Please enclose the code with # CODE START and # CODE END. Assume edges to be an empty list (edges = []) if not provided.\") A complete example of the task description can be seen in Appendix A.\nExemplars follow the task description in the prompt template. As shown in Figure 2, each exemplar has two parts: a question and a sample code. The question begins with 'Q:' and presents a graph question based on the task (e.g., \"Is node 8 connected to node 5?\" for the edge existence task). Then, the question describes a graph where nodes and edges are translated into natural language using encoding functions. The concrete prompt for graph questions is provided in Appendix C.\nFor each example, the sample code starts by mapping the text of the nodes and edges into code, which is stored in Python lists or adjacency lists, depending on the task. It then continues with 'A:' and a Python function. Each graph task has a specific Python function, with nodes and/or edges as input parameters, depending on the task. An example function for the edge existence task is shown at the bottom of Figure 2. To ensure executions of the Python interpreter, we employ special tokens \u2018# CODE START' and \u2018# CODE END' to mark obvious boundaries of the Python function and its inputs. We propose different Python functions to solve the six basic graph tasks. The detailed sample code for each task is provided in Appendix D.\nThe final part of the LLM input is the test example. It follows the same format as the question in the exemplar but uses different graphs. Typically, it employs the same graph encoding function as the exemplar unless specified otherwise. It ends with 'A:' to prompt the LLM to convert the graph into lists or adjacency lists, and then generate a program based on the converted graph."}, {"title": "Experiments", "content": "We conduct experiments on the GraphQA benchmark (Fatemi, Halcrow, and Perozzi 2023), which contains a set of diverse fundamental graph problems. In this paper, there are 500 graphs in the training set and 500 graphs in the testing set. The graph size ranges from 5 to 20 nodes.\nTo enhance the diversity of graph structures for the basic graph tasks, we follow GraphQA to utilize the NetworkX library (Hagberg, Swart, and S Chult 2008) to generate random graphs using several algorithms, including the Erd\u0151s-R\u00e9nyi (ER) (Erd\u0151s and R\u00e9nyi 1959) model, Barab\u00e1si-Albert model (BA) (Albert and Barab\u00e1si 2002), scale-free networks (SFN) (Barab\u00e1si and Albert 1999) and stochastic block model (SBM) (Holland, Laskey, and Leinhardt 1983) as well as generators for path, star, and complete graphs (Fatemi, Halcrow, and Perozzi 2023)."}, {"title": "Metrics", "content": "We adopt exact match scores for evaluation, which measure whether the predicted answer matches the correct answer exactly. For each task, we extract the answer from the LLM's response and compare it with the ground truth following the GraphQA benchmark. Unlike other tasks, connected nodes requires LLMs to return a list of nodes. In this task, we do not consider the order of generated nodes. An answer is considered correct as long as the set of returned nodes is correct."}, {"title": "Baselines", "content": "Experiments are performed on four LLMs: GPT-3.5 Turbo (OpenAI 2023), Llama3-70B Instruct (Dubey et al. 2024), Mixtral-8x7B Instruct (Jiang et al. 2024), and Mixtral-8x22B Instruct (Mistral AI Team 2024). Then, we test them under both zero-shot (Wei et al. 2022) and in-context learning (few-shot) (Brown et al. 2020b) settings. We also test Chain of Thought (CoT) (Wei et al. 2023) under the few-shot setting, allowing the LLMs to learn reasoning steps from sequences of examples and apply them to new inputs. In addition to the above baselines, results for the graph-based methods and transformer models baselines are in Appendix E."}, {"title": "Evaluation with Different Graph Encodings", "content": "In this experiment, we evaluate the performance of CodeGraph on the basic graph tasks: connected nodes, cycle check, edge existence, edge count, node degree and node count with different encoding functions including adjacency, incident, co-authorship, friendship, social network and expert encodings. Table 1 presents the accuracies for our method and baselines, evaluated using the GPT-3.5 Turbo.\nAs can be seen, the baselines perform poorly on nearly all basic graph tasks, especially on node degree, edge count, and connected nodes tasks. In contrast, using code to encode solutions with our prompting technique significantly outperforms the baselines in all tasks. The results show more than 40% improvements for most encoding methods in tasks node degree, edge count, and connected nodes compared to"}, {"title": "Evaluation with Different Graph Structures", "content": "In Section 5.4, we conduct experiments solely on the graphs generated by the ER generator. In this experiment, we evaluate the robustness of CodeGraph across various graph structures. Here, various graph generators are employed to create graphs, including ER, BA, SBM, Star, SFN, Path, and Complete graphs (Fatemi, Halcrow, and Perozzi 2023). We sample 500 graphs for each graph structure. This experiment selects two graph encoding methods: friendship and adjacency. For each task, the LLM is provided with an exemplar that has the same graph structure as the test problem.\nTable 2 shows our method's accuracy on different graph structures for all basic graph tasks. We can see that our method CodeGraph performs robust reasoning in all graph structures. For example, the average accuracy across multiple graph structures in all tasks exceeds 90%. For individual tasks, the accuracy differences between the best and worst performance using our approach with different graph structures are less than 5.4%, including tasks like edge existence, node count, edge count, and connected nodes. This suggests that CodeGraph is robust for the arithmetic tasks requiring the LLM to understand graph structure."}, {"title": "Evaluation with Generalization Across Diverse Graph Structures", "content": "In this experiment, we test the ability of CodeGraph to generalize across graph structures different from that of the exemplars. The difference between Section 5.5 and this experiment is that we use an ER graph as the exemplar, and the test example is sampled using other graph structures, including BA, SBM, Star, SFN, Path, and Complete graphs. This is a more reflective real-world application to test LLMs' ability to extrapolate and enhance their basic knowledge when faced with more complex graph-structured data. This experiment tests whether the LLMs can generate the correct code when provided with exemplars from a different graph structure. Also, we consider the graph encoding methods of"}, {"title": "Evaluation with Different LLMs", "content": "In this experiment, we investigated whether CodeGraph can be applied to other LLMs. We selected Llama3-70B Instruct and Mixtral-8x22B Instruct and followed the same"}, {"title": "Evaluation with Smaller LLM and Weaker Coding Capability", "content": "In this experiment, we evaluate our method on a smaller LLM: Mixtral-8x7B Instruct, whose coding ability is much lower than GPT-3.5 Turbo. For example, Mixtral-8x7B In-"}, {"title": "Conclusion", "content": "In this work, we present the first study on enhancing the graph reasoning capabilities of LLMs using code for basic graph tasks. Previous works usually convert graphs to natural language. Here, we demonstrate that our proposed method can use programming languages to significantly improve performance across various basic graph tasks. The experiments consistently show robustness across different graph encoding functions and various graph structures, highlighting the effectiveness of our approach. We also illustrate that this improvement in graph reasoning can be extended to other LLMs. Then, we also test our method with Mixtral-8x7B Instruct to show the performance of our method with less powerful coding capabilities. These novel results provide valuable insights into the graph reasoning abilities of LLMs using code, which can boost performance on basic"}]}