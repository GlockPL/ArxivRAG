{"title": "Positive Text Reframing under Multi-strategy Optimization", "authors": ["Shutong Jia", "Biwei Cao", "Qingqing Gao", "Jiuxin Cao", "Bo Liu"], "abstract": "Differing from sentiment transfer, positive reframing seeks to substitute negative perspectives with positive expressions while preserving the original meaning. With the emergence of pre-trained language models (PLMs), it is possible to achieve acceptable results by fine-tuning PLMs. Nevertheless, generating fluent, diverse and task-constrained reframing text remains a significant challenge. To tackle this issue, a multi-strategy optimization framework (MSOF) is proposed in this paper. Starting from the objective of positive reframing, we first design positive sentiment reward and content preservation reward to encourage the model to transform the negative expressions of the original text while ensuring the integrity and consistency of the semantics. Then, different decoding optimization approaches are introduced to improve the quality of text generation. Finally, based on the modeling formula of positive reframing, we propose a multi-dimensional re-ranking method that further selects candidate sentences from three dimensions: strategy consistency, text similarity and fluency. Extensive experiments on two Seq2Seq PLMS, BART and T5, demonstrate our framework achieves significant improvements on unconstrained and controlled positive reframing tasks.", "sections": [{"title": "1 Introduction", "content": "The concept of style transfer initially emerges within the domain of computer vision (CV) with the objective of accomplishing image style transfer (Gatys et al., 2016). Inspired by this, Hu et al. (2017) proposed text style transfer (TST), whose main purpose is to automatically control the text style and preserve the style-independent content. There also have been some related research before this, such as paraphrase (Xu et al., 2012). In recent years, there has been an increasing focus on TST, which has gradually evolved into a significant subfield within the domain of natural language generation. Many corresponding task variants also have been proposed, such as text form transfer (Briakou et al., 2021), topic transfer (Huang et al., 2020), text simplification (Cao et al., 2020), and sentiment transfer (Mueller et al., 2017), etc.\nAmong them, sentiment transfer primarily focuses on reversing the sentiment polarity of the original text. However, it relies on the straightforward replacement of opinion words, such as substituting negative opinion words with their positive counterparts of the opposite meaning. On the one hand, it retains the content irrelevant to style to some extent, such as the invariance of described object entities. On the other hand, it also inherently alters the meaning of the original text (Liao et al., 2018; Li et al., 2018). To this end, Ziems et al. (2022) proposed positive reframing. In contrast to sentiment transfer, positive reframing adopts principles from psychology to reframe negative text by introducing a complementary positive viewpoint while simultaneously maintaining the underlying meaning conveyed in the original text. A toy example of their difference can be seen in Figure 1.\nMore specifically, positive reframing encompasses various tasks, including unconstrained positive reframing, controlled positive reframing, and derivative tasks such as reframe strategy classification. The unconstrained positive reframing task focuses on generating reframed text without explicit guidance of the corresponding reframe strategy. In contrast, the controlled positive reframing task involves reframing text based on the given strategy. And the reframe strategy classification task entails determining the specific strategy employed in reframing text. Ziems et al. (2022) gives six positive reframing strategies, namely growth mindset, impermanence, neutralization, optimism, self-affirmation and thankfulness.\nHowever, most of the existing methods only fine-tune PLMs on the corresponding dataset, ignoring the consistency requirement between the model training objective and the target of positive reframing, and also failing to fully utilize the known condition of the reframing strategy under the controlled setting, making it difficult to ensure that the generated text meets the task requirements. Therefore, this paper proposes a multi-strategy optimization framework (MSOF) for positive reframing and our contributions are as follows:\n\u2022 Firstly, from the target of positive reframing, we design and implement the positive sentiment reward and content preservation reward to optimize the sequence-level training objective, and then apply various decoding improvement approaches to alleviate text degeneration and elevate the quality and diversity of the generated text.\n\u2022 Secondly, we propose a multi-dimensional re-ranking approach based on the modeling formula of positive reframing, which comprehensively evaluates the quality of the candidate text based on strategy consistency, text similarity and fluency.\n\u2022 Extensive experimental results demonstrate that our proposed multi-strategy optimization framework achieves significant improvement on both unconstrained and controlled positive reframing task."}, {"title": "2 Related Work", "content": "Early research on text style transfer mostly relied on artificial design features such as syntax (Zhu et al., 2010) and phrase (Xu et al., 2012) modeling, etc. Similar to other tasks in NLP, the advent of deep learning has resulted in the growing application of neural network models to TST. For example, Jhamtani et al. (2017) investigated the utilization of the Seq2Seq model for transforming modern English into Shakespearean-style English. Wang et al. (2019) applied GPT-2 to accomplish the formal-informal transfer. Sancheti et al. (2020) extended the work of Jhamtani et al. (2017) by incorporating a reinforcement learning framework. Lai et al. (2021) further applied this framework to PLMs. Above studies are mainly based on parallel corpora. Although satisfactory results can be achieved, the cost of constructing parallel corpora is expensive. Therefore, semi-supervised learning and unsupervised learning are widely used in TST. The main methods include data augmentation or text retrieval (Zhang et al., 2020; Jin et al., 2019), adversarial learning (Hu et al., 2017; Fu et al., 2018), back-translation (Prabhumoye et al., 2018; Wei et al., 2023), and reinforcement learning (Luo et al., 2019; Gong et al., 2019).\nSpecific to sentiment transfer, the early goal is to extract sentiment words that describe the corresponding entities, and then replace them with expressions of the opposite sentiment attribute. The representative one is the \u201cDelete, Retrieve, Generate\" strategy (Li et al., 2018). Furthermore, Sudhakar et al. (2019) applied the transformer architecture to the above strategy. To better distinguish content and style, Kim and Sohn (2020) divided the model into sentence reconstruction module and style module to complete their respective task. Han et al. (2023) introduced the adaptive clustering and contrastive learning modules to better explore sentence transmission patterns to main and utilize the latent transfer patterns.\nAlthough sentiment transfer preserves attribute-independent content, the intrinsic meaning of the original text expression is also changed. To this end, Ziems et al. (2022) introduced positive reframing, aiming to preserve the original meaning by substituting negative viewpoints with complementary positive expressions, and constructed the corresponding parallel dataset. For unconstrained positive reframing, Xu et al. (2023) decoupled the sentiment and style of the text to complete the positive reframing. Then, Sheng et al. (2023) further decomposed positive reframing into paraphrase generation and sentiment transfer and constructed corresponding pseudo datasets to fuse generation capabilities through multi-task learning, but also led to the inability to apply their method under the controlled setting."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem Definition", "content": "Let (x, y, \u03c8x) be a triple in the positive reframing task, where x = {x1, x2, ..., xn} is the original text with negative sentiment, and y = {y1, y2,..., ym} is the target sentence with complementary positive expressions corresponding to x, m and n represent the sentence length. V\u2286 {Growth Mindset, Impermanence, Neutralizing, Optimism, Self-affirmation, Thankfulness} is the positive reframing strategy used to reframe the negative text x, which can use multiple strategies simultaneously. This paper researches the following three tasks.\nThe target of unconstrained positive reframing is to generate the target sentence y from the original text x without any reframe strategy guidance. This task can be modeled as follows:\n$p(y|x) = \\prod_{t=1}^{m} p(y_t|x, y_{<t})$   (1)\nwhere yt represents what has been generated before time t.\nRegarding reframe strategy classification, its requirement is to predict the positive reframing strategy \u03c8x used to reframe the original sentence x.\nFor controlled positive reframing, the primary objective is to generate the target sentence y from the original text x under given strategy \u03c8x, This problem can be modeled as the following formula.\n$p(y|x, \\psi_x) = \\prod_{t=1}^{m} P(y_t|x, \\psi_x, y_{<t})$   (2)"}, {"title": "3.2 Framework", "content": "As shown in Figure 2, our proposed framework mainly consists of four modules, namely sequence-to-sequence, reinforcement training, decoding improvement and multi-dimensional re-ranking."}, {"title": "3.2.1 Sequence-to-sequence", "content": "Consistent with Ziems et al. (2022), we also use T5 (Raffel et al., 2019) and BART (Lewis et al., 2020) as the basic text generation model, which are both mainly composed of two components, namely encoder and decoder.\nEncoder This part is to encode original sentence x and reframe strategy Vx into hidden vector H. We use T5 and BART as the basic generation model, and the encoder part is as follows:\nH = Encoder([x1, x2,..., xn], \u03c8x)   (3)\nwhere H \u2208 Rl\u00d7d, l is the length of sequence, and d is the hidden dimension.\nDecoder The output yt of the decoder part takes the hidden vector output of the encoder and the output ybefore time t as input, the equation is as follows.\nyt = Decoder(H; y   (4)"}, {"title": "3.2.2 Reinforcement Training", "content": "As shown in Figure 3, based on the objective of positive reframing, the generated text should transform the negative sentiment of the original text and keep the semantics unchanged. Therefore, we design and implement positive sentiment reward and content preservation reward to optimize the overall training process."}, {"title": "Positive sentiment reward", "content": "We first design the positive sentiment reward loss based on binary cross entropy (BCE). Specifically, we fine-tune the binary sentiment classifier RoBERTa (Liu et al., 2019) and utilize it to determine the sentiment change degree of the generated sentence relative to the original text. The positive sentiment reward loss function is formulated as follows:\np(st|y', x) = Sigmoid(RoBERTa(y', x))   (5)\nLcls = -log(p(st|y',x))   (6)\nwhere st represents the target style, and y' is the generated sentence."}, {"title": "Content preservation reward", "content": "Inspired by Lai et al. (2021), we use BLEU score as the reward for content preservation and leverage SCST (Self-Critic Sequence Training) approach (Rennie et al., 2017) as the optimization method. The corresponding loss function is as follows:\nLcont = \\sum_{i}log(p(y^s_i|Y_{1:i-1},x)) (bleu(y', y) - bleu(y^s,y))  (7)\nwhere ys is sampled from the distribution of model outputs at each time step, and y' is the greedy generation from the model.\nThe overall loss is a weighted sum of the positive sentiment reward loss Lcls, content preservation reward loss Lcont, and language modeling loss Lim\u00b7\nLlm = -\\sum_{i}log(p(y_i|Y_{1:i-1}, X))   (8)\nLfinal = \u03b1Lcls + \u03b2Lcont + \u03b3Llm   (9)"}, {"title": "3.2.3 Decoding Improvement", "content": "Although T5 and BART have demonstrated their superiority in the field of NLG, the sentences generated by default greedy search often result in text degeneration (i.e., empty or repeated sequences) during the decoding stage (Fan et al., 2018; Holtzman et al., 2019). Therefore, in this paper, various decoding improvement ways such as Beam search (Wiseman and Rush, 2016), Top-k sampling (Fan et al., 2018), Top-p sampling (Holtzman et al., 2019) and Typical sampling (Meister et al., 2023) are applied to the decoding stage of the Seq2Seq model to improve the quality of text generation. And Eq. 4 is changed as follows.\nyt = Post-Processing(Decoder(H; y (10)"}, {"title": "3.2.4 Multi-dimensional Re-ranking", "content": "According to Bayes Rule, we can decompose Eq. 2 into the product of three probabilities:\np(y|x, \u03c8x) = p(\u03c8x|y,x) \u00d7 p(x|y) \u00d7 p(y)   (11)\nThe first term p(\u03c8x|y, x) can be seen as the consistency of original-to-generative sentence transformation with given reframe strategy. The second term p(x|y) represents the textual similarity. And the last term p(y) can be regarded as the overall fluency of the output."}, {"title": "Strategy consistency", "content": "For this term, we propose Strategy-BERT to evaluate the consistency between text reframing and the given strategy, which draws on the idea of \"breaking the whole into pieces\" and prompt learning to transform the multi-label problem into multiple binary classification tasks, i.e. training the corresponding model for each reframing strategy. For one thing, this approach enables each model to concentrate on its specific aspect and thus not affect each other. For another thing, it facilitates context semantic enhancement by constructing an auxiliary sentence that incorporates supplementary task prompt to effectively mine the implicit task-specific knowledge contained in PLMs and alleviate the task awareness challenge."}, {"title": "Textual similarity", "content": "We still use BLEU to calculate this term because it can ensure that the generated text preserves style-independent content (Sancheti et al., 2020)."}, {"title": "Fluency", "content": "Recent works suggest that the probability of output generated from PLM is an appropriate automatic and referenceless measure of fluency (Suzgun et al., 2022; Ramirez et al., 2023). Therefore, we use GPT-2large (Radford et al., 2019) to calculate the overall fluency of each candidate.\nFinally, we take the product of scores from the above three items as the final score of the candidate sentence and choose the one with the highest score as the final output."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "Positive reframing For unconstrained positive reframing and controlled positive reframing, we adopt the dataset provided by Ziems et al. (2022) , and the specific statistics are given in Table 1.\nReframe strategy classification To verify the effectiveness of Strategy-BERT, we conduct experiments on reframe strategy classification task. Since this paper converts the multi-label classification problem into multiple binary classification tasks, the dataset is also divided accordingly, and the division results are presented in Table 2."}, {"title": "4.2 Evaluating Metrics", "content": "Regarding classification task, following Ziems et al. (2022), we use F1 score as the evaluation metric.\nFor generation task, the following nine automatic metrics are used: (1) Content preservation-related metric, namely ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) (Lin, 2004), BLEU (Papineni et al., 2002) and BERTScore (BScore) (Zhang et al., 2019). (2) \u0394TextBlob (\u0394\u03a4\u0392) (Loria, 2018) is used to report the average change in sentiment. (3) RTQE (Reframing Text Quality Evaluation) is proposed to evaluate the degree of positive text reframing (i.e. style strength), we fine-tune RoBERTalarge (Liu et al., 2019) to evaluate reframing degree and we regard the probability from the model prediction as the degree of positive reframing between the original and generated sentence; on the human reference it has the F1 score of 95.98% and accuracy of 97.41%. (5) Perplexity (PPL) is an indicator of text fluency, and we use GPT-2large as the evaluation model.\nFinally, following Ziems et al. (2022), we randomly selected 50 samples from each generated file and assigned them to 3 well-educated raters with relevant professional backgrounds to score Meaning Preservation (Meaning), Positivity and Fluency of reframed sentences on a scale of 1 to 5. Since the main research of this paper falls on controlled positive reframing task, we only conducted human evaluation on this task."}, {"title": "4.3 Implementation Details", "content": "Reframe strategy classification BERTbase (Devlin et al., 2019) and RoBERTabase (Liu et al., 2019) are used as the backbone model in this task respectively. The maximum text embedding length is set to 110. AdamW is used as the optimizer, and the batch size is 16. In addition, all models in this paper are implemented through Hugging-Face (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) on TITAN Xp GPU.\nPositive reframing Following Ziems et al. (2022), we use T5 (Raffel et al., 2019) and BART (Lewis et al., 2020) with 6 layers in each of the encoder and decoder, and the hidden size of 768. The value of the learning rate is from 3e-5 to 3e-4, the batch size processed by each device is 6, and the text maximum input length is 80. \u03b1, \u03b2, \u03b3 are respectively set to 1, 0.2, 1."}, {"title": "4.4 Main Results", "content": ""}, {"title": "4.4.1 Reframe Strategy Classification", "content": "For this task, this paper selects the Multi-label-BERT and Multi-label-RoBERTa proposed by Ziems et al. (2022) as baselines to compare with the Strategy-BERT and Strategy-RoBERTa proposed in this paper. For fairness, we directly adopt the results reported by Ziems et al. (2022). Since they only report F1 score of their models, we only use it as the evaluation metric in this task. The detailed performance of our proposed models on other metrics can be found in Table 12 in Appendix D.1."}, {"title": "4.4.2 Unconstrained Positive Reframing", "content": "As shown in Table 5, our proposed framework MSOF achieves significant improvements compared to the baselines. When combining positive sentiment reward and content preservation reward only during the training process, i.e. MSOFGreedy, already outperforms the baselines on almost all metrics, especially ROUGE, BScore, RTQE, and PPL. When incorporating decoding optimization and multi-dimensional re-ranking, the performance of the model will be further improved. From the perspective of the model, the T5-based models achieve the best results on metrics such as ATB, RTQE and PPL, while the BART-based models reach SOTA on content preservation-related metrics such as ROUGE, BLEU, and BScore. This may be because BART prioritizes semantic preservation rather than sentiment change when reframing the negative text. Among different decoding methods, both beam search and random sampling-based methods are superior to greedy search. Specifically, Top-k sampling has the best overall performance,"}, {"title": "4.4.3 Controlled Positive Reframing", "content": "Since only Ziems et al. (2022) have studied controlled positive reframing, we use T5 and BART (Ziems et al., 2022) that are fine-tuned on the corresponding dataset as baselines for comparison. The primary experimental results are given in Table 6. It can be concluded that the performance of models under constraints is generally better than unconstrained, which proves that the reframe strategy plays a role in assisting model inference to a certain extent. Consistent with the experimental results under the unconstrained setting, MSOFTop-k still achieves the best results among all variant models. Compared with the baselines, MSOFTop-k achieves an average improvement of 5 points on ROUGE, 1 point in BLEU, more than 10 points on both RTQE and PPL, and an improvement of about 20% on ATB. Moreover, it can be found that although Typical sampling does not perform as well as other decoding approaches on content preservation-related metrics such as ROUGE, BLEU, and BScore, it still achieves impressive results on ATB, RTQE and PPL, suggesting that"}, {"title": "4.4.4 Ablation Experiment", "content": "In addition, from the ablation experimental results shown in Table 7, we can conclude that only applying content preservation reward helps the model perform well on ROUGE, BLEU and BScore, but hinders the model from transferring text style. When using only positive sentiment reward, although the model performs well on \u2206\u03a4\u0392 and RTQE, it is not satisfactory in terms of content preservation. However, when the two are combined, the model can achieve a better balance between sentiment change and content preservation, exhibiting a more comprehensive performance. Furthermore, it can be observed that the multi-dimensional re-ranking significantly improves the model's performance on multiple metrics. This demonstrates that it can effectively select the sentence from the candidate that better meets the requirements of positive reframing. Based on the above experimental results and analysis, the validity and rationality of each component of MSOF can be effectively proved. For more ablation experiments, please refer to Tables 13 and 14 in Appendix D.2 and Tables 15, 16 and 17 in Appendix D.3."}, {"title": "4.4.5 Human Evaluation", "content": "Finally, we adopt human evaluation to manually judge the quality of the reframed text. As can be seen from Table 8, our method is more applicable to T5, but for BART, its performance on Positivity is not satisfactory, which can also be reflected by ATB and RTQE. Combining the relevant experimental results in Table 6, we speculate this is because the BART-based models prioritize content preservation over sentiment change. In general, consistent with the results and conclusion of automatic metrics, our method can effectively improve the model's performance, where the T5-based models perform better on Positivity and have a slightly higher score on Fluency, while BART-based models are better on Meaning."}, {"title": "5 Conclusion", "content": "We propose an original multi-strategy optimization framework (MSOF), which consists of reinforcement training, decoding improvement, and multi-dimensional re-ranking, to enhance the performance of PLMs on positive reframing. By conducting extensive experiments on T5-based and BART-based models separately, our framework achieves significant improvements over the baselines on various metrics. Future work includes further cleaning and expansion of the existing dataset to improve the quality and alleviate the imbalanced distribution of different reframe strategy labels, then exploring how the thought of controlled text generation can be applied to this task, followed by trying different approaches of context enhancement, and finally exploring how to apply large language models (LLMs) to positive reframing."}, {"title": "Limitations", "content": "Firstly, the multi-strategy optimization framework proposed in this paper introduces reinforced reward in the model training stage and the multi-dimensional re-ranking to select the candidate text generated by the model. Therefore, compared with the baselines, our proposed framework needs more memory space and time during training and prediction. Then, this paper finds that the dataset provided by Ziems et al. (2022) has certain noise and label imbalance issues that may hinder the training of the model and there are currently no corresponding datasets in other languages. Finally, we also suggest that if PLMs could be further trained in a rich psychological corpus, the performance would be improved more."}, {"title": "Ethics Statement", "content": "Similar to sentiment transfer, positive reframing has two sides, that is, our method can also be used to generate negative text and cause possible harmful effects on society. However, we still make our code public and hope others will be aware of the possible risks. We welcome any discussion and suggestions to minimize such risks."}, {"title": "A Reframing Text Quality Evaluation", "content": ""}, {"title": "A.1 Problem Statement", "content": "The essence of existing TST metrics such as ROUGE and BLEU is to evaluate the similarity between the generated and reference sentence, so a simple copy can lead to a high score (Fan et al., 2018; Holtzman et al., 2019). And for an original sentence, there may be multiple corresponding reframed sentences, especially in the unconstrained case. Furthermore, existing metrics also cannot directly measure the degree of positive reframing. Therefore, this paper proposes a new metric RTQE (Reframing Text Quality Evaluation), which aims to evaluate the degree of positive reframing relationship between the generated and original text that can avoid the limitation of only compared with human reference given in the dataset."}, {"title": "A.2 Evaluation Model", "content": "Taking the inspiration from Lai et al. (2021), the above problem is simplified into a binary classification task, i.e., judging whether there is a positive reframing relationship between two sentences. In practical evaluation, we regard the probability from the model prediction as the degree of positive reframing between the original and generated sentence. And the RTQE evaluation model established in this paper is shown in Figure 5. Given the original sentence x and the corresponding sentence y, we firstly concatenate them and input into"}, {"title": "A.3 Dataset", "content": "As we simplified the RTQE task as a binary classification question, which determines whether two sentences constitute the positive reframing relationship. Therefore, this paper reconstructs the positive reframing dataset (Ziems et al., 2022) in the following way: for each original sentence, we consider its corresponding reframing sentence as a positive sample, and we pair the original sentence with itself or randomly select other reframing sentences to create negative samples, aiming to enhance the learning depth and generalization ability of the model. The specific statistics are presented in Table 9."}, {"title": "A.4 Implementation Details", "content": "We use BERT (Devlin et al., 2019) and ROBERTa (Liu et al., 2019) as the backbone model respectively. For the base version, the model has 12 transformer encoder layers, and the hidden size is 768. For the large version, the model has 24 transformer encoder layers, and the hidden size is 1024. In this paper, the maximum text embedding length is set to 100 tokens, AdamW with an initial learning rate 1e-5 is used as the optimizer, and batch size is 32."}, {"title": "A.5 Experiment Results", "content": "This paper mainly tests the performance of four models: BERTbase, BERTlarge, RoBERTabase and ROBERTalarge. And the experimental results are shown in Table 10."}, {"title": "B The Approach of Obtaining the Candidate Sentence", "content": "The approach of obtaining the candidate sentence set is as follows: when beam search is used, the number of candidate sentences with the same beam size can be returned directly, and beam size of 4, 5, and 6 are experimented in this paper; for Top-k sampling, the generated sentences of k = 30, 40, 50 and 60 are composed of candidate sentence set; for Top-p sampling, the generated sentences of p = 0.80, 0.85, 0.90 and 0.95 are selected to be composed the candidate sentence set; for Typical sampling, the sentences generated by \u0442 = 0.20 and 0.95 are selected according to the settings recommended by Meister et al. (2023) to form the candidate sentence set."}, {"title": "C The Instruction for Human Evaluation", "content": "The specific instruction for human evaluation is as follows.\nGive the original sentence with negative viewpoint and reframed sentence generated by our models. You need to score the Meaning Preservation (Meaning), Positivity and Fluency of the reframed sentence on a scale of 1 to 5.\nMeaning: Indicate whether the reframed sentence preserves the original meaning.\n1: Completely changed the original meaning.\n3: Meaning related but with slight inconsistency or contradiction.\n5: Faithful to the original meaning.\nChoose 2 or 4 when you are hesitant.\nPositivity: Indicate how positive the reframed sentence is.\n1: As negative as the original sentence.\n3: Neutral Sentiment, i.e. neither negative nor positive.\n5: Very positive compared to the original sentence.\nChoose 2 or 4 when you are hesitant.\nFluency: Indicate the fluency of the reframed sentence.\n1: The reframed sentence does not make sense and it is unreadable.\n3: The reframed sentence contains some minor grammatical errors, but does not affect reading.\n5: The reframed sentence is human-like, without any grammatical errors.\nChoose 2 or 4 when you are hesitant."}, {"title": "D Additional Results", "content": ""}, {"title": "D.1 Reframe Strategy Classification", "content": "We provide the detailed scores of our models on all classification evaluation metrics (i.e., accuracy, precision, recall, and F1 score) for others to compare and refer to, which can be found in Table 12."}, {"title": "D.2 Unconstrained Positive Reframing", "content": "For this task, we provide additional ablation results of unconstrained positive reframing in Tables 13 and 14. It can be seen that when the positive sentiment reward is not used, the model's score on metrics such as ATB and RTQE decrease. And when the content preservation reward is not used, the model's performance on metrics such as ROUGE and BLEU may decline. In addition, it can be found that the improvement brought by multi-dimensional re-ranking is tremendous, significantly improving the model performance on multiple metrics, indicating that it can better select sentences that meet the requirements of positive reframing from the candidate text set. Based on the above experimental results and analysis, the effectiveness and rationality of each component of MSOF can be fully demonstrated."}, {"title": "D.3 Controlled Positive Reframing", "content": "Similar to Appendix D.2, we provide more detailed ablation experimental results of controlled positive reframing in Tables 15 and 16. In addition to the conclusions already drawn in the unconstrained setting, it can be observed that beam search generates sentences with higher content preservation and achieves great results on ROUGE and BLEU. On the other hand, random sampling strategies, namely Top-k, Top-p, and Typical may yield lower scores on ROUGE and BLEU, but achieve better results on ATB, RTQE, and PPL, indicating that their generated text may not overlap much with"}, {"title": "D.4 Case Study", "content": "We provide the generated examples of unconstrained and controlled experiments in Tables 18 and 19. A comparative analysis reveals that our models generate outputs that are more diverse and comprehensive, while effectively preserving the underlying meaning of the original text. Specifically, the outputs of the BART-based models are mostly similar, except for the sentences generated by Typical sampling. On the other hand, the T5-based models outperform the BART-based models and baselines by providing the benefits of weekends consistent with human reference. Additionally, although the text in the dataset may contain colloquialisms and even grammatical errors, our models can generate more formal sentences that avoid these issues. Therefore, we speculate that further cleaning and filtering of the data in the dataset can further improve the model's performance. By comparing the results generated by the model in the unconstrained and controlled settings, it can be inferred that without reframe strategy, the reframing performance of the models will decrease, which proves that the reframing strategy plays an auxiliary role in helping the model generate results that better meet task requirements.\nFinally, to further explore whether different reframe strategy will affect the generation results of the model, Table 20 shows the generation result of using different strategy to reframe the same negative text. It is obvious from the results that the model can generate reframing text with corresponding characteristics under the guidance of different reframe strategy, especially \"Self-affirmation\", \"Thankfulness\" and \"Growth Mindset\". This proves that the model can learn some information from the reframe strategy and it also shows that the research on controlled positive reframing is valuable."}]}