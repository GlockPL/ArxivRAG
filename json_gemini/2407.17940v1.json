{"title": "Positive Text Reframing under Multi-strategy Optimization", "authors": ["Shutong Jia", "Biwei Cao", "Qingqing Gao", "Jiuxin Cao", "Bo Liu"], "abstract": "Differing from sentiment transfer, positive reframing seeks to substitute negative perspectives with positive expressions while preserving the original meaning. With the emergence of pre-trained language models (PLMs), it is possible to achieve acceptable results by fine-tuning PLMs. Nevertheless, generating fluent, diverse and task-constrained reframing text remains a significant challenge. To tackle this issue, a multi-strategy optimization framework (MSOF) is proposed in this paper. Starting from the objective of positive reframing, we first design positive sentiment reward and content preservation reward to encourage the model to transform the negative expressions of the original text while ensuring the integrity and consistency of the semantics. Then, different decoding optimization approaches are introduced to improve the quality of text generation. Finally, based on the modeling formula of positive reframing, we propose a multi-dimensional re-ranking method that further selects candidate sentences from three dimensions: strategy consistency, text similarity and fluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate our framework achieves significant improvements on unconstrained and controlled positive reframing tasks.", "sections": [{"title": "1 Introduction", "content": "The concept of style transfer initially emerges within the domain of computer vision (CV) with the objective of accomplishing image style transfer (Gatys et al., 2016). Inspired by this, Hu et al. (2017) proposed text style transfer (TST), whose main purpose is to automatically control the text style and preserve the style-independent content. There also have been some related research before this, such as paraphrase (Xu et al., 2012). In recent years, there has been an increasing focus on TST, which has gradually evolved into a significant sub-"}, {"title": "2 Related Work", "content": "Early research on text style transfer mostly relied on artificial design features such as syntax (Zhu et al., 2010) and phrase (Xu et al., 2012) modeling, etc. Similar to other tasks in NLP, the advent of deep learning has resulted in the growing application of neural network models to TST. For example, Jhamtani et al. (2017) investigated the utilization of the Seq2Seq model for transforming modern English into Shakespearean-style English. Wang et al. (2019) applied GPT-2 to accomplish the formal-informal transfer. Sancheti et al. (2020) extended the work of Jhamtani et al. (2017) by in-\ncorporating a reinforcement learning framework. Lai et al. (2021) further applied this framework to PLMs. Above studies are mainly based on parallel corpora. Although satisfactory results can be achieved, the cost of constructing parallel corpora is expensive. Therefore, semi-supervised learning and unsupervised learning are widely used in TST. The main methods include data augmentation or text retrieval (Zhang et al., 2020; Jin et al., 2019), adversarial learning (Hu et al., 2017; Fu et al., 2018), back-translation (Prabhumoye et al., 2018; Wei et al., 2023), and reinforcement learning (Luo et al., 2019; Gong et al., 2019).\nSpecific to sentiment transfer, the early goal is to extract sentiment words that describe the corresponding entities, and then replace them with expressions of the opposite sentiment attribute. The representative one is the \u201cDelete, Retrieve, Generate\" strategy (Li et al., 2018). Furthermore, Sudhakar et al. (2019) applied the transformer architecture to the above strategy. To better distinguish content and style, Kim and Sohn (2020) divided the model into sentence reconstruction module and style module to complete their respective task. Han et al. (2023) introduced the adaptive clustering and contrastive learning modules to better explore sentence transmission patterns to main and utilize the latent transfer patterns.\nAlthough sentiment transfer preserves attribute-independent content, the intrinsic meaning of the original text expression is also changed. To this end, Ziems et al. (2022) introduced positive reframing, aiming to preserve the original meaning by substituting negative viewpoints with complementary positive expressions, and constructed the corresponding parallel dataset. For unconstrained positive reframing, Xu et al. (2023) decoupled the sentiment and style of the text to complete the positive reframing. Then, Sheng et al. (2023) further decomposed positive reframing into paraphrase generation and sentiment transfer and constructed corresponding pseudo datasets to fuse generation capabilities through multi-task learning, but also led to the inability to apply their method under the controlled setting."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Definition", "content": "Let $(x, y, \\psi_x)$ be a triple in the positive reframing task, where $x = {x_1, x_2, ..., x_n}$ is the original text with negative sentiment, and $y = {y_1, y_2,..., y_m}$\nis the target sentence with complementary positive expressions corresponding to x, m and n represent the sentence length. $V\\subseteq {Growth Mindset, Impermanence, Neutralizing, Optimism, Self-affirmation, Thankfulness}$ is the positive reframing strategy used to reframe the negative text x, which can use multiple strategies simultaneously. This paper researches the following three tasks.\nThe target of unconstrained positive reframing is to generate the target sentence y from the original text x without any reframe strategy guidance. This task can be modeled as follows:\n$p(y|x) = \\prod_{t=1}^{m} p(y_t|x, y_{<t})$ (1)\nwhere $y_{<t}$ represents what has been generated before time t.\nRegarding reframe strategy classification, its requirement is to predict the positive reframing strategy $\\psi_x$ used to reframe the original sentence x.\nFor controlled positive reframing, the primary objective is to generate the target sentence y from the original text x under given strategy $\\psi_x$, This problem can be modeled as the following formula.\n$p(y|x, \\psi_x) = \\prod_{t=1}^{m} p(y_t|x, \\psi_x, y_{<t})$ (2)"}, {"title": "3.2 Framework", "content": "As shown in Figure 2, our proposed framework mainly consists of four modules, namely sequence-to-sequence, reinforcement training, decoding improvement and multi-dimensional re-ranking."}, {"title": "3.2.1 Sequence-to-sequence", "content": "Consistent with Ziems et al. (2022), we also use T5 (Raffel et al., 2019) and BART (Lewis et al., 2020) as the basic text generation model, which are both mainly composed of two components, namely encoder and decoder.\nEncoder This part is to encode original sentence x and reframe strategy $\\psi_x$ into hidden vector H. We use T5 and BART as the basic generation model, and the encoder part is as follows:\nH = Encoder([x_1, x_2,..., x_n], \\psi_x) (3)\nwhere $H \\in \\mathbb{R}^{l \\times d}$, $l$ is the length of sequence, and d is the hidden dimension.\nDecoder The output $y_t$ of the decoder part takes the hidden vector output of the encoder and the output $y_{<t}$ of the decoder before time t as input, the equation is as follows.\n$y_t$ = Decoder(H; $y_{<t}$) (4)"}, {"title": "3.2.2 Reinforcement Training", "content": "As shown in Figure 3, based on the objective of positive reframing, the generated text should transform the negative sentiment of the original text and"}, {"title": "3.2.3 Decoding Improvement", "content": "Although T5 and BART have demonstrated their superiority in the field of NLG, the sentences generated by default greedy search often result in text degeneration (i.e., empty or repeated sequences)\nduring the decoding stage (Fan et al., 2018; Holtzman et al., 2019). Therefore, in this paper, various decoding improvement ways such as Beam search (Wiseman and Rush, 2016), Top-k sampling (Fan et al., 2018), Top-p sampling (Holtzman et al., 2019) and Typical sampling (Meister et al., 2023) are applied to the decoding stage of the Seq2Seq model to improve the quality of text generation. And Eq. 4 is changed as follows.\n$y_t$ = Post-Processing(Decoder(H; $y_{<t}$)) (10)"}, {"title": "3.2.4 Multi-dimensional Re-ranking", "content": "According to Bayes Rule, we can decompose Eq. 2 into the product of three probabilities:\n$p(y|x, \\psi_x) = p(\\psi_x|y, x) \\times p(x|y) \\times p(y)$ (11)\nThe first term $p(\\psi_x|y, x)$ can be seen as the consistency of original-to-generative sentence transformation with given reframe strategy. The second term $p(x|y)$ represents the textual similarity. And the last term p(y) can be regarded as the overall fluency of the output.\nStrategy consistency For this term, we propose Strategy-BERT to evaluate the consistency between text reframing and the given strategy, which draws on the idea of \"breaking the whole into pieces\" and prompt learning to transform the multi-label problem into multiple binary classification tasks, i.e. training the corresponding model for each reframing strategy. For one thing, this approach enables each model to concentrate on its specific aspect and thus not affect each other. For another thing, it facilitates context semantic enhancement by constructing an auxiliary sentence that incorporates supplementary task prompt to effectively mine the implicit task-specific knowledge contained in PLMs and alleviate the task awareness challenge."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Dataset", "content": "Positive reframing For unconstrained positive reframing and controlled positive reframing, we adopt the dataset provided by Ziems et al. (2022) .\nReframe strategy classification To verify the effectiveness of Strategy-BERT, we conduct experiments on reframe strategy classification task. Since this paper converts the multi-label classification problem into multiple binary classification tasks, the dataset is also divided accordingly, and\nthe division results are presented in Table 2."}, {"title": "4.2 Evaluating Metrics", "content": "Regarding classification task, following Ziems et al. (2022), we use F1 score as the evaluation metric.\nFor generation task, the following nine automatic metrics are used: (1) Content preservation-related metric, namely ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) (Lin, 2004), BLEU (Papineni et al., 2002) and BERTScore (BScore) (Zhang et al., 2019). (2) \u0394TextBlob (\u0394\u03a4\u0392) (Loria, 2018) is used to report the average change in sentiment. (3) RTQE (Reframing Text Quality Evaluation) is proposed to evaluate the degree of positive text reframing (i.e. style strength), we fine-tune RoBERTalarge (Liu et al., 2019) to evaluate reframing degree and we regard the probability from the model prediction as the degree of positive reframing between the original and generated sentence; on the human reference it has the F1 score of 95.98% and accuracy of 97.41%. (5) Perplexity (PPL) is an indicator of text fluency, and we use GPT-2large as the evaluation model.\nFinally, following Ziems et al. (2022), we randomly selected 50 samples from each generated file and assigned them to 3 well-educated raters with relevant professional backgrounds to score Meaning Preservation (Meaning), Positivity and Fluency of reframed sentences on a scale of 1 to 5. Since the main research of this paper falls on controlled positive reframing task, we only conducted human evaluation on this task."}, {"title": "4.3 Implementation Details", "content": "Reframe strategy classification BERTbase (Devlin et al., 2019) and RoBERTabase (Liu et al., 2019) are used as the backbone model in this task respectively. The maximum text embedding length is set to 110. AdamW is used as the optimizer, and the batch size is 16. In addition, all models in this paper are implemented through Hugging-Face (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) on TITAN Xp GPU.\nPositive reframing Following Ziems et al. (2022), we use T5 (Raffel et al., 2019) and BART (Lewis et al., 2020) with 6 layers in each of the encoder and decoder, and the hidden size of 768. The value of the learning rate is from 3e-5 to 3e-4, the batch size processed by each device is 6, and the text maximum input length is 80. \u03b1, \u03b2, \u03b3 are respectively set to 1, 0.2, 1."}, {"title": "4.4 Main Results", "content": null}, {"title": "4.4.1 Reframe Strategy Classification", "content": "For this task, this paper selects the Multi-label-BERT and Multi-label-RoBERTa proposed by Ziems et al. (2022) as baselines to compare with the Strategy-BERT and Strategy-RoBERTa proposed in this paper. For fairness, we directly adopt the results reported by Ziems et al. (2022). Since they only report F1 score of their models, we only use it as the evaluation metric in this task. The detailed performance of our proposed models on other metrics can be found in Table 12 in Appendix D.1."}, {"title": "4.4.2 Unconstrained Positive Reframing", "content": "As shown in Table 5, our proposed framework MSOF achieves significant improvements compared to the baselines. When combining positive sentiment reward and content preservation reward only during the training process, i.e. MSOFGreedy, already outperforms the baselines on almost all metrics, especially ROUGE, BScore, RTQE, and PPL. When incorporating decoding optimization and multi-dimensional re-ranking, the performance of the model will be further improved. From the perspective of the model, the T5-based models achieve the best results on metrics such as ATB, RTQE and PPL, while the BART-based models reach SOTA on content preservation-related metrics such as ROUGE, BLEU, and BScore. This may be because BART prioritizes semantic preservation rather than sentiment change when reframing the negative text. Among different decoding methods, both beam search and random sampling-based methods are superior to greedy search. Specifically, Top-k sampling has the best overall performance,"}, {"title": "4.4.3 Controlled Positive Reframing", "content": "Since only Ziems et al. (2022) have studied controlled positive reframing, we use T5 and BART (Ziems et al., 2022) that are fine-tuned on the corresponding dataset as baselines for comparison. The primary experimental results are given in Table 6. It can be concluded that the performance of models under constraints is generally better than unconstrained, which proves that the reframe strategy plays a role in assisting model inference to a certain extent. Consistent with the experimental results under the unconstrained setting, MSOFTop-k still achieves the best results among all variant models. Compared with the baselines, MSOFTop-k achieves an average improvement of 5 points on ROUGE, 1 point in BLEU, more than 10 points on both RTQE and PPL, and an improvement of about 20% on ATB. Moreover, it can be found that although Typical sampling does not perform as well as other decoding approaches on content preservation-related metrics such as ROUGE, BLEU, and BScore, it still achieves impressive results on ATB, RTQE and PPL, suggesting that"}, {"title": "4.4.4 Ablation Experiment", "content": "In addition, from the ablation experimental results shown in Table 7, we can conclude that only applying content preservation reward helps the model perform well on ROUGE, BLEU and BScore, but hinders the model from transferring text style. When using only positive sentiment reward, although the model performs well on \u2206\u03a4\u0392 and RTQE, it is not satisfactory in terms of content preservation. However, when the two are combined, the model can achieve a better balance between sentiment change and content preservation, exhibiting a more comprehensive performance. Furthermore, it can be observed that the multi-dimensional re-ranking significantly improves the model's performance on multiple metrics. This demonstrates that it can effectively select the sentence from the candidate that better meets the requirements of positive reframing. Based on the above experimental results and analysis, the validity and rationality of each component of MSOF can be effectively proved. For more ablation experiments, please refer to Tables 13 and 14 in Appendix D.2 and Tables 15, 16 and 17 in Appendix D.3."}, {"title": "4.4.5 Human Evaluation", "content": "Finally, we adopt human evaluation to manually judge the quality of the reframed text. As can be seen from Table 8, our method is more applicable to T5, but for BART, its performance on Positivity is not satisfactory, which can also be reflected by ATB and RTQE. Combining the relevant experimental results in Table 6, we speculate this is because the BART-based models prioritize content preservation over sentiment change. In general,"}, {"title": "5 Conclusion", "content": "We propose an original multi-strategy optimization framework (MSOF), which consists of reinforcement training, decoding improvement, and multi-dimensional re-ranking, to enhance the performance of PLMs on positive reframing. By conducting extensive experiments on T5-based and BART-based models separately, our framework achieves significant improvements over the baselines on various metrics. Future work includes further cleaning and expansion of the existing dataset to improve the quality and alleviate the imbalanced distribution of different reframe strategy labels, then exploring how the thought of controlled text generation can be applied to this task, followed by trying different approaches of context enhancement, and finally exploring how to apply large language models (LLMs) to positive reframing."}, {"title": "Limitations", "content": "Firstly, the multi-strategy optimization framework proposed in this paper introduces reinforced re-"}, {"title": "Ethics Statement", "content": "Similar to sentiment transfer, positive reframing has two sides, that is, our method can also be used to generate negative text and cause possible harmful effects on society. However, we still make our code public and hope others will be aware of the possible risks. We welcome any discussion and suggestions to minimize such risks."}, {"title": "A Reframing Text Quality Evaluation", "content": null}, {"title": "A.1 Problem Statement", "content": "The essence of existing TST metrics such as ROUGE and BLEU is to evaluate the similarity between the generated and reference sentence, so a simple copy can lead to a high score (Fan et al., 2018; Holtzman et al., 2019). And for an original sentence, there may be multiple corresponding reframed sentences, especially in the unconstrained case. Furthermore, existing metrics also cannot directly measure the degree of positive reframing. Therefore, this paper proposes a new metric RTQE (Reframing Text Quality Evaluation), which aims to evaluate the degree of positive reframing relationship between the generated and original text that can avoid the limitation of only compared with human reference given in the dataset."}, {"title": "A.2 Evaluation Model", "content": "Taking the inspiration from Lai et al. (2021), the above problem is simplified into a binary classification task, i.e., judging whether there is a positive reframing relationship between two sentences. In practical evaluation, we regard the probability from the model prediction as the degree of positive reframing between the original and generated sentence. And the RTQE evaluation model established in this paper is shown in Figure 5. Given the original sentence x and the corresponding sentence y, we firstly concatenate them and input into"}, {"title": "A.3 Dataset", "content": "As we simplified the RTQE task as a binary classification question, which determines whether two sentences constitute the positive reframing relationship. Therefore, this paper reconstructs the positive reframing dataset (Ziems et al., 2022) in the following way: for each original sentence, we consider its corresponding reframing sentence as a positive sample, and we pair the original sentence with itself or randomly select other reframing sentences to create negative samples, aiming to enhance the learning depth and generalization ability of the model."}, {"title": "A.4 Implementation Details", "content": "We use BERT (Devlin et al., 2019) and ROBERTa (Liu et al., 2019) as the backbone model respectively. For the base version, the model has 12 transformer encoder layers, and the hidden size is 768. For the large version, the model has 24 transformer encoder layers, and the hidden size is 1024. In this paper, the maximum text embedding length is set to 100 tokens, AdamW with an initial learning rate 1e-5 is used as the optimizer, and batch size is 32."}, {"title": "A.5 Experiment Results", "content": "This paper mainly tests the performance of four models: BERTbase, BERTlarge, RoBERTabase and ROBERTalarge. And the experimental results are shown in Table 10."}, {"title": "B The Approach of Obtaining the Candidate Sentence", "content": "The approach of obtaining the candidate sentence set is as follows: when beam search is used, the number of candidate sentences with the same beam size can be returned directly, and beam size of 4, 5, and 6 are experimented in this paper; for Top-k sampling, the generated sentences of k = 30, 40, 50 and 60 are composed of candidate sentence set; for Top-p sampling, the generated sentences of p = 0.80, 0.85, 0.90 and 0.95 are selected to be composed the candidate sentence set; for Typical sampling, the sentences generated by \u0442 = 0.20 and 0.95 are selected according to the settings recommended by Meister et al. (2023) to form the candidate sentence set."}, {"title": "C The Instruction for Human Evaluation", "content": "The specific instruction for human evaluation is as follows.\nGive the original sentence with negative viewpoint and reframed sentence generated by our models. You need to score the Meaning Preservation (Meaning), Positivity and Fluency of the reframed sentence on a scale of 1 to 5."}, {"title": "D Additional Results", "content": null}, {"title": "D.1 Reframe Strategy Classification", "content": "We provide the detailed scores of our models on all classification evaluation metrics (i.e., accuracy, precision, recall, and F1 score) for others to compare and refer to, which can be found in Table 12."}, {"title": "D.2 Unconstrained Positive Reframing", "content": "For this task, we provide additional ablation results of unconstrained positive reframing in Tables 13 and 14. It can be seen that when the positive sentiment reward is not used, the model's score on metrics such as ATB and RTQE decrease. And when the content preservation reward is not used, the model's performance on metrics such as ROUGE and BLEU may decline. In addition, it can be found that the improvement brought by multi-dimensional re-ranking is tremendous, significantly improving the model performance on multiple metrics, indicating that it can better select sentences that meet the requirements of positive reframing from the candidate text set. Based on the above experimental results and analysis, the effectiveness and rationality of each component of MSOF can be fully demonstrated."}, {"title": "D.3 Controlled Positive Reframing", "content": "Similar to Appendix D.2, we provide more detailed ablation experimental results of controlled positive reframing in Tables 15 and 16. In addition to the conclusions already drawn in the unconstrained setting, it can be observed that beam search generates sentences with higher content preservation and achieves great results on ROUGE and BLEU. On the other hand, random sampling strategies, namely Top-k, Top-p, and Typical may yield lower scores on ROUGE and BLEU, but achieve better results on ATB, RTQE, and PPL, indicating that their generated text may not overlap much with"}, {"title": "D.4 Case Study", "content": "We provide the generated examples of unconstrained and controlled experiments in Tables 18 and 19. A comparative analysis reveals that our models generate outputs that are more diverse and comprehensive, while effectively preserving the underlying meaning of the original text. Specifically, the outputs of the BART-based models are mostly similar, except for the sentences generated by Typical sampling. On the other hand, the T5-based models outperform the BART-based models and baselines by providing the benefits of weekends consistent with human reference. Additionally, although the text in the dataset may contain colloquialisms and even grammatical errors, our"}]}