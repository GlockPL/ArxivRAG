{"title": "Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection", "authors": ["Xin Zhou", "Duc-Manh Tran", "Thanh Le-Cong", "Ting Zhang", "Ivana Clairine IRSAN", "Joshua SUMARLIN", "Bach Le", "David Lo"], "abstract": "Software vulnerabilities pose significant security challenges and potential risks to society, necessitating extensive efforts in automated vulnerability detection. There are two popular lines of work to address automated vulnerability detection. On one hand, Static Application Security Testing (SAST) is usually utilized to scan source code for security vulnerabilities, especially in industries. On the other hand, deep learning (DL)-based methods, especially since the introduction of large language models (LLMs), have demonstrated their potential in software vulnerability detection. However, there is no comparative study between SAST tools and LLMs, aiming to determine their effectiveness in vulnerability detection, understand the pros and cons of both SAST and LLMs, and explore the potential combination of these two families of approaches.\nIn this paper, we compared 15 diverse SAST tools with 12 popular or state-of-the-art open-source LLMs in detecting software vulnerabilities from repositories of three popular programming languages: Java, C, and Python. The experimental results showed that SAST tools obtain low vulnerability detection rates with relatively low false positives, while LLMs can detect up 90% to 100% of vulnerabilities but suffer from high false positives. By further ensembling the SAST tools and LLMs, the drawbacks of both SAST tools and LLMs can be mitigated to some extent. Our analysis sheds light on both the current progress and future directions for software vulnerability detection.", "sections": [{"title": "1 INTRODUCTION", "content": "The early detection and mitigation of software vulnerabilities is crucial, as unaddressed vulnerabilities can potentially lead to consequences like massive economic losses and compromised critical infrastructure. With the number of software vulnerabilities significantly increasing from 5,697 in 2013 to 29,065 in 2023 [72], the impact of software vulnerabilities has also amplified. Accurately and promptly identifying vulnerabilities is crucial for mitigating potential risks and has garnered significant attention from both industry and academia.\nStatic Application Security Testing (SAST) tools are commonly used in industries to scan source code and detect vulnerabilities. These tools analyze code without executing it, using techniques such as syntax, data flow, and control flow analysis [18]. SAST is popular due to its low cost, fast operation, and ability to find bugs without running the program [49]. Meanwhile, in the research community, large language model (LLM)-based methods have demonstrated their effectiveness for function-level software vulnerability detection [29, 55], which predict whether a function contains the vulnerability or not. LLMs are deep learning (DL) models based on the Transformer architecture [78], comprising a vast number of parameters and pre-trained on massive amounts of source code, text, and other data modalities [11, 58]. Recently, LLMs have seen a significant increase in the size of their pre-training data and model parameters [56, 59]. The extensive knowledge acquired through the large-scale pre-training offers the potential to substantially improve the effectiveness of vulnerability detection.\nDespite the considerable research interest in either SAST tools or LLMs for vulnerability detection, a comprehensive comparative study between these two popular approaches has been lacking. This gap can be attributed to the following three major challenges:\nLack of a repo-level vulnerability detection formulation: SAST tools are usually used to detect vulnerabilities across entire code repositories. However, current DL-based methods for vulnerability detection, including those utilizing LLMs, primarily focus on identifying vulnerabilities within individual functions [12, 27, 55, 84, 88]. To date, we are unaware of any DL-based methods that are designed to detect vulnerabilities at the level of entire repositories. This gap in methodology hinders a fair and direct comparison between SAST tools and LLMs. Therefore, there is a need for a new task formulation for repo-level vulnerability detection.\nLack of datasets supporting both SAST and LLMs: Existing datasets for DL-based or LLM-based vulnerability detection, such as BigVul [25] and CVEfixes [9], contain labeled data at the function level without the source code of complete repositories, which is not"}, {"title": "2 STUDY DESIGN", "content": "In this section, we present our proposed repo-level vulnerability detection task. We then introduce the data collection and detail the study objects (i.e., SAST tools and LLMs).\n2.1 Repo-level Vulnerability Detection\nWe first introduce our proposed repo-level vulnerability detection task and then compare it with the function-level vulnerability detection task used in existing studies [12, 27, 88].\n2.1.1 Repo-level Vulnerability Detection Task Formation. This task aims to detect the vulnerable functions in an entire repository. It aims to learn a detector M that can map the source code of an entire repository to vulnerability labels at the function level:\n$M: X \\rightarrow Y$, $X = \\{x_0, x_1, ..., x_n\\}$ and $Y = \\{0, 1, ..., 0\\}$\nwhere X denotes the entire input repository and its component $x_i$ refers to the i-th function in the input repository. n refers to the number of functions in the input repository. Y denotes the labels of all functions in the repository, where 1 indicates a vulnerable function and 0 indicates a clean (non-vulnerable) function.\n2.1.2 Comparison with Function-level Vulnerability Detection. The function-level vulnerability detection aims to predict whether a target function contains a vulnerability or not [12, 88]. To understand the difference between function-level and repo-level vulnerability detection, we need to briefly explain what the \"target functions\" are in the function-level vulnerability detection dataset/task.\nMany function-level vulnerability detection datasets, such as BigVul [25], ReVeal [12], CrossVul [62], CVEfixes [9], and DiverseVul [14], employ a heuristic to automatically label functions, as"}, {"title": "2.1.3 Applying LLMs for Repo-level Vulnerability Detection", "content": "For the repo-level vulnerability detection task, the goal is to detect vulnerable functions across all functions within an entire repository. However, repositories can be extremely lengthy, while some of the studied LLMs, such as CodeBERT and GraphCodeBERT, have input length limitations of 512 tokens. This 512-token limit aligns well with function-level data but not with classes or entire repositories. Although some larger studied LLMs allow wider input ranges (e.g., up to 8,192 tokens for StarCoder [50]), we aim to conduct experiments within the same setting for all studied LLMs. To enable all LLMs, including LLMs like CodeBERT, to perform the repo-level task, we employ the following strategy unified across all LLMs: (1) We split a repository into individual functions. (2) We predict the vulnerability label for each function in the repository separately. (3) We aggregate all predictions for the functions within the repository to obtain the predictions for the entire repository. (4) We analyze whether vulnerabilities are detected within the repository based on the aggregated predictions (See Sections 3.1 and 3.2 for details).\n2.2 Vulnerability Datasets\nIn this paper, we investigate three datasets for three popular programming languages, i.e., Java, C, and Python. Specifically, the Java and C datasets are sourced from Li et al. [49] and Lipp et al. [51]. The Python dataset is newly constructed. The statistics of three datasets are shown in Table 1. Please note that for each CVE entry, our datasets provided 1) CVE ID, 2) CWE type, 3) the complete source code of the corresponding vulnerable software, and 4) locations of vulnerabilities at the function level.\nJava and C Vulnerability Datasets. For the Java vulnerability dataset selection, we considered the Java dataset shared by Li et"}, {"title": "2.3 SAST Tools Selection", "content": "Java and C SAST Tools. For Java SAST tools, we followed Li et al. [49] to select 7 free or open-source SAST tools that support Java code: CodeQL [16], Contrast Codesec Scan (Contrast) [69], Horusec [89], Insider [43], SpotBugs [64], Semgrep [70], and SonarQube community edition (SonarQube) [71]. These tools encompass a diverse range of SAST techniques and are popular among developers, as indicated by the number of GitHub stars [49]. For C SAST tools, we followed Lipp et al. [51] to select five free or open-source SAST tools that support C code, i.e., Flawfinder [28], Cppcheck [17], Infer [42], CodeChecker [15], and CodeQL [16]. These tools implemented state-of-the-art analysis techniques [51] and were used in previous studies focusing on evaluating SAST tools [51, 73].\nPython SAST Tools. We aim to select a representative set of SAST tools that support Python code. For the search range, we utilized a comprehensive set of 576 candidate SAST tools identified by Li et al. [49]. They compiled their list by searching tool lists from recent literature (e.g., [4, 32, 52]) and obtaining recommendations for SAST tools from several prominent websites (e.g., GitHub, NIST, and Wikipedia). We designed the following criteria to select SAST tools for Python: 1) Python supported: we included only SAST tools that support Python programs; 2) Free of charge: due to the substantial costs of commercial tools, we followed Li et al. [49] to select the free Python SAST tools; 3) Security-related: we selected tools that identify generalized security vulnerabilities rather than those aimed at detecting specific vulnerabilities or code quality issues; 4) Well-documented with detecting rules: we selected SAST tools with clear documentation. Based on these criteria, we finally selected six SAST tools: Bandit [7], Dlint [23], DevSkim [21], CodeQL [16], Graudit [33], and Semgrep [70]."}, {"title": "2.4 Studied LLMs", "content": "We focus on free and open-source LLMs instead of commercial LLMs such as GPT-3.5 and GPT-4 for two primary reasons. Firstly, the cost of utilizing commercial LLMs to scan entire repositories for vulnerability detection is prohibitively high. As illustrated in Table 1, the datasets contain 312,045 to 3,986,848 functions, making the use of commercial LLMs financially challenging for such large-scale tasks. Secondly, commercial LLMs undergo continuous updates, making it difficult to replicate and validate the results. In contrast, open-source LLMs offer greater transparency and accessibility, facilitating reproducible evaluations.\nTable 2 summarizes the characteristics of the studied LLMs. Please note that in accordance with a recent comprehensive survey on large language models for Software Engineering [39], we categorize CodeBERT, GraphCodeBERT, CodeT5, and UniXcoder as LLMs, though their model sizes may be considered relatively small within the LLM spectrum. Specifically, we select two groups of open-source LLMs: 1) general LLMs and 2) code-related LLMs. General LLMs are pre-trained on diverse data, including natural language and code, and can be utilized for various tasks. In contrast, code-related LLMs are specifically (further) pre-trained on code-related data. Given the empirical nature of this study, we"}, {"title": "2.5 LLM Adaptation Techniques", "content": "We employ two typical techniques for adapting LLMs: prompt-based methods and fine-tuning-based methods. The last column of Table 2 summarizes the corresponding LLM adaptation techniques used for each of the studied LLMs.\n2.5.1 Prompt-based Methods. Prompts act as guides for LLMs, formatting the input data and providing a natural language description of the task [11]. Prompt-based methods mainly leverage the pre-trained knowledge of LLMs to perform the downstream task without the need for extensive task-specific training. The prompt format designed for our vulnerability detection task comprises three components, organized sequentially: 1) Task Description: a natural language description of the vulnerability detection task, i.e., \"If the following code snippet has any vulnerabilities, output Yes; otherwise, output No\"; 2) Formatted Input: the input code is enclosed within two markers \"// Code Start\" and \"// Code End\"; 3) Prediction Marker: a marker (\"// Detection\") instructing the model to start to provide its prediction.\nWe studied three popular prompt-based methods based on the aforementioned prompt designed for vulnerability detection:\nZero-shot Prompting: In this approach, the input source code is formatted based on the prompt above and then being inputted into LLMs. The LLMs then generate a response to detect whether the source code is vulnerable or not. It is named \"zero-shot\" because no labeled data is used in this approach.\nChain-of-thought (CoT) Prompting: We utilized the Chain-of-thought technique proposed by Kojima et al. [47] for improved reasoning, which involves adding \"Let's think step by step\" to the original prompt. Specifically, we add the \"Let's think step by step\" after the task description of the prompt above."}, {"title": "Few-shot Prompting", "content": "In this approach, we provided a few examples of inputs and ground truth label pairs for LLMs. Those input-label pairs are concatenated after the original prompt. Specifically, we randomly select 2 examples from the training set, one vulnerable function, and one clean function. Those examples can help the model to better understand the target task and reply in the same format as the example responses.\n2.5.2 Fine-tuning-based Methods. Fine-tuning is another widely used adaptation technique for LLMs, which often unlocks their full potential for specific tasks [80]. Fine-tuning involves training an LLM on task-specific data, enabling it to acquire domain knowledge and generate more relevant and meaningful output. In this study, we considered two different fine-tuning approaches based on the size of the LLM models:\nFull Fine-Tuning on Lightweight LLMs. For the four lightweight LLMs with fewer than 1 billion parameters, i.e., CodeBERT, Graph-CodeBERT, CodeT5, and UniXcoder, we utilized full parameter fine-tuning. This approach updates all model parameters to adapt the LLMs to the training data. Full parameter fine-tuning is the standard approach for these lightweight LLMs across various code-related tasks such as code search and defect prediction [27, 34, 35, 79].\nParameter-Efficient Fine-Tuning (PEFT) on Large LLMs. For the eight large LLMs with more than 1 billion parameters such as DeepSeekCoder (DSCoder), we utilized the parameter-efficient fine-tuning technique to adapt them to the training data. This is because full-parameter fine-tuning comes at a significant computational cost when the LLM contains billions of parameters, exceeding our current computational resources [82]. Additionally, recent studies [20, 82] suggest that parameter-efficient tuning techniques can achieve comparable or even better performance on LLMs exceeding 1 billion parameters. Technically, parameter-efficient tuning techniques selectively update a subset of the model's parameters, significantly reducing the computational resources required [20, 40, 57]. In this work, we employ a popular and widely used parameter-efficient tuning technique called LORA (Low-Rank Adaptation) [40], which has been widely used in many existing studies [22, 53, 77, 82]. LORA allows the LLM to adapt to the target task while preserving its pre-trained knowledge and reducing the computational requirements compared to full-parameter fine-tuning [82]."}, {"title": "3 EXPERIMENTAL SETUP", "content": "3.1 Vulnerability Detection Scenarios\nAligned with the previous work [51], we assess the vulnerability detection capabilities of SAST tools and LLMs across two distinct scenarios. Specifically, we investigated:\nScenario 1 (S1): A vulnerability is considered detected if the SAST tool or LLM identifies at least one vulnerable function within the vulnerable repository.\nScenario 2 (S2): A vulnerability is only considered detected if the SAST tool or LLM identifies all vulnerable functions present in the vulnerable repository.\nThese scenarios underscore the significance of tools capable of pinpointing vulnerabilities within repositories precisely. By accurately identifying the vulnerable functions associated with potential security weaknesses, such tools streamline the manual search and"}, {"title": "3.2 Evaluation Metrics", "content": "We follow the previous work [51] to employ the following metrics:\n$Vuln. Detection Ratio = \\frac{\\#Detected Vuln.}{\\#All Vuln. in Benchmark}$\n$Marked Function Ratio = \\frac{\\#Marked Function}{\\#All Functions in Benchmark}$\nThe first metric (i.e., the vulnerability detection ratio) computes the proportion of detected vulnerabilities present in the benchmark. A high vulnerability detection ratio signifies superior performance in vulnerability detection. The second metric (i.e., the marked function ratio) approximates the false positive rate [51] because, on average, only 0.008%-0.18% of functions are vulnerable in the repositories, meaning that the majority of marked functions are non-vulnerable. A low marked function ratio is desirable, as it reduces the number of functions developers need to inspect manually.\nWe refrain from employing the commonly used evaluation metric Precision in this study, aligning with previous research [51]. This decision stems from the inherent challenges in assessing real-world programs, which may conceal undiscovered vulnerabilities. Our knowledge is confined to vulnerabilities reported or identified by the community. Consequently, we lack precise information on the total number of positive samples (vulnerable functions), encompassing both known and unknown vulnerable functions. Precision requires precise information on all positive samples, rendering its calculation unfeasible in our context. We also cannot compute the F1 score, as it is based on Precision."}, {"title": "3.3 Implementation Details", "content": "Data Split. Following the previous work [27, 35, 88], we randomly split each vulnerability dataset into disjoint training, validation, and test sets in a ratio of 8:1:1. The training and validation sets are used to fine-tune/prompt LLMs. Both SAST and LLMs are evaluated on the same test sets for fair comparison.\nImplementations. For prompt-based methods, any required labeled data (e.g., for few-shot prompting) is randomly sampled from the training set. In contrast, fine-tuning-based methods employ a different approach. Firstly, we identify and extract all vulnerable functions from the repositories in the training set. Subsequently, we randomly select an equal number of non-vulnerable functions from the repositories in the training set. By combining these two groups of functions, we construct a balanced training set for fine-tuning LLMs. This balancing process is essential for effective model training when dealing with highly imbalanced datasets [38, 83, 84]. Without the balancing, models would be overly biased towards learning the features of non-vulnerable functions due to their dominance in the datasets, resulting in sub-optimal performance in detecting vulnerable samples [86]. Similarly, we also construct a balanced validation set for model validation. Additionally, for LLMs, we develop a generation pipeline in Python, utilizing PyTorch implementations of studied LLMs. We leverage the Hugging Face library [24] to load the model weights and generate outputs. We fine-tune all LLMs for 20 epochs, with a batch size of 8, and select the best model snapshot based on the validation performance. For"}, {"title": "3.4 Research Questions", "content": "Our work aims to answer three research questions (RQs).\nRQ1: How effective are SAST tools and LLMs? In RQ1, we evaluate a comprehensive list of 15 SAST tools and 12 LLMs in their ability to detect vulnerabilities in software repositories.\nRQ2: Which one is more effective for detecting vulnerabilities, SAST tools or LLMs? In RQ2, we compare the effectiveness of SAST tools against LLMs, aiming to determine which approach performs better in detecting vulnerabilities across different programming languages.\nRQ3: To what extent can combining multiple SAST tools or LLMs improve vulnerability detection? In RQ3, we investigate the potential enhancements in vulnerability detection achieved by integrating multiple SAST tools and LLMs."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "4.1 RQ1: Effectiveness of SAST Tools and LLMs\nTable 3 displays the experimental results, encompassing metrics such as \"S1 D,\" \"S2 D,\" and \"Marked.\" The \"S1 D\" metric refers to the ratio of detected vulnerabilities in Scenario 1, where detection is considered successful upon identifying any vulnerable function within the vulnerable repository. The \u201cS2 D\u201d metric represents the detection ratio in Scenario 2, where a vulnerability is considered as detected only when all vulnerable functions in the repository are identified. The \"Marked\" metric indicates the percentage of functions labeled as vulnerable. A higher detection ratio signifies superior performance in vulnerability detection, whereas a lower marked function ratio is preferred, as it reduces the number of functions developers need to check to locate the actual vulnerabilities. In Table 3, bold numbers with green cells indicate the best performance, and underlined numbers with light green cells indicate the second-best performance. Please note that for the best marked function ratio, we consider the tool or model that achieves the lowest marked function ratio while still detecting at least one vulnerability. A tool or model that fails to detect any vulnerabilities, regardless of its marked function ratio, is not considered to have the best marked function ratio.\n4.1.1 Effectiveness of SAST Tools. SAST tools achieved low detection rates. Many SAST tools, such as CodeQT, Contrast, Insider, SpotBugs, SonarQube for Java, FlawFinder, CppChecker, and CodeChecker for C, and CodeQL and Graudit for Python, failed to detect vulnerabilities. Among SAST tools, Horusec performed the best for Java, achieving a 37.5% detection rate in Scenario 1. For C, CodeQL exhibited the best performance, attaining detection rates of 44.4% and 33.3% in Scenario 1 and 2. For Python, DevSkim performed the best with detection rates of 30.0% and 20.0% in Scenario 1 and 2. The advantage of SAST tools is they marked a small percentage of functions as vulnerable. Low marked function ratios could facilitate developers in identifying the actual vulnerability location by inspecting fewer marked functions. SAST tools marked only 0.2% to 5.2% of all functions on average."}, {"title": "4.1.2 Effectiveness of LLMs", "content": "LLMs achieved high detection rates. For Java, many LLMs exhibited detection rates ranging from over 50% to as high as 100% in Scenario 1, and from surpassing 37.5% to as high as 87.5% in Scenario 2. Similarly, for C, LLMs achieved detection rates up to 100.0% and 88.9% in Scenario 1 and Scenario 2. For Python, LLMs achieved detection rates up to 90.0% in both Scenario 1 and Scenario 2. However, the drawback of LLMs is they mark a large portion of functions as vulnerable. This indicates that their high detection rates come at the cost of marking many functions in the repository as suspicious. Consequently, this makes it difficult for developers to pinpoint the actual vulnerability locations, as they need to inspect a large number of marked functions. Specifically, LLMs mark 8.3% to 77.4% of functions in Java, 3.5% to 70.1% in C, and 14.9% to 73.4% in Python.\n4.1.3 Comparisons Among LLMs. Among different adaptations for large LLMs (\u22651B), fine-tuning achieves the highest detection ratios with the fewest marked vulnerable functions. For instance, in Java, the fine-tuned large LLMs detect an average of 73.4% and 51.6% of vulnerabilities in S1 and S2 respectively, across different LLM models. This outperforms the prompt-based methods by 20.5% to 73.7% in terms of average detection ratios. Meanwhile, the fine-tuned large LLMs mark only 19.4% of functions as vulnerable on average, which is much less than the prompt-based methods, ranging from 52.1% to 55.0%. Thus, fine-tuned large LLMs outperform prompted large LLMs. Fine-tuned lightweight LLMs detect more vulnerabilities but also mark more functions as vulnerable compared to fine-tuned large LLMs. For example, in Java, fine-tuned lightweight LLMs outperform the fine-tuned large LLMs by 10.6% and 9.1% in S1 and S2, on average. However, at the same time, fine-tuned lightweight LLMs mark 43.2% more functions as vulnerable on average.\nAnswer to RQ1: SAST tools obtained lower vulnerability detection rates (up to 44.4%) while also marking fewer functions as vulnerable (up to 5.2%). In contrast, LLMs detected more vulnerabilities (up to 100%) but also marked a higher proportion of functions as vulnerable (up to 77.4%)."}, {"title": "4.2 RQ2: SAST Tools vs. LLMs", "content": "4.2.1 Comprehensive Comparison between SAST and LLMs. The advantages and disadvantages of SAST tools and LLMs are evident: 1) SAST tools mark fewer functions but detect fewer vulnerabilities, while 2) LLMs detect more vulnerabilities but require marking substantially more functions. There seems to be a trade-off between detection ratios and marked function ratios: the more vulnerabilities detected, the greater the number of functions marked as potentially vulnerable. Thus, our aim here is to identify the optimal SAST tools and LLMs that reach a sweet spot in the trade-off between detection ratios and marked function ratios, optimizing both metrics for practical vulnerability detection scenarios.\nTo comprehensively understand the capabilities of SAST tools and LLMs, we plot the relationships between detection ratios (Scenario 2) and marked function ratios in Figure 3. The optimal tools/models are those capable of detecting more vulnerabilities with fewer marked functions, typically occupying the upper-left areas of Figure 3. In Figure 3, we exclude prompt-based large LLMs because"}, {"title": "4.2.2 Best vs. Worst Detected Vulnerabilities", "content": "We studied the detection rates of vulnerabilities belonging to different vulnerability classes. Due to the page limit, we simplify the analysis by merging the results corresponding to the three programming languages together. Additionally, we consider the combined results of SAST tools: a vulnerability is considered detected if at least one SAST tool can detect it. Similarly, we consider the combined results of the 2 optimal LLMs. Specifically, we focus on the 2 optimal models highlighted in Figure 3, which have the lowest marked function ratios. For Java, the 2 optimal LLMs are fine-tuned CodeLlama and DeepSeekCoder. For C, the 2 optimal LLMs are fine-tuned Llama3 and CodeLlama. For Python, the optimal LLMs are fine-tuned DeepSeekCoder and StarCoder2. A vulnerability is considered detected if at least one of the 2 optimal LLMs can detect it.\nFigure 4 illustrates the detection rates of vulnerability classes that are best and worst detected by SAST tools and the 2 optimal LLMs. The most frequently detected vulnerability class in Scenarios 1 and 2 is CWE-119. Notably, the LLM group can achieve 100% detection rates for CWE-119. Additionally, CWE-476 is the second-best detected vulnerability class by both the SAST group and the LLM group. In contrast, CWE-835 and CWE-89 are the worst detected"}, {"title": "4.3 RQ3: Combining SAST Tools or LLMs", "content": "4.3.1 Combining SAST Tools. The main limitation of SAST tools is their low detection ratios. Thus, the goal of combining SAST tools is to improve the detection ratios, even if it comes at the cost of higher marked function ratios. To achieve this goal, we employ the following combining strategy to fuse the predictions of SAST tools: if a function is marked as vulnerable by any of the SAST tools, the function is considered to be marked as vulnerable by the SAST combination. The first two columns of Figures 5-7 show the detection ratios and marked ratios of the individual SAST tool with the highest detection ratio, as well as the combination of all SAST tools, for Java, C, and Python, respectively. The results demonstrate that combining all SAST tools substantially boosts detection ratios, ranging from 25.2% to 100.0%. For Java, the detection ratio in Scenario 1 improved from 37.5% to 50.0%, at the cost of increasing the marked function ratio from 1.7% to 4.3%. For C, the detection ratios improved from 44.4% to 55.6% in Scenario 1 and from 33.3% to 44.4% in Scenario 2, at the cost of increasing the marked function ratio from 5.2% to 7.2%. For Python, the detection ratios improved from 30.0% to 50.0% in S1 and from 20.0% to 40.0% in S2, with an increase in the marked functions from 4.5% to 7.7%.\n4.3.2 Combining LLMs. Different from SAST tools, the main limitation of LLMs is their high marked function ratios. Thus, the goal of combining LLM models is to reduce the marked function ratios, even if it comes at the cost of sacrificing some detection ratios. Specifically, we include 12 LLMs in the LLM combination: four fine-tuned lightweight LLMs and eight fine-tuned large LLMs. Regarding the combination strategies, we aim to reduce the number"}, {"title": "4.3.3 Combined SAST v.s. Combined LLMs", "content": "We discuss which combination strategy is better for each programming language. For Java, the C3 LLM combination is better than the SAST combination because it has higher detection ratios and comparable marked function ratios. For C, the SAST combination is better because it has higher detection ratios and a lower marked function ratio compared to the C1 LLM combination. In addition, the SAST combination obtains substantially higher detection ratios with an acceptable increase in marked function ratios compared to the C2 and C3 LLM combinations. For Python, the SAST combination is better. Compared to the C2 and C3 LLM combinations, the SAST combination has comparable detection ratios with a much lower marked function ratio. Moreover, the marked function ratio of the C1 LLM combination is unacceptably high (i.e., 32.6%), making it an unfavorable choice.\nAnswer to RQ3: Combining SAST tools substantially boosts detection ratios, ranging from 25.2% to 100.0%. Combining LLMs substantially reduces the marked function ratio by 40.9%-74.6% on average. The optimal approach differs across programming languages. For Java, the LLM combination provides the most effective solution, while for C and Python, the combination of SAST tools is better."}, {"title": "5 DISCUSSION", "content": "5.1 Evaluating Commercial LLM ChatGPT\nIn this work, we focus on open-source LLMs because using commercial LLMs for this task would be very costly, considering the large number of functions in each repository. However, to make our paper more self-contained, we also conducted small-scale experiments with the famous commercial LLM ChatGPT. Specifically, we set the ratio of vulnerable functions to clean functions as 1:100 and sampled the corresponding number of clean functions from the repository. We used the gpt-3.5-turbo-0125 model [66], the latest version of GPT-3.5, and set the temperature to 0 for more reproducible results. Table 4 presents the results of ChatGPT and two open-source LLMs, Llama3 and CodeBERT, with the same setting, averaged across Java, C, and Python. In general, the results show that the commercial LLM ChatGPT achieved lower vulnerability detection ratios and marked fewer functions as potentially vulnerable compared to the open-source LLMs such as Llama3 and CodeBERT. Among different ways of using ChatGPT, the few-shot prompting detects most vulnerabilities (i.e., 23.2%).\n5.2 Implications\nRepo-level vulnerability detection using LLMs holds substan-tial potential. While LLMs may not yet outperform SAST tools in"}, {"title": "Repo-level vulnerability detection is more practical than the function-level vulnerability detection", "content": "We recommend a gradual shift in vulnerability detection efforts from the existing function-level task formation to the repo-level vulnerability detection task formation introduced in this study. As shown in Figure 2, datasets used in traditional function-level vulnerability detection studies rely on vulnerability-fixing commits to narrow the detection scope from the entire repository to the specific files modified in those commits. However, when the goal is to detect vulnerabilities, the corresponding vulnerability-fixing commit does not yet exist. Consequently, it is impractical to depend on such commits to limit the detection range. The repo-level vulnerability detection formation addresses this issue by eliminating the need to narrow down the detection scope based on the fixing commits.\nAddressing the high marked function ratios of LLMs necessitates further investigation. While LLMs tend to detect more vulnerabilities compared to SAST tools, they suffer from high marked function ratios, akin to high false positive rates. Although we managed to significantly reduce these ratios by combining multiple LLMs, there remains large room for enhancement. Potential solutions may entail developing techniques such as, filtering out less trustworthy predictions from LLMs, integrating additional contextual information (e.e., the structural features in the repository) with LLMs, refining prompting strategies, or employing more advanced ensemble methods.\nExploring advanced combinations of LLMs and SAST tools holds promise. Each category of approach has its strengths and weaknesses in detecting vulnerabilities in software repositories: LLMs detect more vulnerabilities but with higher marked function ratios, whereas SAST tools detect fewer vulnerabilities but with lower marked function ratios. Future research could propose methods that can effectively leverage the strengths of both approaches for enhanced vulnerability detection."}, {"title": "5.3 Threats to Validity", "content": "Threats to internal validity relate to possible errors and biases in our experiments. To mitigate these risks, we utilized the official implementations of the studied tools and models to ensure correctness. We have reviewed and validated our code and data, and they are publicly accessible to promote transparency and reproducibility. Another potential threat concerns the presence of undetected vulnerabilities in the vulnerability benchmarks. However, our objective was to assess whether the selected tools/models could accurately"}, {"title": "6 RELATED WORK", "content": "Studies of SAST Tools. There are many existing studies evaluating static analysis security testing (SAST) tools, e.g., [5, 8, 10, 13, 26, 32, 32, 36, 45, 46, 46, 48, 49, 51, 52, 61, 63, 68, 74, 76", "48": ".", "74": ".", "51": "focused on SAST tools for C programs, evaluating the effectiveness of six tools on real-world vulnerability datasets. Li et al. [49", "68": "."}]}