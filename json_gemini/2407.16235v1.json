{"title": "Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection", "authors": ["Xin Zhou", "Duc-Manh Tran", "Thanh Le-Cong", "Ting Zhang", "Ivana Clairine IRSAN", "Joshua SUMARLIN", "Bach Le", "David Lo"], "abstract": "Software vulnerabilities pose significant security challenges and potential risks to society, necessitating extensive efforts in automated vulnerability detection. There are two popular lines of work to address automated vulnerability detection. On one hand, Static Application Security Testing (SAST) is usually utilized to scan source code for security vulnerabilities, especially in industries. On the other hand, deep learning (DL)-based methods, especially since the introduction of large language models (LLMs), have demonstrated their potential in software vulnerability detection. However, there is no comparative study between SAST tools and LLMs, aiming to determine their effectiveness in vulnerability detection, understand the pros and cons of both SAST and LLMs, and explore the potential combination of these two families of approaches.\nIn this paper, we compared 15 diverse SAST tools with 12 popular or state-of-the-art open-source LLMs in detecting software vulnerabilities from repositories of three popular programming languages: Java, C, and Python. The experimental results showed that SAST tools obtain low vulnerability detection rates with relatively low false positives, while LLMs can detect up 90% to 100% of vulnerabilities but suffer from high false positives. By further ensembling the SAST tools and LLMs, the drawbacks of both SAST tools and LLMs can be mitigated to some extent. Our analysis sheds light on both the current progress and future directions for software vulnerability detection.", "sections": [{"title": "1 INTRODUCTION", "content": "The early detection and mitigation of software vulnerabilities is crucial, as unaddressed vulnerabilities can potentially lead to consequences like massive economic losses and compromised critical infrastructure. With the number of software vulnerabilities significantly increasing from 5,697 in 2013 to 29,065 in 2023 [72], the impact of software vulnerabilities has also amplified. Accurately and promptly identifying vulnerabilities is crucial for mitigating potential risks and has garnered significant attention from both industry and academia.\nStatic Application Security Testing (SAST) tools are commonly used in industries to scan source code and detect vulnerabilities. These tools analyze code without executing it, using techniques such as syntax, data flow, and control flow analysis [18]. SAST is popular due to its low cost, fast operation, and ability to find bugs without running the program [49]. Meanwhile, in the research community, large language model (LLM)-based methods have demonstrated their effectiveness for function-level software vulnerability detection [29, 55], which predict whether a function contains the vulnerability or not. LLMs are deep learning (DL) models based on the Transformer architecture [78], comprising a vast number of parameters and pre-trained on massive amounts of source code, text, and other data modalities [11, 58]. Recently, LLMs have seen a significant increase in the size of their pre-training data and model parameters [56, 59]. The extensive knowledge acquired through the large-scale pre-training offers the potential to substantially improve the effectiveness of vulnerability detection.\nDespite the considerable research interest in either SAST tools or LLMs for vulnerability detection, a comprehensive comparative study between these two popular approaches has been lacking. This gap can be attributed to the following three major challenges:\nLack of a repo-level vulnerability detection formulation: SAST tools are usually used to detect vulnerabilities across entire code repositories. However, current DL-based methods for vulnerability detection, including those utilizing LLMs, primarily focus on identifying vulnerabilities within individual functions [12, 27, 55, 84, 88]. To date, we are unaware of any DL-based methods that are designed to detect vulnerabilities at the level of entire repositories. This gap in methodology hinders a fair and direct comparison between SAST tools and LLMs. Therefore, there is a need for a new task formulation for repo-level vulnerability detection.\nLack of datasets supporting both SAST and LLMs: Existing datasets for DL-based or LLM-based vulnerability detection, such as BigVul [25] and CVEfixes [9], contain labeled data at the function level without the source code of complete repositories, which is not"}, {"title": "2 STUDY DESIGN", "content": "In this section, we present our proposed repo-level vulnerability detection task. We then introduce the data collection and detail the study objects (i.e., SAST tools and LLMs).\n2.1 Repo-level Vulnerability Detection\nWe first introduce our proposed repo-level vulnerability detection task and then compare it with the function-level vulnerability detection task used in existing studies [12, 27, 88].\n2.1.1 Repo-level Vulnerability Detection Task Formation. This task aims to detect the vulnerable functions in an entire repository. It aims to learn a detector M that can map the source code of an entire repository to vulnerability labels at the function level:\n$\u041c: \u0425 \\rightarrow \u0423, X = \\{x_0, X_1, ..., x_n\\} and Y = \\{0, 1, ..., 0\\}$\nwhere X denotes the entire input repository and its component $x_i$ refers to the i-th function in the input repository. n refers to the number of functions in the input repository. Y denotes the labels of all functions in the repository, where 1 indicates a vulnerable function and 0 indicates a clean (non-vulnerable) function.\n2.1.2 Comparison with Function-level Vulnerability Detection. The function-level vulnerability detection aims to predict whether a target function contains a vulnerability or not [12, 88]. To understand the difference between function-level and repo-level vulnerability detection, we need to briefly explain what the \"target functions\" are in the function-level vulnerability detection dataset/task.\nMany function-level vulnerability detection datasets, such as BigVul [25], ReVeal [12], CrossVul [62], CVEfixes [9], and DiverseVul [14], employ a heuristic to automatically label functions, as"}, {"title": "2.2 Vulnerability Datasets", "content": "In this paper, we investigate three datasets for three popular programming languages, i.e., Java, C, and Python. Specifically, the Java and C datasets are sourced from Li et al. [49] and Lipp et al. [51]. The Python dataset is newly constructed. The statistics of three datasets are shown in Table 1. Please note that for each CVE entry, our datasets provided 1) CVE ID, 2) CWE type, 3) the complete source code of the corresponding vulnerable software, and 4) locations of vulnerabilities at the function level.\nJava and C Vulnerability Datasets. For the Java vulnerability dataset selection, we considered the Java dataset shared by Li et"}, {"title": "2.3 SAST Tools Selection", "content": "Java and C SAST Tools. For Java SAST tools, we followed Li et al. [49] to select 7 free or open-source SAST tools that support Java code: CodeQL [16], Contrast Codesec Scan (Contrast) [69], Horusec [89], Insider [43], SpotBugs [64], Semgrep [70], and Sonar- Qube community edition (SonarQube) [71]. These tools encompass a diverse range of SAST techniques and are popular among developers, as indicated by the number of GitHub stars [49]. For C SAST tools, we followed Lipp et al. [51] to select five free or open-source SAST tools that support C code, i.e., Flawfinder [28], Cppcheck [17], Infer [42], CodeChecker [15], and CodeQL [16]. These tools implemented state-of-the-art analysis techniques [51] and were used in previous studies focusing on evaluating SAST tools [51, 73].\nPython SAST Tools. We aim to select a representative set of SAST tools that support Python code. For the search range, we utilized a comprehensive set of 576 candidate SAST tools identified by Li et al. [49]. They compiled their list by searching tool lists from recent literature (e.g., [4, 32, 52]) and obtaining recommendations for SAST tools from several prominent websites (e.g., GitHub, NIST, and Wikipedia). We designed the following criteria to select SAST tools for Python: 1) Python supported: we included only SAST tools that support Python programs; 2) Free of charge: due to the substantial costs of commercial tools, we followed Li et al. [49] to select the free Python SAST tools; 3) Security-related: we selected tools that identify generalized security vulnerabilities rather than those aimed at detecting specific vulnerabilities or code quality issues; 4) Well-documented with detecting rules: we selected SAST tools with clear documentation. Based on these criteria, we finally selected six SAST tools: Bandit [7], Dlint [23], DevSkim [21], CodeQL [16], Graudit [33], and Semgrep [70]."}, {"title": "2.4 Studied LLMs", "content": "We focus on free and open-source LLMs instead of commercial LLMs such as GPT-3.5 and GPT-4 for two primary reasons. Firstly, the cost of utilizing commercial LLMs to scan entire repositories for vulnerability detection is prohibitively high. As illustrated in Table 1, the datasets contain 312,045 to 3,986,848 functions, making the use of commercial LLMs financially challenging for such large-scale tasks. Secondly, commercial LLMs undergo continuous updates, making it difficult to replicate and validate the results. In contrast, open-source LLMs offer greater transparency and accessibility, facilitating reproducible evaluations.\nTable 2 summarizes the characteristics of the studied LLMs. Please note that in accordance with a recent comprehensive survey on large language models for Software Engineering [39], we categorize CodeBERT, GraphCodeBERT, CodeT5, and UniXcoder as LLMs, though their model sizes may be considered relatively small within the LLM spectrum. Specifically, we select two groups of open-source LLMs: 1) general LLMs and 2) code-related LLMs. General LLMs are pre-trained on diverse data, including natural language and code, and can be utilized for various tasks. In contrast, code-related LLMs are specifically (further) pre-trained on code-related data. Given the empirical nature of this study, we"}, {"title": "2.5 LLM Adaptation Techniques", "content": "We employ two typical techniques for adapting LLMs: prompt-based methods and fine-tuning-based methods. The last column of Table 2 summarizes the corresponding LLM adaptation techniques used for each of the studied LLMs.\n2.5.1 Prompt-based Methods. Prompts act as guides for LLMs, for- matting the input data and providing a natural language description of the task [11]. Prompt-based methods mainly leverage the pre-trained knowledge of LLMs to perform the downstream task without the need for extensive task-specific training. The prompt format designed for our vulnerability detection task comprises three components, organized sequentially: 1) Task Description: a natural language description of the vulnerability detection task, i.e., \"If the following code snippet has any vulnerabilities, output Yes; otherwise, output No\"; 2) Formatted Input: the input code is enclosed within two markers \"// Code Start\" and \"// Code End\"; 3) Prediction Marker: a marker (\"// Detection\") instructing the model to start to provide its prediction.\nWe studied three popular prompt-based methods based on the aforementioned prompt designed for vulnerability detection:\nZero-shot Prompting: In this approach, the input source code is formatted based on the prompt above and then being inputted into LLMs. The LLMs then generate a response to detect whether the source code is vulnerable or not. It is named \"zero-shot\" because no labeled data is used in this approach.\nChain-of-thought (CoT) Prompting: We utilized the Chain-of- thought technique proposed by Kojima et al. [47] for improved reasoning, which involves adding \"Let's think step by step\" to the original prompt. Specifically, we add the \"Let's think step by step\" after the task description of the prompt above."}, {"title": "2.5.2 Fine-tuning-based Methods.", "content": "Fine-tuning is another widely used adaptation technique for LLMs, which often unlocks their full potential for specific tasks [80]. Fine-tuning involves training an LLM on task-specific data, enabling it to acquire domain knowledge and generate more relevant and meaningful output. In this study, we considered two different fine-tuning approaches based on the size of the LLM models:\nFull Fine-Tuning on Lightweight LLMs. For the four lightweight LLMs with fewer than 1 billion parameters, i.e., CodeBERT, Graph- CodeBERT, CodeT5, and UniXcoder, we utilized full parameter fine- tuning. This approach updates all model parameters to adapt the LLMs to the training data. Full parameter fine-tuning is the standard approach for these lightweight LLMs across various code-related tasks such as code search and defect prediction [27, 34, 35, 79].\nParameter-Efficient Fine-Tuning (PEFT) on Large LLMs. For the eight large LLMs with more than 1 billion parameters such as DeepSeekCoder (DSCoder), we utilized the parameter-efficient fine- tuning technique to adapt them to the training data. This is because full-parameter fine-tuning comes at a significant computational cost when the LLM contains billions of parameters, exceeding our current computational resources [82]. Additionally, recent studies [20, 82] suggest that parameter-efficient tuning techniques can achieve comparable or even better performance on LLMs exceeding 1 billion parameters. Technically, parameter-efficient tuning techniques selectively update a subset of the model's parameters, significantly reducing the computational resources required [20, 40, 57]. In this work, we employ a popular and widely used parameter-efficient tuning technique called LORA (Low-Rank Adaptation) [40], which has been widely used in many existing studies [22, 53, 77, 82]. LORA allows the LLM to adapt to the target task while preserving its pre- trained knowledge and reducing the computational requirements compared to full-parameter fine-tuning [82]."}, {"title": "3 EXPERIMENTAL SETUP", "content": "3.1 Vulnerability Detection Scenarios\nAligned with the previous work [51], we assess the vulnerability detection capabilities of SAST tools and LLMs across two distinct scenarios. Specifically, we investigated:\nScenario 1 (S1): A vulnerability is considered detected if the SAST tool or LLM identifies at least one vulnerable function within the vulnerable repository.\nScenario 2 (S2): A vulnerability is only considered detected if the SAST tool or LLM identifies all vulnerable functions present in the vulnerable repository.\nThese scenarios underscore the significance of tools capable of pinpointing vulnerabilities within repositories precisely. By accurately identifying the vulnerable functions associated with potential security weaknesses, such tools streamline the manual search and"}, {"title": "3.2 Evaluation Metrics", "content": "We follow the previous work [51] to employ the following metrics:\n$Vuln. Detection Ratio = \\frac{\\#Detected Vuln.}{\\#All Vuln. in Benchmark}$\n$Marked Function Ratio = \\frac{\\#Marked Function}{\\#All Functions in Benchmark}$\nThe first metric (i.e., the vulnerability detection ratio) computes the proportion of detected vulnerabilities present in the benchmark. A high vulnerability detection ratio signifies superior performance in vulnerability detection. The second metric (i.e., the marked function ratio) approximates the false positive rate [51] because, on average, only 0.008%-0.18% of functions are vulnerable in the repositories, meaning that the majority of marked functions are non-vulnerable. A low marked function ratio is desirable, as it reduces the number of functions developers need to inspect manually.\nWe refrain from employing the commonly used evaluation metric Precision in this study, aligning with previous research [51]. This decision stems from the inherent challenges in assessing real- world programs, which may conceal undiscovered vulnerabilities. Our knowledge is confined to vulnerabilities reported or identified by the community. Consequently, we lack precise information on the total number of positive samples (vulnerable functions), encompassing both known and unknown vulnerable functions. Precision requires precise information on all positive samples, rendering its calculation unfeasible in our context. We also cannot compute the F1 score, as it is based on Precision."}, {"title": "3.3 Implementation Details", "content": "Data Split. Following the previous work [27, 35, 88], we randomly split each vulnerability dataset into disjoint training, validation, and test sets in a ratio of 8:1:1. The training and validation sets are used to fine-tune/prompt LLMs. Both SAST and LLMs are evaluated on the same test sets for fair comparison.\nImplementations. For prompt-based methods, any required la- beled data (e.g., for few-shot prompting) is randomly sampled from the training set. In contrast, fine-tuning-based methods employ a different approach. Firstly, we identify and extract all vulnerable functions from the repositories in the training set. Subsequently, we randomly select an equal number of non-vulnerable functions from the repositories in the training set. By combining these two groups of functions, we construct a balanced training set for fine- tuning LLMs. This balancing process is essential for effective model training when dealing with highly imbalanced datasets [38, 83, 84]. Without the balancing, models would be overly biased towards learning the features of non-vulnerable functions due to their dominance in the datasets, resulting in sub-optimal performance in detecting vulnerable samples [86]. Similarly, we also construct a balanced validation set for model validation. Additionally, for LLMs, we develop a generation pipeline in Python, utilizing PyTorch im- plementations of studied LLMs. We leverage the Hugging Face library [24] to load the model weights and generate outputs. We fine-tune all LLMs for 20 epochs, with a batch size of 8, and select the best model snapshot based on the validation performance. For"}, {"title": "3.4 Research Questions", "content": "Our work aims to answer three research questions (RQs).\nRQ1: How effective are SAST tools and LLMs? In RQ1, we evaluate a comprehensive list of 15 SAST tools and 12 LLMs in their ability to detect vulnerabilities in software repositories.\nRQ2: Which one is more effective for detecting vulnerabili- ties, SAST tools or LLMs? In RQ2, we compare the effectiveness of SAST tools against LLMs, aiming to determine which approach performs better in detecting vulnerabilities across different programming languages.\nRQ3: To what extent can combining multiple SAST tools or LLMs improve vulnerability detection? In RQ3, we investigate the potential enhancements in vulnerability detection achieved by integrating multiple SAST tools and LLMs."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "4.1 RQ1: Effectiveness of SAST Tools and LLMs\nTable 3 displays the experimental results, encompassing metrics such as \"S1 D,\" \"S2 D,\" and \"Marked.\" The \"S1 D\" metric refers to the ratio of detected vulnerabilities in Scenario 1, where detection is considered successful upon identifying any vulnerable function within the vulnerable repository. The \u201cS2 D\u201d metric represents the detection ratio in Scenario 2, where a vulnerability is considered as detected only when all vulnerable functions in the repository are identified. The \"Marked\" metric indicates the percentage of functions labeled as vulnerable. A higher detection ratio signifies superior performance in vulnerability detection, whereas a lower marked function ratio is preferred, as it reduces the number of functions developers need to check to locate the actual vulnerabilities. In Table 3, bold numbers with green cells indicate the best performance, and underlined numbers with light green cells indicate the second-best performance. Please note that for the best marked function ratio, we consider the tool or model that achieves the lowest marked function ratio while still detecting at least one vulnerability. A tool or model that fails to detect any vulnerabilities, regardless of its marked function ratio, is not considered to have the best marked function ratio.\n4.1.1 Effectiveness of SAST Tools. SAST tools achieved low detection rates. Many SAST tools, such as CodeQT, Contrast, In- sider, SpotBugs, SonarQube for Java, FlawFinder, CppChecker, and CodeChecker for C, and CodeQL and Graudit for Python, failed to detect vulnerabilities. Among SAST tools, Horusec performed the best for Java, achieving a 37.5% detection rate in Scenario 1. For C, CodeQL exhibited the best performance, attaining detection rates of 44.4% and 33.3% in Scenario 1 and 2. For Python, DevSkim performed the best with detection rates of 30.0% and 20.0% in Sce- nario 1 and 2. The advantage of SAST tools is they marked a small percentage of functions as vulnerable. Low marked function ratios could facilitate developers in identifying the actual vulnerability location by inspecting fewer marked functions. SAST tools marked only 0.2% to 5.2% of all functions on average."}, {"title": "4.1.2 Effectiveness of LLMs.", "content": "LLMs achieved high detection rates. For Java, many LLMs exhibited detection rates ranging from over 50% to as high as 100% in Scenario 1, and from surpassing 37.5% to as high as 87.5% in Scenario 2. Similarly, for C, LLMs achieved detection rates up to 100.0% and 88.9% in Scenario 1 and Scenario 2. For Python, LLMs achieved detection rates up to 90.0% in both Scenario 1 and Scenario 2. However, the drawback of LLMs is they mark a large portion of functions as vulnerable. This indicates that their high detection rates come at the cost of marking many functions in the repository as suspicious. Consequently, this makes it difficult for developers to pinpoint the actual vulnerability locations, as they need to inspect a large number of marked functions. Specifically, LLMs mark 8.3% to 77.4% of functions in Java, 3.5% to 70.1% in C, and 14.9% to 73.4% in Python.\n4.1.3 Comparisons Among LLMs. Among different adaptations for large LLMs (\u22651B), fine-tuning achieves the highest detec- tion ratios with the fewest marked vulnerable functions. For instance, in Java, the fine-tuned large LLMs detect an average of 73.4% and 51.6% of vulnerabilities in S1 and S2 respectively, across different LLM models. This outperforms the prompt-based methods by 20.5% to 73.7% in terms of average detection ratios. Meanwhile, the fine-tuned large LLMs mark only 19.4% of functions as vul- nerable on average, which is much less than the prompt-based methods, ranging from 52.1% to 55.0%. Thus, fine-tuned large LLMs outperform prompted large LLMs. Fine-tuned lightweight LLMS detect more vulnerabilities but also mark more functions as vulnerable compared to fine-tuned large LLMs. For example, in Java, fine-tuned lightweight LLMs outperform the fine-tuned large LLMs by 10.6% and 9.1% in S1 and S2, on average. However, at the same time, fine-tuned lightweight LLMs mark 43.2% more functions as vulnerable on average."}, {"title": "4.1.3 Anser to RQ1: SAST tools obtained lower vulnerability", "content": "detection rates (up to 44.4%) while also marking fewer functions as vulnerable (up to 5.2%). In contrast, LLMs detected more vulnerabilities (up to 100%) but also marked a higher proportion of functions as vulnerable (up to 77.4%).\n4.2 RQ2: SAST Tools vs. LLMs\n4.2.1 Comprehensive Comparison between SAST and LLMs. The advantages and disadvantages of SAST tools and LLMs are evident: 1) SAST tools mark fewer functions but detect fewer vulnerabilities, while 2) LLMs detect more vulnerabilities but require marking substantially more functions. There seems to be a trade-off between detection ratios and marked function ratios: the more vulnerabilities detected, the greater the number of functions marked as potentially vulnerable. Thus, our aim here is to identify the optimal SAST tools and LLMs that reach a sweet spot in the trade-off between detection ratios and marked function ratios, optimizing both metrics for practical vulnerability detection scenarios.\nTo comprehensively understand the capabilities of SAST tools and LLMs, we plot the relationships between detection ratios (Sce- nario 2) and marked function ratios in Figure 3. The optimal tool- s/models are those capable of detecting more vulnerabilities with fewer marked functions, typically occupying the upper-left areas of Figure 3. In Figure 3, we exclude prompt-based large LLMs because"}, {"title": "4.3 RQ3: Combining SAST Tools or LLMs", "content": "4.3.1 Combining SAST Tools. The main limitation of SAST tools is their low detection ratios. Thus, the goal of combining SAST tools is to improve the detection ratios, even if it comes at the cost of higher marked function ratios. To achieve this goal, we employ the following combining strategy to fuse the predictions of SAST tools: if a function is marked as vulnerable by any of the SAST tools, the function is considered to be marked as vulnerable by the SAST combination. The first two columns of Figures 5-7 show the detection ratios and marked ratios of the individual SAST tool with the highest detection ratio, as well as the combination of all SAST tools, for Java, C, and Python, respectively. The results demonstrate that combining all SAST tools substantially boosts detection ratios, ranging from 25.2% to 100.0%. For Java, the detection ratio in Scenario 1 improved from 37.5% to 50.0%, at the cost of increasing the marked function ratio from 1.7% to 4.3%. For C, the detection ratios improved from 44.4% to 55.6% in Scenario 1 and from 33.3% to 44.4% in Scenario 2, at the cost of increasing the marked function ratio from 5.2% to 7.2%. For Python, the detection ratios improved from 30.0% to 50.0% in S1 and from 20.0% to 40.0% in S2, with an increase in the marked functions from 4.5% to 7.7%.\n4.3.2 Combining LLMs. Different from SAST tools, the main limitation of LLMs is their high marked function ratios. Thus, the goal of combining LLM models is to reduce the marked function ratios, even if it comes at the cost of sacrificing some detection ratios. Specifically, we include 12 LLMs in the LLM combination: four fine-tuned lightweight LLMs and eight fine-tuned large LLMs. Regarding the combination strategies, we aim to reduce the number"}, {"title": "3.3.1 Combined SAST", "content": "of marked functions. Thus, we employ three voting mechanisms on the predictions of 12 fine-tuned LLMs: 1) C1 (voting>50%): if more than half of the LLMs agree to mark one function as vulnerable, the combination of LLMs will mark the function as vulnerable; 2) C2 (voting>): if more than $2/3$ of LLMs agree to mark one function as vulnerable, the combination of LLMs will mark the function as vulnerable; 3) C3 (voting>80%): if more than 80% of LLMs agree to mark one function as vulnerable, the combination of LLMs will mark the function as vulnerable.\nThe last four columns of Figures 5-7 present the detection ratios and marked function ratios for one of the best individual LLMs (as determined from Figure 3), as well as for three different combina- tions of multiple LLM models for Java, C, and Python. The results show that combining multiple LLMs can substantially reduce the marked function ratios compared to individual LLMs on average. For instance, in the C2 LLM combination, it could reduce the marked function ratios by 61.7%, 74.6%, and 40.9% in the Java, C, and Python benchmarks, respectively, compared to the average marked function ratios of the 12 fine-tuned LLMs. Additionally, the C2 LLM combination could achieve comparable detection ratios to the best-performing single LLM in each programming language"}, {"title": "Anser to RQ3: Combining SAST", "content": "(fine-tuned CodeLlama for Java, fine-tuned Llama3 for C, and fine- tuned DeepSeekCoder for Python), indicating that the detection ratios are not significantly compromised in this C2 combination. The more models we require LLMs to agree on the vulnerable predictions, the lower detection ratio and marked function ratios we can obtain. For instance, when requiring agreement from more than 80% of LLMs for a vulnerable prediction, the marked function ratios become comparable to those of SAST tools (0.8%- 9.2%). However, this stringent agreement criterion leads to poor detection ratios, reaching only 0%-37.5% in Scenario 2.\n4.3.3 Combined SAST v.s. Combined LLMs. We discuss which com- bination strategy is better for each programming language. For Java, the C3 LLM combination is better than the SAST com- bination because it has higher detection ratios and comparable marked function ratios. For C, the SAST combination is better because it has higher detection ratios and a lower marked function ratio compared to the C1 LLM combination. In addition, the SAST combination obtains substantially higher detection ratios with an acceptable increase in marked function ratios compared to the C2 and C3 LLM combinations. For Python, the SAST combination is better. Compared to the C2 and C3 LLM combinations, the SAST combination has comparable detection ratios with a much lower marked function ratio. Moreover, the marked function ratio of the C1 LLM combination is unacceptably high (i.e., 32.6%), making it an unfavorable choice.\nAnser to RQ3: Combining SAST tools substantially boosts detection ratios, ranging from 25.2% to 100.0%. Com- bining LLMs substantially reduces the marked function ratio by 40.9%-74.6% on average. The optimal approach differs across programming languages. For Java, the LLM combination provides the most effective solution, while for C and Python, the combination of SAST tools is better.\n5 DISCUSSION\n5.1 Evaluating Commercial LLM ChatGPT\nIn this work, we focus on open-source LLMs because using com- mercial LLMs for this task would be very costly, considering the large number of functions in each repository. However, to make our paper more self-contained, we also conducted small-scale ex- periments with the famous commercial LLM ChatGPT. Specifically, we set the ratio of vulnerable functions to clean functions as 1:100 and sampled the corresponding number of clean functions from the repository. We used the gpt-3.5-turbo-0125 model [66], the latest version of GPT-3.5, and set the temperature to 0 for more repro- ducible results. Table 4 presents the results of ChatGPT and two open-source LLMs, Llama3 and CodeBERT, with the same setting, averaged across Java, C, and Python. In general, the results show that the commercial LLM ChatGPT achieved lower vulnerability de- tection ratios and marked fewer functions as potentially vulnerable compared to the open-source LLMs such as Llama3 and CodeBERT. Among different ways of using ChatGPT, the few-shot prompting detects most vulnerabilities (i.e., 23.2%).\n5.2 Implications\nRepo-level vulnerability detection using LLMs holds substan- tial potential. While LLMs may not yet outperform SAST tools in"}, {"title": "Threats to Validity", "content": "every facet, as the first work exploring this direction, we have em- ployed generic LLM techniques without incorporating specialized designs to maximize their effectiveness. However, we have still ob- served that LLMs can achieve competitive performance compared to SAST tools, particularly when looking into individual SAST tools and LLMs. This underscores the significant potential for advancing repo-level vulnerability detection through further development and refinement of LLM-based approaches.\nRepo-level vulnerability detection is more practical than the function-level vulnerability detection. We recommend a gradual shift in vulnerability detection efforts from the existing function-level task formation to the repo-level vulnerability detec- tion task formation introduced in this study. As shown in Figure 2, datasets used in traditional function-level vulnerability detection studies rely on vulnerability-fixing commits to narrow the detection scope from the entire repository to the specific files modified in those commits. However, when the goal is to detect vulnerabilities, the corresponding vulnerability-fixing commit does not yet exist. Consequently, it is impractical to depend on such commits to limit the detection range. The repo-level vulnerability detection forma- tion addresses this issue by eliminating the need to narrow down the detection scope based on the fixing commits.\nAddressing the high marked function ratios of LLMs neces- sitates further investigation. While LLMs tend to detect more vul- nerabilities compared to SAST tools, they suffer from high marked function ratios, akin to high false positive rates. Although we man- aged to significantly reduce these ratios by combining multiple LLMs, there remains large room for enhancement. Potential solu- tions may entail developing techniques such as, filtering out less trustworthy predictions from LLMs, integrating additional contex- tual information (e.e., the structural features in the repository) with LLMs, refining prompting strategies, or employing more advanced ensemble methods.\nExploring advanced combinations of LLMs and SAST tools holds promise. Each category of approach has its strengths and weaknesses in detecting vulnerabilities in software repositories: LLMs detect more vulnerabilities but with higher marked function ratios, whereas SAST tools detect fewer vulnerabilities but with lower marked function ratios. Future research could propose meth- ods that can effectively leverage the strengths of both approaches for enhanced vulnerability detection.\n5.3 Threats to Validity\nThreats to internal validity relate to possible errors and biases in our experiments. To mitigate these risks, we utilized the official implementations of the studied tools and models to ensure correct- ness. We have reviewed and validated our code and data, and they are publicly accessible to promote transparency and reproducibility. Another potential threat concerns the presence of undetected vul- nerabilities in the vulnerability benchmarks. However, our objective was to assess whether the selected tools/models could accurately"}, {"title": "6 RELATED WORK", "content": "Studies of SAST Tools. There are many existing studies evaluating static analysis security testing (SAST) tools, e.g., [5, 8, 10, 13, 26, 32, 32, 36, 45, 46, 46, 48, 49, 51, 52, 61, 63, 68, 74, 76]. Some of these studies utilize synthetic benchmarks to evaluate SAST tools, e .g., [5, 8, 48]. Several studies have evaluated SAST tools on real-world vulnerability datasets in C/C++ or Java [32, 46, 49, 51, 74]. For instance, Lipp et al. [51] focused on SAST tools for C programs, evaluating the effectiveness of six tools on real-world vulnerability datasets. Li et al. [49] evaluated the effectiveness of seven free or open-source SAST tools on real-world vulnerability datasets. Several studies have evaluated SAST tools in other research areas, such as Android [13, 26, 68].\nOur work distinguishes itself from existing studies in the fol- lowing aspects. Firstly, we evaluated SAST tools and LLM models on three popular programming languages: Java, C, and Python. This ensures diversity in the experimental datasets, unlike many previous studies that focused on a single programming language. Secondly, to the best of our knowledge, we are the first to evaluate SAST tools for Python vulnerability detection. Thirdly, we are the first to comprehensively compare SAST tools with LLMs, which have demonstrated strong effectiveness in many other code-related tasks such as code generation [54] and automated repair [44].\nLLM-based Vulnerability Detection. There are many existing studies utilizing large language models (LLMs) for vulnerability detection, e.g., [27, 30, 35, 37, 55, 67, 84, 85, 87]. For instance, Liu et al. [55] leveraged the abstract syntax tree and program dependence graph of functions to pre-train their LLM. The pre-training process enabled the LLM to predict dependencies within the functions effec- tively. Peng et al. [67] employed program slicing, a technique that extracts control and data dependency information, to enhance the capability of LLMs in vulnerability detection. Additionally, Zhang"}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this study, we compare the efficacy of 15 diverse SAST tools alongside 12 popular or leading LLMs. Our experiments focus on real-world vulnerabilities across three prevalent programming lan- guages: Java, C, and Python. Our findings indicate that SAST tools exhibit low vulnerability detection ratios while maintaining a low marked function ratio, akin to a low false positive rate. In contrast, LLMs demonstrate high vulnerability detection ratios but are ac- companied by elevated marked function ratios (akin to high false positive rates). Through ensemble approaches, we demonstrate that combining SAST tools and LLMs mitigates their respective limitations, resulting in improved overall vulnerability detection performance.\nIn the future, we aim to investigate more sophisticated methods for effectively combining multiple LLMs and SAST tools to further improve vulnerability detection performance."}]}