{"title": "DATAMAN: DATA MANAGER FOR PRE-TRAINING LARGE LANGUAGE MODELS", "authors": ["Ru Peng", "Kexin Yang", "Yawen Zeng", "Junyang Lin", "Dayiheng Liu", "Junbo Zhao"], "abstract": "The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by \u201creverse thinking\u201d \u2013 prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from point-wise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score 1=5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) demonstrate performance emergence driven by data scaling laws, the significance of data has become increasingly evident (Kaplan et al., 2020; Brown et al., 2020; Chowdhery et al., 2023). This trend prompts researchers to explore how to select pre-training data, including deduplication (Lee et al., 2022), domain mixing (Gao et al., 2020; Shen et al., 2023), heuristic-based data selection (Rae et al., 2022; Wenzek et al., 2019), and data sampling using LLM quality signals (Gunasekar et al., 2023; Wettig et al., 2024a). Although these efforts aim to enhance data quality and diversity, deduplication and domain mixing are solely used as a priori or post-hoc steps in the data selection process. Furthermore, existing data selection methods typically rely on limited heuristics and human intuition, lacking comprehensive and clear criteria for data selection, thus making the selection of ideal pre-training data for LLMs an unresolved challenge.\nFollowing this line, this paper provides guidelines for selecting pre-training data, including quality criteria, application domains, and a Data Manager (DataMan) with a comprehensive quality rating and domain recognition, equipped by data sampling strategies to enhance the LLM performances. Firstly, we believe that excellent quality criteria must: 1)-apply to a wide variety of texts; 2)-demonstrate a deep understanding of content, capturing semantic levels; and 3)-complement each other. However, existing studies (Rae et al., 2022; Wettig et al., 2024a) rely on limited heuristics and human intuition, while grounded in empirical findings, lack generality and comprehensiveness. To address this issue,"}, {"title": "2 RELATED WORK", "content": "Deduplication. Deduplicating training data is now standard in managing pre-training data for LLMs, as it greatly impacts performance (Lee et al., 2022). While Kaplan et al. (2020) and Hoffmann et al. (2022b) examine scaling laws with unique data trained for one epoch, some studies (Hernandez et al., 2022; Muennighoff et al., 2024; Xue et al., 2024) suggest that repeated data can harm performance, particularly as repetition epochs and model size grow. Additionally, fuzzy or semantic deduplication improves LLM performance (Jiang et al., 2022; Abbas et al., 2023; Tirumala et al., 2023). But deduplication should precede quality-based selection and domain mixing, and it is unsuitable for sampling fixed-size subsets.\nHeuristic-based Selection. This selection approach includes two methods: Rule-based heuristics, which apply manually crafted rules\u2014such as mean word length, stop word fraction, and word repetition thresholds (Lauren\u00e7on et al., 2022; TogetherAI, 2023; Penedo et al., 2023; Soldaini et al., 2024) to filter data, exemplified by the C4 filter (Raffel et al., 2020) and Gopher rules (Rae et al., 2022). These reduce noise effectively but need complex manual design. In contrast, Model-based heuristics use models like binary grammar discriminators (Chowdhery et al., 2023; Touvron et al., 2023a) to select data resembling the target domain, alongside techniques like importance resampling (Xie et al., 2023) and perplexity filtering (Wenzek et al., 2019; Muennighoff et al., 2024; Marion et al., 2023a). However, they require more precise quality ratings for optimal selection.\nDomain mixture. Most pre-training datasets, like the Pile (Gao et al., 2020), comprise mixed data from various sources and domains (Nijkamp et al., 2023; Zhang et al., 2023; Yang et al., 2024b; Maini et al., 2024; Li et al., 2024). As LLMs gain traction, domain-specific data for improved functionalities are increasingly used in model training (Du et al., 2022; Gao et al., 2023). Identifying the optimal domain mixture ratio is essential for effective LLM pre-training (Wang et al., 2023). Early attempts to define this relied on experiments and intuition (Gao et al., 2020; Thoppilan et al., 2022). Recent studies have begun to use automatic methods, such as domain generalization (Xie et al., 2023; 2024), domain gradients (Fan et al., 2023), and loss evaluation (Xia et al., 2023), to assign domain weights and assess model performance across various mixtures. This inspires our work to introduce domain types to assist in pre-training data mixture.\nLLM quality signals. Appropriate LLM quality signals are valuable for selecting pre-training data (Gunasekar et al., 2023). Research shows that data enriched with facts and trivia aids LLMs in accurately addressing niche topics and fictional worlds (Petroni et al., 2019; Korbak et al., 2023). Other studies have emphasized the educational value of data as a key for enhancing LLMs' reasoning capabilities (Wei et al., 2022; Kojima et al., 2022). Recent efforts have integrated these insights,"}, {"title": "3 ANNOTATING TEXT BY DATAMAN", "content": "We developed DataMan (see Figure 1), a data manager that comprehensively annotates 14 quality criteria and domain types, enabling ideal data selection and mixing."}, {"title": "3.1 OVERVIEW OF THE ANNOTATION", "content": "Let t = {t1,..., tn} represent all documents to be annotated. The annotation results of DataMan correspond to querying the ratings of document $t_n$ across all quality criteria and its domain type. Assume that the quality ratings and domain recognition are in a multi-level annotation format as L = {($l_{n,1}$,...,$l_{n,C}$), ..., ($l_{n,1}$, ..., $l_{n,L}$)}, where $l_i \\in$ {1, \u2026, K} is the label for the $C$-th criterion or domain of document $t_n$. For this paper, K is 5 for quality ratings and 15 for domain recognition. Let F be a class of functions, and $f \\in$ F be the annotation function. We use a super LLM as the annotation function, recording the annotation results for each criterion and domain, expressed as: $f(t, L) = \\{(t_1, l_{t_1,1},..., l_{t_1,F}),..., (t_n, l_{t_n,1}, ..., l_{t_n,F})\\}$. Thus, we can quickly create a fine-tuning dataset for DataMan, S = {($t_i, l_{t_{i,1},..., t_{i,F}}$)}. Essentially, our method uses pointwise rating by pointwise learning to rank (L2R) model (Liu et al., 2007; 2009). We minimize the loss function based on the document, annotation labels, and function:\n$\\mathcal{L}(f; t, L) = \\sum_{i=1}^{n} (f(t_i) \u2013 l_i)^2$.\nThis trains DataMan to learn the annotation functions for each quality criterion and domain type."}, {"title": "3.2 PROMPT CURATION", "content": "How to define the quality criteria and domain types of texts? We believe that excellent quality criteria should: 1)-apply to a wide variety of texts, 2)-demonstrate a deep understanding of content, capturing semantic levels, and 3)- complement each other. However, prior studies largely relied on human intuition, such as educational value in Gunasekar et al. (2023), writing style in Wettig et al. (2024a), and toxicity and privacy in Korbak et al. (2023), which lack generality and comprehensiveness. To address this issue, we undertook an in-depth exploration of text quality, motivated by \u201creverse thinking\u201d\u2014prompting the LLM to self-identify which criteria benefit its performance. Specifically, since LLMs' pre-training abilities are closely related to their PPL (Muennighoff et al., 2024; Marion et al., 2023b), where \u201chigh PPL indicates data is difficult to learn, and vice versa\", we focused on the training text from various sources with the top 2% and bottom 2% of PPL. We devised an analytical prompt for a Super LLM to investigate the reason behind these perplexity anomalies in documents, aiming to analyze the traits of both easy-to-learn and difficult-to-learn data. Through iterative refinement, we derived 14 quality criteria as shown in Figure 1. Additionally, we identified the 15 domain types that need to be assessed from typical LLM application industries (Naveed et al., 2023), integrating the \"let's think step by step\" chain-of-thought prompting strategy (Wei et al., 2022) and a thoughtfully designed system prompt. Further details of the entire process are in Appendix A.\nPrompt validation. We validate prompt effectiveness using clear cases before prompt use. Initially, we gathered a pool of documents preliminarily rated by an independent group, splitting them into two sets of ten documents each-high-rated and low-rated to ensure a distinct quality gap. Table 4 of Appendix A details these document sources. These 20 documents were randomly shuffled and then rated on a scale of 1-5 based on each quality criterion by five independent human annotators who had not seen them before. Subsequently, using the super LLM with our prompt, we evaluated the same documents and compared super LLM ratings with human majority votes, finding over 95% agreement. Also, we ensured inter-rater reliability among human annotators by calculating the Kappa coefficient (McHugh, 2012), which validated the consistency of human ratings. However, this human validation remains subjective, we hope the community develops a more rigorous method."}, {"title": "3.3 WHY USE POINTWISE RATING?", "content": "Using LLMs to rate text falls into two categories: pointwise rating and pairwise rating. Here, we explain why we chose the pointwise rating from the following three aspects:\nPairwise rating limitations. We revisited pairwise rating (Wettig et al., 2024a) and observed minimal quality differences among the top-3 documents ranked by writing style, as even humans struggle to discern differences (see Table 5). This highlights the limitations of pairwise ratings when quality differences are marginal, as documents meeting an \u201cacceptable\u201d quality threshold should be accepted, where pointwise rating aligns well with human judgment. Appendix B.1 notes that creating a finetuning dataset requires Super LLM to make 40 predictions per criterion and document pairs, then use LLM preferences as labels. Table 5 shows that document pairs need a \u226550% selection probability difference for quality criteria, also required for inference in Table 6, thus questioning the practicality of pointwise ratings. For N documents, pointwise ratings only need N ratings, whereas pairwise ratings demands $N * (N \u2212 1)$ comparisons with 40 predictions each, greatly increasing costs. Despite both ratings capture different aspects, pointwise rating's broader applicability and cost-effectiveness in fine-tuning dataset curation or pre-training data annotation make it our preferred choice.\nBound pointwise rating error. We present the mathematical connection between rating errors and loss in the pointwise rating model (Chen et al., 2009).\n$1 \u2013 NDCG(f;t, L) \\leq \\frac{15\\sqrt{2}}{N_n} (\\mathcal{L}(f;t, L))^{1/2},$\n$NDCG(f;t, L) = \\frac{1}{N_n} \\sum_{i=1}^{n} G(l(\\pi_f(t_i)))D(t_i),$\n$N_n = max_{\\pi} \\sum_{i=1}^{n} G(l(\\pi(t_i)))D(t_i),$\nwhere NDCG is rating measures defined with respect to K-level ratings L, G is the gain function, \u03c0f is the rating list produced by the rating function f, and D is the position discount function. One usually sets G(z) = $2^z$ \u2212 1, $D(z) = log_2(1+z)$ if z \u2264 M, and D(z) = 0 if z > M (M is a fixed integer). Thus, as the pointwise rating model minimizes loss via training, the rating error also reduces, validating the feasibility of pointwise ratings theoretically.\nFairly compare both rating results. To ensure a fair comparison despite differing evaluation metrics, we ranked annotations from Qurating's pairwise and DataMan's pointwise ratings on the same 58,824 documents from the first split of Qurating's 1B analysis set. We then presented the top, median, and bottom ten documents based on Qurating's four criteria-writing style, facts and trivia, educational value, and required expertise and DataMan's Overall Score with sum of the remaining criteria. All results are provided here. Our conclusions are: i)-Higher Qurating criteria correlate with higher DataMan's Overall Scores for the top and bottom 10 documents, and vice versa. ii)-Median 10 documents show minimal changes in Qurating's criteria, yet their Overall Scores vary from 3 to 5, supporting that \"Pairwise rating struggles with minimal quality differences, while pointwise rating matches human judgment.\" iii)-Rankings by Overall Score highlight top-quality documents, especially in STEM fields. iv)-The \u201capplication_domain\u201d field accurately categorizes domains, aiding in domain-specific continue pre-training, as shown in Table 2."}, {"title": "3.4 TRAINING THE DATAMAN MODEL", "content": "After developing the prompt and rating method, we collected documents from both in-source and out-ofsource of the SlimPajama corpus (Soboleva et al., 2023) to train DataMan. By prompting the Super LLM, we obtained 14 quality ratings and domain types for each document, creating a fine-tuning dataset of 35,700 documents at a cost of $13,858. Documents were limited to 2,048 tokens (averaging 810 tokens), surpassing the [256, 512] token range of Wettig et al. (2024a) when handling sentences with broader length variations. Subsequently, we fine-tuned the DataMan model with"}, {"title": "4 MANAGING DATA BY DATAMAN", "content": "In this section, we apply the DataMan model to manage data by selecting a high-quality, diverse document subset from the pre-training corpus. Each document $d_i$ in the corpus D, with source $s_i$, is annotated by the DataMan model with 14 quality ratings and domain types as L = {($l_{i,1}$,...,$l_{i,14}$,q)}, where q is the domain type. Given the source and domain distribution probabilities P(s) and P(q) respectively, we perform top-k (k is the selected subset size) sampling without replacement for each quality criterion across source and domain distribution, using the probability:\n$P(d_i) = \\frac{P(d_i | l_i, s, q) \\cdot P(s,q)}{\\sum_{d_j \\in D; j \\in top-k(L)} P(d_j | l_i, s, q) \\cdot P(s,q)},$ and $P(d_i|l_i) = \\sum_{q} P(d_i|l_i)$.\nThis method maximizes sample representativeness based on quality rating while ensuring diversity in source and domain distributions, and sampling without replacement prevents duplicate data. To assess if the Overall Score covers all criteria, we substitute top-k with uniform sampling based on fixed Overall Score ratings. These techniques implicitly steer language modeling toward reward-weighted regression (Peters & Schaal, 2007; Korbak et al., 2023), expanding maximum likelihood estimation loss with a data reward mechanism."}, {"title": "5 EXPERIMENTS", "content": "We empirically validate the DataMan method by training the language model from scratch."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "DataPajama. We used the Llama tokenizer (Touvron et al., 2023a) to segment Slimpajama corpus (Soboleva et al., 2023), a cleaned and deduplicated version of the RedPajama (TogetherAI, 2023), into 1024-tokens documents. The DataMan model then annotated these documents with 14 quality ratings and domain types, creating the 447B token DataPajama for pre-training. Although the annotation process is expensive, it can be reduced via large-scale parallelization and cost-effective DataMan models. The quality ratings and domain types in DataPajama can serve various purposes, such as data selection, data mixing, or continued pre-training in specific domains. Detailed statistics of DataPajama can be found in Table 11 of Appendix C.\nData selection methods. For each baseline, we select a 30B token training dataset from DataPajama with one of the following methods, while retaining the original source proportion as the overall dataset. We leave it as future work to explore combinations of quality criteria to broaden our method.\n\u2022 Uniform: We select randomly with a uniform probability across documents. For comparison's sake, we train an additional model on 45B tokens, requiring 50% more compute.\n\u2022 DSIR: We apply data selection with importance resampling (DSIR) (Xie et al., 2023) and select examples that resemble either English Wikipedia or the Book domain (TogetherAI, 2023)\u2014 commonly used as proxies for quality (Brown, 2020; Touvron et al., 2023a; Xie et al., 2023). We follow Xie et al. (2023) and train hashed bigram models on DataPajama and the target data.\n\u2022 Perplexity Filtering: We implement perplexity filtering (Wenzek et al., 2019; Marion et al., 2023a) and select the documents with the lowest/highest perplexity scores, as computed by a pre-trained Sheared-Llama-2.7B model (Xia et al., 2023)-2\u00d7 the size of our DataMan model.\n\u2022 Sample with Qurating: We sample 30B tokens according to each of the four criteria described in Wettig et al. (2024a): writing style, facts and trivia, educational value, and required expertise. Specifically, we normalize the variance of the quality ratings to be 1 and then sample with temperature \u03c4 \u2208 {0.0 (i.e., top-k selection), 2.0}. Additionally, we merge the Qurating-sampled"}, {"title": "5.2 RESULTS", "content": "We report the model's perplexity and ICL results in Table 1, the instruction-following evaluation in Figure 2, the domain-specific continue pre-training performance in Table 2. Appendix C provides comprehensive results for all models, including validation and test perplexity across sources in Tables 12, 13, and ICL results for individual task in Table 14.\nTraditional methods perform poorly. Table 1 indicates that DSIR and perplexity filtering perform worse than random uniform sampling. This indicates that model output-based methods, despite their widespread use, are ineffective for data selection.\nClear quality criteria work, but Qurating's criteria mix fails. Table 1 shows that selecting data with Qurating's four criteria (Wettig et al., 2024a) improves performance compared to uniform sampling, highlighting the importance of clear LLM quality signals. Educational value \u03c4 = 2.0 is the current SOTA baseline. However, the criteria mix of Qurating's four criterion did not perform well, possibly due to a lack of complementarity among the Qurating's criteria.\nDataMan's criteria surpasses SOTA baseline and Overall Score works best. In Table 1, compared to the SOTA baseline (Educational value \u03c4 = 2.0), our 13 individual quality criteria improved ICL performance by 0.4% to 4.3%. For example, the sample-with-creativity model achieved an impressive score of 60.6 in commonsense reasoning tasks. As the comprehensive criterion Overall Score increased from 1 to 5, performance gains significantly, highlighting the necessity of quality ranking. Our Overall Score l=5 works best, even exceeding the Uniform +50% data baseline, validating the feasibility of combining the 13 quality criteria into a composite criterion via LLM weighting. This approach not only avoids the disruption of manual adjustments but also achieves optimal results. The correlations among all criteria in Figure 5 further confirm this point.\nPPL and ICL are not strictly aligned. Our results in Table 1 reveal a trend where PPL and ICL metrics correlate to a degree, increasing or decreasing simultaneously, but they are not aligned strictly. This meets with the intuition that PPL implies general understanding, while ICL focuses more on downstream generalization. Further analysis see Figure 6 in Appendix C. Notably, DataMan's high-rated Overall Score achieves an optimal trade-off between understanding and generalization."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce DataMan, a comprehensive data manager capable of quality rating and domain recognition, developed to facilitate the pre-training data selection and mixing. By using DataMan to annotate pre-training datasets and sampling data based on quality ratings while balancing domain diversity, our trained models demonstrate significant improvements in language modeling, task generalization, and instruction following. Through extensive experiments, we provide valuable insights for the community studying the relationship between data and large language models."}, {"title": "7 LIMITATIONS", "content": "We acknowledge several limitations in our work. First, DataMan's reliance on LLMs for text quality assessment and domain categorization may inherit biases from these models. Second, DataMan's inference accuracy is not yet optimal, sometimes causing misclassification, highlighting the need for a large-scale collection of documents with diverse quality differences for fine-tuning. Third, using SlimPajama alone as a pre-training corpus limits result reliability, incorporating additional corpora would be better. Fourth, the model size is restricted by data and training resources, resulting in models with only 1.3B parameters, whereas increasing parameters might reveal interesting phenomena. Lastly, the considerable costs of developing data managers, data filtering, and pre-training experiments could hinder further research in this domain. We aim to address these in future work. Despite these limitations, DataMan remains a powerful tool for data selection and mixing."}]}