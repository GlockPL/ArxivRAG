{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "authors": ["Gemma Canet Tarr\u00e9s", "Zhe Lin", "Zhifei Zhang", "Jianming Zhang", "Yizhi Song", "Dan Ruta", "Andrew Gilbert", "John Collomosse", "Soo Ye Kim"], "abstract": "Compositing an object into an image involves multiple non-trivial sub-tasks such as object placement and scaling, color/lighting harmonization, viewpoint/geometry adjustment, and shadow/reflection generation. Recent generative image compositing methods leverage diffusion models to handle multiple sub-tasks at once. However, existing models face limitations due to their reliance on masking the original object during training, which constrains their generation to the input mask. Furthermore, obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of unconstrained generative object compositing, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism. Additionally, if an empty mask is provided, our model automatically places the object in diverse natural locations and scales, accelerating the compositing workflow. Our model outperforms existing object placement and compositing models in various quality metrics and user studies.", "sections": [{"title": "1 Introduction", "content": "The rise of generative AI models and social media has sparked widespread interest in image editing techniques. Realistic and controllable image editing is now indispensable for applications like content creation, marketing, and entertainment. One pivotal step in most editing processes is image compositing, seamlessly integrating a foreground object with a background image. However, image compositing presents numerous challenges, including incorporating new shadows or reflections, illumination misalignment, unnatural foreground object boundaries, and ensuring the object's pose, location, and scale are semantically coherent.\nPrevious works on image compositing [5,30,32, 59, 61] focus on specific sub-tasks, such as image blending, harmonization, object placement, or shadow generation. More recent methods [9,36,50,62] demonstrate that some separate compositing aspects (i.e., color harmonization, relighting, object geometry adjustments, and shadow/reflection generation) can be handled simultaneously using diffusion models [18,46]. Such methods are often trained in a self-supervised way, masking an object in the ground truth image and using the masked image as input [9,62] or denoising only within the mask region during the reverse diffusion process [9,50]. Therefore, a mask is required as input during inference, leading to several limitations: (i) drawing an accurate mask can be non-trivial for an ordinary user and may result in unnatural composite images depending on the location, scale and shape of the input mask; (ii) the mask region constrains the generation, and their training data does not consider object effects, limiting the ability to synthesize appropriate effects such as long shadows and reflections; (iii) background areas near the object tend to be inconsistent with the original background, as the model does not see those regions if covered by the mask.\nTherefore, in this paper, we propose a generative image compositing model that generates beyond the mask and even with an empty mask, in which case the model automatically composites the object in a natural location at a suitable scale. Our model is the first end-to-end solution for image compositing that simultaneously addresses all sub-tasks of image compositing, including object placement. To achieve this, we first use image inpainting to create training data comprising image triplets (a foreground object, a full background image, and"}, {"title": "2 Related Work", "content": "Personalized Image Generation. The landscape of image generation has transformed significantly with the introduction of diffusion models [18,46], which iteratively apply transformations to images, creating intricate, diverse, and realistic visual content. Novel approaches in personalized image generation, such as DreamBooth [47], explore finetuning vocabularies to define specific identities. [7,14,35] follow the same idea, while [8,19,49] leverage large-scale upstream training to eliminate the need for test-time finetuning. BLIP-Diffusion [28] proposes accelerating the process by aligning images with text.\nImage Editing. Diffusion models are also revolutionizing image editing. These models, paired with text prompts, enable methods like SDEdit [38], which accepts various inputs (i.e. strokes, image patches) for local editing. Other techniques (Inpaint Anything [64], BlendedDiffusion [4]), utilize text to describe objects to be inserted into images. Semantically disentangling GAN latent spaces [2,6,45] and employing semantic masks [6,15,31,55] for localized edits have also paved the way for controllable editing. However, diffusion models extend these capabilities to more intricate and diverse editing tasks [16,21, 23, 34].\nGenerative Image Compositing. Early compositing methods depended on hand-crafted features [26] and rendering [20] or 3D techniques [22], often being tedious and limited. Nowadays, these methods are predominantly based on diffusion models. ObjectStitch [50] composites an object into an image based on the location and scale provided as a bounding box without extending the generation process outside of this bounded region. [24,62] add more flexibility by enabling free-form mask inputs, which guide the generated object's location and shape. ControlCom [68] offers controllability by independently performing"}, {"title": "3 Methodology", "content": "We define a novel problem of unconstrained generative object compositing, where the generation is not restricted to the input mask when compositing an object into another image. The main reason why existing image compositing models focus on mask-bounded generation stems from their training data. These models often obtain the input background image by masking out the original object from the full image, similar to how image inpainting models gather their data. In other words, their problem can be regarded as a reference-object-based image inpainting problem. However, employing diffusion models for denoising the entire image based on masked inputs does not inherently enable the synthesis of convincing object effects extending beyond the mask. Therefore, a new training data generation scheme is essential for obtaining a diffusion-based model addressing unconstrained generative object compositing. Furthermore, upon observing the sensitivity of off-the-shelf image encoders to object scale, we integrate a multi-scale embedding to help our model generate objects at various scales. We train the full framework in multiple stages to obtain optimal compositing performance."}, {"title": "3.1 Data Generation Pipeline", "content": "Data generation is a crucial component of our method to enable unconstrained compositing. We avoid the necessity for masking the original image by going one step further and inpainting the object within the original image. This approach yields a full background image to be used as input to our model instead of a masked background image, allowing us to eliminate the need for an input mask. Diffusion-based inpainting models [3,37,46,58,63] have shown remarkable performance. However, they may occasionally introduce generated new objects inside the mask. To mitigate this, we first apply a GAN-based inpainting model [72] to remove the object, then use a diffusion-based inpainting model on the GAN results with parameters set to preserve the underlying content. This sequential pipeline (Fig 2) combines best of both worlds: consistent removal capability of GANs and enhanced inpainting quality of diffusion (more details in SuppMat).\nExisting image compositing methods ignore object effects such as shadows and reflections when computing the input masked image, which limits their ability to synthesize such effects. For our model, we carefully consider shadows and reflections when calculating the inpainting mask for each image.\nSpecifically, starting from any natural image, the following steps define the proposed data generation pipeline: (i) Segment the image via EntitySeg [42], obtaining accurate foreground segmentation maps by filtering out those with low segmentation confidence score (<30%). (ii) Filter out objects that are too large (> 80% of the image) and have little background context or too small (<1%) containing insignificant objects (e.g., small fragments of highly occluded objects). (iii) Apply an instance shadow detection model [56] on a valid foreground object and retain shadows with confidence scores exceeding 80% (middle row in Fig 2). An exception is made for confidence scores exceeding 60% if: (a) multiple objects of similar size are present, (b) they all have similar object-shadow vectors (\u03c3 < 2), and (c) some of them have high confidence scores, in which case we"}, {"title": "3.2 Model Architecture", "content": "As shown in Fig 3, the background image $I_{BG}$ and a mask $I_p$ are concatenated with the 3-channel noise $\\epsilon$ and fed into our model's backbone, Stable Diffusion 1.5 (SD) [46], consisting of a variational autoencoder (G, D) and a U-Net. The foreground object $O$ is processed by an object encoder $E$ (CLIP ViT-L/14 [43]) and a content adaptor $A$, aligning embeddings to text embeddings as in [50].\nOptional Position Mask Input. Our model can place objects in diverse natural locations and scales by feeding an empty mask (all values set to -1) instead of a position mask. This eliminates the laborious task of manual mask creation, accelerating the user workflow for image compositing. Moreover, the generated outputs can serve as location/scale suggestions to assist in creative ideation."}, {"title": "3.3 Training Strategy", "content": "Multi-Stage Training. Our model comprises different subnetworks: a U-Net, an image autoencoder (G, D), an object encoder E, and a content adaptor A. To train the network to understand accurate locations and scales without needing a mask, an empty mask with all values set to -1 replaces $I_p$ in the initial training stages. Notably, several training stages are required to reach our final model.\n1) The object encoder $E$ is frozen while both the U-Net and content adaptor $A$ are trained until convergence. This first stage model (S1) generates highly diverse compositions but struggles with foreground object identity preservation.\n2) To improve identity preservation, $E$ is fine-tuned on multiview [65] and video data [39,60], using the paired data generation scheme from [51]. Then, $E$ is frozen"}, {"title": "4 Experiments", "content": "Training Details. During training, the learning rate is set to $10^{-4}$ for $E$ and $4 \\times 10^{-5}$ for the U-Net. All training stages utilize 8 A100 GPUs with the Adam optimizer and an effective batch size of 1024, using gradient accummulation.\nEvaluation Dataset. We collect a real-world test set, DreamBooth, of 113 object-background pairs by coupling object images from [47] with plausible background images from Pixabay. Additionally, to evaluate compositing quality against ground truth, we generate Pixabay-Comp, a synthetic dataset of 1K images where the input object image is obtained by randomly perturbing the object. For evaluating object placement prediction, we rely on OPA [33], a test set with 11K paired foreground and background samples designed for this task.\nEvaluation Metrics. To assess the identity and semantics preservation of the foreground object, CLIP-Score [17], DINO-Score [41], and DreamSim [13] are computed by comparing the input object image and a cropped region of the"}, {"title": "4.1 Comparison to Existing Methods", "content": "Our model can automatically place objects in natural locations while generating realistic composite images. Hence, we comprehensively evaluate our model for each task: Generative Object Compositing and Object Placement Prediction. We additionally compare to sequentially performing both tasks.\nGenerative Object Compositing. For this task, we compare to recent diffusion-based models: ObjectStitch [50], Paint by Example [62], TF-ICON [36], AnyDoor [9] and ControlCom [68], which all require a mask input to guide the generation. Thus, we feed an input mask ($I_p$) to our model as well.\nAll compared compositing models restrict generation to a part of the image defined by a mask. We zoom in on the generated area to boost their performance by expanding the bbox into a square with an amplifier ratio of 2.0, then resize it and paste it back to the original image, as in [9]. As shown in Tab 1, our model outperforms existing diffusion-based compositing methods [9, 36, 50, 62, 68] in most quality metrics. To assess compositing quality and object identity preservation through subjective human evaluation, we performed a user study with 113 pairs of images in the DreamBooth test set shown to at least 5 users each, comparing our model's results with another model side by side, in randomized order. As shown in Fig 6 (left), users preferred our model with up to 89.3% preference rate in quality and 87.4% in faithfulness of object identity.\nFig 4 showcases the different aspects in which our unconstrained model is able to outperform mask-based state of the art image compositing models: (i) allowing the network to leverage information from the entire background image rather than just a masked area leads to better background preservation (rows 3 to 6); (ii) enabling object effects such as shadows and reflections beyond the bounding box allows for more natural and realistic composite images (rows 3-4); (iii) our model's success is not bounded by the accuracy of the bounding box thanks to its ability to adjust any misaligned bounding box (rows 1-2)."}, {"title": "4.2 Effect of Each Training Stage", "content": "We analyze each training stage's impact on our model in Tab 4. Starting with the initial model (S1) trained in our unconstrained compositing framework, each subsequent stage detailed in Sec 3.3 is incrementally integrated. Fine-tuning with multi-view and video data (S2) improves identity preservation but limits object pose diversity, yielding unnatural results. Merging S2 with S1 in S3 retains benefits while broadening object pose variety. Introducing a rough bounding box input with 50% probability in S4 allows the model to offer positioning guidance while also learning more precise object locations and scales. Our multiscale approach in S5 equips the model with more informative encodings of scale, resulting in semantically realistic compositions (Fig 7). More details in SuppMat."}, {"title": "4.3 Applications", "content": "Creative Composite Image Recommendation. Our model can provide a diverse range of composite images varying the object's location, scale and pose (Fig 1, Fig 8). Such results can serve as creative recommendations for the user."}, {"title": "4.4 Limitations", "content": "Fig 10 illustrates limitations of our model that we leave as future work. Compositing objects with hard lighting can sometimes cause unintended relighting of the entire scene rather than just the targeted object. Additionally, generating lateral reflections on glass remains challenging. Potential solutions include: (i) applying lighting perturbations on the object during training, and (ii) adding a symmetric object mask relative to the mirror plane during object inpainting."}, {"title": "5 Conclusion", "content": "In this paper, we define a novel task of unconstrained object compositing and propose the first diffusion-based framework to address this task. Our model is carefully trained with a multi-stage strategy on synthetic paired data generated via inpainting while considering object effects. Through extensive evaluation across diverse datasets and metrics, we demonstrate state-of-the-art results in both object placement prediction and generative object compositing. Additionally, we show the benefits of unconstrained object compositing on various applications. We hope our paper can spur further research in this exciting direction."}, {"title": "1 Unconstrained Image Compositing", "content": "As explained in Section 1, prior generative compositing models tend to restrict the generation to a bounding box provided as part of the input. Hence, the definition of the provided mask becomes a pivotal part of the compositing process. If the region defined in the mask is not completely accurate, providing a bounding box that is slightly misplaced or needs rescaling, the quality of the composition can be highly compromised. In this paper, this limitation is addressed by the introduction of unconstrained image compositing as a novel task. By allowing generation outside the bbox, the need for crafting accurate masks disappears, alleviating the user work and accelerating the compositing work flow. Fig 1 shows visual examples of the main benefits of using an unconstrained approach: (a-c) an inaccurate bbox can lead to unnatural composite results (object floating from the surface, unnatural object scale, etc) when using prior mask-based image compositing models, (d-e) enabling object effects exceeding the bbox boundaries becomes possible with our model leading to more natural results, (f) by providing the entire background image as part of the input (instead of a masked background), our model achieves a more faithful background preservation around the object."}, {"title": "2 Data Generation", "content": "We introduce in Section 3.1 a fully automated data generation pipeline that obtains synthetic background images by leveraging image inpainting techniques, while carefully considering shadows and reflections.\nReflection Mask While an instance shadow detector is used for detecting shadows and estimating their masks, the reflection masks are obtained by using certain heuristics. Only when there is a water body involved in the image, the object mask is vertically flipped to obtain the reflection mask. However, reflections on water are contingent upon various factors including lighting conditions and camera angle, often resulting in reflections that do not align directly beneath"}, {"title": "52 Model", "content": "The row where we place the horizontal axis is computed as:\n$Y_{axis} = y_w + \\nu (y_l - y_w),$ and v=1 . Fig 2 compares the reflection mask obtained using our proposed"}]}