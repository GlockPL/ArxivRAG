{"title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs", "authors": ["Yi Fang", "Jiacheng Shen", "Qiaoyu Tan", "Bowen Jin", "Sirui Ding", "Jiawei Han"], "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework. However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG). It is underexplored how MLLMs can incorporate the relational information (i.e., graph structure) and semantic information (i.e., texts and images) on such graphs for multimodal comprehension and generation. In this paper, we propose GRAPHGPT-0, which supports omni-multimodal understanding and creation on MMAGs. We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs. Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs. Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Models (MLLMs) [6, 20, 22, 31] have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. However, in real-world scenarios, there exists a pervasive graph-structured relationships between texts and images. Such graph-structured relationship can be described as \u201cMultimodal Attributed Graphs\" (MMAGs) [17, 27, 40], where nodes are associated with image and text information. For example, the artwork graph [25] is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references. While MLLMS have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGS.\nIn this context, we formulate the problem of multimodal content generation on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, generating an image and a text for a product node linked to others through co-purchase edges in an e-commerce MMAG is equivalent to recommending [5, 24] potential future products to users. Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork [7, 14] that reflects the subtle styles of various artists and genres.\nHowever, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges: (1) Graph Size Explosion: Although MMAGs provide sub-"}, {"title": "2. Problem Formulation", "content": ""}, {"title": "2.1. Multimodal Attributed Graphs", "content": "Definition 1 (Multimodal Attributed Graphs (MMAGs)) A multimodal attributed graph is defined as $G = (V, E, P,D)$, where $V$, $E$, $P$, and $D$ denote the sets of nodes, edges, images, and documents, respectively. Each node $v_i \\in V$ contains corresponding image information $p_{v_i} \\in P$ and textual information $d_{v_i} \\in D$.\nSome examples of MMAGs include (1) e-commerce produce graphs ($G$), where product nodes ($v \\in V$) are associated with product image ($p \\in P$) and title ($d \\in D$); and (2) artwork graphs ($G$), where artwork nodes ($v \\in V$) contain picture ($p \\in P$) and title ($d \\in D$)."}, {"title": "2.2. Problem Definition", "content": "Definition 2 (Node Multimodal Content Generation on MMAGs) In a multimodal attributed graph $G = (V,E,P,D)$, given a node $v_i \\in V$ within the graph $G$, the goal is to generate $p_{v_i}$ and $d_{v_i}$, the corresponding image and text at $v_i$, with a learned model $(p_{v_i}, d_{v_i}) = f(v_i, G)$.\nThis problem has numerous real-world applications. In the context of e-commerce, this translates to generating an image ($p_{v_i}$) and a title ($d_{v_i}$) for a product ($v_i$) based on a user's purchase history ($G$), framing it as a generative recommendation task. In the art domain, this involves generating an image ($p_{v_i}$) and a title ($d_{v_i}$) for an artwork ($v_i$) based on the associated artist's style or genre ($G$), positioning it as a virtual artwork creation task."}, {"title": "3. Methodology", "content": "In this section, we present our GRAPHGPT-O framework, a novel approach for generating image-text pairs on MMAGs using multimodal LLMs (MLLMs). We begin by introducing graph information into MLLMs in Section 3.1. Next, we describe a personalized PageRank-based graph sampling strategy in Section 3.2, addressing the Graph Size Explosion challenge. In Section 3.3, we propose graph linearization strategies and develop a hierarchical graph aligner to address the Non-Euclidean Nature of graphs and capture Hierarchical Modality Dependency in MMAGs. Finally, in Section 3.4, we explore different generation strategies to manage Inference Dependency across modalities."}, {"title": "3.1. Multimodal LLM on MMAGS", "content": "DreamLLM. The proposed GRAPHGPT-O is built upon DreamLLM [6], an MLLM capable of comprehension and generation on both text and image modalities. To be specific, DreamLLM represents both texts and images as tokens and conducts encoding and generation in an autore-"}, {"title": "3.2. Personalized PageRank Neighbor Sampling.", "content": "A simple approach to obtain $g_{v_i}$, is to encode the entire local subgraph of $v_i$ within $G$. However, this becomes impractical as the subgraph size grows exponentially with each additional hop, resulting in excessively long context sequences. Additionally, irrelevant or extraneous information in the local subgraph could misguide the model. To overcome this, inspired by [10], we utilize personalized PageRank (PPR) to selectively gather information for constructing $g_{v_i}$ from a graph structure perspective.\nTo be specific, PPR [12] utilizes the graph structure to produce a ranking score, $P_{i,j}$, for each node $v_j$ relative to a target node $v_i$. A higher score $P_{i,j}$ indicates a stronger \"similarity\" or relevance between nodes $v_i$ and $v_j$. We represent the PPR scores across all nodes with the PPR matrix $P \\in \\mathbb{R}^{n \\times n}$, where each row $P_{i:}$ corresponds to the PPR vector for node $v_i$. The PPR matrix $P$ is computed by solving the following equation:\n$P = \\beta \\hat{A}P + (1 - \\beta)I$."}, {"title": "3.3. Multimodal Graph as Sequence", "content": "After obtaining $N(v_i)$, the problem is how to extract meaningful graph representations from it. Given that GRAPHGPT-O takes sequential data as input, this involves tokenizing $N(v_i)$ into sequence $g_{v_i}$. Previous studies [23, 36, 41] have explored methods for inputting text-attributed graphs as sequences into LLMs, but handling multimodal attributed graphs presents greater complexity. In this section, we explore two ways to achieve this including (1) Linearization: simply linearizing the textual and image features in $N(v_i)$ into a sequence, and (2) Hierarchical Aligner: a hierarchical graph encoder to obtain deep representations as tokens for $N(v_i)$."}, {"title": "3.3.1 Graph Linear Tokenization", "content": "We first discuss tokenizing $N(v_i)$ with simple sequence linearization. This involves designing rules Linearize() to transform textual and image features in $N(v_i)$ into $g_{v_i}$:\n$g_{v_i} = \\text{Linearize}(N(v_i))$\nGiven that $N(v_i)$ is a set of nodes and each $v_j \\in N(v_i)$ is associated with both text information $d_{v_j}$ and image information $p_{v_j}$, the design of the linearization rule should consider three factors: (1) modality choice; (2) modality order and (3) number of neighbors, which are discussed as follows:\nModality choice. Depending on the graph, it is possible that presenting only texts ${d_{v_j}|V_j \\in N(v_i)}$ or only images ${d_{v_j}|v_j\\in N(v_i)}$ or both of them could benefit the multimodal content generation on MMAGs.\nModality order. Given that we have both text and image modality, it is flexible to adjust the order of different information, including (1) all images first, followed by texts, (2) all texts first, followed by images, and (3) interleaving image and text for each node $v_j \\in N(v_i)$.\nNumber of neighbors. $N(v_i)$ is a list of nodes ranked by PPR score. Including more neighbors $v_j \\in N(v_i)$ into $g_{v_i}$ could potentially add more information but at the same time increase noise."}, {"title": "3.3.2 Graph Hierarchical Tokenization", "content": "Although linearization offers a solution for graph tokenization, it fails to capture hierarchical modality dependencies in MMAGs. To be specific, at the node level, the combined information from associated text and image data contributes to a richer semantic representation of individual nodes. At the subgraph level, features synthesized from node-level semantics, alongside the local graph structure, enable a more comprehensive contextual understanding, thereby enhancing the generation of target nodes. To this end, we design a hierarchical aligner $F(\\cdot)$ with a node feature Q-Former $(\\cdot)$ and a graph structure Q-Former $(\\cdot)$ to capture the node-level and subgraph-level modality dependency respectively:\n$g_{v_i} = F(N(v_i)) = \\psi({\\phi(v_j)|v_j\\in N(v_i)})$.\nNode Feature Q-Former. It is proposed to learn node representations for $v_j \\in N(v_i)$ considering the node-level modality dependency. As shown in Figure 1, the Q-Former comprises two core Transformer [34] modules motivated by [20]: (1) a self-attention module that facilitates deep information exchange between node text features and image features; (2) a cross-attention module that compresses node feature into a fixed number of representations.\nThe associated text $d_{v_j}$ and image $p_{v_j}$ of a node $v_j \\in N(v_i)$ are first transformed into token representations $w_{v_j}$ and $I_{v_j}$ with text tokenizer and pretrained CLIP encoder respectively, which are then concatenated to form the initial input embedding:\n$H_j^{(0)} = [w_{v_j}; I_{v_j}] \\in \\mathbb{R}^{d \\times (|d_{v_j}|+|P_{v_j}|)}$\nThe self-attention Transformer layers are designed to perform text and image modality information exchange calculated by:\n$H_j^{(t)} = \\text{Trans}_\text{sat}(q, k, v = H_j^{(t-1)})$\nFollowing $L_1$ self-attention Transformer layers, a cross-attention Transformer layer is applied, extracting the core feature into a fixed number of representations:\n$H_{v_j} = \\text{Trans}_\text{cat}(q = Q_{v_j}, k, v = H_j^{(L_1)})$\nwhere $Q_{v_j} \\in \\mathbb{R}^{s}$ is a node-level information aggregation soft prompt. The final representation $H_{v_j}$ is leveraged as modality fused node feature representation."}, {"title": "Graph Structure Q-Former.", "content": "It is designed to aggregate the local context semantics inside $N(v_i)$, capturing the subgraph level modality dependency. Similar to node feature Q-Former, graph structure Q-Former also contains two core Transformer modules: (1) a self-attention module that enables deep information integration inside the local subgraph; (2) a cross-attention module that aggregates the local semantics into a fixed number of representations.\nThe node representations $H_{v_j}$ for $v_j \\in N(v_i)$ obtained from the node feature Q-Former are concatenated and serve as the initial inputs to the graph structure Q-Former:\n$G_{N(v_i)}^{(0)} = [H_{v_j} | V_j \\in N(v_i)]$\nThe self-attention Transformer layers are then applied to conduct deep information fusion between nodes inside the local subgraph:\n$G_{N(v_i)}^{(t)} = \\text{Trans}_\text{sat}(q, k, v = G_{N(v_i)}^{(t-1)})$\nAfter the $L_2$ self-attention Transformer layers, a cross-attention Transformer layer is designed to compress essential local graph features into a fixed set of representations:\n$G_{N(v_i)} = \\text{Trans}_\text{cat}(q = Q_G; k, v = G_{N(v_i)}^{(L_2)})$\nwhere $Q_G \\in \\mathbb{R}^{s'}$ is a subgraph-level information aggregation soft prompt. The final representation is leveraged as graph token representations which are inputted into the MLLM: $g_{v_i} = G_{N(v_i)}$."}, {"title": "3.4. Inference Strategy", "content": "Given the inherent interdependence between textual and visual features ($d_{v_i}$ and $p_{v_i}$) within a node $v_i$ and the joint objectives of generating both image and text, the order of inference across these modalities plays a crucial role. To this end, we propose two strategies: (1) sequential inference and (2) parallel inference.\nSequential Inference. The proposed framework employs a sequential dual-generation process, in which one modality is generated first and subsequently serves as a conditioning factor for the generation of the other modality. Specifically, this approach enables us to generate text $d_{v_i}$ by optimizing $p(d_{v_i}|g_{v_i})$ and then generate the corresponding image $p_{v_i}$ by maximizing $p(p_{v_i}|g_{v_i}, d_{v_i})$. Alternatively, we can initiate generation with the image $p_{v_i}$ by maximizing $p(p_{v_i}|g_{v_i})$ and then produce the text $d_{v_i}$ by optimizing $p(d_{v_i}|g_{v_i}, P_{v_i})$. This sequential conditioning strategy ensures that the second generation step is contextually anchored in the outcome of the first, potentially enhancing coherence and consistency across modalities.\nParallel Inference. The framework is designed to enable simultaneous dual generation of text and image by jointly optimizing $p(d_{v_i}|g_{v_i})$ and $p(p_{v_i}|g_{v_i})$. This concurrent generation approach allows the production of $d_{v_i}$ and $p_{v_i}$ to proceed independently, mitigating the risk of error propagation from one modality serving as a conditional input for the other. Consequently, this parallel optimization strategy can reduce dependency on sequential conditioning, enhancing robustness in the generation process."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental Setups", "content": "Datasets. We conduct experiments on three multimodal attributed graphs from distinct domains: ART500K, Amazon-Baby, and Amazon-Beauty. The ART500K dataset represents artworks, where nodes correspond to individual pieces, and edges indicate relationships such as shared authorship or genre. The Amazon datasets, comprising Amazon-Baby and Amazon-Beauty, represent product graphs. Here, nodes denote products, while edges capture co-view relationships. Each node in these graphs is enriched with a title and an image.\nMetrics. To thoroughly assess the comprehension and generation capabilities of GRAPHGPT-O on multimodal attributed graphs, our evaluation focuses on two key aspects:\n\u2022 The quality of the synthesized image and text, and how well they align.\n\u2022 The text/image correspondence between synthesized nodes and the conditioned sub-graphs.\nTo evaluate the quality of the synthesized outputs, we use CLIP (CLIP-I2) scores to compare the synthesized images with the ground truth images, assessing image generation quality. We also measure the perplexity of the generated text to evaluate its coherence. Additionally, we calculate the CLIP (CLIP-IT) score of generated image-text pairs to assess image-text alignment.\nTo evaluate alignment with the conditioned sub-graph, we calculate the KL divergence (KL-DV) between the distributions of the neighbor nodes and generated node image-text CLIP scores."}, {"title": "4.2. Graph Linear Tokenization", "content": "In this section, we study the quantitative results with graph linear tokenization, which are presented in Table 1, from which we observe the following:\n(1) Node Modality Integration. Utilizing both modalities together generally improves model performance, indicating that integrating multiple information sources leads to a more comprehensive understanding of the data.\n(2) Node Modality Order. The order in which the modalities are processed does not consistently or significantly affect model performance.\n(3) Inference Strategy. Generating the image first typically"}, {"title": "4.3. Graph Hierarchical Tokenization", "content": ""}, {"title": "4.3.1 Quantitative Evaluation.", "content": "In this section, we compare the results of the original DreamLLM [6] and Chameleon [33], and DreamLLM fine-tuned with graph linear tokenization prompts named GRAPHGPT-O (Hard), and trained with an additional hierarchical aligner module named GRAPHGPT-O (soft). The default prompt setting for training and inference utilizes both modalities, with text-first in the instruction and generating text-first during inference. The results are shown in Table 2, from which we can observe that GRAPHGPT-0 (soft) outperforms baselines in most cases, especially aligns better with the golden sub-graph."}, {"title": "4.3.2 Qualitative Evaluation.", "content": "We performed a qualitative evaluation by randomly selecting several generated cases, and comparing them with ground-truth, DreamLLM, and ChatGPT-40. The results are presented in Figure 4, which includes sampled neighbor images and text from the graph alongside the ground truth images and text. These findings show that GRAPHGPT-O generates images that align closer with the contextual information derived from the golden sub-graph, while Dream-LLM and ChatGPT- 4o stick to one style and fail to adapt based on the input."}, {"title": "4.4. Ablation Study", "content": "First, we evaluate the effectiveness of our hierarchical aligner module by individually removing the node feature Q-former and the graph structure Q-former. The results, presented in Table 3, demonstrate that both modules contribute significantly to overall performance. Removing the graph structure causes a substantial increase in the KL-DV score while excluding the node features results in a higher perplexity for text.\nTo further evaluate the effect of our hierarchical aligner module, a GNN module is used to replace it and the results are shown in Table 4, which shows that graph structure q-former is much better.\nWe then assess the impact of our Personalized PageRank sampling method. From Figure 3, it can be observed that our proposed Personalized PageRank sampling strategy effectively captures neighbors that contribute most to the ground truth in terms of texture, artistic style, and visual consistency. This results in a generated image that more closely resembles the ground-truth image's detailed patterns and overall aesthetic."}, {"title": "4.5. Other Studys", "content": "Study of generation with partial node feature guidance. We further conduct a study on the performance of GRAPHGPT-O with additional node text guidance or node image guidance. From Figure 4 we can see that the style and the character information is well-captured.\nStudy on the impact of number of neighbors. Figure 5 shows that incorporating information from more neighbors can improve performance, but an excessive number may introduce noise, potentially hindering results."}, {"title": "5. Related Work", "content": ""}, {"title": "5.1. Large Language Models on Graphs", "content": "Large Language Models (LLMs) have driven substantial progress in graph learning applications [16, 29]. Graph data, on the one hand, can be utilized directly to train LLMS [3, 39]. For instance, models like Heterformer [15] and Edgeformer [11] introduce graph-enhanced Transformer architectures, positioning them as foundational models for graph-based LLMs. GraphGPT [32] leverages graph structural data via graph instruction tuning, facilitating robust generalization across supervised and zero-shot graph learning tasks. LLaGA [4] employs a parameter-free GNN and incorporates the graph structure based on the order of node tokens. Similarly, InstructGraph [35] employs a structured format verbalizer to encode graph data, enhancing LLMs in tasks requiring graph reasoning and generation. GraphAdapter [13] incorporates GNNs as efficient adapters for LLMs, while GAugLLM [9] advances self-supervised learning through augmented node features generated by an MoE module, effectively bridging textual and graph structures. UniGLM[8] uses structure information to build positive sample pairs in contrastive learning framework to train a unified text encoder. On the other hand, graph data can be utilized as external knowledge in a plug-and-play manner with LLMs [18, 26]. For example, Graph Chain-of-Thought [18] proposes an iterative framework that enables LLMs to reason, interact, and operate effectively on graphs. GNN-RAG [26] introduces a retrieval-augmented generation framework [19], using a GNN retriever to extract knowledge from graph data. Despite these advances, existing research has primarily focused on graphs with textual attributes, leaving multimodal attributed graphs underexplored."}, {"title": "5.2. Multimodal Large Language Models", "content": "Multimodal Large Language Models (MLLMs) have advanced the field by enabling unified multimodal understanding and generation within a single autoregressive framework [37, 38]. In terms of multimodal comprehension, models like Flamingo [1] process visual data interleaved with text, utilizing a gated cross-attention layer to encode"}, {"title": "6. Conclusions", "content": "In this paper, we address the challenge of multimodal content generation on multimodal attributed graphs (MMAGs). To this end, we propose a graph-enhanced multimodal large language model, GRAPHGPT-O, designed with the following components: (1) A personalized PageRank-based sampling strategy to extract informative neighbors from the graph, effectively mitigating the challenge of graph size explosion; (2) A transformation mechanism that encodes graph information as sequences, employing either linearization or deep graph encoding with a hierarchical aligner, thereby addressing the non-Euclidean nature of graphs and hierarchical modality dependencies; (3) Dual inference modes supporting both sequential and parallel inference to alleviate inference dependency issues. We conduct comprehensive experiments on MMAGs within art and e-commerce domains, demonstrating the effectiveness of our approach against strong baseline methods. Future work includes extending MLLMs for discriminative tasks on MMAGs and capturing the complex heterogeneous relations between texts and images within these graphs."}, {"title": "7. Limitations", "content": "In our current approach, we treat the graph as homogeneous, simplifying all nodes and edges into a single type. However, real-world graphs often consist of multiple node and edge types, each with unique semantic meanings. Future research could address this limitation by extending GraphGPT-o to heterogeneous graphs, allowing for richer and more nuanced representations of complex structures."}, {"title": "8. Ethical Considerations", "content": "GraphGPT-o presents a new method for improving the structural understanding of MLLMs through graph-based alignment. This approach seeks to tackle current issues in MLLMs, such as the uncontrolled generation of unsuitable content and susceptibility to adversarial attacks. Although GraphGPT-o provides enhancements, it still depends on the MLLM foundation, making it subject to these inherent limitations. Ethical concerns, like the potential for misuse, unintended generation of inappropriate content, and exposure to adversarial manipulation, need careful attention when deploying GraphGPT-o in practical applications."}, {"title": "9. Experiment settings.", "content": "For training, we randomly sampled 40,000 nodes from each original dataset. For testing, we randomly selected 50 nodes and its related neighbors from the rest of the dataset.\nIn the implementation of GraphGPT-o, we utilize DreamLLM as the pre-trained backbone. Within the Graph Hierarchical Tokenization module, the learnable tokens, as well as all self-attention and cross-attention layers, are randomly initialized. We employ a pre-trained CLIP encoder as the fixed image and text encoder, with an additional MLP to resolve dimensional discrepancies."}, {"title": "10. More Experiment Results.", "content": "We demonstrate more cases generated by DreamLLM and GRAPHGPT-O with comparision with the ground truth."}]}