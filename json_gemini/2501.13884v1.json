{"title": "Exploring Finetuned Audio-LLM on Heart Murmur Features", "authors": ["Adrian Florea", "Nima Mesgarani", "Xilin Jiang", "Xiaofan Jiang"], "abstract": "Large language models (LLMs) for audio have excelled in recog- nizing and analyzing human speech, music, and environmental sounds. However, their potential for understanding other types of sounds, particularly biomedical sounds, remains largely underex- plored despite significant scientific interest. In this study, we focus on diagnosing cardiovascular diseases using phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN) paradigms are restricted to heart murmur classification (healthy vs unhealthy) and do not predict other acoustic features of the murmur such as timing, grading, harshness, pitch, and quality, which are important in helping physicians diagnose the underlying heart conditions. We propose to finetune an audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG) dataset and evaluate its performance in classifying 11 expert-labeled murmur features. Ad- ditionally, we aim to achieve more noise-robust and generalizable system by exploring a preprocessing segmentation algorithm using an audio representation model, SSAMBA. Our results indicate that the LLM-based model outperforms state-of-the-art methods in 8 of the 11 features and performs comparably in the remaining 3. More- over, the LLM successfully classifies long-tail murmur features with limited training data, a task that all previous methods have failed to classify. These findings underscore the potential of audio LLMs as assistants to human cardiologists in enhancing heart disease diagnosis.", "sections": [{"title": "INTRODUCTION", "content": "Cardiovascular diseases are the leading cause of death worldwide, claiming a life every 33 seconds in the USA [27]. Auscultation, performed with a stethoscope, is a key practice for examining the circulatory system, allowing physicians to detect abnormal heart and breathing functions by listening to blood flow and heart valve activity. A thorough cardiovascular exam is essential for identify- ing heart murmurs and associated diseases [30, 31]. Murmurs are important diagnostic observations, since many conditions present with characteristic murmurs. Distinctive features of phonocardio- gram (PCG) can be very helpful in narrowing a differential diag- nosis. Physicians describe murmurs by evaluating their timing in the cardiac cycle, intensity, location, duration, configuration, pitch, and quality [30, 31]. The careful auscultation allows a very close diagnosis, which is the basis for better care.\nRecent advancements in large language models (LLMs) for health- care [23, 26, 38, 42] enhance tasks like medical QA, record analysis, and licensing exam preparation. While some models [4, 42, 48] provide diagnostic insights directly to patients, they risk harm without medical oversight. Conversely, others works support the use of LLMs as supportive tools to physicians who can validate and safely implement the conclusions these models generate [3, 46]. In the medical domain, multimodal LLMs have been studied in time-series contexts such as electrocardiograms (ECG) [22], MRI [18], and radiology [33]. These assistive models support clinicians by synthesizing large volumes of information, generating detailed medical summaries, and aiding in clinical decision-making.\nMoreover, significant advances have been made in the develop- ment of audio LLMs [6, 20, 39], demonstrating remarkable achieve- ments in understanding diverse speech signals, complex acoustic and semantic reasoning, and performing speech signal analysis. However, to the best of our knowledge, audio LLMs have not yet been adapted for acoustic health tasks such as PCG analysis [17]. This is the first work that systemically assesses the potential of a state-of-the-art audio LLM as a physician tool for analyzing PCGs. We first leverage the unique repeatative and segmentable nature of PCGs and use a state-space audio representation model, SSAMBA [37], to segment the PCGs. We then employ Qwen2-Audio [5] for PCG encoding and medical reasoning. Our key contribution is fine- tuning and evaluating the audio LLM to predict a comprehensive set of 11 physiological PCG features, achieving performance that matches or surpasses traditional methods across all of them. Addi- tionally, the inclusion of a segmentation model enhances robustness, enabling our approach to perform effectively on unseen datasets where PCGs are collected using varied methods."}, {"title": "BACKGROUND", "content": ""}, {"title": "Heart Sounds", "content": "A normal cardiac cycle includes S1, the first heart sound, caused by the closure of the mitral and tricuspid valves at systole's start, and S2, the second heart sound, caused by the closure of the aortic and pulmonary valves between systole and diastole [2]. Additionally, ex- tra heart sounds, i. e., the third heart sound S3 and the fourth heart sound S4, can occur in both normal and pathological conditions [41]. Murmurs, caused by turbulent blood flow in the heart system, are identified as abnormal sounds, and are crucial for diagnosing cardiovascular diseases. Clinically, murmurs consist of two types: systolic murmurs and diastolic murmurs. Aortic stenosis, mitral re- gurgitation, and tricuspid regurgitation occur during systole, while mitral stenosis and tricuspid stenosis occur during diastole [29]."}, {"title": "Related Works", "content": "Classifying PCG signals using discriminative machine learning (ML) models has become standard practice, relying on features like time- domain characteristics (e.g., envelope, energy, amplitude) and spec- tral attributes to identify pathological conditions [21, 34, 41]. Other approaches delve into deeper representations, including graph- based features [40], autoencoder-derived features [16], and sparse coefficients [43]. Recently, end-to-end ML methods, such as 1D CNNs, TCNs, and GRU-RNNs, have emerged, operating directly on raw audio or mel spectrograms to eliminate the need for manual feature engineering [14, 45]. These models show significant poten- tial in detecting subtle cardiac anomalies, such as early indicators of heart failure [9], by learning relevant patterns directly from data. The complexity of feature selection and end-to-end architectures demands robust frameworks that can truly make use of such diversi- fied signal properties. Furthermore, ML models aim to achieve more noise-robust results by employing a segmentation preprocessing step that splits an entire PCG into smaller segments (S1,systole, S2, and diastole) [34]. Studies have show that segmentation of S1 and S2 phases improve classification performance [8] and reduce the need for labeled data [15].\nAnother important achievement in this regard is the use of trans- fer learning to overcome the limitation of small annotated datasets. Transfer learning involves finetuning models trained on a large generic dataset for specific tasks. In some studies [1, 19, 28, 35], pre- trained models are primarily learnt on an image dataset (ImageNet) [7] and an audio dataset (AudioSet) [10]. Finetuned models have even outperformed pretrained models as they adapt to the data distribution of heart sound datasets. Transfer learning is highly effective because finetuning pretrained models enables strong gen- eralization to new data. Consequently, transfer learning reduces the data scarcity problem and improves model accuracy, hence is a crucial approach for developing robust PCG classification systems [34].\nWhile most ML research in heart sound analysis has focused on binary classification of heart murmurs as either healthy or un- healthy, other murmur characteristics, such as timing, grading, shape, quality, and pitch, have received comparatively limited at- tention; evident by works that focus on these features for systolic murmurs [12, 44]. These characteristics are clinically significant as they provide deeper insights into the underlying pathophysiol- ogy of cardiac abnormalities. A notable study introduced models incorporating feature attention modules to tackle these nuanced classifications, achieving state-of-the-art performance [12]. Despite these advancements, research exploring these additional features \u2013 especially diastolic features \u2013 remains sparse, underscoring the need for more comprehensive investigations into the diverse at- tributes of heart murmurs."}, {"title": "METHODS", "content": ""}, {"title": "Models", "content": "Our proposed PCG multi-feature classification system effectively utilizes an Audio LLM finetuned for PCG for feature extraction and medical reasoning, along with a PCG segmentation front-end to separate heartbeats and non-heartbeats. The entire system is shown in Figure 1."}, {"title": "PCG Segmentation Model", "content": "Mamba state space models (SSMs) have demonstrated an exceptional capacity in capturing tempo- ral dependencies across local and global scales [11]. Building on this, Mamba-based audio encoding models like SSAMBA [37] fur- ther enhance temporal modeling by dividing audio spectrograms into patches and processing them in both forward and backward directions. This modeling approach, combined with the audio rep- resentations learned from the AudioSet [10], can be effectively adapted to PCG signals. To achieve this, we added a linear head on top of SSAMBA for segmenting heartbeats into occurrences and silences and finetuned the entire model using only one-third of the training set:"}, {"title": "Audio LLM Adapted for PCGs", "content": "The model architecture of Qwen2-Audio [5] contains an audio encoder based on the Whisper [32] speech recognition model and a large language model Qwen- 7B [47], which encompasses a total of 8.2B parameters. Given the paired data (a, x), where a and x denote the audio sequences and text sequences respectively, the training objective is to maximize the probability of the next text token. This is expressed as:\n$P_{\\theta}(x_t | x_{<t}, \\text{AudioEncoder}_{\\phi}(a)),$\nconditioning on the audio representations and the preceding text tokens $x_{<t}$. Here, $\\theta$ represents the trainable parameters of the LLM, and $\\phi$ represents the trainable parameters of the audio encoder. Qwen2-Audio is the state-of-the-art in various speech and audio understanding benchmarks like AIR-Bench, MMAU, and S2TT [5]. We finetuned Qwen2-Audio with full low-rank approxi- mation (LoRA) [13], which adds low-rank matrics (learnable) to the original weight matrics (frozen) in the audio encoder and the LLM."}, {"title": "Data", "content": "The PhysioNet CirCor DigiScope dataset is currently the largest collection of pediatric heart sound recordings [36]. It includes 5,282 recordings captured from the four primary auscultation sites across 1,568 patients, amounting to over 312 hours of heart sound data sampled at 4 kHz. The patient ages span from 0.1 to 356.1 months. Recording durations range from 4.8 to 80.4 seconds, averaging 22.9 seconds with a standard deviation of 7.4 seconds. Using a semi- supervised annotation process, experts meticulously annotated the dataset, detailing characteristics such as the timing, shape, pitch, grading, quality, and location (aortic, pulmonic, tricuspid, or mitral), and heart beat segmentation (systolic, diastolic, S1, and S2 timings) of each murmur. The dataset comprises 74% murmur-absent cases, 19% murmur-present cases, and 7% uncertain cases due to back- ground noise or interference."}, {"title": "Dataset", "content": "We stratify patients by class (Present/Absent/Unknown), resample to 16,000 kHz, and create a 75/25% split for training and testing. We selected 11 classification tasks based on the expert-annotated labels provided in the PhysioNet dataset. These tasks encompass a compre- hensive set of murmur characteristics, including both systolic and diastolic features. Specifically, they include murmur weighted accu- racy W.acc as defined in [36], systolic murmur attributes such as timing, shape, grading, pitch, and quality, as well as corresponding diastolic timing, shape, grading, pitch, and quality features. Each of these tasks reflects clinically significant dimensions of heart sound analysis, as identified by domain experts. In order to account for model overfitting on the question input, each question in training and testing set is selected randomly from three options with slight variations in how the question is phrased (prior to the presentation of the multiple choices).\nTo optimize model performance, we finetuned our framework using a multiple-choice (MC), single-answer approach. This design allows the model to focus on selecting the most appropriate label from a predefined set of options for each classification task, aligning with the structured nature of the annotated data. In some cases, the classification task contained less possible features than 6, so the MC distractor solutions were padded with additional instances of incorrect answers."}, {"title": "RESULTS", "content": "We evaluate our system's performance across 11 MC tasks on the PhysioNet CirCor DigiScope test split. The performance of our model without the frontend segmentation (N.S.) step is comparable to its performance with the frontend seg- mentation step (W.S), while both obtained optimal classification accuracy for timing, shape, pitch and quality. The classification for both systolic and diastolic grading features exhibits poor perfor- mance in classifying between the grading labels i/vi, ii/vi, and iii/vi. This is likely due to the model's limited understanding of Roman numerals, compounded by the fact that the text encoder was frozen during fine-tuning, restricting its ability to adapt to these specific labels. Though, the segmentation preprocessing step does improve diastolic grading performance above chance level.Diastolic phase features are long-tail murmur characteristics with limited representation in both the training and test datasets. Consequently, these features have not been extensively studied or incorporated into previous murmur models. We observe murmur classification increase with"}, {"title": "CONCLUSION", "content": "We demonstrate the potential of audio LLMs in advancing the analysis of phonocardiograms for cardiovascular disease diagnosis. By finetuning Qwen2-Audio on the PhysioNet CirCor DigiScope dataset, we show that the proposed system outperforms state-of- the-art models in most key murmur features. The incorporation of a segmentation preprocessing step enhanced the model's robustness and generalizability across diverse datasets. Our findings highlight the ability of audio LLMs to capture nuanced cardiac characteris- tics, offering valuable support for cardiologists in diagnosing heart conditions with greater precision. By expanding the set of tasks our system can handle in our benchmark, we aim to drive further innovation in using audio LLMs for comprehensive cardiac sound analysis, ultimately enhancing diagnostic tools in cardiovascular care. While LLMs can assist in medical data analysis, human cardi- ologists remain essential for clinical decision-making, as it requires expert judgment and patient interaction."}]}