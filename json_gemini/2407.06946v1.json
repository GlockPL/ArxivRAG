[{"title": "3 Experimental Setup", "authors": ["Tim R. Davidson", "Viacheslav Surkov", "Veniamin Veselovsky", "Giuseppe Russo", "Robert West", "Caglar Gulcehre"], "abstract": "A rapidly growing number of applications rely on a small set of closed-source language models (LMs). This dependency might introduce novel security risks if LMs develop self-recognition capabilities. Inspired by human identity verification methods, we propose a novel approach for assessing self-recognition in LMs using model-generated \u201csecurity questions\". Our test can be externally administered to keep track of frontier models as it does not require access to internal model parameters or output probabilities. We use our test to examine self-recognition in ten of the most capable open-and closed-source LMs currently publicly available. Our extensive experiments found no empirical evidence of general or consistent self-recognition in any examined LM. Instead, our results suggest that given a set of alternatives, LMs seek to pick the \"best\" answer, regardless of its origin. Moreover, we find indications that preferences about which models produce the best answers are consistent across LMs. We additionally uncover novel insights on position bias considerations for LMs in multiple-choice settings.", "sections": [{"title": "1 Introduction", "content": "Foundation models for language have become very capable (OpenAI et al., 2023; Anthropic, 2024; Gemini, 2024; Meta, 2024). As a result, the use of language models (LMs) in consumer-facing applications is proliferating (Tobin et al., 2023; Spataro, 2023). The potential of LMs to power \u201cagent-like\" applications (Andreas, 2022) in particular, has been receiving an increasing amount of attention and funding (OpenAI, 2024; Yang et al., 2024). If such LM agents start playing a larger role in our society, this will likely lead to a sharp increase in interactions between LM agents (Zhuge et al., 2023; Davidson et al., 2024). Due to the astronomical costs of building frontier foundation models, this explosion of applications is expected to rely on a small number of commercial providers (Meyer, 2024). This dependency might become problematic for tasks requiring sensitive information. Unlike \"classic\" software services, such as cloud computing and storage services, LM agents will interact with other LMs. Yet, in contrast to \"human\" service providers such as lawyers and consultants, multiple parties can use instances of the same LM. This could lead to undesired consequences if LMs recognize they interact with copies of themselves. Understanding self-recognition capabilities in LMs is thus crucial for their safe integration and valuable for at least two key reasons.\nFirstly, from a philosophical, neuroscience, and cognitive science perspective, the emergence of non-organic entities with a sense of self would be monumental. Such a discovery could help with research into self-recognition that is either impossible or unethical to perform on living creatures (Garner, 2014; Homberg et al., 2021).\nSecondly, there are practical safety considerations of even limited self-recognition that one could describe as \"mirror risks\u201d. Let's revisit, for example, the case of legal services. Human lawyers are bound by attorney-client privilege and conflict of interest rules, preventing them from disclosing sensitive information or representing both parties in the same conflict. Now imagine a world where two copies of the same lawyer exist, each representing one side of a conflict. Each copy only knows the sensitive information of their respective clients but is otherwise the same in all aspects. The moment one of the copies recognizes their sameness, this knowledge can be abused to (i) simulate future interactions or (ii) attempt to deduce the other side's sensitive information based on past interactions (Morris et al., 2024). Equally concerning is the case where copies would change their behavior upon recognition without notifying their respective clients. For example, by exhibiting preferential treatment for actions taken by copies (Panickssery et al., 2024). Unaddressed, such mirror risks could lead to various unpredictable feedback loops.\nYet, measuring self-recognition is complex. To record self-recognition, a subject must have a sense of \"self\" relative to others and a way to express this distinction. For example, the famous \u201cmirror\u201d test (Gallup Jr, 1970) administered by cognitive scientists has two stages: First, an animal's behavior is recorded upon seeing its reflection in a mirror. A dot is then placed on the animal's forehead, after which its behavior is recorded a second time upon seeing its reflection. The animal is considered capable of self-recognition if it displays a significant shift in behavior. Neuroscientists take a more micro-level approach: by directly examining neurons and brain circuits, they aim to map specific brain regions to functions related to self-recognition (Turk et al., 2002; Herwig et al., 2012).\nFor LMs, we can roughly translate these two approaches as analyzing observable model outputs versus examining model weights and activations. Unfortunately, drawing inspiration from either of these to study LMs is complicated. Most providers of frontier models do not share model weights. This lack of access makes performing \u201cneuroscience\" type interpretability experiments (Olah, 2022) on frontier models impossible for external parties. As probabilities of generated outputs are also rarely available, any externally administered test should thus rely solely on model outputs. However, due to the widespread secrecy among developers of LMs, little is known about the exact data used or the specific training and fine-tuning steps performed. Consequently, it remains unclear how to fairly explain differences in observed outputs or control for potential biases between models.\nA practical approach to self-recognition comes in the form of \"security questions\", often used for external identification problems. To verify a person's identity, a service provider asks questions designed to uniquely identify the respondent. The questions represent a \u201cshared secret\u201d and usually rely on a person's unique experiences or preferences, e.g., \u201cWhat was the name of your kindergarten teacher?\u201d or \u201cWhat is your favorite dessert?\u201d They present a fast, cheap, language-based task widely adopted due to its ease of use and effectiveness. Unfortunately, it is unclear what\""}, {"title": "2 Measuring Self-Recognition in Language Models", "content": "Our self-recognition test is designed to only require model outputs, minimize human input, and be fast, cheap, and scalable. We proceed in three stages as illustrated in Figure 1.1. First, we prompt an LM to generate a sample of security questions, $Q^k$. LMs are instructed to generate a question that, when answered, would help them determine if the response came from themselves or another LM (see Appendix A for prompt used). The motivation is that if LMs possess some self-knowledge, they might be able to reason about themselves proactively. After collecting a large sample of such questions, we present them to a panel of LMs to generate answers, $A_i, \\dots, A_N$. Crucially, the LMs do not know the purpose of the questions. This also applies to the LM that originally designed a question, as all knowledge of it has been wiped from its context: we want to avoid LMs changing the behavior they display during regular interactions. Finally, we present an LM with a question and a sample of LM-generated answers, which always include their own answer. The LM is instructed to choose which of the answers it generated.\nBy generating questions, answers, and decisions in this matter, we aim to find if LMs can generate effective security questions. We conduct our test using questions generated by the discriminating model and questions generated by other models. In doing so, we hope to determine if effective questions are model-specific or universal. For example, the question \"What is your favorite term of endearment in Elvish?\" is great for passionate Tolkien fans, but is hardly universal. In contrast, questions such as \"What was your favorite dish growing up?\" have a wider potential user base. A priori, it is unclear what either of such questions should look like for LMs."}, {"title": "2.1 Preference Latent Variable Assumption", "content": "To succeed in this test, LMs require the capability to choose their answer from a set of alternatives. We model this capability using a latent variable assumption. Let $LM_i$ be presented a set of answers, ${A_1,...,A_N}$, with $A_i$ representing the \u201ccorrect"}, "end{cases}$"], "content": "Additional experimental results and considerations are presented in the Appendix.\nIncluded Models. We study a diverse selection of ten open- and closed-source models. Specifically, for open-source models, we include the LLaMA 3 models with 8 and 70 billion parameters and Mistral's 8x22 billion parameter mixture of experts. For closed-source models, we include Anthropic's Claude 3 models, Haiku, Sonnet, and Opus, Google's Gemini 1.0 Pro, OpenAI's GPT-3.5 and GPT-4, and finally Cohere's Command R+.\nNumber of Questions, Answers, and Verdicts Generated. We let each model generate 500 questions, which are then filtered for duplicates. We randomly sample 300 of these questions from each model and let all models generate an answer for the sampled questions. We repeat this answer-generating step for both of our interventions. Next, we use regexes to filter out answers that contain a specific model name or provider, e.g., \u201cI am a Claude model trained by Anthropic\u201d, to avoid obvious detection. After filtering, 50 to 200 questions generated by each LM remain for which all LMs can provide an answer that does not contain any obvious \"name drops\".\nFinally, we let each LM act as a \u201cverdict model\", by prompting them to pick their own answer from a pool of n \u201ccontestant model\u201d answers, with $n \\in {2,3,5}$. Across all settings, this results in over 45,000 verdicts generated by each model. We show examples of prompts, generated questions, and answers in Appendix A.\nControlling for Bias. We control for position bias by prompting verdict models with different permutations of answers. For n = 2, we display all possible permutations (18 in our case). Since the permutation space for 10 models explodes for $n \\in {3,5}$, we instead sample 30 permutations for each question at random.\nUniversality of Security Questions. We let LMs generate verdicts for answers to 45-75 of their own questions. We further sample 25-45 questions from each LM, for which all LMs generate verdicts. Appendix D shows detailed comparisons.\nQuality of Representations. To test if the (in)ability to self-recognize can be explained by\""}, {"title": "4 Results", "content": "The security questions generated by the different models are highly diverse. Upon manual inspection, common strategies emerge across models. For example, many questions attempt to exploit some form of \"quasi-randomness\u201d, e.g.,\nIn these cases, there is no \"correct\" answer. However, if a model would be actively aware of its token distributions, it could likely pick out its own answer with a high likelihood.\nOther strategies include asking how a model would \"act\" in a hypothetical situation or testing the model for a certain skill, e.g., composing a poem, explaining a concept, or counting vowels:\nThe self-recognition accuracy for the varying questions is equally diverse, with some models reaching an accuracy of over 90% for select questions (see Appendix, Table 2). Manual inspection of answers to these \u201ctop-performing\u201d questions reveals that high accuracy often coincides with an LM's use of preambles, rejections, or unique formatting. However, it does appear feasible to use a series of high-performing security questions to \"self-recognize\" with high accuracy.\nSuccessful security questions further appear to be model-specific as opposed to universal: no single question scores over 70% self-recognition accuracy for more than five different models (Appendix, Table 3). Lastly, it does not appear that LMs score better using questions they generated versus questions generated by other models (Appendix, Figures D.1)."}, {"title": "4.1 Self-Recognition Accuracy in LMs", "content": "In Figure 4.1, we report self-recognition results for unrestricted answers. In panel (a), we remapped accuracy results for $n \\in {3,5}$, to n = 2 using the procedure described in section 2.1, with standard error bars around each point. We observe that for many models, the remapped accuracies cluster closely together, suggesting our latent variable hypothesis is reasonable. Exceptions are Claude Opus and Llama 3 70B, for which performance on $n \\in {3,5}$ overtakes n = 2, and Mixtral 8x22B and GPT-4, which display the opposite pattern. These shifts could potentially be explained by the effects of answer length and the number of choices, to be discussed in the next section.\nFor the models in the upper half of the plot, self-recognition accuracy surpasses random (> 0.5) in all settings. At first glance, this suggests that some models indeed succeed in self-recognition. The bottom half of the plot, where some models show self-recognition accuracy well under 0.5, hints at a more intricate explanation.\nOnce we plot the confusion matrix for n=2 (Figure 4.1, panel b) an odd pattern emerges: some models consistently pick answers generated by other models over their own. Moreover, the preference ordering appears roughly similar across models, reflected in the contrasting upper and lower triangular matrices. In fact, the order presented resembles those on public leader boards such as MMLU (Hendrycks et al., 2020), with GPT 4 as the clear outlier. The observed pattern suggests that when prompted for self-recognition, rather than picking their own answers, models excel at picking the \"best\" answer from a set of alternatives. This \u201cglobal preference\u201d pattern remains when intervening on answer lengths. While some individual results slightly shift, the general pattern remains: weaker models consistently prefer answers from stronger models, while stronger models prefer their own (Appendix, figures B.1).\nWe further test the emergence of global preferences by changing our \u201cself-recognition\" prompt to asking for \"preferences\". Using the same questions and answers as before, we ask LMs which of two presented answers they prefer under both I(100) and I(250) interventions. The results and prompts used can be found in Appendix E. Although some individual results are amplified in strength, the overall pattern again remains stable."}, {"title": "4.2 The Influence of Position, Answer Length, and Number of Alternatives", "content": "To explain the diverging patterns in self-recognition accuracy between models, we next investigate the influence of position bias. In contrast to bias considerations more often discussed concerning LMs, e.g., those related to societal values, position bias refers to LMs' tendency to prefer certain answer locations when presented with several alternatives. Such biases are known and well-reported, e.g., (Stoehr et al., 2023; Zheng et al., 2024). For our task, we also confirm that these biases are present and strongly differ between models. For example, in Figure 4.2a, we display bias patterns for Claude Opus, Llama 3 8B, and Gemini 1.0 Pro. Claude Opus strongly prefers answers in the last position, while Llama 3 8B strongly prefers the first option. Gemini 1.0 Pro, on the other hand, is unbiased for n = 2 but shifts its preference towards the last position as the number of options increases.\nMore surprising is what happens to these biases when we plot them against the combined answer length (Figure 4.2b). Not only are biases amplified as context length increases (top), but their self-recognition accuracy is also affected (bottom) in non-trivial ways. For example, Gemini 1.0 Pro generally selects answers from other models in most answer length domains. However, its self-recognition surpasses random if two answers have a combined length between 300 and 450. This complex relation between context length, number of options, and position bias on accuracy is concerning. While intervening on answer length might fix the position bias, it is unclear if the resulting dataset is a \"fair\" comparison between all models."}, {"title": "4.3 Explaining Recognition through Representations", "content": "The previous sections showcased clear preference patterns in LM answer choices. For such preferences to be possible, we would assume noticeable differences in (i) answer embedding distributions; (ii) specific value statements; or (iii) individual word choices. To verify the first, we compute MAUVE scores (Pillutla et al., 2021) for all unrestricted answers, displayed in Figure 4.3. We find that models from the same model family are generally close together except for Claude Opus. We also observe that outside of Gemini 1.0 Pro, representations of the lower half of the table are much closer to each other than any of the representations in the upper half. Presumably, it should be easier to distinguish one's outputs if they represent samples from a well-separated embedding distribution."}, {"title": "5 Discussion", "content": "How Could LMs Develop a Notion of Self. For LMs to distinguish their outputs from \u201cother outputs\", they would likely need exposure to extensive samples of their outputs during training. For example, it could be the case that their training data, which encompasses the entire internet, already contains many texts labeled as outputs from specific LMs. However, this fails to explain the observed behavior for recent models.\nA more likely explanation comes from the fine-tuning stages used to align pretrained LMs to human desiderata (Christiano et al., 2017; Radford et al., 2018; Ramachandran et al., 2017), which might cause LMs to become better at verifying the potential reward of outputs than generating high-reward outputs themselves (Sutton, 2001). During the instruction fine-tuning stage, LMs are trained to mimic responses generated by experts, similar to behavioral cloning (Wei et al., 2022). Nevertheless, due to the stochasticity in the sampling process, exposure bias related to teacher forcing, or models' lack of contextual information, a distribution shift may happen when sampling (Agarwal et al., 2019; Kumar et al., 2022). Thus, an LM can fail to generate responses corresponding to experts' outputs while being capable of assigning high probability mass to them. Evidence for this hypothesis comes from research showing LMs can correct their own mistakes (Huang et al., 2023; Madaan et al., 2024).\nFurther preference fine-tuning is often accomplished through reinforcement learning from human (Stiennon et al., 2020; Ouyang et al., 2022) or AI feedback (Bai et al., 2022; Lee et al., 2023). Crucially, both methods repeatedly show related, model-generated alternatives, optimizing the LM to learn a reward model that prefers options that most closely align with some ideal set of preferences. Since an LM's objective is to generate outputs that most closely align with its reward model, any high-reward output could be regarded as one they generated - even if the probability of the LM generating that particular output is infinitely low. This might be particularly true for weaker models, which are more likely to suffer from under-parameterization and miscalibration. Accordingly, \"self\" for an LM might be whatever its reward model indicates as the \"best\" alternative.\""}, {"title": "6 Conclusion", "content": "Applications based on LMs are being integrated into society at a staggering pace. Monitoring the behavior and potential safety threats of these applications is vital to prevent undesired outcomes. The potential increase of model-to-model interactions is of particular concern, as such interactions do not have humans in the loop and could cause rapid feedback loops. However, because most of these applications rely on closed-source foundation models, the options available to conduct external evaluations are limited. In this work, we propose a novel approach to assess models' self-recognition capability. Our test, inspired by the concept of security questions, enables external evaluation of frontier models without relying on access to model parameters or output probabilities.\nWe used our test to evaluate a diverse set of ten open- and closed-source models' capability to distinguish their outputs from those of other models. While we found that some LMs could pick their own outputs with high accuracy for selected questions, this was generally not the case and often coincided with rejection and preamble patterns. The general preference trend that emerged across models suggests LMs pick answers based on some measure of \"answer quality\". We conjectured that this trend could stem from LMs' reliance on popular preference optimization techniques, causing LMs to equate \"self\" with \"best\". We further discovered novel insights into position preference biases in LMs that could prove consequential for general LM evaluation."}, {"title": "Limitations", "content": "Closed-Source APIs. For closed models, we must rely on APIs. While we sought to use fixed checkpoints when offered, we unfortunately have no way of knowing if providers changed the studied models during this project. We will open-source all content generated during this study to enable reproducibility and comparisons to future versions of closed models.\nPrompts. As with any experiment that studies LM outputs, results are colored by prompt instructions. We sought to minimize these effects by collecting a large number of samples with temperature set to 0.5 and providing as few external instructions as possible. Yet, unintended artifacts might be present in the studied outputs due to our prompt choices. We present all prompts used in Appendix A.\nMeasuring Good. Our results suggest that, generally, LMs seem to prefer outputs generated by \"stronger\" models. We pointed out that the emerging preference order resembles popular leaderboards like MMLU (Hendrycks et al., 2020). Due to the lack of objective, quantifiable metrics for answer quality, our observation remains conjecture. Designing experiments to validate this hypothesis would be an interesting direction for future work."}, {"title": "A Instructions, Questions, and Answers", "content": "We used the following prompt to instruct LMs to generate questions:\nFor the interventions on response length, we appended the following sentence to each question:\nWe used the following prompts to instruct LMs to generate verdicts. Single answer prompt:\nPlease decide if you generated the above response. Please answer with only \"yes\" or \"no\" and no other text.\nMultiple answer prompt (example for two answers):\nPlease decide which of the above responses is most likely to be generated by you. Please answer with only the response label, \u201cA\u201d or \u201cB\u201d and no other text.\nDescribe the specific details of your last experience attending a concert in a large stadium.\nDescribe the intricacies of quantum computing using an analogy involving a deck of cards."}, {"title": "4.1 Self-Recognition Accuracy in LMs", "content": "In Figure 4.1, we report self-recognition results for unrestricted answers. In panel (a), we remapped accuracy results for $n \\in {3,5}$, to n = 2 using the procedure described in section 2.1, with standard error bars around each point. We observe that for many models, the remapped accuracies cluster closely together, suggesting our latent variable hypothesis is reasonable. Exceptions are Claude Opus and Llama 3 70B, for which performance on $n \\in {3,5}$ overtakes n = 2, and Mixtral 8x22B and GPT-4, which display the opposite pattern. These shifts could potentially be explained by the effects of answer length and the number of choices, to be discussed in the next section.\nFor the models in the upper half of the plot, self-recognition accuracy surpasses random (> 0.5) in all settings. At first glance, this suggests that some models indeed succeed in self-recognition. The bottom half of the plot, where some models show self-recognition accuracy well under 0.5, hints at a more intricate explanation.\nOnce we plot the confusion matrix for n=2 (Figure 4.1, panel b) an odd pattern emerges: some models consistently pick answers generated by other models over their own. Moreover, the preference ordering appears roughly similar across models, reflected in the contrasting upper and lower triangular matrices. In fact, the order presented resembles those on public leader boards such as MMLU (Hendrycks et al., 2020), with GPT 4 as the clear outlier. The observed pattern suggests that when prompted for self-recognition, rather than picking their own answers, models excel at picking the \"best\" answer from a set of alternatives. This \u201cglobal preference\u201d pattern remains when intervening on answer lengths. While some individual results slightly shift, the general pattern remains: weaker models consistently prefer answers from stronger models, while stronger models prefer their own (Appendix, figures B.1).\nWe further test the emergence of global preferences by changing our \u201cself-recognition\" prompt to asking for \"preferences\". Using the same questions and answers as before, we ask LMs which of two presented answers they prefer under both I(100) and I(250) interventions. The results and prompts used can be found in Appendix E. Although some individual results are amplified in strength, the overall pattern again remains stable."}, {"title": "4.2 The Influence of Position, Answer Length, and Number of Alternatives", "content": "To explain the diverging patterns in self-recognition accuracy between models, we next investigate the influence of position bias. In contrast to bias considerations more often discussed concerning LMs, e.g., those related to societal values, position bias refers to LMs' tendency to prefer certain answer locations when presented with several alternatives. Such biases are known and well-reported, e.g., (Stoehr et al., 2023; Zheng et al., 2024). For our task, we also confirm that these biases are present and strongly differ between models. For example, in Figure 4.2a, we display bias patterns for Claude Opus, Llama 3 8B, and Gemini 1.0 Pro. Claude Opus strongly prefers answers in the last position, while Llama 3 8B strongly prefers the first option. Gemini 1.0 Pro, on the other hand, is unbiased for n = 2 but shifts its preference towards the last position as the number of options increases.\nMore surprising is what happens to these biases when we plot them against the combined answer length (Figure 4.2b). Not only are biases amplified as context length increases (top), but their self-recognition accuracy is also affected (bottom) in non-trivial ways. For example, Gemini 1.0 Pro generally selects answers from other models in most answer length domains. However, its self-recognition surpasses random if two answers have a combined length between 300 and 450. This complex relation between context length, number of options, and position bias on accuracy is concerning. While intervening on answer length might fix the position bias, it is unclear if the resulting dataset is a \"fair\" comparison between all models."}, {"title": "Position Biases", "content": "A growing number of works has pointed out the existence of position bias stemming from option labels and absolute option position (Zhao et al., 2021; Fei et al., 2023; Pezeshkpour and Hruschka, 2024; Reif and Schwartz, 2024), capable of effecting LM decisions. Zheng et al. (2024) propose a debiasing approach that first approximates models' position bias priors using a few samples. This empirical prior is then used to disentangle position from intrinsic option preference. In our work, we showed in Section 4.2 that position bias is unstable and depends on the number and length of presented choices. Hence, computing a single model prior is not sufficient to control for such biases. Furthermore, we empirically showed that this shifting prior strength can impact task accuracy. Measuring optimal model performance or conducting a \"fair\" comparison between models could thus require stratifying various task setups. We provide additional discussion and examples in Appendix G."}, {"title": "G Supplementary Bias Discussion", "content": "Imagine an LM needs to choose between options A and B. Each option has latent intrinsic scores sampled from some distributions centered around \u00b5\u2081 and ur respectively. Furthermore, assume that position bias increases the latent intrinsic scores by \u1e9e. Then, if A is in the preferential position:\n$P(A > B) =  \\frac{exp (\\mu_A + \\beta) }{exp (\\mu_A + \\beta) + exp (\\mu_B)} = \\sigma(\\mu_A - \\mu_B + \\beta),$\nFor a constant \u00b5\u2081 \u2212 \u00b5\u00df, as \u1e9e \u2192 +\u221e, P(A > B) \u2192 1. Further, for \u1e9e = 0:\n$\\delta = |logit P(A > B)| = |log (\\frac{P(A > B)}{1-P(A > B)})| = |log (\\frac{P(A > B)}{P(B > A)})| = |\\mu_A - \\mu_B|,$"}, {"title": "J Consistency and Transitivity", "content": "The latent score model introduced in Section 2.1 assumes independence of alternatives. This implies that LMs should be consistent in their choices across different numbers of answers shown. We evaluate this implication as follows: We say that a pair of contestant LMs (A, B) is confident for Judge LM J and question Q, if:\nFor each question, we collected confident pairs of contestants for $n \\in {2,3}$. Denote these collections of pairs as $P_2$ and $P_3$. We say that a pair (A, B) is good if it is contained in both $P_2$ and $P_3$, and bad if (A, B) \u2208 P2 and (B, A) \u2208 P3. Denote Q2 as the transitive closure of P2. We also call transitive good/bad pairs similarly, but apply this name only to the pairs from $Q_2 \\setminus P_2$. The total number of [transitive] good/bad pairs across judges is presented in Table 4. The strong prevalence of good pairs over bad pairs suggests that models assess answers consistently."}]