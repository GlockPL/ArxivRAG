{"title": "SOWing Information: Cultivating Contextual Coherence with MLLMs in Image Generation", "authors": ["Yuhan Pei", "Ruoyu Wang", "Yongqi Yang", "Ye Zhu", "Olga Russakovsky", "Yu Wu"], "abstract": "Originating from the diffusion phenomenon in physics, which describes the random movement and collisions of particles, diffusion generative models simulate a random walk in the data space along the denoising trajectory. This allows information to diffuse across regions, yielding harmonious out- comes. However, the chaotic and disordered nature of information diffusion in diffusion models often results in undesired interference between image regions, causing degraded detail preservation and contextual inconsistency. In this work, we address these challenges by reframing disordered diffusion as a powerful tool for text- vision-to-image generation (TV2I) tasks, achieving pixel-level condition fidelity while maintaining visual and semantic coherence throughout the image. We first introduce Cyclic One-Way Diffu- sion (COW), which provides an efficient unidirectional diffusion framework for precise information transfer while minimizing disruptive interference. Building on COW, we further propose Selective One-Way Diffusion (SOW), which utilizes Multimodal Large Language Models (MLLMs) to clarify the semantic and spatial relationships within the image. Based on these insights, SOW combines attention mechanisms to dynamically regulate the direction and intensity of diffusion according to contextual relationships. Extensive experiments demonstrate the untapped potential of controlled information diffusion, offering a path to more adaptive and versatile generative models in a learning-free manner.", "sections": [{"title": "I. INTRODUCTION", "content": "In physics, the diffusion phenomenon describes the move- ment of particles from an area of higher concentration to a lower concentration area till an equilibrium is reached [1]. It represents a stochastic random walk of molecules to explore the space, from which originates the diffusion generative models [2]. Similar to the physical diffusion process, it is widely acknowledged that the diffusion generative model in machine learning also stimulates a random walk in the data space [3]\u2013[5], however, it is less obvious how the diffusion models stimulate the information diffusion for real-world data along its walking trajectory. In this work, we start by investigating the diffusion phenomenon in diffusion models for image synthesis, namely \"diffusion in diffusion\", during which the pixels within a single image exchange and interact with each other, ultimately achieving a harmonious state in the data space (see Sec. III-B).\nDespite their success, diffusion models face inherent lim- itations that can hinder their effectiveness in practical appli- cations. One significant challenge is unwanted interference. For instance, tasks like image inpainting can be viewed as strictly unidirectional information diffusion, where information propagates from a known region to an unknown region while preserving the pixel-level integrity of the known content. However, uncontrolled diffusion in this scenario can lead to the intrusion of information from unknown regions into known regions, potentially disrupting the known content and introducing artifacts in the inpainted image. Additionally, in physical diffusion, particles move randomly without specific destinations. Similarly, generative models propagate infor- mation across images without fully understanding the roles and relationships of different regions. This indiscriminate information transfer can result in incorrect information leakage and insufficient guidance, causing suboptimal information distribution. Consequently, current generative models may produce outputs that are visually fragmented and semantically incoherent (see Fig. 1). Furthermore, they may also encounter issues such as catastrophic neglect, attribute misalignment, and attribute leakage [6]\u2013[8]. This underscores the critical demand for more sophisticated generative models that can more adeptly grasp and regulate the intricate dynamics between different components in images.\nIn this work, we focus on a multimodal generation task setting to synthesize images that successfully incorporate both semantic-Text and pixel-Vision conditioning (TV2I), allowing users to create more customized images. Most existing methods [9]\u2013[12] condition on text-vision input by brute-force learning, where they incorporate the visual condition into pre- trained T2I models by introducing an additional finetuning process to minimize the reconstruction error. Despite their abilities to capture the high-level semantics of extra visual conditioning, these methods often struggle with retaining low- level visual details in pixels as illustrated in Fig. 1. Additionally, these tuning-based methods introduce additional learning costs dependent on the pre-trained model and hinder the original distribution modeling ability of the base model. Furthermore, in the TV2I task, mismatches between modalities can further hinder the model from accurately generating complex scenes due to the semantic gap between text and images and the complexity of multimodal data. To this end, the ability to control the direction of information diffusion opens up the potential for a new branch of methodological paradigms to achieve versatile customization applications without the need to change the parameters of existing pre-trained diffusion models or learn any auxiliary neural networks."}, {"title": "II. RELATED WORK", "content": "Diffusion Models. The recent diffusion generative methods [3], [15]\u2013[20] originate from the non-equilibrium statistical physics [2], which simulate the physical diffusion process by iteratively destroying the data structure through forward diffusion and restore it by a reverse annealed sampling process. DDPM [21] shows the connection between stochastic gradient Langevin dynamics and denoising diffusion probabilistic mod- els. DDIM [15] generalizes the Markovian diffusion process to a non-Markovian diffusion process, and rewrites DDPM in an ODE form, introducing a method to inverse raw data into latent space with low information loss. In our work, we utilize DDIM inversion to access the latent space and find an information diffusion phenomenon during the sampling process towards the higher concentration data manifold.\nGenerative Dynamics and Regimes. Recent theoretical research provides insights into the dynamic behaviors of diffusion generative models, marking a promising direction in this field. Raya et al. [22] highlight how spontaneous symmetry breaking can enhance sample diversity. Additionally,"}, {"title": "III. GENERATIVE DYNAMICS IN DIFFUSION MODELS", "content": "In this section, we present the preliminary knowledge of diffusion models, followed by an in-depth investigation into their generative dynamics and regimes from a fresh perspective.\nPreliminaries\nDenoising Diffusion Probabilistic Models (DDPMs) [21] define a Markov chain to model the stochastic random walk between the noisy Gaussian space and the data space, with the diffusion direction written as,\nq(x_t|x_{t-1}) = N(\\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI), (1)\nwhere t represents diffusion step, {\\beta_t}f are usually scheduled variance, and N represents Gaussian distribution. Then, a special property brought by Eq. 1 is that:\nq(x_t/x_{t-k}) = N(\\sqrt{a_t/a_{t-k}}x_{t-k}, \\sqrt{1 - a_t/a_{t-k}}I), (2)\nwhere a_t = \\Pi_{i=0}^{t} (1 - \\beta_i). So we can bring xt to any xt+k in a one-step non-Markov way in our proposed cyclic one-way diffusion (Sec. IV-B) by adding certain random Gaussian noise. DDIM [15] generalizes DDPM to a non-Markovian diffusion process and connects the sampling process to the neural ODE:\ndx(t) = (f(x(t)) - \\sigma^2(t))dt. (3)\nBy solving this ODE using Euler integration, we can inverse the real image xo (the visual condition) to xt in any corresponding latent space eft) [59]\u2013[61] while preserving its information. The symbols \\sigma and x are the reparameterizations of (\\sqrt{1 - \\alpha/\\alpha}) and (x/\\sqrt{\\alpha}) respectively.\nDiffusion in Diffusion\nInternal Interference in Diffusion Generation. Diffusion in physics is a phenomenon caused by random movements and collisions between particles. The diffusion model, drawing inspiration from non-equilibrium thermodynamics, establishes a Markov chain between a target data distribution and the Gaussian distribution. Subsequently, it learns to reverse this diffusion process, thereby constructing the desired data samples from the Gaussian noise. This inherently simulates a gradual,"}, {"title": "IV. SELECTIVE ONE-WAY DIFFUSION", "content": "In this section, we introduce our proposed method Selective One-Way Diffusion for the TV2I task. As illustrated in Fig. 3, We first introduce MLLM-driven stages that intensify prompts and provide conditions for specific regions (Sec. IV-A) to address mismatches between the two modalities and enhance control over the generative model. Subsequently, the out- puts namely the conditional region, condition-related region,\nEnhancing TV2I Generation with MLLM-driven Stages\nIn the TV2I task, the model faces the significant challenge of understanding the semantic information in the textual description and translating this information into visually specific details. To refine the subsequent generative process and address the complex context-driven challenges, we introduce a series of stages leveraging Multimodal Large Language Models (MLLMs). In this work, we exploit Gemini [56], a multi-modal language model that accommodates various data formats like images, text, and audio. By harnessing Gemini's advanced contextual understanding and reasoning, we aim to enhance the alignment between text and visual modalities, optimize spatial layouts, and ensure coherent information flow.\nStage 1: Prompt Intensification. In this stage, we focus on Recognition, where Gemini is tasked with generating descriptions capturing salient visual features from the given visual condition. Subsequently, we fuse the generated textual description with the user's textual input, creating an intensified prompt c'. This refined prompt serves as input for the T2I model, providing a more comprehensive representation of both visual and textual information. As a result, it addresses the inherent global-local misalignment and abstraction differences between text and visual modalities in the TV2I task, improving the model's ability to generate images that accurately reflect the intended context.\nStage 2: Adaptive Position. In parallel with Stage 1, we engage in Reasoning to analyze spatial relationships and element placements. Gemini assesses both the visual and textual conditions, considering all relevant semantic elements. It generates bounding boxes for each visual condition, specified in [x, y, width, height] format where x and y represent the coordinates of the top-left corner of the region. These bounding boxes serve as precise spatial guidelines for seed initialization (Sec. IV-B), ensuring that visual conditions are placed in a sensible and aesthetically pleasing manner. This step facilitates a harmonious arrangement of visual elements, contributing to an overall improved composition in the generated images.\nStage 3: Contextual Refinement. Building on the spatial arrangement from Stage 2, we further apply Reasoning to refine the contextual information. Gemini identifies condition- related regions that directly interact with the visual condition, mitigating information leakage and preventing truncated object outlines. For instance, when considering a face as the visual condition, Gemini recognizes that the body beneath is directly relevant, while the surrounding background is deemed irrelevant. This step is crucial for ensuring that the generated images maintain semantic coherence, such as preventing disconnection between a person's head and body. To achieve this, the attention modulation mechanism (Sec. IV-C) integrates this understand- ing by selectively directing key information flows, ensuring that relevant details (e.g., facial features) reach the appropriate regions (e.g., the body). This targeted approach helps maintain a coherent and contextually relevant output, avoiding issues where elements appear fragmented or misaligned. In summary, these MLLM-driven stages collectively enhance the generative model's performance, ensuring that the output achieves high semantic coherence, accurate spatial relationships, and a refined integration of visual and textual conditions.\nTraining-free Cyclic One-Way Diffusion Framework\nTo address the chaotic interference inherent in disordered diffusion, we introduce Cyclic One-Way Diffusion (COW). COW restructures the diffusion process into a unidirectional flow, effectively minimizing disruptive interference and ensur- ing efficient information transfer. This framework forms the core of our method, enabling versatile and efficient pixel-level and semantic-level visual conditioning without training. COW comprises three key components: Seed Initialization, Cyclic"}, {"title": "One-Way Diffusion Process, and Visual Condition Preservation, as shown in Fig. 3.", "content": "Seed Initialization. The objective of this mechanism is to inject stable high-level semantic information early in the denoising process, effectively reducing the layout conflicts with the visual condition. Classical random initialization sampled from the Gaussian distribution of diffusion model may introduce features or structures that conflict with the given visual condition. For instance, if an object is intended to appear on the left but the initial noise favors the right, there will be a conflict. The model must invest considerable effort during generation to correct this inconsistency, which may still negatively impact the quality of the generated images. To avoid such conflicts, we introduce a novel initialization: embedding visual conditions directly onto a predefined background, usually a semantically neutral pure gray. This process uses adaptive positioning, which leverages semantic cues from multimodal large language models (MLLMs) to determine optimal object placement (discussed further in Sec. IV-A).\nCyclic One-Way Diffusion Process. Practically, we invert the visual condition into its latent representation by solving the probabilistic flow ODE (Eq. 3) and embedding it in the initial random Gaussian noise that serves as the starting point, which can provide a good generative prior to maintain consistency with the visual condition. However, the implanted information will be continuously disrupted by inner diffusion from the surrounding Gaussian region at every denoising step according to the analysis of Sec. III-B. Therefore, we introduced \"one-way\" and \"cyclic\" strategies to maximize the flow of information from the visual condition to the whole image and minimize undesired interference from other image regions. To be specific, we store inverted latents of the visual condition at each inversion step in the middle stage (the semantic formation stage), denoted as xt1, xt1+1,..., xt2 and gradually embed them in the corresponding timesteps during the generation process. Through this step-wise information injection, we can ensure the unidirectional propagation of information, i.e., it only propagates from the visual condition to the other regions without interference from information in the background or other parts of the image. Given the limited generative capacity of the model at each step, noise is injected to regress the generative process to earlier stages, as illustrated in Eq. 1. This cyclic utilization of the model's generative capacity enables the continuous perturbation of inconsistent semantic information, facilitating the re-diffusion of conditional guidance in subsequent rounds. To provide the model with greater exploratory space, we have increased the degree of perturbation by injecting noise at a larger scale in a backward step. The cyclic one-way process benefits the model from one-way guidance from the visual condition, creates additional space by cycles for semantic \"disturb\u201d and \u201creconstruct\u201d, and ultimately achieves harmony among the background, visual condition, and text condition.\nVisual Condition Preservation. Conflicts between the visual and the text conditions often exist (such as a smiling face condition and a \"sad\" text prompt), necessitating a method that can effectively balance these conditions. We observe that the middle stage is still subject to some extent of uncertainty,"}, {"title": "Dynamic Attention Modulation", "content": "While COW effectively manages the unidirectional informa- tion flow from visual conditions, it may still face challenges such as over-penetration of target information into irrelevant regions or insufficient guidance to relevant regions, potentially resulting in artifacts like truncated outputs. For example, in the attribute editing example in the first column of the second row of Fig. 7, when the face serves as the visual condition, there is no cohesive integration around it. This results in a truncated output that disconnects from the background. These issues stem from the intricate, context-driven relationships that are not always fully captured by current generative models. To address the aforementioned issues, SOW employs dynamic attention modulation inspired by DenseDiffusion [50] to refine and precisely control the flow of information. Drawing on contextual insights from Multi-Level Language Models (MLLMs), our approach enhances both the accuracy and coherence of the generated images.\nSuppresses the attention from the conditional region to non-conditional regions (denoted as P\u00af) to preserve the integrity of the conditional information.\nEnhances the attention from condition-related regions to the conditional region (denoted as P+) to direct a more effective and targeted information flow.\nP^- = -M_{c\\rightarrow nc} \\cdot \\gamma_{time}(t) \\cdot w^-, (4)\nP^+ = M_{cr\\rightarrow c} \\cdot \\gamma_{dis} (D_{cr\\rightarrow o^*}) \\cdot \\gamma_{time}(t) \\cdot w^+, (5)\nwhere Mc\u2192nc denotes the attention mask from the conditional region to the non-conditional region, and Mcrc represents the attention mask from the condition-related region to the conditional region. By leveraging the robust reasoning ca- pability of MLLMs, we can employ MLLMs for spatial planning, thereby obtaining the expected condition-relevant regions simultaneously (See Sec. IV-A). Moreover, where Dcro* denotes the distance from condition-related regions to the conditional center o*. w\u00af and w+ are adjustment factors, t is the current cycle count, and \\gamma(x) is a decay function that adjusts attention over time and distance:\n\\gamma(x) = (\\frac{1}{2} (1+cos(\\frac{\\pi x}{T})))^a , (6)\nwhere T and a are predefined parameters that control the rate and extent of decay. This ensures that as the image generation"}, {"title": "V. EXPERIMENTS", "content": "Benchmark. To simulate the visual condition processing in real scenarios, we adopt face images from CelebAMask-HQ [62] as our visual condition. We design three kinds of settings for the text conditions\u2014normal prompt, style transfer, and attribute editing. Subsequently, we pair each prompt with two images, compiling these image-text pairs into the CelebA-TV2I dataset, which serves as the conditions for the TV2I task.\nBaselines. We perform a comparison with four existing works incorporating different levels of the visual condition into pre-trained T2I models: DreamBooth [10] based on the code\u00b9, TI [9], SD inpainting [13], and ControlNet on the canny"}, {"title": "A. Comparisons to the Existing Methods", "content": "Evaluation Metrics. To evaluate the quality of the generated results for this new task, we first consider the assessment of visual fidelity. We adopt a face detection model (MTCNN [63]) to detect the presence of the face, and a face recognition model (FaceNet [64]) to obtain the face feature and thus calculate the face feature distance between the generated face region and the given visual condition as the Face Detection Rate and ID-Distance metric. However, relying solely on model predictions can not fully capture the subtle differences between images and can not reflect the overall quality of the images (e.g., realism, richness), which are critically crucial for human perception. Therefore, we further evaluate our model via human evaluation. We design two base criteria and invited 50 participants to be involved in this human evaluation. The two criteria are: 1. Condition Consistency: whether the generated image well matches the visual and textual conditions; 2. General fidelity: whether the chosen image looks more like a real image in terms of image richness, face naturalness, and overall image fidelity. It's important to note that when assessing the latter criterion, participants are not provided with textual and visual conditions to prevent additional information from potentially interfering with the assessment process."}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "In this paper, we delve into the \"diffusion (physics) in dif- fusion (machine learning)\u201d properties, proposing an innovative approach termed the Cyclic One-Way Diffusion (COW) method. This method transforms the conventional bidirectional diffusion process into a unidirectional one, meticulously directing the inner diffusion. This approach cultivates fertile ground for a wide array of customized application scenarios via a pre- trained yet frozen diffusion model. Building on COW, we further propose a more comprehensive framework Selective One-Way Diffusion (SOW). SOW leverages the formidable reasoning capabilities of Multi-modal Large Language Models (MLLMs) to tackle complex context-driven challenges and utilize dynamic attention modulation in the cyclic process informed by robust prior knowledge of MLLMs, thereby enhancing the adaptability and efficacy of the generative model. SOW novelly explores the intrinsic diffusion properties tailored to specific task requirements. All the experiments and evaluations demonstrate our method can generate images with high fidelity to both semantic-text and pixel-visual conditions in a training-free, efficient, and effective manner.\nLimitations. The pre-trained diffusion model is sometimes not robust enough to handle extremely strong conflicts between the visual and the text conditions. For example, when given the text \u201ca profile of a person\u201d, and a front face condition, it is very hard to generate a harmonious result that fits both conditions. In this case, the model would follow the guidance of the text generally.\nSocial Impact. Image generation and manipulation have been greatly used in art, entertainment, aesthetics, and other common use cases in people's daily lives. However, it can be abused in telling lies for harassment, distortion, and other malicious behavior. Too much abuse of generated images will decrease the credibility of the image. Our work doesn't surpass the capabilities of professional image editors, which mitigates concerns about its potential misuse. Since our model fully builds on the pre-trained T2I model, all the fake detection works distinguishing the authenticity of the image should be able to be directly applied to our results."}, {"title": "A. Sensitivity to Text Condition during Denoising", "content": "We study the sensitivity of diffusion models to text conditions during the denoising process by injecting text conditions at different denoising steps. We replace a few unconditional de-noising steps with text-conditioned ones during the generation of the image randomly sampled from Gaussian noise. We calculate the CLIP cosine similarity between the final generated image and text condition. As shown in Fig. 1, the model is more sensitive to text conditions in the early denoising stage and less sensitive in the late stage. This also demonstrates our method reasonably utilizes the sensitivity to text conditions through proposed Seed Initialization and Cyclic One-Way Diffusion."}]}