{"title": "Memory-Augmented Agent Training for Business Document Understanding", "authors": ["Jiale Liu", "Yifan Zeng", "Malte H\u00f8jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "abstract": "Traditional enterprises face significant challenges in processing business documents, where tasks like extracting transport references from invoices remain largely manual despite their crucial role in logistics operations. While Large Language Models offer potential automation, their direct application to specialized business domains often yields unsatisfactory results. We introduce Matrix (Memory-Augmented agent Training through Reasoning and Iterative eXploration), a novel paradigm that enables LLM agents to progressively build domain expertise through experience-driven memory refinement and iterative learning. To validate this approach, we collaborate with one of the world's largest logistics companies to create a dataset of Universal Business Language format invoice documents, focusing on the task of transport reference extraction. Experiments demonstrate that Matrix outperforms prompting a single LLM by 30.3%, vanilla LLM agent by 35.2%. We further analyze the metrics of the optimized systems and observe that the agent system requires less API calls, fewer costs and can analyze longer documents on average. Our methods establish a new approach to transform general-purpose LLMs into specialized business tools through systematic memory enhancement in document processing tasks.", "sections": [{"title": "1 Introduction", "content": "Combing through large quantities of unstructured data remains a widespread challenge in enterprise operations, particularly in finance functions where efficient invoice processing represents a growing competitive advantage. Despite the prevalent adoption of digital invoicing, many organizations still grapple with the labor-intensive and error-prone task of manually extracting crucial identifiers from business transactions. For logistics companies manual extraction not only slows down operations but also introduces the potential for human error, leading to misrouted shipments and customer dissatisfaction.\nLarge Language Models have demonstrated remarkable capabilities in natural language understanding and processing (Achiam et al. 2023; Yang et al. 2024; Dubey et al."}, {"title": "2 Related Work", "content": "Business Document Reasoning Benchmarks. While general-domain question answering datasets have driven advances in natural language understanding (Abujabal et al. 2019; Rajpurkar et al. 2016; Yang et al. 2018; Joshi et al. 2017; Talmor and Berant 2018; Bajaj et al. 2016; Kwiatkowski et al. 2019), they fail to capture the structured, transactional nature of business documents. Existing research on business document understanding mostly focus on vision-based information extraction from scanned copies (Harley, Ufkes, and Derpanis 2015; Riba et al. 2019; Zhong, Tang, and Yepes 2019; Antonacopoulos et al. 2009), which emphasize layout understanding over semantic reasoning. These datasets, while valuable for OCR and structure recognition tasks, fail to capture the domain-specific patterns essential for business document processing. Moreover, text-based researches tend to use proprietary, undisclosed datasets (Hamdi et al. 2021; Krieger et al. 2021; Palm, Winther, and Laws 2017; Tarawneh et al. 2019), limiting reproducibility and real-world applicability. To address these limitations, we present a publicly available business invoice dataset derived from real-world transactions to facilitate research on practical business scenarios.\nPrompt Optimization. Prompt optimization is a popular paradigm for maximizing LLM's performance to novel tasks without expensive model tuning by finding a optimal task prompt (Zhou et al. 2022b; Pryzant et al. 2023; Cheng et al. 2023; Prasad et al. 2022). In-context learning emerges as a prominent paradigm, where a set of input-output pairs is provided as few shot examples to the LLM (Min et al. 2021; Dong et al. 2022; Brown 2020). By automatically retrieving demonstrations from training set (Zhao et al. 2021; Lu et al. 2021; Liu et al. 2021) or from adaptively annotated samples (Zhang et al. 2023b; Wu et al. 2022; Su et al. 2022), they primarily treat demonstrations as static examples rather than distilled insights. In contrast to this line of work, our proposed paradigm aims to distill trajectories into generalizable heuristics.\nAgent Learning. There has been efforts in exploring inference time performance boost since the emergence of Large Language Models (Shinn et al. 2024; Madaan et al. 2024; Yao et al. 2023, 2024; Sumers et al. 2023; Wei et al. 2022; Zhou et al. 2022a; Guo et al. 2024). Recent works have extended this paradigm to agentic systems. Some works represent and learn the optimal workflow of agentic systems in the form of complex graphs (Zhuge et al. 2024; Wu et al. 2024b), code (Hu, Lu, and Clune 2024), memory (Wang et al. 2024) and trees (Zhang et al. 2024a) to improve the system's performance on complex tasks, while others learns reusable tools (Zhang et al. 2024c; Cai et al. 2023; Qian et al. 2023; Yuan et al. 2023) and experience (Zhao et al. 2024; Wang et al. 2024) for agentic systems. Different from previous memory optimization based works, our proposed approach does not rely on any predefined in-context examples (Zhao et al. 2024) and meanwhile guarantee the diversity and robustness of the learned memory."}, {"title": "3 Matrix: Memory-Augmented agent Training through Reasoning and Iterative eXploration", "content": "In this section, we begin with presenting the formulation of document reasoning, then introduce the framework of Matrix."}, {"title": "3.1 Problem Statement", "content": "We begin with the formal definition of the document reasoning problem. In this problem, we assume the existence of a dataset for a given task, which consists of N instances of document, query, and answer triplets:\nD = { (d_i, q_i, a_i) }^N_{i=1}\n(1)\nwhere \\(d_i\\) represents a document, \\(q_i\\) is an associated query, and \\(a_i\\) is the corresponding correct answer. In the context of transport reference extraction from business documents, this formulation naturally maps to our task of interest: each document \\(d_i\\) represents a UBL invoice, the query \\(q_i\\) requests the location of the transport reference number, and the answer \\(a_i\\) is the correct reference number. This mapping allows us to frame the challenge of invoice processing as a structured document reasoning problem while maintaining the generality of our approach.\nThe overall goal in document reasoning is to train a system that learns a function \\(f : D \\times Q \\rightarrow A\\), where D, Q, A represents the space of documents, queries, and answers respectively. For a given document \\(d_i\\) and query \\(q_i\\), the agent generates an answer \\(\\hat{a}_i\\). The system's performance is evaluated based on the alignment between \\(\\hat{a}_i\\) and \\(a_i\\).\nWe consider an agent with a long-term memory module M, which stores useful contextual information for reasoning. For a given query (d, q), the agent solves the task in an iterative manner. At each timestep t, the agent observes the current state \\(o_t\\), takes an action \\(a_t\\) (which may involve interacting with the document or recalling information from memory), and then receives an updated observation \\(o_{t+1}\\). This sequence of interactions produces a trajectory of observations and actions: \\(\\tau = (o_0, a_0, o_1, a_1, ..., o_t, a_t)\\). Finally, the agent produces the final predicted answer \\(a_i\\) through an answer extraction module g(\\(\\tau\\)), which takes into account the full trajectory \\(\\tau\\) of observations and actions"}, {"title": "3.2 Proposed Method", "content": "To optimize the objective in Eq.3, we follow the paradigm of supervised learning and develop the dataset into train set \\(D_{train}\\) and test set \\(D_{test}\\) to approximate the inaccessible real data distribution. We train an optimal memory and test it on test task set. The pipeline is illustrated in Figure 1.\nTrajectory Sampling. We perform optimization by progressively updating the memory module M over multiple epochs. During each epoch, we sample a mini-batch of tasks from \\(D_{train}\\). For each sampled task \\((d_i, q_i, a_i)\\), the agent interacts through a sequence of actions over time, forming a trajectory \\(\\tau_i = (o_0, a_0, o_1, a_1, ..., o_T, a_T)\\), where \\(o_t\\) and \\(a_t\\) denote the observation and the action at timestep t for the i-th task.\nThe trajectory continues until the agent either reaches a solution or the interaction exceeds a predefined maximum number of steps. We enforce a upper limit \\(T_{max}\\) to the total number of steps per task. This can constrain the token count in each trajectory, and allow us to increase the mini-batch size during optimization. By doing so, the optimizer can process more diverse tasks within a single batch, which increases the model's exposure to varied problem domains, leading to a more robust and generalizable memory M.\nReflection. Despite the impressive reasoning capabilities of LLM-based agents, they are prone to issues such as hallucination (Ji et al. 2023), factual errors (Wang et al. 2023), and reasoning failures (Huang et al. 2023). In a limited number of steps, it is hard for agent to autonomously recognize and rectify its own mistakes without explicit self-correction mechanisms or auxiliary prompts (Huang et al. 2023; Jiang et al. 2024; Pan et al. 2023; Kamoi et al. 2024; Song et al. 2024). To overcome this limitation, we introduce a Reflector module that operates as a post hoc evaluator of the trajectory. The reflector is provided with the trajectory \\(\\tau_i\\) and the ground truth answer \\(y_i\\). It's task is to label the solution as \"Correct/Incorrect\" and identify the key steps where reasoning errors occurred or correct decisions were made that led to the proper solution. The reflection process is described as:\n\\(r_i = LM_{reflect}(\\tau_i, y_i)\\)\n(4)\nwhere \\(LM_{reflect}(\\cdot)\\) represents prompting a LLM to evaluate the trajectory. The reflection phase provides insights for refining the memory and improving the agent's task-solving strategy.\nOptimization. To obtain an optimal solution in Eq.3, an optimizer is needed that can generate new solutions based on performance measurement. The optimization problem operates on the space of natural languages, which perfectly paves the way for borrowing Large Language Models' exceptional capability in natural language understanding (Yang et al. 2023; Zhang et al. 2024c). We propose a meta-optimizer with Large Language Models as the backend. The optimizer takes in three parts of information: (1) The execution trajectory of the agents in solving the tasks in the current training mini-batch (2) The assessment of each trajectory provided by the evaluator. (3) The current memory the agents operate on. Although the memory can be accessed by reading the prompt in the trajectory, we explicitly provide the current memory to let the optimizer progressively update the instruction based on the assessments for improvement. The optimization process can be formulated as:\n\\(M_{i+1} = LM_{optim} (\\mathcal{T}, y, M_i)\\)\n(5)\nwhere \\(LM_{optim}(\\cdot)\\) denotes the process of LLM-based optimization."}, {"title": "4 Evaluations", "content": "4.1 Experiment Setup\nAgent system. We experiment with refining a two-agent system. In the system, an assistant powered by LLM receives and analyzes the task and suggests code for execution, and a user proxy automatically executes code and provide the result. The conversation will end once a final answer is reached by assistant or the max number of conversation turns is reached. The system is implemented by AG22 (formerly AutoGen) (Wu et al. 2023).\nExperiment Dataset. We evaluate our system on a collection of real-world Universal Business Language invoice documents, developed in cooperation with Kuehne+Nagel, one of the world's largest logistics companies. The primary task is to extract transport reference number from these documents. The dataset contains 764 valid invoice document and transport reference pairs. While the real world dataset contains sensitive customer data and cannot be released publicly, we provide an anonymized subset of the dataset and include the evaluation results in appendix.\nThe dataset presents several challenging characteristics that make it an ideal testbed for evaluating iterative learning capabilities. First, it requires specialized domain knowledge of business documents and terminology not commonly found in general language model training. Second, the hierarchical structure of UBL documents and the significant variability in format and identification patterns pose substantial extraction challenges. Additionally, as a novel benchmark without prior literature coverage, this dataset offers unique opportunities to assess agents' adaptive learning abilities in a practical, high-stakes business context.\nEvaluation Protocol. We report the success rate of the agent. The agent system is required to output the final result into a standardized format. The task is considered success when the output transport reference exactly matches the ground truth label. Given the diverse range of possible edge cases in the outputs, manually creating an extraction and comparison module may not cover all scenarios. Therefore, we employ a LLM judge (Zheng et al. 2023a) to compare the agent's output with the ground truth.\nBaselines for Comparison. We compare performance with previous works, including prompting LLM using Chain of Thought (Wei et al. 2022), Reflection (Shinn et al. 2024), and agent system with no memory (Wu et al. 2023). For a detailed description of the implementation details of the baselines, please refer to appendix C."}, {"title": "4.2 Main Results", "content": "We randomly select 60 samples from the dataset for training, the remaining 704 samples are reserved for testing performance. The maximum number of conversational turns between the assistant agent and the user proxy was capped at 5. The batch size was set to 14. That is, for each epoch, 14 tasks are sampled from training set and used for optimization. Since individual trajectories can become excessively lengthy, making the input to optimizer LLM larger than its"}, {"title": "4.3 Analysis of the Optimized System", "content": "In this section, we analyze the metrics of the system across the optimization of memory across epochs. Specifically, we explore three metrics: (1) average api calls it takes to correctly solve a question. This affects the overall latency of the agent system. (2) average cost it takes to correctly solve a question. (3) the distribution of the length of the successfully solved documents.\nAnalysis of the average number of API calls. As shown in Figure 4, the agent system exhibits a notable decrease in the average number of API calls required to solve the document question-answering task. After equipping with the optimized memory, the average number of API calls reduces about 8.12% for gpt-40 backed agent, 21.3% for gpt-40-mini backed agent. This reduction indicates that the system is able to handle the task more efficiently with fewer calls, reducing sources of latency but reaching a better performance.\nAnalysis of the cost. We plot the average cost of successfully solving a task after each epoch in Figure 5. The cost increases during the early stages of optimization. This rise is attributed to the initial memory initialization and optimization phase. During this phase, while the memory is introduced to guide problem-solving, its content is not yet fully refined. As a result, the agent writes more lengthy code for trial and subsequently leading to higher API costs. As optimization goes on, the memory gets gradually refined and more comprehensive, the agent can solve the task more efficiently with less tokens. The slight uptick in at the end of the optimization suggests an adaptive process, where some rebalancing occurs after significant cost reduction. From a broader perspective, the overall trajectory suggests that optimization effectively lowers the long-term computational cost in solving tasks.\nAnalysis of the length of successfully solved documents. As shown in Figure 6, the distribution of the successfully analyzed document lengths by gpt-40 backed agent is illustrated for two different stages of the agent's performance: before optimization and after optimization. The document length is measured by the total number of tokens after tokenization. The distribution of successfully analyzed document lengths shifts notably after optimization. Before training, the performance peaks for document lengths around 4000 and 6000 tokens, and significantly declines after 6000 tokens. After training, we observe a significant improvement, especially for longer documents, with the distribution peak shifting towards 6,000 tokens and an extended tail that stretches beyond. This pattern showcases the enhanced capacity of the agent system to handle larger documents after optimization, reflecting its improved robustness and processing efficiency for more complex and lengthy inputs. We observe similar pattern for gpt-40-mini backed agent and therefore omit the plotting for brevity."}, {"title": "4.4 Optimization with Weaker Language Model", "content": "In the main experiment, we leveraged gpt-40 as backbone for optimization. We next explore the performance with smaller language model used as optimizer. Following the protocol in Section 4.2, we set the backbone of optimizer and agents both to gpt-40-mini.\nFigure 7 demonstrates how the performance changes with respect to the number of training epochs. Although gpt-40-mini has weaker language understanding capability, it can still understand the task and summarize reuable patterns from the task trajectories. However, it misses some regular expression patterns, leading to a less comprehensive optimized memory. Therefore, the final performance of the optimized system remains limited.\nWe then compare the performance between Matrix and Reflexion with gpt-40-mini for optimization and pro-"}, {"title": "5 Conclusion", "content": "This paper explores the application of LLM agents for business document information retrieval, focusing on extracting transport references from invoices. To specialize LLM agents for this domain, we introduce Matrix, a paradigm enabling systematic learning through iterative self-refinement and memory updates. Real-world evaluations show Matrix distills actionable insights, outperforms baselines by large margin, and improves latency, cost, and document processing capability. This work reveals the potential of LLM agents for efficient, scalable enterprise document automation."}, {"title": "A Dataset Details", "content": "The dataset we experiment on contains sensitive data that reveals the customers' business details. Therefore, we are unable to release the original dataset. We will release an anonymized subset of the dataset to facilitate future research in this field. The goal of anonymization is to preserve the original data structure while removing sensitive information. In the following section, we describe the anonymization pipeline."}, {"title": "A.1 Background and data source", "content": "The dataset used in this study originates from real-world invoices processed by Kuehne+Nagel. These invoices represent a diverse range of business transactions, providing a rich source of data for our transport reference and tracking number processing task."}, {"title": "A.2 Data collection process", "content": "The data collection was facilitated through the Beyond Work platform, a human/AI collaboration platform for solving tedious work. Specifically, the invoices were processed within a workblock, which is a container for all technology (code, infrastructure, permission etc) needed to solve a distinct process.\nThe workblock used to collect this data, targets the process of \"Parked Invoices\u201d in which invoices that cannot be automatically processed have to be processed by humans. The workblock uses LLMs to process the invoice and introduces a validation task in which human workers were tasked with validating tracking numbers and transport references extracted from the invoices. This process not only involved identification and verification of the correct information but also required workers to provide explanations for any discrepancies or errors they encountered. This human-in-the-loop approach ensured high-quality, validated data."}, {"title": "A.3 Data preparation and anonymization", "content": "Following the validation process, the data underwent several stages of preparation to ensure its suitability for research purposes while maintaining strict privacy standards. The anonymization of the data included both pseudonymization of the identifiers we were looking to extract for the invoices, as the original human validation of the identifiers was done on the non-anonymized data. This way we ensure to keep the format of the identifier while removing any traceability to the original numbers. For all other sensitive data we completely anonymized it. A separate workblock was authored (the process of using natural language to create workblocks) specifically for the anonymization process. This step was crucial to protect sensitive business information and comply with data privacy regulations."}, {"title": "A.4 Unstructured data pseudonymization", "content": "For unstructured text data, we specifcally used the claude-3.5-Sonnet language model within the anonymization workblock. This helped us in replacing identifying information with pseudonyms while maintaining the contextual and structural integrity of the data."}, {"title": "A.5 Structured data anonymization", "content": "For more structured data fields, we applied complete anonymization. This process involved replacing text data with random strings, integer values with random integers, and float values with random floats. This approach ensures that no traceable information remains in the structured fields."}, {"title": "A.6 Resulting dataset", "content": "The resulting dataset retains the complex structure and challenges of real-world invoice data while being fully anonymized. It preserves the intricate nature of business documents, allowing for realistic evaluation of information extraction techniques while ensuring the confidentiality of the original data sources."}, {"title": "B Results on Anonymized Data", "content": "In this section, we present the main results of the proposed method on the released anonymized data. The dataset comprises 127 task instances, with each instance consisting of an invoice document paired with its corresponding transport reference. In real-world scenarios, an invoice may not always contain a valid transport reference, and this characteristic has been intentionally preserved in the anonymized dataset to reflect real-world conditions. Of the 127 documents, 50 contain valid transport references, which form the primary focus of our study.\nOur work specifically focuses on cases where transport references are present. While we acknowledge the limited size of the anonymized dataset, this constraint arises from the significant manual effort required for anonymization and rigorous inspection. These measures are essential to ensure compliance with data privacy standards and to prevent the leakage of sensitive"}, {"title": "C Implementation Details of baselines", "content": "C.1 Chain-of-thought\nCoT is often used as the default way of prompting LLM. We follow this guideline and prompt the LLM to think step by step and analyze the dataset before generating the final answer.\nC.2 Two-agent\nThe vanilla two agent system is implemented by AG24 (formerly AutoGen) (Wu et al. 2023). In the system, an assistant agent backed by LLM is responsible for analyzing the task environment, reason, and make decisions. A user proxy agent receives the content generated by assistant agent, execute the python code provided, and automatically provides the feedback. For a detailed description of the system, please refer to the official documentation5.\nC.3 Reflexion\nReflexion proposes to improve language agents using verbal feedback. For each task, the agent reflects on their performance and store these reflections in memory to make better decisions in the next trial. The system maintains specific memories for each individual task or instance. We change the ReAct agent of the original paper into the two-agent system we investigate, and run reflexion on each task. The maximum number of trials is set to 7 to ensure a fair comparison. The prompt for generating reflection is identical to that of their original Github repository6."}, {"title": "D Limitations and future work", "content": "As shown in Section 4.4, the performance of Matrix strongly depends on the capability of backbone models. Weaker models cannot distill actionable insights from experiences and correctly guide future task solving attempts.\nThe process of agent training requires a number of task instances that are representative of the full data distribution. Comparing performance of Matrix on full data and the anonymized data, we discover that the method requires larger training data to reach strong performance. The anonymized data contains little samples that cannot fully reflect the dataset distribution, therefore Matrix tends to underperform. How to identify a subset of the most important and influential samples (i.e. coreset selection (Xia et al. 2022, 2024)) for training is an open question for agent training. One potential solution is to leverage AutoML based methods (Wang et al. 2021; Zheng et al. 2023b; Zhang et al. 2023a, 2024b)."}]}