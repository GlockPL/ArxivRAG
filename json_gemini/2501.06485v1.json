{"title": "A Diffusive Data Augmentation Framework for Reconstruction of Complex Network Evolutionary History", "authors": ["En Xu", "Can Rong", "Jingtao Ding", "Yong Li"], "abstract": "The evolutionary processes of complex systems contain critical information regarding their functional characteristics. The generation time of edges provides insights into the historical evolution of various networked complex systems, such as protein-protein interaction networks, ecosystems, and social networks. Recovering these evolutionary processes holds significant scientific value, including aiding in the interpretation of the evolution of protein-protein interaction networks. However, existing methods are capable of predicting the generation times of remaining edges given a partial temporal network but often perform poorly in cross-network prediction tasks. These methods frequently fail in edge generation time recovery tasks for static networks that lack timestamps. In this work, we adopt a comparative paradigm-based framework that fuses multiple networks for training, enabling cross-network learning of the relationship between network structure and edge generation times. Compared to separate training, this approach yields an average accuracy improvement of 16.98%. Furthermore, given the difficulty in collecting temporal networks, we propose a novel diffusion-model-based generation method to produce a large number of temporal networks. By combining real temporal networks with generated ones for training, we achieve an additional average accuracy improvement of 5.46% through joint training.", "sections": [{"title": "I. INTRODUCTION", "content": "The core challenge in reconstructing the evolution of complex networks is inferring the order in which edges are formed in dynamic graphs, thus reproducing the network's evolutionary process ([1], [2]). This task is intrinsically tied to the dependency between network structure and edge formation sequence, as accurately inferring this order can reveal the underlying dynamic mechanisms of the network, as illustrated in Fig. 1. Successfully reconstructing network evolution has significant applications across various domains, such as social network analysis, traffic system optimization, and biological network research ([3], [4], [5]). These fields require a deep understanding of how networks evolve over time to predict future trends and implement effective interventions or optimizations.\nRecent work published in Nature Communications 2024 [6] introduces a novel framework that defines and addresses the important problem of complex network evolution prediction. This approach predicts the generation times of network edges by leveraging the structural features of the network. Specifically, it utilizes the generation times of a subset of edges to infer the temporal information for the remaining edges, effectively linking network structural characteristics with edge generation times. While this represents a significant advancement, many real-world applications involve networks where no temporal information is available, presenting an even more challenging problem. Many networks are observed as static snapshots without temporal information. For example, in ecological predator-prey networks (as shown in Fig. 1), current interactions are easy to capture, but reconstructing evolutionary trajectories over millennia is costly and challenging. Similarly, in social and protein-protein interaction networks, while current structures are observable, tracing their step-by-step evolution remains difficult.\nThese scenarios highlight a common challenge in real-world applications: inferring the evolutionary history of a network solely from its static structure. While the [6] framework is effective in scenarios where partial temporal information is available, it is not well-suited for tasks where the network is entirely static, with no prior temporal data to leverage. This limitation underscores the necessity of developing novel methodologies capable of reconstructing network evolution directly from static structures, addressing a problem of critical importance across diverse domains.\nTo address the limitations of existing methods, we reformulate the task of predicting edge generation times into an equivalent task: predicting the relative temporal order between any two edges within the network. This reformulation simplifies the problem and transforms the previously non-transferable task into one that enables cross-network learning of the relationship between network features and edge generation times.\nUnlike prior models that are typically trained on a single temporal network, our approach leverages multiple temporal networks for training, allowing the model to learn generalized associations between network features and edge generation times across networks. As a result, the model trained on multiple temporal networks demonstrates superior and consistent performance when applied to previously unseen static networks.\nFurthermore, to enhance the model's performance and address the scarcity of real-world temporal network data, we employ a diffusion model to generate an arbitrary number"}, {"title": "II. RELATED WORK", "content": [{"title": "A. Temporal Networks in Complex Networks", "content": "Recent advancements in temporal networks have significantly enriched complex network analysis [7], [8], [9], [10], [11]. Early models such as Erd\u0151s-R\u00e9nyi and Barab\u00e1si-Albert ([12], [13]) mainly addressed static network properties, offering limited applicability to dynamic systems. [14] provided foundational concepts like temporal paths, establishing a basis for subsequent research. [15] highlighted bursty interaction patterns in real-world networks, emphasizing temporal heterogeneity. [16] extended random walk theory to temporal networks, crucial for understanding diffusion processes. [17] introduced methods for detecting communities in dynamic structures, while [18] integrated predictive models for forecasting interactions. [19] reviewed evolving community detection approaches, showcasing the need to address both temporal and topological dynamics in analysis. [6] introduced the problem and method for predicting network evolution based on network structures. By utilizing the timestamps of a subset of network edges, the method effectively predicts and reconstructs the remaining edges, achieving promising results. Furthermore, this work demonstrated that the reconstructed edge timestamps could also enhance performance in link prediction tasks, further underscoring the significance of studying network evolution."}, {"title": "B. Dynamic Graphs and Temporal Network Generation", "content": "Recent developments in temporal network generation have significantly advanced the field, focusing on capturing both the structural and temporal dynamics of evolving networks. [20] improved temporal modeling by transforming temporal interactions into static graphs, while [21] utilized motif-based approaches for generating time-dependent structures, though it assumed fixed rates and lacked adaptability to real-world temporal fluctuations. [22] introduced TIGGER leveraging temporal point processes, combining both inductive and trans-ductive learning to handle large-scale, complex networks with superior efficiency and accuracy. This progression highlights the growing ability of temporal models to replicate the nuanced evolution of real-world dynamic systems."}, {"title": "C. Data Augmentation via Graph Diffusion Models", "content": "Employing generative methods such as generative adversarial networks (GANs) to learn a distribution based on a small sample of data and subsequently conduct sampling to obtain a large quantity of training samples is a commonly adopted method [23], [24], [25], [26]. Diffusion models have recently shown significant advantages in performance, and have done well on graph generation tasks [27], [28], [29], [30], making them suitable for solving the problem of small-sample learning in reconstructing network evolutions. Specifically, [28] first propose to introduce the score-based diffusion model to solve the graph generation problem. The Gaussian noise is adopted to construct the diffusion process, leading to poor sparsity generated graphs. [29] explore jointly modeling the nodes and edges with semantic features. [27] use multinomial noise to make the diffusion process operating in discrete space, which greatly improves the performance. Therefore, we can see that using graph diffusion models to generate sufficient samples to enhance edge order prediction performance is promising."}]}, {"title": "III. PRELIMINARIES", "content": [{"title": "A. Reformulating the Edge Generation Prediction Task", "content": "In the study of network evolution, predicting the exact generation time of edges represents a significant challenge. Directly learning the relationship between network features and edge generation times proves to be difficult due to the inherent complexity and variability of these relationships across different networks. Moreover, such a task lacks generalizability; the features learned in one network often fail to generalize effectively to another, rendering the task unsuitable for cross-network transferability. Thus, learning to predict edge generation times within the same network-where partial timestamps are available for predicting the remaining ones-is appropriate. However, if the goal is to generalize across networks and learn the relationship between network features and edge generation times, the equivalence transformation becomes crucial.\nTo address this limitation, we reformulate the original task of predicting edge generation times into an equivalent task: predicting the relative temporal order of generation between any two edges in the network. This reformulation simplifies the prediction problem into a pairwise classification task, which is more robust and transferable across networks.\nWe rigorously prove the mathematical equivalence between these two tasks, with the final equivalence result expressed as:\n$E_{theory} = \\frac{x(1-x)}{\\|(2x - 1)^2 \\sqrt{M}\\|}$ (1)\nwhere x represents the pairwise prediction accuracy, and M denotes the total number of edges in the network. The detailed proof is provided in Appendix A.\nFurthermore, as illustrated in Fig. 2, experimental results validate this equivalence in real-world networks. These results demonstrate that improving pairwise prediction accuracy directly contributes to reducing the overall prediction error in edge generation times. Equation 1 reveals that the overall sequence error scales inversely with the square root of the edge count, favoring networks with many edges.\nThis equivalence also yields an additional benefit: achieving a low RMSE in predicting edge generation times does not necessitate exceedingly high accuracy in pairwise temporal order predictions. This insight reduces the performance requirements for the underlying pairwise prediction model, enhancing the practical applicability of the approach.\nTo infer the complete temporal order of edge generation from pairwise relationships, we adopt the Borda algorithm. This algorithm aggregates pairwise predictions to construct a global sequence of edge generation times while ensuring consistency across predictions. A detailed explanation of the Borda algorithm, including its derivation and implementation, is provided in the Appendix B."}, {"title": "B. Mathematical Definition of the Task", "content": "In the study of complex networks, the generation time of edges is a critical factor influencing network evolution. Our prediction task aims to learn the relationship between network structure and edge generation time, given a temporal network.\nSpecifically, we focus on how to infer the generation order of edges in the absence of time labels.\nOur ultimate goal is to predict the generation time of each edge for a pure network structure. To simplify the task, we specify it as follows: for any two edges ei and ej in the network, we need to predict their generation order, determining which edge is generated first and which is generated later. By inferring the order of all edges, we can construct the overall generation sequence of edges in the network, thereby gaining a comprehensive understanding of the network's evolutionary process.\n1. Temporal Network Representation: Let the temporal network G consist of a set of nodes V and a set of edges E. Each edge ek \u2208 E represents a connection between a pair of nodes (uk, Uk) and has an associated generation time tk.\n2. Edge Generation Order: For any two edges ei = (Ui, Vi) and ej = (uj, vj), we define the generation order relation R(ei, ej) as follows:\nR(ei, ej) = $\\{ \\begin{array}{ll} 1, & \\text{if } t_i < t_j \\\\ 0, & \\text{if } t_i \\geq t_j \\end{array} \\}$\nHere, R(ei, ej) = 1 indicates that edge er is generated before edge ej, while R(ei, ej) = 0 indicates that the generation time of edge er is later than or equal to that of edge ej.\n3. Edge Generation Time Prediction: Our task can be expressed as a binary relation prediction problem, where the objective is to predict the edge generation order Rei, ej) by learning the structural features of the network S(G):\nR(ei, ej) = f(S(G), ei, ej)\nIn this context, R(ei, ej) is the predicted generation order, and the function f represents the model used to infer the generation order based on network structural features.\n4. Construction of Edge Generation Sequence: Once the generation order for all edges is predicted, we can obtain the overall edge generation sequence T, expressed as:\nT = \\{ek_1, ek_2, ..., ek_m\\}\nHere, k1,k2,..., km are the indices of edges arranged according to the predicted generation order.\nThrough the above description and mathematical definitions, our task clearly demonstrates how to infer edge generation"}]}, {"title": "C. Framework for Comparative Paradigm-based Neural Network", "content": "The framework for predicting the order of two edges in a network is structured as follows. Initially, we utilize an embedding representation learning method to obtain a representation vector for each edge in the network. These embedding vectors are derived from 12 network features, which include the degree, clustering coefficient, betweenness centrality, common neighbors, random walk probability, minimum spanning tree connection, Jaccard coefficient, resource allocation index, Adamic-Adar index, shortest path length, PageRank, and average clustering coefficient [31], [32], [33]. These features are calculated based on the network's adjacency matrix and provide a comprehensive structural description of each edge. Let us denote the edges as e1 and e2, with their respective embedding vectors represented as h\u2081 and h2.\nEach embedding vector is subsequently fed into a fully connected neural network comprising three layers. The architecture is defined as follows:\nInput Layer: The first layer receives the input vectors h\u2081 and h2, which are concatenated to form a single input vector h = [h1; h2] \u2208 Rd, where d is the dimension of the embedding vectors.\nHidden Layer: The second layer is a hidden layer with a dimensionality of d. This layer applies a linear transformation followed by a ReLU activation function, defined as:\nh_{hidden} = ReLU(W_1h + b_1),\nwhere W\u2081 \u2208 Rd\u00d7d and b\u2081 \u2208 Rd are the weight matrix and bias vector for the hidden layer, respectively.\nOutput Layer: The output layer consists of two neurons that produce a scalar output 21 and 22:\nz_1 = W_2h_{hidden} + b_2,\nwhere W2 \u2208 R2\u00d7d and b2 \u2208 R2 are the weights and biases for the output layer.\nAfter obtaining the outputs 21 and 22, we apply the softmax function to convert these into a probability distribution representing the likelihood that each edge was generated first:\np(e_1 \\text{ before } e_2) = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}},  \np(e_2 \\text{ before } e_1) = \\frac{e^{z_2}}{e^{z_1} + e^{z_2}}\nThese probabilities reflect the generation times of the edges, normalized to ensure that they sum to one.\nThe loss function L is defined as the categorical cross-entropy loss between the predicted probabilities and the true labels y \u2208 {0,1}, where y = 1 indicates that e1 occurs before e2:\nL = -y log(p(e\u2081 \\text{ before } e\u2082)) \u2013 (1 - y) log(p(e2 \\text{ before } e\u2081).\nAdditionally, we incorporate L2 regularization to mitigate overfitting. The regularization term is defined as:\nR = \\lambda (||W_1||^2 + ||W_2||^2),\nwhere \\lambda is the regularization strength. Thus, the final loss function to be minimized is:\nL = L + R."}, {"title": "D. Limitations of Existing Method", "content": "Current methods for network evolution prediction rely on partial edge timestamps to train models and predict the temporal order of remaining edges. For instance, the Nature Communications [6] approach performs well on synthetic networks by learning from one fully timestamped network and predicting on another. However, its effectiveness diminishes on real-world networks due to the structural homogeneity of synthetic networks (e.g., Barab\u00e1si-Albert [34], Popularity-similarity-optimization model [35], Fitness model [36]), which share scale-free properties. In contrast, real-world networks"}, {"title": "IV. METHOD", "content": [{"title": "A. Joint Training for Improved Transferability", "content": "To address the limitations of existing methods, we propose a joint training approach that combines multiple networks to train a single model, exposing it to diverse network types and improving its ability to generalize to unseen networks.\nIn Section V-I, we demonstrate why traditional methods struggle with transferability when trained on a single network type. Structural differences across networks lead to significant performance gaps, making these models unreliable on unseen data. Reformulating the prediction task-from predicting edge generation times to predicting pairwise edge order mitigates this issue. Furthermore, Section V-C1 highlights the superior performance of our jointly trained model, which achieves consistent results on unseen networks.\nFig. 4 illustrates our approach. Solid circles represent existing networks, and current methods often exhibit instability, as gaps between networks hinder generalization. Joint training reduces these gaps by incorporating a diverse range of networks, enabling the model to learn more generalizable patterns.\nTo further enhance the model's transferability, we generate augmented networks using diffusion models, represented by the small solid circles in Figure 4. These augmented networks expand the range of each original network, increasing coverage"}, {"title": "B. Enhancing Network Evolution Prediction with Diffusion Model-Based Augmentation", "content": "As shown in Fig. 5, we propose an enhanced framework for network evolution prediction based on diffusion models. The core idea is that we have a limited number of temporal networks. By performing extensive sampling on each temporal network, we input the samples into the diffusion model to learn the underlying generation mechanism of the temporal networks. The diffusion model then generates an arbitrary number of augmented samples. Using a combination of a small number of real temporal networks and a large number of diffusion-augmented temporal networks, we input them into a comparative paradigm-based neural network (CPNN) for training. This approach results in a model with better transferability and stability. In practical applications, when a static network is input into the trained CPNN, we can predict the temporal order of pairs of edges. Further, by applying the Borda count algorithm, we can derive the overall edge order of the entire network, enabling the prediction of edge evolution processes in static networks.\n1) Construction of Training Dataset for the Diffusion Model: The construction of the training dataset for our diffusion model is a crucial step in enhancing the edge order prediction task. Our approach begins with a labeled"}]}, {"title": "2) Graph Denoising Diffusion Model", "content": "Based on the training data sampled from the real temporal network, we utilize the graph diffusion models to learn the distribution of the topology evolutions conditioned on the final topology structure. We refer to our model as the Diffusion Model for Generating Topology Evolution History, abbreviated as TopoEvoDiff. The graph diffusion models are adapted to ordered edges diffusion based on the final topology structure, where the structural features are encoded into the node features, such as node embeddings learned through structural learning.\nThe graph diffusion model consists of two main procedures: the forward diffusion process q and the reverse denoising process p. In the forward diffusion process, the network orders are disrupted by Gaussian noise step by step and reach to pure noise. The forward diffusion process is utilized to construct the training data for the denoising networks used in the reverse denoising process. The computation is defined as: F should mean the timestamps and ij should mean the indices of the edges\nq(\\mathcal{F}^\\tau_1,...,\\mathcal{F}^\\tau_T|\\mathcal{F}_1^\\tau) = \\prod_{t=1}^T q(\\mathcal{F}_t^\\tau|qt-1).\nq(\\mathcal{F}_j^\\tau|\\mathcal{F}^{t-1}) = \\mathcal{N}(\\mathcal{F}_j; \\sqrt{1 - \\beta_t^\\tau}\\mathcal{F}^{\\tau^{t-1}}, \\beta_\\tau I),\nwhere \\tau represents the diffusion steps, \\mathcal{N} denotes the normal distribution, \\beta_t indicates the noise level at step t, and I refers to the identity matrix. The reverse denoising process is to recover the origin edge orders from the pure noise, with the guidance of the final topology structure. In each step, the noised to be removed from the noisy data is predicted by the denoising networks. The denoising networks used in the reverse process are graph transformers [37]. The construction of the prediction model during the denoising process is shown in Fig. 6. In the denoising process, node features serve as guidance to direct the data generation, remaining unchanged throughout. In each step of the denoising process, the noisy edges and the node features used as guidance are fed into the graph transformer as the node inputs and edge inputs, and the small noise that needs to be removed from the noisy edges in a single step is output. The computation of the reverse denoising process is defined as:\np_\\theta(\\mathcal{F}^{t-1}|\\mathcal{F}^t,\\mathcal{C}_R) = \\mathcal{N}(\\mathcal{F}^{t-1}; \\mu_\\theta(\\mathcal{F}^t, t, \\mathcal{C}_R), (1 - \\alpha_t^\\tau)I), (3)\nwhere\n\\mu_\\theta(\\mathcal{F}^t, t,\\mathcal{C}_R) = \\frac{1}{\\sqrt{\\alpha_t}}(\\mathcal{F}^t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(\\mathcal{F}^t, t, \\mathcal{C}_R)), (4)\n\\alpha_t = 1 - \\beta_t, and \\bar{\\alpha}_t = \\prod_{i=1}^t\\alpha_i. Here, \\epsilon_\\theta(\\mathcal{F}^t,t,\\mathcal{C}_R) is the noise predicted by \\theta based on the noisy state \\mathcal{F}^t, diffusion step t and the spatial characteristics \\mathcal{C}_R of the topology of the node\nThe denoising networks are trained to minimize the reconstruction loss between the predicted noise and the true noise. The loss function we adopted is the same as denoising diffusion probabilistic models (DDPMs) [38], which is the mean squared error between the predicted noise and the true noise. The loss function is defined as:\nL = E_{t,\\epsilon\\sim\\mathcal{N}(0,I)} [\\|\\epsilon - \\epsilon_\\theta(\\mathcal{F}^t, t, \\mathcal{C}_R)\\|^2] (5)\nwhere |||| denotes the L2 norm.\nWhen the model is trained, we can sample the temporal networks with similar distribution as the original dataset by the graph diffusion model. First, a pure Gaussian noise is sampled and then the denoising networks iteratively predict the noise to be removed, and the ordered edges will be obtained from the weights of the sampled network gradually. The generated temporal networks are used to augment the original dataset for the edge order prediction model."}, {"title": "V. EXPERIMENTS", "content": [{"title": "A. Datasets", "content": "This study employs 10 real-world datasets from five distinct categories: Protein-Protein Interaction (PPI) Networks, World Trade Web (WTW), Collaboration Networks, Animal Networks, and Transportation Networks. These datasets, covering diverse domains, provide a solid foundation for evaluating the proposed methods. The details of the datasets are summarized in Table I, and the descriptions are as follows:\n\u2022 PPI Networks: This category includes five networks: Fungi, Human, Fruit, Worm, and Bacteria. Each node represents a protein, while edges denote interactions between proteins. The datasets capture the first occurrence of each edge to distinguish the order in which interactions are established. These networks are particularly valuable for studying temporal evolution in biological systems.\n\u2022 WTW: The WTW dataset represents global trade relations between countries from 1997 to 2013. Nodes correspond to countries, and edges signify bilateral trade relationships. The generation time of an edge reflects when trade between two countries was first recorded. This dataset highlights the evolving dynamics of global trade.\n\u2022 Collaboration Networks: We use the Thermody collaboration network, constructed from publication data of the American Physical Society. Nodes represent researchers, and edges denote coauthorship relationships. The earliest collaboration between two authors determines the generation time of an edge. This dataset provides insight into academic collaboration patterns.\n\u2022 Animal Networks: The Weaver network is based on 10 months of social interaction data from colonies of weaver birds in South Africa. Nodes represent individual birds, and edges denote interactions. To improve data density and reduce sparsity, the raw data is aggregated into 8 snapshots, capturing the temporal dynamics of social behavior.\n\u2022 Transportation Networks: Two transportation networks are used: Airplane and Coach. In the Airplane network, nodes are airports, and edges represent flight connections, while in the Coach network, nodes are coach stations, and edges denote transport links. The generation time of an edge is the earliest recorded connection between two nodes. The data is aggregated into daily snapshots, resulting in 5 snapshots for Airplane and 4 snapshots for Coach.\nThese datasets, spanning diverse domains such as biology, economics, collaboration, animal, and transportation, provide comprehensive support for the experiments in this study."}, {"title": "B. Baseline Models", "content": "To evaluate the effectiveness of our proposed method, we compare it against a set of state-of-the-art baseline models. These models are designed to explore the relationship between node, edge, and network features and the edge weights within a network. The baseline models fall into three categories and are described as follows:\n\u2022 Random Forest (RF) [44]: A tree-based machine learning algorithm that uses node and edge-level features to predict edge weights. Its robustness and ability to handle non-linear feature interactions make it a strong benchmark.\n\u2022 TIGGER [22]: A temporal generative model that combines synthetic network generation with node embeddings to capture dynamic relationships and predict edge weights over time.\n\u2022 NetGAN [45]: A generative adversarial network (GAN)-based model designed to recreate realistic network structures by learning the patterns of walks in the graph. The model is adapted here to generate networks with weighted edges.\n\u2022 Digress [46]: A state-of-the-art method for network generation, utilizing a discrete denoising diffusion model to directly operate on graph structures while preserving their sparsity and topology. By employing a Markovian noise process and a graph transformer for reconstruction, Digress sets a new benchmark in generating large-scale graphs."}, {"title": "C. Enhancing Model Performance through Joint Training and Data Augmentation", "content": "1) Impact of Joint Training: Table II provides a detailed comparison of model performance under different training"}, {"title": "E. Comparison of Generated Network Quality", "content": "To evaluate the effectiveness of our augmentation method compared to baseline approaches, we analyzed the discrepancy between generated networks and their corresponding real networks. The primary metric used for this evaluation was the Normalized Root Mean Square Error (NRMSE), which measures the difference between the generated edge generation times and the real timestamps.\nTable IV presents the NRMSE results for networks generated using our method and the baseline approaches. Our"}, {"title": "F. Limited Transferability of Single-Network Trained Models", "content": "Table V illustrates the performance of models trained on one network and tested on others. The results highlight two key findings: First, models perform well on the same network they are trained on. Diagonal entries, such as Airplane (0.9915) and Fruit (0.8589), demonstrate the ability to accurately capture the relationship between network features and edge generation times within the same network. Second, the models often fail when transferred to different networks. As seen in the underlined results, such as Thermody on Fruit (0.4787) and Fruit on Worm (0.5369), accuracies below 0.6 indicate poor transfer performance in binary classification tasks. This inconsistency reflects the complexity of real-world network mechanisms and the challenge of determining whether two networks are compatible for transfer.\nIn summary, models trained on a single network are unstable when applied to new networks. While some networks show decent transfer performance, others exhibit significant failures, highlighting the limitations of single-network training for generalizable predictions."}, {"title": "G. Similarity Between Augmented and Original Networks", "content": "This section evaluates the structural and dynamic similarity between augmented and the original networks. We employ"}]}, {"title": "VI. CONCLUSION", "content": "Our work focuses on predicting edge generation times from given network structures, which aids in understanding network evolution and forecasting future developmental trends. Existing methods are capable of predicting the edge generation sequence for the remaining edges of a temporal network when part of the edges' generation process is known. However, these methods often fail when applied to static networks. In real-world scenarios, a significant number of static networks, which lack timestamps, need to have their evolutionary processes reconstructed. To address this issue, we use the CPNN framework, which leverages the fusion of multiple temporal networks to train a model that can associate network structure with edge generation times across different networks. This approach achieves stable and improved accuracy in the prediction of evolution for unseen static networks. Furthermore, to further enhance the model's stability and performance, we adopt the TopoEvoDiff generation model to produce additional temporal networks. By integrating real temporal networks with the generated ones for training, we achieve further improvements in the model's prediction accuracy."}, {"title": "APPENDIX", "content": [{"title": "A. Equivalence Between Predicting Edge Generation Times and Pairwise Orderings", "content": "In this section, we demonstrate the equivalence between predicting the exact generation times of network edges and predicting their pairwise order of generation. Consider a network with M edges generated sequentially. The normalized generation time sequence is denoted as a = (a1,a2,...,\u0430\u043c), where ai = represents the normalized position of edge i in the sequence, with larger values indicating later generation times. The goal is to reconstruct the sequence a, either by directly estimating ai or by predicting the pairwise order of edges and deriving a from these comparisons.\nFor a pair of edges i and j, the pairwise score Uij is defined as:\n$U_{ij} = \\begin{cases} \\frac{x}{M} \\text{ if } a_i > a_j, \\\\ \\frac{1-x}{M} \\text{ if } a_i < a_j, \\end{cases}$\nwhere x denotes the probability of correctly predicting the order of a pair of edges. The expected value and variance of Uij are given by:\n$E(U_{ij}) = \\begin{cases} \\frac{x}{M} \\text{ if } a_i > a_j, \\\\ \\frac{1-x}{M} \\text{ if } a_i < a_j, \\end{cases}$\n$Var(U_{ij}) = \\frac{x(1-x)}{M^2}$\nUsing the pairwise scores, the total score ui for edge i is calculated as:\n$U_i = \\sum_{j=1,j\\neq i}^{M}W_{ij}$\nThe expected score of edge i is determined by summing its comparisons with all other edges. Specifically, comparisons with the first i - 1 edges contribute \\frac{x}{M}(i - 1), while comparisons with the remaining M \u2013 i edges contribute \\frac{1-x}{M}(M-i).\nThe total expected score is therefore:\n$E(u_i) = \\frac{x}{M}(i - 1) + \\frac{1-x}{M}(M - i) = \\frac{x}{M} (i - 1) + \\frac{1-x}{M}(M - i) = \\frac{2x - 1}{M}i + \\frac{1-x}{M}$\nThis expression shows that E(uz) is a linear function of the edge index i. The scores are evenly distributed over the interval [1 - x, x], allowing u\u2081 to serve as a mean-field approximation for ranking edges. The normalized position \u00e2\u2081 of edge i in the reconstructed sequence can be calculated as:\n$\\hat{a_i} = \\frac{U_i}{\\frac{(2x-1)}{M}}$\nThe variance of ai can be derived as:\n$Var(\\hat{a_i}) = \\frac{Var(U_{ij})}{(2x - 1)^2} = \\frac{x(1-x)}{(2x - 1)^2 M}$\nand the standard deviation is:\n$Std(\\hat{a_i}) = \\sqrt{\\frac{x(1-x)}{(2x - 1)^2 \\sqrt{M}}}$\nThe expected value of ai matches the true value ai, demonstrating that the reconstructed sequence is unbiased:\n$E(\\hat{a_i}) = a_i$.\nThe overall theoretical error in reconstructing the sequence is equivalent to the standard deviation of the position estimates:\n$E_{theory} = \\sqrt{\\frac{x(1-x)}{(2x - 1)^2 \\sqrt{M}}}$\nThis theoretical error decreases as the pairwise accuracy x approaches 1 or as the number of edges M increases. However, the error diverges as x approaches 0.5, emphasizing the importance of achieving reliable pairwise predictions. This equivalence highlights that predicting pairwise order is sufficient to reconstruct the temporal sequence of edges, offering a simpler yet equally effective alternative to directly predicting generation times."}, {"title": "B. Reconstructing Edge Generation Order", "content": "The following algorithm reconstructs the global edge generation order from pairwise comparison results. The pairwise comparison matrix P contains the probabilities Pij, where Pij indicates the likelihood that edge i was generated before edge j. The algorithm computes a score for each edge based on these pairwise probabilities and then sorts the edges by their scores to derive the global order."}, {"title": "C. Validation of Augmentation Strategy in Separate Training", "content": "This section evaluates the effectiveness of combining a single original network with an augmented network in predicting edge generation times for other networks. The experiments were conducted on three types of generated datasets, and the detailed descriptions of these datasets are as follows:\n\u2022 Barab\u00e1si-Albert (BA) model: The BA model generates networks through preferential attachment. Starting with a small fully connected graph, each new node added to the network connects to an existing node with probability p x k, where k is the degree (number of connections) of the existing node. This model reflects the power-law degree distribution found in many natural networks, where a few nodes dominate with many connections.\n\u2022 Popularity-similarity-optimization (PSO) model: The PSO model introduces a similarity space for nodes, where"}]}]}