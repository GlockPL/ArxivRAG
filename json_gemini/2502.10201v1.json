{"title": "Prediction Hubs are Context-Informed Frequent Tokens in LLMs", "authors": ["Beatrix M. G. Nielsen", "Iuri Macocco", "Marco Baroni"], "abstract": "Hubness, the tendency for few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first show, theoretically, that the only representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appeareance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. On the other hand, when other distance computations involving LLM representations are performed, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. In summary, our work highlights, on the one hand, how hubness, while omnipresent in high-dimensional spaces, is not always a negative property that needs to be mitigated, and, on the other hand, it shows that various widely-used LLMs have developed a guessing strategy that consists in constantly assigning a high probability to frequent tokens.", "sections": [{"title": "Introduction", "content": "Hubness is a phenomenon which occurs in high-dimensional data (Radovanovic et al., 2010), where some data points (the hubs) are in the k nearest neighbours of many other points while most points (the anti-hubs) are in the k nearest neighbours of few or no other points. Hubness has been found in many different types of data: for example in time-series, biology and image processing (Toma\u0161ev et al., 2011, 2015) and, in relation to text, in bag-of-words embeddings (Radovanovic et al., 2010; Schnitzer et al., 2012), dense word embeddings (Dinu and Baroni, 2014), dense sentence embeddings (Nielsen and Hansen, 2024) and cross-modal embeddings (Bogolin et al., 2022). Hubs arise due to intrinsic properties of certain distance measures applied to high-dimensional spaces, and they are typically considered a nuisance, as they obfuscate the genuine semantic landscape of the data of interest. Consequently, there is a general interest in techniques to reduce the hubness of a representation space (see for instance Feldbauer and Flexer (2019)).\nAutoregressive large language models (LLMs) also trade in high-dimensional representations, and it is thus natural to ask whether hubs emerge in distance computations in LLMs. This is the question we answer in this study. In order to address it, it is fundamental to distinguish between the comparison operations a model is effectively performing when engaging in next-token prediction and distance-based comparisons we might decide to compute from its representations.\nConcerning the distance-based comparisons actually performed by a standard autoregressive transformer-based LLM (Elhage et al., 2021), we note that the model prediction is accomplished through the softmaxed dot product between a context representation and each row of the unembedding matrix. This operation effectively determines a rank over the whole token vocabulary of a model (typically made up of thousands of elements), and it can be seen as a distance-based measure that could be affected by nuisance hubs."}, {"title": "Related Work", "content": "Radovanovic et al. (2010) showed the ubiquity of hubs in many different kinds of datasets. Hubness is a cause of concern, as it can negatively impact many common tasks in data analysis and machine learning, such as regression, classification, outlier detection and clustering. Hubness was also shown to hinder the performance of nearest-neighbour algorithms in speech recognition, recommendation and multimedia retrieval (see Feldbauer and Flexer (2019) and references therein). Problematic hubness also occurs in distributed text representations analogous to those produced by a LLM. For example Dinu and Baroni (2014), Smith et al. (2017), Lample et al. (2018), Huang et al. (2020) and Nielsen and Hansen (2024) studied hubness in word and text embeddings, while Bogolin et al. (2022), Wang et al. (2023) and Chowdhury et al. (2024) looked at hubness in multimodal language models and cross-modal retrieval.\nGiven the problems posed by hubs, various hubness reduction methods have been proposed, for example Local Scaling (Zelnik-Manor and Perona, 2004), Mutual Proximity (Schnitzer et al., 2012), Globally Corrected Rank (Dinu and Baroni, 2014), Inverted Softmax (Smith et al., 2017), Cross-domain Similarity Local Scaling (Lample et al., 2018), Hubness Nearest Neighbor Search (Huang et al., 2020), Querybank Normalisation (Bogolin et al., 2022), DBNorm (Wang et al., 2023), Dual Inverted Softmax (Wang et al., 2023), F-norm (Nielsen and Hansen, 2024) and Nearest Neighbor Normalization (Chowdhury et al., 2024). These methods have been systematically compared by Feldbauer and Flexer (2019) and Nielsen and Hansen (2024), among others.\nAs shown by the plethora of hubness reduction techniques, the focus has so far been on mitigating hubness, with little attention devoted to the question of whether hubness is actually always a nuisance phenomenon to be mitigated."}, {"title": "Theoretical preliminaries", "content": "We first define the k-occurrence, $N_k$, as in (Radovanovic et al., 2010). Given a set of points, the k-occurrence of a specifix point x, $N_k(x)$, is the number of points for which x is in the k-nearest neighbours. We define hubs as points, h, with high k-occurrence, i.e., where $N_k(h)$ is large. To get a sense of which values of $N_k(x)$ should be considered large, we can analyze the distribution of the"}, {"title": "Hubness and concentration of distances", "content": "Concentration of distances happens when the difference between the largest and smallest distance to a point goes to zero as the dimension increases. Necessary and sufficient conditions for this to happen have been presented in Beyer et al. (1999); Durrant and Kab\u00e1n (2009). When concentration of distances occurs, for every query point, we have that every other point is almost equally far away, see Fig. 1.\nA first effect of the concentration of distances is that, while every point will, trivially, still have a nearest neighbour, just adding a small amount of noise is likely to change which points are the closest. Another consequence is that, in high dimension, all points will be close to lying on a hypersphere, and be quite sparsely distributed. If we take a point which is slightly closer to the mean of the data than most other points, then this point will now be the closest neighbour of many other points (although it is still quite far away from everything), i.e., this point will be a hub.\nTherefore, if we are attempting to compare high-dimensional representations using a distance measure which exhibits concentration of distances, we will get that most representations are far away from each other. However, a few hubs will be the nearest neighbours of many other representations, with no guarantee that they are close in any meaningful sense. We call this kind of hubs, solely arising due to concentration of distances, nuisance hubs."}, {"title": "Probability distance in LLMs and concentration of distances", "content": "When comparing the representations of LLMs, it is common to use Euclidean distance or cosine similarity, which is equivalent to normalized Euclidean distance in terms of neighbour ranking. However, Euclidean distance is affected by concentration of distances (Aggarwal et al., 2001). We thus expect to find nuisance hubs when using it to compare representations.\nDoes this mean that LLMs are adversely affected by hubness? As discussed in the introduction, models are not using Euclidean-distance-based comparisons as part of their inner workings. They are trained instead to compare contexts with possible vocabulary items and give the most likely next items a high probability. We can interpret this as a dissimilarity measure, which we call probability distance, by using 1 \u2013 p(y | x), where p(y | x) is the probability the model associates to item y given the context x. In this way, we construct neighbourhoods for each context, with the closest items being the ones which are most likely.\nThe following theorem shows that, when using probability distance, we do not get concentration of distances unless the probabilities are uniform.\nTheorem 1. Let $x_i \\in X$ be a data point. Let $y_j$, $j\\in \\{1, ..., v\\}$, be the possible labels of points from X, and let p(yj|x) be the probability of label $y_j$"}, {"title": "Experiments", "content": "We experiment with five different autoregressive LLMs, namely OPT-6.7B (Zhang et al., 2022), Llama-3-8B (Meta, 2024), Pythia-6.9B (Biderman et al., 2023), OLMo-7B (Groeneveld et al., 2024), and Mistral-7B (Jiang et al., 2023), hereon referred to as Opt, Llama, Pythia, Olmo, and Mistral, respectively. As input to the models, we use the 3 datasets made available by Cheng et al. (2025). Each of them consists of 50K sequences, or contexts, as we will call them, of 20 orthographic tokens randomly extracted from Bookcorpus (Zhu et al., 2015), Pile10k (Gao et al., 2020) and WikiText-103 (Merity et al., 2017), respectively. Note that these contexts start and end at random points in a text (in particular, the last token is not necessarily a punctuation mark). In order to estimate domain-specific token frequency distributions, we use the full corpora the contexts were extracted from.\nTo measure hubness, we set k = 10 and define a point x as a hub if it has $N_k(x) \\geq 100$. That is, a point is a hub if it is in the 10 nearest neighbours of 10 times more points than we would expect if the"}, {"title": "Probability distance in LLMs", "content": "In this section, we first confirm that the probability distances computed by LLMs do not exhibit concentration of distances. We then show that, despite this, all tested LLMs are characterized by high hubness. We find however that their hubs correspond to context-dependent frequent tokens, that tend to be reasonable prediction candidates.\nFig. 2 shows, for Pythia and Pile10k, that there is no concentration of distances, as predicted by Theorem 1. This fact is confirmed for the other models in Appendix E.\nGiven the lack of concentration of distances,"}, {"title": "Emergence of frequency-sensitive prediction hubs during training", "content": "Having established that hubs in LLMs are the product of a sensible token prediction heuristic, we"}, {"title": "Comparing contexts or vocabulary items with Euclidean distance", "content": "Having shown that the probability distance measure computed by LLMs during next token prediction is not affected by nuisance hubs, we turn to other comparisons that, while not relevant to LLM generation, might arise in LLM analysis or adaptation. In particular, one might want to compute similarities between LLM representations of sequences or vocabulary entries for interpretability purposes or for specific downstream tasks (e.g., a task that requires measuring the similarity between two sentences, represented by their last-token activation vectors). In these cases, it is natural to use Euclidean distance or normalized Euclidean distance (or the rank-equivalent cosine) to compare representations. As we mentioned above, these measures are affected by concentration of distances given various underlying distributions (Aggarwal et al., 2001), and we thus might observe the rise of nuisance hubs. We present here examples using Euclidean distance; normalized Euclidean and full results are in appendices F and G.\nStarting with distance between context represen-"}, {"title": "Conclusion", "content": "We explored the phenomenon of hubness in autoregressive language models. We first observed that the only representation comparison performed by the model that could be affected by hubs consists in"}, {"title": "Limitations", "content": "\u2022 The theoretical result that probability distance does not entail concentration of distances is general. However, the empirical finding that hubs reflect context-dependent frequency distributions only holds for the models we experimented with, and it should be extended to other model families and sizes.\n\u2022 We established that, at least for the models we considered, prediction hubs correspond to context-dependent frequent tokens, and, at least in Pythia, this is an emergent phenomenon during training. We still lack a causal understanding of how these prediction hubs come about.\n\u2022 We found that, for 3/5 models, Euclidean distance applied to unembedding matrix representations does not lead to concentration of distances, although it still leads to nuisance hubs. The nature of the distance distributions of these models and the reason why they lead to nuisance hubs will have to be studied in future work."}, {"title": "Ethics Statement", "content": "The inner workings of language models are still largely unknown. This makes their increasingly common deployment in a variety of settings essentially unreliable and potentially harmful. Our paper constitutes a small contribution towards a better understanding of how language models work, and hence, ultimately, towards increasing their safety."}]}