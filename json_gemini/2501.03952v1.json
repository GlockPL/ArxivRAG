{"title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States", "authors": ["Jurgita Kapo\u010di\u016bt\u0117-Dzikien\u0117", "Toms Bergmanis", "M\u0101rcis Pinnis"], "abstract": "Although large language models (LLMs) have transformed our expectations of modern language technologies, concerns over data privacy often restrict the use of commercially available LLMs hosted outside of EU jurisdictions. This limits their application in governmental, defence, and other data-sensitive sectors. In this work, we evaluate the extent to which locally deployable open-weight LLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian. We examine various size and precision variants of the top-performing multilingual open-weight models, Llama 3, Gemma 2, Phi, and NeMo, on machine translation, multiple-choice question answering, and free-form text generation. The results indicate that while certain models like Gemma 2 perform close to the top commercially available models, many LLMs struggle with these languages. Most surprisingly, however, we find that these models, while showing close to state-of-the-art translation performance, are still prone to lexical hallucinations with errors in at least 1 in 20 words for all open-weight multilingual LLMs.", "sections": [{"title": "Introduction", "content": "Since the fall of 2022, OpenAI and other big tech companies have transformed LLMs from an obscure technology little known outside the academic circles to a major household name. Key to this was the LLMs' ability to perform tasks specified in free-form instructions, making them excel as NLP tools. Furthermore, these models can learn during inference from relevant examples provided as inputs, making them adaptable to new requirements or even tasks. Moslem et al. (2023) showed that in such a setting, GPT-3, provided with relevant translation examples, outperforms machine translation systems of major companies, including Google, DeepL, and ModernMT.\nHowever, data privacy concerns often constrain the use of commercially available LLMs hosted outside EU jurisdiction, limiting their application in governmental, defence, and data-sensitive private sectors. Fine-tuning and operational deployment of adapted models can incur prohibitive costs in the case of commercially available LLMs, emphasizing the need for sovereign AI solutions-locally deployable alternatives that ensure security, control, and compliance. Recently, many powerful alternatives to the commercially available online LLMs have emerged (Jiang et al., 2023; Dubey et al., 2024; Team et al., 2024; Mesnard et al., 2024; Abdin et al., 2024). Although many of these LLMs officially support only a handful of languages with a large speaker base, their training data often incorporate texts from many other languages. Therefore, in practice, these languages receive some degree of support. However, the extent to which these languages are supported, to the best of our knowledge, still needs to be evaluated.\nIn this work, we aim to answer the question of to what extent, if at all, several popular open-weight models support Lithuanian, Latvian, and Estonian. All three languages have relatively small speaker bases and thus are unlikely to be focal points of major multilingual open-weight LLMs. We examine variants of Meta's Llama 3, Google's Gemma2, Mistral's NeMo, and Microsoft's Phi3 in their performance in multiple-choice question answering (MCQA) and machine translation (MT). We also manually assess the text"}, {"title": "Experimental Setting", "content": "We evaluate LLMs on multiple-choice question answering, machine translation, and text generation quality in open-ended question answering. While our experiments focus on the model performance for three languages-Lithuanian, Latvian, and Estonian-each with a speaker base under 3 million, we also include results for Czech and English for comparison purposes. We have chosen to evaluate models that consistently appear on various leaderboards. Specifically, we assess the 8.03B and 70.6B parameter versions of Llama 3 and Llama 3.1, as well as the 3.21B parameter version of Llama 3.2 (Dubey et al., 2024) from Meta; the 9.24B and 27.2B parameter versions of Gemma 2 (Team et al., 2024; Mesnard et al., 2024) by Google; the 3.8B and 14B versions of Phi 3 by Microsoft (Abdin et al., 2024); and the 12.2B parameter NeMo by Mistral AI (Jiang et al., 2023). To provide context for our experiments, we include online models by OpenAI such as GPT-3.5 Turbo and GPT-40 (OpenAI et al., 2024) and DeepL machine translation systems. In experiments assessing the quality of Lithuanian text generation, we incorporate the Lithuanian language-specific fine-tuned versions of Llama 2 (Touvron et al., 2023) with 7B and 13B parameters, developed by Neurotechnology \u2013 Lt-Llama 2 (Nakvosas et al., 2024).\nWe run LLMs on our local hardware using the default inference parameters of the Ollama platform\u00b2, which offers several levels of precision for model quantization: 4bit, 8bit, and full-precision 16bit. By default, we use 4bit precision in all our experiments, albeit at the cost of some performance degradation. We also evaluate the performance drop due to quantization by contrasting the results of quantized models with their full-precision counterparts.\nMachine Translation For MT experiments, we use the FLORES-200 benchmark dataset (Goyal et al., 2021; Costa juss\u00e0 et al., 2022), which comprises parallel sentences in over 200 languages. We use the devtest subset from FLORES-200, which contains 1,012 sentences. We test LLMs in a zero-shot inference scenario. We use the following English prompt to request text translations from the specified source and to the specified target language:\n\u201c{langa}: {sentencelanga}\nTranslate the above {langa} text into {langb}\n{langb}: \"\nThe translation and evaluation are performed at sentence-level; the inference is conducted in a single run for each test sentence. For automatic evaluation of MT quality, we use COMET4 (Rei et al., 2020, 2022) as it has been shown to have a higher correlation with human judgments than BLEU (Papineni et al., 2002) and to be more suitable for unrelated system comparison (Kocmi et al., 2024).\nMultiple-Choice Question Answering For MCQA experiments, we employ the Belebele dataset, a benchmark in multiple-choice machine reading comprehension (Bandarkar et al., 2024). This dataset pairs each question with a short passage from the FLORES-200 dataset. Each question includes four multiple-choice answers, with one being the correct option. The dataset consists of 900 questions involving 488 distinct passages, each linked to one or two related questions. We use LLMs in a zero-shot inference scenario. We use the following English prompt where \u201c{context}\u201d, \u201c{question}\u201d and \u201c{answer#}\u201d are in a specific language (Latvian, Estonian, etc.):\n\u201cThis is the context: '{context}'. This is the question: '{question}'. Here are the 4 candidate answers: '1) {answer\u2081}'; '2) {answer2}'; '3) {answer3}'; '4) {answer4}'. Report only the correct answer's ID (1, 2, 3, 4) using the mandatory JSON format: {answer_id :\u2033}.\u201d\nThe prompt explicitly requests the ID (e.g. '1', '2', '3', or '4') of the correct answer formatted in JSON. Our evaluation metric is accuracy."}, {"title": "Results and Discussion", "content": "MT evaluation results demonstrate the Gemma 2 family as the most capable open-weight model family. Gemma 2 27B emerges as the best locally deployable model, yielding COMET scores on par with OpenAI's GPT-3.5 Turbo and only marginally worse than GPT-40. Although specialised proprietary MT models like DeepL achieve the highest average score (0.91), freely available Gemma 2 models are not far off, with average COMET scores of 0.89 and 0.87 for the 27B and 9B versions, respectively. In this context, the Llama family has little to offer, with the Llama 3.0 and 3.1 70B parameter models surpassing the much smaller Gemma 2 9B only in two out of eight translation directions. Smaller models, like Llama's 3B and 8B versions and Mistral's 12B NeMo, show equally meagre results given the high performance of Gemma 2 9B. Lastly, the results of Phi 3 prove that these models have very little support for multilingualism.\nQuantisation impact on MT quality analysis reveals that while Llama models are negatively affected to some degree, the performance of full-precision models does not justify their use either. Increased inference time and memory requirements for the 70B model are too prohibitive unless several top-of-the-shelf enterprise-grade GPUs are available. However, the full precision 8B parameter Llama 3.1 still does not reach the performance of the Gemma 2 9B 4bit version (0.80 vs 0.87). Gemma 2 family models, on the other hand, show a statistically insignificant drop in translation performance when the 4bit version is used, suggesting that their architecture is very robust to quantization.\nWhile the current MT results provide valuable insights into LLM capabilities, future work could benefit from more fine-grained error analysis using frameworks like MQM (Multidimensional Quality Metrics) and ESA (Error Span Annotation). These approaches allow detailed classification of errors-such as those related to accuracy, fluency, and terminology, and help quantify their impact on text usability. Incorporating these methods could provide deeper insights into model limitations and guide targeted improvements, particularly for smaller languages like Lithuanian, Latvian, and Estonian.\nMCQA results show that the Gemma 2 27B parameter model outperforms GPT-3.5 Turbo across all languages, coming second only to OpenAI's flagship model, GPT-40. Notably, Gemma 2 27B achieves the highest accuracy among the open-weight models, outperforming Llama 3.1 and NeMo models for the Lithuanian, Latvian, Estonian, and Czech. The Phi models, however, perform poorly, particularly in non-English languages, and their results often fail to meet the required JSON output format, providing detailed responses instead of just answer IDs.\nQuantization impact on MCQA analysis unveils a similar picture as the analysis above for MT: Llama models are more sensitive to quantization, while Gemma 2 are more robust. As a result, Gamma 2 models show little performance degradation when much more efficient 4bit models are used. It's worth noting, however, that the accuracy drops because quantization differs depending on the amount of data each language has. Less spoken languages like Lithuanian, Latvian, and Estonian are affected more than English and Czech, for which overall results are better. For example, the Llama 3.1 70B model loses 0.06, 0.12, and 0.08 accuracy for Lithuanian, Latvian, and Estonian, respectively, but only 0.01 and 0.02 for English and Czech.\nText Generation Quality evaluation results show that most models produce more than one error per 100 words. The Lt-Llama 2 models, specifically fine-tuned for Lithuanian, are the exception, with an error rate of just 0.98% and no invented words. Among multilingual models, OpenAI's GPT-40 achieves strong performance, with 0.94 grammatically incorrect or incorrectly inflected words per 100 for Lithuanian and 1.52 for Latvian, while generating a very small number of invented words (0.31 and 0.56 per 100 for Lithuanian and Latvian, respectively). In contrast, Llama 3.1 models show significant shortcomings, with the highest frequency of grammatical errors: 8.01 per 100 for Lithuanian and 11.88 for Latvian. Additionally, Llama 3.1 generates a substantial number of invented words: 4.28 per 100 for Lithuanian and 3.87 for Latvian. Gemma 2 models perform considerably better, with 2.36 grammatical errors per 100 for Lithuanian and 4.10 for Latvian, and fewer invented words: 0.39 per 100 for Lithuanian and 1.45 for Latvian. These findings highlight clear quality differences among models. While Llama 3.1's high error rates make it unsuitable for most commercial applications, Gemma 2 strikes a better balance, approaching GPT-40's quality but still falling short. Notably, Lt-Llama 2 sets the strongest benchmark with near-perfect output, minimal grammatical errors, and no invented words. On average, users can expect at least one linguistic error in every 2-3 sentences from the best open-weight models like Gemma 2, or every sentence for models like Llama 3.1, unless further multilingual specialization becomes available.\nLesser-spoken Languages like Lithuanian, Latvian, and Estonian have less support in open-weight models compared to more populous languages such as Czech. These differences are more pronounced in smaller and lower-quality models, especially in tasks where models generate text in lesser-spoken languages (e.g., MT from English into Lithuanian). Comparatively good results for Czech suggest that these disparities are related to the amount of data each LLM has seen for each language during training, rather than factors such as the structural complexity of the language."}, {"title": "Conclusions", "content": "Our findings demonstrate that certain open-weight LLMs, such as the Gemma 2 family, achieve performance comparable to top-tier commercial products, such as general-purpose models like OpenAI's GPT-40 and specialized machine translation services like DeepL. This progress enables local, secure, and private solutions, supporting the development of sovereign AI for many language tasks in governmental, defence, and other data-sensitive private sectors. Nevertheless, unless specifically fine-tuned for languages like Lithuanian, most multilingual models are still surprisingly prone to lexical hallucinations, highlighting the need for 1) high-quality language data for languages of the Baltic states and 2) research on language-specialized LLMs."}]}