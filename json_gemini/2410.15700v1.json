{"title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale LEAN Problems", "authors": ["Zijian Wu", "Suozhi Huang", "Zhejian Zhou", "Huaiyuan Ying", "Jiayu Wang", "Dahua Lin", "Kai Chen"], "abstract": "Large Language Models (LLMs) have emerged as powerful tools in mathematical theorem proving, particularly when utilizing formal languages such as LEAN. The major learning paradigm is expert iteration, which necessitates a pre-defined dataset comprising numerous mathematical problems. In this process, LLMs attempt to prove problems within the dataset and iteratively refine their capabilities through self-training on the proofs they discover. We propose to use large-scale LEAN problem datasets Lean-workbook for expert iteration with more than 20,000 CPU days. During expert iteration, we found log-linear trends between solved problem amount with proof length and CPU usage. We train a critic model to select relatively easy problems for policy models to make trials and guide the model to search for deeper proofs. InternLM2.5-StepProver achieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet, and Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the MiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus which shows a significant improvement compared to only 9.5% of problems proved when Lean-Workbook-Plus was released. We open-source our models and searched proofs at https://github.com/InternLM/InternLM-Math and https://huggingface.co/datasets/internlm/Lean-Workbook.", "sections": [{"title": "1 Introduction", "content": "Automated theorem proving has been a challenging topic in artificial intelligence which requires complex reasoning and a deep understanding of math. AlphaProof\u00b9 have demonstrated remarkable progress by achieving silver-medal performance on International Mathematical Olympiad problems using the LEAN 4 proof assistant, particularly excelling in number theory and algebra. AlphaProof's training regime, based on AlphaZero methodology encompasses 100 million formal mathematics problems-a scale that significantly surpasses previous efforts. Building on this advancement, our work leverages the Lean-workbook (Ying et al., 2024a), the largest open-source problem collection available, to conduct systematic expert iteration and analyze proving strategies at scale.\nOur extensive experimentation, consuming over 20,000 CPU days, yielded several key insights into the challenge of automated theorem proving:\n\u2022 Success Rate: Only 1.5% of CPU usage resulted in successful proofs or disproofs, highlighting the substantial difficulty of automated theorem proving."}, {"title": "2 Methods", "content": "Based on Lean-workbook-plus Ying et al. (2024a), one of the largest auto-formalized problem sets in Lean 4, we propose to exploit the potential of formal reasoning by expert iteration Anthony et al. (2017); Polu et al. (2022).\nExpert Iteration on Lean-Workbook-Plus For each statement in Lean-Workbook-plus, we try to prove or disprove it following Xin et al. (2024a). We search proofs based on best-first-search and critic-guided search via generating a tactic as an action. Starting with InternLM2-StepProver Wu et al. (2024), our latest model for formal reasoning, we have eliminated the cumbersome bootstrap process. Initially, we conduct a rapid scan of the entire Lean-workbook-plus dataset using a relatively small search budget (i.e. 10 iterations at most per problem and a time limit of 50 seconds). The discovered proofs are added to the training set, and the solved problems, along with their negated statements, are removed from the dataset. This process helps us identify statements that are inherently unprovable, thereby enhancing the efficiency of the iteration.\nWe then repeat this process in multiple rounds, gradually increasing the search budget for subsequent evaluations until a predefined upper bound (at most 2000 iterations and 3600 seconds per problem) is reached. After each round, we retrain our policy and critic models using an expanded set of successful proof trajectories. Since some founded proofs are ill-formed and contain many irrelevant proof steps with larger CPU consumptions. We continue to search for proofs for these problems and use discovered shorter and more learnable proofs to improve our models. After model iterations, we use the critic model to re-evaluate all unproved statements with scores and we only focus on search proofs on the top 50% of problems that the model is most likely to be solvable.\nUpdating Policy Model The traditional proofstep objective, used by GPT-f , generates a PROOFSTEP (a Lean tactic) given a GOAL (current Lean tactic state) and the current DECLARATION (the Lean theorem name to be proved). The actual prompt used by GPT-f includes an additional declaration field, i.e.,  DECL <DECLARATION> \\nGOAL <GOAL> \\nPROOFSTEP <PROOFSTEP>. However, such prompts, though easy to integrate with existing deployment frameworks, lack information regarding the previous proof contents. Hoping to improve the reasoning performance in deep search trees, we augment our prompt template with ongoing proof context. The format of the prompt is modified to include the previous tactics leading to the state in a field called PROOF_BEFORE. An example of the prompt template is shown in Fig.C. We train the policy model following the standard SFT style.\nUpdating Critic Model From our observation, using best-first-search with log-probability scores seldom leads to deep proofs (shown in Figure 1), and limits the proof ability of our model. Therefore, we decide to train a critic model Lample et al. (2022); Polu et al. (2022) to better guide our policy model for proof generation. The critic model (V) uses the proof state (s) as the input and outputs a scalar (V(s) \u2208 R). We train our critic model in a preference style which is similar to reward model training in RLHF (Ouyang et al., 2022) instead of binary targets (the state can be proved or not). We create two types of preference pairs in our training:"}, {"title": "3 Experiments and Results", "content": "As Tab. 1 shows, we are able to prove/disprove a total of 17.0% of the Lean-workbook-plus problems, making a noticeable improvement from we release Lean-workbook-plus. These proved and disproved statements and corresponding tactics and states have been released at https://huggingface.co/datasets/internlm/Lean-Workbook. We also unveiled more facts about the expert iteration process, including the distribution of the auto-formalized problem set, the efficacy of our method, etc.\nThe consumption of CPU/GPU resources. The search process involves a collaboration of CPU and GPU resources. Given the fixed amount of active GPUs and CPUs, the GPU time consumed is proportional to the total CPU time (in our case, approximately 1:11). In summary, we consumed approximately 21,364 CPU days throughout the entire expert iteration process. However, these search budgets are not uniformly distributed across all formalized problems. Easier problems are more likely to be solved in the early rounds of iteration, thereby ceasing to consume search resources in later rounds. The search consumption of each problem is a key indicator of the distribution of problem difficulty and can provide valuable insights for further scaling. In our case, we selected the CPU time consumed per successful proof as an estimate of resource consumption expectations.Consider"}, {"title": "4 Related Work", "content": "Automated Theorem Proving does not have a unified approach. The mainstream paradigm is training a language model on tuples of (proof state, next tactic), followed by a tree search to find proofs . Another line of the work is training to auto-regressive generate a whole proof based on a theorem statement from the model itself or translating from human informal proof (Jiang et al., 2022; Wu et al., 2022; Wang et al., 2024). No matter learning with which paradigms, most methods rely on expert iteration (Anthony et al., 2017) for model self-improving."}, {"title": "5 Conclusion", "content": "In this paper, we introduce InternLM2.5-StepProver which improves its automated theorem-proving ability via large-scale expert iteration and achieves state-of-the-art on multiple benchmarks. We use over 20,000 CPU days to search for proofs on Lean-Workbook-Plus and prove or disprove 17% of them. We analyze the difficult distribution of Lean-Workbook-Plus based on the proof lengths and CPU usage. We observe a log-linear trend between proof length/CPU usage with proof problem amount. To prove more problems in Lean-Workbook-Plus, we need to put more effort into search budgets and improve the search algorithm efficacy. Our findings provide concrete guidance for future developments in automated mathematical reasoning."}, {"title": "Limitations", "content": "This work is mainly focused on context-level math problems and pays less attention to other automated theorem-proving scenarios. We do not have a stable metric to measure critic models which makes iteration of critic models difficult."}, {"title": "A Training details", "content": "Policy Model InternLM2.5-StepProver's policy model is built upon InternLM-math-plus-7B Ying et al. (2024b). We used the same training setting when we performed the expert iteration process: We used a global batch size of 512 and a learning rate of 2 \u00d7 10\u22125. We fine-tuned for 2 epochs to obtain the SFT model. For the learning rate, we used a warm-up in the first 3% steps, followed by a cosine schedule decaying to zero. The entire expert iteration process generated 2.19 billion tokens of data, with the final iteration taking approximately 14 hours on 32 A800 GPUs.\nCritic Model We initialize the critic model from internlm2-chat-1_8b-sft\u00b2(Cai et al., 2024) and fine-tune it for one epoch. We create preference pairs among MiniF2F-valid, Mathlib (mathlib Community, 2020), and Lean-Workbook-Plus (Ying et al., 2024a) using best-first-search. The final-round data includes 454K pairs where we have removed duplicate pairs and reduced the number of pairs containing no_goals to 10% of their original count. We train critic"}, {"title": "B Case studies", "content": "Here we list interesting cases proved by InternLM2.5-StepProver from different datasets.\nNatural Language problem: For natural numbers m and n, if (mn + m + n) mod 6 = 4, then 12 | mn.\nInternLM2.5-StepProver successfully addresses the problem by imposing constraints on the range of variables and then solving it directly using enumeration techniques. This example illustrates the distinction between formal and informal reasoning styles.\nNatural Language problem: If \u221ax + \u221ax + \u221ax + \u221ax + \u2026\u2026 = 9, find x. Show that it is 72.\nIt is an interesting case to show how InternLM2.5-StepProver solves problem whose formalized version is significantly harder than the informal one. The informal solution of this problem is not rigorous, which jumps from the equation \u221ax + \u221ax + \u221ax + \u221ax + \u2026\u2026 = 9 to \u221ax + 9 = 9, involving a substitution that is intuitional but risky. The formalized version of this problem uses series"}, {"title": "C Model Chat Templates", "content": "Here are example inputs of our policy and critic models."}]}