{"title": "ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective Image Enhancement", "authors": ["Eashan Adhikarla", "Kai Zhang", "John Nicholson", "Brian D. Davison"], "abstract": "Low-light image enhancement remains a challenging task in computer vision, with existing state-of-the-art models often limited by hardware constraints and computational inefficiencies, particularly in handling high-resolution images. Recent foundation models, such as transformers and diffusion models, despite their efficacy in various domains, are limited in use on edge devices due to their computational complexity and slow inference times. We introduce ExpoMamba, a novel architecture that integrates components of the frequency state space within a modified U-Net, offering a blend of efficiency and effectiveness. This model is specifically optimized to address mixed exposure challenges-a common issue in low-light image enhancement\u2014while ensuring computational efficiency. Our experiments demonstrate that ExpoMamba enhances low-light images up to 2-3x faster than traditional models with an inference time of 36.6 ms and achieves a PSNR improvement of approximately 15-20% over competing models, making it highly suitable for real-time image processing applications. Model code is open sourced at: github.com/eashanadhikarla/ExpoMamba.", "sections": [{"title": "1. Introduction", "content": "Enhancing low-light images is crucial for applications ranging from consumer gadgets like phone cameras (Liba et al., 2019; Liu et al., 2024) to sophisticated surveillance systems (Xian et al., 2024; Guo et al., 2024; Shrivastav, 2024). Traditional techniques (Dale-Jones & Tjahjadi, 1993; Singh et al., 2015; Khan et al., 2014; Land & McCann, 1971; Ren et al., 2020) often struggle to balance processing speed and high-quality results, particularly with high-resolution images, leading to issues like noise and color distortion in scenarios requiring quick processing such as mobile photography and real-time video streaming.\nLimitations of Current Approaches. Foundation models have revolutionized computer vision, including low-light image enhancement, by introducing advanced architectures that model complex relationships within image data. In particular, transformer-based (Wang et al., 2023b; Chen et al., 2021a; Zhou et al., 2023b; Adhikarla et al., 2024) and diffusion-based (Wang et al., 2023c;a; Zhou et al., 2023a) low-light techniques have made significant strides. However, the sampling process requires a computationally intensive iterative procedure, and the quadratic runtime of self-attention in transformers make them unsuitable for real-time use on edge devices where limited processing power and battery constraints pose significant challenges. Innovations such as linear attention (Katharopoulos et al., 2020; Shen et al., 2018; Wang et al., 2020), self-attention approximation, windowing, striding (Kitaev et al., 2020; Zaheer et al., 2020), attention score sparsification (Liu et al., 2021b), hashing (Chen et al., 2021c), and self-attention operation kernelization (Katharopoulos et al., 2020; Lu et al., 2021; Chen et al., 2021b) have aimed to address these complexities, but often at the cost of increased computation errors compared to simple self-attention (Duman Keles et al., 2023; Dosovitskiy et al., 2021). (More details can be found in Appendix A)\nPurpose. With rising need for better images, advanced small camera sensors in edge devices have made it more common for customers to capture high quality images, and use them in real-time applications like mobile, laptop and tablet cameras (Morikawa et al., 2021). However, they all struggle with non-ideal and low lighting conditions in the real world. Our goal is to develop an approach that has high image quality (e.g., like CIDNet (Feng et al., 2024)) for enhancement but also at high speed (e.g., such as that of IAT (Cui et al., 2022) and Zero-DCE++ (Li et al., 2021)).\nContributions. Our contributions are summarized as:\n\u2022 We introduce the use of Mamba for efficient low-light image enhancement (LLIE), specifically focusing on mixed exposure challenges, where underlit (insufficient brightness) and overlit (excessive brightness) exist in the same image frame.\n\u2022 We propose a novel Frequency State Space Block (FSSB) that combines two distinct 2D-Mamba blocks, enabling the model to capture and enhance subtle textural details often lost in low-light images.\n\u2022 We describe a novel dynamic batch training scheme to improve robustness of multi-resolution inference in our proposed model.\n\u2022 We implement dynamic processing of the amplitude component to highlight distortion (noise, illumination) and the phase component for image smoothing and noise reduction."}, {"title": "2. Exposure Mamba", "content": "Along the lines of recent efficient sequence modeling approaches (Gu & Dao, 2023; Zhu et al., 2024a; Wang et al., 2024), we introduce ExpoMamba, a model combining frequency state-space blocks with spatial convolutional blocks (Fig. 2). This combination leverages the advantages of frequency domain analysis to manipulate features at different scales and frequencies, crucial for isolating and enhancing patterns challenging to detect in the spatial domain, like subtle textural details in low-light images or managing noise in overexposed areas. Additionally, by integrating these insights with the linear-time complexity benefits of the Mamba architecture, our model efficiently manages the spatial sequencing of image data, allowing rapid processing without the computational overhead of transformer models.\nOur proposed architecture utilizes a 2D scanning approach to tackle mixed-exposure challenges in low-light conditions. This model incorporates a combination of U2 \u2013 Net (Qin et al., 2020) and M-Net (Mehta & Sivaswamy, 2017), supporting 2D sRGB images with each block performing operations using a convolutional and encoder-style SSM (x(t) \u2208 R \u2192 y(t) \u2208 R)\u00b9. The subsequent section provides detailed information about our overall pipeline."}, {"title": "2.1. Frequency State Space Block (FSSB)", "content": "We utilize the frequency state space block (FSSB) to address the computational inefficiencies of transformer architectures especially when processing high-resolution image or long-sequence data. The FSSB's motivation is in two parts; first, towards enhancing the intricacies that are unaddressed/missed by the spatial domain alone; and second, to speed deep feature extraction using the frequency domain. The FSS block (as in Fig. 3) initiates its processing by transforming the input image I into the frequency domain using the Fourier transform:\n$F(u, v) = \\int \\int I(x, y)e^{-i2\\pi(ux+vy)}dx \\cdot dy$                                                                                                                                                                                                                                                                                       (1)\nwhere, F(u, v) denotes the frequency domain representation of the image, and (u, v) are the frequency components corresponding to the spatial coordinates (x, y). This transformation allows for the isolation and manipulation of specific frequency components, which is particularly beneficial for enhancing details and managing noise in low-light images. By decomposing the image into its frequency components, we can selectively enhance high-frequency components to improve edge and detail clarity while suppressing low-frequency components that typically contain noise (Lazzarini, 2017; Zhou et al., 2022). This selective enhancement and suppression improve the overall image quality.\nThe core of the FSSB comprises two 2D-Mamba (Visual-SSM) blocks to process the amplitude and phase components separately in the frequency domain. These blocks model state-space transformations as follows:\n$h[t+1] = A[t]h[t] + B[t].x[t]$                                                                                                                                                                                                                                                                (2)\n$y[t] = C[t] h[t]$                                                                                                                                                                                                                                                                                                (3)\nHere, A[t], B[t], and C[t] are the state matrices that adapt dynamically based on the input features, and h[t] represents"}, {"title": "Amplitude and Phase Component Modeling.", "content": "Each component A(u, v) and P(u, v) undergoes separate but parallel processing paths, modeled by:\n$St+1 = A(St) + BF(xt), Yt = CSt$                                                                                                                                                                                                                                                              (5)\nwhere st denotes the state at time t, F(xt) represents the frequency-domain input at time t (either amplitude or phase),"}, {"title": "2.2. Multi-modal Feature Learning", "content": "The inherent complexity of low-light images, where both underexposed and overexposed elements coexist, necessitates a versatile approach to image processing. Traditional methods, which typically focus either on spatial details or frequency-based features, fail to adequately address the full spectrum of distortions encountered in such environments. By contrast, the hybrid modeling approach of \u201cExpoMamba\" leverages the strengths of both the spatial and frequency domains, facilitating a more comprehensive and nuanced enhancement of image quality.\nOperations in the frequency domain, such as the Fourier transform, can isolate and address specific types of distortion, such as noise and fine details, which are often exacerbated in low-light conditions. This domain provides a global view of the image data, allowing for the manipulation of features that are not easily discernible in the spatial layout. Simultaneously, the spatial domain is critical to maintaining the local coherence of image features, ensuring that enhancements do not introduce unnatural artifacts."}, {"title": "2.3. Dynamic Patch Training", "content": "Dynamic patch training enhances the 2D scanning model by optimizing its scanning technique for various image resolutions. In ExpoMamba, 2D scanning involves sequentially processing image patches to encode feature representations. We create batches of different resolution images where in a given batch the resolution is fixed and we dynamically randomize the different batch resolutions of input patches during training. This way the model learns to adapt its scanning and encoding process to different scales and levels of detail (Fig 5). This variability helps the model become more efficient at capturing and processing fine-grained details across different image resolutions, ensuring consistent performance. Consequently, the model's ability to handle mixed-exposure conditions is improved, as it can effectively manage diverse resolutions and adapt its feature extraction process dynamically, enhancing its robustness and accuracy in real-world applications."}, {"title": "3. Experiments and Implementation details", "content": "In this section, we evaluate our method through a series of experiments. We begin by outlining the datasets used, experimental setup, followed by a comparison of our method against state-of-the-art techniques using four quantitative metrics. We also perform a detailed ablation study (Appx E, Tab. 5) to analyze the components of our proposed method."}, {"title": "3.1. Datasets", "content": "To test the efficacy of our model, we evaluated ExpoMamba on four datasets: (1) LOL (Wei et al., 2018a), which has v1 and v2 versions. LOLv2 (Yang et al., 2020a) is divided into real and synthetic subsets. The training and testing sets are split into 485/15, 689/100, and 900/100 on LOLv1, LOLv2-real, and LOLv2-synthetic with 3 \u00d7 400 \u00d7 600 resolution images. (2) LOL4K is an ultra-high definition dataset with 3\u00d73,840 x 2,160 resolution images, containing 8,099 pairs of low-light/normal-light images, split into 5,999 pairs for training and 2,100 pairs for testing. (3) SICE (Cai et al., 2018) includes 4,800 images, real and synthetic, at various exposure levels and resolutions, divided into training, validation, and testing sets in a 7:1:2 ratio."}, {"title": "5. Conclusion", "content": "We introduced ExpoMamba, a model designed for efficient and effective low-light image enhancement. By integrating frequency state-space components within a U-Net variant, ExpoMamba leverages spatial and frequency domain processing to address computational inefficiencies and high-resolution challenges. Our approach combines robust feature extraction of state-space models, enhancing low-light images with high fidelity and achieving impressive inference speeds. Our novel dynamic patch training strategy significantly improves robustness and adaptability to real-world hardware constraints, making it suitable for real-time applications on edge devices. Experimental results show that ExpoMamba is much faster and comparably better than numerous existing transformer and diffusion models, setting a new benchmark in low light image enhancement."}, {"title": "Appendix", "content": "A. Related Work\nA.1. Conventional and Deep CNN-Based Methods\nTraditional methods for low-light image enhancement often rely on histogram equalization (HE) (Dale-Jones & Tjahjadi, 1993; Singh et al., 2015; Khan et al., 2014) and Retinex theory (Land & McCann, 1971; Ren et al., 2020). HE based methods aim to adjust the contrast of the image by uniformly distributing the pixel intensities, which can sometimes lead to overenhancement and noise amplification, which were later investigated more carefully by CegaHE (Chiu & Ting, 2016), UMHE (Kansal et al., 2018), etc. Retinex theory, which decomposes an image into illumination and reflectance, provides a more principled approach to enhancement but still faces limitations in complex lighting conditions.\nConvolutional Neural Networks (CNNs) have significantly advanced this field. Early works like LLNet (Lore et al., 2017) used autoencoders to enhance low-light image visibility. The SID (See-in-the-Dark) network (Chen et al., 2018b) leveraged raw image data for better enhancement by training on paired low-light and normal-light images. Other works in paired training include DSLR (Lim & Kim, 2020), DRBN (Yang et al., 2020b), KinD (Zhang et al., 2019a), KinD++ (Zhang et al., 2021b), MIRNet (Zamir et al., 2020), ReLLIE (Zhang et al., 2021a), DDIM (Song et al., 2020), SCI (Ma et al., 2022), RAUS (Liu et al., 2021a), Restormer (Zamir et al., 2022), CIDNet (Feng et al., 2024), LLFormer (Wang et al., 2023b), SNRNet (Lin et al., 2020), Uformer (Wang et al., 2022b), and CDEF (Valin, 2016). Methods like RetinexNet (Wei et al., 2018b), which decompose images into illumination and reflectance components, also show considerable promise but often struggle with varying lighting conditions."}, {"title": "A.2. Foundation Models in LLIE", "content": "Transformer Models. Such approaches have gained popularity for modeling long-range dependencies in images. LLFormer (Wang et al., 2023b) leverages transformers for low-light enhancement by focusing on global context, significantly improving image quality. Fourmer (Zhou et al., 2023b) introduces a Fourier transform-based approach within the transformer architecture, while IAT (Cui et al., 2022) adapts ISP-related parameters to address low-level and high-level vision tasks. IPT (Chen et al., 2021a) uses a multi-head, multi-tail shared pre-trained transformer module for image restoration. LYT-Net (Brateanu et al., 2024) addresses image enhancement with minimal computing resources by using YUV colorspace for transformer models. Despite their effectiveness, these transformer models often require substantial computational resources, limiting their practicality on edge devices.\nDiffusion Models. Diffusion models have shown great potential in generating realistic and detailed images. The ExposureD-iffusion model (Wang et al., 2023c) integrates a diffusion process with a physics-based exposure model, enabling accurate noise modeling and enhanced performance in low-light conditions. Pyramid Diffusion (Zhou et al., 2023a) addresses computational inefficiencies by introducing a pyramid resolution approach, speeding up enhancement without sacrificing quality. (Saharia et al., 2022) handles image-to-image tasks using conditional diffusion processes. Models like (Zhang et al., 2022) and deep non-equilibrium approaches (Pokle et al., 2022) aim to reduce sampling steps for faster inference. However, starting from pure noise in conditional image restoration tasks remains a challenge for maintaining image quality while cutting down inference time (Guo et al., 2023).\nHybrid Modelling. Hybrid models includes learning features in both spatial and frequency domain has been another popular area in image enhancement/restoration tasks. Mostly it has been explored in three sub-categories: (1) Fourier Transform (Yuan et al., 2024), Fourmer (Zhou et al., 2023b), FD-VisionMamba (Zheng & Zhang, 2024); (2) Wavelet Transform; (3) Homomorphic Filtering (). Such methods demonstrate that leveraging both spatial and frequency information can significantly improve enhancement performance.\nState-Space Models. Recent advancements reveal the efficacy of state space models (SSM) as a robust architecture in foundation model era for sequence modeling, offering a fresh perspective beyond conventional RNNs, CNNs, and Transformers. Pioneering this shift, the S4 (Gu et al., 2021) model demonstrated superior performance in managing long-range dependencies by employing the HiPPO matrix (Fu et al., 2022) to define state dynamics systematically. Initially introduced for audio processing, SSMs have emerged as a alternative, later expanded into language and vision domains for handling long-range model dependencies and temporal dynamics becoming a strong competitor for current transformer based methods. The V-Mamba architecture (Zhu et al., 2024b; Yang et al., 2024) combines state-space models with U-Net frameworks to capture detailed image aspects at multiple scales, proving effective in biomedical image segmentation."}, {"title": "B. The Importance of Inference Time over FLOPs in Real-World Applications", "content": "In our paper, we use inference time as a measure because inference time, unlike the abstract measure of FLOPs (Floating Point Operations Per Second), reflects actual performance in real-world applications, being influenced not only by hardware speed but also by model design and optimization.\nIn practical scenarios, wherein systems requiring real-time processing like autonomous vehicles and interactive AI applications, the agility of model inference directly impacts usability and user experience. Moreover, as inference constitutes the primary computational expense post-deployment, optimizing inference time enhances both the cost-effectiveness and the energy efficiency of AI systems. Thus, we focused on minimizing inference time, rather than merely reducing FLOPs, ensuring that AI models are not only theoretically efficient but are also pragmatically viable in dynamic real-world environments. We believe that this approach not only accelerates the adoption of AI technologies but also drives advancements in developing models that are both performant and sustainable."}, {"title": "C. Detailed Methodology", "content": "C.1. Proof of Phase Manipulation\nFor an image I(x, y), its Fourier transform is given by:\n$F(u, v) = \\int \\int I(x, y)e^{-i2\\pi(ux+vy)}dx dy$                                                                                                                                                                                                                                                                         (8)\nThis can be decomposed into amplitude A(u, v) and phase \u03c6(u, v):\n$F(u, v) = A(u, v)e^{i\\phi(u,v)}$                                                                                                                                                                                                                                                                                                                             (9)\nThe inverse Fourier transform, which reconstructs the image from its frequency representation, is:\n$I(x, y) = \\int \\int \u0391(u, v)e^{i\\phi(u,v)}e^{i2\\pi(ux+vy)} du dv$                                                                                                                                                                                                                                 (10)\nSuppose that the phase component \u03c6(u, v) is uniformly shifted by an angle \u2206, the new phase \u03c6' (u, v) = \u03c6(u, v) + \u0394\u03c6. The modified image I'(x, y) with this new phase is represented as:\n$I'(x, y) = \\int \\int \u0391(u, v)e^{i(\\phi(u,v)+\\Delta\\phi)}e^{i2\\pi(ux+vy)} du dv$                                                                                                                                                                                 (11)\nUsing Euler's formula $e^{i\\Delta\\phi}$ = cos(\u2206\u03c6) + i sin(\u2206\u03c6), the equation becomes:\n$I' =  \\int \\int \u0391(u, v)e^{i\\phi(u,v)} (cos(\\Delta\\phi) + i sin(\\Delta))e^{i2\\pi(ux+vy)} du dv$                                                                                                                                                                        (12)\nGiven that cos(\u0394\u03a6) and sin(\u0394\u03a6) are constants for a particular \u2206, they can be factored out of the integral:\n$I'(x, y) = cos(\\Delta\\phi) \\cdot I(x, y) + i sin(\\Delta) \\cdot \\int \\int \u0391(u, v)e^{i\\phi(u,v)} e^{i2\\pi(ux+vy)} du dv$                                                                                      (13)\n$\\int \\int \u0391(u, v)e^{i\\phi(u,v)} e^{i2\\pi(ux+vy)} du dv$                                                                                                                                                                                                                    (14)\nThis shows that the new image I'(x, y) is a linear combination of the original image I(x, y) and another image derived from the same amplitude and a phase-shifted version of the original phase components. The transformation demonstrates that even a constant shift in the phase component translates into a significant transformation in the spatial domain, affecting the structural layout and visual features of the image."}, {"title": "C.2. Model Robustness through Dynamic Patch Training", "content": "To address the hardware constraints in real-time scenarios such as phones or laptop webcams, which often adjust camera resolutions to optimize performance within design and battery limits, there is a critical need for models that dynamically adapt to these variations. Feeding various image resolutions to the model dynamically also helps avoid spurious correlations that are formed due to strong correlation (Adhikarla et al., 2023) in the data distribution of certain types of images. For instance, the SICEv2 dataset has relatively more mixed-exposure images, and the borders of sudden changes in exposure become more prone to spurious correlations. However, our ExpoMamba uses spatial and temporal components that are inherently designed in Vision Mamba (Zhu et al., 2024a) to handle both the spatial distribution of pixels in images and the temporal sequence of frames in videos."}, {"title": "C.3. Dynamic Adjustment Approximation", "content": "The Dynamic Adjustment Approximation module offers a unique way to enhance images without needing ground truth mean and variance. Instead, it dynamically adjusts brightness by using the image's own statistical properties, specifically the median and mean pixel values. Unlike previous models like KinD, LLFlow, RetinexFormer, which relied on static adjustment factors from the ground truth and often produced less accurate results otherwise, our method calculates a desired shift based on the difference between a normalized brightness value and the image's mean. Then, it adjusts the image's medians toward this shift, taking both the current median and mean into account. This leads to a more balanced and natural enhancement. Adjustment factors are carefully computed to avoid infinite or undefined values, ensuring stability. This approach simplifies the process by not requiring ground-truth data and also improves the efficiency and effectiveness of image enhancement.\n$adjustment\\_factors = \\frac{medians + strength \\times (normalized\\_value - means)}{medians}$                                                                                                              (15)\n$adjusted\\_img = input\\_img \\times adjustment\\_factors$                                                                                                                                                                     (16)"}, {"title": "C.4. Model Configuration", "content": "This model configuration table provides a detailed comparison between the two variants of ExpoMamba, highlighting their configurations and performance metrics. Notably, despite an increase of 125 million parameters, the memory consumption of the larger ExpoMambalarge variant is 5690 Mb, which is a modest increase compared to transformer-based models."}, {"title": "C.5. Algorithm", "content": "The following pseudocode presents the details of ExpoMamba training with FSSB blocks:"}, {"title": "D. Loss function", "content": "The combined loss function as shown in Eq. 7, is designed to enhance image quality by addressing different aspects of image reconstruction. The L1 loss ensures pixel-level accuracy, crucial for maintaining sharp edges. This loss component has been widely utilized by the low light papers and has proven to be a valuable loss component for training variety of image restoration tasks. VGG loss, leveraging high-level features, maintains perceptual similarity. SSIM loss preserves structural integrity and local visual quality, which is vital for a coherent visual experience. LPIPS loss focuses on perceptual differences to generate natural looking image. Additionally, the overexposed regularizer detects and penalizes overexposed areas, crucial for handling HDR content and preserving details. It works in combination with HDR blocks to suppress artifacts in overexposed areas and control enhancement. In Eq. 7, A is the weight for the overexposed regularization term."}, {"title": "E. Ablation Study", "content": "We have performed the ablation study of our model ExpoMamba over LOL-v1 dataset. We used \u2018DoubleConv' Block instead of regular convolutional blocks in the regular U-Net/M-Net architecture. 'Block' represents the residual block inside every upsampling blocks. We implemented two variants of HDR layer, where HDR/HDROut represent the same single layer approach with different locations for layer placement. On the other hand, HDR-CSRNet+ is a deeper network originally design for congested scene recognition is used inside FSSB instead of simple HDR layer.\n\u2022 DoubleConv: Its absence results in lower PSNR and SSIM scores, confirming its importance."}]}