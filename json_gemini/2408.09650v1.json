{"title": "ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective Image Enhancement", "authors": ["Eashan Adhikarla", "Kai Zhang", "John Nicholson", "Brian D. Davison"], "abstract": "Low-light image enhancement remains a challenging task in computer vision, with existing state-of-the-art models often limited by hardware constraints and computational inefficiencies, particularly in handling high-resolution images. Recent foundation models, such as transformers and diffusion models, despite their efficacy in various domains, are limited in use on edge devices due to their computational complexity and slow inference times. We introduce ExpoMamba, a novel architecture that integrates components of the frequency state space within a modified U-Net, offering a blend of efficiency and effectiveness. This model is specifically optimized to address mixed exposure challenges-a common issue in low-light image enhancement\u2014while ensuring computational efficiency. Our experiments demonstrate that ExpoMamba enhances low-light images up to 2-3x faster than traditional models with an inference time of 36.6 ms and achieves a PSNR improvement of approximately 15-20% over competing models, making it highly suitable for real-time image processing applications. Model code is open sourced at: github.com/eashanadhikarla/ExpoMamba.", "sections": [{"title": "1. Introduction", "content": "Enhancing low-light images is crucial for applications ranging from consumer gadgets like phone cameras (Liba et al., 2019; Liu et al., 2024) to sophisticated surveillance systems (Xian et al., 2024; Guo et al., 2024; Shrivastav, 2024). Traditional techniques (Dale-Jones & Tjahjadi, 1993; Singh et al., 2015; Khan et al., 2014; Land & McCann, 1971; Ren et al., 2020) often struggle to balance processing speed and high-quality results, particularly with high-resolution images, leading to issues like noise and color distortion in scenarios requiring quick processing such as mobile photography and real-time video streaming.\nLimitations of Current Approaches. Foundation models have revolutionized computer vision, including low-light image enhancement, by introducing advanced architectures that model complex relationships within image data. In particular, transformer-based (Wang et al., 2023b; Chen et al., 2021a; Zhou et al., 2023b; Adhikarla et al., 2024) and diffusion-based (Wang et al., 2023c;a; Zhou et al., 2023a) low-light techniques have made significant strides. However, the sampling process requires a computationally intensive iterative procedure, and the quadratic runtime of self-attention in transformers make them unsuitable for real-time use on edge devices where limited processing power and battery constraints pose significant challenges. Innovations such as linear attention (Katharopoulos et al., 2020; Shen et al., 2018; Wang et al., 2020), self-attention approximation, windowing, striding (Kitaev et al., 2020; Zaheer et al., 2020), attention score sparsification (Liu et al., 2021b), hashing (Chen et al., 2021c), and self-attention operation kernelization (Katharopoulos et al., 2020; Lu et al., 2021; Chen et al., 2021b) have aimed to address these complexities, but often at the cost of increased computation errors compared to simple self-attention (Duman Keles et al., 2023; Dosovitskiy et al., 2021). (More details can be found in Appendix A)\nPurpose. With rising need for better images, advanced small camera sensors in edge devices have made it more common for customers to capture high quality images, and use them in real-time applications like mobile, laptop and tablet cameras (Morikawa et al., 2021). However, they all struggle with non-ideal and low lighting conditions in the real world. Our goal is to develop an approach that has high image quality (e.g., like CIDNet (Feng et al., 2024)) for enhancement but also at high speed (e.g., such as that of IAT (Cui et al., 2022) and Zero-DCE++ (Li et al., 2021)).\nContributions. Our contributions are summarized as:\n\u2022 We introduce the use of Mamba for efficient low-light image enhancement (LLIE), specifically focusing on mixed exposure challenges, where underlit (insufficient brightness) and overlit (excessive brightness) exist in the same image frame.\n\u2022 We propose a novel Frequency State Space Block (FSSB) that combines two distinct 2D-Mamba blocks, enabling the model to capture and enhance subtle textural details often lost in low-light images.\n\u2022 We describe a novel dynamic batch training scheme to improve robustness of multi-resolution inference in our proposed model.\n\u2022 We implement dynamic processing of the amplitude component to highlight distortion (noise, illumination) and the phase component for image smoothing and noise reduction."}, {"title": "2. Exposure Mamba", "content": "Along the lines of recent efficient sequence modeling approaches (Gu & Dao, 2023; Zhu et al., 2024a; Wang et al., 2024), we introduce ExpoMamba, a model combining frequency state-space blocks with spatial convolutional blocks (Fig. 2). This combination leverages the advantages of frequency domain analysis to manipulate features at different scales and frequencies, crucial for isolating and enhancing patterns challenging to detect in the spatial domain, like subtle textural details in low-light images or managing noise in overexposed areas. Additionally, by integrating these insights with the linear-time complexity benefits of the Mamba architecture, our model efficiently manages the spatial sequencing of image data, allowing rapid processing without the computational overhead of transformer models.\nOur proposed architecture utilizes a 2D scanning approach to tackle mixed-exposure challenges in low-light conditions. This model incorporates a combination of U2 \u2013 Net (Qin et al., 2020) and M-Net (Mehta & Sivaswamy, 2017), supporting 2D sRGB images with each block performing operations using a convolutional and encoder-style SSM ($x(t) \\in \\mathbb{R} \\rightarrow y(t) \\in \\mathbb{R}$). The subsequent section provides detailed information about our overall pipeline."}, {"title": "2.1. Frequency State Space Block (FSSB)", "content": "We utilize the frequency state space block (FSSB) to address the computational inefficiencies of transformer architectures especially when processing high-resolution image or long-sequence data. The FSSB's motivation is in two parts; first, towards enhancing the intricacies that are unaddressed/missed by the spatial domain alone; and second, to speed deep feature extraction using the frequency domain. The FSS block (as in Fig. 3) initiates its processing by transforming the input image I into the frequency domain using the Fourier transform:\n$F(u, v) = \\int \\int I(x, y)e^{-i2\\pi(ux+vy)}dx \\cdot dy$ (1)\nwhere, $F(u, v)$ denotes the frequency domain representation of the image, and $(u, v)$ are the frequency components corresponding to the spatial coordinates $(x, y)$. This transformation allows for the isolation and manipulation of specific frequency components, which is particularly beneficial for enhancing details and managing noise in low-light images. By decomposing the image into its frequency components, we can selectively enhance high-frequency components to improve edge and detail clarity while suppressing low-frequency components that typically contain noise (Lazzarini, 2017; Zhou et al., 2022). This selective enhancement and suppression improve the overall image quality.\nThe core of the FSSB comprises two 2D-Mamba (Visual-SSM) blocks to process the amplitude and phase components separately in the frequency domain. These blocks model state-space transformations as follows:\n$\\begin{aligned}h[t+1] &= A[t]h[t] + B[t].x[t] \\\\y[t] &= C[t] h[t]\\end{aligned}$ (2)\n(3)\nHere, $A[t]$, $B[t]$, and $C[t]$ are the state matrices that adapt dynamically based on the input features, and $h[t]$ represents the state vector at time t. $y[t]$ represents processed feature at time t, capturing the transformed information from the input features. This dual-pathway setup within the FSSB processes amplitude and phase in parallel.\nAfter processing through each of the VSS blocks, the modified amplitude and phase components are recombined and transformed back to the spatial domain using the inverse Fourier transform:\n$\\hat{I}(x, y) = \\int \\int \\hat{F}(u, v)e^{-i\\pi(ux+vy)}du \\cdot dv$ (4)\nwhere, $\\hat{F}(u, v)$ is the processed frequency domain representation in the latent space of each M-Net block. This method preserves the structural integrity of the image while enhancing textural details that are typically lost in low-light conditions, removing the need of self-attention mechanisms that are widely seen in transformer-based pipelines (Tay et al., 2022). The FSSB also integrates hardware-optimized strategies similar to those employed in the Vision-Mamba architecture (Gu & Dao, 2023; Zhu et al., 2024a) such as scan operations and kernel fusion reducing amount of memory IOs, facilitating efficient data flow between the GPU's memory hierarchies. This optimization significantly reduces computational overhead by a factor of $O(N)$ speeding the operation by 20 \u2013 40 times (Gu & Dao, 2023), enhancing processing speed for real-time applications. This can be evidently seen through our Fig. 1, where increasing the resolution size/input length increases the inference time gap tremendously due to which is more for transformer based models due to $O(N^2)$.\nWithin the FSS Block, the amplitude $A(u, v)$ and phase $P(u, v)$ components extracted from $F(u, v)$ are processed through dedicated state-space models. These models, adapted from the Mamba framework, are particularly tailored (dynamic adaptation of state matrices (A, B, C) based on spectral properties and the dual processing of amplitude and phase components.2) to enhance information across frequencies, effectively addressing the typical loss of detail in low-light conditions.\nAmplitude and Phase Component Modeling. Each component $A(u, v)$ and $P(u, v)$ undergoes separate but parallel processing paths, modeled by:\n$S_{t+1} = A(S_t) + BF(x_t), \\quad Y_t = CS_t$ (5)\nwhere $s_t$ denotes the state at time t, $F(x_t)$ represents the frequency-domain input at time t (either amplitude or phase), and A, B, C are the state-space matrices that dynamically adapt during training.\nFrequency-Dependent Dynamic Adaptation. The matrices A, B, C are not only time-dependent but also frequency-adaptive, allowing the model to respond to varying frequency components effectively. This adaptation is crucial for enhancing specific frequencies more affected by noise and low-light conditions. Specifically, these matrices evolve based on the spectral properties of the input: $A(u, v, t)$, $B(u, v, t)$, $C(u, v, t)$ adjust dynamically during the processing. This means that A, B, and C change their values according to both the time step t and the frequency components $(u, v)$, enabling targeted enhancement of the amplitude and phase components in the frequency domain. By evolving to match the spectral characteristics of the input, these matrices optimize the enhancement process.\nAfter separate processing through the state-space models, the modified amplitude A", "P": "u, v) are recombined and transformed back into the spatial domain to reconstruct the enhanced image:\n$I'(x, y) = F^{-1}(A''(u.v) + i \\cdot P''(u.v))$ (6)\nwhere, $I^{-1}$ denotes the inverse Fourier transform.\nFeature Recovery in FSSB. The HDR (High Dynamic Range) tone mapping process within the Frequency State Space Block (FSSB) is designed to enhance visibility and detail in low-light conditions by selectively normalizing brightness in overexposed areas. Feature recovery in FSSB aims to address the challenges of high dynamic range scenes, where standard methods often fail to maintain natural aesthetics and details. By implementing a thresholding mechanism set above 0.90, the HDR layer selectively applies tone mapping to overexposed areas, effectively normalizing brightness without compromising detail or causing unnatural halos often seen in standard HDR processes (Fig. 4). This selective approach is crucial as it maintains the natural aesthetic of the image while enhancing visibility and detail. The HDR layer is consistently applied as the final layer within each FSSB block and culminates as the ultimate layer in the ExpoMamba model, providing a coherent enhancement across all processed images.\nWe leverage the ComplexConv function from complex networks as introduced by Trabelsi et al. (Trabelsi et al., 2018). This function is incorporated into our model to capture and process additional information beyond traditional real-valued convolutions. Specifically, the ComplexConv function allows the simultaneous manipulation of amplitude and phase information in the frequency domain, which is essential to preserve the integrity of textural details in low-light images. The dual processing of amplitude and phase ensures that each component to be optimized separately. Tone mapping and ComplexConv have proven to be effective in overcoming limitations of traditional image processing techniques (Hu et al., 2022; Liu, 2024). We integrate these methods into our FSS design to address adverse lighting conditions in low light environments.\nThe input components in the frequency representation are processed through dynamic amplitude scaling and phase continuity layer, as shown in Fig. 3. As claimed by Fourmer (Zhou et al., 2023b), we have determined that the primary source of image degradation is indeed amplitude, specifically in the area between the amplitude and phase division within the image. Moreover, we found that the amplitude component primarily contains information about the brightness of the image, which directly impacts the visibility and the sharpness of the features within the image. However, the phase component encodes the positional information of these features, defining the structure and the layout of the image. Previously, it has been found that phase component of the image has a close relation with perceptual analysis (Xiao & Hou, 2004). Along those lines, we show that the human visual system is more sensitive to changes in phase rather than amplitude (proof in Appx C.1)."}, {"title": "2.2. Multi-modal Feature Learning", "content": "The inherent complexity of low-light images, where both underexposed and overexposed elements coexist, necessitates a versatile approach to image processing. Traditional methods, which typically focus either on spatial details or frequency-based features, fail to adequately address the full spectrum of distortions encountered in such environments. By contrast, the hybrid modeling approach of \u201cExpoMamba\" leverages the strengths of both the spatial and frequency domains, facilitating a more comprehensive and nuanced enhancement of image quality.\nOperations in the frequency domain, such as the Fourier transform, can isolate and address specific types of distortion, such as noise and fine details, which are often exacerbated in low-light conditions. This domain provides a global view of the image data, allowing for the manipulation of features that are not easily discernible in the spatial layout. Simultaneously, the spatial domain is critical to maintaining the local coherence of image features, ensuring that enhancements do not introduce unnatural artifacts."}, {"title": "2.3. Dynamic Patch Training", "content": "Dynamic patch training enhances the 2D scanning model by optimizing its scanning technique for various image resolutions. In ExpoMamba, 2D scanning involves sequentially processing image patches to encode feature representations. We create batches of different resolution images where in a given batch the resolution is fixed and we dynamically randomize the different batch resolutions of input patches during training. This way the model learns to adapt its scanning and encoding process to different scales and levels of detail (Fig 5). This variability helps the model become more efficient at capturing and processing fine-grained details across different image resolutions, ensuring consistent performance. Consequently, the model's ability to handle mixed-exposure conditions is improved, as it can effectively manage diverse resolutions and adapt its feature extraction process dynamically, enhancing its robustness and accuracy in real-world applications."}, {"title": "3. Experiments and Implementation details", "content": "In this section, we evaluate our method through a series of experiments. We begin by outlining the datasets used, experimental setup, followed by a comparison of our method against state-of-the-art techniques using four quantitative metrics. We also perform a detailed ablation study (Appx E, Tab. 5) to analyze the components of our proposed method."}, {"title": "3.1. Datasets", "content": "To test the efficacy of our model, we evaluated ExpoMamba on four datasets: (1) LOL (Wei et al., 2018a), which has v1 and v2 versions. LOLv2 (Yang et al., 2020a) is divided into real and synthetic subsets. The training and testing sets are split into 485/15, 689/100, and 900/100 on LOLv1, LOLv2-real, and LOLv2-synthetic with 3 \u00d7 400 \u00d7 600 resolution images. (2) LOL4K is an ultra-high definition dataset with 3\u00d73,840 x 2,160 resolution images, containing 8,099 pairs of low-light/normal-light images, split into 5,999 pairs for training and 2,100 pairs for testing. (3) SICE (Cai et al., 2018) includes 4,800 images, real and synthetic, at various exposure levels and resolutions, divided into training, validation, and testing sets in a 7:1:2 ratio."}, {"title": "3.2. Experimental setting", "content": "The proposed network is a single-stage end-to-end training model. The patch sizes are set to 128 \u00d7 128, 256 \u00d7 256, and 324 \u00d7 324 with checkpoint restarts and batch sizes of 8, 6, and 4, respectively, in consecutive runs. For dynamic patch training, we use different patch sizes simultaneously. The optimizer is RMSProp with a learning rate of 1 \u00d7 10-4, a weight decay of 1 \u00d7 10-7, and momentum of 0.9. A linear warm-up cosine annealing (Loshchilov & Hutter, 2016) scheduler with 15 warm-up epochs is used, starting with a learning rate of 1 \u00d7 10\u22124. All experiments were carried out using the PyTorch library (Paszke et al., 2019) on an NVIDIA A10G GPU.\nLoss functions. To optimize our ExpoMamba model we use a set of loss functions:\n$L = L_{l1} + L_{vgg} + L_{ssim} + L_{lpips} + \\lambda \\cdot L_{overexposed}$ (7)"}, {"title": "4. Results", "content": "The best performance for Tab. 1, Tab. 2, and Tab. 3 are marked with Red, Green, and Blue, respectively.\nTab. 1 compares our performance to 31 state-of-the-art baselines, including lightweight and heavy models. We evaluate ExpoMamba's performance using SSIM, PSNR, LPIPS, and FID. ExpoMamba achieves an inference time of 36 ms, faster than most baselines (Fig. 1) and the fastest among comparable models. Models like DiffLL (Jiang et al., 2023), CIDNet (Feng et al., 2024), and LLformer (Wang et al., 2023b) have comparable results but much longer inference times. Traditional algorithms (e.g., MSRCR (Jobson et al., 1997), MF (Fu et al., 2016a), BIMEF (Ying et al., 2017), SRIE (Fu et al., 2016b), FEA (Dong et al., 2011), NPE (Wang et al., 2013), LIME (Guo et al., 2016)) generally perform poorly on LOL4K (Tab. 2). Fig. 1.b shows that increasing image resolution to 4K significantly increases inference time for transformer models due to their quadratic complexity. Despite being a 41 million parameter model, ExpoMamba demonstrates remarkable storage efficiency, consuming ~1/4th memory (2923 Mb) compared to CIDNet, which, despite its smaller size of 1.9 million parameters, consumes 8249 Mb. This is because ExpoMamba's state expansion fits inside the GPU's high-bandwidth memory and removes the quadratic bottleneck which significantly reduces memory footprint. Current SOTA models CIDNet (Feng et al., 2024) and LLformer (Wang et al., 2023b) are slower and less memory-efficient."}, {"title": "5. Conclusion", "content": "We introduced ExpoMamba, a model designed for efficient and effective low-light image enhancement. By integrating frequency state-space components within a U-Net variant, ExpoMamba leverages spatial and frequency domain processing to address computational inefficiencies and high-resolution challenges. Our approach combines robust feature extraction of state-space models, enhancing low-light images with high fidelity and achieving impressive inference speeds. Our novel dynamic patch training strategy significantly improves robustness and adaptability to real-world hardware constraints, making it suitable for real-time applications on edge devices. Experimental results show that ExpoMamba is much faster and comparably better than numerous existing transformer and diffusion models, setting a new benchmark in low light image enhancement."}, {"title": "C.1. Proof of Phase Manipulation", "content": "For an image I(x, y), its Fourier transform is given by:\n$F(u, v) = \\int \\int I(x, ,y)e^{-i2\\pi(ux+vy)}dx dy$ (8)\nThis can be decomposed into amplitude A(u, v) and phase \u03c6(u, v):\n$F(u, v) = A(u, v)e^{i\\varphi(u,v)}$ (9)\nThe inverse Fourier transform, which reconstructs the image from its frequency representation, is:\n$I(x, y) = \\int \\int A(u, v)e^{i\\varphi(u,v)} e^{i2\\pi(ux+vy)} du dv$ (10)\nSuppose that the phase component \u03c6(u, v) is uniformly shifted by an angle \u2206, the new phase \u03c6' (u, v) = \u03c6(u, v) + \u0394\u03c6. The modified image I'(x, y) with this new phase is represented as:\n$I'(x, y) = \\int \\int A(u, v)e^{i(\\varphi(u,v)+\\Delta\\varphi)} e^{i2\\pi(ux+vy)} du dv$ (11)\nUsing Euler's formula $e^{i\\triangle\\varphi} = cos(\\triangle\\varphi) + i sin(\\triangle)$, the equation becomes:\n$I' = \\int \\int A(u, v)e^{i\\varphi(u,v)} (cos(\\Delta\\varphi) + i sin(\\Delta))e^{i2\\pi(ux+vy)} du dv$ (12)\nGiven that cos(\u0394\u03a6) and sin(\u0394\u03a6) are constants for a particular \u2206, they can be factored out of the integral:\n$I'(x, y) = cos(\\Delta\\phi) \\cdot I(x, y) + i sin(\\Delta\\phi) \\cdot \\int \\int A(v, v)e^{i\\varphi(v,v)} e^{i2\\pi(ux+vy)} du dv$ (13)\n(14)\nThis shows that the new image I'(x, y) is a linear combination of the original image I(x, y) and another image derived from the same amplitude and a phase-shifted version of the original phase components. The transformation demonstrates that even a constant shift in the phase component translates into a significant transformation in the spatial domain, affecting the structural layout and visual features of the image."}, {"title": "C.2. Model Robustness through Dynamic Patch Training", "content": "To address the hardware constraints in real-time scenarios such as phones or laptop webcams, which often adjust camera resolutions to optimize performance within design and battery limits, there is a critical need for models that dynamically adapt to these variations. Feeding various image resolutions to the model dynamically also helps avoid spurious correlations that are formed due to strong correlation (Adhikarla et al., 2023) in the data distribution of certain types of images. For instance, the SICEv2 dataset has relatively more mixed-exposure images, and the borders of sudden changes in exposure become more prone to spurious correlations. However, our ExpoMamba uses spatial and temporal components that are inherently designed in Vision Mamba (Zhu et al., 2024a) to handle both the spatial distribution of pixels in images and the temporal sequence of frames in videos."}, {"title": "C.3. Dynamic Adjustment Approximation", "content": "The Dynamic Adjustment Approximation module offers a unique way to enhance images without needing ground truth mean and variance. Instead, it dynamically adjusts brightness by using the image's own statistical properties, specifically the median and mean pixel values. Unlike previous models like KinD, LLFlow, RetinexFormer, which relied on static adjustment factors from the ground truth and often produced less accurate results otherwise, our method calculates a desired shift based on the difference between a normalized brightness value and the image's mean. Then, it adjusts the image's medians toward this shift, taking both the current median and mean into account. This leads to a more balanced and natural enhancement. Adjustment factors are carefully computed to avoid infinite or undefined values, ensuring stability. This approach simplifies the process by not requiring ground-truth data and also improves the efficiency and effectiveness of image enhancement.\n$\\begin{aligned}\\text{adjustment\\_factors} &= \\frac{\\text{medians} + \\text{strength} \\times (\\text{normalized\\_value} - \\text{means})}{\\text{medians}} \\\\ \\text{adjusted\\_img} &= \\text{input\\_img} \\times \\text{adjustment\\_factors}\\end{aligned}$ (15)\n(16)"}, {"title": "C.4. Model Configuration", "content": "This model configuration table provides a detailed comparison between the two variants of ExpoMamba, highlighting their configurations and performance metrics. Notably, despite an increase of 125 million parameters, the memory consumption of the larger ExpoMambalarge variant is 5690 Mb, which is a modest increase compared to transformer-based models."}, {"title": "C.5. Algorithm", "content": "The following pseudocode presents the details of ExpoMamba training with FSSB blocks:"}, {"title": "D. Loss function", "content": "The combined loss function as shown in Eq. 7, is designed to enhance image quality by addressing different aspects of image reconstruction. The L1 loss ensures pixel-level accuracy, crucial for maintaining sharp edges. This loss component has been widely utilized by the low light papers and has proven to be a valuable loss component for training variety of image restoration tasks. VGG loss, leveraging high-level features, maintains perceptual similarity. SSIM loss preserves structural integrity and local visual quality, which is vital for a coherent visual experience. LPIPS loss focuses on perceptual differences to generate natural looking image. Additionally, the overexposed regularizer detects and penalizes overexposed areas, crucial for handling HDR content and preserving details. It works in combination with HDR blocks to suppress artifacts in overexposed areas and control enhancement. In Eq. 7, A is the weight for the overexposed regularization term."}, {"title": "E. Ablation Study", "content": "We have performed the ablation study of our model ExpoMamba over LOL-v1 dataset. We used \u2018DoubleConv' Block instead of regular convolutional blocks in the regular U-Net/M-Net architecture. 'Block' represents the residual block inside every upsampling blocks. We implemented two variants of HDR layer, where HDR/HDROut represent the same single layer approach with different locations for layer placement. On the other hand, HDR-CSRNet+ is a deeper network originally design for congested scene recognition is used inside FSSB instead of simple HDR layer.\n\u2022 DoubleConv: Its absence results in lower PSNR and SSIM scores, confirming its importance."}]}