{"title": "CHEMISTRY-INSPIRED DIFFUSION WITH NON-DIFFERENTIABLE GUIDANCE", "authors": ["Yuchen Shen", "Chenhao Zhang", "Sijie Fu", "Chenghui Zhou", "Newell Washburn", "Barnab\u00e1s P\u00f3czos"], "abstract": "Recent advances in diffusion models have shown remarkable potential in the con-\nditional generation of novel molecules. These models can be guided in two ways:\n(i) explicitly, through additional features representing the condition, or (ii) implic-\nitly, using a property predictor. However, training property predictors or condi-\ntional diffusion models requires an abundance of labeled data and is inherently\nchallenging in real-world applications. We propose a novel approach that at-\ntenuates the limitations of acquiring large labeled datasets by leveraging domain\nknowledge from quantum chemistry as a non-differentiable oracle to guide an un-\nconditional diffusion model. Instead of relying on neural networks, the oracle pro-\nvides accurate guidance in the form of estimated gradients, allowing the diffusion\nprocess to sample from a conditional distribution specified by quantum chemistry.\nWe show that this results in more precise conditional generation of novel and\nstable molecular structures. Our experiments demonstrate that our method: (1)\nsignificantly reduces atomic forces, enhancing the validity of generated molecules\nwhen used for stability optimization; (2) is compatible with both explicit and im-\nplicit guidance in diffusion models, enabling joint optimization of molecular prop-\nerties and stability; and (3) generalizes effectively to molecular optimization tasks\nbeyond stability optimization.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models have received increasing attention in molecular design. Their ability to generate\nnovel molecules with desired properties (i.e., guided diffusion) has fostered advances in material\nscience (Manica et al., 2023; Yang et al., 2023), chemistry (Anstine & Isayev, 2023), protein de-\nsign (Watson et al., 2023; Abramson et al., 2024), etc. To achieve guided diffusion, one can explic-\nitly condition the diffusion process on specific properties in training (Hoogeboom et al., 2022; Xu\net al., 2023), such that the model is naturally a conditional generator during inference. Alternatively,\nan unconditional model can be trained without labels, and the diffusion process is guided implicitly\nat inference time (Dhariwal & Nichol, 2021; Vignac et al., 2022) using a property predictor that\nprovides guidance gradients to steer the model toward sampling from the conditional distribution.\nBoth explicitly and implicitly guided diffusion require a labeled dataset to train the model or the\nproperty predictor. The acquisition of labels, however, can be expensive, time-consuming, and of-\nten impracticable\u00b9. When only small labeled datasets are available (Power et al., 2022), the model\nand the property predictor can potentially struggle to generalize beyond seen structures (see Ap-\npendix F.2), degrading the performance of the diffusion process when generating novel molecules.\nImplicitly guided diffusion can help address such challenges. This approach (i) requires no labels\nto train the model and (ii) can replace the property predictor with domain knowledge from quantum\nchemistry, fulfilled by quantum chemistry software such as xTB (Bannwarth et al., 2019; 2021) and\nGaussian (Frisch et al., 2016). Such software can act as an expert oracle to create labeled datasets;\nthereby, avoiding the extrapolation shortcomings of neural networks.\nDespite following certain computation procedures, quantum chemistry is by nature a non-\ndifferentiable oracle\u00b2 that can not be backpropagated with neural networks, as those procedures"}, {"title": "2 PRELIMINARY", "content": "In this section, we introduce the key concepts of diffusion models and explore the architecture of\na specific diffusion model designed for 3D molecule generation. We will explain how to achieve\nequivariance in the generated molecules and how the semi-empirical quantum chemical method\nGFN2-xTB can be utilized for guidance. Additionally, we will also outline our motivation for inte-\ngrating CHEMGUIDE with 3D diffusion models to optimize molecular stability.\nDiffusion Models In general, a diffusion model (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song\net al., 2020) consists of a forward diffusion process and a reverse denoising process. The diffusion\nprocess is a Markov chain that gradually adds Gaussian noises with a predefined variance schedule\n$\\beta_{1:T}$ from timestep 1 to T to the original datapoint $x_0$, which is chosen such that $x_T \\sim \\mathcal{N}(0,I)$.\nThe forward diffusion process $q$ is usually defined as a fixed schedule by the following:\n\n$q(x_{1:T} | x_0) = \\prod_{t=1}^{T} q(x_t | x_{t-1}) \\qquad q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$ (1)"}, {"title": "3 METHODOLOGY", "content": "3.1 GUIDANCE FROM NEURAL REGRESSOR\nNoisy Guidance The goal of neural guidance is to direct the denoising process towards a target\nproperty value y. Dhariwal & Nichol (2021) propose to modify the denoising process to achieve\nconditional generation with an unconditional model and using a scalar s that controls the guidance\nstrength:\n\n$x_{t-1} \\sim \\mathcal{N}(\\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\alpha_t^t}}\\epsilon_\\theta(x_t, t)) + s\\sigma_t^2\\nabla_{x_t} \\log p_\\phi(y|x_t), \\sigma_t^2I)$ (6)\nHere $p_\\phi(y|x_t)$ is parameterized by a classifier, and y is a categorical label, such that the modification\n$s\\sigma_t^2\\nabla_{x_t} \\log p_\\phi(y|x_t)$ shifts the mean of the sampling distribution to provide guidance. Let $y \\in \\mathbb{R}$\nand $f_n: \\mathcal{G} \\rightarrow \\mathbb{R}$ be the neural regressor for the molecular property of interest (Vignac et al., 2022).\nNow, assuming $y|x_t \\sim \\mathcal{N}(f_n(x_t), \\sigma^2 I)$ and $\\sigma^2 = 1$, we have:\n\n$\\nabla_{x_t} \\log p_\\phi(y | x_t) \\propto -\\nabla_{x_t} ||y - f_n(x_t)||^2 = -\\nabla_{x_t} \\mathcal{L}(y, f_n(x_t))$ (7)\nwhere $\\mathcal{L}(y, f_n(x_t))$ is the Mean Square Error (MSE) between the target and the prediction of $f_n(\\cdot)$.\nHowever, in the early stage of the denoising process, $x_t = \\alpha_tx_0 + \\sqrt{1 - \\alpha_t^2}\\epsilon$ might not be infor-\nmative enough to predict y as it consists mostly of Gaussian noise. For effective prediction during\nthe denoising process, we estimate the denoised version of $x_t$ as Kawar et al. (2022); Song et al.\n(2020):\n\n$\\hat{x}_0(x_t) := x_0 = \\frac{x_t - \\sqrt{1 - \\alpha_t^2}\\epsilon_\\theta(x_t, t)}{\\alpha_t}$ (8)\nwhere $f_n(\\hat{x}_0)$ is used in place of $f_n(x_t)$ as the predicted molecular property.\nClean Guidance Besides applying the gradient as guidance on noisy $x_t$, we build insight from\nBansal et al. (2023) and derive guidance in the clean (=noise free) space $x_0$ as:\n\n$\\Delta x_0 = \\arg \\min_\\Delta \\mathcal{L}(y, f_n(x_0 + \\Delta))$ (9)\nwhere $\\Delta x_0$ is approximated using $K$ steps of gradient descent starting from $\\Delta = 0$. Note that $\\Delta x_0$\nis in clean data space, so we need to translate it back to the noisy space while recovering $x_t$:\n\n$x_t = \\alpha_t(x_0 + \\Delta x_0) + \\sqrt{1 - \\alpha_t^2}\\tilde{\\epsilon}$ (10)\nwhere $\\tilde{\\epsilon}$ is the augmented noise used to to sample $x_{t-1}$ and is thus given by:\n\n$\\tilde{\\epsilon} = \\epsilon_\\theta(x_t, t) - \\frac{\\alpha_t \\Delta x_0}{\\sqrt{1-\\alpha_t^2}}$ (11)\n3.2 GUIDANCE FROM A NON-DIFFERENTIABLE ORACLE\nWe aim to tackle a challenging problem where the guidance is specified by a non-differentiable\noracle (e.g., the GFN2-xTB method). We change our notation from $x_t$ to $z_t$ at each time step to\nindicate diffusion in the latent space. With the guidance target y being 0, our non-differentiable\nfunction $f$ (Eq. 5) for force guidance is defined as follows.\n\n$f(\\mathcal{G}) = \\frac{1}{N} \\sum_i^N f_i(\\mathcal{G})$ (12)\nwhere the molecular graph is decoded as $\\mathcal{G} = D(z_0)$ with $z_0 = \\hat{x}_0(z_t)$ estimated by Eq. 8, and\nwe aim to achieve $f(\\mathcal{G}_\\theta) < f(\\mathcal{G}_0)$. Here $\\mathcal{G}_\\theta, \\mathcal{G}_0$ are provided by CHEMGUIDE and an unguided\ndiffusion model. As $f$ is non-differentiable across different molecules, we can not directly add\nguidance using Eq. 7. Instead, we estimate the gradient analytically. Recall that $\\mathcal{L}(y, f(\\mathcal{G}))$ is\nthe MSE loss, $f: \\mathcal{G} \\rightarrow \\mathbb{R}$ is the non-differentiable oracle, and $D$ is the decoder. Let $F$ be the\ncomposition $f \\circ D \\circ \\hat{x}_0$, and we estimate the gradient as:\n\n$\\nabla_{z_t} \\log p_\\phi(y|z_t) \\propto -\\nabla_{\\mathcal{F}(z_t)}\\mathcal{L}(y, \\mathcal{F}(z_t)) \\nabla_{z_t} \\mathcal{F}(z_t)$ (13)\n$\\approx -\\nabla_{\\mathcal{F}(z_t)}\\mathcal{L}(y, \\mathcal{F}(z_t)) \\lim_{\\zeta \\rightarrow 0} \\frac{\\mathcal{F}([z_{x,t} + \\zeta\\mathbb{1}_{N\\times 3}, z_{h,t}]) - \\mathcal{F}([z_{x,t} - \\zeta\\mathbb{1}_{N\\times 3}, z_{h,t}])}{2\\zeta}$ (14)"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENT SETTING\nDataset The models in our experiment are trained on the QM9 dataset (Ramakrishnan et al.) and\nthe GEOM dataset (Axelrod & G\u00f3mez-Bombarelli). The QM9 dataset is a catalog with 134K small\ndrug-like molecules consisting of up to nine heavy (non-hydrogen) atoms. The Geometric Ensemble\nOf Molecules (GEOM) dataset includes 450K molecules with up to 91 heavy atoms (on average,\n24.9), where 37 million molecular conformations are generated and reported with their geometries,\nenergies, and statistical weight.\nGuidance Property We study guided generation for force and thus energy optimization on QM9\nand GEOM. For neural and combined guidance, we evaluate on QM9 as there are no labels in\nGEOM to train the regressors. We consider the following 6 properties reported in QM9: the norm\nof static polarizability ($\\alpha$, Bohr\u00b3), the norm of dipole moment ($\\mu$, Debye), heat capacity at room\ntemperature ($C_v$, cal/(mol\u00b7K)), the energy of the electron in the highest occupied molecular orbital\n($\\epsilon_{HOMO}$, eV), the energy of the electron in the lowest unoccupied molecular orbital ($\\epsilon_{LUMO}$, eV), and\nthe HOMO-LUMO energy gap ($\\Delta\\epsilon$, eV). We choose s (Eq. 6) from {1,10\u207b\u00b9,10\u207b\u00b2,10\u207b\u00b3,10\u207b\u2074}\nfor all experiments, and additionally {2, 5, 10, 20, 25, 30, 40, 50} for the 6 properties.\nModel We integrate CHEMGUIDE with GeoLDM (Xu et al., 2023), an improvement from\nEDM (Hoogeboom et al., 2022). For non-differentiable guidance on force, we compare our method\nto unconditional EDM and GeoLDM\u00b3, since there is no available ground-truth force to explicitly\ntrain a conditional model. For noisy neural guidance and combined guidance, we choose condition-\nally trained EDM (C-EDM) and GeoLDM (C-GeoLDM) as the baselines.\nEvaluation Metric For non-differentiable guidance on the force, we use the Root Mean Square\n(RMS) of the forces calculated at GFN2-xTB as the evaluation metric. We also report validity,\nuniqueness, atom stability, molecule stability, and energy above the ground state. For differentiable\nneural guidance on the 6 properties ($\\alpha, \\mu, C_v, \\epsilon_{HOMO}, \\epsilon_{LUMO}, \\Delta\\epsilon$), we follow Xu et al. (2023) and\nuse the Mean Absolute Error (MAE) between the target property y and the predicted value \u0177 from\nthe regressor of the generated molecule as the evaluation metric.\nWe add guidance to the last 400 of the 1000 diffusion steps (Han et al., 2024), and calculate the\nchange percentage between our method and the GeoLDM/C-GeoLDM baseline. The implementa-"}]}