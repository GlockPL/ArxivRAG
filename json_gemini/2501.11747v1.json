{"title": "OPTIMIZING PRETRAINING DATA MIXTURES\nWITH LLM-ESTIMATED UTILITY", "authors": ["William Held", "Bhargavi Paranjape", "Punit Singh Koura", "Mike Lewis", "Frank Zhang", "Todor Mihaylov"], "abstract": "Large Language Models improve with increasing amounts of high-quality train-\ning data. However, leveraging larger datasets requires balancing quality, quantity,\nand diversity across sources. After evaluating nine baseline methods under both\ncompute- and data-constrained scenarios, we find token-count heuristics outper-\nform manual and learned mixes, indicating that simple approaches accounting for\ndataset size and diversity are surprisingly effective. Building on this insight, we\npropose two complementary approaches: UtiliMax, which extends token-based\nheuristics by incorporating utility estimates from reduced-scale ablations, achiev-\ning up to a 10.6x speedup over manual baselines; and Model Estimated Data Util-\nity (MEDU), which leverages LLMs to estimate data utility from small samples,\nmatching ablation-based performance while reducing computational requirements\nby ~200x\u00b9. Together, these approaches establish a new framework for automated,\ncompute-efficient data mixing that is robust across training regimes.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Model (LLM) pretraining data increasingly consists of sub-corpora from many\nsources covering multiple domains and varying in size (Gao et al., 2020; Du et al., 2022; TogetherAI,"}, {"title": "2 BACKGROUND", "content": "Given an arbitrary utility function \u00b5, data mix optimization aims to improve model perfor-\nmance by optimizing sampling weights w over datasets D := {d1,d2,...,dn}. Based on\nexternal constraints such as computational resources or data, models are trained for a fixed\nbudget Bc, in terms of a number of training examples. Given a budget, we want to find\narg maxw \u03bc (UdeD{\u20ac1,\u20ac2,\u2026\u2026\u2026, ewa\u2022Be } ~ d) where ei represents an example from dataset di.\nClearly, the target budget is a core factor for this problem, as well as data curation broadly (Goyal\net al., 2024). However, existing works run experiments at a single scale and budget (Rae et al., 2021;\nSoldaini et al., 2024; Xie et al., 2024; Albalak et al., 2023). To determine data mixes for the varying\nconstraints of new models, data mixing methods must perform well across budgets and model scales.\nThis makes evaluating this form of generalization critical for identifying effective methods.\nWith budget in mind, data mixing is a resource allocation problem. While resource allocation has\nbeen extensively studied in convex optimization (Boyd & Vandenberghe, 2004), optimizing utility\n\u03bc directly is intractable given the cost of full-scale training runs. Furthermore, benchmarks only\nestimate \u03bc based on a sample of the domain they represent. Instead, data mix optimization requires\nmethods that are robust to error induced by estimated utility.\nMost relevant is the Markowitz model (Markowitz, 1952), a portfolio optimization method that\nbalances an expected reward w\u03c4\u03bc(D) with risk w\u0f0b\u03a3w, where \u2211 is the covariance matrix across\nassets. Using this risk-adjusted return, data mix optimization can be formulated as:\narg max w\u00b9\u00b5(D) \u2013 ww subject to 1Tw = 1, min(w) > 0, (1)\nw\nIn applying this approach to data mix optimization, we make a few key assumptions. First, we\nassume the utility contributed by a dataset is linear with respect to its weight in the data mix. Second,\nwe assume that the utility of each dataset is independent. Finally, to avoid compounding estimation"}, {"title": "2.1 RELATED WORK", "content": "Token-Size Heuristic Data Mixes Data mixes are often found using heuristics using the number\nof tokens per dataset t, the target budget B\u2081 in terms of tokens, and the sampling proportion Btw.\n1\nUniform sampling is the simplest baseline data mixing method and defines w = . Despite the\nsimplicity, uniform sampling is a strong baseline in multi-task learning (Michel et al., 2021).\nt\nt\nProportional sampling is more common in efforts for large-scale training runs (Rae et al., 2021;\nGroeneveld et al., 2024) and defines w . This holds the sampling proportion constant across\ndatasets at any budget, minimizing the maximum sampling proportion or number of epochs.\nOLMO V1.7 utilizes near proportional weights, with Wikipedia up-sampled and CommonCrawl data\ndown-sampled \u2013 both by a factor of two\u00b2. A methodology for these adjustments is not released,\nbut likely stems from a combination of researcher intuition and results from the data mix ablations\nshown in Soldaini et al. (2024). Since we use Dolma V1.7, we compare to this as a manual baseline.\nUniMax (Chung et al., 2023) interpolates between proportional and uniform sampling by setting an\n\u0392\u03c4\u03c9\nepoch cap C and finding arg minw ww s.t. < C. Through the lens of portfolio optimiza-\ntion, UniMax purely minimizes risk under an assumption of uniform \"linguistic utility\" from the\nmultilingual setting it was designed for (Blasi et al., 2022). UtiliMax is a generalization of UniMax\nwhich allows for arbitrary non-uniform utility functions over datasets.\nLearning Data Mixes There is also significant appeal to methods that learn data mixes.\nDoReMi (Xie et al., 2024) does this using a sequence of training runs. First, a reference model\nis trained using a prior data mix. Then, a proxy model is trained using Distributionally Robust\nOptimization (Sagawa et al., 2020) to find weights which minimize worst-case excess loss with\nrespect to the reference model. w is defined as the average of these weights throughout training.\nOnline Data Mixing (Albalak et al., 2023, ODM) treats data mixing as a multi-armed bandit problem\nand uses a variant of the EXP3 algorithm (Auer et al., 2002) to dynamically sample data during train-\ning. Bandit methods are another natural formulation of data mixing, and have also been explored in\nworks for multilinguality (Schioppa et al., 2023) and translation (Kreutzer et al., 2021).\nModel Based Quality Filtering Related to MEDU, many prior works develop methods to filter\nout \"low-quality\" data points entirely. Albalak et al. (2024) offer a systematic survey of this area.\nPerplexity filters use n-gram or other low-cost language models, such as KenLM (Heafield, 2011),\ntrained on high-quality text to assess data quality in new data (Wenzek et al., 2020). High perplexity\ndata points are excluded based on the assumption that they are likely not natural language.\nQuality classifiers operate in a similar manner, but model both low and high quality data to dis-\ntinguish the two. Brown et al. (2020) popularized this approach by using a classifier trained to\ndistinguish high-quality web pages from random web pages. Recently, LLMs have been used to cre-\nate zero-shot quality classifiers based on natural language specifications of high-quality data (Wettig\net al., 2024; Penedo et al., 2024). This approach has been validated at frontier model scale by Llama\n3 (Dubey et al., 2024), but requires a single manually-written specification of \"high-quality\" data.\nHow UtiliMax Differs Pior data mixing work avoids making assumptions about use-cases to im-\nprove generality. On the other hand, most practitioners have a set of intended use-cases measured by\nbenchmarks which have strong correlation with various LLM capabilities (Ruan et al., 2024). Util-\niMax maintains generality by optimizing for multiple downstream tasks with terms for data utility,\ndiversity, and size. This is applicable to any estimator, such as concurrent work which identifies loss\ncorrelations across open-source models (Thrush et al., 2024). Separately, MEDU proposes an ap-\nproach to automatically construct quality specifications for each downstream task and then leverages\nthis specification to provide more compute efficient utility estimates to UtiliMax."}, {"title": "3 EXPERIMENTAL SETUP", "content": "3.1 TRAINING SETUP\nTraining Data Overview We use Dolma V1.7 (Soldaini et al., 2024), which is released under the\nOpen Data Commons License, for our experiments. While prior works have used the Pile (Gao\net al., 2020) for data mixing experiments, it has since had sections removed due to copyright issues\nwhich prevents direct comparison. Dolma is made up of 15 separate corpora including 2 corpora\nwhich are bucketed at higher granularity using KenLM (Heafield, 2011) perplexity. We report the\nnames and sizes of the Dolma corpora using the Llama tokenizer in Table 1.\nImportantly, Dolma is large-enough for and has been validated through large-scale training runs\nthrough OLMO (Groeneveld et al., 2024). Dolma has also undergone document filtering, deduplica-\ntion, and cleaning, which allows this work to focus solely on mixing similarly preprocessed corpora.\nGeneral Hyperparameters We train compute-optimal models from 6 \u00d7 1019 to 3 \u00d7 1021 FLOPs\nbased on the scaling law presented in the Llama 3 paper (Dubey et al., 2024), using the same archi-\ntecture and tokenizer. The models range in size from 550M to 4.1B parameters and are trained on\n14B to 110B tokens across compute scales. Across all training runs, we use a cosine learning rate\nschedule with a linear warmup for 2,000 training steps decaying to 0.1 of the peak learning rate.\nThe peak learning rate is 2 \u00d7 10-4 for all models, except for the largest run which uses 3 \u00d7 10\u22124.\nExamples are packed to a sequence length of 8192 and batch size increases from 32 to 256 such that\nmodels train for approximately the same number of steps (58k on average, \u00b19.2k)3.\n3.2 EVALUATING ACROSS TRAINING BUDGET CONSTRAINTS\nIn this work, we explore two realistic settings corresponding to different budgets discussed in Section\n2. In the compute-constrained scenario, we have less compute than data, so any data mix discards\nmuch of the available data. In the data-constrained scenario, we have more compute than we have\ndata, so most data mixes will require epoching over at least one of, if not all of, our datasets.\nCompute-Constrained Experiments Prior works on learned data mixes have focused on this set-\nting Xie et al. (2024); Albalak et al. (2023), training for 50-100B tokens. Our first set of experiments\naligns with this as our largest model is trained for 100B out of 2.1T tokens. However, frontier models\nare increasingly trained longer and becoming \"data-constrained\" (Muennighoff et al., 2024).\nData-Constrained Experiments To identify data mixing methods applicable to frontier models,\nunderstanding the effects of epoching is essential for optimal performance (Goyal et al., 2024). Since\ntraining for the full 2.1T tokens in Dolma is infeasible for a large number of baselines, we instead\nsimulate data constraints using sub-sampling.\nOur simulation sub-samples each dataset to have T. De tokens where T is the total tokens in the\ndataset, Dt is the number of tokens we will actually train for, and Ds is the number of tokens we\nare simulating constraints for. This causes the epoching behaviour in the experiment to behave as it"}, {"title": "3.3 EVALUATION TASKS AND METRICS", "content": "Per-Task Evaluation Metrics Evaluating methods of Large Language Model pretraining with\nrespect to downstream performance is often challenging, since discrete metrics like accuracy can\nbe noisy at smaller scales (Wei et al., 2022; Schaeffer et al., 2024a). On the other hand, negative\nlog-likelihood (NLL) on pretraining data may not correlate with model utility for downstream tasks.\nTo strike a balance between scaling predictability and correlation with downstream performance,\nwe utilize the NLL per token on the correct answers from downstream benchmarks as our metric\nacross tasks. For multiple-choice tasks, we normalize the NLL by the probability assigned to all\noptions to produce a metric that correlates with accuracy improvements but improves predictably\nwith scale (Schaeffer et al., 2024b; Dubey et al., 2024).\nWe evaluate the impacts of data mixing on these metrics across benchmarks in 5 commonly-tested\nLLM capabilities: coding (Chen et al., 2021; Austin et al., 2021, HumanEval; MBPP), mathemat-\nics (Hendrycks et al., 2021b, MATH), translation (Goyal et al., 2022, FLoRes), reasoning (Clark\net al., 2018, ARC), and general knowledge (Hendrycks et al., 2021a, MMLU). We use the NLL\nover the correct answer for MBPP, HumanEval, MATH, and FLoRes and normalized NLL over the\ncorrect answer for ARC and MMLU.\nAcross-Task Evaluation Metrics We also report measures to ease comparison across tasks.\nSpeedup or slowdown is the ratio of FLOPs it takes to achieve the same performance as a baseline\nat a specific point based on the fit scaling curve for each task. In our experiments, we provide this\nmeasure with comparison to the performance at the largest scale of 3 \u00d7 1021.\nMean rank is a metric across all tasks. First each approach is ranked based on performance within\neach task, these ranks are then averaged across all tasks. We draw this metric from Thrush et al.\n(2024) as a way to succinctly capture the relative performance of methods across several tasks."}, {"title": "4 GENERALIZATION OF DATA MIXING METHODS", "content": "Model configuration, tokenizers, and shuffling are all likely to impact data mix experiments. To\ncontrol for these confounders, we re-implement\u00b9 and re-compute data mixes for Dolma using a\nunified setup and consistent random seed. In Figure 2, we plot the comparison of all baselines."}, {"title": "4.1 BASELINES", "content": "We compute Proportional and Uniform data mixes using the formulas from Section 2.1. For OLMO\nV1.7, we use the sampling proportions shared by the authors. We re-implement the UniMax algo-\nrithm using CVXPY (Diamond & Boyd, 2016) and compute two data mixes: one for our \u201ccompute-\nconstrained\" scenario and another for our \"data-constrained\" scenario. In the 100B scenario, we use\na single epoch cap while for the 1.6T token budget we use the two epoch cap used by OLMo V1.7.\nWe use DoReMi and Online Data Mixing (ODM) as our baselines for learned data mixes. While\nDoReMi uses the Proportional prior for reference models, it is unclear whether this is the optimal\nprior for Dolma. To account for this, we train three DoReMi variants with Uniform, Proportional,\nand OLMO V1.7 priors. Reference and proxy models are trained for 6 \u00d7 1019 FLOPs and are trained\nseparately for compute- and data-constrained experiments.\nIn the ODM paper, given reward \u00ca\u0125, the weights wt at step t are computed as (1 \u2013 \u039a\u0395t)o(Et\u22121R) +\nEt, where o denotes the softmax. We call this variant \u201cODM Paper\". However, Et\u22121 \u2192 0 as t\u2192\u221e\nwhich causes the softmax to become uniform independent of the rewards across datasets. We noted\nthat the open-source release of ODM removes Et-1 from the softmax, and confirm with the first\nauthor that the reported experiments used this code. We call this variant \u201cODM Github\u201d."}, {"title": "4.2 RESULTS", "content": "Generalization to data-constraints Our results emphasize the importance of simulating data con-\nstraints that occur in frontier training. Methods behave dramatically differently at different budgets.\nData mixes which are close to uniform perform well in compute-constrained settings and perform\npoorly in data-constrained settings, while the opposite is true for near proportional data mixes. Re-\nliable data mixing methods should perform well in both settings.\nThe Unreasonable Effectiveness of Uniform Utility Our second finding is that UniMax outper-\nforms other methods in both settings. Given the simplicity of UniMax it may be surprising that it\noutperforms learned and manual baselines. However, the effectiveness of assuming uniform utility\nis consistent with results in portfolio optimization (DeMiguel et al., 2009).\nThe superior performance of UniMax suggests that maintaining data diversity and scale is the major\ndriver of performance, particularly as training runs become data-constrained. Furthermore, it makes\nno assumptions of downstream tasks and can be computed at near zero cost. Due to these results we\ncompare primarily to UniMax throughout the rest of this work for clarity, with full results across all\nbaselines and experiments reported in A.8."}, {"title": "5 ESTIMATED DATA UTILITY OPTIMIZATION", "content": "Despite the effectiveness of UniMax, it is reasonable to believe that data sources have non-uniform\nutility for downstream tasks. The challenge is in estimating these utilities accurately and robustly\nhandling estimation errors. In Figure 3, we compare our proposed method, UtiliMax, using perfor-\nmance on validation sets as an estimate of utility."}, {"title": "5.1 ISOLATED DATA ABLATIONS", "content": "As an intrinsic measure of utility, we train proxy models for 6 \u00d7 1019 FLOPs and evaluate on\ndownstream tasks. We use the released held-out validation splits for this, except for MBPP where\nwe use HumanEval as held-out validation data. We estimate utility for each Dolma dataset, treating\neach perplexity bucket of CommonCrawl data separately, resulting in seventeen proxy models.\nGiven a set of D datasets and T downstream evaluations, this gives us a metric matrix M \u2208 R|D|\u00d7|T|.\nSince NLL metrics for each task vary significantly in scale we normalize the range of values for each\ntask to a normal distribution with mean 0.5 and range [0, 1] creating a utility matrix U."}, {"title": "5.2 UTILIMAX OPTIMIZATION METHODOLOGY", "content": "In equation 1, we formulate data mix optimization as maximizing risk-adjusted utility in abstract.\nHere, we describe the specific problem that we solve using the Splitting Conic Solver (O'Donoghue\net al., 2016; O'Donoghue, 2021) through CVXPY (Diamond & Boyd, 2016).\nUtiliMax maximizes utility by minimizing the L2 distance between the expected utility vector w\u2122U\nof our data mix across tasks and a theoretical optimal data mix which has a utility of 1 for all tasks.\nIn this work, we estimate risk associated by assuming that increasing allocation to a single dataset\nlinearly corresponds to the number of alternative datasets. Therefore we set the risk term to |D|ww.\nThis could also be interpreted in two ways: (1) as maximizing utility with a specific L2 regularization\nor (2) as interpolating between the utility maximizing solution and UniMax dependent on |D| .\nFinally, following UniMax, we set an epoching cap C on each dataset. With all of this established,\nUtiliMax is formulated concretely as follows:\narg max ||wTU \u2013 1||2 + |D|ww subject to 1Tw = 1, min(w) > 0, (2)\nw\n\n\u0412\u0442 \u0448\nt<C"}, {"title": "5.3 RESULTS", "content": "Our experimental results shows that diversity, in addition to utility, is integral to effective data mix-\ning. Using either the softmax or greedy optimization leads to poor results in at least one setting,\ndespite using natural estimates of dataset utility at significant computational cost. Only with the\nadded risk-adjustment in UtiliMax does the information from ablations lead to consistent perfor-\nmance and improve over UniMax in either setting.\nHowever, using ablations to estimate utility has major shortcomings compared to assuming uniform\nutility. First, the quality of ablation utility estimates depends on the validation set of the benchmark.\nIncreasingly, high-quality LLM benchmarks, such as GPQA (Rein et al., 2023) or HumanEval (Chen\net al., 2021), have either very small or non-existent validation sets. Furthermore, even when large\nhigh-quality validation sets do exist, running thes ablations can be prohibitively expensive."}, {"title": "6 REDUCING COST OF UTILITY ESTIMATION WITH LLMS", "content": "In order to address these shortcomings, we propose a method to use existing LLMs to estimate data\nutility at a vastly reduced cost, similar to the model-based quality filtering described in Dubey et al.\n(2024). Beyond reducing costs by removing the need for ablations, model-based utility estimates\nhave the advantage of being able to generalize specific validation data into a more general description\nof desirable data formats and domains. In Figure 4, we compare Model Estimated Data Utility\n(MEDU) to ablation-based and uniform utility estimates."}, {"title": "6.1 MODEL ESTIMATED DATA UTILITY METHODOLOGY", "content": "Our method prioritizes the following requirements. First, it must be end-to-end automated such that\nthere is minimal prompt engineering required to incorporate new benchmarks. Secondly, it must\nprovide estimates that are effective for UtiliMax at a fraction of the cost of ablations."}, {"title": "6.2 RESULTS", "content": "Our comparison of UniMax, Ablation-Based UtiliMax, and MEDU-Based UtilMax in Figure 4\nshows that MEDU does not significantly change results: UtiliMax outperforms UniMax using either\nMEDU or ablations. The largest regressions from MEDU are for MMLU and ARC Challenge, both\nmultiple choice tasks. Furthermore, in data-constrained settings, using MEDU improves results on\naverage at the largest scale compared to running ablations.\nIn Table 2, we show that across all methods both UtiliMax approaches achieve the best mean ranks,\nboth in the largest scales and across all scales. Given that MEDU is much cheaper to compute,\ncombining MEDU with UtiliMax is a pareto-optimal data mixing approach."}, {"title": "7 CONCLUSION", "content": "We highlight four takewaways from this work, both broadly for experiments on data mixing methods\nand for the methods we propose: UtiliMax and MEDU.\n\u2022 Effective data mixing requires balancing utility, diversity, and scale. While token-heuristic\nmethods focus on diversity and scale, learned data mixing approaches focus primarily on\ndata utility. UtiliMax succeeds largely because the optimization procedure it leverages\nconsiders all three factors, allowing to produce effective data mixes in a pareto-optimal\nfashion when combined with MEDU. Future work should consider improving the measures\nof each of these factors, such as \u2211 derived from multiple proxy models as in Thrush et al.\n(2024) with principled covariance estimators (Ledoit & Wolf, 2017).As our understanding\nof the utility of data increases, we see the potential of UtiliMax to serve as a principled\napproach for converting any utility estimate into a data mix to help train better LLMs faster.\n\u2022 Scalable text analysis from LLMs can improve LLMs themselves. Across disci-\nplines (Dubois et al., 2023; Ziems et al., 2024; Demszky et al., 2023; Guha et al., 2024),\nLLM-based data analysis is becoming a common approach to scale \"qualitative\" data anal-\nsis into quantifiable metrics. Although these likely contain measurement error and vari-\nance (Messeri & Crockett, 2024), as with all metrics, we show that they can be combined\nwith principled approaches which account for this to improve the models themselves.\n\u2022 Data mixing experiments must consider intended token-budget. At small token budgets,\nsome data mixes, such as uniform sampling, are very effective. However, only some of\nthese approaches generalize to data mixes at higher budgets. Similar is true in reverse, with\nsome methods which are weak at small-scales performing well at larger budgets. For data\nmixing methods to deliver predictable value, they must be tested under varied constraints.\n\u2022 An extremely simple baseline, UniMax, outperforms subsequent data mixing work. This re-\nsult is consistent across settings. UniMax even performs on par with UtiliMax for multiple\nbenchmarks which makes it a good baseline comparison candidate for those introducing\nnew methods."}, {"title": "A APPENDIX", "content": "A.1 CONTRIBUTIONS\nWill and Todor were the project leads for this work. Todor scoped the project, conceptualized\nMEDU, and oversaw the project from start to finish. Will made MEDU concrete, conceptualized\nUtiliMax, and ran all experiments presented in this work. Bhargavi, Mike, and Frank all contributed\ncore ideas resulting in the UtiliMax algorithm. Bhargavi debugged and refined early UtiliMax imple-\nmentations. Frank led efforts which identified and defined the evaluation methodology for scaling\nexperiments. Punit implemented the simulated epoching sub-sampler to enable data-constrained\nexperiments. All authors contributed to writing and refining the paper.\nA.2 ACKNOWLEDGEMENTS\nThis work was made possible by the collective expertise of the entire Llama team at Meta AI, espe-\ncially the Pretraining Data team. Additionally, experiments would not have been possible without\nthe Meta ML infrastructure teams which made running large-scale training runs and Llama inference\njobs a smooth process.\nWe are grateful to Sang Michael Xie and Alon Albalak for their helpful discussion of details on\ntheir related works. Furthermore, we would like to thank Kushal Tirumala, Niladri Chatterji, Tristan\nThrush, and Mirac Suzgun for conversations and comments which improved this work.\nA.3 OPTIMIZER ABLATIONS ON RELEVANCE SCORES"}, {"title": "A.4 MEDU VARIANCE AND SENSITIVITY ANALYSIS", "content": "A.4.1 VARIANCE INDUCED BY RANDOM SAMPLING"}, {"title": "A.4.2 VARIANCE ACROSS MODEL CHOICES", "content": "For the core experiments in the paper, we utilize the Llama 3.1 70B model. This scale was selected\nas it offers strong performance, but can still be run on a single node for inference. Furthermore,\nsince it is open-access it makes it easier to reproduce our results, while API models are subject to\nbreak reproducibility with version changes.\nHowever, in order to assess how sensitive MEDU may be to the selection of an underlying language\nmodel we did further analysis on several different language models. First, we assess the effects of\nscale comparing Llama 3.1 8B, 70B, and 405B. Then, we assess the variance across frontier model\nfamilies by comparing Llama, GPT, and Claude models.\nFor each model, we use the same prompts provided in Appendix A.6. This means that each model\nis used for the entire process, including generating benchmark descriptions which are then used for\nutility classification.\nIn Figure 9, we visualize the Pearson correlations between utility-scores estimated using each model.\nOverall, the correlation is strong (> 0.75) for all models, except for Llama 3 8b. This suggests that,\neven with different models, MEDU tends to capture similar signal from the underlying data. While\nthis signal may not be optimal, this consistency is important as it suggests that methods built on top\nof model-estimated data utility are unlikely to see significant shifts due to model selection, at least\nwithin the current generation of frontier models.\nTo add further evidence to this, in Figure 10, we run full experiments of MEDU UtiliMax with\nClaude Sonnet 3.5 and GPT-40. Since our experiments showed that data mixing methods have the\nlargest impact in compute constrained settings, we run this set of experiments only in that setting.\nOverall, we see that the performance of MEDU UtiliMax is robust to model selection, with Llama\n3 70B performing the best by a small margin overall but very little variance in downstream perfor-\nmance in any individual task."}, {"title": "A.5 COMPARING MEDU AND ABLATION-BASED UTILITY ESTIMATES", "content": "A.6 PROMPTS\nA.6.1 BENCHMARK DESCRIPTION\nf\"\"\"\n{corpus}\nHelp me decide the types of training data to look for to train a\nlanguage model for an evaluation with data similar to the\nabove.\nYou should keep the description brief and it is okay to generalize\nor abstract specific details to do so.\nGive your answer in three sections, first write what type of test\nthis might be from, then write out the languages, skills and\nknowledge the language model would need, and finally write a\ndescription of the ideal training data for the evaluation.\n\"\"\"\nA.6.2 DESCRIPTION MERGING\nf\"\"\"\n<BEGIN CORPUS DESCRIPTION A>\n{description_a}\n<END CORPUS DESCRIPTION A>\n<BEGIN CORPUS DESCRIPTION B>"}, {"title": "A.6.3 UTILITY CLASSIFICATION", "content": "f\"\"\"\nThe following document is being considered as training data for a\nLarge Language Model.\nProvide a concise description of the document and an assessment of\nthe quality of the text or code in the document.\nKey Attributes to Mention\nLanguages contained in the document\nThe coherence of the document\nThe skills the document demonstrates\nThe topics the document contains facts and information about\nDocument {prompt_addition}:\n{example}\n...\nBased on your previous reasoning, give me a concrete decision\nabout the utility of the document as training data for the\nfollowing benchmark. If a benchmark is Multilingual, you\nshould assume a high-degree of importance is placed on high-\nquality content in languages other than English.\n{test_description}\nOutput your decision about the utility of the data as one of the\nfollowing single words Great/Good/Okay/Poor/Useless without\nformatting.==\n\"\"\"\nA.7 DATA SHUFFLING, SAMPLING, AND PACKING\nConsider a data mix containing a list of datasets Di with their associated weights wi, where i \u2208\n{1..n}. We assume here that the weights wi have been normalized to 1. Additionally, let us suppose\nthat the overall batch size is B and sequence length is S. Therefore, in each step, we need to sample\nB sequences, each of length S tokens, from the different datasets Di.\nIn the beginning of training, we initialize a dataset iterator for each dataset Di, denoted as Iter(Di),\nwhich is responsible for packing together S tokens from dataset Di into a dense sequence. A seed\nseed(epoch) which is a function of the epoch count is specified to determine how the dataset is"}]}