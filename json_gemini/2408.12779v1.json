{"title": "Investigating LLM Applications in E-Commerce", "authors": ["Chester Palen-Michel", "Ruixiang Wang", "Yipeng Zhang", "David Yu", "Canran Xu", "Zhe Wu"], "abstract": "The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently gained ubiquitous use in many domains [19, 26, 28]. In the e-commerce domain in particular, LLMs have the potential to facilitate the creation of product descriptions, summarize reviews, expand queries, and answer buyer and seller questions, among other potential use cases.\nWe simulated a common practice in the e-commerce industry: adapting open source state of the art models for domain specific tasks. We compared the feasibility of using an LLM and explored to what extent using an LLM for different e-commerce tasks leads to gains in the evaluation metrics. Training LLMs from scratch requires a significant amount of resources, but a common practice to efficiently train the model is to use parameter efficient methods like low rank adapters (LoRA) [13]. Important questions arise when attempting to adapt these models for specific tasks and domains. We attempt to answer in this work: How much training data is needed to adapt a model to a task? How much do LLMs outperform more traditional approaches? What ways do different tasks interact with each other when doing mixed dataset training or merging LoRA weights trained on indivudal tasks? There are various approaches for adapting LLMs for tasks in a specific domain. Specifically, we focus on LoRA supervised fine-tuning (SFT), multi-task training, zero-shot inference, and LoRA merging.\nOur contributions are as follows: 1) Organizing and formatting four e-commerce datasets for the evaluation of large language models (LLMs). 2) Conducting comprehensive experiments to compare fine-tuning a large language model with conventional industrial baselines, such as BERT and T5, using varying amounts of data for e-commerce tasks; additionally, we examine the effectiveness of in-context learning with a highly competitive, very large LLM. 3) Exploring the use of mixed LoRA merging across different tasks and comparing this approach to traditional mixed dataset training.\nOur findings indicate that for e-commerce-specific tasks, conventional methods, such as training smaller language models, can achieve performance that is comparable to or even surpasses that"}, {"title": "2 Background", "content": "Large Language Models (LLMs) [1, 3, 6, 21, 27] have seen increasing attention in recent years as models that perform natural language generation have begun to be used for multiple tasks. They differ from prior pre-trained language models (PLMs), such as BERT [7] or T5 [24], in their amount of training data and number of parameters."}, {"title": "2.1 Instruction Fine-tuning", "content": "Instruction fine-tuning represented a pivotal advancement in the optimization of large language models (LLMs), such as GPT-4, for enhanced task-specific performance, especially in domain-specific applications [14, 33]. This method involved the supplementary training of a pre-trained based model such as GPT [21], Llama [27], or Falcon [1] on a task specific dataset consisting of prompts paired with their optimal responses. The objective was to refine the model's capacity to comprehend and execute instructions with increased accuracy and contextual relevance. Instruction fine-tuning has emerged as an invaluable technique for augmenting the proficiency of LLMs across various specialized domains, ensuring their outputs align more closely with user expectations and requirements."}, {"title": "2.2 Low-Rank Adaptation Training", "content": "Low-Rank Adaptation (LoRA) [13] is an innovative technique designed to fine-tune (LLMs) in a resource-efficient manner. This method addresses the challenge of adapting pre-trained models to specialized tasks without the extensive computational costs associated with traditional full-model fine-tuning. At the heart of LORA is the strategic introduction of trainable low-rank matrices that target specific components of the LLM's architecture, namely the attention and feed-forward neural network layers inherent to the transformer model. Specifically, it freezes the pre-trained layers of the LLM, and for each layer, it trains a rank-decomposition matrix and injects them into each layer of the pre-trained model to accomplish the LLM fine-tuning.\nLORA involves the addition of low-rank matrices A and B to the existing weight matrices W of the model. The model's original parameters are kept frozen during fine-tuning while A and B are updated. These matrices A and B are much smaller in size compared to W, enabling significant reductions in the number of trainable parameters. The adaptation occurs through the equation W' = W + A * BT, where W' represents the adapted weights. This process selectively fine-tunes the model, allowing it to acquire new capabilities or improve performance on specific tasks with minimal adjustments to its pre-trained parameters. This selective updating is particularly beneficial for domain-specific applications, where only certain aspects of the model's knowledge need refinement."}, {"title": "2.3 Evaluation", "content": "With the rise of text generation models that are seemingly capable of performing large numbers of tasks and able to answer many questions, a number of evaluation strategies have been proposed.\nEvaluation leaderboards often consist of evaluation tasks like Hellaswag [32], MMLU [12], and others [10\u201312] which cover a broad range of multiple choice questions. These types of multiple choice question evaluations, where the answer is chosen based on the choice with the highest likelihood can be brittle as rankings can be sensitive to minute details [2]. Other evaluation benchmarks like GLUE [29] consist of a bundle of different tasks with task specific evaluation metrics. Another approach to evaluating LLM performance is the approach of LLM as a judge. Chatbot arena [35] is an example of this style of evaluation and while it can correlate with human judgment, like other LLM applications it can be subject to hallucinations. In this work, we focus on directly evaluating the tasks of interest with existing scoring practices."}, {"title": "2.4 LLMs for E-commerce", "content": "Zhang et al. [33] find that fine-tuning LLM scaling factors appear to be very task dependent; however, there has been little published work examining fine-tuning focusing on the e-commerce domain. There has been some prior work investigating the use of LLMs on e-commerce tasks. ECOMGPT [17] looked at framing e-commerce tasks as instruction fine tuning, but doesn't explore LORA [13] or how different tasks enhance or interfere with each other or the amount of data required for reasonable performance."}, {"title": "3 E-Commerce Datasets", "content": "There are a limited number of e-commerce datasets publicly available. Currently, there are few e-commerce benchmarks for evaluating LLMs on e-commerce tasks. We collected four datasets covering classification, sequence labeling, and product description generation, and review summarization in order to evaluate the performance of LLMs in the e-commerce domain."}, {"title": "3.1 ESCI Multi-class Product Classification", "content": "The Shopping Queries ESCI dataset [25] contains search queries, released with the aim of fostering research in the area of semantic matching of queries and products. The dataset contains three tasks: Query-Product Ranking, Multi-Class Product Classification, Product Substitute Identification. We use the ESCI Multi-Class Product Classification task. The task is to classify a query and product pair as an exact match (E), a substitute (S), a complementary product (C) or Irrelevant (I). Because Query-Product Ranking and Product Substitute Identification involve more complexity and longer input strings, we do not include them for LLM evaluation in this work."}, {"title": "3.2 QueryNER", "content": "QueryNER [23] is an e-commerce query segmentation dataset. The task in QueryNER is not to extract aspects, but rather to segment"}, {"title": "3.3 Review Summarization", "content": "AMASUM [4] is a dataset for summarizing product reviews. The product reviews are in English and come from bestreviews.com, cnet.com, pmag.co.uk, runrepeat.com, which mainly consist of electronics and running shoes reviews. The dataset contains a list of product reviews and a summary with the \"verdict\" on the product and also lists of pros and cons. The original paper focuses on selecting useful reviews in order to summarize.\nThe goal of our evaluation of LLM performance is not to assess its review selection capability, so we select a small number of reviews to summarize. We select only 4 reviews to be used to generate the verdict summary. Since the dataset has a field with helpful votes where users voted that the review was helpful, we take the top four reviews with the most helpful votes as our selection process."}, {"title": "3.4 Product Description Generation", "content": "There appears to be a lack of standard benchmark datasets in English for product description generation despite a decent amount of prior work. Koto et al. [16] stated they were not able to release the dataset due to copyright issues. Chan et al. [5] and Zhang et al. [34] collected data from Taobao\u00b9. Zhang et al. [34] also stated that there was no other standard dataset for product description generation. Wang et al. [30] created their dataset from attribute values and descriptions from Amazon but only in the category \"Computers and Tablets\".\nWithout a clear prior benchmark for this task, we create a simple product description task from the ESCI Shopping Queries Dataset [25]. We assemble an input consisting of a product title, brand, color, and bullet points. The bullet points in the original dataset are aspect-value pair information about a product or short snippets about the product. The expected output is the product description. We filter out items where there is no title, no description, no bullet points, or items where the description is an exact match of the title or bullet points."}, {"title": "3.5 Task Alignment & Prompt Design", "content": "To enable instruction fine-tuning, each individual dataset was required to be aligned to a sequence to sequence style task in order to be used with an LLM. Review summarization and product description generation already were easily treated as sequence to sequence tasks. However it was less obvious how to best treat classification and sequence labeling tasks. We treated the classification"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Baselines", "content": "We chose to use the most common and competitive baselines for each e-commerce task. For classification tasks, ESCI Task 2 and QueryNER, we chose to use BERT (BERT-base) [7] as the baseline model with a learning rate of 3e-5 following Devlin et al. [7]. The training of BERT followed the conventional formulation of the sequence classification problem for the ESCI task and token classification for the QueryNER task.\nFor generative tasks, review summarization, and product description generation, we chose to use T5 (T5-base) [24] as a baseline model. T5 and BERT are fine-tuned with default parameters released by Hugging face [31]. Because in-context learning has been shown to be effective [8], we use we include Mixtral 8 x 22b [15] as a theoretical state of the art zero-shot and few-shot baseline."}, {"title": "4.2 Mix Tasks Training", "content": "Mixing tasks (datasets) for fine-tuning large language models (LLMs) can enhance the model's performance, generalization ability, and adaptability to various tasks, which closely mirrors the industrial application. The trained model was not required to accomplish one task but rather several domain-specific tasks such as query named entity recognition (NER), text summarization, description generation, and classification. Fine-tuning LLMs with mixed and diverse training datasets could help improve performance on each task."}, {"title": "4.3 Mix LoRA Merging", "content": "Furthermore, thanks to the nature of the LORA framework, mixed tasks (datasets) training could also be achieved by merging LoRA weights independently trained on different tasks. Specifically, assume there were n tasks with a specific task number i \u2208 {0, . . ., n - 1}. LoRA weights trained independently from each task could be defined as Ai and Bi. The adaptation equation for mixed LoRA merge was W' = W + \\frac{1}{n} \\sum_{i=0}^{n-1} A_i B_i^T. Mixing LoRA merging provided additional flexibility since additional tasks could be added later instead of retraining whenever a new task was added. Additionally, some types of tasks may benefit each other, while others may lower another tasks performance when merged. Mixing LoRA merge enabled more efficient experimentation with different combinations of tasks compared with directly training with the mixed data set."}, {"title": "4.4 Implementation Details", "content": "The foundation model used in all LoRA fine-tuning experiments was Llama-2-7b as its moderate size and considerable performance on various tasks [27]. The supervised fine-tune was conducted with"}, {"title": "5 Results", "content": null}, {"title": "5.1 Metrics on different tasks", "content": "Since the datasets we explored contained both classification and generative tasks, we use the task-specific metric to evaluate the performance of the model performance on various datasets. For classification tasks, we use F1 score as evaluation metrics and report both micro-average and macro-average results, while for the generative task, we use the Rouge-1 F1 (Rouge-1) and Rouge-L F1 (Rouge-L) [18] to evaluate the performance."}, {"title": "5.2 Evaluating LLM on Classification Tasks", "content": "To ensure accurate mapping of the generated text to the actual class label in the classification task, we implemented a simple but strict evaluation approach. For ESCI classification, the LLM output was required to match the corresponding label exactly. Any deviation from the exact label resulted in the classification being considered incorrect. For QueryNER, labels were extracted using a regular expression expecting a list of tuples of tokens with BIO tags. If the LLM output deviated from the expected output, the labels were considered all Os (no entities identified). Because we noticed the model sometimes being inconsistent with the output format, the regular expression also handles comma separated output without parentheses. In the case of further formatting issues or when the model does not predict labels for all tokens, it is assumed the model failed to generate a valid label sequence and that particular example is assigned all Os. Palen-Michel et al. [22] highlighted issues with NER scoring. For scoring procedure clarity, once the model output has been extracted from the linearized form, we use"}, {"title": "5.3 SFT LLMs vs Baseline PLMS", "content": "We performed SFT training of Llama2-7b on each dataset with different portions of the data to explore the impact of training data size for fine-tuning an LLM on each task."}, {"title": "5.3.1 Classification tasks", "content": "Table 2 shows the performance comparison on classification tasks on ESCI Classification and QueryNER dataset among Llama2-7b Supervised fine-tuning, BERT model finetuning and in context learning using Mixtral 8 x 22b in zero and few-shot setup. Note that, instead of generating the distribution like BERT, the task for the LLM is to generate the classification result in text. As the number of the training samples increased the performance of the model generally increased. However, there was a clear performance boost of LLM as the number of training samples increased (from 10k to 50k on ESCI task 2 dataset and from 1k to 5k on Query NER dataset). In general, the LLM and BERT performed comparable in these classification tasks when given sufficient training data.\nIn domain-specific tasks such as ESCI classification and Query NER, the application of in-context learning with very large language models like the Mixtral 8x22b often does not meet the performance benchmarks achieved through fine-tuning. Despite the introduction of extensive context, these models frequently struggle to deliver the level of accuracy required for industrial applications. This observation underscores a critical limitation: while LLMs are versatile and powerful, they may not be inherently optimized for tasks that demand high precision within a specialized domain.\nIn contrast, fine-tuning enables models to be specifically tailored to the intricacies of the task at hand, allowing for a deeper understanding of domain-specific patterns and nuances. As a result, training smaller, task-specific models such as BERT, particularly those employing a softmax classification layer, often emerges as a more effective strategy. These models not only demonstrate superior performance but also offer advantages in computational efficiency, making them more suitable for deployment in resource-constrained industrial environments where both accuracy and efficiency are paramount."}, {"title": "5.3.2 Generation task", "content": "Table 3 shows the performance comparison of text generation tasks on the review summarization and description generation datasets. Similar to the classification tasks, there was a significant increase in model performance as more data samples were used for training. Notably, the LLM consistently outperformed the conventional T5 model across both datasets. This superior performance can be attributed to the LLM's larger model capacity and enhanced quality of pre-training.\nFine-tuned models (Llama2-7b and T5) outperformed the zero-shot capabilities of the much larger Mixtral 8x22B model in review summarization, while for description generation, the performance is comparable. Despite the Mixtral model's strong standing on LLM leaderboards like [9], which suggests competitive summarization abilities, the observed performance gap between zero-shot and few-shot scenarios highlights a key limitation: without in-context guidance, the model struggles to achieve sufficient capability on domain-specific tasks (Review Summarization). However, when in-context information is provided, the model demonstrated significantly improved outcomes. In review summarization, to achieve even higher levels of performance, task-specific training becomes crucial. Notably, even with smaller model architectures, fine-tuning can yield superior results (using Llama2-7b).\nIn contrast, the description generation task is more aligned with general-purpose text generation, where the model's ability to understand and leverage general knowledge is the primary factor in determining performance. Consequently, in this task, larger models like Mixtral, equipped with in-context guidance, could achieve top-tier performance, even surpassing fine-tuned smaller models."}, {"title": "5.4 LORA Merge", "content": "We experimented with merging different pairs of LoRA weights for each pair of tasks. For this experiment, we used the LoRA weights from the 5k training set size. To merge the LoRA weights we took the average of the two. The results of merging LoRA weights, shown in Table 4, demonstrated that when weights trained on a task requiring a more rigid structure in the output like ESCI classification or QueryNER, the performance on those tasks suffers. However, the performance of description generation and review summarization remained comparable with the performance from independent training with the same number of examples.\nUpon reviewing model output, we found that at least a portion of this degradation in performance on the tasks requiring a more strict output format was attributable to the output formatting requirements. However, some of this apparent degradation may not truly be quite as bad as it appears.\nIn Section C of the appendix, we show an example of model output for the QueryNER task was shown where the model did in fact output BIO labels some of which are correct labels, but with formatting unrecognized by the scoring script."}, {"title": "5.5 Comparison with Mix Tasks Training", "content": "We merged the LoRA (Low-Rank Adaptation) weights from all four tasks for inference and performance analysis on each respective test set. The weights were chosen based on training with 5k samples from each dataset to ensure balanced information contribution. To establish a fair comparison between training with mixed data and mixing through LoRA weight merging, we trained the foundation model (Llama-2 7B) on 5k samples from each dataset, resulting in a total of 20k training samples, using the same hyperparameters as in the previous experiments.\nThe results, detailed in Table 5, indicated that mixed dataset training resulted in lower performance compared to training each task independently. This performance decrease was attributed to the limited capacity of the LoRA adapter and the distinct nature of each task, which deteriorated the model's ability to consistently produce outputs in the correct format especially in the classification tasks. Furthermore, the LoRA weight merging approach generally showed inferior performance compared to both mixed dataset training and independent task training. In mixed LoRA merging, classification tasks particularly suffered, with output format issues noted (see Section 5.3.2). However, for text generation tasks, the performance remained competitive."}, {"title": "6 Conclusion", "content": "In this paper, we explored the application potential of LLMs in addressing common e-commerce tasks, benchmarking against conventional industrial models like BERT and T5. We collected four e-commerce datasets, containing classification and text generation tasks, and adapted both task types to a text generation framework for LLMs. Our findings reveal that: (1) Zero-shot with a larger LLM is not a clear win and that smaller models fine-tuned on a specific task can have better task specific performance. (2) LLMs required a certain volume of training data to reliably produce the correct formats in classification tasks, yet they achieve competitive performance against the conventional BERT baseline and surpass the T5 baseline in text generation tasks. (3) Mix task training appears to perform slightly worse but comparable to independent training. (4) While LoRA merging for classification tasks did not consistently maintain output format, it demonstrated that merging in text generation tasks could still deliver competitive performance on individual tasks. (5) Few shot performance seems to be better than zero shot or fine tuning with very small amounts of data, but in all tasks except description generation, fine-tuning a model with enough data still performed better. Overall, we demonstrated that for specific tasks in the e-commerce domain, out-of-the box zero-shot LLM inference like our use of mixtral 8 x 22b does not outperform fine-tuning approaches on target tasks and there are opportunities for further exploration of adapting LLMs to the e-commerce domain."}]}