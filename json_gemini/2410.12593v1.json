{"title": "EXPAND AND COMPRESS: EXPLORING TUNING PRIN- CIPLES FOR Continual SpatiO-TEMPORAL GRAPH FORECASTING", "authors": ["Wei Chen", "Yuxuan Liang"], "abstract": "The widespread deployment of sensing devices leads to a surge in data for spatio- temporal forecasting applications such as traffic flow, air quality, and wind energy. Although spatio-temporal graph neural networks (STGNNs) have achieved suc- cess in modeling various static spatio-temporal forecasting scenarios, real-world spatio-temporal data are typically received in a streaming manner, and the net- work continuously expands with the installation of new sensors. Thus, spatio- temporal forecasting in streaming scenarios faces dual challenges: the inefficiency of retraining models over newly-arrived data and the detrimental effects of catas- trophic forgetting over long-term history. To address these challenges, we pro- pose a novel prompt tuning-based continuous forecasting method, EAC, follow- ing two fundamental tuning principles guided by empirical and theoretical analy- sis: expand and compress, which effectively resolve the aforementioned problems with lightweight tuning parameters. Specifically, we integrate the base STGNN with a continuous prompt pool, utilizing stored prompts (i.e., few learnable param- eters) in memory, and jointly optimize them with the base STGNN. This method ensures that the model sequentially learns from the spatio-temporal data stream to accomplish tasks for corresponding periods. Extensive experimental results on multiple real-world datasets demonstrate the multi-faceted superiority of EAC over the state-of-the-art baselines, including effectiveness, efficiency, universality, etc.", "sections": [{"title": "INTRODUCTION", "content": "Spatio-temporal data is ubiquitous in various applications, such as traffic management (Avila & Mezi\u0107, 2020), air quality monitoring (Liang et al., 2023), and wind energy deployment (Yang et al., 2024). Spatio-temporal graph neural networks (STGNNs) (Jin et al., 2023; 2024) have become a predominant paradigm for modeling such data, primarily due to their powerful spatio-temporal representation learning capabilities, which consider the both spatial and temporal dimensions of data by learning temporal representations of graph structures. However, most existing works (Li et al., 2017; Wu et al., 2019; Bai et al., 2020; Cini et al., 2023; Han et al., 2024) assume a static setup, where STGNN models are trained on the entire dataset over a limited time period and maintain fixed parameters after training is completed. In contrast, real-world spatio-temporal data (Liu et al., 2024; Yin et al., 2024) typically exists in a streaming format, with the underlying network structure expanding through the installation of new sensors in surrounding areas, resulting in a constantly evolving spatio-temporal network. Due to computational and storage costs, it is often impractical to store all data and retrain the entire STGNN model from scratch for each time period.\nTo address this problem, several straightforward solutions are available, as illustrated in Figure 1. The simplest approach involves pre-training an STGNN (using node-count-free graph convolution operators) for testing across subsequent periods. However, due to distribution shifts (Wang et al., 2024a), this method often fails to adapt to new period data. Another approach involves model retraining and prediction on different data windows due to graph expansion. Unfortunately, this neglects the informational gains from historical data, leading to limited performance improvements. To simultaneously resolve the challenges posed by these two issues, a more effective solution is to adopt a continual learning paradigm (Wang et al., 2024d), which is a research area focused on how"}, {"title": "RELATED WORK", "content": "Spatio-temporal Forecasting. Spatio-temporal forecasting originates from time series analysis and can be viewed as a temporal data modeling problem within an underlying network. Traditional statistical models, such as ARIMA (Box & Pierce, 1970) and VAR (Biller & Nelson, 2003), as well"}, {"title": "PRELIMINARIES", "content": "Definition (Dynamic Streaming Spatio-temporal Graph). We consider a dynamic streaming spatio-temporal graph $\\mathcal{G} = (\\mathcal{G}_1, \\mathcal{G}_2, ..., \\mathcal{G}_T)$, for every time interval $\\tau$, the network dynamically grows, i.e., $\\mathcal{G}_\\tau = \\mathcal{G}_{\\tau-1} + \\Delta \\mathcal{G}_\\tau$. Specifically, the network in the $\\mathcal{T}$-th time interval is modeled by the graph $\\mathcal{G}_\\tau = (\\mathcal{V}_\\tau, \\mathcal{E}_\\tau, \\mathcal{A}_\\tau)$, where $\\mathcal{V}_\\tau$ is the set of nodes corresponding to the $|\\mathcal{V}_\\tau| = n_\\tau$ sensors in the network, and $\\mathcal{E}_\\tau$ signifies the edges connecting the node set, which can be further represented by the adjacency matrix $\\mathcal{A}_\\tau \\in \\mathbb{R}^{n_\\tau \\times n_\\tau}$. The node features are represented by a three-dimensional tensor $\\mathcal{X}_\\tau \\in \\mathbb{R}^{n_\\tau \\times t \\times c}$, denoting the $c$ features of the records of all $n_\\tau$ nodes observed on the graph $\\mathcal{G}_\\tau$ in the past $t$ time steps. Following (Chen et al., 2021b), $c$ here is usually only a numerical value.\nProblem (Continual Spatio-temporal Graph Forecasting). The continual spatio-temporal graph forecasting can be viewed as learning the optimal prediction model for the current stage from dynamic streaming spatio-temporal graph data. Specifically, given the training data $\\mathcal{D} = \\{\\mathcal{D}_\\tau = (\\mathcal{G}_\\tau, \\mathcal{X}_\\tau, \\mathcal{Y}_\\tau)\\}_{T=1} \\sim \\mathcal{P}_{tr}$ from a sequence of streaming data, our goal is to incrementally learn the optimal model parameters $f_{\\theta^*}$ from the sequential training set. For the current $\\tau$-th time interval, the model is optimized to minimize:\n$f_{\\theta(\\tau)^*} = \\underset{\\theta(\\tau)}{\\operatorname{argmin}} \\mathbb{E}_{\\mathcal{D}_\\tau \\sim \\mathcal{P}(\\tau)} [\\mathcal{L}(f_{\\theta(\\tau)} (\\mathcal{G}_\\tau, \\mathcal{X}_\\tau), \\mathcal{Y}_\\tau)],$ (1)\nwhere $f_{\\theta(\\tau)^*}$ represents the optimal model that achieves the minimum loss when trained on the data from the current period $\\tau$. The loss function $\\mathcal{L}(\\cdot)$ measures the discrepancy between the predicted signals $\\hat{\\mathcal{Y}}_\\tau = f_{\\theta(\\tau)^*} (\\mathcal{G}_\\tau, \\mathcal{X}_\\tau) \\in \\mathbb{R}^{n_\\tau \\times t \\times c}$ for next $t$ time steps and the ground-truth $\\mathcal{Y}_\\tau$"}, {"title": "METHODOLOGY", "content": "In this section, we present detailed empirical observations and theoretical analysis to propose two tuning principles. Based on these principles, we introduce two minimalist modules, expand and compress of prompt parameter pool, to implement the continual spatio-temporal graph forecasting framework, EAC (as shown in Figure 2). Specifically, for the expand process, we observe from empirical studies that the prompt parameter pool adapts to dynamic heterogeneity, and we further provide a theoretical analysis of this phenomenon. Building on this, we introduce the implementa- tion of prompt parameter pool expansion to effectively adapt to heterogeneity in continuous spatio- temporal scenarios. For the compress process, we observe through empirical studies that the prompt parameter pool exhibits low-rank properties, followed by a detailed formal analysis. Based on this, we introduce the implementation of prompt parameter pool compression to effectively alleviate the parameter inflation issue caused by the expansion in continuous spatio-temporal scenarios. We summarize the workflow of EAC in Algorithm 1, and provide a detailed explanation of the continual spatio-temporal graph learning process in Appendix B."}, {"title": "EXPAND: HETEROGENEITY-GUIDED CONTINUOUS PROMPT POOL GROWTH", "content": "Insight. As mentioned above, fine-tuning an existing STGNN model with new data streams often leads to catastrophic forgetting (van de Ven et al., 2024). While previous methods have proposed some mitigative strategies (Chen et al., 2021b; Wang et al., 2023a;b), these solutions are not entirely avoidable. A straightforward solution is to isolate parameters, freeze the old model, and dynamically adjust the network structure to incorporate adaptable learning parameters. Recently, there has been an increasingly common consensus in spatio-temporal forecasting to introduce node-specific trainable parameters as spatial identifiers to achieve higher performance (Shao et al., 2022; Liu et al., 2023; Dong et al., 2024; Yeh et al., 2024). Although some empirical evidence (Shao et al., 2023; Cini et al., 2024) supports their predictive performance in static scenarios, there has been no root analysis to explain why they are useful, when they are applicable, and in what contexts they are most suitable. However, we find this closely aligns with our motivation and extend it to the continual spatio-temporal forecasting setting by providing a reasonable explanation from the perspective of heterogeneity to address these questions. Specifically, spatio-temporal data generally exhibit two characteristics: correlation and heterogeneity (Geetha et al., 2008; Wang et al., 2020). The former is naturally captured by various STGNNs, as they automatically aggregate local spatial and temporal information. However, given the message-passing mechanism of STGNNs, the latter is clearly not captured. Therefore, we argue that the introduction of node prompt parameter pool likely enhances the model's ability to capture heterogeneity by expanding the expressiveness of the feature space.\nEmpirical Observation. To quantitatively analyze heterogeneity, we consider the dispersion of node feature vectors in the feature space (Fan et al., 2024). We first define the Average Node Deviation (D(\u00b7)) metric as: $D(\\mathcal{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} ||\\mathcal{X}_{ik} - \\mathcal{X}_{jk}||^2$, where $\\mathcal{X} \\in \\mathbb{R}^{n \\times d}$ represent the feature matrix composed of $n$ node vectors, each with $d$ dimensions. This metric quantifies the degree of dispersion between pairs of node vectors within the feature matrix, reflecting the ability to express heterogeneity. We use this indicator to plot the dispersion degree of the feature matrix"}, {"title": "Theoretical Analysis", "content": "Below, we provide a theoretical analysis of the above empirical results.\nProposition 1. For an original node input feature matrix $X = [x_1,\\ldots, x_n] \\in \\mathbb{R}^{n \\times d}$, we introduce a node prompt parameter matrix $P = [P_1,\\ldots,\\ldots,P_n] \\in \\mathbb{R}^{n \\times d}$. Through a spatio-temporal learning function $f_{\\theta}$ with invariance, a new feature matrix $X^{\\theta} = f(\\theta; X, P)$ is obtained, satisfying:\n$D(X^{\\theta}) - D(X) = \\frac{2}{n} \\sum_{i=1}^{n}(||p_i||^2 - ||\\mu||^2) \\geq 0,$(2)\nwhere $P^{\\theta} = [p_1,\\ldots,p_n] \\in \\mathbb{R}^{n \\times d}$ represents the optimized prompt parameter matrix, and $\\mu = \\frac{1}{n}\\sum_{i=1}^n p_i^{\\theta}$ is the mean vector of the parameter matrix.\nProof. For more details, refer to the supplementary materials in appendix A.1."}, {"title": "Tuning Principle I: Prompt Parameter Pool Can Continuously Adapt to Heterogeneity Property.", "content": "Implementation Details. Based on the above analysis, we present the implementation details for expand process in continuous spatio-temporal forecasting scenarios. Specifically, we continuously maintain a prompt parameter pool P. For the initial static stage, we provide each node with a learnable parameter vector, and the matrix $P^{(1)}$ of all such vectors is added to the parameter pool $\\mathcal{P} = [P^{(1)}]$. Follow the Occam's razor, we adopt a simple yet effective fusion method, where the prompt pool and the corresponding input node features are added element-wise. The prompt parameter pool P is then trained together with the base STGNN model. For subsequent period $\\tau$, we only provide prompt parameter vectors for newly added nodes, and the resulting matrix $P^{(\\tau)}$ is added to the prompt pool $\\mathcal{P} = [P^{(1)}, P^{(2)}, . . ., P^{(\\tau-1)}]$. As we analyzed, we freeze the STGNN backbone and only tuning the prompt pool P, effectively reducing computational costs and accelerate training."}, {"title": "COMPRESS: LOW-RANK-GUIDED CONTINUOUS PROMPT POOL REDUCTION", "content": "Insight. While node-customized prompt parameter pools are highly effective, an unavoidable chal- lenge arises in our scenario of continuous spatio-temporal forecasting: the number of prompt pa- rameters continuously increases with the addition of new nodes across consecutive periods, leading to parameter inflation. Despite the existence of numerous well-established studies that enhance the efficiency of spatio-temporal prediction (Bahadori et al., 2014; Yu et al., 2015; Chen et al., 2023; Ruan et al., 2024) and imputation (Chen et al., 2020; 2021a; Nie et al., 2024) tasks using techniques such as compressed sensing and matrix / tensor decomposition, these study typically focus solely on the original spatio-temporal data. An intuitive solution is to similarly apply low-rank matrix approximations to the prompt learning parameter pool, thereby reducing the number of learnable parameters while maintaining performance. However, for the prompt learning parameter pool, it remains to be validated whether it exhibits redundancy characteristics akin to spatio-temporal data and whether these properties hold in the continuous spatio-temporal forecasting setting.\nEmpirical Observation. To explore redundancy, we conduct a spectral analysis of the prompt parameter pool P. Specifically, for the models optimized annually on the PEMS-Stream dataset, we first apply singular value decomposition to the extended prompt parameter pool introduced in the"}, {"title": "Theoretical Analysis", "content": "Below, we provide a theoretical analysis of the above empirical results.\nProposition 2. Given the node prompt parameter matrix $P \\in \\mathbb{R}^{n \\times d}$, there will always be two matrices $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times d}$ such that P can be approximated as AB when the nodes n grow large, and satisfy the following probability inequality:\n$\\operatorname{Pr}(||P - AB||_F < \\epsilon ||P||_F) \\geq 1 - o(1)$ and $k = O \\left(\\log (\\min (n, d))\\right)$ where $o(1)$ represents a term that becomes negligible even as n grows large.\nProof. For more details, refer to the supplementary materials in appendix A.2."}, {"title": "Tuning Principle II: Prompt Parameter Pool Can Continuously Satisfy the Low-rank Property.", "content": "Implementation Details. Formally, we present the implementation details for compress process in continuous spatio-temporal forecasting scenarios based on the aforementioned analysis. Specifically, for the initial static stage, we approximate the original prompt parameter $P^{(1)}$ using the prod- uct of the subspace parameter matrix $A^{(1)}$ and the adjustment parameter matrix B. For subsequent periods $\\tau$, we provide only the subspace parameter matrix $A^{(\\tau)}$ for the newly added node vectors, approximating the prompt parameter $P^{(\\tau)}$ through the product with the adjustment parameter matrix B. As analyzed, the dimensionality of the subspace parameter matrix A is significantly smaller than that of the prompt parameter P, while the number of parameters in the adjustment matrix B remains constant; thus, we effectively mitigate the inflation issue."}, {"title": "EXPERIMENTS", "content": "In this section, we conduct extensive experiments to investigate the following research questions:\n\u2022 RQ1: Can EAC outperform previous methods in accuracy across various tasks? (Effectiveness)\n\u2022 RQ2: Can EAC have a consistent improvement on various types of STGNNs? (Universality)\n\u2022 RQ3: How efficient is EAC compared to different methods during the training phase? (Efficiency)\n\u2022 RQ4: How many parameters does EAC require tuning compared to baselines? (Lightweight)\n\u2022 RQ5: How does EAC compare to other common prompt-adaptive learning method? (Simplicity)"}, {"title": "EXPERIMENTAL SETUP", "content": "Dataset and Evaluation Protocol. We use real-world spatio-temporal graph datasets from three do- mains: transportation, weather, and energy, encompassing common streaming spatio-temporal fore- casting scenarios. The transportation dataset, PEMS-Stream, is derived from benchmark datasets in previous research (Chen et al., 2021b), covering dynamic traffic flow in Northern California from 2011 to 2017 across seven periods. The weather dataset, Air-Stream, originates from the real-time urban air quality platform of the Chinese Environmental Monitoring Center capturing dynamic"}, {"title": "EFFECTIVENESS STUDY (RQ1)", "content": "Overall Performance. We report a comparison between EAC and typical schemes (including rep- resentative improved methods 2) in Table 1, where the best results are highlighted in bold pink and the second-best results in underlined blue. A indicates the reduction of MAE compared to the second-best result, or the increase of other results relative to the second best result. Moreover, due to the unavailability of official source code for PECMP and TFMoE, along with the more complex backbone network used by TFMoE, the comparisons may be unfair. Nonetheless, we also include comparable reported values, as shown in Table 2. Based on the results, we observe the following:\nPretrain-ST methods generally yield the poorest results, especially on smaller datasets (i.e., Engery-Stram), aligning with the intuition that they directly use a pre-trained model for zero-shot forecasting in subsequent periods. Even with better pre-training on larger dataset (i.e., Air-Stream), performance remains mediocre. Retrain-ST methods also exhibit unsatisfactory results, as they rely on limited data to train specific phase models without effectively utilizing historical informa- tion gained from the pretrained model. Online-ST-NN methods perform poorly, as they fine-tune the pretrained model using only new node data differing from the old pattern. Despite TF- MoE's improvements through complex design, severe catastrophic forgetting remains an issue. Online-ST-MN methods strike a balance between performance and efficiency, showing some im-"}, {"title": "UNIVERSALITY STUDY (RQ2)", "content": "Setting. We further aim to demonstrate the universality of our EAC in enhancing the performance of various STGNN backbones, an aspect largely overlooked in previous studies. Specifically, STGNN can be categorized into spectral-based and spatial-based graph convolution operators, as well as recurrent-based, convolution-based, and attention-based sequence modeling operators. We select a representative operator from each category to form six distinct models, where the core architecture consists of two interleaved graph convolution modules and one sequence module. A detailed de- scription of the different operators can be found in Appendix D. Additionally, we adapt different models to the prompt parameter pool proposed by EAC to compare the performance impact."}, {"title": "EFFICIENCY & LIGHTWEIGHT STUDY (RQ3 & RQ4)", "content": "Overall Analysis. We first conduct a comprehensive comparison of the EAC with other baselines in terms of performance, training speed, and memory usage. All models are configured with the same batch size to ensure fairness. As illustrated in Figure 6, we visualize the performance, average tuning parameters, and average training time (per period) of different methods on both the smallest dataset (Energy-Stream) and the largest dataset (Air-Stream). Our observations are as follows:\nOn datasets with a smaller number of nodes, our EAC consistently outperforms the others, achiev- ing superior performance with only half the number of tuning parameters. Furthermore, the average training time per period accelerates by a factor of 1.26 to 3.02. In contrast, other methods such"}, {"title": "SIMPLICITY STUDY (RQ5)", "content": "Simplicity Analysis. Lastly, we aim to explore the simplicity of the node parameter prompt pool, as well as the effectiveness of the expan- sion and compression principle. We selected a common low-rank adaptation (LoRA) (Hu et al., 2021; Ruan et al., 2024) technique, which has recently been widely used in large language models. Following the default architecture, we added low-rank adaptation layers to the sequence operators, setting the rank to 6, and fine-tuned the back- bone model during each period. As shown in Table 4, we observe that simply applying LoRA layers without considering the specific spatio-temporal context of streaming parameters may not be highly effective. Moreover, our method enjoy shorter training times compared to LoRA-based approaches, further validating the superiority of our proposed expansion and compression tuning principle."}, {"title": "CONCLUSION", "content": "In this paper, we derive two fundamental tuning principle: expand and compress for continual spatio- temporal forecasting scenarios through empirical observation and theoretical analysis. Adhering to these principle, we propose a novel prompt-based continual forecasting method, EAC, which ef- fectively adapts to the complexities of dynamic continual spatio-temporal forecasting problems. Experimental results across various datasets from different domains in the real world demonstrate"}, {"title": "MORE DISCUSSION", "content": "In this paper, we thoroughly investigate methods for continual spatio-temporal forecasing. Based on empirical observations and theoretical analysis, we propose two fundamental tuning principle for sustained training. In practice, we consider this kind of continual learning to fall under the paradigm of continual fine-tuning. However, given the superiority and generality of our approach, we believe this provides an avenue for future exploration of continual pre-training. While we have made a small step in this direction, several limitations still warrant attention.\n\u25cf All current baselines and datasets primarily focus on scenarios involving the continuous expansion of spatio-temporal graphs, with little consideration given to their reduction. This focus is reason- able, as spatio-temporal data is typically collected from observation stations, which generally do not disappear once established. Consequently, existing streaming spatio-temporal datasets are predom- inantly expansion-oriented. Nonetheless, there are exceptional circumstances, such as monitoring anomalies at stations or the occurrence of natural disasters leading to station closures, resulting in node disappearance. But, we want to emphasize that our method can effectively handle such situ- ations, as our prompt parameter pool design is node-level, allowing for flexible selection of target node sets for parameter integration.\nAll current baselines and datasets span a maximum of seven years. We believe this is a reasonable constraint, as a longer time frame might lead to drastic changes in spatio-temporal patterns. A more practical approach would be to retrain the model directly in the current year. However, research extending beyond this time span remains worthy of exploration, which we leave for future work.\nDue to the node-level design of our prompt parameter pool, the issue of parameter bloat is in- evitable. Although we have proposed effective compression principles in this paper, other avenues for compression, such as parameter sparsification and pruning, also merit investigation. Given that we have innovatively provided guiding principles for compression in this study, we reserve further improvement efforts for future research."}, {"title": "FUTURE WORK", "content": "One lesson learned from our experiments is that the initially pre-trained spatio-temporal graph model is crucial for the subsequent continuous fine-tuning process. This is highly analogous to today's large language models, which compress rich intrinsic knowledge from vast corpora, allowing them to per- form well with minimal fine-tuning in specific domains. Therefore, we consider a significant future research direction to be the training of a sufficiently large foundation spatio-temporal model from scratch, utilizing data from diverse fields and scenarios. While some discussions and studies have emerged recently, we believe that a truly foundation spatio-temporal model remains a considerable distance away. Thus, we view this as a long-term goal for future work."}, {"title": "A THEORETICAL PROOF", "content": "A.1 PROMPT PARAMETER POOL CAN CONTINUOUSLY ADAPT TO HETEROGENEITY PROPERTY\nProof. First, based on the definition of the average node vector dispersion, the original feature matrix can be further rewritten as:\n$D(X) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n}  ||x_i - x_j ||^2.$(3)\nFor the spatio-temporal learning function $f_{\\theta}$ with invariance (i.e., a frozen STGNN backbone net- work), we have $X^{\\theta} = f(\\theta; X, P) = X + P^{\\theta}$, if the network converges, $P^{\\theta}$ can be obtained through nonlinear fitting. Therefore, for vectors in the matrix, we denote:\n$x^{\\theta} = x + p^{\\theta}.$(4)\nSimilarly, substituting this into the new feature matrix, it can be rewritten as:\n$D(X^{\\theta}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} ||x_i + p_i^{\\theta} - x_j - p_j^{\\theta} ||^2$(5)\nBased on the following equation property:\n$\\sum_{i,j} ||x_i - x_j ||^2 = 2n \\sum_{i=1}^{n} ||x_i||^2 - 2 \\sum_{i=1}^{n} x_i$.(6)\nThus, the difference between Equation 5 and Equation 3 can be rewritten as:\n$D(X^{\\theta}) \u2013 D(X)$\n$= \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} ||x_i + p_i^{\\theta} - x_j - p_j^{\\theta} ||^2 - \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n}  ||x_i - x_j ||^2$\n$= \\frac{1}{n^2}(2n \\sum_{i=1}^{n} ||x_i + p_i^{\\theta} ||^2 - 2 \\sum_{i=1}^{n}(x_i + p_i^{\\theta})^2) - (2n \\sum_{i=1}^{n} ||x_i||^2 - 2 \\sum_{i=1}^{n} x_i)^2)$(7)\n$= \\frac{2}{n} \\sum_{i=1}^{n} ||x_i + p_i^{\\theta} ||^2 - \\frac{2}{n^2} \\sum_{i=1}^{n}(x_i + p_i^{\\theta}) - \\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + \\frac{2}{n^2} \\sum_{i=1}^{n} x_i$\nThe mean vectors of the original feature matrix and the prompt parameter matrix are defined as follows:\n$\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i,  \\mu^{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} p_i^{\\theta}$(8)\nTherefore, we have:\n$\\sum_{i=1}^{n} x_i = n \\mu, \\sum_{i=1}^{n} p_i^{\\theta} = n \\mu^{\\theta}$(9)"}, {"title": "Theoretical Proof", "content": "Substituting this into Equation 7 and further simplifying yields:\n$D(X^{\\theta}) \u2013 D(X)$ \n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i + p_i^{\\theta} ||^2 - \\frac{2}{n} ||\\mu + \\mu^{\\theta} ||^2 + \\frac{2}{n} ||\\mu||^2$\n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i + p_i^{\\theta} ||^2 - 2||\\mu + \\mu^{\\theta} ||^2 - \\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + 2||\\mu||^2$\n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + ||p_i^{\\theta} ||^2 + 2x_i p_i - 2 (||\\mu||^2 + 2 \\mu \\mu^{\\theta} + ||\\mu^{\\theta} ||^2) + 2||\\mu||^2$\n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 + 2x_i p_i - 2 (||\\mu||^2 + 2 \\mu \\mu^{\\theta} + ||\\mu^{\\theta} ||^2) + 2||\\mu||^2$\n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 + 2\\frac{1}{n} (\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} p_i^{\\theta}) - 2||\\mu^{\\theta} ||^2$ \n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + \\frac{2}{n} \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 + 4 (\\frac{1}{n} \\sum_{i=1}^{n} x_i )^T (\\frac{1}{n} \\sum_{i=1}^{n} p_i^{\\theta}) - 2n ||\\mu^{\\theta} ||^2$\n$=\\frac{2}{n} \\sum_{i=1}^{n} ||x_i||^2 + \\frac{2}{n} \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 + 4 ||\\mu||^T ||\\mu^{\\theta} || - 2n ||\\mu^{\\theta} ||^2$\n$= \\frac{2}{n} \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 - 2n ||\\mu^{\\theta} ||^2$(10)\nAccording to the corollary of the Cauchy-Schwarz inequality (Horn & Johnson, 2012):\n$||\\sum_i x_i||^2 \\leq n \\sum_i ||x_i||^2$(11)\nTherefore, for any set of vectors $\\{p_i^{\\theta}\\}$, we have:\n$\\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 \\geq \\frac{1}{n} ||\\sum_{i=1}^{n} p_i^{\\theta} ||^2 -  n ||\\mu^{\\theta} ||^2$(12)\nThat is:\n$\\frac{1}{n} \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 - ||\\mu^{\\theta} ||^2 \\geq 0$(13)\nThus,\n$D(X^{\\theta}) - D(X) = \\frac{2}{n} (\\frac{1}{n} \\sum_{i=1}^{n} ||p_i^{\\theta} ||^2 - ||\\mu^{\\theta} ||^2) \\geq 0$(14)\nThis completes the proof."}, {"title": "Theoretical Proof", "content": "A.2 PROMPT PARAMETER POOL CAN CONTINUOUSLY SATISFY THE LOW-RANK PROPERTY\nProof. We first construct a Random Matrix $\\Phi \\in \\mathbb{R}^{k \\times n}$. Let $\\Phi$ be a random matrix with entries $\\phi_{ij}$ drawn independently from the standard normal distribution scaled by $1/\\sqrt{k}$:\n$\\phi_{ij} \\sim \\mathcal{N}\\left(0, \\frac{1}{k}\\right)$.\nNext, we can define matrices $A = \\Phi^T \\in \\mathbb{R}^{n \\times k}$ and $B = \\Phi P \\in \\mathbb{R}^{k \\times d}$. Thus, we have $AB = \\Phi^T (\\Phi P) = (\\Phi^T \\Phi) P$.\nConsider the approximation error:\n$||P - AB||_F = ||(I_n - \\Phi \\Phi^T) P||_F \\leq ||I_n - \\Phi \\Phi^T||_2 ||P||_F$."}, {"title": "METHOD DETAIL", "content": "B.1 METHOD ALGORITHM\nAlgorithm 1 The workflow of EAC for continual spatio-temporal graph forecasting\nInput:\nDynamic streaming spatio-temporal graph $\\mathcal{G"}, "mathcal{G}_1, \\mathcal{G}_2,\\ldots,\\mathcal{G}_T)$\nObservation data $\\mathcal{X} = (\\mathcal{X}_1, \\mathcal{X}_2,\\cdots, \\mathcal{X}_T)$.\nOutput:\nA prompt parameter pool P (in memory).\nPipeline:\n1: while Stream Graph G remains do\n2: if $\\tau == 1$ then\n3: Construct an initial prompt parameter pool: $\\mathcal{P} = A^{(\\tau)} B$. Tuning Principle II: Compress\n4: Fusion of observed data X and prompt parameter pool P: $\\mathcal{X}_{\\tau} = \\mathcal{X}_{\\tau} + \\mathcal{P}$.\n5: Jointly optimize the base STGNN $f_{\\theta}$ and P: $f_{\\theta}^* = \\operatorname{argmin}_{\\theta} f_{\\theta}(\\mathcal{X}_{\\tau}, \\mathcal{G}_{\\tau})$.\n6: else\n7: Reload the prompt pool P and model $f_{\\theta}^*$.\n8: Detect new nodes and construct a prompt parameter matrix $A^{(\\tau)}$.\n9: Add new node prompts to the prompt parameter pool: $\\mathcal{P} = \\mathcal{P}.append(A^{(\\tau)} B)$ Tuning Principle I: Expand\n10: Fusion of observed data X and prompt parameter pool P: $\\mathcal{X}_{\\tau} = \\mathcal{X}_{\\tau} + \\mathcal{P}$.\n11: Jointly optimize the frozen STGNN $f_{\\theta}^*"]}