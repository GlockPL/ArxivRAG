{"title": "A Stock Price Prediction Approach Based on Time Series Decomposition and Multi-Scale CNN using OHLCT Images", "authors": ["Zhiyuan Pei", "Jianqi Yan", "Jin Yan", "Bailing Yang", "Ziyuan Li", "Lin Zhang", "Xin Liu", "Yang Zhang"], "abstract": "Stock price fluctuations are influenced by a variety of factors, including macroeconomic conditions, government policies and market sentiment, which together make price movements complex and difficult to be predicted. Despite many studies aimed at enhancing stock price prediction models, the challenges such as data noise, model overfitting and lack of interpretability are still encountered. To address these issues and improve prediction accuracy, this paper proposes a novel method, named Sequence-based Multi-scale Fusion Regression Convolutional Neural Network (SMSFR-CNN), for predicting stock price movements in the China A-share market.\nFirstly, the historical opening price, highest price, lowest price, closing price, and turnover rate (OHLCT) of stocks are converted into the images, separated by weekends, incorporating specific time information to help the CNN learn the impact of different trading periods. Secondly, traditional CNN methods for stock image recognition often suffer from overfitting and difficulty in finding the correct convergence directions. To mitigate this, long sequences of stock features are decomposed into multiple time periods, and OHLCT images at different time scales are utilized as inputs, thus significantly reducing overfitting. Thirdly, in order to overcome the problem that classification labels lose information about the magnitude of stock price changes, we introduce regression labels so that the model can learn more latent features of stock price fluctuations. Meanwhile, we further integrate time series features into the model.\nBy utilizing CNN to learn sequential features and combining them with image features, we improve the accuracy of stock trend prediction on the A-share market stock dataset. This approach reduces the search space for image features, stabilizes and accelerates the training process. Extensive comparative experiments on 4,454 A-share stocks show that the proposed model achieves 61.15% for positive predictive value and 63.37% for negative predictive value of the stock price trend over the next 5 days, resulting in a total profit of 165.09%.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of financial markets and the acceleration of globalization, stock investment has become an essential avenue for many investors to achieve wealth appreciation [1, 2, 3]. However, the complexity and uncertainty of the stock market make accurately predicting stock trends a challenging task. The China A-share market, being one of the largest and most dynamic globally, has attracted significant quantitative analysis and research due to the growing impact of economic globalization [4, 5, 6].\nTraditional stock analysis methods heavily reliant on financial data and macroeconomic indicators and often fall short in capturing the dynamic and non-linear relationships within the market [7, 8, 9, 10]. Therefore, quantitative trading has emerged as a pivotal component of modern financial strategies. By leveraging computational techniques, it enables the execution of trading strategies devoid of emotion-based decision-making, thereby uncovering patterns that may elude human analysts [11]. The potential quantitative trading in stock market forecasting has been well-documented [12]. This progress has led to the development of advanced deep models that can handle the inherent complexity, nonlinearity, and noise of financial markets, making"}, {"title": "2. Related Work", "content": "The application of CNNs in predicting stock prices has gained significant attention for their capacity to capture complex patterns in time-series data. Early approaches primarily relied on traditional machine learning techniques. However, the emergence of deep learning brought innovative ways to handle the inherent non-linearities and temporal dependencies in financial data.\nThe use of CNNs in stock price prediction was motivated by their success in image recognition tasks, where spatial hierarchies of features are learned. This concept was extended to financial data by treating time-series data as a form of sequential image data, allowing CNNs to extract meaningful features from raw inputs. Aadhitya et al. [30] highlighted how CNNs could be employed to capture the short-term patterns in stock data, which were crucial for making accurate predictions. By combining CNNs with the Long Short-Term Memory (LSTM) networks, their model leveraged both spatial and temporal features, and thus improved the prediction accuracy compared to traditional models. Another significant contribution was given by Hoseinzade et al. [24], where the authors proposed a CNN-based model that incorporated a wide range of financial indicators. The study demonstrated that CNNs could effectively process and learn from diverse data representations, enhancing the robustness of stock price predictions. The model's ability to integrate different data sources was particularly advantageous in the volatile stock market environment, where multiple factors simultaneously influenced stock prices.\nSeveral studies proposed enhancements to the basic CNN architecture to better suit the unique characteristics of financial data. For instance, Hoseinzade et al. [31] proposed a universal CNN framework that adapted to various stock markets by tuning the network's hyperparameters. This adaptability was crucial for deploying models across different markets with varying levels of volatility and liquidity.\nMoreover, the integration of CNNs with other deep learning models was explored to address the limitations of standalone CNNs. The work by Kim et al. [32] combined the strengths of CNNs and LSTMs allow the model to capture both local dependencies in stock prices and long-term trends. This hybrid model demonstrated superior performance in predicting stock prices over different time horizons, showcasing the synergy between CNNs and recurrent architectures.\nA novel application of CNNs was in the analysis of candlestick charts. The study by Hu et al. [33] employed CNNs to automatically learn features from candlestick chart images, enabling the model to make informed investment decisions. This approach automated the traditionally manual task of chart analysis and also enhanced the accuracy of predictions by capturing intricate patterns that might have been overlooked by human analysts. Similarly, the work by Rosdyana et al. [34] further validated the effectiveness of CNNs in analyzing visual representations of stock data. The study illustrated how CNNs could be trained to recognize specific candlestick patterns, which were then used to predict future stock movements. Their method highlighted the potential of CNNs to transform visual financial analysis into a data-driven, automated process.\nWhile CNNs showed great promise in stock price prediction, several challenges remained. The primary concern was the model's capability to generalize across various market conditions, which could vary drastically over time. Furthermore, the high dimensionality of financial data posed a risk of"}, {"title": "3. Materials and Methods", "content": "In this part, we primarily focus on presenting the various data modalities utilized in our framework and introducing a novel data format employed in our experiments. Using time series as feature inputs in deep models is crucial and widely used in stock market analysis and forecasting. By integrating several features, including fundamental statistical characteristics, daily returns, price volatility, SMA results, trading volume, transaction volume and lag feature, a set of multivariate time series are obtained as the time series input features for the proposed model.\nIn addition, many image formats as inputs for deep models are commonly utilized in quantitative trading, including candlestick charts [34, 35], line charts [36], the Open-High-Low-Close-Volume (OHLCV) charts [29, 37], etc. Some researches constructed the images for convolutional neural networks (CNNs) by applying the sliding window approach to time series [38]. Various image formats are employed to depict crucial information in the stock data and act as input features for the deep learning model. This demonstrates the efficacy and immenses possibilities of utilizing visuals.\nWe give the main reason for using the turnover rate feature to replace the volume feature. Adjustment has an impact on the volume feature of stock trading in the A-share market. Adjustment refers to the procedure of adjusting historical price data to reflect the impact of corporate actions (e.g., dividends, stock bonuses, stock placements, etc.) on the share prices. Volumes also need to be adjusted for the adjustment ratios, and the adjusted volume data make direct comparisons between different time points or different stocks difficult. Therefore, given the historical trading volume problem, the concept of utilizing alternative attributes in place of trading volume has emerged. Our findings indicate that substituting turnover rate for trading volume yields superior outcomes. Turnover rate is the proportion of stock transactions in the market compared to the total volume of stock issued during a specific time period. Unlike trading volume, which is challenging to directly compare across different stocks, turnover rate offers a standardized benchmark for comparing stocks with varying issuances. This allows a direct comparison of trading activity among different companies or markets of different sizes. Moreover, the turnover rate provides a more precise indication of the extent of market players' activity and the liquidity of the market. On the other hand, volume does not take into account the total number of shares issued, which makes it challenging to effectively gauge the actual liquidity of the market. Hence, this type of specific image data format is designated as the Open-High-Low-Close Turnover-rate (OHLCT) chart.\nFurthermore, the proposed approach is specifically tailored for the China A-shares stock dataset. Within the China A-shares stock dataset, there is a notable phenomenon known as the weekend effect [39]. This effect is observed in the weekly dimensionality, where the A-shares stock market tends to exhibit lower performance on Fridays and greater performance at the opening on Mondays. This behavior might be ascribed to investors' inclination to decrease their exposure to risk prior to the weekend and thereafter re-enter the market on Mondays. This phenomenon can result in a downward movement of stock prices on Fridays and an upward movement on Mondays. Market-related news and events can potentially affect the weekly dimensionality.\nGenerally, significant corporate announcements, economic data releases, or political events take place during trading days, whereas weekends are comparatively tranquil. Hence, in the event of noteworthy news or occurrences during the weekend, the market may see abrupt swings upon the commencement of trading on Mondays.\nThe weekend effect and weekly dimensionality have significant impacts on the forecast and analysis of the A-shares stock market. To accommodate the A-shares stock market's unique trading processes, weekends are included as time separators in the OHLCT chart. This involves adding a column of blank pixels at the positions corresponding to the weekends. For consistency, three black pixel columns are used as separators for rest days, aligning them with the three pixels showing the stock prices (close-open/high-low) of each day. An advantage of this method is that it It boosts the model's capability to comprehend and internalize the recurring patterns of trade days more effectively. As a result, this particular format for image data is designated as the Time Segmented OHLCT (TS-OHLCT) Chart."}, {"title": "3.2. Proposed Methods", "content": "Traditional CNN are prone to overfitting issues when employed for stock prediction [29]. The findings of their research indicate that the use of 5-day stock feature maps is more effective than the use of 20-day feature maps, which in turn is more effective than the use of 60-day feature maps. The performance gradually worsens as the number of days increases. This is due to the fact that convolutional neural networks can perform well in terms of fitting capabilities and are able to utilise any portion of local features in order to fit the labels. However, for predicting current stock price movements, the features from the most recent trading days are of particular importance. Training a CNN model with stock feature maps over longer periods can result in a locally optimal solution based on specific features. However, using too short a period of data for stock prediction can be highly risky. With a short feature time frame, it becomes difficult to understand the historical performance of the stock, making it challenging to assess its current state and trend. This, in turn, increases the risk associated with trading predictions. Therefore, we proposes a multi-scale cascade image feature approach to address the aforementioned issues.\nIn this feature decomposition method, a raw feature map G with a given time length of n days can be decomposed into multiple sub-feature maps,\nrepresented by $X = {X_1,..., X_C}$, where C is the number of sub-maps. Since there are 5 trading days in a week, 5 is chosen as the base for the sub-map feature window, with the calculation formula $C = [log_5(n)]$. The raw feature map contains n time windows, with each window corresponding to 1 day of OHLCT (Open, High, Low, Close, Turnover rate) information. Therefore, the time resolution of the raw feature map is 1 day.\nEach decomposed sub-feature map $X_i$ has a time window count of 5, with a time resolution of $M_i$ days, meaning each window contains $M_i$ days of OHLCT information from the original feature map. The value of $M_i$ is determined by the following formula: For each sub-feature map $X_i$ (where i \u2208 [1, C]), the time resolution $M_i$ is calculated as\n$M_i = min(5^{(i-1)}, \\frac{n}{5}).$\nThis implies that as the sub-map index i increases, the number of days covered by each time window $M_i$ gradually increases, meaning the time resolution gradually decreases. In other words, sub-maps with smaller indices $X_i$ have higher time resolutions, reflecting more recent local information; whereas sub-maps with larger indices reduce the resolution by merging more days of OHLCT information, better reflecting global information. For each sub-feature map $X_i$, the feature merging for the j-th window (where j\u2208 [1,5]) can be determined using the equation below:\n$X_{open_j}^i = G_{open((j-1)\u00d7M_i+1)}^i,$\n$X_{high_j}^i = max (G_{High((j-1)\u00d7M_i+1)}^i,...,\u0120_{High(j\u00d7M_i)}^i)$,\n$X_{low_j}^i = min (G_{Low((j-1)\u00d7M_i+1)}^i,..., G_{Low(j\u00d7M_i)}^i)$,\n$X_{close_j}^i = G_{Close j\u00d7M_i}^i,$\n$X_{turnover_j}^i = \\sum_{k=(j-1)\u00d7M_i+1}^{j\u00d7M_i} G_{Turnover_k}^i$\nWe propose a Multi-Scale Resolution (MSR) CNN to capture features from longer stock sequences. In this context, Multi-Scale Feature (MSF) modules are utilized to extract stock features from sub-feature maps at different resolution scales. The basic building blocks of these modules include three main operations: convolution, activation, and pooling.\nThe convolution operation is similar to kernel smoothing, where it scans both horizontally and vertically across the image to extract features from each element in the image matrix, and thus generating contextual features. The output of the convolution filters is then processed through an activation function, specifically using the Leaky ReLU activation function, which introduces non-linearity. The final operation in the network module is max pooling, which performs another round of scanning over the input matrix and returns the maximum value from adjacent regions in the image. This process effectively reduces the dimensionality of the data and minimizes noise, thereby generating a high-dimensional feature set.\nAfter these operations, features from different scales are merged, and a fully connected layer is added. This fully connected layer is activated using the Softmax function to produce the prediction results. The primary goal of the proposed MSR-CNN is to perform binary classification, specifically to predict whether the value of a given stock will rise (labeled as 1) or fall (labeled as 0) over a specified period. As such, the CNN's prediction can be interpreted as an estimate of the probability of positive returns.\nAs shown in Figure 3, we first decompose the stock feature map with a time length of n into C sub-feature maps at different resolution scales. Each sub-feature map has a different feature weight $weight_i$; (where i \u2208 [1, C']). As i increases, $X_i$ contains stock features from further back in time, and so the designed feature weights gradually decrease. This approach allows the model to focus more on recent local features. The formula for the feature weights is as follows:\n$weight_i = \\begin{cases} 0.5, & \\text{if } i = 1, \\\\ \\frac{0.5}{2^{i-2}}, & \\text{if } 2 \\leq i \\leq C - 1, \\\\ \\frac{0.5}{2^{C-2}}, & \\text{if } i = C. \\end{cases}$\nWe further compare a cascaded CNN architecture based on multi-scale decomposition, decomposing the image features of 20 days and 60 days, and inputting the sub-feature maps into the network according to their weights. The input of the first method, MSR, consists purely of image features, with experimental hyperparameters and loss functions consistent with those of the Original Stock Data Experiment. The second method, SMSFR, includes both multi-scale image features and sequence features in its input. The prediction labels of the sequence part are also treated as a regression problem. The rise and fall amplitude of the stock 5 days later (up or down) is used as the label y, consistent with the time period of the classification labels. The training loss function for regression is defined as the mean squared error (MSE) loss, which is defined as follows:\n$L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2,$"}, {"title": "4. Experiments", "content": "The Science and Technology Innovation Board (STAR Market) and Beijing Stock Exchange stocks are characterized by small market capitalization, having been listed after July 2019, and are subject to various trading restrictions. In our experiments, the dataset encompasses all stocks in the China A-share stock market, excluding those from the STAR Market (codes starting with 688*) and the Beijing Stock Exchange (codes starting with 8*), totaling 4,454 stocks.\nWe first evaluate the accuracy of different methods in predicting stock price movements. This task can be viewed as a binary classification problem, where the classes are typically divided into positive and negative examples. When a stock is expected to rise in the future, the label is defined as y = 1. Otherwise, if the stock is expected to fall, the label is defined as y = 0. During the training process for classification problems, we use the cross-entropy loss function to minimize the standard objective function. The cross-entropy loss function is defined as follows:\n$L(y, \\hat{y}) = -y \\log(\\hat{y}) \u2013 (1 \u2013 y) \\log(1 \u2013 \\hat{y}),$\nwhere \u0177 represents the output from the softmax function in the final layer of the model. We select the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) as the metrics for evaluation, which are defined as follows:\n$PPV = \\frac{TP}{TP + FP}, NPV = \\frac{TN}{TN + FN},$\nwhere TP and FP denote the true positives and false positives, respectively. TN and FN denote the true negatives and false negatives, respectively.\nTo eliminate experimental bias caused by random seeds and stochastic gradient descent, we conduct 10 repeated training processes for each subsequent method. In each training process, we apply an early stopping method after the 5th epoch and selected the model with the best performance on the validation set. Finally, we take the average of the results from the 10 testing to reflect the overall performance of the method.\nAdditionally, to prevent early overfitting during the training process, we employ the regularization procedure mentioned in [40]. In each layer, we apply the Xavier weight initializer [41] to ensure that the variance of the initial weights is comparable to the variance of the labels, thereby promoting faster convergence of the model. The optimization of the loss function was carried out using stochastic gradient descent and the Adam algorithm [42], with an initial learning rate set to 3 \u00d7 10-5 and a batch size of 256. All experiments were conducted on 8 Tesla V100 GPUs with 32GB memory each (a total of 256GB) and 80-core CPUs.\nWe select the original stock features with duration of 5 days, 20 days, and 60 days to predict the probability of stock price movements over the next 5 days. The input features include basic price and volume indicators such as open, high, low, close, ma5, and turnover rate. Additionally, we substitute the trading volume with the turnover rate and introduced time delimiters to verify whether these factors contribute to improved stock prediction.\nIn this experiment, we choose Timemixer [43], Timesnet [44], Dlinear [45], and PatchTST [46] as the baseline models for time series analysis. Additionally, we select the OHLCV+CNN model [29] as the baseline model for image-based analysis. This experimental design aims to validate whether the time series models can capture more information as the input time length increases and to assess whether the image-based models are prone to overfitting.\nExperimental results indicate that the accuracy of both multi-scale feature decomposition methods increases as the input duration of the stock data is extended. This suggests that decomposition-based methods are better at capturing both the short-term local features and long-term trend features of stocks, thereby addressing the issue of local overfitting in the original image features.\nMeanwhile, the Sequence Multi-Scale Fusion Regression (SMSFR) CNN achieved the best results when the input features spanned 60 days, with an accuracy of 61.15% for predicting stock price increases and 63.37% for predicting decreases after 5 days. This indicates that integrating sequential features into image features, along with incorporating stock price fluctuation labels, allows the model to gain a deeper understanding of the underlying dynamics of stocks. Since the fluctuation magnitude reflects the strength and volatility characteristics of individual stocks, which cannot be captured by simple classification labels, the model is better equipped to grasp the principles underlying stock price movements, thereby enhancing prediction accuracy."}, {"title": "4.3. Profitability Backtest Experiments", "content": "To better evaluate the performance of the models in the A-share market, we conducte a simulated backtest for a two-and-a-half-year period from January 1, 2022, to June 30, 2024, using all the methods described above. For each method, we select the model that performed best on the validation set across 10 repeated experiments. To simulate real trading conditions, we set the maximum open positions for each model to 5. When the model predicts a stock's probability of rising to be greater than 80%, we buy the stocks in descending order of probability and sell them after 5 trading days. To account for the slippage effects in actual trading signals, we calculate a transaction cost of 0.3%. The final result for each method is the average of the simulation results of the 10 corresponding models. We use PF to represent total profit and max drawdown (MDD). The experimental results demonstrate that our proposed MSR and SMSFR methods significantly improve the model's returns. The SMSFR-based method introduces regression labels, which limit the model's convergence space and achieve the smallest maximum drawdown and the best returns across different time feature inputs. The best result is achieved by the 60-day SMSFR, which yielded a return of 165.09% and a maximum drawdown of 27.92% during the period from January 1, 2022, to June 30, 2024, significantly outperforming other methods."}, {"title": "5. Conclusions", "content": "In this study, we apply temporal separators to image features, successfully addressing the issue of traditional stock feature maps failing to capture holiday information in the A-share market. Additionally, we propose a novel multi-scale image decomposition method aimed at solving the problems of overfitting in image features for stock price prediction and the difficulty of capturing the complexity of temporal dynamics in financial data.\nThe experimental data indicates our proposed Multi-Scale Resolution CNN (MSR) effectively prevents the model from falling into local overfitting when handling long time-series features by utilizing the multi-scale features of stock image representations. By decomposing long-sequence stock features into sub-feature maps at different time scales, our model can simultaneously capture short-term fluctuations and long-term trends in stock prices, thereby significantly improving prediction accuracy.\nA key contribution of this study is the integration of regression labels into the convolutional neural network framework, further proposing the Sequence Multi-Scale Fusion Regression CNN (SMSFR). This approach better learns and predicts the magnitude of stock price changes, which is particularly important in financial markets where the magnitude of price fluctuations is often more critical than the direction. By combining image feature extraction with sequential data, our model can comprehensively understand the complex relationships between different time periods and trading patterns, achieving optimal prediction results.\nHowever, this study also has some limitations. Firstly, the stock market is an extremely complex system, and many traditional methods are prone to failure in practical applications, especially when market dynamics change dramatically, making predictions more challenging. Currently, most deep learning methods are based on time series data, while the development and application of image features are relatively rare. Although we have proposed a solution for image features in this study, its long-term profitability and robustness have not been fully validated. As more image-related research progresses, the practical application effects of image feature methods and the sustainability of their profitability in the stock market still require further in-depth investigation.\nDespite the encouraging results, there are several avenues for future work. First, this study did not evaluate the regression labels in the SMSFR. The role of regression labels in this work is to introduce additional information about stock price fluctuations to help the image feature component converge, enabling the model to better understand stock volatility and underlying trends. In future work, we will use regression labels as an important factor in evaluating stock prediction performance. Additionally, exploring other deep learning architectures, such as attention mechanisms or graph neural networks, may further improve the capture of complex dependencies in stock data.\nThe SMSFR-CNN model has made significant progress in stock price prediction by effectively integrating multi-scale temporal features and sequential data into a unified framework. This approach enhances the interpretability of predictions and also provides more accurate and reliable forecasts, which are of great value to investors and financial analysts. Future research will focus on evaluating the model's prediction of stock price fluctuations, expanding the model architecture, and exploring its application in other financial markets."}]}