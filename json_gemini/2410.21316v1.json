{"title": "Deep Optimizer States: Towards Scalable Training of Transformer Models Using Interleaved Offloading", "authors": ["Avinash Maurya", "Jie Ye", "M. Mustafa Rafique", "Franck Cappello", "Bogdan Nicolae"], "abstract": "Transformers and large language models (LLMs) have seen rapid adoption in all domains. Their sizes have exploded to hundreds of billions of parameters and keep increasing. Under these circumstances, the training of transformers is very expensive and often hits a \"memory wall\u201d, i.e., even when using 3D parallelism (pipeline, tensor, data) and aggregating the memory of many GPUs, it is still not enough to hold the necessary data structures (model parameters, optimizer state, gradients, activations) in GPU memory. To compensate, state-of-the-art approaches offload the optimizer state, at least partially, to the host memory and perform hybrid CPU-GPU computations. However, the management of the combined host-GPU memory is often suboptimal and results in poor overlapping between data movements and computations. This leads to missed opportunities to simultaneously leverage the interconnect bandwidth and computational capabilities of CPUs and GPUs. In this paper, we leverage a key observation that the interleaving of the forward, backward and update phases generate fluctuations in the GPU memory utilization, which can be exploited to dynamically move a part of the optimizer state between the host and the GPU memory at each iteration. To this end, we design and implement Deep Optimizer States, a novel technique to split the LLM into subgroups, whose update phase is scheduled on either the CPU or the GPU based on our proposed performance model that addresses the trade-off between data movement cost, acceleration on the GPUs vs the CPUs, and competition for shared resources. We integrate our approach with DeepSpeed and demonstrate 2.5\u00d7 faster iterations over state-of-the-art approaches using extensive experiments.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers and large language models (LLMs) have seen increasing adoption in various domains ranging from scientific research to industrial applications [47]. While traditionally used for creative text generation, prompt completion, and comprehension/summarization, these learning models are successfully tackling multi-modal data sources, thanks to cross-attention [42]. Additionally, recent initiatives such as LLMs for science (e.g., AuroraGPT [39], ScaleFold [48], and DeepSpeed4Science [35]) are beginning to explore use cases that involve specialized domain-specific languages for tasks, such as, genome sequencing, protein structure prediction, and equilibrium distribution prediction. The versatility and democratization [28, 31] of LLMs have led to an unprecedented scale of development across multiple fields.\nMotivation. In a quest to improve the quality, LLMs are routinely made of billions of parameters with models like GPT-3 [2], LLaMA-2 [36], and BLOOM [40] requiring hundreds of gigabytes of GPU memory just to store the model parameters. Several predictions anticipate LLMs will soon reach trillion scale parameters, e.g., Google Switch-C (1.6T) [7], WuDao 2.0 (1.75T) [44], M6-10T [17], and AuroraGPT [39]. Despite advances in technologies that enable LLM training to scale (hybrid data-, pipeline- and tensor parallelism, sharding of model parameters and optimizer state, layout and communication optimizations, etc.), the rapid growth in the number of model parameters has resulted in large optimizer states, which has outpaced the available GPU memory, creating a significant \"memory wall\" that makes it challenging to train and run these massive models efficiently [4] on limited GPU setups. In this"}, {"title": "Limitations of State-of-the-Art.", "content": "To address the challenge of hitting the memory wall, approaches such as DeepSpeed Offload [28], DeepSpeed TwinFlow [37], and Zero-Infinity [29] have explored the idea of moving large data structures required during training to the host memory, notably the optimizer state. This makes it feasible to train LLMs with a much smaller aggregated GPU memory footprint, albeit at the cost of performance penalty. Specifically, for commonly used adaptive learning rate optimizers [14, 43] e.g., ADAM, the optimizer state, which includes parameters, momentum, and variance, is stored on the host memory in high FP32 precision, while the forward pass and backward pass can operate with model parameters in lower FP16 precision to calculate FP16 gradients, which are then flushed to the host memory and upscaled to FP32 precision. Then, the update of parameters can proceed directly on the CPU and a downscaled FP16 copy can be transferred to the GPUs for the next iteration. In this case, a critical bottleneck is the limited I/O bandwidth between the host and GPU memories, which is constrained by PCIe links (typically in the order of 25-50 GB/s). This bottleneck is further exacerbated by contention for PCIe links for inter-node communication needed to implement tensor, pipeline and data parallelism, which results in additional overhead during the forward pass (wait for the copy of updated model parameters from the host to the GPU memory) and the backward pass (wait to flush the gradients from the GPU to the host memory). Another important bottleneck is the low computational capability of the CPUs, which are orders of magnitude slower than the GPUs. For instance, on our testbed (\u00a7 5.1), the 4\u00d7H100 GPUs update ~100 Billion parameters of the model per second (P/s), while the 192 CPUs update the model at ~8 Billion P/s and copy updated parameters to the GPU at 12 Billion P/s, resulting in 20\u00d7 slower updates. Under such circumstances, despite being simple and embarrassingly parallel, the operations involved in updating the model parameters and the optimizer state lead to a significant runtime overhead, which otherwise is negligible when running them on the GPUs."}, {"title": "Key Design Ideas and Contributions.", "content": "In this paper, we propose Deep Optimizer States to address the two bottlenecks mentioned above to accelerate the training of LLMs. We summarize our contributions as follows:\n(1) We perform a detailed study of the behavior of the training iterations when offloading the optimizer state to the host memory. Specifically, we highlight important observations that drive our proposal: computations remain efficient despite fine-grain sharding of large optimizer states into subgroups; GPU memory utilization during the update phase decreases dramatically; and PCIe links are underutilized during the backward pass and the update phase (\u00a7 3).\n(2) We introduce a series of key design principles: interleaved offloading of parameter updates on the GPUs; overlapping optimizer subgroup movement and execution across GPU and CPU; efficient placement and movement of gradients for GPU and"}, {"title": "Limitations of the Proposed Approach.", "content": "The proposed approach relies on the model and the optimizer being sharded into smaller subgroups, allowing them to be updated one subgroup at a time. This capability is currently implemented in state-of-art LLM training frameworks such as DeepSpeed [30], but may not be universally available (e.g., not available in Nanotron [12]). Furthermore, this capability may be allowed only in combination with other capabilities, such as the partitioning of the subgroups across data-parallel ranks to eliminate redundancy (illustrated by DeepSpeed ZeRO). In this case, the benefits of dynamic GPU offloading of model updates may be offset by the behaviour of complementary capabilities (e.g., redundancy elimination incurs higher communication overhead compared with replicated data parallelism). However, most of these limitations are implementation-specific and do not affect the general principles. Furthermore, while Deep Optimizer States accelerates the update phase by leveraging fast GPU-based updates for a fraction of the optimizer states, it is still constrained by slow data movement over PCIe and slow CPU-based updates for remainder subgroups. Therefore, while it mitigates part of the slow CPU-based updates, it does not completely eliminate them to perform as fast as GPU-only updates.\nZeRO-3 Offload, described in \u00a7 2, offers additional optimizations for tight memory capacity bound scenarios using quantization, parameter/optimizer offloading to NVMe, activation offloading to NVMe, etc., but in this paper, we specifically focus on and evaluate the scenarios where we have sufficient aggregated GPU memory to store all the model parameters, but not enough to hold the subgroups of optimizer state (consisting of FP32 parameters, momentum, and gradients)."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Data Parallelism. Data parallelism is the most widely used technique to accelerate the training of deep learning models [15, 32]. It creates replicas of the learning model on multiple workers, each of which is placed on a different device and/or compute node. The input data is randomly shuffled and partitioned among the workers at each epoch. During the forward pass, the workers simply process their mini-batches from the partition of their dataset in an"}, {"title": "Pipeline and Tensor Parallelism.", "content": "Pipeline and tensor parallelism are two conventional techniques used to split large models that cannot fit in a single GPU memory by vertically or horizontally sharding the model layers [6, 9, 13, 30, 45]. As shown in Figure 1(a), tensor parallelism shards individual layers of the model horizontally (denoted by the magenta dotted box), incurring significant communication overheads during the training. On the other hand, pipeline parallelism (denoted by blue dotted boxes in Figure 1(a)), splits the model layers into distinct stages (or pipeline parallel partitions), each of which is placed on a separate GPU. This method requires relatively fewer communications compared to tensor parallelism. Therefore, in real-world training, the tensor-parallelism degree is typically restricted to the maximum number of GPUs available in a single node to leverage high-speed NVLinks, while pipeline stages can be distributed across multiple nodes. Each stage in the pipeline parallel setup can run forward and backward passes of different mini-batches in parallel using gradient accumulation [11, 27, 34] such that the idle time of GPUs waiting for activations (or gradients) from predecessor (or successor) stages can be minimized by typically using the efficient one-forward one-backward (1F1B) parallelism schedule [23]. However, when tensor and pipeline parallelism techniques cannot fit the model on GPUs, offloading techniques are used to store the large-sized optimizer state (either fully or partially) to the host memory, as shown in Figure 1(a)."}, {"title": "Mixed Precision Training.", "content": "To improve the throughput of training and reduce the GPU memory required for training, LLMs are routinely trained using mixed-precision [22] without negatively impacting the convergence or training accuracy. This method allows certain parts of the LLM training to operate in low 16-bit"}, {"title": "Hybrid CPU-GPU Optimizer Offloading.", "content": "Several efforts [3, 10, 26, 38] have introduced hybrid training approaches that combine the memory of GPU and other devices for deep learning training. For LLMs, to reduce the amount of GPU memory required for training, large optimizer states are either partially or fully offloaded to the host memory or NVMe, using offloading engines, such as ZeRO-Offload [31], ZeRO-Offload++ or TwinFlow [37], and CoTrain [16]. When the optimizer state is offloaded to the host memory or NVMe, the low-precision gradients generated on the GPU are moved to the host memory, where it is upscaled to FP32 and consumed by the optimizer for computing updated parameters in high-precision. The updated high-precision parameters are then downscaled and fetched by the GPU to train the next iteration using the updated parameters. Such offloading also accelerates checkpointing (needed at regular intervals for fault tolerance, surviving model spikes, intermediate model analytics, etc.), because the large host-resident optimizer states can be asynchronously flushed to persistent storage using several techniques without blocking the GPUs [1, 18\u201320, 24]. To accelerate the update phase for the cases when GPU memory can partially but not fully accommodate the optimizer state, the state-of-the-art LLM training framework, DeepSpeed, offers a partial optimizer offloading optimization using TwinFlow, also known as ZeRO-Offload++. Based on the \"user-defined ratio\", a fraction of the optimizers reside statically on the GPU and the remainder resides on the CPU. Determining the amount of spare GPU memory available for statically storing a subset of the optimizer states is"}, {"title": "ZeRO-3 Varying Subgroup Sizes.", "content": "We first evaluate the impact of varying subgroup sizes on the ZeRO-3 training runtime for different model sizes with the optimizer state completely offloaded to host memory as shown in Figure 1(b). As shown in Figure 2, we observe that varying the subgroup sizes from 100M to 1B parameters per subgroup does not impact the training iteration for any of the 7B to 20B parameters models. The slight 4% difference in iteration times can be attributed to uneven partitioning of the model parameters across the GPUs. Therefore, the subgroup size does not impact the LLM training time."}, {"title": "GPU Memory Utilization.", "content": "We characterize the GPU memory utilization at different stages, i.e., forward, backward, and update stages of the LLM training. For the 20B parameters model running with optimizer fully offloaded to the host memory, Figure 3 (top) shows the GPU memory utilized for a single GPU when all activations are stored on the GPU during the forward pass. Figure 3 (bottom) shows the memory utilization for the case when activation checkpointing is used to reduce the GPU memory footprint, wherein instead of saving all activations, only a subset of activations at specified intervals (detailed in ZeRO-Infinity [29] Section-3) are stored on the GPU memory and the remainder are discarded. During the backward pass, the discarded activations are recomputed from checkpoints, resulting in 33% additional recomputations in the backward pass [31]. When all the activations are stored (Figure 3 (top)), we observe that the GPU memory utilization steeply rises during the forward pass. During the backward pass, these activations are freed and gradients are generated, which get offloaded to host-memory because the optimizer updates are scheduled on the CPU. Lastly, during the update phase, we observe that the GPU only consists of the model parameters, which will get updated once the updates of the CPU offloaded optimizer are complete. A similar trend can be observed for the case when activation checkpointing is used (Figure 3 (bottom)), however, with a lower GPU memory utilization because the activation checkpoints only consume a fraction of the memory in forward pass, which are freed during the backward pass. Irrespective of storing complete activations or activation checkpointing, we observe significant fluctuations in GPU memory utilization which can be leveraged to store and run a part of the optimizer update step on the GPU."}, {"title": "PCIe Link Utilization.", "content": "For the 20B parameters model with the optimizer fully offloaded to the host memory, Figure 4 shows that both the host-to-device (H2D) and device-to-host (D2H) channels are sparsely utilized using <10% of the peak transfer throughput (~50 GB/s). During the backward pass, we observe non-negligible H2D and D2H transfers, primarily due to gradient movement. Here, the D2H transfers are caused by the flushing of gradients generated on the GPU by backward pass, which will be used on the host memory to compute model updates by the CPU offloaded optimizer. Surprisingly, during the backward pass, we also observe H2D transfers over the PCIe in Figure 4. This is primarily for faster gradient accumulation; i.e., the gradients are accumulated on the host-memory, and since the accumulation (old_grad.add_(new_grad)) operations are magnitudes of order faster on the GPU compared to the CPU, the previously accumulated gradients are transferred on the GPU, accumulated on the GPU, and flushed back to the host memory, where the optimizer uses it to compute the updates. Lastly, during"}, {"title": "4 SYSTEM DESIGN", "content": "4.1 Design Principles\nInterleaved Optimizer Updates Across GPU and CPU. The uneven memory consumption and low PCIe link utilization (studied in \u00a7 3) during different training phases provide an opportunity to exploit the idle GPU memory (released by activations) and PCIe link during the update phase. To exploit this opportunity, during the update phase, parts of the optimizer state can be dynamically fetched on the GPU to compute a fraction of the parameter updates in parallel while the CPU computes updates of the remainder fraction. A key requirement to update the parameters for a given subgroup is to stage its parameters (p), momentum (m), variance (v), and gradients on the target device on which updates are scheduled in FP32 precision (see \u00a7 2). In case the p, m, v, and/or gradients of the subgroup are not present on the target device, the update operation will trigger reads from the slower memory tier (e.g., host memory or NVMe), where the subgroup is offloaded, causing I/O"}, {"title": "Overlapping Optimizer Subgroup Movement and Execution Across CPU and GPU.", "content": "The data movement observed when the state-of-the-art middleware enabling hybrid optimizer offload (e.g., TwinFlow [37]) runs an update operation is shown in Figure 5 (top). We observe that after the updates corresponding to a given subgroup i are computed on the CPU, the updated parameters p\u2071 are H2D transferred to the GPU to continue training with updated model parameters in the subsequent iteration. Only when all the subgroups are updated and all the updated parameters transferred to the GPU, the subsequent iteration can begin. Given the embarrassingly parallel nature of optimizer updates (\u00a7 2), the optimizer subgroups can be updated and transferred out-of-order, and do not impact the accuracy of the training. Irrespective of some subgroups statically residing on the GPU, the slow updates using the existing offloading solutions can be attributed to (a) idle CPU when GPU is computing updates of GPU-resident subgroups (S\u00b9 and S\u00b2); (b) blocking H2D transfer of updated subgroup parameters, i.e., the CPU and GPU remain idle when the parameters corresponding to CPU update subgroup are copied to the GPU; and (c) slow FP32\u2192 FP16 downscaling of update parameters during H2D transfers (not shown in figure for simplicity).\nTo mitigate the aforementioned challenges, we propose an overlap-centric design illustrated in Figure 5 (bottom) for efficient interleaving of CPU and GPU updates. It works as follows: while the CPU computes the update of the initial subgroups (S\u00b9 and S\u00b2), the optimizer state corresponding to the GPU-scheduled subgroup (S\u00b3), including momentum (m), variance (v), and parameters (p), are being prefetched using asynchronous H2D transfers; thereby overlapping CPU computations with GPU subgroup prefetching. Next, the GPU update for subgroup S\u00b3 and FP32\u2192FP16 downscaling of CPU updated parameters (S\u00b9 and S\u00b2) are done in parallel on the GPU and the CPU, respectively. After this, three operations happen in parallel: (1) H2D transfer of (a) updated parameters of S\u00b9 and S\u00b2 and (b) prefetching of next subgroup to be updated on the GPU (S\u2076); (2) flushing out (D2H transfer) of the previous subgroup updated on the GPU (S\u00b3); and (3) CPU updates of the subsequent subgroups (S\u2074 and S\u2075); thereby exploiting full-duplex D2H and H2D transfers and parallel CPU computations. Furthermore, instead of statically placing the first two subgroups (S\u00b9 and S\u00b2) on the GPU, we propose to place the last two subgroups (S\u2077 and S\u2078) statically on the GPU to overlap the pending H2D and D2H transfers of previous subgroups updated across host or GPU devices."}, {"title": "Efficient Management of Gradients for GPU and CPU Scheduled Subgroup Updates.", "content": "During the training, the gradients generated during the backward pass on the GPU are used by the optimizer to compute the parameter update. State-of-the-art hybrid optimizer offloading solutions (e.g., TwinFlow), shown in Figure 5 (top), by default retain the gradients corresponding to the statically GPU-resident subgroups (S\u00b9 and S\u00b2) on the GPU during the backward pass; and for the remainder of the subgroups, which are scheduled to be updated on the CPU, gradients are offloaded to the host memory during the backward pass. In our approach, we extend this design and leverage the GPU memory released by activations (or activation checkpoints) to store the gradients corresponding to the subgroups scheduled for updates on the GPU, which can be known apriori using the lightweight performance model described in \u00a7 4.2. In cases where the GPU memory freed by the activations (or activation checkpoints) is not large enough to store the gradients of all GPU-scheduled subgroups, the gradients are offloaded to the host memory and fetched back to the GPU along with the subgroup's optimizer states (FP32 momentum, parameter and variance)."}, {"title": "PCIe Transfers with Higher Precision to Avoid Costly Memory Allocation for On-the-fly Upscaling.", "content": "When the models are trained with mixed-precision (\u00a7 2), the gradients generated on the GPU during the backward pass are typically produced in low FP16 precision, whereas the optimizer computes the updates using high FP32 precision gradients. Flushing the FP16 gradients from the GPU to the FP32 gradient buffer on the host is non-trivial, which requires both precision conversion (FP16\u2192FP32) and data movement (D2H transfer). As shown in Figure 6, for a subgroup size of 0.1B parameters, which generates ~0.2 GB worth of FP16 gradient tensor, the D2H transfer takes place at 2.5 GB/s even when the destination FP32 host gradient buffer is pinned, thereby showing 22\u00d7 slower D2H transfer throughput as compared to the peak D2H throughput. When this gradient transfer is zoomed in (rightmost upper block), we observe that this slowdown is because of three different operations involved in the D2H gradient flushing: (1) allocate unpinned memory at ~4 GB/s on the host to hold the"}, {"title": "4.2 Performance Model to Determine the Optimal Fraction of Subgroups to be Updated on the GPU", "content": "To achieve an efficient overlap of computation and transfers during interleaved optimizer updates, we propose a performance model that computes the \"update stride\", i.e., after how many CPU-based updates should we schedule a subgroup to be updated on the GPU, such that the PCIe link, GPU and CPU are maximally utilized. The key idea of this performance model is to balance the overlap time between CPU-based subgroup updates, GPU-based subgroup updates, and D2H and H2D transfers.\nConsider that a single subgroup consists of S number of parameters in high FP32 precision and the CPU to GPU update ratio is k : 1, i.e., k subgroups are updated on the CPU for every one subgroup updated on the GPU. Furthermore, the update throughput on CPU and GPU are given as Uc and Ug parameters per second, respectively; and the FP32\u2192FP16 downscaling throughput on the CPU is given as Dc parameters per second. In a given system with H2D and D2H throughputs as B parameters per second, the time to run CPU update and downsampling of k parameters is given by k * (S/Uc) and k * (S/Dc), respectively. For each subgroup updated on the CPU, the downscaled FP16 parameters will be sent to the GPU, resulting in k * S/(2 * B) seconds of transfer over the H2D link (S/2 instead of S due to FP16 precision). Finally, swapping out the previous optimizer subgroup from the GPU and swapping in the next subgroup on the GPU requires the transfer of FP32 parameters, momentum, and variance, and will require 3 * S/B seconds of transfer across D2H and H2D PCIe links, respectively. Specifically, Equation 1 formulates the aforementioned computations and data movement to derive the optimal CPU-to-GPU subgroup update ratio. An interesting observation here is that the value of k is not dependent on the subgroup size, therefore, selecting any arbitrary subgroup size results in the same performance improvements of the updates. However, smaller subgroups enable the TwinFlow approach to statically store a fraction of optimizer states which is close to the user-supplied ratio, e.g., for a 3B parameters model partitioned in 1B parameters subgroups (i.e., every subgroup is 33% of the model), if the TwinFlow static GPU-resident optimizer-state ratio is set to 20%, no subgroup will be scheduled on the GPU; thereby leading to slow updates of all subgroups and GPU underutilization.\n$k * (\\frac{S}{U_c} + \\frac{S}{D_c}) = max \\begin{cases} \\frac{3 * S}{B} \\\\ \\frac{k * S}{2 * B} + \\frac{S}{U_g} \\end{cases}$\n= max {D2H transfersH2D transfers\n$\\frac{3* S}{B} \\\\\\ \\frac{k * S}{2 * B} + \\frac{S}{U_g} $}\n$k = \\frac{1}{\\frac{1}{2 * B} + \\frac{1}{D_c}} \\\\\\\\\\ \\frac{1}{\\frac{3}{B} - (\\frac{1}{U_c} + \\frac{1}{U_g})}$"}, {"title": "4.3 Optimizer Update Scheduling Algorithm", "content": "Based on the design principles and performance model, the update process of Deep Optimizer States is shown in Algorithm 1. In a single update phase, each process invokes the run_update function using the optimizer subgroups (S), the optimal \"GPU update stride\" k, i.e., CPUto GPU update ratio derived from the performance model \u00a7 4.2, and the static GPU-resident subgroups (R)- configured by the user at runtime, similar to TwinFlow [37].\nIn Algorithm 1, we first check if the given subgroup i is a static GPU resident (R) or if it corresponds to the \"update stride\" k. Since"}, {"title": "4.4 Deep Optimizer States Implementation", "content": "We implement Deep Optimizer States as an open-source\u00b9 middleware for the DeepSpeed ZeRO-3 stage engine. For software packaging, Deep Optimizer States is meticulously engineered and optimized as a Python module that can be enabled and configured through a single JSON entry in the configuration file given to the training"}, {"title": "5 PERFORMANCE EVALUATION", "content": "5.1 Experimental Setup\nWe conduct our experiments on ALCF's JLSE testbed consisting of 4\u00d7H100 GPUs with 80 GB HBM3 each (aggregated GPU memory of 320 GB), 2x Intel Xeon Platinum 8468 processors with 48 CPUs each (total 96 cores, 192 threads), and 2\u00d7 Gen4 NVMe of 1.5 TB each. The 512 GB DDR5 RAM is split across 2 NUMA domains, and shared by consecutive GPU IDs, i.e., GPU0 and GPU1 are mapped to NUMA0, and GPU2 and GPU3 are mapped to NUMA1. The GPUs are inter-connected through NVLinks, providing 133 GB/s unidirectional D2D transfer throughput; and every GPU is independently connected to the host with PCIe Gen 5 interface, providing ~55 GB/s unidirectional D2H and H2D throughput for pinned host memory. For pageable host memory, the peak unidirectional D2H and H2D throughput are 16 GB/s and 9 GB/s, respectively."}, {"title": "5.2 Compared Approaches", "content": "DeepSpeed ZeRO-3. This represents the state-of-the-art technique developed by Microsoft for efficiently training LLMs on GPU-memory-constrained systems. The forward and backward passes of this approach are illustrated in Figure 6 (top), and the update phase can be illustrated using Figure 5 (top) with the exception of all subgroups statically residing on the host memory.\nDeepSpeed TwinFlow. This approach is representative of the state-of-the-art hybrid optimizer offloading solution, wherein the optimizer is statically partitioned between host and GPU memory based on the \"user-supplied ratio\". The update phase of this approach is illustrated in Figure 5 (top).\nDeep Optimizer States. This represents our proposed approach and is highlighted in Figure 5 (bottom), which uses the design principles and algorithm described in Section 4."}, {"title": "5.3 Methodology and Performance Metrics", "content": "Models and Datasets. The model architectures of the models used in our evaluations, which are based on widely used real-world LLM training, are summarized in Table 2. The sizes include the size of FP16 and FP32 gradients model and optimizer states, based on Zero-Infinity (\u00a7 3) [29]. We restrict our evaluations to 20B parameters models as the next smallest model, LLaMA-33B [36], has a larger optimizer state than the DRAM (512 GB) capacity of our testbed.\nFor our evaluations, we use a subset of the OSCAR-en dataset consisting of 79K records, included in the repository of the Bloom model [40], and use the default LLaMA2 [36] tokenizer for preprocessing the dataset into tokens. Similar to OPT training [46], we use the default sequence length of 2048 for all configurations and set the micro-batch size to 1 to avoid OOM errors in any configuration.\nRuntime Configurations. As described in \u00a7 2, ZeRO-3 partitions the model layers across available GPUs in hybrid tensor and data-parallel form. Therefore, we do not use explicit pipeline (unsupported with ZeRO-3) or tensor parallelism. The data-parallel degree is set to 4, which is the maximum number of GPUs in a single node. Unless otherwise noted, for all experiments, we use a subgroup size of 100M trainable parameters per subgroup. Although the subgroup sizes do not impact the iteration duration, as observed in Figure 2 or the performance model (\u00a7 4.2), as opposed to DeepSpeed's default 1B subgroup size, we choose a smaller subgroup for better static partitioning of optimizer between GPU and CPU with TwinFlow (as detailed in \u00a7 4.2). Given the limited GPU memory setup targetted in this paper, similar to Turing-NLG 17.2B, GPT-3 175B, BLOOM-176B [29, 40], for all experiments, we used activation checkpointing for reducing the GPU memory utilization at the expense of 33% additional recomputations during the backward pass. Even if all activations could be saved on the GPU without running OOM, the backward phase would require 33% less recomputations for all approaches, and we would observe speedup due to overlapping transfers (Figure 6). Furthermore, since the activations (or activation checkpointing) are released in the backward phase, they do not impact the update phase and therefore result in the same speedup in the update phase in Deep Optimizer States compared to other approaches.\nThroughout our evaluations, we consider that the collective GPU memory is adequate to store the following: (1) FP16 model parameters; (2) activations or activation checkpoints generated by the forward pass; (3) FP16 gradients generated during the backward pass; and (4) at least one FP32 optimizer-state subgroup. Note that a small-sized subgroup consisting of 100M parameters would produce"}, {"title": "5.4 Experimental Results", "content": "Optimizer States Completely Offloaded to the CPU Memory. In our first set of experiments, we evaluate the per iteration time breakdown between forward, backward, and update phases for all the compared approaches when the entire optimizer state resides on the CPU memory for different model sizes listed in Table 2. This evaluation studies increasingly large models trained on GPU memory-constrained systems. We run the training for 10 iterations, from which the first 2 iterations are considered warmup and the timings report are the average times observed in 8 iterations. This metric is important because although the subgroups are nearly equally partitioned across all GPU resources, for each subgroup, the backward and update phases invoke blocking allreduce communication collectives (refer [29] for details), because of which the slowest process in the group dictates the iteration time.\nAs observed in Figure 7, the iteration time for larger models with the default DeepSpeed CPU optimizer approach grows linearly in proportion to the model size. The 8.3B parameters model shows higher execution time than the 10B parameters model particularly because of a large number of layers (72) with smaller hidden dimensions (3072) as compared to the other models which consist of at least 4096 hidden dimensions. However, for all model sizes, our proposed Deep Optimizer States shows at least 2\u00d7 and up to 2.5\u00d7 faster iteration times than DeepSpeed's ZeRO-3 approach. When we analyze the speedup obtained with the 20B parameters model with our approach, we observe that asynchronous transfers during the backward pass constitute 1.9\u00d7 of the speedup, and the update phase further accelerated the iteration by 60%, resulting in 2.5\u00d7 total speedup.\nNext, we analyze the optimizer update step for different model sizes by evaluating the update throughput, which is measured as the total number of optimizer parameters updated per second. As shown in Figure 8, the update throughput of Deep Optimizer States is 70% higher than that of ZeRO-3 on average. This is because of efficient overlapping of 50% the GPU-based updates using our dynamic"}, {"title": "Fraction of Optimizer States Statically Resident on the GPU Memory.", "content": "In the next series of evaluations, we consider the case when a subset of the optimizer subgroups is statically pinned to the GPU memory. This study shows performance of different approaches when the updates are not completely dependent on slow CPU computations. We use the 20B parameters model as the representative model for subsequent experiments because the longer runtime allows us to better analyze the performance characteristics.\nWe evaluate the time for the update phase of the 20B parameters model at varying fractions of optimizer states statically residing on the GPU. Figure 10 shows how the time taken by the update phase decreases with increasing percentage of optimizers statically pinned to the GPU memory. This is because larger proportions of optimizer states residing on the GPU memory lead to faster update computations on the GPU and fewer H2D transfers of the CPU-updated parameters. Irrespective of the proportion of optimizer state on the GPU, we observe at least 1.7\u00d7 faster updates with Deep Optimizer States as compared to TwinFlow; thereby showing relevance for efficient training on future GPUs capable of hosting larger proportions of the optimizer state on the GPU memory.\nNext, we characterize the performance of a single iteration for the 20B parameters model for varying proportions of optimizer states"}, {"title": "Increasing Microbatch Sizes.", "content": "In the next set of experiments, we evaluate the performance of the 20B parameters model for an increasing microbatch size. To accommodate the largest microbatch on the GPU, the optimizer state resides fully on the host memory during this experiment. We record the total iteration time and the computational throughput achieved (reported as TFLOPs) for an increasing microbatch size per GPU. As shown in Figure 13, the average iteration time increases linearly in proportion to the growing microbatch size until microbatch of 8 samples, after which, it triggers an OOM error. Although the number of samples increases by 2x for every x-tick, the iteration time does not grow linearly, leading to higher TFLOPs (reported by the minor y-axis). We observe that the proposed Deep Optimizer States outperforms DeepSpeed's ZeRO-3 by 1.6-2.5\u00d7 and scales with increasing microbatch sizes."}, {"title": "Scaling the CPU Cores per GPU.", "content": "We next measure the impact of varying the number of CPU cores available per GPU, which allows us to study different configurations, e.g., ALCF Polaris contains 4\u00d7A100 GPUs and 64 CPUs in a single node, AWS p3dn.24xlarge contains 8\u00d7 V100 GPUs and 96 vCPUs. Similar to previous experiments, we consider the optimizer state fully offloaded to the CPU for the 20B parameters model and focus on the performance of a single iteration. As observed in Figure 14, for lower CPU to GPU ratio, we observe up to 3x faster iteration with Deep Optimizer States as"}, {"title": "Verifying the Correctness of Our Performance Model.", "content": "We ran several experiments to demonstrate the correctness"}]}