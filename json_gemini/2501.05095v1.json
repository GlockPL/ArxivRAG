{"title": "Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment", "authors": ["Haoyi Xiu", "Xin Liu", "Taehoon Kim", "Kyoung-Sook Kim"], "abstract": "The pre-training and fine-tuning paradigm has revolutionized satellite remote sensing applications. However, this approach remains largely underexplored for airborne laser scanning (ALS), an important technology for applications such as forest management and urban planning. In this study, we address this gap by constructing a large-scale ALS point cloud dataset and evaluating its impact on downstream applications. Our dataset comprises ALS point clouds collected across the contiguous United States, provided by the United States Geological Survey's 3D Elevation Program. To ensure efficient data collection while capturing diverse land cover and terrain types, we introduce a geospatial sampling method that selects point cloud tiles based on land cover maps and digital elevation models. As a baseline self-supervised learning model, we adopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point clouds, and pre-train it on the constructed dataset. The pre-trained models are subsequently fine-tuned for downstream tasks, including tree species classification, terrain scene recognition, and point cloud semantic segmentation. Our results show that the pre-trained models significantly outperform their scratch counterparts across all downstream tasks, demonstrating the transferability of the representations learned from the proposed dataset. Furthermore, we observe that scaling the dataset using our geospatial sampling method consistently enhances performance, whereas pre-training on datasets constructed with random sampling fails to achieve similar improvements. These findings highlight the utility of the constructed dataset and the effectiveness of our sampling strategy in the pre-training and fine-tuning paradigm. The source code and pre-trained models will be made publicly available at https://github.com/martianxiu/ALS_pretraining.", "sections": [{"title": "1. Introduction", "content": "Airborne Laser Scanning (ALS) is an important remote sensing technology that captures high-resolution, three-dimensional spatial data by emitting laser pulses from an airborne platform and analyzing the reflected signals. This process generates dense light detection and ranging (LiDAR) point clouds, which accurately represent the Earth's surface including both natural and built environments. A significant advantage of ALS is its ability to penetrate vegetation and provide precise measurements, making it particularly valuable for applications such as terrain mapping [1], forest management [2], urban planning [3], and disaster management [4]. Large-scale pre-training and fine-tuning paradigms have been transformative across various artificial intelligence (AI) fields [5, 6]. These paradigms involve extensive pre-training on diverse datasets, enabling models to adapt effectively to a wide range of downstream tasks through fine-tuning. Commonly referred to as foundation models [7], they leverage large-scale self-supervised/unsupervised training to learn generalizable representations. Satellite remote sensing has also greatly benefited from this trend. By pre-training on large-scale unlabeled datasets, such as Sentinel-2, remote sensing foundation models [8, 9, 10] achieve state-of-the-art performance on a variety of downstream tasks, including scene classification, land cover mapping, and multi-temporal cloud imputation. However, large-scale pre-training and fine-tuning paradigms have yet to demonstrate their full impact on ALS applications. Although several large-scale datasets, such as OpenGF [1] and PureForest [2], exist, they lack the scale and land cover diversity necessary for training versatile models. Furthermore, while numerous freely available ALS LiDAR data sources, such as the United States Geological Survey 3D Elevation Program (USGS 3DEP) [11] and the Actueel Hoogtebestand Nederland (AHN) [12], provide extensive resources, there is currently no efficient method to extract data from these sources. Leveraging all available data is computationally prohibitive and often redundant. These limitations collectively hinder progress in adopting the pre-training and fine-tuning paradigm for ALS applications."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Remote sensing foundation models", "content": "Recently, AI has experienced a major paradigm shift from the supervised learning paradigm to the pre-train-fine-tune paradigm, where a large model is pre-trained with self-supervision on large-scale data and subsequently fine-tuned on downstream or target tasks [7]. These pre-trained models, known as foundation models, are designed to be broadly applicable across a wide range of tasks with minimal adaptation. Prominent examples include GPT-3 [5], CLIP [6], and LLaVA [15]. The remote sensing community has quickly embraced this trend, with many researchers exploring the potential of foundation models in this domain. Recently, numerous vision-based foundation models for remote sensing have emerged. Early works adopted contrastive learning approaches to construct foundation models without the need for annotations [16, 17, 18]. These methods often extend existing approaches from computer vision while incorporating characteristics unique to satellite imagery. For example, Ayush et al. [16] adapted MoCo-v2 [19] for remote sensing data by reformulating the pretext task to utilize geolocation and temporal image pairs. SeCo [17] designed self-supervision tasks that leverage the seasonal and positional invariances in remote sensing data, acquiring representations invariant to seasonal and synthetic augmentations. Similarly, CaCo [18] introduced a novel objective function that contrasts long- and short-term changes within the same geographical regions. More recently, SkySense [8] was introduced as a billion-scale foundation model pre-trained on multitemporal optical and SAR images. It adopts multi-granularity contrastive learning to capture representations across different modalities and spatial granularities, achieving state-of-the-art (SoTA) performance on seven downstream tasks. In addition to contrastive learning, numerous approaches based on MAE [20] have also been proposed. For instance, SatMAE [21] incorporates temporal and spectral position embeddings to effectively utilize temporal and multi-spectral information. RingMo [22] introduces a novel masking strategy to better preserve dense and small objects during masking. GFM [9] leverages the strong representations learned from ImageNet-22k [23] and enhances remote sensing image representation through continual pre-training. To better use spectral information in satellite imagery, SpectralGPT [10] introduces a 3D masking strategy and spectral-to-spectral reconstruction. Meanwhile, ms-GFM [24] incorporates cross-sensor pre-training using four different modalities to learn unified multi-sensor representations. This approach outperforms single-sensor foundation models across four downstream datasets. Research into vision-language models (VLMs) is also highly active, as these models enable zero-shot applications. For instance, RemoteCLIP [25] and SkyCLIP [26] adapt CLIP for remote sensing datasets, outperforming standard CLIP baselines. GRAFT [27] introduces a pre-training framework that uses ground images as intermediaries to connect text with satellite imagery, enabling pre-training without textual annotations. GeoChat [28] fine-tuned LLaVA-1.5 [29] on a proposed instruction-following dataset, demonstrating promising zero-shot performance across a wide range of tasks, including image and region captioning, visual question answering, and scene classification. Despite the significant advancements in developing foundation models for satellite imagery, to the best of our knowledge, no prior work has investigated foundation models specifically for ALS data\u2014an area we aim to explore in this study."}, {"title": "2.2. Datasets for 3D geospatial applications", "content": "With the rapid advancements in 3D acquisition technologies, the availability of outdoor point cloud datasets has grown significantly, driving progress in 3D geospatial data analysis through deep learning techniques. Existing datasets can be broadly categorized based on their data collection methods. Photogrammetric 3D datasets, such as Campus3D [30], SensatUrban [31], HRHD-HK [32], and STPLS3D [33], are generated using photogrammetry techniques but lack ground points beneath dense vegetation canopies due to the limitations of passive image capture, making them unsuitable for ALS applications requiring dense vegetation analysis. Terrestrial and mobile laser scanning (TLS/MLS) datasets, including Semantic3D [34], Paris-Lille-3D [35], SemanticKITTI [36], and Toronto-3D [37], are collected at street level and focus on roadway scene understanding. While they provide high point density and large data volumes, their limited geographic coverage, as well as restricted diversity in land cover and terrain, making them inadequate for broader ALS applications. ALS datasets, such as ISPRS Vaihingen 3D [38], DublinCity [39], LASDU [40], DALES [3], and OpenGF [1], are collected using airborne LiDAR sensors and primarily target urban classification and environmental perception by identifying common urban objects like ground, grass, fences, cars, and facades. However, their limited scale and coverage restrict their utility for training versatile models across diverse ALS tasks. Unlike these datasets, which are designed with specific applications in mind, this study aims to construct a large-scale dataset encompassing a wide range of terrains and land cover types to support the pre-training of generalizable 3D models tailored for ALS applications."}, {"title": "2.3. SSL methods for 3D Point Clouds", "content": ""}, {"title": "2.3.1. SSL methods for general 3D point clouds", "content": "SSL enables neural networks to learn from unlabeled data, making it ideal for 3D point clouds where annotations are costly. We mainly focuses on masked autoencoding based SSL methods as it is most relevant to this study. Masked autoencoding methods learn meaningful representations by reconstructing randomly masked input portions, such as image patches or text tokens, capturing structural and contextual information. Early works focus on generalizing BERT [41]'s masked language modeling to point clouds [42, 43, 44]. A representative example is Point-BERT [42], which trains a transformer encoder to predict masked dVAE [45]-generated tokens. Following the proposal of MAE, this idea was extended to point clouds. For example, Point-MAE [46] applies the concept by treating local point neighborhoods as patches for reconstruction. MaskPoint [47] introduces a masked discrimination task, replacing reconstruction with real/noise discrimination to improve robustness against sampling variance. Point-M2AE [48] incorporates a hierarchical masking strategy for multi-scale pre-training, while PointGPT [49] adopts GPT-style generative pre-training with a decoder tasked to generate point patches. MaskFeat3D [50] instead reconstructs surface properties, like normals, to learn higher-level features. Some methods explore multi-modal approaches to enhance representation quality. I2P-MAE [51] incorporates 2D-guided masking and reconstruction using knowledge from pre-trained 2D models. Joint-MAE [52] performs joint masking and reconstruction of 2D and 3D data with shared encoders and decoders. RECON [53] combines masked autoencoding and contrastive learning, leveraging their respective strengths while handling inputs from points, images, and text. Extensive work has focused on autonomous driving. Unlike indoor or synthetic point clouds, outdoor point clouds are sparse and have varying density. Traditional masked point modeling strategies often create overlapping patches, discarding important points. To address this, Voxel-MAE [54] uses a voxel-based masking strategy, predicting point coordinates, point counts per voxel, and voxel occupancy to better capture outdoor data distributions. Geo-MAE [55] improves further by predicting centroids, surface normals, and curvatures. GD-MAE [56] introduces a generative decoder, eliminating the need for complex decoders or masking strategies. Recently, BEV-MAE [14] explicitly focuses on learning BEV representations, achieving superior and efficient performance. In this work, we focus on the impact of pre-training on downstream tasks for ALS data rather than designing a new network. We adopt BEV-MAE as it suits ALS data well, with details provided in Section 4."}, {"title": "2.3.2. SSL methods for ALS 3D point clouds", "content": "SSL has recently been applied to ALS. [57] uses Barlow Twins [58] to improve semantic segmentation, especially for under-represented categories. [59] proposes a deep clustering and contrastive learning approach for unsupervised change detection, outperforming traditional methods. [60] pre-trains customized transformers under the MAE framework for 3D roof reconstruction, surpassing general MAE-based methods like Point-MAE and Point-M2AE. HAVANA [61] enhances contrastive learning by improving negative sample quality through AbsPAN. While effective in their respective applications, none have developed large-scale datasets or conducted large-scale pre-training on ALS point clouds for building general-purpose models, which is our primary goal."}, {"title": "3. Data for pre-training", "content": "In this section, we present the data source, describe the methodology used to develop the dataset, and provide key statistics of the resulting dataset to offer valuable insights into its characteristics. The overall procedure is shown in Figure 1."}, {"title": "3.1. Data source", "content": "Since ALS is often used for extracting objects on the Earth's surface or terrain information, the pre-training dataset should encompass diverse types of land cover and terrain to ensure that the trained models achieve strong generalizability. In this work, we use LiDAR data from the USGS 3DEP [62] as the primary data source for building a large-scale dataset for pre-training. 3DEP is a collaborative program designed to accelerate the collection of three-dimensional (3D) elevation data across the United States to meet a wide variety of needs [11]. High-quality LiDAR data are collected for the conterminous United States (CONUS), Hawaii, and U.S. territories, while interferometric synthetic aperture radar (IfSAR) data are collected for Alaska. The program has established specifications for collecting 3D elevation data and developed data management and delivery systems to ensure public access to these datasets in open formats [11]. The data are readily accessible through tools such as LiDAR Explorer [63] and The National Map [64]. These datasets have been widely used to support U.S. local economies [65, 66, 67] and advance scientific research [68, 69, 70]. The 3DEP data are well-suited for this study for two reasons: 1) its base specifications are designed for consistent data acquisition and the production of derived products, allowing the entire collection to be treated as a unified \u201c3DEP\u201d dataset; and 2) it captures the U.S.'s diverse land cover and varied terrain, making it an excellent foundation for constructing robust and versatile models with broad applicability across a wide range of geospatial and environmental contexts. The point cloud data are accessible via AWS [71], allowing for programmatic downloads. Additionally, each LiDAR point cloud includes boundary data, enabling users to easily define and select their area of interest. The point cloud boundaries used in this study are depicted in Figure 2."}, {"title": "3.2. Geospatial sampling", "content": "While 3DEP offers abundant resources for conducting pre-training, utilizing the entire dataset (>300TB) is practically infeasible. Therefore, a sampling strategy is required to extract representative data from U.S. regions while ensuring pre-training remains feasible. Motivated by the fact ALS is often used for analyzing terrain and land cover, we designed the sampling method to maximize diversity in both land cover and terrain."}, {"title": "3.2.1. Land cover", "content": "We use the NLCD as the source of land cover information. The NLCD product suite provides comprehensive data on nationwide land cover and changes over two decades (2001\u20132021), offering detailed, long-term insights into land surface dynamics. In this study, we utilize the latest NLCD2021 release, which includes land cover maps for 2001, 2004, 2006, 2008, 2011, 2013, 2016, 2019, and 2021. The NLCD2021 follows the same protocols and procedures as previous releases, ensuring compatibility with the 2019 database. As a result, analysis conducted for NLCD 2019 can be useful for understanding NLCD2021. For instance, the validation report for the 2019 release [13] is used to understand classification accuracy of the land cover classes, which will later inform the selection of reliable land covers for data downloads. The NLCD land cover product uses an adapted version of the Anderson Level II classification system, which includes 16 land cover classes (excluding those specific to Alaska). This system is derived from the original Anderson land use and land cover classification framework [72], designed to balance compatibility with U.S. federal classification systems, distinguishability of classes using primarily remote sensing data, and a hierarchical structure among classes. In this study, we use the Level I classification system by merging Level II classes, addressing the moderate per-class accuracy reported for the Level II system [13]. The Level I and Level II classification systems are presented in Table 1, with detailed definitions available in [73]."}, {"title": "3.2.2. Terrain", "content": "We utilize the seamless DEM provided by the National Map to obtain elevation data. The seamless DEM is a high-quality geospatial dataset developed by the USGS to support a wide range of geospatial research and applications. It integrates elevation data from various sources, including airborne LiDAR, photogrammetry, and cartographic contours, into a standardized format. Available at multiple resolutions\u20141/3 arc-second (about 10 meters), 1 arc-second (about 30 meters), and 2 arc-seconds (about 60 meters)\u2014these DEMs provide flexibility for detailed local analyses and broader regional studies. For this study, we use the 1 arc-second DEM to match the resolution of the land cover maps for joint analysis. The DEM is converted into a slope classification map to reflect topographic complexity. This slope map is further classified according to the system shown in Table 2, adapted from the USDA classification [74], to facilitate the joint analysis of land cover and terrain. The dataset is downloaded programmatically using the py3dep Python package [75]."}, {"title": "3.2.3. Data sampling", "content": "Due to the highly skewed distribution of land surface elements and the sheer data volume of 3DEP LiDAR point clouds, sampling is necessary to create a diversified dataset while keeping pre-training feasible. However, random sampling risks over-representing prevalent landscapes, such as forests, while under-representing less common features, leading to an unbalanced dataset. To this end, we develop a geospatial sampling method to create a dataset representative of diverse land cover and terrain types. This method leverages land cover and slope classification maps to select point cloud tiles from the 3DEP LiDAR database. Specifically, we first extract a land cover map from NLCD2021 by aligning the map year with the point cloud capture year. If an exact match is unavailable, the map from the closest year is used."}, {"title": "3.3. Dataset statistics", "content": "Based on the sampling strategy outlined above, we constructed the dataset. Although the number of tiles per LiDAR point cloud (or a LiDAR project) is limited to 40 in our experiments, this approach can be scaled to include any number of tiles, up to covering the entire area of the LiDAR projects. The distribution of tiles across classes is summarized in Table 3. A total of 73,762 tiles are sampled, with Flat areas being the most dominant category, accounting for 63.2%. In contrast, Sloped areas contribute 19,795 tiles (26.8%), where Forest dominates with 16,021 tiles compared to only 3,774 tiles for Developed areas. The Steep areas, representing the smallest category, account for 7,373 tiles (10.0%), with the majority (7,065 tiles) being Forest and only 308 tiles are classified as Developed. This distribution highlights the tendency for Developed land cover to be concentrated in Flat areas, while Forest land cover extends significantly into Sloped and Steep areas, where development is minimal. Overall, the Developed and Forest tiles are balanced. Therefore, our sampling strategy ensures a relatively balanced representation of different land cover whereas the topographic classes are limited by real-world conditions where steep slopes are predominantly forested, and development occurs primarily on flat terrain. Table 4 lists some representative ALS datasets for comparison. As shown, our dataset is the largest in terms of both geographical coverage and the number of points. Additionally, while other datasets are often specialized for specific target tasks and therefore include limited land cover or terrain types, our dataset encompasses a diverse range of land cover and terrain types at scale. Although the OpenGF dataset includes various land cover and terrain types, its geographical coverage is significantly smaller compared to ours, making it less suitable for large-scale pre-training and fine-tuning paradigms. Furthermore, we analyze key characteristics of the constructed dataset, including point density per square meter, ground point standard deviation, and return attributes. Due to the dataset's large size, we conduct this analysis on a subset created through random sampling. Specifically, we randomly select 30% of the dataset, amounting to 22,129 tiles."}, {"title": "4. Model architecture and pre-training", "content": "We adopt BEV-MAE [14] as our pre-training method. BEV-MAE is a cutting-edge pre-training method for 3D point clouds. The method is originally proposed for autonomous driving. It is designed for outdoor data and is designed to handle data from a Birds-Eye-View (BEV) perspective. The BEV-MAE inherits its design from Masked AutoEncoders (MAE) [20] in 2D image processing, which is an SSL technique that learns the representation by masking out a large portion of data and reconstructing them. This way of pre-training is assumed to be helpful for the model to learn high-level representation by forcing the model to reconstruct the invisible parts. BEV-MAE follows a similar way of processing pipeline as MAE: masking, backbone network, and reconstruction. An element of the mask (corresponding to a masked patch in image processing) in BEV-MAE is a 3D pillar (or a BEV cell) that encapsulates a volume of 3D points. Specifically, given a defined x and y ranges, all points that fall in the range are removed. All masked elements are replaced by the same learnable mask token. After masking, the remaining point clouds together with mask tokens are put into the encoder network. the encoder network is a 3D sparse CNN [80, 81], which is an memory efficient variant of voxel-based 3D CNN. It learns the multi-scale representation of point clouds with successive convolutions and downsampling. After the encoder, the masked point clouds are reconstructed from the corresponding mask tokens with a series of convolutions. In addition, the average number of points for each masked region is also reconstructed to learn about the local density. We consider BEV-MAE to be suitable for this study for the following reasons. Unlike other MAEs for point clouds such as MaskPoint [47] and Point-MAE [46], BEV-MAE is explicitly designed for handling outdoor data. Second, BEV-MAE adopts sparse CNN as its backbone which is more scalable to large-scale point clouds compared to Point-based methods. Last but not least, it is explicitly designed to utilize BEV perspective which is even more natural for ALS as it fits the way ALS point clouds are captured. For pre-training, we use the AdamW optimizer with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.99, a batch size of 16, and a one-cycle cosine annealing scheduler with a maximum learning rate of 10\u207b\u00b2. The models are trained for 50 epochs, with each epoch exposing the entire training dataset to the model once. We increase the number of parameters of the model to 60 M by increasing the channel widths without changing the depths. The input point cloud is a square tile with a side length of 500 meters. During training, we randomly crop smaller 144m \u00d7 144m tiles from the original tile. Each cropped tile is voxelized with a voxel size of 0.6m, and up to 200,000 voxels are sampled, with each voxel containing a maximum of 5 points. For the ground truth of point coordinate reconstruction, the point cloud is voxelized using a voxel size of 4.8 m \u00d7 4.8 m \u00d7 288 m to create BEV voxels, where the maximum number of voxels is set to 200,000 and each voxel can contain up to 30 points. The ground truth density is computed on the fly from the BEV voxels. Several common data augmentations including random flipping, scaling, and translation are performed for the input point clouds. The input features consist of the 3D coordinates of the points. All pre-training runs were conducted on the AI Bridging Cloud Infrastructure (ABCI) 2.0 using up to 4 NVIDIA V100 GPUs."}, {"title": "5. Experiments", "content": "We evaluate the pre-trained model on several downstream tasks, specifically focusing on tree species classification, terrain scene recognition, and point cloud semantic segmentation. In the following sections, we introduce the tasks, datasets, and fine-tuning architectures used for these downstream tasks. We then describe the evaluation metrics used in this study."}, {"title": "5.1. Task", "content": ""}, {"title": "5.1.1. Tree species classification", "content": "Tree species classification is a vital task for managing forests. The identification of tree species supports public policies for forest management and helps mitigate the impact of climate change on forests. To validate the effectiveness of the approach for tree species classification, we use the PureForest [2] dataset, a comprehensive collection tailored for analyzing forest environments. The dataset comprises 135,569 patches, each measuring 50 m \u00d7 50m, and covers a total area of 339 km\u00b2 across 449 distinct closed forests located in 40 departments in southern France. It includes 18 tree species, categorized into 13 semantic classes: Deciduous oak, Evergreen oak, Beech, Chestnut, Black locust, Maritime pine, Scotch pine, Black pine, Aleppo pine, Fir, Spruce, Larch, and Douglas. The dataset provides two modalities: colored ALS point clouds (40 points/m2) and aerial images (spatial resolution of 0.2 m). The task is to classify a patch into one of the 13 semantic categories. We use only the 3D point coordinates as input. Since each tile measures 50m \u00d7 50m, we use the entire tile as input during both training and testing. Following the pre-training settings, we limit the number of voxels after voxelization to 200,000. All the other settings remain the same as the pre-training ones. The architecture used for this task is illustrated in Figure 5 (middle). The model outputs a classification label representing the tree species of the input patch. To achieve this, the encoder's output is spatially pooled to form a global vector that summarizes the input point cloud. We concatenate the average-pooled and max-pooled vectors to emphasize both sharp and smooth features. The resulting global descriptor is then classified through a series of fully connected layers."}, {"title": "5.1.2. Terrain scene recognition", "content": "3D terrain scene recognition is crucial for understanding and classifying landforms, which plays a significant role in geography-related research areas such as digital terrain analysis and ecological environment studies [82]. Therefore, it is vital to validate the effectiveness of the pre-trained model on terrain scene classification. However, there are very few publicly available datasets for this task. While a prior study [82] exists, the data used in the study remains private. To address this limitation, we develop our own dataset to evaluate our model on terrain scene recognition. We base our dataset on OpenGF [1], which was originally designed for ground filtering. In OpenGF, the authors divided the data into four prime terrain types\u2014Metropolis, Small City, Village, and Mountain\u2014consisting of 160 500m \u00d7 500m point cloud tiles for training and validation. These terrain types are further subdivided into nine scenes: Metropolis is divided into regions with large roofs (S1) and dense roofs (S2); Small City is divided into tiles with flat ground (S3), locally undulating ground (S4), and rugged ground (S5); Village consists only of tiles with scattered buildings (S6); Mountain is divided into tiles with gentle slopes and dense vegetation (S7), steep slopes and sparse vegetation (S8), and steep slopes and dense vegetation (S9). Given the well-defined scene categories and the large size of the dataset, we create a dataset for terrain scene recognition based on OpenGF. To construct the dataset, we first combine the training and validation tiles from OpenGF. We then split the combined tiles into training, validation, and test sets, assigning 106 tiles (about 66%) to training, 27 tiles (about 17%) to validation, and 27 tiles (about 17%) to testing. For the training tiles, we divide each 500 m \u00d7 500 m tile into smaller 100 m \u00d7 100 m tiles using a sliding window cropping algorithm with an overlap of 50 m. For validation and test tiles, no overlap is applied. This process results in 10,591 tiles for training, 675 tiles for validation, and 675 tiles for testing. Examples of the dataset are shown in Figure 6. The task is to classify a patch into one of the nine semantic categories (S1\u2013S9). Similar to the tree species classification task, we use only 3D coordinates as input. During training, the entire tile is fed into the network. Following the pre-training settings, we limit the number of voxels after voxelization to 200,000. All the other settings remain the same as the pre-training ones. For the fine-tuning architecture, we use the same architecture as that used for tree species classification."}, {"title": "5.1.3. Point cloud semantic segmentation", "content": "Urban point cloud semantic segmentation provides vital information about ground objects for urban modeling. It involves classifying points in 3D data into meaningful categories such as buildings, roads, vegetation, and other urban features. In this work, we use the Dayton Annotated Laser Earth Scan (DALES) dataset [3], an aerial LiDAR dataset with nearly half a billion points spanning 10 square kilometers, to evaluate the performance of the pre-trained models. DALES consists of 40 scenes of dense, labeled aerial data covering multiple scene types, including urban, suburban, rural, and commercial. The dataset is hand-labeled by expert LiDAR technicians into eight semantic categories: ground, vegetation, cars, trucks, poles, power lines, fences, and buildings. While sensor intensity and return information are available, we use only the x, y, and z features as input. Each patch measures 500m \u00d7 500 m, and the task is to classify points into one of the semantic classes. Similar to the pre-training setup, we sample 144m \u00d7 144 m tiles from the 500 m \u00d7 500m patches during training. The maximum number of voxels is set to 200,000 during training and 1,000,000 during testing to ensure that all points are classified. All the other settings remain the same as the pre-training ones. The architecture used for this task is shown in Figure 5 (bottom). The model outputs a point cloud with per-point classification labels. To achieve this, we append a decoder to the pre-trained BEV-MAE encoder to recover the full resolution of the points. The decoder receives the downsampled high-level representations from the encoder and gradually transforms and upsamples the point cloud until it reaches full resolution. The resulting architecture is a U-Net [83]-style 3D CNN, which connects the encoder and decoder using skip connections. After passing through the decoder, each point is classified using a series of fully connected layers."}, {"title": "5.2. Evaluation Metrics", "content": "We use two commonly adopted metrics to evaluate the performance of the models: Mean Intersection over Union (mIoU) and Overall Accuracy (OA). mIoU evaluates a model's performance by measuring the overlap between predicted and ground truth points or point clouds. The IoU for each class i is defined as:\n$\\IoU_i = \\frac{TP_i}{TP_i + FP_i + FN_i},$ (1)\nwhere C is the total number of classes, TP\u1d62 represents the true positives for class i, and FP\u1d62 and FN\u1d62 denote the false positives and false negatives, respectively. IoU is considered a stricter and more comprehensive metric compared to metrics like precision and recall, as it penalizes both over-prediction (FP\u1d62) and under-prediction (FN\u1d62). The mIoU is then calculated as the average IoU across all classes:\n$\\mIoU = \\frac{1}{C}\\sum_{i=1}^C IoU_i $ (2)\nmIoU averages performance across classes, ensuring that both major and minor classes are equally weighted. Consequently, a high mIoU requires the model to perform well on all classes, regardless of their prevalence in the dataset. OA measures the proportion of correctly classified points or point clouds across all classes. Mathematically, OA is defined as:\n$OA = \\frac{\\sum_{i=1}^C TP_i}{\\#\\text{all points or point clouds}}$ (3)\nOA does not account for class imbalance, meaning that a high OA can be achieved by performing well on major classes, even if performance on minor classes is poor. These two metrics complement each other: mIoU emphasizes the model's ability to accurately predict each class, providing a balanced evaluation of model performance, while OA captures the overall classification accuracy across all classes."}, {"title": "6. Results", "content": "In this section, we present the pre-training results through the reconstruction performance of BEV-MAE. We then discuss the results of various downstream tasks, including tree species classification, terrain scene recognition, and point cloud semantic segmentation."}, {"title": "6.1. Pre-training results", "content": "To validate the quality of the learned representations, we visualize the reconstructed coordinates and densities in this section. We feed unseen point cloud tiles (not used during pre-training) into the model and visualize the reconstructed coordinates and densities. As shown in Figure 7, our first observation is that the model effectively reconstructs the overall patterns of the point clouds. The reconstructed surface objects, including buildings, trees, and the ground, align roughly well with the ground truth point clouds. However, a significant amount of detail is missing in the reconstructions, suggesting that detailed shape information is not fully recovered. Specifically, points within individual BEV cells often reconstruct as simple plane-like shapes, failing to capture the fine-grained details of the objects. This indicates that the network primarily learns abstract shapes of the point clouds rather than their fine-grained geometry. Furthermore, this limitation suggests the model may struggle to recognize smaller objects, as their relatively small size and detailed shape information are more easily obscured during the masked autoencoding process."}, {"title": "6.2. Downstream application results", "content": ""}, {"title": "6.2.1. Tree species classification", "content": "As shown in Table 9, the pre-trained model outperforms the scratch model by 3.4% in mIoU and 0.3% in OA, highlighting the effectiveness of the dataset and pre-training for tree species classification. Specifically, in Table 10, the pre-trained model demonstrates either slight or substantial improvements over the scratch model across nearly all categories. This suggests that pre-training enables the model to learn generalizable shape-related features beneficial for distinguishing between tree species. The most notable improvements are observed in the Black locust and Douglas classes, with performance gains of 8.5% and 14.5%, respectively. We hypothesize that these significant improvements stem from these species being native to the United States and likely included in the pre-training dataset. Furthermore, the improvement in the Douglas class may also be attributed to its small sample size (as shown in the Patches column), which limits the scratch model's ability to learn transferable features effectively. Moreover, our models including both scratch and pre-trained ones have largely outperformed the baseline models reported by [2"}]}