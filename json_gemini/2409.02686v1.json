{"title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs", "authors": ["Ruoyu Wang", "Xiaoxuan Li", "Lina Yao"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but recent studies reveal that these models often fail to achieve satisfactory results on questions involving reasoning, such as mathematics or physics questions. This phenomenon is usually attributed to the uncertainty regarding whether these models could genuinely comprehend the knowledge embedded in the text or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems we observe in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable progress on Large Language Models (LLMs) [38], especially those instruction-following models such as ChatGPT and GPT-4 [17]. Numerous studies have demonstrated that these models exhibit strong capabilities across a wide range of tasks. However, despite the effectiveness of these models, existing work [11] shows that they perform poorly on Out-of-Distribution tasks, so fine-tuning with specific tasks and datasets is required to achieve satisfactory results."}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 LLAMA-Adapter", "content": "LLaMA-Adapter [37] is a lightweight adaption method to fine-tune LLaMA into an instruction-following model, which has demonstrated the capability to generate high-quality responses. We conducted our study and built our method based on LLaMA-Adapter due to its effectiveness and efficiency.\nThe architecture of LLaMA-Adapter is illustrated in Figure 2a. For each of the topmost L Transformer layers of LLaMA, an adaption prompt T\u2081 \u2208 R^{M\u00d7C} is concatenated to the original prompt Pi \u2208 R^{K\u00d7C} along the token dimension:\n[Pi;Ti] \u2208 R^{(K+M)\u00d7C} (1)\nwhere M denotes the length of the adapter to be concatenated, K denotes the original prompt length for each transformer layer, and C denotes the feature dimension of LLaMA's transformer. This concatenation operation is applied to the corresponding dimension in Key and Value in the self-attention mechanism.\nFurther, a zero-init attention mechanism with zero gating is proposed to improve the training by injecting the new instructional cues into LLaMA. While calculating the attention score, the softmax function is applied independently to the two components in Equation 1, and multiplies the concatenated term by a gating factor g\u0131, as illustrated in Equation 2 and Figure 2a.\nS = [Softmax(SK); Softmax(SM) \u00b7 g\u0131]^T (2)\nWe highlighted part of the architecture of the LLaMA-Adapter that is closely related to our method. We direct interested readers to [37] for comprehensive details of this method."}, {"title": "2.2 Causal Inference", "content": "In the domain of Causality [18], causal relationships are usually denoted by Directed Acyclic Graph (DAG). For example, in Figure 2b, X \u2192 Z denotes that X is a direct cause of Z. There are three basic building blocks in a causal graph: Chain, Fork, and Collider. Chain is the case where one element causally influences another, then leading to the causal impact on a third element, such as X \u2192 Z \u2192 Y in Figure 2b. Fork is the case where one element causally influences two other elements, such as X \u2190 C \u2192 Y in Figure 2b. Collider is the case where two elements causally influence a third element such as C \u2192 Y Z in Figure 2b.\nConfounder If a variable is the common cause of two other variables, it is called a confounder. Confounders will induce spurious correlations between the two variables, thus disturbing the recognition of the causal effect between them. For example, in Figure 2b, C is a confounder between X and Y. The association between X and Y include the spurious correlations created by the confounder C (X \u2190 C \u2192 Y) which is non-causal, and the goal of causal inference is to deconfound the spurious correlations so that the true causal relationships between X and Y (X \u2192 Z \u2192 Y) can be measured.\nIntervention In order to measure the causal effect between X and Y, we need to avoid the association flow through the fork X \u2190 C \u2192 Y by blocking the path C\u2192 X. To this end, we force the variable X = x regardless the value of C. In that case, C no longer affects the value of X and thus path C \u2192 X is blocked. This process is called intervention in causal inference and is denoted as do(X=x) (Figure 2c). In contrast to P(Y|X), which comprises both causal association and spurious correlations caused by confounder, P(Y|do(X)) allows us to measure the genuine causal effect between X and Y."}, {"title": "3 Our Method", "content": ""}, {"title": "3.1 Investigation and Motivation", "content": "As discussed in Section 1, we aim to investigate if the prompt-following models have genuine causal reasoning capabilities. To this end, we conduct the following experiments. Since models such as ChatGPT and GPT-4 are not available in open-source form, we conduct our study using LLaMA-Adapter [37] to gain access to attention values and representations at each layer.\nFirst, we fine-tune the LLaMA 7B model with LLaMA-Adapter using the Letter Concatenation dataset, which will be introduced in Section 4.2. Then, we test the model with two prompts below. The only difference between these two prompts lies in the string within the quotation marks, and as a result, the model answered Prompt A correctly, but failed on Prompt B.\nPrompt A: Take the second last letters of the words in \u201cGALLEGOS MORAN\" and concatenate them;\nPrompt B: Take the second last letters of the words in \u201cDAVENPORT MAGANA\" and concatenate them.\nTo explore the cause of the model's failure on Prompt B, we visualize the attention values in the text generation process by adapting BertViz [28], and conduct a thorough comparison between the two test cases on the attention heat map of each attention head across all transformer layers. Consequently, we found that the model's failure on Prompt B can be attributed to the malfunctioning of some particular adapter structures.\nFigure 3a-3b provides an example of such malfunctioning structures, where we present the attention values of the sixth element in the adapter (adap_6) located in the 32nd attention head of the last transformer layer of LLaMA-Adapter. We observed that when the model correctly predicts the answer (Figure 3a), adap_6 tends to focus on the question rather than the value of the string. However, in Figure 3b, where the model failed to provide the correct answer, it exhibits a focus on a portion of the string, such as token \"AG\" and \"AN\" as highlighted. Similar patterns can also be observed in many other cases. Therefore, we empirically conclude that such malfunctioning units are the root cause of the mistake the model made on Prompt B.\nIn other words, simply replacing the string within the quotation marks significantly affects the thinking process of the model. This behaviour starkly contrasts with how humans solve such questions. From a human perspective, Prompt A and Prompt B are nearly identical, if we understand how to solve one of these problems, we inherently possess the method to solve all similar questions. This is because humans understand the world through causal relationships, enabling us to recognize and comprehend the underlying rationales. In contrast, LLMs were constructed based on statistical associations, leading to a deficiency in their capacity to comprehend the question and to do causal reasoning."}, {"title": "3.2 Method Specification", "content": "We formulate the reasoning process of LLMs into a causal framework, as illustrated in Figure 3c. In this framework, X denotes the encoded feature of the prompt, K denotes the relevant knowledge to solve the problem provided by the LLM, and Y denotes the LLM's response to the query.\nLLM \u2192 X When a prompt is presented to the LLM, it encodes the prompt into feature X. Therefore, LLM is the direct cause of X.\nLLM \u2192 K \u2190 X Once the prompt is encoded, the LLM offers the relevant knowledge K required to solve the problem in X. Therefore, both the LLM and X are direct causes of K.\nK \u2192 Y \u2190 X The knowledge K encompasses the method on how to solve the problem described in X, while X contains the question-specific information, such as the values involved in the problem. So both X and K are a cause of Y."}, {"title": "3.3 Implementation of Causal Intervention", "content": "In this section, we introduce our method to implement the intervention on XG, as illustrated in Figure 3d. First, we assume that the general problem-solving information XG and the problem-specific information Xs can be identified by comparison across samples in a dataset, i.e., the differences between data samples are problem-specific, and thus belong to Xs, and the general problem-solving knowledge, denoted as XG, is common across all samples. For instance, in the example given in Section 3.1, XG contains the method of fetching the desired characters and performing concatenation, and Xs contains the order of the characters to be fetched and from which string are these characters to be selected.\nWith this assumption, performing the intervention do(XG) is equivalent to holding XG invariant across all data samples so that it can maintain the general problem-solving information consistently while changing Xs. For example, we aim to hold adap_6 invariant across Figure 3a and Figure 3b, to avoid it possessing information of Xs, such as the token \"AG\" and \"AN\" in Figure 3b.\nThus, we introduce a causal constraint into the training process to encourage XG to remain invariant across all data samples. Mathematically, we penalize a larger value of variance on XG by introducing a regularization term in Equation 4\nmin L_{CE} + \u03b1L_{causal} (3)\nL_{causal} = E_{l\u2208L'} [Var(X_{G})] (4)\nwhere LCE is the Cross-Entropy Loss used to train the token prediction accuracy, and \u03b1 is the weight of our causal regularization term. We apply this causal regularizer on the topmost L' transformer layers, so we take expectation over these layers, where L' < L is a tunable hyper-parameter.\nIn order to estimate XG in each of the topmost L' layers in Equation 4, we divide the concatenated adapter Ti into two separate pieces, Ti,1 with the length H, and T1,2 with length M \u2013 H. Therefore, we rewrite Equation 1 as:\n[Pi; T_{1,1}; T_{1,2}] \u2208 R^{(K+H+(M-H))\u00d7C} (5)\nSimilar to the vanilla LLaMA-Adapter, this affects the dimension setting of the Key and Value in the self-attention module. Therefore, we rewrite these two modules as Equation 6 and Equation 7.\nK\u2081 = [K_{vanilla}; K_{adap1}; K_{adap2}] (6)\nVi = [V_{vanilla}; V_{adap1}; V_{adap2}] (7)\nThen, instead of applying the softmax function on the three components independently, we first apply the softmax function on the two original components and multiply with the gating module introduced in the vanilla LLaMA-Adapter, then separate the score matrices into three pieces. Therefore, we have Equation 8.\nS = [S_{vanilla}; S_{adap1}; S_{adap2}] (8)"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental Settings", "content": "We build our method by fine-tuning LLaMA 7B model [26], thus all the parameters related to dimensions and layers remain unchanged, such as the number of transformer layers is 32, and each transformer layer has 32 attention heads. Also, the feature dimension is 128 for each attention head, thus the total feature dimension is 4096. We train the model with a maximum sequence length of 256, and use AdamW for optimization with a learning rate equal to 1e-3. All the models are fine-tuned for 5 epochs with a batch size of 4 for a fair comparison.\nIn terms of the parameters introduced by vanilla LLaMA-Adapter, we set L = 20 and M = 10, which means we fine-tune the top 20 transformer layers by appending an adapter prompt of length 10 on each of them. For the parameters H and \u03b1 introduced by our method, we set Has 2 and \u03b1 as 1 in all experiments. The parameter L' is data-dependent, and we use 20 for Letter Concatenation, 10 for Date Understanding, 3 for AddSub and Math10k, and 1 for Math401. All other settings, if not specified here, remain the same as in [37]."}, {"title": "4.2 Tasks for Evaluation", "content": "We evaluate the performance of our method by three types of reasoning tasks:\nSymbolic Reasoning We construct a more challenging version of the last letter concatenation problem in [30] because the models could almost perfectly solve the problems if the models are fine-tuned with it. Therefore, we ask the model to perform second last letter concatenation, such as Take the second last letters of the words in \"Lady Gaga\" and concatenate them.\nCommonsense Reasoning We test the models with Date Understanding data [23], where each data sample asks a multiple-choice question such as If today is Jan 1, 2023, what day is tomorrow in MM/DD/YYYY?\nArithmetic Reasoning We test the models on three datasets, Math401 [34], which comprises basic arithmetic questions such as 1+2=?, AddSub [6] and Math10k [9], both comprises math word questions such as Tom found 7 seashells but 4 were broken. How many unbroken seashells did Tom find?."}, {"title": "4.3 Baselines and Comparison Methods", "content": "We compare our method with other methods from three perspectives to conduct a comprehensive comparison:\n1) We compare our method with the vanilla LLaMA-Adapter [37]. Since we build our method based on LLaMA-Adapter, this comparison allows us to understand the direct impact of implementing our method. All common settings between the two methods such as parameters are kept the same to ensure a fair comparison. The results of this comparison is presented in the bottom block of Table 1, and we highlight the margin achieved by our method in green.\n2) We compare our method with the other parameter-efficient fine-tuning (PEFT) methods, as listed in the middle block of Table 1. We apply these methods on LLaMA 7B, and the results are obtained with the library and hyper-parameters provided by [16,9]. We present the results and the number of learnable parameters allowing us to compare our method with the baseline methods in terms of both effectiveness and efficiency.\n3) We compare our method with several pre-trained prompt-following models with the size of 7B, as listed in the top block of Table 1. These models do not lie in the domain of PEFT and thus are not directly comparable to our method. They are either obtained by full fine-tuning or pre-trained with massive conversational data. We compare our method with these models to investigate their performances on the reasoning tasks and evaluate if task-specific fine-tuning is necessary to achieve satisfactory results."}, {"title": "4.4 Overall Results", "content": "The results are presented in Table 1, where the numbers denote the accuracies the methods achieve on each dataset. While comparing our method with the three types of baselines outlined above, our findings also fall into three aspects:\n1) Compared with LLaMA-Adapter: Our method consistently outperforms LLaMA-Adapter by a considerable margin on all datasets, as highlighted in green in Table 1. Since all the common settings of the two methods remain the same, the results directly demonstrate the impact of our causal method.\n2) Compared with the other PEFT methods: We found that while the vanilla LLaMA-Adapter does not always outperform the baseline methods, our method, in contrast, achieves either the highest or the second highest score across all datasets. Even though a few methods may perform better than our method on some particular datasets, it is worth noting that our method has only 1.2M learnable parameters, which is the least among all methods. In summary, our method achieves better or comparable results with other PEFT methods, with much less learnable parameters.\n3) Compared with pre-trained models: We found that the performance of pre-trained models is generally not satisfactory compared with the PEFT methods. While these models achieve fair performances on some datasets, they face significant challenges in the LConcat task. Notably, it was observed that none of the pre-trained models under consideration could accurately respond to the Letter Concatenation questions. To ensure this phenomenon is not due to the bias in our prompt, we endeavoured to rephrase the questions in LConcat, however, the models consistently exhibited an inability to comprehend the prompts and frequently provided irrelevant or meaningless responses. We speculate that this is due to the insufficient inclusion of training data of this specific nature during the model's fine-tuning phases.\nSummary Our experiments suggest that fine-tuning on specific tasks is necessary to achieve satisfactory results. And, among the Parameter-Efficient Fine-Tuning methods, our method achieves better or comparable results with much less learnable parameters and computational resources."}, {"title": "4.5 Effects of New Parameters", "content": "To further investigate the mechanism of our method, we study the impact of parameters introduced by our method, namely, the length H of adaption prompts to be treated as XG, the weight \u03b1 of the regularization term Lcausal, and the number of layers L' to be used to calculate Lcausal.\nChoice of H and \u03b1 We visualize the effect of H and \u03b1 on the Letter Concatenation dataset in Figure 5a 5b, where the x-axis denotes the value of the parameters, and the y-axis denotes the accuracy obtained by the model. Similar trends can be observed in both charts that increasing the value of H and \u03b1 can improve the performance of the model, but excessive values can be detrimental. This aligns with our intuition. For H, if a substantial fraction of the adapter remains fixed as XG, then only a limited part of the adapter could be left to address Xs, which compromises its efficacy in managing problem-specific information. For \u03b1, if a large weight is employed for Lcausal, the module to handle XG might remain constant and cannot encode any information.\nChoice of L' We found the optimal choice of L' is data-dependent. On datasets like Letter Concatenation, where all the prompts follow the same format, a larger L' is beneficial to the performance. In contrast, on datasets like AddSub, where the questions are not necessarily in the same template, a smaller L' is preferable. This is intuitively reasonable, because for those datasets where the prompts are close enough in the first place, encouraging the model to extract XG from the bottom layers grants us more control over the reasoning process. In contrast, for those datasets where the prompts are not sufficiently close, XG can only be extracted and controlled when the representations have been aggregated to a certain level. In that case, a large L' would limit the model's potential for aggregating the high-level information."}, {"title": "4.6 Further Discussions", "content": "Applicable scenarios We illustrate the motivation and idea of our method in Section 3.1. However, it is worth noting that our method is not limited to the case of the same pattern questions. Instead, prompts in different formats also benefit from our method. As demonstrated in Section 4, our method benefits a wide range of reasoning tasks with various datasets. This is because we encourage the model to extract the \"approach\" of solving problems. In other words, as long as a prompt involves reasoning, there will be some problem-solving skills (XG), and our method is applicable to the scenario. For example, in date understanding and math word questions, where the prompts vary significantly, our method still benefits the performance as illustrated in Table 1, because we encourage the model to extract the high-level knowledge, such as the meaning of \"tomorrow\", \"end of the month\" or the math operations such as \"Add\", \"Subtract\", and keep these problem-solving skills invariance across all data samples. In contrast, our method does not apply to the general Q&A questions, such as Tell me about Alpaca, because these questions do not require reasoning capabilities and there is no \"approach\" to answer these questions.\nFew-shot experiments Few-shot prompt method such as Chain-of-Thought (COT) [30] is known to be useful on large models like ChatGPT/GPT4, but it does not apply to PEFT methods, so we did not include these experiments in our paper. To elaborate, COT works well on ChatGPT/GPT4 because those models are fine-tuned by a massive amount of prompt-answer pairs with one-shot examples, enabling the model to utilize one-shot information effectively. In contrast, our method fine-tunes a non-prompt-following LLMs (LLaMA) with task-specific data aiming for improved performance on the task. Since the data does not contain any one-shot prompts, the model will not be able to utilize the one-shot information. As a matter of fact, our experiments reveal that COT is even harmful to the result in such cases.\nFinetuning a prompt-following model We also conduct experiments to apply our method on prompt-following models such as Alpaca. As a result, it achieves an accuracy of 75.3 on LConcat, and 79.8 on Date Understanding datasets, which is not comparable to the result we achieved using the original non-prompt following LLaMA. We speculate this is because such instruction-tuned LLMs (such as Alpaca/Vicuna) are also based on the original foundation model such as LLaMA, and it has been fine-tuned with the data that are not closely related to our downstream tasks, thus dropping some information relevant to our task, thus harming the performance. Therefore, we empirically conclude that it would be a better practice to fine-tune the foundation model, rather than an existing instruction-following model."}, {"title": "5 Related Works", "content": "Reasoning in LLMs. Instruction-following LLMs have been employed on many tasks involving reasoning recently, including but not limited to Mathematics, Logical Reasoning, and Symbolic Reasoning [20,30,38]. Many of these methods investigate LLM's reasoning capabilities from its output using Chain-of-Thought prompting strategy [30]. Apart from these, some works build thinking pipelines [1,33] to achieve the final goal step-by-step.\nCausal Inference in Machine Learning. Causal inference has been applied to many vision tasks in recent years such as image recognition [35,24] and Image Generation [12]. These works first construct causal graphs to explain the task, then use causal inference methods to eliminate the spurious association and improve the performance of the models. Besides, causal inference techniques are also used in Representation Learning [22,32]."}, {"title": "5.1 Relationships with our method", "content": "Existing works typically discuss LLMs' reasoning abilities based on their input and output [20]. However, we argue that solving causality-related tasks or providing the thinking processes by words do not necessarily indicate the model's reasoning capability, because simply mimicking token distribution could achieve equivalent outcomes. Our work, in contrast, discusses the reasoning capabilities of LLMs in the level of attention and representation, thus offering a novel perspective on this matter. Besides, the novelty of our method also involves applying causality in LLM fine-tuning, which was rarely discussed in earlier literature."}, {"title": "6 Conclusion", "content": "In this paper, we first investigated the reasoning capabilities of the prompt-following LLMs by visualizing the attention values in the thinking process, and empirically suggest that these models lack genuine causal reasoning capabilities. Then, we formulate the reasoning process of LLMs into a causal inference framework to explain the issues observed in the visualization. Finally, we propose Deconfounded Causal Adaptation (DCA), a causal fine-tuning method to improve the model's reasoning capability. Experiments show our method effectively enhances the reasoning capabilities of the models and outperforms baseline methods consistently. Besides, we also discuss the applicable scenarios of our method and analyze the effect of our method with different settings thoroughly."}]}