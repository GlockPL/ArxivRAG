{"title": "Learning to Verify Summary Facts with Fine-Grained LLM Feedback", "authors": ["Jihwan Oh", "Jeonghwan Choi", "Nicole Hee-Yeon Kim", "Taewon Yun", "Hwanjun Song"], "abstract": "Training automatic summary fact verifiers often faces the challenge of a lack of human-labeled data. In this paper, we explore alternative way of leveraging Large Language Model (LLM) generated feedback to address the inherent limitation of using human-labeled data. We introduce FineSumFact, a large-scale dataset containing fine-grained factual feedback on summaries. We employ 10 distinct LLMs for diverse summary generation and Llama-3-70B-Instruct for feedback. We utilize this dataset to fine-tune the lightweight open-source model Llama-3-8B-Instruct, optimizing resource efficiency while maintaining high performance. Our experimental results reveal that the model trained on extensive LLM-generated datasets surpasses that trained on smaller human-annotated datasets when evaluated using human-generated test sets. Fine-tuning fact verification models with LLM feedback can be more effective and cost-efficient than using human feedback. The dataset is available at https://github.com/DISL-Lab/FineSumFact", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have significantly enhanced the text summarization performance (Tang et al., 2024; Zhang et al., 2024). State-of-the-art models such as GPT-4 excel at generating coherent summaries from extensive datasets, processing input contexts exceeding 100k tokens, thereby significantly enhancing their summarization capabilities (Ravaut et al., 2023). However, hallucination issues still occur in summaries, highlighting the importance of summary fact verification (Cao et al., 2022).\nVerifying the fact of the summaries inevitably necessitates considerable human effort, rendering the evaluation process both time-intensive and cost-prohibitive. In manual evaluation, non-expert human evaluators are often tasked with labeling summaries across diverse domains (Geiger et al., 2020). In particular, this process gets more costly and challenging to reproduce at a fine-grained level evaluation, such as error localization and explainable evaluation.\nTo mitigate the human cost involved, an alternative way is to employ AI-assisted labeling approaches (Desmond et al., 2021; Wang et al., 2021) and the training of language models using LLM-generated labels, also known as knowledge distillation (Pangakis and Wolken, 2024). However, the application of knowledge distillation for fact verification remains unexplored.\nIn this paper, we unveil the potential of using LLM-generated fine-grained feedback to train an efficient and effective fact verification model. As shown in Figure 1, our pipeline consists of four"}, {"title": "3 Preliminary", "content": "Dataset with Human Feedback. Datasets with human fact labels are widely used to train and test automated fact verifiers. For a more complete evaluation, we aggregate all the available human-labeled datasets for sentence-level fact verification, including AggreFact (Tang et al., 2022), DiaSumFact (Zhu et al., 2023), TofuEval (Tang et al., 2024), and Ramprasad' 24 (Ramprasad et al., 2024). The aggregated data contains 6,546 document-summary pairs, each of which has sentence-level binary labels - \"0\" for no error and \"1\" for a fact error. 85% of pairs are used for training a fact verifier (one of our baselines) and the remaining 15% of those are used for testing all the compared verifiers. See the details in Appendix A."}, {"title": "4 Learning with LLM Feedback", "content": "We build a large-scale dataset with LLM feedback to train a fact verifier capable of generalizing across various input contexts. Our dataset contains 10,877 documents, encompassing multiple domains, varying lengths, and two types (i.e., non-dialogue, dialogue). Particularly, the domains represented in the dataset include news (CNN/DM: Hermann et al. 2015), interview (MediaSum: Zhu et al. 2021), daily (DialogSum: Chen et al. 2021), meeting (MeetingBank: Hu et al. 2023), knowledge (WikiHow: Koupaee and Wang 2018), report (GovReport: Huang"}, {"title": "5 Evaluation", "content": "Methods. We compare our fine-tuned model with several counterparts: (1) QA- and NLI-based methods, including QAFactEval (Fabbri et al., 2022) and SummaC (Laban et al., 2022); (2) Llama-3-8B-Instruct with zero-shot inference with FineSurE's prompt; (3) fine-tuned with human feedback. Contrary to (1) and (3), our model is only exposed to fine-grained LLM-generated feedback. In addition, for (3), it is not possible to localize error types due to the lack of available human error types annotated.\nMetrics. We follow the widely used metrics in recent works (Song et al., 2024; Liu et al., 2023), verifying the agreement with human in three different levels: balanced accuracy (bAcc), an indicator of sentence-level verification accuracy; summary-level correlation, an indicator of agreement with humans' summary-level scores; system-level correlation, an indicator of agreement with humans' ranking across different summarizers. Detailed description is provided in Appendix F."}, {"title": "5.1 Agreement with Humans", "content": "Table 1 shows the agreement with human judgment on test datasets, as described in Section A.3"}, {"title": "5.2 Factuality Error Localization", "content": "Another advantage of using LLM-based feedback is its fine granularity, which allows for the specification of even factuality error types. Table 2 presents the accuracy of error localization across seven categories. Despite the 57.4% of bAcc achieved by zero-shot inference, it only achieves very low performance in localization, which is almost the same as the mean accuracy of random guessing. However, when fine-tuned with LLM feedback, the mean accuracy improves from 14.3% to 27.8%\u00b9. Therefore, fine-tuning with LLM feedback enhances the error localization capability over zero-shot inference."}, {"title": "5.3 Ablation on Feedback Granularity", "content": "We adjust the granularity of LLM feedback in three ways: (1) using only the binary labels indicating whether each sentence is factually correct or not (see Figure 2); (2) adding a reasoning step like the chain-of-thought in prompt engineering (see Figure 3); and (3) transforming the task to"}, {"title": "5.4\nAblation on Feedback Size", "content": "To value the effectiveness of LLM feedback, we ablate the size of training data in fine-tuning, as summarized in Table 4. 25.0% of our training data (25,660 LLM feedback) ensures better agreement than using 5,853 human feedback in fine-tuning. This explains that 5 LLM feedback are likely worth 1 human feedback. Moreover, increasing the volume of training data with LLM feedback shows almost continuous improvement in fact verification performance."}, {"title": "5.5 Inference Latency", "content": "Table 5 shows that our fine-tuned model is more cost- and computing-efficient than other LLMs while keeping high performance. From the perspective of knowledge distillation, our model achieved performance close to 95% of the teacher model, Llama-3-70B-Instruct, while delivering over 3x faster inference time. Furthermore, when compared to the more affordable commercial model, ChatGPT-3.5-Turbo, our model exhibited"}, {"title": "5.6 Understanding Why It Works", "content": "In this section, we discuss why training with LLM-generated feedback outperforms human feedback. Human evaluation becomes unreliable when summary feedback is fine-grained, such as identifying error types or providing explainable reasons. In the Appendix, Table 6 shows that existing fine-grained human-labeled datasets have an inter-annotator agreement (Kappa) below 0.5, indicating low reliability of human labels. Therefore, the quality difference between LLM-generated labels and human labels is not significant. Based on this observation, according to the scaling law for LLM (Kaplan et al., 2020), an increase in the amount of training data is expected to enhance the performance of our model."}, {"title": "6 Conclusion", "content": "We release FineSumFact, a large-scale training dataset with LLM feedback, which can be used to train a fact verification model. We test multiple strategies to fine-tune LLMs w.r.t the granularity and the size of LLM feedback. The results indicate that fine-tuning with LLM feedback has the potential to create an effective and efficient fact verifier, addressing the lack of human feedback in training automated fact verification models."}, {"title": "Limitations", "content": "We report two main limitations in our study.\nFirstly, summary feedback was generated from a single model, Llama-3-70B-Instruct. Therefore, we are unable to reflect feedback from diverse distributions. If we generate feedback using various LLMs, we would be able to generate more accurate feedback. Additionally, our training model, Llama-3-8B-Instruct, is fine-tuned using data comprised of summaries generated by 10 LLMs and feedback generated by Llama-3-70B-Instruct. Consequently, from a knowledge distillation perspective, the performance of the fine-tuned model may not surpass that of the LLMs used to generate the LLM feedback.\nSecondly, as discussed in Section 5.2, our dataset with LLM feedback presents some error category imbalance. Despite generating summaries using 10 LLMs, there was a lack of diversity in terms of error types. In the generated summary, there is significant inclusion of out-of-context error (OutE) and entity error (EntE), while coreference error (CorefE) is notably less frequent. Therefore, it was challenging to analyze performance by error type in error localization. Generating summaries synthetically to include a variety of error types could be a solution.\nThese challenges remain as future work."}, {"title": "Ethics Statement", "content": "There are no significant ethical concerns related to this work. Since there is no procedure involving human participation, there are no issues of bias. Additionally, we followed the copyright regulations, and there are no related concerns."}]}