{"title": "TemporalBench: BENCHMARKING FINE-GRAINED TEMPORAL UNDERSTANDING FOR MULTIMODAL VIDEO MODELS", "authors": ["Mu Cai", "Reuben Tan", "Jianrui Zhang", "Bocheng Zou", "Kai Zhang", "Feng Yao", "Fangrui Zhu", "Jing Gu", "Yiwu Zhong", "Yuzhang Shang", "Yao Dou", "Jaden Park", "Jianfeng Gao", "Yong Jae Lee", "Jianwei Yang"], "abstract": "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~ 30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a \"centralized\" description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.", "sections": [{"title": "INTRODUCTION", "content": "The ability to understand and reason about events in videos is a crucial aspect of artificial intelligence, with applications ranging from activity recognition and long-term action anticipation to perception for autonomous driving and robotics. Recently, there has been an emergence of highly capable multimodal generative models, including proprietary ones such as GPT-40 (OpenAI, 2024) and Gemini (Gemini Team, 2024) as well as open-sources ones (Liu et al., 2023a; Zhu et al., 2024b; Bai et al., 2023), that have demonstrated impressive results on existing video benchmarks (Xu et al., 2016; Chen & Dolan, 2011; Yu et al., 2019a; Mangalam et al., 2024). However, these benchmarks often do not truly evaluate the abilities of the aforementioned models to understand video content due to their generally coarse-grained annotations.\nThe lack of fine-grained temporal details in the annotations often leads to existing video understanding benchmarks suffering from a strong language prior bias. This is similar to observations in visual"}, {"title": "RELATED WORK", "content": "Large Multimodal Models. Large Language Models (LLMs) like ChatGPT (OpenAI, 2023b), GPT-4 (OpenAI, 2023c), and Llama (Touvron et al., 2023) have demonstrated impressive reasoning and generalization capabilities for text. The introduction of models that integrate visual data has brought about a significant shift in the landscape of LLMs, such as GPT-4V(ision) (OpenAI, 2023a). Building upon open-source LLMs (Touvron et al., 2023; Chiang et al., 2023), a wide range of multimodal models has achieved remarkable progress, led by pioneering models such as LLaVA (Liu et al., 2023a; 2024a) and MiniGPT-4 (Zhu et al., 2024b), which combine LLMs' capabilities with a CLIP (Radford et al., 2021) based image encoder. Recently, a growing number of LMMs have been developed to handle a wider range of tasks and modalities, such as region-level LMMs (Cai et al., 2024a; Zhang et al., 2023c; Chen et al., 2023; Peng et al., 2023; Zhang et al., 2023b), 3D LMMs (Hong et al., 2023), and video LMMs (Lin et al., 2023; Zhang et al., 2023a; 2024b).\nMultimodal Understanding Benchmarks. The recent significant advancements have resulted in more versatile multimodal models, making it imperative to thoroughly and extensively evaluate their visual understanding and reasoning abilities. Conventional multimodal benchmarks like VQA (Antol et al., 2015), GQA (Hudson & Manning, 2019) and VizWiz (Gurari et al., 2018) have been revitalized and used for evaluating the general visual question answering performance for LMMs. Some other question answering benchmarks like TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021) and InfoVQA (Mathew et al., 2022) have also been employed to validate the text-oriented understanding. Recent studies have introduced a variety of new benchmarks, such as SEED-Bench (Li et al., 2023a), MMBench (Liu et al., 2023b) and MM-Vet (Yu et al., 2024b) for evaluating the models' integrated problem-solving capabilities, and MMMU (Yue et al., 2024a) and MathVista (Lu et al., 2024) for scientific and mathematical reasoning. In addition, the commonly known hallucination problem also appears in LMMs, and is also investigated in POPE (Li et al., 2023b), MMHal-Bench (Sun et al., 2023) and Object HalBench (Yu et al., 2024a), etc.\nVideo Understanding Benchmarks. Recently, an increasing amount of research is transitioning its focus from the image to the video domain. Videos differ from images in that they possess more complex content with temporal dynamics. This unique aspect calls for a different set of metrics and benchmarks. Many efforts have leveraged existing video question answering benchmarks (Xu et al., 2017; Yu et al., 2019b; Xiao et al., 2021) built on top of video-text datasets (Chen & Dolan, 2011; Xu et al., 2016; Zhang et al., 2019). More recently, several LMM-oriented benchmarks have been proposed for different aspects such as long-form egocentric understanding with EgoSchema (Mangalam et al., 2024), and temporal understanding and ordering like Tempcompass (Liu et al., 2024c). MV-Bench (Li et al., 2024b) compiles existing video annotations from different disciplines into a new benchmark, while Video-MME (Fu et al., 2024) and MMWorld (He et al., 2024b) claim to support a comprehensive evaluation of video understanding and world modeling, respectively. Our TemporalBench serves the common goal of evaluating models for video understanding but differs in several aspects. On the one hand, we exhaustively curate videos from different domains and ask human annotators to annotate the visual contents with as much detail as possible. On the other hand, we particularly focus on temporal dynamics such as human actions and human-object interactions that exist exclusively in videos and which are crucial for video understanding, reasoning and forecasting. While the ShareGPT4Video dataset (Chen et al., 2024) also contains long captions, theirs differ from ours by being entirely generated by GPT-40 instead of annotated by humans."}, {"title": "TemporalBench", "content": "Compared to static images, videos inherently contain significantly more fine-grained temporal information, as they capture the unfolding of actions and events over time. Existing multimodal video understanding benchmarks (Xu et al., 2016) mostly evaluate models' coarse-level understanding of videos. An example from the recent Seed-Bench dataset is the question, \"What action is happening in the video?\" with the answer, \u201cmoving something up.\u201d However, such types of coarse-level video questions have been demonstrated to be easily solved with just a single frame (Wu, 2024) or even by a text-only LLM (Tan et al., 2024; Mangalam et al., 2024).\nSuch phenomena arises due to a fundamental limitation in the text descriptions in those benchmarks. As a result of their coarseness, the positive and negative options for video question-answering can usually be distinguished without understanding the temporal dynamics, such as the models only needing to choose between \"The man is cooking\" and \"The man is exercising\"."}, {"title": "VIDEO COLLECTION", "content": "We collect video clips from a wide range of sources across diverse domains, where the majority comes from existing video grounding benchmarks. Our dataset includes a wide spectrum of video types from seven sources, including (1) procedure videos e.g., COIN (Tang et al., 2019), (2) human activities e.g., ActivityNet-Captions (Yu et al., 2019a) and Charades (Krishna et al., 2017), (3) ego-centric videos e.g., EgoExo4D (Grauman et al., 2024), (4) movie descriptions (Rohrbach et al., 2015), (5) professional gymnasium videos e.g., FineGym (Shao et al., 2020), and (6) unexpected humor videos Oops (Epstein et al., 2020). We sample around 300 video clips from the validation and test sets of each video dataset, which results in 2K videos. The statistics of TemporalBench is shown in Table 1.\nWe intentionally filter out video clips that (1) are mostly static by leveraging optical flow (Farneb\u00e4ck, 2003), (2) contain multiple scene transitions by leveraging PySceneDetect \u00b9 and (3) last longer than 20 seconds. We observe that the large amount of information in long videos make it difficult for annotators to provide detailed action descriptions. The distribution of video lengths is shown in Figure 4 (a). Additionally, we remove the audio from the videos during annotation to ensure that all informative signals come solely from the visual frames, preventing the answers from being influenced by the audio."}, {"title": "VIDEO CAPTION ANNOTATION PROCESS", "content": "Positive Captions Annotation. We employ a two-stage human labeling process for curating video captions with fine-grained activity descriptions, where the qualified Amazon Mechanical Turk (AMT) workers are first instructed to give a detailed video caption. Then, the authors of this work refine the caption by correcting the mistakes and adding missing details w.r.t. the actions. The overall pipeline is shown in Figure 2. All video clips are annotated following the same pipeline except for Finegym (Shao et al., 2020) as it has already provided accurate and detailed action descriptions for professional gymnasium videos. Consequently, we reuse its annotations.\nWe first use 3 probing video captioning questions with 2 in-context examples as the onboarding task for AMT master workers. We manually inspect the soundness and amount of temporal details of the AMT worker captions to select high quality AMT video captioning workers. During the annotation process by AMT workers, we also continue to remove the unqualified workers based on the ratio of the captions that authors in this paper refined. In this way, we ensure that the AMT provides a high quality initial point for positive captions.\nNegative Caption Annotation. Our negative captions are aimed at confusing multimodal video models with respect to fine-grained activity details, such as changing \"cut a ginger twice using a knife\" to \"cut a ginger three times using a knife\". We construct negatives upon two granularities: word level and event level. Specifically, word level negatives denote the case where a certain word or phrase is replaced while event level negatives denote the case where the order of two events are reversed. Empirically, we find that LLMs can produce more creative and diverse negatives compared to AMT workers and authors. Therefore, we leverage three leading LLMs, GPT-40 (OpenAI, 2024),"}, {"title": "A PITFALL IN MULTI-CHOICE QUESTION ANSWERING", "content": "A conventional approach to evaluate large multimodal models is using the multi-choice question-answering format, which is adopted by the majority of current benchmarks including MMMU (Yue et al., 2024a), MathVista (Lu et al., 2024), EgoSchema (Mangalam et al., 2024) etc. However, indicated by recent studies by (Cai et al., 2024b) and (Yue et al., 2024b), a pure LLM can achieve comparable or even stronger performance on those benchmarks without looking at the visual content at all. Recent studies argue that (1) some questions are not designed well so that the question can be answered without looking at the visual content, or (2) the model memorizes the QA pairs, i.e., data contamination occurs.\nWhile developing our benchmark, we notice another previously ignored but critical pitfall for multi-choice QA. Specifically, if every negative answer choice is generated by changing a small part of the correct answer, the LLM can detect those changes to find a \u201ccentralized\u201d description and use that cue for its prediction. To study this, given a positive caption $C$ and its associated negative caption $N(C')$, we intentionally derive a few negatives from $N_1(C)$ (instead of for $C$), resulting in $N_1(N_1(C))$ and $N_2(N_1(C))$, resulting in $[C, N_1(C), N_1(N_1(C)), N_2(N_1(C))]$ as options, so that $N_1(C)$ becomes the \"centralized\" description (see Fig. 5). Surprisingly, we find that 66.4% of text-only GPT-40's predictions correspond to $N(C')$, while only 6.4% of its predictions correspond to $C$. Our findings also align with human behavior analysis from psychology (Furman & Wang, 2008), where humans can achieve better than random chance performance on multi-choice QAs using similar cues.\nMotivated by this findings, we propose to decompose a single multi-choice QA into multiple binary QAs. In this case, we eliminate the \u201ccentralized option\u201d due to the fact that there are only two options to choose from. As a result, given $M$ negatives, the multiple binary QAs will query a model $M$ times, where the random chance performance changes from $\\frac{1}{M+1}$ to $(\\frac{1}{2})^M$. Given that $(\\frac{1}{2})^M > \\frac{1}{M+1}$ for every $M > 2$, multiple binary QA is a more difficult task than multi-choice QA."}, {"title": "EXPERIMENTS", "content": "EXPERIMENT SETUP\nWe evaluate both (1) multimodal video text generation models, including GPT-40 (OpenAI, 2024), Gemini-1.5-Pro (Gemini Team, 2024), Claude-3.5-Sonnet (Anthropic, 2024), Qwen2VL (Wang et al., 2024), LLaVA-OneVision (Li et al., 2024a), LLaVA-Next-Video (Zhang et al., 2024b), Phi-3.5-Vision (Abdin et al., 2024), MiniCPM-2.6 (Yao et al., 2024), MA-LMM (He et al., 2024a), VideoLLaVA (Lin et al., 2023), InternLM-Xcomposer-2.5 (Zhang et al., 2024a), Matryoshka Multimodal Models (M\u00b3) (Cai et al., 2024b), and (2) multimodal video embedding models, including XCLIP (Ni et al., 2022), ImageBind (Girdhar et al., 2023), and LanguageBind (Zhu et al., 2024a).\nWe exponentially increase the number of frames to study its effect on video understanding. More details can be found in Appendix D."}, {"title": "HUMAN PERFORMANCE", "content": "We use Amazon Mechanical Turk to evaluate human performance. Note that we exclude the positive caption annotators to ensure that there is no data contamination. Again, we use an onboarding test using a held out binary video QA evaluation set which has clear answers. Next, we show the performance on each task."}, {"title": "FINE-GRAINED VIDEO QUESTION ANSWERING ON SHORT VIDEOS", "content": "The results for multimodal generative models and embedding models are shown in Table 2 and Figure 7 (a). Note that we show the result with the best average multiple binary QA (MBA) performance for each model with respect to the number of frames. Results under different frames can be found in Appendix D. Several interesting findings arise:\nThe performance of any video model is far from human performance. As shown in the table, humans show an average performance of 67.9%, which is significantly higher than the best models, GPT-40 and Qwen2VL-72B, by ~30%. Therefore, there is a large gap between model's performance and human performance. Note that we are employing standard AMT workers instead of domain experts, meaning that the expert-level accuracy can be even higher, especially for professional video understanding like FineGym.\nModels show limited performance gains with more frames. As shown in Figure 6, with more frames, multimodal video models usually show better performance. However, performance generally saturates around 8-16 frames, meaning that models struggle to improve fine-grained activity understanding even with more frames. This is a clear contrast with human performance, showing that there is still a large space for multimodal video models to improve.\nMultiple Binary QA is a more challenging metric. Multiple Binary QA, as proposed in Section 3.3, prevents a model from exploiting cues in the answer choices, and evaluates whether a model truly understands the temporal dynamics in the video by splitting a single M + 1-way multiple choice question into M binary choice questions. For example, GPT-40 receives 75.7% accuracy but only 38.5% on multiple binary accuracy, showing a huge gap. These results indicate that understanding"}, {"title": "VIDEO CAPTIONING", "content": "Our detailed video captions also enables analyzing a model's fine-grained video captioning capabilities. For this, we prompt multimodal video models to generate a caption for an input video, with 3 captioning examples in the prompt as guidance to mimic the style of our detailed video captions. Note that we remove the FineGym captions due to its different structure compared to other video captions, resulting in 1891 samples. We evaluate the resulting video captioning performance using classical image captioning metrics, CIDEr (Vedantam et al., 2015), BLEU (Papineni et al., 2002) at different n-gram levels, ROUGE (Lin, 2004), as well as the embedding similarity with sentence transformer (Reimers & Gurevych, 2019) between the ground truth caption and the generated caption. Note that we for each model, we use the same number of frames as in Section 4.3.\nResults in Table 3 show that GPT-4o achieves the best performance. Interestingly, the results indicate that the embedding similarity aligns most closely with the video QA task results from Sec 4.3. Other classical captioning metrics show inconsistent results. For example, GPT-40 obtains similar performance with one compared to 64 frames on both CIDEr and BLEU scores (e.g., for BLEU_1 24.1 vs. 25.1). On the other hand, all models show similar ROUGE scores. Thus, for the zero-shot captioning task, our findings indicate that text embedding similarity may be the most reliable metric."}, {"title": "LONG VIDEO UNDERSTANDING", "content": "Since our benchmark is annotated at the video clip level, we can easily extend it to long video understanding by concatenating the captions of different video clips within the same original video. In our study, we choose video datasets from AcitivityNet, Charades, EgoExo4D, COIN and FineGym. We randomly sample video clips within the same original video, and then crop a new video segment whose starting time corresponds to that of the earliest sampled video clip and whose ending time corresponds to that of the latest sampled video clip. We then concatenate all the sampled video captions together to form a single long detailed description corresponding to the new video segment. Given this positive caption, we generate negative captions for it by replacing the positive caption of one of the sampled video clips with its negatives. The model is then tasked to choose the correct long caption out of multiple choices. We control the random chance multiple binary QA performance to be ~9.5%, resulting in an apple-to-apple comparsion with in Sec 4.3. In this way, we investigate"}, {"title": "WHY MULTIPLE BINARY QA INSTEAD OF MULTI-CHOICE QA?", "content": "As discussed in Section 3.3, in the standard multi-choice QA setting, if negatives are all slightly variations of the positive caption, we find that LLMs can determine the \u201ccentralized\" caption, and take a shortcut to achieve better performance. To demonstrate this, based on one negative caption N(C) in TemporalBench, we intentionally generate two negative captions derived from N(C) (instead of C), resulting in $N_1(N(C))$ and $N_2(N(C))$. Given two set of options [C, $N_1(C)$, $N_2(C)$), $N_3(C))$ and [C, $N_1(C)$, $N_1(N_1(C))$, $N_2(N_1(C))$] shown in Figure 5, text-only GPT-40 displays different behaviors. As shown in Table 5, under the intentionally designed negative options, GPT-40 will choose $N_1(C)$ under 66.4% cases. This again demonstrates the necessity and advantage of our multiple binary QA accuracy (MBA) metric design over the standard multi-choice QA setting."}, {"title": "PERFORMANCE ON CATEGORIES", "content": "Broadly, TemporalBench evaluates word level replacement and event level re-ordering. Here we further breakdown the word level replacement into following categories: (1). Action order (change the order); (2). Action frequency (1 times v.s. two times); (3). Action type (put v.s. pull); (4). Motion magnitude (slightly v.s. intensively); (5). Motion Direction/Orientation (forward v.s. backward, circular v.s. back-and-forth). (6). Action effector (cutting with left hand v.s. cutting with right hand) (7). Others. We prompt GPT-40 to perform 7-way classification and show the per-category performance in Table 7 and Figure 7 (b). Results indicate that multimodal video models shows better performance on \u201cothers\u201d category rather than the other categories related to actions. Among the seven categories, models struggle most on action frequency (counting), which show that they do not memorize repeated occurrences well. The visualizations of failture cases in GPT-40 is shown in Figure 8."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We propose TemporalBench, a novel video understanding benchmark, to evaluate the fine-grained temporal understanding abilities of multimodal video models. The video captions in our benchmark are significantly denser than existing datasets such as MSRVTT and TGIF, offering detailed temporal"}, {"title": "ETHICS STATEMENT", "content": "This research primarily utilizes publicly available video datasets, which have been collected and annotated by qualified annotators and authors, ensuring compliance with ethical standards. We have made every effort to ensure that the data used respects privacy and contains no personally identifiable information. Furthermore, we acknowledge the potential implications of fine-grained video understanding, especially in sensitive applications such as surveillance and autonomous systems. As such, we advocate for responsible and ethical use of this research, urging caution in deploying these models in real-world scenarios to avoid harmful or unintended consequences."}, {"title": "BROADER IMPACT", "content": "TemporalBench, a comprehensive benchmark for video understanding, has the potential to significantly advance research in this field by offering improved metrics for model evaluation. Our work aims to enhance the temporal reasoning capabilities of future video understanding models. However, the broader impact of more advanced video understanding technologies raises important societal concerns, including the risk of mass surveillance, privacy violations, and the development of harmful applications like autonomous weapons. Therefore, we strongly encourage thoughtful consideration when deploying these models in real-world scenarios to mitigate negative or unintended consequences."}, {"title": "MORE VISUALIZATIONS OF OUR BENCHMARK", "content": "In this section, we present comprehensive visualizations of our fine-grained annotations with both positive and negative descriptions. For each benchmark mentioned in Table 1, we provide one video example with its positive annotation and one of the corresponding negative descriptions (there are more than one negative for a single video in our dataset) in Figures 9 & 10. The video examples (a - f) are displayed in the same order as their sources in Table 1 (7 in total)."}, {"title": "PER SUBSET RESULTS FOR SHORT AND LONG VIDEO QA UNDER BINARY ACCURACY (BA)", "content": "The per subset results (denoted as \"T-\") for short and long video QA under Binary Accuracy (BA) are shown in Table 8, and Table 9, respectively. Still, human achieve much better performance than all multimodal videos. Interestingly, both human and Finegym, the professional subset,"}, {"title": "MORE RESULTS WITH EXTENDED FRAMES", "content": "In the main paper, we only report the performance of each multimodal video models with the the number of frams that leads to the best performance. Here we extend the results to show the results of more frames in Table 10."}, {"title": "DATA ANNOTATION PLATFORM", "content": "Positive Captions We use Amazon Mechanical Turk (AMT) 2 for positive caption annotation, and then use Label Studio 3 to let authors refine the caption. As shown in Figure 11, authors can edit the caption from AMT workers. Also, we provide the original short video captions to let people better understand our task.\nNegative Captions We first prompt LLMs (GPT-40, Gemini, and Llama-3.1-405b) to get initial negative captions, and then ask authors to choose the negatives that can reflect the temporal dynamic. The visualization of the multi-choice platform in shown in Figure 12."}]}