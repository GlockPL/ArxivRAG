{"title": "Transformers Utilization in Chart Understanding:\nA Review of Recent Advances & Future Trends", "authors": ["Mirna Al-Shetairy", "Hanan Hindy", "Dina Khattab", "Mostafa M. Aref"], "abstract": "In recent years, interest in vision-language tasks has grown, especially those\ninvolving chart interactions. These tasks are inherently multimodal, requiring\nmodels to process chart images, accompanying text, underlying data tables, and\noften user queries. Traditionally, Chart Understanding (CU) relied on heuris-\ntics and rule-based systems. However, recent advancements that have integrated\ntransformer architectures significantly improved performance. This paper reviews\nprominent research in CU, focusing on State-of-The-Art (SoTA) frameworks that\nemploy transformers within End-to-End (E2E) solutions. Relevant benchmarking\ndatasets and evaluation techniques are analyzed. Additionally, this article iden-\ntifies key challenges and outlines promising future directions for advancing CU\nsolutions. Following the PRISMA guidelines, a comprehensive literature search\nhas been conducted across Google Scholar, focusing on publications from January\n2020 to June 2024. After rigorous screening and quality assessment, 32 studies\nhave been selected for in-depth analysis. The CU tasks are categorized into a\nthree-layered paradigm based on the cognitive task required. Recent advance-\nments in the frameworks addressing various CU tasks are also reviewed. These\nframeworks are categorized into single-task or multi-task based on the number\nof tasks solvable by the E2E solution. Within multi-task frameworks, pre-trained\nand prompt-engineering-based techniques are explored. Furthermore, this review\nprovides an overview of leading architectures, datasets, and pre-training tasks.\nDespite significant progress, challenges remain in OCR dependency, handling\nlow-resolution images, and enhancing visual reasoning. Future directions include\naddressing these challenges, developing robust benchmarks, and optimizing model\nefficiency. Additionally, integrating explainable AI techniques and exploring the\nbalance between real and synthetic data are crucial for advancing CU research.", "sections": [{"title": "1 Introduction", "content": "Visual representations are powerful tools for conveying information in a compact and\nunderstandable way. Data visualization as a discipline encompasses a wide variety of\ntools used in conveying ideas. Any visual figure could be categorized into either an\nimage, a diagram or a chart [1]. Charts are a well-known visualization tool across dif-\nferent domains. A chart image is defined as a visual representation of data that helps\nto summarize its underlying data, reflect insights, and draw conclusions. However,\nchart images on their own can be ambiguous to the reader and therefore require addi-\ntional context to be able to extract meaningful information from them. Furthermore,\nthe predominant storage format for chart images on the web is the bitmap format,\nwhich poses accessibility challenges for visually impaired users [2, 3] The issue with\nthe raw bitmap format is that screen readers cannot parse an image if it lacks an alter-\nnative text (alt-text) description, which was reported as a common case according to\na report made by WebAIM\u00b9 with 21.6% of web page's images missing alt-text.\nWith the rise of visual language processing, the automation of several chart-focused\ntasks has witnessed an increased interest, which recently appeared in the literature\nas Chart Understanding (CU) [1, 4] (refer to Figure 1a). These tasks include but are\nnot limited to Chart Question Answering (CQA), Chart Captioning, and Chart-to-\nText Summarization. CU tasks often involve extracting the underlying data from a\ngiven chart, such as de-rendering the image into code or its data table. It is considered\na subdomain of Visual Language Processing that combines different modalities to\nretrieve information and build knowledge from chart images (as shown in Figure 2).\nAutomating a CU task could involve the usage of a chart image, its data table and/or\na user prompt for said task. Similarly, the target of the task could result in any of the\npreviously mentioned modalities. For example, one could potentially interact with a\nline chart in order to gain a better understanding of its insights. In such cases, the\nsystem can result in a modified chart image highlighting the abrupt changes along\nwith a text summarizing the overall line trends.\nThroughout the literature, the emerging discipline of CU could be seen at the\nheart of four main disciplines, namely, data visualization, Computer Vision (CV),\nNatural Language Processing (NLP) and information retrieval (refer to Figure 3). A\nprime example of this intersection is the CQA task. In CQA, a user poses a question\n(NLP) about a visual element (data visualization and CV) within a chart. To provide\na comprehensive answer, the system must not only accurately interpret the visual"}, {"title": "1.1 Related Surveys", "content": "This section provides an overview of the latest review articles focusing on chart\nunderstanding techniques. A taxonomy of seven data visualization techniques is intro-\nduced by [6] along with platforms and tools that help produce those visualizations.\nBuilding upon existing research in data visualization and automated comprehension,\nnumerous surveys have explored various aspects of visualization identification and\nunderstanding tasks. These surveys delve into tasks such as chart recognition and clas-\nsification [7], data extraction [2], reconstruction or generation of stylized charts [8],\nand question answering or caption generation [1, 3, 9, 10]. The works of [11-15] review\nadvancements of natural language interfaces focusing on visualization generation and\nrecommendation."}, {"title": "1.2 Survey Methodology", "content": "A systematic search strategy has been followed while building up this research\nregarding the identification of the relevant literature. A preliminary search through\nGoogle Scholar was made using the keywords: \"transformer\", and \"chart question\nanswering\". Furthermore, the resulting survey articles have been investigated to\nexpand our list of keywords. Through an iterative refinement process, we arrived\nat the final search string of: \"Transformer (\"Chart Derendering\" |\" Chart question\nanswering\" |\"Chart Understanding\" |\"Chart captioning\" |\" Chart Reasoning\")\". This\nprocess involved exploring various combinations of keywords and Boolean operators to\nensure comprehensive coverage of relevant literature while minimizing retrieval of irrel-\nevant studies. All searches spanned the period from January 2020 until June 2024 and\nincluded journal articles, conference papers, and review papers published in English\nonly arriving at a collection of 227 articles.\nThe selection criteria were based on the PRISMA Statement [19]. The inclusion\nand exclusion criteria for this study are outlined as follows."}, {"title": "Inclusion Criteria", "content": "1. Focus on frameworks utilizing at least one transformer model in their pipeline.\n2. Focus on the field of computer science.\n3. Publication within the period from 2020 to June 2024."}, {"title": "Exclusion Criteria", "content": "1. Published before 2020.\n2. Solely focusing on data narration through visualization.\n3. Solely focusing on benchmarking commercial Large Language Models (LLMs),\nVision-Language Models (VLMs), and Multi-modal Large Language Models\n(MLLMs).\n4. Solely focusing on the generation of chart images from textual prompts.\n5. Solely focusing on visualization recommendation.\nThis study is based only on original research articles, review papers and conference\npapers. To maintain the quality of the review, all duplications were checked thoroughly.\nAbstracts of the articles were checked deeply for the analysis of the articles to ensure\nthe quality and relevance of academic literature included in the review process. The\ninitial screening phase resulted in discarding 129 articles keeping 98 articles only. A\ncareful evaluation of each research paper was carried out according to the mentioned\ncriterion. After assessing each article on the aforementioned inclusion and exclusion\ncriteria, The final selection approached 32 articles."}, {"title": "1.3 Contributions", "content": "This work presents the following key contributions to the field of CU:\n\u2022 Bridging the knowledge gap for early-career researchers and practitioners in CU\nby providing a comprehensive survey of key pre-trained transformer models.\n\u2022 Structuring the CU domain taxonomy into three distinct layers: perception, com-\nprehension, and reasoning. This taxonomy is a refinement of existing taxonomies\nby placing established tasks within these layers and introducing a new task, \"chart\nderendering,\" resulting in a total of seven CU tasks.\n\u2022 Summarizing and evaluating the current benchmarks and datasets employed in\nthe development of CU frameworks.\n\u2022 Categorizing CU frameworks based on the scope of CU tasks addressed, distin-\nguishing between frameworks that handle single or multiple tasks.\n\u2022 Conducting a comparative analysis between the reviewed frameworks on the\nbenchmarking datasets available.\n\u2022 Discussing the current challenges, potential applications, and future research\ndirections necessary to advance the field of CU."}, {"title": "1.4 Paper Organization", "content": "This paper is organized into eight sections, providing a comprehensive overview of CU\nresearch using transformer models. Section 2 establishes key pre-trained transformer\narchitectures observed to be utilized in multiple CU settings. It covers advancements in\ntransformer models relevant to CU tasks. Section 3 introduces a taxonomy of the fun-\ndamental CU tasks including CQA, Chart Derendering, and Chart-to-Text detailing\nthe characteristics of each task. Section 4 describes the datasets utilized across various\nCU tasks and the available benchmarks. Section 5 reviews frameworks and how the\nCU tasks developed over time. Section 6 outlines the metrics used to assess the qual-\nity of the developed frameworks and provides a comparative analysis of the reviewed\nframeworks. Section 7 highlights the current challenges that affect CU frameworks\nand proposes potential directions for improvement, applications and future research.\nFinally, Section 8 concludes the review."}, {"title": "2 Background", "content": "This section provides an overview of the advancements made in transformer models\nfrequently utilized in the CU domain across different modalities including textual,\nvisual and tabular modalities.\nTextual Modality. The original transformer model architecture (i.e. transformer-\nou), introduced by [5], has seen numerous extensions across various domains, among\nwhich is CU. The architecture leverages the attention mechanism which enables mod-\nels to learn dependencies and contextual relationships within input representations,\nbeing text sequences, image patches, or multimodal representation. As illustrated in\nFigure 4, the transformer comprises encoder and decoder blocks commonly employed\nin natural language processing tasks. The encoder processes input sequences, gener-\nating embeddings, while the decoder generates output sequences conditioned on both\nthe encoder's embeddings and its own input. The self-attention mechanism, a key\ninnovation, empowers the model to focus on relevant input components, facilitating\nthe learning of long-range dependencies. In essence, the encoder maps input data to\na latent space, while the decoder generates new content based on this latent repre-\nsentation. Encoder and decoder blocks can incorporate multiple attention heads for\nspecialized tasks such as classification or entity recognition.\nBART [20] and T5 [21] are two influential sequence-to-sequence extensions of the\ntransformer-ov that are utilized in summarization, QA, and comprehension tasks.\nThey vary across six dimensions: (1) their pretraining objective, (2) the used activation\nfunctions, (3) their parameter initialization, (4) the pretraining corpus, (5) how the\npositional embedding is encoded, and (6) their tokenizers. Changes in these dimensions\ncan be applied to other architectures as well.\nVisual Modality. The adoption of the transformer-ov for vision tasks broad-\nened the scope of its applicability with the introduction of the concept of patching\nimages and the Vision Transformer (ViT) [22]. Once an image is divided into a grid of\npatches, each patch is then represented as a sequence of pixels, and the transformer\nis used to learn the relationship between these patches. Hence, by applying ViT or\none of its successors to a chart image, we can obtain effective representation and\nembeddings of the chart components. As shown in Figure 5, the ViT model utilizes\nan encoder block with its input sequence being a linear projection of the flattened\npatches. In contrast to the ViT [22] architecture, which generates single-resolution\nfeature maps and exhibits quadratic computational complexity with respect to image\nsize, the Swin Transformer [23] introduces a hierarchical feature map structure and"}, {"title": "3 Chart Understanding", "content": "Building upon the advancements in transformer architectures across diverse modali-\nties, this section outlines the CU domain, encompassing its core tasks and applications."}, {"title": "3.1 Key Concepts", "content": "Definition 1 (Chart Images). Images that serve as versatile visual representations\nof data, finding application across diverse domains. Within academic and research\ncontexts, they facilitate data dissemination and interpretation. In the business and\nindustrial spheres, chart images are instrumental in data-driven decision-making and\nstrategic planning. Additionally, they contribute to knowledge transfer in education,\ninform public discourse through media, and support governmental policy formulation.\nDefinition 2 (Optical Character Recognition (OCR)). A prevalent component in CU\nframeworks that enables the extraction of textual content and corresponding spatial"}, {"title": "3.2 Layer 1: Perception", "content": "Chart perception constitutes the initial stage of human-chart interaction, focusing\non the immediate visual interpretation of chart elements. This foundational process"}, {"title": "3.3 Layer 2: Comprehension", "content": "Chart comprehension constitutes the subsequent phase in chart understanding, build-\ning upon the foundational processes of perception. This stage involves the cognitive\nintegration of visually extracted chart elements to derive semantic meaning. Core tasks\nwithin chart comprehension encompass the extraction of textual and numerical data,\nthe classification of chart types, the alignment of quantities, and the reconstruction\nof the underlying data structure through chart derendering. These processes collec-\ntively facilitate a deeper understanding of the chart's underlying information and its\nrepresentation."}, {"title": "3.3.1 Text Role Classification", "content": "Chart text role classification is a task that involves identifying and classifying the\nsemantic roles of textual elements within scientific charts. The goal is to determine\nthe function or purpose of different text components, such as titles, labels, and anno-\ntations, within the context of the chart. In the context of scientific charts, text role\nclassification aims to categorize text into specific roles, which may include, as pre-\nsented in [65, 66]: (1) Chart Title: The main title of the chart that describes what the\nchart represents; (2) Legend Title: The title of the legend that explains the symbols\nor colours used in the chart; (3) Legend Label: The labels that correspond to the dif-\nferent elements represented in the legend; (4) Axis Title: The titles for the axes that\nindicate what data is being represented; (5) Tick Label: The labels on the axes that\ndenote specific values or categories; (6) Tick Grouping: Text that groups tick labels\ntogether for clarity; (7) Mark Label: Text that differentiates between various marks\nor data points on the chart; (8) Value Label: Text that displays quantitative data for\nspecific points or areas on the chart; (9) Other: Any other text that does not fit into\nthe above categories. This task enhances the readability and interpretability of scien-\ntific charts, as it helps ensure that all necessary elements are present and correctly\nlabelled. Effective text role classification can also support automated systems that\nprovide feedback to authors on the completeness and clarity of their charts [30]."}, {"title": "3.3.2 Chart Data Extraction", "content": "Chart data extraction constitutes the subsequent phase in chart understanding, focus-\ning exclusively on the semantic interpretation of detected chart markers. This process\ninvolves the precise identification and extraction of numerical or categorical data val-\nues associated with these markers. Importantly, chart data extraction is bound to the\nextraction of raw data points, excluding the generation of structured data formats\n[2-4]. For instance, chart data extraction enables the identification of axis ranges, the\nquantitative values of bar segments or pie slices, among other data elements."}, {"title": "3.3.3 Chart Type Classification", "content": "Chart-type classification is a foundational task in chart understanding, analogous to\nimage classification in computer vision [7]. It involves the categorization of a chart\nimage into predefined classes based on its visual composition. This process necessitates\na comprehensive analysis of chart elements, including axes, labels, data points, and\ntheir spatial relationships [10]."}, {"title": "3.3.4 Quantity Alignment", "content": "The innovative research by [41] introduces a unique challenge of understanding numeri-\ncal information presented in various formats\u2014text, tables, and charts. TTC-QuAli [41]\nis a groundbreaking resource for linking related quantities across these modalities by\nintroducing a quantity alignment task. Quantity alignment can be considered primar-\nily a comprehension task over charts, as it involves understanding and interpreting\nthe relationships between textual descriptions, tables, and visual elements in charts.\nTwo quantity alignment settings are introduced: single (one-to-one) and composite\n(one-to-many). In single alignment, each quantity mentioned in the text is linked to\na single corresponding cell in a table or a single element in a chart. The focus is on\nestablishing a direct relationship between one specific quantity and one specific tar-\nget, ensuring a clear and straightforward mapping. For example, if a sentence states,\n\"The sales in Q1 were 100 units,\" it would link the quantity \"100\" to a specific cell in\na table that represents Q1 sales. Conversely, composite alignment involves linking a\nsingle quantity in the text to multiple cells in a table or multiple elements in a chart.\nIt can include scenarios where a quantity may require aggregation (e.g., summing val-\nues from multiple cells) or where multiple related quantities are involved in a single\nstatement. For instance, if a sentence states, \"The total sales for Q1 and Q2 were 250\nunits,\" it would link the quantity \"250\" to multiple cells representing sales for both\nQ1 and Q2, potentially requiring the model to sum those values."}, {"title": "3.3.5 Chart Derendering", "content": "The term \"Chart Derendering\" was first introduced by [38], with MatCha being the\npioneering framework in this area. MatCha aims to not only recover the underlying\ndata table of a chart but also the code used to generate it. To advance chart under-\nstanding and enable models to better comprehend chart layout and visual attributes,\nchart derendering can be categorized into two primary tasks: transforming the chart\nimage into a structured textual or visual format, or reconstructing the original code.\nStructure Extraction. This task focuses on extracting the underlying data from\na chart image and representing it in a structured format. This format can include a\ndictionary, a table, a Vega-Lite JSON specification or a saliency map of the chart's\nlayout. Typically, this process involves combining computer vision techniques with\nheuristics to extract visual features, which are then incorporated into a transformer\nmodel [47, 50]. Recent approaches utilize transformer-based chart encoders to learn\nfrom visual embeddings [31, 32, 35-37]."}, {"title": "3.4 Layer 3: Reasoning", "content": "Chart reasoning is the cognitive process of deriving new insights from visual data\npresented in a chart. Building upon the foundational tasks of chart perception and\ndata extraction, chart reasoning involves inferring relationships, trends, and patterns\nwithin the data. Common chart reasoning tasks include question answering and the\ngeneration of textual summaries, requiring models to interpret the chart's underlying\nmeaning and communicate it effectively [1]."}, {"title": "3.4.1 Chart Question Answering", "content": "CQA constitutes a specialized domain within the broader Visual Question Answer-\ning (VQA) paradigm. While VQA typically involves answering questions based on\narbitrary images, CQA focuses on answering questions pertaining to chart visual-\nizations [9]. Distinguishing CQA from general VQA is a requirement for advanced\nreasoning capabilities, often necessitating mathematical operations to derive accurate\nresponses. To emulate human-like interaction with charts, CQA research has explored\nmethods for machines to comprehend and reason over chart representations. Early\nworks in this field predominantly relied on heuristic approaches [67, 68], while present-day research leverages transformer architectures, reflecting State-of-The-Art (SOTA)\nadvancements [43, 47, 48]."}, {"title": "3.4.2 Chart-to-Text", "content": "Chart-to-text generation surpasses the simple conversion of visual data into textual\nformat. The objective is to produce natural language descriptions or summaries that\naccurately encapsulate the chart's content. This process enhances accessibility by pro-\nviding alternative text formats for chart images. Generated text can be categorized\nas either captions or summaries, each serving distinct communicative purposes. The\nemphasis lies on creating concise yet informative textual representations. Aligning\nwith [71] hierarchical taxonomy of natural language descriptions, captioning pri-\nmarily focuses on describing visual components and their arrangement. In contrast,\nsummarization encompasses higher-level semantic interpretations, including abstract\nstatistical concepts, perceptual insights, and contextual knowledge.\nCaptioning (i.e., L1 [71]). Often synonymous with generating detailed chart\ndescriptions, this category primarily focuses on extractive text generation. This task\ndemands a model's ability to accurately describe the chart's visual layout and con-\nstituent elements, without necessitating complex reasoning. Early approaches to chart\ncaptioning relied on template-based methods [72, 73]. In contrast, more recent frame-\nworks utilized transformer-based architectures to generate more sophisticated and\ninformative captions [34, 42]."}, {"title": "4 Datasets", "content": "The rapid progress in CU is driven by the development of datasets offering multi-\nmodal representations of charts and their data. Building high-quality datasets is just\nas important as improving the design of CU architectures. This section will compare\nand review relevant datasets used for CU research.\nChart images come in a variety of shapes and visualize different use cases. Accord-\ning to a study done by Datylon, there are over 60 chart types that exist at the time of\nsubmission of this publication. Another important aspect of the chart image is its stor-\nage format, which can be in bitmap (.jpeg, .png), Scalable Vector Graphics (SVG),\nor defined by a tool (e.g., .json using Vega-Lite). SVG and Vega-Lite formats facil-\nitate accurate reconstruction of the data table as the information is encoded within\nthe image file [33, 47]. In contrast, bitmap formats necessitate specific data extraction\npipelines.\nSome datasets are equipped with a textual corpus that provides question-answer\npairs and/or chart summaries. Additionally, some datasets include annotations of the\nchart elements in the format of bounding boxes. Finally, the ground truth data table\ncould also be included in (.csv) format. Table 3 provides an overview of the reviewed\ndatasets in this domain, highlighting their common characteristics and suitability for\nvarious CU tasks. The listed datasets in this section are ordered chronologically."}, {"title": "4.1 FigureQA", "content": "The idea behind the creation of FigureQA [70] is to define a CQA corpus that presents\nCU-specific challenges in addition to VQA challenges.\nDataset acquisition. The dataset consists entirely of synthetic data. Both the data\ntables and the images are synthetically generated. Data tables are sampled from pre-\ndefined numerical ranges, while images are created through a parameterized process\nthat determines figure type, color scheme, number of elements, and the shapes and\nquantity of data points. The images are plotted using Bokeh 5 open-source plotting\nlibrary. The QA corpus is created from a template-based generation process, with 15\npredefined question templates focusing on (yes/no) answers.\nDataset details. FigureQA offers train/dev/test subsets across five chart types. It\nincludes a total of 140,000 images with over 1.55 million questions."}, {"title": "4.2 DVQA", "content": "To define the challenges of CQA questions, [63] introduced DVQA, a dataset exploring\nCU-specific question types that extend beyond yes/no questions. DVQA is another\nsynthetically generated dataset focusing on bar charts.\nDataset acquisition. Focusing on bar charts, three types of underlying data are\ndefined: linear, percentage and exponential. Values for each type are sampled based on\nheuristics. Matplotlib is then used to generate visualizations with stylistic variations.\nFollowing a template-based generation, the QA corpus contains three question types:\nstructure understanding, data retrieval, and reasoning questions. Structure under-\nstating questions assess the system's capabilities of understanding the overall layout,\nsuch as the number of bars present, or their orientation (vertical or horizontal). Data\nretrieval questions focus on the system's capability of parsing the data values of chart\ncomponents. Finally, reasoning questions assess the ability to infer new information\nby performing mathematical operations across multiple chart components.\nDataset details. The DVQA dataset consists of 300,000 images and over 3.4 million\nquestions. It provides pre-defined train/dev/test splits for evaluation. Interestingly,\nthere are only 1,576 unique answers across all the questions. This limited vocabu-\nlary size within the DVQA dataset necessitates the characterization of its answers\nbeing \"fixed vocabulary\". Across the three question types, there are 1.6 million\nreasoning questions, 1.1 million data retrieval questions, and 0.47 million structure\nunderstanding questions."}, {"title": "4.3 Beagle", "content": "Focusing on crawling chart images in the SVG format, [74] created a dataset of five\ncollections. Each collection relates to the library/database with which the images\nwere created. Although the original corpus was intended to be used in a chart-type\nclassification task, it can still be utilized in chart derendering tasks due to the rich\ninformation provided in the SVG format.\nDataset acquisition. The authors crawled 20 million web pages with the target\nof finding SVG-based images. They then extracted a screenshot of the visualization\nfrom the web page along with its raw SVG specification. Five visualization tools or\ndatabases dominated the crawled SVG specifications. These were then used to iden-\ntify the different collections offered by the Beagle corpus, namely into D3, Plotly,\nChartblocks, Fusion Charts, and Graphiq collections.\nDataset details. The Beagle corpus offers over 42,000 SVG-based visualizations.\nThe largest collection is Chartblocks as it comprises 52.4% of the data, followed by\nthe Plotly collection which comprises 35.7%. The five collections offer a variety of\nchart types with D3 offering up to 22 different types. However, the line and bar chart\ntypes dominate the five collections, while pie charts are rarely observed and comprise\nonly 5% of the collected collections."}, {"title": "4.4 PlotQA", "content": "To address the limitation of FigureQA and DVQA being entirely synthetic datasets,\nPlotQA [75] leverages real-world data sources. Unlike DVQA, PlotQA introduces more\nquestions that require \"open vocabulary\" answers.\nDataset acquisition. The acquisition of the dataset followed a four-stage framework.\nFirstly, online data sources were crawled to curate various data tables. Using real-\nworld data, synthetic plots were generated, although the tool of generation was not\ndisclosed in the original paper. Sample questions were then collected for a subset of\n1,400 plots from Amazon Mechanical Turk (AMT)6 workers. Each worker was tasked\nwith writing complex reasoning questions for 5 plots. Finally, after manual analysis of\nthe collected questions, the authors defined 74 question templates. The question types\nwere categorized similarly to those in DVQA.\nDataset details. PlotQA offers three chart types and includes 224,377 images with\n28.9 million question-answer pairs. Notably, 80.76% of these questions require \"open\nvocabulary\" responses, resulting in a dataset with 5.7 million unique answers."}, {"title": "4.5 FigCAP", "content": "For addressing the task of chart captioning (Section 3.4.2), FigCAP [72] offers two\nlevels of caption details (FigCAP-H and FigCAP-D). FigCAP-H provides high-level\nchart captions, while FigCAP-D provides detailed captions.\nDataset acquisition. Following the generation framework of FigureQA, this dataset\ncontains chart-caption pairs. Ten caption templates are used for generating the cap-\ntions. The images encompass five chart types: horizontal bar charts, vertical bar charts,\nline plots, dotted line plots, and pie charts.\nDataset details. The total number of chart-caption pairs is 210,024, evenly divided\nacross FigCAP-H and FigCAP-D."}, {"title": "4.6 ExcelChart400K (EC400K)", "content": "Since many CU tasks rely on the ability to derender chart images, large datasets\nthat map these images to their corresponding ground truth tables are essential.\nExcelChart400K [76] offers such a dataset.\nDataset acquisition. The process begins by crawling publicly available Excel sheets\ncontaining plots. Next, Excel APIs are employed to reconstruct the underlying data\ntables. However, as the textual chart elements might contain sensitive information, an\nanonymization process is applied to ensure privacy protection.\nDataset details. The dataset is entirely composed of real-world data and encom-\npasses three chart types: bar, line, and pie charts. It contains 386,966 images with\ntheir corresponding data tables. Bar charts make up the largest portion of the dataset\nat 48.4%, followed by line charts at 31.8%, and pie charts at 19.8%."}, {"title": "4.7 Chart-to-Text", "content": "Shifting from template-based caption generation, Chart-to-Text [53] offers two real-world datasets: Statista and Pew.\n\u2022 Statista\nDataset acquisition. The dataset is collected from (statista.com), a website hosting\nvarious statistics across diverse topics. A total of 34,811 charts were crawled, along with\ntheir data table and human-written descriptions. Leveraging the data table structures,\nthe charts were classified into two categories: simple (with two columns) and complex\n(with three or more columns). Annotating the charts with their summaries and data\ntables was a straightforward process, as both elements were readily available. A portion\nof 20% of the charts had missing information that the authors manually annotated.\nDataset details. Three chart types appear in the dataset: Bar, Line, and Pie. Of\nthe total 34,811 charts, 27,869 are categorized as simple, while the remaining 6,942 as\ncomplex. Bar charts make up the largest portion of the dataset at 87.9%, followed by\nline charts at 10.2%.\n\u2022 Pew\nDataset acquisition. Pew Research (pewresearch.org) is a website that hosts data-\ndriven articles exploring public opinion on various social issues and demographic\ntrends. The dataset curation included a collection of 3,999 articles, resulting in a cor-\npus of 9,285 chart images. Notably, individual articles may contain multiple chart\ninstances, however, only 143 images had an accompanying data table. In addition to\nthe scarcity of data tables, annotating charts with their summaries was more chal-\nlenging as paragraphs in the article do not directly refer to the relevant charts. Thus,\nthe annotation was conducted in a three-stage process. First, an OCR was employed\nin conjunction with a chart-component element classifier to recreate the data tables.\nNext, a set of heuristics identified a subset of potentially relevant paragraphs that\nmight contain candidate summaries. These heuristics classified the relevance of each\ncandidate paragraph as either \"relevant\" or \"ambiguous.\" To address the ambigu-\nous cases, AMT workers were tasked with assessing the relevance level of the selected\nparagraphs.\nDataset details. Five chart types appear in the dataset: Bar, Line, Pie, Area, and\nScatter plots. Of the total dataset, 1,486 instances represent simple chart structures,\nwhile the remaining 7,799 are more complex. Bar charts make up the largest portion\nof the dataset at 67.9%, followed by line charts at 26.4%."}, {"title": "4.8 ChartQA", "content": "The ChartQA [47] dataset, followed the same methodology as PlotQA by utilizing\nonline sources for real-world data tables, yet their generated charts remained synthetic.\nDataset acquisition. To guarantee a broad spectrum of chart topics and styles,\nfour different sources were crawled, i.e., Statista (statista.com), Pew Research\n(pewresearch.org), OWID (ourworldindata.org), and OECD (oecd.org). To address\nthe issue of template-based questions, question-answer pairs were collected through\ntwo processes. First, human authors created question-answer pairs. Second, machine-\ngenerated question-answer pairs were created from human-authored summaries using\na pre-trained T5 model (Raffel et al., 2019). This resulted in two subsets: ChartQA-H\nand ChartQA-M. The focus was on complex reasoning questions. Complex reason-\ning questions are questions that require different arithmetic and logical operations to\nreach an answer or may require analysis of different visual attributes of the chart to\nretrieve the desired values.\nDataset details. ChartQA dataset is a large-scale collection of real-world charts\nand human-authored questions. It includes 9.6K human-authored questions over 4.8K\ncharts, and 23.1K questions generated from human-authored summaries over 17.1K\ncharts."}, {"title": "4.9 OpenCQA", "content": "OpenCQA [52] introduces a new challenge in the CQA task, in which it prompts\nChatGPT-like answers.\nDataset acquisition. The dataset curates 7.7K humanly authored questions while\nutilizing real-world charts collected from Pew Research (pewresearch.org). The ques-\ntion types are categorized into four categories, the identify type is essentially a\ndata-retrieval from the chart, compare are questions that require analysis over two\nor more data items or attributes, summarize are statistical analysis questions, and\ndiscover are reasoning questions that require drawing insights.\nDataset details. Five chart types appear in the dataset: Bar, Line, Pie, Area, and\nScatter plots. Regarding the distribution of question-types, 100 randomly selected\nquestions were analyzed. It was observed that people frequently engage in tasks\nrequiring the identify and compare of specific chart sections to answer questions."}, {"title": "4.1"}]}