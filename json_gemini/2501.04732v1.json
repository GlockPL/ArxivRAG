{"title": "SNR-EQ-JSCC: Joint Source-Channel Coding with SNR-Based Embedding and Query", "authors": ["Hongwei Zhang", "Meixia Tao"], "abstract": "Coping with the impact of dynamic channels is a critical issue in joint source-channel coding (JSCC)-based semantic communication systems. In this paper, we propose a lightweight channel-adaptive semantic coding architecture called SNR-EQ-JSCC. It is built upon the generic Transformer model and achieves channel adaptation (CA) by Embedding the signal-to-noise ratio (SNR) into the attention blocks and dynamically adjusting attention scores through channel-adaptive Queries. Meanwhile, penalty terms are introduced in the loss function to stabilize the training process. Considering that instantaneous SNR feedback may be imperfect, we propose an alternative method that uses only the average SNR, which requires no retraining of SNR-EQ-JSCC. Simulation results conducted on image transmission demonstrate that the proposed SNR-EQ-JSCC outperforms the state-of-the-art SwinJSCC in peak signal-to-noise ratio (PSNR) and perception metrics while only requiring 0.05% of the storage overhead and 6.38% of the computational complexity for CA. Moreover, the channel-adaptive query method demonstrates significant improvements in perception metrics. When instantaneous SNR feedback is imperfect, SNR-EQ-JSCC using only the average SNR still surpasses baseline schemes.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic communication is a promising paradigm for future wireless communications. Leveraging neural network (NN)-enabled joint source-channel coding (JSCC) techniques, semantic communication can efficiently transmit the meaning of source data (i.e., semantic information). It has demonstrated significant potential to surpass conventional communications in both reliability and transmission efficiency.\nAs an early work on JSCC, a convolutional neural network (CNN)-based architecture called DeepJSCC is proposed in [1], which has demonstrated superior image reconstruction performance over traditional digital communications, especially at low signal-to-noise ratios (SNRs). However, CNNs compress all elements indiscriminately, leading to a limited ability to extract semantic information. To address this issue, Transformer-based semantic coding methods have been proposed in [2]\u2013[4]. Unlike convolutional layers that can only capture local information due to their fixed-sized convolutional kernels, the attention mechanism in the Transformer allows the semantic encoder to focus on extracting multi-scale semantic information from source data. Consequently, Transformer-based semantic coding approaches consistently outperformed CNN-based methods.\nTo achieve JSCC, semantic communications need to cope with the impact of dynamic channels through channel adaptation (CA). One approach is to adaptively adjust the encoder\u2019s output based on feedback from the channel output [5]. However, the overhead associated with this feedback is excessively high. Other methods depend on feedback channel state information to achieve CA, which can be categorized into three types. The first type embeds the SNR before compression [6], [7], though the SNR information may be lost during the lossy compression process. The second type refines feature vectors after data compression [2]. However, this kind of approach may lead to undesired information loss during the non-channel-adaptive compression process. The third type achieves CA during the compression process through the channel attention mechanism [8]\u2013[10]. These methods enhance the utilization of the SNR information by embedding it in each channel attention layer. Most importantly, these existing methods achieve CA with unaffordable costs of storage overhead and computational complexity.\nTo this end, we propose a novel architecture called SNR-EQ-JSCC in this paper, which can be seamlessly applied to Transformer-based semantic coding methods. Our contribution lies in designing an efficient method for leveraging SNR information through a lightweight NN structure that achieves channel adaptability. The innovations of the proposed method are two-fold. First, a channel-adaptive query (CAQ) method is proposed, which explicitly adjusts attention scores in the multi-head attention (MHA) block according to the SNR information. Second, inspired by positional embedding in Transformers [11], we propose an SNR embedding method, which embeds the SNR into the inputs of the MHA block to enhance CA. To further stabilize the training process, penalty terms corresponding to the CAQ method are introduced. Besides, considering that instantaneous SNR feedback may be imperfect, we propose an alternative method that uses only the average SNR, substituting it for the instantaneous SNR in SNR-EQ-JSCC without the need for retraining. Simulation results conducted on image transmission show that our proposed SNR-EQ-JSCC outperforms SwinJSCC [2] across a wide range of channel conditions, with storage overhead and computational complexity for CA in SNR-EQ-JSCC being 0.05% and 6.38% of those in the CA module of SwinJSCC (Channel ModNet), respectively. In addition, the CAQ method significantly enhances perception metrics. When instantaneous SNR feedback is imperfect, SNR-EQ-JSCC using only the average SNR outperforms SwinJSCC and ADJSCC [8]."}, {"title": "II. SYSTEM MODEL", "content": "In this paper, we consider an image semantic coding system with SNR feedback, as illustrated in Fig. 1(a). In this system, the average SNR \\(\\mu \\in \\mathbb{R}\\) is estimated at the receiver without error and then fed back to the transmitter. Then, to enable the sender to obtain the instantaneous SNR \\(\\mu\\) given \\(\\overline{\\mu}\\), the receiver feedback the fading coefficient. During the transmission of one image, the instantaneous SNR changes M times while the average SNR stays the same.\nThe source image is denoted as \\(\\mathbf{x} \\in \\mathbb{R}^{3 \\times U \\times W}\\), where U and W denote its height and width, respectively. The coded results are denoted as \\(\\mathbf{y} \\in \\mathbb{R}^{d_y}\\), where \\(d_y\\) is the dimension of \\(\\mathbf{y}\\) and also the number of channel uses. For the noisy channel, the channel noise is denoted as \\(\\mathbf{n} \\in \\mathbb{R}^{d_y}\\). Each component of \\(\\mathbf{n}\\) is sampled independently from a Gaussian distribution, i.e., \\(\\mathbf{n} \\sim \\mathcal{N}\\left(0, \\sigma_n^2 \\mathbf{I}_{d_y \\times d_y}\\right)\\), where \\(\\mathbf{I}_{d_y \\times d_y}\\) denotes the identity matrix of size \\(d_y \\times d_y\\) and \\(\\sigma_n^2\\) denotes the noise power. The channel fading coefficient within the j-th fading block is denoted as \\(h_j\\), where \\(j \\in \\{1, 2, \\dots, M\\}\\). Additionally, it is assumed that the expected value of \\(\\mathbb{E}\\left(h_j^2\\right) = 1\\), which implies that \\(\\overline{\\mu} = \\mu \\times h_j^2\\).\nDue to the fast fading, the entire image cannot be encoded based on instantaneous SNR. Therefore, we encode the entire image using the average SNR to leverage global information, while also further processing corresponding partial data based on the instantaneous SNR to address the impact of instantaneous fading. Specifically, the transmitter first encodes the source image into intermediate variables \\(\\mathbf{z}\\) according to \\(\\overline{\\mu}\\), which is represented as follows.\n\\[\\mathbf{z} = f_{\\text{en}, \\overline{\\mu}}(\\mathbf{x}, \\overline{\\mu}),\\]\nwhere \\(\\mathbf{z} = \\{\\mathbf{z}_1, \\dots, \\mathbf{z}_M\\}\\). Then, each \\(\\mathbf{z}_j\\) is further encoded into \\(\\mathbf{y}_j\\) based on the corresponding \\(\\mu_j\\), i.e., \\(\\mathbf{y}_j = f_{\\text{den}, \\mu}(\\mathbf{z}_j, \\mu_j)\\). To satisfy the average power constraint of the transmitted signal, each \\(\\mathbf{y}_j\\) is scaled such that \\(\\mathbb{E}(\\mathbf{y}_j \\mathbf{y}_j^T) \\le \\frac{1}{d_y}\\), and then sent to the channel directly to ensure its transmission spans within a coherent time interval, and the received signal can be represented as follows,\n\\[\\mathbf{\\hat{y}}_j = h_j \\mathbf{y}_j + \\mathbf{n}.\\]\nAfter receiving \\(\\mathbf{\\hat{y}}_j\\), the receiver first decodes \\(\\mathbf{\\hat{y}}_j\\) according to \\(\\mu_j\\) to get \\(\\mathbf{\\hat{z}}_j\\), i.e., \\(\\mathbf{\\hat{z}}_j = f_{\\text{dde}, \\mu}(\\mathbf{\\hat{y}}_j, \\mu_j)\\). Once the receiver recovers the complete \\(\\mathbf{\\hat{z}}\\), it reconstructs the source image, which can be represented as follows,\n\\[\\mathbf{\\hat{x}} = f_{\\text{de}, \\overline{\\mu}}(\\mathbf{\\hat{z}}, \\overline{\\mu}).\\]\nWhen \\(\\overline{\\mu}\\) feedback is imperfect, we substitute \\(\\overline{\\mu}\\) for \\(\\mu\\) in \\(f_{\\text{den}, \\mu}\\) and \\(f_{\\text{dde}, \\mu}\\), which is treated as an alternative method for the proposed method without requiring retraining the NN."}, {"title": "III. SNR-EQ-JSCC", "content": "Different from traditional Transformer-based semantic coding methods, SNR-EQ-JSCC leverages channel-adaptive MHA (CAMHA) blocks, as illustrated in Fig. 1(b). This block incorporates two main innovations: the CAQ and SNR embedding methods, which are elaborated in the following. For simplicity, we take the CAMHA block based on the average SNR as an example.\nWe first briefly introduce the scaled dot-product attention block and the MHA block [11]. In the scaled dot-product attention block, input variables are first mapped into queries \\(\\mathbf{q} \\in \\mathbb{R}^{m_q \\times d_q}\\), keys \\(\\mathbf{k} \\in \\mathbb{R}^{m_q \\times d_q}\\), and values \\(\\mathbf{v} \\in \\mathbb{R}^{m_q \\times d_v}\\) through matrix multiplication, respectively. Here, \\(d_q\\) and \\(d_v\\) denote the dimensions of \\(\\mathbf{q}/\\mathbf{k}\\) and \\(\\mathbf{v}\\), \\(m_q\\) denotes the number of queries, keys, and values. Then, the outputs of the scaled dot-product attention block are computed by\n\\[\\text{Atten}(\\mathbf{q}, \\mathbf{k}, \\mathbf{v}) = \\text{softmax}\\left(\\frac{\\mathbf{q} \\mathbf{k}^T}{\\sqrt{d_q}}\\right) \\mathbf{v},\\]\nwhere \\(\\text{softmax}(\\mathbf{x})_{i,j} = \\frac{e^{\\mathbf{x}_{i,j}}}{\\sum_j \\sum_i e^{\\mathbf{x}_{i,j}}}\\). Additionally, the attention score is defined as \\(\\text{softmax}\\left(\\frac{\\mathbf{q} \\mathbf{k}^T}{\\sqrt{d_q}}\\right)\\), which indicates the importance of elements.\nIn the Transformer, the MHA block, an extension of the scaled dot-product attention block, is commonly applied. For an MHA block with \\(m_{he}\\) heads, the j-th attention head applies separate linear transformations to the queries, keys, and values. This process can be represented as follows,\n\\[\\mathbf{q}^j = \\mathbf{q} \\mathbf{w}_q^j, \\quad \\mathbf{k}^j = \\mathbf{k} \\mathbf{w}_k^j, \\quad \\mathbf{v}^j = \\mathbf{v} \\mathbf{w}_v^j \\quad (j \\in \\{1, 2, \\dots, m_{he}\\}\\),\\]\nwhere \\(\\mathbf{w}_q^j \\in \\mathbb{R}^{d_{\\text{out}} \\times d_q}\\) and \\(\\mathbf{w}_k^j \\in \\mathbb{R}^{d_{\\text{out}} \\times d_q}\\), and \\(\\mathbf{w}_v^j \\in \\mathbb{R}^{d_{\\text{out}} \\times d_v}\\), with \\(d_{\\text{out}}\\) denoting the output dimensions of the MHA block. Then, the attention score of the i-th head \\(\\text{Atten}_i\\) can be calculated by Eq. (4). Finally, a linear transformation is applied to the concatenation of all attention values to generate the outputs of the MHA block as\n\\[\\text{MHA}(\\mathbf{q}, \\mathbf{k}, \\mathbf{v}) = \\text{concat}(\\text{Atten}_1, \\dots, \\text{Atten}_{m_{he}}) \\mathbf{w}_{\\text{out}},\\]\nwhere \\(\\mathbf{w}_{\\text{out}} \\in \\mathbb{R}^{(m_{he} d_q) \\times d_{\\text{out}}}\\) is the weight matrix of the outputs.\nThe traditional MHA block generates the attention scores based solely on the source data. In semantic communications where the channel state varies dynamically, the MHA block should adjust attention scores according to the SNR. As mentioned above, the key intermediate variables of the MHA block are \\(\\mathbf{q}\\), \\(\\mathbf{k}\\), and \\(\\mathbf{v}\\). Since attention scores are independent of \\(\\mathbf{v}\\), the channel-adaptive \\(\\mathbf{v}\\) is not utilized. Additionally, due to the multiplicative relationship between \\(\\mathbf{q}\\) and \\(\\mathbf{k}\\), the impact of the SNR on \\(\\mathbf{k}\\) can be incorporated into its effect on \\(\\mathbf{q}\\). Hence, we utilize the CAQ, which are generated by\n\\[\\mathbf{q}' = \\alpha_{\\mu} \\mathbf{q} + \\mathbf{b}_{\\mu},\\]\nwhere \\(\\alpha_{\\mu} = g_{\\theta_{q,1}}(\\overline{\\mu}) \\in \\mathbb{R}^+\\) and \\(\\mathbf{b}_{\\mu} = g_{\\theta_{q,2}}(\\overline{\\mu}) \\in \\mathbb{R}\\). Meanwhile, \\(\\theta_{q,1}\\) and \\(\\theta_{q,2}\\) are the parameters of the NNs that generate \\(\\alpha_{\\mu}\\) and \\(\\mathbf{b}_{\\mu}\\), respectively. The constraint \\(\\alpha_{\\mu} > 0\\) is imposed to prevent the reversal of attention score rankings, which leads to instability during the training process. This constraint is enforced by applying the ReLU activation function at the output layer of \\(g_{\\theta_{q,1}}\\).\nInspired by the positional embedding method in the Transformer, we incorporate the SNR into the inputs of the MHA block. Unlike the positional sequence, the SNR lacks sequential information. Therefore, the traditional positional embedding method based on the cosine function is unsuitable for SNR embedding. Instead, we propose the SNR embedding method inspired by learnable positional embedding methods:\n\\[\\mathbf{x}_{\\text{model}}' = \\mathbf{x}_{\\text{model}} + G_{\\theta_{\\text{SNR}}}(\\overline{\\mu}),\\]\nwhere \\(\\mathbf{x}_{\\text{model}}\\) denotes the inputs of the CAMHA block and \\(G_{\\theta_{\\text{SNR}}}\\) denotes the NN generating the SNR embeddings. By embedding the SNR into the inputs of the CAMHA block, the CA of SNR-EQ-JSCC can be further enhanced.\nSince \\(\\alpha_{\\mu}\\) and \\(\\mathbf{b}_{\\mu}\\) directly influence the attention scores in all CAMHA blocks, they significantly impact semantic coding performance. To stabilize the training process after incorporating SNR, penalty terms are introduced into the loss function.\nAt a lower \\(\\overline{\\mu}\\), SNR-EQ-JSCC needs to omit more non-essential elements to provide more protection for the crucial elements containing semantic information. To this end, as \\(\\overline{\\mu}\\) decreases, SNR-EQ-JSCC needs to accomplish two objectives. First, it needs to increase the differentiation in attention scores, which requires increasing \\(\\alpha_{\\mu}\\). Second, it needs to increase the proportion of elements with low attention scores, which requires increasing \\(\\mathbf{b}_{\\mu}\\). Correspondingly, it needs to ensure that both \\(\\alpha_{\\mu}\\) and \\(\\mathbf{b}_{\\mu}\\) are negatively correlated with \\(\\overline{\\mu}\\). Concerning \\(\\alpha_{\\mu}\\), the following penalty term is introduced.\n\\[\\mathcal{L}_a(\\overline{\\mu}, \\alpha_{\\mu}) = \\text{ReLU}(\\text{Corr}(\\overline{\\mu}, \\alpha_{\\mu})),\\]\nwhere \\(\\text{Corr}(\\cdot)\\) denotes the Pearson correlation function and \\(\\text{ReLU}(\\cdot) = \\text{max}(0, \\cdot)\\). Directly using correlation function as a penalty term is unsuitable, as this would drive the correlation coefficient between \\(\\overline{\\mu}\\) and \\(\\alpha_{\\mu}\\) to -1, while it is sufficient for them to be negatively correlated. The step function is not used to avoid drastic gradient changes that could destabilize the training process. Following this, we define \\(\\mathcal{L}_b\\) in the same manner, which can be represented as follows,\n\\[\\mathcal{L}_b(\\overline{\\mu}, \\mathbf{b}_{\\mu}) = \\text{ReLU}(\\text{Corr}(\\overline{\\mu}, \\mathbf{b}_{\\mu})).\\]\nHence, the loss function of SNR-EQ-JSCC is expressed as\n\\[\\mathcal{L}(\\mathbf{x}, \\mathbf{\\hat{x}}, \\overline{\\mu}, \\alpha_{\\mu}, \\mathbf{b}_{\\mu}) = \\text{MSE}(\\mathbf{x}, \\mathbf{\\hat{x}}) + \\lambda \\times (\\mathcal{L}_a(\\overline{\\mu}, \\alpha_{\\mu}) + \\mathcal{L}_b(\\overline{\\mu}, \\mathbf{b}_{\\mu})),\\]\nwhere \\(\\text{MSE}(\\mathbf{x}, \\mathbf{\\hat{x}})\\) aims to reconstruct the source image and \\(\\lambda > 0\\) is a large constant."}, {"title": "IV. EXPERIMENT RESULTS", "content": "The experiments are conducted on an Intel Xeon Silver 4310 CPU and eight 48 GB Nvidia RTX A40 graphics cards. We compare the following schemes in the experiments:\n\u2022 SNR-EQ-JSCC: This method employs both SNR embedding and CAQ. The backbone follows the design of SwinJSCC [2] without its CA module, but the attention block near the channel is modified to align with the divided z.\n\u2022 SNR-EQ-JSCC w/ \\(\\overline{\\mu}\\): This method is an alternative method of the SNR-EQ-JSCC, which replaces \\(\\mu\\) with \\(\\overline{\\mu}\\) in the inputs of the SNR-EQ-JSCC.\n\u2022 SNR-EQ-JSCC w/o CAQ: This method is the SNR-EQ-JSCC without CAQ.\n\u2022 SNR-EQ-JSCC w/o EM: This method is the SNR-EQ-JSCC without SNR embedding.\n\u2022 SNR-EQ-JSCC w/o CA: This method is the SNR-EQ-JSCC without CAQ and SNR embedding.\n\u2022 SwinJSCC [2]: This method utilizes a fully connected layer-based network to achieve CA.\n\u2022 SwinJSCC w/o CA: This method applies the SwinJSCC architecture without its CA module.\n\u2022 ADJSCC [8]: This method is based on CNN, which achieves CA by the channel attention mechanism.\nThe experiments are conducted on the widely-used DIV2K dataset [12], which comprises 1,000 RGB images with about 2K resolution. Therein, the training, validation, and testing datasets consist of 800, 100, and 100 images, respectively. To standardize the image sizes, they are randomly cropped into patches with resolutions of 256 \u00d7 256. In the experiments, we define the compression rate (CR) as\n\\[r = \\frac{d_y}{3 \\times U \\times W},\\]\nwhere \\(r = 1/32\\) and \\(r = 1/8\\) are set as typical examples of low and high compression rates, respectively. We refine the structure of the ADJSCC model for the DIV2K dataset, whose detailed parameters are listed in Table I. In contrast, we retain the SwinJSCC structure, which was specifically designed for the DIV2K dataset. During training, we set \\(\\mu\\) to be uniformly distributed within [-10,20] dB, and the channel fading magnitude h follows a Rayleigh distribution. Following 5G standards, we assume the transmission of an image spans M = 8 fading blocks. Since the communication overhead for feedback is much lower than that for transmitting y, the overhead for feedback is neglected in the experiments."}, {"title": "B. Effects of X in the Loss Function", "content": "Fig. 2(a) and Fig. 2(b) show performance of the SNR-EQ-JSCC with different \\(\\lambda\\)atr = 1/32 and r = 1/8, respectively. As illustrated, the decrease of \\(\\lambda\\) leads to a decline across all performance metrics. For example, when r = 1/32 and \\(\\mu_{\\text{test}} = 10\\) dB, setting \\(\\lambda = 10^5\\) yields gains of 0.14 dB, 0.004, and 0.003 across the three metrics compared to \\(\\lambda = 0\\). When the penalty weight is zero, applying the CAQ method degrades subjective metrics such as MS-SSIM and LPIPS. This is because CAQ explicitly adjusts attention scores, which represent human focus. Therefore, unstable training can degrade perception metrics. In summary, a sufficiently large penalty weight is required to stabilize the training process."}, {"title": "C. Performance Comparison of Different JSCC Schemes", "content": "Fig. 3(a) and Fig. 3(b) show performance at r = 1/32 and r = 1/8, respectively. Therein, the SNR-EQ-JSCC demonstrates significant performance gains compared to the SwinJSCC. For example, at \\(\\mu = 20\\) dB and r = 1/32, the SNR-EQ-JSCC outperforms the SwinJSCC by 0.32 dB in PSNR, 0.0056 in MS-SSIM, and 0.0095 in LPIPS. This is because the SNR is incorporated into each attention block of the SNR-EQ-JSCC. In contrast, the SwinJSCC only utilizes the SNR to adjust the outputs of the last attention block after compression. Additionally, both the SNR-EQ-JSCC w/o CAQ and the SNR-EQ-JSCC w/o EM outperform the SwinJSCC. Meanwhile, the performance of the SNR-EQ-JSCC w/o CAQ is slightly higher than that of the SNR-EQ-JSCC w/o EM. Among the methods based solely on u, the SNR-EQ-JSCC w/ \\(\\overline{\\mu}\\) consistently outperforms other methods. For example, when r = 1/8 and \\(\\mu_{\\text{test}} = 20\\) dB, the SNR-EQ-JSCC w/ \\(\\overline{\\mu}\\) achieves gains over the SwinJSCC of 0.56 dB in PSNR, 0.009 in MS-SSIM, and 0.035 in LPIPS.\nAlthough the SNR-EQ-JSCC w/o EM shows slightly lower PSNR performance than the SNR-EQ-JSCC w/o CAQ, it achieves superior MS-SSIM and LPIPS. This is because the CAQ method explicitly adjusts attention scores, effectively using the attention mechanism to mimic human focus, thereby enhancing subjective metrics like MS-SSIM and LPIPS."}, {"title": "D. Performance with Imperfect SNR Feedback", "content": "To account for unreliable feedback links, SNR quantization, and outdated information, we consider the scenario of imperfect feedback for \\(\\mu\\). Specifically, the feedback error for the fading coefficient is assumed to be a complex Gaussian distribution with variance \\(\\sigma_n^2\\). Note that due to the relatively long persistence of \\(\\mu\\), it is assumed to be accurate. Since methods using only \\(\\overline{\\mu}\\) are unaffected by imperfect SNR feedback, simulations for these methods are not repeated. For r = 1/32 and \\(\\mu_{\\text{test}} = 0\\) dB, the results in Table II indicate that when \\(\\sigma_n\\) exceeds 0.2, the performance of the SNR-EQ-JSCC falls below that of the SNR-EQ-JSCC w/ \\(\\overline{\\mu}\\). In such cases, the SNR-EQ-JSCC w/ \\(\\overline{\\mu}\\) needs to be applied, which only requires replacing \\(\\mu\\) with \\(\\overline{\\mu}\\) in \\(f_{\\text{den}, \\mu}\\) and \\(f_{\\text{dde}, \\mu}\\)."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel channel-adaptive semantic coding architecture called SNR-EQ-JSCC, which can be seamlessly applied to Transformer-based approaches. The main novelty of this design is two-fold. Firstly, SNR-EQ-JSCC adaptively adjusts query values according to the SNR. Secondly, SNR-EQ-JSCC embeds the SNR into the inputs of the attention block to enhance CA. The penalty terms are introduced to stabilize the training process. The simulations on the DIV2K datasets demonstrate that our proposed SNR-EQ-JSCC outperforms SwinJSCC, while the storage overhead and computational complexity for CA in the SNR-EQ-JSCC are only 0.05% and 6.38% of those in the Channel ModNet, respectively. Furthermore, the CAQ method can significantly improve perception metrics, while the storage overhead and computational complexity to generate CAQ are only one hundred-thousandth and one ten-thousandth of those in the Channel ModNet, respectively. In summary, this work sheds light on achieving CA with extremely low storage overhead and computational complexity. In future work, a potential direction is to extend SNR-EQ-JSCC for multi-carrier or multi-antenna scenarios."}]}