{"title": "NewsEdits 2.0: Learning the Intentions Behind Updating News", "authors": ["Alexander Spangher", "Kung-Hsiang Huang", "Hyundong Cho", "Jonathan May"], "abstract": "As events progress, news articles often update with new information: if we are not cautious, we risk propagating outdated facts. In this work, we hypothesize that linguistic features indicate factual fluidity, and that we can predict which facts in a news article will update using solely the text of a news article (i.e. not external resources like search engines). We test this hypothesis, first, by isolating fact-updates in large news revisions corpora (Spangher et al., 2022). News articles may update for many reasons (e.g. factual, stylistic, narrative). We introduce the NewsEdits 2.0 taxonomy, an edit-intentions schema that separates fact updates from stylistic and narrative updates in news writing. We annotate over 9,200 pairs of sentence revisions and train high-scoring ensemble models to apply this schema. Then, taking a large dataset of silver-labeled pairs, we show that we can predict when facts will update in older article drafts with high precision. Finally, to demonstrate the usefulness of these findings, we construct a language model question asking (LLM-QA) abstention task. Inspired by Kasai et al. (2022), we wish the LLM to abstain from answering questions when information is likely to become outdated. Using our predictions, we show, LLM absention reaches near oracle levels of accuracy.", "sections": [{"title": "1 Introduction", "content": "News is the \"first rough draft of history\" (Croly, 1943). Its information is both valuable and fluid, prone to changes, updates, and corrections. As shown in Figure 1, the first sentence on the left has a factual update, while the second does not. Intuitively, we might be able to predict this: an \u201cadvisory\u201d is not likely to indefinitely stay in effect, while details about the \"quake\u201d are less likely to change. Indeed, if someone asks \u201cQ: Is an advisory still in place?\u201d, we might want to abstain from answering definitively. However, \u201cQ: How large was the quake?\u201d can be answered directly.\nRecent work has recognized the importance of testing LLM-QA in dynamic settings (Jia et al., 2018; Liska et al., 2022). Kasai et al. (2022)'s RealTimeQA benchmark specifically measures LLM-QA performance for updating news documents. However, current approaches rely on search engines retrieving updated information\u00b9. This neglects potentially salient linguistic and common-sense information. As the example shown in Figure 1 demonstrates, cues exist that we, as humans, intuitively understand to signal fluidity. Can we learn these cues, and predict which facts in a news article will update? Can this help LLMs better abstain from answering questions they may not have updated information for?\nWe answer these questions in three steps, shown in Figure 2. In Part 1, we start by studying update patterns in NewsEdits, a large corpus of article revision histories (Spangher et al., 2022). Articles update for many different reasons (e.g. factual,"}, {"title": "2 Related Work", "content": "Although most LLM Q&A benchmarks assume that information is static, recent work has increasingly explored LLM performance in the presence of dynamic, updating information (Jia et al., 2018; Liska et al., 2022). This growing direction is concisely captured by Kasai et al. (2022)'s statement: \"GPT-3 tends to return outdated answers when retrieved documents [are outdated]. Can [we] identify such unanswerable cases?\"\nTo our knowledge, the use of revision-histories to address this question, which we discuss in Section 5, is novel. News updates are an especially crucial domain to study: (1) news is socially important (Cohen et al., 2011); (2) LLMs are increasingly using news to better serve users (Hadero and Bauder, 2023); (3) news is more likely to deal with updating events than other domains (Spangher et al., 2022). Indeed, Kasai et al. (2022)'s RealTimeQA benchmark is built entirely on news data.\nEdit-intention schemas have been developed for other types of revision histories, like Wikipedia (Yang et al., 2017), and Student Learner Essays (Zhang and Litman, 2015). In these works, researchers categorize the intention of each edit us-"}, {"title": "3 Part 1: Learning Edit Intentions in Revision Histories", "content": "News articles update for different reasons, especially during breaking news cycles where facts and events update quickly (Saltzis, 2012). In this section, we introduce the edit-intentions schema we use for NewsEdits 2.0, our annotation, and our models to label edit-pairs. This lays groundwork for Section 4, where we will predict when facts change. We wish to identify categories of edits, in order to enable different investigations into these different update patterns. In other words, we describe the following update model:\n$p(l|s_i, s_j, D, D')$\nwhere l is an intention (e.g. a \u201cCorrection\u201d needs to be made), D and D' represent the older and newer versions of a news article, respectively, and"}, {"title": "3.1 Edit Intentions Schema", "content": "We work with two professional journalists and one copy editor to develop an intentions schema. Building off work by Zhang and Litman (2015) and Yang et al. (2017), we start by examining 50 revision-pairs sampled from NewsEdits. We developed our schema through 4 rounds of conferencing: tagging examples finding edge-cases and discussing whether to add or collapse schema categories. Figure 3 shows our schema, which we organize into coarse and fine-grained labels. We incorporate existing theories of news semantics into our schema. For instance, \u201cEvent Updates\u201d incorporates definitions of \"events\" (Doddington et al., 2004), while \u201cAdd Background\u201d incorporates theories of news discourse (Van Dijk, 1998). \u201cAdd Quote\" incorporates definitions from informational source detection (Spangher et al., 2023) and \u201cAdd Anecdote\u201d incorporates definitions from editorial analysis (Al-Khatib et al., 2016). See Appendix B.2 for a deeper discussion of the theoretical schemas that inform the NewsEdits 2.0 schema. Finally, \u201cIncorrect Link\" is an attempt to correct sentence pairs that were erroneously (un)linked in NewsEdits."}, {"title": "3.2 Schema Annotation", "content": "We build an interface for annotators to provide intention labels for news article sentence pairs (see Appendix C.2). Annotators are shown definitions for each fine-grained intention and the articles to"}, {"title": "3.3 Edit Intentions Modeling", "content": "Now, we are ready to classify edit intentions between sentences in article revisions. Edit intentions are labeled on the sentence-level, and each sentence addition, deletion or update has potentially multiple intention-labels. Document-level context is important: as shown in Figure 1, understanding that Sentence 2, right, adds background (\u201cIt hit the Fukushima plant, site of previous disaster.\u201d) is aided by the surrounding sentences contextualizing that a major event had just occurred. So, we wish to construct models that can produce flexible outputs and reason about potentially lengthy inputs.\nGenerative models have recently been shown to outperform classification-based models in document understanding tasks (Li et al., 2021; Huang et al., 2021). Inspired by this, we develop a sequence-to-sequence framework using Long-"}, {"title": "4 Part 2: Predicting Factual Updates", "content": "In Section 3, we learned high-scoring models to categorize edit pairs (Equation 1). Now, we wish to leverage these to learn a predictive function:\n$p(l = Factual-Update|s_i, D)$\nWhere si and D are the older half of a revision pair. Eq 2 seeks to predict how D might change.\nThe problem statement builds off of a line of inquiry introduced in Spangher et al. (2022). Authors introduced tasks aimed at predicting news article developments across time. They tried to predict whether a \u201csentence will be Added to, Deleted from, or Updated in\" an older draft, to induce reasoning about article changes. However, authors stopped at this \u201csyntactic\u201d analysis. Here, we build off of this mode of inquiry: with the semantic understanding of edits introduced in the prior section, we try to predict how information will change."}, {"title": "4.1 Factual Edit Prediction Dataset", "content": "To construct our task dataset, we sample revision pairs with a non-negligible amount of updates. We sample a set of 500,000 articles from NewsEdits that have > 10% sentences added and > 5% deleted. We acnkowledge that this introduces bias into our dataset, as we focus solely on a subsection of data we know will update. However we build off Spangher et al. (2022)'s broader analysis of syntactic edits patterns, where they found that these kinds of articles could be predicted with reasonable accuracy. We reason that our construction makes it more likely that we are focusing on factual updates"}, {"title": "4.2 Predicting Factual Edits", "content": "For training and development, we chronologically split our dataset into train/development sets with 80/20 ratios. The earliest 80% is our training set, the next 20% for development, etc. To keep cost reasonable, we sample 16,000 sentences for the training set and 2,000 for the development set. We test all approaches on the same gold-labeled doc-We test different variants of Equation 2 to provide different degrees of article context to the model. This helps us understand how much local vs. global article features predict Factual Updates.\n(1) Sentence-Only, $p(l|s_i)$;\n(2) Direct Context, $p(l|S_{i-1}, s_i, S_{i+1})$\n(3) Full Article, $p(l|s_i, D)$.\nFor each variant we test zero-shot (i.e. prompted gpt-3.5-turbo and gpt-4); and fine-tuning approaches (i.e. longformer models)."}, {"title": "5 Part 3: Question Answering with Outdated Documents", "content": "We are ready to test whether the prediction models learned in the last section, to predict whether a sentence will have a Factual update, can help us in dy-"}, {"title": "5.1 LLM-QA Experiments", "content": "Experimental Design We take pairs of sentences in the gold test set of our annotated data where an update occurred, and we ask GPT4 to ask questions based on the older sentence.\n(1) No-Conflict: 5 questions based on information in the older sentence that does NOT update in the newer one.\n(2) Maybe-Conflict: 5 questions based on information in the older sentence that might update in the newer one.\n(3) Likely-Conflict: 5 questions based on information from the older sentence likely updates with a newer one. (For all prompts, see Appendix D).\nExperimental Variants We devise the following experimental variants. Each variant take in the old sentence and a question, generated previously.\n(1) No Warning (Baseline #1): We formulate a basic prompt to GPT4, without alerting it to any possibly outdated material.\n(2) Uniform Warning (Baseline #2) We warn GPT4 that some information might be outdated."}, {"title": "6 Discussion and Conclusion", "content": "The ability of our prediction tags to recover near-oracle performance signals that factual edit prediction can serve a useful role in LLM Q&A. Although we have mainly tested our results in a high-likelihood region of the problem domain as a proof of concept, we suspect that if future work improves the models trained in Section 4.1, then we will see an increase in the ability to drive such abstentions.\nWe do suspect there to be an inherent upper bound in our ability to model such revision patterns. Randomness undoubtedly exists in the editing and revision process; for many factual updates where, perhaps, the ethical stakes of outdated information are lower, journalists may choose not to go back and revise. We still see such work as promising. Indeed, it is surprising that, despite low scores on the modeling components for Part 1 (Edit-Intention Tagging) and Part 2 (Factual Edit Prediction), we still observe useful downstream applications in Part 3. The linguistic insights we are observe concord with human intuition, and identify known shortcomings of current language models.\nThus, we hope more broadly that the taxonomy introduced in NewsEdits 2.0 has many rich directions for yielding linguistic insights and better benchmarks. We hope in future work to revise directions around stylistic and narrative edits, both of which we believe can lead to better tools for computational journalists."}, {"title": "7 Ethical Considerations", "content": "The experiments in our paper require computational resources. Our models run on a single 30GB NVIDIA V100 GPU or on one A40 GPU, along with storage and CPU capabilities provided by our campus. While our experiments do not need to leverage model or data parallelism, we still recognize that not all researchers have access to this resource level.\nWe use Huggingface models for our predictive tasks, and we will release the code of all the custom architectures that we construct. Our models do not exceed 300 million parameters."}, {"title": "7.1 Dataset", "content": "NewsEdits is a publicly_and licensed dataset under an AGPL-3.0 License\", which is a strong \"Copy-Left\" license.\nOur use is within the bounds of intended use given in writing by the original dataset creators, and is within the scope of their licensing."}, {"title": "7.2 Privacy", "content": "We believe that there are no adverse privacy implications in this dataset. The dataset comprises news articles that were already published in the public domain with the expectation of widespread distribution. We did not engage in any concerted effort to assess whether information within the dataset was libelious, slanderous or otherwise unprotected speech. We instructed annotators to be aware that this was a possibility and to report to us if they saw anything, but we did not receive any reports. We discuss this more below."}, {"title": "7.3 Limitations and Risks", "content": "The primary theoretical limitation in our work is that we did not include a robust non-Western language source. As our work builds off of NewsEdits as a primary corpora, it contains only English and French.\nThis work should be viewed with that important caveat. We cannot assume a priori that all cultures necessarily follow this approach to breaking news and indeed all of the theoretical works that we cite in justifying our directions also focus on English-language newspapers. One possible risk is that some of the information contained in earlier versions of news articles was updated or removed for the express purpose that it was potentially unprotected speech: libel, slander, etc. Instances of First Amendment lawsuits where the plaintiff was successful in challenging content are rare in the U.S. We are not as familiar with the guidelines of protected speech in other countries.\nWe echo the risk of the original NewsEdits authors: another risk we see is the misuse of this work on edits for the purpose of disparaging and denigrating media outlets. Many news tracker websites have been used for good purposes (e.g. holding newspapers accountable for when they make stylistic edits or try to update without giving notice). But"}, {"title": "7.4 Computational Resources", "content": ""}, {"title": "7.5\nAnnotators", "content": "We recruited annotators from professional journalism networks like the NICAR listserve, which we mention in the main body of the paper. All the annotators consented to annotate as part of the experiment, and were paid $1 per task, above the highest minimum wage in the U.S. Of our 11 annotators, all were based in large U.S. cities. 8 identify as white, 1 as Asian, 1 as Latinx and 1 as black. 8 annotators identify as male and 3 as female. This data collection process is covered under a university IRB. We do not publish personal details about the annotations, and their interviews were given with consent and full awareness that they would be published in full."}, {"title": "A Additional EDA", "content": "We show the following different analyses to support the findings in the main body.\nTable 10 shows the kinds of edits in 6 different categories of news determined \"socially beneficial\u201d, by (Spangher et al., 2023). As can be seen, even though Factual updates are rarer overall in sentence-level updates, they are more represented in Disaster and Safety categories.\nIn Figure 7, we perform an error analysis on our best-performing ensemble model, which includes tags from Argumentation and Discourse. We inspect the categories we are most likely to get wrong. As can be seen, our fine-grained accuracy is actually quite low, indicating the value of future work, perhaps collecting more training data or employing LLMs to label more silver-standard data. Many categories on the diagonal have 0 labels, both because many categories are low-count categories (e.g. \"Define Term\", which does not have any gold-truth labels in the test set), as well as that more dominant categories capture many of the predictions (e.g. \"Tonal Edits\").\nHowever, the problem is slightly less sever on the coarse-grained level, shown in Figure 6. By comparing these two categories, we can see that many of the errors we observed are on the fine-grained level are within the same coarse-grained category. We suspect that to raise accuracy for fine-grained labels further, we need further experimentation is needed. Perhaps we can experiment with approaches involving more specific fine-grained models or with data augmentation."}, {"title": "A.1 Further details about high-precision sentences", "content": "Figure 4 shows more details of our exploration into the predictability of higher-precision fact-update"}, {"title": "A.2 Technical Improvements over NewsEdits Edit-Action Algorithm", "content": "Spangher et al. (2022) identified \"edit-actions\", or \"syntactic\" edits in article revision histories (i.e. sentence additions, deletions and updates), which requires them to match sentences across article versions. They report a 89.5 F1 efficacy at matching sentences, a significantly higher rate than we might expect for lexical matching. We examined NewsEdits's sentence matches and found that a large source of errors stem from poor sentence boundary detection (SBD). Poor SBD creates an abundance of sentence stubs, which often over-match across revisions. We reprocessed the dataset from scratch using spaCy instead of SparkNLP for SBD, which we qualitatively observe to be better. For word-matching, we use albert-xxlarge-v2's embeddings (Lan et al., 2019) instead of TinyBert (Jiao et al., 2019). These steps, we find, increase our linking accuracy to 95 F1-score. We reprocess and re-release NewsEdits. In addition, we release a suite of visualization tools, based on D3 to enable further exploration of the corpus. See Appendix C.2 for an example."}, {"title": "B Details of the LED Model", "content": "In this section, we describe the specifications of the LED model described in Section 3.3."}, {"title": "B.1 Input Template", "content": "The input to the LED model is shown below:\nPredict the edit intention from\nversion 1 to version 2.\nVersion 1: SOURCE_SENTENCE\nVersion 2: TARGET_SENTENCE\nVersion 1 Document: SOURCE_DOCUMENT\nVersion 2 Document: TARGET_DOCUMENT\nHere, SOURCE_DOCUMENT (D) and TARGET_DOCUMENT (D') refer to the newer and older articles, while SOURCE_SENTENCE (Si) and TARGET_SENTENCE (sj) represent a sentence with these articles."}, {"title": "B.2 Additional Schema", "content": "NLI We use textual entailment from (Dagan et al., 2005), which consists of Entail, Contradict and Neutral. These categories indicate whether two pieces of information refute each other, complement each other, or are neutral. We use a trained model by (Nie et al., 2020), which is an adversarially-trained Albert-xxlarge model, to label pairs of sentences (one from the old version, one from the new version)."}, {"title": "D Prompts for Use-Case", "content": ""}, {"title": "D.1 Question-Asking Prompts", "content": ""}, {"title": "D.1.1 No-Conflict", "content": ""}, {"title": "Prompt Outline", "content": "I will give you a sentence and you\nwill give me 5 different questions.\nIt should be directly answerable by\nthe sentence.\nHere are some examples:\nExample 1: EXAMPLE\nExample 2: EXAMPLE\nExample 3: EXAMPLE\nOk, now it's your turn.\nHere is a sentence: SENTENCE Ask 5\ndifferent questions, output in a list.\nDon't say anything else."}, {"title": "D.1.2 Maybe-Conflict", "content": ""}, {"title": "Prompt Outline", "content": "I will give you a sentence and you\nwill give me an answer. It should\nbe timely and related to the facts in\nthe sentence. It should be a question\nthat could go stale, especially for\nongoing events, or facts like death\ncounts that might update.\nHere are some examples:\nExample 1: EXAMPLE\nExample 2: EXAMPLE\nExample 3: EXAMPLE\nOk, now it's your turn.\nHere is a sentence: SENTENCE Ask 5\ndifferent questions, output in a list.\nDon't say anything else."}, {"title": "D.1.3 Likely Conflict", "content": ""}, {"title": "Prompt Outline", "content": "I will give you two sentences from\nan updating news article and you will\ngive me 5 different questions. They\nshould ideally focus on information\nthat changes in between the sentences.\nSo, if someone were to just look\nat the old sentence and you asked\nthem your question, they would get\nit wrong.\nOk, now it's your turn. Here is the\nold sentence: OLD_SENTENCE Here is\nthe new sentence: NEW_SENTENCE Ask 5\ndifferent questions, output in a list.\nDon't say anything else."}, {"title": "D.2 Question Answering Prompts", "content": ""}, {"title": "D.2.1 Experimental Prompt", "content": "You are a helpful assistant who\nanswers questions based on this news\ninformation:\nNEWS_ARTICLE_SENTENCE\nWe give this a HIGH/MEDIUM/LOW\nchance of there being a fact update\nin this sentence. That might mean\nsome new information could make\nsome of the information in this\nsentence outdated. The user will ask\na question. Answer cautiously and\ndo not give the user wrong/outdated\ninformation. If the user's question\nlooks like it will still be relevant\neven if the facts change, answer it\ndirectly. If the user's question\nlooks like it will be outdated, say\n\"I don't have the most up-to-date\ninformation\" and that's it. Say\nnothing else. Do NOT say \"I don't\nhave the most up-to-date information\"\nAND something else.\nKeep our estimate in mind."}, {"title": "D.2.2 Baseline 1", "content": "You are a helpful assistant who\nanswers questions based on this news\ninformation:\nNEWS_ARTICLE_SENTENCE\nTry to directly answer the users\nquestion and say nothing else."}, {"title": "D.2.3 Baseline 2", "content": "You are a helpful assistant who\nanswers questions based on this news\ninformation:\nNEWS_ARTICLE_SENTENCE"}, {"title": "D.2.4 Oracle", "content": "You are a helpful assistant who\nanswers questions based on this news\ninformation:\nNEWS_ARTICLE_SENTENCE\nThis sentence DOES / DOES NOT\nhave a major fact update. That might\nmean some new information, updating\ninformation. Answer cautiously and\ndo not give the user wrong/outdated\ninformation. If the user's question\nlooks like it will still be relevant\neven if the facts change, answer it\ndirectly. If the user's question\nlooks like it will be outdated, say\n\"I don't have the most up-to-date\ninformation\" and that's it.\nSay nothing else. Do NOT say\n\"I don't have the most up-to-date\ninformation\u201d AND something else."}, {"title": "D.3 Evaluation Prompts", "content": "You are a helpful assistant. You will\nbe shown an old sentence, a revised\nsentence, and a user-question.\nYou will answer the following 2\nquestions:\n1. Is this question answerable given\nJUST the old sentence?\nAnswer with \"yes\" or \"no\". Do not\nanswer anything else. If the answer\nto 1 was yes, then proceed to the\nsecond question, otherwise respond to\nquestion 2 with n/a\n2.\nDoes the question ask\nabout something that is factually\nconsistent with the information\npresented in the revised sentence?\nAnswer with \"yes\", \"no\" or \"n/a.\" Do\nnot answer with anything else."}]}