{"title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models", "authors": ["Yangguang Li", "Zi-Xin Zou", "Zexiang Liu", "Dehu Wang", "Yuan Liang", "Zhipeng Yu", "Xingchao Liu", "Yuan-Chen Guo", "Ding Liang", "Wanli Ouyang", "Yan-Pei Cao"], "abstract": "Recent advancements in diffusion techniques have propelled image and video generation to unprecedented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data processing, and insufficient exploration of advanced techniques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capability, and alignment with input conditions. We present TripoSG, a new streamlined shape diffusion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high-quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D generative models. Through comprehensive experiments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit enhanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input images. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong generalization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large-scale visual datasets have propelled remarkable progress in generative models. These models effectively compress high-dimensional visual data, such as images and videos, into latent spaces, enabling the generation of high-quality visual content conditioned on various input modalities. State-of-the-art generative AI models, including SD3, FLUX, and Sora, exemplify this capability, producing strikingly realistic images and videos from diverse conditional inputs. This breakthrough has revolutionized human visual creation, opening new avenues for artistic expression and content generation.\nIn the domain of 3D content creation, the pursuit of high-quality, production-ready 3D generation remains a primary objective for researchers, artists, and designers. Substantial progress has been made in generating 3D models from single images, with approaches broadly categorized into two paradigms: large-scale reconstruction-based methods and diffusion-based methods. Large-scale reconstruction-based methods primarily utilize a network to regress the 3D model in a deterministic way. While effective, these approaches often struggle with inconsistencies in overlapping regions from multiple input views (which can be generated from a single view by multi-view diffusion models) and exhibit artifacts in occluded areas. Conversely, diffusion-based methods train on 3D representations or latent representations compressed by Variational AutoEncoders (VAEs). As generative rather than regression methods, they circumvent some challenges inherent to reconstruction approaches. However, current methods predominantly rely on occupancy representations, often necessitating additional post-processing to mitigate aliasing artifacts and lacking fine-grained geometric details. Moreover, vanilla diffusion architectures and sampling strategies yield suboptimal 3D model quality, resulting in a significant alignment gap between generated models and input images. A common limitation across both approaches is their heavy reliance on the Objaverse dataset. The necessity for rigorous data filtering often reduces the usable data samples by nearly half, high-presenting a substantial challenge in scaling data compared to the image and video domains.\nGiven these challenges in 3D generation and the substantial successes observed in image and video synthesis, we posit a critical question: What is the optimal paradigm for generating high-fidelity 3D models with precise alignment to input conditions?\nIn response to this inquiry, we present TripoSG, a high-fidelity 3D generative model that leverages a rectified flow transformer trained on meticulously curated, large-scale data. Our approach achieves unprecedented quality in image-to-3D generation, characterized by finer details and superior input condition alignment. Inspired by 3DShape2VecSet, we train our genera-"}, {"title": "2. Related Work", "content": "2.1. Lifting 2D Prior to 3D Modeling\nThe diffusion model has demonstrated strong generative capabilities in image or video generation. However, due to the limitations of high-quality 3D data, it has long been challenging to directly transfer the techniques from text and image generation to 3D tasks. DreamFusion pioneered the use of image diffusion priors for 3D generation by proposing a Score Distillation Sampling method enabling iterative optimization of the 3D representation via differentiable volume rendering. Subsequent work introduced numerous improvements in areas such as 3D represetation, sampling strategy, incorporating additional geometric cues, and multiview image generation consistency. Different from text-to-3D generation methods using off-the-shell text-to-image diffusion model, e.g., Stable Diffusion, many works explore to train a viewpoint-aware image diffusion based on input images. When image generation across\nmultiple views becomes more consistent or with normal or depth generation, the 3D model can be directly optimized by pixel-level loss instead of time-consuming distillation sampling, resulting in 3D generation in a few minutes.\n2.2. Large 3D Reconstruction Modeling\nUnlike the previously introduced methods, which require a time-consuming optimization process lasting several minutes or even hours, various works propose to learn geometry with diverse representation types (e.g., point cloud, voxel, mesh or implicit field) from input images in a deterministic process, with encoder-decoder network architecture. Recently, equipped with a ginormous collection of high-quality 3D models in Objaverse (-XL) as well as an advanced and scalable Transformer-based architecture, Large Reconstruction Model (LRM) and its many subsequent variants has greatly promoted the development of reconstruction-based methods. MeshFormer additionally leverages sparse UNet to downsample the voxel for Transformer layers, leading to impressive reconstruction quality. One-2-3-45 first proposes combining a 2D image diffusion model and a multiview reconstruction model, achieving generation capabilities while maintaining fast reconstruction speed. Bridged with the text-to-image and image-to-multi-view diffusion models, these multiview reconstruction methods can be easily extended to text-to-3D or image-to-3D generation tasks and achieve impressive results. However, the inconsistency between input images from different views can lead to a decline in reconstruction quality, and the unobserved regions may yield blurred results. Thus, these are only 'reconstruction' methods rather than \u2018generation' methods, fundamentally limiting the quality ceiling of such methods.\n2.3. 3D Diffusion Modeling\nTraining a 3D diffusion model for 3D generation is a natural idea that stems from advancements in the field of image and video generation. Many previous works train a diffusion model based on various 3D representations, such as voxel, point cloud, triplane, or Occupancy/SDF grid. Some other works utilize a VAE to transfer the original representation to a compact latent space, and then train a diffusion model on the latent space. For a long time, these 3D diffusion methods have struggled to match the performance of the two major categories of approaches mentioned above due to the lack of large and high-quality 3D model datasets. These methods are mostly trained on simple 3D datasets (e.g., ShapeNet), with limited generation ability and effectiveness, which hinders their practical application. Recently, some researchers have attempted to train a latent 3D diffusion model based on a large amount of high-quality 3D models and have demonstrated impressive 3D generation results. However, these methods still have limitations on high-fidelity generation with image alignment. In this paper, we adopt a 3D representation with better geometry expression ability and improve the diffusion model architecture and training strategy, achieving state-of-the-art performance on 3D shape generation."}, {"title": "3. TripoSG", "content": "This section outlines the specific framework of the TripoSG paradigm, which consists of three main parts: the flow-based generation architecture and sampling schedule; the scaling-up strategy; the VAE architecture and supervision.\n3.1. Rectified Flow Transformer\nLeveraging a meticulously designed VAE architecture and robust supervision information, TripoSG's VAE, described in detail in Sec.3.3, following extensive training on large-scale datasets, is capable of encoding arbitrary 3D shapes into multi-scale latent representations $X = L \\times C, L \\in \\{512, 2048\\}, C = 64$, as well as decoding them back into 3D meshes. Drawing inspiration from models such as LDM and 3DShape2VecSet, we further train a rectified flow model on these latent representations, aiming to generate high-quality, semantically consistent 3D shapes under image-controlled conditions.\n3.1.1. IMAGE-TO-3D FLOW ARCHITECTURE\nOur flow architecture is inspired by DiT and 3DShape2VecSet, utilizing standard transformer blocks to construct the backbone. While this architecture has demonstrated success in class-conditional tasks on ImageNet and ShapeNet, we found that naively stacking multiple transformer blocks leads to suboptimal modeling capabilities due to insufficient information fusion between shallow and deep feature. Drawing inspiration from U-ViT and the UNet structure in Stable Diffusion, we follow Michelan-"}, {"title": "3.1.2. RECTIFIED FLOW BASED GENERATION", "content": "We trained the 3D generation model using our designed flow architecture, exploring sampling strategies including DDPM, EDM, and Rectified Flow, and ultimately selected Rectified Flow for the final generative model.\nDDPM leverages a Markov chain to establish a connection between Gaussian noise space and the data distribution, enabling high-quality data generation. Specifically, noise e is progressively added to the data $x_0$, transforming it into a standard Gaussian distribution. The data sample $x_t$ at any time step t can be expressed by the following equation:\n$x_t = \\sqrt{\\bar{a}_t} x_0 + \\sqrt{1 - \\bar{a}_t} e$ (7)\nWhere $\\bar{a}_t = \\Pi_{t=1}^T a_s, a_t = 1 - \\beta_t$ and $\\beta_t$ is the predefined noise scheduling parameter. From the perspective of interpolation, DDPM models a relatively complex curved trajectory from $x_0$ to $x_t$.\nEDM redesigns the noise schedule and sampling method, adopting a continuous-time framework to improve both the sampling speed and generation quality of DDPM. The data sample $x_t$ at any time step t is modeled using the original data $x_0$ and noise e as follows:\n$x_t = x_0 + \\sigma(t)e$ (8)\n$\\sigma(t)$ is a continuous noise standard deviation function, allowing for more flexible noise scheduling strategies, such as the power form $\\sigma_{min} ((\\sigma_{max}^{1/p} / \\sigma_{min}^{1/p})^t + \\sigma_{min}^{-1/p})^{-p}, \\sigma_{max}$ and $\\sigma_{min}$ are the minimum and maximum noise standard deviation, and p is the hyperparameter that controls the shape of the curve. EDM provides a more streamlined approach to modeling $x_t$ compared to DDPM. From an interpolation perspective, EDM also models a curved trajectory from $x_0$ to $x_t$.\nIs there a simpler linear trajectory modeling process from $x_0$ to $x_t$? To explore this, we further investigated Rectified Flow, which learns a vector field to map the noise distribution to the data distribution. The data sample $x_t$ at any time step t is modeled using the original data $x_0$ and noise e as follows:\n$x_t = tx_0 + (1 - t)e$ (9)\nThis represents a simpler linear trajectory, offering a more efficient and streamlined approach compared to DDPM (Eq.7) and EDM (Eq.8).\nRectified flow's linear sampling simplifies network training, making it more efficient and stable, which we leverage to train our 3D flow model. Additionally, drawing inspiration from SD3 logit-normal sampling, we increase the sampling weight for intermediate steps, as predictions for t in the middle of the range (0, 1) are more challenging during Rectified Flow training. The sampling weight is adjusted using the following equation, where m is the biasing location parameter and s is the distribution width parameter.\n$\\eta_{in}(t; m, s) = \\frac{1}{\\sqrt{2\\pi} s t (1 - t)} exp(-\\frac{(log(t/(1-t)) - m)^2}{2s^2})$ (10)\nIt is well-known that higher resolutions require more noise to sufficiently disrupt the signal. As resolution increases, the uncertainty in the noised latent at the same timestep decreases. Therefore, following SD3, we introduce Resolution-Dependent Shifting of Timestep to adjust the timestep during both training and sampling. By remapping to a new timestep, we maintain the same level of uncertainty as with the original resolution. We define the resolution of the first stage of our progressive training as the base resolution, denoted as n, with its timestep represented as $t_n$. The subsequent stage's resolution is defined as the fine-tune resolution, denoted as m, with its timestep represented as $t_m$. The relationship between $t_m$ and $t_n$ is expressed by the following equation.\n$t_m = \\frac{\\sqrt{\\frac{m}{n}} t_n}{1 + (\\sqrt{\\frac{m}{n}} - 1) t_n}$ (11)\nLeveraging Rectified Flow with logit-normal sampling and resolution-dependent shifting of timestep, we train our 3D flow model."}, {"title": "3.2. Model and Resolution Scale-up Strategy.", "content": "Larger latent resolutions and more extensive models undoubtedly lead to performance improvements. To generate even better results, we aim to scale up both latent resolution and model size while minimizing training and inference costs. Specifically, we increased the latent resolution from 2048 to 4096, and scaled the model parameters from 1.5B to 4B using a Mixture-of-Experts (MoE).\nSince the VAE training does not incorporate additional positional encoding in its input, and the varying number of query points used to learn the latent representations are downsampled from a fixed set of surface points, the VAE can generalize to resolutions beyond the training set. And higher number of query points (latent resolution) improves modeling capacity. This extrapolation ability eliminates the need for retraining the VAE, allowing us to directly encode and decode at a 4096 resolution using the VAE trained on \\{512, 2048\\} resolutions. By leveraging this method to directly increase the latent resolution to 4096, we provide the flow model with finer geometric latent representations for training.\nAdditionally, to mitigate the risk of unstable training and potential loss divergence during mixed-precision training, recommend normalizing Q and K before attention operations. Following this approach, during fine-tuning at higher resolutions in our flow architecture, we apply learnable RMSNorm to normalize Q and K within the transformer blocks.\nDirectly scaling a dense model is the most straightforward approach for increasing model parameters. While this enhances performance, it significantly increases the computational resource demands and inference latency. Rather than"}, {"title": "3.3. 3D Variational Autoencoder (VAE)", "content": "3.3.1. 3D MODEL REPRESENTATION\nMost existing 3D shape generation works adopt occupancy field or semi-continuous occupancy as the neural implicit representation for 3D model. For each query position $x \\in \\mathbb{R}^3$, these methods utilize a neural network D to predict the occupancy value o from the latent features f, supervised by the ground-truth occupancy value \\^o with Binary Cross Entropy (BEC) loss:\no = D (x, f), (14)\n$L_o = E_{x \\in \\mathbb{R}^3} [BCE (o, \\hat{o})]. (15)\nLearning geometry through occupancy representation as a classification task is easier to train and converge compared to the signed distance function (SDF) as a regression task. However, the occupancy representation has limited geometric representation capabilities compared to SDF, which provides more precise and detailed geometry encoding. Additionally, models reconstructed using occupancy representation often exhibit noticeable aliasing artifacts and typically require further post-processing (e.g., smooth filter or super-sampling) to address these issues. Without post-processing, these aliasing artifacts sometimes also impact the subsequent texture generation. Fig.5 shows some examples of geometry reconstruction and texture generation results based on occupancy and SDF, respectively.\nGiven these considerations, we adopt neural SDF as our 3D model representation. This method, built upon a set of latent tokens, provides a stronger geometric detail than occupancy-based approaches. Specifically, we predict the SDF value s of each query position as:\ns = D (x, f). (16)\nFor efficiency, we employ the truncated signed distance function (TSDF) in our VAE model. In the following paragraph, we use s to represent TSDF for simplicity.\n3.3.2. GEOMETRY LEARNING WITH SURFACE NORMAL GUIDANCE\nMore importantly, SDF representation theoretically ensures the effectiveness of supervision in the gradient domain of the neural implicit field. We think geometric details are relevant to the gradient domain of the neural implicit field, which represents the higher-order information compared to the value domain of the implicit field. Therefore, we apply"}, {"title": "3.3.3. NETWORK ARCHITECTURE", "content": "Following the design of , we choose the latent vector set as our latent representation, which encodes a point cloud into latent space and subsequently decodes a geometry function (i.e., SDF) from it. To facilitate more efficient scaling up, we adopt a state-of-the-art transformer-based encoder-decoder architecture. Specifically, we choose the downsampled version in that subsamples M points X' from the full set of surface points X, and directly utilizes the point cloud itself as initial latent queries instead of learnable embeddings. Then, the surface points information, encoded by concatenating positional embedding and surface normal, is integrated into latent queries via cross-attention, resulting in compact latent tokens Z rich in geometric information, as shown in the following:\n$Z_o = CrossAttn(PosEmb(X), PosEmb(X')), (21)\n$Z_i = Linear(SelfAttn^{(i)}(Z_{i-1})), i \\in \\{0, 1, ..., L_{enc}\\}, (22)\nwhere CrossAttn denotes a cross-attention layer, $SelfAttn^{(i)}$ denotes the self-attention layers, and Linear is a linear layer.\nAfter obtaining the latent representation, we can decode the signed distance value for each query position $x \\in \\mathbb{R}^3$:\n$Z_i = SelfAttn^{(i)}(Linear(Z_{i-1})), i \\in \\{0,1, ..., L_{dec}\\}, (23)\ns = CrossAttn(PosEmb(x), Z). (24)\nFinally, the mesh of the 3D model can be extracted by applying Marching Cubes at a given resolution.\nTo implement progressive flow model training for faster convergence, we follow to adopt a multi-resolution VAE with M \u2208 {512, 2048} tokens, where the VAE weights are shared across different resolutions. This training strategy, combined with the position-encoding-free feature of the VAE transformer, provides the VAE with strong extrapolation capabilities, allowing it to directly infer the 3D models with higher-resolution (e.g., 4096) tokens without requiring additional fine-tuning. Unlike previous works, which used only a few surface points (only 2048 or 8192 points) as the VAE input, we opted to use a denser surface point for each 3D model. We think the purpose of the VAE is to capture as much geometric information of the 3D model as possible, rather than functioning as a sparse point cloud reconstruction task. The more input points provided, the more geometric information is encoded in the latent space, resulting in higher-quality geometry being decoded."}, {"title": "4. Data-Building System.", "content": "TripoSG is trained on existing open-source datasets such as Objaverse (-XL) and ShapeNet, which contains approximately 10 million 3D data. Since most of these data are sourced from the Internet, their quality varies significantly, requiring extensive preprocessing to ensure suitability for training. To overcome these challenges, TripoSG developed a dedicated 3D data processing system that produces high-quality, large-scale datasets for model training. As illustrated in Fig.6, the system comprises four processing stages (Data Process I~IV), responsible for data scoring, filtering, fixing and augmentation, and field data producing, respectively."}, {"title": "5. experiment", "content": "5.1. Implementation Details\nThe shape generation experiments are divided into two parts. In the TripoSG experiment, we progressively scaled both resolution and model size. First, we trained a 1.5B parameter model on a 2M dataset with a latent resolution of 512 tokens, using a learning rate of 1e-4 for 700k steps. Next, we switched to a latent resolution of 2048 tokens and continued training for an additional 300k steps with a learning rate of 5e-5. Finally, to scale up, we expanded the model parameters to 4B using MoE and increased the latent resolution to 4096 tokens. Training resumed on 1M high-quality dataset with a learning rate of 1e-5 for 100k steps. The batch size of the three processes is set to 16, 10, and 8 per GPU respectively. The entire training process took approximately 3 weeks across 160 A100 GPUs.\nIn the ablation experiments, we still use a small dataset (180K) filtered from Objaverse and a 975M parameter model for training. For the non-scaling ablation experiments, as shown in Tab.1, we trained the model with a latent resolution of 512 tokens, a learning rate of 1e-4, for about 300k steps, over approximately 3 days on 32 A100 GPUs. For the scaling-up ablation experiments, as shown in rows 2-4 of Tab.2, we progressively continued training from the previous experiment with latent resolutions of 2048 tokens, 4096 tokens, and 4096 tokens with the MoE model architecture, respectively, for an additional 100k steps. The learning rates were 5e-5, 1e-5, and 1e-5, respectively. These three scaling-up experiments took around 9 days in total on 32\nA100 GPUs. For all ablation experiments, the batch size was set to 16 per-GPU.\nIt is also worth mentioning that during training, the image foreground is resized to a fixed ratio (90%) and rotated around the center within a range of [-10\u00b0, 10\u00b0] with a probability of 0.2. This setting enables the model to generalize well to various input images. During inference, the image is first detected for the foreground and then resized to the same ratio as the training foreground to obtain the best generation effect.\nFollowing our VAE model adopts a network architecture with an 8-layer encoder and a 16-layer decoder. We use a larger decoder to enhance the ability to decode geometry from the latent space, without increasing the inference cost of the VAE during the flow model training stage. The weights of surface normal loss $A_{sn}$, eikonal regularization $l_{eik}$, and KL-regularization $\\lambda_{kl}$ are set to 10, 0.1, and 0.001, respectively. For each training data item, our model takes 20, 480 surface points as input and randomly samples 8, 192 near-surface points, 8, 192 volume points, and 8, 192 on-surface points for supervision.\nThe VAE experiments are divided into two parts as well: TripoSG experiments and ablation experiments. In the TripoSG experiment, we train the VAE via SDF supervision with surface normal guidance and eikonal regularization using a learning rate of 5e-5 and a per-GPU batch size of 6 for 2.5M steps. The training process takes approximately 12 days on 32 A100 GPUs, then the VAE is used for the scale-up flow model training. For the ablation experiment, we evaluate the VAE reconstruction quality from different experiment settings on a small dataset (180K filtered data from Objaverse). We train the VAE with different settings using a learning rate of 1e-4 and a per-GPU batch size of 8 for 286K steps on 8 A100 GPUs."}, {"title": "5.3.1. \u0421\u043eMPARISON WITH DIFFERENT METHODS IN VISUALIZATION", "content": "As shown in Fig.7, we compare TripoSG with the most popular image-to-3D generation methods. It is worth noting that for Craftsman, we used the online demo of Craftsman-1.5 on Huggingface for inference, which is a more advanced version of the Craftsman. The first row in the figure shows the original input image, while rows 2-7 present a comparison between the generation 3D models of other methods and TripoSG. We compared their geometric quality by rendering normal maps. The results shown in the figure are all 3D normal maps rendered from the same viewpoint. Notably, we preprocessed the original image by removing the background and fed the processed images to different open-source models via Huggingface demos for online inference and generation. Unlike previous works that typically compare 3D generation results on simple, standard images, we conducted comparisons on complex and widely varying cases.\nSpecifically, we evaluated the methods across five dimensions from left to right: (1) Semantic Consistency: TripoSG generates 3D models with better semantic consistency, as shown in the first and second cases, with greater detail and semantic alignment. (2) Detail: The third and fourth cases demonstrate TripoSG's ability to capture finer details, such as clothing textures and accessories, providing richer visual fidelity. (3) Generalization: The fifth and sixth cases highlight TripoSG's ability to generate high-quality 3D models from both comic-style and cartoon-style images, showcasing its strong generalization capability. (4) Spatial Structure Generation: The seventh and eighth cases show TripoSG excels at generating complex spatial structures, demonstrating superior spatial modeling capabilities. (5) Overall Performance: We compared TripoSG with the latest and most advanced open-source methods, including both reconstruction and generation approaches, and it is evident that TripoSG delivers significantly superior results, leaving a strong impression and outperforming previous approaches by a wide margin."}, {"title": "6. Texture Generation.", "content": "Thanks to the finely detailed and high-quality 3D geometry generated by TripoSG, referring to Meta 3D Texture-Gen, we can leverage the rendered normal maps as input conditions for existing mature multi-view generation methods to produce consistent multi-view texture images. These multi-view texture images are then projected onto the geometric surface to obtain detailed texture maps."}, {"title": "7. ablation and analysis", "content": "7.1. Ablation for Flow Model\nTo validate the effectiveness of the proposed flow model improvements and scaling-up strategies, we performed specific ablation experiments and comparative analyses for each improvement. Using a further filtered 180K high-quality Objaverse dataset, we conducted ablation experiments following the training settings in 5.1, and evaluated the results using the Normal-FID metric introduced in 5.2.2.\nFor the evaluation of Normal-FID, we selected 1K data samples from the 180K dataset as a dedicated test set for 3D generation performance validation, with the remaining data samples used for training. For the test set, we rendered each 3D ground-truth model's front view paired RGB and normal map using a 50mm camera focal length and a 10\u00b0 elevation (the test set rendering settings are included within the training set settings). The RGB images are used to generate 3D shapes, and the normal maps are compared with the normal maps rendered from the generated 3D shapes to calculate the Normal-FID.\nOur flow model ablation experiment consists of two parts: flow model improvement training and flow model scaling up. For the flow model improvement experiments, as shown in Tab.1, we used a 975M parameter model with a latent resolution of 512 tokens and trained for 300K steps for each experiment on the high-quality Objaverse dataset. We conducted comparative analyses on Condition, Skip - connection, and Sampling \u2013 schedule improvements. From the last three rows in Tab.1, we observe that R-Flow sampling yields better generation results compared to EDM and DDPM. Combined with its training efficiency, R-Flow demonstrates clear advantages in 3D generation tasks. Comparing rows 2 and 5 shows that the skip-connection operation significantly affects generation results, with the fusion of deep and shallow features improving flow modeling. Additionally, the comparison between the first two rows indicates that the CLIP condition also slightly improves generation results. From the overall quantitative results, the skip-connection operation has the most obvious effect among these ablations.\nFor the flow model scaling-up experiments, as shown in rows 2-4 of Tab.2, we used a 975M parameter model with CLIP-DINOv2 dual-conditioning, skip-connection operations, and rectified-flow sampling schedule. These models are trained for a total of 300K steps on the high-quality Objaverse data to conduct comparative analyses on latent resolution and MoE. The last row of Tab.2 represents our largest TripoSG model, encompassing the largest data, model size, resolution, and training cost. From the first three rows of Tab.2 we observe that as the latent resolution increases, the generated results consistently improve, with the most significant improvement occurring from 512 to 2048 tokens. Comparing rows 3 and 4 shows the gains from increasing model parameters through MoE. Comparing rows 4 and 5 demonstrates the performance improvement achieved by increasing high-quality data size. When combined with the results from row 1, we can see that the improvement from the increased high-quality data size surpasses that from higher resolution. Overall, the large-scale dataset, large model size, and high resolution contribute to significant performance improvements, allowing TripoSG to achieve remarkable 3D generation results."}, {"title": "7.2. Ablation for VAE", "content": "To evaluate the effectiveness of neural SDF implicit representation with surface normal guidance, we experiment with different VAE model settings, including the formulation of neural implicit representation, training supervision, and training dataset. Tab.3 demonstrates the qualitative results of VAE reconstruction quality with different training settings. We can observe that the SDF representation, combined with surface normal guidance and eikonal regularization, improves the reconstruction quality and geometry details, achieving lower Chamfer distance and higher normal consistency compared to occupancy-based results. As the amount of training data increases (as demonstrated by TripoSG), the reconstruction quality of the VAE further improves. Fig.9 provides qualitative comparisons between them. Occupancy-based reconstruction results suffer from aliasing artifacts (highlighted by the blue box), thin structures, and floaters (highlighted by the red boxes). While SDF representation avoids aliasing artifacts, there remains a gap in achieving high-quality reconstruction, particularly for thin-shell structures where performance may worsen. Incorporating surface normal guidance can result in sharper reconstructions with finer details. However, over-emphasizing surface normal guidance during training introduces slight aliasing artifacts (as seen in the first row of Fig.9), which can be mitigated by introducing eikonal regularization."}, {"title": "7.3. Ablation for Data-Building System", "content": "To demonstrate the effectiveness of the data-building system proposed by TripoSG, we implement ablation experiments on both data quality and quantity. Using the optimal R-Flow training settings (first row of Tab.1), we replaced the 180K Objaverse dataset produced by TripoSG with the original 800K Objaverse dataset, which had not undergone scoring, filtering, orientation fixing, untextured model processing, or internal processing of the converted watertight model. This experiment demonstrates the effect of data quality. Similarly, under the same R-Flow settings, we expanded the high-quality dataset from 180K Objaverse to 2M TripoSG to evaluate the effect of data quantity.\nAs shown in the first two rows of Tab.4, although our data-building system reduced the 800K Objaverse dataset to 180K, the improved data quality resulted in better generation results, demonstrating that, when training with in-the-wild data, quality outweighs quantity. Furthermore, as seen in last two rows of Tab.4, increasing the high-quality dataset from 180K to 2M led to a significant boost in generation performance, showing that with high-quality data, scaling up data size is crucial for achieving better results. Additionally, the overall quantitative results in the Tab.4 show that the performance improvement gained from 2M high-quality data size is greater than that from improving data quality alone. Furthermore, after enhancing data quality, performance continues to improve with an increase in data size, without encountering a bottleneck at the current training scale."}, {"title": "7.4. The Visualization for Flow Model Ablation", "content": "In addition to the quantitative results, we also performed a visualization analysis of the core experiments, as shown in Fig.10. The rows 1, 2, 4 correspond to the three experimental results in Tab.4, while the row 3 corresponds to the row 4 of results in Tab.2. The visualization reveals several insights consistent with the quantitative results: (1) Data quality is more important than the size of raw in-the-wild data (row 1 vs. row 2). (2) The improvement from increasing high-quality data size is more obvious than the improvement from resolution (row 2 vs. row 3 vs. row 4). (3) Increasing the size of high-quality data (2M) provides a greater boost to performance than merely improving data quality. After improving data quality, performance continues to improve with increased data size without encountering bottlenecks at the current training scale."}, {"title": "8. conclusion and discussion", "content": "8.1. Conclusion\nWe present TripoSG, a new image-to-3D generation model via the rectified-flow-based transformer. To efficiently train the model for high-fidelity shape generation, we propose a data-building system to process data from original datasets. Compared to using all in-the-wild 3D models in the training dataset, filtered and fixed high-quality data can be properly reproduced into training data and effectively improve the model's training performance. Additionally, we leverage the advance of SDF representation with surface normal guidance and eikonal regularization for finer geometry details and avoid aliasing artifacts. Furthermore, a rectified-flow-based transformer with MoE and a high-resolution strategy is introduced for the scale-up training. Experiments demonstrate that TripoSG can generate high-fidelity 3D shapes, leading to a new state-of-the-art performance.\n8.2. Discussion\nIn recent years, 3D generation has followed a unique exploration route, with methods such as using text-to-image models as priors for 3D generation via the SDS solution, such as DreamFusion, and leveraging decoder-only transformer architectures to reconstruct 3D models from single or multiple views, such as LRM. However, due to the scarcity of large-scale datasets and limited experience in scaling up training for 3D generation tasks, the large-scale flow models, which have proven highly successful in 2D image and video generation, have not been widely applied to 3D generation. TripoSG has deeply explored the 3D flow"}]}