{"title": "Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting", "authors": ["Raja Kumar", "Vanshika Vats"], "abstract": "3D Gaussian splatting has surpassed neural radiance field methods in novel view synthesis by achieving lower computational costs and real-time high-quality rendering. Although it produces a high-quality rendering with a lot of input views, its performance drops significantly when only a few views are available. In this work, we address this by proposing a depth-aware Gaussian splatting method for few-shot novel view synthesis. We use monocular depth prediction as a prior, along with a scale-invariant depth loss, to constrain the 3D shape under just a few input views. We also model color using lower-order spherical harmonics to avoid overfitting. Further, we observe that removing splats with lower opacity periodically, as performed in the original work, leads to a very sparse point cloud and, hence, a lower-quality rendering. To mitigate this, we retain all the splats, leading to a better reconstruction in a few view settings. Experimental results show that our method outperforms the traditional 3D Gaussian splatting methods by achieving improvements of 10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and 14.1% in perceptual similarity, thereby validating the effectiveness of our approach. The code will be made available at https://github.com/raja- kumar/depth-aware-3DGS.", "sections": [{"title": "1 Introduction", "content": "Novel View Synthesis (NVS), where the goal is to render unseen viewpoints given a set of input images, is a long-standing problem in the field of 3D vision. Neural Radiance Field (NeRF) [20] emerged as a state-of-the-art representation to achieve the highest quality renderings. Although NeRF achieves high- quality rendering, it suffers from high computational overhead due to the use of a Multi-Layer Perceptron (MLP) and differential volumetric rendering [20,22,31]. Recently, Kerbl et al. [9] introduced a 3D Gaussian based representation with point-based differentiable rendering, achieving a very high-quality and real-time rendering."}, {"title": "2 Related Works", "content": "Although this method achieves great results, it relies on many input views and dense coverage of the scene for a faithful reconstruction. However, in real-world applications such as AR/VR, autonomous driving, and robotics, the number of available views for any object is few and sparse. In such scenarios, the rendering quality of the existing methods drops significantly. Existing works try to address this challenge by using various regularizations on geometry and appearances [8,22,31]. In our study, we propose a few-shot novel view synthesis using depth- aware 3D Gaussian splatting. We achieve this by using off-the-shelf monocular depth prediction as a prior to constrain the 3D shape of a scene. The original method uses high-order spherical harmonics (SH) to represent the color of each point which helps them recover high-frequency details. However, it is difficult to recover high-frequency intricacies given only a few input views since it leads to over-fitting the training views. Therefore, we model the color using lower-order spherical harmonics. Our experimental results show that the proposed depth- aware approach with just 5 views outperforms the existing state-of-the-art 3DGS method under few-shot conditions.\nThus, the main contribution of our study is synthesizing good quality novel views from a very limited data to begin with, using depth-aware 3D Gaussian splatting and achieving state-of-the-art performance on PSNR, SSIM and LPIPS metrics."}, {"title": "2.1 Neural Representations of Scenes", "content": "Coordinate-based Neural Representation [4, 17, 18] has gained popularity in re- cent years. Mildenhall et al. [20] introduced NeRF, an MLP-based scene represen- tation that predicts the density and color of a 3D point given its coordinate and view direction. It achieved state-of-the-art results in synthesizing novel views of complex scenes. This work laid the foundation of several studies applying neu- ral radiance fields in various domains including model reconstruction [23, 33], video representations [2, 13, 16], pose estimation [3, 29] and editing [7, 11]. The main challenge of the NeRF based method lies in the heavy computation cost requiring hours of training even for a single scene. The challenge arises from the large parameter MLP and a slow volumetric rendering process. To improve the training time, M\u00fcller et al. [21] proposes a Multiresolution Hash Encoding- based representation which avoids the MLP and also provides a faster rendering leading to drastic improvement in the training time. FastNeRF [6] uses graphics- inspired factorization to compactly cache a deep radiance map at each position in space and efficiently query that map using ray directions to improve the ren- dering speed of NeRF. To further improve the rendering quality and avoid the aliasing issue of NeRF, Mip-NeRF [1] replaces the point-based ray tracing using anti-aliased conical frustums.\nA recent breakthrough in NVS was achieved when Kerbl et al. [9] proposed a 3D Gaussian representation with a point-based rendering that trains a scene within a few minutes and a rendering speed of more than 100FPS at 1080p"}, {"title": "2.2 Few-Shot Novel View Synthesis", "content": "One of the common approaches to solving the problem of few-views recon- struction is to use a prior to constraint the optimization [12]. Several works [8,22,24,31] have tried to achieve few-shot reconstruction using NeRF represen- tation. Niemeyer et al. [22] overcome this issue of NeRF's performance degra- dation with sparse input views by regularizing the geometry and appearance of rendered patches and using a normalizing flow model to regularize color. Jain et"}, {"title": "3 Methodology", "content": "In this section, we discuss our proposed method in detail. Fig. 1 shows the overall pipeline of our method. Given 5 input views, we use COLMAP [27, 28] to get the initial point cloud and camera parameters. Using this sparse point cloud, we initialize our 3D Gaussians for training. As shown in Fig. 2, the point cloud generated using Structure From Motion (SFM) from 5 views is very sparse com- pared to the one generated using many views. After 3D Gaussian initialization, we perform the projection and adaptive density control followed by rendering to get the rendered image. We modify the original renderer to render depth in addition to the RGB image. We use the implementation provided in [10] for rendering depth. Once we have the rendered image and depth, we compute the loss. In the original 3DGS method, pixel-wise photometric loss, a combination of L1 and SSIM loss, is used to train the model. However, in the case of few views, this photometric loss alone is not enough as it tends to overfit the training views while the 3D shape is still inaccurate. To this end, we propose to use a depth prior to constrain the 3D shape. In the following sections, we first discuss the details of the original 3D Gaussian splatting method, followed by our depth prior and scale-invariant depth loss regularization term."}, {"title": "3.1 3D Gaussian Splatting", "content": "3DGS, originally proposed by Kerbl et al. [9], represents a scene using 3D Gaus- sians characterized by position, covariance, opacity, and spherical harmonics for color. The optimization of this compact scene representation involves adjust- ing these parameters and controlling Gaussian density. Efficiency is achieved"}, {"title": "3.2 Depth Regularization", "content": "The original 3DGS [9] algorithm produces its best results using many viewpoints. However, the rendering quality decreases significantly when only a few views are available. This loss in quality is mainly due to the model fitting the training views without learning the intrinsic 3D shape correctly. With the advent of deep"}, {"title": "Gaussian Splatting Depth Rendering", "content": "3DGS uses a neural point-based ap- proach to render the color of pixels. The color C of a pixel is computed by blending N ordered points overlapping the pixel as:\n$$C = \\sum_{i \\in N} C_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j)$$\nwhere $c_i$ is the color of each point and $\\alpha_i$ is the opacity of the projected 2D splats computed by multiplying the covariance $\\Sigma$ with opacity. In order to use depth prior as supervision, we need to modify the renderer to render the depth as well. To this end, we use differential Gaussian rasterization [10], which implements depth and alpha rendering in addition to color. Similar to pixel rendering, it uses alpha blending of z-buffer from the ordered Gaussians. The depth D of a point is computed as:\n$$D = \\sum_{i \\in N} d_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j)$$\nwhere $d_i$ is the z-buffer value for $i^{th}$ gaussian and $\\alpha_i$ is same as equation 2."}, {"title": "Scale Invariant Depth Loss", "content": "Given the depth estimated from the monocular depth estimation model and the rendered depth, we can calculate the loss be- tween these two for supervision. One of the challenges of calculating the depth loss is the scale of depth values. The scale of the monocular model and the 3DGS depth renderer can be different. Therefore, based on Eigen et al.'s [5] work, we use a scale-invariant loss function that takes into account the scale of both monocular and 3DGS depth renderer. From a rendered depth map y and predicted depth (using monocular model) y*, each with n pixels index by i, we compute scale invariant mean squared error (in log space) as\n$$L_{depth}(y, y^*) = \\frac{1}{2n} \\sum_{i=1}^{n} (log \\space y_i - log \\space y_i^* + a(y, y^*))^2$$\nwhere, $a(y, y^*) = \\sum (log \\space y_i^* - log \\space y_i)$ is the value of a that minimizes the error for a give (y, y*).\nIn addition to the photometric pixel-wise loss used in the original 3DGS paper, we use this scale invariant depth loss. Therefore our final loss function would become\n$$L = (1 - \\lambda)L_1 + \\lambda L_{D-SSIM} + \\lambda_{depth} L_{depth}$$\nWe set the value of $\\lambda = 0.2$ and $\\lambda_{depth} = 0.005$ for our tests."}, {"title": "3.3 Other Optimizations", "content": "In addition to using monocular depth prior, we also make modifications to the Spherical Harmonics (SH) model and density pruning step to make it fit for few view settings. In the original 3DGS, a SH coefficient of order 3 is used to estimate the color of each point. A higher order harmonics is basically fitting a higher degree polynomial [25], and hence a more detailed reconstruction can be achieved. However, when we only have a few views available for training, it is challenging to reconstruct high-frequency details. Based on this observation, we decrease our SH coefficient order to 1, making our scene less prone to overfitting.\nIn the original 3DGS, the Gaussians with low opacity are removed periodically. However, we observe that removing splats with lower opacity periodically leads to removing a large amount of the splats since the initial point cloud is very sparse in a few view inputs. This leads to an unstable and inaccurate optimization. We address this by retaining all the splats, which leads to a better reconstruction in a few view settings."}, {"title": "4 Experiments", "content": "Here, we first discuss the datasets, evaluation metrics, and implementation de- tails of the experiments. We then perform a qualitative and quantitative com- parison to the original 3DGS method to show the effectiveness of our method."}, {"title": "4.1 Implementation Details", "content": "We perform our experiments on the NeRF LLFF dataset [19], which is widely acclaimed for NVS tasks. The original dataset contains 15-20 viewpoints for each scene. For our experiments, we sample 5 train and 3 test views uniformly, containing both interpolated and extrapolated viewpoints. By interpolated, we mean a view from a point in space that lies between the training camera posi- tions, and by extrapolated, we assess the viewpoints extending beyond the range of the original camera position where no direct visual information is available from the original images. This is done to assess the robustness of our method under the challenging viewpoints. We set the order of spherical harmonics (SH) to 1 instead of the original 3, owing to the fact that it is challenging to model high-frequency details using only a few views. Hence a lower order SH can better represent the color. Also, as explained in Sec. 3, we do not reset the opacity of the splats periodically. We train each scene for 10000 iterations and report the widely used SSIM, PSNR, and LPIPS as evaluation metrics."}, {"title": "4.2 Experimental Results", "content": "To evaluate the efficacy of our proposed method, we compare it against the original 3DGS method. We use the same few view settings to train the original 3DGS method and perform both quantitative and qualitative comparisons."}, {"title": "5 Limitations and Future Work", "content": "We achieve SOTA results on NVS with few input views. However, our method has limitations. Since we use an off-the-shelf monocular depth estimation method, our accuracy is dependent on the accuracy of the depth estimation model. There- fore, there remains a scope for improvement by using a better depth estimation model. In future work, Our method can be further improved by using a depth loss that matches the depth distribution rather than the absolute value. Another challenge of few-shot reconstruction is generating the initial point cloud for ini- tialization. If the input views are too sparse or do not have enough matching points, the COLMAP reconstruction fails and hence the initialization. In future work, we plan to work on solving this by using a 3D bounding box prediction and then using random initialization."}, {"title": "6 Conclusion", "content": "In this work, we propose a depth-aware 3D Gaussian splatting method for novel view synthesis that achieves state-of-the-art performance under few shot condi- tions. We use monocular depth estimation as a prior to supervise the training. Such a constraint helped us recover a more faithful 3D representation and, hence, a better rendering even for challenging viewpoints. We also make optimizations to the SH order and Gaussian density pruning technique to improve the ren- dering quality. Finally, We evaluate our method on the LLFF dataset, and our method outperforms the existing methods in terms of both quantitative and qualitative evaluation."}]}