{"title": "A Hybrid Model for Weakly-Supervised Speech Dereverberation", "authors": ["Louis Bahrman", "Mathieu Fontaine", "Ga\u00ebl Richard"], "abstract": "This paper introduces a new training strategy to improve speech dereverberation systems using minimal acoustic information and reverberant (wet) speech. Most existing algorithms rely on paired dry/wet data, which is difficult to obtain, or on target metrics that may not adequately capture reverberation characteristics and can lead to poor results on non-target metrics. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. The system's output is resynthesized using a generated room impulse response and compared with the original reverberant speech, providing a novel reverberation matching loss replacing the standard target metrics. During inference, only the trained dereverberation model is used. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics used in speech dereverberation than the state-of-the-art.", "sections": [{"title": "I. INTRODUCTION", "content": "Acoustic signals captured in closed rooms are affected by reflections from room walls and diffraction from obstacles on its path, in a process coined as reverberation. These effects may not be desirable in speech recordings as they lower speech intelligibility [1]. This justifies the need for dereverberation methods to mitigate the reverberation phenomenon in speech-related tasks such as speech enhancement and automatic speech recognition [2]. Dereverberation task has been historically solved by using statistical signal-processing methods [3]\u2013[5]. The nonlinearity of the task naturally calls for deep neural networks (DNNs) extensions requiring in practice a large amount of annotated data and learning strategies. These learning-based approaches can be supervised in various ways.\nBest-performing discriminative approaches as TFGrid-Net [6] learn to predict a dry signal from a reverberant one decomposing the time-frequency signal into time and spectral subband modules and hence require paired dry/wet data. FullSubNet [7] aims to estimate a Complex Ratio Mask (CRM) for retrieving the dry signal. However, these techniques require generating a large amount of paired data and may lack robustness if the test data significantly differs from the training dataset. This lack of paired data has motivated the development\nof new approaches which can dereverberate using unpaired signals only as in Cycle-consistent Generative Adversarial Networks (GANs) [8], [9].\nOn the other hand, generative models like variational au-toencoders [10], [11] or diffusion models [12] are used to learn the prior dry signals without having access to any reverberant signal at training. Although these models require less supervision, they do not solve the problem of data scarcity, since dry speech data is harder to obtain than reverberant speech data.\nFew approaches are designed to require only reverberant signals at training time. The current best-performing model for dereverberation supervised only by reverberant signals is MetricGAN-U [13]. Its training framework is based on a GAN, where the discriminator is trained to mimic the behaviour of a target metric, and the generator to optimize its performance with respect to this evaluation metric. It has been successfully applied on the dereverberation task, using Speech to Reverberant Modulation energy Ratio (SRMR) [14] as a target metric to be optimized.\nBesides, both supervised and unsupervised approaches for dereverberation have been improved by leveraging reverber-ation models. Such approaches can be considered as hybrid deep learning, in the sense that they combine DNN priors with statistical models or signal-based representations of the reverberation. Indeed, reverberation has been classically rep-resented as a convolutive distortion and approaches have been developed to concurrently estimate the convolutive model and the dry signal [3], [4], [15]. The reverberation model can be implicitly modelled, or explicitly used. A popular choice to implicitly model reverberation is the Convolutive Transfer"}, {"title": "II. REVERBERATION MODEL", "content": "Assuming fixed source and microphone positions and no additive noise, a monaural reverberant signal y can be rep-resented as a convolution between a dry signal s and the room impulse response (RIR) h between the source and the microphone:\n$y(n) = (s * h)(n)$, (1)\nwhere n denotes the time index and the convolution operator.\nThe RIR h can be divided into three parts: the direct path corresponds to its first peak $h_d$ followed by early echoes $h_e$ and, after the mixing time, late reverberation $h_l$:\n$h = h_d + h_e + h_l$, (2)\nThe support of $h_d$, $h_e$ and $h_l$ are disjoint, and several defini-tions for the mixing time $n_m$ have been proposed. In [29], the\nmixing time in samples is defined using statistical properties of ergodic rooms, as a multiple of the mean free path [30]:\n$n_m = \\frac{4V f_s}{cA}$, (3)\nwhere V, $f_s$, c and A are respectively the room volume, sampling frequency, speed of sound and area of the walls."}, {"title": "B. Polack's late reverberation model", "content": "A simple yet powerful model for late reflections is Polack's model [31]. This model states that the late reverberation $h_l$ is a realization of an exponentially decaying white noise:\n$h_l(n) = b(n)e^{-(n+3n_m)/\\tau}$, (4)\nwhere $b(n) \\sim \\mathcal{N}(0,\\sigma^2)$ is a centered Gaussian distribution, and $\\tau$ depends on the reverberation time RT60 and the sam-pling frequency $f_s$ as:\n$\\tau = \\frac{RT_{60} \\cdot f_s}{3 \\ln(10)}$. (5)"}, {"title": "C. Convolution in Time-Frequency domain", "content": "The time-invariant linear system of Eq. (1) can be formu-lated in the short-time Fourier transform (STFT) domain as interband and interframe convolution [32]:\n$Y_{f,t} = \\sum_{f'=0}^{F-1} \\sum_{t'=0}^{min(t,T_h-1)} H_{f,f',t'}S_{f',t-t'}$, (6)\nwhere $Y \\triangleq {Y_{f,t}}_{f=0}^{F-1} \\in \\mathbb{C}^{F \\times T_y}$ are the STFT coeffi-cients of the reverberant signal at frequency $f = 0, ..., F - 1$ and time $t = 0, ..., T_y - 1$, $H \\triangleq {H_{f,f',t'}}_{f,f'=0}^{F-1} \\in \\mathbb{C}^{F \\times F \\times T_h}$ is a tridimensional representation of the RIR and $S \\triangleq {S_{f,t}}_{f,t=0}^{F-1} \\in \\mathbb{C}^{F \\times T_s}$ is the STFT of the dry signal.\nAs shown in [32], H can be obtained in closed form from the RIR $h \\in \\mathbb{R}^N$ as:\n$H_{f,f',t'} = \\sum_{m=-N+1}^{N-1} h(t'L - m)W_{f,f'}(m)$, (7)\nwhere N is the STFT window length, L the hop-size and\n$W_{f,f'}(m) = \\frac{1}{N} \\sum_{n=0}^{N-1} w_s(n + m)w_a(n)e^{\\frac{-j2\\pi n (f' - f)}{N}}$. (8)"}, {"title": "III. PROPOSED METHOD", "content": "We propose to supervise the training of a dereverberation deep neural network (DNN) using a conventional reverberation model. The general training procedure is as follows. Given a reverberant signal Y as in the previous section, the DNN outputs a dry signal estimate $\\hat{S} \\triangleq {\\hat{S}_{f,t}}_{f,t=0}^{F-1} \\in \\mathbb{C}^{F \\times T_s}$. In parallel, a reverberation model R synthesizes an approx-imated RIR h from a few reverberation model parameters $\\Theta \\equiv {RT_{60}, \\sigma, V, A}$. Both the estimated dry STFT $\\hat{S}$ and the synthesized RIR $h \\in \\mathbb{R}^{N_h}$ are convolved in a cross-band convolutive model C (see Eq. (10)), to compute an"}, {"title": "B. RIR synthesizer", "content": "The RIR synthesizer aims at synthesizing an RIR for which the late reverberation $h_l$ matches Polack's model, and the direct path $h_d$ is a peak of amplitude 1. To better match Polack's model with our data distribution without changing its energy distribution, and based on preliminary experiments, we decided to synthesize an RIR using the absolute value of the Gaussian distribution used in Polack's model. According to the mean free path property, the direct path peak should be on average positioned at the sample corresponding to the mean free path of the room $n_m$. As stated in [29], the mixing time, corresponding to the beginning of the late reverberation $h_l$, is set at 3 times the mean free path. However, to better align the dry and reverberant signals, we discard the RIR samples before the first peak. Hence, the synthetic RIR becomes:\n$\\mathcal{R}(\\Theta)(n) = \\begin{cases}\n0 & \\text{if } n < 2n_m \\\\\nb(n)e^{-\\frac{3 \\ln(10)}{RT_{60}f_s}n} & \\text{if } n > 2n_m\\\\1 & \\text{otherwise},\n\\end{cases}$ (9)\nwhere b(n) is drawn from a Gaussian distribution $\\mathcal{N}(0,\\sigma^2)$, and $n_m$ corresponds to Eq. (3). In this model, at fixed RT60, $\\sigma^2$ is proportional to the inverse of the direct-to-reverberant ratio (DRR), which has been proven to have great influence on dereverberation performance [33]."}, {"title": "C. Convolutive model and reverberation matching loss", "content": "To better backpropagate the training gradient to the derever-beration model whose output might be in the time-frequency plane, we consider a time-frequency cross-band convolutive model and reverberation matching loss. Given $h = \\mathcal{R}(\\Theta)$, and $\\hat{S}$ the dry speech estimate outputted by the DNN, we define the time-frequency convolutive model as:\n$\\hat{Y}_{f,t} \\triangleq \\mathcal{C}(\\hat{S}, h) = \\sum_{f'=f-F'}^{f+F'} \\sum_{t'=0}^{min(t,T_h)} H_{f,f',t'}\\hat{S}_{f',t-t'}$, (10)\nwith $H_{f,f',m} = \\sum_{m=-N+1}^{2N+1} h(t'L - m)W_{f,f'}(m)$ and the notations in Eq. (10) coinciding to those of Eq. (6-8). Based on [32], we set the number of crossbands F' to 4.\nOur reverberation-matching loss corresponds to the com-monly used mean-squared error estimator for the deconvolu-tion problem. Since this problem can be ill-posed for low-amplitude signals, a regularization term matching the log-magnitudes of the reverberant estimate and ground truth is"}, {"title": "IV. EXPERIMENTS", "content": "We compare our proposed \"reverberation-based weak su-pervision\u201d with a baseline \"metrics-based weak supervision\u201d as implemented in MetricGAN-U."}, {"title": "A. DNN variants", "content": "We assess several variants of our method with FullSubNet (FSN) [7]. The ability of FullSubNet to process complex STFT representations both in the full-band and subband directions is required to be paired with our proposed cross-band convolutive model and reverberation matching loss. It has also been proven to be able to jointly model physical properties of a room and dry speech [21], and to be paired with reverberation-informed training strategies [35]. We also consider the baseline BiLSTM model [36] used as a generator in MetricGAN-U. This model is much simpler as it only allows to processing STFT magnitude masks, and will serve as an indicator for the behaviour of our proposed loss with a less expressive model."}, {"title": "B. Supervision variants", "content": "We considered several supervision variants classified as weak supervision and strong supervision.\nWeak supervision (WS): WS variants include using Polack's model with either\n$\\Theta \\equiv {RT_{60}, \\sigma, V, A}$: all the parameters, including those used to estimate the mixing time.\n${RT_{60}, \\sigma}$: a fixed mixing time set as 20 ms after the peak, corresponding to the mean of all mixing times in the training dataset.\n${RT_{60}}$: a fixed mixing time at 20 ms and a median value of Polack's variance $\\sigma = 0.02$ over the training dataset.\nThis is the least-supervised model and is motivated by realistic scenarios where only the reverberation time can be computed from a reverberant signal [26].\nStrong Supervision: Those variants include using more oracle information such as\nthe exact RIR h as an oracle RIR synthesis model.\nThis variant should be considered as an upper bound for our proposed reverberation-based weak supervision's performance, as it is equivalent to having access to pairs of dry and reverberant signals as supervision.\neach model's original paired training loss as supervision.\nBiLSTM is trained using the mean squared error between dry and dereverberated magnitude spectra. FSN is trained to minimize the euclidean distance between its estimated and the ground-truth ideal complex ratio mask (CRM).\nWe also consider MetricGAN-U's metrics-based weak super-vision as a baseline. It corresponds to the BiLSTM model trained with the weak supervision of the SRMR metric."}, {"title": "C. Miscellaneous configurations", "content": "As in the original FullSubNet, 49151 sample excerpts (around 3 s at 16 kHz) reverberant audios are processed in the STFT domain using a 512-sample Hann window with an overlap of 50%. We use a learning rate decay based on the training loss on a validation set, and early stopping based on the SISDR metric on a validation set."}, {"title": "D. Dataset", "content": "Similarly to [6], [21], we simulated a training dataset by dynamically convolving dry speech signals with simulated RIRs. The dry speech signals are randomly sampled from the close-talking microphone recordings in the WSJ1 dataset [37]. The training set is composed of a total of 73 hours of recordings split into 60307 audio excerpts. The simulated RIR dataset consists of 32,000 RIRs drawn from 2000 rooms simulated using the image source method implemented in the pyroomacoustics library [38]. Room dimensions and RT60 are uniformly sampled in the respective ranges of [5, 10] \u00d7 [5, 10] \u00d7 [2.5, 4] m\u00b3, and [0.2,1.0] s. The source-microphone distance is uniformly distributed in [0.75, 2.5] m, and both source and microphone are at least 50 cm from the walls. At training time, we use a dynamic mixing procedure consisting of randomly selecting a dry signal and RIR pair. In order to align the dry signal target and the direct-path, the samples before the direct path are discarded and it is normalised (so that the direct-path is of amplitude 1). This does not change the RIR distribution and compensates for the delay induced by the direct-path to match the RIR synthesis procedure."}, {"title": "V. RESULTS AND DISCUSSION", "content": "We evaluate the performance of our proposed reverberation-based weak supervision for the dereverberation task on unseen speakers from WSJ1 (Hub and Spokes S1 to S4) and rooms. The performance is evaluated using the Scale-Invariant Signal-to-Noise ratio (SISDR) [39], Extended Short-Time Objective Intelligibility (ESTOI) [40], Wide-Band Perceptual Evalua-tion of Speech Quality (WB-PESQ) [41] and SRMR [14] metrics. To assert the statistical significance of our result analysis despite the high measured variances, we opted for a non-parametric Wilcoxon test with a significance level of 0.001 for the null hypothesis to be rejected. The results are presented in Table I. The line denoted \"Reverberant\" corresponds to unprocessed signals and the best significant weak supervision variant for each metric and dereverberation model is highlighted. All of the proposed methods show an improvement of the SISDR, ESTOI and WB-PESQ metrics, meaning that they can successfully dereverberate speech. The baseline (BiLSTM + SRMR) excels in terms of SRMR, but this performance comes at the cost of its SISDR and ESTOI results, which are degraded compared to the reverberant input. This result confirms the main drawback of metrics-based weak supervision, in the sense that it tends to solely optimize the metric it is trained on, disregarding the others. Indeed, all our proposed methods perform better than the baseline on all other metrics than SRMR. This demonstrates the superiority of reverberation-based weak supervision over metrics-based weak supervision. The best-performing method FullSubNet benefits from strong supervision, both when trained on its original complex masking loss or using the oracle RIR. On the other hand, the less-complex BiLSTM widely benefits from weak supervision, and performs better in terms of SISDR when weakly supervised by Polack's model than when it has access to the ground-truth RIR h. For this model, the weakest supervision by RT60 yields not only superior results to other weak-supervision variants for all metrics except SRMR, but even improves the model's SISDR performance above its original supervision based on magnitude spectra. This is due to the BiLSTM's design, which is meant only for a spectral magnitude masking loss, without alleviating the STFT phase. Hence, when the estimated dry signal is reverberated using a ground-truth RIR h, the estimated reverberant STFT phase is perturbed to a large extent, whereas reverberation by Polack's model yields a phase that is closer to the complex circular Gaussian model at the core of BiLSTM's design. Another noticeable result occurs for both models in the reverberation-based weakly supervision by Polack's model. Comparing reverberation-weak supervision approaches, we remark that they perform better in terms of SISDR when having no access to the acoustic parameters used to estimate the mixing time and Polack's model \u03c3. Hence, fixing \u03c3, V and A is equivalent to making the DRR only dependant on the RT60, which can be easily computed from reverberant speech [26], and seems to regularize our proposed training procedure for dereverberation when evaluated with synthetic RIRs."}, {"title": "VI. CONCLUSION", "content": "We have proposed a novel approach for weakly-supervised speech dereverberation, by training a deep neural network to predict a dry estimate from a reverberant signal, such that a reverberation model applied on the dry estimate matches its reverberant input. Results demonstrate the superiority of our reverberation-based weak supervision over metrics-based weak supervision. This method opens the path to a variety of dereverberation techniques for data-scarce scenarios and various signals such as music. Future work will be dedicated to leveraging a more powerful RIR synthesis model that can estimate the RT60 from reverberant signals only, and to ex-tending this work to weakly-supervised generative approaches for dereveberation to better model the probabilistic RIR model."}]}