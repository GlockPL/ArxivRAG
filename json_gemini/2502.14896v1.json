{"title": "A Comprehensive Survey on Concept Erasure in Text-to-Image Diffusion Models", "authors": ["Changhoon Kim", "Yanjun Qi"], "abstract": "Text-to-Image (T2I) models have made remarkable progress in generating high-quality, diverse visual content from natural language prompts. However, their ability to reproduce copyrighted styles, sensitive imagery, and harmful content raises significant ethical and legal concerns. Concept erasure offers a proactive alternative to external filtering by modifying T2I models to prevent the generation of undesired content. In this survey, we provide a structured overview of concept erasure, categorizing existing methods based on their optimization strategies and the architectural components they modify. We categorize concept erasure methods into fine-tuning for parameter updates, closed-form solutions for efficient edits, and inference-time interventions for content restriction without weight modification. Additionally, we explore adversarial attacks that bypass erasure techniques and discuss emerging defenses. To support further research, we consolidate key datasets, evaluation metrics, and benchmarks for assessing erasure effectiveness and model robustness. This survey serves as a comprehensive resource, offering insights into the evolving landscape of concept erasure, its challenges, and future directions.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Text-to-Image (T2I) models significantly enhance their ability to generate high-quality images that align closely with user-provided textual prompts [Rombach et al., 2022]. These models empower users with unprecedented creative freedom, enabling the production of sophisticated and photorealistic images without requiring professional expertise. However, as T2I models are trained on vast text-image datasets scraped from the internet, they inherit both the strengths and biases of their training data. Specifically, they can replicate copyrighted artistic styles, explicit content, and private or sensitive visual concepts [Somepalli et al., 2023; Somepalli et al., 2022], raising ethical and legal concerns. The unconstrained generative capabilities of these models can be exploited for malicious purposes, such as creating misleading or harmful content, generating deepfake imagery, or manipulating public opinion.\nTo address these concerns, researchers have developed post-hoc safety mechanisms that integrate security protocols into the T2I model pipeline [Kim et al., 2023; Fernandez et al., 2023; Kim et al., 2021; Nie et al., 2023; Wen et al., 2023]. These methods include watermarking, model attribution, and forensic tracking techniques that help identify the sources responsible for AI-generated content. While these approaches provide valuable forensic tools for mitigating misuse, they remain reactive solutions that only take effect after potentially harmful images have been generated and disseminated.\nGiven the constraints of reactive methods, an emerging line of research explores proactive solutions, specifically concept erasure, which aims to systematically remove targeted concepts from a model's generative capability. As illustrated in Fig. 1, concept erasure techniques suppress a model's ability to generate protected or undesired content, ensuring that even when explicitly prompted, the model does not produce outputs containing erased concepts. These methods operate by either modifying the model's internal components or intervening in the inference process, preventing the unauthorized reproduction of sensitive or restricted concepts. Other related approaches include image editing, which modifies attributes within a given image. In contrast, concept erasure enforces persistent modifications that prevent the generation of targeted concepts across all inputs [Arad et al., 2023].\nResearch on securing T2I models has gained significant attention, leading to comprehensive surveys on various aspects of generative model security. Recent studies from [Zhang et al., 2024a] and [Truong et al., 2024] focus on surveying adversarial attacks and defenses, with the former specifically addressing T2I diffusion models and the latter examining threats in diffusion-based image generation. While these works offer valuable insights, they do not provide a focused analysis of concept erasure in T2I models. The absence of a dedicated survey makes it difficult to systematically understand and compare existing concept erasure techniques.\nTo bridge this gap, we present a structured and comprehensive survey on concept erasure techniques in T2I diffusion models, offering the following key contributions:\n\u2022 Comprehensive Taxonomy of Concept Erasure\nMethods. We systematically classify existing concept erasure techniques based on their optimization strategies and the model components they modify, as illustrated in Fig. 2. This categorization provides a structured taxonomy for understanding different concept erasure approaches.\n\u2022 Analysis of Adversarial Attacks and Defense Mechanisms. We investigate adversarial attacks designed to circumvent concept erasure, categorizing them based on access to the T2I model's internal components. Additionally, we explore emerging defense strategies aimed at enhancing the robustness of concept erasure techniques.\n\u2022 Evaluation Benchmark for Concept Erasure. We consolidate widely used datasets and metrics for assessing erasure effectiveness, model fidelity, and resilience against adversarial attacks, providing a standardized benchmark for future research and evaluation.\n\u2022 Future Research Directions and Open Challenges. We introduce research opportunities in concept erasure, including improving generalization across different modalities such as text-to-video, developing fine-grained benchmarks to evaluate erasure effectiveness while capturing unintended distortions in related attributes, and addressing novel adversarial threats.\nTo provide a comprehensive understanding of concept erasure in T2I models, we structure this paper as follows. Sec. 2 provides background knowledge on the architecture of T2I models and introduces key technical concepts relevant to concept erasure. Sec. 3 categorizes concept erasure techniques based on their optimization methods and the model components they modify. Sec. 4 examines adversarial attacks that attempt to circumvent concept erasure, along with emerging defense strategies designed to enhance robustness. Sec. 5 reviews commonly used evaluation metrics and datasets for benchmarking concept erasure methods. Based on these findings, Sec. 6 discusses future research directions, open challenges, and opportunities in this field, followed by conclusions in Sec. 7."}, {"title": "2 Backgrounds", "content": "This section presents an overview of the Text-to-Image (T2I) diffusion model with a particular focus on Stable Diffusion (SD) [Rombach et al., 2022]. As shown in Fig. 1, SD comprises three main components: a vision decoder for reconstructing images from latent representations, a latent diffusion model for iterative denoising, and a conditional text encoder that transforms textual prompts into conditioning vectors. We outline both the training and inference mechanisms of SD, which are essential for understanding how concept erasure techniques modify key model components or inference steps to suppress undesired concepts."}, {"title": "2.1 Three Components of Stable Diffusion", "content": "Stable Diffusion comprises three primary components:\n(1) Image Autoencoder. The model leverages a pre-trained autoencoder to compress high-dimensional image data into a low-dimensional latent representation. The encoding network $\\mathcal{E}(\\cdot)$ maps an image x to a latent variable $z = \\mathcal{E}(x)$, and the decoding network $\\mathcal{D}(\\cdot)$ reconstructs the image from the latent space such that $\\mathcal{D}(z) = x \\approx x$. This design ensures effective data compression while minimizing reconstruction error, preserving essential image features critical for generative tasks.\n(2) Latent Diffusion Model. The core generative process in SD is governed by a U-Net-based Latent Diffusion Model (LDM) that progressively refines noisy latent representations toward high-fidelity outputs. The training objective is formulated as:\n$\\mathcal{L}_{SD} = \\mathbb{E}_{\\eta \\sim \\mathcal{N}(0,1), z, c, t} [||\\eta - \\Phi_{\\theta}(z_t, c) ||^2],$ (1)\nwhere c is the text embedding derived from the input prompt and integrated via cross-attention, t denotes the diffusion timestep, $\\eta$ is a noise vector sampled from a standard Gaussian distribution $\\mathcal{N}(0, 1)$, and $z_t$ is the noisy latent variable at timestep t. The LDM $\\Phi_{\\theta}$, parameterized by $\\theta$, is trained to predict and remove noise at each step, progressively refining the latent variable along the diffusion trajectory.\n(3) Conditional Text Encoding. The model employs a text encoder to transform user-provided text prompts into conditioning vectors, enabling fine-grained control over the generation process. Specifically, the textual prompt y is embedded as $c = \\mathcal{E}_{txt}(y)$, where $\\mathcal{E}_{txt}$ typically textual encoder of CLIP [Radford et al., 2021]. These text embeddings are integrated through the cross-attention layers within the latent diffusion model [Rombach et al., 2022], allowing the textual context to dynamically influence each denoising step."}, {"title": "2.2 Inference in Stable Diffusion", "content": "Classifier-free guidance [Ho and Salimans, 2022] enhances the conditionality of the image synthesis process during the inference phase of SD. The process starts with initializing latent representations $z_T$ sampled from a Gaussian distribution. The denoising trajectory is steered by classifier-free guidance, which modifies the denoising function as follows:\n$\\Phi_{\\theta}(z_t, c) = \\Phi_{\\theta}(z_t, \\phi) + \\alpha (\\Phi_{\\theta}(z_t, c) - \\Phi_{\\theta}(z_t, \\phi)),$ (2)\nwhere $\\Phi_{\\theta}(z_t, c)$ and $\\Phi_{\\theta}(z_t, \\phi)$ represent the conditioned and unconditioned latent noises, respectively. The guidance scale $\\alpha > 1$ amplifies the influence of the conditioned path, embedding the textual information into the generative process. Iterative refinement reduces noise through sequential calculations of $z_{t-1} = \\Phi_{\\theta}(z_t, c)$, progressing until t = 0. The final coherent image representation $z_0$ is transformed into the output image $\\hat{x}$ by the decoder, $\\hat{x} = \\mathcal{D}(z_0)$. The T2I generation process can be succinctly expressed as $SD(y) = \\mathcal{D}(\\Phi_{\\theta}(z_T, \\mathcal{E}_{txt}(y)))$."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Concept Erase", "content": "Concept erasure in T2I models, particularly SD, involves modifying model parameters or adjusting inference procedures to selectively suppress or eliminate the generation of specific, unwanted concepts. This technique is crucial for addressing the risks associated with generating potentially harmful or copyrighted content in the model's outputs. The primary goal of concept erasure is to condition the model so that it does not produce images corresponding to undesired prompts. For instance, to erase the influence of a copyrighted artist's style, the model is adjusted such that a prompt like \"A painting in the style of [artist]\" results in outputs that bear no resemblance to that artist's work. This objective can be succinctly stated as $SD(y_{erase}) \\nsubseteq \\{x_{erase}\\}$, where $y_{erase}$ is the prompt that includes the concept to be erased, and $x_{erase}$ denotes any image typically representative of that concept.\nConcept erasure can be achieved through various optimization methods. And in each optimization group, methods can get classified by which components are modified to achieve the goal. This section, therefore, categorizes existing methods by their optimization strategies and components they modify. A comprehensive taxonomy with detailed explanations is provided in Tab. 1. For a detailed explanation of each component and inference stage, please refer to Sec. 2."}, {"title": "3.2 Fine-tuning Methods", "content": "Fine-tuning is one of the most intuitive methods to erase undesired concepts from the T2I models. These methods iteratively optimize weights of component of Stable Diffusion (SD) to match erasing concept to its designed corresponding concept. For example, match representation of erasing concept $C_{erase}$, \u201cVan Gogh\u201d to $C_{target}$, \u201cArtist\u201d. We categorize this by which component is updated to erase concept.\nFine-tuning Latent Diffusion Model. To edit SD's Latent Diffusion Models (LDM) component, fine-tuning-based concept erasure methods selectively update model parameters to remove undesired concepts while preserving overall generative capabilities. These approaches can get categorized further based on the specific LDM's model components they modify, as different architectural elements govern distinct aspects of the image synthesis process.\nA general formulation of fine-tuning for concept erasure in LDMs is as follows:\n$\\min_{\\theta} || \\Phi_{\\theta}(z_t, C_{erase}) - \\Phi_{\\theta}(z_t, C_{target}) ||^2,$ (3)\nwhere $\\Phi_{\\theta}$ represents the pretrained LDM model, and $\\tilde{\\Phi}$ denotes the fine-tuned LDM with updated parameters $\\theta$. The terms $C_{erase}$ and $C_{target}$ correspond to the text embeddings $\\mathcal{E}_{txt}(y_{erase})$ and $\\mathcal{E}_{txt}(y_{target})$, respectively. The objective enforces alignment between the erased concept $C_{erase}$ and the target concept $C_{target}$, ensuring that the model learns to replace undesired representations in the latent space.\nOne class of methods targets to update the cross-attention layers in LDM, which determine how textual prompts influence the generated visual output. FMN [Zhang et al., 2023a] fine-tunes cross-attention module to re-steer attention mechanisms to eliminate certain concepts while maintaining generative quality. AC [Kumari et al., 2023] introduces an anchor-based fine-tuning strategy, aligning erased concepts with broader semantic categories to suppress their stylistic or object-based representations.\nAnother set of methods fine-tunes the LDM backbone, directly modifying the denoising process to eliminate specific concepts from the model's latent representations. ESD [Gandikota et al., 2023a] fine-tunes the LDM to match the noise prediction of $C_{erase}$ to that of $C_{target}$, ensuring erased concepts remain irrecoverable. This optimization is guided by classifier-free guidance (Eq. (2)), which directs the model's learning signal. During fine-tuning, ESD modifies either cross-attention or non-attention modules to reinforce robustness against adversarial prompts. SALUN [Fan et al., 2024] applies saliency-guided erasing, selectively updating high-impact weights to maximize forgetting while minimizing unintended side effects. DT [Ni et al., 2023b] conditions the model to generate structurally degraded outputs when prompted with erased concepts, effectively neutralizing their representation in the latent space.\nSeveral approaches incorporate geometric constraints, continual learning, or robustness against personalization to enhance fine-tuning methods. Geom-Erasing [Liu et al., 2023b] removes implicit visual concepts, such as watermarks and hidden signals, by introducing geometric constraints that disrupt structured artifacts without degrading unrelated content.\nSA [Heng and Soh, 2023] leverages continual learning techniques, employing regularization-based forgetting to erase targeted concepts while preserving generalization and mitigating catastrophic forgetting. Lastly, IMMA [Zheng and Yeh, 2023] adopts a preventive fine-tuning approach, modifying model weights preemptively to resist unauthorized adaptation via fine-tuning techniques, thereby preventing the downstream personalization of diffusion models for unethical or restricted purposes. Additionally, SPM [Lyu et al., 2024] introduces a lightweight, one-dimensional adapter that enables precise and transferable concept erasure across different diffusion models.\nFine-tuning CLIP. Fine-tuning latent diffusion model showed great success for concept erasure, plus their interpretability to understand erasing concepts. However, to apply these methods to the updated LDM or other structured LDM, they have to get changed or redesigned since these methods are designed only to specific LDM. As one of the strength of fine-tuning CLIP model for concept erase, the CLIP text encoder whose concepts get erased, $\\mathcal{E}_{txt}$, can transfer to the other structure LDM as long as they still depends on CLIP model. To fine-tune the CLIP model, [Poppi et al., 2023] generates a dataset composed of quadruplets of safe and unsafe text-image pairs (ViSU dataset). Similarly, [Liu et al., 2024] generates a dataset composed of safe and unsafe text pairs (CoPro dataset), where unsafe prompts are synthesized using a large language model, and safe counterparts are created by removing harmful concepts while preserving context. After generating datasets, this study finetunes CLIP based on designed loss inspired by contrastive loss. This finetuned CLIP text encoder, $\\mathcal{E}'_{txt}$, leads $\\mathcal{E}'_{txt}(y_{erase}) \\approx \\mathcal{E}_{txt}(y_{target})$. Even if $y_{erase}$ is given to the SD model, it will generate images that are not aligned with concept to erase, $SD'(y_{erase}) \\approx x_{target}$, where $SD'(y) = \\mathcal{D}(\\Phi_{\\theta}(z_T, \\mathcal{E}'_{txt}(y)))$. These methods offer better adaptability than LDM fine-tuning approaches. However, this approach requires a carefully curated and extensive dataset to fine-tune the existing text encoder, unlike other concept erasure methods."}, {"title": "3.3 Closed-form Model Editing Methods", "content": "Fine-tuning methods are intuitive and effective for modifying SD. However, they require iterative optimization through gradient descent, making them computationally expensive and time-consuming. Moreover, fine-tuning introduces risks of overfitting and unintended degradation of the model's capabilities, necessitating careful hyperparameter tuning. In contrast, closed-form solutions provide a direct mathematical update to model parameters without iterative training. This enables faster application of model modifications while eliminating the need for extensive hyperparameter tuning.\nA general formulation of closed-form model editing follows a least squares based optimization:\n$\\min_{W} || W C_{erase} - W_0 C_{target} ||^2,$ (4)\nwhere W denotes the editable parameters of the model, primarily the key and value projection matrices in the cross-attention module, while $W_0$ represents the pre-trained weights. Closed-form solutions directly compute the optimal update for W directly, enabling efficient modification while preserving overall model coherence. To enhance stability and prevent unintended interference, regularization terms are incorporated into Eq. (4), balancing alignment with the target concept while minimizing deviations from the original model.\nNotable closed-form methods include ReFACT [Arad et al., 2023], which applies a low-rank memory update to the CLIP text encoder, ensuring persistent factual knowledge updates while minimizing interference by unrelated concepts. TIME [Orgad et al., 2023] modifies the LDM's cross-attention projection matrices, aligning implicit assumptions in generated images with desired attributes. Unified Concept Editing (UCE) [Gandikota et al., 2023b] introduces a closed-form method for simultaneous multi-concept editing in T2I models, enabling scalable erasure, moderation, and debiasing by modifying cross-attention projections while minimizing interference against unedited concepts. MACE [Lu et al., 2024] further refines cross-attention weights by integrating adapter-based concept erasure, achieving precise removal of up to 100 concepts in a more memory-efficient manner.\nOther recent works have explored additional extensions of closed-form model editing. EMCID [Xiong et al., 2024] introduces a two-stage framework combining self-distillation and closed-form updates, scaling to over 1,000 concurrent modifications. MUNBa [Wu and Harandi, 2024] formulates concept erasure as a Nash bargaining problem, deriving an equilibrium update that balances forgetting and preservation objectives.\nOverall, closed-form methods offer a computationally efficient alternative to fine-tuning by providing direct parameter updates. These methods ensure fast and stable modifications."}, {"title": "3.4 Inference-time Intervention Methods", "content": "Both fine-tuning and closed-form methods demonstrate intuitive and effective performance in concept erasure. Fine-tuning LDM and closed-form parameter updates enable direct control over the generative process. Among them, CLIP fine-tuning provides a flexible plug-and-play concept erasure solution for T2I models that rely on CLIP encoders.\nHowever, these approaches require weight modifications to SD components, limiting their adaptability and deployment efficiency. In contrast, inference-stage control methods enable concept erasure without modifying SD's parameters. These methods instead intervene at the inference stage by modifying classifier-free guidance (Eq. (2)), editing textual embeddings c, or sanitizing input prompts y using large language models.\nModifying Classifier-Free Guidance. A core approach for inference-stage concept erasure is adjusting classifier-free guidance (Eq. (2)) to steer the generative process away from undesired content. Safe Latent Diffusion [Schramowski et al., 2023] modifies the classifier-free guidance signal in SD's denoising process, redirecting latent activations to prevent the generation of unsafe concepts. Anti-Memorization Guidance [Chen et al., 2024] introduces despecification and dissimilarity constraints that adjust classifier-free guidance dynamically, ensuring that models do not overfit to specific training instances or regenerate memorized images. Both methods leverage guidance re-weighting strategies to suppress undesired features while maintaining high image quality.\nEditing Textual Embeddings. Rather than modifying the diffusion process, another class of inference-stage methods operates on text embeddings to enforce concept erasure. SAFREE [Yoon et al., 2024] applies subspace projection and adaptive re-attention to detect and suppress undesirable content within CLIP text embeddings before they get used for image synthesis. Similarly, Content Suppression in T2I Models [Li et al., 2024] employs soft-weighted regularization to refine textual embeddings during sampling, ensuring that forbidden concepts do not appear in generated outputs. These methods enable fine-grained, token-level control over the generation process while preserving overall model flexibility.\nSanitizing Input Prompts Using LLMs. A third category of inference-stage control leverages Large Language Models (LLM) to preprocess prompts, ensuring that user inputs do not contain prohibited content before the diffusion process begins. ORES [Ni et al., 2023a] employs LLM-based query rewriting to automatically sanitize user prompts, replacing restricted terms with conceptually aligned yet safe alternatives. On the other hand, GuardT2I [Yang et al., 2024] detects adversarial prompts that attempt to bypass safety mechanisms, leveraging a fine-tuned LLM to analyze and reject unsafe queries before image generation.\nBy operating externally to the SD components and modifying only the diffusion process at inference, these methods remain model-agnostic and scalable across different T2I model architectures."}, {"title": "4 Robustness", "content": "Concept erasure methods aim to prevent T2I models from generating undesired concepts. For instance, when a model undergoes fine-tuning to erase a copyrighted artist's style (e.g., Van Gogh), prompts such as \"cypresses by Van Gog\" should ideally produce outputs that bear no resemblance to the artist's original style, as shown in Fig. 1.\nHowever, studies demonstrate that minor perturbations to the prompt-such as the addition of unrelated tokens or imperceptible modifications-can effectively circumvent concept erasure, allowing target T2I model to regenerate removed concepts. In some cases, even semantically meaningless inputs would exploit the underlying representations within SD models to reconstruct erased concepts.\nWe categorize adversarial attacks based on whether they require access to the Latent Diffusion Model (LDM) of Stable Diffusion (SD). Following this, we also discuss existing defense strategies against such attacks."}, {"title": "4.1 Adversarial Attacks", "content": "Adversarial attacks against T2I models manipulate prompt inputs or textual embeddings to reconstruct erased concepts. Given a concept-erased model, an adversarial prompt is optimized to elicit outputs that closely resemble those generated by an unaltered model. A general formulation of adversarial attacks against concept erasure is:\n$\\underset{Z_{adv.}, Or \\space y_{adv.}}{argmin} ||SD'(z_{adv.}, y_{adv.}) - x_{erase}||,$ (5)\nwhere $SD'$ denotes the concept-erased Stable Diffusion model, and $z_{adv.}$ and $y_{adv.}$ represent adversarial latents and prompts, respectively, that restore the erased concept within $SD'$.\nAttacks with LDM Access These attacks access LDM's latent representations, enabling adversaries to design strategies that systematically bypass concept erasure mechanisms. While such access is rare in real-world scenarios, these attacks remain essential for stress-testing erasure techniques.\nOne class of such attacks invert concept erasure transformations to recover removed concepts. Circumventing Concept Erasure [Pham et al., 2024] optimizes adversarial embeddings via inversion using LDM, successfully retrieving erased concepts. Similarly, Concept Arithmetics [Petsiuk and Saenko, 2024] reconstructs erased concepts by manipulating latent representations and leveraging semantic composition to synthesize forbidden attributes through linear combinations of concept embeddings.\nAnother category of attacks exploits prompt tuning and adversarial optimization to bypass safety-driven concept removal. P4D [Chin et al., 2024] systematically tunes adversarial prompts by iteratively refining textual inputs based on model feedback, demonstrating that safety filters and erasure techniques remain vulnerable to adversarial prompt engineering. Additionally, UnlearnDiffAtk [Zhang et al., 2023b] specifically targets models trained with safety-driven unlearning by crafting prompts that reverse-engineer unlearning constraints, showing that adversarially optimized inputs can still induce the generation of prohibited content.\nAlthough these attacks require privileged access to LDM, they provide valuable insights into the transferability of adversarial prompts across models. By assessing how an attack generalizes to different versions of SD and CLIP, these studies reveal broader vulnerabilities in concept erasure methods.\nAttacks without LDM Access Even without LDM access, adversarial methods effectively bypass concept erasures by manipulating prompt or textual embeddings, without interacting with the diffusion model's internal denoising process. PEZ [Wen et al., 2024] formulates a discrete optimization problem to recover a text prompt that closely aligns with a given erased concept image, $x_{erase}$, leveraging CLIP's vision-language similarity for optimization without requiring access to the underlying diffusion model. Similarly, MMA-Diffusion [Yang et al., 2023], like PEZ, employs a surrogate model for adversarial attacks, operating within CLIP's text encoder embedding space or its multi-modal space. By optimizing adversarial prompts and introducing imperceptible perturbations, MMA-Diffusion successfully bypasses safety mechanisms, demonstrating its effectiveness in both text-to-image generation and image editing tasks.\nRing-A-Bell [Tsai et al., 2023] proposes a model-agnostic red-teaming framework that reconstructs erased concepts by optimizing adversarial prompts using a genetic algorithm in the text embedding space. Likewise, RIATIG [Liu et al., 2023a] employs a genetic optimization strategy to iteratively refine adversarial queries, enabling content moderation evasion across multiple T2I models. Meanwhile, UPAM [Peng et al., 2024] utilizes gradient-based prompt tuning combined with semantic-enhancing learning to systematically generate adversarial prompts capable of bypassing API-level safety mechanisms.\nThese approaches expose a fundamental vulnerability in concept erasure techniques-even without LDM access, adversarial optimization in the prompt space alone can effectively reconstruct erased content. This underscores the need for stronger prompt filtering mechanisms and adversarially resilient diffusion models to prevent circumvention through external manipulations."}, {"title": "4.2 Defensive Methods", "content": "To strengthen concept-erased models against adversarial prompt attacks (Sec.4.1), recent research integrates adversarial training into concept erasure techniques, enhancing robustness while preserving generation fidelity. These defenses align with the categorization of concept erasure methods (Fig.2, Tab.1), targeting distinct architectural components. By addressing vulnerabilities across these optimization spaces, defensive methods improve the reliability of concept erasure while maintaining the expressiveness of T2I models.\nR.A.C.E.[Kim et al., 2024] employs a single-timestep adversarial attack to efficiently identify vulnerabilities in SD and leverages this attack for adversarial fine-tuning, significantly reducing attack success rates in both white-box and black-box settings. Receler[Huang et al., 2023] integrates a lightweight robust eraser within cross-attention layers of LDM, utilizing concept-localized regularization and adversarial prompt learning to improve robustness against paraphrased attacks while preserving non-target concepts. AdvUnlearn [Zhang et al., 2024b] advances the robust erasing paradigm by applying adversarial training on the CLIP text encoder, enhancing prompt-space robustness while maintaining the alignment between textual prompts and image generation. RECE [Gong et al., 2024] extends closed-form concept erasure by incorporating adversarial fine-tuning on matrix-modified cross-attention layers, efficiently discovering and erasing adversarial embeddings in a fully closed-form manner. Additionally, in inference-stage control, SAFREE [Yoon et al., 2024] demonstrates strong robustness compared to other defense methods, despite not employing adversarial training.\nBy integrating adversarial robustness into concept erasure, these methods significantly improve the reliability of T2I safety mechanisms. However, challenges remain in optimizing the balance between robustness, generation quality, and computational efficiency, warranting further exploration in adaptive adversarial training strategies for future diffusion models."}, {"title": "5 Evaluation", "content": "Evaluating concept erasure methods is essential for quantifying their effectiveness and enabling fair comparisons across different approaches. This section reviews widely adopted metrics and datasets to assess both the success of concept removal and the preservation of general model capabilities."}, {"title": "5.1 Metrics", "content": "Concept erasure methods are typically evaluated in two key aspects: erasure effectiveness and model fidelity.\nThe Erasure Success Rate (ESR) measures how effectively a method removes a target concept. This is commonly assessed using classification accuracy, where a pre-trained classifier determines whether the erased concept remains present in the generated images. Formally, ESR is defined as:\n$ESR = \\frac{1}{N} \\sum_{i=1}^{N} 1 (f (SD' (y_i)) = c_{erase}),$ (6)\nwhere $y_i$ represents the prompt, $c_{erase}$ is the erased concept, f is a classifier, and N is the total number of prompts. Lower ESR values indicate more successful erasure. ESR can also be extended to evaluate robustness against adversarial attacks by replacing standard prompts $y_i$ with adversarially optimized prompts $y_{adv.}$ or modifying latent variables $z_{adv.}$, allowing an assessment of how well the model resists attempts to reconstruct erased concepts.\nTo ensure that erasure does not degrade the model's ability to generate non-erased content, model fidelity is evaluated by measuring both image quality and text-image alignment before and after concept removal. Fr\u00e9chet Inception Distance (FID) [Heusel et al., 2018] is widely used to quantify changes in the perceptual quality of generated images. In addition to image quality, maintaining alignment between textual prompts and generated outputs is crucial. CLIP score [Hessel et al., 2021] is commonly employed for this purpose, providing a similarity measure between the generated image and its corresponding textual prompt. Furthermore, ESR can also serve as a fidelity metric by computing it on prompts unrelated to the erased concept, denoted as $y_{non-erase}$."}, {"title": "5.2 Datasets", "content": "Dataset selection depends on the nature of the concepts being erased, with commonly used datasets categorized according to their evaluation objectives. For assessing NSFW content removal, the I2P dataset [Schramowski et al., 2022], which consists of 4,703 real-world user-generated prompts, is widely employed. Object concept removal is typically evaluated using structured prompts such as \u201cA photo of [object class]\u201d, enabling controlled experiments on whether erased objects still appear in generated images. Artistic style erasure often relies on ESD's artist prompt dataset [Gandikota et al., 2023a], which provides standardized prompts referencing specific artistic styles.\nTo evaluate model fidelity, the COCO dataset [Lin et al., 2015] is commonly used. This dataset enables FID-based image quality assessment and supports CLIP score evaluation for measuring text-image alignment. Beyond standard datasets, robustness evaluation requires datasets explicitly designed for testing adversarial vulnerabilities. For example, the MMA-Diffusion benchmark [Yang et al., 2023] and Ring-A-Bell dataset [Tsai et al., 2023] feature adversarial prompts designed to evade concept erasure and systematically test its vulnerabilities.\nTogether, these metrics and datasets establish a comprehensive framework for evaluating concept erasure, ensuring that methods are assessed not only for their effectiveness in removing targeted concepts but also for their ability to maintain generative quality and resist adversarial attacks."}, {"title": "6 Future Research Directions", "content": "Concept erasure techniques for T2I models have demonstrated promising results in mitigating the generation of undesired content. However, several open challenges remain. In this section, we outline key future research directions that can further advance the development of responsible and robust generative models.\nExtending to Other Modalities: While this survey primarily focuses on concept erasure for T2I models, extending these techniques to other generative modalities such as text-to-video and text-to-audio-remains an open challenge. Despite structural similarities, these modalities introduce additional complexities, such as temporal consistency in videos and waveform coherence in audio. Understanding how concept erasure methods generalize across different modalities and identifying modality-specific adaptations will be crucial for developing responsible generative AI systems.\nDesigning More Comprehensive Benchmarks: Existing evaluations primarily assess erasure effectiveness by detecting the presence of erased concepts in generated images. However, the broader impact of concept erasure on non-target concepts remains under-studied. For instance, erasing the concept of \"Van Gogh\" may inadvertently alter representations of other Impressionist artists. Future benchmarks should move beyond dataset-level assessments and introduce fine-grained, concept-level evaluations that capture unintended effects on related visual attributes. Establishing standardized, interpretable, and reproducible benchmarks will enable more rigorous comparisons across different erasure techniques.\nEnhancing Robustness Against Adversarial Attacks: Current concept erasure methods remain vulnerable to adversarial attacks that exploit weaknesses. While various defensive strategies have been proposed, adversarial attacks continue to evolve, exposing new vulnerabilities. Future work should focus on developing more robust and adaptive defenses that generalize across different attack strategies. This may include integrating adversarial training, certifiable robustness techniques, and multi-modal alignment to mitigate adversarial circumvention while preserving model fidelity.\nBy addressing these challenges, future research can contribute to the development of more responsible, robust, and generalizable concept erasure techniques for T2I models and beyond."}, {"title": "7 Conclusion", "content": "Concept erasure in Text-to-Image (T2I) models is crucial for ensuring ethical and legal compliance in generative AI. This survey categorizes existing approaches based on their optimization strategies and modified model components, covering fine-tuning, closed-form solutions, and inference-time interventions. We also discuss adversarial attacks and emerging defenses. Despite progress, challenges remain in balancing erasure effectiveness, model fidelity, and adversarial robustness. Expanding concept erasure to other modalities, refining evaluation benchmarks, and developing stronger defenses are key directions for future research. We hope this survey serves as a foundation for advancing responsible and secure generative AI."}]}