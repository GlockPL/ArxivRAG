{"title": "Neurosymbolic AI for Enhancing Instructability in Generative AI", "authors": ["Amit Sheth", "Vishal Pallagani", "Kaushik Roy"], "abstract": "Generative AI, especially via Large Language Models (LLMs), has transformed content creation across text, images, and music, showcasing capabilities in following instructions through prompting, largely facilitated by instruction tuning. Instruction tuning is a supervised fine-tuning method where LLMs are trained on datasets formatted with specific tasks and corresponding instructions. This method systematically enhances the model's ability to comprehend and execute the provided directives. Despite these advancements, LLMs still face challenges in consistently interpreting complex, multi-step instructions and generalizing them to novel tasks, which are essential for broader applicability in real-world scenarios. This article explores why neurosymbolic AI offers a better path to enhance the instructability of LLMs. We explore the use a symbolic task planner to decompose high-level instructions into structured tasks, a neural semantic parser to ground these tasks into executable actions, and a neuro-symbolic executor to implement these actions while dynamically maintaining an explicit representation of state. We also seek to show that neurosymbolic approach enhances the reliability and context-awareness of task execution, enabling LLMs to dynamically interpret and respond to a wider range of instructional contexts with greater precision and flexibility.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models such as OpenAI's GPT-4 that is used by ChatGPT and Meta's Llama have demonstrated unprecedented capabilities in following natural language instructions through prompting. These models can be adapted to a broad spectrum of tasks by simply receiving appropriately structured prompts, showcasing their versatility and utility across various domains. For example, LLMs can successfully generate programming code from descriptions of desired functionalities or craft well-structured essays based on outlined themes, showcasing their adaptability and precision in tasks with clear, well-defined goals. However, despite their impressive performance, the current teaching paradigm in LLMs encounters several challenges. These include (i) the handling of complex, multi-step instructions, (ii) inconsistencies in interpreting instructions due to ambiguous language or contextual nuances, (iii) a limited ability to generalize to novel task compositions that deviate from trained examples, and (iv) a lack of explicit reasoning mechanisms that can delineate and manage the execution processes involved in following instructions. A major drawback is evident in planning travel itineraries where LLMs often fail to consider real-world constraints and user preferences effectively. For instance, when tasked with creating a multi-city travel plan as shown in Figure 1, LLMs might generate a sequence that is theoretically correct but impractical, such as suggesting flights that don't exist or ignoring necessary travel recovery times and local conditions [1]."}, {"title": "II. INSTRUCTABILITY SO FAR", "content": "Traditionally, instructability in intelligent systems was rooted in symbolic AI, which utilized explicit, rule-based programming and logical frameworks to provide deterministic guarantees for instruction execution [5]. As the LLMs have evolved, use and support for instructability has evolved. Currently, LLMs are widely used as chatbots, popularized by OpenAI's ChatGPT. This trend can be traced back to the world's first chatbot, Eliza, developed in the 1960s. With the increasing adoption of ChatGPT and similar LLM-based chatbots, the need for instructability has become more pronounced as people rely on these chatbots to obtain information across a wide range of domains.\nOne of the major challenges with LLMs is aligning with users' objectives, primarily due to a mismatch between model's training goals and the users' needs: LLMs are typically trained on minimizing the contextual word prediction error on large corpora, while users want the model to \u201cfollow their instructions helpfully and safely\u201d [6], [7]. To address this mismatch, instruction tuning was proposed, which involves further training LLMs using (INSTRUCTION, OUTPUT) pairs, where INSTRUCTION denotes the human instruction for the model, and OUTPUT denotes the desired output that follows the INSTRUCTION. The main challenges of instruction tuning are: (a) crafting high-quality instructions that cover target behaviors is challenging due to limited quantity, diversity, and creativity in existing datasets, (b) instruction tuning improves on tasks that are heavily represented in the training dataset, raising concerns about its effectiveness on less represented tasks [8], and (c) instruction tuning captures only surface-level patterns and styles rather than comprehending and learning the task; limiting required understanding to perform a specific task [2] as seen in Figure 1.\nInstruction tuning is essentially a rebranded version of well-known supervised fine-tuning, and as such, it shares many common issues with training a model on large datasets and expecting reliable and consistent performance on tasks that require more advanced, system 2 thinking abilities [3]. However, instruction tuning remains a very active area of research, and we encourage readers to refer to this Github repository for the latest developments.\nFollowing instructions is critically important in applications or domains such as planning-like tasks [9], robotics, and mental health. Observations across these applications indicate that LLMs frequently fail to accurately follow instructions to reach the user-desired goal state [4], [10], [11]. The outputs often exhibit inaccuracies, including hallucinations and significant issues with grounding affordances, reflecting a disconnect between the models' outputs and real-world applications [12]. This disconnect arises because instructions often involve complex, multi-step processes that necessitate further decomposition and contextual understanding for LLMs to interpret and execute them accurately.\nDomain-specific neurosymbolic approaches have been proposed to enhance plan generation [13], [14] and adherence to mental health guidelines [4]. However, there remains a lack of a unified neurosymbolic framework aimed at improving the general-purpose instructability of LLMs. Developing such an architecture could significantly enhance LLMs' consistency, reliability, explainability, and safety across various applications [15]."}, {"title": "III. TOWARDS NEUROSYMBOLIC APPROACH TO INSTRUCTABILITY", "content": "In this section, we describe our proposed neurosymbolic instruction following framework.\nWe propose employing a symbolic task planner that leverages Hierarchical Task Networks (HTNs) to decompose complex instructions into clearly defined, manageable subtasks. This planner is equipped with an extensive library of task schemas, which serve as blueprints for decomposing specific types of tasks. For this, we can use process knowledge graphs which capture detailed task-specific guidelines or protocols relevant to solving the problems, as shown in Figure 3. Such guidelines exist for many applications, for example, in case of assisting a clinician, the process may be prescribed in the clinical practice guidelines (see Figure 3b. By recursively breaking down high-level instructions (see Figure 4), the planner translates them into finer, executable primitive actions, guided by predefined planning rules and constraints inherent to the HTNs. The integration of process knowledge graphs is crucial as they provide the necessary contextual and operational knowledge for each task. These graphs ensure that each decomposed action is not only logically consistent but also enriched with relevant domain-specific information."}, {"title": "IV. DEPENDENCY OF INSTRUCTABILITY ON GROUNDING AND ALIGNMENT", "content": "The LLMs fundamentally hinge on two critical capabilities: grounding and alignment. Grounding refers to the ability of LLMs to connect the language constructs used in their outputs with real-world entities and contexts. Alignment, on the other hand, involves the LLMs' ability to produce outputs that are not only contextually appropriate but also closely aligned with the users' intentions and ethical guidelines. Both grounding and alignment are pivotal for ensuring that LLMs can follow instructions in a way that meets the practical and ethical expectations of their human users.\nGrounding in LLMs [16] involves the translation of abstract linguistic representations into concrete, actionable entities and scenarios. This process is essential for LLMs as it impacts their ability to interpret and execute complex instructions accurately within a specified context. For example, in task-oriented applications such as navigating an environment or executing a series of physical actions, the LLM must understand and map its instructions onto the physical world (see Figure 8). This requirement extends beyond simple recognition of terms; it necessitates an understanding of the relationships and interactions between various entities. Without effective grounding, LLMs are prone to generating outputs that, while linguistically correct, are made up, infeasible or irrelevant. The integration of knowledge graphs are kept up-to-date can significantly enhance grounding by providing a rich, interconnected database of real-world entities and their attributes, enabling LLMs to draw on a vast reservoir of structured information to better interpret and relate instructions to tangible, real-world applications.\nAlignment involves ensuring that LLMs' actions and responses are effective, ethically sound, and aligned with user expectations (see Figure 9). This aspect of LLM behavior is crucial for maintaining trust and reliability, particularly in sensitive applications such as healthcare, legal advice, or educational settings. Alignment ensures that the LLM's responses adhere to ethical standards and user-specific requirements, preventing scenarios where the model's behavior diverges from human values or produces harm 1. Effective alignment strategies involve technical measures, such as adjusting model parameters and training data, and policy measures, such as incorporating feedback loops that allow users to report and rectify misaligned behavior. A knowledge graph can further support alignment by directly embedding a structured understanding of ethical norms and user preferences into the model's reasoning processes, providing a foundational layer that helps guide the LLM's responses to ensure they remain within desired ethical and practical parameters."}, {"title": "V. CONCLUSION", "content": "We introduced a neurosymbolic framework to enhance the instructability of generative AI, with LLMs as a prime example, addressing the limitations of traditional instruction tuning approaches. By integrating symbolic task planners with neural semantic parsers and neurosymbolic executors, we discuss how to achieve superior task decomposition, semantic grounding, and execution reliability. Knowledge graphs further enrich this integration, providing essential real-world context and ensuring alignment with ethical standards and user expectations."}]}