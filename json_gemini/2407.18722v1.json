{"title": "Neurosymbolic AI for Enhancing Instructability in Generative AI", "authors": ["Amit Sheth", "Vishal Pallagani", "Kaushik Roy"], "abstract": "Generative AI, especially via Large Language Mod-els (LLMs), has transformed content creation across text, images,and music, showcasing capabilities in following instructionsthrough prompting, largely facilitated by instruction tuning.Instruction tuning is a supervised fine-tuning method whereLLMs are trained on datasets formatted with specific tasks andcorresponding instructions. This method systematically enhancesthe model's ability to comprehend and execute the provideddirectives. Despite these advancements, LLMs still face challengesin consistently interpreting complex, multi-step instructions andgeneralizing them to novel tasks, which are essential for broaderapplicability in real-world scenarios. This article explores whyeurosymbolic AI offers a better path to enhance the instructabil-ity of LLMs. We explore the use a symbolic task planner todecomposse high-level instructions into structured tasks, a neuralsemantic parser to ground these tasks into executable actions,and a neuro-symbolic executor to implement these actions whiledynamically maintaining an explicit representation of state. Wealso seek to show that neurosymbolic approach enhances thereliability and context-awareness of task execution, enablingLLMs to dynamically interpret and respond to a wider rangeof instructional contexts with greater precision and flexibility.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models such as OpenAI's GPT-4 that is usedby ChatGPT and Meta's Llama have demonstrated unprece-dented capabilities in following natural language instructionsthrough prompting. These models can be adapted to a broadspectrum of tasks by simply receiving appropriately structuredprompts, showcasing their versatility and utility across variousdomains. For example, LLMs can successfully generate programming code from descriptions of desired functionalities or craft well-structured essays based on outlined themes, showcasing their adaptability and precision in tasks with clear, well-defined goals. However, despite their impressive perfor-mance, the current teaching paradigm in LLMs encounters several challenges. These include (i) the handling of complex, multi-step instructions, (ii) inconsistencies in interpreting instructions due to ambiguous language or contextual nuances, (iii) a limited ability to generalize to novel task compositions that deviate from trained examples, and (iv) a lack of explicit reasoning mechanisms that can delineate and manage the execution processes involved in following instructions. A major drawback is evident in planning travel itineraries where LLMs often fail to consider real-world constraints and user preferences effectively. For instance, when tasked with creating a multi-city travel plan as shown in Figure 1, LLMs might generate a sequence that is theoretically correct but impractical, such as suggesting flights that don't exist or ignoring necessary travel recovery times and local conditions [1]."}, {"title": "II. INSTRUCTABILITY SO FAR", "content": "Traditionally, instructability in intelligent systems was rooted in symbolic AI, which utilized explicit, rule-based programming and logical frameworks to provide deterministic guarantees for instruction execution [5]. As the LLMs have evolved, use and support for instructability has evolved. Cur-rently, LLMs are widely used as chatbots, popularized by Ope-nAI's ChatGPT. This trend can be traced back to the world's first chatbot, Eliza, developed in the 1960s. With the increasing adoption of ChatGPT and similar LLM-based chatbots, the need for instructability has become more pronounced as people rely on these chatbots to obtain information across a wide range of domains.\nOne of the major challenges with LLMs is aligning with users' objectives, primarily due to a mismatch between model's training goals and the users' needs: LLMs are typ-ically trained on minimizing the contextual word prediction error on large corpora, while users want the model to \u201cfollow their instructions helpfully and safely\" [6], [7]. To address this mismatch, instruction tuning was proposed, which involves further training LLMs using (INSTRUCTION, OUTPUT) pairs, where INSTRUCTION denotes the human instruction for the model, and OUTPUT denotes the desired output that follows the INSTRUCTION. The main challenges of instruc-tion tuning are: (a) crafting high-quality instructions that cover target behaviors is challenging due to limited quantity, diversity, and creativity in existing datasets, (b) instruction tuning improves on tasks that are heavily represented in the training dataset, raising concerns about its effectiveness on less represented tasks [8], and (c) instruction tuning captures only surface-level patterns and styles rather than comprehending and learning the task; limiting required understanding to perform a specific task [2] as seen in Figure 1.\nInstruction tuning is essentially a rebranded version of well-known supervised fine-tuning, and as such, it shares many common issues with training a model on large datasets and expecting reliable and consistent performance on tasks that require more advanced, system 2 thinking abilities [3]. However, instruction tuning remains a very active area of research, and we encourage readers to refer to this Github repository for the latest developments.\nFollowing instructions is critically important in applications or domains such as planning-like tasks [9], robotics, and mental health. Observations across these applications indicate that LLMs frequently fail to accurately follow instructions to reach the user-desired goal state [4], [10], [11]. The outputs often exhibit inaccuracies, including hallucinations and significant issues with grounding affordances, reflecting a disconnect between the models' outputs and real-world applications [12]. This disconnect arises because instructions often involve complex, multi-step processes that necessitate further decomposition and contextual understanding for LLMs to interpret and execute them accurately.\nDomain-specific neurosymbolic approaches have been pro-posed to enhance plan generation [13], [14] and adherence to mental health guidelines [4]. However, there remains a lack of a unified neurosymbolic framework aimed at improving the general-purpose instructability of LLMs. Developing such an architecture could significantly enhance LLMs' consistency, reliability, explainability, and safety across various applica-tions [15]."}, {"title": "III. TOWARDS NEUROSYMBOLIC APPROACH TO INSTRUCTABILITY", "content": "In this section, we describe our proposed neurosymbolic instruction following framework.\nWe propose employing a symbolic task planner that lever-ages Hierarchical Task Networks (HTNs) to decompose com-plex instructions into clearly defined, manageable subtasks. This planner is equipped with an extensive library of task schemas, which serve as blueprints for decomposing specific types of tasks. For this, we can use process knowledge graphs which capture detailed task-specific guidelines or protocols relevant to solving the problems, as shown in Figure 3. Such guidelines exist for many applications, for example, in case of assisting a clinician, the process may be prescribed in the clinical practice guidelines (see Figure 3b. By recur-sively breaking down high-level instructions (see Figure 4), the planner translates them into finer, executable primitive actions, guided by predefined planning rules and constraints inherent to the HTNs. The integration of process knowledge graphs is crucial as they provide the necessary contextual and operational knowledge for each task. These graphs ensure that each decomposed action is not only logically consistent but also enriched with relevant domain-specific information."}, {"title": "A. Symbolic Task Planner", "content": "We propose employing a symbolic task planner that lever-ages Hierarchical Task Networks (HTNs) to decompose complex instructions into clearly defined, manageable subtasks. This planner is equipped with an extensive library of task schemas, which serve as blueprints for decomposing specific types of tasks. For this, we can use process knowledge graphs which capture detailed task-specific guidelines or protocols relevant to solving the problems, as shown in Figure 3. Such guidelines exist for many applications, for example, in case of assisting a clinician, the process may be prescribed in the clinical practice guidelines (see Figure 3b. By recur-sively breaking down high-level instructions (see Figure 4), the planner translates them into finer, executable primitive actions, guided by predefined planning rules and constraints inherent to the HTNs. The integration of process knowledge graphs is crucial as they provide the necessary contextual and operational knowledge for each task. These graphs ensure that each decomposed action is not only logically consistent but also enriched with relevant domain-specific information."}, {"title": "B. Neural Semantic Parser", "content": "Following the structured decomposition achieved by the planner, we employ a neural semantic parser, specifically fine-tuning a pre-trained compact language model for this purpose. This parser is tasked with translating the hierarchi-cally organized subtasks and natural language instructions into a grounded representation of actions (see Figure 5) along with their requisite parameters and arguments. This translation process is crucial for converting the symbolic planner's output, which organizes tasks at a high conceptual level, into detailed, executable commands. By grounding these decomposed tasks into actionable language forms, the neural semantic parser acts as a critical bridge, transforming high-level linguistic constructs into precise, actionable outputs that are ready for execution. This step not only ensures that the instructions are executable but also maintains semantic fidelity to the original user intent, thereby enhancing the system's ability to accurately follow complex instructions."}, {"title": "C. Neurosymbolic Executor", "content": "The neurosymbolic executor is the operational core of our framework, responsible for implementing the grounded in-structions provided by the neural semantic parser. It maintains an explicit symbolic representation of the state, crucial for assessing and managing the ongoing changes within the execu-tion environment. This executor integrates neural components, which are adept at perception and dynamic action execution, with robust symbolic reasoning mechanisms. Such integration enables precise tracking of state changes and effective man-agement of control flow.\nThe hybrid nature of this executor allows for dynamic adjustments, adapting in real-time to the complexities and unpredictability encountered during instruction execution. By leveraging both the predictive strengths of neural models and the deterministic nature of symbolic logic, the executor ensures a seamless and coherent execution process. This approach not only enhances the reliability and accuracy of following complex instructions but also supports the system's ability to handle interruptions, unexpected conditions, and varying contextual cues with high resilience and adaptability (see Figure 6). Figure 7 shows the comparison between current approach to instruct LLMs to perform a task and the proposed neurosymbolic instructability approach."}, {"title": "IV. DEPENDENCY OF INSTRUCTABILITY ON GROUNDING AND ALIGNMENT", "content": "The LLMs fundamentally hinge on two critical capabilities: grounding and alignment. Grounding refers to the ability of LLMs to connect the language constructs used in their outputs with real-world entities and contexts. Alignment, on the other hand, involves the LLMs' ability to produce outputs that are not only contextually appropriate but also closely aligned with the users' intentions and ethical guidelines. Both grounding and alignment are pivotal for ensuring that LLMs can follow instructions in a way that meets the practical and ethical expectations of their human users."}, {"title": "A. Grounding in LLMs", "content": "Grounding in LLMs [16] involves the translation of abstract linguistic representations into concrete, actionable entities and scenarios. This process is essential for LLMs as it impacts their ability to interpret and execute complex instructions accurately within a specified context. For example, in task-oriented applications such as navigating an environment or executing a series of physical actions, the LLM must under-stand and map its instructions onto the physical world (see Figure 8). This requirement extends beyond simple recognition of terms; it necessitates an understanding of the relationships and interactions between various entities. Without effective grounding, LLMs are prone to generating outputs that, while linguistically correct, are made up, infeasible or irrelevant. The integration of knowledge graphs are kept up-to-date can signif-icantly enhance grounding by providing a rich, interconnected database of real-world entities and their attributes, enabling LLMs to draw on a vast reservoir of structured information to better interpret and relate instructions to tangible, real-world applications."}, {"title": "B. Alignment in LLMs", "content": "Alignment involves ensuring that LLMs' actions and re-sponses are effective, ethically sound, and aligned with user expectations (see Figure 9). This aspect of LLM behavior is crucial for maintaining trust and reliability, particularly in sensitive applications such as healthcare, legal advice, or educational settings. Alignment ensures that the LLM's responses adhere to ethical standards and user-specific requirements, preventing scenarios where the model's behavior diverges from human values or produces harm 1. Effective alignment strategies involve technical measures, such as adjusting model parameters and training data, and policy measures, such as incorporating feedback loops that allow users to report and rectify misaligned behavior. A knowledge graph can further support alignment by directly embedding a structured understanding of ethical norms and user preferences into the model's reasoning processes, providing a foundational layer that helps guide the LLM's responses to ensure they remain within desired ethical and practical parameters."}, {"title": "V. CONCLUSION", "content": "We introduced a neurosymbolic framework to enhance the instructability of generative AI, with LLMs as a prime exam-ple, addressing the limitations of traditional instruction tuning approaches. By integrating symbolic task planners with neural semantic parsers and neurosymbolic executors, we discuss how to achieve superior task decomposition, semantic grounding, and execution reliability. Knowledge graphs further enrich this integration, providing essential real-world context and ensur-ing alignment with ethical standards and user expectations."}]}