{"title": "Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally", "authors": ["Xiuzhan Guo"], "abstract": "In this paper, we study the machine learning elements which we are interested in together as a machine learning system, consisting of a collection of machine learning elements and a collection of relations between the elements. The relations we concern are algebraic operations, binary relations, and binary relations with composition that can be reasoned categorically. A machine learning system transformation between two systems is a map between the systems, which preserves the relations we concern. The system transformations given by quotient or clustering, representable functor, and Yoneda embedding are highlighted and discussed by machine learning examples. An adjunction between machine learning systems, a special machine learning system transformation loop, provides the optimal way of solving problems. Machine learning system transformations are linked and compared by their maps at 2-cell, natural transformations. New insights and structures can be obtained from universal properties and algebraic structures given by monads, which are generated from adjunctions.", "sections": [{"title": "1 Introduction", "content": "In the age of artificial intelligence, data is from various platforms with multiple formats, noisy, and keeps changing continuously, which results in tremendous potential for dynamic relationships. Data is not static but dynamic. Machine learning (ML) elements and systems, driven by data, producing new data, must be robust enough to capture the changes.\nNatural numbers are not isolated but connected by their mathematical operations, e.g., +, -, \u00d7, and \u00f7, so that, natural numbers can be used not only to count but also to solve real life problems. The set of all natural numbers, along with the operations, forms an algebraic system and so one can study numbers and their relations together by their properties and extend the system to more complex system and solve more complex problems naturally. Hence the relations (operations) between natural numbers and their properties make more sense than the isolated numbers.\nIn ML, algorithms learn from the data one inputs and can only learn effectively if the data is in the format required, clean and complete. ML models are driven by data and on the other hand, ML models generate data usually. Real world datasets are usually with multiple formats, from multiple silos. These datasets are prepared and transformed to train their ML models. Therefore, not only are data and ML models connected but also are there relations among datasets and between ML models. Hence ML elements are not isolated but connected together with certain structures, e.g., operations, relations, and compositional relations. These relations make more sense than the isolated ML elements, similar to the natural numbers.\nData changes constantly. ML models, driven by data, are tested and retrained to ensure them remain accurate, relevant, and effective during data changing. So all elements and their relations in an ML system must be modified together coherently. All ML elements and the relations between the elements, which we concern, must be viewed together to form an ML system.\nReal world problems can be solved by modelling them mathematically and implementing the models into computational tools. Some problems are challenges in one mathematical area but can be solved through mapping them to another mathematical environment, e.g., mapping topological problems and number theory problems to algebraic settings. Similarly, some problems might be easier to be solved in one ML system than another. To find a reliable ML system for certain class of problems, we need to compare some ML systems and map one system to another without breaking the existing relations. Therefore, we also consider how to map one ML system to another.\nIn Section 2, we first consider the collection M of all ML elements and a collection R of their relations we concern together as an ML system (M, R). Then we link and compare ML systems using relations preserving maps, ML system transformations. In this section, the ML element relations we are interested in are binary algebraic operation, directed graph, directed graph with identities and composition. Clustering aims to group a class of objects in such a way that objects in the same cluster are more similar to each other. Clustering amounts to partitioning or an equivalent relation on the class of objects or a surjective map from the class to the quotient space of the equivalent relation. If clustering is compatible with the relations of an ML system, then we have a quotient ML system by identifying elements in the same cluster and a quotient ML system transformation from the original ML system to its quotient ML system functorially.\nSets are concrete mathematical objects. Given an ML system (M, R) and an ML element M\u2208 \u039c, we have a map hom(-, M) from (M, R) to Set, sending X \u2208 M to the set of all edges from X to M. The corresponding, sending M to hom(-, M), is called Yoneda embedding. The hom(-, M) is set valued and determined totally by M, called reprsentable functor/transformation when (M, R) has composition and identities. These representable functors/transformations, e.g., hom(-, M), provide the optimal ways to"}, {"title": "2 Machine Learning Systems and Transformations", "content": "ML elements are the foundational components and blocks that can be used to build ML systems. Essential ML elements include data, features, algorithms, models, performance metrics, validation, testing, deployment, outputs, etc. Let M be a collection of ML elements one concerns. The elements in M are not isolated but connected by the collection of relations between the elements. For instance, the collection of relations can be specified by algebraic operations, certain dependencies and relations on M so that the elements work together to enable the ML systems to learn, grow, and perform tasks. A collection M of ML elements and a collection of relations we concern form an ML system.\nData is flowing. The elements and the relations in an ML system, driven by data, must be updated and transformed to fit the present setting dynamically. The map, preserving the relations concerned, between ML systems is an ML system transformation.\n1. An ML system (M, R) consists of a collection M of ML elements and a collection R of relations between the elements. Write $e_1Re_2$ or $(e_1, e_2) \\in R$ or $e_1 \\rightarrow e_2$ if $e_1$ and $e_2$ are related by R, for $e_1, e_2 \\in M$.\n2. Let $(M_1, R_1)$ and $(M_2, R_2)$ be ML systems. An ML system transformation T from $(M_1, R_1)$ to $(M_2, R_2)$ is a function $T: M_1 \\rightarrow M_2$ that preserves the collection $R_1$ of relations: $e_1R_1e_2$ implies $T(e_1)R_2T(e_2)$, denoted by $T: (M_1, R_1) \\rightarrow (M_2, R_2)$.\n1. If $R_1$ is given a (partial) binary operation, e.g., table join on a collection of data tables, then $R_1$ can be viewed as a ternary relation. Assume that $R_1$ is given by a (partial) binary operation $o$ and $R_2$ by $*$ respectively, $T: (M_1, R_1) \\rightarrow (M_2, R_2)$ can be considered as a homomorphism, a structure preserving function $T: M_1 \\rightarrow M_2$, namely, $T(e_1 o e_2) = T(e_1) * T(e_2)$.\n2. If ML systems $(M_1, R_1)$ and $(M_2, R_2)$ have only some general relations, e.g., dependencies, similarities, implications, etc., between their elements, then these ML systems can be modelled by (multi)directed graphs and so the ML transformations between the ML systems are given by directed graph homo- mophisms, namely, a function $T: M_1 \\rightarrow M_2$ that takes each edge (relation) $e_1 \\rightarrow e_2$ in $M_1$ to an edge (relation) $T(e_1) \\rightarrow T(e_2)$ in $M_2$.\n3. If ML systems $(M_1, R_1)$ and $(M_2, R_2)$ have the transitive and associative relations and identity re- lations, then both $(M_1, R_1)$ and $(M_2, R_2)$ can be modelled by directed graphs with identities and composition, which are categories, a general mathematical structure. An ML system transformation $T: (M_1, R_1) \\rightarrow (M_2, R_2)$ is a functor. Hence ML systems can be reasoned categorically. See Appendix for the basic notations, concepts, and results of relation, directed graph, and category theory.\nAlgebraic or graph transformation between ML systems that have algebraic operations or binary relations, can be factored as a surjective to a quotient space, followed by an injective transformation by the similar process in [10] at the set level. An ML system that has a compositional relation and forms a category can be quotiented by either a congruence equivalence relation on its hom sets or an equivalence relation on objects. See Subsection B.4 for the quotient category details.\nLet (M, R) be an ML system and $p$ an equivalence relation on M. $p$ is compatible with R if R can be induced to the equivalence relation $R_p$ on $M/p$, namely, $e_1 R e_2 \\rightarrow [e_1]_p R_p [e_2]_p$ is well-defined.\n1. ML system $(M_1, R_1)$ is transformed to its quotient ML system $(M_1/p, R_{1p})$ by the obvious canonical ML system transformation\nsending $f: e_1 \\rightarrow e_2$ to $[f]_p: [e_1]_p \\rightarrow [e_2]_p$.\n2. If $\u03c3$ is an equivalence relation on $M_1$ and compatible with $R_1$ such that $p \\subseteq \u03c3$, then there is a unique surjective ML system transformation $(p \\le \u03c3)^*: M_1/p \\rightarrow M_1/\u03c3$ such that\ncommutes.\n3. Each ML system transformation $T: (M_1, R_1) \\rightarrow (M_2, R_2)$, which preserves the congruence equivalence relation $p$, that is, $(f, g) \\in p$ implies $T(f) = T(g)$, factors through $Q_p$, followed by a unique induced ML system transformation $T_{\\flat}: M_1/p \\rightarrow M_2$\n4. If $T(p) \\subseteq \u03c3$, then there is a unique ML system transformation $T^*: M_1/p \\rightarrow M_2/\u03c3$ such that\ncommutes. If T is surjective and so is $T^*$.\nto increase the predictability of the next record or a few next records, one may group the records and compute the average of each cluster, for instance, monthly cluster. Hence the equivalence relation pon S is given by:\n$(s_i, s_j) \\in p \\Leftrightarrow s_i$ and $s_j$ were transacted at the same month.\nThe representation function on clusters, e.g., the average avg, can be chosen to represent each cluster. Therefore we have:\nwhere$\\le_p$ is a partial order on R and compatible with the represent function avg to induce $avg_p$.\nThe uncertainty/randomness of an ML element can be viewed from different points of view by clustering and representing. The outcomes of a variable, e.g., coin flip, can be either any point from {head, tail} at the point level or a certain set {head, tail} at the distribution level.\nLet $\\Omega$ be a sample space of a random variable X and bin an equivalence relation on $\\Omega$. bin clusters $\\Omega$ to a family of disjoint subsets. If dist: $\\Omega \\rightarrow V$ is a function that is compatible with bin and represents the clusters of bin, then there is a transformation:\nSince equivalence relations (clustering or surjective maps) have the partial order < given by $\\subseteq$, we have the following commutative diagrams:\nWord embedding intends to map all words in a large corpus to a vector space, with the relations between words, e.g., semantic similarity, syntactic similarity, contextual similarity, analogical relationships, etc., being preserved. Word2vec [16, 17] is a popular machine learning technique for learning word embeddings from a large text corpus. Let $W_o$ be the set of all words in a corpus. Applying Word2vec to $W_o$, one obtains a function $w2v_o: W_o \\rightarrow R^n$. Since there exist the relations, e.g., similar meaning, between the words in $W_o$, $W_o$ is enriched to a directed graph $W_1$ and $w2v_o$ is lifted to $w2v: W_1 \\rightarrow R^n$, where n is a natural number and $R^n$ is ordered partially by the closeness (neighbourhood). If the words that have similar meaning are closer in the real vector space $R^n$, then one has an ML system transformation:\n$w2v : W_1 \\rightarrow R^n$"}, {"title": "3 Transforming and Comparing ML System Transformations", "content": "In this section and Section 4", "categories": "directed graphs with identities and composition so that categorical results are applicable. Hence ML system transformations between ML systems are functors and relations/maps between functors are natural transformations categorically. See B.3 for the details of functors and natural transformations.\nLet $(M_1", "objects": "the collection of ML system transformations from $(M_1, R_1)$ to $(M_2, R_2)$"}]}