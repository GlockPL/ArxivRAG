{"title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding", "authors": ["Ryan Sun", "Tianyi Zhou", "Xun Chen", "Lichao Sun"], "abstract": "Large Language Models (LLMs) have become essential in advancing natural language processing (NLP) tasks, but their sequential token generation limits inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising solution by using a smaller draft model to generate multiple token sequences, which the target LLM verifies in parallel. However, current heuristic approaches, such as Recursive Rejection Sampling (RRS), suffer from low acceptance rates in subsequent drafts, limiting the advantages of using multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can theoretically improve acceptance rates, but its computational cost is too high for real-time use. We present SpecHub, a novel, efficient sampling-verification method for MDSD that improves acceptance rates with only linear computational overhead. By simplifying the OTM problem into a compact Linear Programming model, SpecHub significantly reduces computational complexity. It further accelerates sampling by leveraging a sparse joint distribution, focusing computation on high-probability token sequences. In extensive experiments, Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step than RRS and RRS without replacement. We attach our code at https://github.com/MasterGodzilla/Speculative_decoding_OT.", "sections": [{"title": "1 Introduction", "content": "With the growing adoption of Large Language Models (LLMs) in diverse applications, there is a significant demand for faster inference and lower latency in both local computing and online API services. However, the sequential generation process of autoregressive language models complicates parallel computation. This challenge is exacerbated by the memory limitations of current hardware architectures, where RAM and cache communication latencies often constrain performance, resulting in underutilized computing capacity.\nSpeculative decoding (Leviathan et al., 2023; Chen et al., 2023a) accelerates LLM inference while preserving the model's output distribution. By generating a sequence of draft tokens in advance using a smaller model, it leverages GPUs to verify tokens simultaneously through rejection sampling. Recent advancements (Chen et al., 2024; Jeon et al., 2024; Sun et al., 2024; Miao et al., 2023) have further enhanced this approach by introducing tree-structured multi-drafts, where each path represents a draft. These tokens are verified in parallel during a single forward pass of the LLM. Using a token tree increases the number of accepted tokens by providing multiple options for each token position, thus increasing the overall acceptance rate of the algorithm and generation efficiency.\nDespite having various tree constructions, draft model designs, and hardware optimizations, existing Multi-Draft Speculative Decoding (MDSD) methods depend on recursive rejection sampling (RRS) for acceptance, which is far from optimal. While RRS greedily accepts the token from the first draft, it does not consider the subsequent drafts and misses the opportunity to dynamically adjust the current token's acceptance strategy to improve the acceptance rates of the later drafts. RRS prioritizes the first draft's tokens but fails to dynamically adjust acceptance strategies for subsequent drafts. As a result, later iterations use a residual distribution modified by previous acceptances, leading to misalignment with the original draft distribution and lower acceptance rates (Chen et al., 2023b). Sun et al. (2024) shows that the acceptance rule could be optimized through an Optimal Transport problem with Membership Cost (OTM), which maximizes acceptance rates by better aligning draft tokens with the accepted token. However, OTM requires tremendous computation overhead and is not practically feasible."}, {"title": "2 Background and Related Work", "content": "Here, we review the sampling-verification schema of speculative decoding. We discuss the theory behind rejection sampling and explain why naively extending it to Multi-Draft Speculative Decoding (MDSD) becomes inefficient.\nSpeculative Sampling Language model decoding is intrinsically serial. Let V denote the vocabulary, a discrete set of tokens that the language model may generate. Let $x_{1:t} = (x_1,...,x_t) \\epsilon V^t$ denote a sequence of tokens. Then, the target language model produces a conditional probability $p(\\cdot|x_{1:t})$, from which we sample the next token $x_{t+1} \\sim p(\\cdot|x_{1:t})$. However, this process is slow for its serial execution.\nSpeculative decoding (Chen et al., 2023a; Leviathan et al., 2023) addresses the issue by parallelizing the decoding process with a draft and verify phase. It first uses a smaller draft model $q(x_{1:t})$ to generate a draft $(x_{t+1},...,x_{t+d})$ sequentially. The depth of the draft, $d$, is usually"}, {"title": "3 Mathematical Formulation of Multi-Draft Speculative Decoding", "content": "In this section, we lay out the mathematical formulation of the sampling and verification paradigm of MDSD. We start by reviewing the Optimal Transport with Membership Cost framework by Sun et al. (2024) in Section 3.1. We show that it can simplified and propose an equivalent LP formulation that greatly reduces computation complexity in Section 3.2. Lastly, we point out that changing the design of sampling can make the LP feasible for real-world calculation in Section 3.3 while preserving the acceleration. We also discuss some considerations for a real-world algorithm.\n3.1 Optimal Transport with Membership Cost\nWe show how we can find the optimal sampling and verification algorithm of MDSD that maximizes the acceptance rate as solving an Optimal Transport problem with Membership Cost (Sun et al., 2024). Let the target distribution be $p$ and the joint draft distribution $Q = q^{\\otimes k} \\epsilon \\Delta^{|V|^k-1}$ be the Cartesian product of the draft distributions that gives the probability of sampling any particular series of draft tokens $X_{1:k}$, SO $Q(x_{1:k}) = \\prod_{i=1}^k q(x_i)$. Let $y$ denote the accepted token. We define the coupling between $p$ and $Q$ or equivalently a transport plan from $Q$ to $p$ be a joint distribution $\\pi(x_{1:k}, y) \\epsilon \\Delta^{|V|^{k+1}-1}$ whose marginal distributions satisfies $\\Sigma_{y\\epsilon V} \\pi(x_{1:k}, y) = Q(x_{1:k})$ and $\\Sigma_{x_{1:k}\\epsilon V^k} \\pi(x_{1:k}, y) = p(y)$. We use the terms coupling and transport plan interchangeably. The Membership Cost is $c(x_{1:k}, y) = \\prod_{i=1}^k \\mathbb{I}_{y \\neq x_i}$, an indicator function of whether the accepted token $y$ equals any of the draft tokens $x_i$. The transport cost then calculates the expected rejection rate:\n$C(\\pi) = E_{x_{1:k}, y \\sim \\pi} [\\prod_{i=1}^k \\mathbb{I}_{y \\neq x_i}]$"}, {"title": "3.2 A Simplified Linear Programming Formulation", "content": "While the Optimal Transport formulation provides a theoretical framework for understanding Multi-Draft Speculative Decoding, its computational complexity renders it impractical for real-time applications. To address this, we introduce a simplified Linear Programming (LP) formulation that significantly reduces the number of variables while preserving the essence of the problem.\nThe key insight behind this simplification is that the acceptance rate is primarily determined by how the sampled draft tokens are handled. Once a token is rejected, the subsequent actions, which involve recalculating the residual distribution and resampling, can be performed efficiently without explicitly considering the full coupling.\nInstead of representing the entire coupling $\\pi$, which has $O(|V|^{k+1})$ variables, our simplified LP formulation focuses on $\\pi(x_{1:k}, y = x_i)$, $i = 1,..., k$, a smaller subset of transport plan which denotes the probability of sampling the series of drafts and accepting the $i$-th token $x_i$. This effectively reduces the number of variables to $O(|V|^{k})$, making the problem more tractable. The remaining probabilities in the coupling, which correspond to cases where the target token does not match any of the draft tokens, are implicitly handled by the residual distribution.\nThe simplified LP formulation is then:\n$\\min \\quad 1 - \\sum_{x_{1:k} \\epsilon V^k} \\sum_{i=1}^k \\pi(x_{1:k}, x_i)$\nsubject to\n$\\pi(x_{1:k}, x_i) \\geq 0, \\quad \\forall x_{1:k} \\epsilon V^k, i$\n$\\sum_{i=1}^k \\pi(x_{1:k}, x_i) \\leq Q(x_{1:k}), \\quad \\forall x_{1:k} \\epsilon V^k$\n$\\sum_{i=1}^k \\sum_{x_{1:k}\\epsilon V^k, x_i=y} \\pi(x_{1:k}, y) \\leq p(y), \\quad \\forall y \\epsilon V$\nGiven a solution to this simplified LP formulation, we can reconstruct the complete transport plan $\\pi(x_{1:k}, y)$. For any series of drafts $x_{1:k}$ and target token $y$, if $y$ does not equal one of the draft tokens in $x_{1:k}$, the entry is calculated as:\n$\\pi(x_{1:k}, y) = \\frac{p(y) - \\sum_{i=1}^k \\sum_{x_{1:k}\\epsilon V^k, x_i=y} \\pi(x_{1:k}, y)}{\\sum_{y \\epsilon V} p(y) - \\sum_{i=1}^k \\sum_{x_{1:k}\\epsilon V^k, x_i=y} \\pi(x_{1:k}, y)} \\cdot (Q(x_{1:k}) - \\sum_{i=1}^k \\pi(x_{1:k}, x_i))$ #where y \\neq x_i \\forall i = 1, ..., k$\nThe first term is the unallocated target probability mass or the residual probability of $y$ normalized."}, {"title": "3.3 Design of Sampling", "content": "While the simplified LP formulation significantly reduces the computational burden compared to the OTM, it remains computationally expensive for large vocabularies. Directly solving the LP problem is impractical, and previous research has predominantly focused on developing heuristics to approximate the optimal solution. These heuristics, such as Recursive Rejection Sampling (RRS) or SpecTr(Sun et al., 2024), operate under a fixed joint draft distribution, typically assuming independent sampling with $Q = q^{\\otimes k}$ or without replacement $(Q(x_{1:k}) =  \\frac{\\prod_{i=1}^k q(x_i)}{\\prod_{i=1}^k (1-\\sum_{i=1}^k q(x_j))}$.\nHowever, a crucial and often overlooked aspect"}, {"title": "4 SpecHub", "content": "Building on the aforementioned insights, we introduce SpecHub, a faster sampling-and-verifying paradigm with only linear computational overhead. It effectively captures the transport features of OTM solutions to enhance the acceptance rate and can be applied to various multi-draft speculative sampling algorithms. Since using more than two drafts offers little gains in efficiency, SpecHub uses two drafts (i.e., $k = 2$) to reduce complexity. We thoroughly discuss expanding the algorithm to more drafts in Appendix C.\nFirst, we identify the token with the highest draft probability, denoted as $a$, and sample it alongside other tokens. We only populate the first column and the first row in the joint draft distribution $Q$. In particular, we define the joint draft distribution $Q(x_1, x_2)$ as follows:\n$Q(x_1, x_2) = \\begin{cases} q(x_1) & \\text{if } x_2 = a,  \\\\ \\frac{q(a)q(x_2)}{1-q(a)} & \\text{if } x_1 = a,  \\\\ 0 & \\text{otherwise.} \\end{cases}$\nThis specific design of $Q$ makes the solution to the simplified LP formulation straightforward. $\\forall x \\epsilon"}, {"title": "5 Experiments", "content": "In this section, we empirically show that SpecHub improves batch efficiency in speculative multi-draft decoding. We first show that SpecHub gives a significantly higher acceptance rate for its better coupling properties in the second draft acceptance rate. We then illustrate how the improvement transfers to higher batch efficiency.\n5.1 Experiment Setup\nOur experimental setup is based on the Llama and Vicuna models. To mimic the setup of Chen et al. (2024), we utilize the JackFram/Llama-68m and JackFram/Llama-160m (JF68m, JF160m) (Miao et al., 2023) models as our draft models and the Llama2-7B (Touvron et al., 2023) models as our target models. We evaluate our results on the Open-WebText (Gokaslan and Cohen, 2019) and CNN DailyMail (See et al., 2017) datasets. For each run, we use 200 examples to measure the acceptance rate vector and sample another 200 examples for evaluation. The prompt length and generation length are both set to 128 tokens. We evaluate our system on a single RTX A5000 GPU.\nWe also implement our algorithm on EAGLE (Li et al., 2024). In short, EAGLE trains an autoregressive decoding head that takes both the embedding in the last layer of the target model and the draft tokens to predict a draft. We test its performance on Vicuna-7b (Zheng et al., 2024), a fine-tuned LLaMA chatbot using ChatGPT (OpenAI et al., 2024) to generate responses. We use the MT-Bench dataset and temperatures T = 0.6, 1.0 with binary trees and binary Sequoia trees.\n5.2 Main Experiments"}, {"title": "6 Related Work", "content": "Speculative Decoding Speculative decoding aims to execute multiple decoding steps in parallel. Early work (Stern et al., 2018) predicts future tokens to accelerate greedy decoding. Speculative Sampling (Chen et al., 2023a; Leviathan et al., 2023) extends to non-greedy decoding and uses rejection sampling to recover target distribution optimally. Recent works focus on reducing the running time of the draft model and increasing the acceptance rate. OSD (Liu et al., 2023) and DistillSpec (Zhou et al., 2023) train draft models on text generated by the target model. REST (He et al., 2023) constructs drafts through retrieval. Lookahead Decoding (Fu et al., 2024) breaks the sequential dependency with Jacobi Iterations. Self-Speculative Decoding (Zhang et al., 2023; Elhoushi et al., 2024) avoids additional models and generates draft tokens by skipping intermediate layers. Several works, such as MEDUSA (Cai et al., 2024) and EAGLE (Li et al., 2024), reuse the feature embedding of LLMs' last attention layer to predict multiple future tokens in a non-causal or autoregressive manner.\nMulti-Draft Speculative Decoding Recent research explores using tree attention to generate multiple drafts for speculative decoding (Miao et al., 2023; Spector and Re, 2023; Li et al., 2024). Sun et al. (2024) formulate the acceptance of multiple drafts as a maximal coupling problem between the drafts and the target distributions and propose SpecTr with 1 - optimality guarantee. CS Drafting (Chen et al., 2023b) swaps in a lower-quality model to generate drafts for less relevant branches. MEDUSA (Cai et al., 2024) establishes candidates according to the Cartesian product of the multi-head predictions. Independently, Jeon et al. (2024) and Yang et al. (2024) notice that a rejected token has zero probability in the residual distribution and use sampling-without-replacement in the draft generation. Hu and Huang (2024) accelerates MDSD Tree Monte Carlo methods, which treat language model generation as a tree-space problem. Sequoia (Chen et al., 2024) designed a dynamic programming algorithm to search for the optimal tree topology."}, {"title": "7 Conclusion", "content": "We presented SpecHub, a versatile and provably faster sampling-verification paradigm for Multi-Draft Speculative Decoding. Using an optimal transport map between a sparse draft and target distributions, SpecHub increases the acceptance rate of the second draft by 1 \u2013 5%, which leads to higher batch efficiency of LLM inference by up to 0.27 tokens per iteration. In addition to practical speedups, SpecHub also provides insight into the underlying mathematical structure in MDSD. We hope to promotes future research in this area."}, {"title": "CA discussion on more drafts", "content": "C.1 Diminishing Returns of Increasing Drafts\nWhile theoretically appealing, using more drafts in practice offers diminishing returns. As we increase the number of drafts, the probability mass of the residual distribution decreases, leading to lower acceptance rates for subsequent drafts. This phenomenon is illustrated in Figure 9, where we present the acceptance rates for up to 10 drafts using both RRSw and RRS with temperature T = 1.0. As evident from the plots, the acceptance rate drastically decreases after the first few drafts, suggesting that the benefit of using more than 5 drafts is negligible.\nC.2 Curse of Dimensionality\nThe computational complexity of finding the optimal coupling in Multi-Draft Speculative Decoding grows exponentially with the number of drafts. This is often referred to as the curse of dimensionality. Specifically, the number of variables in the LP formulation is on the order of O(|V|k+1), where |V| is the vocabulary size and k is the number of drafts. As k increases, solving the LP becomes computationally intractable for even moderately sized vocabularies.\nC.3 Potential for Sparse Algorithms on more drafts\nThe diminishing returns of additional drafts and the curse of dimensionality suggest that a practical approach should focus on a small number of drafts while ensuring an efficient probability of mass transport. One promising direction is to explore sparse algorithms that leverage the structure"}, {"title": "D Comparing SpecHub to OTM in Toy Settings", "content": "Here, we seek to compare OTM, RRS, and SpecHub's performance by measuring the acceptance rate of the three algorithms using a few toy example drafts and target distributions with a small vocab size |V| = 50 in Table 4. Given temperature T and a hyperparameter \u03bb that controls the similarity between the two distributions, we generate two logits using uniform distributions such that Up ~ Unif(0, 1)|V| and Uq ~ Unif(0, 1)|V|. The corresponding target and draft distributions are p = softmax(U/T) and q = softmax(U/T + (1 \u2212 \u03bb)). We calculate the acceptance rate for all methods theoretically except for RRS without replacement, where we perform a Monte-Carlo Simulation with a thousand repetitions. We conduct the experiment on a hundred pairs of toy distributions and report the average. The results in Table 4 quantitatively illustrate the performance differences among SpecHub, Recursive Rejection Sampling (RRS), RRS without replacement, and Optimal Transport (OTM) methodologies under varying conditions of temperature T and similarity parameter \u03bb. In high similarity scenarios (\u03bb = 0.7), SpecHub outperforms other methods significantly at lower temperatures (T = 0.1), achieving the best acceptance rate of 0.7402, closely followed by OTM without replacement at 0.7345. At higher temperatures (T = 0.5), OTM methods, particularly OTM without replacement, dominate, marking the best performance with 0.9150 at T = 0.5 and \u03bb = 0.7. This suggests that SpecHub is particularly effective in tightly controlled environments with high similar-"}, {"title": "E Maximum Flow Problem Formulation", "content": "At k = 2, our Linear Programming (LP) formulation describes an equivalent Maximum Flow Problem formulation. This formulation effectively models the Multi-Draft Speculative Decoding process as the transportation of probability mass through a network of pipes.\nGiven an LP formulation with vocabulary set V, pair sampling distribution Q \u2208 \u0394||2\u22121, and target distribution p \u2208 \u0394||\u22121, we construct a graph G = (V, E) where the vertex set V consists of the vocabulary V, a source vertex s, and a sink vertex t. The capacity function g : (u, v) \u2208 \u0395 \u2192 [0, 1] is defined for each edge as follows:\n$g(u, v) = \\begin{cases} \\sum_{x_2} Q_{vx_2}, & \\text{if } u = s \\text{ and } v \\in V,  \\\\ p(v), & \\text{if } u \\in V \\text{ and } v = t,  \\\\ Q_{uv}, & \\text{if } u, v \\in V \\text{ and } u \\neq v,  \\\\ 0, & \\text{otherwise.} \\end{cases}$\nIn this formulation, the source vertex s distributes the total probability mass to the vertices in the vocabulary set V, while the sink vertex t collects the transported probability mass from the vocabulary vertices. The edges between the vocabulary vertices represent the possible transitions dictated by the pair sampling distribution Q. This network flow model not only provides an intuitive visualization of the probability mass transport process but also allows us to leverage well-established algorithms"}, {"title": "F More Experiment Details", "content": "JF68m on Full Binary Trees and Binary Sequoia Unbalanced Trees We conducted experiments to measure the batch efficiency of the JF68m model on both full binary trees and binary Sequoia unbalanced trees. For the full binary trees, we tested tree depths ranging from d = 2 to d = 5, and for the binary Sequoia trees, we used an unbalanced tree structure with varying depths. The results demonstrate that SpecHub consistently outperforms both RRS and RRSw across all tree depths. In the full binary tree configuration, SpecHub achieves a batch efficiency improvement of 0.02 \u2013 0.10 over RRSW and 0.04-0.20 over RRS at temperatures T = 0.6 and 1.0. For the binary Sequoia unbalanced trees, SpecHub maintains a higher batch efficiency, confirming its robustness on the more efficient unbalanced tree structures.\nJF160m on Binary and Ternary Trees We also evaluated the batch efficiency of RRS and RRSw using the JF160m model on both binary and ternary trees. For binary trees, we tested tree depths from d = 2 to d = 6, and for ternary trees, we considered depths up to d = 4. The JF160m model shows significant improvements in batch efficiency when using SpecHub. At temperatures T = 0.6 and 1.0, SpecHub outperforms RRS by 0.03 0.12 and RRSw by 0.05 \u2013 0.15 in binary tree configurations. The performance of RRS and RRSw in the ternary tree setting is worse than SpecHub on binary trees, suggesting the benefit of using more drafts is less significant.\nEAGLE Decoding Head To further explore the efficiency of our proposed method, we imple-"}, {"title": "B.3 Acceptance Rate", "content": "We here prove a sufficient condition for SpecHub to run faster than RRS.\nTheorem 3 (Superiority over RRS). Let \u03b1 = \u2211x\u2208V min(q(x), p(x)) be the acceptance rate of the first draft. SpecHub has a higher acceptance rate in the second draft if  (q(a)/(1-q(a))) > 1 \u2212 \u03b1.\nProof. First, by Lemma 1, SpecHub generates the top token a with probability p(a). This maximizes the acceptance rate for a. Next, we calculate the second draft acceptance rate for every other token x \u2208 V \\ {a}.\nFor RRS, the acceptance rate for token x in the first draft is min(p(x), q(x)). The residual probability for token x after the first draft, denoted as r(x), is:\np'(x) = (p(x) \u2013 min(p(x), q(x)))/(1-\u03b1)\nwhere \u03b1 = \u2211x\u2208V min(q(x), p(x)) is the overall acceptance rate in the first draft. The second draft acceptance rate for token x under RRS is then:\n(1 \u2212 \u03b1) min((p(x) \u2013 min(p(x), q(x)))/(1-\u03b1), q(x))\nwhich simplifies to:\nmin (p(x) \u2013 min(p(x), q(x)), (1 \u2212 \u03b1)q(x))\nFor SpecHub, the second draft acceptance rate for token x is:\nmin (p(x) - min(p(x), q(x)), (q(a)/(1-q(a))q(x))\nComparing these rates shows that SpecHub has a higher acceptance rate if (q(a)/(1-q(a)) > 1 \u2212 \u03b1.\nIn practice, this condition is usually satisfied. For example, if \u03b1 = 0.5, then as long as the top token has probability q(a) > 1/3 = 0.333, we guarantee acceleration. Meanwhile, since SpecHub accepts top tokens up to p(a), the above sufficient conditions become necessary only in unusual cases when p(a) \u2248 0.\nUsing a similar proof strategy, we can show it guarantees to outperform OTM with independent sampling in rare cases.\nTheorem 4 (Superiority over OTM). SpecHub guarantees a higher total acceptance rate compared to OTM with independent sampling if q(a) > 1/2.\nProof. Let Q = q\u22972. Then, for a token x, the highest rate acceptance is upper bounded by the probability that it is contained in any draft pair with probability 1\u2212(1\u2212q(x))2  2q(x). Meanwhile, for the first and second drafts, the acceptance rate when using SpecHub is \ud835\udfd9\ud835\udfd9x\u0338=aq(x)+q(x) . Thus,\nwe can accept more tokens x if (q(x)/1-q(a))  > 2q(x), or\nq(a) > 1/2."}]}