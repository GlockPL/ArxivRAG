{"title": "Tabular Embeddings for Tables with Bi-Dimensional Hierarchical Metadata and Nesting", "authors": ["Gyanendra Shrestha", "Chutain Jiang", "Sai Akula", "Vivek Yannam", "Anna Pyayt", "Michael Gubanov"], "abstract": "Embeddings serve as condensed vector representations for real-world entities, finding applications in Natural Language Processing (NLP), Computer Vision, and Data Management across diverse downstream tasks. Here, we introduce novel specialized embeddings optimized, and explicitly tailored to encode the intricacies of complex 2-D context in tables, featuring horizontal, vertical hierarchical metadata, and nesting. To accomplish that we define the Bi-dimensional tabular coordinates, separate horizontal, vertical metadata and data contexts by introducing a new visibility matrix, encode units and nesting through the embeddings specifically optimized for mimicking intricacies of such complex structured data. Through evaluation on 5 large-scale structured datasets and 3 popular downstream tasks, we observed that our solution outperforms the state-of-the-art models with the significant MAP delta of up to 0.28. GPT-4 LLM+RAG slightly outperforms us with MRR delta of up to 0.1, while we outperform it with the MAP delta of up to 0.42.", "sections": [{"title": "1 INTRODUCTION", "content": "Embeddings are dense numerical representations of real-world objects, expressed as vectors. In NLP and Information Retrieval (IR), embedding vectors commonly correspond to terms in text, and the corresponding vector space is expected to quantify the semantic similarity between them. While not the first, LLMs such as GPT-4 [54], Llama2 [74], FLAN-T5 [17] and others also heavily depend on embeddings. These trained models, sometimes also referred to as Generative AI or GenAI, store millions of such vectors, which are used to generate the response to the user's question. More recently, GenAI models were also trained for images - e.g., DALL \u2013 E2 and videos [39]. Other methods adapt NLP embeddings to obtain embeddings for relational tables [21]. Even though some of these approaches such as transformers [5, 22, 87] attend to every token in all sections of a table, including metadata and data, hence implicitly encode its 2D context, they are not explicitly optimized for complex structured data. To mitigate this limitation, there were a series of efforts [4, 12, 33, 36, 87] to construct more accurate embeddings capturing the intricacies of relational data. These include adding specialized embedding layers or an attention mechanism and pre-training the models on tasks such as table cells or segments recovery [21, 33, 36, 80], thereby making them aware of the tabular structure. The majority of these efforts are devoted only to relational and spreadsheet tables (0.9% and 22% of all tables in the Common Web Crawl [46]) [16]. They overlook the other widely used type of tables that we refer to as \"non-relational\". Unlike relational, they can exhibit not only single-header horizontal, but also multi-level hierarchical vertical, horizontal metadata, as well as nested tables [40]. This creates a gap in understanding these widely used tables in practice. Hence, it is important to take steps to bridge it by enabling machine table understanding for such tables. Several recent attempts [22, 23, 29, 44, 63] try to identify hierarchies and classify cells in such tables. However, these approaches are supervised, and labeling large amounts of such structured data is labor intensive, especially for large-scale datasets. Other recent approaches [14, 21, 33, 37, 48, 75, 84, 87], despite being unsupervised, are optimized for relational or primitive non-relational tables (e.g., matrix tables having just singular non-hierarchical metadata without the rest of their more complex features). \nIn 1st Normal Form [18] a relational table has a set of labeled homogeneous columns, which is not the case for the majority of tables in the real world, especially in medical, financial, and government tables. \nInspired by these major differences compared to more primitive tables, we introduce TabBiN a novel self-supervised, transformer-based architecture to train fine-grained structurally-aware embeddings, optimized for tables with Bi-dimensional hierarchical metadata and Nesting. During pre-training the data cells from such tables are encoded using our novel Bi-dimensional hierarchical coordinates calculated based on their hierarchical and spatial in-table location. Different from the uni-tree structure [67, 80], TabBiN supports both explicit and implicit coordinate encodings, including those for nested tables with their own separate metadata. To enable vertical, horizontal metadata, and data to efficiently aggregate their local neighboring 2D contexts, we propose a metadata-aware attention mechanism that is different from the regular transformer practices of bottom-up attention [61], and constituent attention [79] in NLP domain. We also adopt the Masked Language Model (MLM) pre-training objective from BERT [22] and Cell-level Cloze (CLC) to learn the representations of tokens and cells across a large volume of tables. We make the following contributions in this paper:\nFor non-relational tables, not in 1st Normal Form, exhibiting hierarchical vertical, horizontal metadata, and nesting [40], we propose tabular Bi-dimensional hierarchical coordinates ."}, {"title": "2 PRELIMINARIES", "content": "The following definitions have been taken verbatim from our group's publication [40].\nRelational tables [18], have the following properties: values are atomic, each column has values of the same type, each column has unique name (i.e., attribute name). The set of all attribute names is called table schema or metadata.\nDe f1: Metadata is a set of attributes of a table. Metadata can be stored in a row - e.g., rows \u21161-2 in Figure 1, or in a column - e.g., columns \u21161-2 in Figure 1.\nDe f2: Cell is a data value (i.e., can be a number, string, etc.) found at the intersection of a row and a column in a table. A relational table has C*R cells total, where C is the number of columns and R is the number of rows.\nDe f3: A table with hierarchical metadata is a table that, similar to a relational table, has metadata (i.e., attributes), but unlike a relational table it may be found not only in a row, but also in a column. It may also take several rows or columns. Such rows with metadata are called horizontal metadata (HMD). On the other hand, such columns with metadata are called vertical metadata (VMD) [40]."}, {"title": "2.2 Datasets", "content": "To ensure we have a wide variety of tables we use 5 large-scale structured datasets. These datasets include both relational and non-relational tables.\nWebtables [46]: we took a sample of 20,000 tables in English including both relational and complex non-relational tables. On average, the tables have 14.45 rows and 5.2 columns. The most frequent topics covered in these tables include magazines, cities, universities, soccer clubs, regions, baseball players, and music genres. The cell values contain strings and numbers with and without units and ranges.\nCovidKG is a subset of CORD-19 [56], a public COVID-19 research dataset. We took a sample of 20,000 tables, related to COVID-19 and its vaccination, such as Moderna, Covaxin, Alpha variant, and Gamma variant. The table columns exhibit both VMD and HMD. The cell values contain strings, numbers with and without units, ranges, Gaussians, and nested tables.\nCancerKG dataset has 44,523 tables, extracted from all recent medical publications (up to 12/2023) on colorectal cancer, obtained via PubMed.com. The tables have 227,279 columns total, exhibiting both hierarchical VMD and HMD. The cell values contain strings, numbers with and without units, ranges, Gaussians, and nested tables.\nThe 2010 Statistical Abstract of the United States (SAUS) comprises 1,320 tables [30, 80], which can be downloaded from the U.S. Census Bureau. The tables have 52.5 rows and 17.7 columns on average. It covers a variety of topics, including finance, business, crime, agriculture, and health care.\nThe CIUS dataset [30, 80] is from the Crime In the US (CIUS) database and consists of 489 tables. The tables have 68.4 rows and 12.7 columns on average.\nThe non-relational tables that we defined in this paper are prevalent in our two datasets, CancerKG and CovidKG, constituting over 40% of each dataset. Additionally, approximately 10% of these complex tables exhibit nested structures in both datasets [77]. On average, the complex tables in our datasets consist of approximately 12 rows and 10 columns."}, {"title": "2.3 Bi-dimensional Coordinates", "content": "Figure 1 illustrates the Bi-dimensional coordinates that we introduce for non-relational tables, not in 1st Normal Form with hierarchical vertical and horizontal metadata, with nesting, defined in [40]. Our coordinates correspond to the cell location and the path through the metadata hierarchy to the cell. There are two coordinate-trees \u2013 horizontal and vertical (on the left and top of Figure 1). Both coordinate values correspond to the paths from the root nodes of the trees to the cell. For example, the coordinates of the table, nested in the upper right cell in Figure 1 (Efficacy End Point \u2192 Other Efficacy; Patient Cohort \u2192 Previously Untreated) are (<2,7>;<1,3>). In turn, the coordinates of the second horizontal metadata label (HR) in the nested table are (<3,5>;<4,3>). Notice that our bi-dimensional coordinates also apply to relational tables, whereby they reduce to the regular Cartesian coordinates. Tables in our corpora as well as in large-scale structured datasets in general usually come with unlabeled or noisy metadata. We designed and trained our own binary metadata classifiers based on Deep-learning bi-GRU and CNN architectures specifically for highly accurate labeling of multi-layer metadata - both horizontal and vertical [40]. One can also use other existing techniques for labeling metadata [50, 63]."}, {"title": "3 TABBIN MODEL", "content": "Here we shed some light and provide more details on our Transformer-based self-supervised architecture with metadata-aware structural attention [5, 22] that we created for non-1st normal form tables with nesting and hierarchical metadata. Each encoder block in the Transformer is composed of a multi-head-self-attention layer and fully connected layer [5]. The configuration of our N-layer Transformer encoder model is aligned with BERTBASE [22]. However, we changed the standard BERT multi-head attention form $Q, K, V \\in R^{H \\times H}$ [5], with our metadata-aware mask attention as follows:\n$$T abBiN \\textit{Attention}(Q, K, V) = \\textit{Attention}(Q, K, V) M$$"}, {"title": "3.1 Embedding Layer", "content": "We introduced 6 new embeddings into the embedding layer of our new transformer-based architecture explicitly encoding the bi-dimensional coordinates, semantic information about the entity type (for strings), units (for numerical data), and nested tables. We partition the tables into three segments - data, HMD, and VMD and process them separately to separate contexts for each of these types of data that carry different semantics. We iterate over the table cells row by row to train our data row model. We iterate over the table cells column by column to train our data column model. We tokenize cells using [22], embed tokens jointly and create 6 new embeddings corresponding to: token semantics $E_{tok}$, numerical properties $E_{num}$, in-cell position $E_{cpos}$, in-table position $E_{tpos}$, cell features $E_{fmt}$, and inferred type $E_{type}$ .\nToken. To learn token semantics, we use the vocabulary V defined in [45]. The numbers are tokenized using the special token [VAL] (as indicated in \u201cToken", "as": "n$$E_{tok}=W_{tok} X_{tok}$$\nwhere $x_{tok}$ is the index of the token in V.\nNumber. The numbers are encoded in our embedding vectors using four discrete features, magnitude $x_{mag} \\in [0, M]$, precision $x_{pre} \\in [0, P]$, the first digit $x_{fst} \\in [0, F ]$, and the last digit $x_{lst} \\in [0, L]$ as in [80]. These features are then one-hot encoded. \n$$E_{num} = E_{num_{mag}} \\oplus E_{num_{pre}} \\oplus E_{num_{fst}} \\oplus E_{num_{lst}}$$\n$$E_{num_{mag}} = W_{mag}X_{mag}$$\nwhere  and $E_{num_{lst}} = W_{lst}. X_{lst}$ and $\\oplus$ denotes vector concatenation operator.\nIn-position. The in-cell position refers to the index of a token within a cell (Figure 3 \"In Pos"}, {"as": "n$$E_{c pos} = W_{c pos} X_{pos}$$\nwhere  represents the learnable weight, $x C_{pos}$ is the the one-hot encoded position, $I = 64$ is the pre-defined maximum allowable number of tokens within a cell. We trim tokens in each cell where the length exceeds this limit.\nOut-position. \"Out pos"}, {"title": "3.2 Visibility Matrix", "content": "We introduce a custom visibility matrix to make the attention mechanism attend only to the neighboring structural context of the same kind (i.e., carrying the same semantics), thus avoiding redundant information. The standard self-attention mechanism allows every token in a table to attend to every other token, regardless of where the tokens are \u2013 in the cell, in the same tuple, column or one in the data cell another in the metadata. Spatial information is valuable as it is representative of separate segments of a table carrying different semantics (HMD, VMD, D). Hence it is important to precisely capture and encode it, which we accomplish through our visibility matrix. The standard transformer attention mechanism is also capable of capturing it, but our visibility matrix makes it more explicit [14, 21, 36, 80]. Our visibility matrix is a binary matrix used as an attention mask in the transformer layer during calculation of a multi-head self-attention."}, {"title": "3.3 Pre-training Methodology", "content": "We took the vocabulary and pre-trained token embeddings and encoder weights from BioBERT [45] to initialize TabBiN for pre-training on our 5 datasets. We trained each version of our model for 50,000 steps, batch size 12, learning rate 2e-5. We trained 4 models - 2 for data \u2013 tuples, columns; 2 for metadata \u2013 horizontal, vertical metadata. While reading a row or a column and generating the training sets, we are keeping track of the respective Bi-dimensional coordinates for each cell so that we can include the positional information in our embeddings We add [CLS] at the start of each row/column and [SEP] between the cells. We use table sequences with no more than 256 tokens that we found to be sufficient for our datasets We separate the model pre-training for data and metadata, so their context is treated separately. For example, in TabBiN data column model we pre-trained the model to learn the columnar data context, excluding metadata. We used AWS p3.2xlarge instances. Pre-training of each model took approximately five hours."}, {"title": "3.4 Composite Embeddings (CE)", "content": "For using BioBERT embeddings for numerical values we came up with the idea to have composite structure concatenating (+) embeddings for the attribute, its value and the unit."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "We evaluated our TabBiN embeddings on 3 popular downstream tasks - Column Clustering (CC), Table Clustering (TC) by topic, Entity Clustering (EC). We performed our evaluation on 5 large-scale datasets described above in section 2.2 \u2013 Webtables [46], CovidKG [56], CancerKG, CIUS [30, 80] and SAUS [30, 80]. To compare against the SOTA transformer-based model supporting structured data we fine-tuned TUTA [80]. We also fine-tuned one of the top transformer-based models for biomedical data BioBERT [45], classic Word2Vec [58] embeddings model, and DITTO [49] entity matching model on our data sets."}, {"title": "4.1 Column Clustering (CC)", "content": "For CC we create a composite embedding by concatenating the embedding Ec; for an attribute cj in HMD from our TabBiN-HMD model and the average embedding Ea over data cell tokens for corresponding cj column from our TabBiN-column model (i.e., trained only on columns). We match two columns by calculating the cosine similarity between their TabBiN embedding vectors. We use LSH-based blocking [28] to avoid quadratic complexity for the entire dataset. To cluster columns, for each column, we create a list of similar columns, sorted by the cosine similarity in descending order, the top 20 entries form a cluster. We separate the columns that we have (i.e., 227,279 in CancerKG) into columns having strictly numerical or string values."}, {"title": "4.2 Table Clustering (TC)", "content": "Similarly for TC we create the composite embedding by concatenating the average embedding E\u00afd for data cells from the TabBiN-row model, the average embedding E c for HMD from our TabBiN-HMD model, the average embedding Er for VMD from our TabBiN-VMD model, and the average embedding ET C for the table caption taken from the BioBERT model fine-tuned on our datasets We use cosine similarity as a distance measure between our TabBiN embedding vectors corresponding to the tables to form cohesive clusters. To form clusters, we first calculate a centroid embedding vector for a given topic table. Then, we compute distance from other tables to this centroid vector, sorted in descending order to form the cluster with top 20 entries. We did it for centroids corresponding to different topics and report the MAP/MRR@20."}, {"title": "4.3 Entity Clustering (EC)", "content": "We took sets of columns with labels specific to our datasets (i.e., drugs, vaccines, symptoms, diseases, crime, states, cities, etc.) and extracted their corresponding data values. This approach resulted in very large and high-quality catalogs of entities, both domain-specific and more generic For each dataset the average precision (AP) was calculated by taking a sample of size 40 and having two annotators label them. Next, we selected entities of each of 18 entity types that we work with in each dataset and calculated the cosine similarity between each entity and the remaining entities in the dataset, sorted in descending order, calculated AP@20 for each cluster (formed by taking top 20 entities) corresponding to an entity type and averaged it. We used TabBiN-column model for this EC task."}, {"title": "4.4 TabBiN Performance Highlights", "content": "Column Clustering (CC, Table 4): TabBiN outperforms both TUTA and BioBERT SOTA models on numerical CC task on Webtables with a significant MAP delta of 0.28. Also, TabBiN outperforms BioBERT on large tables by a significant MAP delta of 0.17 on CovidKG. For small tables TabBiN again outperforms BioBERT with a large MAP delta 0.14 on CancerKG. The highest CC MAP of TabBiN is 0.98 and it is achieved on large tables from Webtables, small tables from CIUS, and ranges from SAUS.\nTable Clustering (TC, Table 5, Table 6): TabBiN outperforms TUTA on nested table clustering with a significant MAP delta of 0.17 on CancerKG. On tables with HMD from CovidKG TabBiN outperforms TUTA with a large MAP margin of 0.14. TabBiN outperforms TUTA by a large MAP delta of 0.14 on Webtables with string data. TabBiN achieves the highest TC MAP of 0.95 on Webtables with mixed data. On relational tables from CancerKG, TUTA outperforms us in-significantly, with MAP delta of 0.2.\nEntity Clustering (EC, Table 8): In Table 8, we can see that TabBiN attains the highest MAP across all datasets for EC. TabBiN outperforms TUTA by a small MAP margin of 0.06 for both CancerKG and SAUS respectively. On entity matching (to compare to DITTO, Table 9), TabBiN outperforms Ditto with a small F1 score margin of 1.92%. on structured Amazon-Google dataset. Ditto outperforms TabBiN on Abt-Buy dataset by a small margin of 1.21%. Similarly, on our datasets Ditto insignificantly outperforms TabBiN by 1.24% and 0.37% deltas in F1 measure."}, {"title": "4.5 Composite Embeddings Analysis", "content": "We employed separate composite embedding vectors for CC and TC tasks, as illustrated in Figure 5 earlier. Column Clustering. From Table 10, we can conclude that on both numerical and textual tabular data TabBiN composite embeddings perform the best.\nTable Clustering. From Table 11 we can conclude that on tables with nesting, tables only with HMD, tables with both HMD and VMD and relational tables our composite embeddings perform the best."}, {"title": "4.6 Ablation Studies", "content": "We conduct four ablation studies to demonstrate the efficiency of our visibility matrix, type inference, units and nesting, and bi-dimensional coordinates. For each ablation study, we train the models removing the corresponding target embedding component and then perform TC and CC evaluation tasks on our datasets. \nTabBiN1. Removing our visibility matrix makes TabBiN resort to the standard transformer attention mechanism. We observe that this leads to a substantial MAP/MRR drop on all datasets.\nTabBiN2. Without type inference CC MAP on columns with string data in CancerKG, Webtables, and SAUS drops by 0.1. For TC in relational, non-relational , and Webtables with string data MAP drops by 0.15.\nTabBiN3. Removing Units and Nesting embedding components decreases MAP on nested tables (CancerKG) by 0.25. There is 0.22 decrease in MAP on numerical Webtables. For numerical columns on CC the drop in MAP is 0.21\nTabBiN4. Removing our bi-dimensional coordinates erases the explicit encoding of the positions of all data and metadata cells in two dimensions in the main table as well as in the nested in-cell tables.The removal leads to a significant drop in MAP on CC for both numerical and string columns in CancerKG by 0.12 and 0.11 respectively. Similarly on TC, MAP for nested tables drops by 0.15, MAP for numerical tables in CovidKG drops by 0.13 and MAP for relational tables drops by 0.12.\nWe conclude that removing either of the visibility matrix, type inference, units and nesting, or bi-dimensional coordinates significantly hurts TabBiN performance as evidenced by four ablation studies."}, {"title": "4.7 Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)", "content": "Motivated by the ongoing popularity of LLMs, we compared our embeddings on two large-scale datasets against several major LLMs on two downstream tasks - column and table clustering. We fine-tuned Llama2 and GPT-2 due to their availability in open-source repositories, hence affordability for fine-tuning. We used llama-2-7b-chat model,\nFrom our experiments we conclude that RAG can be used both to improve LLM's performance on our downstream tasks as well as significantly reduce the size of the datasets processed by the LLM, which substantially reduces the cost of using commercial LLMs, especially for large-scale datasets."}, {"title": "5 RELATED WORK", "content": "The authors in [12] construct entity-centric embeddings for relational data. The embedding training sentence generation algorithm in [12] uses a graph, constructed per each entity found in tables (i.e., Paul in Figure 1). It does not take into account the intricacies of structure of the 2D neighboring context as well as does not distinguish data from metadata.\nTABERT model [87] on Wikitables and show it outperforming BERT [22] on two benchmarks - SPIDER text-to-SQL [88] and WikiTableQuestions. Similarly, there are more questions answering models for tables [24, 33, 53, 84] built using a standard transformer architecture. TabPrompt [37] adapts graph contrastive learning using Graph Neural Network to encode tabular data and prompt-based learning to alleviate scarcity of labelled tabular training sets. MotherNet [59] adapts the TabPFN [34] transformer architecture and focuses on supervised classification for small numeric tabular datasets from the OpenML-CC18 Benchmark. HYTEL [14] employs hypergraph-structure-aware transformer to encode tables and uses it for a series of downstream tasks, including column type annotation, column property annotation, table type detection and table similarity prediction TURL [21] is a relational structure-aware transformer, trained and evaluated on several tasks for table understanding, such as relation extraction, row population, cell filling, schema augmentation, entity linking, and column type annotation. Auto-Tables [48] learns a pipeline of data transformation operators using deep learning to transform non-relational tables into relational for query processing using SQL-based tools. NumSearchLLM [71] also leverages LLMs as well as enterprise Knowledge Graphs to perform table search over purely numeric tables. [82] proposes Chain-of-Table method for table understanding tasks, such as table-based question answering and fact verification. [25] extends data discovery process in Data Lakes across two modalities of structured and unstructured data using a model capturing similarities between text documents and tabular columns Tabllm [32] fine-tuned T0 [6] and GPT-3 [62] models for tabular classification. Tabbie [36] and TUTA [80] train embeddings and evaluate them on several different downstream tasks row population, column population, column type prediction, cell and table type classification. Unlike Tabbie, TUTA and other SOTA solutions for relational tables, TabBiN supports complex non-relational tables with nesting, distinguishes data and metadata context, recognizes both vertical and horizontal hierarchical metadata, performs type inference on both metadata and data, uniquely embeds not only numerical values but also ranges, recognizes units and encodes them as separate embeddings vectors."}, {"title": "6 CONCLUSION", "content": "We introduced TabBiN - a structure- and metadata-aware transformer for tables not in 1st Normal Form with hierarchical vertical and horizontal metadata, having nested tables, data values in different units, and numerical ranges. We refer to them as non-relational or BiN tables. Relational tables constitute only 0.9% of all tables in the common Web crawl [46] and 22% of spreadsheet tables, while the rest are non-relational. To the best of our knowledge, TabBiN is the first transformer-based unsupervised architecture optimized for intricacies of structural context in these tables, respecting units in numerical values, and treating ranges and gaussians according to their semantics, not blindly as a sequence of numbers as in many SOTA solutions. TabBiN also performs semantic type inference on the table content as well as its metadata and encodes inferred types as an additional component in the embedding layer. This fine-grained understanding and \"special treatment\" of non-relational tables with hierarchical metadata and nesting, allows TabBiN to outperform SOTA on three popular downstream tasks on five large-scale structured datasets with the significant MAP delta of up to 0.28. GPT-4 LLM+RAG slightly outperforms us with MRR delta of 0.1, but we significantly outperformed it with the MAP delta of up to 0.42."}]}