{"title": "LAUREL: Learned Augmented Residual Layer", "authors": ["Gaurav Menghani", "Ravi Kumar", "Sanjiv Kumar"], "abstract": "One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which has led to significantly better model convergence and quality. Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures, the backbone of LLMs.\nIn this paper we introduce Learned Augmented Residual Layer (LAUREL)\u2014a novel generalization of the canonical residual connection-with the goal to be an in-situ replacement of the latter while outperforming on both model quality and footprint metrics. Our experiments show that using LAUREL can help boost performance for both vision and language models. For example, on the ResNet-50, ImageNet 1K task, it achieves 60% of the gains from adding an extra layer, while only adding 0.003% more parameters, and matches it while adding 2.6\u00d7 fewer parameters.", "sections": [{"title": "1. Introduction", "content": "Model efficiency is of critical importance in the age of extremely large language and vision models. Even if a given model's quality is good, its footprint metrics such as train-time compute required, inference latency, resident memory size, etc. dictate if it can be experimented with and/or deployed in real-world settings. These metrics are directly tied to the financial costs of deploying the model in production and user-perceived responsiveness of systems dependent on these models.\nConsequently, improving the Pareto-frontier of model quality vs footprint, via efficient deep learning methods has been an area of active research in the past few years. Areas of interests span from algorithmic techniques (Menghani, 2023), to efficient hardware (Sze et al., 2017), to best practices around model efficiency (Dehghani et al., 2022), etc.\nOne of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which had led to significantly better model convergence and quality (He et al.). Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures (Vaswani et al., 2017), the backbone of LLMs.\nIn this paper we introduce learned augmented residual layer, LAUREL, which generalizes the canonical residual connection. Recall that deep-learning models with residual connections have a 'block' structure, with many blocks chained together between the input and final output; these could be convolution/identity blocks within a ResNet, a transformer block in a transformer encoder/decoder, etc. Within a block, a typical residual connection is given by:\n$X_{i+1} = f(x_i) + X_i$.\nHere, f(\u00b7) can be any non-linear function such as attention, MLP, multiple non-linear layers, etc., $x_i$ is the input to the said non-linear function, and $x_{i+1}$ is the combined output of the non-linear function and the residual component. Refer to Figure 1 for an illustration. To simplify exposition, we ignore pre-processing functions such as layer norm, which can be folded into f(\u00b7) without loss of generality."}, {"title": "2. Learned Augmented Residual Layer", "content": "In this section we describe the main idea behind LAUREL. In its most general form, we reformulate the residual connection to be the following:\n$X_{i+1} = a. f(x_i) + g(x_i, X_{i-1},...,x_o)$.\nHere a is a learned scalar parameter, and g(\u00b7) is a learned linear function with $X_i, X_{i-1},...,x_o$ as inputs, where $x_j$ is the output of the jth residual connection. The intuition behind LAUREL is that one can learn a richer set of (linear) functions than just using $x_i$ as the residual component. One motivation behind seeking these richer linear functions is the concept of a \"residual stream\" (Elhage et al., 2021), where the residual connection is considered to be part of a stream of information that passes through each layer without being exposed to any non-linearities. This allows the learning process to focus on the non-linear components better.\nEach layer / operation can read from, and subsequently write to this residual stream based on what it read. Given that"}, {"title": "2.1. Residual Weights Version (LAUREL-RW)", "content": "In this version, we keep a learnable and set $g(x_i, ..., X_o) = \\beta x_i$. Therefore, (2) can be rewritten as:\n$X_{i+1} = af(x_i) + \\beta x_i$.\nNotice that this version assigns learnable weights to the $f(x_i)$ and $x_i$ from (1). In practice, we found that we cannot let a and \u03b2 grow unbounded, and using a normalization function such as softmax helps. Clearly, this version will add only two new parameters per LAUREL layer. If necessary, we can always replace these two parameters by a single learnable parameter and use the sigmoid function to define a, \u03b2 in terms of this single parameter."}, {"title": "2.2. Low-Rank Version (LAUREL-LR)", "content": "In this version, we fix a = 1, and $g(x_i) = Wx_i$ in (2) to obtain\n$X_{i+1} = f(x_i) + Wx_i,$\nwhere W is learnable. Note that, as written, W is a D\u00d7D matrix, where D is the model dimension; hence this will add $D^2$ new parameters (per LAUREL layer) to the model."}, {"title": "2.3. Previous Activations Version (LAUREL-PA)", "content": "In this version, we use use activations from previous blocks. In particular, we set $g(x) = \\sum_{j=0}^{i} \\gamma_j h(x_j)$, where $\\gamma_0,..., \\gamma_j$ are learned parameters and h is another linear function.\u00b9 This allows us to rewrite (2) as:\n$X_{i+1} = f(x_i) + \\sum_{j=0}^{i} \\gamma_j h(x_j)$.\nIn practice, we replace h by a low-rank product similar to the LAUREL-LR version. The number of new parameters is 2rD + N, where N is the number of layers.\nNote that, all three versions above are a combination of scalar and/or low-rank products on top of the vanilla resid-ual connection in (1). This makes LAUREL especially light-weight in terms of its impact on model size and la-tency. Moreover, the framework is generic enough to allow combinations of the above versions, as well as new versions.\nFor simplicity, we fix a = 1."}, {"title": "3. Experiments", "content": "We experiment with LAUREL in two domains, namely, vision and language. For the first case, our goal is to improve the image classification accuracy of the ResNet-50 model on the ImageNet-1K dataset (Deng et al., 2009). For the second case our goal is to improve the performance of an LLM, evaluated after the pre-training stage, on common benchmarks.\nThe underlying motivation behind these experiments is not necessarily to improve on the SOTA results, but to show how LAUREL can be easily integrated on top of common model architectures with residual/skip connections in order to achieve a better model quality and footprint trade off."}, {"title": "3.1. ResNet-50 on ImageNet-1K", "content": "In this setup we train a standard ResNet-50 model on the ImageNet 1K dataset (Deng et al., 2009) using 16 Cloud TPUv5e chips over one epoch with data-augmentation turned on. In order to obtain a strong baseline, we fine-tuned the model learning rate schedule and picked a schedule that maximized the average of the best accuracy@1 values over 5 trials (which we simply refer to as accuracy in this subsection). The baseline model that we obtained achieves an accuracy of 74.95\u00b10.016%.\nIn addition, we also find that if we simply add another layer to the ResNet-50 model (i.e., naive scaling), we can increase the model's accuracy by 0.25% to reach 75.20%, while adding 4.37% new parameters. With that in context, applying LAUREL on the model leads to better results (see Table 1).\nIf we only use the LAUREL-RW version, we get an im-provement of 0.15% on average with only 0.003% extra parameters, which is essentially negligible. When we try the LAUREL-RW+LR version with r = 16, we achieve an accuracy of 75.20% while adding only 1.68% extra param-eters; this matches the performance of the baseline with an extra layer, while using 2.6\u00d7 fewer extra parameters. Addi-tionally, when we use the combined LAUREL-RW+LR+PA version we improve the accuracy to 75.25% while still us-ing 1.82\u00d7 fewer extra parameters than the baseline with one extra layer, demonstrating that LAUREL is superior to naively scaling the model. Notably even though we make fundamental changes to the residual connection we did not find any training instabilities when using LAUREL."}, {"title": "3.2. Decoder-only LLM Pre-training", "content": "In this setup, our goal is to test the performance of LAUREL with Large Language Models (LLMs). For our baseline, we chose a 3B parameter decoder-only model based on the Transformer architecture. We pre-trained both the baseline,"}, {"title": "3.3. LAUREL-LR: Rank vs Accuracy", "content": "We note that for the LAUREL-LR version on the ResNet-50/ImageNet combination, there is a pattern in terms of the best accuracy achieved with different values of r. In the combined LAUREL-RW+LR version we experimented with different values of r, and computed the average of the best accuracy@1 achieved over 5 trials; see Figure 3. From Table 1, with the LAUREL-RW version alone we already achieve an average best accuracy@1 of 75.10%, therefore for the combined LAUREL-RW+LR version we would like to see the accuracy exceeding that.\nWe observe that when r is small (r \u2208 {4,8}), there is not a significant improvement over the baseline LAUREL-RW experiment. This could be because a very small r acts as an information bottleneck in the low-rank product in (3). As r increases, the accuracy reaches the maximum for r = 16, 32; beyond this, the accuracy seems to drop though still higher than the LAUREL-RW baseline. We believe this unimodal phenomenon could be due to the number of parameters added to the model, which increases linearly in r, which would require appropriate tuning of hyper-parameters such as the learning rate as well as the regularization penalty."}, {"title": "4. Related Work", "content": "Since our larger goal is to improve the training and inference efficiency of deep learning models, we briefly discuss some research directions aimed at improving model efficiency."}, {"title": "Architectural Changes:", "content": "Our work is inspired by recent model architecture improvements such as LoRA (Hu et al., 2022) and AltUp (Baykal et al., 2023) amongst others. However, they are not directly relevant to LAUREL. Indeed, LoRA is designed to efficiently fine-tune large pre-trained models and it works directly on the model weight matrices level by introducing low-rank \u2018adapter' weights that are learned during the fine-tuning stage, while other model weights are held constant. In contrast, LAUREL works at the residual connection level, which likely spans multiple weight matrices involved in the function f; furthermore, it is applied during the pre-training stage.\nAltUp (Baykal et al., 2023) is designed to replicate the quality improvements of a model with a large model dimension, without having to pay the additional cost. It operates at the transformer-block level, constructing parallel 'lightweight' transformer blocks to approximate the model dimension scaling effect. In contrast, LAUREL operates at the residual connection level and does not aim to replicate the dimension scaling effect.\nInterestingly, LAUREL can be applied in conjunction with both LoRA (during fine-tuning) and AltUp (during pre-training and fine-tuning).\nHe & Hofmann (2023) proposes several changes to trans-former blocks to help improve model convergence; however these proposals are limited to transformer blocks."}, {"title": "Compression Techniques:", "content": "Model compression techniques (Bucilu\u0103 et al., 2006) such as quantization (Krishnamoorthi, 2018; Jacob et al., 2018), including ternary networks (Li et al., 2016; Ma et al., 2024) are commonly used to reduce model size and inference latency. Similarly, pruning and model sparsity (Gale et al., 2019; Liu et al., 2019) techniques have also been explored and implemented in hardware."}, {"title": "Learning Techniques:", "content": "Distillation (Hinton et al., 2014) is a popular technique for improving a smaller (student) model quality using \"soft-labels\" from a larger, impractical (teacher) model (Sanh et al., 2019). Some distillation variants propose learning intermediate representations as well (Zagoruyko & Komodakis, 2016; Kim et al., 2023). Other techniques include works like Stacking (Reddi et al., 2023) and RaPTr (Panigrahi et al., 2024), which progressively grow and train the network to achieve an improved model quality while reducing model training time."}, {"title": "5. Conclusion", "content": "In this paper we introduce the LAUREL framework, which is a novel architectural change and a generalization of the residual / skip connection aimed at improving the model quality without significantly increasing the model size or latency. We study three versions (LAUREL-RW, LAUREL-LR, LAUREL-PA) that can be mixed-and-matched together, as we show in our experiments.\nThrough experiments, we demonstrate the efficacy of replac-ing the conventional residual connection with LAUREL on both vision and language tasks, while also providing evi-dence for its advantages over naive model scaling methods. In the future, we would like to try LAUREL and its variants on other architectures such as Vision Transformers (ViT) (Dosovitskiy et al., 2020) and related tasks."}]}