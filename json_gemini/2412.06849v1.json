{"title": "GL-FUSION: RETHINKING THE COMBINATION OF GRAPH NEURAL NETWORK AND LARGE LANGUAGE MODEL", "authors": ["Haotong Yang", "Xiyuan Wang", "Qian Tao", "Shuxian Hu", "Zhouchen Lin", "Muhan Zhang"], "abstract": "Recent research on integrating Large Language Models (LLMs) with Graph Neural Networks (GNNs) typically follows two approaches: LLM-centered models, which convert graph data into tokens for LLM processing, and GNN-centered models, which use LLMs to encode text features into node and edge representations for GNN input. LLM-centered models often struggle to capture graph structures effectively, while GNN-centered models compress variable-length textual data into fixed-size vectors, limiting their ability to understand complex semantics. Additionally, GNN-centered approaches require converting tasks into a uniform, manually-designed format, restricting them to classification tasks and preventing language output. To address these limitations, we introduce a new architecture that deeply integrates GNN with LLM, featuring three key innovations: (1) Structure-Aware Transformers, which incorporate GNN's message-passing capabilities directly into LLM's transformer layers, allowing simultaneous processing of textual and structural information and generating outputs from both GNN and LLM; (2) Graph-Text Cross-Attention, which processes full, uncompressed text from graph nodes and edges, ensuring complete semantic integration; and (3) GNN-LLM Twin Predictor, enabling LLM's flexible autoregressive generation alongside GNN's scalable one-pass prediction. GL-Fusion achieves outstand performance on various tasks. Notably, it achieves state-of-the-art performance on OGBN-Arxiv and OGBG-Code2,", "sections": [{"title": "INTRODUCTION", "content": "Research in Graph Neural Networks (GNNs) has long focused on learning from graph with pre-processed vector features, often overlooking the rich textual information contained in raw data. Recently, many studies have recognized that better utilization of these text features can enhance performance. This has led to a focus on text-attributed graphs (TAGs), graphs with node, edge, and graph-level text attributes. In addition, some tasks may contain a task description, questions, and candidate answers in natural language. The combination of GNNs and pretrained Large Language Models (LLMs), which can efficiently encode these text attributes and information, has garnered significant interest.\nTwo main approaches have emerged for combining GNNs and LLMs: GNN-centered methods and LLM-centered methods. GNN-centered methods focus on typical graph tasks like node classification and link prediction. These methods convert text into representation vectors for graph nodes"}, {"title": "PRELIMINARY", "content": "For a tensor $Z \\in \\mathbb{R}^{a\\times b}$, let $Z_i \\in \\mathbb{R}^{b}$ denote the i-th row, $Z_{:,j} \\in \\mathbb{R}^{a}$ denote the j-th column, and $Z_{ij} \\in \\mathbb{R}$ denote the element at the (i, j)-th position.\nText-Attributed Graph (TAG) A TAG is represented as $G = (V, E, X)$, where $V = {1, 2, 3, ..., n}$ is the set of n nodes, $E \\subseteq V \\times V \\times \\mathbb{Z}^{L_e}$ is the set of edges. Each edge $(i, j, E_{ij}) \\in E$ connects node i and node j with text $E_{ij}$ consisting of $L_e$ tokens. The node text feature matrix is denoted as $X \\in \\mathbb{Z}^{n\\times L_n}$, where $X_i$ represents the $L_n$-token text feature of node i. Here, $L_n$ and $L_e$ represent the maximum lengths of node and edge text, respectively. Text of varying lengths is padded to the same length.\nProblem Settings We consider a TAG G and a task description $T \\in \\mathbb{Z}^{L_t}$. The task can be node classification, link prediction, graph classification, or text generation. As illustrated in Figure 1, our model processes a TAG along with a task description. The graph is integrated into the task description as part of the input sequence to our model, where the graph forms a subsequence starting with a special <graph_start> token and ending with a <graph_end> token. Each node is represented by a <node> token. For each specific task, we use a task-specific prediction head, and the model also leverages the LLM itself to generate the answer in text form. We focus on the supervised learning setting, where the dataset is split into training, validation, and test sets. Models are trained on the training set, hyperparameters are tuned on the validation set, and test set performance is reported."}, {"title": "GL-FUSION: A HIGHLY INTEGRATED GNN-LLM MODEL", "content": "This section presents the architecture of our GL-Fusion model, which integrates structure-aware transformer layers, graph-text cross-attention blocks, and GNN & LLM twin predictors. Given input sequence $T \\in \\mathbb{Z}^{L_t}$ and input graph $G = (V, E, X)$, we first convert T to sequence representation $t \\in \\mathbb{R}^{L_t\\times d}$ with token embedding layer. Then we use an existing text encoder (BehnamGhader et al., 2024) to convert node text sequences X to uncompressed text embeddings $x \\in \\mathbb{R}^{n\\times L_n\\times d}$, and we also convert edge text $E_{ij} \\in \\mathbb{Z}^{L_e}$ to compressed text embedding $e_{ij} \\in \\mathbb{R}^d$. Our GL-Fusion model will update input sequence representations t in each transformer layer and use t to produce the final prediction."}, {"title": "STRUCTURE-AWARE TRANSFORMER LAYER", "content": "To encode the input sequence, a mixture of text and node tokens, we propose a structure-aware transformer layer that fulfills the following properties: (1) Preserving causality for text generation. (2) Maintaining invariance to node permutation. (3) Encoding edges between nodes. Figure 2a illustrates this structure-aware architecture. Compared to ordinary transformer layers, it includes the following extensions.\nGraph-aware attention mask and positional encodings To preserve causality for text generation, a causal mask keeps only the lower triangle of the attention matrix, setting all other entries to zero. In other words, each token can only aggregate embeddings from previous tokens, prohibiting the use of representations from tokens that come after it. While directly applying the causal mask to the mixed sequence preserves causality for text generation, it disrupts permutation equivariance; that is, each graph token can only aggregate information from graph tokens preceding it, making the order of graph tokens affect the output. This violates the permutation invariance of node indices, which is crucial for graph learning.\nTo address this, we propose the following attention masks. All tokens can aggregate information from any token before them. However, for node tokens, other node tokens within the same graph are also visible to them, preserving permutation invariance. Additionally, causality for text generation is maintained as graph tokens are not generated. Furthermore, to preserve causality, we only allow graph tokens to attend to text tokens that come before them, and text tokens can only attend to graph tokens that come before them. The resulting attention mask is shown in Figure 2b.\nTo further ensure permutation invariance, we assign the same positional encodings to all node tokens in the same graph as the corresponding <graph_start> token. Additionally, all graph tokens use a single index, as shown in the positional encoding (PE) of Figure 2a, preventing the model from running out of the context window for large graphs.\nMessage Passing with Multiple Aggregators. To encode graph structure, we involve a message passing layer (Gilmer et al., 2017) in our transformer layer. Our message-passing layer updates the representation of node u as follows:\n$h_u' = COMBINE(h_u, AGGREGATE({h_v \\odot e_{uv} | v \\in N(u)})),$ (1)"}, {"title": "GRAPH-TEXT CROSS-ATTENTION", "content": "One significant drawback of previous GNN-centered models is that they compress node, edge text features, and task descriptions into fixed-size representations, while LLM-centered methods tend to compress graph structure into fixed-size representations, leading to significant information loss. With our structure-aware transformer layer, we encode the graph structure and task description directly in each layer without compression. To avoid compression of node/edge text features, we introduce the Graph-Text Cross-Attention block to enable node token representations and text tokens generated after the node token to extract information from the raw node text. Note that we can also use the cross-attention block to extract information from edge text. However, most datasets currently have simple and unvaried edge features, so we primarily focus on extracting information from raw node text.\nThe architecture is shown in Figure 3. The cross-attention block extracts features from the node text representation x to update the task representation t as follows.\nTo update $t_i$, the representation of token i, first, each node's text representation of $L_n$ tokens is aggregated into a single representation. For node v,\n$x_{iv}' = softmax(\\frac{t_i W_Q W_K^T x_v}{\\sqrt{d}}) x_v \\in \\mathbb{R}^d$, (3)\nwhere $t_i \\in \\mathbb{R}^d$ is the query of cross attention, $W_{Q1}, W_{K1} \\in \\mathbb{R}^{d \\times d}$ are learnable linear layer for query and key, and $x_v \\in \\mathbb{R}^{L_n\\times d}$ is the uncompressed text representation of node v. $x_{iv}'$ is the feature extract from node v for updating token i's representation. Then, all nodes' features are further aggregated:\n$\\hat{x_i} = \\begin{cases} x_{iv}' & \\text{if i is the node token w.r.t. node v}, \\\\ softmax(t_i^T W_Q W_K x_v) x_v & \\text{if i is a text token}, \\end{cases}$ (4)\nwhere $v$ is the set of nodes whose token in input sequence is before i, $\\hat{x_v} \\in \\mathbb{R}^{|v|\\times d}$ is the sequence of $x_{iv}'$ for node v in v, and $W_{Q2}, W_{K2} \\in \\mathbb{R}^{d \\times d}$ are learnable linear layers. Then $\\hat{t_i} \\in \\mathbb{R}^d$ is used"}, {"title": "GNN & LLM TWIN PREDICTOR", "content": "Since our model uses a transformer architecture and maintains causality, it can naturally generate text outputs, which we refer to as the LLM predictor. Additionally, since graph nodes are treated as tokens in the input sequence, the transformer's output representations for these tokens can be considered node-level GNN representations, which can be fed into pooling layers and linear layers to produce predictions\u2014referred to as the GNN predictor. Both predictors have their advantages and disadvantages, as outlined below:\nRationale for GNN as a Predictor. While LLMs can perform various language tasks, they face limitations in autoregressive generation, including:\nNatural Language Output: LLMs can natively produce text predictions, which is challenging for the GNN predictor.\nNumerical Output: GNN predictors naturally output numerical values, making them well-suited for regression or ranking tasks. In contrast, LLMs produce token sequences, making it difficult to generate numerical outputs as text.\nScalability: LLMs predict outputs one by one in an autoregressive manner, which can be inefficient for tasks requiring multiple predictions for all nodes or edges. In contrast, GNNs generate all predictions for all nodes or edges in parallel.\nTraining Efficiency: Training models end-to-end through LLM-generated sequences is more indirect and harder to control, as the LLM's loss function is constrained by \"perplexity\" (i.e., the cross-entropy loss on token space), which often includes task-irrelevant tokens instead of focusing solely on the task-relevant labeling space. With GNNs, we can provide direct and dense supervision on the target classes, improving training efficiency.\nTo leverage both approaches, we employ a twin-predictor architecture, allowing for simultaneous predictions from both the GNN and LLM. A graph readout component, tailored to specific tasks, is added after the final layer for GNN predictions. This component is trained using cross-entropy loss for classification tasks or mean squared error for regression tasks. During inference, it can generate predictions for graph nodes as needed."}, {"title": "RELATED WORK", "content": "LLM-Centered Approaches LLM-centered methods leverage pretrained language models to handle graph data by translating graph structures into text formats, like adjacency lists (Liu & Wu, 2023; Guo et al., 2023b; Chen et al., 2024). While these methods have shown some promise, they often suffer from issues related to node and edge ordering and can struggle with large graphs due to the limited context windows of LLMs. A more effective solution has been to use GNNs to generate node representation sequences that are fed into LLMs. For example, Chai et al. (2023) used embeddings from Message Passing Neural Networks (MPNNs) to represent target nodes, enabling the LLM to answer basic structural questions. Similarly, Tang et al. (2024) aligned both structural and textual inputs using GNNs and Transformers, achieving better accuracy by processing all node"}, {"title": "EXPERIMENTS", "content": "To evaluate the potential of GL-Fusion as a new architecture combining GNN and LLM, we conducted experiments on various tasks. These include synthetic tasks to validate its capacity to capture basic graph properties, traditional GNN tasks such as node classification and link prediction to assess its ability to solve graph-related tasks, commonsense question-answering tasks to test its ability to leverage knowledge graphs for language generation, and code graph tasks to evaluate its capacity to generate text based on graph structures. Through these experiments, we demonstrate GL-Fusion's strong potential to effectively combine GNN and LLM architectures. Further details on the experiments can be found in Appendix A."}, {"title": "BASIC GRAPH PROPERTY PREDICTION", "content": "Following Guo et al. (2023a), we evaluate our model on basic graph property prediction tasks. We consider two tasks: degree (to predict a node's degree with the whole graph as input) and edge (to predict whether there exists an edge between two nodes in the graph). These two properties are simple for GNN-centered methods. However, for LLM-centered methods, graph structural features are often compressed and thus incomplete, leading to difficulties. The baselines include LLM-centered methods such as EdgePrompt, GML, and GraphML from Guo et al. (2023a). We also conduct an ablation study on these two tasks. Additionally, we introduce a node text retrieval task, where models attempt to predict the input sentence of a node. The results are shown in Table 1. In general, GL-Fusion outperforms all these baselines and captures basic graph properties perfectly."}, {"title": "NODE CLASSIFICATION", "content": "We demonstrate our model's ability to leverage both textual features and graph structure in TAG through a node classification task. We evaluate GL-Fusion on two datasets: ogbn-arxiv (Hu et al., 2020) and Cora (Yang et al., 2016). The baselines include GCN (Kipf & Welling, 2017), the GNN-centered model GLEM (Zhao et al., 2023), XRT (Chien et al., 2022), OFA (Liu et al., 2023), and the LLM-centered methods GPT4graph (Guo et al., 2023a), MuseGraph (Tan et al., 2024), and GraphGPT (Tang et al., 2024). The results are shown in Table 2. We also conduct experiments in few-shot settings following (Huang et al., 2023). The results are shown in Table 3. #shot means the number of training set per class. Baselines are from Huang et al. (2023). Our model outperforms existing models significantly. Besides these datasets, we also conduct experiments on CSTAG benchmark (Yan et al., 2023). We use the datasets and baseline in (Yan et al., 2023). PLM-Based models uses LLM to encode target node text only. GNN-Based methods uses GNN to encode graph with node feature provided by fixed LLM. Co-Training methods denote methods training GNN and LLM simultaneously. The results are shown in Table 4. Our GL-Fusion significantly outperforms all baselines on most datasets, verifying the model's capacity for ordinary graph tasks."}, {"title": "KNOWLEDGE GRAPH COMPLETION", "content": "We demonstrate our model's ability to leverage both textual and structural information through the knowledge graph completion task, using the Wikidata Knowledge Graph, which provides rich textual descriptions and structural relationships. We employ the FB15k-237-ind dataset (Teru et al., 2020), extracted from Freebase (Bollacker et al., 2008), featuring four standard training and test splits with shared relation types but disjoint entities. Each node and edge is annotated with textual attributes: entities include names and brief descriptions, while edges retain their textual representations from Freebase. Following approaches like NBFNet (Zhu et al., 2022) and UniLP (Mikhail et al., 2024), we annotate nodes with distances to their corresponding head or tail nodes for prediction tasks.\nFor baselines, we select several representative inductive learning GNNs and KG-BERT (Yao et al., 2019) for LLM-based completion. Recent methods using LLMs for KG completion, such as few-shot prompting (BertRL)(Zha et al., 2021) or explicit rule learning (KRST)(Su et al., 2023), are also included as they focus on different techniques. Our results, shown in Table 5, indicate that our model outperforms all baselines across the four splits, effectively utilizing both linguistic and structural information."}, {"title": "COMMON SENSE QUESTION ANSWERING", "content": "CommonsenseQA (Talmor et al.) is a 5-way multiple choice Question Answering task that requires reasoning with commonsense knowledge, containing 12,102 questions. It can be solved with an"}, {"title": "GRAPH-TO-LANGUAGE GENERATION", "content": "While image-to-text and video-to-text generation are well-studied, graph-to-language generation remains underexplored. We evaluate our GL-Fusion model on the ogbg-code2 dataset (Hu et al., 2020), where each graph represents the Abstract Syntax Tree (AST) of a Python function, and the goal is to predict the function name from the AST. Previous methods have treated this as a classification task, assuming uniform function name lengths and a limited token set, which is impractical. In contrast, our model generates text directly. The results are shown in Table 6. Our GL-Fusion model outperforms state-of-the-art methods (Velickovic et al., 2018; Luo et al., 2023) signficantly, verifying the strong potential of our architecture."}, {"title": "ABLATION STUDY", "content": "To verify the effectiveness of our designations, we conduct ablation study on ogbn-arxiv dataset. The results are shown in Table 8. In the table, Low rank denotes using model with LoRA rank=8. w/o cross atten removes the graph-text cross attention. w/o gate removes the gate mechanism. w/o aggrs removes multiple aggregators in the message passing modules in transformer layers and uses mean aggregator only. w/o gnn pred removes the gnn prediction and training loss on it. w/o text pred removes the text prediction and training loss on it. Each module contributes to the overall performance, and removing any of them results in a performance drop."}, {"title": "CONCLUSION", "content": "This paper addresses key challenges in integrating GNNs and LLMs, such as independent encoding of text and structure, task-agnostic text representation, excessive information compression, and issues with output scalability and flexibility. We propose the GL-Fusion architecture, featuring three innovations: 1. Integrated Structural and Textual Learning by combining MPNNs with self-attention layers; 2. Graph-text cross-attention modules that preserve full textual content to prevent information loss; and 3. A GNN-LLM twin-predictor that facilitates predictions as both LLM and GNN for enhanced scalability and flexibility. Our experiments on the various datasets demonstrate that GL-Fusion effectively leverages language and structure, significantly improving reasoning performance in graph-text tasks."}, {"title": "LIMITATIONS & BROADER IMPACTS", "content": "We recognize several limitations that can guide future research. First, while our architecture unifies GNNs and LLMs and shows effectiveness across various tasks, we have not tested it across a comprehensive range of task types due to the diversity of domains and data properties. Future work will involve refining our model on a broader spectrum of tasks to address these gaps.\nAdditionally, although our architecture provides a solid foundation for graph-text integration, our experiments have relied on training separate models individually. We have not yet established a unified set of pretrained parameters for all tasks due to resource and data limitations. Large-scale multi-task pretraining remains an objective.\nRegarding societal impacts, while our model aids in processing graph data, it also risks generating incorrect or misleading information. Research focused on enhancing the reliability of LLM outputs is crucial, and our work can support these efforts."}, {"title": "EXPERIMENTAL DETAILS", "content": "Architecture All our models are based on Llama-3-8b, which utilizes a 32-layer Transformer, with message passing at layers 0, 4, 8, 12, 16, 20, 24, and 28, and node cross-attention at layers 3, 7, 11, 15, 19, 23, 27, and 31. For the original LLaMA-3 layers, we employ LoRA with rank r and weight \u03b1 = 2r, while newly added layers are trained with full parameters. The LLM used for sentence encoding is LLM2Vec (BehnamGhader et al., 2024) model based on Llama-3-8b, is fine-tuned with Lora on its last layer. For ogbl-arxiv dataset, we use r = 64. For CSTAG datasets, we use r = 4. For all other datasets, we use r = 32. GNN in our model in implemented with torch geometric (Fey & Lenssen, 2019). Some parameters are loaded from pretrained LLM.\nFor structure-aware Transformer Layers, we leverage the pretrained Llama-3-8B model as the backbone. As detailed in Section 3.1, we introduce new parameters through modifications to positional encoding and attention masks. Additionally, for the <graph_node> token, we simply add an embedding to the existing layer. The newly added MPNN component is initialized randomly. We fine-tune the transformer parameters using the Low-Rank Adaptation (LoRA) method from the peft library. This approach freezes the original transformer layers and optimizes only the newly introduced low-rank layers. The MPNN parameters, on the other hand, are trained entirely from scratch.\nFor the Graph-Text Cross-Attention layers in our paper, all parameters are initialized randomly and trained from scratch.\nFor the GNN prediction modules, which are simple linear layers for generating outputs, parameters are initialized randomly and trained from scratch.\nOverall, most parameters are initialized from pretrained models and frozen. Only about 10% of the parameters in the entire model are optimized.\nTraining Process All experiments are trained on A800 GPUs with one epoch. The optimizer is AdamW with learning rate=3e-5, weight decay=0.1. For node classification and csqa datasets, we ensemble the prediction of GNN and LLM. For link datasets, we use GNN output only. For graph level tasks and synthetic tasks, we use text output only. The entire model is trained end-to-end, with joint training of all components.\nBaselines Baselines and experiments setting used in different works on combining GNN and LLM varies a lot. Therefore, we directly use the results reported by baseline works as shown in the maintext."}, {"title": "DATASET DETAILS", "content": "DETAILS OF FB15K-237-IND DATASETS\nThe statistics of the FB15k-237-ind is in Table 9. To generate the text attributes of the datasets, we first map Freebase objects to wikidata\u00b9, and then used the detailed texts from the wikiKGv2- 90m dataset in OGB-LSC (Hu et al., 2021) as the textual representations for each node. Like other"}]}