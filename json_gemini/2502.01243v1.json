{"title": "OPHTHBENCH: A COMPREHENSIVE BENCHMARK FOR EVALUATING LARGE LANGUAGE MODELS IN CHINESE OPHTHALMOLOGY", "authors": ["Chengfeng Zhou", "Ji Wang", "Juanjuan Qin", "Yining Wang", "Ling Sun", "Weiwei Dai"], "abstract": "Large language models (LLMs) have shown significant promise across various medical applications, with ophthalmology being a notable area of focus. Many ophthalmic tasks have shown substantial improvement through the integration of LLMs. However, before these models can be widely adopted in clinical practice, evaluating their capabilities and identifying their limitations is crucial. To address this research gap and support the real-world application of LLMs, we introduce the OphthBench, a specialized benchmark designed to assess LLM performance within the context of Chinese ophthalmic practices. This benchmark systematically divides a typical ophthalmic clinical workflow into five key scenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each scenario, we developed multiple tasks featuring diverse question types, resulting in a comprehensive benchmark comprising 9 tasks and 591 questions. This comprehensive framework allows for a thorough assessment of LLMs' capabilities and provides insights into their practical application in Chinese ophthalmology. Using this benchmark, we conducted extensive experiments and analyzed the results from 39 popular LLMs. Our evaluation highlights the current gap between LLM development and its practical utility in clinical settings, providing a clear direction for future advancements. By bridging this gap, we aim to unlock the potential of LLMs and advance their development in ophthalmology.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized a wide range of applications driven by their remarkable ability to understand and interpret real-world contexts [1, 2, 3, 4, 5]. In the medical domain, LLMs have demonstrated significant potential across diverse applications, such as disease management [6], medical document summarization [7, 8], clinical workflows optimization [9], and patient-clinical trial matching [10, 11]. Despite these advancements, ongoing debates persist regarding the ethical use of LLM technology in healthcare, addressing concerns such as \u201challucination\"(generating plausible-sounding but inaccurate or misleading content), biases inherent in training data, and broader ethical considerations [12, 13, 14, 15]. Accurately assessing LLMs' potential and inherent limitations is urgently needed to ensure their beneficial use for patients and practitioners."}, {"title": "2 Related Work", "content": "Benchmarking is a critical step in advancing large language models, serving as a cornerstone to guide their application and development. However, as expectations for LLMs continue to rise, general language understanding benchmarks no longer suffice for specialized professional domains. In the medical domain, numerous datasets and benchmarks [16, 19, 20, 3, 39, 21, 25, 26, 27, 40, 32, 24, 28, 33] have emerged to evaluate the medical capabilities of LLMs. These studies aim to enhance clinical utility and alleviate the workload of healthcare professionals. For example, Pal et al. [16] established their benchmark with 194k high-quality AIIMS & NEET PG entrance exam multiple-choice questions, and Wang et al. [40] introduced the CMB, a localized medical benchmark designed entirely within the native Chinese linguistic and cultural framework.\nAmong these studies [16, 19, 20, 3, 39, 21, 25, 26, 27, 40, 32, 24, 28, 33], multiple-choice questions are typically preferred because they simplify the evaluation process and offer an objective assessment. In contrast, open-ended questions require subjective input from medical experts, which can be time-consuming and labor-intensive, yet they provide greater flexibility and deeper insights into an LLM's capabilities. With the development of \u201cLLM-as-a-judge\" methods [41, 42], the dependence on human evaluation has been reduced, allowing assessments to be conducted automatically by LLMs. Therefore, combining multiple-choice and open-ended questions within a single benchmark is preferable to provide a more comprehensive assessment of an LLM's capabilities [43].\nDespite the above progress, evaluating LLMs in the professional medical field remains challenging due to variations in specialized knowledge across different departments or diseases, as well as regional differences in medical systems and language use. To address these complexities, benchmarks have become increasingly specialized, focusing on particular departments [44, 36], regions [33, 34], or diseases [35]. With regard to departmental disparities, Zhang et al. [44] introduced a Chinese pediatric dataset to compensate for the limited evaluations of LLMs in pediatrics."}, {"title": "3 The Proposed Benchmark", "content": "LLMs have garnered significant attention within the medical community, primarily due to their expert-level performance on standardized medical question-answering tasks, including licensing exams [32], case studies [14], and diseases diagnoses [45]. In the field of ophthalmology, Betzler et al. [46] highlighted the potential of LLMs for clinical practice and ophthalmic education. However, due to the lack of qualified evaluation benchmarks, the practical utility of LLMs in real-world Chinese ophthalmology scenarios cannot be quantified. To this end, we present a specialized benchmark for Chinese ophthalmology with the effort of three experienced ophthalmologists."}, {"title": "3.1 The Taxonomy of OphthBench", "content": "Drawing on the current clinical workflow in tertiary eye centers [46] and as illustrated in Fig. 1, we identify five key application scenarios for LLMs in ophthalmology: education, triage, diagnosis, treatment, and prognosis.\nEducation: This scenario involves a broad range of activities, such as patient education, continuing medical education, and academic instruction for medical students and residents. To streamline the design, we dedicate our efforts exclusively to the examination task, as it is essential for disseminating knowledge and effectively assesses the LLM's ability to retain ophthalmic information that underpins other activities.\nTriage: This scenario focuses on assessing the urgency of a patient's condition to determine the priority of care. Based on this principle, two main tasks are defined: a pre-consultation task, which assists patients in conducting an initial self-assessment, and a triage task, which directs patients to the appropriate department according to their symptoms.\nDiagnosis: This scenario focuses on identifying ocular diseases or conditions based on clinical data. The LLM's ability to complement an ophthalmologist's expertise is evaluated through an inquiry task, which provides additional information to support clinical decisions. In parallel, a differential diagnosis task assesses the automated diagnosis capability to distinguish among potential ocular conditions.\nTreatment: This scenario emphasizes summarizing treatment guidelines and recommending appropriate medication regimens or other treatments based on the patient's clinical data. Accordingly, beyond the treatment plan task, we also include two additional tasks: medical ethics and medication safety to evaluate the overall safety and reliability of the LLMs.\nPrognosis: This scenario focuses on predicting disease progression to enhance risk management. Therefore, we introduce a prognosis prediction task, which asks LLMs to estimate patient outcomes following treatment.\nFinally, these five scenarios collectively enable a more fine-grained evaluation of LLMs in ophthalmic practice and guide their future development."}, {"title": "3.2 Construction and Statistics", "content": "OphthBench is developed using representative exercises from the Chinese Medical Licensing Exam (CNMLE), Resident Standardization Training Exam, Doctor in-charge Qualification Exam, authoritative Chinese ophthalmology textbooks, and real-world clinical cases, ensuring that they accurately reflect Chinese ophthalmic practice. Three experienced ophthalmologists participated in the construction process: three junior ophthalmologists created and refined questions based on the aforementioned materials, while a senior ophthalmologist with an associate-level title reviewed all questions to ensure accuracy and quality.\nOphthBench includes three question types: single-choice questions (SCQs), multiple-choice questions (MCQs), and open-ended questions (OEQs). Answers to SCQs and MCQs are provided by ophthalmologists with their agreement, guaranteeing reliability and clinical relevance. For OEQs, initial answers are generated by GPT-40, and then thoroughly reviewed and refined by ophthalmologists to maintain accuracy and align with the style of typical LLM responses. Each application scenario in OphthBench is associated with an adequate number of questions for a comprehensive and satisfactory evaluation. Fig. 2 presents an overview of the dataset's statistics."}, {"title": "3.3 Evaluation Protocol", "content": "Numerous studies have highlighted that the sensitivity of large language models to prompts can lead to biased evaluations. To mitigate these effects, we report two distinct performances for LLMs. The first uses a straightforward, concise prompt to mimic typical LLM common usage. The second applies advanced prompting techniques to optimize LLM performance. As shown in Fig. 3, we use standard instructions to guide LLMs in generating common responses across various question types. Then, we refine the above prompts with GPT4-0 and add a fixed example to enhance performance for SCQs and MCQs. For OEQs, the prompts for advanced LLM performance are tailored to the tasks, which follow the CO-STAR framework [47] and are auto-generated by GPT4-0 using three randomly selected QA pairs."}, {"title": "4 Main Results", "content": "A total of 39 recent LLMs, including both open-sourced and commercial, were selected to evaluate their clinical utility in Chinese ophthalmology. These LLMs are the most widely used, and their basic information is summarized in Table 2.\nWe accessed commercial LLMs through their APIs, while for open-source models, we relied on the transformers\u00b9 library for local deployment. We adopted regular expressions to extract option numbers from the commercial LLM responses to ensure each model could respond in a multiple-choice format. For open-source models, the Outlines2 library was used to constrain outputs to specific options, thereby guaranteeing consistent answer formats.\nTable 3 and 4 present ranking list for common and advanced performance respectively. With a comprehensive analysis, we draw out the following findings.\nLLMs currently face significant challenges in practical applications within ophthalmology. As shown in Fig. 4, both open-source and commercial large language models (LLMs) demonstrate a performance rate of approximately 70%, underlining the difficulties these models face when applied to ophthalmic tasks. Among the five scenarios assessed, the prognosis scenario stands out as the most promising, while the education scenario remains notably more challenging, reflecting the varied levels of performance across different ophthalmic applications. Fortunately, as data quality and training techniques continue to evolve, their potential utility in ophthalmology shows an encouraging improvement. This is evident from the progress observed in the internlm series [64], where the model ranking improved from 28th (internlm2) to 14th (internlm3) in Table 3 and from 27th (internlm2) to 16th (internlm3) in Table 4. These advancements\nLLMs developed by Chinese companies or institutions demonstrate superior performance in OphthBench. Although LLMs tested in our experiments could handle multiple languages, those developed by Chinese companies or", "equations": ["Score_{task} = \\frac{Score_{task}}{Score_{task}} \\times 90,", "Score_{scenario} = \\frac{n}{\\sum_{i=1}^{n} \\frac{1}{Score_{task_i}}},"]}, {"title": "5 Discussion", "content": "To support the real-world application and advance the development of large language models in Chinese ophthalmology, we introduce a specialized benchmark, OphthBench, designed to assess LLM performance within the context of Chinese ophthalmic practices. OphthBench offers several key advantages over previous LLM benchmarking efforts: (1) Practicality: Developed by three ophthalmologists, OphthBench is meticulously constructed to align with real-world ophthalmic practices, ensuring its accurate assessment in practicality evaluation. (2) Diversity of question types: The benchmark includes three question types, single-choice questions, multiple-choice questions, and open-ended questions, as well as a dedicated scoring strategy, avoiding the one-dimensionality often seen in other benchmarks. (3) Prompt design: OphthBench uses two distinct prompt versions, one for asking the LLMs to generate common responses to simulate the common users, and another one designed to aid the LLMs generate advanced responses to maximize their performance, to provide objective and multifaceted assessments.\nAfter conducting extensive experiments and in-depth analysis of 39 popular LLMs on this benchmark, we draw several conclusions regarding their real-world applications. Consistent with broader trends in LLM research, we observe that improvements in model performance are closely linked to increases in the number of parameters and general capabilities, as measured by benchmarks like MMLU. This supports the validity of the OphthBench benchmark (see Table 3 and 4).\nHowever, this study also has limitations: it does not include all LLMs, especially ophthalmology-specific models, many of which are currently inaccessible. Additionally, the dataset used is limited in both size and the types of tasks it covers. Future work could extend the question set under this framework to address these limitations."}, {"title": "6 Conclusion", "content": "In this paper, we present OphthBench, a specialized benchmark designed to assess LLM capabilities in Chinese ophthalmic practices. It focuses on 5 core ophthalmic scenarios, Education, Triage, Diagnosis, Treatment, and Prognosis, covering 9 distinct tasks with single-choice, multiple-choice, and open-ended question formats. Evaluation results from 39 widely-used LLMs on OphthBench highlight a substantial gap between current model capabilities and practical requirements. Meanwhile, the effectiveness of OphthBench is confirmed through its discernible scoring for scenarios and models. We hope that OphthBench will not only facilitate the standardization and refinement of LLM applications for Chinese ophthalmology but also inspire the development of more advanced and domain-specific LLMs in the future."}]}