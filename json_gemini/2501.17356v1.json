{"title": "On the Coexistence and Ensembling of Watermarks", "authors": ["Aleksandar Petrov", "Shruti Agarwal", "Philip H.S. Torr", "Adel Bibi", "John Collomosse"], "abstract": "Watermarking, the practice of embedding imperceptible information into media such as images, videos, audio, and text, is essential for intellectual property protection, content provenance and attribution. The growing complexity of digital ecosystems necessitates watermarks for different uses to be embedded in the same media. However, to detect and decode all watermarks, they need to coexist well with one another. We perform the first study of coexistence of deep image watermarking methods and, contrary to intuition, we find that various open-source watermarks can coexist with only minor impacts on image quality and decoding robustness. The coexistence of watermarks also opens the avenue for ensembling watermarking methods. We show how ensembling can increase the overall message capacity and enable new trade-offs between capacity, accuracy, robustness and image quality, without needing to retrain the base models.", "sections": [{"title": "1. Introduction", "content": "Watermarking, the encoding of information into media such as images, video, audio or text in imperceptible ways, has long been a cornerstone tool for intellectual property protection. While watermarking is not a new technology, it is experiencing a resurgence in interest as a result of the recent growth of AI-generated content and the increased societal expectations and scrutiny. Watermarking has been proposed as a tool for restoring stripped content provenance metadata, and for content attribution, where it can be used to trace what training data influenced a newly generated sample. In spite of concerns about stripping and spoofing watermarks, standards such as C2PA implement solutions with visual similarity search, manifest databases, and robust fingerprinting.\nWatermarking adoption is increasing, especially if legally required as recently proposed, making it necessary to consider complex ecosystems of watermark providers. However, in a world with many watermarking algorithms, a user, or more likely, their web browser, would not know which detector to use. As such, a sign-posting super watermark added alongside the original watermark can indicate how the original watermark has been encoded and what detector should be used to decode it. Furthermore, different actors need to encode different watermarks in the same media, e.g. the author might want to add content provenance watermark, the distributor could apply an intellectual property tracking watermark, and a generative model developer might need a content attribution watermark. Hence, we will likely see more and more cases of multiple watermarks in the same media.\nEmbedding multiple watermarks in one image requires their coexistence without mutual interference, enabling independent and accurate decoding of every watermark. Yet, the coexistence of multiple deep learning-based watermarks has not been studied, possibly with the assumption that they overwrite each other. That is why we conducted a comprehensive analysis demonstrating they can indeed effectively coexist within the same image. While coexistence incurs minor reductions in image quality and decoding robustness, it persists even when controlling for these factors.\nThe coexistence of watermarks opens up an avenue for building watermarking ensembles. If two watermarks, with capacity $m_1$ and $m_2$ bits, generated with different methods can both be present in a media, then one can effectively encode $m_1+m_2$ bits in the image. Combining such watermark ensembling with watermark strength clipping and error-correcting codes, can modify the capacity-accuracy-robustness-quality trade-offs of existing methods with no need to retrain. We show how one can improve existing watermarking techniques by ensembling them with others.\nIn summary, the contributions of this paper are as follows:\ni. We evaluate whether image watermarking techniques can coexist when applied to the same image and demonstrate that coexistence happens to a much greater degree than expected.\nii. We demonstrate that some level of coexistence persists even when one controls for the small quality and"}, {"title": "2. Preliminaries", "content": "Image watermarking. Image watermarking is the act of encoding a string of bits (a secret) by perturbing the pixel values of an image (cover image) in a way that is minimally disruptive and is robust to edits. We will consider the setting when the detector does not have access to the cover image (blind watermarking). Watermarking requires a trade-off between four competing objectives:\ni. Capacity: the length of the secret message (in bits);\nii. Image quality: the amount of distortion added to the cover image to embed the secret, often measured in peak signal-to-noise ratio (PSNR), larger values indicating better quality;\niii. Accuracy: the fraction of correctly decoded secrets, usually over a test dataset of images;\niv. Robustness: the fraction of secrets we can decode correctly when certain transformations or edits have been added to the image after watermarking. There is no commonly agreed on set of transformations, so we consider the augmentations used for training several popular methods, namely RivaGAN, SSL, TrustMark (low, medium, high), see the details in App. D.\nIncreasing capacity often lowers the image quality, accuracy, and robustness. Similarly, improving accuracy and robustness typically reduces quality and/or capacity (if using error-correction), while enhancing image quality tends to lower accuracy and robustness. These trade-offs make watermarking a multi-objective problem with no single \"best\" method, with only Pareto-optimal solutions. In the literature, bit accuracy is often measured instead of full secret accuracy. We believe that full secret accuracy is a better indicator of performance, as recovering the entire message is usually necessary. Therefore, we report the fraction of cases with all bits being correct decoded."}, {"title": "3. Watermarking methods can coexist", "content": "Why might we need more than one watermark in the same image? Although media manipulation and disinformation are longstanding issues, recent advancements in generative models have intensified concerns. This has led to a push for comprehensive tools to assure media authenticity, often relying on watermarking technology, with some proposals to mandate it by law. With many actors introducing watermarks using diverse techniques, it is challenging for users \u2014and their web browsers to determine which decoder to use for a given image. For example, C2PA, the most widely adopted content provenance standard, is agnostic to watermarking technology and supports any watermark. Na\u00efvely trying all decoders is impractical, as it requires storing all of them locally or making numerous API calls, which is costly and inefficient. Furthermore, different providers might use the same watermarking method but encode the messages in different ways, leading to multiple watermarks being decoded with only one being the intended one.\nWe need an efficient way to identify the decoder for a given watermark. One solution is adding a second watermark carrying the provider identifier, similarly to a disk partition table. We call this identification watermark a super watermark. With a unified standard for the super watermark, a web browser could support many decoders without evaluating every image against all of them. For a super watermark to be effective, it must coexist with the main watermark without hindering its decodability. Another scenario requiring watermark coexistence is the increasingly complex media production and distribution chains, with more and more actors adopting watermarking. For instance, a photo might come with a watermark and editing it in Photoshop might add another to recover its provenance metadata, if stripped. Publishers may add watermarks for copyright detection and developers of generative models might watermark the image to track its contribution to new content and remunerate the copyright holder. These varied uses necessitate that watermarks can be applied and decoded independently, i.e., watermarks that can coexist.\nThese two settings, super watermarks for selecting the correct decoder and multi-actor content provenance chains, depend on different watermarks coexisting in the same image. Yet, to our knowledge, watermark coexistence has not been studied before. While coexistence has been proposed for classical frequency-based methods , these techniques do not apply to modern deep-learning-based approaches.\nWatermarks can coexist in the same image. Considering that watermark coexistence is critical for multi-actor provenance chains and super watermarking, we study to what extent existing open-source techniques can coexist. In the simplest setting, we can apply two watermarks sequentially and measure the accuracy (fraction of correctly decoded secrets) for both methods. We expect the second watermark to be detectable, as the presence of the first should not affect the addition of the second. However, we anticipate that the second watermark would overwrite the first, resulting in 0% accuracy for the first watermark. Similarly, we expect that applying the same watermarking method twice with different secrets would yield low accuracy for the first secret. In Table 1, we present results for 8 watermarking methods, applying each pair in both possible orders. We report the accuracy of the first method, followed by the second. The accuracy when a method is applied alone is shown in grey. When we apply SSL after itself, we use two different carrier vectors.\nOur first observation is that no watermarking method can coexist with itself: decoding the first secret yields 0% accuracy across the diagonal in Table 1. In other words, watermarking methods overwrite their previous message. That holds true also for different but similar methods like TrustMark Qand TrustMark B. Recovering the first secret is unlikely because even if both secrets were theoretically preserved, decoders can output only one secret. SSL is an exception since both encoder and decoder are conditioned on the carrier vector, yet it too cannot coexist with itself."}, {"title": "4. Ensembling as a watermark model modification tool", "content": "Applications of invisible watermarks have varying requirements. An image generator developer might be less concerned about artifacts, since users lack a non-watermarked image for comparison, but may wish to encode extra information like the prompt used. In contrast, content provenance frameworks need high image quality to ensure adoption among creators. Conversely, print media watermarking might need higher robustness to remain detectable after printing and photographing, with lower capacity and quality requirements. Unfortunately, watermarking methods are not easily customizable. Once trained for a specific trade-off between capacity, quality, accuracy and robustness, adjusting this balance typically requires retraining. Consequently, each application might require a slightly different model. Ideally, one would want to modify a watermarking method post-training to adapt it to a new application. We have only seen two tools used for this purpose:\ni. Strength clip: One can perform a linear interpolation between the original image and the watermarked image. By bringing the final image closer to the original image, one can improve the image quality, while reducing the detection accuracy and robustness.\nii. Error-correcting codes (ECCs): Error-correcting codes are used for reducing errors in data transmission over unreliable or noisy communication channels by reducing the number of information carrying bits from $n$ to $k$ and introducing $n \u2212 k$ redundancy bits in their place . Applying ECCs to the watermarked message reduces the effective capacity of the watermarking method but improves the accuracy and robustness.\nWhile these tools can improve the image quality, accuracy and robustness of existing methods, they a. cannot increase the information capacity, and b. leave one of the objectives worse off and hence cannot be used to obtain a strictly better watermarking method without retraining.\nWatermark ensembling boosts capacity. Our observation of watermark coexistence in Sec. 3 offers a potential avenue for a new post-training modification tool: watermark ensembling. If the watermarks generated by two different models coexist, then the application of the two methods can be effectively considered to be a new watermarking method having the sum of the capacities of the individual methods. Hence, ensembling directly addresses the first limitation of strength clip and ECCs: it can be used to increase the capacity without retraining (see the third column in Fig. 4). For example, ensembling TrustMark B and SSL (42 dB, 100 bits) gives us 200bit capacity at the cost of lower accuracy and robustness (the capacity doubling can be seen in the first column of Fig. 5A).\nCombining strength clip, ECC and ensembling could, in principle, result in strictly better models. As seen in Sec. 3, coexistence may come at a small quality, accuracy and robustness cost, thus ensembling alone cannot result in strictly better models. Still, the application of all three tools together could, in principle, outperform either of the two"}, {"title": "5. Discussion and open questions", "content": "While we showed that different watermarks can coexist in the same image, we still lack understanding and control of coexistence. Classic frequency analysis offers a good mental model but cannot be directly applied to non-linear deep models. We need a new theory and tools to characterize and evaluate the non-linear channels for encoding information in images, given resolution, quality and robustness constraints. Such a theory would enable coexistence by design: by limiting the channels individual methods use, designing techniques with explicit channel selection allowing multiple coexisting watermarks with the same method, or, conversely, building methods that fail to coexist with any other watermarks thus fully utilising the information-carrying capacity of the image.\nDespite ensembling being used frequently for increasing the accuracy of classification and regression tasks, for boosting robustness, or for achieving computational efficiency for large models , we have not seen it applied to watermarking, possibly because it was not obvious that watermarks can coexist. As we showed that watermarks do coexist, we opened an avenue for their ensembling. We observed that ensembling offers nuanced benefits: it boosts the capacity of existing models and open new trade-off points, but sometimes one might be better-off by simply applying ECCs and strength clipping to a single well-performing model as linear codes correct few errors relative to the extra capacity they use. Perhaps, modern ECCs -for instance soft-decision ones which factor in the confidence with which each bit is decoded - could better utilise the extra capacity. Beyond parallel and series ensembling, there could be more advanced adaptive techniques or small learnable mixers that one could use for more performant ensembles. Finally, a further boost to the accuracy and robustness of the ensembles could be possible by fine-tuning their decoders on the jointly watermarked images.\nOur analysis was limited in several ways. We used PSNR as a proxy for image quality despite it not tracking human quality perception well; in practice one might want to also consider other quality metrics. Ensembling typically has a computational cost: both the encoding and the decoding steps require inference through both watermarking models. This could be addressed by distilling the two models into a single one. Finally, we focused on images, but watermarking (and hence their coexistence and ensembling) could be applied to other domains such as video, audio and text."}, {"title": "Impact statement", "content": "This paper explores the coexistence and ensembling of deep learning-based watermarking methods, with implications for advancing digital media provenance. This work could positively benefit industries reliant on content provenance, such as journalism, entertainment, and AI-generated media. From an ethical perspective, these advancements may strengthen media authenticity, reduce disinformation, and enable fair attribution in the growing digital ecosystem."}, {"title": "Reproducibility statement", "content": "To ensure reproducibility of our experiments, we restricted ourselves to watermarking methods with open-source implementations. We used the invisible-watermark implementations of DwtDct, DwtDctSvd and RivaGAN and the official implementations of SSL, RoSteALS and TrustMark. For HiDDeN, we used the Stable Signature reimplementation. Furthermore, we provide pseudo-code for our ensembling strategies (in App. B), detailed description of the error-correcting codes we used (in App. C), details about the augmentations for the robustness measures we benchmarked against (in App. D) and comprehensive tables with all the experiments discussed in the paper (in App. F)."}, {"title": "A. Geometric intuition as to why watermarks can coexist", "content": "We would like to offer a geometric intuition as to why watermarks could coexist in the same image without overwriting one another. One explanation is that watermarks are designed to be robust to various augmentations, which induces sparsity into what perturbations can be considered valid watermarks. However, there are many possible sparse patterns that can satisfy the robustness requirements and different watermarking methods might be\u2014either by random selection, or due to particular design decisions\u2014 picking different such patterns. As a result, the perturbations used by different methods could be non-overlapping and might be sufficiently different so that they are still identifiable even after composing.\nConsider the space of all images $I = [0,1]^{3 \\times h \\times w}$. Given an image $x \\in I$, a watermarking method selects a set of images $W(x)$ and maps these images to messages. The capacity of the watermarking method in bits is thus $\\log_2 |W(x)|$. For our illustration we will take $h=w=1$, and will consider discretized pixel values (as used in practice). Hence, our space of all images $I$ would look like this:\nWhen we watermark an image, we typically want to limit the perturbation $\\delta$ applied to an image $x \\in I$ as a quality constraint. For example, a quality constraint formulated as a lower-bound on the PSNR of the watermarked image can be described as limiting $\\delta$ to lie in $B_2(x,\\epsilon)$, an $l_2$ ball centred at $x$ with radius $\\epsilon = \\sqrt{3wh} \\cdot 10^{\\log_{10}R - \\text{minPSNR}/10}$, with $R$ being the range of pixel values ($R = 1$ in our case). That means that out of the whole image domain above, we can only use the images near our clean image $x$. The following illustrates these images in blue, with $x$ at the centre of the $3 \\times 3 \\times 3$ cube:\nFinally, when watermarking, one typically has robustness constraints as well. That is, we want that the watermarks are identifiable even after they are perturbed a little. Practically, that means that we can only consider subsets of the above blue cube such that all their elements are sufficiently spread out. Formally, the robustness requirement can be formulated as a tolerance relation, that is, a non-transitive equivalence relation $\\sim$. For two images $a, b \\in I$ the relation $a \\sim b$ holds if there exists an augmentation under the robustness requirement that transforms $a$ into $b$ or vice-versa. For a watermarking method to satisfy such a robustness requirement, the set of images it uses to encode messages cannot contain two images that can be confused under such a transformation, that is:\n$a \\not\\sim b, \\qquad \\forall a, b \\in W(x)$.\nHowever, there can exist many such sets $W(x)$ under the same image quality and robustness constraints.\nFor the sake of our example, consider that we want our watermarking algorithm to be robust to changing a single pixel value by one discretization step. With this robustness constraint, we can formulate two sets of images that a watermarking scheme might use (with the original image $x$ being marked in red):"}, {"title": "B. Pseudo-code for ensembling and strength clipping", "content": "Listing 1 Psudo-code for the series and parallel we use in our ensembling experiments.\n1 def series_ensemble (\n original: Image,\n wm1: WatermarkingMethod, wm2: Watermarking Method, # callable watermarking methods\n m1: List[bool], m2: List [bool] # the binary secrets for the coressponding watermarking methods\n ) -> Image:\n watermarked1 = wm1 (original, m1)\n watermarked2 = wm2(watermarked1, m2)\n return watermarked 2\n11 def parallel_ensemble (\n original: Image,\n wm1: Watermarking Method, wm2: WatermarkingMethod, # callable watermarking methods\n m1: List[bool], m2: List [bool] # the binary secrets for the coressponding watermarking methods\n ) -> Image:\n watermarked1 = wm1 (original, m1)\n watermarked2 = wm2(original, m2)\n residual1 = watermarked1 original\n residual2 = watermarked2 original\n parallel = original + 0.5 residual1 + 0.5 residual2\n return clip (parallel, MIN_PIXEL_VALUE, MAX_PIXEL_VALUE)\nListing 2 Psudo-code for the strength clipping we use in our ensembling experiments.\n1 def psnr_clip (watermarked: Image, original: Image, target_psnr: float) -> Image:\n diff = watermarked original\n mse mean (diff^2)\n scaling_factor = sqrt(10^(2*np.log10(MAX_PIXEL_VALUE-MIN_PIXEL_VALUE) - target_psnr/10.0) / mse)\n if scaling_factor >= 1: # if no clipping is necessary\n return watermarked\n return clip(original + scaling_factor diff, MIN_PIXEL_VALUE, MAX_PIXEL_VALUE)\n9 def clip_to_strength (\n watermarked: Image, original: Image,\n strength: float,\n wm1: Watermarking Method, wm2: Watermarking Method, # callable watermarking methods\n m1: List[bool], m2: List [bool] # the binary secrets for the coressponding watermarking methods\n ) -> Image:\n watermarked1 = wm1 (original, m1)\n watermarked2 = wm2(original, m2)\n psnr1 = psnr (original, watermarked1)\n psnr2 = psnr (original, watermarked2)\n target_psnr = min(psnr1, psnr2) + strength (max (psnr1, psnr2) min (psnr1, psnr2))\n return psnr_clip (watermarked, original, target_psnr)"}, {"title": "C. Construction of the error-correcting codes", "content": "Listing 3 Recipe for constructing a LC[64, 36, 12] binary linear error-correcting code capable of correcting up to 5 bit flips.\nRecipe taken from CodeTables.de (Grassl, 2006).\nConstruction of a linear code [64,36,12] over GF (2):\n [1]: [64, 36, 12] Linear Code over GF (2)\n Extended BCHCode with parameters 63 11\nListing 4 Recipe for constructing a LC[100, 32, 25] binary linear error-correcting code capable of correcting up to 12 bit flips.\nRecipe taken from CodeTables.de (Grassl, 2006).\nConstruction of a linear code [100,32,25] over GF (2):\n [1]: [102, 33, 26] Quasicyclic of degree 2 Linear Code over GF (2)\nQuasiCyclicCode of length 102 with generating polynomials: x^46 + x^44 + x^41 + x^39 + x^35 + x^33 + 1, x^46 + x^45 + x^44 + x^43\n+ x^42 + x^41 + x^40 + x^39 + x^38 + x^37 + x^35 + x^34 + x^33 + x^30 + x^29 + x^26 + x^25 + x^23 + x^22 + x^20 + x^19 + x\n^17 + x^14 + x^11 + x^10 + x^9+ x^7 + x^6 + x^4\n [2]: [101, 32, 26] Linear Code over GF (2)\nShortening of [1] at { 102 }\n [3]: [100, 32, 25] Linear Code over GF (2)\nPuncturing of [2] at { 101 }\nListing 5 Recipe for constructing a LC[100, 48, 20] binary linear error-correcting code capable of correcting up to 9 bit flips.\nRecipe taken from CodeTables.de (Grassl, 2006).\nConstruction of a linear code [100,48,20] over GF (2):\n [1]: [104, 52, 20] Linear Code over GF (2)\nExtend the QRCode over GF (2) of length 103\n [2]: [100, 48, 20] Linear Code over GF (2)\nShortening of (1) at { 101 104 }"}, {"title": "D. Augmentations used for robustness evaluation", "content": "Throughout the paper we use five different robustness measures, each corresponding to a set of augmentations. In this section, we outline the specific augmentations that each robustness measure consists of.\nRivaGAN augmentations:\n\u2022 Random crop with scale between 0.8 and 1.0 and with probability 0.5.\n\u2022 Random scale with scale between 0.8 and 1.0 and with probability 0.5.\n\u2022 Random compress by maintaining between 50% and 100% of the frequencies with probability 0.5.\nSSL augmentations:\n\u2022 Random horizontal flip with probability 0.5.\nFollowed by one transform randomly sampled from the following list:\n\u2022 Do nothing.\n\u2022 Random rotation with range (-30, 30).\n\u2022 Random crop with scale between 0.2 and 1.0 and ratios between 3/4 and 4/3.\n\u2022 Random resize with scale ratio between 0.2 and 1.0.\n\u2022 Random blur with kernel of maximum size 17.\nTrustmark Low augmentations:\n\u2022 Random horizontal flip with probability 0.5.\n\u2022 Random resized crop with scale between 0.7 and 1.0 and ratios between 3/4 and 4/3.\nFollowed by two transforms randomly sampled from the following list:\n\u2022 JPEG compression with quality 70 and with probability 0.5.\n\u2022 Random brightness adjustment to range (0.9, 1.1) with probability 0.5.\n\u2022 Random contrast adjustment to range (0.9, 1.1) with probability 0.5.\n\u2022 Random color jiggle with factor (0.05, 0.05, 0.05, 0.01) and with probability 0.5.\n\u2022 Random grayscale with probability 0.5.\n\u2022 Random Gaussian blur with kernel size 3, sigma (0.1, 1.0) and with probability 0.5.\n\u2022 Random Gaussian noise with std 0.02 and with probability 0.5.\n\u2022 Random hue adjustment with factor (-0.1, 0.1) and with probability 0.5.\n\u2022 Random motion blur with kernel size (3,5), angle (-25, 25), direction (-0.25, 0.25) and with probability 0.5.\n\u2022 Random posterize to 5 bits with probability 0.5.\n\u2022 Random RGB shift with limit for all channels 0.02 and with probability 0.5.\n\u2022 Random saturation to range (0.9, 1.1) and with probability 0.5.\n\u2022 Random sharpness to 1.0 and with probability 0.5.\n\u2022 Random median blur with kernel size 3 and with probability 0.5.\n\u2022 Random box blur with kernel size 3, border type reflect and with probability 0.5."}, {"title": "E. Residuals of various watermarking methods", "content": ""}, {"title": "F. Extended results", "content": ""}]}