{"title": "FINE-TUNING LARGE LANGUAGE MODELS FOR DOMAIN ADAPTATION: EXPLORATION OF TRAINING STRATEGIES, SCALING, MODEL MERGING AND SYNERGISTIC CAPABILITIES", "authors": ["Wei Lu", "Rachel K. Luu", "Markus J. Buehler"], "abstract": "The advancement of Large Language Models (LLMs) for domain applications in fields such as materials science and engineering depends on the development of fine-tuning strategies that adapt models for specialized, technical capabilities. In this work, we explore the effects of Continued Pretraining (CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization approaches, including Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO), on fine-tuned LLM performance. Our analysis shows how these strategies influence model outcomes and reveals that the merging of multiple fine-tuned models can lead to the emergence of capabilities that surpass the individual contributions of the parent models. We find that model merging is not merely a process of aggregation, but a transformative method that can drive substantial advancements in model capabilities characterized by highly nonlinear interactions between model parameters, resulting in new functionalities that neither parent model could achieve alone, leading to improved performance in domain-specific assessments. We study critical factors that influence the success of model merging, such as the diversity between parent models and the fine-tuning techniques employed. The insights underscore the potential of strategic model merging to unlock novel capabilities in LLMs, offering an effective tool for advancing AI systems to meet complex challenges. Experiments with different model architectures are presented, including the Llama 3.1 8B and Mistral 7B family of models, where similar behaviors are observed. Exploring whether the results", "sections": [{"title": "1 Introduction", "content": "The development of Large Language Models (LLMs) [1, 2, 3, 4, 5, 6] has enhanced our abilities for natural language processing (NLP) in scientific and engineering applications, due to significant advancements across diverse domains, from general-purpose applications to specialized fields such as materials science and engineering [7, 8, 9, 10, 11, 12, 13, 14]. These models, including prominent open-source architectures like Llama and Mistral, have demonstrated strong capabilities in understanding and generating human-like text. However, their application in technical fields require fine-tuning strategies that adapt these models to specific domain challenges and technical requirements, which are often poorly understood. In the field of biomateriomics, for instance, researchers aim to develop systematic explorations of knowledge across scales, domains and application areas including biological material design inspiration [15, 16, 17, 18, 19, 7]. These and other challenges can be addressed synergistically using multimodal reasoning engines that have, at its core, capabilities derived from LLMs. One rationale is that LLMs have shown strong capabilities to integrate diverse concepts and provide an integrative modeling strategy for diverse contexts seen in biological materials engineering [20, 7].\nFine-tuning LLMs for domain-specific applications involves more than simply retraining on specialized data; it requires the exploration of strategies to endow the model with new knowledge while retaining capabilities learned in earlier training stages, to yield optimal model performance. This is particularly challenging since most of the time it is not feasible to train models from scratch due to cost, or because the original datasets are not available. This is a particular concern in open-source models like Llama or Mistral, where certain details about the training process have been released, but the full datasets during pre-training, fine-tuning, and alignment phases are unknown [6, 4].\nAn often-effective strategy that has been used in earlier work is low-rank adaptation (LoRA) [21], where a set of small trainable low-rank tensors is added to linear layers of a larger model to adapt it towards new capabilities [21, 22, 14]. While this can be an effective method, there are limits to how much a model can be improved and how much new knowledge can be incorporated. Other research suggested that continued pre-training (CPT) on domain-specific corpora can help better introduce new knowledge within the target domain [23], enhancing its relevance and accuracy. However, this typically requires a host of additional strategies to make a model useful for downstream applications, such as instruction following, chat interaction, agentic use, tool calling, and others. Supervised fine-tuning (SFT) is a method used to refine these models by directly teaching them to perform well on specific tasks through curated datasets. However, the potential for further enhancing model performance and unlocking new capabilities through advanced optimization techniques remains a critical area of exploration. In this context, preference-based optimization strategies, such as Direct Preference Optimization (DPO) [24] and Odds Ratio Preference Optimization (ORPO) [25], have emerged as promising approaches. Unlike traditional Reinforcement Learning (RL) methods [26], which often require explicit reward functions and complex environment models, DPO and ORPO focus on optimizing models based on direct feedback or preferences. These methods offer a flexible and efficient means of refining model behavior, particularly when the goal is to align the model's outputs with human expectations or domain-specific criteria, such as being able to reason over or logically deduce answers in a particular domain, such as materials science.\nAnother area of recent interest is the practice of model merging [27, 28, 29], where multiple, differently trained models are combined to create a new model with potentially superior capabilities. Earlier experiments have shown that this process is not simply additive; as we will show it leads to highly nonlinear interactions between the parameters of the merged models, resulting in the emergence of new functionalities that neither parent model possessed individually. Such emergent behavior suggests that model merging could be a powerful tool for advancing LLM capabilities, enabling the development of models that are not only more accurate but also more adaptable to complex, real-world challenges.\nAs this brief review shows, there are a myriad of possible strategies, but relatively little data is available in terms of systematic explorations. LLMs are highly complex models, and training is expensive and time-consuming, and"}, {"title": "2 Results and Discussion", "content": "We follow the process depicted in Figure 2 in developing models and conducting assessments. Figure 2A shows a conventional linear training pipeline where a base model undergoes Continued Pre-Training (CPT), followed by Supervised Fine-Tuning (SFT), and then optimized using methods like Direct Preference Optimization (DPO) or Odds Ratio Preference Optimization (ORPO) to produce a trained model. Figure 2B shows an alternative pipeline where, after CPT, SFT, and optimization (e.g., DPO, ORPO), the model is further enhanced by merging it with another fine-tuned model (e.g., a general-purpose model). We note that model merging can be done with models extracted from various training stages, such as after CPT, SFT or at the final stage.\nFor the purpose of the analysis, we go into the details of model merging strategies. In this work we focus on (Spherical Linear Interpolation (SLERP, details see Materials and Methods, Section 4), as we found it to be the most effective method. SLERP is a mathematical technique originally introduced in the field of computer graphics for smoothly interpolating between rotations represented by quaternions [30]. SLERP has found widespread application in various domains that require smooth transitions between orientations or states, including robotics, physics simulations, as well as real-time graphics. For instance, in robotics, SLERP is used for the practical parameterization of rotations, allowing for seamless motion planning and control [31]. In physics simulations and computer graphics, SLERP is crucial for visualizing and animating rotations in a way that preserves the continuity and smoothness of motion [32, 33]. By maintaining the geometric relationships between interpolated states, SLERP ensures that transitions are both smooth and physically meaningful, making it a useful tool in scenarios where precise and continuous interpolation is required. Figure 3 shows the basic concepts of SLERP (versus linear interpolation, LERP), visually. A key aspect of this strategy is that the smooth, nonlinear path helps to preserve the underlying structure of the model parameters. The sphere in this context represents the inherent structure of the model's parameter space, and by maintaining the geometric relationship between the parameters, SLERP ensures that the interpolation respects this original structure and does not puncture it (as linear combination of points would), leading to a more meaningful and coherent blending of capabilities rather than random, unstructured changes. Because the merged points are both congruent with the model geometry (that is, they lie on the sphere used here for demonstration) and because they realize new points previously not accessed, emergent features and capabilities could potentially be unlocked.\nIn the following, we present a series of results from assessment experiments conducted with different model families and training/merging strategies (details on training, models, datasets, and assessment benchmarks, see Materials and Methods). Figure 4 depicts a series of performance evaluations of Llama-3.1 Model variants across benchmarks. We use two basic models as foundation for our training. First, meta-llama/Meta-Llama-3.1-8B, the base model of the Llama family that has not been fine-tuned and aligned. Second, the meta-llama/Meta-Llama-3.1-8B-Instruct model that has been fine-tuned and aligned to conduct question-answer interactions, along with a host of other capabilities [6]. Except for the LoRA case [14], all of our experiments include CPT (see Table 1 for an overview of the training stages and acronyms used) as the first step, with the aim to endow the base model with domain knowledge from our materials science corpus of papers and distilled, extracted and processed data sourced from scientific studies. We then implement a range of variations, such as CPT only, CPT-SFT, CPT-SFT-ORPO and CPT-SFT-DPO. At each stage, we also implement model merging with the with the meta-llama/Meta-Llama-3.1-8B-Instruct model. Overall, the results reveal that the models that have undergone SLERP merging (especially those combined with DPO and ORPO strategies) generally show the highest accuracy across benchmarks. The best strategy without model merging is found to be the Instruct-CPT-SFT-DPO strategy.\nWe now conduct the same series of experiments using Mistral-v0.3 model variants [4] across benchmarks. As in the previous set of results, we use the same dataset across all cases, and we present both non-merged cases and merges with the mistralai/Mistral-7B-Instruct-v0.3 model. Figure 5 depicts an overview the performance evaluations across benchmarks for this case. As before, the results show that these models that have also undergone SLERP merging generally show the highest accuracy across benchmarks. The best strategy without model merging is found to be the Base-CPT-SFT strategy, albeit the performance of the Instruct-CPT-SFT strategy is very similar.\nThe CPT stage involves five epochs. To explore the effect of the number of epochs in this phase, we computed performance of the direct CPT-SLERP merges for the Mistral models from different training epochs. It is noted that the original merges assessed (and SFT, DPO/ORPO training stages) in Figure 5 were conducted based on CPT results from epoch 5. Figure 6 depicts a comparison of averaged scores across different epochs for both the Base and Instruct models, using the SLERP method. Figure 6A shows an overview of the results, in similar format as the earlier performance assessments, depicting performance across all models and variants of CPT epochs used. Figure 6B shows the performance of the Instruct model as a function of the number of CPT training epochs, and Figure 6C illustrates the performance of the Base model. We can see that the Instruct model demonstrates a consistent improvement in performance with each epoch, peaking at the best score by epoch 5, indicating that it benefits significantly from continued training. In contrast, the Base model shows a more fluctuating performance, with its highest score at epoch 1, followed by slight declines and only a minor recovery at epoch 5. This suggests that while the Base model starts strong, it does not consistently improve with additional training, potentially indicating a saturation point. Both models, however, consistently outperform the baseline score set by the original mistralai/Mistral-7B-Instruct-v0.3 model, underscoring the effectiveness of the SLERP method, and consistent with the earlier results. The more substantial improvement of the Instruct model over the baseline highlights its robustness in instruction-tuned tasks, making it the preferable choice for such applications, particularly when extended fine-tuning is feasible."}, {"title": "2.1 Detailed analysis of key factors in model merging", "content": "As the results in Figures 4 and 5 clearly reveal, SLERP appears to significantly improve model performance due to its ability to respect the geometric properties of the parameter space. However, this analysis did not yet reveal whether we have a significant synergistic effect. To examine this, we plot the results differently, comparing the actual measured performance with an expected performance that is computed by simply averaging the scores of the two parent models. To properly define all key variables, the performance of a merged model is defined as $P_{merged}; P_1, P_2$ (measured per the benchmark), while the expected, averaged score $E(P_1, P_2)$ is calculated as the linear average of the performances of the two parent models:\n$E(P_1, P_2) = \\frac{P_1 + P_2}{2}$\nUsing these metrics, Figure 7 shows a detailed exploration of performance of SLERP variants for different cases, plotting the actual observed performance over an estimated, expected score based on a simple average of the score of both parent models (linear combination).\nNotably, the strong deviation from the diagonal reveals nonlinear, synergistic effects, where the actual observed model performance is much greater than a simple averaging of the capabilities of the parent models alone. Results are shown for both the Llama-3.1-8B and Mistral-7B-v0.3 model series, respectively, for a variety of training strategies and datasets used in the process. We find that the results are similar for both models. An important distinction that can be seen in the analysis is that for the Llama models, the best performing model (lamm-mit/Llama3.1-8b-Instruct-CPT-ORPO-SLERP) is based off the Llama Instruct model, whereas for the Mistral model (lamm-mit/mistral-7B-Base-v0.3-CPT-SFT-SLERP) it is based off the Mistral Base model.\nTo better understand the mechanics behind the observed effects, we briefly discuss the mathematical underpinnings of SLERP merging. Unlike linear interpolation, which assumes a flat Euclidean space, SLERP explores a richer parameter space by interpolating along a curved path on a unit sphere (we refer also to Figure 3). This approach allows SLERP to uncover regions in the parameter space that might represent combinations of parameters more effective than those found in either model alone. SLERP further balances the specialized knowledge learned by each model, combining their strengths without simply averaging them. By avoiding high-loss regions that linear interpolation might pass through, SLERP ensures a smoother transition, potentially leading to better generalization in the merged model. The non-linear nature of SLERP's path also considers the complex interactions between parameters, which can reveal beneficial interactions that a simple linear combination would miss. Furthermore, SLERP may act as a form of regularization, preventing overfitting to the idiosyncrasies of a single model's training data, thus enhancing generalization. Finally, SLERP helps mitigate the effects of catastrophic forgetting, preserving knowledge from both models when one has been fine-tuned or trained after the other. These factors combine to make SLERP a powerful tool for model merging, leading to a merged model that often performs better than either of the original models on their own.\nHence, we believe that the observed effectiveness of SLERP in merging models can be attributed to its ability to enhance non-linear interactions between parameters by exploring the spherical geometry of the parameter space. Given two sets of model parameters $\\theta_1$ and $\\theta_2$, each parameter can be seen as a vector in a high-dimensional space. The interpolation performed by SLERP respects the curvature of this space, allowing for combinations of parameters that are not simply linear but involve deeper, non-linear synergies (see, Figure 3). Consider the parameters $\\theta_1$ and $\\theta_2$ as consisting of individual components $\\theta_{1,i}$ and $\\theta_{2,i}$ in a given layer of the neural network. SLERP combines these parameters as follows:\n$\\theta_{i, merged} = ||\\theta_1|| \\cdot ||\\theta_2|| \\left( \\frac{\\sin((1-t)\\omega)}{\\sin(\\omega)} \\theta_{1, i} + \\frac{\\sin(t\\omega)}{\\sin(\\omega)} \\theta_{2, i} \\right)$  This combination allows for interactions between $\\theta_{1,i}$ and $\\theta_{2,i}$ that are non-linear in nature. For example, if $\\theta_{1,i}$ and $\\theta_{2,i}$ represent weights connected to different features in the network, their spherical combination could activate a new feature $\\phi_i$ that is not present in either model individually:\n$\\phi_i = f(\\theta_{i, merged} \\cdot x_i)$\nwhere $x_i$ is the input feature and $f(\u00b7)$ is the activation function. The non-linear combination of parameters may lead to new behaviors or capabilities, as the interpolated parameters could synergistically enhance or suppress features in ways that the individual models cannot.\nSLERP avoids destructive interference by maintaining the angular relationships between the parameter vectors, which can prevent the loss of specialized features learned by either model. The spherical symmetry imposed by SLERP introduces a regularization effect, smoothing the transition between the models and enabling the merged model to generalize better. This process often results in the emergence of new capabilities or improvements in performance that neither of the original models possessed.\nThe ability of SLERP to uncover these new capabilities can also be understood through the lens of overparameterization and the principles of ensemble methods. Overparameterized neural networks are known to generalize well, even when trained to zero error, due to their increased capacity to capture complex patterns [34]. SLERP leverages this capacity by combining parameters in a non-linear fashion, effectively utilizing the high-dimensional space in which these parameters reside. As a result, the merged model can exhibit emergent properties that are not apparent in either of the original models. SLERP's mechanism resembles ensemble methods, where combining diverse models leads to better generalization [35]. In this case, the diversity comes from the different training histories and learned features of the two models. The spherical interpolation pathway created by SLERP acts as a continuum of model ensembles, where at each point along the path, the combined parameters may activate new and beneficial feature interactions. SLERP not only preserves the strengths of the individual models but also has the potential to generate entirely new capabilities through its sophisticated interpolation method. This makes it a useful tool for our goal to merge models that complement each other or to create a more versatile and generalizable model from existing pre-trained models."}, {"title": "2.2 Mechanistic analysis to elucidate key steps with highest impact on performance", "content": "As a next step in the analysis we focus on correlation heatmaps to illustrate the relationships between various model attributes and the performance of merged models. As shown in Figure 10, the performance of a merged model is denoted as $P_{merged}$, while the performance of the two parent models is denoted as $P_1$ and $P_2$. Performance improvement is defined as the difference between the performance of the merged model and the maximum performance of the two parent models:\nPerformance Improvement = $P_{merged} \u2013 max(P_1, P_2)$\nDiversity between parent models is measured as the absolute difference between their individual performances:\nDiversity = $|P_1 - P_2|$\nTo elucidated overall trends that can be gleaned from the results, Figure 10 depicts correlation heatmaps for the fine-tuned Llama and Mistral models. The data reveals distinct relationships between various metrics. In Llama models, a strong negative correlation between diversity and SFT suggests that higher diversity reduces reliance on supervised fine-tuning, whereas performance improvement shows moderate positive correlations with both merged performance and SFT, indicating that these factors contribute to improved outcomes. In contrast, Mistral models exhibit a more robust positive correlation between performance improvement and merged performance, especially in instruction-tuned models, where the Base model type significantly enhances merged performance. ORPO, while contributing to performance improvements in both models, has a more pronounced impact in Mistral models. Overall, the findings suggest that diversity tends to reduce SFT dependency, particularly in Llama models, while instruction-tuned Base models in Mistral benefit more from merging strategies, emphasizing the importance of model selection and optimization methods."}, {"title": "2.3 Contrasting assessments with very small LLMs", "content": "While the models studied earlier were modest in size, around 7-8 billion parameters, recent research has resulted in even smaller, yet useful, models that can be particularly useful for edge computing applications, or deployment on devices such as mobile phones or robotic systems. We now examine whether such models also show the marked effects observed earlier due to model merging. We conduct this analysis using the SmolLM model series, specifically the 1.7 billion parameter model. This choice is partially motivated by the complete open access of the model, training strategy, and training data. As in the earlier analyses, we start with the base model and successively apply CPT, SFT and DPO (we found that for this small model, DPO worked better than ORPO). Though never reaching the level of absolute performance of finetuned 7B or 8B models, in almost all fine-tuning cases with SmolLM, we find the most"}, {"title": "2.4 Further quantification of the effects of model merging across all model architectures", "content": "To better understand whether or not and to what degree model merging improves performance over either one of the two models used for merging, we present the analysis shown in Figure 13. The plot shows performance deviation of SLERP merged models compared to the best original model used as source.\nThe results reveal that the deviation in performance between models merged using SLERP and their best-performing original counterparts, whereby the deviation is calculated as the difference between the best original model's performance and the SLERP model's performance. Hence, negative deviations, where SLERP underperforms relative to the best original model, are marked in red. Positive deviations, indicating better performance of the SLERP model, are shown in shades of blue, with darker blue representing greater improvements."}, {"title": "2.5 Interactive examples for open-ended cross-material reasoning and material design tasks", "content": "In our next experiment, we conduct interactive conversations with a set of the models, using consistent system prompts and identical user input. We aim to test multi-turn capabilities of the models, assess responsiveness to system prompts and instructions, and capability to produce structured output (JSON). We will further assess the quality of synthesis of each model, along a set of criteria that include depth of reasoning, creativity, clarity and whether or not quantitative predictions are featured. Each of the conversations unfolds as follows:"}, {"title": "2.6 Agentic use in image generation: Applications in cross-domain knowledge integration for materials and urban design", "content": "We show several examples to highlight the capabilities and potential of fine-tuned LLMs, showcasing a particular application in materials design. The overall goal is to explore how the LLMs developed here can be used to reason over complex materials principles and use the insights developed through multi-step prompting to create a prompt for image generation. We note that our fine-tuned SmolLM based model performs extremely well for this task and yields quite creative prompts that integrate various ideas and concepts (underscoring its potential as a creative agent).\nWhile slight variations of prompting are used to yield different examples (each result presented here includes a detailed presentation of all features), the general goal is to think about design principles that we can extract from combining different biological materials. For instance, we ask the model to think about ways to combine design elements from spider silk and collagen to make a strong, lightweight but tough material, and to also incorporate design cues from leaf microstructures. The approach can be used to focus directly on material microstructures but can also be used to yield cross-domain results, such as architectural ideas or city design.\nIn our first example we prompt the lamm-mit/SmolLM-Base-1.7B-CPT-SFT-DPO model as follows (Text Box 9 shows the entire conversation):"}, {"title": "3 Conclusions", "content": "This study addressed fundamental questions in the fine-tuning of large language models (LLMs) for domain-specific knowledge, exploring how different optimization strategies and datasets influence model performance, and assessed effects of model size and capabilities. Our investigation focused on a host of techniques applied consistently across models/architectures and parameter numbers. These included Continued Pre-Training (CPT), Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Odds Ratio Preference Optimization (ORPO). The goal was to determine how these methods impact the specialization of LLMs, particularly in the context of engineering or science domains.\nA key finding of our research is that model scale plays a crucial role in the efficacy of fine-tuning strategies. Larger models, such as those with 7B and 8B parameters, not only exhibited substantial improvements in domain-specific tasks but also showed the emergence of novel capabilities\u2014an outcome not observed in smaller models like the 1.7B parameter SmolLM model. This observation, as shown in a comparative plot (Figure 13) suggests a threshold effect, where the benefits of advanced optimization techniques, including model merging through SLERP (Spherical Linear Interpolation), become significantly more pronounced as model size increases. There are notable differences between the Llama and Mistral family of models that deserve further investigation. However, such investigations are hampered by the lack of detailed insights into the datasets used for training (during pre-training and fine-tuning) as well as a lack of details on specific training approaches. A summary of the key insights including the most effective approach is shown in Table 4.\nThe comparison between the Llama-3.1-8b and Mistral-7B-v0.3 models highlighted how SLERP, when combined with SFT, DPO and ORPO, can effectively unlock synergistic properties, leading to refined performance outcomes that cluster distinctly in our analysis. However, these improvements were not mirrored in the smaller SmolLM-1.7B models, which showed a deterioration of performance under model merging, underscoring the importance of scale in realizing"}]}