{"title": "Side-Channel Analysis of OpenVINO-based Neural Network Models", "authors": ["Dirmanto Jap", "Jakub Breier", "Zdenko Lehock\u00fd", "Shivam Bhasin", "Xiaolu Hou"], "abstract": "Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential.\nIn this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement in artificial intelligence (AI) and machine learning (ML) technologies has spread into various critical domains for our everyday lives, such as healthcare, finance, automotive, and more. Among the myriad AI frameworks available, OpenVINO [15] (Open Visual Inference and Neural Network Optimiza- tion) stands out as a powerful open-source toolkit. OpenVINO enables develop- ers to optimize and deploy deep learning models on heterogeneous computing platforms, especially targeting edge and embedded devices.\nEmbedded devices, equipped with neural network accelerators offer great benefits in terms of efficiency and performance. However, they also introduce new security challenges, particularly related to hardware attack vectors such as side-channel analysis (SCA) attacks [2]. SCA attacks exploit information leak- age from the physical implementation of a system rather than vulnerabilities in the algorithm itself [18]. These attacks can reveal sensitive data, such as neural network parameters and architecture, through indirect means such as power con- sumption, electromagnetic emissions, or execution time analysis [3]. This com- plements model stealing/extraction attacks that have been a well-research topic within the past decade [21]. The assumption for these attacks is that the creator of the model invested significant resources into preparing the training data (ob- taining the initial data, labeling, and pre-processing it), and also into training the model. Thus, to have a competitive advantage, they prefer to keep the de- tails of the model confidential. Apart from that, protecting the model parameters effectively prevents white-box adversarial attacks.\nThere are two main directions the attacker can take when extracting the model [21]:\n1. obtaining the exact model parameters,\n2. approximating the model behavior.\nWhen it comes to the first direction, recovering the exact model parameters has been shown possible by fault injection attacks, but only for one layer of a network obtained by transfer learning [5]. The goal of model extraction by the means of SCA attacks ultimately leads to the same [3], however, as the bit-width of the variables used for storing model parameters is generally up to 64 bits, the noise makes it very hard to determine the exact values from the SCA leakage.\nThe majority of the literature in this direction thus focuses on approximating the model behavior, generally aimed at creating a substitute model exhibiting similar behavior to the original one.\nOur contribution. In this paper, we focus on investigating SCA attacks on neu- ral networks implemented on embedded devices using OpenVINO. These models are implemented in a quantized manner, thus the model parameters are normally stored in 8-bit variables, making them more efficient in terms of storage and computational complexity, which is crucial for embedded implementations. For our experiments, we utilized an OpenVINO implementation of GoogleNet v1 with 6.7977M parameters. The SCA attack allowed us to reconstruct the model that exhibits very similar accuracies to the original one. More specifically, the difference in Top 1 accuracy is 1% and in Top 5 accuracy it is just 0.64%.\nOrganization. The rest of the paper is organized as follows. Section 2 provides the background on the basic concepts utilized in this paper such as neural net- works and SCA attacks, and gives an overview of the related work. Section 3 presents the methodology for extracting the model parameters using side chan- nels. Section 4 details our evaluation and highlights the results. And finally, Section 5 concludes this work and provides potential future directions."}, {"title": "2 Background and Related Work", "content": "Neural networks (NNs) are a fundamental concept in the field of artificial intelli- gence and machine learning, inspired by the structure and function of the human brain [16]. NNs can recognize patterns, learn from data, and make decisions with minimal human intervention. They have become important in a wide range of applications, including image and speech recognition, natural language process- ing, and autonomous systems. They are supervised ML algorithms labeled dataset is used to train their model parameters with an optimization algorithm called (stochastic) gradient descent [11].\nTheir structure includes an input layer that receives the initial data for pro- cessing, one or more hidden layers (if there are more, we talk about deep neural networks (DNNs)), where the data is transformed through various weights and biases, and an output layer, which produces the result such as classification or prediction.\nThe original design of an NN, a multilayer perceptron, has been adjusted to fit specific problems. On the other hand, convolutional neural networks (CNNs) can adaptively learn spatial hierarchies of features, making them effective for image recognition tasks [17]. Another type, recurrent neural networks (RNNs) are capable of capturing temporal dependencies by using cycles within the net- work, allowing them to analyze sequential data such as time series or natural language."}, {"title": "2.2 Quantized Neural Networks", "content": "Conventional NNs rely on high-precision floating-point arithmetic, utilizing ei- ther 32-bit or 64-bit variables. While such precision helps to achieve state-of- the-art performance, it requires substantial resources and memory bandwidth. Such a requirement limits their deployment in resource-constrained devices like mobile phones, embedded systems, and edge devices.\nQuantized neural networks (QNNs) address this limitation by using lower precision for the weights and activations in the network [13]. QNNs utilize low bit-width numbers such as 8-bit or 4-bit integers, or even binary values. The quantization significantly reduces memory requirements for storing the model and also the amount of computation required, resulting in faster computation and smaller power consumption.\nThe quantization can either be uniform, where all values are evenly dis- tributed across the available range, or non-uniform which allows for a more tailored approach based on the data distribution. Despite the reduced num- ber precision, QNNs often achieve accuracies comparable to their high-precision counterparts, especially when quantization-aware training and tuning are used."}, {"title": "2.3 OpenVINO Framework", "content": "OpenVINO [15] is an open-source software toolkit for optimizing and deploying DNN models, mostly on Intel devices but currently also on ARM processors. It supports a wide range of different platforms from edge to cloud, working with models in TensorFlow, PyTorch, ONNX, TensorFlow Lite, and PaddlePaddle model formats.\nIn this paper, we also utilize the Neural Network Compression Framework (NNCF) [15]. NNCF provides a suite of post-training and training-time algo- rithms for optimizing the inference of neural networks in OpenVINO."}, {"title": "2.4 Side-Channel Analysis Attacks", "content": "SCA is a method originally proposed for recovering secret information from cryptographic implementations [14]. Classical cryptanalysis methods focus on analyzing the weaknesses in the algorithms, utilizing techniques such as differen- tial and linear cryptanalysis. As these are well-studied and understood, the new algorithms are always evaluated against them, rendering cryptanalytic attacks ineffective for a full-round cipher. However, as of now, there is no cryptographic algorithm inherently resistant to SCAs that analyze the information leakage dur- ing the physical operation of the cryptographic device. Such leakage can come in various forms such as power consumption, electromagnetic emanation, timing of the execution, and even acoustic leakage.\nSCAs can recover the secret key very efficiently, sometimes requiring only a single measurement [1]. There are generally two types of attacks: simple and differential. The simple attacks only utilize one or a few leakage traces for the attack. They can be either used as a starting point for the differential attacks, to determine the cipher rounds and round operations in block ciphers, or for a full attack, which is normally the case for public key algorithms [18]. Differential attacks, on the other hand, utilize higher numbers of measurements and utilize statistical methods to correlate the processed data and the leakage.\nLeakage models are used to describe and quantify the information leaked from the implementation under test through side channels during its execution. These models help in understanding, analyzing, simulating, and mitigating side- channel attacks. The main models used in SCA are as follows:\nHamming weight model: assumes that the information leaked is proportional to the number of bits set to 1 in the data being processed. It is commonly used when the underlying implementation is of software character.\nHamming distance model: assumes that the information leaked is propor- tional to the number of bit transitions (changes 1\u21920 or 0\u21921) between two consecutive states of the data.\nStochastic model: goes further than the previous two and assumes that every bit of the analyzed variable leaks differently.\nWe give a more thorough explanation of leakage models in Section 3.2."}, {"title": "2.5 Related Work - SCAs on Neural Networks", "content": "In the first part of this section, we will focus on various attacks on NNs, in the second part we will outline some of the possible defenses.\nAttacks. The first comprehensive work in this area by Batina et al. [3] explored how neural networks can be reverse-engineered using electromagnetic (EM) anal- ysis. The paper demonstrated that a passive, non-invasive attacker can extract model details such as activation functions, number of layers, neurons, output classes, and weights from neural networks by analyzing EM signals. The ex- periments were conducted on an ARM Cortex-M3 microcontroller, showing the feasibility of such attacks on widely used hardware platforms.\nWang et al. [24] investigated the vulnerability of in-memory computing (IMC) systems to side-channel attacks. By simulating power traces of IMC macros, the researchers demonstrated that attackers can reverse-engineer neural network models, extracting details like layer types and convolution kernel sizes without prior knowledge.\nGao et al. [10] presented a novel attack method called DeepTheft, which targets DNN models deployed in Machine Learning as a Service (MLaaS) envi- ronments. By exploiting the Running Average Power Limit (RAPL)-based power side channel, the work demonstrated that it is possible to accurately recover com- plex DNN model architectures, including detailed layer-wise hyperparameters, even with low sampling rates.\nRyu et al. [22] introduced a novel attack method called Gamma-Knife. This attack leverages software-based power side channels to extract the architecture of neural networks without requiring physical access or high-precision measur- ing equipment. By utilizing statistical metrics, the Gamma-Knife attack can accurately determine key architectural details such as filter size, depth of con- volutional layers, and activation functions. The researchers demonstrated the effectiveness of this attack on popular neural networks like VGGNet, ResNet, GoogleNet, and MobileNet, achieving an accuracy of approximately 90%.\nNagarajan et al. [20] investigated the vulnerabilities of spiking neural net- works (SNNs) to power SCAs. The authors demonstrated that different synaptic weights and neuron parameters in SNNs produce distinct power and spike tim- ing signatures, making them susceptible to SCAs. Through eight unique attacks, they showed that an adversary can reverse-engineer the specifications of an SNN.\nCountermeasures. Dubey et al. [9] proposed a novel hardware design that incorporates masking techniques. This design includes masked adder trees for fully connected layers and masked Rectifier Linear Units for activation functions. Experiments on a SAKURA-X FPGA board show that the proposed protection significantly increases the latency and area cost but effectively mitigates first- order differential power analysis attacks.\nIn [8], the authors proposed using modular arithmetic to make neural net- works more compatible with masking techniques. They demonstrated this ap-"}, {"title": "3 Methodology", "content": "When a value v is processed by a computational device, it is represented as a binary string. For integer values, v is encoded using two's complement represen- tation. For floating-point values, the IEEE 754 standard is employed.\nFor example, the two's complement representations for integers between -8 and 7 are shown in Table 1."}, {"title": "3.2 Leakage Models", "content": "In SCA, the leakage model characterizes the relationship between the computa- tional leakage and the secret value v processed by the device. Suppose\n$$v = v_{n-1}v_{n-2}...v_1v_0 \\in F_2$$\nis represented as a binary string of length n in the computer memory. The Hamming weight of v, denoted HW(v), is given by the number of bits that are equal to 1 in v. For example,\n$$HW(110) = 2, \\qquad HW(000) = 0, \\qquad HW(110101) = 4.$$\nLet $$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$$\ndenote the noise in the leakage, modeled as a normal distribution with mean 0 and variance $$\\sigma^2$$.\nA Hamming weight leakage model [12, Section 4.2] suggests that the leakage\n$$\\mathcal{L}(v)$$\nis given by\n$$\\mathcal{L}(v) = HW(v) + \\epsilon.$$\nA stochastic leakage model [12, Section 4.3] expresses the leakage as\n$$\\mathcal{L}(v) = \\sum_{s=0}^{n-1} \\alpha_s v_s + \\epsilon,\\qquad(1)$$\nwhere $$\\alpha_s (s=0,1,...,n-1)$$\nare real numbers. These numbers $$\\alpha_s$$\nare referred to as the coefficients of the stochastic leakage model."}, {"title": "3.3 Correlation Power Analysis", "content": "We employ the correlation power analysis (CPA) methodology originally devised for cryptographic implementation attacks [18]. CPA focuses on calculating Pear- son's correlation coefficient between observed leakages and hypothetical leakages derived from a guessed secret value. The correct secret value is anticipated to yield the highest correlation coefficient, indicating a closer match between actual and modeled leakages.\nNext, we outline the application of this approach to recover the secret weight and bias values of QNNs. Let w denote a secret value, and let\n$$w_1, w_2, ..., w_N$$\nbe all the possible values of the secret weight. For 8-bit QNNs,\n$$w_1, w_2, ..., w_N$$\nare given by\n$$-127, -126, ..., -2, -1, 0, 1, 2, ..., 126, 127$$\nand N = 256. During the inference phase, the weight value w is multiplied with a neuron input, and the resulting product is utilized for subsequent computations. To recover the value of w, our focus lies on this multiplication operation. We assume the attacker has knowledge of the neuron input:\nWhen w belongs to the first hidden layer, the neuron input can be inferred from the NN input, which is known to the attacker.\nFor inner layers, we anticipate the attacker first recovering parameters from preceding layers and subsequently computing the input of the inner layer using the recovered parameters.\nLet x represent the neuron input multiplied by w, yielding the product v.\nIn our experiments, we simulate actual leakage using stochastic leakage mod- els with different coefficients. We assume that the attacker either uses the Ham- ming leakage model or possesses profiling capabilities to correctly identify these coefficients for the stochastic leakage model."}, {"title": "4 Evaluation", "content": "In this section, we will report the results for our experimental evaluation. We conducted a simulation for weight and bias recovery and presented the results.\nWe then extended the work and report the findings for full QNN model."}, {"title": "4.1 Evaluation Scenarios", "content": "We first conducted the experiment on the recovery of weight and bias in the neural network. For the experimental evaluation, the leakage simulations were conducted employing stochastic leakage models with varying bit coefficients $$\\alpha_s$$\n(see Equation 1).\nWe considered two distinct leakage settings, each with different bit coefficients for the stochastic leakage model. Additionally, we examined two attack settings: the first assumes that the attacker does not have the full capability to profile the device and thus uses a Hamming weight leakage model, which may be inaccurate in this context; the second assumes that the attacker could profile the exact bit coefficients for the stochastic leakage model.\nPreliminary tests were conducted on several boards to profile the leakage be- havior. Upon profiling, we observed the common variances of the bit coefficients for the stochastic leakage model, which will then be used and adopted in our experiments. The following three scenarios were considered:\nScenario 1: The bit coefficients for the stochastic leakage model are ran- domly generated from a normal distribution with mean 1 and variance 0.09. In this setting, there are deviations between each bit coefficient. However, they are closer to the Hamming weight leakage model. Here, we consider an attacker who cannot profile the device and use a Hamming weight leakage model for recovery of the weights and biases."}, {"title": "4.2 Application to Full Networks", "content": "After analyzing the results for weight and bias recovery, we would like to evaluate the attack on full network. To evaluate the attack's performance on complete networks, we simulated the recovery of existing quantized neural networks and assessed the test accuracy.\nAs the target model, we consider quantized GoogleNet v1 model [23] from OpenVINO Model Zoo, on the ImageNet dataset. GoogleNet, also known as Inception v1, is a deep convolutional neural network that was developed by Google for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2014. It introduced the Inception module, which allows the network to capture multi-scale features while maintaining computational efficiency. The network has \u22486.7977M parameters, its high-level architecture is depicted in Figure 1. As such, it is a decent target for evaluating our model extraction approach.\nImageNet is a large-scale visual database designed for use in visual object recognition research [7]. It contains over 14 million images that have been hand- annotated to indicate the objects present in them. These images are organized according to the WordNet hierarchy, which includes more than 20,000 categories, such as \"balloon\" or \"strawberry.\""}, {"title": "5 Conclusion", "content": "In this paper, we presented an SCA attack on quantized neural network mod- els implemented in the OpenVINO framework. These models are meant to be deployed in embedded devices, thus allowing the attacker physical access to re- alize the side-channel measurements. Moreover, the quantization, restricting the model variables to be stored in 8-bit variables, greatly increases the precision of the parameter recovery. Our results show that for GoogleNet v1, the top model recovered by the SCA attack achieves similar accuracy to the original model, differing by 1% in Top 1 and by only 0.64% in Top 5 accuracies."}]}