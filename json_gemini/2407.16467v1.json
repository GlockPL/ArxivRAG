{"title": "Side-Channel Analysis of OpenVINO-based Neural Network Models", "authors": ["Dirmanto Jap", "Jakub Breier", "Zdenko Lehock\u00fd", "Shivam Bhasin", "Xiaolu Hou"], "abstract": "Embedded devices with neural network accelerators offer great versatility for their users, reducing the need to use cloud-based services. At the same time, they introduce new security challenges in the area of hardware attacks, the most prominent being side-channel analysis (SCA). It was shown that SCA can recover model parameters with a high accuracy, posing a threat to entities that wish to keep their models confidential.\nIn this paper, we explore the susceptibility of quantized models implemented in OpenVINO, an embedded framework for deploying neural networks on embedded and Edge devices. We show that it is possible to recover model parameters with high precision, allowing the recovered model to perform very close to the original one. Our experiments on GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference in the Top 5 accuracies.", "sections": [{"title": "Introduction", "content": "The rapid advancement in artificial intelligence (AI) and machine learning (ML) technologies has spread into various critical domains for our everyday lives, such as healthcare, finance, automotive, and more. Among the myriad AI frameworks available, OpenVINO [15] (Open Visual Inference and Neural Network Optimization) stands out as a powerful open-source toolkit. OpenVINO enables developers to optimize and deploy deep learning models on heterogeneous computing platforms, especially targeting edge and embedded devices.\nEmbedded devices, equipped with neural network accelerators offer great benefits in terms of efficiency and performance. However, they also introduce new security challenges, particularly related to hardware attack vectors such as side-channel analysis (SCA) attacks [2]. SCA attacks exploit information leakage from the physical implementation of a system rather than vulnerabilities in the algorithm itself [18]. These attacks can reveal sensitive data, such as neural network parameters and architecture, through indirect means such as power consumption, electromagnetic emissions, or execution time analysis [3]. This complements model stealing/extraction attacks that have been a well-research topic within the past decade [21]. The assumption for these attacks is that the creator of the model invested significant resources into preparing the training data (obtaining the initial data, labeling, and pre-processing it), and also into training the model. Thus, to have a competitive advantage, they prefer to keep the details of the model confidential. Apart from that, protecting the model parameters effectively prevents white-box adversarial attacks.\nThere are two main directions the attacker can take when extracting the model [21]:\n1. obtaining the exact model parameters,\n2. approximating the model behavior.\nWhen it comes to the first direction, recovering the exact model parameters has been shown possible by fault injection attacks, but only for one layer of a network obtained by transfer learning [5]. The goal of model extraction by the means of SCA attacks ultimately leads to the same [3], however, as the bit-width of the variables used for storing model parameters is generally up to 64 bits, the noise makes it very hard to determine the exact values from the SCA leakage. The majority of the literature in this direction thus focuses on approximating the model behavior, generally aimed at creating a substitute model exhibiting similar behavior to the original one.\nOur contribution. In this paper, we focus on investigating SCA attacks on neural networks implemented on embedded devices using OpenVINO. These models are implemented in a quantized manner, thus the model parameters are normally stored in 8-bit variables, making them more efficient in terms of storage and computational complexity, which is crucial for embedded implementations. For our experiments, we utilized an OpenVINO implementation of GoogleNet v1 with 6.7977 M parameters. The SCA attack allowed us to reconstruct the model that exhibits very similar accuracies to the original one. More specifically, the difference in Top 1 accuracy is 1% and in Top 5 accuracy it is just 0.64%.\nOrganization. The rest of the paper is organized as follows. Section 2 provides the background on the basic concepts utilized in this paper such as neural networks and SCA attacks, and gives an overview of the related work. Section 3 presents the methodology for extracting the model parameters using side channels. Section 4 details our evaluation and highlights the results. And finally, Section 5 concludes this work and provides potential future directions."}, {"title": "Background and Related Work", "content": "Neural Networks\nNeural networks (NNs) are a fundamental concept in the field of artificial intelligence and machine learning, inspired by the structure and function of the human brain [16]. NNs can recognize patterns, learn from data, and make decisions with minimal human intervention. They have become important in a wide range of applications, including image and speech recognition, natural language processing, and autonomous systems. They are supervised ML algorithms labeled dataset is used to train their model parameters with an optimization algorithm called (stochastic) gradient descent [11].\nTheir structure includes an input layer that receives the initial data for processing, one or more hidden layers (if there are more, we talk about deep neural networks (DNNs)), where the data is transformed through various weights and biases, and an output layer, which produces the result such as classification or prediction.\nThe original design of an NN, a multilayer perceptron, has been adjusted to fit specific problems. On the other hand, convolutional neural networks (CNNs) can adaptively learn spatial hierarchies of features, making them effective for image recognition tasks [17]. Another type, recurrent neural networks (RNNs) are capable of capturing temporal dependencies by using cycles within the network, allowing them to analyze sequential data such as time series or natural language.\nQuantized Neural Networks\nConventional NNs rely on high-precision floating-point arithmetic, utilizing either 32-bit or 64-bit variables. While such precision helps to achieve state-of-the-art performance, it requires substantial resources and memory bandwidth. Such a requirement limits their deployment in resource-constrained devices like mobile phones, embedded systems, and edge devices.\nQuantized neural networks (QNNs) address this limitation by using lower precision for the weights and activations in the network [13]. QNNs utilize low bit-width numbers such as 8-bit or 4-bit integers, or even binary values. The quantization significantly reduces memory requirements for storing the model and also the amount of computation required, resulting in faster computation and smaller power consumption.\nThe quantization can either be uniform, where all values are evenly distributed across the available range, or non-uniform which allows for a more tailored approach based on the data distribution. Despite the reduced number precision, QNNs often achieve accuracies comparable to their high-precision counterparts, especially when quantization-aware training and tuning are used."}, {"title": "OpenVINO Framework", "content": "OpenVINO [15] is an open-source software toolkit for optimizing and deploying DNN models, mostly on Intel devices but currently also on ARM processors. It supports a wide range of different platforms from edge to cloud, working with models in TensorFlow, PyTorch, ONNX, TensorFlow Lite, and PaddlePaddle model formats.\nIn this paper, we also utilize the Neural Network Compression Framework (NNCF) [15]. NNCF provides a suite of post-training and training-time algorithms for optimizing the inference of neural networks in OpenVINO."}, {"title": "Side-Channel Analysis Attacks", "content": "SCA is a method originally proposed for recovering secret information from cryptographic implementations [14]. Classical cryptanalysis methods focus on analyzing the weaknesses in the algorithms, utilizing techniques such as differential and linear cryptanalysis. As these are well-studied and understood, the new algorithms are always evaluated against them, rendering cryptanalytic attacks ineffective for a full-round cipher. However, as of now, there is no cryptographic algorithm inherently resistant to SCAs that analyze the information leakage during the physical operation of the cryptographic device. Such leakage can come in various forms such as power consumption, electromagnetic emanation, timing of the execution, and even acoustic leakage.\nSCAs can recover the secret key very efficiently, sometimes requiring only a single measurement [1]. There are generally two types of attacks: simple and differential. The simple attacks only utilize one or a few leakage traces for the attack. They can be either used as a starting point for the differential attacks, to determine the cipher rounds and round operations in block ciphers, or for a full attack, which is normally the case for public key algorithms [18]. Differential attacks, on the other hand, utilize higher numbers of measurements and utilize statistical methods to correlate the processed data and the leakage.\nLeakage models are used to describe and quantify the information leaked from the implementation under test through side channels during its execution. These models help in understanding, analyzing, simulating, and mitigating side-channel attacks. The main models used in SCA are as follows:\nHamming weight model: assumes that the information leaked is proportional to the number of bits set to 1 in the data being processed. It is commonly used when the underlying implementation is of software character.\nHamming distance model: assumes that the information leaked is proportional to the number of bit transitions (changes 1\u21920 or 0\u21921) between two consecutive states of the data.\nStochastic model: goes further than the previous two and assumes that every bit of the analyzed variable leaks differently.\nWe give a more thorough explanation of leakage models in Section 3.2."}, {"title": "Related Work - SCAs on Neural Networks", "content": "In the first part of this section, we will focus on various attacks on NNs, in the second part we will outline some of the possible defenses.\nAttacks. The first comprehensive work in this area by Batina et al. [3] explored how neural networks can be reverse-engineered using electromagnetic (EM) analysis. The paper demonstrated that a passive, non-invasive attacker can extract model details such as activation functions, number of layers, neurons, output classes, and weights from neural networks by analyzing EM signals. The experiments were conducted on an ARM Cortex-M3 microcontroller, showing the feasibility of such attacks on widely used hardware platforms.\nWang et al. [24] investigated the vulnerability of in-memory computing (IMC) systems to side-channel attacks. By simulating power traces of IMC macros, the researchers demonstrated that attackers can reverse-engineer neural network models, extracting details like layer types and convolution kernel sizes without prior knowledge.\nGao et al. [10] presented a novel attack method called DeepTheft, which targets DNN models deployed in Machine Learning as a Service (MLaaS) environments. By exploiting the Running Average Power Limit (RAPL)-based power side channel, the work demonstrated that it is possible to accurately recover complex DNN model architectures, including detailed layer-wise hyperparameters, even with low sampling rates.\nRyu et al. [22] introduced a novel attack method called Gamma-Knife. This attack leverages software-based power side channels to extract the architecture of neural networks without requiring physical access or high-precision measuring equipment. By utilizing statistical metrics, the Gamma-Knife attack can accurately determine key architectural details such as filter size, depth of convolutional layers, and activation functions. The researchers demonstrated the effectiveness of this attack on popular neural networks like VGGNet, ResNet, GoogleNet, and MobileNet, achieving an accuracy of approximately 90%.\nNagarajan et al. [20] investigated the vulnerabilities of spiking neural networks (SNNs) to power SCAs. The authors demonstrated that different synaptic weights and neuron parameters in SNNs produce distinct power and spike timing signatures, making them susceptible to SCAs. Through eight unique attacks, they showed that an adversary can reverse-engineer the specifications of an SNN.\nCountermeasures. Dubey et al. [9] proposed a novel hardware design that incorporates masking techniques. This design includes masked adder trees for fully connected layers and masked Rectifier Linear Units for activation functions. Experiments on a SAKURA-X FPGA board show that the proposed protection significantly increases the latency and area cost but effectively mitigates first-order differential power analysis attacks.\nIn [8], the authors proposed using modular arithmetic to make neural networks more compatible with masking techniques. They demonstrated this approach on binarized neural networks (BNNs) and developed novel masking gadgets using Domain-Oriented Masking (DOM). Their implementation on an FPGA showed that this method can achieve similar latency while reducing flip-flop (FF) and lookup table (LUT) costs by 34.2% and 42.6%, respectively, compared to state-of-the-art protected implementations. They verified the first-order side-channel security of their design with up to 1 million traces.\nBreier et al. [4] proposed a method to protect neural networks from side-channel attacks by making the timing analysis of activation functions more difficult. The authors introduced a desynchronization technique that adds random delays to the computation of activation functions, effectively hiding the dependency on the input and activation type. They experimentally verified the effectiveness of this countermeasure on a 32-bit ARM Cortex-M4 microcontroller, showing that it significantly reduces side-channel information leakage. The overhead of this method varies depending on the number of neurons, with an example overhead of 2.8% to 11% for a fully connected layer with 4,096 neurons in the VGG-19 network\nAs the topic of SCA on NNs is comprehensive and the number of papers in this area increases every year, we suggest interested readers to explore one of the recently published surveys for a full overview of the state-of-the-art [2,6,19]."}, {"title": "Methodology", "content": "Binary Representation of Numbers in Memory\nWhen a value $v$ is processed by a computational device, it is represented as a binary string. For integer values, $v$ is encoded using two's complement representation. For floating-point values, the IEEE 754 standard is employed.\nFor example, the two's complement representations for integers between -8 and 7 are shown in Table 1.\nA 32-bit (single precision) floating-point representation, following the IEEE 754 standard, comprises 1 sign bit, 8 exponent bits, and 23 fraction bits (also known as the significand or mantissa). The sign bit indicates the number's sign: 0 for positive and 1 for negative. The exponent is encoded using a biased representation with a bias of 127, enabling the exponent to represent both positive and negative values. The actual exponent is calculated by subtracting 127 from the 8-bit exponent. The mantissa encodes the significant digits of the number, incorporating an implicit leading bit of 1, which is assumed but not stored. Specifically, the binary string $b_{31}b_{30}...b_0$ represents the integer given by\n$(-1)^{b_{31}} \\times 2^{b_{30}b_{29}...b_{23} - 127} \\times 1.b_{22}b_{21}...b_0$,\nwhere the exponent $b_{30}b_{29}...b_{23}$ represents the integer given by\n$2^8\\times b_{30} + 2^7\\times b_{29} + ... + 2^0b_{23}$\nand $1.b_{22}b_{21}...b_0$ represents the number\n$1 + \\frac{1}{2^{1}}b_{22} + \\frac{1}{2^{2}}\\times b_{21} + ... + \\frac{1}{2^{23}}b_0$\nFor example, the binary string\n01000001001101100000000000000000\nhas sign bit 0, exponent\n10000010 = 130\nand mantissa\n01101100000000000000000.\nThis mantissa represents\n$1.011011_2 = 1 + \\frac{1}{4} + \\frac{1}{8} + \\frac{1}{32} + \\frac{1}{64} = 1.421875$.\nThe number the binary string represents is then given by\n$(-1)^0 \\times 2^{130-127} \\times 1.421875 = 11.375$.\nLeakage Models\nIn SCA, the leakage model characterizes the relationship between the computational leakage and the secret value $v$ processed by the device. Suppose\n$v = v_{n-1}v_{n-2}...v_1v_0 \\in F_2$\nis represented as a binary string of length n in the computer memory. The Hamming weight of $v$, denoted HW($v$), is given by the number of bits that are equal to 1 in $v$. For example,\nHW(110) = 2, HW(000) = 0, HW(110101) = 4.\nLet $ \\epsilon \\sim N(0, \\sigma^2)$ denote the noise in the leakage, modeled as a normal distribution with mean 0 and variance $ \\sigma^2$."}, {"title": "Leakage Models", "content": "A Hamming weight leakage model [12, Section 4.2] suggests that the leakage L($v$) is given by\nL($v$) = HW($v$) + $ \\epsilon$.\nA stochastic leakage model [12, Section 4.3] expresses the leakage as\n$L(v) = \\sum_{s=0}^{n-1} \\alpha_s v_s + \\epsilon$,\nwhere $ \\alpha_s (s = 0, 1, ..., n - 1)$ are real numbers. These numbers $ \\alpha_s$ are referred to as the coefficients of the stochastic leakage model.\nCorrelation Power Analysis\nWe employ the correlation power analysis (CPA) methodology originally devised for cryptographic implementation attacks [18]. CPA focuses on calculating Pearson's correlation coefficient between observed leakages and hypothetical leakages derived from a guessed secret value. The correct secret value is anticipated to yield the highest correlation coefficient, indicating a closer match between actual and modeled leakages.\nNext, we outline the application of this approach to recover the secret weight and bias values of QNNs. Let w denote a secret value, and let\n$W_1, W_2,..., W_N$\nbe all the possible values of the secret weight. For 8-bit QNNs,\n$W_1, W_2,..., W_N$\nare given by\n-127, -126, . . ., -2, -1, 0, 1, 2..., 126, 127\nand N = 256. During the inference phase, the weight value w is multiplied with a neuron input, and the resulting product is utilized for subsequent computations. To recover the value of w, our focus lies on this multiplication operation. We assume the attacker has knowledge of the neuron input:\nWhen w belongs to the first hidden layer, the neuron input can be inferred from the NN input, which is known to the attacker.\nFor inner layers, we anticipate the attacker first recovering parameters from preceding layers and subsequently computing the input of the inner layer using the recovered parameters.\nLet x represent the neuron input multiplied by w, yielding the product v. In our experiments, we simulate actual leakage using stochastic leakage models with different coefficients. We assume that the attacker either uses the Hamming leakage model or possesses profiling capabilities to correctly identify these coefficients for the stochastic leakage model."}, {"title": "Correlation Power Analysis", "content": "For the attack procedure, we first provide the network with randomly generated M inputs to obtain random neuron inputs x, denoted by\n$x_1, x_2, ..., x_M$\nSubsequently, the leakage associated with the product $v$ is simulated according to the predefined leakage model.\nLet $l_i$ denote the leakage corresponding to the input value $x_i$ for $i = 1, 2, ..., M$. In this case, $l_i$ simulates the leakage of\n$v_i: x_i \\times w$.\nThe simulation follows a stochastic leakage model (Equation 1) with pre-defined coefficients and random noise.\nNext, we make hypotheses of the value v: For each possible value, or hypothesis, of $w, w_1, w_2,..., w_N$ and each input value of $x, x_1, x_2,..., x_M$, we compute the hypothetical value of the product:\n$v_{ij} = w_i \\times x_j$.\nWe then calculate the hypothetical leakage, $H_{ij}$, of this hypothetical product $v_{ij}$ using the attacker's leakage model. This leakage model can either be the Hamming weight model or a profiled stochastic leakage model, assumed to match the one used for simulating leakages.\nFor example, if the Hamming weight leakage model is employed for the attack, the hypothetical leakage of $v_{ij}$ is given by\n$H_{ij} = HW(v_{ij})$.\nWith CPA, we compute the Pearson correlation between the simulated leakage and the hypothetical leakage for each weight hypothesis using the formula:\n$r_i := \\frac{\\sum_{j=1}^{M}(H_{ij} - \\overline{H_i})(l_j - \\overline{l})}{\\sqrt{\\sum_{j=1}^{M}(H_{ij} - \\overline{H_i})^2 \\sum_{j=1}^{M}(l_j - \\overline{l})^2}}, i = 1, 2, ..., N$.\nHere,\n$\\overline{H_i} = \\frac{1}{M} \\sum_{j=1}^{M} H_{ij}$\nis the averaged hypothetical leakage for weight hypothesis $w_i$, and\n$\\overline{l} = \\frac{1}{M} \\sum_{j=1}^{M} l_j$\nis the averaged simulated leakage."}, {"title": "Correlation Power Analysis", "content": "In case the hypothesis of the weight value, $w_i$, is correct, we expect the corresponding coefficient $r_i$ to have a high absolute value. Our guessed weight is then given by\n$w_{guessed} = w_{i_0}$, where $i_0 = arg \\max_i r_i$.\nFurthermore, in case multiple weight hypotheses achieve the highest absolute value of $r_i$, we select the one with the smallest absolute difference between the hypothetical and simulated leakages:\n$w_{guessed} = w_{i_0}$, where $i_0 = arg \\min_{iw_i achieves the highest absolute value of r_i} \\sum_{j=1}^{M}(H_{ij} - l_j)$.\nThe attack to recover bias values is similar. Instead of focusing on the multiplication operation, we concentrate on the addition operation, which adds the bias to the product of weights and neuron inputs. We assume the attacker has already recovered the weight values in the given layer. With knowledge of the neuron inputs, the attacker can compute the value added to the secret bias."}, {"title": "Evaluation", "content": "In this section, we will report the results for our experimental evaluation. We conducted a simulation for weight and bias recovery and presented the results. We then extended the work and report the findings for full QNN model.\nEvaluation Scenarios\nWe first conducted the experiment on the recovery of weight and bias in the neural network. For the experimental evaluation, the leakage simulations were conducted employing stochastic leakage models with varying bit coefficients $ \\alpha_s$ (see Equation 1).\nWe considered two distinct leakage settings, each with different bit coefficients for the stochastic leakage model. Additionally, we examined two attack settings: the first assumes that the attacker does not have the full capability to profile the device and thus uses a Hamming weight leakage model, which may be inaccurate in this context; the second assumes that the attacker could profile the exact bit coefficients for the stochastic leakage model.\nPreliminary tests were conducted on several boards to profile the leakage behavior. Upon profiling, we observed the common variances of the bit coefficients for the stochastic leakage model, which will then be used and adopted in our experiments. The following three scenarios were considered:\nScenario 1: The bit coefficients for the stochastic leakage model are randomly generated from a normal distribution with mean 1 and variance 0.09. In this setting, there are deviations between each bit coefficient. However, they are closer to the Hamming weight leakage model. Here, we consider an attacker who cannot profile the device and use a Hamming weight leakage model for recovery of the weights and biases.\nScenario 2: The bit coefficients for the stochastic leakage model are randomly generated from a normal distribution with mean 1 and with a higher variance value of 1. Similarly, for the attack, the attacker does not know the exact bit coefficient values and assumes a Hamming weight leakage model.\nScenario 3: The bit coefficients for the stochastic leakage model are randomly generated from a normal distribution with mean 1, with a similar higher bit variance of 1. In this scenario, the attacker is assumed to have accurately profiled the leakage and identified the correct bit coefficient values and, as such, can build a more precise leakage model for the key recovery attack.\nAfter setting the leakage model bit coefficients, we generate the simulated side-channel traces. For our experiment, our main focus will be on investigating the impact of varying bit coefficients of the leakage model. As such, we will be fixing the noise level. The noise in the leakage model, $ \\epsilon \\sim N(0,0.5)$, follows a normal distribution with mean 0 and variance 0.5. This ensures a fair comparison of all the attack scenarios.\nUsing this setting, we generate a set of simulated traces. We simulate weight and bias recovery using CPA with 100,000 traces for one simulated attack, i.e. M = 100,000 following the notations from Section 3. For the recovery of weights, a total of 250 attacks were conducted, with randomly generated weight values for each attack. Similarly, 250 attacks were performed for the recovery of biases, with randomly generated bias values for each attack.\nWe record the accuracy as the rate at which the correct weight or bias is successfully recovered. We denote the average error as the absolute difference between the actual weight or bias and the recovered value. The results for each weight and each bias under different scenarios are reported as follows:\nScenario 1: We achieved accuracy 80.7% with average error 1.86 for weight recovery and accuracy 29.6% with error 0.19 for bias recovery.\nScenario 2: We achieved accuracy 33.5% with average error 1.04 for weight recovery and accuracy 7.6% with error 26014.7 for bias recovery.\nScenario 3: We achieved accuracy 46.7% with average error 2 for weight recovery and accuracy 100% for bias recovery.\nAs observed from the results, when the variance of the bit coefficient is low, the Hamming weight leakage model proves sufficient, as indicated by the high accuracy in weight recovery. For bias recovery, although the accuracy is not high, the low average error can be attributed to targeting the addition operation rather than the multiplication operation, as is the case with weight recovery.\nInterestingly, when the attacker can profile the bit coefficients, even with high variance, the accuracy for weight recovery decreases, but perfect accuracy is achieved for bias recovery. Nonetheless, for high variance in the bit coefficient, the use of the Hamming weight leakage model results in poor performance."}, {"title": "Application to Full Networks", "content": "After analyzing the results for weight and bias recovery, we would like to evaluate the attack on full network. To evaluate the attack's performance on complete networks, we simulated the recovery of existing quantized neural networks and assessed the test accuracy.\nAs the target model, we consider quantized GoogleNet v1 model [23] from OpenVINO Model Zoo, on the ImageNet dataset. GoogleNet, also known as Inception v1, is a deep convolutional neural network that was developed by Google for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2014. It introduced the Inception module, which allows the network to capture multi-scale features while maintaining computational efficiency. The network has \u22486.7977 M parameters, its high-level architecture is depicted in Figure 1. As such, it is a decent target for evaluating our model extraction approach.\nImageNet is a large-scale visual database designed for use in visual object recognition research [7]. It contains over 14 million images that have been hand-annotated to indicate the objects present in them. These images are organized according to the WordNet hierarchy, which includes more than 20,000 categories, such as \"balloon\" or \"strawberry.\""}, {"title": "Application to Full Networks", "content": "We considered the recovery of the quantized GoogleNet v1 model under 3 scenarios presented earlier. For each scenario, we repeat the procedure 5 times. We reported the results in Table 2. We report the top 1 and top 5 accuracy (%) using the accuracy checker from OpenVINO. As a comparison, we also reported the same accuracies for the original GoogleNet v1 model.\nAs can be seen from Table 2, we observe that the recovered model from Scenario 1 and 3 performs almost similarly to the original GoogleNet model. This makes sense since the leakage model quite matches the leakage behavior. However, for Scenario 2, when the attacker is using the Hamming weight leakage model when targeting leakage with high variance for their bit coefficients, the performance degrades significantly.\nIn this case, the observed results indicate that the leakage model plays a crucial role in the success of the model recovery. As an attacker, the main target is then to develop a leakage model closer to the actual leakage. However, this task is not trivial, and without profiling, the attacker can only rely on the standard Hamming weight model. As such, solving this issue will be a main priority for future work."}, {"title": "Conclusion", "content": "In this paper, we presented an SCA attack on quantized neural network models implemented in the OpenVINO framework. These models are meant to be deployed in embedded devices, thus allowing the attacker physical access to realize the side-channel measurements. Moreover, the quantization, restricting the model variables to be stored in 8-bit variables, greatly increases the precision of the parameter recovery. Our results show that for GoogleNet v1, the top model recovered by the SCA attack achieves similar accuracy to the original model, differing by 1% in Top 1 and by only 0.64% in Top 5 accuracies."}]}