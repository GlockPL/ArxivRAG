{"title": "Problem-oriented AutoML in Clustering", "authors": ["Matheus Camilo da Silva", "Gabriel Marques Tavares", "Eric Medvet", "Sylvio Barbon Junior"], "abstract": "The Problem-oriented AutoML in Clustering (PoAC) framework introduces a novel, flexible approach to automating clustering tasks by addressing the shortcomings of traditional AutoML solutions. Conventional methods often rely on predefined internal Clustering Validity Indexes (CVIs) and static meta-features, limiting their adaptability and effectiveness across diverse clustering tasks. In contrast, POAC establishes a dynamic connection between the clustering problem, CVIs, and meta-features, allowing users to customize these components based on the specific context and goals of their task. At its core, PoAC employs a surrogate model trained on a large meta-knowledge base of previous clustering datasets and solutions, enabling it to infer the quality of new clustering pipelines and synthesize optimal solutions for unseen datasets. Unlike many AutoML frameworks that are constrained by fixed evaluation metrics and algorithm sets, PoAC is algorithm-agnostic, adapting seamlessly to different clustering problems without requiring additional data or retraining. Experimental results demonstrate that PoAC not only outperforms state-of-the-art frameworks on a variety of datasets but also excels in specific tasks such as data visualization, and highlight its ability to dynamically adjust pipeline configurations based on dataset complexity.", "sections": [{"title": "1 Introduction", "content": "Most Machine Learning (ML) applications are composed by a sequence of steps (i.e., a pipeline) involving the selection of algorithms and the setting of their hyperparameters. However, carefully designing good performing pipelines is a tedious, error-prone task that often relies on expert knowledge, which is frequently unavailable or expensive(Olson et al., 2016). Automated Machine Learning (AutoML) provides the tools to make the creation and configuration of optimal ML solutions more accessible to practitioners with varying levels of expertise by automating the complex and time-consuming aspects of ML development (Hutter et al., 2019).\nThis automation is particularly straightforward for supervised tasks where labeled data is available, allowing for objective evaluation metrics like accuracy, precision, or F1 score to guide optimization. By systematically testing different pipeline configurations,\nAutoML can identify the most effective solutions for a given dataset (He et al., 2021; Truong et al., 2019).\nIn the case of unsupervised tasks, the lack of labels, pose a challenge for the evaluation of generated pipelines. A conventional approach is the reliance on internal Clustering Validity Indexes (CVI) to synthesize and optimize clustering solutions (Bahri et al., 2022), while ignoring any external information of the clustering problem.\nThis approach's only concern is to optimize CVIs (Kryszczuk and Hurley, 2010; Marutho et al., 2018; Tschechlov et al., 2021). This practice however can be limiting as each internal score evaluates only a facet of the relationships between data points in a clustering setting. Moreover, different evaluation metrics may lead to different conclusions about the quality of a clustering solution (Rokach and Maimon, 2005). For this reason, properly defining the clustering problem (i.e., the goal of the clustering analysis) and a corresponding optimization function is crucial for generating high quality automated clustering solutions (von Luxburg et al., 2012).\nAnother approach in AutoML for clustering is to create solutions based on Meta- learning, focusing on leveraging prior experience to enable models to quickly adapt to new tasks by learning from past clustering problems and using this knowledge to inform future decisions (Brazdil et al., 2022; Hutter et al., 2019). By accumulating insights from diverse datasets, meta-learning provides a way to transfer knowledge, thus reducing the time and computational resources needed to optimize new tasks. However existing works, such as (Poulakis et al., 2020; Liu et al., 2021; ElShawi et al., 2021; Treder-Tschechlov et al., 2023), tend to restrict their application of meta-learning by attaching a fix set of algorithms, hyperparameter configurations, and CVIs to their meta-database, which can hinder generalization. This presents an opportunity for more flexible solutions that can adapt across a wider variety of clustering tasks and optimization objectives.\nSince there is not a universal metric capable of describing every clustering goal equally well (Bonner, 1964), the definition of clustering quality can be subjective and dependent on the context of the problem that the practitioners conducting the clustering aim to solve (Bonner, 1964). That is, the clear association between the clustering task at hand and suitable metrics to measure performance should be considered when developing clustering solutions (von Luxburg et al., 2012; Hennig, 2015; Van Mechelen"}, {"title": "2 Theoretical Background", "content": "AutoML refers to the application of techniques to automate the process of designing and optimizing ML pipelines (Hutter et al., 2019), i.e., acting in the intersection of automation and ML (Yao et al., 2019). Considering the plethora of algorithms and techniques for each step of a ML pipeline, the configuration space grows exponentially (Hutter et al., 2019). In practical scenarios, the abundance of options available for pipeline development introduces significant complexity, posing challenges even for seasoned experts. The conventional approach to this complexity involves iterative processes, frequently relying on trial-and-error methodologies (Z\u00f6ller and Huber, 2021). Unfortunately, this process is inherently resource-intensive and time-consuming, often resulting in sub optimal pipelines due to the limited exploration of potential configurations.\nAutoML literature has developed specific methods to solve different optimization problems, that is, breaking down the pipeline search to smaller, simpler tasks. The"}, {"title": "3 Related Works", "content": "The exploration of AutoML applications specifically tailored for clustering problems has gained some prominence (Ferrari and de Castro, 2015; Pimentel and de Carvalho, 2019; de Souto et al., 2008; Pimentel and de Carvalho, 2018; Nascimento et al., 2009; Ferrari and de Castro, 2012; Soares et al., 2009; Fernandes et al., 2021; Pimentel and de Carvalho, 2019; Gabbay et al., 2021). For that, many apply meta-learning techniques, which are known to perform quite well for AS. However, they are only able to select an algorithm out of a portfolio. Considering (i) the complexity of real environments (i.e., necessity of configuring several pipeline steps), (ii) the importance of tuning hyperparameters, specially for clustering, and (iii) that our solution searches for the complete pipeline, we limit the scope of our related work to research that tackles at least the CASH problem.\n(Poulakis et al., 2020) presented a framework for clustering problems based on a meta-learning approach. For this, the authors first build a meta-database containing information about datasets behavior (captured by meta-features) combined with CVIs"}, {"title": "4 Problem-oriented AutoML in Clustering (PoAC)", "content": "An overview of the proposed approach is displayed in Figure 1. It consists of four main stages: Problem Space Design, Feature Space Mapping, Surrogate Modeling, and Function Optimization. Each of these stages is discussed in detail below.\nThe first stage is the composition of a problem space with labeled clustering datasets related to specific goals, such as visualization, denoising, dimensionality reduction, etc. This foundational process aims to assemble a sizable and diverse problem space that encapsulates a broad spectrum of clustering challenges, patterns, and complexities.\nIn the second stage, the datasets are mapped to a feature space, allowing PoAC to build an enriched knowledge representation for surrogate model induction. The\nenriched feature vector is composed of unsupervised meta-features extracted from the datasets and CVIs. These CVIs are derived by adding increasing levels of noise to the cluster labels of each dataset, providing a range of performance values.\nThe meta-features provide a higher-level representation of characteristics extracted from the datasets, capturing essential information about their structure, complexity, and statistical properties, as well as from pipeline candidate solutions.\nThe third stage is the training of a surrogate model to predict a continuous target value (external CVI). In this scenario, it is possible to employ various regression algorithms (or ensembles) for training the surrogate model. By leveraging regression algorithms for the surrogate model, one can empower it with the capacity to discern intricate relationships between meta-features, internal and external CVIs, paving the way for a robust and task-specific automated PS in the final step of the PoAC framework.\nThe fourth stage is the stage where a given optimization method takes advantage of the trained surrogate model as a objective function to synthesize complete pipelines for clustering according to the specific problem defined in the first stage."}, {"title": "4.1 Problem statement: surrogate-based PS for clustering", "content": "We define a dataset $D = {x^{(i)}}_{i=1}^{n} \\in P(X)$ and $X$ as a set of observations, each observation being $x^{(i)} \\in X$, and $n$ is the cardinality of $X$.\nWe define a problem $u \\in U$ as a user-defined clustering goal to partition the data $D$.\nWe define meta-features $\\mu : P(P(X)) \\rightarrow \\mathbb{R}^{l}$ as a function that maps a dataset $D\\in P(X)$ to a $l$-dimensional numerical space.\nWe define a preprocessing technique $p: P(X) \\rightarrow P(X')$ as a function that maps a dataset $D \\in P(X)$ to another dataset $p(D, A_{p}) \\in P(X')$; we denote by $P_{X, X'}$ the set of preprocessing techniques compatible with $X$ and $X'$; we denote $A_{p}$ as the set of hyperparameters associated with the selected preprocessing method.\nWe define a clustering algorithm as function $a : P(X') \\rightarrow P(P(X'))$ that takes a dataset $D \\in P(X')$ and returns a partition $a(D, \\Lambda_{a}) \\in P(P(X'))$ of $D$, i.e., a set of subsets of $D$ such that $\\bigcup_{D'\\in a(D)} D' = D$ and $\\forall D', D'' \\in a(D), D' \\cap D'' = \\emptyset$; we denote by $A_{a}$ the set hyperparameters associated with the selected algorithm;\nWe define a CVI $m : P(P(X')) \\rightarrow \\mathbb{R}$ as a function that measures the quality of a partition for simplicity, we assume that for the values of $m$, the greater, the better; we denote by $M_{X}$ the set of CVIs compatible with $X$.\nWe define a clustering pipeline over $X$ a tuple $((p_{1}, A_{p_{1}}), ..., (p_{k}, A_{p_{k}}), (\\alpha, \\Lambda_{\\alpha}))$ such that $(p_{1}, A_{p_{1}}) \\in P_{X, X_{1}}, (p_{2}, A_{p_{2}}) \\in P_{X_{1}, X_{2}}, ..., (p_{k}, A_{p_{k}}) \\in P_{X_{k-1}, X_{k}}$ is a sequence of mutually compatible preprocessing techniques and their corresponding hyperparameters and $(\\alpha_{1}, \\Lambda_{\\alpha_{1}}) \\in A_{X_{1}}$ is a clustering algorithm and its corresponding hyperparameters compatible with the output of $(p_{k}, A_{p_{k}})$, i.e., $X_{k}$. For the sake of clarity and without a practical loss of generality, provided that the used $X$ is general enough, we consider, from now on, only clustering pipelines where all the elements (preprocessing techniques and clustering algorithm) operate on the same $X$. Under this assumption, and omitting the subscript $X$, the set of all clustering pipelines defined over $X$ is $P^* \\times A \\times A_{p^*} \\times \\Lambda_{A}$, where $P^*$ denote the sequence of elements of $P$.\nGiven a CVI $m \\in M$ and a dataset $D \\in P(X)$, finding the best pipeline $(p^*, a^*, *)$ corresponds to solving the following optimization problem:\n$(p^*, a^*, *) = \\underset{(p, \\alpha, \\lambda) \\in P^* \\times A \\times \\Lambda}{\\arg \\max} (m \\circ a \\circ p)(D, \\Lambda_{a}, \\lambda_{\\mu})$\nwhere $p = {p_{1}, ..., p_{k}}$ is a sequence of preprocessing techniques, which can be empty, and the operator $o$ denotes the composition of functions.\nIt can be seen that solving the optimization problem defined above by enumeration would require actually applying each candidate pipeline to the dataset $D$. We here propose to use a surrogate function $f_{m} : P^* \\times A \\times \\Lambda \\times \\mu(P(X)) \\rightarrow \\mathbb{R}$ that, given a pipeline $(p, \\alpha, \\lambda)$ and a dataset $D$, approximates the value of the CVI $m$ computed on the partition obtained applying the pipeline on the dataset, i.e.:\n$f_{m} ((p,a, \\lambda), \\mu(D)) \\approx (m\\circ a\\circ p)(D)$\nSolving the problem of finding the best pipeline for a CVI $m\\in M$ and a dataset $D\\in P(X)$ can be hence cast to optimizing $f_{m}$:\n$\\underset{(p, \\alpha, \\lambda) \\in P^* \\times A \\times \\Lambda}{\\arg \\max} f_{m}((p,a, \\lambda), \\mu(D), u)$"}, {"title": "5 PoAC for the visualization problem", "content": "In this work, we consider the visualization as the goal of a PoAC instance. Recently, the relevance of visualization for the interpretation of ML methods is increasing (Chatzimparmpas et al., 2020), specially in unsupervised learning, where it provides a unique lens through which complex data structures can be interpreted. The objective of clustering for visualization is to uncover meaningful patterns or groupings within datasets and represent them in a visually comprehensible manner (Ezugwu et al., 2022; Al-Jabery et al., 2019). The aim is not only to identify clusters but also to convey their inherent relationships and structures graphically. This task finds applications across various domains, from exploratory data analysis to pattern recognition, where understanding the inherent organization of data fosters insights and informed decision- making (von Luxburg et al., 2012). Effective visualization-driven clustering strategies contribute to the development of interpretable and actionable representations (Assent, 2012), facilitating a deeper understanding of intricate dataset structures, even in high-dimensional data (Strehl and Ghosh, 2003)."}, {"title": "5.1 Problem Space Design", "content": "We have synthesized 6.130 datasets to represent a vast range of clustering problems to compose the $P(X)$. These datasets were created by combining different ranges of important clustering characteristics (Zellinger and B\u00fchlmann, 2023), namely: number of dimensions, number of clusters, quantity of data points, imbalance ratio as well as the shapes of the clusters, described in Table 1."}, {"title": "5.2 Feature Space Mapping", "content": ""}, {"title": "5.2.1 Dataset Meta-features", "content": "The meta-features selected for this work were chosen based on (Lorena et al., 2019) given the comprehensibility of the proposed set of descriptors. In summary the relevant ones in the context of this work amount to 37, and are described in Table 2."}, {"title": "5.2.2 Internal CVI", "content": "We opted to use Silhouette Index (SIL) (Rousseeuw, 1987) and Davies-Bouldin Score (DBS) (Davies and Bouldin, 1979) as CVIs. Their selection as visualization descriptors for clustering problems is grounded in their ability to provide comprehensive insights into the quality and distinctiveness of clustering solutions. A high SIL suggests visually distinct and well-separated clusters, enhancing the interpretability of visual representations (Shahapure and Nicholas, 2020; Bagirov et al., 2023). Complementing this, a low DBS reinforces the notion of visually cohesive and distinguishable clusters, contributing to an enhanced visual understanding (Maulik and Bandyopadhyay, 2002; Thomas et al., 2013). The two CVIs assess complementary aspects in regards to visualization, their combination allows for a more reliable optimization function, i.e., providing more sound clustering solutions. We chose not to include other internal CVIs due to (i) not being complementary (i.e., measuring the same facets of the problem) and (ii) including many dimensions to optimize makes the optimization problem more challenging and resource costly.\nSIL ranges from -1 to 1: a value close to 1 for a data point indicates that it is well-clustered, meaning it is further away from neighboring clusters than its own; a"}, {"title": "5.3 Surrogate Modeling", "content": ""}, {"title": "5.3.1 External CVI", "content": "For m, we chose the Adjusted Random Index, as defined in Equation 4, to quantify the difference between the augmented partitions and the ground truth according to the original dataset. The intent for the composition of \u00b5 and m in this way is to create a range of possible partitionings that represents how visually different they are to the original clustering. By jointly employing these metrics with the meta-features, we aim to capture a nuanced understanding of the visual separability and coherence of clusters in our evaluation, ensuring a robust assessment of the visibility of clustering patterns in diverse datasets.\nThe Adjusted Random Index (ARI) is a essentially an expansion of the Rand Index (RI), which is a measure of similarity between two partitionings (Hubert and Arabie,"}, {"title": "5.3.2 Surrogate Model", "content": "As the surrogate model $f_{m}$, we trained a Random Forest regressor (Breiman, 2001) considering the observations $X$ and the independent variables $\\mu$, while also considering"}, {"title": "5.4 Function Optimization", "content": "Lastly, we extended the capabilities of the AutoML framework Tree-based Pipeline Optimization Tool (TPOT), presented by (Olson and Moore, 2016), to allow it to synthesize pipelines for clustering problems. This extension consists of a set of important modifications. Mainly, the incorporation of the surrogate model to serve as a fitness function for the evolutionary optimization process. This entails the extraction of meta- features on new unseen clustering datasets, and the measuring of both SIL and DBS for each synthesized clustering pipeline solution, as displayed in Figure 4. TPOT was chosen due to its robustness, high level of maintenance, and widespread use within the machine learning community, making it a reliable and well-supported tool for extending into new domains. These qualities make TPOT particularly suitable for our purposes, as it provides a stable and flexible framework for automating complex machine learning tasks."}, {"title": "5.5 Baselines", "content": "We designed and performed experiments to evaluate the quality of PoAC when compared to the reproducible state-of-the-art frameworks that focus on CASH, namely ML2DAC from (Treder-Tschechlov et al., 2023), Autocluster proposed by (Liu et al., 2021), cSmartML as presented in (ElShawi et al., 2021) and, finally, AutoML4Clust presented in (Tschechlov et al., 2021). As for PoAC, we specifically created an instance of the framework for visualization as a clustering problem (Ezugwu et al., 2022) and assessed the performance of pipeline optimizations. In the context of the proposed problem- oriented methodology, prioritizing visualization as a clustering problem underscores its significance as a user-defined goal, catering to scenarios where the interpretability and visual clarity of clustering solutions are paramount.\nWe considered two distinct groups of datasets to comprehensively evaluate the frameworks in the context of clustering. The first group is composed of datasets used in the benchmark work done by (da Silva et al., 2024), it consists of one hundred synthetic clustering datasets with ranging degrees of instances distributions, clusters separations and densities, among others, as described in Table 4.\nThe second group (Table 5), encompassed twenty two datasets sourced from the UCI repository (Markelle Kelly, 2017), providing a real-world dimension to our analysis, they were chosen for their usage in the validation of related works, namely: Ml2dac and AutoCluster."}, {"title": "6 Results and Discussion", "content": "We applied each one of the frameworks, as well as the proposed POAC, on the group of one hundred validation datasets. The results are presented in Table 6. The obtained results pertaining ARI will be discussed in subsection 6.1.\nPOAC exhibited outstanding results, obtaining an average SIL of 0.54, suggest- ing well-defined and separated clusters, surpassing other frameworks in this regard."}, {"title": "6.1 Clustering Performance Analysis", "content": "When comparing the similarities between the solutions from the evaluated frameworks and the original clusters for the dataset validation group, the results unveiled a noteworthy performance. Our proposed PoAC method achieved a mean ARI of 0.70.\nML2DAC secured the second position with a 0.67 mean ARI, followed by AutoML4Clust at 0.59, Autocluster at 0.58, and cSmartML at 0.38. We conducted a statistical analysis for 5 populations with 100 paired samples of ARI values. The first analysis showed that some of the populations did not present normal distributions, in particular the populations of AutoML4Clust, Autocluster, and PoAC. Since we have a set of more than two populations and some of them are not normal, we applied the non-parametric Friedman test with a confidence level of 95% that determined that there is a significant difference between the median values of the populations. Based on the post-hoc Nemenyi test (Nemenyi, 1963), with a critical distance (CD) of 0.61, as shown in Figure 6, we can affirm that there are no significant differences within the following groups: AutoML4Clust, Autocluster, and ML2DAC; ML2DAC and PoAC. All other differences are significant, namely cSmartML when compared to any other population and PoAC when compared to AutoML4Clust and Autocluster.\nTo better assess the differences in capabilities between ML2DAC and PoAC in gen- erating clustering solutions, we replicated the experiments conducted in the ML2DAC paper (Treder-Tschechlov et al., 2023) by evaluating the two frameworks using the datasets from the second validation group, thereby enabling a direct comparison with our proposed approach using real-world scenarios. The configuration identified by ML2DAC achieved a maximum ARI of 0.27, with their simplest configuration attain- ing 0.19. In comparison, our proposed method exhibited a competitive performance, achieving an ARI of 0.22.\nThe comparison of our proposed method with other existing frameworks reveals distinctive advantages. In the first experiment, PoAC exhibited the best performance showcasing the efficacy of using a surrogate model as an fitness function for PS in clustering. Furthermore, an important aspect is the adaptability of the surrogate model trained by POAC on clustering solutions with a specific emphasis on the visualization goal. Despite its specialization, the surrogate model demonstrated a comparable performance to ML2DAC when applied to datasets not specifically clustered for visualization, as evident in the UCI group. Remarkably, PoAC achieved this level of performance without requiring any additional fine-tuning or configuration adjustments."}, {"title": "6.2 Generated Pipeline Complexity", "content": "Through an analysis of the mean pipeline complexity (i.e., number of pipeline steps) across generations, our findings reveal a notable trend within the PoAC framework. Over successive generations, PoAC exhibits a consistent evolution towards pipelines with a mean complexity of at least two steps, as shown in Figure 7. The stabilization around a complexity of two steps signifies an optimal balance between the incorporation of essential preprocessing actions and a clustering algorithm. This trend not only underscores the adaptability of PoAC but also highlights its efficiency in autonomously converging towards suitable.\nAs depicted in Figure 8, the density of pipeline complexity values shows a clear differentiation based on ARI values. Pipelines with lower ARI scores are associated with a somewhat even distribution of complexities, reflecting a more exploratory nature in the search for optimal solutions. Conversely, pipelines achieving higher ARI values tend to cluster around a complexity of one and two steps, aligning with the observed results in Figure 7. However, it is important to note that there is a small, but not insignificant volume of pipelines with three and four steps. This reinforces the notion that within the PoAC framework, the evolution towards simpler pipelines is not at the expense of performance, but rather a convergence towards the most efficient configurations for the task at hand.\nIn the realm of clustering problems, the choice and sequence of ML techniques can significantly impact performance. Figure 9 illustrates this phenomenon in the context of the UCI dataset dermatology (Ilter and Guvenir, 1998), where two distinct clustering solutions are showcased using a Principal Component Analysis (PCA) plot. PCA is commonly used in clustering to reduce high-dimensional data to two or three dimensions for visualization, helping to reveal patterns or separations between clusters. By projecting data onto a lower-dimensional space, PCA aids in assessing how well clusters are formed, though it may miss non-linear structures (Jolliffe and Cadima, 2016). The first solution given by ML2DAC solves the CASH task by recommending the Kmeans algorithm using 2 as the number of clusters hyperparameter. PoAC is\ncapable of generating a pipeline for the dermatology dataset with multiple steps: (i) MinMaxScaler as a preprocessing step; (ii) MiniBatchKMeans as the clustering algorithm; (iii) HPO by setting the batch size to 10 and the number of clusters to 6. Notably, the more complex pipeline presented a more faithful clustering regarding the original one, underscoring the importance of solving the complete pipeline (i.e., PS) instead of only limiting to choose the algorithm and hyperparameters (i.e., CASH). This empirical demonstration highlights the potential benefits of incorporating automated PS techniques, such as those offered by PoAC, to elevate clustering performance.\nAn inherent strength of the PoAC framework lies in its algorithm-agnostic nature, which sets it apart from other AutoML techniques that rely on predefined clustering algorithms and hyperparameter configurations to build a meta-database. In contrast, POAC introduces a novel augmentation step during the meta-learning phase, where noise is systematically added directly to the cluster labels of synthetic datasets, rather than depending on a limited set of clustering algorithms and hyperparameters, as is common in related work. This approach allows the surrogate model to learn patterns of partitioning that are less tied to specific algorithms, adding flexibility not only during the training phase but also in the optimization phase. This capability allows POAC to remain adaptable during the optimization process, as the search space can be dynamically altered without requiring re-training of the surrogate model. As a result, the framework is able to optimize over a broader, more flexible space of potential solutions, further reinforcing its algorithm-agnostic stance."}, {"title": "6.3 Ablation Study", "content": "It is crucial to ensure that the performance/complexity trade-off is justified. To this end, we conduct an ablation study to evaluate the efficacy of the complete PoAC method against sub-configurations of its components in the task of clustering PS. We named the different optimization strategies considered in this study respectively:\n1. PoAC SIL optimizes pipelines based solely on their SIL score, favoring those that demonstrate higher clustering quality as indicated by this metric.\n2. PoAC DBS focuses exclusively on the DBS (Dunn's Boundary Separation) score, selecting pipelines that achieve superior performance according to this criterion.\n3. PoAC CVI adopts a different approach by creating a surrogate model that com- bines SIL and DBS scores, specifically examining their non-linear correlation without incorporating the meta-features group. This surrogate model, trained using a Ran- dom Forest on the same dataset as the complete PoAC method, provides a more nuanced understanding of the relationship between SIL and DBS.\nWe ran each of the four optimization strategies ten times for every dataset in the first validation group (Table 4), and recorded the mean ARI per dataset for each of the strategies.\nThe boxplot analysis in Figure 10 reveals distinct performance characteristics across the four aforementioned optimization strategies, based on their ARI scores. PoAC presents a relatively narrow interquartile range (IQR), indicating less variability in clustering performance. The upper quartile of the data extends close to the maximum possible ARI score of 1.0, and numerous individual data points are clustered near this upper bound. The median ARI is significantly higher than the other strategies, around 0.8. This suggests that PoAC frequently achieves highly accurate clustering results. While the lower whisker does extend down to approximately 0.1, highlighting\nsome instances of lower performance, PoAC generally offers robust and dependable clustering outcomes.\nPOAC CVI, while somewhat similar to PoAC, exhibits a lower median ARI score, around 0.6, and a slightly wider IQR. This wider range might indicate more variability in performance. The lower whisker reaches down to 0.0, meaning that POAC CVI occasionally produces very poor clustering results. Nevertheless, it also achieves a significant number of high ARI scores, demonstrating that while it may be less consistent than PoAC, it can still be effective in certain cases.\nPOAC SIL, shows a wide spread of ARI values. The median ARI is around 0.5, which suggests moderate clustering performance. The IQR is substantial, indicating variability in the results. While it can achieve results around the maximum value, suggesting some instances of excellent performance, the overall inconsistency is notable.\nLastly, the ARI values for POAC DBS are also widely distributed, covering the full range from 0.0 to 1.0. However, the median ARI is lower than PoAC SIL, and the larger IQR reflects even greater variability in clustering performance, it is in fact the largest in the whole group of strategies. This indicates that while PoAC DBS can occasionally achieve high ARI values, its performance is highly inconsistent, leading to a broad range of outcomes.\nAs shown in Figure 11, the optimization strategies that rely on a single CVI tend to have a higher concentration of lower ARI values when compared to the complete and the ablated PoAC. The green line, representing PoAC, has the steepest and most direct rise, indicating that this strategy achieves higher ARI scores more consistently. Most of the density for PoAC is concentrated above the 0.8 ARI mark, suggesting that it tends to produce highly accurate clustering outcomes. The orange line, representing PoAC CVI, also shows a steady rise, though it lags behind PoAC. This indicates that while PoAC CVI is generally effective, it produces slightly lower ARI scores on average compared to POAC.\nThe blue (PoAC SIL) and pink (PoAC DBS) lines display more gradual curves, indicating a broader range of ARI scores. PoAC SIL starts to rise steeply around the 0.2 ARI mark, while PoAC DBS has a more pronounced curve starting from 0.1. These curves suggest that these strategies have a more variable performance, with a significant proportion of their density at lower ARI scores, indicating that they are less likely to consistently produce high-quality clustering results.\nBoth of these analysis reveal that the complete PoAC method is the most effective and consistent optimization strategy based on ARI values, showing high clustering quality and minimal variability. PoAC SIL shows moderate performance with significant variability, while POAC DBS exhibits greater variability and less consistent performance. POAC CVI shows relatively consistent performance but is not as reliable or efficient as POAC. These findings highlight the importance of selecting the appropriate optimization strategies in relation to the specific clustering problem.\nTo understand the specific cases in which the optimization strategies can be at least as suited to perform as the complete PoAC method, we conducted further analysis to compare the performance of the four optimization strategies side by side, while identifying the features that define the archetype of the validation datasets, as shown in Figure 12. This analysis aimed to correlate the performance of the strategies with the dataset features described in Table 4, e.g. number of clusters, number of dimensions, and number of features. By visualizing the data in a heatmap, we were able to observe patterns and relationships that highlight how different optimization strategies perform across datasets with varying characteristics. Each cell in the heatmap represents the mean ARI for a given dataset-optimization strategy combination, with darker colors indicating higher ARI values.\nFor instance, the analysis revealed that strategies such as the PoAC method consistently performed well across datasets with a high number of dimensions, whereas POAC SIL showed variable performance that was influenced more by the number of clusters. This analysis underscores the importance of considering dataset-specific features when selecting an optimization strategy for clustering, as the effectiveness of each strategy can vary significantly depending on these characteristics. This nuanced understanding enables more informed decisions when applying clustering techniques to diverse datasets, ensuring better alignment between the chosen strategy and the inherent properties of the data.\nThe heatmap reveals that PoAC consistently achieves high ARI values across a wide range of datasets, indicating robust performance regardless of the dataset characteristics. This strategy particularly excels in datasets with high dimensionality, a larger number of clusters, and varying levels of imbalance. PoAC maintains superior clustering quality even in datasets with higher aspect ratios (elongated clusters), and varying ratios between the maximum and minimum cluster sizes.\nIn contrast, the PoAC Sil strategy shows moderate performance, with variability influenced by the number of clusters and dimensions. Notably, PoAC Sil performs better in datasets with fewer clusters and lower dimensions, as well as those with lower aspect ratios and more spherical clusters. However, its performance diminishes in datasets with high aspect ratios and greater elongation. That happens because silhouette suffers from increasing dimensions since it is based on euclidean distances. So it does not handle well large dimensionalities\nPOAC DBS demonstrates wide variability in performance, struggling particularly with higher dimensional datasets and those with an increased number of clusters. The inconsistent performance of POAC DBS is evident from the lighter color cells dispersed throughout the heatmap. PoAC DBS also performs poorly in datasets with high aspect ratios and significant elongation of clusters, as well as those with high radius values.\nLastly, POAC CVI, while generally outperforming POAC SIL and POAC DBS, still shows significant variability and is less reliable compared to PoAC. Its performance is more consistent in datasets with moderate to high dimensions but falls short in achieving high ARI values across all datasets. This strategy also exhibits variability in datasets with different levels of imbalance, aspect ratios, and radii, indicating a less robust performance in comparison to PoAC.\nIn conclusion, while sub-configurations of PoAC such as POAC SIL and POAC DBS are capable of achieving high values for their respective CVIs, it is important to balance them with a surrogate model. The data indicate that there are numerous instances where the highest SIL and DBS do not necessary correspond to the best partitioning results. For example, as shown in Table 7, POAC SIL achieves the highest mean SIL (0.75) and PoAC DBS achieves the lowest mean DBS (0.27), yet their mean ARI values (0.53 and 0.41, respectively) are significantly lower than that of the complete PoAC method (0.70). This discrepancy underscores the need for a balanced approach that incorporates surrogate modeling to better represent the true clustering quality according to a particular training dataset. Moreover, the results for POAC CVI, which operates without the meta-features group, further reinforce the necessity of meta-features to accurately represent the datasets and their optimal SIL and DBS values. The mean ARI for POAC CVI (0.57) is higher than that for PoAC SIL and POAC DBS but still lower than the complete PoAC method, demonstrating the critical role of meta-features in achieving superior clustering performance. Thus, the complete PoAC method, with its integration of surrogate modeling and meta-features, provides a more robust and effective approach to clustering optimization."}, {"title": "7 Limitations", "content": "The primary limitations of this study revolve around the selection of CVIs and the assembly of datasets used during the training phase. The selection of CVIs is critical to the quality of the clustering results. The effectiveness of CVIs directly impacts the ability to accurately describe how a partitioning should be performed. Therefore, the relevance of the chosen CVIs to the specific clustering problem at hand is paramount. Higher correlation between the CVIs and the characteristics of the clustering problem (such as visualization) will yield more meaningful and effective pipeline optimization outcomes"}]}