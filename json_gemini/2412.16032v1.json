{"title": "A Framework for Streaming Event-Log Prediction in Business Processes", "authors": ["Benedikt Bollig", "Matthias F\u00fcgger", "Thomas Nowak"], "abstract": "We present a Python-based framework for event-log prediction in streaming mode, enabling predictions while data is being generated by a business process. The framework allows for easy integration of streaming algorithms, including language models like n-grams and LSTMs, and for combining these predictors using ensemble methods. Using our framework, we conducted experiments on various well-known process-mining data sets and compared classical batch with streaming mode. Though, in batch mode, LSTMs generally achieve the best performance, there is often an n-gram whose accuracy comes very close. Combining basic models in ensemble methods can even outperform LSTMs. The value of basic models with respect to LSTMs becomes even more apparent in streaming mode, where LSTMs generally lack accuracy in the early stages of a prediction run, while basic methods make sensible predictions immediately.", "sections": [{"title": "1 Introduction", "content": "Digital data is omnipresent and continuously produced in today's world. A particular type of data that is available to many organizations and institutions, either through explicit mining or implicitly within business processes, are event logs, i.e., timed traces of events recorded along the execution of a process. Examples are examinations of patients entering a hospital, server log files, and clicks of a user browsing a website.\nWhile such event logs are often readily available at organizations, process models are typically absent. More so, quantitative models that allow one to describe, analyze, optimize, and predict possible future events are challenging to obtain."}, {"title": "2 Language Models and Ensemble Learning", "content": "We start with a unified, automata-based view of various base language models, including frequency prefix trees [12], n-grams [37], and probabilistic deterministic finite automata [54]. This not only allows one to compare the above approaches within a single setting but has also been adapted within our Python framework, where concrete language models are obtained by refining basic automata provided by the framework."}, {"title": "2.1 Base Language Models", "content": "We fix a nonempty finite set \u2211 of activities, also referred to as activities. The set is not necessarily known in advance to a language model, though an algorithm (e.g., a neural network) may require specifying an upper bound on its size (as part of the embedding dimension in case of the neural network). We write * for the set of finite sequences of activities. In particular, the set includes the empty sequence \u025b. Activities are referred to as a, b, etc. We denote by stop a symbol that is not contained in \u2211 and that is used by a language model to signal the end of a sequence. The set Estop = \u03a3\u222a {stop} contains all activities and the stop symbol. In the following, let \u03c3,\u03c4 be from Estop. Activities are associated to cases. We assume that case IDs are taken from a countably infinite set CaseID. Specifically, in our benchmarks, case IDs will be from the set of strings or the set of natural numbers.\nA language model can take several forms. Examples are models based on n-grams, bags, probabilistic automata, and recurrent neural networks. In essence, every such language model defines a probabilistic prediction function or, simply, prediction function p: \u03a3* \u2192 \u0394(stop), where (stop) is the set of probability distributions over Estop. Given a sequence w \u2208 \u03a3*, applying p yields a probability distribution over the set of possible next activities, including stop. In analogy to conditional probabilities, one usually writes p(\u03c3 | w) for the probability p(w)(\u03c3).\nThe focus in this work is on predicting the most likely next activity, which is readily obtained from a probabilistic prediction function. Note that, under certain conditions, the prediction function also allows one to generate sequences according to a probability distribution over \u03a3*.\nAs previously stated, we follow an automata-based approach to obtain pre-diction functions. This is motivated by two reasons: (i) It unifies widely-used approaches, and (ii) its formalism is close to streaming learning with incremental state updates, allowing us to compare batch and streaming mode within a single framework. Given that state updates due to learning will be deterministic, we will use a deterministic automaton to describe these. Following [28], we define:\nDefinition 1. A probabilistic deterministic finite automaton (PDFA) is com-posed of a finite set S of states with a dedicated initial state so \u2208 S, a partial deterministic transition function \u03b4: S \u00d7 \u03a3 \u2192 S (also called update function), and a mapping \u03c0: S \u2192 \u0394(Estop) assigning a probability distribution to every state."}, {"title": "2.2 Batch Learning", "content": "We next define the prediction problem for batch learning and then adapt the solutions to the streaming case.\nIn batch learning, we are given an event log L \u2286 (CaseID \u00d7 \u2211)*, which is a finite set of finite sequences of case ID-activity pairs. The goal is to train a language model on L, i.e., to construct a prediction function p: \u03a3* \u2192 \u0394(stop) that generalizes well and enables informed predictions based on event data not necessarily seen before. Recall that, in our setting, a prediction for a given case ID only depends on the projection of the event log to this case. Consequently, it is sufficient"}, {"title": "2.4 Ensemble Methods", "content": "Ensemble methods [56] combine multiple base models, such as FPTs or n-grams, into more complex models using voting mechanisms. Voting mechanisms are distinguished based on how they handle updates and inference queries. In both soft voting and hard voting, each base model individually processes updates from incoming events. Similarly, queries are forwarded to all base models, each of which produces a prediction in the form of a probability distribution. The key difference lies in how these distributions are aggregated: In soft voting, the final probability distribution is obtained by averaging all individual distributions, while in hard voting (also referred to as majority voting), each probability distribution is converted into a concrete activity, and the activity chosen most frequently constitutes the final verdict (which may be represented as a Dirac probability distribution).\nWhile hard and soft voting apply to both batch and streaming modes, adaptive voting is specific to streaming mode. The idea is to track the current accuracy of each base model. A query is then forwarded to the model with the highest accuracy. An update on an incoming event involves two steps. In step 1, the accuracy is adjusted: the activity a of the event is compared to the prediction of each base model. Base models that correctly predict a see their accuracy increase, while the accuracy of all other models decreases. Step 2 updates the base models in the same manner as in soft and hard voting.\nFinally, we use ensemble methods as a fallback method to compensate for slowly-learning language models in streaming learning. For example, we apply the FPT with a minimum number of 10 visits\u00b3 whenever possible. If the minimum number has not been reached, as a fallback option, the prediction of an n-gram is returned, where, for our experiments, we use n = 5 or n = 7 depending on the dataset."}, {"title": "3 Computational Framework", "content": "A wide range of stream-processing frameworks is available, with examples being Dataflow [11], Faust [16], Flink [17], Pathway [31], as well as River [29]. Towards the goal of a combined batch and stream processing with easy general-purpose function modules, local deployment, analysis via dashboards, low overhead and latencies, and stop-resume functionalities, we developed the logicsponge Python library [24].\nA data pipeline in logicsponge is made up of terms that are connected via sequential or parallel composition. We distinguish two basic types of terms: source terms and function terms. The output of a source term does not explicitly depend any other terms in the pipeline. On the other hand, a function term receives data from other terms as input and can produce output to other downstream terms. A function term that does not produce any outputs for other terms is a sink. It may, of course, produce outputs not modeled in the pipeline, like appending data to files or user console output. In the sequential composition t1 t2 of terms t1 and t2, the input of t2 is equal to the output of t1. In their parallel composition t1 | t2, both t1 and t2 receive the same upstream input and its downstream output is the union of outputs of t1 and t2, accessible as separate streams. Function terms that merge these streams into single streams are provided by the framework. Independently of where they appear in the pipeline, all terms are executed in separate threads.\nInputs and outputs of terms are handled via data streams. A data stream encapsulates a list of data items. A data item is a Python dictionary with some associated metadata. Each data stream is owned by a unique term to which it is associated as an output. The owner of a data stream can append data items to it. A data stream can be the input to other terms as defined by the sequential and parallel composition. Terms that have a data stream as an input do not have write access to it and can access it only via a read-only data stream view object. The underlying list data structure of a data stream is protected from concurrent access via readers-writer locks [34], minimizing conflicts. When starting a pipeline, setting an optional persistent function argument configures the contents of the data streams to be stored in a Python object database [57]. Restarting the pipeline after an interrupt or crash restores the state from the database and resumes computation. The library contains predefined terms to generate plots, compute basic statistics, and create interactive web dashboards.\nWe implemented the process-mining algorithms used for this work as function terms in the logicsponge-processmining package [25]. We first define a list of language models:"}, {"title": "4 Experimental Evaluation", "content": "To evaluate our framework and approach, we performed experiments based on seven well-known real-world business-process datasets."}, {"title": "4.2 Streaming Mode", "content": "In streaming mode, we did not add stop-symbols to mark the end of a sequence, as adding them to pre-mortem data is unsuitable unless explicitly present in the dataset. On the other hand, to handle the first event of a case in a data stream, we insert an \"init\" symbol to mark the start of a sequence allowing an LSTM to make a prediction from the very beginning.\nFor querying an LSTM, we parse the current sequence through the model and retrieve the corresponding outcome. LSTMs are not inherently designed for streaming learning. We update a model efficiently with each incoming event as follows: After each new event, we perform a training iteration. Each training step uses a batch size of 8, always including the updated case sequence. The remaining sequences are selected in a round-robin fashion to ensure all sequences are considered repeatedly.\nNote that, unlike in batch learning, there is no separation of the dataset in training, validation, and test set."}, {"title": "4.3 Results", "content": "Experimental results are depicted in Table 2. In batch mode, LSTMs perform best across most datasets. However, in most cases, an n-gram model comes very close. Combining these models, along with frequency prefix trees and bags, into ensemble methods further boosts the performance of simple base models: in two cases, LSTMs are even outperformed (by soft voting).\nIn streaming mode, LSTMs appear to struggle during the initial stage of a stream. As illustrated in Figure 2a for the sepsis dataset, which depicts the accuracy evolution for various models, LSTM performance is initially well below that of other base models, catching up only later (Figure 2b). Ensemble methods can outperform LSTMs on several datasets. Notably, the average latency (the total time required for making one prediction and one update) of base models is significantly lower that of LSTMs."}, {"title": "5 Conclusion and Future Work", "content": "We presented a computational framework for event-log predictions in streaming mode. Experimental results on business processes carried out in this framework suggest that process data often exhibits behavior that is consistent with computationally efficient n-gram predictions. Moreover, ensemble methods considerably boost smaller base models, at the expense of only little computational overhead.\nOne direction for future research is refining the methodology. Fallback algorithms show promising potential, but more detailed experiments are required to explore more sophisticated fallback strategies that rely, for example, on statistical tests. A potential starting point is the work in automata learning [12]) to compare states as well as the work on probabilistic bisimilarity distances [41]. It would be interesting to investigate whether ensemble methods involving nested"}]}