{"title": "Social Science Is Necessary for Operationalizing\nSocially Responsible Foundation Models", "authors": ["Adam Davies", "Elisa Nguyen", "Michael Simeone", "Erik Johnston", "Martin Gubri"], "abstract": "With the rise of foundation models, there is growing concern about their potential\nsocial impacts. Social science has a long history of studying the social impacts\nof transformative technologies in terms of pre-existing systems of power and how\nthese systems are disrupted or reinforced by new technologies. In this position\npaper, we build on prior work studying the social impacts of earlier technologies\nto propose a conceptual framework studying foundation models as sociotechnical\nsystems, incorporating social science expertise to better understand how these\nmodels affect systems of power, anticipate the impacts of deploying these models in\nvarious applications, and study the effectiveness of technical interventions intended\nto mitigate social harms. We advocate for an interdisciplinary and collaborative\nresearch paradigm between AI and social science across all stages of foundation\nmodel research and development to promote socially responsible research practices\nand use cases, and outline several strategies to facilitate such research.", "sections": [{"title": "Introduction", "content": "While the rapid recent development of generative foundation models is exciting for many potential\napplications (see, e.g., [1, 2, 3, 4], etc.), important social impacts come along with rapid adoption,\nincluding worker displacement [5, 6, 7], use of copyrighted data for training models [8, 9, 10, 11],\nenergy requirements and associated climate impact [12, 13], and data privacy [8, 14, 15, 16]. \u03a4\u03bf\ndevelop socially responsible foundation models, we argue for proactive consideration of such concerns\nacross the whole research and development (R&D) lifecycle from ideation to retirement of the\ntechnology. Anticipating social concerns can enable early discovery of unintended problems in the\npipeline (e.g., biased data collection [17]) and inform interventions to mitigate undesired impacts\n[18, 19, 20].\nUnderstanding and considering social impacts in the research and development of AI technology\nrequires knowledge and experience studying complex social systems and interactions \u2013 i.e., expertise\nin social science. However, the domains of AI and social science research are largely siloed [21, 22,\n23], manifesting in differences in vocabluary [24], publishing venues, and publishing practices (e.g.,\nthe prestige of journals vs. conferences). Often, simplifying assumptions are made in AI research\nabout social structures which may not hold in real life [23] (e.g., crowdsourcing annotators to align\nLLMs with so-called \u201chuman values\" via reinforcement learning from human feedback [25], despite\nthe fact that such \u201cvalues\u201d are unique to the individual and may vary widely across cultures). To\nfacilitate more socially responsible research, we advocate for an interdisciplinary paradigm integrating"}, {"title": "Background", "content": "Social Systems of Power In one relevant but expansive intellectual tradition of social science,\nsystems of power \u2013 the structures and institutions that shape, maintain, and distribute power within\na society\nare researched and described using theoretical approaches such as post-structuralism,\ninstitutional analysis, network analysis, and decolonial theory. Scholars explore how institutions\n(such as governments, corporations, and social norms) distribute power and privilege, shaping\nsocial outcomes like inequality and marginalization. Many approaches starting toward the end of\nthe 20th century emphasize the interconnectedness of race, class, gender, and other identities in\nunderstanding power dynamics [26, 27]. Such systems of power are often reproduced by socially\ndisruptive technologies, such as social media platforms, recommendation algorithms, and search\nengines, which can amplify existing biases by encoding them into technological systems [28, 29, 30].\nTechnological Affordances In an oft-cited example of how racial systems of power can be codified\nin technology, [29] examines how Google Search in 2011-2013 reinforced longstanding harmful\nmisrepresentations of Black women in, e.g., racist and sexist stereotypes that appeared in top\nautocompletions beginning with \"why are black women so\" versus \u201cwhy are white women so", "black girls": "We may understand such instances through the lens of technological\naffordances - i.e., the technology-mediated actions that are enabled, encouraged, or constrained by\na technology with respect to an environment [31] \u2013 in the specific context of information seeking\n[32, 33], where the actions taken by web users (e.g., selecting an autocompletion or following a\nsearch result) are influenced by technologies that can implicitly reproduce existing systems of power\n(such as harmful stereotypes or sexual objectification), reciprocally shaping the digital information\nenvironment by driving search traffic and influencing users' beliefs to reinforce the social harms and\ninequities embedded in the technology [34]. In this work, we consider the technological affordances\nof foundation models, and the importance of social science for understanding how these affordances\ncan reproduce or reshape existing systems of power.\nSocial Media and Teen Mental Health Before discussing foundation models, we first consider a\nmore established technology where failing to take findings from social science and psychology into\naccount has led to serious real-world harms: social media use (SMU) among teens, and its impact\non their mental health. Many studies have found a strong correlation between SMU and diagnoses\nof mood and body-image disorders [35, 36, 37, 38]; and while it is difficult to establish a direct\ncausal relationship, the limited causal evidence available suggests that SMU is indeed an important\ncontributor to these negative impacts [38, 39]. One possible solution that has been proposed to help\nmitigate such harms is to redesign content recommendation feeds to de-prioritize engagement metrics,\nas there is clear evidence that recommender systems optimized for user engagement suggest harmful\ncontent at a far higher rate than systems that do not [40]. For instace, despite early internal user\nstudies conducted at Facebook and Instagram finding that simple adjustments to engagement-based\nalgorithmic design choices could indeed mitigate negative impacts on teen mental health [41, 42], the\nteams conducting this research were shuttered and the corresponding changes were never adopted\nat scale because they also led to lower advertising revenues [42, 43, 44, 37]. That is, an internal,\ninterdisciplinary research team of social scientists and machine learning experts had already partially\nresolved the question of how to re-design recommendation algorithms in order to mitigate real-world\nharms, illustrating the importance of both doing such research and acting on it."}, {"title": "Operationalizing Socially Responsible Foundation Models", "content": "To provide and deploy foundation models in a socially-responsible manner, we argue that it is\nnecessary to involve social science expertise throughout the foundation model R&D process. In\nparticular, we propose a conceptual framework to decompose this task into three key components:\n1.  Understanding systems of power: How do disruptive technologies like foundation models\nreproduce or reshape existing systems of power? What affordances would best promote\ndesirable effects on these systems?\n2.  Designing technical interventions: How can the foundation model R&D pipeline be\nmodified in order to align models with target affordances?\n3.  Anticipating social impacts: What social impacts may result from deploying a model with\nthe target affordances in a specific context?\nWhile AI researchers are well-positioned to study technical interventions, this is an entirely different\nquestion from understanding systems of power or anticipating social impacts, which are better\nsuited to social scientists. However, there is still a role in each component for AI research, as it is\nnonetheless important to provide robust, quantifiable, and computationally tractable definitions of\ndesired foundation model affordances (e.g., it is necessary to specify affordances in terms that can\nbe learned by models in encouraging socially representative model outputs, prohibiting the use of\nmodels for generating toxic content, etc.), as well as to carry out systematic empirical evaluation of\ncorresponding model behaviors in order to predict alignment with the intended affordances, which\nare both tasks where AI expertise is essential. As such, interdisciplinary collaboration between AI\nand social science is required to address the challenges associated with each of these components.\nSpecifically, we argue that it is critical to involve social science in foundation model research,\ndevelopment, and deployment in order to (1) proactively consider interactions between foundation\nmodel affordances and sociotechnical systems of power, and (2) anticipate the impacts associated\nwith deploying these models in a given context, as explored below.\nModel providers must take systems of power into account. Sociotechnical systems of powers\nshape web-scraped data used to train foundation models in subtle, complex, and systematic ways that\nsocial science can identify. For instance, Wikipedia, an essential component of nearly all LLM training\ndatasets [1, 46, 47, 48], underrepresents women and non-binary figures [49, 50, 51, 52, 53] \u2013 e.g.,\nonly 19% of biographies are about women [52]. This Wikipedia gender gap is well studied in social"}, {"title": "Facilitating Socially Responsible Foundation Model Research", "content": "What can be done to facilitate the kind of interdisciplinary AI + social science collaborations for\nwhich we advocate in this work? The following is a preliminary list of suggestions for attenuating the\ncost of interdisciplinary collaboration, though it is not intended to be exhaustive:\n5\n\u2022 The evaluation criteria for social scientists' careers may place less value on (1) AI-area or\ninterdisciplinary AI + social science venues, or (2) conference publications compared to\njournal publications, which could discourage them from publishing in top-tier AI venues;\nand vice-versa for AI researchers. Like the FAccT conference,4 more AI conferences could\noffer the optional choice of non-archival paper submissions (in addition to the standard\narchival submission), allowing researchers from other fields to later submit their conference\npapers to discipline-specific journals.\n\u2022 To incentivize research on interdisciplinary AI + social science topics, institutions could\nbetter consider interdisciplinary work in career advancement [77] and funding proposals\nassessment [78], and offer specialized funding opportunities and sabbaticals, allowing\nresearchers to explore new ideas and collaborations in a wider context [79].\n\u2022 Promoting interdisciplinary education can better prepare the next generation of researchers\n\u2013 e.g., awarding degree credit for courses taken in other fields encourages students to learn\nthe essentials of other disciplines [80], providing them with the necessary foundations to\nintegrate methods from, and facilitate collaborate with, fields beyond their primary research\narea.\n\u2022 Researchers who wish to embrace a more interdisciplinary agenda could broaden their ex-\npertise with workshops, tutorials, or short courses provided by researchers from other fields.\nFor example, we recommend AI conferences to open tutorial calls to non-AI researchers;\nand likewise for social science venues."}, {"title": "Conclusion", "content": "In this work, we have advocated for interdisciplinary research between AI and social science in\nthe context of foundation models like LLMs, focusing on the importance of social science in\nunderstanding the affordances and social impacts of such transformative technologies. We outlined\nthe importance of interdisciplinary expertise and collaboration throughout the foundation model R&D\npipeline, highlighted the associated responsibilities and benefits for model providers and deployers,\nand provided actionable suggestions to promote collaboration between AI and social science.\nTakeaways We recommend that AI experts and labs researching, developing, or deploying foun-\ndation models reflect on incorporating interdisciplinary collaboration within their team and their\nresearch topic more broadly, particularly in promoting socially responsible affordances and studying\npotential social impacts of their work. Neither AI nor social science holds all the answers regarding\nhow to develop safe, beneficial, and socially responsible foundation models; and it is critical that\nboth disciplines work more closely together toward this goal, rather than \"siloing\" research for such a\npotentially transformative technology.\nLimitations and Future Work The primary limitation of this work is that interdisciplinary research\ncan be expensive and time-consuming [77], and bringing in diverse perspectives always carries\nthe potential to dilute research focus with competing visions and priorities [81]. In Section 4, we\noutline a few tentative suggestions to reduce this cost; but they are by no means comprehensive. We\nrecommend that future work consider performing more comprehensive cost-benefit analyses along"}]}