{"title": "Enhancing Agent Learning through World Dynamics Modeling", "authors": ["Zhiyuan Sun", "Haochen Shi", "Marc-Alexandre C\u00f4t\u00e9", "Glen Berseth", "Xingdi Yuan", "Bang Liu"], "abstract": "While large language models (LLMs) have been increasingly deployed across tasks in language understanding and interactive decision-making, their impressive performance is largely due to the comprehensive and in-depth domain knowledge embedded within them. However, the extent of this knowledge can vary across different domains. Existing methods often assume that LLMs already possess such comprehensive and in-depth knowledge of their environment, overlooking potential gaps in their understanding of actual world dynamics. To address this gap, we introduce Discover, Verify, and Evolve (DiVE), a framework that discovers world dynamics from a small number of demonstrations, verifies the correctness of these dynamics, and evolves new, advanced dynamics tailored to the current situation. Through extensive evaluations, we analyze the impact of each component on performance and compare the automatically generated dynamics from DiVE with human-annotated world dynamics. Our results demonstrate that LLMs guided by DiVE can make better decisions, achieving rewards comparable to human players in the Crafter environment.", "sections": [{"title": "Introduction", "content": "By absorbing internet-scale knowledge autoregressively, Large Language Models (LLMs) develop a nuanced understanding of world dynamics (Achiam et al., 2023; Team et al., 2023; Brown et al., 2020; Touvron et al., 2023). This understanding enables them to perform well on tasks like question answering (Yang et al., 2018; Clark et al., 2019; Lin et al., 2021), planning (Song et al., 2023; Brown et al., 2020; Huang et al., 2022), and reasoning over commonsense knowledge (Yao et al., 2022, 2024; Besta et al., 2024). However, such understanding requires the training dataset to be diverse, precise, and in-depth enough to cover the domain information needed; otherwise, this may result in a knowledge gap between LLMs and the target domain, as illustrated in Figure 1.\nAs LLMs tend to follow the most common patterns within the dataset (Gunasekar et al., 2023), this mandates that the necessary domain-specific information not only appears on the internet but also appears frequently. For downstream tasks that are either newly established or less popular, such a requirement is often infeasible and leads to suboptimal decision-making for LLMs.\nTo make optimal decisions, LLMs require high-quality, domain-specific information, which is often impractical to source and verify within the vast expanse of internet data (Dziri et al., 2022a). Evaluating such information necessitates locating it within the dataset and verifying it against ground-truth domain knowledge, a process that is both labor-intensive and error-prone. Consequently, ensuring LLMs possess the necessary comprehensive and accurate knowledge for decision-making remains a formidable challenge.\nEven if LLMs have a general understanding of a downstream domain, achieving optimal decision-making requires in-depth knowledge specific to each state. By providing LLMs with such tailored and in-depth information, their understanding of both the environment and the current state can be further improved. For example, mastering the game of Go involves not only understanding the basic rules but also strategies tailored to specific board states. Moreover, this specialized knowledge can vary not only across domains but also within different states of the same domain, making it impractical to collect in an offline manner.\nTo address these challenges, we propose Discover, Verify, and Evolve (DiVE). Leveraging the concept of World Model (Ha and Schmidhuber, 2018; Hafner et al., 2020, 2023), DiVE not only discovers and verifies the primitive world dynamics from the demonstrations but also evolves state-specific knowledge for the downstream domain. By providing LLMs with all these dynamics and knowledge, DiVE can bridge the understanding gap between LLMs and the downstream domain, thereby enhancing LLMs' decision-making abilities.\nDiVE comprises three distinct components: i) The Discoverer: This component iteratively uncovers the dynamics of the environment based on the provided demonstrations. It begins by discovering the primitive dynamics of actions and objects, progresses to subtask steps, and finally determines the topological order of subgoals. This curriculum learning method ensures both learning efficiency and a comprehensive understanding of the environment's dynamics. ii) The Verifier: This component filters out inaccuracies caused by the tendency of LLMs to hallucinate, ensuring that only precise and reliable information is retained. This step is crucial for maintaining the integrity of the knowledge base. iii) The Evolver: This component reasons in-depth, state-appropriate dynamics, such as strategic plays or contextual decisions. It ensures that the agent is equipped with actionable knowledge tailored to the current state, enhancing real-time decision-making.\nWithin the Crafter environment (Hafner, 2021), DiVE learns comprehensive and accurate dynamics from demonstrations and guides the agent through the decision-making process by developing in-depth knowledge. This guidance enables the agent to outperform strong baselines, including the best reported systems, and achieve rewards comparable to those of human players. We also provide both quantitative and qualitative analyses that help understanding DiVE's behavior, demonstrating the effectiveness of the proposed framework.\nIn summary, our primary contribution is a general framework that autonomously learns world dynamics from demonstrations and guides LLMs in decision-making by evolving contextual knowledge. This approach not only enhances the capabilities of LLMs in specific domains but also provides a versatile foundation for creating agents capable of mastering complex tasks across various environments through continuous learning and refinement."}, {"title": "Problem Formulation", "content": "Environment with Hierarchical Achievements\nEach environment with hierarchical achievements, $C_i \\in C$, operates interactively. At every time step $t$, an agent observes $o_t \\in O$ and is expected to respond by selecting an action $a_t \\in A$. Subsequently, the environment returns a new observation $o_{t+1}$ of the resulting state. Within each $C_i$, the achievement graph $G = (V, E)$ is structured as a directed acyclic graph (DAG). In this graph, each vertex $v \\in V$ denotes an achievement and each edge $(u, v) \\in E$ indicates that the achievement $v$ has a dependency on the achievement $u$. To unlock achievement $v$, one must first unlock all its prerequisite ancestors. The main objective is to maximize the number of achievements unlocked by satisfying these dependency conditions.\nThe Crafter Environment We primarily employ the Crafter environment as a benchmark to evaluate an agent's performance in a setting characterized by hierarchical achievements. The Crafter environment (Hafner, 2021) is a long-horizon environment typically necessitating hundreds of steps to successfully collect a diamond. Within this environment, the agent must not only determine the topological order of the technology tree as illustrated in Appendix E, but also survive by overcoming dangerous creatures and managing diminishing resources.\nChallenge: The Knowledge Gap In this work, we assume a setting where a pre-trained LLM-based agent is used to solve tasks in the Crafter environment. Conceptually, we use $K_{data}$ to denote the set of knowledge being embedded in the LLM through its training process (including both pre-training and fine-tuning). We use $K_{target}$ to denote the universal set of knowledge required to solve the target domain (i.e., Crafter). For the LLM to be useful, we hope $K_{rel}$, the subset of knowledge in $K_{data}$ that is relevant to $K_{target}$ to be as large as possible. We also hope that $K_{rel}$ contains more accurate knowledge $K_+$ than inaccurate knowledge $K_-$, where $K_{rel} = K_+ \\cup K_\u2212$. Therefore, we define"}, {"title": "Method", "content": "In an ideal world, one could alleviate the aforementioned knowledge gap by fine-tuning the LLM to adapt to the target domain. However, this approach often show less practical due to its reliance on abundant annotated data and significant computational overhead (Hu et al., 2021; Zheng et al., 2024; Carta et al., 2023; Ouyang et al., 2022). Our framework, DiVE, is designed to bridge the knowledge gap taking into consideration all the three desirable properties (i.e., recall, precision, and depth), but without the need of collecting extensive data from the target domain. It is a prompt-based method that learns world dynamics $W$ from the environment.\nAs illustrated in Figure 2, DiVE is initially bootstrapped from a handful of human demonstration trajectories $H = {T_i} = {{(o_t, a_t, r_t)}^N_{t=0}}_{i=0}^{N_o}$. We verbalize an observation $o_t$ into the language space as $\\tilde{o_t}$ using a Verbalizer (i.e., $\\tilde{o_t}$ = Verbalizer($o_t$)), resulting in transformed trajectories $T_i = {(\\tilde{o_t}, a_t, r_t)}_i$. Next, the Discoverer distills a set of world dynamic candidates $W = {W_+, W_\u2212}$ from human demonstrations $H$, where $W_+$ and $W_\u2212$ represent the correct and inaccurate world dynamic sets, respectively.\nEmpirically, we find that it is often inevitable to have $W_\u2212$ in $W$, either due to the backbone LLM's imperfection in discovering meaningful knowledge from trajectory data, or its tendency of hallucination. Therefore, we use a Verifier to filter out potential invalid and conflicting world dynamic candidates within $W$, we denote the remaining valid dynamics as $\\hat{W}$. Next, we use an Evolver, which is"}, {"title": "Offline Dynamics Learning", "content": "designed to derive advanced game-playing strategies $I$ based on the world dynamics $\\hat{W}$ and the language description of observation $\\tilde{o_t}$.\nThe final decision-making process on primitive actions $a_t \\in A$ is hierarchically decomposed as planning tasks on sub-goals $SG$, sub-tasks $ST$, and actions $A$. The planning procedure is further guided by both $\\hat{W}$ and $I$. In cases where $\\hat{W} \\neq \\emptyset$, $R$, $P$, and $D$ are guaranteed to increase as formulated below:\n$R: \\frac{|K_+|\\text{Discoverer}}{K_{target}|H} + |W|$ (1)\n$P: \\frac{|K_+| \\text{Verifier}}{K_{rel}W, H}+ |W|}+\n|W|\\hspace{0.5cm}$(2)\n$D:\u00d8\nVerifier\\text{Evolver}\\text{Discoverer}I\\cup W\\hat{W}$\n(3)\nThe DiVE framework can be split into two stages: an offline dynamics learning stage and an online strategy learning stage.\nOffline Dynamics Learning\nThe offline dynamics learning procedure aims at bridging the gap between LLM's understanding and the basic rules of the Crafter world by learning the world dynamics $W$ as a prior for further decision-making. Rather than extracting world dynamics from human-authored game manuals or handbooks (Wu et al., 2024b,a), which can be difficult to obtain in many real-world scenarios, we explore learning world dynamics $W$ from experiences $H$, which is arguably more accessible.\nHierarchical Curriculum Learning Given the varying complexities in learning the dynamics of different elements in Crafter, we adopt curriculum learning (Bengio et al., 2009). We propose a sequential learning approach progressing from simple to more challenging dynamics, facilitating effective learning. Specifically, we propose a method to learn the dynamics of each element within the task decomposition hierarchy, denoted as $TD = {A \\cup (O), ST, SG}$, where $O$ is the set of objects in Crafter. Our approach starts with elements of lower abstraction, such as actions $a \\in A$, and progresses to higher abstraction elements, like sub-goals $sg_i \\in SG$. The sub-goals sequence $SG = [sg_1, sg_2,...]$ is an ordered sequence to unlock achievements in the achievement graph $G = (V, E)$, with $SG = TopologicalSort(G)$ and each $sg_i$ being a vertex in $V$. We use the Discoverer to extract this order from human demonstrations $H$. The sub-tasks $ST$ are defined as the nodes in the achievement graph G, i.e., $ST = V$. Achieving a sub-goal $sg_i$ may require completing several sub-tasks multiple times. This approach ensures a logical progression through the tasks, enhancing the learning and application of Crafter's dynamics.\nDiscoverer The Discoverer is designed to identify dynamic candidates W relating to the elements within the task decomposition hierarchy TD. A single dynamics discovering step for an element $E \\in TD$ involves following three main steps:\nConstruction of a Semantic Experience Bank ($B^E$): For each element E, a semantic experience bank $B^E$ is constructed from human demonstrations H. This bank stores experiences that are transformed from H into a suitable granularity for analyzing dynamics related to E. The transformation involves chunking and summarizing the verbalized human demonstrations H to capture essential semantic details.\nSampling of Relevant Experiences: For each attribute of an instance $e \\in E$, a subset of experiences $B^E$ that are relevant to the instance e is sampled from $B^E$.\nIdentification of Dynamic Candidates: A dynamic candidate $w$ is identified from the subset $B^E$ by recognizing patterns that are consistent across all experiences within $B^E$.\nThe action-level semantic experience bank, denoted as $B^A$, stores transition tuples derived from verbalized human demonstrations $\\hat{H}$, specifically $B^A = {\\{ (\\tilde{o_t}, a_t, \\tilde{o_{t+1}}) \\}^B_{i=1} }_i$. Similarly, the object-level semantic experience bank $B^O$ collects individual observations related to specific object interactions from $\\hat{H}$, stored as $B^O = {\\{\\tilde{o_i}\\}}_{i=1}^{\\tilde{B}^O_i}$.\nThe sub-task-level semantic experience bank $B^{ST}$ aggregates trajectory segments that represent the completion of sub-tasks, formatted as $B^{ST} = {\\{\\tilde{o_t}, ..., a_{tst}, \\tilde{o_{tst+1}} \\}}_i$, where $tst$ denotes the time step at which a sub-task $st \\in ST$ is completed. For sub-goals, the sub-goal-level experience bank $B^{SG}$ records sequences of sub-tasks that lead to the completion of sub-goals, expressed as $B^{SG} = {\\{ (st_t, ..., st_{tsg}) \\}}_i$, where $tsg$ is the time step at which the sub-goal $sg$ is achieved.\nFor action-level dynamics learning, the relevant experiences $B^A$ are compiled by randomly sampling transition tuples from $B^A$ where the action"}, {"title": "Online Strategy Learning", "content": "a is successfully executed. A similar approach is applied to the dynamic learning for other elements within the task decomposition hierarchy TD.\nFor action-level dynamics discovery, we identify the prerequisites and outcomes of each action (e.g., The action MakeWoodPickaxe requires 1 wood). For object-level dynamics, we concentrate on the co-occurrence relationships between objects and their occurrences over time. The attribute set for a sub-task typically includes the general steps required to complete the task, along with its prerequisites, outcomes, and termination conditions. The only attribute of interest for a sub-goal is its correct position within the sub-goal sequence SG.\nVerifier Dynamic discovery processes are susceptible to noise arising from several sources, such as confounders, hallucinations by the LLM, and imperfections in the LLM's ability to derive meaningful knowledge from trajectory data. To mitigate these issues, we introduce a dynamic verifier designed to filter out all noisy dynamic candidates from W. For each dynamic candidate $w \\in W$, the Verifier initiates the process by sampling a subset of relevant semantic experiences $B^E$ from the corresponding semantic experience bank $B^E$. In this context, $w$ represents a dynamic candidate related to one attribute of the instance $e \\in E$, where $E \\in {A \\cup (O), ST, SG}$ is an element of the task decomposition hierarchy TD. The verification of w proceeds as follows: w is identified as inaccurate and consequently filtered out if it does not consistently hold across all experiences within $B^e$ or if it conflicts with any other established dynamics. The dynamics that withstand this verification process are classified as verified dynamics, denoted as $\\hat{W}$.\nOnline Strategy Learning\nTo effectively integrate the learned world dynamics $\\hat{W}$ into the Crafter environments, we deploy an LLM-based agent defined by $\\pi : S \\times \\hat{W} \\rightarrow P(A)$. Here, S represents the state space, A denotes the action space, and P symbolizes the probability distribution over the action space. Rather than directly mapping the world dynamics $\\hat{W}$ and the observation $o_t$ of the current state $s_t$ to the action $a_t$, we address the challenge of long-horizon planning by enhancing decision-making with an online strategy learning method. This approach decomposes the planning process into three distinct tasks: sub-goal planning, sub-task planning, and action planning.\nSub-goal Planning Given that the sub-goal sequence $SG = [sg_1, sg_2, ...]$ is derived from human demonstrations H and treated as a fixed sequence, we employ a simple heuristic for sub-goal planning. When a sub-goal is completed, the current sub-goal will be updated as the first uncompleted sub-goal within SG.\nSub-task Planning For a given current sub-goal $sg_i$, we have developed an LLM-based sub-task planner. This planner evaluates and ranks all sub-tasks $st \\in ST$ based on the learned world dynamics $\\hat{W}$, the verbalized current observation $\\tilde{o_t}$, and the most recently planned sub-task $st_{t\u22121}$. The top-ranked sub-task is then set as the current sub-task $st_t$. To ensure precise execution, the completion of a sub-task $st$ is contingent upon meeting its specific termination condition. This condition is assessed by querying an LLM with the current verbalized observation, the verbalized observation from the time step when the sub-task began, and the termination conditions of the current sub-task.\nOnline Strategy Deriving In addition to learning the basic rules of the Crafter environment (i.e., world dynamics $\\hat{W}$), we also explore the development of advanced game-playing strategies based on these dynamics. Unlike learning the world dynamics, the strategy space is often too vast to fully explore. To address this, we propose evolving the dynamics into strategies I using an online learning method. This method reduces the search space by conditioning not only on the dynamics $\\hat{W}$ but also on the verbalized current observation $\\tilde{o_t}$ and the sub-task $st_t$. This targeted approach helps generate strategies that are more grounded and responsive to current game scenarios than those developed through offline methods. To facilitate this, we have designed an LLM-based Evolver that develops strategy candidates I through deductive reasoning applied to $\\hat{W}$. Specifically, the Evolver derives strategy candidates by rules of inference, such as modus ponens. These strategy candidates I are evaluated for their validity and are ranked based on their usefulness by an LLM-based critic. Lastly, the valid and useful candidates are added to the situational strategy set I.\nAction Planning The final action selection process is carried out in two primary steps:\nInvalid Action Masking: This step involves masking all actions that are not feasible under"}, {"title": "Experiment", "content": "In this section, we start by detailing our experimental settings including evaluation metrics and the set of baselines we are comparing against (Section 4.1). Then, we report the main results (Section 4.2). Additionally, we conduct a set of controlled experiments, we provide quantitative and qualitative analyses to obtain better understanding on DiVE (Section 4.3 and 4.4).\nEnvironment Settings\nCrafter (Hafner, 2021) is a set of open-world survival games on 64 \u00d7 64 grid maps, featuring a diverse array of materials such as tree, stone, and coal, as well as entities including cow, zombie, and skeleton semi-randomly spawn on the maps. The games include an achievement graph with 22 unique achievements across 7 levels. The agent observes its surroundings through a local 7 \u00d7 9 window and is also aware of its status within the game environment.\nVerbalizer The text description from the verbalizer includes: the nearest object of each type within the accumulated observations, the objects located between these nearest objects, the objects in each direction, and the agent's current inventory and status. We provide an example of the verbalization process in Appendix D.\nEvaluation metrics Following previous works (Hafner, 2021; Wu et al., 2024b), we evaluate the agent with two primary metrics: reward and score. Agents receive a +1 reward for each new achievement unlocked (e.g., make wood pickaxe, place furnace). Additionally, they earn a \u00b10.1 reward for every health point gained or lost. The score metric is computed by aggregating the success rates for each achievement:\n$S = exp(\\frac{1}{N}\\sum_{i=1}^{N}ln (1 + s_i))-1.$\n(4)\nBaseline systems We compare DiVE with three categories of baselines. First, LLM-based methods, including SPRING (Wu et al., 2024b), ELLM (Du et al., 2023), and chain-of-thought (CoT) (Wei et al., 2022). Second, reinforcement learning (RL) based approaches such as DreamerV3 (Hafner et al., 2023), PPO (Schulman et al., 2017), AD (Moon et al., 2024). Third, we consider demonstrations from human experts, which provide insights from actual human players."}, {"title": "Overall Results", "content": "Table 1 demonstrates that DiVE significantly outperforms all other baselines across two distinct evaluation metrics. Notably, DiVE exceeds the previous state-of-the-art (SOTA) LLM-based method, SPRING\u00b9, by a substantial margin, achieving a 337.8% relative improvement in score and a 110.1% enhancement in reward. Additionally, DiVE also surpasses the prior SOTA RL-based approach, DreamerV3, with a 21.4% absolute improvement in score and a 2.8 absolute increase in reward. Notably, DiVE achieves rewards comparable to domain-familiar human players using just 10 human demonstrations.\nQuantitative Analysis\nAblation study We conduct a set of ablation study to help elucidate how individual elements contribute to the overall performance of the method. Notably, the significant performance disparity"}, {"title": "Offline Dynamics Learning", "content": "W\\cup \\hat{H^+}}{W+W} and precision as P = |\\cup \\hat{H^+}|/\\hat{H} + . As illustrated in Figure 3, both the discovered dynamics W and the verified dynamics \\hat{H^+}| demonstrate an increase in recall with the progression of discovery steps, indicating that the richness of the learned dynamics enhances as more discovery steps are taken. Furthermore, the gap in recall between W and W| decreases as the number of discovery steps increases, suggesting that the Verifier effectively filters out \"noisy\" dynamic candidates while retaining those that generalize across different trajectory segments.\nTo further investigate whether the Verifier retains correct world dynamic candidates H^+ while filter-"}, {"title": "Qualitative Analysis", "content": "The correctness of the learned and verified dynamics is categorized as either correct or as mistakes due to confounders, in-domain hallucinations, or out-of-domain hallucinations. As illustrated in Table 3, in the example of defeating a zombie, the mistake attributed to confounders is primarily due to the simultaneous increase in health points at the step where the zombie is defeated. In this case, the Discoverer categorizes this increase as a result of defeating the zombie. For the in-domain hallucination example, it is impossible to increase the wood while defeating the zombie, though the wood may appear in the observation. Lastly, the out-domain hallucination is responsible for discovering dynamics that contain nonexistent objects in the observation or even the Crafter environment.\nCompared to the dynamics from SPRING in Table 4, we found that DiVE's dynamics are not only more precise but also more informed. For example, regarding the dynamics of placing a stone, SPRING only found that placing a stone requires stones; however, DiVE found that placing a stone requires exactly one stone and the precise facing condition for successful placement. Furthermore, based on this information, the Evolver can derive advanced dynamics for placing a stone, such as its potential to act as a barrier between the agent and dangerous creatures."}, {"title": "Related Work", "content": "Embodied Agent Reinforcement learning (RL) has long been an effective method for developing embodied agents through trial and error (Moon et al., 2024; Hafner et al., 2023; Schulman et al., 2017; Jiang et al., 2022; Hafner et al., 2020; Hessel et al., 2018; Sekar et al., 2020; Weir et al., 2022). With recent advancements in LLMs (Achiam et al., 2023; Team et al., 2023; Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023), their reasoning and planning capabilities have emerged as an important component for the embodied agent (Wang et al., 2023a,b; Ma et al., 2023; Liang et al., 2023; Brohan et al., 2023; Nottingham et al., 2023; Silver et al., 2024; Driess et al., 2023; Shi et al., 2024; Wu et al., 2024b).\nDiscovering and Verifying LLMs can uncover knowledge by inducing and deducing rules for reasoning tasks (Zhu et al., 2023). Additionally, they can directly discover the underlying domain knowledge from previous trajectories and interactive experience (Colas et al., 2023; Majumder et al., 2023; Fu et al., 2024a,b; Zhong et al., 2024; Zhao et al., 2024)."}, {"title": "Ethical Concerns", "content": "We do not anticipate any immediate ethical or societal impact arising from our work. In this study, we aimed to bridge the knowledge gap of LLMs towards the target domain. However, despite our efforts, using LLMs as the backbone, DiVE might still hallucinate due to the inherent tendency of LLMs to hallucinate."}, {"title": "Conclusion", "content": "In this work, we present DiVE, a framework designed for long-horizon interactive planning tasks. Specifically, given a handful of demonstrations, a Discoverer can extract useful world dynamics such as an action's preconditions and outcomes. A Verifier subsequently filters out inaccurate dynamic candidates. In an online setting, conditioned on the offline-learned knowledge, an Evolver learns situational strategies through interaction. DiVE bridges the knowledge gap between a pre-trained LLM and the target domain by autonomously learning hierarchical world dynamics and evolving contextual knowledge. Extensive experiments demonstrate its effectiveness. This work may pave the way for developing LLM-based frameworks to achieve the mutual enhancement of dynamics learning and policy learning without extensive human annotation."}, {"title": "Limitations", "content": "Experimenting with DiVE in a stochastic yet simple environment may not adequately represent all the potential real-world scenarios that an embodied agent might encounter upon deployment. Additionally, DiVE requires collecting human expert trajectories, which can be either impossible or resource-intensive in certain situations. Finally, obtaining perfect object information might not be feasible in some environments."}]}