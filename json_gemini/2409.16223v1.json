{"title": "Fine-Tuning is Fine, if Calibrated", "authors": ["Zheda Mai", "Arpita Chowdhury", "Ping Zhang", "Cheng-Hao Tu", "Hong-You Chen", "Vardaan Pahuja", "Tanya Berger-Wolf", "Song Gao", "Charles Stewart", "Yu Su", "Wei-Lun Chao"], "abstract": "Fine-tuning is arguably the most straightforward way to tailor a pre-trained model (e.g., a foundation model) to downstream applications, but it also comes with the risk of losing valuable knowledge the model had learned in pre-training. For example, fine-tuning a pre-trained classifier capable of recognizing a large number of classes to master a subset of classes at hand is shown to drastically degrade the model's accuracy in the other classes it had previously learned. As such, it is hard to further use the fine-tuned model when it encounters classes beyond the fine-tuning data. In this paper, we systematically dissect the issue, aiming to answer the fundamental question, \"What has been damaged in the fine-tuned model?\" To our surprise, we find that the fine-tuned model neither forgets the relationship among the other classes nor degrades the features to recognize these classes. Instead, the fine-tuned model often produces more discriminative features for these other classes, even if they were missing during fine-tuning! What really hurts the accuracy is the discrepant logit scales between the fine-tuning classes and the other classes, implying that a simple post-processing calibration would bring back the pre-trained model's capability and at the same time unveil the feature improvement over all classes. We conduct an extensive empirical study to demonstrate the robustness of our findings and provide preliminary explanations underlying them, suggesting new directions for future theoretical analysis. Our code is available at https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated.", "sections": [{"title": "Introduction", "content": "Pre-trained models (e.g., foundation models) have become an indispensable component in modern AI development [2]. Building upon neural networks with millions if not billions of parameters and trained with gigantic amounts of data, these models have led to groundbreaking results in various domains [30, 32, 38] and shown several emerging capabilities not observed priorly [21, 27, 2].\nYet, to obtain superior downstream performance, fine-tuning is still often needed. Typically, fine-tuning optimizes the model's performance on the available downstream data. Taking image classification as an example, end-users typically fine-tune the pre-trained classifier to maximize the accuracy of a certain set of classes at hand, no matter whether they know up front that it is the complete set of classes or not. As a result, it is hard to further apply the model to some other classes, even if the model had learned about those classes in pre-training. (Please see Figure 1 for an illustration.)\nA recent work by Tu et al. [48] systematically evidenced such a problem. They fine-tuned a pre-trained classifier with a subset of classes it had learned (i.e., fine-tuning classes) and found this led to a drastic accuracy drop in the other classes (i.e., absent classes). Tu et al. [48] viewed this as an instance of catastrophic forgetting [12, 24, 7] and suggested two ways to address it: (1) identifying the model updating direction that can improve both the fine-tuning and absent classes; (2) preserving class relationships. They presented a strong baseline, combining a new stochastic gradient descent (SGD)"}, {"title": "Related Work", "content": "Fine-tuning. The basic methods are linear probing and full fine-tuning [26]. Parameter-efficient fine-tuning (PEFT) [58] has attracted increasing attention lately (mainly for Transformers [50]), aiming to update only a fraction of pre-trained parameters on top of linear probing. We focus on full fine-tuning because it is model-agnostic and arguably still the most widely used method. That said, we expect the insights and implications from our study not to be limited to full fine-tuning and potentially transferable to other fine-tuning methods as well.\nRisk in fine-tuning. When the downstream data is scarce, fine-tuning is prone to over-fitting and needs certain forms of regularization [28]. Even with sufficient data to represent the downstream task, fine-tuning may risk losing valuable knowledge the pre-trained model had learned. For example, [55] showed that fine-tuning with data from a specific domain (e.g., ImageNet-1K real images) degrades the pre-trained model's generalizability to other domains (e.g., ImageNet-Sketch/Rendition). On an orthogonal dimension, [48] showed that fine-tuning a pre-trained classifier with a subset of classes it had learned led to a huge accuracy drop in the other classes. Similar phenomenons have been observed in [64, 39]. Along with these findings come several proposed solutions. [55] showed that a weight interpolation between the pre-trained and fine-tuned models reclaims the pre-trained model's generalizability. [48] investigated many approaches, including weight interpolation, but found they are insufficient to preserve the accuracy in the other classes. They presented a novel SGD strategy that helps identify the gradient direction benefiting both fine-tuning and other classes. [64] developed dedicated solutions to CLIP-based vision-language models to preserve accuracy for absent classes.\nWe 1) consider the fine-tuning scenario studied in [48] and 2) use the standard neural network classifier architecture with a fully connected layer on top. Our focus is not to propose a brand-new solution and compete with existing ones, but to understand the underlying cause of the accuracy drop.\nContinual learning and catastrophic forgetting. Fine-tuning a pre-trained model with a subset of classes it had learned is related to continual learning [7] but has several differences. Class-incremental learning [33, 35] aims to expand the model's label space. In contrast, we suppose the pre-trained model had learned a wide range of classes; fine-tuning is meant to tailor it to a downstream domain (e.g., a different image style), not to learn extra categories. Compared to domain-incremental learning [33, 31], we do not ask the fine-tuned model to retain its performance in the pre-training domain. That said, the accuracy drop observed in our study and continual learning can all be considered certain forms of catastrophic forgetting [36, 12]. Indeed, a recent survey by Wang et al. [54] argued that forgetting is a common issue in various research fields, including foundation models, meta-learning,\nPost-processing calibration. Training with class-imbalanced data is known to produce biased logits towards major classes [59]. Such an issue not only appears in long-tailed recognition [20, 18] but also few-shot learning [60, 44] and continual learning with replay buffers [47, 46, 34]. Post-processing calibration [3, 37, 56, 57] is a widely applicable method to address this issue, which adjusts model confidence during inference. Popular methods include normalizing classifier norms [20, 17] and adjusting logits according to class sizes [59, 37, 22, 63]. Unlike the machine learning problems above, whether or not post-processing calibration can address the accuracy drop in our problem is not immediately clear. Without access to the absent class data, fine-tuning may have ruined the features or linear classifiers corresponding to these classes. If so, simply performing post-processing calibration cannot reclaim the accuracy of the absent classes. Our main contribution is therefore not merely the solution, but the systematic study that identifies post-processing calibration as an effective solution.\nOther paradigms. There are several other machine learning paradigms related to the fine-tuning setting we study. We refer the readers to [48] for an in-depth discussion and comparison."}, {"title": "Background", "content": "Problem definition. We study the problem of fine-tuning a pre-trained classifier capable of recognizing C classes, using data from a subset of C* classes at hand. The goal is to tailor the classifier to the downstream domain (e.g., a different image style).\nMore formally, let us denote by $\\mathcal{D}_{t r}=\\left{\\left(x_{i}, y_{i} \\in \\mathcal{S}\\right)\\right}_{i=1}^{N}$ the data set for fine-tuning, where $\\mathcal{S}$ is a strict subset of the pre-trained model's label space $\\mathcal{Y}$, i.e., $|\\mathcal{S}|=C^{*}<C=|\\mathcal{Y}|$. Let us denote a neural network classifier by $\\hat{y}=\\arg \\max _{c \\in \\mathcal{Y}} \\mathbf{W}_{c}^{\\top} f_{\\theta}(x)$, where $x$ is an input sample, $f_{\\theta}$ is the feature extractor parameterized by $\\theta$, and $\\mathbf{W}=\\left{\\mathbf{w}_{c}\\right\\}_{c=1}^{C}$ is the set of linear classifiers, a.k.a., the fully-connected (FC) layer. We call {$\\theta$, W} the model parameters. The value $\\mathbf{w}_{c}^{\\top} f_{\\theta}(x)$ is often referred to as the decision value or logit of class $c$.\nWithout loss of generality, we define $\\mathcal{Y}={1, \\ldots, C}$ and $\\mathcal{S}={1, \\cdots, C^{*}}$. That is, they share the first $C^{*}$ classes. We call $\\mathcal{S}$ the fine-tuning classes and use $\\mathcal{U}=\\{(C^{*}+1), \\ldots, C\\}$ to denote the absent classes during fine-tuning, where $\\mathcal{S} \\cap \\mathcal{U}=\\emptyset$ and $\\mathcal{S} \\cup \\mathcal{U}=\\mathcal{Y}$.\nFine-tuning and its issue. Full fine-tuning updates the pre-trained model {$\\theta_{0}, \\mathbf{W}_{0}$} by\n$\\left{\\theta_{T}, \\mathbf{W}_{T}\\right\\}=\\arg \\min _{\\{\\theta, \\mathbf{w}\\}} \\sum_{i=1}^{N} \\ell\\left(x_{i}, y_{i} ; \\theta, \\mathbf{W}\\right)$, with {$\\theta, \\mathbf{W}$} initialized by {$\\theta_{0}, \\mathbf{W}_{0}$},\nwhere $\\ell$ denotes the cross-entropy loss; {$\\theta_{T}, \\mathbf{W}_{T}$} denotes the fine-tuned model.\nSince $\\mathcal{D}_{t r}$ only covers a subset of classes $\\mathcal{S}$, the fine-tuned model {$\\theta_{T}, \\mathbf{W}_{T}$} was observed to degrade drastically in classifying data from the absent classes $\\mathcal{U}$ [48]. Figure 2 shows one example: the absent class accuracy in the y-axis drops from ~ 23% (*) to only ~ 3% (marked *) after fine-tuning, even though the fine-tuning class accuracy in the x-axis increases hugely by roughly 60%.\nTerminology. Tu et al. named the above setting Holistic Transfer (HT) [48]. The core challenge is how to maintain and even improve the fine-tuned model's ability to recognize the C - C* absent classes in the downstream domain. For ease of reference, we use the following terms interchangeably.\n\u2022 fine-tuning classes & seen classes; absent classes & unseen classes;\n\u2022 downstream domain & target domain; pre-training domain & source domain;\n\u2022 fine-tuning & naive fine-tuning.\nWe emphasize that the \u201cunseen\" classes are indeed seen in pre-training but absent in fine-tuning."}, {"title": "A Systematic Study of Fine-Tuning (FT)", "content": "Seeing the ability to classify the absent classes \u201cdisappears\u201d after fine-tuning, we are curious about\n\u2022 how each component of the fine-tuned model {$\\theta_{T}, \\mathbf{W}_{T}$} contributes to it;"}, {"title": "Is the fine-tuned feature extractor damaged?", "content": "We first investigate the feature extractor $f_{\\theta}$, i.e., whether the fine-tuned extractor $f_{\\theta_{T}}$ forgets the discriminative ability to differentiate absent-class samples. We apply the NCM classifier [5, 43, 53], whose accuracy is solely governed by the feature quality, to examine the feature extractor. Given a test example $x$, the NCM classification rule is\n$\\hat{y}=\\arg \\min _{c \\in B} \\frac{\\left|f_{\\theta_{T}}(x)-\\mu_{c}\\right|}{\\left|f_{\\theta_{T}}(x)\\right|_{2}}$,\nwhich outputs the class $\\hat{y} \\in B$ whose feature mean $\\mu_{\\hat{y}}$ is the closest to $f_{\\theta_{T}}(x)$. We hold out a subset of the downstream data to calculate the class mean, for both fine-tuning and absent classes."}, {"title": "What is damaged in the fine-tuned neural network classifier?", "content": "The findings in subsection 4.2 eliminate the feature extractor $\\theta_{T}$ from the candidate root causes of the degraded FT model {$\\theta_{T}, \\mathbf{W}_{T}$}. The drastic drop of absent class accuracy thus must come from the FC layer $\\mathbf{W}_{T}$ or its alignment with the feature extractor. To analyze what goes wrong, we decompose the softmax probability induced by the neural network classifier as follows,\np(c | x)=\\frac{\\exp \\left(\\mathbf{w}_{c}^{\\top} f_{\\theta}(x)\\right)}{\\sum_{c^{\\prime} \\in \\mathcal{Y}} \\exp \\left(\\mathbf{w}_{c^{\\prime}}^{\\top} f_{\\theta}(x)\\right)}=\\frac{z_{c}(x)}{\\sum_{c^{\\prime} \\in \\mathcal{S} \\cup \\mathcal{U}} z_{c^{\\prime}}(x)}=\\frac{z_{c}(x)}{\\sum_{c^{\\prime} \\in \\mathcal{S}} z_{c^{\\prime}}(x)+\\sum_{c^{\\prime} \\in \\mathcal{U}} z_{c^{\\prime}}(x)},\\\np(c | x)=\\frac{z_{c}(x)}{\\sum_{c^{\\prime} \\in \\mathcal{S}} z_{c^{\\prime}}(x)+\\sum_{c^{\\prime} \\in \\mathcal{U}} z_{c^{\\prime}}(x)}=\\frac{z_{c}(x)}{\\sum_{c^{\\prime} \\in \\mathcal{S}} z_{c^{\\prime}}(x)+\\sum_{c^{\\prime} \\in \\mathcal{U}} z_{c^{\\prime}}(x)},\nwhere $z_{c}(x)=\\exp \\left(\\mathbf{w}_{c}^{\\top} f_{\\theta}(x)\\right)$. Let $c$ be an absent class, the $1^{s t}$ term stands for the predicted probability that $x$ belongs to the absent classes $\\mathcal{U}$, not the fine-tuning classes $\\mathcal{S}$; the $2^{n d}$ term stands for the probability that within the absent classes $\\mathcal{U}, x$ belongs to class $c$. Correctly classifying an absent class example $(x, y \\in \\mathcal{U})$, i.e., $y=\\arg \\max _{c \\in \\mathcal{Y}} p(c | x)$, thus requires 1) obtaining a high probability in the $1^{s t}$ term and 2) correctly classifying the example among absent classes.\nBuilding upon this insight, we analyze the accuracy of taking arg max of the $2^{n d}$ term to classify absent class examples. We note that this is exactly the $A c c_{\\mathcal{U} / \\mathcal{U}}$ defined in subsection 4.1. As shown in Figure 4, $A c c_{\\mathcal{U} / \\mathcal{U}}$ does not degrade but improves in the first few epochs of fine-tuning and stays stable afterward. This result implies two key messages. First, the FT model does not forget its ability to distinguish among absent classes. Second, the drastic accuracy drop of $A c c_{\\mathcal{U} / \\mathcal{Y}}$ results from the $1^{s t}$ term in Equation 4\u2014the binary classifier separating fine-tuning and absent classes.\nAs shown in Figure 5, the predicted probability that absent class examples belong to absent classes (i.e., the average of the $1^{s t}$ term over absent class examples) reduces notably along with the fine-tuning"}, {"title": "Post-Processing Calibration for the Rescue", "content": "The systematic study in section 4 highlights several key characteristics of the FT model {$\\theta_{T}, \\mathbf{W}_{T}$}. First, the model retains and even improves the accuracy of classifying absent class samples when the label space is limited to absent classes (i.e., $A c c_{\\mathcal{U} / \\mathcal{U}}$). Second, the model tends to assign much higher decision values $\\mathbf{w}_{c}^{\\top} f_{\\theta}(x)$, a.k.a. logits, to the fine-tuning classes, ending up misclassifying most absent class samples into fine-tuning classes and thus hurting $A c c_{\\mathcal{U} / \\mathcal{Y}}$.\nThese characteristics suggest that a simple, post-processing calibration of the FT model's logits could potentially bring back the pre-trained model's ability to classify absent classes correctly. To this end, we apply the calibration approach proposed in a different context of zero-shot learning [4], which adds a factor $\\gamma$ uniformly to the logits of all absent classes, leading to a new classification rule\n$\\hat{y}=\\arg \\max _{c \\in \\mathcal{Y}} \\mathbf{w}_{c}^{\\top} f_{\\theta}(x)+\\gamma \\mathbb{1}[c \\in \\mathcal{U}]$.\nThis calibration factor lifts the logits of absent classes to a level comparable with those of fine-tuning classes. Suppose an absent class example is correctly classified among absent classes but misclassified into fine-tuning classes, adding a sufficiently large $\\gamma$ could reclaim the correct prediction.\nWe design two approaches to properly set $\\gamma$ without accessing"}, {"title": "Ablation Study and Additional Analysis", "content": "Data split and fine-tuning class size. In the default setup of [48], fine-tuning classes and absent classes are uniformly randomly sampled and have each portion close to 50%. In practice, however, end-users may have a smaller number of classes at hand or collect data from semantically or conceptually similar classes whose appearances are positively correlated. To explore such practical situations, we investigate how 1) a biased sampling such that the fine-tuning classes are conceptually or distributionally similar to each other than the absent classes and 2) a smaller size of fine-tuning classes would impact the performance of fine-tuning. Specifically, for Office-Home, classes that are similar in the pre-trained model's feature space are selected as fine-tuning classes, leaving the rest as absent ones. We also vary the number of fine-tuning classes. For ImageNet-S, we leverage the WordNet [11] hierarchy to select coherent groups, such as dog (118 classes) and mammal (218 classes) as the fine-tuning classes. Please see the Appendix for details.\nFigure 8 illustrates the performance gain on AUSUC and NCM $A c c_{\\mathcal{U} / \\mathcal{Y}}$ (from the pre-trained model to the FT model). Notably, the hierarchical split poses a greater challenge for fine-tuning in transferring domain knowledge from fine-tuning classes to absent classes, as evidenced by its relatively minor AUSUC improvements compared to the random split. This difficulty is attributed to the inherent challenge of transferring features learned from dogs or animals to distinctly different classes like TVs and trucks. Furthermore, smaller fine-tuning class sizes present additional difficulties, leading to less pronounced improvements, especially in Office-Home. In summary, our analysis suggests that the benign behaviors of fine-tuning are robust in more practical and difficult splits but its performance requires further improvement when the fine-tuning class size is exceptionally small. Detailed experimental setup and additional results are available in the Appendix.\nOptimizer. Prior work [48] predominantly used the SGD optimizer. To investigate optimizers' influence in fine-tuning, we scrutinize six popular optimizers including SGD with Momentum, Adam [23], AdaBelief [65], Adadelta [61], AdaGrad [10] and RMSprop. Our study includes 1) a variation across learning rates (LR), adjusted as multipliers of the default ones, i.e., [10, 1, 0.1, 0.01] \u00d7 default LR, and 2) three distinct weight decays: [0, 5e - 4, 5e - 3]. We report the AUSUC here and leave additional metrics in the Appendix. The results, as illustrated in Figure 9, reveal the robustness of the benign behaviors to different hyperparameter settings of the SGD optimizer. Conversely, more advanced, adaptive optimizers show higher sensitivity when hyperparameters are not properly chosen. Nevertheless, under small enough learning rates (and weight decay), they perform similarly to SGD and notably improve the pre-trained model (whose AUSUC is around 0.5 in Office-Home).\nShould We Freeze the Linear Classifier or Feature Backbone? Adhering to the practice in source-free DA [29], Tu et al. [48] advocate for freezing the linear classifier $\\mathbf{W}_{0}$ during fine-tuning (termed the frozen classifier approach) to preserve the absent class relationship and hence accuracy $A c c_{\\mathcal{U} / \\mathcal{Y}}$. Surprisingly, our analysis reveals that while preserving class relationships within the FC"}, {"title": "Conclusion", "content": "\"What happens if one fine-tunes a pre-trained classifier with a subset of classes?\u201d Prior work showed that while it improves the fine-tuning class accuracy in the downstream domain, it drastically degrades the model's ability to recognize the other classes the model had previously learned. Our systematic study, however, provides a different opinion. We found that fine-tuning does not degrade but often improves the model's ability to recognize the other classes, if the classifiers' logits are well-calibrated. We expect our study to serve as a valuable reference for practical fine-tuning of pre-trained models."}, {"title": "Appendix", "content": "We provide details omitted in the main paper.\n\u2022 Appendix A: experiment and dataset details\n\u2022 Appendix B: additional details for calibration\n\u2022 Appendix C: additional analysis of fine-tuning\n\u2022 Appendix D: detailed results of different architectures, datasets, and splits.\nOur study is built upon [48]. Tu et al. [48] named the above the setting Holistic Transfer (HT). For ease of reference, we use the following terms interchangeably.\n\u2022 fine-tuning classes & seen classes; absent classes & unseen classes;\n\u2022 downstream domain & target domain; pre-training domain & source domain;\n\u2022 fine-tuning & naive fine-tuning.\nWe emphasize that the \u201cunseen\" classes are indeed seen in pre-training but absent in fine-tuning."}, {"title": "A Experiment and Dataset Details", "content": "A.1 Main Investigation (cf. section 3.1 in the main paper)\nA.1.1 Dataset Details\nImageNet-Variants includes ImageNet-R(endition) [15] and ImageNet-S(ketch) [52]. ImageNet-R comprises 30,000 images across 200 ImageNet classes with various renditions (e.g., paintings, embroidery, etc.) while ImageNet-S consists of 50,000 sketch-like images for 1K ImageNet classes. Each class is randomly divided into training and testing sets following an 8:2 split. 50% of the classes are randomly selected as fine-tuning classes (100 for ImageNet-R and 500 for ImageNet-S), with the remainder as absent. The downstream test set encompasses all the 200 classes for ImageNet-R and 1K classes for ImageNet-S to evaluate model performance across the full class spectrum.\nOffice-Home [51] is a popular domain adaptation dataset, comprising 65 classes from 4 domains (Art, Clipart, Real, and Product). Following the setup in [48], Art and Real are used as pre-trained domains and each pre-trained model is then transferred to each of the three remaining downstream domains individually, resulting in six (pre-training, downstream) pairs. Within each downstream domain, each class is randomly split into training and testing sets following a 7:3 split. 30 classes are randomly selected as fine-tuning classes; the remaining 35 classes are absent classes. The downstream test set contains all the 65 classes in each downstream domain.\nVTAB [62] encompasses a diversity of image classification tasks. To enable zero-shot predictions with CLIP, Tu et al. [48] only uses the tasks that provide text names for classes: Caltech101, CIFAR100, DTD, EuroSAT, Flowers102, Pets, Resisc45, SVHN, and SUN397. Notably, the SVHN dataset was excluded from our experiments due to CLIP's difficulty in accurately predicting numerical labels as shown in [48]. Adhering to the practice in [48], we randomly sample half of the classes as fine-tuning and the remaining as absent. The downstream training set only includes images of the fine-tuning classes; the test set contains images from all classes.\nTable 4 summarizes the statistics of all the datasets used in this paper.\nA.1.2 Training Details\nFor the ImageNet-Variants benchmark, we use an ImageNet-1K pre-trained ResNet-50 (results in the main paper) and ViT-B/32 (results in the appendix) as pre-trained models. The pre-trained model is fine-tuned on downstream tasks for 50 epochs using the SGD optimizer with a learning rate 1e-3, momentum 0.9, weight decay 1e-4, and batch size 64. For the compared method proposed in [48], we set the hyper-parameters $\\mathcal{L}_{distill} = 10$ and $\\mathcal{L}_{rank} = 100$ for ImageNet-R and $\\mathcal{L}_{distill} = 1$ and $\\mathcal{L}_{rank} = 5$ for ImageNet-S."}, {"title": "Ablation Study (cf. section 5 in the main paper)", "content": "In section 5 of the main paper, we investigate how 1) a biased sampling of fine-tuning and absend classes and 2) a smaller size of fine-tuning classes would impact the performance of fine-tuning. Here, we provide details of how we split the data.\nIn the Office-Home dataset, we strategically select fine-tuning classes that are similar in the pre-trained model's feature space, aiming to create a meaningful distinction between fine-tuning and absent classes. To identify classes that are closely related, we apply the metric of total intra-group distance: a lower value indicates higher similarity among classes within a group. This ensures finding fine-tuning classes that exhibit tight clustering in the feature space. We employ a greedy strategy to grow the fine-tuning class set towards a pre-defined size. Figure 13 ad Figure 14 present t-SNE visualizations that illustrate the distribution of chosen fine-tuning and absent classes for varying fine-tuning class sizes across two distinct pre-trained domains.\nFor the ImageNet-S dataset, we leverage the WordNet [11] hierarchy to select coherent groups as fine-tuning classes. Specifically, we explore two different hierarchical splits. The details are shown in Table 5."}, {"title": "Toy Example (cf. section 6 in the main paper)", "content": "To elucidate the impact of similar fine-tuning training data on the feature representation of absent data, in section 6 and figure 12 of the main paper, we construct a toy example featuring four classes of 2-dimensional data, each represented by distinct colors (blue, cyan, red, magenta). Figure 15 shows the same data. In this example, pre-training data (\u25a1) are generated from Gaussian distributions with a standard deviation of 0.2 and four different means: (10, 2), (10, 3), (10, 8), and (10, 7). To simulate domain shift, the target domain data (\u25cb) undergoes a horizontal shift, with the cyan and magenta classes moving to the right and the blue and red classes to the left by an identical distance. This setup, intentionally restricting data to non-negative values, mirrors the effect of a ReLU activation.\nWe employ a two-layer multi-layer perceptron (MLP) with a configuration of 2-2-4 (input dimension: 2, hidden layer dimension: 2, and output dimension: 4). The MLP is initially pre-trained on the four-class pre-training dataset, with the first layer weights set as an identity matrix to simplify visualization. Subsequent fine-tuning on target domain data incorporates only two classes (blue and cyan), without the constraint of freezing the first layer. Both the pre-training and fine-tuning phases utilize the SGD optimizer, applying a learning rate of 0.01 for 100 epochs with cross-entropy loss as the objective."}, {"title": "B Additional Details for Calibration (cf. section 4 in the main paper)", "content": "We provide details for the calibration methods in section 4 of the main paper.\nB.1 Average Logit Gap (ALG)\nIn a well-calibrated pre-trained model, the scale of average non-Ground-Truth (non-GT) logits is expected to be similar across different classes. Figure 16 demonstrates that using the pre-trained"}, {"title": "Pseudo Cross-Validation (PCV)", "content": "To address the challenge of selecting $\\gamma$ without access to target validation data, which ideally encompasses both fine-tuning and absent classes, we introduce a novel Pseudo Cross-Validation (PCV) method, demonstrated in Figure 18. Specifically, we partition the target training dataset $\\mathcal{D}_{t r}$ into pseudo training data $\\mathcal{D}_{pseudo-tr}$ and pseudo validation data $\\mathcal{D}_{pseudo-val}$. $\\mathcal{D}_{pseudo-tr}$ is further divided into two subsets with disjoint label spaces, simulating pseudo-fine-tuning data $\\mathcal{D}_{pseudo-f t}$ and pseudo-absent data $\\mathcal{D}_{pseudo-absent}$. We then naively fine-tune the pre-trained model on $\\mathcal{D}_{pseudo-absent}$ and evaluate $A c c_{\\mathcal{S} / \\mathcal{Y}}$ and $A c c_{\\mathcal{U} / \\mathcal{Y}}$ using $\\mathcal{D}_{pseudo-val}$. We select a $\\gamma$ by balancing these two accuracies. To enhance the robustness of $\\gamma$ estimation, we employ bootstrapping, repeating the pseudo-splitting and fine-tuning process three times with varying partitions. The selected $\\gamma$ is applied to the fine-tuning model {$\\theta_{T}, \\mathbf{W}_{T}$}, which is fine-tuned from the pre-trained model {$\\theta_{0}, \\mathbf{W}_{0}$} on the entire target training data $\\mathcal{D}_{t r}$."}, {"title": "C Additional Analysis of Fine-tuning (cf. section 3.4 and section 6 in the main paper)", "content": "In this section, we provide more analysis including an analysis of feature improvements, an investigation of the tendency for logits to be biased toward fine-tuning classes, and the effects of freezing the classifier and backbone during fine-tuning. Additionally, we quantitatively examine the class relationship change between the pre-trained and fine-tuning models. Finally, we extend our analysis to examine the impact of fine-tuning on the iWildCam dataset [1].\nC.1 Feature Improvement by the Fine-Tuned Model"}]}