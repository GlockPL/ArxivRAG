{"title": "EventAug: Multifaceted Spatio-Temporal Data Augmentation Methods for Event-based Learning", "authors": ["Yukun Tian", "Hao Chen", "Yongjian Deng", "Feihong Shen", "Kepan Liu", "Wei You", "Ziyang Zhang"], "abstract": "The event camera has demonstrated significant success across a wide range of areas due to its low time latency and high dynamic range. However, the community faces challenges such as data deficiency and limited diversity, often resulting in over-fitting and inadequate feature learning. Notably, the exploration of data augmentation techniques in the event community remains scarce. This work aims to address this gap by introducing a systematic augmentation scheme named EventAug to enrich spatial-temporal diversity. In particular, we first propose Multi-scale Temporal Integration (MSTI) to diversify the motion speed of objects, then introduce Spatial-salient Event Mask (SSEM) and Temporal-salient Event Mask (TSEM) to enrich object variants. Our EventAug can facilitate models learning with richer motion patterns, object variants and local spatio-temporal relations, thus improving model robustness to varied moving speeds, occlusions, and action disruptions. Experiment results show that our augmentation method consistently yields significant improvements across different tasks and backbones (e.g., a 4.87% accuracy gain on DVS128 Gesture). Our code will be publicly available for this community.", "sections": [{"title": "Introduction", "content": "Event camera, which is also known as Dynamic Vision Sensors(DVS)(Lichtsteiner, Posch, and Delbruck 2008a), is a new kind of bio-inspired device. Unlike conventional RGB frame cameras, event camera only focuses on the changes but not the absolute value of brightness, thus it has several unique features, including low-latency, low energy consumption, and extremely high dynamic range. These advantages make event camera a powerful tool in research areas like classification(Deng et al. 2022), depth estimation(Lichtsteiner, Posch, and Delbruck 2008b), flow estimation(Ponghiran, Liyanagedera, and Roy 2023) and motion segmentation(Stoffregen et al. 2019). These advantages greatly stimulate the research in event-based learning area.\nExisting works in event-based learning community mainly focus on backbone design and task-specific network building. Representative works include the Group Event Transformer (Peng et al. 2023), Spikepoint (Ren et al. 2023), video deraining (Wang et al. 2023), and motion deblurring (Zhang et al. 2023). However, event data is sparse and limited in quantity, leading to annotation difficulties and a scarcity of high-quality labeled data. This results in over-fitting and insufficient feature extraction, constraining the performance and application of event data. Spike Neural Networks (SNNs), considered more suitable for handling sparse event data, require greater data diversity due to challenges in optimization and training. Hence, developing methods to reduce over-fitting and enhance model performance across various tasks is a critical research priority.\nData augmentation, which has been proved to be an effective way to improve the generalization ability of models for RGB images, is a practical method to solve the problems above. Yet, rare research focuses on data augmentation in event community. Currently, there are two main kinds of event augmentation strategies. (i) Directly transferring the conventional data augmentation methods for images to the event frames, for example, applying the geometry transformation for RGB images on event data, like flipping, rolling and rotation (Li et al. 2022). These augmentation strategies apply the paradigms designed for RGB modality and ignore the sparsity and temporal characteristics of event data, resulting in limited augmentation effects and a failure to generalize across complex real-world situations like varied moving speed, occlusion, and action disruption. (ii)Utilizing the temporal information of the event data in a coarse manner. Representative work includes randomly dropping event data (Gu et al. 2021) and mixing up two event streams with a three-dimensional mixing strategies generated by a random Gaussian Mixture Model (Shen, Zhao, and Zeng 2023). These methods rely heavily on randomness and prior assumptions(e.g., Gaussian distribution, uniform distribution) which ignores the uneven spatial and temporal distribution of event data, for example, when augmentation is applied to irrelevant spatial or temporal area, the result will be considerably poor and inefficient. Thus the augmentation performance is very limited and lacks generality. Moreover, since they do not reveal the rich motion and object information inside event data, they only make elementary use of spatio-temporal information in event data, making it incapable of effectively enhancing the spatio-temporal diversity of the dataset.\nGiven the problems of the existing methods described above, we believe that a customized event data augmentation approach should be designed to enhance training data diversity efficiently by considering both the sparsity characteristics of event data and their uneven spatio-temporal distribution. By utilizing such augmentation techniques, we can enhance the diversity of the datasets and improve the model's generalization capabilities (Figure 1,2).\nTo this end, we propose the EventAug (Figure 3), which contains three novel spatio-temporal augmentation methods for event data such as Multi-scale Temporal Integration(MSTI), Spatial-salient Event Mask(SSEM) and Temporal-salient Event Mask(TSEM). They are proposed to fully utilize the rich spatio-temporal information inside event data and enhance the diversity of training samples considering comprehensively the sparse and uneven spatio-temporal distribution properties of event data.\nFor MSTI, since motion speed determines the completeness of motion cues and the clarity of object boundaries within an event frame, varied moving speeds can capture different temporal and spatial patterns. Therefore, we enrich the diversity of motion speeds by adjusting integration scale to form a multi-scale temporal integration. Precisely, by applying a multi-scale augmentation strategy, we actually simulated different motion patterns, which enable us to generate samples under diverse motion scenarios utilizing a single event-based sample. Features such as edges, textures, and motion also change with the integration scale, thereby enhancing diversity and making the network more robust to diverse motion patterns. Also, MSTI can boost the generalization ability across sensor noise since frames generated from different integration scales possess diverse noise levels.\nFor MSTI and TSEM, we employ spatial and temporal mask to diversify local spatial and temporal correlations, thus enriching the high-level semantics of event data and inducing the models to learn more spatio-temporal relations. Specifically, to address the uneven spatial-temporal distribution of event data, we propose a fast, training-free spatial saliency and temporal saliency calculation method to obtain saliency with low computational cost. With the guidance of saliency information, our augmentation is highly effective, stable, and adaptive to different event datasets. Comparing to former methods with strong prior assumptions (Gu et al. 2021; Shen, Zhao, and Zeng 2023), we only apply augmentation in salient spatial and temporal regions and the amplitude of augmentation is defined adaptively w.r.t the strength of saliency. Moreover, the design philosophy of MSTI and TSEM make them able to greatly improve the robustness towards occlusion and motion disruption, thus significantly improving generalization ability of models on downstream tasks on complex real-world scenarios.\nIn summary, our main contributions are as follows:\n(1) We propose the Multi-scale Temporal Integration (MSTI) technique to enhance the diversity of motion speeds. MSTI allows an event model to learn additional motion cues and spatial features. This provides the model with enhanced generalization capabilities across different scenarios involving objects moving at different speeds.\n(2) We introduce two methods, namely Spatial-salient Event Mask (SSEM) and Temporal-salient Event Mask (TSEM), to diversify local correlations using fast spatial and temporal saliency guidance. These methods address the uneven spatial and temporal distribution of event data, significantly improve the diversity of local spatio-temporal correlations, and enhance robustness to occlusion and motion disruption in complex scenarios.\n(3) Experimental results on both ANN and SNN networks demonstrate that our proposed methods comprehensively enhance spatio-temporal diversity with high efficiency. These improvements lead to significant enhancements in accuracy and generalization ability."}, {"title": "Method", "content": "Overview\nThe focus of EventAug is to simultaneously enhance the diversity of both the temporal and spatial dimensions, while also considering the uneven distribution of event data and the complexities of real-world scenarios. Therefore, we have designed a systematic enhancement framework that takes into account both temporal and spatial diversity, and increases the diversity in aspects such as occlusion and moving speed. Specifically, this includes three methods: Multi-scale Temporal Integration(MSTI), Spatial-salient Event Mask(SSEM) and Temporal-salient Event Mask(TSEM)\nMulti-scale Temporal Event Integration\nEvent Frame Integration State-of-the-art ANNs mainly deal with RGB frames, and can not directly process the sparse event streams. And for SNNs, directly inputting frames without preliminary encoding process has become a widely adopted strategy in deep spiking neural networks (Fang et al. 2023). Therefore, to apply the existing powerful ANN and SNN models to the event vision and extract discriminative spatial cues, a mainstream solution is to transform event streams to frame-like data(Kaiser, Mostafa, and Neftci 2020; Fang et al. 2021a; Wu et al. 2019; Fang et al. 2023; Rebecq, Horstschaefer, and Scaramuzza 2017). We will introduce the details in the following:\nLet E denotes the sequence of an event stream:\n$E_i = (x_i, y_i, p_i, t_i)$ (1)\n$(x_i, y_i)$ is the coordinate where the event $E_i$ generates, $t_i$ is the timestamp indicates when the event is generated, and $p_i$ is the polarity with 1 and -1 indicating positive and negative events respectively. We pre-arrange the event stream in timestamp order.\nFor the integration, we evenly divide the event stream into T slices. Let F(j) denotes the frame that generates from the jth event slice, istart and iend as the start and end timestamp of an event frame. So we have:\n$i_{start} = \\lfloor \\frac{N}{T} j \\rfloor$ (2)\n$i_{end} = \\lfloor \\frac{N}{T} (j+1) \\rfloor$ (3)\nThen, we perform temporal integration in the target time region:\n$F(j)_{x_i, y_i, p_i} = \\sum_{k=i_{start}}^{i_{end}} I(E_k)$ (4)\n$I(E_k) = \\begin{cases} 1, & x_i = x_k, y_i = y_k, p_i = p_k \\\\ 0, & otherwise \\end{cases}$ (5)\nwhere I is the indicator function, N is the total number of event in the event stream. After integration, each event stream transforms into T event frames, and each event frame can be treated as a 2 channel image with a resolution of [W, H].\nMulti-scale Integration This method is inspired by our observations that motion speed determines the completeness of motion cues and the clarity of object boundaries within an event frame. For a long-term temporal scale integration, more motion information is revealed including moving orbit, the speed of movement and so on. For a short-term temporal scale of integration, more information about the object itself is revealed(e.g., contours, shapes). These can be easily discerned from the visualization (Figure 2). Also, frames with different integration scales contain varying degrees of noise which is beneficial for feature extraction as diversity is improved. Based on our observation above, we design the multi-scale temporal integration to let neural network learn different kinds of pattern. We can also add diversity of motion speed to eliminate the negative effects brought by different moving speed and make the network more robust. In implementation, we apply a speed-aware policy: Choose both n and the m-fold of the base scale (n and m are hyper parameters), together with the base scale. This augmentation policy is highly general and can be applied to all datasets. Moreover, we find in experiments that we can achieve a great augmentation performance by simply setting hyper parameters to double and half scale without carefully tuning. The experimental results and analysis can be found in section .\nSpatial and Temporal Salient Event Mask\nTo enrich local spatial and temporal correlation and improve robustness to occlusion and motion disruption, we propose two saliency-guided spatial and temporal masking method, namely Spatial-salient Event Mask(SSEM) and Temporal-salient Event Mask(TSEM). Concretely, We first calculate the spatio-temporal saliency based on the distribution of event density in temporal and spatial patches, and then selectively apply masking on the temporal and spatial dimensions based on the saliency information.\nSpatial-salient Event Mask Considering the uneven spatial distribution of event data, we aim to improve the efficiency of our augmentation methods through guidance from saliency information. We first obtain the spatial saliency of the event data by a fast and training-free method we propose bellow. This method utilize the unique sparse nature of event data, which is different from the dense RGB modal images. Therefore, we can acquire the spatial saliency map of the event frame by observing the density distribution of it. With the sparse nature of event, the density distribution for event frame is a great approximation of event saliency map with very low computation cost since there is no need for training. To elaborate, for spatial-saliency, we first divide the event frame into 16 \u00d7 16 patches like in (He et al. 2022; Dosovitskiy et al. 2021), then we obtain the saliency information by calculating the density distribution of event. Our detailed methods are described in Algorithm 1. Given\nIdx = idx[kr - 1] (6)\n$\u20ac = Dense(X_{Idx})$ (7)\nThen, we obtain the spatial-salient mask of event frames by determining whether each patch is salient:\n$\\begin{cases} Pi \\&ps, Dense(pi) > \u0454\\\\ Pi \\& Ps, otherwise \\end{cases}$ (8)\n$M_{ij} = \\begin{cases} 0, & (i, j) \u2208 ps \\\\ 1, & otherwise \\end{cases}$ (9)\nFinally, we acquire the masked frame FM by applying the Hadamard product of original frame Fo and mask M.\n$F_M = F_O \u2297 M$ (10)\nNow we get the masked frame FM.\nTemporal-salient Event Mask To address the uneven temporal distribution of event data, we propose Temporal-salient Event Mask. We first calculate the temporal saliency of the event data by our proposed algorithm bellow. Just like Algorithm 1, this method utilizes the sparse nature of event data, thus it is fast and training-free Therefore, we can acquire the temporal saliency map of the event frame by observing its density distribution in temporal dimension. We first divide the event stream into T slices, where T is decided by the policies in. Detailed method is described in the left side of Algorithm 2. For temporal-salient event mask, we first get the temporal saliency of an event stream by the algorithm above, then we apply the temporal-salient mask to let the network learn the rich spatio-temporal information of the event data better. Inside the salient frame slice, we first get the minimum density of the target salient frame slice \u0442, and we define a base mask rate p which is a hyper parameter. The mask rate ps of each target salient slice will be decided by their event density and p. For every event e in the slice, the mask probability of it is equal to ps. We describe this method in the right side of Algorithm 2 in detail."}, {"title": "Experiment Setup", "content": "Experiment\nOur experiments are conducted using Pytorch (Paszke et al. 2017), with Adam (Kingma and Ba 2017) optimizer and a learning rate of 0.001. For convergence, we train SNNs for 80 epochs and ANNs for 200 epochs. Detailed implementation information is provided in the supplementary materials. To assess the generalization of our methods, we evaluate the augmentation technique on two distinct deep neural network architectures:\n\u2022 Spiking Neural Network (SNN): SNNs, due to their event-driven computing and temporal coding, are considered the most suitable network architecture for processing event data. Therefore, we choose the convolution spiking neural network (CSNN) defined and implemented by (Fang et al. 2023) as the backbone for experiment, which is a simple SNN with 5 convolution layers and 3 full connection layers. The scale of the parameters is 1.7M.\n\u2022 Artificial Neural Network(ANN): We follow former studies (Gehrig et al. 2019; Gu et al. 2021) to use Resnet-34 (He et al. 2016) as the backbone, which contains 4 residential layers for feature extraction. The scale of the parameters is 21.8M.\nDatasets\nWe follow previous works (Shen, Zhao, and Zeng 2023; Li et al. 2022; Gu et al. 2021) to use two challenging public datasets CIFAR10-DVS (Cheng et al. 2020) and DVS128 Gesture (Amir et al. 2017) for evaluation. For CIFAR10-DVS, we follow (Shen, Zhao, and Zeng 2023; Li et al. 2022; Fang et al. 2023) to divide the training and test sets by 9 : 1 (9k train samples and 1k validation samples). DVS128 Gesture is a real-world gesture recognition dataset collected by the DVS camera. We follow (Shen, Zhao, and Zeng 2023; Fang et al. 2023, 2021b) to divide the training and test sets by 8:2 (1176 train samples and 288 validation samples)."}, {"title": "Efficacy of our EventAug", "content": "Table 1 and 2 compares our EventAug with other state-of-the-art event augmentation methods across different backbone architectures, using identical hyperparameters. EventAug consistently achieves significant performance improvements, showcasing its efficacy in enriching data diversity and reducing over-fitting.\nNDA(Li et al. 2022) naively transfers RGB data augmentation to event data, neglecting the sparse and spatio-temporal aspects of event streams, leading to limited augmentation effects. EventDrop(Gu et al. 2021) relies heavily on randomness and fails to address the uneven distribution of events, often resulting in the loss of crucial information or ineffective augmentation in irrelevant areas. In contrast, EventAug utilizes the sparse and spatio-temporal nature of event data, significantly boosting dataset diversity by capturing a broader spectrum of motion patterns and spatial-temporal correlations. This tailored approach enables EventAug to significantly outperform existing augmentation methods.\nAblation Study\nWe performed ablative experiments on CIFAR10-DVS and DVS128 Gesture datasets using two backbones to evaluate our EventAug methods. The outcomes are summarized in Tables 1 and 2.\nTable 3 shows that our three augmentation methods significantly improve most of models and tasks, especially on SNNs. For instance, we achieved a 3.30% accuracy gain on CIFAR10-DVS and 4.87% gain on DVS128 Gesture. However, our method does not show significant improvement when tested on the CIFAR10-DVS dataset with Resnet-34. We believe that the reason for this is that the dataset is based on static images, and the details will be discussed in the Limitation section."}, {"title": "Limitation", "content": "Our EventAug and prior methods yield sub-optimal CIFAR-10 classification with ResNet-34. We believe this is due to the dataset being derived from static images (CIFAR-10(Krizhevsky 2009)) rather than being directly captured from the real world. This leads to high similarity between consecutive frames (Figure 4), emphasizing spatial information over temporal information. ResNet-34, unlike SNNs, emphasizes spatial embedding in event frames, neglecting temporal semantics, thus limiting the enhancement from our temporal augmentation techniques like MSTI."}, {"title": "Conclusion", "content": "In this work, we introduce a spatio-temporal data augmentation method that diversifies motion speeds and local correlations using three strategies. EventAug improves model robustness in challenging scenes and shows strong generalization across different network architectures. Our approach achieves significant improvements, as validated by experiments with multiple backbones and tasks. In the future, we will expand this augmentation method to other event-based learning tasks like detection, estimation and segmentation."}]}