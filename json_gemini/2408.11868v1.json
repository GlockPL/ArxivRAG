{"title": "Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores", "authors": ["Jun Lu", "David Li", "Bill Ding", "Yu Kang"], "abstract": "This paper presents an approach to improve text embedding models through contrastive fine-tuning on small datasets augmented with expert scores. It focuses on enhancing semantic textual similarity tasks and addressing text retrieval problems. The proposed method uses soft labels derived from expert-augmented scores to fine-tune embedding models, preserving their versatility and ensuring retrieval capability is improved. The paper evaluates the method using a Q&A dataset from an online shopping website and eight expert models. Results show improved performance over a benchmark model across multiple metrics on various retrieval tasks from the massive text embedding benchmark (MTEB). The method is cost-effective and practical for real-world applications, especially when labeled data is scarce.", "sections": [{"title": "Introduction", "content": "Text embedding models are fundamental in natural language processing (NLP), serving as low-dimensional vector representations that capture semantic similarity between texts (Aggarwal & Zhai, 2012; Angelov, 2020). They are critical for tasks such as text classification, retrieval, question answering, and dialogue systems. Recent advancements in large language models (LLMs) have spurred interest in retrieval-augmented systems that integrate LLM reasoning with the efficiency of text embeddings.\nTwo main research directions exist: enhancing performance in semantic textual similarity (STS) through supervised fine-tuning, normalization, and unsupervised contrastive learning; and addressing text retrieval through dual-encoder architectures and self-supervised pre-training (Li et al., 2023; Wang et al., 2022; Izacard & Grave, 2020; Ren et al., 2021;; Devlin, 2018; Vaswani, 2017). Recent studies have aimed to create unified text representation models through large-scale contrastive learning and prompt-based learning (Muennighoff, 2022), evaluated on benchmarks like the massive text embedding benchmark (MTEB) (Muennighoff et al., 2022), which assesses models across 56 datasets and seven categories.\nInnovative models, such as GTE and E5 (Li et al., 2023; Wang et al., 2022), have been developed to address the challenges of creating general-purpose text embeddings. GTE uses multi-stage contrastive learning over diverse datasets, while E5 introduces a progressive learning mechanism to refine embeddings by focusing on zero-shot learning.\nEmbeddings play a crucial role in retrieval augmented generation (RAG) (Lewis et al., 2020; Es et al., 2023; Salemi & Zamani, 2024). They represent input text and retrieved knowledge sources in a vector space for efficient similarity comparisons. High-quality embeddings improve retrieval accuracy by capturing the semantic and syntactic information of the text. Embeddings can be combined with metadata or context information to enhance retrieval and generation. During the generation process, retrieved knowledge, represented through embeddings, guides the generation of the output text, ensuring it is informed by relevant and accurate information.\nIn this work, we propose a novel fine-tuning framework for embedding models that utilizes soft labels derived from expert-augmented scores. This approach enables the fine-tuning of embedding models for specific downstream tasks while preserving the versatility of the original embeddings and ensuring that the retrieval capability of the models does not degrade after fine-tuning. Our"}, {"title": "Related Work", "content": "Text embeddings represent texts using low-dimensional vectors and capture semantic similarity through vector operations. There has been a sustained interest in converting texts into compact, low-dimensional representations. Early approaches included methods such as text embedding models like Glove (Pennington et al., 2014) (which lacks context awareness and are thus commonly labeled as word embedding models), latent semantic analysis (LSA) (Deerwester et al., 1990) (which achieves embedding by decomposing a matrix that represents co-occurrences of words and documents to create document embeddings), low-rank matrix factorization with its variants (Acharya et al., 2019; Lu, 2024) (can be incorporated to find the compression of embedding models), and latent Dirichlet allocation (LDA) (Blei et al., 2003) (which uses probabilistic graphical models to infer topic distributions).\nModels like Sentence-BERT (SBERT) (Reimers & Gurevych, 2019) and Sentence-T5 (Ni et al., 2021) have been fine-tuned on supervised datasets to learn embeddings customized for specific tasks, such as passage retrieval and semantic textual similarity. On the other hand, methods like inverse cloze task (ICT) (Chang et al., 2020), random cropping (Izacard et al., 2021), and neighboring text spans (Neelakantan et al., 2022) have been used to construct text pairs for unsupervised contrastive learning.\nMore recently, GTE (general text embeddings) (Li et al., 2023) is designed to provide robust and versatile text representations for a wide range of NLP and code-related tasks. GTE employs a multi-stage contrastive learning approach, which involves unsupervised pre-training on a large-scale dataset of text pairs sourced from various domains, followed by supervised fine-tuning on high-quality text pairs with human labels from multiple sources. Despite having a relatively modest parameter count of 110 million, GTE demonstrates exceptional performance, outperforming existing text embedding models and commercial APIs on various benchmarks.\nE5 (Embeddings from bidirectional Encoder rEpresentations) (Wang et al., 2022) is trained with weak supervision signals from a large-scale curated text pair dataset CCPairs, and it provides strong off-the-shelf text embeddings that perform exceptionally well in both zero-shot and fine-tuned settings. The core strength of E5 lies in its ability to transfer effectively across different tasks, making it a versatile choice for applications that require a single-vector representation of texts. A key feature of E5's training methodology is the use of a contrastive learning framework, which leverages the weak supervision signals from CCPairs to learn robust text representations. This approach allows E5 to capture the nuanced semantic relationships within texts, leading to high-quality embeddings that generalize well to unseen data.\nGTE and E5 are examples of multi-stage methods that first train on large-scale datasets and then fine-tune to incorporate human knowledge. In contrast, Wang et al. (2023) introduces a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1,000 training steps. Unlike existing methods, which often involve complex, multi-stage intermediate pretraining with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, this method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. The authors utilize proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages, and then fine-tune open-source decoder-only LLMs on the synthetic data using a standard contrastive loss. Experiments demonstrate that this method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data.\nMuennighoff et al. (2022) introduces the massive text embedding benchmark (MTEB), which was initially designed to provide a comprehensive evaluation of text embedding models across a diverse"}, {"title": "Proposed Method", "content": "In this section, we outline the formulation of contrastive fine-tuning for embedding models using soft labels derived from expert-augmented scores. We begin by discussing the traditional approach of contrastive fine-tuning with hard labels and then introduce our proposed method of contrastive fine-tuning with soft labels."}, {"title": "Contrastive Fine-Tuning with Hard Label", "content": "Fine-tuning embedding models using labeled data can incorporate human knowledge into the model, thereby enhancing its performance. Traditional contrastive fine-tuning methods leverage medium-sized datasets for downstream tasks. For example, in a question-and-answer system (Q&A system) based on RAG, the same question can be expressed in multiple ways, and the embedding model can be fine-tuned based on these varied formulations of questions (contrastive for irrelevant questions and positive for relevant questions). In the contrastive fine-tuning framework, contradiction sentences are considered hard negatives and relevant sentences are considered hard positives. The loss function for hard labels is the mean squared error (MSE):\n$\\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^{n}(f_{\\theta}(q_i) \\cdot f_{\\theta}(p_i) - y_i)^2,$\nwhere $f_{\\theta}$ represents the fine-tuned model, and $q_i$ and $p_i$ are the i-th query and passages, respectively. Here, $y_i$ is the hard label; $y_i = 1$ if $q_i$ and $p_i$ are relevant sentences and $y_i = 0$ otherwise. However, hard labels have their limitations. Although they encapsulate human knowledge, they might introduce overly strict information that can be difficult for embedding models to learn effectively. Furthermore, while fine-tuning on hard labels can benefit specific downstream tasks, it may negatively impact the overall performance of the embedding models, particularly when evaluated on more diverse or open datasets."}, {"title": "Contrastive Fine-Tuning with Expert-Augmented Scores", "content": "Suppose we have K expert embedding models, the similarity $s_k$ between a query q and a passage p for each expert k is defined using the cosine similarity (dot product) of $E_k(q)$ and $E_k(p)$: $s_k = E_k(q) \\cdot E_k(p)/(||E_k(q)||\\cdot ||E_k(p)|)$. The loss function is a soft label based on the K experts:\n$\\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^{n}(f_{\\theta}(q_i) \\cdot f_{\\theta}(p_i) - \\hat{y}_i)^2,$\nContrastive fine-tuning is to distinguish the relevant text pairs from other irrelevant or negative pairs. Therefore, the soft labels can be obtained by\nSoft-1: $\\hat{y}_i = \\begin{cases} \\max\\{s_{1,i}, s_{2,i},..., s_{K,i}\\} \\quad \\text{if } y_i = 1 \\\\ \\min\\{s_{1,i}, s_{2,i},..., s_{K,i}\\} \\quad \\text{if } y_i = 0 \\end{cases}$ and $s_{ki} = \\frac{E_k(q_i) \\cdot E_k(p_i)}{||E_k(q_i)||\\cdot ||E_k(p_i)||} \\qquad (1)$\nThe K experts serve as the source of data distillation, meaning that their collective judgments are distilled into the fine-tuned model. The rationale behind this approach is that hard labels (strict binary relevance) can be too aggressive for the embedding models to achieve. Instead, the expert-augmented scores provide a softer target that is more attainable by the models.\nMoreover, since most expert models can differentiate the relevant and irrelevant texts to some extent, the hard label information {y} can be omitted, and the expert information alone can be utilized such that\nSoft-2: $\\hat{y}_i = mean\\{s_{1,i}, s_{2,i}, ..., s_{K,i}\\} \\qquad (2)$"}, {"title": "Experiments", "content": "We adopt a Q&A dataset from an online shopping website. The Q&A data contains 26 questions with answers provided. We use open-sourced language models to generate 60 pairs of questions for each question in the Q&A data. Specifically, we generate 20 similar questions each from Llama3.1-8B, Gemma2-9B, and Qwen2-7B (Team et al., 2024; Yang et al., 2024). These are relatively small-sized language models, making the generation of rewritten questions cost-effective.\nIn other words, for each set of \"question\u201d and \u201canswer\u201d pair, there are 61 questions and one answer. Out of these, 21 questions are reserved for held-out query evaluation, and 40 questions are used to generate pairs of query and passage datasets for fine-tuning. The answer texts are not used for training and retained for retrieval evaluation. Therefore, for each question and answer pair, we extract 40 \u00d7 40 relevant query and passage text pairs (we consider that the order matters). Moreover, within the question and answer pair, 'Question1.' and 'Question2.' are considered relevant; this indicates that \u2018Question1. And Question2.' and 'Question2.' or \u2018Question1. And Question2.' and 'Question1.' are also relevant. This triples the number of relevant text pairs. For the negative/irrelavant pairs, for each question, we randomly select three questions from a different set of question and answer pair to match the number of positive pairs. As a result, the total number of samples is 249,600 (half positive and half negative), with hard labels provided. Overall, the effort required to obtain these data was minimal; the source of the dataset is a Q&A system with 26 questions and answers.\nWe use eight expert models, namely M3e-small, BCE-embedding-base_v1, BGE-large-zh-v1_5 Text2vec-base-chinese, Stella_en_1.5B_v5, UAE-Large-V1, Mxbai-embed-large-v1, GTE-large-en-v1.5. These models were chosen based on their relatively good performance in the MTEB benchmark and because their sizes are smaller than 2GB. By varying the threshold from 0 to 1, we differentiate the positive and negative samples to obtain both precision and recall values, followed by a precision-recall curve (PR curve) in Figure 1(i). The PR curve shows that the performance of these models varies. The BCE-embedding-base_v1, Mxbai-embed-large-v1, UAE-Large-V1, and GTE-large-en-v1.5 models perform relatively better on this dataset; while Stella_en_1.5B_v5 performs relatively worse.\nFor the Soft-1 label defined in Equation (1), we define the active set as the set of labels that are equal to the Soft-1 label for each expert model. The proportions of the active sets over the total number of samples for M3e-small, BCE-embedding-base_v1, BGE-large-zh-v1_5, Text2vec-base-chinese, Stella_en_1.5B_v5, UAE-Large-V1, Mxbai-embed-large-v1, and GTE-large-en-v1.5 are 40.20%, 38.23%, 2.32%, 1.12%, 4.79%, 0.70%, 2.71%, and 10.48%, respectively. Hence, our approach does not heavily depend on the models that perform well in the PR curve analysis.\nThe benchmark model selected is BCE-embedding-base_vl, which was recently released and achieved state-of-the-art results in various datasets . Models are fine-tuned with a batch size of"}, {"title": "Evaluation Datasets", "content": "The MTEB Benchmark, proposed in (Muennighoff et al., 2022), is designed for evaluating text embedding models across a wide range of tasks. Although MTEB includes bitext mining datasets that contribute to its various subcategories, the majority of the datasets pertain to retrieval. The evaluation metrics used are mean average precision at 10 (mAP@10), normalized discounted cumulative gain at 10 (nDCG@10), mean reciprocal rank at 10 (mRR@10), and more (Muennighoff et al., 2022). In this work, we focus on the retrieval subsets. For retrieval tasks, the default metric suggested by MTEB is nDCG@10, while we show both mAP@10 and nDCG@10 metrics for evaluation. The mRR@10 shows similar results, and we will not report here for brevity.\nMoreover, the dataset we collected contains answer texts, and 21 questions are held out for evaluation for each raw question (with 40 questions used for training). We then use the held-out set and answer texts for retrieval evaluation. In this context, each passage contains 40 questions and the corresponding answer. Therefore, for each held-out query (a question out of 21 questions), there is only one relevant passage (see Figure 4). The metrics mentioned above are not suitable for this evaluation. We then plot the similarity of the queries and passages, and vary the thresholds of the similarity scores for evaluating the precisions and recalls. The metric of the area under precision-recall curve (AUPRC) is considered."}, {"title": "Results on MTEB", "content": "Table 1 and 2 present the evaluation of nDCG@10 and mAP@10 metrics, respectively, for different models across various datasets from MTEB retrieval tasks. The average nDCG@10 scores for Benchmark, Soft-1, Soft-2, and Hard label models are 39.675, 40.633, 40.334, and 37.574, respectively, with standard deviations of 29.963, 28.552, 28.167, and 27.081, respectively. And the average mAP@10 for Benchmark, Soft-1, Soft-2, and Hard label models are 34.419, 35.323, 35.04, and 32.243, respectively, with standard deviations of 29.693, 28.587, 28.221, and 26.585, respectively. The win rate of Soft-1 over the Benchmark is 50.37% in terms of nDCG@10, and is 55.38% with respect to mAP@10. This again confirms that no single text embedding method dominates across all tasks (Muennighoff et al., 2022). The Soft-1 and Soft-2 models demonstrate promising results with higher scores and smaller standard deviations compared to the Benchmark model, suggesting they perform well across various datasets and their performance is consistently stable. The Hard label model, on the other hand, has worse nDCG@10 and mAP@10 scores compared to the Benchmark; although it has a smaller standard deviation.\nThe improvement seen in the fine-tuning with Soft-1 and Soft-2 labels might be attributed to the reduced anisotropy in the fine-tuned models (meaning the text embeddings occupy a larger cone in the vector space after fine-tuning). This property is further supported by the results on the held-out set: the Soft-1 and Soft-2 models have better results in terms of area under precision-recall (PR) curve (see Section 4.3). The text embeddings of irrelevant pairs are then distributed across a wider range of the vector space."}, {"title": "Results on Held-Out Set", "content": "Building upon the held-out Q&A dataset, since for each held-out question, there is only one related passage, we examine the similarity between the embedding vectors for inter- and intra-questions; see Figure 4. The goal of a retrieval process for a Q&A system is to determine whether the embeddings can help differentiate related passages from irrelevant ones, so that the system can use the related passage for downstream tasks (such as RAG). Define further the term \u201cIntraSample\u201d as the similarity between a question from the i-th query (there are 21 questions for each query) and the i-th passage, and \"InterSample\" as the similarity between a question from the i-th query and the j-th passage (j \u2260 i, and since there are 26 questions in the Q&A dataset, $i, j \\in \\{1,2,...,26\\}$). The method is frequently used in recommender systems; see, for example, Lu (2021). Figure 2(a), 2(b), 2(c), and 2(d) depict the bin plots of the distributions of IntraSample and InterSample under cosine similarity for the Benchmark, Hard label, Soft-1, and Soft-2 models, respectively."}, {"title": "Distributional Results", "content": "We use LLaMA-Factory fine-tuning dataset to evaluate the distributional results. The set contains 1,000 instructions intended for fine-tuning large language models. We generate 20,000 pairs of these instructions and evaluate the cosine similarities under different models. Figure 5 shows the distribution of cosine similarities between the embedding vectors of different instruction texts. Table 4 presents the symmetric KL divergences for different models. The Hard label model exhibits the greatest divergence from the Benchmark, while the Soft-2 model is the closest. The Soft-1 model exhibits a balance between the Soft-2 and Hard label models. This is expected, as the Soft-2 model utilizes the least aggressive labels. On the other hand, there is a notable difference between the Soft-1 and Hard label models, indicating that they fine-tune towards distinct spaces."}, {"title": "Conclusion", "content": "In this paper, we have presented a method for improving text embeddings through contrastive fine-tuning on small datasets augmented with expert scores. Our approach leverages a limited amount of fine-tuning data, making it cost-effective and practical for real-world applications. We have demonstrated the effectiveness of our method through extensive experiments using a Q&A dataset sourced from an online shopping website. Our findings show that the Soft-1 and Soft-2 models, which use less aggressive labels during fine-tuning, outperform the benchmark model in terms of nDCG@10 and mAP@10 metrics, achieving higher scores and showing more stable performance across various datasets. The Hard label model, on the other hand, exhibits worse performance but a smaller standard deviation, suggesting that it may be more consistent but less effective overall. Additionally, we have analyzed the distribution of cosine similarities between embedding vectors of different instruction texts and observed a clear distinction between \"IntraSample\" and \"InterSample\u201d distributions. This indicates that the embeddings are capable of effectively distinguishing between related and unrelated passages, which is crucial for retrieval tasks. Overall, our method offers a practical solution for enhancing the quality of text embeddings in both the downstream tasks and general-purpose tasks, particularly in scenarios where labeled data is scarce. Future work could explore further improvements in the fine-tuning process and the integration of additional features/labels based on the dataset itself to enhance the representativeness and utility of the embeddings in diverse NLP applications. Recent researches show that transformer-based models are anisotropic (Godey et al., 2023; 2024). It remains interesting to show whether the anisotropic issue is less severe in the high-dimensional space after fine-tuning with soft labels or not."}]}