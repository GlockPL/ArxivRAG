{"title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep Model", "authors": ["Lianyu Wang", "Meng Wang", "Huazhu Fu", "Daoqaing Zhang"], "abstract": "Model intellectual property (IP) protection has attracted growing attention as science and technology advancements stem from human intellectual labor and computational expenses. Ensuring IP safety for trainers and owners is of utmost importance, particularly in domains where ownership verification and applicability authorization are required. A notable approach to safeguarding model IP involves proactively preventing the use of well-trained models of authorized domains from unauthorized domains. In this paper, we introduce a novel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which serves as a barrier against illegal transfers from authorized to unauthorized domains. Drawing inspiration from human transitive inference and learning abilities, the CUPI-Domain is designed to obstruct cross-domain transfers by emphasizing the distinctive style features of the authorized domain. This emphasis leads to failure in recognizing irrelevant private style features on unauthorized domains. To this end, we propose novel CUPI-Domain generators, which select features from both authorized and CUPI-Domain as anchors. Then, we fuse the style features and semantic features of these anchors to generate labeled and style-rich CUPI-Domain. Additionally, we design external Domain-Information Memory Banks (DIMB) for storing and updating labeled pyramid features to obtain stable domain class features and domain class-wise style features. Based on the proposed whole method, the novel style and discriminative loss functions are designed to effectively enhance the distinction in style and discriminative features between authorized and unauthorized domains, respectively. Moreover, we provide two solutions for utilizing CUPI-Domain based on whether the unauthorized domain is known: target-specified CUPI-Domain and target-free CUPI-Domain. By conducting comprehensive experiments on various public datasets, we validate the effectiveness of our proposed CUPI-Domain approach with different backbone models. The results highlight that our method offers an efficient model intellectual property protection solution.", "sections": [{"title": "1 INTRODUCTION", "content": "EEP Neural networks have made significant strides in diverse areas of machine learning. However, recent achievements heavily relied on abundant and high-quality data , dedicated training resources , and meticulous manual fine-tuning . Acquiring a proficiently trained deep model demands substantial time and effort in practice. While some DNN models, such as VGG , Inception-v3 , and SWIN , are publicly available for non-commercial use, many owners of models used in commercial applications prefer to keep their trained DNN models private. This is often due to business considerations and concerns related to privacy and security, especially in critical applications like autonomous driving, face recognition, and intrusion detection . Unfortunately, once these models are sold, they can be easily copied and redistributed, infringing on the interests of the developers and potentially increasing security risks in safety-critical applications, causing incalculable damage. For instance, in commercial Machine Learning as a Service (MLaaS) platforms , well-trained deep models have high business value. They should be treated as the intellectual property (IP) of their creators/owners. When these models are uploaded to public platforms or deployed as remote services on the cloud, malicious users may steal them for financial gain. One of the most concerning threats is \"Will releasing the model make it easy for the main competitor to copy this new feature and hurt owner differentiation in the market?\". Therefore, it is crucial to regard and protect these models as valuable scientific and technological intellectual property (IP) . Traditionally, model owners design, train, deliver, and deploy efficient deep models based on the authorized domain (represented by blue squares in Fig. 1). They grant specific users the right to use their well-trained models, enabling them to obtain correct predictions on the authorized domain, as depicted in Fig. 1 (a). However, since these models are trained with the overall features of the authorized domain, they may inadvertently cover unauthorized domains as well. Here, the unauthorized domain refers to a domain that shares the same task but exhibits different features compared to the authorized domain. This difference is primarily attributed to variations in the device, which in real cases is associated with the model owner rather than the user. This coverage has led to the emergence of freelancers who illegally steal well-trained deep models and deploy them on unauthorized domains (indicated by red squares in Fig. 1). They process the models using techniques such as domain adaptation or domain generation to obtain correct predictions and claim these models as their own products for profit. Such behavior infringes on the interests of model developers, and may even increase the potential security risks on safety-critical applications, causing incalculable damage and hindering the long-term development of deep model . Thus, protecting the rights of model owners and ensuring the safety of model deployment is essential.\nComprehensive model IP protection for deep models includes both ownership verification and applicability authorization . Ownership verification refers to \"Who can use the model\u201d. Model owners can grant usage permission to specific users, and any other users will be infringing on the owner's IP rights when employing the model . However, it is important to acknowledge that there is no guarantee that the trained model will remain secure and not be leaked. There are two potential risks to consider: 1) Cyber hackers may attempt to steal the model from the cloud storage or other platforms where it is stored. 2) Specific users who have been granted permission may transfer the model to unauthorized or illegal users, which undermines the control and ownership of the model, potentially leading to misuse. These risks highlight the importance of implementing ownership verification mechanisms to protect against unauthorized access, sharing, and misuse of trained models. Common ownership verification mechanisms include embedding watermarks , utilizing model fingerprints , or employing predefined triggers . However, it is essential to recognize that these methods can be vulnerable to various techniques such as fine-tuning , classifier retraining , elastic weight consolidation algorithms , and watermark overwriting. These vulnerabilities can potentially weaken the effectiveness of the model's IP protection measures. Therefore, it is crucial to explore and develop more robust ownership verification techniques.\nApplicability authorization refers to \"Where can use the model\". The model owner trains the deep model on the authorized domain and grants specific users the right to utilize the model within that domain, ensuring correct predictions . However, in practice, legal users and illegal users may employ techniques such as domain adaptive , domain generalization , etc., to process the model and utilize it on other domains with similar tasks , thereby achieving usable performance. This kind of infringement is not only easier to carry out but also more common and often concealed. This poses a significant challenge for maintaining control over the model's usage and protecting the IP of the model owner. Therefore, it is necessary to develop a more robust applicability authorization mechanism that restricts the model's performance to the tasks specified by the owner, ensuring that the model's performance on unauthorized domains is not superior to that of an untrained model. This will discourage unauthorized users from attempting to steal the model. To achieve this, a Non-Transferable Learning (NTL) method is proposed , which uses an estimator with a characteristic kernel from Reproducing Kernel Hilbert Spaces to approximate and increase the maximum mean difference between two distributions on finite samples. However, the authors only considered using limited samples to increase the mean distribution difference of features between domains and ignored outliers. The convergence region of NTL is not tight enough. Moreover, the calculation of the maximum mean difference is class-independent, which reduces the model's feature recognition ability in the authorized domain to a certain extent.\nTo address these challenges, we propose a novel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which serves as a barrier against illegal transfers from authorized to unauthorized domains, as depicted in Fig 1 (b). Our proposed CUPI-Domain considers the overall features of each domain, comprising two components: shared features and private features. Shared features refer to semantic features, while private features include stylistic cues such as texture, saturation, perspective, brightness, background, etc. Initialized with the unauthorized domain, our proposed CUPI-Domain particularly emphasize the private features of the authorized domain, strategically fusing them with its semantic features, constructing richer style features for adaptively style augmentation. By intentionally diminishing the recognition capability of the CUPI-Domain, we can implicitly impede illegal transfers to unauthorized domains that possess irrelevant private style features, thereby leading to wrong predictions. Additionally, we design Domain-Information Memory Banks (DIMB) for storing and updating labeled pyramid features to obtain stable domain class features and domain class-wise style features. The novel style loss function is designed to effectively enhance the style difference between the authorized and unauthorized domains, whereas the discriminative loss functions are tailored to optimize the discriminative difference between the authorized/unauthorized domain and the CUPI-Domain. Moreover, we further develop two distinct solutions for leveraging the CUPI-Domain, depending on whether the unauthorized domain is known or not: target-specified CUPI-Domain and target-free CUPI-Domain.\nIn general, we highlight our five-fold contributions:\nWe propose a novel IP protection approach, named CUPI-Domain, to block illegal transfers from authorized to unauthorized domains. Meanwhile, CUPI-Domain generators are designed to generate labeled and style-rich CUPI-Domain by emphasizing the private features of the authorized domain, implicitly leading to failure in recognizing irrelevant private style features on unauthorized domains.\nWe introduce externel DIMB to store specified features to obtain stable domain class features and domain class-wise style features for the subsequent computation.\nStyle and discriminative loss functions are designed to further improve inter-domain differences while maintaining semantic consistency between the unauthorized domain and CUPI-Domain.\nTwo distinct solutions are developed for utilizing CUPI-Domain based on whether the unauthorized domain is known or not: target-specified CUPI-Domain and target-free CUPI-Domain.\nFinally, comprehensive experimental results on several public datasets demonstrate that CUPI-Domain effectively reduces the recognition ability on unauthorized domains while maintaining strong recognition on authorized domains. As a plug-and-play module, our CUPI-Domain can be easily implemented within different backbones and provide efficient solutions.\nThis paper is based on and extends our previous CVPR2023 version in the following aspects. 1) We have implemented more standardized generators known as the CUPI-Domain generators. This generator effectively eliminates the style features of the input CUPI-Domain and subsequently integrates them with the style features of the authorized domain, resulting in enhanced similarity between the two. 2) We also designed external DIMB, which store and update labeled pyramid features to obtain stable domain class features and domain class-wise style features for the subsequent computation. 3) We designed novel style loss function and discriminative loss functions to improve the style difference and discriminative difference between authorized and unauthorized domains, while maintain the semantic consistency between the unauthorized domain and CUPI-Domain. 4) We conducted additional experiments on the Office-home-65 (65 categories, 4 domains) and DomainNet (345 categories, 6 domains) to demonstrate the continued effectiveness of our method as the data complexity increases. 5) We have invested substantial efforts in enhancing the presentation and organization of our paper, specifically focusing on the introduction, framework, key results, and discussion. We have provided more comprehensive explanations in the introduction section to offer a clearer understanding of the research context. Beside, we have introduced several new sections to elaborate on our novel framework, encompassing the method formulation, corresponding technical components, and loss functions. Additionally, we have diligently rewritten multiple sections to improve readability and provide more detailed explanations, particularly in the sections addressing quantitative comparisons, ablation experiments and discussion. These revisions aim to enhance the clarity and coherence of our"}, {"title": "2 RELATED WORK", "content": "Currently, there are two primary categories of methods for protecting model IP: ownership verification and applicability authorization. In terms of ownership verification, watermark embedding is a widely used classic method. proposed a quantifiable watermark embedding method that aims to minimize the variations caused by embedding watermarks. introduced a black-box tracking mechanism for ownership verification. proposed a model watermarking framework that utilizes spatial invisible watermarking to protect image processing models. However, it has been observed that watermark embedding approaches are vulnerable to certain watermark removal methods. Our experiments employ a simple watermark embedding in the model to verify ownership by triggering misclassification. Through comprehensive experimental results, we demonstrate that the proposed CUPI-Domain exhibits resistance against common methods of watermark removal.\nApplicability authorization is an extension of usage authorization, where model owners typically employ a preset private key to encrypt the entire or partial network. This encryption ensures that only authorized users can obtain the private key and subsequently use the model. Various advanced methods have been developed for usage authorization. For instance, introduced an explicit locking mechanism that utilizes S-Boxes with strong cryptographic properties to secure each training parameter of a lightweight deep neural network. Unauthorized access without knowledge of the legitimate private key can significantly degrade the model's accuracy. analyzed and calculated the critical weight of deep neural network models and reduced time costs by encrypting these critical weights to protect against unauthorized use. In terms of applicability authorization, proposed a data-based approach called Non-Transferable Learning (NTL), which aims to preserve model performance on authorized data while intentionally degrading performance in other data domains. This method allows the model to act well within authorized domains while exhibiting reduced effectiveness when applied to unauthorized domains.\nIn contrast to these methods, we propose a novel approach that involves constructing a new class-dependent CUPI-Domain with infinite samples. This CUPI-Domain is designed to have features that are more similar to the authorized domain. By deliberately reducing the model's performance on both the CUPI-Domain and the target domain, we can achieve a tighter generalization bound for the well-trained model, thereby constraining the model's performance within the authorized domain."}, {"title": "2.2 Domain Transferring", "content": "In recent years, deep learning models have achieved revolutionary success in diverse areas of machine learning. To inherit the advantages of deep models, domain adaptive and domain generalization have"}, {"title": "3 METHODOLOGY", "content": "We first provide a comprehensive explanation of our proposed CUPI-Domain in Section 3.1, which serves as a barrier to restrict the model's performance within the authorized domain and diminish its feature recognition capabilities on unauthorized domains. Subsequently, we introduce external DIMB for storing and updating labeled pyramid features in Section 3.2. Additionally, we present two distinct solutions based on whether the unauthorized target domain is known or not: the target-specified CUPI-Domain and the target-free CUPI-Domain in section 3.3, these solutions are designed to safeguard the IP of the model."}, {"title": "3.1 Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain)", "content": "In the deep neural network model, the overall features extracted by the feature extractor include two abstract components, i.e., shared features, and private features . Shared features refer to semantic features which reflect the structural information of samples and play a leading role in sample recognition . Private features refer to a collection of subtle, weak semantically related cues in the feature, such as lighting conditions, texture, hue, color saturation, and background . Samples from different domains with the same task usually exhibit consistent semantic information while significant stylistic variations.\nMost previous works on domain adaptive and domain generalization have primarily focused on enhancing feature transferability between domains. This has involved strengthening the model's emphasis on shared features while suppressing private style features that may appear as disturbances. However, to ensure the protection of the model's IP, this paper aims to restrict the model's feature recognition capability by accentuating the private style features of the authorized domain through style transfer techniques. This emphasis leads to failure in recognizing irrelevant private style features on unauthorized domains.\nStyle transfer studies have conjectured that styles exhibit homogeneity and consist of repeated structural motifs. In this context, two images are considered to have similar styles if the features extracted by a trained classifier exhibit shared statistics , such as first- and second-order statistics, which are commonly used due to their computational efficiency. First- and second-order statistics refer to the mean and variance of the extracted features, which are called style features. Semantic features capture pure structural information without incorporating style cues. Following , the semantic features $f_{se}$ of the extracted feature $f$ can be obtained by removing style features as:\n$f_{se} = \\frac{f - \\mu(f)}{\\sigma(f)}$\nwhere $\\mu(f)$ and $\\sigma(f)$ denote the mean and variance of $f$. Furthermore, style can be re-assign by $f \\cdot \\gamma + \\beta$, where $\\gamma$ and $\\beta$ are learned parameters. Afterward, further explored adapting $f$ to an arbitrarily given style by using style features of another extracted feature instead of learned parameters.\nBuilding upon these concepts, we present a novel CUPI-Domain that combines style features from the authorized domain with semantic features from the CUPI-Domain. We achieve this by initializing the CUPI-Domain with the unauthorized domain and then randomly extracting style features from the authorized domain, integrating them with the semantic features of the CUPI-Domain. This process results in the private style features of the CUPI-Domain becoming more similar to those of the authorized domain. By employing this strategy, we effectively diminish the model's feature recognition capabilities on both the CUPI-Domain and the unauthorized domain. Consequently, we implicitly obstruct the pathway between the authorized and unauthorized domains, thereby confining the model's performance exclusively to the authorized domain.\nOur CUPI-Domain is generated by the CUPI-Domain generator, which is a lightweight, and plug-and-play module, as depicted in Fig. 2. $f_s$ and $f_i$ represent the deep features of the $l$-th feature extractor block in the authorized domain and the CUPI-Domain, respectively. To begin, $f_s$ and $f_i$ are concurrently fed into the CUPI-Domain generator. Subsequently, the mean $\\mu(f)$ and variance $\\sigma(f)$ of $f_s$ are calculated along the channel dimension to represent private style features of the authorized domain, followed by a 1\u00d71 convolution layer $Conv$. Next, removing style features of $f_i$. Finally, the $\\mu(f)$ and variance $\\sigma(f)$ of $f_s$ are multiplied and added channel-wisely as:"}, {"title": "3.2 Domain-Information Memory Banks (DIMB)", "content": "In our method, we employ external DIMB comprising multiple memory cells to store and update domain-dependent pyramid features via sample index. This enables us to obtain stable domain class features and domain class-wise style features throughout the training process, as depicted at the bottom of Fig. 3. In this section, we provide a comprehensive description of the workflow associated with our proposed DIMB."}, {"title": "3.2.1 Initialization Strategy of DIMB", "content": "Before training, the pre-trained feature extractor with frozen model parameters is utilized to initialize the DIMB as follows:\nSince the output features of the $l$-th feature extractor $f^l_s \\in R^{B \\times N \\times C}$ in the authorized domain contain rich semantic information with style clues, and feature vector $p_i \\in R^{B \\times CP}$ before the classifier layer contains refined class-discriminative information in the CUPI-Domain, we store $f^l_s$ and $p_i$ in the memory cell $M^l$"}, {"title": "3.2.2 Update Strategy of DIMB", "content": "During training, we directly discard those obsolete features $f^l_s$ and $p_i$ according to the index of the samples and replace them with the latest features, then calculate and update the corresponding features in memory cell $M^l = \\{M^l_N, M^l_{\\sigma}, M^K_\\mu, M^K_{\\sigma} \\} (l \\in 1, 2, ..., L)$ and $M_D = \\{M^N_p, M^K_P\\}$ as in the initialization strategy. Among them, $M^B$ stores $K$ class features, while $M^K_{\\mu}$ and $M^K_{\\sigma}$ store style features of $K$ classes, which are used for downstream computation. Furthermore, the proposed DIMB is only an external repository and does not participate in the back-propagation calculation of the network."}, {"title": "3.3 Model IP Protection with CUPI-Domain", "content": "We first introduce our solutions for model IP protection with a given unauthorized target domain, termed target-specified CUPI-Domain."}, {"title": "3.3.1 Target-Specified CUPI-Domain", "content": "Fig. 3 provides an overview of the entire framework trained using our proposed CUPI-Domain. The framework comprises L feature extractor blocks, L CUPI-Domain generators, L + 1 external DIMB, a bottleneck layer, and a classifier layer. The data from the authorized domain, CUPI-Domain, and unauthorized domain are denoted as $X_s$, $X_i$ (Initialized by $X_t$), and $X_t$, respectively. During the training process, $X_s$, $X_i$, and $X_t$ are simultaneously fed into the feature extractor blocks, followed by a CUPI-Domain generator. The classifier at the end of the network is responsible for predicting the category of the sample, and the corresponding prediction results are represented as $Y_s'$, $Y_i'$, and $Y_t'$, respectively.\nBased on the designed framework and DIMB, we proposed a novel joint loss function that incorporates the classification loss $L_{cls}$, style loss $L_{stl}$ and discriminative loss $L_{Dis}$ to enhance the distinction between authorized and unauthorized domains, while maintaining the classification performance of authorized domain. The overall loss function $L$ is as follows:\n$L = L_{cls} + L_{Stl} + L_{Dis}$"}, {"title": "3.3.2 Target-Free CUPI-Domain", "content": "To tackle scenarios where the unauthorized domain is unknown, we introduce the Target-free CUPI-Domain. In such cases, it is not feasible to directly incorporate the unauthorized domain and CUPI-Domain into the model training process. To address this challenge, a synthesized unauthorized domain can be employed instead. For instance, proposed a GAN-based approach that freezes parameters to generate synthesized samples in various directions as a substitute for the unauthorized domain training set. Although their method is capable of"}, {"title": "4 EXPERIMENT", "content": "We evaluate our proposed CUPI-Domain on several popular domain adaption/generation benchmarks:"}, {"title": "4.1 Datasets", "content": "Digit datasets: MNIST , USPS , SVHN and MNIST-M are widely used digit datasets, each consisting of ten digits ranging from 0 to 9. These datasets include samples extracted from various scenes, providing a diverse range of digit images for evaluation and analysis.\nCIFAR10 and STL10 are both ten-class classification datasets commonly used in computer vision research. To ensure consistency between the datasets, we adopt the processing procedure outlined by French et al. . This ensures that the classes in both datasets are aligned and can be directly compared during experimentation and evaluation.\nVisDA-2017 is a Synthetic-to-Real dataset that consists of training (VisDA-T) and validation (VisDA-V) sets, each containing samples from 12 different categories.\nOffice-Home-65 consists of images from four distinct domains: Artistic, Clipart, Product, and Real-World. Each domain contains 65 object categories, resulting in a total of 15,500 images in the dataset.\nDomainNet consists of images from six domains, including clipart, infograph, painting, quickdraw, real and sketch, with a total of 345 scene-level categories."}, {"title": "4.2 Implementation Details", "content": "The implementation of our comprehensive experiments is based on the platform Pytorch and an NVIDIA GeForce RTX 3090 GPU with 24GB of memory. The Adam optimizer with an initial learning rate of 0.0001 is adopted to optimize the model. The batch size for each domain and the epoch number is set to 32 and 30, respectively. To accommodate tasks of varying complexity, we employ different backbone architectures for various datasets followed by for fair comparison. Specifically, we used VGG-11 for digit datasets, VGG-19 for CIFAR10 & STL10 and VisDA-2017 , and SWIN for Office-Home-65 and DomainNet . Pre-trained models are used for a fair comparison. We set L to 4 for SWIN and 5 for VGG to align with the number of feature extractor blocks in each architecture. Consistent with standard evaluation protocols, we utilize accuracy (%) as the primary performance metric for each task."}, {"title": "4.3 Result of Target-Specified CUPI-Domain", "content": "We conducted experiments on digital datasets, as depicted in Table 1. The vertical axis represents the authorized domain, while the horizontal axis represents the unauthorized domain. We explored 16 transfer tasks by selecting various combinations of authorized and unauthorized domains from the four domains of the digital datasets. In each task, the data on the left of '$\\Rightarrow$' presents the test accuracy of supervised learning (SL) on the unauthorized domain, while the data on right of '$\\Rightarrow$' presents the test accuracy of CUPI-Domain on the unauthorized domain. 'Authorized/Unauthorized Drop' indicates the drop (relative drop) of CUPI-Domain relative to SL in authorized/unauthorized domains.\nFrom the results, we observe that the average drop of the CUPI-Domain on the unauthorized domain is 56.83 (86.89%), while the drop on the authorized domain is only 0.02 (0.02%). Comparatively, and exhibit lower average performance degradations on the unauthorized domain but higher on the authorized domain. We further conduct statistical significance tests achieved by different methods, denoted with * (CUPI-Domain vs. CUTI-Domain ) and \u25ca (CUPI-Domain vs. NTL ). Our proposed CUPI-Domain shows a statistical difference (p < 0.05) over and. These results indicate that CUPI-Domain effectively reduces the sample recognition ability of the model for the unauthorized domain while having minimal impact on the authorized domain."}, {"title": "4.4 Result of Ownership Verification", "content": "In this section, we focus on conducting ownership verification of the model by intentionally triggering classification errors. To this end, we apply a regular backdoor-based model watermark patch to the authorized domain dataset, followed by NTL . This processed authorized domain is then treated as the new unauthorized domain. The accuracy of SL and CUPI-Domain on the authorized domain with/without the watermark patch is presented in Table 3 and Supplemeatary Table 5.\nComparing the second and third columns of the table, we can observe that SL shows minimal disparity in accuracy with and with the watermark patch, indicating a lack of sensitivity to the watermark patch. Conversely, CUPI-Domain exhibits a significant reduction in accuracy on the unauthorized domain with watermark embedding. This performance difference can be effectively utilized for model ownership verification.\nAdditionally, we evaluate the robustness of CUPI-Domain against five state-of-the-art model watermark removal methods, including FTAL , RTAL , EWC , AU , and watermark overwriting followed by NTL . The last three columns depict the drop in accuracy between data with and without the watermark. Notably, CUPI-Domain, CUTI and NTL demonstrate effective resistance against watermark removal methods, with CUPI-Domain outperforming CUTI and NTL by approximately 4.7% and 8.2% in terms of performance respectively, and also demonstrating statistical difference (p < 0.05)."}, {"title": "4.5 Result of Target-free CUPI-Domain", "content": "In Section 3.3.2, we proposed the target-free CUPI-Domain to address the scenario where the unauthorized domain is unknown by leveraging synthesized samples as a substitute for the unauthorized domain training set. To evaluate the performance of our proposed target-free CUPI-Domain on digit datasets, we conducted 4 transfer tasks. For each task, we selected one authorized domain from the digit datasets and used the remaining unknown domains as the unauthorized domain testing sets. We report the 'Authorized/Unauthorized Drop' metric and statistical difference (p < 0.05), as presented in Table 4 (with more details in Supplementary Table 6).\nAnalyzing the results, we observe that CUPI-Domain exhibits the highest average drop on the test sets, with a degradation of 55.10 (83.77%). Additionally, CUPI-Domain demonstrates a lower drop in the unauthorized domain in comparison to CUTI and NTL , further highlighting its enhanced IP protection capability."}, {"title": "4.6 Result of Applicability Authorization", "content": "To assess the applicability authorization, we conduct experiments by constraining its generalization capabilities solely to the authorized domain. We adopted three evaluation schemes: 1) We introduce a specified watermark patch to the authorized domain (same as in Section 4.4), creating a new authorized domain training set. Subsequently, we combine the authorized domain, synthesized domain, and synthesized domain with the specified watermark patch to construct the unauthorized domain training set. 2) We use clean authorized data without a watermark patch as an authorized domain training set and then combine the authorized domain with a watermark patch, synthesized domain, and synthesized domains with a watermark patch to construct the unauthorized domain training set. 3) We consider clean authorized data without a watermark patch as an authorized domain training set and the synthesized domain as the unauthorized domain training set. During the testing phase, we evaluated the model's performance on several unknown domains, both with and without specified watermark patches.\nThe experimental results are reported in Table 5, with additional details available in Supplementary Table 11, 12, 13, 14, 15 and 16. The results of scheme 1 indicate that the proposed CUPI-Domain performs well on the authorized domain with the specific watermark patch but exhibits low accuracy on other unknown domains. In scheme 2, the proposed CUPI-Domain performs well only on the authorized domain without the watermark patch. When the watermark is considered insufficient to change the domain in scheme 3, the CUPI-Domain performs effectively on authorized domains, regardless of the presence of watermarks. This aligns with our expectation that the model's generalization ability is successfully constrained within the authorized domain, regardless of the presence or absence of the watermark patch.\nFurthermore, in scheme 1/2/3, our proposed CUPI-Domain achieves an average drop of 88.57 (88.71%) / 85.50 (85.71%) / 86.36 (87.16%), surpassing CUTI's drop (relative drop) of 83.75 (84.27%) / 85.85 (83.27%) / 84.00 (85.05%) and NTL's drop (relative drop) of 81.03 (81.81%) / 80.75 (81.28%) / 81.74 (82.88%) with a statistical difference (p < 0.05). This improvement is attributed to the construction of the CUPI-Domain, where we leverage infinite samples that closely resemble the authorized domain to create a more compact generalization boundary. Consequently, CUPI-Domain exhibits a stronger ability to protect IP compared to NTL , which relies solely on limited features from the source and target domains for distance maximization."}, {"title": "4.7 Ablation Study of Components", "content": "To gain further insights into the effectiveness of each component in our proposed CUPI-Domain, we conduct an ablation study on five random tasks, as detailed in Table 6. The vertical axis represents experimental settings with different component combinations (indicated by '+'), and the check-mark '$\\surd$' indicates the corresponding loss function on the horizontal axis. For each task, we randomly selected two domains from each dataset, designating one as the authorized domain (left of '$\\Rightarrow$') and the other as the unauthorized domain (right of '$\\Rightarrow$'), as shown on the horizontal axis.\nThe complexity of the dataset varies, leading to differences in the difficulty of feature extraction and the construction of the CUPI-Domain. Consequently, the accuracy of CUPI-Domain with different partial combinations on the unauthorized domain also varies. However, three key observations remain consistent across all tasks: firstly, the accuracy of different variants on the unauthorized domain is significantly lower than that of SL, indicating the positive impact of each component; secondly, the variants with DIMB perform better than those without it, indicating that DIMB can obtain stable domain class features and domain class-wise style features, further amplifying the difference between authorized domains and unauthorized domains, thereby reducing the identification of unauthorized domains. thirdly, the complete combinations consistently achieve the lowest accuracy on the unauthorized domain,"}, {"title": "4.8 Ablation Study of Backbone", "content": "In this section, we investigate the IP protection capability of the proposed CUPI-Domain in combination with different backbone architectures, as illustrated in Fig. 6. We selected four distinct backbone networks, including VGG , ResNet , Inception-v3 , and SWIM . We perform a total of five transfer tasks based on the number of available datasets. For each transfer task, we randomly selected two domains from each dataset, designating one as the authorized domain (left of '$\\Rightarrow$') and the other as the unauthorized domain (right of '$\\Rightarrow$'). In each subfigure, the pink bars indicate the accuracy of SL on the unauthorized domain, while the blue bars depict the accuracy of CUPI-Domain on the unauthorized domain.\nBased on the experimental results, we can observe a consistent pattern: irrespective of the dataset, CUPI-Domain consistently outperforms SL in reducing the recognition ability on the unauthorized domain when integrated with various backbones. Furthermore, when combined with SWIM , CUPI-Domain exhibits higher accuracy on the authorized domain and lower accuracy on the unauthorized domain. This can be attributed to the stronger feature extraction ability of SWIM on complex datasets compared to the other backbones. As a result, when integrated with SWIM , CUPI-Domain becomes more similar to the authorized domain, leading to a more compact representation of the model's performance within the authorized domain. This signifies a stronger IP protection ability with a stronger feature extractor."}, {"title": "5 DISCUSSION", "content": "In the target-specified scenario, CUPI-Domain effectively diminishes the identification ability of unauthorized domains by constraining the generalization boundary of the authorized domain while preserving its universality. However, in target-free scenarios, where the distribution of unauthorized domains varies, the IP protection mechanism of the model naturally results in a certain degree of loss of universality.\nThe trade-off between protection and generalization is a critical consideration. While our framework prioritizes model IP protection, it is equally imperative to ensure that the protected model remains effective across diverse deployment scenarios. We acknowledge the need for further investigation and research to address this trade-off, such as exploring methods to enhance the generalization ability of protected models through additional training or data augmentation strategies. This area is one that we plan to delve into in future work to ensure that our framework remains practical and effective in real-world applications."}, {"title": "5.2 Attacks from Input Data", "content": "The design of our proposed CUPI-Domain is grounded in the deep features of the domain, extracted by the feature extractor of the backbone. By emphasizing the private features of authorized domains, CUPI-Domain can establish more restrictive generalization boundaries for trained models, implicitly preventing unauthorized transfer to unauthorized domains with irrelevant private-style features. In this section, we further explore whether CUPI-Domain can resist input-based IP attacks.\nRecently, several image translation methods based on CycleGAN have been proposed to translate images between two different domains, aiming to reduce the domain gap . To verify the resistance of CUPI-Domain to such methods, we applied CycleGAN to generate transformed images for each task pair (i.e., any two different domains with the same task) and tested the corresponding trained CUPI-Domain, CUTI-Domain and NTL on these images to assess their IP protection capabilities. The experimental results of the trained target-specified model and target-free model are presented in Table 7 (with more details in Supplementary Table 17).\nTable 7 showcases the average accuracy of the trained target-specified/free models on translated images. The mean accuracy of different methods ranges from 12% to 25%, demonstrating poor recognition ability. These results suggest a certain level of resistance to input-based attacks of these IP protection methods, with CUPI-Domain demonstrating the best performance with a statistical difference (p < 0.05). This underscores the effectiveness of CUPI-Domain in preserving the IP of the model against potential adversarial attacks."}, {"title": "5.3 Attacks from Model Stealing", "content": "We further verify the ability of the proposed CUPI-Domain to resist some of the latest model stealing attack methods"}, {"title": "6 CONCLUSION", "content": "Protecting model intellectual property (IP) in the field of artificial intelligence is a significant and crucial challenge, and its significance cannot be overstated. To tackle this issue, we propose a novel approach called CUPI-Domain, which acts as a protective barrier to confine the model's performance within authorized domains."}]}