{"title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep Model", "authors": ["Lianyu Wang", "Meng Wang", "Huazhu Fu", "Daoqaing Zhang"], "abstract": "Model intellectual property (IP) protection has attracted growing attention as science and technology advancements stem from human intellectual labor and computational expenses. Ensuring IP safety for trainers and owners is of utmost importance, particularly in domains where ownership verification and applicability authorization are required. A notable approach to safeguarding model IP involves proactively preventing the use of well-trained models of authorized domains from unauthorized domains. In this paper, we introduce a novel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which serves as a barrier against illegal transfers from authorized to unauthorized domains. Drawing inspiration from human transitive inference and learning abilities, the CUPI-Domain is designed to obstruct cross-domain transfers by emphasizing the distinctive style features of the authorized domain. This emphasis leads to failure in recognizing irrelevant private style features on unauthorized domains. To this end, we propose novel CUPI-Domain generators, which select features from both authorized and CUPI-Domain as anchors. Then, we fuse the style features and semantic features of these anchors to generate labeled and style-rich CUPI-Domain. Additionally, we design external Domain-Information Memory Banks (DIMB) for storing and updating labeled pyramid features to obtain stable domain class features and domain class-wise style features. Based on the proposed whole method, the novel style and discriminative loss functions are designed to effectively enhance the distinction in style and discriminative features between authorized and unauthorized domains, respectively. Moreover, we provide two solutions for utilizing CUPI-Domain based on whether the unauthorized domain is known: target-specified CUPI-Domain and target-free CUPI-Domain. By conducting comprehensive experiments on various public datasets, we validate the effectiveness of our proposed CUPI-Domain approach with different backbone models. The results highlight that our method offers an efficient model intellectual property protection solution.", "sections": [{"title": "1 INTRODUCTION", "content": "EEP Neural networks have made significant strides in diverse areas of machine learning. However, recent achievements heavily relied on abundant and high-quality data [1], [2], dedicated training resources [3], [4], and meticulous manual fine-tuning [5], [6]. Acquiring a proficiently trained deep model demands substantial time and effort in practice. While some DNN models, such as VGG [7], Inception-v3 [8], and SWIN [9], are publicly available for non-commercial use, many owners of models used in commercial applications prefer to keep their trained DNN models private. This is often due to business considerations and concerns related to privacy and security, especially in critical applications like autonomous driving, face recognition, and intrusion detection [10].\nUnfortunately, once these models are sold, they can be easily copied and redistributed, infringing on the interests of the developers and potentially increasing security risks in safety-critical applications, causing incalculable damage. For instance, in commercial Machine Learning as a Service (MLaaS) platforms [11], well-trained deep models have high business value. They should be treated as the intellectual property (IP) of their creators/owners. When these models are uploaded to public platforms or deployed as remote services on the cloud, malicious users may steal them for financial gain. One of the most concerning threats is \"Will releasing the model make it easy for the main competitor to copy this new feature and hurt owner differentiation in the market?\". Therefore, it is crucial to regard and protect these models as valuable scientific and technological intellectual property (IP) [12], [13], [14].\nTraditionally, model owners design, train, deliver, and deploy efficient deep models based on the authorized domain (represented by blue squares in Fig. 1). They grant specific users the right to use their well-trained models, enabling them to obtain correct predictions on the authorized domain, as depicted in Fig. 1 (a). However, since these models are trained with the overall features of the authorized domain, they may inadvertently cover unauthorized domains as well. Here, the unauthorized domain refers to a domain that shares the same task but exhibits different features compared to the authorized domain. This difference is primarily attributed to variations in the device, which in real cases is associated with the model owner rather than the user. This coverage has led to the emergence of freelancers who illegally steal well-trained deep models and deploy them on unauthorized domains (indicated by red squares in Fig. 1). They process the models using techniques such as domain adaptation [15], [16] or domain generation [17], [18] to obtain correct predictions and claim these models as their own products for profit. Such behavior infringes on the interests of model developers, and may even increase the potential security risks on safety-critical applications, causing incalculable damage and hindering the long-term development of deep model [19]. Thus, protecting the rights of model owners and ensuring the safety of model deployment is essential.\nComprehensive model IP protection for deep models includes both ownership verification and applicability authorization [12], [13], [14], [20]. Ownership verification refers to \"Who can use the model\u201d. Model owners can grant usage permission to specific users, and any other users will be infringing on the owner's IP rights when employing the model [12], [13]. However, it is important to acknowledge that there is no guarantee that the trained model will remain secure and not be leaked. There are two potential risks to consider: 1) Cyber hackers may attempt to steal the model from the cloud storage or other platforms where it is stored. 2) Specific users who have been granted permission may transfer the model to unauthorized or illegal users, which undermines the control and ownership of the model, potentially leading to misuse. These risks highlight the importance of implementing ownership verification mechanisms to protect against unauthorized access, sharing, and misuse of trained models. Common ownership verification mechanisms include embedding watermarks [21], [22], utilizing model fingerprints [23], or employing predefined triggers [24]. However, it is essential to recognize that these methods can be vulnerable to various techniques such as fine-tuning [25], classifier retraining [25], elastic weight consolidation algorithms [26], and watermark overwriting. These vulnerabilities can potentially weaken the effectiveness of the model's IP protection measures. Therefore, it is crucial to explore and develop more robust ownership verification techniques.\nApplicability authorization refers to \"Where can use the model\u201d. The model owner trains the deep model on the authorized domain and grants specific users the right to utilize the model within that domain, ensuring correct predictions [14], [20]. However, in practice, legal users and illegal users may employ techniques such as domain adaptive [15], [16], domain generalization [17], [18], etc., to process the model and utilize it on other domains with similar tasks [14], thereby achieving usable performance. This kind of infringement is not only easier to carry out but also more common and often concealed. This poses a significant challenge for maintaining control over the model's usage and protecting the IP of the model owner. Therefore, it is necessary to develop a more robust applicability authorization mechanism that restricts the model's performance to the tasks specified by the owner, ensuring that the model's performance on unauthorized domains is not superior to that of an untrained model. This will discourage unauthorized users from attempting to steal the model. To achieve this, a Non-Transferable Learning (NTL) method is proposed [14], which uses an estimator with a characteristic kernel from Reproducing Kernel Hilbert Spaces to approximate and increase the maximum mean difference between two distributions on finite samples. However, the authors only considered using limited samples to increase the mean distribution difference of features between domains and ignored outliers. The convergence region of NTL [14] is not tight enough. Moreover, the calculation of the maximum mean difference is class-independent, which reduces the model's feature recognition ability in the authorized domain to a certain extent.\nTo address these challenges, we propose a novel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which serves as a barrier against illegal transfers from authorized to unauthorized domains, as depicted in Fig 1 (b). Our proposed CUPI-Domain considers the overall features of each domain, comprising two components: shared features and private features. Shared features refer to semantic features, while private features include stylistic cues such as texture, saturation, perspective, brightness, background, etc. Initialized with the unauthorized domain, our proposed CUPI-Domain particularly emphasize the private features of the authorized domain, strategically fusing them with its semantic features, constructing richer style features for adaptively style augmentation. By intentionally diminishing the recognition capability of the CUPI-Domain, we can implicitly impede illegal transfers to unauthorized domains that possess irrelevant private style features, thereby leading to wrong predictions. Additionally, we design Domain-Information Memory Banks (DIMB) for storing and updating labeled pyramid features to obtain stable domain class features and domain class-wise style features. The novel style loss function is designed to effectively enhance the style difference between the authorized and unauthorized domains, whereas the discriminative loss functions are tailored to optimize the discriminative difference between the authorized/unauthorized domain and the CUPI-Domain. Moreover, we further develop two distinct solutions for leveraging the CUPI-Domain, depending on whether the unauthorized domain is known or not: target-specified CUPI-Domain and target-free CUPI-Domain.\nIn general, we highlight our five-fold contributions:\nWe propose a novel IP protection approach, named CUPI-Domain, to block illegal transfers from authorized to unauthorized domains. Meanwhile, CUPI-Domain generators are designed to generate labeled and style-rich CUPI-Domain by emphasizing the private features of the authorized domain, implicitly leading to failure in recognizing irrelevant private style features on unauthorized domains.\nWe introduce externel DIMB to store specified features to obtain stable domain class features and domain class-wise style features for the subsequent computation.\nStyle and discriminative loss functions are designed to further improve inter-domain differences while maintaining semantic consistency between the unauthorized domain and CUPI-Domain.\nTwo distinct solutions are developed for utilizing CUPI-Domain based on whether the unauthorized domain is known or not: target-specified CUPI-Domain and target-free CUPI-Domain.\nFinally, comprehensive experimental results on several public datasets demonstrate that CUPI-Domain effectively reduces the recognition ability on unauthorized domains while maintaining strong recognition on authorized domains. As a plug-and-play module, our CUPI-Domain can be easily implemented within different backbones and provide efficient solutions.\nThis paper is based on and extends our previous CVPR2023 version [27] in the following aspects. 1) We have implemented more standardized generators known as the CUPI-Domain generators. This generator effectively eliminates the style features of the input CUPI-Domain and subsequently integrates them with the style features of the authorized domain, resulting in enhanced similarity between the two. 2) We also designed external DIMB, which store and update labeled pyramid features to obtain stable domain class features and domain class-wise style features for the subsequent computation. 3) We designed novel style loss function and discriminative loss functions to improve the style difference and discriminative difference between authorized and unauthorized domains, while maintain the semantic consistency between the unauthorized domain and CUPI-Domain. 4) We conducted additional experiments on the Office-home-65 (65 categories, 4 domains) [28], and DomainNet (345 categories, 6 domains) [29] to demonstrate the continued effectiveness of our method as the data complexity increases. 5) We have invested substantial efforts in enhancing the presentation and organization of our paper, specifically focusing on the introduction, framework, key results, and discussion. We have provided more comprehensive explanations in the introduction section to offer a clearer understanding of the research context. Beside, we have introduced several new sections to elaborate on our novel framework, encompassing the method formulation, corresponding technical components, and loss functions. Additionally, we have diligently rewritten multiple sections to improve readability and provide more detailed explanations, particularly in the sections addressing quantitative comparisons, ablation experiments and discussion. These revisions aim to enhance the clarity and coherence of our paper, offering a more comprehensive and insightful reading experience."}, {"title": "2 RELATED WORK", "content": "Currently, there are two primary categories of methods for protecting model IP: ownership verification and applicability authorization. In terms of ownership verification, watermark embedding [30] is a widely used classic method. Kuribayashi et al. [31] proposed a quantifiable watermark embedding method that aims to minimize the variations caused by embedding watermarks. Adi et al. [32] introduced a black-box tracking mechanism for ownership verification. Zhang et al. [33] proposed a model watermarking framework that utilizes spatial invisible watermarking to protect image processing models. However, it has been observed that watermark embedding approaches are vulnerable to certain watermark removal methods. Our experiments employ a simple watermark embedding in the model to verify ownership by triggering misclassification. Through comprehensive experimental results, we demonstrate that the proposed CUPI-Domain exhibits resistance against common methods of watermark removal.\nApplicability authorization is an extension of usage authorization, where model owners typically employ a preset private key to encrypt the entire or partial network. This encryption ensures that only authorized users can obtain the private key and subsequently use the model. Various advanced methods have been developed for usage authorization. For instance, Alam et al. [34] introduced an explicit locking mechanism that utilizes S-Boxes with strong cryptographic properties to secure each training parameter of a lightweight deep neural network. Unauthorized access without knowledge of the legitimate private key can significantly degrade the model's accuracy. Song et al. [35] analyzed and calculated the critical weight of deep neural network models and reduced time costs by encrypting these critical weights to protect against unauthorized use. In terms of applicability authorization, Wang et al. [14] proposed a data-based approach called Non-Transferable Learning (NTL), which aims to preserve model performance on authorized data while intentionally degrading performance in other data domains. This method allows the model to act well within authorized domains while exhibiting reduced effectiveness when applied to unauthorized domains.\nIn contrast to these methods, we propose a novel approach that involves constructing a new class-dependent CUPI-Domain with infinite samples. This CUPI-Domain is designed to have features that are more similar to the authorized domain. By deliberately reducing the model's performance on both the CUPI-Domain and the target domain, we can achieve a tighter generalization bound for the well-trained model, thereby constraining the model's performance within the authorized domain."}, {"title": "2.2 Domain Transferring", "content": "In recent years, deep learning models have achieved revolutionary success in diverse areas of machine learning. To inherit the advantages of deep models, domain adaptive [15], [16] and domain generalization [17], [18] have been proposed to rapidly improve the performance of well-trained deep models on target datasets.\nDomain adaptive [15], [16] involves transferring a model from a labeled source domain to an unlabeled but relevant target domain, with the target domain's data being accessible during the training process [36]. Sun et al. [37] and Zhuo et al. [38] minimized inter-domain differences by aligning the second-order statistics of the source and target distributions. Saito et al. [39] adopted a new adversarial paradigm, where the adversarial way occurs between the feature extractor and the classifier, rather than between the feature extractor and the domain discriminator.\nDomain generalization [17], [18] differs from domain adaptive in that the target domain is inaccessible during training phase [40], [41]. Tobin et al. [40] used domain randomization to generate more training data from simulated environments for generalization in real environments. Prakash et al. [41] further considered the structure of the scene when randomly placing objects for data generation, enabling the neural network to learn how to utilize context when detecting objects. Recently, several methods have been proposed and effectively applied to cross-domain applications. These methods aim to find an intermediate state that lies between the source domain and the target domain. By emphasizing the similarity between domains, they enhance the transferability of the model [42], [43], [44], [45], [46].\nIn contrast, the objective of our proposed CUPI-Domain is to explore an intermediate state that accentuates the distinctions between the authorized and unauthorized domains, thereby limiting the transferability of the model and ensuring the protection of the model owners' intellectual property with respect to their scientific and technological achievements."}, {"title": "3 METHODOLOGY", "content": "We first provide a comprehensive explanation of our proposed CUPI-Domain in Section 3.1, which serves as a barrier to restrict the model's performance within the authorized domain and diminish its feature recognition capabilities on unauthorized domains. Subsequently, we introduce external DIMB for storing and updating labeled pyramid features in Section 3.2. Additionally, we present two distinct solutions based on whether the unauthorized target domain is known or not: the target-specified CUPI-Domain and the target-free CUPI-Domain in section 3.3, these solutions are designed to safeguard the IP of the model."}, {"title": "3.1 Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain)", "content": "In the deep neural network model, the overall features extracted by the feature extractor include two abstract components, i.e., shared features, and private features [47], [48]. Shared features refer to semantic features which reflect the structural information of samples and play a leading role in sample recognition [49]. Private features refer to a collection of subtle, weak semantically related cues in the feature, such as lighting conditions, texture, hue, color saturation, and background [50]. Samples from different domains with the same task usually exhibit consistent semantic information while significant stylistic variations.\nMost previous works on domain adaptive and domain generalization have primarily focused on enhancing feature transferability between domains. This has involved strengthening the model's emphasis on shared features while suppressing private style features that may appear as disturbances. However, to ensure the protection of the model's IP, this paper aims to restrict the model's feature recognition capability by accentuating the private style features of the authorized domain through style transfer techniques. This emphasis leads to failure in recognizing irrelevant private style features on unauthorized domains.\nStyle transfer studies have conjectured that styles exhibit homogeneity and consist of repeated structural motifs. In this context, two images are considered to have similar styles if the features extracted by a trained classifier exhibit shared statistics [50], [51], such as first- and second-order statistics, which are commonly used due to their computational efficiency. First- and second-order statistics refer to the mean and variance of the extracted features, which are called style features. Semantic features capture pure structural information without incorporating style cues. Following Dumoulin et al. [49], the semantic features $f_{se}$ of the extracted feature $f$ can be obtained by removing style features as:\n$f_{se} = \\frac{f - \\mu(f)}{\\sigma(f)}$\nwhere $\\mu(f)$ and $\\sigma(f)$ denote the mean and variance of $f$. Furthermore, style can be re-assign by $f \\cdot \\gamma + \\beta$, where $\\gamma$ and $\\beta$ are learned parameters. Afterward, Huang et al. [50] further explored adapting $f$ to an arbitrarily given style by using style features of another extracted feature instead of learned parameters.\nBuilding upon these concepts, we present a novel CUPI-Domain that combines style features from the authorized domain with semantic features from the CUPI-Domain. We achieve this by initializing the CUPI-Domain with the unauthorized domain and then randomly extracting style features from the authorized domain, integrating them with the semantic features of the CUPI-Domain. This process results in the private style features of the CUPI-Domain becoming more similar to those of the authorized domain. By employing this strategy, we effectively diminish the model's feature recognition capabilities on both the CUPI-Domain and the unauthorized domain. Consequently, we implicitly obstruct the pathway between the authorized and unauthorized domains, thereby confining the model's performance exclusively to the authorized domain.\nOur CUPI-Domain is generated by the CUPI-Domain generator, which is a lightweight, and plug-and-play module, as depicted in Fig. 2. $f_s$ and $f_i$ represent the deep features of the l-th feature extractor block in the authorized domain and the CUPI-Domain, respectively. To begin, $f_s$ and $f_i$ are concurrently fed into the CUPI-Domain generator. Subsequently, the mean $\\mu(f)$ and variance $\\sigma(f)$ of $f_s$ are calculated along the channel dimension to represent private style features of the authorized domain, followed by a 1\u00d71 convolution layer $Conv$. Next, removing style features of $f_i$. Finally, the $\\mu(f)$ and variance $\\sigma(f)$ of $f_i$ are multiplied and added channel-wisely as:\n$f_i^l = \\frac{f^l_i}{\\sigma(f^l_i)} \\otimes Conv(\\sigma(f^l_s)) \\oplus Conv(\\mu(f^l_s))$.\nSubsequently, $f_i^{l}$, $f_{il}'$, and $f^l_{st}$ are simultaneously fed into the l + 1-th feature extractor block in parallel. Through continuous feature fusion, as shown in Fig. 2, CUPI-Domain generator can build style-rich latent feature space for each class and then construct a labeled CUPI-Domain containing similar private styles to the authorized domain."}, {"title": "3.2 Domain-Information Memory Banks (DIMB)", "content": "In our method, we employ external DIMB comprising multiple memory cells to store and update domain-dependent pyramid features via sample index. This enables us to obtain stable domain class features and domain class-wise style features throughout the training process, as depicted at the bottom of Fig. 3. In this section, we provide a comprehensive description of the workflow associated with our proposed DIMB."}, {"title": "3.2.1 Initialization Strategy of DIMB", "content": "Before training, the pre-trained feature extractor with frozen model parameters is utilized to initialize the DIMB as follows:\nSince the output features of the l-th feature extractor $f_l \\in \\mathbb{R}^{BNC}$ in the authorized domain contain rich semantic information with style clues, and feature vector $p_i \\in \\mathbb{R}^{B \\times C^P}$ before the classifier layer contains refined class-discriminative information in the CUPI-Domain, we store $f_l$ and $p_i$ in the memory cell $M_l$"}, {"title": "3.2.2 Update Strategy of DIMB", "content": "During training, we directly discard those obsolete features $f_l$ and $p_i$ according to the index of the samples and replace them with the latest features, then calculate and update the corresponding features in memory cell $M^l = \\{M^N,M_\\sigma^N, M^K, M_\\sigma^K \\}$ ($l \\in 1, 2, . . ., L$) and $M^D = \\{M^N, M^S\\}$ as in the initialization strategy. Among them, $M_B^K$ stores K class features, while $M_\\sigma^K$ and $M_\\mu^K$ store style features of K classes, which are used for downstream computation. Furthermore, the proposed DIMB is only an external repository and does not participate in the back-propagation calculation of the network."}, {"title": "3.3 Model IP Protection with CUPI-Domain", "content": "We first introduce our solutions for model IP protection with a given unauthorized target domain, termed target-specified CUPI-Domain."}, {"title": "3.3.1 Target-Specified CUPI-Domain", "content": "Fig. 3 provides an overview of the entire framework trained using our proposed CUPI-Domain. The framework comprises L feature extractor blocks, L CUPI-Domain generators, L + 1 external DIMB, a bottleneck layer, and a classifier layer. The data from the authorized domain, CUPI-Domain, and unauthorized domain are denoted as $X_s$, $X_i$ (Initialized by $X_t$), and $X_t$, respectively. During the training process, $X_s$, $X_i$, and $X_t$ are simultaneously fed into the feature extractor blocks, followed by a CUPI-Domain generator. The classifier at the end of the network is responsible for predicting the category of the sample, and the corresponding prediction results are represented as $Y_s^\\prime$, $Y_i^\\prime$, and $Y_t^\\prime$, respectively.\nBased on the designed framework and DIMB, we proposed a novel joint loss function that incorporates the classification loss $L_{cls}$, style loss $L_{stl}$ and discriminative loss $L_{Dis}$ to enhance the distinction between authorized and unauthorized domains, while maintaining the classification performance of authorized domain. The overall loss function $L$ is as follows:\n$L = L_{cls} + L_{Stl} + L_{Dis}$"}, {"title": "The classification loss function", "content": "The classification loss function $L_{cls}$ is devised to enhance the class recognition capability within the authorized domain while reducing the class recognition ability of the CUPI-Domain and unauthorized domain, which consists of the respective domain loss functions of the authorized domain $L_s$, CUPI-Domain $L_i$ and unauthorized domain $L_t$, as:\n$L_{cls} = L_s - L_i - L_t$.\nFor all three domains, the network can be trained using the Kullback-Leibler divergence loss function, which serves as a measure of dissimilarity between the predicted distribution and the target distribution:\n$L_s = \\alpha * KL(Y_s^\\prime \\| Y_s)$,\n$L_i = \\alpha * KL(Y_i^\\prime \\| Y_i)$,\n$L_t = \\alpha * KL(Y_t^\\prime \\| Y_t)$,\nwhere $KL(\\cdot)$ stands for Kullback\u2013Leibler divergence, $Y_s$, $Y_i$ and $Y_t$ denote the label of the authorized domain, CUPI-Domain and the unauthorized domain, respectively. The rising factor $\\alpha = (\\frac{epoch\\_number}{total\\_epoch})^{0.9}$ is used to speed up the convergence of the model on the authorized domain in the early stage of training. Additionally, to mitigate the impact of infinite amplification, an upper bound is set to limit the negative loss function, as suggested by Wang et al. [14].\nThe style loss function $L_{stl}$ is designed based on pyramid DIMB to enhance the distinction in style between authorized and other domains. It achieves this by incorporating stable multi-scale authorized domain style information, and real-time multi-scale style information of CUPI-Domain and unauthorized domain:\n$L_{Stl} = -\\alpha * \\sum_{l=1}^{L}MSE(M_\\mu^K[Y_s], \\mu(f_l^s)) -\\alpha * \\sum_{l=1}^{L}MSE(M_\\sigma^K[Y_i], \\sigma(f_l^s))$.\nWhere $LMSE$ denotes the mean squared error. The discriminative loss $L_{Dis}$ is used to maintain the semantic consistency between the restyled CUPI-Domain and the unauthorized domain while enhancing the difference between CUPI-Domain and the authorized domain, which is defined as follows:\n$L_{Dis} = \\alpha * MSE(M_B[Y_s], P_s) + \\alpha * MSE(M_B[Y_t], P_t)$.\nWe summarize the strategy of our proposed target-specified CUPI-Domain in Supplementary Algorithm 1."}, {"title": "3.3.2 Target-Free CUPI-Domain", "content": "To tackle scenarios where the unauthorized domain is unknown, we introduce the Target-free CUPI-Domain. In such cases, it is not feasible to directly incorporate the unauthorized domain and CUPI-Domain into the model training process. To address this challenge, a synthesized unauthorized domain can be employed instead. For instance, Wang et al. [14] proposed a GAN-based approach that freezes parameters to generate synthesized samples in various directions as a substitute for the unauthorized domain training set. Although their method is capable of producing high-quality synthesized samples, the generator's direction is predetermined, leading to certain limitations. Additionally, Huang et al. [50] developed an adaptive instance normalization (AdaIN) technique based on GAN, which can generate synthesized images with a specific style to complement a given content image.\nTo fully leverage the benefit, we introduce Gaussian noise to the AdaIN technique, enabling the generation of synthesized samples with random styles. Finally, we combine the synthesized samples obtained by the two aforementioned methods (e.g., GAN and AdaIN) to replace the unauthorized domain training set and initialize the CUPI-Domain.\nIt is important to emphasize that our focus is on evaluating the effectiveness of the CUPI-Domain in preventing unauthorized feature transfer specifically within the context of synthesized images. Our primary focus remains on reducing the feature recognition ability of the model on unauthorized domains while preserving it on the authorized domain. Any data synthesis method could be utilized in our work for generating the unauthorized domain.\nThe framework of the Target-free CUPI-Domain is consistent with the target-specified CUPI-Domain and the training process is detailed in Supplementary Algorithm 2.\nDuring the testing phase, the model is evaluated on the authorized domain test set and other unknown domains with the same task."}, {"title": "4 EXPERIMENT", "content": "We evaluate our proposed CUPI-Domain on several popular domain adaption/generation benchmarks:"}, {"title": "4.1 Datasets", "content": "Digit datasets: MNIST [52], USPS [53], SVHN [54] and MNIST-M [55] are widely used digit datasets, each consisting of ten digits ranging from 0 to 9. These datasets include samples extracted from various scenes, providing a diverse range of digit images for evaluation and analysis.\nCIFAR10 [56] and STL10 [57] are both ten-class classification datasets commonly used in computer vision research. To ensure consistency between the datasets, we adopt the processing procedure outlined by French et al. [58]. This ensures that the classes in both datasets are aligned and can be directly compared during experimentation and evaluation.\nVisDA-2017 [59] is a Synthetic-to-Real dataset that consists of training (VisDA-T) and validation (VisDA-V) sets, each containing samples from 12 different categories.\nOffice-Home-65 [28] consists of images from four distinct domains: Artistic, Clipart, Product, and Real-World. Each domain contains 65 object categories, resulting in a total of 15,500 images in the dataset.\nDomainNet [29] consists of images from six domains, including clipart, infograph, painting, quickdraw, real and sketch, with a total of 345 scene-level categories."}, {"title": "4.2 Implementation Details", "content": "The implementation of our comprehensive experiments is based on the platform Pytorch and an NVIDIA GeForce RTX 3090 GPU with 24GB of memory. The Adam optimizer with an initial learning rate of 0.0001 is adopted to optimize the model. The batch size for each domain and the epoch number is set to 32 and 30, respectively. To accommodate tasks of varying complexity, we employ different backbone architectures for various datasets followed by Wang et al. [14] for fair comparison. Specifically, we used VGG-11 [7] for digit datasets, VGG-19 [7] for CIFAR10 & STL10 and VisDA-2017 [59], and SWIN [9] for Office-Home-65 [28] and DomainNet [29]. Pre-trained models are used for a fair comparison. We set L to 4 for SWIN and 5 for VGG to align with the number of feature extractor blocks in each architecture. Consistent with standard evaluation protocols, we utilize accuracy (%) as the primary performance metric for each task."}, {"title": "4.3 Result of Target-Specified CUPI-Domain", "content": "We conducted experiments on digital datasets, as depicted in Table 1. The vertical axis represents the authorized domain, while the horizontal axis represents the unauthorized domain. We explored 16 transfer tasks by selecting various combinations of authorized and unauthorized domains from the four domains of the digital datasets. In each task, the data on the left of '\u21d2' presents the test accuracy of supervised learning (SL) on the unauthorized domain, while the data on right of '\u21d2' presents the test accuracy of CUPI-Domain on the unauthorized domain. 'Authorized/Unauthorized Drop' indicates the drop (relative drop) of CUPI-Domain relative to SL in authorized/unauthorized domains. Table 2 displays the 'Authorized/Unauthorized Drop' of CUPI-Domain, CUTI-Domain [27], and NTL [14].\nFrom the results, we observe that the average drop of the CUPI-Domain on the unauthorized domain is 56.83 (86.89%), while the drop on the authorized domain is only 0.02 (0.02%). Comparatively, CUTI-Domain [27] and NTL [14] exhibit lower average performance degradations on the unauthorized domain but higher on the authorized domain. We further conduct statistical significance tests achieved by different methods, denoted with *(CUPI-Domain vs. CUTI-Domain [27]) and (CUPI-Domain vs. NTL [14]). Our proposed CUPI-Domain shows a statistical difference (p < 0.05) over CUTI-Domain [27] and NTL [14]. These results indicate that CUPI-Domain effectively reduces the sample recognition ability of the model for the unauthorized domain while having minimal impact on the authorized domain.\nFig. 4 illustrates the performance on three different transfer tasks: CIFAR10 \u2192 STL10, STL10 \u2192 CIFAR10 and VisDA-T\u2192 VisDA-V. Each subgraph displays the accuracy of the corresponding method in the authorized domain, the accuracy in the unauthorized domain, and their drop (relative drop). SL achieves the highest classification accuracy on the unauthorized domain due to its wide generalization region. In contrast, the other three methods all reduced accuracy in the unauthorized domain, indicating the success of blocking the transfer pathway. Comparatively, CUPI-Domain shows higher degradation than others, demonstrating its superior ability to compress the model's generalization region.\nTo showcase the continued effectiveness of our method under increased numbers of categories and samples, we conducted experiments on Office-Home-65 [28] and DomainNet [29], as shown in Supplementary Table 1, 2, 3 and 4. As the data complexity increases, we observed a decrease in the transfer performance of SL on the unauthorized domain (left of the \u21d2) compared to the results presented in Table 1. This decline poses a challenge to further reduce the accuracy of the unauthorized domain. However, our proposed CUPI-Domain demonstrates consistent effectiveness, with relative degradation of 83.41% and 76.14% on Office-Home-65 [28] and DomainNet [29], respectively, exhibiting significantly better performance compared to CUTI-Domain [27]. Despite the fact that NTL [14] exhibits a relatively higher degradation on DomainNet [29], its performance experiences a significant drop of 16.06 (35.06%) in the authorized domain, which is deemed unacceptable. Besides, our CUPI-Domain achieve statistical difference (p < 0.05) over CUTI-Domain [27] and NTL [14]. This phenomenon can be attributed to the fact that the calculation of its maximum mean difference is class-independent, thereby reducing the model's feature recognition capability in the authorized domain to some extent, especially when the complexity of the dataset increases."}, {"title": "4.4 Result of Ownership Verification", "content": "In this section, we focus on conducting ownership verification of the model by intentionally triggering classification errors. To this end, we apply a regular backdoor-based model watermark patch to the authorized domain dataset, followed by NTL [14", "25": "RTAL [25", "26": "AU [26"}]}