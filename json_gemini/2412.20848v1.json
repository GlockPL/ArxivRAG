{"title": "Analog Alchemy: Neural Computation with In-Memory Inference, Learning and Routing", "authors": ["Yi\u011fit Demira\u011f"], "abstract": "As neural computation is revolutionizing the field of Artificial Intelligence (AI), rethinking the\nideal neural hardware is becoming the next frontier. Fast and reliable von Neumann architecture\nhas been the hosting platform for neural computation. Although capable, its separation of\nmemory and computation creates the bottleneck for the energy efficiency of neural computation,\ncontrasting the biological brain. The question remains: how can we efficiently combine memory\nand computation, while exploiting the physics of the substrate, to build intelligent systems? In this\nthesis, I explore an alternative way with memristive devices for neural computation, where the\nunique physical dynamics of the devices are used for inference, learning and routing. Guided by\nthe principles of gradient-based learning, we selected functions that need to be materialized, and\nanalyzed connectomics principles for efficient wiring. Despite non-idealities and noise inherent in\nanalog physics, I will provide hardware evidence of adaptability of local learning to memristive\nsubstrates, new material stacks and circuit blocks that aid in solving the credit assignment\nproblem and efficient routing between analog crossbars for scalable architectures. First, I address\nlimited bit precision problem in binary Resistive Random Access Memory (RRAM) devices for\nstable training. By introducing a new device programning technique that precisely controls the\nfilament growth process, we enhance the effective bit precision of these devices. Later, we prove\nthe versatility of this technique by applying it to novel perovskite memristors. Second, I focus on\nthe hard problem of online credit assignment in recurrent Spiking Neural Networks (SNNs) in the\npresence of memristor non-idealities. I present a simulation framework based on a comprehensive\nstatistical model of Phase Change Material (PCM) crossbar array, capturing all major device\nnon-idealities. Building upon the recently developed e-prop local learning rule, we demonstrate\nthat gradient accumulation is crucial for reliably implementing the learning rule with memristive\ndevices. Moreover, I introduce PCM-trace, a scalable implementation of synaptic eligibility\ntraces, a functional block demanded by many learning rules, using volatile characteristics by\nspecifically fabricated PCM devices. Third, I present our discovery of a novel memristor material\ncapable of switching between volatile and non-volatile modes. This reconfigurable memristor,\nbased on halide perovskite nanocrystals, offers a significant advancement in emerging memory\ntechnologies, enabling the implementation of both static and dynamic neural variables with the\nsame material and fabrication technique, while holding the world record in endurance. Finally,\nI introduce Mosaic, a memristive systolic architecture for in memory computing and routing.\nMosaic, trained with our novel layout-aware training methods, efficiently implements small-world\ngraph connectivity and demonstrates superior energy efficiency in spike routing compared to\nother hardware platforms.", "sections": [{"title": "INTRODUCTION", "content": "How do we imbue the spark of intelligence into lifeless computational physical substrates?\nThis question has been my quest, inspired by the early pioneers such as McCulloch and Pitts [1],\nAlan Turing [2] and von Neumann [3], who laid the foundation of modern neural computing. As\nthe field of AI progressively gains superiority in numerous benchmarks, the quest to understand\nintelligence and to rethink its ideal implementation on a physical substrate has never been more\npressing.\nWhile intelligence remains elusive with numerous definitions, learning\u00b9 seems to me to be\na cornerstone of intelligence. Whether it is natural or artificial, the intelligent agent must adapt\nto survive and replicate. Bacteria can learn to swim away from environments that lower the\nprobability of successful replication [4]. And Artificial Neural Network (ANN) architectures\nabsorbing the datasets better are preferred by AI researchers and industry [5]. This is a common\ntheme; intelligent systems should learn well to last. And any physical implementation of an\nintelligent system, likewise, needs to implement learning dynamics. However, the computational\ndemands of learning place an enormous burden on existing hardware.\nFor over 75 years, computing hardware has relied on the von Neumann architecture: syn-\nchronous, deterministic, binary logic driving a processing unit that interfaces with a separate\nmemory subsystem. This design excels at executing arbitrary sequential instructions, but it\nnecessitates constant shuttling of data between memory and compute. \u00b2 Memory hierarchies,\nwith their layers of progressively larger and slower storage, have been the stopgap solution, but\nfundamentally the bottleneck remains. This non-local memory access is a leading factor in the\nlatency and energy consumption of modern AI systems. In stark contrast, neural computation in\nbiology is inherently intertwined with memory, operating asynchronously, sparsely, and stochasti-\ncally. This calls for a fundamental rethinking of computing, where neural models and hardware\nare co-designed with locality and physics-awareness as first principles.\nAn Alternative Path\nThis thesis departs from the well-established path of digital accelerator design. The inherent noise\ntolerance of neural networks presents an opportunity to relax strict precision and determinism re-\nquirements in both compute and memory subsystems of digital electronics. In turn, this relaxation\nunlocks some exotic modes of computation, where subthreshold regime of transistors and raw\nphysics of novel materials can be exploited for neural computation and storage. Historically, this\nis the essence of neuromorphic engineering, where a deliberate trade-off can be made, favoring\nlow-power and scalability offered by statistical physical processes over theoretical precision of\nboolean algebra.\nThis thesis optimizes neural computation across Marr's computational, algorithmic, and\nimplementational levels [6], advocating for the co-design of neural models and hardware with\nlocality and physics-awareness as guiding principles. By pushing critical neural operations to\nthe fundamental level of material physics, engaging electrical, chemical, or even mechanical\nproperties, we explore a new frontier in low-power neural computing.\nSpecifically, we investigate the following key strategies, which are detailed in the subsequent\nsections:"}, {"title": "INTRODUCTION", "content": "\u00b2 dynamic process of adapting to the environmental pressure to improve the probability of survival or replication.\n\u00b2 partially due to the rapid time-multiplexing of resources.\n\u2022 Analog In-Memory Computing: We exploit the physics of volatile (temporary) or non-\nvolatile (permanent) materials to perform critical neural network operations directly within\nthe memory units. This non-von Neumann architecture fundamentally eliminates the need\nfor data movement between memory and compute units, which are traditionally decoupled\nsystems.\n\u2022 Local Learning: We depart from the computationally intensive Back-Propagation Through\nTime (BPTT) for training, opting instead for local and online gradient-based learning rules.\nThese rules inherently exhibit varying degrees of variance and bias in their estimates of the\ngradient of an objective function [7]. However, they offer significant advantages in hardware\nin terms of power efficiency and simplified implementation due to local availability of\nweight update signals and the elimination of the need for buffering intermediate values.\n\u2022 Analog In-Memory Routing: We bring out locally dense and globally sparse connectivity on\nneural networks. This connectivity prior allows high utilization of routers with non-volatile\nmaterials to efficiently transmit neural activations within cores of analog systolic arrays.\n\u2022 Physics-aware Training: We utilize data-driven optimization to counteract non-idealities\ninherent to analog technologies. This involves collecting extensive component measure-\nments to model their collective behavior, tailoring learning algorithms and circuits ac-\ncordingly. Additionally, we employ gradient-based architectural adaptations and weight\nre-parameterizations for robust on-chip inference and training. Our methods are validated\non small-scale fabrications to assess their on-device performance and scalability potential.\nIn the following section, I will explain memristors, the prima materia of our endeavor, that\nenables and unifies these strategies explored in this thesis.\nMemristors for Analog in Memory Neural Operations\nNeural network computation, biological or artificial, is fundamentally memory-centric. The\nhuman brain operates on O(10\u00b9\u2075) synapses [8], while Large Language Models (LLMs) like GPT-4\nperform non-linear operations on O(10\u00b9\u00b2) parameters [9]. Scaling laws reveal a direct link between\nparameter count and performance [10], suggesting increasing network size is a reliable path to\nimproved performance in future neural networks.\nGiven that memory requirements scale quadratically\nwith the number of layers, memory becomes the primary\ndesign constraint in neural computing hardware, impacting\nscalability, throughput, and power efficiency. \u00b3\nAn ideal memory system for neural computing should\nbe high density, low-energy, quick access, and on-chip.\nHowever, an ideal memory does not yet exist, as these\nrequirements are often conflicting. High density memories\n(i.e., DRAM, High Bandwidth Memory (HBM), 3D NAND\nflash) are off-chip with long time to access, faster memo-\nries that don't use capacitors (i.e. SRAM) are larger in area\ndue to transistor count, and high-bandwith memories (i.e.,\nHBM) are expensive as they require additional banks, ports\nand channels. That's why memory hierarchy exists, to ad-\ndress this trade-off by using multiple levels of progressively\nlarger and slower storage. Yet, the von Neumann bottleneck\npersists, with memory access to each layer in the hierarchy\n(i.e., DRAM \u2192 on-chip SRAM \u2192 local SRAM), incurring roughly an order of magnitude more\nenergy and latency penalty. Even in the ideal case of local SRAM, the cost of memory access\nexceeds that of computation by an order of magnitude. In-memory computing offers a compelling"}, {"title": "INTRODUCTION", "content": "\u00b3 On edge Tensor Processing Unit (TPU) for instance, memory access can consume over 90% of the total energy, throttle\nthroughput to below 10% peak capacity, and in general dominate the majority of chip area [12].\nalternative to data movement by processing data where it is stored, and one of the most promising\ntechnologies for in-memory computing is memristors.\nMemristors are two-terminal analog resistive memory devices capable of both computation\nand memory [13]. They have a unique ability to encode and store information in their electrical\nconductance, which can be altered based on the history of applied electrical pulses [14]. Because a\nsingle memristor's conductance can transition between multiple levels between Low Conductive\nState (LCS) and High Conductance State (HCS), it can store more than one bit of information\n(typically between to 3 and 5 bits) improving the memory density. \u2074 Various memristor types exist,\neach with distinct operating principles and advantages. For example, PCM relies on the contrasting\nelectrical resistance of amorphous and crystalline phases in chalcogenide materials [16, 17], RRAM\noperates by altering the resistance of dialectric material through the drift of oxygen vacancies or\nthe formation of conductive filaments [18], and Ferroelectric Random Access Memory (FeRAM)\nuses the polarization of ferroelectric materials to store and change information [19]. Although the\nelectrical interface to these types does not differ significantly (applying electrical pulses to read or\nprogram the conductance), these underlying mechanisms determine the device's switching speed,\nfootprint, stochasticity, endurance, and energy efficiency.\nWhen implementing functions with memristor forms, it is helpful to categorize by volatility.\nNon-volatile memristors retain conductance after programming, ideal for weight storage in neural\nnetworks. This property extends to in-memory computing to perform dot-product operations [20-\n22]. When an array of N memristive devices store vector elements  $G_i$  in their conductance states,\napplying voltages  $V_i$  to the devices and measuring the resulting currents  $I_i$ , the dot product\n$\\Sigma_{i=1}^{N} G_iV_i$  can be computed with O(1) complexity by Ohm's Law and Kirchoff's current law.\nThis principle extends to matrix-vector multiplication, enabling neural network inference directly\nthrough the analog substrate's physics. In-memory neural inference has been demonstrated in\nlarge prototypes like the PCM-based HERMES chip [23] and RRAM-based NeuRRAM chip [18].\nVolatile memristors, however, provide functionality beyond mere storage, and are often less\nexplored. Their conductance decay within 10 ms to 100 ms allows for continuous-time, stochastic\naccumulate-and-decay functions, approximating low-pass filtering, signal averaging, or time\ntracking. In neural computations, volatile memristors have been used to implement short-term\nsynaptic [24, 25] and neuronal dynamics [26, 27].\nWHY SPIKES? In this thesis, I focus on SNNs, where neural activations are represented as\ndiscrete pulses called spikes. This choice is primarily motivated by hardware considerations.\nSpikes enable extreme spatiotemporal sparsity, aligning well with asynchronous computation\nwhere energy consumption directly correlates with spiking events that trigger localized cir-\ncuits [28]. Mixed-signal spiking neuron circuits inherently act as sigma-delta Analog-to-Digital\nConverters (ADCs), converting analog neural computation into digital spikes [29] for noise-robust\ntransmission over long distances without signal degradation [30]. Additionally, spiking neu-\nrons seamlessly interface with reading and programming of memristive devices and also with\nevent-based sensors [31, 32] through Address-Event Representation (AER) [33] protocol. Spiking\ncommunication has even been proposed to mitigate severe heating issues in 3D fabrication using\nn-ary coding schemes [34].\nFrom a computational perspective, a common argument for spikes is efficient information\nencoding through precisely utilizing the temporal dimension (e.g., time-to-first-spike [35], phase-\ncoding [36], or inter-spike-interval [37]). Furthermore, the spiking framework allows to formulate\nhypotheses about biophysical spike-based learning mechanisms in the brain [38-40] and explore\nthe computational capabilities of spiking neural networks [41], as demonstrated in our recent\nwork. While a comprehensive exploration of the advantages of spikes is beyond this thesis's scope,\nthe primary focus here is their potential for energy-efficient communication on analog substrates,\nas their unique computational benefits remain to be conclusively demonstrated."}, {"title": "INTRODUCTION", "content": "\u2074 These attributes are interestingly similar to biological synapses, where the synaptic efficacy is modulated by the history\nof pre- and post-synaptic activity, and is suggested to take 26 distinguishable strengths (correlated with spine head\nvolume) [15].\nState of the Art\nDespite their potential, memristors are not without their challenges for neural computing. In\nthis section, I will outline some of these challanges in inference, learning and routing, and\nstate-of-the-art attempts to address them.\nLimited bit precision. Programming nanoscale memristive devices, modifies their atomic\narrangement, inherently stochastic, non-linear and of limited granularity [42\u201345]. This analog\nnon-idealities are known to cause significant performance drops in training networks compared\nto software simulations [46]. Controlled experiments by Sidler et al. [22] have demonstrated that\nthe poor training performance is primarily due to insufficient number of pulses switching the\ndevice between High Resistive State (HRS) and Low Resistive State (LRS).\nFollowing this, various attempts have been made to improve the bit precision of memristive\ndevices. Optimizing RRAM materials has increased the number of bits per device, but often at the\ncost of lower ON/OFF ratios (i.e., the ratio of the LRS and HRS), making it harder to distinguish\nstates with small footprint circuits [47-50]. Furthermore, architectural optimizations have been\nexplored, including using multiple binary devices per synapse [51], assigning a number system\nto multiple devices [52], leveraging stochastic switching [53, 54], and complementing binary\nmemristive devices with capacitors [55]. However, these methods still require complex and large\nsynaptic architectures, limiting scalability.\nIn Chapter 2, we propose a novel approach to program intrinsically 1-bit RRAM devices to\nincrease their effective bit resolution by precise control of the filament formation.\nThe credit assignment problem. Learning in any systems is fundamentally about adjusting its\nparameters to improve its performance. In neural networks, learning involves adjusting weights,\nrepresented by the vector W, to optimize the performance, measured by an objective function,\nF(W). The credit assignment problem refers to the challenge of determining the precise weight\nadjustments needed for improvement, especially in deep networks where the relationship between\nindividual neurons and overall performance is less clear [56].\nTraditionally, Hebbian mechanisms [57] leveraging the timing of pre- and post-synaptic spikes,\nhave been the go-to solution for on-chip learning in the neuromorphic field. This is because\nHebbian rules explain a numerous neuroscientific observations [58, 59], posses interesting variance\nmaximization properties [60] but more practically, utilize local signals to the synapse in a simple\nway, making them easily adaptable to silicon circuits [61\u201364]. However, Hebbian rules alone had\nlimited success while scaling to large networks, and require heavily crafted architectural biases to\nachieve hiererchical, disentangled representations. \u2075\nBackpropagation [68], on the other hand, remains the state-of-the-art algorithm for training\nmodern ANNs, and is one of the pillars of the deep learning revolution. To give an intuition why\nit works, following the insight from Richards et al. [7], let's consider weight changes,  $\\Delta W$ , to be\nsmall and objective function to be maximized, F, to be locally smooth, then resulting  $\\Delta F$  can be\napproximated as  $\\Delta F \\approx \\Delta W^T \\cdot \\nabla_W F(W)$ , where  $\\nabla_W F(W)$  is the gradient of the objective function\nwith respect to the weights. This means that to guarantee improving learning performance\n($\\Delta F > 0$ ), a principled approach to weight adjustment is to take a small step in the direction of\nthe steepest performance improvement, guided by the gradient ($\\Delta F \\approx \\eta \\nabla_W F(W)^T \\cdot \\nabla_W F(W) \\ge$\n0). Backpropagation explicitly calculating gradients, while powerful, is unsuitable for online\nlearning on analog substrates due to its need for symmetric feedback weights and distinct\nforward/backward phases [56]. And for temporal credit assignment, BPTT unrolls the network in\nreverse time to backpropagate the error gradients, resulting in memory complexity scaling with\nO(kT), where k is the number of time steps and T is the number of neurons. This temporal non-\nlocality, necessitates the usage of memory hierarchy to save silicon space, resulting in sacrificing\nenergy efficiency and latency. For this reason, various alternatives have been proposed to estimate\ngradients while offering better locality, such as feedback alignment [69], Q-AGREL [70], difference"}, {"title": "INTRODUCTION", "content": "\u2075 While this is true, it is possible that end-to-end credit assignment might not be needed [65]. Recent works successfully\nreplacing global backpropagation with truncated layer-wise backpropagation supports this view [66, 67]. Nevertheless,\ndesigning the right architecture and local loss functions to guide the truncated credit assignment is still an open question.\ntarget propagation [71], predictive coding [72] and weight perturbation-based methods [73]. These\nmethods exhibit varying degrees of variance (resulting in slower convergence) and bias (leading\nto poor generalization [74]) in their estimations [7].\nHowever, as long as variance and bias are within\nreasonable bounds, the learning rule can still be\neffective, potentially offering a favorable sweet spot\nbetween locality and performance. The challenge\nof local and effective spatio-temporal credit assign-\nment is still a critical frontier in neural processing,\nin analog substrates, demanding new and creative\napproaches.\nIn Chapter 3, we address this challenge for the\nfirst time, designing material and circuits evolved\naround gradient calculations, and implementing\ne-prop [75] local learning rule for Recurrent Spik-\ning Neural Networks (RSNNs) on a memristive\nchip.\nProgramming memristors with non-idealities\nfor online learning. On-chip learning requires the ability to program the network weights in\naccordance with the demands of the learning rule. While digital systems often rely on quanti-\nzation to optimize memory access [76] and associated mitigation techniques such as stochas-\ntic rounding [77], gradient scaling [78], quantization range learning [79] and optimizing the\nweight representation [80], analog memristive systems present unique challenges due to non-\nidealities such as conductance-dependent, non-linear, stochastic, and time-varying programming\nresponses [81]. Then, it is crucial to identify which digital methods can be effectively transferred\nto analog systems while promising small footprint and energy overhead.\nIn Section 3.1 and 3.2, we analyze several practical weight update schemes implementing an\nonline learning rule for mixed-signal systems on a custom simulator, and later validate on a real\nneuromorphic chip.\nScalable synaptic eligibility traces for local learning. Many high-performing local rules\nrely on eligibility traces [39, 82\u201386], slow synaptic memory mechanisms that carry information\nforward in time. These traces bridge the temporal gap between synaptic activity occuring on\nmillisecond timescale with network errors arising seconds later, helping to solve the distal reward\nproblem [87, 88]. While several neuromorphic platforms [89\u201391] have incorporated synaptic\neligibility traces for learning, this mechanism is one of the most costly building blocks in neural\ncomputation, due to quadratic scaling of the number of synapses. Digital implementations suffer\nfrom the memory-intensive nature of numerical trace calculations, leading to a von Neumann\nbottleneck [92, 93]. Even in mixed-signal designs, the slow dynamics of eligibility traces require\nlarge capacitors leading to sacrifice of the scalability [94].\nIn Section 3.3, we propose a novel and scalable implementation of synaptic eligibility traces\nusing volatile memristors.\nUnifiying volatile and non-volatile materials. Different neural building blocks require different\nvolatility, bit precision, and endurance characteristics from the memristive devices, which are then\ntailored to meet these demands. For example, ANN inference workloads require linear non-volatile\nconductance response over a wide dynamic range for optimal weight update and minimum\nnoise for gradient calculation [21, 22, 95]. Whereas SNNs often demand richer and multiple\nsynaptic dynamics simultaneously e.g., short term conductance decay (to implement Short-Term\nPlasticity (STP) and eligibility traces [82]), non-volatile device states (to represent synaptic efficacy)\nand a probabilistic nature (to mimic synaptic vesicle releases [24]). However, optimizing different\nactive memristive material for each of these features limits their suitability to a wide range of\ncomputational frameworks and ultimately increases the system complexity for most demanding\napplications. Moreover, these diverse specifications cannot always be implemented by combining"}, {"title": "INTRODUCTION", "content": "different types of memristors on a monolithic circuit e.g., volatile and non-volatile, binary\nand analog, due to the incompatibility of the fabrication processes. Although some prototype\nmaterials have been proposed to exhibit dual-functional memory [96, 97], the dominance of one\nof the mechanisms often results in poor switching performance. Therefore, the lack of universal\nmemristors capable of realizing diverse computational primitives has been a challenge.\nIn Chapter 4, we present our discovery of a novel memristor type that can be used for both\nvolatile and non-volatile operations based on a simple programming scheme, while achieving a\nworld-record in endurance.\nRouting of multi-crossbar arrays for scaling. Scaling neural networks, by increasing layer\nwidth or depth, has proven to be a powerful technique for improving performance [98]. However,\nscaling memristive crossbar array dimensions is hindered by analog non-idealities such as current\nsneak-paths, parasitic resistance, capacitance of metal lines, and yield limitations [99\u2013101]. For\nthis reason, large-scale systems need to adapt multiple crossbars of managable dimensions [102],\nbut this introduces the overhead of routing activities, especially with long wires along the source,\nrouter and destination. To reduce wiring length, three-dimensional (3D) technology to vertically\nstack logic, crossbar arrays, and routers has been proposed [34, 102], but the fabrication complex-\nity and cost of 3D integration are currently prohibitive. Today's most advanced multi-crossbar\nneuromorphic chips, e.g., HERMES [23] and NeuRRAM [18], still rely on off-chip communica-\ntion for routing, strongly diminishing communication energy efficiency. This necessitates the\ndevelopment of efficient on-chip routing mechanisms to achieve energy-efficient communication.\nWhen the routing is optimized for communicating events through the on-chip AER protocol, the\ndesigner faces a trade-off between source-based and destination-based routing. Source-based\nrouting offers the flexibility of per-neuron Content Addressable Memory (CAM) memory as\nused by DYNAP-SE [103], but this comes at the cost of increased chip area and slower memory\naccess. Destination-based routing, while more area-efficient, sacrifices some degree of network\nconfigurability.\nIn Chapter 5, we propose and fabricate a novel memristive in-memory routing core that can be\nreconfigured to route signals between crossbars, enabling dense local, sparse global connectivity\nwith orders of magnitude more routing efficiency compared to other SNN hardware platforms.\nThesis Overview\nThis thesis explores the path towards intelligent and low-power analog computing substrates,\nembracing the Bitter Lesson [104] and addressing some of the challenges in learning and scale.\nIt consists of six selected publications, which I have coauthored with amazing electrical engineers,\ncomputer scientists, material designers and neuroscientists. My individual contributions to the\nwork presented in each chapter are clarified and outlined in Appendix 4.\nIn the first part, we focus on developing mixed-signal learning circuits targeting memristive\nweights in single-layer feedforward SNN architectures. We address the limited resolution and\ndevice-to-device and cycle-to-cycle variability of binary RRAM weights, aiming to enable on-chip\nlearning. Building upon the observation of Ielmini [105] that filament size in RRAM can be\nprecisely controlled by compliance current, we introduce a programming circuitry that modu-\nlates synaptic weights based on the estimated gradients using a modified Delta Rule [106]. This\napproach achieves multi-level weight resolution within the conductance of intrinsically binary\nswitching RRAM devices. We model variability of device responses to our new compliance cur-\nrent programming scheme,  $I_{cc}$ to GLRS, from experimental measurements on a 4kb HfO\u2082-based\nRRAM array, and adjust our implementation accordingly. We validate our approach and circuits\nthrough simulations for standard Complementary Metal-Oxide-Semiconductor (CMOS) 180nm\nprocesses and system simulations on the MNIST dataset. This codesign of algorithm, material,\nand circuit properties establishes a significant building block for single-layer on-chip learning\nwith memristive devices. Furthermore, in Chapter 4, we demonstrate that our programming"}, {"title": "ENHANCING BIT PRECISION OF BINARY MEMRISTORS FOR ROBUST\nON-CHIP LEARNING", "content": "Analog neuromorphic circuits with memristive synapses offer the potential for power-efficient\nneural network inference, but limited bit precision of memristors poses challenges for gradient-\nbased training. In this chapter, we introduce a programming technique of the weights to enhance\nthe effective bit precision of initially binary memristive devices, enabling more robust and\nperformant on-chip training. To overcome the problems of variability and limited resolution of\nReRAM memristive devices used to store synaptic weights, we propose to use only their HCS\nand control their desired conductance by modulating their programming compliance current,\n$I_{cc}$ . We introduce spike-based CMOS circuits for training the network weights; and demonstrate\nthe relationship between the weight, the device conductance, and the $I_{cc}$ used to set the weight,\nsupported with experimental measurements from a 4kb array of HfO\u2082-based devices. To validate\nthe approach and the circuits presented, we present circuit simulation results for a standard\nCMOS 180 nm process and system-level simulations for classifying hand-written digits from the\nMNIST dataset."}, {"title": "INTRODUCTION", "content": "The neural networks deployed on the resource-constrained devices can benefit greatly from online\ntraining to adapt to shifting data distributions, sensory noise, device degradation, or new tasks\nthat are not seen in the pretraining. While CMOS architectures with integrated memristive devices\noffer ultra-low power inference, their use for online learning has been limited [114, 115].\nIn this chapter, we propose novel learning circuits for SNN architectures implemented by 1T1R\narrays. These circuits enable analog weight updates on binary ReRAM devices by controlling their\nSET operation  $I_{cc}$ . In addition to increasing the bit precision of network weights, the proposed\nstrategy allows compact, fast, and scalable event-based learning scheme compatible with the AER\ninterface [116].\nPreviously significant efforts have aimed to increase the bit precision of memristive devices for\nonline learning through material and architectural optimizations.\nMATERIAL OPTIMIZATION Several groups reported TiO\u2082-based [47\u201349] and HfO\u2082-based [50]\nReRAM devices with up to 8 bits of precision. However, in all these works, the analog behavior is\ntraded off with the lower available ON/OFF ratio. While the analog behavior is an important\nconcern for training neural networks, cycle-to-cycle and device-to-device variability harms the\neffective number of bits further when ON/OFF ratio is small. Also, tuning the precise memory\nstate is not always easily achievable in a real-time manner, requiring recursively tuning with\nan active feedback scheme [50, 117]. Furthermore, some efforts have been focused on carefully\ndesigning a barrier level using exhaustive experimental search over a range of materials [47, 48]\nwhich makes it difficult to fabricate.\nARCHITECTURE OPTIMIZATION Increasing the effective bit resolution has also been demon-\nstrated with architectural advancements. Strategies such as using multiple binary switches to\nemulate n-bit synapses [51] or exploiting stochastic switching properties for analog-like adapta-\ntion [53, 54] have been explored. Alternatively, IBM's approach of using a capacitor alongside\ntwo PCM devices to act as an analog volatile memory, increases combined precision but incurs\nsignificant area overhead [55]. Recently, a mixed-precision approach has been employed to train"}, {"title": "RERAM DEVICE MODELING", "content": "To find the average relationship between the mean of the cycle-to-cycle distribution of the HCS\nand the SET programming  $I_{cc}$ , we performed measurements on a 16 \u00d7 256 (4kb) array of\nHfO\u2082-based ReRAM devices integrated onto a 130 nm CMOS process between metal layers 4\nand 5 [119]. Each device is connected in series to the drain of an n-type selector transistor which\nallows the SET programming  $I_{cc}$ to be controlled based on the voltage applied to its gate. The\n1T1R structure allows a single device to be selected for reading or programming by applying\nappropriate voltages to a pair of Source/Bit Lines (SL/BL) and a single Word Line (WL).\nAll 4kb devices were initially formed in a\nraster scan fashion by applying a large voltage\n(4V typical) between the SL and BL to induce\na soft breakdown in the oxide layer and in-\ntroduce conductive oxygen vacancies. After\nforming, each device was subject to sets of 100\nRESET/SET cycles over a range of SET  $I_{ccs}$ be-\ntween 10\u00b5A and 400\u00b5A, where the resistance\nof each device was recorded after each SET\noperation. The mean of all devices' median\nresistances over the 100 cycles, at a single $I_{cc}$ ,\ngives the average relationship between HCS\nmedian and SET  $I_{cc}$ as in Fig. 2.1. The rela-\ntionship is seen to follow a line in the log-log\nplot (power law) and over this  $I_{cc}$ range, it\nallows precise control of the conductance me-\ndian of the cycle-to-cycle distribution between\n50k\u03a9 and 2\u039a\u03a9."}, {"title": "BIT-PRECISION ENHANCING WEIGHT\nUPDATE RULE", "content": "The learning algorithm is based on the Delta rule, the simplest form of the gradient descent for\nsingle-layer networks. In our implementation, the objective function is defined as the difference\nbetween the desired target output signal  $y$  and the network prediction signal  $\\hat{y}$ , for a given set\nof input patterns signals  $x$ , weighted by the synaptic weight parameters  $w$ . Then the Delta rule\ncan be used to calculate the change of the weights connecting a neuron  $i$  in the input layer and a\nneuron  $j$  at the output layer as follows:\n$\\Delta w_{ji} = \\eta (y_j - \\hat{y}_j)x_i = \\eta \\delta_j x_i$  ,  (2.1)"}, {"title": "ONLINE TEMPORAL CREDIT ASSIGNMENT WITH NON-VOLATILE AND\nVOLATILE MEMRISTORS", "content": "Training RSNNs on ultra-low-power hardware remains a significant challenge. This is primarily\ndue to the lack of spatio-temporally local learning mechanisms capable of addressing the credit\nassignment problem effectively, especially with limited weight resolution and online training\nwith a batch size of one. These challenges are accentuated when using memristive devices for\nin-memory computing to mitigate the von Neumann bottleneck, at the expense of increased\nstochasticity in recurrent computations.\nTo investigate online learning in memristive neuromorphic Recurrent Neural Network (RNN)\narchitectures, we present a simulation framework and experiments on differential-architecture\ncrossbar arrays based on an accurate and comprehensive PCM device model. We train a spiking\nRNN on regression tasks, with weights emulated within this framework, using the recently\nproposed e-prop learning rule. While e-prop truncates the exact gradients to follow locality\nconstraints, its direct implementation on memristive substrates is hindered by significant PCM\nnon-idealities. We compare several widely adopted weight update schemes designed to cope with\nthese non-idealities and demonstrate that only gradient accumulation can enable efficient online\ntraining of RSNNs on memristive substrates."}, {"title": "Introduction", "content": "RNNs are a remarkably expressive [129", "130\u2013135": ".", "137": ".", "83": "."}]}