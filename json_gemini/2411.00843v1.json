{"title": "THE GRAPH'S APPRENTICE: TEACHING AN LLM LOW-LEVEL KNOWLEDGE FOR CIRCUIT QUALITY ESTIMATION", "authors": ["Reza Moravej", "Saurabh Bodhe", "Zhanguang Zhang", "Didier Ch\u00e9telat", "Dimitrios Tsaras", "Yingxue Zhang", "Hui-Ling Zhen", "Jianye Hao", "Mingxuan Yuan"], "abstract": "Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. In this paper, we introduce VeriDistill, the first end-to-end machine learning model that directly processes raw Verilog code to predict circuit quality-of-result metrics. Our model employs a novel knowledge distillation method, transferring low-level circuit insights via graphs into the predictor based on LLM. Experiments show VeriDistill outperforms state-of-the-art baselines on large-scale Verilog datasets and demonstrates robust performance when evaluated on out-of-distribution datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Rapid technological advancements in computing power has taken an increasingly important role in the past decades in driving scientific research in biology (Schatz, 2012), chemistry (Akimov & Prezhdo, 2015), physics (Dongarra & Keyes, 2024) and especially artificial intelligence, where it has been estimated that at least half of all performance gains in the past ten years have stemmed from hardware improvements alone (Hernandez & Brown, 2020; Dorner, 2021; Karpathy; Erdil & Besiroglu, 2022; Ho et al., 2024). This ever-rising demand for compute power means that efficient and effective electronic chip design has become increasingly critical.\nModern electronic chip design is a complex, multi-stage endeavor that begins with a chip architect specifying the digital circuit's functionality in a Hardware Description Language (HDL), such as Verilog (Thomas & Moorby, 2008) or VHDL (Coelho, 2012). This HDL code is then subjected to a series of transformations and optimizations, ultimately yielding a physical circuit design that can be manufactured (LaMeres, 2023). In a previous era where circuits were small and limited in functionality, this logic synthesis process was quick and the chip architect could quickly receive feedback and iterate on its HDL code. However, with the increasing complexity of industrial designs, which now can comprise hundreds of millions of logic gates (Amar\u00fa et al., 2017), even a single synthesis run has become massively expensive. This has driven the need for alternate ways of providing feedback on HDL code without running the actual logic synthesis process.\nA natural way to tackle this problem is to train a machine learning model that can take the HDL code as input, and output estimates of circuit quality such as wire length or delay that could have been computed had the logic synthesis process been run. A few works have approached this topic, by extracting graphical information about the code and using hand-designed statistics of those graphs as features (Zhou et al., 2019; Sengupta et al., 2022; Fang et al., 2023). Although these works had encouraging results, their performance has been limited by the relatively shallow understanding of the semantics of the code that these statistics can provide."}, {"title": "2 RELATED WORK", "content": "Closest to ours is the work of Sengupta et al. (2022). Their approach consists in computing the Abstract Syntax Tree (AST) induced by Verilog code, and extracting from this free vector- and graph-based features. They then train several machine learning models to predict from these features the total negative slack and dynamic power of the circuit. Among all the models evaluated, the XGBoost Regressor performs best and achieves 95% R2-score. The analysis was however limited to different runs of a single circuit and it is not clear how the performance would generalize to different circuits. Since the Abstract Syntax Tree is essentially the raw Verilog code with extra syntactic information, which can be obtained at little cost at inference time by a grammar parser, we include it (along with variants) as baselines in our experimental section."}, {"title": "2.2 LLMS FOR VERILOG", "content": "Large language models (LLMs) such as GPT (Ouyang et al., 2022) and Llama (Touvron et al., 2023) have achieved exceptional success in various natural language tasks and have expanded their success to programming languages as well. While decoder-only code LLMs such as Codex (Chen et al., 2021) and CodeLlama (Roziere et al., 2023) have become the most popular due to their exceptional performance in generation tasks like code generation and code translation, older encoder-only models such as CodeBERT (Feng et al., 2020) and encoder-decoder code LLMs such as CodeT5 (Wang et al., 2021) have retained applications in code comprehension tasks such as clone detection and code retrieval.\nAlthough excellent on generalist programming languages like Python or C++, these models have been trained on the relatively small amount of HDL code that is publicly available on the internet, and therefore have performed poorly on Verilog benchmarks like VerilogEval (Liu et al., 2023b) and RTLLM (Lu et al., 2024). This has motivated further work to build LLMs with a higher-degree of knowledge of hardware description languages. Both CodeGen-Verilog (Thakur et al., 2023) and VeriGen (Thakur et al., 2024) used a combination of customized Verilog datasets from code repository website GitHub\u00b2 and various textbooks to fine-tune code LLMs. Finally, RTLCoder (Liu et al., 2023c) used the GPT 3.5 language model (Brown et al., 2020) to generate further Verilog data, in a form of data augmentation, while CodeV (Zhao et al., 2024) used the same model to generate natural language description of real world Verilog code through multi-level summarization.\nBesides Verilog code generation from natural language description, LLMs were also explored for other EDA-related tasks. RTLFixer (Tsai et al., 2023) employed Retrieval-Augmented Generation (RAG) and ReAct prompting techniques to interactively debug syntax errors in Verilog code, and achieved remarkable improvement in success rates in the VerilogEval benchmark. ChipNemo (Liu et al., 2023a) explored the application of LLMs in chip design process and adopted several domain adaptation techniques to train a LLM for various applications including assistant chatbots, EDA script generation, and bug summarization and analysis. Finally, ChatEDA Wu et al. (2024) used code LLMs as an agent to autonomously complete the entire chip design flow from HDL code to the Graphic Data System Version II (GDSII) by managing task planning, script generation and task execution. We refer the reader to the extensive survey of Zhong et al. (2023) for more details on the application of LLMs in electronic design automation and future research directions in this field."}, {"title": "2.3 ALIGNMENT OF LLM AND GNN EMBEDDINGS", "content": "The multimodal alignment regularizer we propose during training also relates to the broader literature on tuning large language models to align with a pre-trained graph neural network, to incorporate its capabilities.\nThe work closest to ours is that of Mavromatis et al. (2023), who train a language model to perform a node classification task while adding a regularizer that encourages the predictive distributions to match a pre-trained graph neural network model. The language model makes predictions by passing the graph as input, and extracting the representation corresponding to a final [CLS] classification token. Also similar is Zou et al. (2023), which jointly trains a language model and a graph neural network on a common context graph prediction\" task which encourage alignment of their representations. They then discard the graph neural network and only keep the language model, so that topological characteristics best captured by graph convolutions can be said to have been incorporated in the language model.\nMore generally, there is a large literature on integrating pretrained graph neural networks with language models by training an adaptive module (Liu et al., 2024; 2023d; Chai et al., 2023; Tang et al., 2024; Cao et al., 2023), allowing the language model to receive inputs from the graph neural network. Alternatively, multiple works have interlaced graph neural network layers and language model layers (Yasunaga et al., 2021; Yang et al., 2021; Zhang et al., 2022; Yasunaga et al., 2022; Jin et al., 2023). In either case, some kind of training is necessary to allow for interactions between the graph neural network and the language model, although the result is not distillation of the graph neural network's perspective into the language model per se."}, {"title": "3 METHODOLOGY", "content": "We now present our VeriDistill approach in detail. As described in the introduction, turning a high-level description of a circuit in a Hardware Description Language like Verilog into a physical description ready for manufacturing is a computationally expensive process involving several steps, each with an associated intermediate representation describing progressively lower-level elements of the circuit. Our goal is to predict low-level quality-of-result metrics, like area and delay, from the highest-level representation, namely the HDL code."}, {"title": "3.1 MODEL", "content": "To do so, we propose to exploit the recent development of Verilog LLMs to extract rich representations from which these quality-of-result metrics can be predicted. Namely we propose to input the Verilog into the CodeV-7B Zhao et al. (2024) language model, and use the average over all tokens"}, {"title": "3.2 KNOWLEDGE DISTILLATION", "content": "Intermediate representations produced by logic synthesis incorporate lower-level insights about the resulting circuit, and therefore facilitate a more straightforward estimation of quality-of-result metrics like area and delay. This raises the question of whether the representation obtained from the graph representation of the circuit could be used to further improve the performance of the model.\nWe therefore additionally propose to add a cross-modal knowledge distillation term in our training procedure. In this procedure, an external teacher neural network is trained on intermediate representations extracted at high cost on instances in the training set, to produce high accuracy area and delay estimates. Subsequently, the main model (which acts as a student) is encouraged during training to align its last layer activations with those of the teacher network.\nIn practice, this teacher model is trained on the Look-Up-Table (LUT) representation of the circuit, which is obtained from And-Inverter Graph (AIG) through the synthesis process. This representation, which can be seen as a graph with binary inputs implementing arbitrary logic functions, is particularly suitable for GNN training as it is compact with rich node information. We feed the LUT graph to a GCN Kipf & Welling (2017) encoder, where the node features are initialized as one-hot vector based on the node type (input, output, AND, OR, and so on). The node level embeddings are transformed to a graph-level representation through a pooling layer and fed to feedforward neural network. The model is trained on QoR prediction tasks simply by the mean-squared-error loss. The representations from the last latent layer of the teacher model are used for training the main model."}, {"title": "3.3 OVERALL FRAMEWORK", "content": "Combining these elements provides the following overall VeriDistill framework, which is described by Figure 2. Denote $D_{train} = \\{(v^{(1)}, g^{(1)}, t^{(1)}), ..., (v^{(N)}, g^{(N)}, t^{(N)})\\}$, as the train dataset, where $v^{(i)}$ denotes a Verilog instance and $g^{(i)}$ its corresponding low-level LUT graph representation obtained via synthesis. The goal is to train our network to estimate the target $t^{(i)}$ (area or delay) directly from the Verilog input $v^{(i)}$. The objective for training the model is:\n$L = \\frac{1}{N} \\sum_{i=1}^{N} \\alpha ||p(v^{(i)}) - t^{(i)}||^2 + (1 - \\alpha) ||z(v^{(i)}) - z_{teacher}(g^{(i)})||^2$"}, {"title": "4 EXPERIMENTS", "content": "This section is organized as follows: We begin by presenting the implementation details of our experimental setup in Section 4.1, including hardware, model, and training hyperparameters. Next, we describe the dataset used and the data preprocessing steps for training and evaluation in Section 4.2. We then introduce the baseline methods and their implementation details in Section 4.3. Following this, we outline the metrics used to evaluate the performance of our model and the baselines in Section 4.4. Finally, we present the results on the main datasets and a study on unseen out-of-distribution circuits in Sections 4.5 and 4.6."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We implement VeriDistill and the baselines with the PyTorch and PyG libraries. We train the models on a server with 8 NVIDIA V100 GPUs. The model's decoder feedforward network depicted in Fig. 1 is designed so that the 512-dimensional representations from the CodeV model are passed through two linear layers with 512 neurons each, followed by a final linear layer with a single-dimensional scalar output. We use ReLU activations throughout.\nThe teacher network takes a LUT graph with 16-dimensional node attributes, and passes it through three GCN layers with 64 neurons each, interleaved with BatchNorm layers. Then the result of a mean and max pooling operation is concatenated together, yielding a 128-dimensional vector, which is passed through a linear layer to upcast it to 512 dimensions. The resulting vector is finally passed to a feedforward architecture with a similar architecture as the one of the main model, namely two linear layers with 512 neurons followed by a final scalar linear layer. We use ReLU activations throughout.\nWe train the models which do not involve knowledge distillation using the ReduceLROnPlateau scheduler with initial learning rate 1e-3, with patience set to 30 epochs and factor set to 0.5. Models utilizing knowledge distillation are trained using the CosineAnnealingLR Loshchilov & Hutter (2017) scheduler, with an initial learning rate of 1e-3 and number of iterations set to 50. We start the training process with \u03b1 = 0.5, and increase \u03b1 to 0.75 and 1 at epochs 150 and 250. The idea is put less emphasis on knowledge distillation at every warm re-start. We find that this approach results in marginal gain compared to other optimization methods. All models are trained until full convergence."}, {"title": "4.2 DATASETS", "content": "We train and evaluate on two separate datasets. The first dataset is used for training, validation, and testing of all the methods, while OpenABCD contains out-of-distribution circuits aiming to challenge VeriDistill and determine its ability to generalize.\nCustomized Dataset To train and evaluate our proposed method, we collect 18.4k Verilog data provided by Pei et al. (2024) and 5.8k data from Thakur et al. (2022). These Verilog data are obtained from open-source GitHub repositories and textbooks and have been verified for syntax correctness. We use an open-sourced EDA platform OpenROAD Ajayi et al. (2019) with 7nm technology PDK provided to conduct logic synthesis and record post-synthesis labels of area and delay. The AIG graphs after optimization are converted into LUT graphs and saved for teacher model training. While these customized data have been verified for syntax correctness, a large portion of these Verilog data cannot pass the logic synthesis pipeline due to functional incorrectness. Additionally, we eliminate"}, {"title": "4.3 BASELINES", "content": "While numerous prior works have attempted to predict post-synthesis circuit quality at RTL-stage, none of them predicts with source Verilog files. Several works rely on lower-level circuit representation that requires extra processing using logic synthesis tools (Zhou et al., 2019; Fang et al., 2023). Using low-level circuit representation as input is advantageous for the circuit quality prediction task but it is unfair to compare them to our method which takes un-processed Verilog as input.\nWe adopt the method proposed by Sengupta et al. (2022) as our baseline. It relies on AST representation that can be easily converted from Verilog source file. We implement the method based on description in Sengupta et al. (2022). Verilator (Snyder, 2004) is used to convert each source Verilog into its respective AST representation, which can be represented as as a graph. The nodes in the graph represent one of the following five semantic categories from the source Verilog (root, variable, operation, constant, edge), while edges are created between nodes with connections.\nWe implement three variants of the AST-based method:\nAST-XGBoost We compute the following features (i) total number of input bits (ii) total number of output bits (iii) the longest path in the AST (iv) frequency of each node type in the graph, and (v) frequency of each logic type in the graph. The features are concatenated to form a feature vector with 108 features 3. We perform a thorough hyper-parameter selection using grid search and employ early stopping to prevent over-fitting.\nAST-GNN w/o KD The AST-GNN model takes in the following features per node: (i) total number of input bits (ii) total number of output bits (iii) node semantic type (iv) node operation type. Each feature is represented via a one-hot vector and is projected to a 4-dimensional space via a linear layer. The final node features consists of a (4 \u00d7 4) = 16-dimensional vector. We cap the number of input/output bits to 200, since 99.9 percent of the nodes in the dataset have less than 200 input/outputs. The AST-GNN model utilizes the same hyperparameters and encoder/decoder architecture as the GCN teacher model used for LUT input."}, {"title": "4.4 METRICS", "content": "We use the following metrics to evaluate the performance of our method:\nMAE (Mean Absolute Error) It is the average value of absolute differences between the actual values and the predicted values.\nR2-score Also known as coefficient of determination. It measures how well the predicted values of the model match the actual data, essentially showing portion of the variance that can be explained by the model.\nMAPE (Mean Absolute Percentage Error) It measures the average magnitude of errors between predicted values and actual values, as a percentage of the actual values.\nRSE (Relative Standard Error) It is a measure of the relative size of the standard error in comparison to the sample mean. It provides a sense of the precision of the estimate in relation to the magnitude of the estimate itself."}, {"title": "4.5 MAIN RESULTS", "content": "We first summarize the results of our main experiment, where we train and test the model on the large Customized Dataset (see Section 4.2). Table 1 outlines the performance of different models on the test set. As can be seen, our proposed method, utilizing both CodeV as an encoder and knowledge distillation, outperforms other baselines across all the metrics, especially with area prediction. Interestingly, simply using a decoder on the LLM representation performs worse than the previous state-of-the-art, while knowledge distillation on the AST-GNN model has almost no effect. Only when both are used together is there profound impact on performance, which suggests our knowledge distillation procedure is crucial in fully exploiting the richness of the CodeV LLM representations.\nWe can further insight on the benefits of our combined approach by analyzing scatter plots of the predictions against the targets, shown in Figure 4. As can be seen, most models' good performance is mostly concentrated on circuits with small delay and area, at the expense of larger circuits, perhaps because the latter are more rare in the training set. In contrast, our model performs mostly uniformly well on circuits on every size. This contrast is particularly pronounced when comparing against the same model without knowledge distillation (CodeV+Decoder), which indicates that our knowledge distillation procedure is crucial in allowing our model to perform well across the whole range of circuit sizes."}, {"title": "4.6 ADDITIONAL OUT-OF-DISTRIBUTION RESULTS", "content": "Finally, we evaluate how our knowledge-distillation procedure can impact the ability of the trained model to generalize to new out-of-distribution circuits. For this, we take our model, trained with and without knowledge distillation on our Customized Dataset, and apply it to instances in the Open-ABCD benchmark (see Section 4.2). As can be seen in Table 2, our knowledge distillation procedure systematically improves the LLM-based model's ability to transfer prediction performance on out-of-distribution instances, which differ significantly from those seen during training."}, {"title": "5 CONCLUSION", "content": "In summary, in this work we propose a novel procedure to predict quality-of-result electronic circuit metrics from Verilog code, by training a small neural network model on Verilog LLM representations"}]}