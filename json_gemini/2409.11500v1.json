{"title": "Multi-Document Grounded Multi-Turn Synthetic Dialog Generation", "authors": ["Young-Suk Lee", "Chulaka Gunasekara", "Danish Contractor", "Ram\u00f3n Fernandez Astudillo", "Radu Florian"], "abstract": "We introduce a technique for multi-document-\ngrounded multi-turn synthetic dialog genera-\ntion that incorporates three main ideas. First,\nwe control the overall dialog flow using\ntaxonomy-driven user queries that are gener-\nated with Chain-of-Thought (CoT) prompting.\nSecond, we support the generation of multi-\ndocument-grounded dialogs by mimicking real-\nworld use of retrievers to update the ground-\ning documents after every user-turn in the dia-\nlog. Third, we apply LLM-as-a-Judge to filter\nout queries with incorrect answers. Human\nevaluation of the synthetic dialog data suggests\nthat the data is diverse, coherent, and includes\nmostly correct answers. Both human and auto-\nmatic evaluations of answerable queries indi-\ncate that models fine-tuned on synthetic dialogs\nconsistently out-perform those fine-tuned on\nexisting human generated training data across\nfour publicly available multi-turn document\ngrounded benchmark test sets.", "sections": [{"title": "1 Introduction", "content": "As instruction-tuned language models have proven\nhighly effective to generalize to new tasks, (Chung\net al., 2022; Wei et al., 2021; Ouyang et al., 2022;\nMishra et al., 2022; Wang et al., 2022b), there has\nbeen growing interest to acquire synthetic data sets\ngenerated from pre-trained language models with a\nminimal or no human supervision, (Honovich et al.,\n2022; Wang et al., 2023; Xu et al., 2023; Lee et al.,\n2023). While there has been an exploration of syn-\nthetic data generation for persona-grounded dialog\ngeneration (Jang et al., 2022; Bao et al., 2023),\ntopic-grounded dialog generation (Abbasiantaeb\net al., 2024), summary-grounded dialog genera-\ntion (Gunasekara et al., 2021) and conversation\ngeneration from dialog scenarios (Mohapatra et al.,\n2021), the study of methods for generating multi-\ndocument grounded multi-turn dialogs by mim-\nicking real-world deployments with retrievers has\nbeen limited (Bao et al., 2023; Abbasiantaeb et al.,\n2024). Generating a realistic multi-turn document\ngrounded dialog can be challenging as one needs\nto ensure that the (i) the questions are diverse (ii)\nconversation flows naturally - i.e, it is coherent and\nthe follow-up questions are more than just a col-\nlection of question-answer pairs with co-references\n(iii) the responses are faithful to the documents\nin the retrieved set and not generated from model\nparameters.\nIn this paper, we address these challenges and\npresent a technique for generating high quality doc-\nment grounded multi-turn synthetic dialogs. Our\ndata generation flow incorporates two novel ideas\nfirst, we control the dialog flow according to a\nquestion-taxonomy with chain-of-thought (CoT)\nprompting for query generation. Second, we gener-\nate dialogs grounded on both single and multiple re-\ntrieved documents. We also apply self-consistency\n(Wang et al., 2022a) and an LLM-as-judge (Zheng\net al., 2023) to ensure the answers are correct.\nFor multi-document grounded dialog generation,\nuser queries and agent answers are based on top-k\nretrieved passages. In particular, we generate an\ninitial user query from a single document source\nand generate the agent answer from top-k passages\nretrieved on the initial user query. Subsequent\nuser queries and all agent answers are grounded\non the retrieved passages and dialog history. We\nuse a series of carefully designed prompts to en-\nsure generated agent answers continue to remain\nmeaningful in the presence of retrieved passages,\noften noisier than human generated documents.\nWe use MIXTRAL-8X7B-INSTRUCT as our lan-\nguage model for both data generation and LLM-as-\na-Judge throughout this paper. However, the pro-\nposed synthetic data generation framework is not\ntied to any specific language model. An overview\nof the proposed technique is shown in Figure 1.\nTo assess the quality of the synthetic data, we\ncarry out extensive human evaluations on 294 di-"}, {"title": "2 Multi-turn Dialog Generation", "content": "Our multi-turn dialog generation consists of four\ncomponents: (1) Two question taxonomies one\ndesigned for the first turn of the dialog (ST-QT) and\nthe other for subsequent turns (MT-QT). (2) Chain-\nof-Thought (CoT) prompting to generate desired\nquery types according to question taxonomy, (3)\nTwo separate synthetic data generation pipelines\nfor single-document and multi-document grounded\ndialogs. (4) LLM-as-a-Judge to filter out dialogs\ncontaining incorrect answers for any given queries."}, {"title": "2.1 Question Taxonomy", "content": "To ensure that generated dialogs are diverse we\ndevise a question type taxonomy. This taxonomy\nis a subset of contemporaneous ideas presented\nin (Yang et al., 2024) and covers the most critical\nquery types found in RAG datasets. We adopt the\nfollowing question taxonomy for all initial user\nqueries:\nDirect: Free form questions whose answer can be\ndirectly extracted from the grounding document\nComparative: Questions that request comparisons\nbetween different entities and/or concepts\nAggregate: Questions that require synthesis of in-\nformation from multiple documents or paragraphs\nUnanswerable: Questions that request information\nunavailable in the given documents\nFor subsequent user turns, we use the following:\nFollow-up: Questions that build on previous\nresponses, typically with elliptical phrases such as\nwhat about, how about, etc.\nClarification: Questions that seek to resolve\nambiguities, as in you mean, does that mean\nCorrection: Questions that aim to rectify errors or\nmisunderstandings from earlier conversations, as\nin no, that's not what I meant"}, {"title": "2.2 Query generation with CoT Prompting", "content": "To ensure that the generated queries conform the\npre-specified question taxonomy, we incorporate\nquestion-type specific CoT prompts that instruct an\nLLM to reason through the grounding document.\nWhile most previous work adopts CoT prompts\nfor accurate answer generation in reasoning tasks\n(Wei et al., 2022; Kojiman et al., 2022; Yao et al.,\n2023), we found it effective for steering an LM to\ngenerate queries of desired properties. We use a\nCoT prompt in zero-shot setting for initial turns and\nfew-shot setting for subsequent turns. In our exper-\niments, we found that using 3 in-context learning\n(ICL) examples of a question type (ex: comparative\nquestions) without CoT prompt resulted in only 5%\nof the intended query type, whereas CoT prompt\nfor a specific query type without any ICL examples\nresulted in more than 90% of the generated queries\nadhering to the intended query type.\nOur data generation framework is flexible\nenough to incorporate any changes to the taxon-\nomy, as a user could create new prompts for addi-\ntional question types, and run the rest of the data\ngeneration pipeline."}, {"title": "2.3 Response generation with CoT Prompting", "content": "For generating the agent answers, we utilize an-\nother CoT prompt. This CoT prompt includes a\nreasoning step to explain how the model looks for\nthe answer in the available documents, and then\ngenerates the answer grounded on the documents\nand generated reasoning. To ensure that the model\ngenerates consistent (w.r.t. the generated reason-\ning) and faithful (w.r.t. the documents), the CoT\nprompt instructs the LLM to perform a consistency\ncheck (inspired by Wang et al. (2022a)), and ex-\ntract evidences from the documents. This dual\nuse of CoT for both query and answer generation\nensures that the interactions are both rich and logi-\ncally consistent. We use greedy decoding for each\nuser and agent turn generation and top-k sampling\n(k=50) (Fan et al., 2018) for all other steps. All of\nthe CoT prompt templates used in this study are\navailable in Figures 3-9 (Appendix)."}, {"title": "2.4 Dialog Generation", "content": "Once the initial user queries are generated accord-\ning to the question taxonomy in \u00a72.1, initial agent\nanswers and all subsequent user/agent turns are gen-\nerated by two separate pipelines - single-document\ngrounded pipeline in \u00a72.4.1 and multi-document-\ngrounded pipeline in \u00a72.4.2."}, {"title": "2.4.1 Single-document grounded dialog", "content": "Algorithm 1 Single-document Grounded Dialog\nGeneration\nInput: Grounding document docg\nOutput: dialogn\n1: history \u03a6\n2: for i in 1 ... n do\n3: qi\u2190 LLM(historyi-1, docg)\n4: ai\u2190 LLM(qi, historyi-1, docg)\n5: history historyi-1 U qi Uai\n6: end for\n7: dialogn historyn\n8: return dialogn\nIn this setup all user and agent turns are\ngrounded on the same gold document. In Algo-\nrithm 1, each query qi (line 3) is generated by an\nLM on the basis of dialog history and the ground-\ning document docg. Given query, dialog history\nand docg, the LM generates an answer ai (line 4).\nThe query and the answer generated at each turn\nis added to the dialog history history; (line 5). Fi-\nnally, historyn is returned as the output dialogn\n(lines 7-8)."}, {"title": "2.4.2 Multi-document grounded dialog", "content": "For multi-document grounded dialog, we integrate\na retriever that dynamically selects top-k relevant\npassages from a pre-constructed document index.\nIn Algorithm 2, a dialog starts with the initial query\ngenerated by Algorithm 1. At turn 1, the retriever\nretrieves top-k passages passage1,k given the seed\nquery q1 and store them as passagem (lines 3-4).\nGiven the query and grounding passages, an LM\ngenerates an answer a\u2081 (line 5) and q\u2081 and a\u2081 get\nstored in the dialog history history1 (line 6).\nFor subsequent turns, the LM generates a query\ngiven dialog history and retrieved passages (line\n9). The retriever retrieves top-k passages given\nquery and dialog history. Each retrieved passage\nis added to passagem if and only if the passage is\nnew (lines 11-15). The LM generates an answer\ngiven query, dialog history and unique passages\nretrieved so far, passagem (line 16). Finally, the\ndialog historyn is returned as the output dialogn\nalong with passagem, a collection of unique pas-\nsages retrieved during the session."}, {"title": "2.4.3 Retriever", "content": "Each document is split into passages with max-\nimum 512 tokens with 100 overlapping tokens\nbetween passages for indexing. We indexed the\n11,377,951 passages from (Qu et al., 2020) to\ncompare the performance of models fine-tuned\non human generated benchmark training set and"}, {"title": "2.5 LLM-as-a-Judge for Correctness", "content": "We incorporate an LLM-as-a-Judge module to filter\nout queries with incorrect answers. For each gen-\nerated dialog, we first extract all context-response\npairs, consisting of dialog history plus the current\nquery and its answer, as shown in Figure 1. We then\nask an LLM to judge whether the current answer is\ncorrect or not given the dialog history and the cur-\nrent query. We filter out all context-response pairs\nwhich are judged to be incorrect. We include the\nCoT prompt of the LLM-as-a-Judge in Figure 10\n(Appendix)."}, {"title": "2.6 Data Quality Evaluation", "content": "To assess the quality of the synthetic data (Mehri\nand Eskenazi, 2020; Smith et al., 2022), we carry\nout human evaluations on 294 dialogs comprising\n712 user-agent turns. We use the following crite-\nria for evaluation: plausibility and answerability\nfor user queries, correctness for agent answers, di-\nversity and coherency for the overall dialog. Each\ncriterion is described in Table 7 (Appendix). We\ntrained 7 professional annotators with 3 or more\nyears of work experience with the instructions in\nFigure 11 (Appendix).\nResults: The results are summarized in Table 1.\nThe synthetic dialogs exhibit a very high degree\nof diversity and coherency and most queries are\nconsidered highly plausible. Answer correctness\nis 73.1%. The ratio between answerable and unan-\nswerable queries is 85.2 vs. 14.8. Inter-annotator\nagreements range from 42% to 90% with around\n70% on average, as shown in Table 2. Table 3\nindicates an apparent correlation between query\nanswerability and answer correctness. Answerable\nqueries are more likely to lead to correct answers\nthan unanswerable queries."}, {"title": "3 Experimental Results", "content": "We answer the following questions through our\nexperiments: (1) What is the difference in per-\nformance when using generated synthetic data in"}, {"title": "3.1 Automatic Evaluations", "content": "We evaluate the model output qualities with 4 au-\ntomatic metrics: (1) F1, standard metric used in\nChoi et al. (2018), Reddy et al. (2019), Feng et al.\n(2021), (2) RougeL, (3) Bert-Recall, (4) Recall,\nthe metric reported to achieve the highest corre-\nlation with human evaluation for answer correct-\nness of instruction following models in question\nanswering (Adlakha et al., 2023). We study the\nmodel performance in four settings: (i) zero-shot,\n(ii) human-generated benchmark training data (hu-\nman benchmark), (iii) synthetic data with unan-\nswerable queries and without the LLM-as-a-Judge"}, {"title": "3.2 Human Evaluation", "content": "We designed a human evaluation protocol that asks\nfor human preference on two model outputs. In-\nstructions and sample questions are given in Fig-\nure 13 (Appendix). We randomly draw samples\nfrom answerable queries, 135 from QuAC, 100\nfrom MultiDoc2Dial, and 100 from CoQA.7 We re-\ncruited 1 annotator for QuAC and MultiDoc2Dial,\nand 3 others for CoQA among professional annota-\ntors with 3 or more years of experience. The results\nare shown in Figure 2. Across all three test sets,\nhuman annotators highly prefer the outputs from\nthe models fine-tuned on the synthetic data to those\nfine-tuned on the human benchmark data. Espe-\ncially for CoQA, for which automatic evaluations\nwere not decisive in determining the quality of the\ntwo models, three human annotators overwhelm-\ningly prefer synthetic (66%) to human benchmark\n(7.4%)."}, {"title": "3.3 Multi-Document Grounded Dialogs", "content": "To the best of our knowledge, we present the first\nmulti-document grounded multi-turn dialog gener-\nation pipeline that mimics RAG deployments for\npassage retrieval, \u00a72.4.2. To evaluate the effective-\nness of this technique, we evaluate the OR-QuAC\ntest set on two fine-tuned models: One fine-tuned"}, {"title": "6 Limitations", "content": "The method employs MIXTRAL-8X7B-INSTRUCT-\nv0.1 in the public domain. Therefore, the gener-\nated synthetic data can be susceptible to the limita-\ntions of the language model, particularly the biases\ninherent in the training data which may be harmful\nleading to synthetic data with hate, abuse and social\nstereotypes.\nFor multi-document grounded generation, there\nis no straightforward way of applying unanswer-\nable query taxonomy to document retrieval, which\nalways tries to retrieve relevant documents to a\ngiven user query. The query answerability may be\ndetermined only after examining whether or not the\nretrieved passages contain the correct answer to the\nquery. This is unlike the single-document grounded\ndata generation pipeline for which the grounding\ndocument is provided before query generation and\ntherefore, one can steer an LM to generate a query\nwhose answer cannot be found in the provided doc-\nument. In addition, the benchmark test sets used\nin the evaluations have dataset specific categorical\nanswers for unanswerable queries whereas the syn-\nthetically generated dialogs have often correct nat-\nural, contextual responses with no lexical overlap\nwith the categorical reference answers. This may\nunder-report any benefit from the use of our syn-\nthetic data for evaluation of unanswerable queries.\nWe leave evaluation and generation of unanswer-\nable queries in a multi-document grounded dialog\ngeneration setup to future work."}]}