{"title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects", "authors": ["ISLEM BOUZENIA", "MICHAEL PRADEL"], "abstract": "The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that installs arbitrary projects, configures them to run test cases, and produces project-specific scripts to reproduce the setup. Inspired by the way a human developer would address this task, our approach is a large language model-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/55 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of 0.16 dollars, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.", "sections": [{"title": "1 INTRODUCTION", "content": "Executing the test suite of a software project is a critical step in various activities during software development and software engineering research. For human developers who are contributing to open-source projects, running the tests before submitting a pull request ensures that their changes do not introduce regressions. Likewise, the increasing popularity of large language model (LLM) agents that autonomously edit a project's code [3, 20, 32, 41, 44] creates a huge demand for a feedback mechanism to validate modifications [31], and executing the test suite provides such a mechanism. Finally, researchers also depend on running tests, e.g., to evaluate the effectiveness of dynamic analyses [8] or to create benchmarks involving test execution, such as Defects4J [16], SWE-Bench [14], and DyPyBench [4].\nUnfortunately, executing the tests of an arbitrary project is far from straightforward in practice. Projects are developed in different programming languages and ecosystems, each with its own set of tools, dependencies, and conventions. Complex dependencies can pose significant challenges, especially when specific versions of libraries or tools are required. Documentation is often incomplete, inconsistent, or entirely missing, forcing developers to infer the necessary steps. Moreover, projects may have implicit assumptions about the environment, such as operating system specifics or required system configurations, that are not explicitly stated. The diversity of testing frameworks further complicates the process, as each framework has a unique setup and execution procedure. Currently, there are three primary methods for executing the tests of a given project, each with notable limitations. The first method involves manually following the project's documentation and resolving any issues through trial-and-error. This approach is time-consuming and does not scale well with the number of projects. The second method is to reuse existing continuous integration/continuous deployment (CI/CD) workflows employed by project maintainers. However, not all projects have such workflows, and even when they exist, their execution often depends on a specific CI/CD platform, such as GitHub Actions, which is not fully accessible to the public. Directly using CI/CD workflows is further complicated by the fact that there are several popular platforms, each with its own configuration scripts and technology stack. The third method is to implement an automated, heuristic script designed to cover common cases. Such scripts typically focus on a single programming language and ecosystem. They lack the flexibility to handle arbitrary projects, as evidenced by studies highlighting the limitations of such scripts in diverse environments. For example, Flapy [12] works only on Python projects that are available on PyPy, and even within the Python ecosystems fails to successfully run many test suites. Another example is the pipeline used to create GitBug-Java [30], which starting from 66,042 repositories eventually managed to execute the tests of only 228 repositories.\nAn effective solution would need to address multiple challenges. First, the approach should be aware of the latest technologies and tools for a range of popular programming languages. Second, the technique should be capable of understanding incomplete and partially outdated documentation, as often encountered in practice. Third, the approach needs a way to interact with the system, such as executing commands, monitoring outputs, and handling errors. Finally, the approach must be able to assess whether the setup process has been successful, and if not, address any problems until the test suite runs successfully. To the best of our knowledge, there is currently no existing technique that addresses these challenges. This gap presents a substantial obstacle for developers, automated coding techniques, and researchers who need a reliable and scalable solution for running tests across a wide variety of projects. Motivated by these challenges, the question addressed in this work is: How can we automatically install and run the test suite of an arbitrary project?\nThis paper presents ExecutionAgent, the first LLM-based agent for autonomously setting up arbitrary projects and executing their test suites. The approach addresses the four challenges described above as follows. For the first challenge, we present the novel concept of meta-prompting, which asks a recently trained LLM to automatically generates up-to-date, technology-specific, and programming language-specific guidelines instead of manually engineering and hard-coding a prompt. We address the second challenge by using the LLM to parse and understand the project's documentation and web resources related to the project. To handle the third challenge, we connect the agent to a set of tools, such as a terminal, to execute commands, monitor outputs, and interact with the system. Finally, we address the fourth challenge by enabling the agent to iteratively refine its process based on feedback from previous steps, similar to how a human developer would work.\nTo evaluate ExecutionAgent, we apply it to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully sets up and executes the test suites of 33/50 projects. Comparing to several baselines, such as manually designed and LLM-generated scripts targeting projects in a specific language, as well as a general-purpose LLM-based agent, we find ExecutionAgent to outperform the best available technique by 6.6x. To validate the test executions, we compare them against a manually established ground truth and find that the test results (in terms of passed, failed, and skipped tests) closely resemble the ground truth, with an average deviation of 7.5%. Studying the costs of the approach, we find that the average execution time is 74 minutes per project, and the average LLM costs are 0.16 dollars per project. Overall, these results demonstrate that ExecutionAgent has the potential to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.\nIn summary, this paper contributes the following:"}, {"title": "2 APPROACH", "content": "The following presents our approach for executing the test of arbitrary projects. We start by defining the problem we aim to address (Section 2.1), then provide an overview of our approach (Section 2.2), and finally describe the components of our approach in detail (Sections 2.3 and 2.4)."}, {"title": "2.1 Problem Statement", "content": "The problem we aim to address is the following: Given a software project, identified, e.g., by a URL of a git repository or a path on the local disk, we want to automatically generate scripts to setup and run the tests of the project. Specifically, the desired output consists of two scripts: one to create an isolated environment, such as a container, and one to setup up the project and run its tests within the isolated environment. By running the tests, we mean executing the project's test suite and collecting the results, such as the number of tests that run, pass, and fail.\nWe aim to address this problem in a way that offers two important properties. First, the approach should be technology-agnostic, i.e., it should support projects written in different programming languages, using different build systems, and using different testing frameworks. This property is crucial as it allows the approach to be used across a wide range of projects. Second, the approach should be fully automated, i.e., it should not require any manual intervention or additional information beyond the project itself. This property is essential to ensure that the approach can be used at scale and without human intervention.\nThe formulated problem has, to the best of our knowledge, not been addressed by prior work. In particular, existing approaches either focus on specific programming languages or ecosystems [12, 30] or are based on significant manual intervention [4]."}, {"title": "2.2 Overview of ExecutionAgent", "content": "We address the problem of executing the test suite of arbitrary projects using an approach that leverages LLMs and the concept of LLM agents. By LLM agent we here mean a system that uses an LLM to autonomously interact with a set of tools in order to achieve a specific goal. The tools we use are similar to those a human tasked with setting up a project might use, such as commands available via a terminal. The intuition behind this approach is that the LLM can \u201cunderstand\u201d various sources of information, such as project documentation, existing scripts, and the output of tools, and use this understanding to decide on the steps necessary to execute the tests of the given project. As shown in Figure 1, the approach encompasses two phases, which we briefly describe in the following.\nPhase 1: Preparation. Given a project repository, this phase gathers information required to construct the initial prompt of the agent, called prompt ingredients. A key challenge is creating these prompt ingredients in a technology-agnostic way, as the project may be written in any programming language and use any testing framework. Our approach addresses this challenge through meta-prompting, a novel concept that allows the agent to query an LLM for the latest guidelines and technologies related to installing and running test suites. Specifically, Execution-Agent uses meta-prompting to generate language-specific guidelines, guidelines about the latest containerization technology, and possible locations of commonly used CI/CD scripts. In addition, the approach queries the web to gather hints on installing the given project. The gathered information is then passed as prompt ingredients to the second phase.\nPhase 2: Feedback loop. This phase repeatedly invokes tools, as guided by the LLM, in order to install the project and execute its test suite. Specifically, ExecutionAgent repeatedly iterates through three steps. Step 1 queries the LLM for the next command to execute, using a dynamically updated command prompt that contains the prompt ingredients from Phase 1 and a summary of the already invoked commands. Step 2 executes the command suggested by the LLM by invoking one of the tools. Because the output of a tool may be verbose and contain irrelevant information, step 3 requests the LLM to summarize the output and extract the most relevant information. The summarized output is then used to update the command prompt, and the three steps repeat until the agent determines that the test suite has been successfully executed. This entire process is managed by a component we call the control center.\nThe steps taken by ExecutionAgent are described in more detail in Algorithm 1, which we will explain in the following sections."}, {"title": "2.3 Preparation Phase", "content": "The preparation phase is motivated by two goals: (1) to gather project-specific information that could be helpful for installing the given project and running its tests, and (2) to obtain guidelines that will help the LLM agent in invoking the right tools and commands during the feedback loop in phase 2. To reach these goals, we follow a set of steps that are detailed in Algorithm 1 between lines 1 and 6, and which we present in detail in the following. Since several of these steps use the idea of meta-prompting, we begin by explaining this concept.\n2.3.1 Meta-Prompting. A key challenge faced by ExecutionAgent is that technologies and best practices for installing software projects and running test suites are constantly evolving. One way to address this challenge would be to spend significant amounts of time in engineering suitable prompts for the LLM agent. Such prompts should provide guidelines on how to use common build tools, package managers, testing frameworks, etc. for all the programming languages, software ecosystems, and tools that the agent might encounter. However, this approach comes with several drawbacks: it is time-consuming, is unlikely to cover all relevant technologies, and may become outdated quickly."}, {"title": "2.3.2 Language-Specific Guidelines", "content": "During initial experiments, we observed that the LLM agent benefits from guidelines that describe how to typically install and run tests of projects written in a specific programming language. Adopting the idea of meta-prompting, ExecutionAgent creates such guidelines by querying an up-to-date LLM to generate a list of language-specific guidelines. To this end, we heuristically determine the main programming language of the project (using GitHub's Linguist library\u00b9) and use this information to query the LLM. Figure 2 illustrates an example of a meta-prompt for a Java project and the response from the LLM."}, {"title": "2.3.3 Container Guidelines", "content": "To ensure that the project's tests are executed in an isolated environment, ExecutionAgent aims at installing and testing the project inside a container. Similar to the language-specific guidelines, we use meta-prompting to query the LLM for guidelines on the latest and most used containerization technology. For example, at the moment of writing, the GPT-40 model responds to our meta-prompt by stating that Docker is the best containerization technology for setting up an isolated environment and by providing advice on using Docker."}, {"title": "2.3.4 Existing CI/CD Scripts", "content": "Some projects include CI/CD scripts that provide hints about installation steps and project dependencies. To benefit from such scripts, ExecutionAgent searches for such files in the repository. While sometimes outdated and incomplete, the existing scripts may give hints about the overall process and non-standard steps to take. Instead of hard-coding specific file paths, such as .github/workflows/main.yml, ExecutionAgent uses meta-prompting to query the LLM for common locations, file names, and file extensions of CI/CD scripts within a repository. Based on the response, the approach searches for such files within the project. Because the files identified may be long and contain irrelevant information, we try to extract the most relevant parts of the scripts. To this end, ExecutionAgent queries an LLM with the raw file, asking the model to extract four kinds of information: a short natural language summary of the script, a list of dependencies of the project, any commands that are important for setting up and testing the project, and a list of relevant files or links. We call these four pieces of information a summary. Figure 3 shows an example of the LLM's response to the summarization prompt."}, {"title": "2.3.5 Web Search", "content": "For popular projects, there is often information available on the web that can help with the installation process and that goes beyond the information available in the repository itself. To retrieve such information, ExecutionAgent queries a search engine with the following query \u201cHow to install the <LANGUAGE> project '<PROJECT>' from source code?\u201d, where <LANGUAGE> and <PROJECT> are replaced with the main programming language and the name of the project, respectively. The approach then extracts all text from the top-five results and asks an LLM to summarize it into the same structure as shown in Figure 3. In case a search result does not contain any relevant installation hints, the model is instructed to return \u201cIrrelevant web page\u201d."}, {"title": "2.4 Feedback Loop", "content": "Based on the prompt ingredients obtained in the preparation phase, the second phase of our approach is a feedback loop that iteratively invokes tools to install the project and execute its test suite. As shown in lines 8 to 31 of Algorithm 1, the feedback loop consists of two nested loops. The inner loop runs a series of commands, guided by the LLM, until either the test suite completes successfully or a configurable command limit (default: 40) is reached. The outer loop allows for multiple attempts to install and execute the test suite, with a configurable maximum number of attempts (default: 3). This design allows recovery from occasional failures where the installation process encounters an error, preventing the tests from running. By allowing multiple attempts, the system can adjust its approach to overcome such errors. At the start of each outer loop iteration, the system incorporates lessons learned from the previous attempt (if any) into the command prompt. These lessons are generated by prompting the LLM to analyze the previous sequence of thoughts and commands, identify challenges encountered, and suggest adjustments for the next iteration (line 30). Each iteration of the inner loop consists of three steps, as outlined in Figure 1 and detailed in the following."}, {"title": "2.4.1 Step 1: LLM Agent to Select the Next Command", "content": "The core of our approach is an LLM that selects the next command to execute based on the current state of the installation process. To guide the LLM in selecting the next command, the approach constructs a command prompt that contains the prompt ingredients obtained in the preparation phase, as well as a summary of the commands executed so far. The command prompt consists of static sections, which are the same any time ExecutionAgent is used, and dynamic sections, which are either created based on the preparation phase (line 12) or updated at the end of each iteration of the inner loop (line 26). Specifically, the prompt contains the following sections:"}, {"title": "2.4.2 Step 2: Invoking Tools", "content": "To enable our approach to take the steps necessary for installing the project and running its tests, we provide four tools to the agent. ExecutionAgent invokes these tools based on the LLM's response to the command prompt (lines 17 to 22).\nTerminal. Similar to human developers, access to a terminal is crucial to successfully set up a project and run its tests, e.g., to install dependencies, configure the environment, or list available files. We provide the agent with the capability to execute any command available in a Linux terminal via the terminal tool. The tool takes a command to execute as input and returns the output of the command.\nFile I/O Tools. While the terminal would, in principle, be sufficient to perform any kind of operation on the system, we provide two additional tools for interacting with the file system. These tools are meant to emulate the developer's interaction with text editors, where they open a file, read parts of it, and sometimes make changes by writing to it. The read_file tool takes a file path as input and returns the content of the file. The write_file tool takes a file path and content as input and writes the content to the file. The latter tools is particularly useful for writing the scripts expected as the output of ExecutionAgent, e.g., a Dockerfile that creates a container and a install.sh script that installs the project and runs its tests.\nEnd of Task. In addition to the tools described above, we provide the agent with a special tool called task_done. This tool is used to signal that the agent has successfully completed its task, i.e., that the project has been installed and its tests have been executed. The tool takes a natural language description that explains why the agent has finished the task as its argument."}, {"title": "2.4.3 Step 3: Summarization and Extraction", "content": "The output of tools can be verbose and contain lots of irrelevant information. For example, suppose the agent executes a command to list all files in a directory or reads the content of a file, then simply returning the output of the tool to the LLM for the next command would quickly fill up the available prompt size. Even with LLMs that offer a large prompt size, it is beneficial to reduce the amount of information in the prompt to keep the agent focused on the most important information and to reduce the overall costs of the approach. To reduce the amount of text that results from a tool invocation, ExecutionAgent shortens the output whenever the output exceeds a certain number of tokens (default: 200). The approach asks another LLM to summarize the output and extract the most relevant information (lines 24 to 25). The expected format for the summary is the same as illustrated in Figure 3. Once the output of the most recently invoked command has been summarized, the approach appends the command and its output summary to the tool invocation history section of the command prompt (line 26)."}, {"title": "2.4.4 Control Center", "content": "The control center combines the three steps described above and manages the interaction between the LLMs and the tools. In particular, it performs the following tasks:\n\u2022 Parse the LLM output and validate whether it conforms to the specified output format. We empirically observe that the LLM output is syntactically well-formed in almost all cases.\n\u2022 Invoke the next step as specified in Algorithm 1, e.g., by invoking a specific command.\n\u2022 For any tool invocation, check whether the command is returning within a configurable timeout (default: five minutes). In case the command has not terminated within this timeout, the control center provides the so far produced output to the LLM agent and asks to decide between three options: wait another five minutes, provide some input to the running tool, or kill the command. Providing some input to the already running tool is useful for interactive tools that require user input, e.g., a user confirming with \u201cy\u201d that some package should be installed.\n\u2022 Clean the output of the tools by removing terminal color characters and other special characters, e.g., printed to display a progress bar.\nWhen the agent selects the task_done command, it indicates that the agent believes that the project has been successfully installed and that its test suite has been executed. Before terminating ExecutionAgent, the control center validates that the agent has indeed met all goals. This involves checking whether three files exist: a file to create a container (e.g., a Dockerfile), an installation script to be executed within the container, and a file that contains the test results (i.e., the number of passed, failed, and skipped tests). If all goals are met, ExecutionAgent returns the installation and test scripts to the user (line 21). Otherwise, the control center provides feedback to the LLM agent, pointing out what exactly is missing, and asks for a new command.\nFigure 5 gives an example of the scripts output by ExecutionAgent for the OpenVPN project. The two scripts show the configuration chosen by ExecutionAgent, the installed dependencies, and the steps to install and execute the tests of the project."}, {"title": "3 EVALUATION", "content": "Our evaluation aims to answer to following research questions:\n\u2022 RQ1 (effectiveness): How effective is ExecutionAgent at correctly setting up projects and running their test cases?\n\u2022 RQ2 (costs): What are the costs of ExecutionAgent in terms of execution time and token usage when interacting with the LLM?\n\u2022 RQ3 (ablation study): What is the impact of different components and configurations of Execution-Agent?\n\u2022 RQ4 (trajectories): How does ExecutionAgent interact with the tools, and what trajectories does it take to reach the goal?"}, {"title": "3.1 Experimental Setup", "content": "3.1.1 Implementation and Model. ExecutionAgent is implemented in Python and bash. To allow for an isolated execution of ExecutionAgent itself, we use a Docker container. The agent is empowered by OpenAI's GPT-40-mini model, which we access through their Python API."}, {"title": "3.1.2 Metrics", "content": "The task performed by ExecutionAgent consists of two sub-tasks: (i) building and/or installing the project, and (ii) running the test suite. We measure how effective the approach is at performing these tasks by measuring the successful build rate, i.e., the proportion of projects where the approach manages to correctly build and install the project, and the successful testing rate, i.e., the proportion of projects where the approach manages to run the test suite. To determine these rates, we manually inspect the produced scripts and the test results.\nTo better understand the output of ExecutionAgent, we measure the script size of the produced scripts, which we define as the number of commands, excluding any comments, white spaces, and \u201cecho\u201d commands. We split composite commands and count their components individually. For example, the line make && make test is counted as two commands make. and make test.\nTo validate the test suite executions, we compare the test execution results produced by Execution-Agent with a manually established ground truth. The ground truth is obtained by searching for CI/CD logs of the targeted projects, and by extracting the number of passed, failed, and skipped tests. We compare the results of ExecutionAgent with the ground truth in terms of these three numbers. Specifically, we compute the relative deviation compared to the ground truth as deviation = $\\frac{| (nb_{EA}-nb_{GT}) |}{nb_{GT}} *100$, where $nb_{EA}$ is the number of tests produced by ExecutionAgent and $nb_{GT}$ is the number of tests in the ground truth. For example, if among the tests executed by ExecutionAgent, there are 95 passing tests, while the ground truth has 100 passing tests, then the deviation is 5%. We compute a separate deviation for the number of passing, failing, and skipped tests, and then average these deviations to obtain the overall deviation."}, {"title": "3.1.3 Dataset", "content": "We gather a set of 50 open-source projects from GitHub that provide three properties: (i) The projects should cover different programming languages. To this end, we sample for each of the following languages ten projects that have that language as their dominant language or second dominant language: Python, Java, C, C++, and JavaScript. Because many projects contain code in multiple languages, the dataset overall covers 14 programming languages.\n(ii) There exists a ground truth of test execution results that we can extract from the logs on a CI/CD platform. As different projects use different platforms, we collect ground truth data from GitHub Actions, CircleCI, Jenkins, CirrusCI, and occasionally, a project's website (e.g, tensorflow).\n(iii) Each project has at least 100 starts and 100 commits with at least one commit in the past 6 months (prior to collection). Table 1 provides an overview of the projects in the dataset. Projects in our dataset have an median number of 10K commits, 45K stars, and 1.4K test cases."}, {"title": "3.1.4 Baselines", "content": "To the best of our knowledge, there is no existing technique that addresses the same problem as ExecutionAgent. However, we compare our approach to three related baselines, which represent the state of the art in this domain.\nLLM scripts. Given that LLMs have seen large amounts of data, including documentation on how to install projects, we leverage this feature to ask an LLM to generate a script that installs and runs test cases of an arbitrary project in a specific programming language. The prompt we use is given in Figure 6. As our dataset is gathered by focusing on five main languages, we generate one specialized script for each of these five languages. Following that, each project is attempted to be installed using the script corresponding to its main language.\nAutoGPT. Instead of creating a specialized agent for the task of automatically setting up arbitrary projects, one could also use a general-purpose LLM-based agent. We compare against such an agent, AutoGPT\u00b2 [40], which given a task description, autonomously reasons about a task, makes a plan, executes, and updates the plan over multiple iterations. Similar to ExecutionAgent, AutoGPT\n\u00b2https://github.com/Significant-Gravitas/AutoGPT\nmay call tools, such as web search, reading and writing files, and executing Python code. As a task description, we provide the input shown in Figure 7. We use the same model and provide the same budget (max number of iterations) to AutoGPT as for ExecutionAgent.\nFlapy. This baseline is a human-written script written to automatically setup arbitrary Python projects and run their test cases\u00b3. The script was originally developed as part of a study of test flakiness [12], and by design, is limited to Python projects."}, {"title": "3.2 Effectiveness", "content": "ExecutionAgent. The results of applying ExecutionAgent to the 50 projects are reported in Table 1 and summarized in Table 2. ExecutionAgent successfully installs and tests 33 out of 50 projects. Out of the 33 successfully tested projects, ExecutionAgent achieves results identical to the ground truth in 17/33, while the rest (16 out 33) have an average deviation of 15.4%. In Table 2, we consider ExecutionAgent results to be close to the ground truth if the average deviation is less than 10%. Overall, these results show that ExecutionAgent is effective at executing the test suites of a large number of projects, and that the results are close to or equal to the ground truth in most cases.\nTo better understand the results, we analyze the projects by their main language (Figure 8). For each language, we show the number of projects that fail to build, are successfully built, and have their test suites executed. The results show that ExecutionAgent is most effective for Java, where it successfully builds and tests all projects. The two languages that are the most difficult to handle are C and C++, which we attribute to the less standardized build and test processes in these languages, which often requires recompiling packages to be compatible with current system dependencies and project requirements.\nComparison with LLM scripts. In Table 2, we compare ExecutionAgent to the three baselines. The general-purpose, LLM-generated scripts successfully build many projects (29/50), but then often often fail to execute the test suites (only 5/29 succeed). Inspecting the results, we find that the LLM scripts often do not account for the differences between the setup required by a regular\n\u00b3https://github.com/se2p/FlaPy\nuser and the development setup. The development setup often involves additional steps, such as installing additional software (e.g., a compiler, glibc-32, or a testing framework), recompiling some dependencies, or changing configuration files. In contrast, the usage setup is much simpler, often using an already complete requirement/setup file, such as setup.py for Python and pom.xml for Java. Because the LLM scripts do not account for these differences, they often fail to execute the test suites, whereas ExecutionAgent is able to iteratively fix unexpected errors.\nComparison with AutoGPT. Even though AutoGPT is given the same budget as ExecutionAgent, it builds only 9/50 projects and successfully runs the tests suites for only four of them. The results show that the task of running the tests of arbitrary projects is non-trivial, and that a specialized agent, such as ExecutionAgent is more effective at such tasks. Fundamentally, ExecutionAgent differs from AutoGPT by using meta-prompting, by managing the memory of already performed commands more effectively, and by using more sophisticated and robust tools.\nComparison with Flapy. The right-most column of Table 2 compares with the Flapy baseline. Because this baseline only targets Python, we apply it only to the ten projects in our dataset that have Python as their main language. The results show that Flapy reaches results similar to LLM-generated, general-purpose scripts in terms of building projects. However, like the LLM scripts, it also struggles to execute the test suites, and cannot successfully complete the tests of any of the ten projects. Given the same ten Python projects, ExecutionAgent successfully builds and tests six of them, all with testing results identical to the ground truth."}, {"title": "3.3 Costs", "content": "We evaluate the costs incurred by ExecutionAgent in terms of execution time and token usage. The average time required by ExecutionAgent to process a project is 74 minutes (on a 256GB RAM Xeon(R) Silver 4214 machine with five projects being processed in parallel). Given that the most reliable alternative to our approach is to manually set up and test the projects, we consider the time costs of ExecutionAgent to be acceptable.\nExecutionAgent relies on a large language model, which incurs costs computed in terms of token usage. Figure 9 shows the monetary costs due to the usage of the LLM. Based on current pricing, the average cost of processing a project is 0.16 USD. The costs differs significantly depending on whether ExecutionAgent succeeds in building and testing the project. For projects that fail to build or test, the costs are higher because ExecutionAgent tries to fix any issued until exhausting the budget, leading to an average cost of 0.34 dollars. In contrast, for projects where the approach succeeds, the costs are lower, with an average of only 0.10 dollars. Overall, we consider the current costs to be acceptable given the benefits of the approach, and expect costs to decrease over time as LLMs become more efficient and cheaper."}, {"title": "3.4 Ablation", "content": "To understand the importance of the two phases of ExecutionAgent, we conduct an ablation study featuring two variants of our approach: (1) ExecutionAgent without the preparation phase: In this variant, the approach starts directly with the feedback loop, i.e., without information obtained via meta-prompting and web search. (2) ExecutionAgent without the feedback loop: This variant replaces the feedback loop with a single query that provides the information collected in the preparation phase and asks the LLM to generate a full script that installs the given project and runs its tests. Due to budget limitations, we run theses variants on ten projects subsampled randomly while ensuring to include two projects per main language.\nThe results of these two variants, as well as the results of the full approach, are presented in Table 3. We find that the full approach is the most effective, with a successful build rate of 8/10 and a successful test execution rate of 7/10. In contrast, omitting either of the two phases leads to a significant decrease in effectiveness, with only 1 successfully executed test suites achieved by the no-feedback variant. Because the variants perform overall less work, they also incur lower costs. However, the moderate cost per project, along with the clear benefits of the full approach, show the value of the ExecutionAgent.\nOmitting the preparation phase led to the agent not having a concrete plan and also ignoring best practices. Without the feedback loop, the Agent is similar to the LLM-scripts baseline but with more information and context in the prompt."}, {"title": "3.5 Analysis and Discussion", "content": "3.5.1 Tools Usage. To understand the behavior of ExecutionAgent", "end of task": "ool accounts for 1% of all tool invocations. This high-level analysis shows that ExecutionAgent uses all available tools", "other": "eans all commands beyond those shown explicitly. We find that commands for exploring directories and files"}, {"other": "hows that ExecutionAgent uses a wide range of commands", "trajectory": "efers to the sequence of steps that ExecutionAgent takes when dealing with a specific project. The following describes three observations made during this analysis.\nRecurring phases: Setup", "phases": 1, "Setup": "The agent typically starts with preparation commands, such as 1s, followed by reading necessary files (e.g., README."}]}