{"title": "SVIP: TOWARDS VERIFIABLE INFERENCE OF OPEN-SOURCE LARGE LANGUAGE MODELS", "authors": ["Yifan Sun", "Yuhang Li", "Yue Zhang", "Yuchen Jin", "Huan Zhang"], "abstract": "Open-source Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language understanding and generation, leading to widespread adoption across various domains. However, their increasing model sizes render local deployment impractical for individual users, pushing many to rely on computing service providers for inference through a blackbox API. This reliance introduces a new risk: a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby delivering inferior outputs while benefiting from cost savings. In this paper, we formalize the problem of verifiable inference for LLMs. Existing verifiable computing solutions based on cryptographic or game-theoretic techniques are either computationally uneconomical or rest on strong assumptions. We introduce SVIP, a secret-based verifiable LLM inference protocol that leverages intermediate outputs from LLM as unique model identifiers. By training a proxy task on these outputs and requiring the computing provider to return both the generated text and the processed intermediate outputs, users can reliably verify whether the computing provider is acting honestly. In addition, the integration of a secret mechanism further enhances the security of our protocol. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that SVIP is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, SVIP achieves false negative rates below 5% and false positive rates below 3%, while requiring less than 0.01 seconds per query for verification.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Large Language Models (LLMs) have achieved unprecedented success across a broad array of tasks and domains (Achiam et al., 2023; Dubey et al., 2024; Yang et al., 2024). Alongside this progress, open-source LLMs have proliferated, offering increasingly sophisticated and capable models to the broader research community (Touvron et al., 2023b; Black et al., 2022; Le Scao et al., 2023; Jiang et al., 2023; Almazrouei et al., 2023; Zhang et al., 2023). Many of these open-source LLMs now rival, or even surpass, their closed-source counterparts in performance (Chiang et al., 2023; Almazrouei et al., 2023; Dubey et al., 2024), while remaining freely accessible. However, as model capacity grows, it typically comes with a corresponding increase in the number of model parameters, which directly drives up the computational demands, particularly in terms of memory and processing power (Kukreja et al., 2024). For example, the Llama-3.1-70B model requires 140 GB of GPU memory for inference under FP16 (Hugging Face, 2024). These escalating computational costs have made local deployment increasingly impractical, pushing many users to turn to computing service providers for inference.\nTo ease the deployment of LLM inference, computing providers may provide API-only access to users, hiding implementation details. A new risk arises in this setting: how to ensure that the outputs from a computing provider are indeed generated by the requested LLM? For instance, a user might request the Llama-3.1-70B model for complex tasks, but a dishonest computing provider could substitute the smaller Llama-2-7B model for cost savings, while still charging for the larger model. The smaller model demands significantly less memory and processing power, giving the computing provider a strong incentive to cheat. Restricted by the black-box API access, it is difficult for the user to detect model substitution. Particularly, future computing service may be provided by decentralized individuals (Uriarte & DeNicola, 2018) who sell their computation power at a competitive price, but the computation outputs may not be trustworthy. This highlights the need for verifiable inference, a mechanism designed to ensure that the model specified by the user is the one actually used during inference. Implementing verifiable inference for LLMs is essential not only for safeguarding users' interests but also for fostering trust in the open-source LLM ecosystem, driving low-cost and wider adoption and continued development of more advanced LLMs.\nAn effective verifiable inference solution for LLMs must accurately confirm that the specified model is being used during inference, while maintaining computational efficiency. Simple solutions, such as probing the model with established benchmark datasets, may be easily detected and bypassed. On the other hand, cryptographic verifiable computing methods, which rely on generating mathematical proofs (Yu et al., 2017; Setty et al., 2012) or secure computation techniques (Gennaro et al., 2010; Laud & Pankova, 2014), are often too computationally expensive for real-time LLM infer-ence. For instance, zkLLM, a recent Zero Knowledge Proof-based technique, requires over 803 seconds for a single prompt query (Sun et al., 2024). Game-theoretic protocols such as Zhang et al. (2024) involve the interaction of multiple computing providers with carefully designed penalties and rewards, assuming all providers are rational, flawless, and non-cooperative, which might be unreal-istic in practice. Meanwhile, watermarking and fingerprinting techniques (Kirchenbauer et al., 2023; Xu et al., 2024) are mostly implemented by model publishers, making them unsuitable for verifiable inference, where the verification primarily occurs between the user and the computing provider.\nIn this paper, we propose SVIP, a Secret-based Verifiable LLM Inference Protocol using interme-diate outputs. The core idea of our method is to require the computing provider to return not only the generated text but also the processed intermediate outputs (hidden state representations) from the LLM. We carefully design and train a proxy task exclusively on the hidden representations pro-duced by the specified model, effectively transforming these representations into a distinct identifier for that model. During deployment, users can verify whether the processed hidden states returned by the computing provider come from the specified model by assessing their performance on the proxy task. If the returned outputs perform well on this task, it provides strong evidence that the correct model was used for inference, while poor performance suggests otherwise.\nOur approach does not depend on expensive cryptographic proofs or protocols, and is highly effi-cient. Furthermore, it does not involve retraining or fine-tuning the LLMs, operates independently of the model publisher, and can be applied to any LLM with publicly available weight parameters, making it widely applicable. Next, we formalize and illustrate our proposed framework in detail."}, {"title": "2 PROBLEM STATEMENT", "content": "We begin by formalizing the verifiable inference problem in the context of LLMs. We consider three parties: the user, the computing provider, and a trusted third party. The user intends to use a specified LLM, $M_{spec}$, to perform inference on a prompt $x \\in V^*$, where $V^*$ represents the set of all possible string sequences for a vocabulary set V. Lacking sufficient computational resources to run $M_{spec}$ locally, the user relies on the computing provider to execute the model and accordingly pays for the service. Ideally, the computing provider would run $M_{spec}$ as requested and return the completion. However, a dishonest provider might stealthily substitute an alternative LLM, $M_{alt}$, which could be significantly smaller than $M_{spec}$ in terms of model size, and return an inferior result.\nThe goal of a trusted third party, is to design and implement a verification protocol that verifies whether the computing provider uses $M_{spec}$ for inference. Based on this protocol, the user can determine with high confidence whether the computing provider used $M_{spec}$ (True) or not (False) for any query x. A satisfactory protocol should meet the following criteria:\n1. Low False Negative Rate (FNR): The protocol should minimize instances where the computing provider did use $M_{spec}$ but is wrongly flagged as not using it.\n2. Low False Positive Rate (FPR): The protocol should minimize cases where it incorrectly con-firms that the computing provider used $M_{spec}$ when, in fact, it used an alternative model, $M_{alt}$.\n3. Efficiency: The verification protocol should be computationally efficient and introduce minimal overhead for both the computing provider and the user.\n4. Preservation of Completion Quality: The protocol should not compromise the quality of the prompt completion returned by the computing provider."}, {"title": "3 RELATED WORK", "content": "Verifiable Computing Verifiable Computing (VC) allows users to verify that an untrusted com-puting provider has executed computations correctly, without having to perform the computation themselves (Walfish & Blumberg, 2015; Yu et al., 2017; Costello et al., 2015; Kosba et al., 2018). VC approaches can be broadly categorized into cryptographic methods and game-theoretic methods.\nCryptographic VC techniques either require the provider to return a mathematical proof that con-firms the correctness of the results (Ghodsi et al., 2017; Setty et al., 2012; Parno et al., 2016), or rely on secure computation techniques (Gennaro et al., 2010; Madi et al., 2020; Laud & Pankova, 2014). These techniques cryptographically guarantee correctness and have been applied to machine learning models and shallow neural networks (Niu et al., 2020; Zhao et al., 2021; Hu et al., 2023a; Lee et al., 2024; Ghodsi et al., 2017; Lee et al., 2022). However, they typically require the compu-tation task to be expressed as arithmetic circuits. Representing open-source LLMs in circuit form is particularly challenging due to their complex architectures and intricate operations. Moreover, the sheer size of these models, with billions of parameters, introduces substantial computational overhead. A recent work, zkLLM (Sun et al., 2024), attempts to verify LLM inference using Zero Knowledge Proofs. For the Llama-2-13B (Touvron et al., 2023b) model, generating a proof for a single prompt takes 803 seconds, and repeating this process for large batches of prompt queries becomes computationally prohibitive."}, {"title": "4 METHODOLOGY", "content": "Motivation Despite the fact that larger language models typically offer superior text generation quality (Kaplan et al., 2020), it is often challenging to verify whether a computing provider is using $M_{spec}$ for inference based solely on the returned completion text. Our framework addresses this by requiring the computing provider to return not only the generated text but also the processed intermediate outputs (hidden state representations) from the LLM inference process.\nWe design and train a proxy task specifically to perform well only on the hidden representations generated by $M_{spec}$ during the protocol's training stage. The intuition behind is that the proxy task transforms the hidden representations into a unique identifier for the model. During deployment, the user can evaluate the performance of the returned intermediate outputs on the proxy task. Strong performance on the proxy task indicates that the correct model was used for inference, while poor performance suggests otherwise.\nOur approach does not depend on expensive cryptographic proofs or protocols, and is highly effi-cient. Furthermore, it does not involve retraining or fine-tuning the LLMs, operates independently of the model publisher, and can be applied to any LLM with publicly available weight parameters, making it widely applicable. Next, we formalize and illustrate our proposed framework in detail."}, {"title": "4.1 A SIMPLE PROTOCOL BASED ON INTERMEDIATE OUTPUTS", "content": "Protocol Overview We denote the prompt input as $x \\in V^*$. For any LLM M, let $h_M(x) \\in \\mathbb{R}^{L \\times d_M}$ represent the last-layer hidden representations of x produced by M, where L is the length of the tokenized input x, and $d_M$ denotes the hidden dimension of M. The computing provider receives x from the user, runs M, and returns $h_M(x)$ to user for subsequent verification. However, to reduce the size of the intermediate outputs returned, we additionally apply a proxy task feature extractor network $g_\\theta(\\cdot) : \\mathbb{R}^{L \\times d_M} \\rightarrow \\mathbb{R}^{d_g}$, parameterized by $\\theta$, where $d_g$ represents the proxy task feature dimension. The computing provider now also runs $g_\\theta(\\cdot)$ and returns a compressed vector $z(x) := g_\\theta(h_M(x))$ of dimension $d_g$ to the user, significantly reducing the communication over-head. Specifically, for each prompt query, the compressed vector only takes approximately 4 KB when $d_g$ is set to 1024.\nThe user is required to perform two tasks locally: obtaining the predicted proxy task output and the label. First, the user runs $f_\\phi(\\cdot)$, using the returned proxy task feature z(x) as input to compute $f_\\phi(z(x))$. Here, $f_\\phi(\\cdot) : \\mathbb{R}^{d_g} \\rightarrow \\mathcal{Y}$ is the proxy task head parameterized by $\\phi$, where $\\mathcal{Y}$ denotes the label space. Second, the user applies a labeling function for the proxy task. We adopt a self-labeling function $y(x) : \\mathcal{V}^* \\rightarrow \\mathcal{Y}$, which derives the label directly from the input, eliminating the need for external labels or specialized annotators\\u2071. Note that the label can either be a scalar or a vector.\nFinally, the user checks whether $f_\\phi(z(x))$ matches y(x). Our training process below ensures that, with high probability, $f_\\phi(z(x)) = y(x)$ when $M_{spec}$ is used for inference, and that this does not hold for other models, as the proxy task is exclusively trained on the hidden representation distribution induced by $M_{spec}$. This completes our protocol. Refer to Figure 2a for a detailed illustration.\nProxy Task Training A trusted third party is responsible for implementing the protocol. With a properly defined loss function $l : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ and a training dataset $\\mathcal{D}$, the trusted third party trains the proxy task according to the following training objective:\n$\\phi^*, \\theta^* = \\arg \\min_{\\Phi,\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}} [l (f_\\phi(g_\\theta(h_{M_{spec}}(x))),y(x))].$ (1)\nProtocol Deployment With the optimized $\\phi^*$ and $\\theta^*$, we define the verification function as $V(x, z(x); \\phi^*, \\theta^*) = 1 (f_{\\phi^*}(z(x)) = y(x))$, where $z(x) = g_{\\theta^*} (h_M(x))$ is returned by the computing provider. If the value of the verification function is 1 (or 0), we conclude that the computing provider is indeed (or is not) using $M_{spec}$ for inference with high probability. Now, the low FNR and low FPR criteria introduced in Section 2 can be formally expressed as follows:\nLow FNR : $P (V(x, z(x); \\phi^*, \\theta^*) = 0|M_{spec} \\text{ is used for inference}) \\leq \\alpha$;\nLow FPR : $P (V(x, z(x); \\phi^*, \\theta^*) = 1|M_{spec} \\text{ is not used for inference}) < \\beta$. (2)\nWhile a single prompt query may occasionally yield an incorrect verification result due to FNR or FPR, in practice, users can perform the verification over a batch of queries and apply a hypothesis testing to reach a conclusion with high confidence. Refer to Appendix A.3 for a detailed discussion."}, {"title": "4.2 SVIP: A SECRET-BASED PROTOCOL FOR VERIFIABLE LLM INFERENCE", "content": "From Simple Protocol to Secret-based Protocol The simple protocol, despite its strong potential in discriminating whether the specified model is actually used, is vulnerable to malicious attacks from the computing provider. A dishonest provider may attempt to bypass the verification process without running $M_{spec}$. Since all the provider needs to return is a vector of dimension $d_g$, an attacker could adversarially optimize a vector $\\tilde{z} \\in \\mathbb{R}^{d_g}$ directly, without actually running $g_{\\theta^*}(\\cdot)$ and using any LLM. We refer to this as a direct vector optimization attack. Specifically, if the self-labeling function is public, the adversary can run the labeling function y(x) themselves for each input x and then directly find $\\tilde{z}$ so that\n$\\tilde{z}^* = \\arg \\min_{\\mathcal{Z}} l (f_{\\phi^*}(\\mathcal{Z}), y(x)) .$ (3)"}, {"title": "4.3 SECURITY ANALYSIS", "content": "As previously discussed, the direct vector optimization attack described in Eq. (3) is no longer feasible due to the introduction of the secret mechanism. In this section, we discuss other potential attacks as a comprehensive security analysis towards our secret-based protocol. A more detailed discussion of other possible attacks is provided in Appendix C."}, {"title": "Adapter Attack Under Single Secret", "content": "A malicious attacker could attempt an adapter attack if they collect enough prompt samples $\\mathcal{D}' = \\{x\\}_{M_1}$ under a single secret s. The returned vector from an honest computing provider should be $z(x) = g_{\\theta^*}(t_{\\psi^*}(s) +h_{M_{spec}}(x))$. The attacker's goal is to train an adapter that mimics the returned vector, but by using an alternative LLM, $M_{alt}$.\nTo this end, we define the adapter $\\alpha_\\lambda(\\cdot) : \\mathbb{R}^{d_{M_{alt}}} \\rightarrow \\mathbb{R}^{d_{M_{spec}}}$, parameterized by $\\lambda$, which transforms the hidden states of $M_{alt}$ to approximate those of $M_{spec}$. The returned vector is then $g_{\\theta^*}(t_{\\psi^*}(s) \\oplus \\alpha_\\lambda(h_{M_{alt}}(x)))$. The attacker's objective is to minimize the $L_2$ distance between the returned vector generated by $M_{spec}$ and the vector produced by $M_{alt}$ with the adapter. This can be expressed as:\n$\\Lambda^* = \\arg \\min_{\\Lambda} \\mathbb{E}_{x \\sim \\mathcal{D}'} \\|g_{\\theta^*}(t_{\\psi^*}(s) \\oplus h_{M_{spec}}(x)) - g_{\\theta^*}(t_{\\psi^*}(s) \\oplus \\alpha_\\lambda(h_{M_{alt}}(x)))\\|_2.$ (9)\nBy minimizing this objective, the attacker seeks to make the output of $M_{alt}$ with the adapter in-distinguishable from that of $M_{spec}$, effectively bypassing the protocol. Once the adapter is well-trained, as long as the secret s remains unchanged, the attacker can rely solely on $M_{alt}$ in future verification queries without being detected."}, {"title": "Secret Recovery Attack Under Multiple Secrets", "content": "The secret mechanism is enforced by distribut-ing the secret s to the user, while only providing the secret embedding $t_{\\psi^*}(s)$ to the computing provider. However, a sophisticated computing provider may attempt to recover the original secret by posing as a user and collecting multiple secrets and corresponding embeddings. A straightfor-ward approach would involve recovering s from $t_{\\psi^*}(s)$, thereby undermining the secret mechanism.\nSuppose the attacker has curated a dataset of secret-embedding pairs, $\\mathcal{D}_{secret} = \\{(s_j,t_{\\psi^*}(s_j))\\}_{j=1}^N$. The attacker could then train an inverse model $i_\\rho : \\mathbb{R}^{d_M} \\rightarrow \\mathcal{S}$, parameterized by $\\rho$, to map the secret embedding back to the secret space. If S is continuous, the training objective can be formalized as:\n$\\rho^* = \\arg \\min_{\\rho} \\mathbb{E}_{s \\sim \\mathcal{D}_{secret}} \\|i_\\rho(t_{\\psi^*}(s)) - s\\|_2.$ (10)\nOnce the inverse model is optimized, the true label y(x, s) again becomes accessible to the malicious computing provider. Consequently, the secret-based protocol effectively collapses to the simple protocol without secret protection, leaving it vulnerable to the direct vector optimization attack."}, {"title": "Defense: The Update Mechanism", "content": "To defend against the attacks discussed above, we propose an update mechanism for our secret-based protocol: (1) In defense of the adapter attack, once the prompt queries for a given secret reach a pre-defined threshold $M^*$, the next secret is activated. Meanwhile, we enforce a limit on how often the next secret can be activated, preventing attackers from acquiring too many secrets within a short period. (2) When a total of $N^*$ secrets have been used, the entire protocol should be retrained by the trusted third party\\u00b2. In practice, the values of $M^*$ and $N^*$ can be determined empirically, as discussed in Section 5.4."}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate our proposed protocol SVIP through comprehensive experiments to ad-dress the following research questions: (1) How accurate is SVIP in verifying whether the specified model is used? (Section 5.2) (2) What are the computational costs associated with SVIP? (Section 5.3) (3) How robust is SVIP against various adaptive attacks? (Section 5.4)"}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Datasets and Models To simulate realistic LLM usage scenarios, we use the LMSYS-Chat-1M con-versational dataset (Zheng et al., 2023a), which consists of one million real-world conversations. We filter the dataset to keep only English conversations and extract the user prompts for each con-versation. For the models, we select 5 widely-used LLMs as the specified models, ranging in size from 13B to 70B parameters and spanning multiple model families. As alternative models, we use 6 smaller LLMs, each with parameters up to 7B. Refer to Appendix D.1 for further details."}, {"title": "5.2 RESULTS OF PROTOCOL ACCURACY", "content": "We evaluate the accuracy of our protocol by examining the empirical estimate of FNR and FPR, as outlined in Eq. (16). To apply the verification function in Eq. (8), we first determine the threshold $\\eta$ on a validation dataset during proxy task training. We then evaluate the empirical FNR and FPR on a held-out test dataset with 10,000 samples. For each test prompt, we pair it with 30 randomly sampled secrets to ensure a reliable evaluation result. For FPR calculations, we simulate scenarios where the computing provider uses an alternative, smaller LLM to produce the hidden representa-tions, and applies $g_{\\theta^*}(\\cdot)$ on those outputs\\u00b3. Additionally, we implement a Random baseline where the computing provider generates random hidden representations directly without using any LLM.\nAs shown in Table 1, SVIP consistently achieves low FNR and FPR across all specified LLMs, demonstrating its effectiveness in verifying whether the correct model is used. The FNR remains below 5%, indicating that our protocol rarely falsely accuses an honest computing provider. More-over, when faced with a dishonest provider, the FPR stays under 3% regardless of the alternative model employed, highlighting the protocol's strong performance in detecting fraudulent behavior.\nEvaluation on Unseen Dataset To assess the generalizability of our protocol, we evaluate its accuracy on unseen datasets using the proxy task model and threshold initially trained on the"}, {"title": "5.3 COMPUTATIONAL COST ANALYSIS OF THE PROTOCOL", "content": "During the deployment stage, a practical protocol should introduce minimal computational cost for both the computing provider and the user, specifically in terms of runtime and GPU memory usage. Table 3a details the runtime per query and GPU memory consumption. Across all specified models, the verification process takes under 0.01 seconds per query for both the computing provider and the user. For example, verifying the Llama-2-13B model for each query takes only 0.0017 seconds for the computing provider and 0.0056 seconds for the user. The proxy task feature extractor $g_{\\theta}(\\cdot)$, run by the computing provider, consumes approximately 980 MB of GPU memory, imposing only minimal overhead. On the user side, the proxy task head $f_\\phi(\\cdot)$ and labeling network $y_\\gamma(\\cdot)$ require a total of 1428 MB, making it feasible for users to run on local machines without high-end GPUs. Additionally, we record the required proxy task retraining time in Table 3b. Overall, retraining the proxy task takes less than 1.5 hours on a single GPU, allowing for efficient protocol update."}, {"title": "5.4 RESULTS OF PROTOCOL SECURITY", "content": "Robustness Evaluation Against Adapter Attack To simulate the adapter attack, we assume an attacker collects a dataset of size $M$, consisting of prompt samples associated with a single secret s. The attack follows the optimization process outlined in Eq. (9), and is considered successful if the resulting adapter passes the verification function when secret s is applied. We repeat this process with 30 independently sampled secrets, and report the average ASR on the test dataset as a function of the number of prompt samples collected. The experiment is conducted with 3 specified LLMs, each paired with 3 smaller alternative models. Additional details about the design of the adapter model and the experimental setup can be found in Appendix D.5.\nAs shown in Figure 4, using a 50% ASR threshold, Llama-2-13B and OPT-30B resist attacks with up to 400 prompt samples, regardless of the alternative model used. For Llama-3.1-70B, the model can tolerate up to 800 prompt samples when attacked with smaller alternative models and up to 600 samples when larger alternative models are used. Based on these results, we recommend setting $M^*$, the maximum number of prompt queries allowed under a single secret before a new secret is activated, in the range from 400 to 600, depending on the specified LLM.\nRobustness Evaluation Against Secret Recovery Attack In this attack scenario, we assume the attacker has collected $N$ secret-embedding pairs and uses a 3-layer MLP as the inverse model to predict the original secret from its embedding. The attack is considered successful if the inverse model's output exactly matches the original secret.\nTable 4 demonstrates the ASR across different specified models as a function of N. The attacker is unable to recover any secrets when N \u2264 10,000. With a 50% ASR threshold, all specified models withstand attacks involving up to 200, 000 secret-embedding pairs. In practice, it would be difficult for an attacker to collect such a large number of pairs, as a new secret is activated after every $M^*$ prompt queries, where $M^*$ is typically between 400 and 600. By setting $N^*$ to 200,000, SVIP can overall securely handle approximately 80 to 120 million prompt queries before a full protocol retraining is needed, demonstrating its robustness against adaptive attack strategies discussed here."}, {"title": "6 CONCLUSION", "content": "In this paper, we formalize the problem of verifiable inference in the context of LLMs. We introduce a novel framework SVIP, which transforms the intermediate outputs from LLMs into unique model identifiers through a carefully designed proxy task. To bolster the security of our protocol, we further incorporate a secret mechanism. We also provide a thorough analysis of potential attack scenarios. Our protocol demonstrates high accuracy, strong generalization, low computational overhead, and resilience against strong adaptive attacks. We hope that our work will spark further exploration into verifiable inference techniques for LLMs, fostering trust and encouraging wider adoption, with the ultimate goal of accelerating the development of more advanced open-source LLMs."}, {"title": "ETHICS STATEMENT", "content": "In this work, we address the challenge of verifiable LLM inference, aiming to foster trust between users and computing service providers. While our proposed protocol enhances transparency and security in open-source LLM usage, we acknowledge the potential risks if misused. Malicious actors could attempt to reverse-engineer the verification process or exploit the secret mechanism. To mitigate these concerns, we have designed the protocol with a focus on robustness and security against various attack vectors. Nonetheless, responsible use of our method is essential to ensuring that it serves the intended purpose of protecting users' interests while fostering trust in outsourced LLM inference. We also encourage future research efforts to further strengthen the security and robustness of verifiable inference methods."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "Our code repository is available at https://github.com/ASTRAL-Group/SVIP LLM InferenceVerification. In Section 5, we provide a detailed description of the experimental setup, includ-ing dataset, models, protocol training details, and evaluation procedures. Additional experimental details can be found in Appendix D."}, {"title": "A DISCUSSIONS", "content": "In our SVIP protocol, although the labeling network $\\gamma(\\cdot)$ can be applied to multiple specified mod-els once trained, the proxy task head $f_\\phi(\\cdot)$, proxy task feature extractor $g_\\theta(\\cdot)$, and secret embedding network $t_\\psi(\\cdot)$ need to be optimized for each specified model. Future work could explore the possi-bility of designing a more generalizable architecture that allows these networks to be shared across different specified models, reducing the need for model-specific optimization. Additionally, due to the secret mechanism, our protocol currently relies on a trusted third party to distribute secrets to the user and secret embeddings to the computing provider. Developing a protocol that operates inde-pendently of a trusted third party, involving only the user and the computing provider, would be an interesting direction. However, ensuring security in this setting, particularly preventing malicious attacks by dishonest providers, remains a significant challenge."}, {"title": "A.2 SIMPLE APPROACHES TO VERIFIABLE LLM INFERENCE CAN BE VULNERABLE", "content": "One straightforward solution to verifiable LLM inference, as briefly mentioned in Section 1, involves the user curating a small set of prompt examples from established benchmarks and sending them to the computing provider. If the provider's performance significantly deviates from the reported benchmark metrics for the specified model, the user may question the provider's honesty. However, a malicious provider can easily bypass this method by detecting known benchmark prompts and selectively applying the correct model only for those cases, while using an alternative model for all other queries. Additionally, testing such benchmark prompts also increases the user's inference costs.\nAnother seemingly promising approach is to directly train a binary (or one-class) classifier on the returned intermediate outputs to verify if the hidden representations come from the specified model. However, a simple attack involves the provider caching hidden representations from the correct model that are unrelated to the user's input. The dishonest provider could then use a smaller LLM for inference and return these cached irrelevant representations to deceive the classifier while saving costs."}, {"title": "A.3 HYPOTHESIS TESTING FOR VERIFICATION USING A BATCH OF PROMPT QUERIES", "content": "A single prompt query may occasionally yield an incorrect verification result due to FNR or FPR. In practice, users often have a batch of prompt queries $\\{x_i\\}_{i=1}^B$, where B denotes the batch size. To achieve a reliable conclusion with high confidence, hypothesis testing can be applied to this batch. Specifically, denote $V_i := V(x_i, z(x_i); \\phi^*,\\theta^*), i \\in [B]$. We assume $V_i, i \\in [B]$ follows an independent and identically distributed Bernoulli distribution with parameter p, such that $V_i \\stackrel{i.i.d.}{\\sim} Bernoulli(p), i \\in [\\Beta]$.\nThe null hypothesis assumes that the computing provider is acting honestly, i.e., the specified model is used. Under the null hypothesis, p corresponds to the True Positive Rate (TPR) of our protocol:\n$p = \\mathbb{P}(V_i = 1| M_{spec} \\text{ is used for inference}) = TPR$ (11)\nWe define the following test statistic:\n$\\varepsilon = \\frac{\\sum_{i=1}^B V_i - B\\cdot TPR}{\\sqrt{B \\cdot TPR \\cdot (1 - TPR)}}$ (12)"}, {"title": "B EXTENDED RELATED WORK", "content": "Open-source LLMs Open-source LLMs are freely available models that offer flexibility for use and modification. Popular examples include GPT-Neo (Black et al., 2022), BLOOM (Le Scao et al., 2023), Llama (Touvron et al., 2023a;b; Dubey et al., 2024), Mistral (Jiang et al., 2023), and Falcon (Almazrouei et al., 2023). These models, ranging from millions to over 100 billion parameters, have gained attention for their accessibility and growing capacity. However, larger models like Falcon-40B (Almazrouei et al., 2023), and Llama-3.1-70B (Dubey et al., 2024) come with steep computational costs, making even inference impractical on local machines due to the significant GPU memory required. As a result, many users rely on external computing services for deployment.\nAdditional Background on Cryptographic VC Techniques Among cryptographic VC tech-niques, proof-based methods involve the generation of mathematical proofs that certify the correct-ness of outsourced computations. Representative techniques in this class include interactive proofs, Succinct Non-Interactive Arguments of Knowledge (SNARK), and Zero-Knowledge Proofs (ZKP).\nInteractive proofs involve multiple rounds of interaction between a verifier (the user) and a prover (the computing provider) to ensure the computation's integrity. SNARK allows a verifier to validate a computation with a single, short proof that requires minimal computational effort. ZKP further enhances privacy by enabling the prover to convince the verifier of a statement's truth without revealing any additional information beyond the validity of the claim. Due to their rigorous guarantees of correctness and privacy, these techniques have been widely applied in blockchain and related areas."}, {"title": "C ADDITIONAL ATTACKS", "content": "In this section, we outline additional attacks that can be applied to the simple protocol described in Section 4.1. Note that these attacks do not apply to the secret-based protocol."}, {"title": "Fine-tuning Attack", "content": "When the hidden dimension of the alternative LLM", "loss": "n$\\hat{M}_{alt} = \\arg \\min_{M_{alt}} \\mathbb{E}_{x \\sim \\mathcal{D}_{attack}} [l (f_{\\phi^*}(g_{\\theta^*}(h_{\\hat{M}_{alt}} (x))), y(x))"}]}