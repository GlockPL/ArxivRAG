{"title": "Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting", "authors": ["Qiaoyi Xue", "Youdan Feng", "Jiayi Liu", "Tianming Xu", "Kaixin Shen", "Chuyun Shen", "Yuhang Shi"], "abstract": "This study explores a workflow for automated segmentation of lesions in FDG and PSMA PET/CT images. Due to the substantial differences in image characteristics between FDG and PSMA, specialized preprocessing steps are required. Utilizing YOLOv8 for data classification, the FDG and PSMA images are preprocessed separately before feeding them into the segmentation models, aiming to improve lesion segmentation accuracy. The study focuses on evaluating the performance of automated segmentation workflow for multitracer PET images. The findings are expected to provide critical insights for enhancing diagnostic workflows and patient-specific treatment plans. Our code will be open-sourced and available at https://github.com/jiayiliu-pku/AP2024.", "sections": [{"title": "1. Introduction", "content": "The increasing global cancer incidence demands advanced diagnostic and therapeutic technolo-gies to enhance precision and personalization in cancer management. Molecular theranostics, which integrates diagnostic imaging with targeted therapy, exemplifies this trend by offering personalized treatment opportunities with unprecedented accuracy. Positron emission tomog-raphy (PET) combined with computed tomography (CT) plays a pivotal role in oncological diagnostics, utilizing radiotracers such as Fluorodeoxyglucose (FDG) and prostate-specific membrane antigen (PSMA) to effectively detect and manage various cancers. FDG is partic-ularly effective in highlighting metabolically active cancer cells, facilitating the evaluation of multiple cancer types (Dholakia et al., 2014). PSMA, highly expressed in prostate cancer cells, is essential for diagnosing and treating prostate cancer, serving as a valuable target for both imaging and therapeutic interventions (Nickols et al., 2021; Zhao et al., 2019).\nFor FDG PET/CT scans, the adoption of the deep learning methods improve lesion segmentation accuracy and overcomes the challenges associated with differentiating patho-logical changes from physiological uptake in organs like the liver and brain (Im et al., 2017). Advances in multi-label segmentation techniques enable simultaneous delineation of lesions and high-uptake organs, further improving segmentation accuracy (Weisman et al., 2020; Barrington et al., 2020). As PSMA PET imaging has become increasingly vital for early detection of lymph node metastases and monitoring treatment responses, recent research also shows the superiority of using the deep learning methods in segmenting lesion of the PSMA PET images (Fr\u00fch et al., 2021; Anttinen et al., 2021).\nHowever, a significant challenge lies in the differences between FDG and PSMA PET images, which necessitate specific and targeted preprocessing steps to handle their unique properties. This study aims to develop a lesion segmentation workflow that can effectively manage both FDG and PSMA PET/CT images. Specifically, the study utilizes YOLOv8 to classify FDG and PSMA data and subsequently applies tailored preprocessing techniques before inputting the classified data into dedicated segmentation models for each tracer (Varghese and M., 2024). By evaluating the impact of organ-specific labeling and preprocessing strategies on model performance, this research seeks to optimize PET/CT imaging for broader oncological applications, particularly for individualized prostate cancer interventions."}, {"title": "2. Methods", "content": "The automated lesion segmentation process for FDG and PSMA PET images consists of two steps. First, a classification model was trained for distinguishing FDG-PET and PSMA-PET medical images. Second, two 3D Unets were trained independently with FDG or PSMA data for the organ and lesion segmentation (shown in Fig.1).\n2.1 Data and preprocessing\n2.1.1 DATASETS FOR LESION SEGMENTATION\nThe training of the FDG lesion segmentation models was conducted using whole-body FDG PET/CT data from a cohort of 900 patients, encompassing 1014 studies supplied by the AutoPET challenge III in 2024. The challenge consists of patients with malignant melanoma, lymphoma, lung cancer and negative control patients. The data was split into a training set"}, {"title": "2.1.2 DATASETS FOR IMAGE CLASSIFICATION", "content": "It is observed that PSMA-PET shows higher uptake than FDG in submandibular glands, kidneys, liver, spleen and bladder. Based on the observation, two steps were conducted to classify FDG-PET and PSMA-PET images. The maximum-intensity projection (MIP) images were generated by projecting the voxel with the highest FDG uptake value on coronal view throughout the volume onto a 2D image. Besides the Data supplied by the AutoPET"}, {"title": "2.2 Model architecture and training", "content": "2.2.1 YOLO MODEL\nThe YOLOv8 architecture was adapted for classification of PSMA-PET and FDG-PET. The model was trained with hyperparameters optimized: initial learning rate set to 0.0001 and batch size to 16. Training spanned 200 epochs, incorporating data augmentation techniques like flipping to enhance model robustness and mitigate overfitting.\n2.2.2 NNU-NET MODEL\nThe models were trained based on the nnU-Net framework to segment multiple organs and lesions (Isensee et al., 2021). 3D nnU-Net was used with the ResNet18 backbone structure. The input patch size of the 3D U-Net was set to 160x160x160. The loss function is set to a combination of the Dice loss and focal loss to combat overfitting.\n\u2022 Dice Loss: The Dice Loss is based on the Dice coefficient, which is a measure of overlap between two sets. The formula for Dice Loss is:\nDice Loss = 1 - \\frac{2 \\Sigma_i p_i g_i}{\\Sigma p_i^2 + \\Sigma g_i^2} (1)\nwhere pi is the predicted probability for pixel i, gi is the ground truth label for pixel i. A higher Dice coefficient indicates a greater overlap between the predicted segmentation and the ground truth, reflecting a more accurate segmentation result."}, {"title": "\u2022 Focal Loss", "content": "The Focal Loss is designed to address class imbalance by down-weighting the loss assigned to well-classified examples. The Focal Loss is defined as:\nFocal Loss(pt) = \u2212at(1 \u2212 pt)\u03b3 log(pt) (2)\nwhere pt is the predicted probability of the true class. at is a weighting factor for class imbalance. \u03b3 is the focusing parameter that controls the rate at which easy examples are down-weighted.\nThe models are trained with 1000 epochs and a batch size of 4, using the SGD optimizer and an initial learning rate of 0.01. This model was then formatted in docker and submitted to the challenge portal for testing and benchmarking.\nThe evaluation of model performance is conducted using metrics such as the Dice score, false positive volume (FPvol) and false negative volume (FNvol), which provide a comprehensive assessment of the segmentation methodologies."}, {"title": "3. Results and Discussion", "content": "The PET model achieved a classification accuracy of 99.85% which showed superior perfor-mance in differentiating FDG and PSMA PET images.\nThe evaluation of our lesion segmentation models for FDG and PSMA PET/CT images produced the following outcomes (shown in Table.1): the Dice coefficients were 0.8408 for FDG and 0.7385 for PSMA. False Positive volumes (FPvol) were 1.7979 for FDG and 9.3574 for PSMA, while False Negative volumes (FNvol) were 2.3625 for FDG and 5.0745 for PSMA. These results indicate differences in segmentation performance between the two imaging modalities."}, {"title": "4. Conclusion", "content": "In conclusion, this study demonstrated the feasibility of the proposed lesion segmentation workflow for both FDG and PSMA PET/CT images. YOLOv8 demonstrated its superior performance in classifying the PSMA and FDG PET image which allows for using tailored preprocessing techniques in segmenting the lesion in PET image with different tracers."}]}