{"title": "Robust Autonomy Emerges from Self-Play", "authors": ["Marco Cusumano-Towner", "David Hafner", "Alex Hertzberg", "Brody Huval", "Aleksei Petrenko", "Eugene Vinitsky", "Erik Wijmans", "Taylor Killian", "Stuart Bowers", "Ozan Sener", "Philipp Kr\u00e4henb\u00fchl", "Vladlen Koltun"], "abstract": "Self-play has powered breakthroughs in two-player and multi-player games. Here we show that self-play is a surprisingly effective strategy in another domain. We show that robust and naturalistic driving emerges entirely from self-play in simulation at unprecedented scale \u2013 1.6 billion km of driving. This is enabled by GIGAFLOW, a batched simulator that can synthesize and train on 42 years of subjective driving experience per hour on a single 8-GPU node. The resulting policy achieves state-of-the-art performance on three independent autonomous driving benchmarks. The policy outperforms the prior state of the art when tested on recorded real-world scenarios, amidst human drivers, without ever seeing human data during training. The policy is realistic when assessed against human references and achieves unprecedented robustness, averaging 17.5 years of continuous driving between incidents in simulation.", "sections": [{"title": "1. Introduction", "content": "Self-play has been an effective strategy for training policies for board games, card games, 3D multiplayer games, real-time strategy games, robotic manipulation, and even bio-engineering (Silver et al., 2017; 2018; Jaderberg et al., 2019; Berner et al., 2019; Brown & Sandholm, 2019; Vinyals et al., 2019; Plappert et al., 2021; Perolat et al., 2022; Wang et al., 2023a). In this work, we demonstrate the effectiveness of self-play in another domain. We show that simulated self-play yields naturalistic and robust driving policies, while using only a minimalistic reward function and never seeing human data during training.\nWe demonstrate that qualitatively new levels of realism and robustness emerge when self-play training is taken to unprecedented scale \u2013 orders of magnitude beyond prior experiments (Feng et al., 2023; Zhang et al., 2023). This discovery is enabled by GIGAFLOW, a batched simulator architected from the ground up for self-play reinforcement learning on a massive scale. GIGAFLOW is capable of simulating and learning from 4.4 billion state transitions (7.2 million km of driving, or 42 years of continuous driving experience) per hour on a single 8-GPU node. It simulates urban environments with up to 150 densely interacting traffic participants 360 000 times faster than real time at a cost of under $5 per million km driven (based on public cloud rates). A full training run simulates over one trillion state transitions, 1.6 billion km driven, or 9500 years of subjective driving experience, and completes in under 10 days one 8-GPU node.\nWe use GIGAFLOW to train a parameterized family of driving policies. The parameters specify the type of traffic participant controlled by the policy (passenger vehicle, large truck, bicyclist, or even a pedestrian) and the driving style (e.g.,, aggressive vs. cautious). These parameters can be modified at test time with no additional training (Dosovitskiy & Koltun, 2017), such that a single trained policy can be used to control a variety of traffic participants, with a variety of behavioral styles. During training, this parameterized policy architecture enables all simulated traffic participants to be collecting experience in parallel, all flowing through a single neural network. This supports self-play simulations where more than a hundred agents are all controlled by a single neural network, which is learning from all of their experiences, yet the agents exhibit diverse outward manifestations (truck vs. bicycle), functional characteristics (turning radius), and behavioral styles (adherence to traffic laws).\nThe result is a robust and naturalistic driving policy that achieves state-of-the-art performance when tested in recorded real-world scenarios, amidst recorded human drivers, without ever seeing human data during training. We test the GIGAFLOW policy in three leading independent third-party benchmarks: CARLA (Dosovitskiy et al., 2017), nuPlan (Caesar et al., 2022), and the Waymo Open Motion Dataset (Ettinger et al., 2021) (through the Waymax simulator (Gulino et al., 2023)). State-of-the-art performance on each benchmark was previously achieved by specialist agents that were trained specifically for that benchmark,"}, {"title": "2. GIGAFLOW", "content": "The goal of GIGAFLOW is to train a generalist policy \\(\u03c0(\u03b1|W, S, A, C)\\) in simulation (Fig. 2d). The policy observes the static world W, its own state S, and other dynamic agents A to produce an action a (Fig. 2c). A conditioning parameter C modulates the policy's behavior. Learning this generalist policy requires careful modeling of two core concepts: uncertainty and other-agent behaviors.\nUncertainty in driving stems from partial or incomplete observations. The driver is generally unaware of the goals and intentions of other agents, or even their exact location, speed, or acceleration. Objects or parts of the static world may be hidden or occluded. Real-world sensors often introduce noise. GIGAFLOW models uncertainty directly through noise on the state S, noise in the state transitions, stochasticity in the dynamic agents, and partial observability on dynamic agents A and the static world W. GIGAFLOW agents observe the positions and speeds of nearby agents but not their acceleration, and crucially, neither their goals nor conditioning.\nModeling agent behaviors is a particularly impactful and complex aspect of driving. Prior work approached behav-"}, {"title": "2.1. GIGAFLOW world", "content": "The GIGAFLOW world is simple: we do not script scenarios, use human driving traces, or design delicate reward terms. We show that simulation at massive scale makes up for much of this simplicity (Fig. 2).\nAgents train on one of eight maps, randomly perturbed with rescaling, shears, flips and reflections. Total drivable lanes per map range from four to 40 km for a total of 136 km of road across the eight maps (Fig. 2a). In each map, we spawn one to \\(N_a\\) agents at random locations and orientations on the road and ask them to reach goal points sampled uniformly over the map. This creates a world in which agents drive for long distances before reaching their destinations (Fig. 2b). Agents are tasked with visiting a variable number of intermediate waypoints, requiring the ability to follow complex routes (see Appendix B for details).\nDense traffic flows with diverse interactions emerge as agents navigate to their destinations. As training progresses, we can observe agents executing zipper merges and tight maneuvers in traffic jams, managing congested roundabouts and uncontrolled intersections, resolving occasional grid-locks, and performing multi-point turns to reroute around accidents or obstructions.\nGIGAFLOW agents train fully in self-play. All dynamic agents - vehicles, pedestrians, and cyclists \u2013 use the same single reactive parametric policy \u03c0; their behaviors are varied through conditioning C (Fig. 2d). The policy is aware of the dynamics of the agent it controls as part of the conditioning \\(C_{dynamics}\\). The agent reward is a mixture of incentives to reach its goal, avoid collisions, drive centered and lane aligned, as well as penalties for running red lights or stop signs, and exceeding acceleration and jerk limits. The weights on each of these reward components are randomized per agent and provided as conditioning \\(C_{reward}\\) to the agent (see Appendix B for details). This allows a single reactive policy \u03c0 to exhibit a wide range of behaviors. The result is a diverse training world where agents learn a continuum of driving styles: some drive cautiously, others are likely to run traffic lights, while a rare few are willing to drive against the flow of traffic. Because the policy only observes"}, {"title": "2.2. GIGAFLOW simulation and training", "content": "The GIGAFLOW simulation and training framework is designed to optimize driving data collection and training throughput per unit of computation. We simulate and learn from 4.4 billion state transitions per hour on a single 8-GPU node, rolling out urban commute simulations 360 000 times faster than real time at the cost of under $5 per million kilometers driven (based on public cloud rates). These training rates require three core ingredients: a fast batched simulator (Shacklett et al., 2021; Petrenko et al., 2021; Shacklett et al., 2023), a compact and expressive policy for fast inference and backpropagation, and a high-throughput training algorithm.\nGIGAFLOW simulation. GIGAFLOW simulates 38 400 environments in parallel across 8 GPUs with up to \\(N_a = 150\\) vehicles each (Fig. 2a). Basic operations, such as policy inference and dynamics updates are batched across all agents. Agent localization, collision checking, and observation construction rely on dedicated optimized data structures. Due to the large map sizes, we precompute and cache all map observations in a spatial hash and perform fast, GPU-based runtime lookup and retrieval. Agents perceive the map by observing sets of points sampled sparsely along drivable lanes \\(W_{lane}\\), and densely along the nearby road edges for precise maneuvering \\(W_{boundary}\\) (Fig. 2c).\nBeyond map features, agents get observations of nearby traffic participants A - containing nearby vehicles' sizes, locations, orientations, and velocities \u2013 and nearby stop lines and traffic lights \\(W_{stop}\\). GIGAFLOW models static obstacles as immobile vehicles \\(A_{static}\\). To reduce memory, we do not store these observations in the rollout buffer, but calculate them on demand from stored world states. See Appendix A for a detailed description of the simulator.\nGIGAFLOW policy. GIGAFLOW can simulate diverse actor types, from pedestrians to heavy trucks, by parameterizing a single unified feed-forward policy (Fig. 2d). The decision to use the same underlying neural network policy for all traffic participants significantly impacts the overall throughput: we need only a single (batched) forward pass per simulation step to calculate actions for all agents. The policy resembles a Deep Sets architecture (Zaheer et al., 2017) and is invariant to permutation w.r.t. each observation type. Critically, the entire trainable artifact is relatively compact at six million parameters. On an 8-GPU A100 node, the policy allows inference throughput of 7.4 million decisions per second during experience collection at a batch size of 2.6 million, and eight gradient updates per second in the training phase with a batch size of 256000. See Appendix D for more details.\nGIGAFLOW training. We train the GIGAFLOW policy using Proximal Policy Optimization (PPO) (Schulman et al., 2017). One of the main challenges associated with autonomous driving is the inherent imbalance in the data distribution. As training progresses, the on-policy data is dominated by ordinary traffic configurations, such as orderly driving in a straight line between intersections. The critic is often able to accurately predict the returns for such trajectories, resulting in a large portion of samples with near-zero advantage (Greensmith et al., 2004) that consequently yield vanishingly small gradients.\nWe use a variant of Prioritized Experience Replay (Schaul et al., 2016) that filters samples that have minimal impact on learning. The filtering is based on the absolute value of the estimated advantage. We filter up to 80% of samples with low absolute advantage, which significantly increases learning throughput without sacrificing sample efficiency. Our approach, which we refer to as advantage filtering, focuses training on the most informative state transitions, prioritizing learning from the underexplored tails of the data distribution where selected actions are measurably better or worse, and makes more efficient use of the data we generate. See Appendix C for more details."}, {"title": "3. Zero-shot evaluation on driving benchmarks", "content": "We evaluate a trained GIGAFLOW policy on the leading closed-loop driving benchmarks: CARLA (Dosovitskiy et al., 2017), nuPlan (Caesar et al., 2022), and the Waymo Open Motion Dataset (Ettinger et al., 2021) through the Waymax simulator (Gulino et al., 2023). These benchmarks encompass a wide range of actor behaviors, driving scenarios, maps, traffic densities, durations, and scoring methodologies. The CARLA benchmark consists of routes with hand-designed scenarios based on the NHTSA pre-crash topology (Najm et al., 2007). It evaluates long distance driving (several minutes per 1\u20133 km route). nuPlan and Waymax evaluate short distance driving (8\u201314 seconds per scenario, < 100 m) in scenarios derived from recorded real-world driving with the associated sensor data.\nA generalist GIGAFLOW policy outperforms state-of-the-art specialists. For each benchmark, we compare to specialist state-of-the-art policies that are either trained (Gulino et al., 2023) or carefully hand-designed (Chitta et al., 2023; Jaeger et al., 2023; Dauner et al., 2023) to perform well on that specific benchmark. In contrast, we use a single policy across all benchmarks. Our policy is trained purely in self-play and is evaluated zero-shot in each benchmark environment. Without any fine-tuning, our policy surpasses the state of the art in CARLA, nuPlan, and Waymax (Fig. 1 with details in Tables A5 to A7 and Appendix E). This demonstrates robust driving with strong generalization. Our self-play policy outperforms the state of the art on real driving traces with human traffic participants, without ever seeing human data during training.\nGIGAFLOW policy generalizes to diverse actor behaviors. The benchmarks implement a diverse set of environment actors. CARLA uses reactive rules-based vehicles with lane-changing capabilities, combined with events triggered by the driver's behavior (e.g., a pedestrian that darts suddenly in front of the driver). The actors in nuPlan and Waymax are controlled by different variants of the Intelligent Driver Model (Treiber et al., 2000). Vehicles in nuPlan follow the lane center line, whereas vehicles in Waymax follow the paths of logged human drivers. The GIGAFLOW policy exhibits robust driving amongst all of these actor types.\nGIGAFLOW policy generalizes to diverse maps and driving situations. GIGAFLOW trains on variants of synthetic maps with closed road networks (Dosovitskiy et al., 2017), but generalizes to the real-world maps in nuPlan (Caesar et al., 2022) and in the Waymo Open Motion Dataset (WOMD) (Ettinger et al., 2021). The WOMD maps are small, with incomplete road networks constructed from logs of instrumented vehicles in several US cities. The nuPlan benchmark is based on driving logs of human drivers in locales with both right-handed and left-handed driving (Caesar et al., 2020); it contains larger maps that encompass the entire testing area of the vehicle. Both nuPlan and WOMD scenarios include merges, unprotected turns, and interactions with pedestrians and cyclists (Ettinger et al., 2021). The GIGAFLOW policy achieves state-of-the-art results in these benchmarks without any training on recorded driving logs or any human-designed scenarios.\nGIGAFLOW policy generalizes to real-world observation noise. Both Waymax and nuPlan construct observations, maps, and other actors with auto-labeling tools from real-world perception data. This brings occlusion, incorrect or missing traffic-light states, and obstacles revealed at the last moment. Despite the minimalistic noise modeling in GIGAFLOW, the GIGAFLOW policy generalizes zero-shot to these conditions.\nGIGAFLOW policy is state-of-the-art according to multiple scoring methodologies. Each benchmark brings its own definition of 'good driving'. Those definitions are distinct and sometimes contradictory. For example, running a red light in CARLA incurs nearly the same penalty as colliding with another vehicle. Yet the same action can be advantageous in nuPlan, where red light violations are ignored by the scoring criteria, hard braking causes comfort penalties, and forward progress is strongly rewarded. Despite such variations, the single generalist GIGAFLOW driver outperforms specialist policies optimized for individual benchmark scores.\nThe GIGAFLOW policy approaches the ceiling of benchmark performance. The vast majority of the infractions sustained by the GIGAFLOW policy during testing on the benchmarks can be attributed to limitations of the benchmarks. For instance, 20% of the reported infractions in CARLA are caused by pedestrians or cyclists darting from the sidewalk into the roadway without reacting to the evasive maneuver of the driver or other traffic participants. Preventing such collisions would require drastic overfitting to this type of scenario (Jaeger et al., 2023). Other exemplary limitations are gridlocks caused by CARLA-controlled traffic (33% of all infractions) or fuzzy stop sign and red light checks (16% of all infractions).\nIn nuPlan our policy sustains 15 collisions in 1118 scenarios. We analyzed each of them. Nine are unavoidable due to invalid initialization or sensor noise (agents appearing inside the vehicle's bounding box). Four are caused by non-reactive pedestrian agents walking into the vehicle while the vehicle was stopped or in an evasive maneuver. Two collisions are due to traffic light violations of other agents.\nIn Waymax our policy sustains 187 collisions in 44097 scenarios. We again analyzed each of them. 55.6% were caused by unavoidable IDM agent behavior (Treiber et al., 2000) of the traffic participants controlled by the benchmark, such as swerving directly into the ego vehicle. 41.7% were caused by initialization in a state of collision, typically with a pedestrian. 2.7% (i.e. five scenarios) were considered at-fault and avoidable by the GIGAFLOW policy. Of the at-fault collisions, there were additional contributing factors such as perception issues or aggressive and spurious IDM behaviors. One example is when the GIGAFLOW policy seeks to avoid a rear-end collision with an IDM agent approaching from behind at high speed.\nWe include videos of all reported infractions in the supplementary material."}, {"title": "4. Analysis", "content": "GIGAFLOW training employs two neural networks: the policy (actor) that chooses actions and the value function approximator (critic) that estimates the expected cost-to-go from a given state. We examine how the policy's driving behavior changes over the course of training (Fig. 3) and how the policy and value networks respond to targeted changes to their inputs in various scenarios (Fig. 4).\nReinforcement learning at scale yields mastery of complex skills. The scale of GIGAFLOW training enables the policy to handle complex scenarios despite never seeing real-world or hand-designed driving scenarios during training. The policy learns to execute unprotected left turns, drive in crowded roads used by both pedestrians and vehicles, and handle vehicles dangerously merging into the driver's lane (Fig. 3a). In diagnostic tests designed for analysis,"}, {"title": "5. Discussion", "content": "Many questions remain to fully understand the long-term role of self-play in delivering broad-competence robust autonomy. First, our work has been conducted entirely in simulation. Techniques for transferring policies from simulation to reality will have to be brought to bear before claims can be made regarding the efficacy of self-play policies in the physical world (M\u00fcller et al., 2018; Lee et al., 2020; Kaufmann et al., 2023).\nSecond, our work has focused on planning and decision-making, largely abstracting the perception stack. To integrate the presented findings into an operational system, sensing and perception will have to be modeled much more closely. An exciting possibility is to combine large-scale self-play training with data-driven simulation of the associated perceptual inputs (e.g.,, camera images) (Ost et al., 2021; Yang et al., 2023; Hu et al., 2023). It is likely feasible in the coming years due to ongoing improvements in simulation methodology (Shacklett et al., 2021; Petrenko et al., 2021; Shacklett et al., 2023), computing hardware, and system architectures. Combining self-play with photorealistic sensor simulation would substantially increase the computational footprint of each experience, but the wall-clock training time can be maintained by scaling out over a commensurate number of compute nodes (Dubey et al., 2024).\nThird, our work has demonstrated that training without real-world driving traces can yield policies that are surprisingly human-like (Montali et al., 2023) and highly robust when tested in recorded real-world scenarios with human participants (Caesar et al., 2022; Gulino et al., 2023). By contrast, common perspectives on learning-based autonomous driving hold that recorded datasets will play a key role in training driving policies (Jain et al., 2021; Hawke et al., 2021; Chen et al., 2023). How do we reconcile our findings with these views? One possibility is to combine large-scale self-play training with training on recorded scenarios, perhaps via a combination of reinforcement learning and imitation learning (Lu et al., 2023; Zhang et al., 2023). This can further increase robustness and help bridge simulation and reality.\nOur findings may inspire broader application of self-play in training agents that act in the presence of (and in close coordination with) humans in physical and digital environments. Such coordinated action may be called for in mobile robotics, in both consumer and industrial settings, and in digital domains such as online games. We have shown that policies that function effectively in the presence of human actors in complex dynamic environments can be trained without utilizing human data. Broader application of this methodology may substantially reduce the cost and complexity of training autonomous policies by meaningfully reducing the need for human data collection."}, {"title": "A. Simulator Design", "content": "GIGAFLOW is a batched simulator (Makoviychuk et al., 2021; Freeman et al., 2021; Petrenko et al., 2021; Shacklett et al., 2021), implemented in PyTorch (Ansel et al., 2024) and designed for GPU acceleration. A single instance of GIGAFLOW simulates thousands of worlds, enabling it to leverage the parallelism of modern GPUs. Implementing GIGAFLOW efficiently required the development of custom batched operators for the kinematics, collision checking, initialization of urban driving environments, and many other features. Below we outline the overall flow of GIGAFLOW and describe the acceleration techniques used.\nAt every timestep, GIGAFLOW takes the current state of the i-th world \\(s_i^{(t)}\\) and vehicle controls \\(a_i^{(t)}\\) to produce the state at the next timestep \\(s_i^{(t+1)}\\). Instead of running multiple copies of the environment to simulate N = 38 400 worlds, a single instance of GIGAFLOW simulates N worlds in parallel. The state and vehicle controls of all N worlds are denoted \\(s^{(t)}\\) and \\(a^{(t)}\\)."}, {"title": "A.1. World initialization", "content": "The simulation process begins with the initialization of urban driving environments by placing up to \\(N_a = 150\\) vehicles at random positions on the map. We first draw a random sample over map locations, vehicle headings, and vehicle bounding box dimensions and reject states that are off-road (off-road checking is detailed below). A naive application of this process results in a marginal distribution of vehicle locations that is biased towards wider road sections. To correct for this, we estimate this marginal distribution from an initial sample and use it to adjust the proposal distribution used in subsequent rejection sampling. Given the set of valid vehicle states, we then select a collision-free subset with the desired number of agents (collision detection is detailed below). This subset becomes the initial traffic configuration at t = 0.\nFor each retained vehicle, we select a sequence of its waypoints (goals). The first waypoint is sampled uniformly over the map and additional waypoints are sampled such that given the jth waypoint, the (j + 1)th waypoint is at least 20 m away and no more than 200 m away, and has lane heading that is within 60 degrees of the jth waypoint's lane heading. There are cases where the jth waypoint is in a location where these constraints cannot be met (e.g., a dead end). In these cases, we gradually relax the constraints as we try to sample points that fit them. The intent of this sampling procedure is to generate waypoint sequences that resemble realistic driving routes where intermediate destinations are reached in a natural succession.\nWe use two acceleration techniques specific to initialization. First, we draw a large buffer of vehicle states and goals all"}, {"title": "A.2. Dynamics model update", "content": "The dynamics model (described below) produces \\(s^{(t+1)}\\) given \\(s^{(t)}\\) and \\(a^{(t)}\\). This is a set of element wise operations (trigonometric functions, multiplications, divisions, etc.) that are parallelized on a GPU using their respective PyTorch implementations."}, {"title": "A.3. Road localization", "content": "The next step is to localize \\(s^{(t+1)}\\) on the road surface.\nRoad representation. GIGAFLOW represents the road surface as a set of potentially overlapping polygons, exclusively using convex quadrilaterals. We find quadrilaterals to offer a good balance between expressivity and the simplicity of operations. A given lane on the road surface is approximated by quadrilaterals that have the same width as the lane and are 1 m in length. We find this resolution of polygons to be a good trade-off between the accuracy of road geometry approximation and the total number of primitives.\nIt should be noted that the number of geometric primitives could be minimized further by merging polygons in the regions with simple geometry, such as straight road segments. However, we retain the uniform polygon density irrespective of the geometric complexity because this polygonal subdivision additionally serves as a spatial hash map for certain types of queries (e.g., map observations are pre-computed for all polygon center points).\nFrenet coordinates. Let q represent the distance along a lane, d be the distance from lane center, and polyId be a unique identifier of the polygon approximating the road geometry at the current location. The Frenet coordinate for position (x, y) is then (q, d, polyId).\nWe convert world-frame state \\(s^{(t)}\\) to Frenet-frame state \\(f^{(t)}\\) as this information is useful for constructing actor observations and covers off-road checking in the majority of cases. We construct the Frenet-frame state by first finding the polygon that contains (x, y). Given this polygon, we can compute (q, d) by transforming (x, y) into the coordinate frame defined by the polygon's heading and center point then adding the distance from the start of the lane to the"}, {"title": "A.4. Off-road checking", "content": "Given \\(s^{(t)}\\), we localize the vehicle's bounding box center and corners on the road surface using the spatial hash procedure described above. In the majority of cases, a vehicle is off-road if any of these 5 points could not be localized onto the road surface. However, there are two edge cases that are important to handle.\nCurved roads and islands. These are various situations where all 5 of these points can be on the road surface but the vehicle should still be considered off-road. Two such examples are curved roads (where part of the bounding box overhangs the edge of the road) and pedestrian safety islands (where the vehicle can straddle the islands). To handle both these cases, we generate a set of out-of-bounds (OOB) points by taking the mid-point of each polygon edge, nudging it slightly in the outwards direction, and keeping all points that do not lie within any other polygon. A vehicle is classified as off-road if any of these OOB points are found to be within its bounding box."}, {"title": "A.5. Collision detection", "content": "Given \\(s^{(t)}\\) and \\(s^{(t+1)}\\), we detect collisions as follows. Let \\(s_i^{(t)}\\) and \\(s_i^{(t+1)}\\) be successive states of agent i, and \\(s_j^{(t)}\\) and \\(s_j^{(t+1)}\\) be the successive states of agent j. We perform two checks to see if a collision occurred. First we transform \\(s_j^{(t)}\\) into the coordinate frame of \\(s_i^{(t)}\\) and \\(s_j^{(t+1)}\\) into the coordinate frame of \\(s_i^{(t+1)}\\). Then we check to see if any of the lines defined by the movement of agent j's corners intersectv with agent i's bounding box (the bounding box is centered at the origin). We then swap the roles of agents i and j and perform this check again. A collision occurred between agents i and j if either check is positive. Note that we only perform collision detection, not collision simulation.\nWe accelerate this using our spatial hash by constructing, for all agents, the axis-aligned bounding box (AABB) that contains the vehicle's bounding box at both states \\(s^{(t)}\\) and"}, {"title": "A.6. 2.5-D simulation", "content": "Simulating driving can be largely approximated without errors as a two-dimensional problem. This approximation enables performance improvements and reduces code complexity (thereby limiting the surface area for coding errors). However, certain cases cannot be accurately approximated in two dimensions, like overpasses. GIGAFLOW handles this with \"2.5-D\" simulation. We simulate the world as if it were 2-D and then correct for these errors. For example, we perform collision detection in two dimensions and then remove collisions that would not have occurred in 3-D. We maintain the vehicle's z-coordinate by applying the dynamics model purely in 2-D and then looking up new \\(z^{(t+1)}\\) for all vehicles from the map that correspond to new locations \\(s^{(t+1)}\\). We use \\(z^{(t)}, s^{(t)}\\), and the maximum slope of a given map to filter out conflicting z values in cases like overpasses."}, {"title": "A.7. Hardening", "content": "Given the extremely low collision and off-road rates seen in GIGAFLOW training we found that extremely rare bugs would dominate the collisions and off-road events when present. We iterated extensively; training numerous policies to very high fidelity, watching videos of collisions and off-road events, and sieving those caused by coding errors and numerical inaccuracies rather than the agent's poor decision making. Each iteration would yield new bug fixes, improving both training and downstream benchmark performance.\nTo address these bugs, we built numerous visualization tools, as merely knowing that a rare bug existed was not enough to fix it - we had to find the exact steps to reproduce it as well. The visualization, diagnostic, and recording tools we developed were instrumental to the overall success of the project."}, {"title": "B. Defining the partially observed stochastic game", "content": "We model the environment that our agents learn in as a partially observed stochastic game (POSG) (Hansen et al., 2004); an extension of POMDPs to the multi-agent setting in which there are multiple agents with conflicting goals. We define each of the components of the POSG below."}, {"title": "B.1. Observations", "content": "We render the world state \\(s^{(t)}\\) into a relatively low dimensional vector representation. In order to drive safely, a GIGAFLOW agent utilizes information about vehicle's dynamics and its position w.r.t. the lane \\(S^{(t)}\\), approximate map of the surrounding area, including roads and traffic lights (\\(W_{lane}^{(t)}, W_{boundary}^{(t)}, W_{stop}^{(t)}\\)), observations of other traffic participants \\(A^{(t)}\\), desired destination and intermediate waypoints \\(G^{(t)}\\), as well as the agent's conditioning \\(C_{reward}\\) (Table A2).\nLocal observation \\(S^{(t)}\\) can be further broken down as follows (time indices omitted for clarity):\n\\(c, \\theta\\): the distance from the current lane center and the angle relative to lane heading.\n\\(\u03ba\\): local road curvature.\n\\(v\\): current speed of the vehicle.\n\\(V_{lim}\\): maximum allowed speed.\n\\(\u03c8\\): current steering angle.\n\\(a_{long}, a_{lat}\\): current longitudinal and lateral acceleration.\nDriver's acceleration limits \\(C_{acc}\\).\n\\(C_{throttle}, C_{steer}\\): randomized coefficients determining the vehicle's responsiveness to throttle and steering inputs.\nl, w: driver's vehicle's length and width."}, {"title": "B.2. Actions and dynamics", "content": "Our agents use a discrete set of actions to control the vehicle's change in acceleration using a jerk-actuated bicycle dynamics model. The action space includes 12 total actions", "training)": "n\\(a_{long"}, {"as": "n\\(X(\u03b1) = 0.5 \\mathcal{U"}, "\u03b1^{-1}, 1) + 0.5 \\mathcal{U}(1, \u03b1), \u03b1 > 1\\)\nThis distribution generates an equal number of samples smaller and greater than one, thereby allowing for a balanced randomization of dynamics properties.\nWe apply a small modification to the Eqs. (1) and (2), setting values \\(a_{long}^{(t)}, a_{lat}^{(t)}\\) to exactly 0 when acceleration changes sign (i.e. when \\(a_{long}^{(t-1)} a_{long}^{(t)} < 0\\)). We found that this modification makes it easier for the agent to wait in place or drive at a constant velocity, producing smoother trajectories.\nThe acceleration components are then clipped ensuring the g-forces stay within the specified limits (here \\(C_{acc} \\sim X(1.5)\\)):\n\\(a_{long} \\leftarrow clip(a_{long}^{(t)}, -5, 2.5 C_{acc})\\)\n\\(a_{lat} \\leftarrow clip(a_{lat}^{(t)}, -4, 4)\\)\nWe update the velocity magnitude using the trapezoidal rule (averaging previous and current accelerations):\n\\(v^{(t)} = v^{(t-1)} + 0.5 (a_{long}^{(t-1)} + a_{long}^{(t)}) \u2206t\\)\nJust as for the accelerations, we set \\(v^{(t)}\\) to exactly 0 when its value changes sign. We then clip \\(v^{(t)}\\) to stay within the randomized speed limit (\\(C_{vel} \\sim X(1.5)\\)):\n\\(v^{(t)} \\leftarrow clip(v^{(t)}, -2, 20 C_{vel})\\)\nTo reach the acceleration \\(a_{lat}^{(t)}\\), the vehicle would have to follow the arc with radius \\(|\u03c1|\\) and signed"]}