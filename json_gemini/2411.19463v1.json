{"title": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG Systems", "authors": ["Shengming Zhao", "Yuheng Huang", "Jiayang Song", "Zhijie Wang", "Chengcheng Wan", "Lei Ma"], "abstract": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the capability of large language models (LLMs) and has demonstrated promising efficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show superior performance, they face unique challenges in stability and reliability. Their complexity hinders developers' efforts to design, maintain, and optimize effective RAG systems. Therefore, it is crucial to understand how RAG's performance is impacted by its design. In this work, we conduct an early exploratory study toward a better understanding of the mechanism of RAG systems, covering three code datasets, three QA datasets, and two LLMs. We focus on four design factors: retrieval document type, retrieval recall, document selection, and prompt techniques. Our study uncovers how each factor impacts system correctness and confidence, providing valuable insights for developing an accurate and reliable RAG system. Based on these findings, we present nine actionable guidelines for detecting defects and optimizing the performance of RAG systems. We hope our early exploration can inspire further advancements in engineering, improving and maintaining LLM-driven intelligent software systems for greater efficiency and reliability.", "sections": [{"title": "1 INTRODUCTION", "content": "Advancements in natural language processing (NLP) have brought Large Language Models (LLMs) [1]\u2013[3] to the forefront, revolutionizing both academic research and practical applications across diverse domains. Recently, Retrieval-Augmented Generation (RAG) [4], as a typical and representative LLM-enabled system, has emerged to further enhance the capabilities of LLMs. By retrieving relevant and up-to-date information from external knowledge bases, RAG effectively mitigates key limitations of standalone LLMs, such as the high cost of training and fine-tuning [3], the phenomenon of hallucination [5], and inherent input window limitations [6]. Given its promising performance and cost-effectiveness, RAG has achieved success across various tasks and scenarios, including chatbots [7], factual QA [8], long-form generation [9], and automatic code completion [10]\u2013[13]. Moreover, RAG has already become a foundational technology in various real-world products like Contextual AI [14] and Cohere [15].\nAlthough the RAG technique has strong potential across various tasks, maximizing its effectiveness is neither straightforward nor consistently guaranteed. Unlike standalone LLMs that rely on fine-tuning, LLM-driven RAG systems require huge integration efforts and careful optimization to achieve high performance, and their effectiveness varies on a case-by-case basis. We highlight the typical workflow of a typical RAG system [16] into two phases, as shown in Figure 1: (1) the retrieval phase takes the user input query and uses a retriever to search for relevant information from an indexed database, which are retrieved documents; (2) the generation phase uses a prompt generator to create a prompt with retrieved documents and the query, and passes it to a frozen LLM to obtain the final answer.\nWhile seemingly straightforward, the LLM-driven RAG system involves sophisticated implementations, with numerous design choices in its pipeline that significantly impact overall system performance. For instance, there are various candidate retrievers [17]\u2013[20], as well as advanced retrieval techniques such as reranking [21], rewriting [22], iterative retrieving [23], [24], that could be employed. Yet, there is a lack of understanding of their effects on system performance, whether they should be adopted, and the concrete scenarios that one technique is better than the others. Therefore, it is hard to diagnose the performance issues of the retrieval phase. As another example, developers have to manually decide the number of retrieved documents and the prompt techniques [25]\u2013[32], where the systematical guidance is in an urgent need. Each of these design choices in RAG system construction may have a huge impact on performance and thus should be carefully made.\nYet, the design process of RAG still heavily depends on human expertise and often demands considerable efforts \u2014 sub-optimal design choices may greatly degrade system performance, as demonstrated by our preliminary study in Section 3.5. Thus, the complexity of LLM-driven RAG systems highlights a critical challenge:\nHow to systematically build a reliable, high-performing\nLLM-driven RAG system?\nTo the best of our knowledge, no existing work has systematically studied the system integration of LLM-driven RAG systems. Therefore, we initiate this early exploratory study of LLM-driven RAG systems, aiming to understand the underlying factors for system performance changes, ultimately providing guidance for the development and optimization of LLM-driven RAG systems.\nOur study approach focuses on understanding the impact of two critical artifacts within RAG systems \u2014 retrieved documents and prompt (Figure 2). To gain insights into these artifacts, we argue that four key factors could influence RAG systems' quality: Retrieval Document Type and Retrieval Recall as the main aspects of retrieved documents, and Document Selection and Prompt Techniques as the key aspects of prompt. By examining these factors, we can understand the ultimate effects of various design choices, techniques, and architectures on RAG system performance without needing to study every possible configuration directly.\nTo ensure a broad and comparative perspective, our study is grounded in three code-related datasets and three question-answering datasets, along with two representative LLMs. We evaluated the LLM-driven RAG systems in terms of correctness and confidence, offering a comparative assessment of their performance and behavior characteristics. The analyses take over 200,000 API calls and consumed more than 1,000 GPU hours."}, {"title": "2 BACKGROUND", "content": "In this section, we provide an overview of the retrieval and generation phases in RAG systems, detailing their design and implementation."}, {"title": "2.1 Retrieval Phase", "content": "As illustrated in Figure 1, the retrieval phase of a LLM-driven RAG system often contains two main components: the indexed database and the retriever.\nThe indexed database is a structured collection of documents \\(D_1, D_2, ..., D_n\\), containing domain knowledge and information related to potential user queries. Organized and optimized for efficient searching, it serves as the foundation for retrieving pertinent information.\nThe retriever ranks all documents in the indexed database based on their similarity to a query. Given a query Q and the indexed database comprising n documents \\(D_1, D_2, ..., D_n\\), the retriever identifies the order of these documents according to a similarity criterion sim. Formally,\n\\{Di_1, Di_2,..., Di_n\\} = sort (\\{D_1, D_2, ..., D_n\\}, sim(Q, D_i)) \\(1\\)\nThere are two primary types of retrievers, based on their utilization of the sparse or dense representations of queries and documents [33]. Sparse retrievers compute similarity by projecting the query and documents into a sparse vector space that aligns with the vocabulary of the documents, typically using traditional Bag-of-Words methods such as TF-IDF or BM25 [17]. \u03a4\u03bf overcome the limitations of these methods, which may struggle with synonyms or varying contextual meanings, dense retrievers [18], [19] obtain similarity scores by encoding queries and documents as dense vectors that capture their semantic meaning."}, {"title": "2.2 Generation Phase", "content": "The generation phase contains two components: the prompt generator and the frozen LLM. A straightforward implementation of the prompt generator is to concatenate the documents and the query in a simple sequence, represented as < \\(D_1, D_2,...,D_k, Q\\) >. Additionally, more sophisticated strategies may include applying various prompt techniques [27], [29], [31], [32] to better align the prompt with the LLM's capabilities, or using iterative methods that involve multiple rounds of retrieval and generation [23], [24].\nThe frozen LLM is an off-the-shelf LLM used without modification; therefore, this paper does not focus on it."}, {"title": "3 STUDY DESIGN", "content": "This section, we introduce our study design, including datasets, evaluation metrics, retrieval methods, and model settings. Following this, we present the two preliminary studies conducted."}, {"title": "3.1 Datasets", "content": "We conduct our study on six representative datasets covering scenarios in both software engineering (SE) and natural language processing (NLP) domains, as listed in Table 1.\nIn the SE domain, automatic code generation and completion are critical tasks [39] and are considered core capabilities of LLMs [1], [3], [40], [41]. In this study, we select three Python library-oriented datasets: CoNaLa [34], DS1000 [35] and Pandas_Numpy_Eval(PNE) [12], since their primary retrieved documents are API documentation [10], [42], allowing for well-defined and accessible oracle (ground-truth) documents. Specifically, we utilize 200 randomly selected samples from DS1000, all 200 samples PNE, and incorporated 100 unit-tested samples from CoNaLa [10]. We exclude open-domain code exercises and repository-level code completion datasets, as they rely on different retrieval forms. For instance, RAG systems for code exercises typically retrieve similar code snippets [43]\u2013[45], while repository-level tasks rely on project-specific contextual information [46]\u2013[49]. To the best of our knowledge, no robust methods currently exist to define and obtain oracle documents for these tasks, which complicates the setup of controlled experiments.\nFor the NLP domain, question answering (QA) tasks often benefit from RAG techniques due to their demand for factual knowledge. In this study, we select the three most prominent QA datasets: Natural Questions (NQ) [36], TriviaQA [37], and the multi-hop QA dataset HotpotQA [38], randomly extracting 2,000 queries from each.\nWe select code generation and open-domain QA as representative tasks of SE and NLP domains, as they are representative and cover a diverse range of challenges. We believe that our findings could be generalized to other tasks in these two domains."}, {"title": "3.2 Metrics", "content": "We evaluate the RAG system from two aspects.\n\u2022 Correctness For Code Generation and Completion tasks, we use the pass@1 metric to measure the functional correctness of the generated code. A pass occurs if the generated code produces the expected output when executed with a given input. This metric is widely adopted for robustly assessing the quality of code [40]. For question-answering (QA) tasks, we use the has_answer metric [50]. This approach addresses the difficulty of ensuring that the LLM output exactly matches the correct answer by checking if any of the canonical answers appear in the generated output, providing a more flexible yet reliable evaluation of LLM accuracy.\n\u2022 Confidence measures the system's uncertainty about its generated answer. Given the inherent instability in LLM generation, confidence is an important aspect of ensuring a reliable and trustworthy system. We apply perplexity (PPL), a standard metric for assessing the quality of language model generation. Perplexity quantifies how well a probability distribution predicts a sequence, with lower values indicating greater model confidence and quality. Given the probabilities of generated tokens \\(p(x_1), ..., p(x_n)\\), the perplexity is calculated as\nPPL = exp\\(-\\frac{1}{n}\\sum_{i=1}^n log p(x_i)\\) \\(2\\)"}, {"title": "3.3 Retrieval Settings", "content": ""}, {"title": "3.3.1 Code Corpus Preparation", "content": "While there exists a collection of Python API documentations [10], it has several flaws making it unfit for our study: 1) it covers a limited range of third-party libraries, notably lacking API documentation for libraries, such as SciPy; 2) it omits API version information, which is crucial since different versions can introduce significant changes (e.g., TensorFlow 1.0 vs. 2.0); and 3) the corpus highly relies on string matching to determine retrieval success (i.e., locating the Oracle API documentation), which is likely to match APIs incorrectly with shared names (e.g., tensorflow.keras.layers.add and tensorflow.python.ops.math_ops.add).\nTo address these problems, we have developed a new automated documentation collection and API usage identification approach. It contains four steps:\n1) API Signatures Collection: Given the name and version of a Python library, we first install the library and traverses its modules to identify all attributes. Then we find public and callable attributes and collects their API signatures.\n2) Document Collection: We first use Pydoc to gather documentation for each API signature before merging duplicates since different signatures can share the same documentation.\n3) Oracle Function Collection: For each code generation task, we parse an Abstract Syntax Tree (AST) from the unit-test program and its canonical solution to extract the names of all functions used in the canonical solution.\n4) Oracle Document Identification: For each function name, we truncate all possible prefixes from the canonical solution, and then augments the unit tests with the help() from Pydoc for each prefix-function combination. For each function, API signatures are obtained through executing and then matched with the previously collected API documentations to identify the oracle documents.\nWe then gathered library requirements from all three code datasets, collected API documentation, and identified Oracle documents with this automated tool. Samples without API utilization are omitted. We use 10% of the samples as exemplars for few-shot learning. Finally, we created a corpus containing 70,956 unique API documents, aggregated from an initial 365,691 documents before deduplication."}, {"title": "3.3.2 QA Corpus Preparation", "content": "For NQ and TriviaQA datasets, we adopt the corpus constructed by [19], which uses the English Wikipedia dump from Dec. 20, 2018 as the source documents, then apply the pre-processing code to remove semi-structured data to extract the clean, text-portion of articles. Each article is then split into multiple, disjoint 100-word text blocks as passages, yielding a total of 21,015,324 passages. For the HotpotQA dataset, we use the accompanying corpus provided with the dataset, as described in [38], which consists of 5,233,235 documents sourced from Wikipedia.\nTo identify the documents containing supporting facts for each question (i.e., oracle documents), we follow the methodology outlined by [19] for NQ and TriviaQA. For HotpotQA, oracle documents are identified by verifying their alignment with the provided supporting facts."}, {"title": "3.3.3 Studied Retrievers", "content": "We explore both sparse and dense retrieval methods, as listed in Table 2. For sparse retrieval, we use BM25 [17] as the similarity metric, implemented with ElasticSearch [51] as the search engine. For dense retrieval, we select two representative off-the-shelf embedding models: ALL-MINILM-L6-v2 [20] and OpenAI's TEXT-EMBEDDING-3-SMALL. Additionally, we incorporate pre-trained retrieval models: Contriever [18], specialized for question answering, and CodeT5, fine-tuned specifically for code generation problems [10]. Similarity scores for embedded vectors are calculated using the FAISS library [52]."}, {"title": "3.4 LLM Settings", "content": ""}, {"title": "3.4.1 Prompt Generator", "content": "We use a zero-shot prompt style as the default. To ensure the LLMs comprehend each task accurately and provide clear, extractable answers, we crafted specific instructions I tailored to each dataset and model pair. Consequently, the prompt generator can be represented as < I, \\(D_1, D_2,..., D_k, Q\\) >. In our analysis of prompt techniques, we further examine how different prompt techniques affect RAG systems performance."}, {"title": "3.4.2 Studied LLMs", "content": "To ensure thorough experimentation, we employ both open-source and close-source Large Language Models from the Llama 2 [3] and GPT 3.5 [1] families, each well-represented in the field:\n\u2022 Llama 2. For QA tasks, we select Llama2-13b-chat, and for code tasks, we use CodeLlama-13b-instruct, which is fine-tuned with Python code from Llama2-13b to enhance robustness in code-related outputs.\n\u2022 GPT 3.5. We utilize gpt-3.5-turbo-0125 for both QA and code generation tasks.\nTo mitigate instability in LLM outputs and ensure reproducibility, we use greedy decoding for the Llama models and set the temperature to 0 for GPT 3.5. We apply regular expressions to extract the answer from LLM Responses."}, {"title": "3.5 Preliminary Study", "content": "We conducted two preliminary studies: (1) evaluating the effectiveness of a vanilla RAG system on our selected tasks to establish an initial baseline and motivation; and (2) comparing the performance of various retrievers to determined the optimal retrieval settings for further experiments."}, {"title": "3.5.1 A Vanilla RAG System", "content": "We conduct a preliminary experiment using a commonly applied RAG system configuration: BM25 as the retriever, incorporating the top 5 retrieved documents into the prompt, and applying a zero-shot prompt with instructions as described in Section 3.4.1.\nThe results, shown in Table 3, suggest that, despite the addition of retrieval and extended prompt length, the vanilla RAG system underperforms compared to the corresponding standalone LLMs. When using GPT-3.5 as the frozen LLM, the RAG system performs worse on 4 out of 6 datasets. Similarly, with Llama2-13B, the RAG system shows clear improvement on only 1 dataset, with negative or minimal improvements on the remaining five ones.\nThese findings highlight that a basic RAG system configuration is unlikely to deliver satisfactory performance. Therefore, in Sections 4 \u2013 7, we conduct further studies to explore how to effectively build a reliable, high-performing LLM-driven RAG system and identify the key issues contributing to poor performance."}, {"title": "3.5.2 Retriever Selection", "content": "We compare the retriever performance using the metric of retrieval recall, which is defined as the proportion of oracle (ground-truth) documents retrieved relative to the total oracle (ground-truth) documents. As shown in Figure 3, OpenAI's text-embedding-3-small achieves the highest performance across both QA and code datasets. The all-MiniLM-L6-v2 model performs comparably to text-embedding-3-small on code datasets, though slightly weaker on QA datasets. In contrast, BM25 and the specialized retrievers - Contriever and fine-tuned CodeT5 - show inferior retrieval recall on both QA and code datasets. Therefore, the rest of this study uses OpenAI's text-embedding-3-small as the retriever, given its adequate performance advantage."}, {"title": "4 RQ1: RETRIEVAL DOCUMENT TYPE", "content": "Our preliminary experiments reveal that a vanilla RAG system underperforms corresponding standalone LLMs in half of the commonly applied scenarios. This counterintuitive finding motivates us to investigate the reasons behind this underperformance and provide guidelines for building and optimizing RAG systems. To achieve this, we systematically analyze the key factors within RAG systems and their impact on overall system performance."}, {"title": "4.1 Experimental Setup", "content": "Retrieved documents can be classified into two main categories: oracle documents and distracting documents. Oracle documents contain part or all of the necessary knowledge required for problem-solving, whereas distracting documents, although deemed highly similar to the problem by the retriever, do not contain useful knowledge. To examine the impact of the retrieval document type, we introduce a third category: irrelevant documents. These documents neither contain useful information nor exhibit high similarity to the problem. We define three types of irrelevant documents: random (arbitrarily selected from the same knowledge base), diff (from a different domain, e.g., Wikipedia articles for a code generation task), and dummy (composed of meaningless word combinations).\nTo ensure a fair comparison, we keep the number of documents and the token length per document consistent. This setup mitigates the effect of document type, allowing accurately assessing its impact on RAG system performance."}, {"title": "4.2 Results and Discussion", "content": "The correctness (has_answer for QA tasks and pass@1 for code tasks), as well as the confidence of RAG systems under different document types are respectively displayed in Figure 4&5.\nDistracting documents degrade RAG system performance. RAG systems incorporating distracting documents (second column in each sub-figure of Figure 4) display the poorest correctness across all datasets and models. Since irrelevant are supposed to have minimal influence on RAG systems outputs due to their lack of useful information, the superior performance of RAG systems with irrelevant documents compared to those with distracting documents in most scenarios highlights the negative impact of the latter.\nThe confidence of RAG systems with distracting documents is higher than with irrelevant documents, as shown in Figure 5, which is as expected since irrelevant documents introduce noise that reduces the certainty of LLMs.\nIrrelevant documents outperform oracle documents in code tasks. As we expected, irrelevant documents are utilized as the baselines that are neither useful (like oracle) nor distracting to the LLMs generation (which is as so in QA datasets). However, both the correctness and confidence of RAG systems with irrelevant documents are comparable or even better than those with oracle documents in multiple scenarios, especially for diff documents (last column in each sub-figure).\nOracle documents consistently enhance system confidence. From Figure 5, we can observe that the perplexity of RAG systems with oracle documents is always lower than with distracting documents. Especially in GPT-3.5, QA tasks (three upper-left sub-plots), RAG systems with oracle documents are extremely lower and have an evident difference compared to RAG systems with distracting documents. The finding can be utilized to evaluate the distribution of oracle and distracting documents when the oracle documents are difficult or expensive to identify (which is common in real-world scenarios)."}, {"title": "4.3 Follow-up Experiments: The effect of irrelevant documents", "content": "Motivated by the intriguing phenomenon that RAG systems with irrelevant documents outperform those with oracle documents, we conduct additional experiments to study a follow-up question to RQ1:"}, {"title": "What is the effect of irrelevant documents on code generation for LLMs and RAG systems?", "content": "We investigate this question from two perspectives: 1) Extent of Improvement: To what extent can irrelevant documents boost the code generation capabilities of LLMs and RAG systems? 2) Characteristics of Effectiveness: What are the characteristics of this effectiveness, and how does it compare to previously observed effects? Understanding these aspects will shed light on how irrelevant documents impact RAG systems performance and may uncover new strategies for optimizing code generation in LLMs and RAG systems. Therefore, we hypothesize two possible explanations for this effect: 1) combining irrelevant documents with retrieved documents may amplify RAG performance, as suggested by [53], and 2) extending the input length could improve LLM performance, as indicated by [54]. Based on these hypotheses, we design the following experiments.\nSpecifically, we define four initial settings: RAG systems with oracle, distracting, retrieved top-5 documents, and standalone LLMs with no documents, using the same configuration as described in Section 4.1. These initial settings are then augmented with three types of irrelevant documents - random, diff, and dummy - as detailed in Section 4.1. To explore the characteristics of the effectiveness of irrelevant documents, we introduce a new document type, ellipsis, consisting solely of ellipses. Additionally, we include two control groups: 1) augmenting the initial RAG systems by repeating their original documents, referred to as repeat, and 2) using RAG systems with only irrelevant documents.\nIn all augmented LLMs and RAG systems, we maintain a consistent prompt length of 4000 tokens to ensure that the variations in results are not influenced by prompt length differences.\nIrrelevant Documents improve the code generation in all scenarios. As illustrated in Table 4, augmenting with irrelevant documents, especially diff and dummy documents, consistently increases the pass rate of generated code across all settings, including RAG systems with oracle, distracting, retrieved top-5 documents, as well as standalone LLMs with no documents. The most significant improvement is observed in RAG systems using only diff documents, achieving a 13.2% higher pass rate compared to the RAG system with retrieved top-5 documents, which is considered as the baseline. Notably, this performance is on par with the RAG system using oracle documents, underscoring the substantial effect of irrelevant documents in enhancing code generation capabilities.\nThe effect of irrelevant documents resembles a \u201cmagic word\u201d effect. We propose that the impact of irrelevant documents presents a novel effect, which we term the \u201cmagic word\u201d effect, based on the following observations: 1) the lack of improvement from augmenting RAG systems with ellipsis documents, repeating original documents, or extending the length of irrelevant documents suggests that the enhancement is not merely due to longer prompts, as emphasized by [54]; and 2) the superior performance of RAG systems using only irrelevant documents, compared to those using combinations of irrelevant and retrieved documents, challenges the findings of [53], which suggested that irrelevant documents enhance system accuracy when combined with relevant documents and appropriately positioned within a prompt. 3) Among the three types of irrelevant documents, diff documents consistently deliver the best performance, while random documents exhibit the least improvement in most scenarios.\nThese observations collectively indicate that irrelevant documents, particularly diff documents, function like a \u201cmagic word\u201d in enhancing LLMs and RAG systems performance. Despite being unrelated to the task, the inclusion of irrelevant documents significantly boosts code generation results. The absence of similar benefits from merely extending prompt length or combining with retrieved documents reinforces the distinctiveness of this effect, highlighting how simply adding certain irrelevant words can uniquely optimize LLM outputs.\nBased on the findings from Section 4.2&4.3, we summarize two guidelines for developers:"}, {"title": "5 RQ2: RETRIEVAL RECALL", "content": "In this research question, we consider another critical dimension in RAG systems: Retrieval Recall. Retrieval recall quantifies the proportion of oracle documents retrieved and utilized by RAG systems over the total oracle documents, providing an overall measure of retrieved document quality. Given its critical role in determining RAG system performance [19] and the observation that vanilla RAG systems often underperform compared to standalone LLMs (Section 3.5), we aim to quantify the influence of retrieval recall on RAG systems performance and determine the threshold retrieval recall required for RAG systems to surpass standalone LLMs across different datasets and models."}, {"title": "5.1 Experimental Setup", "content": "To investigate the impact of retrieval recall, we first configure RAG systems with only oracle documents for each sample, representing a scenario with perfect retrieval recall. We then control the retrieval recall to different levels: 0.8, 0.6, 0.4, 0.2, and 0, observing the corresponding performance changes in RAG systems. To ensure a controlled setup, we maintain consistency in the number of documents and prompt token length across all retrieval recall levels. This is achieved by randomly replacing a portion of the oracle documents with distracting documents, simulating scenarios with varying degrees of retrieval recall."}, {"title": "5.2 Results and Discussion", "content": "The performance (correctness, confidence) of RAG systems under varying levels of retrieval recalls are shown in Figure 6&7. We further summarize the retrieval recall requirements for different datasets and models in Table 5.\nCorrectness increases with retrieval recall. In Figure 6, an upward trend is observed between retrieval recall and correctness across all QA datasets, as well as the code datasets CoNaLa and PNE. However, for DS1000, while the trend is positive, notable fluctuations occur. This is likely due to the complexity of problems in DS1000, as highlighted in Table 6.\nThe larger number of oracle documents and higher problem complexity may make it challenging for LLMs to establish clear relationships between the retrieved documents and the problem, thereby diminishing the effectiveness of increased retrieval recall.\nHigher retrieval recall aligns with higher confidence in QA tasks but not in code tasks. In Figure 7, perplexity decreases (confidence increases) with higher retrieval recall in QA datasets, reflecting a clear inverse relationship between retrieval recall and system confidence. However, this trend is inconsistent in code tasks, as seen with CodeLlama-13B on CoNaLa at 0.8 retrieval recall and GPT-3.5 on DS1000 at 0.6 retrieval recall. This suggests that perplexity may serve as a useful indicator for detecting retrieval recall quality in QA tasks when direct evaluation is challenging; however, it is not a reliable metric for code tasks.\nRetrieval recall requirements range widely, from 20% to 100%. As shown in Table 5, the minimum retrieval recall required for RAG systems to outperform corresponding standalone LLMs varies dramatically, ranging from 0.2 to 1.0 across different models and datasets. Some datasets benefit from moderate levels of retrieval recall, while others demand near-perfect recall to achieve similar improvements. Notably, for the PNE and TriviaQA datasets, only 100% retrieval recall allows RAG systems to match the performance of the standalone LLMs. This may be because tasks in these two datasets are relatively straightforward for modern LLMs, with models like GPT-3.5 already achieving exceptionally high scores, exceeding 0.8.\nRAG systems with 100% retrieval recall fail on instances where standalone LLMs succeed. To further investigate why only RAG systems with 100% retrieval recall match the performance of standalone LLMs, we conduct a prediction distribution analysis. Specifically, we compare the percentage of samples correctly solved by standalone LLMs but not by RAG systems with 100% retrieval recall. To account for LLM instability, we also evaluate the instances correctly solved by RAG systems with 80% retrieval recall but not by those with 100% retrieval recall.\nAs shown in Table 7, RAG systems exhibit similar behavior between 100% and 80% retrieval recall but differ significantly from standalone LLMs. Even with 100% retrieval recall, RAG systems fail on a notable number of instances (up to 12% in code generation tasks and 6.9% in QA tasks) that standalone LLMs solve correctly. Conversely, cases only correctly solved by RAG systems with 80% retrieval recall are minimal. This highlights a performance degradation in RAG systems, which may stem from the LLMs' difficulty in properly understanding and effectively utilizing retrieved information. Developers should be cautious of this degradation, as it could be a key contributor to the underperformance of RAG systems.\nBased on the four findings outlined above, we propose three practical guidelines for developers:"}, {"title": "6 RQ3: DOCUMENT SELECTION", "content": "As discussed in Section 2, RAG systems typically utilize pretrained and frozen LLMs for answer generation, making the quality of the prompt the primary decisive factor in this phase. In this research question, we first investigate the impact of document selection. Since only a limited subset of the highest-ranked documents, as determined by the retriever, can be incorporated into the prompt, selecting the optimal number of documents involves a crucial yet underexplored trade-off: while adding more documents increases the likelihood of including useful information, it also raises the risk of introducing distracting content that could mislead the LLM."}, {"title": "6.1 Experimental Setup", "content": "In our experiments, we test different configurations for code generation and QA datasets: For code tasks, we evaluate the impact of appending top-1/5/10/15/20 retrieved documents. For QA datasets, we follow the same configuration\u2014top-1/5/10/15/20\u2014on Llama2-13B, with additional tests for appending top-40/60/80 documents. This setup examines the minimum to the maximum number of documents within the token limits of the models. (GPT-3.5 and CodeLlama-13B support up to 16k tokens, while Llama2-13B has a 4k token limit.)"}, {"title": "6.2 Results and Discussion", "content": "The performance data is illustrated in Figure 8&9.\nConfidence does not reflect correctness as the number of documents varies, especially in code tasks. We observe inconsistencies between correctness and confidence (measured by perplexity). As more documents are included, in QA tasks, the perplexity of RAG systems first sightly decreases and then increases, yet correctness does not exhibit a corresponding decline. In code tasks, perplexity remains stable or even decreases while the pass rate continues to drop. For QA datasets, this inconsistency may be attributed to the inclusion of distracting information, which confuses the model and lowers its confidence, leading to an increase in perplexity. However, in code datasets, perplexity shows no clear trend and behaves paradoxically with correctness\u2014for example, in the DS1000 dataset, perplexity even decreases as correctness declines. This inconsistency and the observations in RQ2 suggest that perplexity is not a reliable metric for evaluating code generation quality in LLMs and RAG systems.\nRetrieval recall does not determine RAG system performance when the number of documents varies. In Figure 10, we summarize the retrieval recalls of RAG systems under varying numbers of documents to explore the impact of document numbers. For QA tasks, correctness initially increases, stabilizes, and then slightly decreases (Figure 8.a&c), while retrieval recall rises sharply at first, then more gradually (Figure 10.a). Although the trends for correctness and retrieval recall align at the beginning, they diverge later. In code tasks, this divergence is even more pronounced: while RAG system correctness initially rises and then declines (Figure 8.b&d), retrieval recall continues to steadily increase (Figure 10.b). This suggests that the benefit of higher retrieval recall is offset by other factors when more documents are added, resulting in stable performance in QA tasks and declining accuracy in code tasks.\nAdding more documents causes RAG systems to make errors on previous correct instances. To analyze the side effects of adding more documents, we use RAG systems with the top-5 retrieved documents as the baseline and calculate the percentage of instances correctly solved by the baseline but rendered incorrect as larger numbers of documents are included. As shown in Figure 11, RAG systems make progressively more mistakes as additional documents are added, particularly in Llama and code tasks. For every five additional documents, error rates increase by an average of 1% in code tasks and 1.5% in QA tasks using Llama. While the mistakes made by the RAG system with the top-10 documents, compared to the baseline (top-5) system, may partly result from the inherent instability of LLMs, the steadily increasing error rate demonstrates that including more documents has a clear negative side effect on RAG systems. The actual impact of this side effect is likely even greater than shown in the figure, as RAG systems with top-15 documents not only make mistakes on instances correctly solved by the top-5 system but also on those correctly handled by the top-10 system and so on.\nThis degradation is likely caused by the growing presence of distracting information, which confuses LLMs and leads to incorrect answers for problems they could initially solve. This explains the stabilization or decline in RAG system correctness with increasing document numbers despite the continuous rise in retrieval recall.\nBased on our findings regarding document selection, we summarize the following guidelines for developers:"}, {"title": "7 RQ4: PROMPT TECHNIQUES", "content": "Following RQ3, we examine another crucial design choice: Prompt Techniques. Although prompt techniques are known to significantly enhance LLM performance, the actual impact of different prompt methods on RAG systems across different contexts remains unknown and unexplored. To bridge this gap, RQ4 systematically investigates how different prompt techniques influence RAG system performance."}, {"title": "7.1 Experimental Setup", "content": "We first systematically survey various kinds of representative prompt techniques and integrate them into our experimental contexts. Our processes of collecting prompt techniques collecting are as follows:\n1) Review Existing Surveys: We begin by reviewing prompting surveys [5"}]}