{"title": "VSFormer: Value and Shape-Aware Transformer with Prior-Enhanced Self-Attention for Multivariate Time Series Classification", "authors": ["Wenjie Xi", "Rundong Zuo", "Alejandro Alvarez", "Jie Zhang", "Jessica Lin", "Byron Choi"], "abstract": "Multivariate time series classification is a crucial task in data mining, attracting growing research interest due to its broad applications. While many existing methods focus on discovering discriminative patterns in time series, real-world data does not always present such patterns, and sometimes raw numerical values can also serve as discriminative features. Additionally, the recent success of Transformer models has inspired many studies. However, when applying to time series classification, the self-attention mechanisms in Transformer models could introduce classification-irrelevant features, thereby compromising accuracy. To address these challenges, we propose a novel method, VSFormer, that incorporates both discriminative patterns (shape) and numerical information (value). In addition, we extract class-specific prior information derived from supervised information to enrich the positional encoding and provide classification-oriented self-attention learning, thereby enhancing its effectiveness. Extensive experiments on all 30 UEA archived datasets demonstrate the superior performance of our method compared to SOTA models. Through ablation studies, we demonstrate the effectiveness of the improved encoding layer and the proposed self-attention mechanism. Finally, We provide a case study on a real-world time series dataset without discriminative patterns to interpret our model.", "sections": [{"title": "1 Introduction", "content": "A multivariate time series (MTS) consists of multiple time-ordered measurements or observations across different variables. Multivariate Time Series Classification (MTSC) is one of the key tasks for MTS, and it is widely applied in various fields such as motion recognition [21], solar flare prediction [1], and human activity recognition [26]. As a result, it has garnered increasing research interest recently [22].\nWhile numerous methodologies have emerged showcasing promising results [22], one popular kind of approach revolves around identifying and exploiting repeated patterns (subsequences) in time series, treating them as discriminatory features for classification. Such subsequences typically undergo normalization to underscore their shape characteristics. In [27], the authors introduced the notion of Shapelets the subsequences that can best represent a class and use them with traditional classifiers such as decision trees to classify time series. Building upon this idea, recent deep learning methods [15, 19, 31] have targeted the search for such patterns through the use of neural networks resulting in improved performances.\nHowever, a crucial limitation of these methods is their foundational assumption for the presence of discriminative patterns for MTSC. Many real-world time series data lack such specific patterns that can effectively distinguish one sequence from another, rendering these methods sub-optimal. A related example as presented in Figure 1 is found in solar flare prediction. The two time series on the left correspond to conditions leading to flaring events and those on the right to non-flaring events. While it may be challenging to identify discriminative patterns (also known as shape), the raw numerical information (the value), differing by orders of magnitude, provide a significant distinction between the two classes (The values from the first class (0.08-0.12) are at least one order of magnitude larger than those from the second class (0.0014-0.008)). Shape-based methods re-"}, {"title": "2 Related Work", "content": "2.1 Multivariate Time Series Classification Existing works for MTSC can be roughly categorized into distance-based, pattern-based, and deep-learning-based methods. Distance-based methods rely on measuring the dissimilarity between time series using distance measures such as Euclidean Distance (ED) and Dynamic Time Warping (DTW) [23] and use 1-nearest-neighbor for classification. Pattern-based methods extract bag-of-patterns or discriminative patterns from raw time series. A prominent example of using bag-of-patterns is WEASEL+MUSE [24], which constructs a multivariate feature vector from each variable of the MTS using various sliding windows, extracts discrete features, and undergoes feature selection to remove non-discriminative features. Meanwhile, several approaches have been devoted to using shapelets [27], such as Generalized Random Shapelet Forests (gRSF) [18] that generate shapelet-based decision trees by randomly choosing a subset of shapelets.\nMore recently, deep learning models have shown notable success in MTSC [22]. MLSTM-FCNs [17] employ a combination of LSTM and stacked CNN layers for feature extraction from time series. TapNet [29] further introduces the attentional prototype learning for fully and semi-supervised MTSC. ROCKET [9] and its optimized counterpart, MiniRocket [10], use random convolutional kernels for time series transformation and subsequently train classifiers on these transformed features.\nSeveral deep-learning models also focus on finding discriminative patterns. For instance, ShapeNet [19] utilizes embedding learning to map subsequences into a unified space and employs clustering to discern these patterns. RLPAM [15] uses reinforcement learning to detect patterns beneficial for classification tasks. Meanwhile, SVP-T [31] uses k-means to capture vast amounts of time series subsequences and feed them into a Transformer encoder. Despite their remarkable performances, these models have the assumption that some discriminative patterns exist in the time series, and overlook scenarios where such patterns are absent in the datasets.\n2.2 Transformers for Time Series Classification The success of the Transformer model has inspired numerous researchers to adapt it for time series classification. Beyond SVP-T, which is designed specifically for MTSC, several representation learning methods have taken classification as their downstream task. For ex-"}, {"title": "3 Problem Formulation", "content": "A multivariate time series, $X = {X^1, ..., X^V} \\in R^{V \\times T}$, is a collection of several univariate time series, $X^v \\in {x_1,...,x_T}$. $T \\in Z^+$ is the total number of observations, and $V \\in Z^+$ represents the number of variables. An MTS dataset consists of pairs of MTS and associated labels, which can be represented as ${(X_1,Y_1), ..., (X_N,Y_N)}$. Here, $y = {Y_1,\u2026\u2026,Y_C} \\in R^C$ denotes the corresponding class, and N is the total count of MTS instances in the dataset.\nA multivariate time series classification problem aims to train a classifier that can predict the class label for an unlabeled, previously unseen MTS."}, {"title": "4 Methodology", "content": "4.1 Overall Architecture Figure 2 shows the overall structure of VSFormer. The time series dataset is initially subjected to two distinct preprocessing steps for shape and value, yielding input tokens and class-specific prior information. These tokens are then encoded using our improved Time Series Information (TSI) Encoding and fed into the Trans-"}, {"title": "4.2 Input Token Generation", "content": "In this section, we outline the data preprocessing steps for generating both shape and value tokens.\nShape Tokens. Figure 3 shows the shape token generation process. We employ STOMP [30], a well-known motif (repeated patterns) discovery algorithm to capture significant shapes within the data. All time series from the same class are concatenated by their variable, which results in a lengthy multivariate sequence represented as $X^{v,c} = {X_1^{v,c}, ..., X_T^{v,c}}$, where v denotes the variable, and c denotes the class. Subsequently, we search for the top-k motif pairs within this extended sequence (for illustrative clarity, in Figure 3, Step 2, we set k = 1). This results in a total of M=k\u00d7V\u00d7C motif pairs. From each motif pair, we choose one motif instance, denoted by $\\hat{S}_c$, as the prototype shape specific to its variable and class. The intuition is to identify a small set of patterns (shapes) representing each variable per class. Next, we search within each time series instance in the corresponding variable to locate subsequences similar to these prototype shapes (Figure 3, Step 3). Thus, for the i-th instance, we acquire shape token set $S = {\\hat{s}_{1,1}, \\hat{s}_{1,2}, ..., \\hat{s}_{V,C}}$ and their associated Z-normalized Euclidean Distances, represented as $D = {D_{1,1}, D_{1,2}, ..., D_{V,C}}$ (Figure 3, Step 4). To simplify notation, these are rewritten as $S = {S_1, S_2, ..., S_M}$ and $D = {D_1, D_2, ..., D_M}$.\nValue Tokens. Individual time points within a time"}, {"title": "4.3 Class-Specific Prior Information", "content": "Class-specific prior information corresponds to each input token and can be directly calculated from supervised information. We introduce it to enrich the encoding process and guide self-attention learning to enhance classification-relevant features and reduce noise.\nPrior information for shape. For shape tokens, we use the corresponding distances of the shapes to calculate the weight as their prior information.\nFirst, we compute the weights for prototype shapes. For each prototype shape $S_{v,c}$, we compute two distinct types of distances, $D_1$ and $D_2$. The former, $D_1$, represents the mean intra-class distance for each prototype shape and is computed by averaging all distances within the same class c. In contrast, $D_2$ describes the mean inter-class distance and is determined by averaging all distances corresponding to different classes. Using both, we calculate a ratio $\\hat{D} = \\frac{D_2}{D_1+D_2}$ that quantifies the distinctiveness of the prototype shapes across classes. A value of $\\hat{D} > 0.5$ indicates that the shape is discriminative and significantly contributes to classification. Conversely, $\\hat{D} = 0.5$ implies that the shape is common across all classes and does not contribute to discrimination. Note that $\\hat{D} < 0.5$ indicates that the prototype shape is a repeated pattern that does not belong to its class, which is obviously contrary to our method and is therefore unlikely to happen.\nWe then assign a weight $w_{\\hat{s}}$ to each prototype shape $\\hat{S}_{v,c}$, through the following computations:\n(4.1) $x = max(\\hat{D} \u2013 0.5,0)$,\n(4.2) $w_{\\hat{s}} = e^{ax}, a \\geq 0$,\nwhich ensures that prototype shapes with higher discriminative power (larger $\\hat{D}$) receive greater weights.\nSubsequently, for each shape token $S_i$, we determine its distance d to the prototype shape $\\hat{S}_{v,c}$, and assign it a weight $w_{s_i}$, using the formula:\n(4.3) $w_{s_i} = \\beta e^{-d} + 1, \\beta \\geq 0$,\nwhich ensures the less similar the shape, the smaller the weight.\nFinally, we compute the final weight $w_{final, wo}$, which serves as the prior information for each shape token:\n(4.4) $\\rho_i = w_{final} = w_{\\hat{s}_{v,c}} \\times w_{s_i}$.\nThe procedure ensures that a shape token more similar to a discriminative prototype shape receives a higher weight. Note that \u03b1 and \u03b2 are weighting hyperparameters. Setting either one to zero will remove the influence of its corresponding weight on the final weights.\nPrior information for value. Generating prior information for value tokens involves an entropy-based feature importance calculation. Given the set of value tokens A and corresponding class labels from the training set, we first compute the entropy H(A), quantifying the uncertainty associated with these tokens. The entropy is defined as:\n(4.5) $H(A) = - \\sum_{i=1}^C Pr(i|A) log_2 Pr(i|A)$,\nwhere Pr(i|A) represents the probability of class i given a value token A, and C denotes the total number of classes.\nNext, the conditional entropy H(A|Y) measures the average entropy of A given the class label Y:\n(4.6) $H(A|Y) = - \\sum_{j=1}^C Pr(Y = j) \\sum_{i=1}^C Pr(i|A, Y = j) \\times log_2 Pr(i|A, Y = j)$,"}, {"title": "4.4 Time Series Information Encoding", "content": "While the relative position of shapes within a time series instance is often more crucial than sequence order, the authors of SVP-T [31] proposed a positional encoding scheme to encode the variable ID, start timestamp, and end timestamp of a shape, with all information normalized to the range of [0, 1] (by dividing them with the number of variables or time series length). This approach, however, exhibits a limitation: While variable information is inherently discrete, normalization transforms it into a continuous value, leading to artificial ordering. For instance, with three variables, the positional encoding scheme normalizes them to 1/3, 2/3, and 1, respectively. As a result, the third variable appears numerically three times greater than the first, which is a misleading representation. To preserve the discreteness of variable information, we implement binary encoding to transform the variables into binary digits $B_i$. We further extend this approach by incorporating class-specific prior information into the encoding process. As a result, we introduce Time Series Information (TSI) Encoding. The structure of TSI Encoding for the i-th token is out-"}, {"title": "4.5 Encoding Layer", "content": "In the encoding layer, the output from TSI Encoding, $I_i$, is passed through a linear transformation. Simultaneously, the input termed Token is processed by a distinct linear projection layer. The outcomes from both processes are then added to form the input $U_i$ for the Transformer Encoder. The process can be represented as:\n(4.8) $U_i = I W_1 + W_s Token$,\nwhere $W_i \\in R^{d_1 \\times d_{model}}$ and $W_s \\in R^{d_{model} \\times d_{Token}}$ are trainable weights, $d_{model}$ denotes the input dimension of"}, {"title": "4.6 Prior-Enhanced Self-Attention", "content": "Here, we introduce a time series classification-oriented self-attention, Prior-Enhanced Self-Attention (PESA) mechanism, which incorporates class-specific prior information into the attention computation to enhance classification-relevant features and attenuate noise.\nAs presented by [25], the conventional self-attention mechanism employs the query matrix Q, key matrix K, and value matrix V. The attention score matrix A is given by:\n(4.9) $A = softmax(\\frac{Q K^T}{\\sqrt{d}})$,\nwhere d is the dimensionality of the queries and keys.\nWe bring in the prior score matrix P, constructed using class-specific prior information $p_i$. Specifically, every element $P_{i,j}$ of matrix P is characterized as:\n(4.10) $P_{i,j} = \\begin{cases} p_i p_j & \\text{if } i \\neq j \\\\ p_i & \\text{if } i = j. \\end{cases}$\nThis matrix P effectively captures the interactions among different input tokens, emphasizing those that are highly informative for classification within the attention mechanism.\nThe final PESA is deduced by taking an element-wise product (represented by \u2299) of the attention score matrix A and the prior score matrix P, followed by a softmax operation, and finally multiplying the resulting matrix with the value matrix V:\n(4.11) $PESA(Q, K, V, P) = softmax(A \\odot P)V$.\nBy integrating class-specific prior information, we infuse the supervised knowledge externally, which optimizes the self-attention learning process, enhancing its performance for time series classification."}, {"title": "4.7 Decision Layer", "content": "The decision layer plays an instrumental role in the fusion of shape and value representations learned from previous network layers. These representations, denoted as $R_{shape}$ and $R_{value}$, are transformed into class probability spaces, G and H, respectively, through two distinct linear layers:\n(4.12) $G = W_{shape} R_{shape}$,\n(4.13) $H = W_{value} R_{value}$,\nwhere $W_{shape}$ and $W_{value}$ are the learnable weights for the shape and value representations, correspondingly."}, {"title": "5 Experiments", "content": "5.1 Datasets We evaluate our method on all 30 datasets from the well-known UEA MTSC archive [5]. We also conduct a case study using the extensive Space Weather Analytics for Solar Flares (SWAN-SF) dataset [3] in Section 5.7.\n5.2 Comparison Methods We compare our proposed approach with three benchmark methods [5] in MTSC: (1) EDI, the 1-nearest neighbor with Euclidean Distance, (2) DTWI, Dimension-Independent Dynamic Time Warping, and (3) DTWD, Dimension-Dependent Dynamic Time Warping. We also compare with ten SOTA methods: (1) WEASEL+MUSE [24], the state-of-the-art bag-of-patterns model which extracts features into word representations, (2) SRL [14], a representation learning approach using negative sampling with an encoder network structure, (3) MLSTM-FCNs [17], a deep learning framework combining an LSTM layer and the FCN layer with Squeeze-and-Excitation mechanism, (4) TapNet [29], an attentional prototypical network for semi- and fully-supervised learning in MTSC, (5) ShapeNet [19], a network which projects subsequences into a unified space through embedding learning and uses clustering to find discriminative patterns, (6) ROCKET [9], a method using random convolutional kernels to transform time series and using the transformed features to train classifier, (7) MiniRocket [10], a streamlined version of ROCKET,"}]}