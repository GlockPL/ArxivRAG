{"title": "DREAMWEAVER: LEARNING COMPOSITIONAL WORLD REPRESENTATIONS FROM PIXELS", "authors": ["Junyeob Baek", "Yi-Fu Wu", "Gautam Singh", "Sungjin Ahn"], "abstract": "Humans have an innate ability to decompose their perceptions of the world into objects and their attributes, such as colors, shapes, and movement patterns. This cognitive process enables us to imagine novel futures by recombining familiar concepts. However, replicating this ability in artificial intelligence systems has proven challenging, particularly when it comes to modeling videos into compositional concepts and generating unseen, recomposed futures without relying on auxiliary data, such as text, masks, or bounding boxes. In this paper, we propose Dreamweaver, a neural architecture designed to discover hierarchical and compositional representations from raw videos and generate compositional future simulations. Our approach leverages a novel Recurrent Block-Slot Unit (RBSU) to decompose videos into their constituent objects and attributes. In addition, Dreamweaver uses a multi-future-frame prediction objective to capture disentangled representations for dynamic concepts more effectively as well as static concepts. In experiments, we demonstrate our model outperforms current state-of-the-art baselines for world modeling when evaluated under the DCI framework across multiple datasets. Furthermore, we show how the modularized concept representations of our model enable compositional imagination, allowing the generation of novel videos by recombining attributes from different objects.", "sections": [{"title": "1 INTRODUCTION", "content": "The primary function of the brain is believed to be the construction of an internal model of the world from sensory inputs like vision\u2014a concept often referred to as world models (Ha & Schmid-huber, 2018). This construction involves developing two fundamental processes: knowing and thinking (Summerfield, 2022; Lake et al., 2017; Fodor et al., 1975). The knowing function entails how to compositionally structure and encode knowledge from experiences through representation learning. The thinking function enables the utilization of this encoded knowledge for abilities such as reasoning, planning, imagining, and causal inference (Goyal & Bengio, 2022; Sch\u00f6lkopf et al., 2021a). Due to its generative and inferential nature, the thinking function necessitates some form of generative learning (Parr & Friston, 2018; Kurth-Nelson et al., 2023; Schwartenbeck et al., 2023).\nA key aspect distinguishing the world models in humans and AI currently is compositionality (Smolen-sky et al., 2022; Sch\u00f6lkopf et al., 2021c; Lake et al., 2017; Goyal & Bengio, 2022; Greff et al., 2020; Behrens et al., 2018), the capability to understand or construct complex concepts as a composition of simpler concepts. Recent studies in neuroscience suggest that humans can understand and adapt to various novel situations because the brain supports compositional representation and genera-tion (Kurth-Nelson et al., 2023; Schwartenbeck et al., 2023; Behrens et al., 2018; Bakermans et al., 2023). However, current world models in AI are limited in this ability for the following reasons.\nMost of compositionality in AI is currently approached via language-conditioned image/video generation models such as DALL\u00b7E (Ramesh et al., 2021; 2022) and Sora (Cho et al., 2024; Milli\u00e8re). These models do offer a degree of compositionality (e.g., generating an image of an avocado chair) through language as a medium, which is convenient for human interaction. However, achieving compositionality more broadly, without relying on language, remains a challenge. This is crucial because there is a vast amount of world knowledge that is difficult to accurately articulate through"}, {"title": "2 RECURRENT BLOCK-SLOT UNITS", "content": "A Recurrent Block-Slot Unit or RBSU is a general-purpose recurrent unit for sequence modeling that we propose in this work. Given an input sequence $E_1,..., E_T$, where each item in the sequence $E_t \\in \\mathbb{R}^{L \\times D_{input}}$ is a collection of L vectors, RBSU works by processing the sequence recurrently. It starts with an initial state representation $S_0$, and for each item $E_t$ in the input sequence, applies the RBSU to update the state from $S_{t-1}$ to $S_t$:\n$S_0 \\leftarrow \\text{Initialize()}$ \n$S_t \\leftarrow \\text{RBSU}(E_t, S_{t-1})$.\nImportantly, RBSU maintains a disentangled state representation $S_t$ called the block-slot representa-tion or simply block-slots. The block-slot representation is a collection of N vectors called slots i.e., $S_t \\in \\mathbb{R}^{N \\times MD}$. We denote each n-th slot in $S_t$ as $s_{t,n} \\in \\mathbb{R}^{MD}$. Furthermore, each slot is internally disentangled and constructed by concatenating M vectors of size D called blocks. We denote the m-th block within a slot $s_{t,n}$ as $s_{t,n,m} \\in \\mathbb{R}^{D}$. For modeling multi-object video inputs, a slot $s_{t,n}$ can represent an object while a block $s_{t,n,m}$ within the slot can represent an intra-object reusable concept e.g., color, shape, or direction of motion."}, {"title": "2.1 BOTTOM-UP ATTENTION", "content": "In the first step in an RBSU, the N slots of the previous time-step $S_{t-1}$ act as queries and attend over the current L'input features $E_t$ to obtain N bottom-up readout vectors $U_t \\in \\mathbb{R}^{N \\times MD}$. Following Singh et al. (2023); Locatello et al. (2020), this is performed via inverted attention and renormalization (Wu et al., 2023a) as follows:\n$A_t = \\text{softmax} \\left( \\frac{q(S_{t-1}) k(E_t)^T}{\\sqrt{MD}} \\right)  \\rightarrow A_{t,n,l} = \\frac{A_{t,n,l}}{\\sum_{l'=1}^L A_{t,n,l'}}   \\rightarrow U_t = A_t v(E_t)$"}, {"title": "2.2 INDEPENDENT BLOCK ATTRACTOR DYNAMICS", "content": "The next step in an RBSU is to update each block in the current block-slot state representation via per-block independent recurrent modules. Importantly, we incorporate attractor dynamics within these recurrent modules to facilitate the convergence of each block's state to a reusable representation. We perform the following per-block operations:\nGRU and MLP. First, each previous block state $S_{t-1,n,m}$ is independently updated using its corre-sponding readout chunk $u_{t,n,m}$ by applying a GRU to incorporate the bottom-up information. The resulting block is then fed to an MLP with a residual connection as follows:\n$\\tilde{S}_{t,n,m} = \\text{GRU}_{\\Theta_m} (S_{t-1,n,m}, U_{t,n,m}) \\rightarrow S_{t,n,m} = \\tilde{S}_{t,n,m} + \\text{MLP}_{\\Theta_m} (LN(\\tilde{S}_{t,n,m}))$.\nWe maintain separate GRU and MLP weights for each m to encourage modularity between blocks. Here, the $S_{t,n,m}$ and $\\tilde{S}_{t,n,m}$ denote the intermediate block states after applying GRU and the residual MLP, respectively.\nBlock-Slot Attractor Dynamics. Since GRU and MLP alone are found insufficient for making a block's state reach a reusable attractor state (Singh et al., 2023), we explicitly incorporate a learned memory of prototypes and make each block perform dot-product attention over this learned memory to retrieve a state as follows:\n$S_{t,n,m} = [\\text{softmax} \\left( \\frac{\\tilde{S}_{t,n,m} C_m^T}{\\sqrt{d}} \\right) ] C_m$.\nTo maintain modularized representations from GRU and MLP, we utilize a separate library of prototypes $C_m \\in \\mathbb{R}^{\\text{Nprototypes} \\times D}$ for each m. Notably, each library is initialized with Nprototypes learnable vectors, which are subsequently learned through backpropagation to capture emergent semantic concepts."}, {"title": "2.3 BLOCK INTERACTION", "content": "Finally, although we maintain independent information processing pathways, for improved flexibility, it is desirable to let the blocks in $S_t$ interact: $S_t = \\text{BlockInteraction}(\\hat{S}_t)$. To implement this interac-tion step, we first flatten the slots into a collection of NM block vectors, feed them to a single-layer transformer, and then reshape the output back to the original shape $S_t \\in \\mathbb{R}^{N \\times MD}$."}, {"title": "3 DREAMWEAVER: COMPOSITIONAL WORLD MODEL VIA PREDICTIVE\nIMAGINATION", "content": "In this section, we propose and describe a novel compositional world model called Dreamweaver using the proposed RBSU. Broadly, Dreamweaver takes a video $x_1,..., x_T$ as input, constructs the world state in terms of composable tokens, and predicts K future video frames $X_{T+1},..., X_{T+K}$:\n$X_{T+1},..., X_{T+K} = \\text{Dreamweaver}(x_1,...,x_T)$,\nwhere T is a temporal context window size and the frames belong to $\\mathbb{R}^{C \\times H \\times W}$. Dreamweaver is implemented as an encoder-decoder architecture:\n$S_T = f_\\theta(x_1,...,x_T) \\rightarrow X_{T+k} = g_\\theta(S_T, k)$\nwhere the encoder $f_\\theta$ encodes the video $x_1,..., x_T$ into slots $S_T$ leveraging RBSU. Given the slots $S_T$ and a step indicator k, where k is the relative temporal index of the target frame, the decoder $g_\\theta$ predicts the target frame $X_{T+k}$ via an autoregressive image transformer (Singh et al., 2022a;b; 2023)."}, {"title": "3.1 ENCODING VIA SPATIOTEMPORAL CONCEPT BINDING", "content": "In the encoder of Dreamweaver, each input frame $x_t$ is processed by a CNN to output a feature map. Next, we add positional embeddings to this feature map, flatten it, and feed it to an MLP to obtain"}, {"title": "3.2 DECODING VIA AUTOREGRESSIVE IMAGE TRANSFORMER", "content": "The extracted slots $S_T$ are decoded leveraging an autoregressive image transformer to predict the future frames $X_{T+1},..., X_{T+K}$. Similar to the powerful architectures used in (Singh et al., 2022a;b; 2023), our transformer decoder does not directly predict the target images in pixel space but instead predicts their discrete token representation. We employ a Discrete VAE (dVAE) (Ramesh et al., 2021; Singh et al., 2022a) to transform an image into a sequence of L' integer tokens $Z_{T+k,1},\\cdots,Z_{T+k,L'} \\in V$ and vice-versa, where V denotes a vocabulary.\nTo train the transformer, we adopt the standard practice in language modeling (Vaswani et al., 2017; Singh et al., 2022a) and perform parallelized training of the autoregressive transformer via causal masking. That is, we first map each token $z_{T+k,l} \\in V$ to an embedding $e_{T+k,l} \\in \\mathbb{R}^D$ by retrieving the token's embedding from a learned dictionary and adding a positional encoding as follows:\n$e_{T+k,l} = \\text{Dictionary}_{\\theta}(Z_{T+k,l}) + p_{\\theta,l}$. Next, we provide the embeddings $e_{T+k,1},..., e_{T+k,L'-1}$ to the transformer decoder that utilizes a beginning-of-sequence (BOS) token and causal masking to generate the next-token log probabilities for each input token:\n$\\mathcal{O}_{T\\rightarrow T+k,1},..., \\mathcal{O}_{T\\rightarrow T+k,L'} = \\text{Transformer}_\\theta(e_{\\text{BOS}_{0,k}}, e_{T+k,1},...,e_{T+k,L'-1}; \\text{cond} = S_T)$,\nHere, the notation $\\mathcal{O}_{source \\rightarrow ttarget,l} \\in \\mathbb{R}^{|V|}$ denotes the predicted log probabilies over $V$ for the l-th token of the image at time-step $ttarget$ given the slots inferred at time-step tsource. Additionally, the BOS token serves a dual purpose: it provides the initial input to the transformer and acts as a step indicator, representing the relative temporal index k. We maintain a dedicated BOS token for each k, denoted as $e_{BOS_{0,k}}$. The detailed method for the conditional prediction of the k-th frame is described in Appendix B.2."}, {"title": "3.3 TRAINING OBJECTIVE", "content": "The complete model is trained by minimizing a cross-entropy loss averaged over all values of k = 1, ..., K, and all token indices l = 1, . . ., L':\n$\\mathcal{L}_{Dreamweaver}(\\theta, \\phi) = \\frac{1}{KL'} \\sum_{k=1}^K \\sum_{l=1}^{L'} \\text{CrossEntropy}(z_{T+k,l}, \\mathcal{O}_{T\\rightarrow T+k,l})$\nIn practice, we train the dVAE jointly with the Dreamweaver by using a combined loss $\\mathcal{L}(\\theta, \\phi) = \\mathcal{L}_{Dreamweaver}(\\theta, \\phi) + \\mathcal{L}_{dvae}(\\theta, \\phi)$. For more details about the dVAE, see Appendix section B.1.\nNeed for a Predictive Objective. The purpose of introducing a predictive objective in contrast to a simple reconstruction objective commonly used in prior works (Kipf et al., 2021; Singh et al., 2022b; Elsayed et al., 2022) is two-fold: i) The predictive objective incentivizes RBSU to capture not just static factor primitives, such as color or shape, but also dynamical primitives, such as direction or speed, since the latter is necessary when performing future prediction but much less important when the goal is to merely reconstruct the present frame. ii) The ability to generate future frames naturally provides a world model that can be utilized to roll out long future trajectories by autoregressively feeding each generated frame back into the model."}, {"title": "4 RELATED WORKS", "content": "Learning Object-centric Compositional Representations. Our work is influenced by recent research in unsupervised object-centric representation learning (Locatello et al., 2020; Engelcke et al., 2020; Burgess et al., 2019; Greff et al., 2017; 2019; 2020; Van Steenkiste et al., 2018; Zoran et al.,"}, {"title": "5 EXPERIMENTS", "content": "We evaluate the following questions: (1) How effectively can Dreamweaver infer modular con-cepts-both static and dynamic-from RGB videos without any supervision? (2) Can Dreamweaver generate new videos by composing novel configurations of the inferred concepts? (3) How well does Dreamweaver's representation support out-of-distribution (OOD) generalization on downstream reasoning tasks? Finally, we conduct an ablation study to evaluate various design choices.\nDatasets. We experiment on five datasets spanning two axes of complexity: Dynamical Complexity and Visual Complexity. Along the axis of dynamical complexity, our datasets Moving-Sprites, Moving-CLEVR, and Moving-CLEVRTex exhibit low dynamical complexity i.e., objects move uniformly in a specific direction throughout the video (e.g., up, down, left, right). On the other hand, our datasets Dancing-Sprites and Dancing-CLEVR exhibit high dynamical complexity, featuring multi-step \"dance\" patterns (e.g., a clockwise square dance pattern: $right \\rightarrow down \\rightarrow left \\rightarrow up$, as shown in Figure 14) or multi-step color-change patterns (see Figure 15). Along the axis of visual complexity, we have 3 levels: (i) The simplest are Moving-Sprites and Dancing-Sprites with 2D sprites on a black canvas. (ii) Moving-CLEVR and Dancing-CLEVR significantly increase visual complexity, with 3D scenes featuring solid-colored 3D rubber objects, realistic lighting, and shading. (iii) In line with previous work (Singh et al., 2023; Wu et al., 2024), the highest level of visual complexity is in Moving-CLEVRTex, where objects have complex textures on them.\nBaselines. We compare our model against three unsupervised representation learning baselines: RSSM (Hafner et al., 2018), STEVE (Singh et al., 2022b), and SysBinder (Singh et al., 2023). RSSM is a widely used world modeling framework that represents a video frame via single-vector representation. STEVE is a state-of-the-art method that represents each video frame as a set of per-object latent vectors (called slots). STEVE uses Slot Attention (Locatello et al., 2020) to represent scenes in a structured manner by spatially binding objects into slot representations. Like STEVE, SysBinder also provides a slot representation of each video frame; however, it further disentangles a slot as a concatenation of several blocks-each block representing one factor e.g., color, shape, etc. Since SysBinder was originally designed for image data, it can only be applied independently per video frame and is not capable of utilizing the temporal context. Therefore, for a fair comparison with our model, we modify SysBinder for videos to obtain a stronger baseline as follows. We equip SysBinder with a recurrent encoder (Kipf et al., 2021) to handle multiple frames. We refer to this modified model as SysBinder for simplicity. For all baselines, we use the same length of conditioning frames as our model. The baselines are trained with a reconstruction objective."}, {"title": "5.1 UNSUPERVISED MODULAR CONCEPT DISCOVERY FROM VIDEOS", "content": "Metrics. For quantitative evaluation of learned representations, we use the DCI (Eastwood & Williams, 2018) framework to evaluate Disentanglement (D), Completeness (C), and Informativeness (I). Note that the DCI is computed using ground truth object factors that not only include the static factors e.g., color or shape, but also the dynamical factors e.g., direction of motion, \"dance\" pattern, etc. Additionally, we also compute a metric called Informativeness-Dynamic (I-D), which is the informativeness score evaluated only on the dynamic factors.\nDCI Performance. In Figure 3, we compare the DCI performance of our model with the baselines. We note that our model consistently surpasses the other baselines across all datasets, achieving the highest scores in Disentanglement (D), Completeness (C), and Informativeness (I).\nEmergence of Dynamical Factor Representation. Importantly, we also note in Figure 3 that our model strongly surpasses all other baselines in terms of the I-D score which captures how informative our representation is about the dynamic factors. Our model achieves scores more than twice those of the other baselines. This strong performance points to the effectiveness of the predictive objective which incentivizes our model to capture dynamical information. As such, to the best of our knowledge, ours is the first unsupervised representation learning model capable of representing both static and dynamic concepts while simultaneously providing disentanglement.\nVisualizing the Feature Space of Blocks. We visualize the semantics of each block's feature space in Appendix D.1. We can see that our model allocates specific regions of the feature space of specific blocks to capture specific factor values e.g., the star shape, the yellow color, etc. We also see qualitatively that our model captures the dynamical factor values such as the up & down dance and the horizontal sliding movement."}, {"title": "5.2 COMPOSITIONAL IMAGINATION", "content": "In this section, we demonstrate how our block-slot representation can be recomposed into novel configurations and be used to generate compositionally novel videos.\nSetup. We feed the initial frames of a video (called the context frames) into our pre-trained RBSU encoder. We take the block-slot representation from the last context frame and manipulate it in one of the following two ways: (1) We can perform a factor swap by taking two slots corresponding to distinct objects, selecting the blocks that correspond to a specific factor (e.g., color), and swapping them. (2) We can perform a factor change by taking the block-slot representation from a source video, selecting the block that captures a certain factor e.g., a block that represents a certain \"dance\" movement and using it to replace a block in the block-slot representation of the video that we seek to manipulate. The manipulated block-slot representation is fed to a pre-trained Dreamweaver decoder"}, {"title": "5.3 COMPOSITIONAL SCENE PREDICTION AND REASONING", "content": "To evaluate the quality of the learned representations, we construct a downstream task that requires predicting the future states of the objects in the scene and reasoning among the factors of the objects.\nTask. The task is to take the latent representation of the last frame of a given context video and predict a target value corresponding to a range of future frames at various offsets e.g., 0, . . ., 5. The target value corresponding to a frame is defined as follows: (1) We first assign an integer number to each possible value of each ground truth factor following Wu et al. (2024). (2) We then take the maximum value of each ground truth factor across objects and sum these maximum values to obtain the target value. To solve this task well, the learner must, either explicitly or implicitly, predict the future factor values of each object and compare these values to determine the maximum value.\nProbe. We train a downstream probe that takes as input the latent representation of the last context frame from each pre-trained model and predicts the target value for a range of frame offsets.\nSetup. We evaluate the probing performance on the Dancing-Sprites and Dancing-CLEVR datasets. For Dancing-Sprites, we use the shape, color, and position of each object as the ground truth factors"}, {"title": "5.4 ABLATION STUDY", "content": "We conduct a series of ablations on the different architectural components of Dreamweaver as well as several of the key hyperparameters."}, {"title": "6 LIMITATIONS & CONCLUSION", "content": "In this paper, we introduced Dreamweaver, a novel neural architecture for unsupervised learning of compositional world models from videos. The key component of Dreamweaver is the Recurrent Block-Slot Unit, which encodes a temporal context window of past images and maintains a set of independently updated block-slot states, enabling the emergence of abstraction for both static and dynamic concepts. Our model outperforms state-of-the-art object-centric methods and demonstrates the ability to generate videos through novel compositional imagination. This is the first time such compositional imagination has been shown in object-centric learning from videos.\nWhile Dreamweaver represents a significant advancement in the unsupervised learning of composi-tional world models from visual data, it does have some limitations that future work may address. First, extending Dreamweaver to a probabilistic model could improve its ability to manage uncertainty and generate more diverse and realistic videos. Second, although Dreamweaver has demonstrated the emergence of static and dynamic concepts, exploring the emergence of other abstract concepts, such as numbers, could be fascinating. Lastly, our model shares the general limitation of current state-of-the-art object-centric learning, which is not yet applicable to very complex scene images. Addressing these areas could lead to improved world models."}, {"title": "A ADDITIONAL RELATED WORKS", "content": "Learning Compositional Mechanisms. Unlike our model is built on slot-based models, there is another line of work for learning compositional representations motivated by the Independent Causal Mechanisms principle and the Sparse Mechanism Shift hypothesis (Sch\u00f6lkopf et al., 2012; 2021b). RIMs architecture family (Goyal et al., 2021b; 2020; Madan et al., 2021; Assouel et al., 2022) decomposes state space representations into separate recurrent components that operate independently, with sparse interactions among them. These models are designed to learn a set of reusable mechanisms that are selectively activated through sparse communication, enabling them to capture recurring concepts across frames. Similarly, our model also learns reusable and modularized representations, however, our representations are always used and more structured by introducing a hierarchical structure, where these blocks are encapsulated within individual slot representations. Lastly, NPS (Goyal et al., 2021a) model, as a recent work of RIMs, learns a set of independent mechanisms capturing the interaction between objects, while ours is still limited in this ability.\nCompositional World Models. Previous work has studied world models from the perspective compositional generalization (Zhao et al., 2022; Sehgal et al., 2024; Zhou et al., 2024). In particular, Cosmos (Sehgal et al., 2024) is a framework for object-centric world modeling that leverages a frozen pretrained vision-language foundation model to decompose objects into symbolic attributes. Du et al. (2023); Zhou et al. (2024); Yu et al. (2022); Cho et al. (2024) similarly relies on language to enable compositional generalization. Our model, on the other hand, is able to learn a compositional world model from pixels without the use language input or pretrained foundation models. COMET (Lei et al., 2024) is also a related model that learns disentangled modes of interaction between objects, but does not learn attribute-level representations of the objects."}, {"title": "B ADDITIONAL MODEL DETAILS", "content": "B.1 IMAGE TOKENIZATION VIA DISCRETE VAE\nSince we leverage an autoregressive image transformer to predict the frames $X_{t+1},..., X_{t+K}$, we train a discrete Variational Autoencoder (dVAE) to generate discrete token representations of these frames and act as prediction targets. The dVAE consists of an encoder $f_{dVAE}$ and a decoder $g_{dVAE}$. The dVAE encoder takes an image as input and outputs a sequence of patch-level tokens or latent codes $z_{t,1},..., z_{t,L'}$ where $z_{t,l} \\in \\mathbb{I}$, while the dVAE decoder takes a sequence of tokens as input and decodes it to an image. The dVAE encoder and decoder are trained with an autoencoding objective as follows:\n$z_{t,1},...,z_{t,L'} = f_{dVAE}(x_t)$\n$\\hat{x}_t = g_{dVAE} (z_{t,1},..., z_{t,L'})$\n$\\mathcal{L}_{dVae}(\\theta, \\phi) = ||X_t - \\hat{x}_t||_2$,\nwhere we use Gumbel-Softmax relaxation for the discrete latent codes to facilitate training and a simple squared error as the reconstruction loss."}, {"title": "B.2 CONDITIONAL PREDICTION WITH STEP INDICATORS", "content": "We train our model to predict multiple future frames $x_{t+1},...,x_{t+K}$, which encourages the encoder $f_\\theta$ to extract consistent dynamic features by preventing the model from overfocusing on image generation. However, predicting all frames simultaneously is inefficient. Instead, we improve efficiency by randomly sampling one frame $x_{t+k}$ to predict during each training step. This is achieved through conditional prediction using a single transformer decoder $g_\\theta$ and a step indicator $i_k$.\nBOS token as a step indicator. In Dreamweaver, the Beginning-of-Sequence (BOS) token in the transformer decoder functions as a step indicator k. Specifically, we initialize a distinct $BOS_{\\theta,k}$ token for each step k and input the corresponding $BOS_{\\theta,k}$ token, based on the index k sampled during each training step, as the first token for autoregressive generation.\nSelf-modulation with step indicators. In more complex and realistic datasets, relying solely on the BOS token limits conditional prediction performance. To address this, we employ the Self-modulation technique (Karras et al., 2019; Lee et al., 2021). Specifically, we enhance the transformer decoder $g_\\theta$ by replacing every layer normalization with self-modulated layer normalization (SLN), computed as"}, {"title": "C ADDITIONAL IMPLEMENTATION DETAILS", "content": "C.1 TRAINING AND IMPLEMENTATION DETAILS\nWe utilized images with a 64x64 resolution for all datasets except Moving-CLEVRTex, which uses a 128x128 resolution. Each model was trained on NVIDIA GeForce RTX 4090 GPUs with 24GB of memory. The Dreamweaver model underwent 400,000 iterations of training, taking 20 hours for the Moving-Sprites and Moving-Sprites-OOD datasets, 30 hours for the Dancing-Sprites, Moving-CLEVR, and Dancing-CLEVR datasets, and 48 hours for Moving-CLEVRTex. Additionally, we employed the self-modulation technique for conditional prediction exclusively in the Moving-CLEVRTex dataset.\nC.2 HYPERPARAMETERS\nTable 1 details the hyperparameters employed for the various datasets in our Dreamweaver exper-iments. We trained all models using the Adam optimizer (Kingma & Ba, 2014) with \u03b21 set to 0.9 and \u03b22 set to 0.999. Furthermore, we utilized the architecture and hyperparameters of the backbone image encoder as specified in Singh et al. (2023)."}, {"title": "C.3 BASELINES", "content": "In this paper, we train three baseline models: STEVE, SysBinder, and RSSM. For STEVE, we use the original architecture and hyperparameters, modifying only the slot size to 64 dimensions. For SysBinder, we adapt the architecture to handle video data with a recurrent structure and align the hyperparameters with those of our model for a more accurate comparison, including block size, the number of prototypes, and the number of blocks. For RSSM, we implement the world model from DreamerV1 (Hafner et al., 2019), using 32 dimensions for both deterministic and stochastic latents, while adhering to the other recommended hyperparameters.\nFor implementation, we utilize the following open-source resources:"}, {"title": "C.4 DOWNSTREAM MODEL ARCHITECTURE", "content": "For the downstream experiments on all models except for RSSM, we use a transformer architecture with 4 layers, 4 heads, 0.1 dropout, and model dimension of 128. We use the output of a learned class token to predict the target label.\nFor RSSM, we use an MLP to make the prediction. While we experimented with matching the number of parameters with the transformer model, we found better performance using a smaller network with 4 layers and hidden dimension of 128 and ReLU activation.\nDuring training, we freeze the up-stream models and train with a learning rate of 3e-4."}, {"title": "C.5 IDENTIFYING BLOCK-FACTOR CORRESPONDENCE.", "content": "To swap or change a factor, it is important to identify which block index corresponds to which object factor. For this, we adopt the following procedure. (1) Take a large batch of videos with per-object factor labels given. (2) Extract block-slot representation from the videos using a pre-trained RBSU encoder. (3) Match slots with ground truth labels via Hungarian matching using mask overlap. (4) Train a probe to predict ground truth factor labels from the block-slot representation. Use probing methods that provide feature importance (e.g., LASSO or Decision Trees). (5) Manually inspect the feature importances to identify which block(s) represent a specific ground truth factor."}, {"title": "D ADDITIONAL EXPERIMENT RESULTS", "content": "D.1 VISUALIZATION OF CAPTURED CONCEPTS\nFigure 7 and 8 illustrates the concepts represented by each block within learned block-slot represen-tations. To achieve this, we gather block representations with the same index and apply clustering methods, such as k-means, following the approach in Singh et al. (2023).\nFor instance, in the Moving-Sprites dataset, block 0 captures shape concepts such as Star and Square, block 2 captures color concepts such as Yellow and Red, and block 1 captures dynamic concepts such as Lift Up & Down and Slide Leftside & Rightside motion. Similarly, in the Dancing-CLEVR dataset, block 7 captures shape concepts, block 0 captures color concepts, and block 4 captures dynamic concepts."}, {"title": "D.2 OUT-OF-DISTRIBUTION COMPOSITIONAL IMAGINATION", "content": "In Section 5.2, we demonstrate that our model is capable of synthesizing novel videos by recombining block-slot representations. In this section, we further investigate whether Dreamweaver can generate videos featuring objects with compositionally out-of-distribution (OOD) properties. To this end, we introduce the Moving-Sprites-OOD dataset, a modified version of the Moving-Sprites dataset. In this version, the model is trained with only 90% of the possible (shape, color, moving direction)"}]}