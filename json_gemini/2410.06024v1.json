{"title": "JET EXPANSIONS OF RESIDUAL COMPUTATION", "authors": ["Yihong Chen", "Xiangxiang Xu", "Yao Lu", "Pontus Stenetorp", "Luca Franceschi"], "abstract": "We introduce a framework for expanding residual computational graphs using jets, operators that generalize truncated Taylor series. Our method provides a systematic approach to disentangle contributions of different computational paths to model predictions. In contrast to existing techniques such as distillation, probing, or early decoding, our expansions rely solely on the model itself and requires no data, training, or sampling from the model. We demonstrate how our framework grounds and subsumes logit lens, reveals a (super-)exponential path structure in the recursive residual depth and opens up several applications. These include sketching a transformer large language model with n-gram statistics extracted from its computations, and indexing the models' levels of toxicity knowledge. Our approach enables data-free analysis of residual computation for model interpretability, development, and evaluation. The project website can be found here.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning models, particularly large-scale foundation models, have become increasingly prevalent and impactful across a wide range of domains (Wei et al., 2021; Bommasani et al., 2023; Touvron et al., 2023b). While delivering strong results, their black-box nature has led to the development of techniques to assess their behavior and gain insights into their internal mechanisms.\nIn this space, mechanistic interpretability (MI) (see e.g. Bereska & Gavves, 2024; Ferrando et al., 2024, for recent surverys) has emerged as an alternative to more classic local attribution methods such as SHAP (Lundberg, 2017) or integrated gradient (Sundararajan et al., 2017). Contrary to these methods, which seeks to trace output behavior back to the network input, MI focuses on tracing behavior back to the model itself. It seeks to uncover learned \u201calgorithms\u201d that are embedded in the model weights and computational structure, with the aim of developing a global understanding of \u2013 and, ultimately, to reverse engineer \u2013 neural computation.\nThe great majority of MI work uses a hypothesis-and-dataset-driven approach (see for example Goldowsky-Dill et al. (2023)), in that it first formalizes a hypothesis, then chooses or curates a dataset to probe the model, it applies techniques such as path patching (Wang et al., 2022) or causal tracing (Meng et al., 2022), and then possibly refines the initial hypothesis. While this approach to MI is valuable, it can limit the ability to perform open-ended exploration-driven studies aimed at uncovering global behavior and charting \u201cmaps\" that connect computation to behavior. In this regard, studies such as Veit et al. (2016) or Elhage et al. (2021) focus on the intrinsic computation that is carried out by a model, offering complementary views to the hypothesis-and-dataset-driven approach. Yet, these studies often make unrealistic assumptions of the model, making it unclear how much of the derived understanding can be transferred to real-world models and applications.\nThis paper contributes to this latter direction, presenting a general-purpose framework to manipulate the computational graph of a neural model with the aim of identifying individual input-to-output computational paths, which we can then further analyze to extract behavior. Our method is based on the simple observation that we can recursively expand the computation of a network by selectively applying jet operators (Ehresmann, 1951), which one can think of as the functional counterpart of truncated Taylor series. This process, which we call the jet expansion of a model, gives rise to a class of equivalent functional rewritings of the original network into the sum of polynomial terms (which we see as input-to-output functions and dub jet paths) and non-linear remainders."}, {"title": "2 RESIDUAL NETWORKS AND THEIR REWRITINGS", "content": "We start by reviewing the archetypal computational structure of residual networks and discuss the case of linear residual networks as a canonical example of functions that are intrinsically expanded.\nResidual networks. We focus on network architectures whose main body consists of multiple recursive residual blocks, while the input and output are managed respectively by an encoding and a decoding module. Let Z be an input space (e.g., sequences of tokens), $c \\in \\mathbb{N}+$ be the number of classes (e.g., a vocabulary size), $Y = \\mathbb{R}^c$ be a space of output logits and $d \\in \\mathbb{N}+$ be a hidden dimension. Formally, we are concerned with functions $q: \\mathcal{Z} \\rightarrow Y$ described as follows:\n$q = v \\circ h_\\mathrm{L}, \\ \\ h_\\mathrm{L}: \\mathcal{Z} \\rightarrow \\mathbb{R}^d$,  \\ \\ $h_\\mathrm{L} = \\underset{l=1}{\\overset{L}{\\bigcirc}} \\beta_l \\circ \\eta$,\t(1)\nwhere $L \\in \\mathbb{N}+$ is the number of residual blocks (e.g. recursive depth), $\\eta: \\mathcal{Z} \\rightarrow \\mathbb{R}^d$ is an input encoding module (e.g. token embedding layer), $\\bigcirc$ denotes repeated functional composition, and\n$\\beta_l: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ for $l \\in [L]$\n$\\upsilon: \\mathbb{R}^d \\rightarrow Y$ \\qquad\\qquad $\\beta_l = \\mathrm{id} + \\gamma_l$,\t(2)\n$\\gamma_l: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, \\qquad \\qquad$\\upsilon(x) = U \\Upsilon_\\mathrm{L+1}(x)$\t(3)\n$U \\in \\mathbb{R}^{c \\times d}$, are respectively residual blocks with nonlinearities $\\gamma_l$'s (e.g., input-normalized causal self-attentions or MLPs), and the output decoding module (e.g., an unembedding projection U after a layer nor-malization $\\Upsilon_\\mathrm{L+1}$); id is the identity map. We leave all parameters implicit and assume all func-tions are $C^{\\infty}$. Optimized for classification (e.g., next token prediction for autoregressive lan-guage models), the function $q$ outputs unnormalized conditional probabilities (or logits) in that $\\mathbb{P}_q$($\\text{``z belongs to class i''} | z) = \\mathrm{Softmax}[q(z)]_i$, for $z \\in \\mathcal{Z}$. In residual networks, the recursive links allow the \"storage\" of computation from all previous layers and the embedded input, leading to an accumulation of information across depths. This is highlighted by unrolling the computation of Equation (1) up to a block $l \\in [L]$, setting $h_0 = \\eta$:\n$h_l = \\underset{j=1}{\\overset{l}{\\bigcirc}} \\beta_j \\circ \\eta = \\eta + \\sum_{j=1}^l \\gamma_j \\circ h_{j-1}; \\ \\  q = \\upsilon \\circ \\eta + \\sum_{i=1}^L \\upsilon \\circ \\eta \\circ h_{i-1}$\t(4)\nElhage et al. (2021) introduces the term residual stream to describe $h_l$, a concept that can be traced back to Hochreiter & Schmidhuber (1997) and Srivastava et al. (2015). Veit et al. (2016) describe and study the unrolled structure of the final residual stream $h_L$, which reveals a number of paths from the input to the decoder that grows linearly with the network depth.\nLinear residual networks. The presence of non-linearities at each block (and at the decoding module) prevents us from directly expanding the input-to-output computation further. Linear residual networks, represented in Equation (5), do not have this impediment. Indeed, if $\\gamma_l(x) = A_l x$ for"}, {"title": "3 RECURSIVE EXPANSION OF RESIDUAL NETWORKS WITH JETS", "content": "To tackle non-linearities and enable expansions in general residual networks similar to that of Equation (5), we turn to jets (Ehresmann, 1951), which generalize Taylor expansions. In this section, we first introduce key concepts pertaining jets that are instrumental in developing our framework. Then we move to develop jet_expand, the general algorithm for expanding residual nets into atomic input-output computational paths.\nJet operators and their convex combinations We recall that, for a function $f \\in C^{k+1}(\\mathbb{R}^d, \\mathbb{R}^d)$ and $x, y \\in \\mathbb{R}^d$, Taylor's theorem asserts that\n$f(y) = f(x) + \\sum_{j=1}^k (j!)^{-1} D^j f(x) (y - x)^{\\otimes i} + O(||y - x||^{k+1})$\t(6)\nwhere $x, y$ are respectively the center and variate, $D^j$ denotes the j-th differential, $(y -x)^{\\otimes i}$ denotes the j-fold tensor product, and $O(||y \u2013 x||^{k+1})$ denotes the class of functions that vanish at least as fast as a degree-(k + 1) polynomial $M||y - x||^{k+1}$ as $y \\rightarrow x$ for some $M > 0$. The k-th order jet operator of a function $f$ maps vectors to equivalence classes of degree-k polynomial functions (we denote the resulting quotient space by $P_k$ in the equation below, details in the appendix) as follows:\n$J^k f : \\mathbb{R}^d \\rightarrow \\mathcal{P}_k$ \\ \\  $J^k f(x) = f(x) + \\sum_{j=1}^k (j!)^{-1} D^j f(x)$.\t(7)\nEvaluating the jet at a variate $y \\in \\mathbb{R}^d$ yields the truncated Taylor expansion $J^k f(x)(y) \\in \\mathbb{R}^d$, that is, Equation (6) without the \u201cO\u201d term. The main advantage of working with jets rather than Taylor expansions is that we can work directly with functions rather than vectors. We will make extensive use of the following lemma, of which the proof can be found in the appendix, along with further details about jets.\nLemma 1 (Convex combinations of jets). Let $f \\in C^{\\infty}(\\mathbb{R}^d, \\mathbb{R}^d)$, $k \\in \\mathbb{N}$, $N \\in \\mathbb{N}+$, {$x_i$}$_{i \\in [N]}$ be a set of jet centers, $w \\in \\triangle^{N-1} \\subset \\mathbb{R}^N$ be a set of jet weights, and $r = \\mathrm{max}\\_i${$w\\_i||x\\_i - \\sum\\_j x\\_j ||$}. Then\n$J^k f (x) = \\sum_{i=1}^N  w_i J^k f(x_i) + O(r^{k+1})$.\nRemark 1 (Jet centers and variates as functions). We will often want to trace the computation of a jet back to the input space $\\mathcal{Z}$. In such cases, we interpret the jet centers $x$'s and the variates $y$'s as functions of the original network input $z \\in \\mathcal{Z}$ onto $\\mathbb{R}^d$ or $Y$. Thus, we have that $J^k f(x)(y) : \\mathcal{Z} \\rightarrow \\mathbb{R}^d$ (or $Y$) which evaluates as follows: $J^k f(x)(y)(z) = J^k f(x(z))(y(z))$"}, {"title": "Exponential expansion of a two-blocks network.", "content": "Before introducing the main algorithm, we start with a minimal example of an expansion of a network with two residual blocks into four input-to-output paths. The network, represented in Figure 2 (a) and (a-bis), is given by:\n$q = \\upsilon \\circ h; \\ \\  h_2 = \\beta_2 \\circ \\beta_1 \\circ \\eta = \\eta + \\gamma_1 \\circ \\eta + \\gamma_2 \\circ (\\eta + \\gamma_1 \\circ \\eta)$\t(8)\nThe final residual stream $h_2$ is a sum of three terms (input-to-hidden-space functions). In a trans-former network, $\\gamma_1$ could represent a self-attention block and $\\gamma_2$ an MLP block \u2013 typically both transformations being input-normalized. Critically, the last term $\\gamma_2 \\circ (\\eta + \\gamma_1 \\circ \\eta)$ does not allow us to directly single out contributions that involve $\\gamma_2$ and $\\eta$ or $\\gamma_1 \\circ \\eta$ alone. To recover such paths, we can jet-expand $\\beta_2$ and apply Lemma 1 choosing as centers $x_{\\O} = \\eta$ and $x_{\\{1\\}} = \\gamma_1 \\circ \\eta$, obtaining:\n$J^k \\beta_2 (x_{\\O} + x_{\\{1\\}}) = w_1 J^k \\beta_2 (x_{\\O}) + w_2 J^k \\beta_2 (x_{\\{1\\}}) + O(r^{k+1}) =x_{\\O} + x_{\\{1\\}} + w_1 J^k \\gamma_2 (x_{\\O}) + w_2 J^k \\gamma_2 (x_{\\{1\\}}) + O(r^{k+1})$\t(9)\nwhere the last equality holds for $k > 1$. This operation is represented in Figure 2 (b). These terms still do not yield input-to-output paths, as in general $3 \\neq \\mathrm{id}$ (in transformer architecture this is typically a normalization operation, e.g. layer norm). We can again proceed with a jet expansion, this time of the decoding module $\\upsilon = U \\circ \\Upsilon_3$. Continuing with our example, we apply Lemma 1 using as centers the outputs of the previous expansion, namely $x_{\\O}$, $x_{\\{1\\}}$, $x_{\\{2\\}} = w_1 J^k \\gamma_2 (x_{\\O})$ and $x_{\\{1,2\\}} = w_2 J^k \\gamma_2 (x_{\\{1\\}})$, obtaining\n$J^k \\upsilon(x_{\\O} + x_{\\{1\\}} + x_{\\{2\\}} + x_{\\{1,2\\}}) = \\sum_{S \\in 2^{[2]}} w_1 U J^k \\Upsilon_3 (x_S) + O(r^{k+1})$\t(10)\nwhere $w \\in \\triangle^3$ is a vector of jet weights. With this operation, represented by Figure 2 (c), we have obtained four input-to-output paths, mimicking the exponential rewriting of the linear case; cf. Equation (5). For instance, the zeroth order ($k = 0$) path that passes through the second non-linearity only, skipping the first, is given by the function $z \\in \\mathcal{Z} \\rightarrow w_3 U \\Upsilon_3 (w_1 \\Upsilon_2 (\\eta(z))) \\in V$. This example demonstrates the key principles of our approach: recursive expansion of the computational graph using jets, and the use of convex combinations to isolate specific paths. However, for deeper networks with many blocks, manually expanding each layer becomes impractical. To address this, we generalize this process into an algorithmic framework, which we develop next.\nThe jet-expand algorithm. Algorithm 1 presents the key operation of the framework. The algorithm applies Lemma 1 to a residual transformation or to the decoding non-linearity for a given (user-defined) set of centers $C$. It yields a set of expanded polynomial terms $\\mathcal{E}$, which can be seen as a set-valued function $\\xi : \\mathcal{Z} \\times \\triangle^{N-1} \\rightarrow \\mathcal{E}$, where $\\mathcal{E}$ is an appropriate power set of functions, and a non-linear remainder $\\delta : \\mathcal{Z} \\times \\triangle^{N-1} \\rightarrow \\mathbb{R}^d$. The remainder encompasses both the residu-als stemming from Equation (6) and Lemma 1. As we showed above, centers can be the outputs of previous expansions, enabling the propagation of the expansion through the entire network and effectively 'unrolling' the computation graph into distinct paths. Importantly, once we apply the algorithm for $l = L$ we obtain a way to rewrite the computational graph of $q$ as a sum of ex-panded terms (input-to-output paths), which we call expansion, and a non-linear remainder. Indeed, if $(\\mathcal{E}_L, \\delta_L) = \\mathrm{jet}\\_\\mathrm{expand}(q, l, C, k)$ for some $C$ and $k$, the following class of functional equiva-lences holds:\n$q = \\sum_{e \\in \\mathcal{E}_L} U e(\\cdot, w) + \\delta\\_L(\\cdot, w)$ \\  for w \\in \\triangle^N-1.\t(11)"}, {"title": "4 NOTABLE EXPANSIONS AND THEIR IMPLICATIONS", "content": "We introduce some particular expansions as application of the introduced jet_expand algorithm, setting the stage for the numerical case studies of the next section.\n(Super)exponential expansion. Algorithm 2 generalizes the exponential expansion we performed onto the two-blocks network in Section 3, using uniform jet weights. One can interpret the algorithm as performing a \u201cmaximal\u201d expansion (when remaining at the grain of the blocks) which yields $2^L$ input-to-output paths. In fact, for $k \\geq 1$, we can further isolate each degree of the expanded terms into separate input-to-output paths that highlight interactions among various blocks. This further refinement, which we will focus on in future work, may suggests that residual networks may in fact behave as super-exponential ensembles of (shallower) functions.\nJet lenses and logit lens. The logit lens (nostalgebraist, 2021b; Geva et al., 2021; 2022; Merullo et al., 2023; Belrose et al., 2023) is an interpretability method that consists in applying the decoder to intermediate representations as follows:\n$\\mathrm{LogitLens}\\_l(z) = U \\Upsilon(\\gamma(h\\_l(z))) = J^0 \\upsilon(h\\_l(z))(h\\_l(z))$.\nThe logit lens, aimed at highglighting the iterative refinement of the prediction across blocks, is related to early exiting (or early decoding) in the context of conditional computation (see e.g. Panda"}, {"title": "5 INTERPRETING LLMS WITH JET EXPANSIONS", "content": "Our framework provides users with freedom in terms of choosing the computational paths they wish to focus on. Jet expansions support studies across various levels, including model-level global analysis (jet n-grams), component-level analysis (jet paths), and example-level analysis (jet lens). We experiment with several popular open-sourced large language models families: GPT-2 (Radford et al., 2019), GPT-Neo Black et al. (2021), Llama (Touvron et al., 2023a;b; Rozi\u00e8re et al., 2024) and OLMO (Groeneveld et al., 2024), showcasing the generality of the algorithm. Our main experiments run on 128 CPU servers with 1 TB memory, while jet lens experiment run on a single laptop."}, {"title": "5.1 ANALYZING LLM INNER WORKING", "content": "LLMs are notorious for their lack of interpretability due to their inherent model complexity and size, made worse by the usual opaque training process and unknown training data. Understanding their inner working contributes to calibrating trust for users to use them appropriately. We showcase how jet expansion along user-selected computational paths (jet paths) can help us discover and locate learned associations akin to studies in mechanistic interpretability Templeton et al. (2024).\nJet lenses. We use jet lenses introduced in Section 4 to analyze LLM's mechanism when process-ing individual examples. We observe that the joint jet lens captures the synergy among different blocks, as the model prediction is decomposed into several jet paths. Our preliminary analysis supports recent work on super-position (Elhage et al., 2022) and neuron pol-ysemy (Bricken et al., 2023), suggesting that interactions among components may have ensemble effects, which can broadly vary across model families. In this sense, the jet lenses with $k > 0$ may serve as tools to systematically discover such synergic behaviors. We also find that higher-orders (k > 0) help iterative lenses deliver more meaningful interpretations than the logit lens (k = 0) for GPT-Neo-2.7B"}, {"title": "5.2 ANALYZING PRETRAINING DYNAMICS", "content": "Pretraining an LLM is usually extremely resource intensive. Therefore it is crucial to monitor the progress of a pretraining run to prevent wasting of time and compute. In this section, we show how jet bi-grams can serve as an effective signaling tool to trace the pretraining dynamics, providing insights about the model's maturity. Such signals are especially useful to understand what happens with the model when the pretraining loss shows marginal improvements and fails to reflect the changes inside the model.\nIdentifying the top bi-grams. To assess the model's progression, we extracted jet bi-grams from OLMO-7B model checkpoints across 555K pretraining steps. Initially, the network exhibits nonsensical jet bi-grams, such as \u201cICUirling\u201d. As training advances, it gradually learns more meaningful combinations, like \u201cat least\u201d. This process of acquiring sensible bi-grams stabilizes around step 200K, indicating that the model is reaching a level of maturity where the top 10 bi-grams capture common meaning.\nAnalyzing bi-grams learning speed. To evaluate the learning speed of these jet bi-grams, we chart the hit ratios of these ground-truth bi-grams at each pretraining step. Interestingly, even though the pretraining loss (the blue curve) shows only minor improvements after the initial 50K steps, the model's acquisition of effective bi-grams continues to progress in a steady, consistent manner. This observation aligns with known phenomena in neural network training, such as double-descent and grokking, which highlight the model's ability to improve generalization capabilities even when the loss appears to stagnate (Zhang et al., 2021; Power et al., 2022).\nLearning schemes for different bi-grams. To understand if there are any differences between the learning schemes of different bi-grams, we can trace the progression of the jet bi-gram scores for selected bi-grams. The different slopes and levels of the lines indicate varying rates of learning for the respective bi-grams. We observe that, the model first acquires ran-dom bi-grams due to random parameter initialization. These random bi-grams, like \u201cICUirling\u201d and \"VENT thanks\", are quickly suppressed in the early steps and never regain high scores. In contrast, one-to-many bi-grams like \u201cat least\u201d are first promoted to very high scores but then get suppressed perhaps due to the model seeing more of the scope of the token \u201cat\u201d. One-to-one"}, {"title": "5.3 ANALYZING FINE-TUNING EFFECT", "content": "Fine-tuning is an important phase where the raw pretrained LLMs are guided to perform particular tasks. We would like to understand how the model inner knowledge changes during fine-tuning pro-cesses. While parameter diffing can be a straightforward solution, jet n-grams provides an alternative approach, where the diffs are human readable and directly reflect the change of knowledge retained by the LLMs. Such insights would allow us to better decide the mixture of data for fine-tuning, and the number of steps for fine-tuning, which are currently a mix of heuristics and trial-and-error.\nCode fine-tuning promotes coding-relevant bi-grams. We analyze the changes due to code fine-tuning via diffing jet bi-grams extracted from Llama-2-7B and its fine-tuned versions, Codellama-7B and Codellama-Python-7B. As highlighted the jet bi-gram diff re-veals coding-relevant keywords, such as \u201c**kwargs\u201d, \u201cgetters\u201d and \u201cAssertion\u201d, suggest-ing jet bi-gram can be a tool for verifying if fine-tuning is effective in acquiring relevant knowledge.\nDoes RLHF fine-tuning remove toxicity? We compare the original pretrained model, Llama-2-7B, with its RLHF version, Llama-2-7B-Chat. RLHF alignment (Bai et al., 2022) is widely believed to detoxify LLMs, as indicated by the ToxiGen scores (Hartvigsen et al., 2022). In, we demonstrate this with dataset-based toxicity scores on a subset of challenging prompts in the Real-ToxicityPrompts (RTP) dataset (Gehman et al., 2020): the gap in toxicity potential between the two models narrows as we prepend to RTP prompts increasingly \u201cexplicit\u201d (short) context. Specifically, for hard context, Llama-2-7B-Chat shows an 84% probability of producing toxic content, close to that of Llama-2-7B. This suggests that the RLHF model is not completely detoxified but rather hides the toxicity knowledge from the \u201csurface\u201d, which however can be easily triggered by specific con-texts. To quantify the toxicity knowledge embedded in these models, we use jet bi-gram probability scores and calculate the cumulative conditional probability mass for a set of \"toxic\" bi-grams, which are combinations of tokens associated with toxic meanings from a predefined list of keywords."}, {"title": "6 RELATED WORK", "content": "Interpreting transformers. There has been much effort in interpreting the inner computations of transformer models. In particular, mechanistic interpretability Ferrando et al. (2024), focuses on reverse-engineering such computations by identifying, clustering and labelling model behavior in human understandable terms and attributing them with certain model components, e.g., MLPs Geva et al. (2021; 2022), or typical \"circuits\" (Conmy et al., 2023; Ferrando & Voita, 2024). Authors discussed limitations of cur-rents approaches to MI. For example, Templeton et al. (2024) found it generally hard to conclude neuron-level intepretabilities, compared with feature representations; while Bolukbasi et al. (2021); Goldowsky-Dill et al. (2023) points out that conclusions drawn are generally limited to the cho-sen data distribution. As our approach focuses on manipulating functions, it does not require extra datasets that are used for probe fitting in methods such as Belrose et al. (2023) nor sampling, as needed in (Conmy et al., 2023; Ferrando & Voita, 2024; Voita et al., 2024). On a high level, allowing taking any portion of compute out of the original transformer, jet expansions abstract and generalize previous characterizations on the computational paths where non-linear components with significant roles, e.g. layernorm and MLPs, are either ig-nored or over-simplified for the ease of analysis. Additionally, zero ablations (or knock out) (Olsson et al., 2022) and direct logits attributions (Wang et al., 2022) are linked to particular instantiations of zeroth order jet expansions.\nn-gram models. The early applications of n-gram models in languages dates back to where n-grams modeled the statistics of English. The n-gram based approaches have been an important baseline in language processing, e.g., general language modelling with applications like machine translation (Brants et al., 2007). There have been regained interests on combining n-gram with neural network model-based approaches (e.g. Liu et al., 2024). Several recent works have explored the relationships between LLMs and n-gram language models, such as analyzing the representational capacity of transformers to simulate n-gram LMs (Svete & Cotterell, 2024) and measuring agreement between LLM predictions and n-gram rulesets\nTaylor expansion and jets Taylor expansions are popular tools in analyzing learning behaviours, notably linearization (k = 1). For example, Belrose et al. (2024) applied Taylor expansion on the loss function to demonstrate the learning preference of neural network models. Xu et al. (2022) introduced a second-order Taylor expansion over the data distribution to interpret optimal features. The generalized jet notions was introduced in machine learning in the context automatic differentiation tools by Bettencourt et al. (2019), and is an experimental feature in Jax , but has been studied before (see e.g. Griewank & Walther, 2008)."}, {"title": "7 CONCLUSION AND DISCUSSION", "content": "We introduced jet expansion, a novel framework for expanding the computational graphs of neu-ral networks. The method, which we specialize in this paper to deep residual nets, can be used to disentangle contributions of user-selected computational paths from the overall graph. Comple-mentary to other dataset-dependent methods in MI, our method enables various dataset-free global interpretability studies, such as mapping computation to linguistic roles. We have validated jet ex-pansions in terms of cosine similarity against model outputs and through interventional experiments (Section 5.1). We applied our data-free method to transformer LMs, showing how we can sketch the original model with input-output probability databases, extracting LM bi-and-tri-gram statistics.\nLimitations. Although rooted in Taylor series theory, expansions obtained via our frameworks do not (seek to) approximate the input function in any strict sense. Rather, our framework is amed at facilitating interpretation of model behavior: we can use jet expansion to rewrite an input compu-tational graph as a sum of \u201cinterpretable\" polynomial terms and a (computable) remainder. How"}]}