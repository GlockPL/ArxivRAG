{"title": "Exploring reinforcement learning for incident response in autonomous military vehicles", "authors": ["Henrik Madsen", "Gudmund Grov", "Federico Mancini", "Magnus Baksaas", "\u00c5vald \u00c5slaugson Sommervoll"], "abstract": "Unmanned vehicles able to conduct advanced operations without human intervention are being developed at a fast pace for many purposes. Not surprisingly, they are also expected to significantly change how military operations can be conducted. To leverage the potential of this new technology in a physically and logically contested environment, security risks are to be assessed and managed accordingly. Research on this topic points to autonomous cyber defence as one of the capabilities that may be needed to accelerate the adoption of these vehicles for military purposes. Here, we pursue this line of investigation by exploring reinforcement learning to train an agent that can autonomously respond to cyber attacks on unmanned vehicles in the context of a military operation. We first developed a simple simulation environment to quickly prototype and test some proof-of-concept agents for an initial evaluation. This agent was then applied to a more realistic simulation environment and finally deployed on an actual unmanned ground vehicle for even more realism. A key contribution of our work is demonstrating that reinforcement learning is a viable approach to train an agent that can be used for autonomous cyber defence on a real unmanned ground vehicle, even when trained in a simple simulation environment.", "sections": [{"title": "1 Introduction", "content": "Drones for surveillance and targeting, underwater vehicles for mine hunting, and ground vehicles for logistics are all examples of how unmanned autonomous vehicles can support military missions. The typical advantages are the ability to collect and process large sets of sensor data faster, with higher precision, and automating dangerous or repetitive tasks while freeing up personnel. On the other hand, these vehicles' technology and their use in military operations can introduce new security risks that we currently do not know how to manage effectively. Since failing to implement adequate security may threaten both the operation they are supposed to support, and other military assets, such as classified data and technology, new security capabilities are needed to facilitate the adoption of these vehicles in a military setting.\nOne security capability deemed necessary to achieve a sufficient level of trust in vehicles with a higher degree of autonomy is autonomous cyber defence [13]. This is the ability of the vehicle itself to detect and respond to cyber attacks. The reason is that highly autonomous vehicles are likely to be used for operations where a link to a remote operator is either undesirable due to operational requirements like covertness, or unavailable due to limited network access. This would, in turn, also prevent a security operation centre (SoC) from remotely monitoring the systems' status and detecting and managing potential cyber attacks. The risk of employing a vehicle in this context without the capability of proactively protecting itself to some degree, could outweigh the operational advantages in many cases. Well-established preventive security measures such as encryption, integrity verification and anti-tampering are still needed, but are neither sufficient nor as effective on unmanned vehicles as they are for other systems, which can be kept in controlled facilities or under constant monitoring. Therefore, we explore the possibility of complementing preventive security measures with an autonomous agent that can respond to cyber security incidents, given that a reliable detection capability is in place.\nWhile autonomous agents for cyber defence is not a new research area, most of what is available in the literature pertains to classical computer networks or internet-based threats. Some complicating factors for unmanned vehicles in military operations make developing autonomous agents also more challenging than in other contexts, as they would have to operate at the intersection between cyber security, safety and mission assurance. Anomalies in any of those domains could be a symptom of a cyber attack, and responses aimed at handling an incident in one of the domains, could negatively affect the others. For instance, a light-diode blinking in an anomalous manner could indicate an attempt to exfiltrate data by using it as a side channel. However, shutting down the computing unit controlling the diode could cause other actuators to stop working properly, and causing the vehicle to crash. Hence, response actions need to be assessed based on many conflicting concerns. Reinforcement learning seems like a promising approach to create such an autonomous agent that can consider these different conflicting concerns. Our overall research hypothesis is:\nReinforcement learning can produce autonomous agents that can respond to cyber incidents while balancing conflicting security concerns that autonomous vehicles may have to consider in a military operation.\nIn this paper, we take some initial steps to address this hypothesis. We focus on incident response and a limitation of our work is that we assume that detection is achieved by other means. The contributions of this paper are that we develop, apply and compare different reinforcement learning agents for an Unmanned Ground Vehicle (UGV) for different simulation environments as well as on a real military UGV. The applied nature of work, where the agents are used on a real"}, {"title": "2 Reinforcement learning for incident response", "content": "Reinforcement learning (RL) is a machine learning (ML) paradigm that learns what to do, how to map situations to actions, in order to maximize a numerical reward signal [26]. Here, an agent is not told what actions it should take, but instead explores an environment by selecting actions and observing the change. This observed change can, and often will, result in a change of state. The agent's state gives how the agent percieves the environment at a given time-step. The RL problem is often seen as an agent that interacts with the environment through actions, and the environment responds with a new state and a reward for the change. This reward can be positive, giving positive reinforcement for the selected action in the given state, or it can be negative giving negative reinforcement for the action in the given state. In RL, a policy defines an agents behaviour for a given state, i.e. how the the agent selects actions, and the goal of RL is to find a good, or ideally optimal, policy. A reward signal is an immediate reward, and provides the logic in which actions are good or bad in the short term. In contrast to the reward signal, a value function also tries to consider long-term rewards.\nThe exploration rate ($\\epsilon$) hyperparameter\u00b3 describes how much an agent will explore an environment or exploit the knowledge it has learned about the environment [14]. The learning rate ($\\alpha$) hyperparameter controls the rate in which the model learns new information, while the discount factor ($\\gamma$) determines the importance of future rewards compared to immediate rewards. A challenge in RL is balancing exploration and exploitation \u2013 to obtain most reward it needs to commit to the best action(s) that it has used in previous states, however before it can exploit it needs to explore the different actions and progressively favour the actions that are best [26]. In epsilon-greedy action selection [5], the agent reduces the probability of exploiting over time. We consider two RL algorithms:\nQ-learning was one of the early breakthroughs in the field of RL. The algorithm involves a table (Q-table) which contains values for each state-action pair that is provided by the environment. With more states and actions, the Q-table becomes larger, so while it is highly effective for simple reinforcement learning problems, there can be memory constraints for more complicated problems."}, {"title": "3 Operational context & experimental design", "content": "For our experiments, we use an actual Unmanned Ground Vehicle which is actively being deployed in Ukraine [20]: The Milrem THEMIS [21]. The Moscow-based Center for Analysis of Strategies and Technologies (CAST) announced a reward to anyone who manages to capture and deliver a Milrem THeMIS to the Russian Armed Forces [28]. This supports our motivation that such vehicles will become important intelligence targets and that collected intelligence can be used for more targeted attacks against them later, possibly as part of a (joint) operation. Such possible attacks were explored in a previous work on the same vehicle [10]. Here, we assume specifically that an opponent has managed to gain some privileged access to the vehicle's systems thanks to a successful supply chain compromise, so that some component will be able to initiate an attack internally without external intervention. The focus is thus exclusively on response with the following assumption:\nWe assume that an attack is detected (by other means) and our objective is to find a suitable response action.\nThis UGV is modular in functionality, which has proven to be important in reducing the risk of losing human lives. It uses natively ROS2 [11] for internal communication between sensors, control modules and actuators, but the version used for our tests has also been extended with custom autonomy capabilities and additional navigation sensors that communicate with each other by using the ROS1 system [27]. ROS systems consist of nodes that communicate through topics. A topic could be about a component like \"Brakes\", and messages about the state of that component can be published and read by any other ROS node on the vehicle subscribing to that topic. Accompanying the UGV, there is a simulation tool [27] and two visualization tools. The first is visualization tool is Veranda, which is an open-source 2D robotics visualization tool written to interface with applications via the ROS2 communication layer [25]. The THEMIS Simulator publishes the individual simulated track speeds to Veranda, allowing the UGV to be simulated in a virtual 2D world. The second tool is Mapviz, a ROS-based visualization tool focusing on visualizing 2D data [18]. Both Veranda and Mapviz are\nTo model realistic operations, we used actual experiences from the deployment of the THEMIS UGV in Ukraine. Seven of these vehicles were there configured for casualty evacuation and were delivered at the end of 2022. Another seven were delivered during the second quarter of 2023 with a configuration for route clearance [20]. The agent had therefore the following mission objective:\nThe mission objective is to ensure that the UGV safely reaches a specific point within a given time.\nThe agent's main goal is therefore to ensure, or at least increase the likelihood of, this mission's success by correctly responding to adversarial activities that can cause the UGV to e.g. stop, be captured, or destroy its cargo.\nTo represent adversarial activity by an attacker, random events are implemented that may occur at any time while the mission is in progress. These events are created by selecting a random component and altering its state. This may result in the UGV coming to a halt. The agent must respond correctly by returning the compromised component to its original state so the UGV can drive again. The cost of response actions must be considered, as changing the state of a system component may cause physical harm to that component or other components it communicates with. Furthermore, the agent must not simply exploit specific actions to attain the most reward when no threat exists. We achieve this by having the agent remain idle or monitor the environment while no adversarial activity has been detected.\u2074 The available response options may vary based on travel time constraints and how they can affect, respectively, the people being transported or the payload mounted on the UGV.\nThe two RL algorithms we have explored, Q-learning and DQN, are based repsectively on Sutton & Barto's pseudocode [26] and Stable-Baselines3 (SB3). In order to quickly prototype the agents, or, in other words, to quickly develop and implement the code for applying RL to intrusion response in a CPS, the Python programming language was used."}, {"title": "4 Results and discussion", "content": "Simple simulation environment. In the simple environment we test three competing strategies: random action (baseline), Q-learning (epsilon-greedy), and an argmax strategy. Where the baseline strategy is a strategy where an agent selects a random action at each timestep, and serves as a sanity check of the problems complexity and that the Q-learning agents are learning something meaningful. Initial hyperparameter testing for Q-learning tested combinations of 0.1, 0.5 and 0.9 for \u03b1, \u03b3 and $\\epsilon$, and found that \u03b1 = 0.1, \u03b3 = 0.9 and $\\epsilon$ = 0.9 gave favorable results.\nThe parameters for the environments was set as follows for all the experiments: max_timesteps was set to 1800, goal_step was set to 800 and attack_prob was set to 0.9.\nFigure 4 shows the total reward for the argmax, epsilon-greedy and random choice. A key observation is that both strategies outperform random choice as was used for baseline. The result for experiment 2 is similar and not provided here.\nTo extend the simulation to both experiment 1 and experiment 2 we also trained and tested a DQN agent. During testing, the parameters remained the same as above, except that attack_prob was set to 0.1, i.e. a 10% chance of being attacked. Table 3 shows the results of the training of the reinforcement learning agents for experiment 1 and experiment 2 with the two RL algorithms. One thing to notice is that the training of the deep learning model DQN is much slower than the more simplistic Q-learning approach. This is most likely due a\ndifference in how they were trained. The Q-learning approach was trained for 1000 episodes with max timesteps of 1800, while the DQN was trained on 1800 \u00b7 1000 = 1 800 000 total timesteps spread across as many episodes as possible. This means that the more episodic training the Q-learning agent undertook is faster, as the agent learns to combat the adversary's attacks, leading to shorter episodes. This approach also prevents the agent getting stuck on early runs potentially wasting time. However, the fixed timestep approach that was used for DQN ensures that even though later runs are fast, it still has time to optimize its defence further. In this case, it seems like the added exploration paid off as the epsilon-greedy DQN approach both achieves a better mean reward and better mean timesteps used on the tests conducted after training. The difference, however, is not overly large and the main take-away is that both algorithms are successfull in the simple environment.\nIntegrated simulation environment. Both RL agents were also successful in the integrated environment. This indicates that the simple environment captured the essential elements of the more complex integrated environment. The initial promising results in the integrated environment also bodes well for the agents to also be effective in a real world setting on the UGV.\nExperiments on the Milrem THEMIS UGV. The experiments on the Milrem THEMIS UGV were done over two days. The first day was mainly used for debugging, but we did already on the first day get positive results. Once the parameters and values had been adjusted correctly, the RL agents and environments were able to interact with the UGV. This could be seen from several graphical interfaces that accompanied the UGV. The data sent from the RL agents was successfully transmitted via our interface, and the states of the concerned topics were correctly changed. The transmission was also seemingly instantaneous, as the UGV switched between modes immediately.\nOn the second day of testing, more complete tests were conducted. The initialization of agents and environments worked the same way as in the simulations, and the trajectory and commands for changing between different modes were sent correctly. When the mission started, the UGV achieved a high veloc-"}, {"title": "5 Conclusion & future work", "content": "Our overall research hypothesis for this work is that reinforcement learning can produce autonomous agents that can respond to cyber incidents while balancing conflicting security concerns that autonomous vehicles may have to consider in a military operation. As a target platform to address this research hypothesis, we used a Milrem Themis 4.5 UGV which has been extended with autonomous capabilities. It also comes with its own simulation tool, which has been complemented with the autonomy modules deployed on the actual UGV. To design and test S2S scenarios, we developed an environment where RL agents can be quickly prototyped, and later used in the UGV simulation tool. This allowed to later deploy the trained agents on to the actual UGV (S2R) with relative ease. We tested both traditional and deep reinforcement learning (Q-learning and DQN). The mission that was simulated was quite simple: the UGV is tasked with travelling a given route, with the main operational objective to complete it within a given time. Here, random components are made to fail with a given frequency because of cyber attacks that the agent needs to respond to by choosing from a selection of predefined actions that have effect not only in the cyber domain, but also to the physical state of the vehicle. Detection of such attacks was assumed to be in place and working as intended.\nThe results show that reinforcement learning gives better response outcomes than if the agent had taken random choices, albeit on simple scenarios, and thus reason to believe that this can be promising approach to produce agents for incident response on autonomous military vehicles. The tested reinforcement learning algorithms confirm the results found in the literature, where Q-learning and DQN proved to be suitable for scenarios with discrete spaces. Most of the trained RL agents also successfully transferred for S2S and S2R scenarios, showing that it is feasible to use an operational military autonomous vehicle as a test platform already at an early prototyping stage. This will allow us to model and test more realistic threat and response scenarios that can take place in actual missions. We have also had to omit some details, such as additional hyper-parameter tuning and testing of other RL algorithms.\nAnother valuable outcome of this work is a better understanding of what other challenges need to be resolved to give a more definitive answer to our research hypothesis. One aspect where there is room for improvement, and that may present significant challenges, is how to represent mission-critical assets in the reward function. This requires to understand the relation between mission objectives and vehicle states down to component level. This is relatively straightforward for one simple mission objective with clear dependencies, like the engine being critical to move and therefore complete the given route. If multiple components contribute to the objective in a different way under different circumstances, or multiple conflicting objectives are considered, this will likely result in much larger observation and action spaces, and could produce different results than we have seen here. If at all possible to model in a reasonable way. Another topic that should be investigated further, is how to represent components and states. A mix of strings in lists for components, and numerical values attached to them that indicated their status proved sufficient to compute a numerical representation of the overall state of the UGV to be presented to the RL agent. A more effective way could be to introduce a vector or dictionary representation of states. This could also provide better human readability and reduce computational costs, which can become significant when action and state spaces increase with more complex environments.\nTo conclude, our experiments have shown promise in using reinforcement learning for autonomous incident response and given us valuable insight into what is needed to further investigate our research hypothesis. Promising research avenues to build on our work include handling more realistic and complex sce-"}]}