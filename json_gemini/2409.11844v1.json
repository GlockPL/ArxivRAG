{"title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts", "authors": ["Tianle Gu", "Kexin Huang", "Ruilin Luo", "Yuanqi Yao", "Yujiu Yang", "Yan Teng", "Yingchun Wang"], "abstract": "Large Language Models (LLMs) can memorize sensitive information, raising concerns about potential misuse. LLM Unlearning, a post-hoc method to remove this information from trained LLMs, offers a promising solution to mitigating these risks. However, previous practices face three key challenges: 1. Utility: successful un-learning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based un-learning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them. We evaluate MEOW on the commonly used unlearn benchmark, ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks. Results demonstrate significant improvement of MEOW in forget quality without substantial loss in model utility. Meanwhile, MEOW does not exhibit significant drop in NLU or NLG capabilities, and there is even a slight increase in NLU performance.", "sections": [{"title": "1 Introduction", "content": "Recent research (Hartmann et al., 2023; Tirumala et al., 2022) highlights that LLMs have the potential to memorize training data, which can be exposed through red teaming attacks (Nasr et al., 2023) like Membership Inference Attack (MIA) (Shokri et al., 2017; Shi et al., 2024) and Prompt Injection (Khom-sky et al., 2024). Such vulnerabilities raise con-cerns about privacy leakage and copyright viola-tions. For instance, in medical LLMs, malicious users could extract training data to guess whether a patient has a specified disease. Meanwhile, un-intended data leakage, without the awareness or consent of data owners, may result in violations of related laws, such as the General Data Protection Regulation (Parliament and of the European Union, 2016) in the European Union.\nSo, how to protect sensitive information from potential leakage? Data pre-processing (Aura et al., 2006; Dernoncourt et al., 2016; Lison et al., 2021; Kandpal et al., 2022; Ghosh et al., 2024) and Dif-ferential Privacy (DP) (Dwork et al., 2006; Dwork, 2008; Abadi et al., 2016; Anil et al., 2021; Li et al., 2022a; Yu et al., 2022) are widely stud-ied and established to prevent data leakage. Data pre-processing involves data audit and removing all sensitive information from training data, while DP adds random noise to data, making sensitive and normal information indistinguishable. How-ever, data pre-processing requires numerous anno-tations, and both approaches necessitate retraining the model an impractical solution for LLMs.\nTherefore, applied in a post-processing man-ner, LLM unlearning offers a promising solution. Based on the access of the model, previous re-search can be divided into three schools of thought:\nBlack Box Setting (BBS), where model weights are totally inaccessible. Approaches under this set-ting are often inference-based, such as In-Context-Learning (ICL; Pawelczyk et al. (2024)). Grey Box Setting (GBS), where partial access to the model is available, such as logits or embedding space. Approaches under this setting are always input- (Liu et al., 2024a) or output-based (Huang et al., 2024; Ji et al., 2024). White Box Set-ting (WBS), where the full model weights are accessible."}, {"title": "2 Settings, Goals, and Evaluation", "content": "Suppose we have a dataset D = (x, y) and an un-trained LLM Mu. After training Mu on D, we obtain a trained LLM, Mo, which serves as the original model for the unlearning task. Mean-while, we divide the dataset into Df = (xf, yf) and Dr = (x, y), representing the dataset to forget and the dataset to retain. We train Mu on Dr to obtain the retain model M\u2081 as the ground truth for unlearn-ing tasks. Furthermore, we introduce an additional dataset Dg = (x, y) to evaluate the general capa-bilities of the model after unlearning, such as its NLU and NLG abilities."}, {"title": "2.2 Goals", "content": "After unlearning, the origin model M\u00b0 is trans-formed into the target model Mt. We categorize the unlearning goals into hard unlearning and soft unlearning, based on the format of responses \u1ef9 that Mt generates to prompts in Df. Hard unlearning refers to responses where the target model Mt either avoids answering, providing blank or template an-swers like \"I don't know\", or generates completely nonsensical responses. Soft unlearning, however, involves providing incorrect but understandable an-swers. For general-purpose LLMs, hard unlearning would greatly harm the user experience. Therefore, soft unlearning is more suitable for ideal LLM un-learning, which is the goal of our paper. We discuss the potential limitations in Sec. 8."}, {"title": "2.3 Evaluation", "content": "Nearly all the LLM unlearning algorithms are try-ing to address the problem of balancing model utility and forget quality, i.e., how to effectively unlearn without causing catastrophic collapse on Dr and Dg. Therefore, this paper utilizes 2 metrics:\nModel utility: typically measures the perfor-mance of Mt on Dr and Dg. Forget quality: can be assessed in two ways, measuring the differ-ence between M\u2081 and Mo on Df, or the similarity between Mt and Mr. For the former way, both hard and soft unlearning can achieve high forget quality. However, for the latter, hard unlearning typically fails to maintain high forget quality due to its negative impact on model utility. Therefore, we believe the latter one is more rigorous and aligns better with real-world scenarios, and use it for the measurement of forget quality."}, {"title": "3 Methodology", "content": "We propose an easy yet effective approach, MEOW, simultaneously considering utility, efficiency, and robustness. Un-der WBS, MEOW is a gradient descent-based method that avoids loss divergence and eliminates the need for auxiliary models or retain datasets. It modifies the model's weights to unlearn target data, after which the modified model can be safely open-sourced while preventing attackers from extracting the removed information, ensuring the robustness of unlearning. In detail, we argue that accurately quantifying the mem-orization of sensitive information is the first step toward effective unlearning, in LLMs stems from this memorization. To address this, we introduce a novel metric, MEMO, to measure the memoriza-tion of individual/group sequences in LLMs. Next, we generate a set of alternative answers based on undesired responses from the forgetting dataset. Guided by MEMO's memorization signals, we se-lect the largest/smallest k memorized answers as labels to form a perturbation dataset. Finally, we fine-tune the origin model on this dataset. Exten-sive experiments, on the unlearning, NLG, and NLU benchmarks, demonstrate the superior perfor-mance over existing methods of MEOW.\nWe summarize our contributions as follows:\nWe propose MEMO, a novel metric for quantify-ing memorization in LLMs, offering superior effec-tiveness, efficiency, and compatibility with MEOW compared to traditional methods.\nOur simple yet effective method, MEOW, shows a significant improvement in forget quality with-out causing a substantial decline in model util-"}, {"title": "3.1 Quantifying memorization in LLMs", "content": "MEMO Given a question x = {xi | 0 \u2264 i < |x|} and an answer y = {yi | 0 \u2264 i < |y|}, we segment x and y according to different modes, as shown in Fig. 2. Specifically, in the prefix mode, we truncate x to form prompt Tp = xe, where e represents the truncation endpoint. In the suffix mode, we truncate y to form Tp = x+y. And the remaining part of the sequence is the ground truth Tgt to be compared, defined as:\n$$Tgt =\n\\begin{cases}\nxe+1+y, & \\text{if prefix mode,}\nxb + yb, & \\text{if suffix mode.}\n\\end{cases}$$\nThen, we feed Tp into the model M, obtaining the output Tr. We compare T\u2081 with Tgt using Rouge, as specified in Eq. 1:\n$$MEMO(x, y) = \\frac{\\sum_{i=1}^{N} Rouge-N(T_r, T_{gt})}{S}$$\nwhere Rouge-N refers to the Rouge (Lin, 2004), and S denotes the total number of sliding windows. Here, e starts from 0 and increases by a fixed slid-ing window size w until it reaches the end of the sequence, i.e., e \u2264 |EOS]. The pseudocode for MEMO is provided in the App. A.\nMEMO Strength For any dataset, we measure the memorization of a model for a certain prompt-response pair (x, y) by calculating MEMO(x, y) and obtain the average value, denoted as \u03bc.\n$$\u03bc(D, M) = \\frac{\\sum_{i=1}^{N} MEMO(x_i, y_i)}{S}$$"}, {"title": "MEMO Consistency", "content": "We introduce c\u2081(D, M) to represent the variance of memorization in M for a given sample set D, i.e., the consistency of memo-rization across different samples.\n$$\u03c3(D, M) = \\sqrt{\\frac{1}{S} \\sum_{i=1}^{S} (MEMO(x_i, y_i) - \u03bc(D, M))^2}$$\n$$cv(D, M) = \\frac{\u03c3(D, M)}{\u03bc(D, M)}$$"}, {"title": "3.2 LLM Unlearning Via Inverted Facts", "content": "Conceptual Motivation In our method, we build on the Information Overloaded Theory (Himma, 2007), which suggests that excessive information can impair normal understanding and decision-making. Applied to LLMs, we interpret direct expo-sure to specific sensitive information as a \u201cstrong belief\" in a particular fact. However, when pre-sented with more similar but different or even con-tradictory facts, the model becomes hesitant and tends to discard the original belief.\nFact Inversion For the forgetting dataset Df and the facts that need to be forgotten, we use an offline LLM (Achiam et al., 2023) to generate inverted facts. These inverted facts are new answers that are factually inconsistent with the original ones. For instance, in Fig. 1, for the fact \"The kitty likes to memo,\" we generate three reversed facts: \"The kitty likes to meow\u201d, \u201cThe kitty likes to fish\", and \u201cThe kitty likes to dance\u201d. We provide the prompt used for fact inversion in App. E.\nMemory Supervised For the generated inverted facts, we use MEMO to calculate the memorization of each fact. Then, we select the top or bottom k facts with the highest or lowest memorization to form a new fact set. Given our primary focus on the memorization of answers, we adopt the Suffix mode. Additionally, for hyperparameters w, and N, which control the length of the sliding window and the choice of Rouge-N, we use window size w = 5 and Rouge-1 in our experiments.\nFine-tuning with Inverted Facts Finally, we fine-tune the model using the selected inverted facts and train it with the next-token prediction task. We employ cross-entropy loss (CE) that constrains the similarity between estimated and ground-truth to-kens, which can be presented as\n$$L = CE(\u1ef9, \u0177),$$ where y is the predicted token, and \u0177 is the ground-truth token."}, {"title": "4 Experiments", "content": "The unlearning method under the WBS can be con-sidered as fine-tuning the original model with an unlearning objective function, which is a specific combination of the loss on the forget data and the loss on the retain data, as shown in Eq. 2 (Liu et al., 2024b). The forget losses include: \u25cf GA (Yao et al., 2024): performs gradient ascent on forget data. DPO (Rafailov et al., 2024): direct prefer-ence optimization, encouraging the model to give responses like \u201cI don't know\u201d. NPO (Zhang et al., 2024): negative preference optimization, a variant of DPO where only the correct answer is used as a negative label. The retain losses include: GD (Maini et al., 2024; Jia et al., 2024): sub-tracts the loss on forget data from the loss on retain data. KL (Wang et al., 2024a; Maini et al., 2024): calculates the KL-divergence on retain data before and after unlearning to ensure that the model retains its original performance on retain data. We term each baseline by combining the specific forget loss and retain loss, e.g., GA+KL indicates the use of GA as the forget loss and KL as the retain loss.\n$$Lf = E_{(x,y)\u2208Df} [l (y | x; \u03b8)]$$\n$$Lr = E_{(x,y)\u2208D_r} [l(y | x; \u03b8)]$$\n$$L = L_f + \u03bbL_r$$\nHere, \u03bb controls the retain strength, and l(y | x; \u03b8) denotes the prediction loss of using \u03b8 when given the input x with respect to the response y."}, {"title": "4.2 Experiments on Unlearning Dataset", "content": "Setup ToFU (Maini et al., 2024) is a QA dataset for unlearning knowledge about virtual authors. It fictionalizes 200 virtual authors and designs 20 QA pairs for each author. ToFU is divided into three tasks of varying forgetting difficulty based on the proportion of authors to be forgotten. The datasets Df contain 1%, 5%, and 10% of the authors to be forgotten, respectively. We use the fine-tuned Llama2-chat-7B (Touvron et al., 2023) and Phi-1.5 (Li et al., 2023) released by ToFU paper as the origin LLM Mo.\nMetrics We evaluate the forgetting performance using forget quality, as defined in (Maini et al.,"}, {"title": "4.3 Experiments on NLG and NLU Datasets", "content": "Setup We select PIQA (Bisk et al., 2020), ARC-E (Clark et al., 2018), and ARC-C (Clark et al., 2018) datasets to compile an NLU dataset, which is employed to evaluate the natural language under-standing abilities of LLMs after unlearning. More-over, we curate an NLG dataset by sampling 5,000 instances from WikiText (Merity et al., 2016) and CC-News (Hamborg et al., 2017) to evaluate the natural language generation capabilities.\nMetrics For NLU datasets, we use their respec-tive metrics (accuracy). For NLG datasets, we eval-uate the quality of the generation of LLMs using MAUVE (Pillutla et al., 2021), BLEU (Papineni et al., 2002), and Rep3 (Welleck et al., 2019)."}, {"title": "5 Additional Analysis", "content": "In this section, we further explore MEMO in dif-ferent settings, and have the following findings:\nFinding 1: LLMs with stronger memoriza-"}, {"title": "5.1 Analysis on MEMO", "content": "In this section, we further explore MEMO in dif-ferent settings, and have the following findings:\nFinding 1: LLMs with stronger memoriza-"}, {"title": "5.2 Analysis on MEOW", "content": "Ablation Study of MEMO\nAblation study of the number of inverted facts and selection strategy\nStability of Unlearning"}, {"title": "6 Related Work", "content": "Memorization in LLMs Memorization is an inherent capability, but the rise of LLMs has brought about unforeseen consequences, such as privacy (Brown et al., 2022) and confidential-ity (Mozes et al., 2023). Consequently, quantifying memorization in LLMs emerges as a critical yet highly challenging research focus. A na\u00efve defini-tion of memorization might encompass all informa-tion stored in weights of models, but determining exactly what a model retains is impractical. Thus, researchers have shifted towards extractability the information that can be retrieved, particularly through verbatim memorization (Hartmann et al., 2023). Carlini et al. (2019) explore the out-of-distribution (OOD) secrets memorized by language models and define the exposure metric to measure the computational complexity required to guess the secrets. These approaches necessitate multiple inferences and often involve retraining. Extractabil-ity (Carlini et al., 2021) assesses whether a string y is extractable from an LM p with high proba-bility given a prefix x. Counterfactual memoriza-tion (Zhang et al., 2023), instead, measures how much a model architecture memorizes examples from a distribution on average without assessing memorization in a specific model.\nLLM Unlearning LLM Unlearning (Si et al., 2023; Yao et al., 2024; Liu et al., 2024b; Qu et al., 2024; Li et al., 2024) has its roots in Machine Un-learning (MU) (Cao and Yang, 2015), a concept originally developed to safeguard data privacy, par-ticularly in response to regulations like the Right to be Forgotten (RTBF). MU has been applied across various domains, including image classifica-tion (Ginart et al., 2019; Golatkar et al., 2020; Neel et al., 2020; Ullah et al., 2021; Sekhari et al., 2021), text-to-image generation (Gandikota et al., 2023; Zhang et al., 2023; Kumari et al., 2023; Fan et al., 2024), federated learning (Liu et al., 2021; Wang et al., 2022; Che et al., 2023; Liu et al., 2024c; Hal-imi et al., 2023), graph neural networks (Chen et al., 2022b; Chien et al., 2022; Wu et al., 2023), and rec-ommendation systems (Sachdeva et al., 2024; Chen et al., 2022a; Xu et al., 2023; Li et al., 2022b; Wang et al., 2024b). However, traditional MU meth-ods face key challenges when applied to LLMs:\nScale of Parameters: LLMs typically consist of billions of parameters, making retraining from scratch computationally expensive and often im-practical. Generative Nature of LLMs: unlike traditional NLP models, LLMs are predominantly used for generative tasks like text generation and sentiment analysis, requiring unlearning strategies tailored to their specific nature. Recent research begin to address these challenges, leading to the development of various LLM-specific unlearning techniques. In the Introduction section (Sec. 1), we categorize these methods to provide a comprehen-sive overview of current LLM Unlearning."}, {"title": "7 Conclusion", "content": "This paper introduces MEMO, a new metric quan-tifying memorization in LLMs, balancing both ef-ficiency and effectiveness. Leveraging the memo-rization signals provided by MEMO, we introduce a novel LLM unlearning method, MEOW. Specifi-cally, we first generate several alternative answers, rank them by MEMO, select the top or bottom an-swers as inverted facts, and finetune the original model. Experiments on the Unlearning Dataset ToFU demonstrate that MEOW demonstrates a clear improvement over existing methods in terms of forget quality while maintaining model utility without notable decline. Additionally, experiments show that MEOW can even enhance the NLU ca-pability of models. Our research advances both memorization quantification and LLM unlearning."}, {"title": "8 Limitations", "content": "While MEOW greatly enhances the forget quality and stability of the unlearning process, we consider the following limitations:\nSensitivity to hyper-parameters During base-line reproduction, we find that the performance of models is highly sensitive to certain hyperparam-eters, such as \u03bb in Eq. 2 and \u03b2 in NPO, leading to potential variations in previous results. In the App. C, we provide the hyperparameters used for the baselines to ensure reproducibility.\nPotential increase in hallucination MEOW leverages hallucination as a beneficial concept, which may inherently lead to an increase in hal-lucination due to the nature of soft unlearning.\nDecrease in model utility While MEOW signifi-cantly improves forget quality and stability, there is still a slight decline in model utility. Further work could explore ways to better maintain model util-ity, a challenge that is common among many WBS LLM unlearning methods."}, {"title": "A Pseudo-code of MEMO", "content": "In this section, we present MEMO algorithms in two modes, as shown in Alg. 1 and Alg. 2. For detailed descriptions, please refer to Sec. 3.1."}, {"title": "B Prior metrics for quantifying memorization in LLMs", "content": "Prior metrics for quantifying memorization in LLMs"}, {"title": "C Experimental Setup on ToFU", "content": "In this section, we present the implementation de-tails of each method when conducting experiments on ToFU. For LLaMA2-7B-Chat, we use the re-sults from (Ji et al., 2024), and for Phi-1.5, we use the official results published by (Maini et al., 2024). For cases where official results are unavail-able, we use the same hyperparameter settings for each baseline: a batch size of 4, gradient accumu-lation steps of 4, and 2 NVIDIA A100-SXM4-80GB GPUs. For methods using GA and DPO as the forget loss, we follow ToFU, selecting the peak value from 5 epochs (prioritizing Forget Quality, followed by Model Utility). The experimental re-sults are shown in Fig. 9, Fig. 10 and Fig. 11. For the NPO-based method, we report the results for 10 epochs. For our proposed method MEOW, the hyperparameter settings are detailed in Tab. 5."}, {"title": "D Example Generation on Forget Set", "content": "In this section, we present the responses of the model to the same prompt after being unlearned us-ing different methods. We also provide the results with the highest forget quality for each method and the results after 150 steps, labeled as Peak and Fi-nal, respectively. The peak results are shown in Tab. 6, where most models retain good language generation capabilities. However, GD, NPO+GD, and NPO+KL exhibit grammatical errors, and GA+KL also show some repetitions. The final re-sults are shown in Tab. 7, where most models retain good language generation capabilities. However, GA, GD, GA+KL, and NPO exhibit repetition."}, {"title": "E Prompt used for Fact Inversion", "content": "Here we present the prompt used for fact inversion"}, {"title": "Algorithm 1 Split Function", "content": "The first part of algorithm 1 is shown in the code"}, {"title": "Algorithm 2 MEMO", "content": "The first part of algorithm 2 is shown in the code"}]}