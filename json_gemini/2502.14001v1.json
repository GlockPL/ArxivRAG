{"title": "Towards a perturbation-based explanation for medical AI as differentiable programs", "authors": ["Takeshi Abe", "Yoshiyuki Asai"], "abstract": "Recent advancement in machine learning algorithms reaches a point where medical devices can be equipped with artificial intelligence (AI) models for diagnostic support and routine automation in clinical settings. In medicine and healthcare, there is a particular demand for sufficient and objective explainability of the outcome generated by AI models. However, AI models are generally considered as black boxes due to their complexity, and the computational process leading to their response is often opaque. Although several methods have been proposed to explain the behavior of models by evaluating the importance of each feature in discrimination and prediction, they may suffer from biases and opacities arising from the scale and sampling protocol of the dataset used for training or testing. To overcome the shortcomings of existing methods, we explore an alternative approach to provide an objective ex-planation of AI models that can be defined independently of the learning process and does not require additional data. As a preliminary study for this direction of research, this work examines a numerical availability of the Jacobian matrix of deep learning models that measures how sta-bly a model responses against small perturbations added to the input. The indicator, if available, are calculated from a trained AI model for a given target input. This is a first step towards a perturbation-based explanation, which will assist medical practitioners in understanding and interpreting the response of the AI model in its clinical application.", "sections": [{"title": "1 Introduction", "content": "Recent innovation of machine learning algorithms, including deep learning, has promoted the use of AI in various scientific and technological fields. In clinical medicine and healthcare, it is highly expected to introduce AI for diagnostic sup-port and automation of routine tasks. However, there are several pressing issues in AI's medical application. One of them is the fragility of mechanisms to ensure explainability for the results of classification or prediction task generated by AI models. This is of practical importance because a stricter standard is imposed on the responsibility for explaining decisions made by medical practitioners than in other fields, such as marketing, leading the applications of AI ahead."}, {"title": "2", "content": "In the realm of explainable AI (XAI), there are two typical approaches to conventional explainability. One approach is to explain the tendencies of the output from the inherent characteristics of the modelling or learning algorithm. For example, there are methods that clarify the contribution of each feature to prediction or discrimination in generalized linear regression models or decision tree-based models. The other approach forms a group of methods to measure the importance of features independently of the model type, i.e., in a model-agnostic way. Notable examples of this group include local algorithms for individual tar-gets, such as LIME [5] and SHAP [3]. In both approaches, the indices of explain-ability are calculated based on the accuracy or other performance measures on correct responses. It is also assumed that the dataset used for learning and/or testing follows the distribution of the target population. There is a risk of bias when this premise fails to hold. Moreover, there is opacity when information about the dataset in use cannot be accessed [1]. Some criticism of current X\u0391\u0399 technology argues that it is more appropriate to use models designed to be inter-pretable from the start, rather than trying to explain from black-boxed models post-hoc [6]. However, methodological significance resides in methods improving explainability of the AI models actually utilized in clinical practice.\nIn this work, focusing on deep learning models, we explore an alternative approach to explain how sensitive the model's response is with respect to a per-turbation to each instance. Here an instance means an input subject of the model, and perturbation refers to a small change in the input features of an instance. We aim at providing this approach with the following desirable properties. (a) It can be applied to trained AI models and does not depend on the learning process of them. (b) It is free from additional data collection. (c) It measures the stability of the output against the virtual variation of input features. (d) It returns a value for each instance, not the average value for the entire dataset, making it local. In addition, (e) it offers explainability not only for a single feature but also for interactions between features. We call such an approach a perturbation-based explanation (PBX). Complementing existing indicators for XAI with a PBX will facilitate medical practitioners to appropriately understand and interpret the behavior of a medical A\u0399.\nBy considering the AI model as a function with argument of many features, the impact of perturbations on each feature is often represented as a gradient. From the components of the partial derivative at the point of the instance, it can be judged how stably each feature is outputting against perturbations. Examples of such gradient-based approaches include SmoothGrad [7] and Integrated Gra-dients [8], but the unification with perturbation-based approaches is still under development.\nFor a preliminary study of a PBX, this work examines when and how a partial derivative of the deep learning model, as a multivariate function, is computation-ally available. In particular, since the Jacobian matrix is the most fundamental kind of partial derivatives, we concentrate on the numerical computability of the model's Jacobian matrix in this work. To formalize our argument for general applicability, the next Methods section describes the standard model of deep"}, {"title": "2 Methods", "content": "To represent the standard model of deep learning, we employ the following no-tation adapted from Higham et al. [2].\nL: the number of layers in the model\nn[]: the number of nodes in l-th layer (l = 1, . . ., L)\nx: the column vector of input (of length m = n[1])\n$x = a^{[1]} \\in \\mathbb{R}^m$\nW[]: weight matrix for weighted input at l-th layer (l = 2, . . ., L):\n$W^{[l]} = (w_{ij}^{[l]});  n^{[l]} \\times n^{[l-1]} $ matrix\nz[]: weighted input at l-th layer (1 = 2, . . ., L):\n$z^{[l]} = W^{[l]}a^{[l-1]}$\n\u03c3[]: activation function at 1-th layer (l = 2, . . ., L):\n$\\sigma^{[l]} : \\mathbb{R}^{n^{[l-1]}} \\rightarrow \\mathbb{R}^{n^{[l]}}$"}, {"title": "3 Results", "content": "The following theorem states a sufficient condition that the model's Jacobian matrix can be computed.\nTheorem 1. Let be f a model represented in the above notation. if o[1] has a Jacobian matrix that is computable at any point for each layer l = 2, ..., L, then f's Jacobian matrix Jf is computable at a given point.\nProof. Since $y = \\sigma^{[L]}(W^{[L]}o^{[L-1]}(W^{[L-1]} ... \\sigma^{[2]} (W^{[2]}x)......))$, the matrix ver-sion of the chain rule tells that f's Jacobian matrix is decomposed into o[1] $(W^{[l]}).$'s Jacobian matrices:\n$J_f = \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial a^{[L-1]}} \\frac{\\partial a^{[L-1]}}{\\partial a^{[L-2]}} ... \\frac{\\partial a^{[2]}}{\\partial x} = \\prod_{l=2}^{L} \\frac{\\partial a^{[l]}}{\\partial a^{[l-1]}} $                                                                                                    (1)\nThe multiplication in the above equation means the matrix product. In addition, each element of $\\frac{\\partial a^{[l]}}{\\partial a^{[l-1]}} $ is as follows:\n$\\frac{\\partial a_{i}^{[l]}}{\\partial a_{j}^{[l-1]}} = \\frac{\\partial \\sigma_{i}^{[l]}}{\\partial z_{i}^{[l]}}  \\cdot \\frac{\\partial z_{i}^{[l]}}{\\partial a_{j}^{[l-1]}} = \\frac{\\partial \\sigma_{i}^{[l]}}{\\partial z_{i}^{[l]}}  \\cdot w_{ij}^{[l]}$\n$\\nabla \\sigma_{i}^{[l]} (z^{[l]}) \\cdot (\\cdots, w_{i j}^{[l]}, \\cdots)^{T}$  (2)\nwhere is the dot product and $\\nabla \\sigma_{i}^{[l]} (z^{[l]})$ denotes the gradient of the i-th node's activation at layer l, which is computable because its transpose corresponds to a row vector of o[1]'s Jacobian matrix."}, {"title": "4 Discussion", "content": "The numerical computation of the Jacobian matrix of a deep learning model demonstrated in our results can be applied to a wide variety of pre-trained deep learning models potentially used for classification and regression tasks in a medical AI. Our treatment of activation functions is so general that it covers not only the usual activation functions of a scalar argument, e.g., the logistic or softplus function, but also the one taking the argument of two or more nodes, such as the softmax function often appended in the output layer. When applied, this local method calculates for each individual input data, so it does not require additional data. Since Algorithm 1 is implemented as a simple extension of the forward differentiation algorithm, it is straightforward to port the algorithm to any libraries of recent real-world machine learning frameworks, such as PyTorch and TensorFlow.\nThe proposed algorithm calculates the exact value of the Jacobian matrix at given instance, provided that the computation of the gradient of activation func-tions is exact. This fact makes a difference when comparing with the numerical calculation of the Jacobian matrix by the classical finite difference method [4]."}, {"title": "5 Conclusion", "content": "This work is our first step towards a PBX that makes an objective interpretation of medical AI models without the burden of data cost. The Jacobian matrix, once computed, favorably satisfies PBX's properties (a) to (d) mentioned in the Introduction section. However, to fulfill (e), the matrix is insufficient for revealing the stability of responses around given perturbed input since it lacks the information of interactions among input variables. That is, higher-order partial derivatives are called for when adding perturbations to multiple input features at the same time.\nFor future development of PBX, it is promising to utilize a wider range of mathematical tools, including the concept of perturbation theory developed in the analysis of mathematical models succeeded in physics and engineering. Moreover, some extension of this work is required for models built with machine learning algorithms other than deep learning."}]}