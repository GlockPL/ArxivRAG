{"title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics", "authors": ["Anique Tahir", "Lu Cheng", "Manuel Sandoval", "Yasin N. Silva", "Deborah L. Hall", "Huan Liu"], "abstract": "Social media discourse involves people from different backgrounds, beliefs, and motives. Thus, often such discourse can devolve into toxic interactions. Generative Models, such as Llama and ChatGPT, have recently exploded in popularity due to their capabilities in zero-shot question-answering. Because these models are increasingly being used to ask questions of social significance, a crucial research question is whether they can understand social media dynamics. This work provides a critical analysis regarding generative LLM's ability to understand language and dynamics in social contexts, particularly considering cyberbullying and anti-cyberbullying (posts aimed at reducing cyberbullying) interactions. Specifically, we compare and contrast the capabilities of different large language models (LLMs) to understand three key aspects of social dynamics: language, directionality, and the occurrence of bullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit promising results in some social media understanding tasks (understanding directionality), they presented mixed results in others (proper paraphrasing and bullying/anti-bullying detection). We also found that fine-tuning and prompt engineering mechanisms can have positive effects in some tasks. We believe that a understanding of LLM's capabilities is crucial to design future models that can be effectively used in social applications.", "sections": [{"title": "1 Introduction", "content": "In today's digitally connected world, social media platforms have become integral arenas for communication, allowing individuals from diverse backgrounds to engage in discussions, share ideas, and express their opinions. However, this increased connectivity has brought with it a range of challenges, including the emergence of toxic online behavior, cyberbullying, and the propagation of harmful content [2]. As the online landscape becomes more complex and dynamic, the need to understand and address these issues at scale becomes ever more pressing."}, {"title": "2 Related Work", "content": "There is considerable prior research that studies social dynamics [33]. While this paper aims to evaluate general LLMs capabilities in social contexts, we use cyberbullying and anti-bullying as case studies in our analysis."}, {"title": "2.1 LLMs in social media analysis", "content": "LLMs have been extensively used in prior work related to social media analysis. Transformer based models [38], such as BERT [10], encode words into tokens and learn attention weights signifying relationship between sequences. Although these models were originally developed for machine translation, they have been repurposed for classification tasks."}, {"title": "Sentiment Analysis and Emotion Detection", "content": "One prominent application of LLMs in social media analysis is sentiment analysis, which involves determining the emotional tone behind a piece of text. Prior researchers have utilized models like BERT and GPT-2 to gauge sentiment polarity in tweets [6,1]. Emotion detection has also been explored, with models being fine-tuned to recognize emotions such as joy, anger, and sadness [12,13]."}, {"title": "Irony and Sarcasm Detection", "content": "Identifying irony and sarcasm presents a unique challenge in online discourse due to the absence of vocal cues and facial expressions. LLMs, including ROBERTa, have been leveraged to detect instances of irony and sarcasm in tweets [27,44]. These models employ contextual understanding to differentiate between literal and figurative language, contributing to a deeper comprehension of social media interactions."}, {"title": "Hate Speech and Offensive Language Detection", "content": "The issue of hate speech and offensive language on social media platforms has spurred efforts to develop automated tools for detection and mitigation. Researchers have employed LLMs to create models capable of identifying hate speech [3,24] and offensive content [42,32,9]. The contextual awareness of LLMs aids in distinguishing between genuine expressions of opinion and harmful speech."}, {"title": "2.2 Improving Generative Language models: Fine-tuning vs. Prompt Engineering", "content": "For a few years now, standard practice has been to pretrain a model using abundantly available textual data, and then fine-tune the model to perform a more specific task using a new task-specific objective function and/or task-specific dataset. This approach gives the model an opportunity to learn the various general-purpose features of the pre-training data, and avoid biases that could result from being trained on a small dataset. In practice, the task-specific dataset can be difficult to collect, in short supply, or expensive to produce. Prompt engineering addresses the need for these datasets by having the model instead learn from prompts, a text template engineered specifically to get a better response from the language model. The drawback to prompt engineering is the complexity of identifying how to correctly structure the templates for each task the model will perform [22].\nThe expanding repertoire of LLM applications in social media analysis underscores their potential in unraveling the nuanced behaviors on these platforms. We aim to shed light on LLMs' strengths and areas in need of additional work to effectively understand intricate social dynamics."}, {"title": "3 Prompt Generation for Detecting and Explaining User behavior", "content": ""}, {"title": "3.1 Background", "content": "Many conversations in social media revolve around certain topics of interest. For instance, information on Reddit is structured in a hierarchical format where sub-reddits"}, {"title": "Input", "content": "The input dataset $D = {S_1, S_2,\\cdots, S_N}$ consists of N instances of social media sessions, where each instance $d_{ij} \\in S_i$ is as a sequence of text tokens. Each instance $d_{i}$ includes the $j$'th comment of session $S_i$ and the previous 1 to $j-1$ comments. This representation allows the model to rely on the context of a session."}, {"title": "Output", "content": "The objective is to produce the following output components:\n1. Behavior Identification (Quantitative): For each instance $d_{ij}$, let $b_{ij} \\in {0,1}$ represent a binary classification of the behavior as a cyberbully. Similarly, we let $m_{ij} \\in {0,1}$ represent a binary classification of behavior as anti-bullying.\n2. Behavior and Motivation Explanations (Qualitative): For each identified cyberbullying and anti-bullying comment, $b_{ij}, m_{ij} \\in S_i$ respectively, generate human-interpretable explanations that shed light on the behavior, actions, and motivations behind the comment.\nGenerative LLMs have demonstrated general purpose capabilities in question answering, surpassing humans in some cases [15]. Several methods focused on enhancing the output of these models have been proposed. For our task of social behavior analysis, we created a taxonomy of methods for enhancing responses. The most common approach for querying foundational LLMs is to use zero-shot prompting. However, because these models are trained on general data, they may fail to perform niche tasks. In addition, because natural language is ambiguous, the models may respond in a manner that is not expected for a specific use-case. For instance, a model trained as a 'comedy bot' might respond in a humorous style, whereas such a style would not be relevant across a broader range of applications.\nThere are several approaches to guide prompting for enhancing generation. The most common ones are chain-of-thought (CoT) prompting [41] and exemplar-based"}, {"title": "3.2 Prompt Generation", "content": "Because social media sessions have a specific structure, prompt design needs to take four major factors into consideration: (i) The task objective, i.e., identifying and explaining behavior; (ii) context, i.e., what is the topic of the subset of the conversation we are analysing; (iii) the classification instance $d_{ij}$, i.e., how can we differentiate the query objective from the context; and (iv) the response format, i.e., how do we want the LLM to structure its generated response.\nGiven that a social media conversation is sequential in nature, we use the tabular format such that various characteristics of the conversation are encoded alongside the comments, e.g., the post id, comment author, and a label specifying which comment is the target of classification. In addition, the instruct-tuning prompt format [36] allows the instruction and input to be structured together in a prompt query. Because we are interested in classification, we expect the LLM to return the output in a structured manner such that class labels, $b_{ij}$ and $m_{ij}$, can be extracted from the generated open-ended response. We employ templating of the procedural generation through constraints in the generation process. Fig. 1 shows one of the sample prompts used in our analysis. The first question, which asks for a summary of the conversation around the post of interest, acts as a CoT mechanism."}, {"title": "3.3 Fine-tuning", "content": "In addition to prompt and response engineering, we also studied the effect of fine-tuning on the veracity of the responses. For this work, each phase of the fine-tuning process is performed using the Low Rank Adaptation (LoRA) strategy [19]. This strategy allows us to target particular layers in the model for fine-tuning. Consider a weight matrix, $W_o \\in \\mathbb{R}^{m\\times n}$, representing the original weights. We use a new set of corresponding weights, $A \\in \\mathbb{R}^{m\\times r}$ and $B \\in \\mathbb{R}^{r\\times n}$, where $r << min(m,n)$. Our models' corresponding weight is now $W_o+AB$. Thus, backpropogating on A and B, we can fine-tune on a significantly smaller number of parameters.\nMachine learning models are vulnerable to mode collapse and catastrophic forgetting when training on a certain type of data. That is, training on structural understanding might make the model forget about previously learned knowledge in lieu of learning structure. To prevent such a hazard, we use a combination of task-specific data and general instruction data. For the instruction data, we make use of Alpaca [36] instructions, which are combined with the task data with a certain probability."}, {"title": "4 Do LLMs understand language in social context?", "content": "Large language models are pretrained mostly on formal and semi-formal text corpora. Given our motivation to analyze social media discourse, we compare the capa-"}, {"title": "4.1 Similarity Analysis", "content": "We use four main metrics to evaluate the quality of generations: (i) BLEU score, (ii) ROGUE score, (iii) Jaccard similarity, and, (iv) Semantic similarity. One of the quirks with generation is that in many cases, the LLMs reproduce the provided text verbatim. This leads to an increase in most of the similarity metrics. Thus, we also report the Levenshtein ratio for each strata. BLEU score measures the correspondence between the model's output and the reference paraphrase based on n-gram overlap. ROGUE score also evaluates the overlap of n-grams, but with a focus on recall, thus capturing the extent to which the reference n-grams are present in the generated text. Jaccard similarity assesses the similarity and diversity of the sets of n-grams between the generated and reference texts. More specifically, it is the ratio of the intersection and union of the n-grams in the two texts. Semantic similarity explores the likeness in meaning between the texts. In this instance, it is extracted from the encodings of the BERT (base) model."}, {"title": "4.2 Comparison of Social Understanding among LLMs", "content": "We compare different LLMs to understand their ability to comprehend social conversations. Specifically, GPT-2, Llama-2 7B/13B, and ChatGPT are used in our comprehension analysis. Some of the comments include language of toxic nature that ChatGPT refuses to answer due to ethical constraints. For some of our analysis, we used ChatGPT (GPT 3.5) generations near the timeframe of its release when the restrictions were comparatively lax. Unfortunately, current versions of ChatGPT do not allow most of these prompts. We compare generations with and without exemplars to gauge any improvement in understanding. For the social media content, we use a dateset of Instagram sessions [18]. The results are tabulated in Table. 1.\nCommon LLM mis-generations We identify three common types of generation mistakes made by the models: (i) verbatim generation; (ii) repetition of exemplars; and (iii) gibberish generation. When a model generates responses verbatim, the Levenshein distance is 0. In some cases, exemplars serve as a double-edged sword, as the model reproduces an example instead of producing novel paraphrasing. In many cases, LLMs produce gibberish unrelated to the query comment or the provided exemplars. In such cases, we observe a low similarity score and a high edit distance since the generated text is semantically dissimilar to the origin text and contains few common sub-sequences."}, {"title": "Significance of Exemplars", "content": "Adding exemplars to the prompt is a common technique to guide generations toward a specific goal. However, in our analysis, we only see a significant change in generation statistics for the 7B variant of Llama-2. For this case, the Levenshtein distance also decreases, hinting at verbatim repetitions of the input. These changes are evident in the distributions represented in the violin plots in Fig. 2."}, {"title": "Importance of Distributional Analysis", "content": "Although the semantic scores in Table. 1 appear to favor Llama-2 13B, even compared to ChatGPT, qualitative analysis and the distributional stratification in Fig. 2 better contextualises the tradeoffs between textual and semantic similarity. ChatGPT generations produce text that is consistently semantically similar (similarity \u2192 1) to the input, but avoids being verbatim repetition (Levenshtein ratio \u2192 0) or gibberish (Levenshtein ratio \u2192 1). Thus, ChatGPT shows a robust understanding of social context, while GPT-2 tends to produce gibberish. The Llama-2 based models tend to produce a mixture of repetition, valid responses, and gibberish. A possible explanation for this phenomenon could be the pre-training corpus, which includes mostly academic text in the case of Llama-2. The results also suggest that exemplars may be insufficient to improve language model understanding, as they only guide the generation process rather than improving the model's inherent understanding (encoded in the weights).\nFor social understanding, we conclude that even though common metrics such and BLEU and ROGUE allude to language understanding, we find problems with regurgitation and verbatim reproductions. We also find that multi-faceted evaluation paints a more nuanced picture."}, {"title": "5 Can language models understand directionality in social contexts?", "content": "Many social platforms enable their users to express whom they target with their comments. A majority of social media platforms provide some variation of the 'mention' functionality. In the context of this work, we are specifically interested in the ability of LLMs of follow the directionality pattern."}, {"title": "5.1 Language Models and Directionality Detection", "content": "The primary objective of training an LLM for next-word prediction is to learn the conditional probability distribution of the next word given the context of preceding words. Formally, let $W_1, W_2, ..., W_{n-1}$ be a sequence of $n-1$ words, and $w_n$ be the target word to predict. The goal is to learn the probability $P(W_n|W_1,W_2,..., W_{n\u22121})$, which captures the likelihood of different words occurring next in the sequence. To train LLMs, large text corpora are used as datasets. These corpora contain a wide variety of sentences and documents, providing diverse linguistic contexts for the model to learn from. Common sources of data include books, articles, websites, and other textual sources."}, {"title": "LLMs capabilities under fine-tuning", "content": "Because prompt tuning is insufficient for models to develop social understanding, as noted in Section 4, we use a fine-tuning process consisting of two phases to add knowledge to the LLM (illustrated in Fig. 3). Both phases of fine-tuning use Parameter Efficient Fine-Tuning (PEFT)5. In our problem setting, rather than just predicting the next words, we aim to gain an understanding of the relation between different comments. For instance, a comment in a session may target the previous comment, the original post that spawned the session, or some comment in the middle of the discourse. To glean insight into the target of the comment in terms of its context, reasoning about the structure of the conversation is critical. Unfortunately, the LLM pre-training does not consider these relationships specifically and there is no public data related to reasoning at the comment level in social media discourse. Thus, we rely on other general purpose structured data to act as a surrogate to learn structure and reasoning. We use the WikiTableQuestions [30] dataset to infuse structural intelligence into the model. This dataset consists of a large variety of independent tables, questions based on one of the tables, and a corresponding answer. To answer a question, it is vital to use the data in the table.\nPhase 2 of the PEFT process aims to improve the social understanding of underprivileged LLMs. ChatGPT shows improved social language understanding compared to competitors6. Even though, the advantage of ChatGPT may be attributed to a more generalized and customized training corpus, this phase of PEFT seeks to study the transfer learning abilities of competitor models in the confines of our problem setting."}, {"title": "5.2 Can LLMs emergent abilities improve directional understanding?", "content": "For the directionality analysis task, we utilized a corpus of 4chan threads [29] and efficient fine-tuning using JORA [34]. Since 4chan permits its users to tag the individuals to whom they are replying, we employ this data as the ground truth for directionality information. Our objective is to determine whether our designed PEFT phases (i) enhance the model's ability to identify the post being targeted for behavior comprehension, and (ii) improve the model's capacity to discern the individual being targeted by the poster. Given that 4chan users can reference multiple comments as the target of their replies,"}, {"title": "6 Can LLM's detect instances of Cyberbullying and Anti-Bullying?", "content": "To further validate our findings with respect to understanding social context and to analyze the veracity of behaviour classification as cyberbullying or anti-bullying, we design our experiment on a dataset of Instagram sessions [16] which tests both language and directional understanding. We use 100 labeled sessions. Each session consists of a post and a set of comments. This dataset serves two purposes: (i) Phase 2 of our PEFT"}, {"title": "7 Discussion", "content": "Making inferences on social data is a multi-faceted problem. In this work, we study three dimensions relevant to exploiting LLMs for social behaviour analysis. Foundational models pretrained on large corpora of text, such as Llama, have enabled a larger audience to enjoy their benefits by tuning them for specific use-cases. First, we compare the ability of different models to understand social context by measuring how well they"}, {"title": "8 Conclusion, Limitations, and Future Work", "content": "LLMs have seen a spike in popularity recently due to their astounding ability in zero-shot response generation for general-purpose prompts. There have been various prior studies that either question [37,28] or praise [20,7] their abilities in different domains. In this work, we focus on the subdomain of social analysis through the lens of social media discourse. More specifically, we considered the detection of cyberbullying and anti-bullying behavior. We highlight strengths and limitations throughout a taxonomy of learning in language models through prompt engineering, fine-tuning, and constrained generation. We apply relevant parts of this taxonomy to answer research questions related to social context understanding, directionality understanding, and identification of cyberbullying and anti-bullying activity, which requires both social and directional understanding. We believe future breakthroughs in this domain will require larger coded social datasets as well as new model features that better capture the complex nature of social interaction semantics."}]}