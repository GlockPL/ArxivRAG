{"title": "MiceBoneChallenge: Micro-CT public dataset and six solutions for automatic growth plate detection in micro-CT mice bone scans", "authors": ["Nikolay Burlutskiy", "Mehdi Boroumand", "Yeman Brhane Hagos", "Paula Sawczuk", "Marija Kekic", "Julia Jurkowska", "Borjan Venovski", "Roksana Malinowska-Traczyk", "Karlo Pintaric", "Jordi de la Torre", "Philipp Plewa", "Maria Chiara Biagi", "Yibo Wang", "Fariba Yousefi", "Jacek Zalewski", "Leif Hultin", "AstraZeneca R&D"], "abstract": "Detecting and quantifying bone changes in micro-CT scans of rodents is a common task in preclinical drug development studies. However, this task is manual, time-consuming and subject to inter- and intra-observer variability. In 2024, Anonymous Company organized an internal challenge to develop models for automatic bone quantification. We prepared and annotated a high-quality dataset of 3D \u00b5CT bone scans from 83 mice. The challenge attracted over 80 AI scientists from around the globe who formed 23 teams. The participants were tasked with developing a solution to identify the plane where the bone growth happens, which is essential for fully automatic segmentation of trabecular bone. As a result, six computer vision solutions were developed that can accurately identify the location of the growth plate plane. The solutions achieved the mean absolute error of 1.91 \u00b1 0.87 planes from the ground truth on the test set, an accuracy level acceptable for practical use by a radiologist. The annotated 3D scans dataset along with the six solutions and source code, is being made public, providing researchers with opportunities to develop and benchmark their own approaches. The code, trained models, and the data will be shared (link in supplementary material).", "sections": [{"title": "1. Introduction", "content": "Throughout the lifespan of vertebrates, bone tissue undergoes continuous and active remodeling, regulated by a complex interplay between two essential cell types: osteoblasts, responsible for bone formation, and osteoclasts, which handle bone resorption. This dynamic equilibrium is strongly influenced by the endocrine and immune systems, creating a finely tuned process crucial to bone health [4].\nAssessing bone morphology and bone mass is valuable not only for understanding and diagnosing bone diseases such as arthritis, osteoporosis, and osteoblastomas but also for evaluating potential bone safety concerns associated with new therapeutic drugs. Preclinical studies in small animals play a critical role in examining these disease mechanisms and assessing drug safety before clinical trials in humans.\nMicro-computed tomography (\u00b5CT) is a widely used technique for assessing bone remodeling effects in both clinical and preclinical specimens. It enables the acquisition of high-resolution 3D images of bone microarchitecture, allowing for the quantification of key structural parameters. Furthermore, by calibrating the scanner with hydroxyapatite of known density, image intensity can be accurately correlated with bone mineral density.\nThe assessment involves scanning dissected bones, followed by the manual identification and quantification of bone growth regions. These techniques are widely applied in drug development. Consequently, protocols and parameters have been established for \u00b5CT analyses in mice, specifically for long bones, vertebrae, and palms in aging mice [24].\nTypical endpoints of \u00b5CT analysis include trabecular bone volume fraction, trabecular number, thickness and separation, as well as cortical cross-sectional area, area fraction, and thickness [1]. These parameters are measured within a defined volume of interest (VOI), specified based on its distance from a distinct anatomical landmark, such as the epiphyseal growth plate of the femur or the tibial metaphysis [25].\nWithin the VOI, cortical and trabecular bone regions are identified and segmented independently. Traditionally, separating these two bone regions has been a manual task, achieved by contouring the outer trabecular perimeter. However, these manual tasks are time-consuming and susceptible to inter- and intra-observer variability. Recent advances in AI have allowed researchers to develop and automate this process, leading to more efficient analysis of bone remodeling. For instance, automated or semi-automated solutions based on dual-thresholding [1], atlas-based methods [19], and deep learning approaches [16, 18] have been implemented in recent years.\nHowever, the selection of the VOI remains a manual and time-consuming task, prone to inter-operator variability. Additionally, only a limited number of preclinical bone \u00b5CT datasets are publicly available at resolutions suitable for bone remodeling analysis. In [23] the authors provide a full skeleton segmentation dataset of 228 mice at a resolution of 28 \u00b5m, while [22] offer a dataset with 21 scans of the proximal tibiae of wild-type mice at 5.06 \u00b5m pixel size, and [28] has made available scans of 19 female C57BL/6 mice lumbar vertebrae at a resolution of 2.5 \u00b5m.\nSeveral studies have developed AI models to quantify changes in skeletal bones. For instance, in [20], the authors introduced a model using active shape models and random forest regression for automated bone age assessment. These early, pre-deep learning models required extensive feature engineering and struggled to capture the complex non-linear relationships within the data. The advent of deep learning has allowed researchers to bypass such feature engineering; for example, in [5], the authors employed convolutional neural networks (CNNs) to predict bone age from pediatric hand radiographs. This model achieved performance levels comparable to human experts, underscoring the promise of deep learning approaches. Another similar model for bone age assessment using deep learning was developed in [26].\nHowever, there has been limited research on the automatic quantification of bone remodeling in animals, such as rats or mice, despite its potential to significantly accelerate preclinical drug development.\nFortunately, an increasing number of public datasets, including radiology data, have become available to accelerate Al development in this field. However, compared to human datasets, there are only a few annotated animal datasets. For bone remodeling quantification, there are currently no public datasets available and no public or commercial models for automatic bone growth identification and quantification. To address this gap, we organized an internal challenge at Anonymous Company, which ultimately led to the development of six high-performing deep learning models capable of accelerating preclinical drug development.\nIn this work, we tackle the challenge of automating VOI selection by developing and comparing six state-of-the-art deep learning methods for growth plate detection in distal femur scans of mice. Additionally, we prepared, annotated, and publicly shared a high-quality 3D \u00b5CT dataset of mice bone scans, including annotations for trabecular bone volume and the growth plate cortical plane.\nThe main technical challenge when working with high-resolution 3D data is its dimensionality. While the full 3D structure can be analyzed using techniques such as 3D convolutions, this approach is often computationally infeasible. Processing 2D slices offers a more stable model, though it sacrifices information across slices. To address this, several pseudo-3D (or 2.5D) methods have been proposed, typically involving the stacking of multiple 2D slices either before applying convolutions or by stacking the outputs from convolutional layers later in the architecture [10]. A review of popular 2D, 3D, and 2.5D methods can be found in [14, 29].\nThe main contributions of this paper are:\n\u2022 We prepared, annotated, and publicly shared a high-quality 3D dataset of \u00b5CT mouse bone scans;\n\u2022 We developed and compared six state-of-the-art deep learning methods for the automatic identification of the growth plate plane in the bone scans;\n\u2022 We publicly shared all code and the models for benchmarking and further development."}, {"title": "2. Motivation: why the challenge was set up", "content": "Bone growth quantification is an integral part of the safety assessment in drug development. Clinical research organizations and pharmaceutical companies routinely run thousands of preclinical studies per year where bone quantification is involved. Each study may include hundreds of animal bones for quantification. Currently, the quantification process has many manual steps, including visual exploration of each bone, the identification of the growth plate, and then the quantification of its parameters. Finally, the domain expert needs to segment out the growth area at the growth plate and quantify the area. This process takes approximately five minutes per bone and can take several days for a whole bone quantification study. An automated process will not only eliminate the manual annotation step but will also eliminate inter- and intra-observer variability. Minimizing observer variability is important since even relatively small drug-induced changes of \u00b1 10% in bone volume or bone density can have a significant impact on the progression of therapies. Finally, the automated approach can be easily scaled, allowing one to run several studies in parallel."}, {"title": "3. Challenge set up", "content": "The end-to-end process was split into two tasks for the challenge: the first was to identify the growth plate, and the second was to segment and quantify the bone at the growth plate. The second step is quite straightforward even with simple U-net like architectures, but the first task is quite challenging, so the paper covers six independent solutions that were developed by six Anonymous Company teams to solve the task."}, {"title": "3.1. Growth plate plane (GPP) definition", "content": "In the bone analyses, dissected femur bones from mice are scanned at a high resolution (10 \u00d7 10 \u00d7 10\u00b5m) and reconstructed as CT volume images, with voxel intensity given in Hounsfield units (HU). The primary output of the analysis is trabecular bone density and volume, calculated in a volume of interest (VOI). This VOI is a volume of 1 mm length, starting 0.3 mm distal to the growth plate plane (GPP) of the femur.\nThe GPP that determines the VOI to quantify, is defined as the lowest plane of the growth plate. The blue areas in Figure 2 depict the growth plate. The GPP is highlighted in red in Figure 2, it is the axial plane in which all four protrusions (blue oval areas in the axial plane) at the head of the femur merge (the axial slice is outlined in red, corresponding to the red arrow).\nCurrently, the GPP is identified visually by the image analysts, comparing the consecutive slices along the bone main axis. Once the GPP is identified, trabecular areas (the green areas in the cyan axial plane in Figure 2) are manually outlined in the axial slices in the 1 mm long volume, starting 0.3 mm below the GPP, and the 3D trabecular ROI is outputted."}, {"title": "3.2. Data: \u00b5CT scanning and 3D reconstruction", "content": "To set up the challenge, a \u00b5CT dataset was prepared, annotated, curated, and divided into training and test sets before being shared with the challenge participants.\nData acquisition and preparation. Mice femur were dissected, fixed in formalin for 48 hours, and then stored in 70% ethanol. The bones, in batches of four per sample, were fixed with wax mold within a 3D printed holder and wrapped in ethanol soaked cloth before undergoing \u00b5CT scanning. Scanning was performed at a 10 \u03bcm resolution using a SCANCO vivaCT-40 system (SCANCO Medical AG, Fabrikweg, Switzerland; 55 peak kilovoltage and 145-\u03bc\u0391 X-ray source). The 3D reconstruction of the scanned \u00b5CT images were completed using the Scanco scan software and exported as DICOM files. The resulting DICOM images, with dimensions of 2048 \u00d7 2048 \u00d7 642 and 8-bit pixels format, were further cropped into per-bone volumes. Each DICOM scan produced four individual volumes, approximately sized 400 \u00d7 400 \u00d7 642. The prepared dataset comprised images from three preclinical studies, including 83 mice, with one image per mouse, totaling 83 \u00b5CT images.\nAnnotations. Once the imaging datasets was acquired, the scans were annotated by a domain expert with 20 years of experience of conducting in-vivo \u00b5CT scanning and quantifying the images for preclinical studies. The annotation process involved two steps. First, identifying the growth plate in the 3D image and then annotating trabecular bones in a corresponding VOI (see Figure 2 and the previous subsection for more explanations). The annotated GPP index (GPPI) is then served as a reference plane for the challenge described in this paper. Finally, a quality control of the annotations and the data were performed by another domain expert.\nTraining and test split. The annotated dataset was split into training and test sets, stratified by the preclinical studies. The training set had 70 3D images and the test set had 13 3D images. The training set was shared with the teams for model development and then the test set was used to calculate performance metrics for the developed models and finally to rank the teams and to determine the winners."}, {"title": "3.3. Challenge logistics and evaluation", "content": "The challenge attracted more than 80 AI scientists and engineers who formed 23 teams. The teams received the problem statement, the training data as well as got connected to domain experts who had provided guidance regarding bone anatomy, data acquisition process, as well as guidance for computer vision algorithms and deep learning tools. Then after 4 months the teams received the test dataset and were asked to provide predictions of the GPPI for each of 13 3D bone scans. Finally, six teams provided solutions for the challenge of automatic GPPI detection. Then the submitted models were used to run inference on the test set followed by calculating a mean score, standard deviation, and the sum of scores for each team. The score for each bone was calculated using an evaluation function (more details in subsection 3.3.1) to penalize GPPI predictions far from the ground truth. Then the teams were ranked based on the sum of the scores and the top 3 teams were awarded with prizes."}, {"title": "3.3.1 Evaluation Function", "content": "To assess model performance, we employed a scaled survival function based on the standard normal distribution. The evaluation function is defined as:\n$Score = 2 \\times \\Phi(\\frac{p-t}{3})$"}, {"title": "3.4. Challenge restrictions", "content": "For the challenge, an AWS environment with access to GPU enabled instances was set up for the participants. The organizers prepared pre-processing and evaluation scripts to facilitate a quick onboarding for the challenge. The participants had the freedom to choose any model architecture, provided that the training would be possible to run on 1 GPU NVIDIA T4 with 16GB of memory. Ensembling the models was allowed with no restrictions on the number of the models in the ensemble. The participants were allowed to use any pre-trained models if they could find any in the public domain, same freedom was provided for using any external datasets. Communication and knowledge sharing between the teams were allowed so the teams could share ideas and code, for example, code for pre-processing that facilitated collaboration. No restrictions on software, frameworks, programming language, software libraries were imposed. The organizers reserved the right to disqualify the participants if the participants did not follow the challenge's rules."}, {"title": "4. Submitted Solutions", "content": "An overview of the six solutions from six teams submitted for evaluation, is summarized in Figure 3. The teams were SafetyNNet, Matterhorn, CodeWarriors2, Exploding Kittens, Subvisible, and ByteMeIfYouCan. Later in the paper these teams are referred as Teams SN, MH, CW, EK, SV, and BM accordingly.\nNext, we outline data pre-processing, modeling approaches, data augmentation, and finally cross validation and ensembling approaches the teams employed in their solutions. More details for each approach are in Supplementary material."}, {"title": "4.1. Data Pre-processing", "content": "In the data pre-processing phase, images underwent clipping, normalization, cropping of input voxel values, and resizing to enable batching of the images. Several teams also scaled the images. Finally, for modeling, the teams used 3D, 2.5D and 2D images. Team SN used 3D voxels as input for the modeling, MH and EK used 2.5D images - 2D images enriched with partial information from the longest axis. CW, SV, and BM used 2D input using an independence assumption for the 2D images across the longest axis. The pre-processing details for each team are in Table 2.\n\u2022 The majority of teams applied clipping to HU values using ranges from - 1000 to 500 for the low range and from 1900 to 4000 for the high range. This clipping of HU scale reduced image noise, focusing specifically on bone density.\n\u2022 Due to variations in the axial (xy) dimensions of the images, all the teams utilized cropping and then interpolation techniques to resize the images to the desired dimensions, depending on the deep learning network backbone used."}, {"title": "4.2. Modeling Approaches", "content": "All six solutions can be divided into three groups, each group representing the approach chosen, namely i) 3D CNN regression approach utlizing full 3D volume data (Team SN), ii) 2.5D long axis regression approach from an appropriate 2D sagittal or coronal view (Teams MH, EK), and finally iii) 2D/2.5D axial plane classification approach (Teams CW, SV, BM).\nAll 2.5D approaches used 3rd dimension as channels, effectively applying 2D convolutions across axial plane and fully connected layer across the 3rd dimension."}, {"title": "4.2.1 3D CNN Regression Approach", "content": "The utilization of full 3D information provides the most comprehensive data for the model to derive insights. However, due to high dimensionality, only crops of the full volume could be processed on the GPU.\nTeam SN opted for a 3D convolutions using a sliding 3D window approach along the long axis of the bone. They used ResNet34 for feature extraction and a decoupled head, similar to object detection pipelines [7]. The two outputs of the two heads were a probability of whether the GPPI was contained in the crop and a regression offset of the GPPI from the beginning of the crop, scaled to the crop length. The loss function was a weighted sum of the two terms, comprising Sigmoid Focal Loss for classification and Mean Squared Error for regression, with the second term being masked in cases where the GPPI was not within the crop. During prediction, a window with the maximal classification prediction was selected, and the exact GPPI was calculated using the regression offset prediction.\nThe performance across the five folds for SN and three other teams who used cross validation for model selection is summarized in Table 3."}, {"title": "4.2.2 2.5D Long Axis Regression", "content": "GPPI can be inferred from an appropriate 2D view of a long axis (sagittal or coronal). To ensure the correct long axis view is contained in the input images, one can utilize stacking of various slices across the channel dimension. Two teams, MH and EK, utilzed such stacking approach. The final output of the network is a regression coefficient offset of GPPI from the image beginning, scaled to the image length to produce a label in the 0 \u2013 1 range.\nTeam MH used sorted random slices from the sagittal plane, chosen from an internal portion of the dimension. This approach maximizes the probability of selecting locations containing useful bone information and avoids slices that contain only a small portion of the bone volume. The backbone was based on EfficientNet B3 with modified input channel dimensions. Additionally, the team applied a dual training process, passing images in both low resolution (containing the full length of the bone) and high resolution (containing only the crop around the GPPI). The inference process then consists of two evaluations: an initial interpolated prediction to broadly identify the growth plane's location, followed by a more precise cropped prediction within the boundaries established in the first step."}, {"title": "4.2.3 2D/2.5D Axial Plane Classification", "content": "In principle, binary classification across the axial plane can be used to classify GPPI versus other planes. However, subsequent planes would be too similar, making naive classification ineffective.\nTwo teams, CW, SV, solved the challenge by redefining the classification task, taking notice that slices before and after the GPPI are distinguishable. They redefined the classification into pre-GPPI and post-GPPI. The main challenge with this approach was to denoise the possibly inconsistent output of the classifier, especially for adjacent to the GPPI planes.\nTeam CW, utilized morphological closing filters with kernel size 5 \u00d7 1 to denoise the sequence of outputs from the binary classifier, thus making the distinction between the 'before' and 'after' classes explicit. The last slice predicted as 'before' was used as the prediction of GPPI. The CNN backbone was a pre-trained ResNet18 [8] with an adapted architecture to accept grayscale input and output binary classification.\nTeam SV, opted for a two-stage training approach using two lightweight CNN networks, one for classification and one for regression, each with less than 40k total parameters. The solution is based on the observation that axial plane images display four distinct blobs, each corresponding to a protrusion, prior to the GPP, and these blobs merge and disappear after the GPP. A binary classifier was trained to identify images with these four blobs, marking the last image in the positive series as a rough estimate of the GPP. To refine these estimates, the team leveraged cross plane information, employing a regression CNN to examine 25 slices before and after the initial estimate, forming a stack of 51 images. This stack was used to generate more accurate predictions of the GPP, combining initial binary classification with regression-based refinement for better precision. These two low-parameter models combined with reducing the input size to 96 \u00d7 96 keep the computational cost of this approach low and make it very fast to train. Table 3 shows the performance of this approach on the five-fold cross-validation. The mean and standard deviation for 10 runs (training from scratch) on a single fold, specifically fold 5, were 0.517 and 0.036, respectively.\nFinally, Team BM used sliding window approach similar to Team SN but instead of full 3D convolution utilized stacking across channels and applying 2D convolutions. Team BM passed each 3D crop through a custom DenseNet backbone to predict a value P = 0...1, where P= 0 indicated that the GPPI was not contained in the window, P = 1 indicated that it was located at the center, and P varied linearly with the distance from the GPPI. Binary Cross Entropy Loss was used to minimize the difference between the predicted and true P target. During inference, a sliding window approach was used, where the window with the maximum value of P was selected as the one with the centered GPPI."}, {"title": "4.3. Data Augmentation", "content": "The standard data augmentation techniques included random flipping, blurring, noise addition, motion simulation, spike introduction, and ghost artifact generation. Additionally, Team MH implemented axial plane displacement augmentation to ensure a uniform distribution of the GPPI across the image when passing full-resolution images through the network."}, {"title": "4.4. Cross validation and ensembling", "content": "Due to the limited amount of data, four teams including SN, MH, SV, and BM employed five-fold cross-validation, see Table 3 for detailed results. Also, all teams except CW applied ensembling strategy for their final predictions. The teams used ensambles of up to five models to run inferences and then averaged the predictions, more details on the final models used for the inference are in Table 4."}, {"title": "4.5. Code, Models, Data and Annotations", "content": "All the code and the models for each team as well as detailed instructions how the models were trained and how to run inference are in the supplementary material.\nWe share the data with the annotated GPPIs and the pixel wise segmented bone annotations. A link to the data is in the supplementary material, along with a link to the code and the trained models."}, {"title": "5. Results and Evaluation", "content": "For evaluation, we utilized both qualitative and quantitative measurements. The qualitative assessment included comparing the predicted GPPIs and True GPPI visually (see Figure 4) whereas the quantitative measurements included the differences in the number of planes between the predicted GPPI and the True GPPI (see Table 5), as well as the scores using the survival function (see Table 6). We also compared the approaches in terms of complexity, number of parameters, and computational cost (see Table 4)."}, {"title": "5.1. Test Set Evaluation", "content": "First, each team ran inference on the test set of 13 3D \u00b5CT bone scans and predicted the GPPI for each bone in the test set. The differences between the predicted GPPIs and the ground truth for each bone of the test set by the teams are presented in Table 5. The positive numbers in the table indicates that a team predicted the plane above the GPP and below the GPP for the negative numbers. The teams achieved the mean absolute error (MAE) ranging from 3.62 for EK down to 1.23 for BM with the mean MAE of 1.91\u00b10.87 that is acceptable performance for practical use by a radiologist.\nFor ranking the teams in the challenge, the evaluation function 1 was applied to penalize predictions far from the ground truth. Then the mean score and the standard deviation for the whole test set were calculated. The test set performance metrics include the mean score, the standard deviation of the score (STD score), and the sum of the scores that is 1 for a perfect prediction per bone and has a maximum of 13 for the whole test set of 13 bones. The sum of the scores was used for ranking the teams and determine the winner. Information on the final results, including the mean scores, standard deviation, sum of the scores are in Table 6 and information on the complexity of the models, their ensembles, the number of inferences and the number of model parameters are in Table 4.\nExamples of the True GPPI and the predictions by all the six teams for three bones, 5dd1c0c131, f27da128ab and 64d33d4c9c are in Figure 4. One can notice that the differences between the True GPPI and the predicted GPPIs that are adjacent growth plate planes are subtle and distinguishable only by experienced radiologists."}, {"title": "6. Discussions", "content": "The results from each team were sufficiently accurate for practical use by Anonymous Company's internal domain experts. However, to establish precise acceptable error rates, study-dependent endpoints obtained from both manual and automatic region-of-interest (ROI) segmentations need to be compared. Therefore, until the model demonstrates a high enough level of trustworthiness to be fully automated, a user experience (UX) design should incorporate an efficient expert-in-the-loop system, enabling experts to conduct quality control and make corrections when necessary. Implementing such a system, for instance, within Slicer3D software 1, would require not only adherence to acceptable performance metrics but also reasonable inference time per image and compatibility with CPU-only machines, to minimize data transfer delays. Utilizing full 3D convolutions (Team SN) did not yield accuracy improvements over the computationally simpler approach of stacking across axial plane and applying 2D convolutions (Teams CW, SV, and BM). Among the proposed methods, the long-axis regression approaches, which require only one to two inference passes per image, proved to be the fastest solutions. However, these approaches rely on selecting the 'correct' view along the long axis, which may limit their accuracy. Potentially, a hybrid approach of combining the long-axis regression approach for the rough GPPI region with the 2.5D approaches across axial plane for finer estimation, could offer the optimal balance of speed and accuracy.\nWhile this paper primarily addresses the challenge of identifying the GPPI, other studies have highlighted additional characteristics of the GPP itself, such as the width of the growth plate [17], which can only be obtained by segmenting the the GPP region. Additionally, this segmentation could offer valuable insights into instances of incorrectly predicted GPPIs. Future research will explore the feasibility of extracting such segmentation details from GPPI prompts, potentially enhancing model performance and interpretability."}, {"title": "7. Conclusions", "content": "We prepared a comprehensive, high-quality 3D \u00b5CT imaging dataset with 83 mouse bones, including growth plate plane annotations, and thorough quality curation of the dataset. Following this, we organized an internal Anonymous Company challenge that led to the development of six unique solutions utilizing different data pre-processing methods, 3D CNN regression, 2.5D long axis regression, and 2D axial plane classification modeling approaches. The proposed solutions achieved performance acceptable for practice use by radiologists.\nWe are now sharing this valuable dataset along with its annotations, the developed models, and the code. One of the primary challenges in medical imaging lies in the substantial volume of data, rendering many existing models, which are trained on natural images, unsuitable for use in medical applications. Our aim is that, by providing this dataset and a range of solutions, more specialized models and pipelines can be developed to accelerate pre-clinical drug development.\nRecently, there have been large interest in building foundation AI models that require diverse high quality datasets like we are sharing. Ultimately, these AI models have the potential to enable drug developers to bring new medicines to patients more quickly, contributing to a significant positive impact on patient care and outcomes as well as minimize animal use in the research contributing to animal well-being."}, {"title": "9. Code, data, and models are shared", "content": "The source code, annotated training and test data, and models are shared for review at anonymized link. If the paper is accepted then the code, models for all six teams, and all the 83 annotated 3D \u00b5CT images will be publicly shared."}, {"title": "10. Additional details on approaches", "content": "More details on the approaches chosen by six teams in the challenge including training and inference parameters."}, {"title": "10.1. Team SN: SafetyNNet", "content": "Input images are first downsampled and padded to a size of 321 x 244 \u00d7 244. From these, a crop of size 32 \u00d7 244 \u00d7 244 is selected. Pixel values are clipped within the range [-1000, 4000] and then normalized to a 0 \u2013 1 range. These crops are then passed through a 3D ResNet34 backbone, initialized with pretrained weights from [3]. The resulting 3D features are processed through a MaxPool layer across the H x W dimensions and then flattened across the D dimension to obtain a feature vector of size 2048. A dropout layer is applied after flattening to prevent overfitting.\nA decoupled head, consisting of two fully connected (FC) layers, is utilized to predict objectiveness and the growth plate plane (GPP) offset from the start of the crop. The loss function is constructed as a sum of:\n\u2022 Sigmoid Focal Loss: Applied where the target is positive only if the GPP is contained within the crop.\n\u2022 Mean Squared Error (MSE): Computed between the true growth plate plane index (GPPI) offset from the start of the window and the predicted offset (after passing the regression head prediction through a sigmoid function to squash it into the 0 1 range). This term is included only if the GP index lies within the selected window. To ensure effective learning of both tasks, the MSE loss is multiplied by a factor \u5165, which we set to 6.\nThe overview of the training procedure applied by SN team is given in Figure 5. During training, crops are randomly selected to ensure balanced batches containing samples with and without growth plates. Images are augmented using the torchio library [21] with random flipping, blur, noise, motion, spikes, and ghost artifacts. Adam optimizer with exponentially decaying learning rate (0.001 with decay of y = 0.98) and 10 warm-up epochs is used. Early stopping after 100 epoch without improvement in validation performance is applied. Training is repeated five"}, {"title": "10.2. Team MH: Matterhorn", "content": "We designed a convolutional neural network (CNN) architecture [13] based on EfficientNet [27] to predict the GPPI of mice femur bones using \u03bcCT scans [12] (see Figure 6)."}, {"title": "10.3. Team EK: Exploding Kittens", "content": "To predict the GPPI, we used a convolutional neural network (CNN) with a 2.5D approach. From each bone scan, 14 different sagittal and coronal slices were taken, starting at the center (xo, yo) and offsetting further slices by \u00b15. Slices were then combined to create 3-channel images, the first matching the sagittal slice, the second the coronal slice, and the third the blend of the two. This resulted in 49 total images per scan to be used as input in a 2D CNN.\nInput images were then resized to H\u00d7W = 515\u00d7515 by cropping the diaphysis side and zero padding the other sides if necessary. A \u00b5CT bone window was applied by clipping HU to remove soft tissue and noise W = 2000, L = 500. Finally, pixel values were scaled and images saved as 8-bit PNG using a naming convention to note image ID, offset from center, and GPPI."}, {"title": "10.3.2 Model, Augmentation, and Parameters", "content": "For our model we used an EfficientNet-es-pruned backbone, initialized with pretrained weights from the [6] dataset. The head was set to a single output neuron resulting in a real number and a sigmoidal layer for 0 - 1 scaling. To find the predicted growth plate plane, the model output is multiplied by the respective image size and rounded to the closest integer.\nModel training was done after unfreezing all layers with the following parameters:\n\u2022 Optimizer: AdamW\n\u2022 Max learning rate: 1 \u00d7 10-3\n\u2022 Weight decay: 1 \u00d7 10-2\n\u2022 Learning rate scheduler: One cycle (see Figure 9)\n\u2022 Loss: RMSE\n\u2022 Epochs: 12\n\u2022 Batch size: 32\nWhen training the model, images were augmented to improve generalization by incorporating: RGB shift, random brightness and contrast, horizontal flip, and normalization [6]. This was done using Albumentations [2], an open source image augmentation library. Finally, to ensure proper sampling, four-fold cross-validation was run on the center slices of the images."}, {"title": "10.4. Team CW: CodeWarriors2", "content": "To detect GPPI of the bone", "8": "was utilized. The model was fine-tuned on the provided mice bone 3D imaging data. The process of determining GPPI is presented in Figure 11. The process consists of three steps:\n1. Performing binary classification of all axial plane bone slices that were assigned to each image one of the two classes: before GPPI and after the GPPI.\n2. Applying morphological closing filters to denoise the sequence of outputs of the binary classifier to make the junction between the 'before' class and the 'after' class explicit.\n3. Assigning an index of the last image that is classified as 'before' as the GPPI.\nGiven that the train set images had different shapes, their size was reduced along x and y axes, this resizing not only ensured input consistency, but also lessen amount of background noise. To achieve this, each axial plane was masked using HU range corresponding to bone tissue as logical condition. Then normalized sum was calculated (the sum of 1s corresponding to the bone values divided by the area) per each slice index for each \u00b5CT scan. First and last indexes for which this value was above threshold set to 0.03 (meaning the bone values were on 3% of the slice area) were considered cutoff coordinates.\nResNet18 [8", "6": "."}]}