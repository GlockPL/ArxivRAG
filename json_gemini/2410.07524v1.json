{"title": "Upcycling Large Language Models into Mixture of Experts", "authors": ["Ethan He", "Abhinav Khattar", "Ryan Prenger", "Vijay Korthikanti", "Zijie Yan", "Tong Liu", "Shiqing Fan", "Ashwath Aithal", "Mohammad Shoeybi", "Bryan Catanzaro"], "abstract": "Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel \"virtual group\" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models.", "sections": [{"title": "Introduction", "content": "Sparse Mixture of Experts (MoE) models [1] are becoming increasingly popular [2, 3, 4, 5, 6, 7] since they can help achieve better accuracy without a commensurate increase in model training compute. Most recently, state-of-the-art LLMs like Grok-1\u00b9, DBRX2, Phi-3.53, Mixtral 8x22B [3], DeepSeek-V2 [8] and Qwen2 [9] are MoE models. However, an immense amount of compute has been spent on pre-training dense LLMs with only one MLP layer (one expert) [10, 11, 12, 13, 14]. These existing dense models may be able to achieve better accuracy for the same compute cost if they had access to more parameters through MoEs. Upcycling pre-trained dense language models into sparse mixture-of-experts models (referred to as simply \u2018upcycling' in this work) has emerged as an efficient approach to increase model capacity without the need for training from scratch [15, 16, 17, 3]. By leveraging the knowledge captured in existing dense checkpoints, upcycling enables the creation of large-scale mixture-of-experts (MoE) models while reducing the computational cost and time required for training.\nMost previous work on upcycling either does not provide details into how their models were up- cycled [3], or provides experiments only at a small scale [15]. We also find that the recommendations in [9] lead to sub-optimal models. To improve general knowledge on upcycling methods, we therefore publish this study of upcycling methods and best practices for billion-parameter scale language models. In this work, we conduct an extensive study of upcycling techniques and hyperparameters. Our contributions are as follows:\n1. We recommend training recipes to consistently upcycle billion-parameter scale LLMs.\n2. We perform a comprehensive study to find the best hyperparameters for upcycling including learning rate, batch size, and load balancing loss."}, {"title": "Methodology", "content": null}, {"title": "Sparse Mixture of Experts", "content": "In this work, we only investigate MoEs on the MLP layer of the transformer. These layers comprise the majority of compute and treat each token individually, avoiding issues with kv-cache consistency. A routing layer routes the tokens to a subset of multiple possible MLP layers. This increases the parameter count and presumably the model capacity without necessarily increasing the amount of compute required (measured in total training FLOPs)."}, {"title": "Upcycling", "content": "Upcycling is the approach of converting a trained dense model into an MoE [15]. The most obvious way to convert a dense model into an MoE model without losing accuracy is to duplicate the MLP layer weights multiple times and use a randomized router, weighting the output of MLP layers by their probabilities\n$E_1(x) := E_2(x)... := E_n(x) := FFN(x)$\n$MoE\\_activation = \\sum_{i=1}^{T} P_i \\times E_i(x)$\n(1)\nwhere $E_i$ are MLP experts with router probability $P_i$ such that $P_i \\in topK$. $x$ is the output from attention, $N$ is the total number of experts in the MoE layer, and $T$ is the number of experts every token is routed to (topK)."}, {"title": "Softmax - TopK Order", "content": "The standard MoE router formulation [1, 19] performs a softmax on router logits followed by a topK (softmax-then-topK). The activations from the experts are then multiplied by the softmax probability. In this case, the weight of each expert is found using:\n$TopK(Softmax(x \\cdot W_r))$\nwhere $x$ is the input to the MoE block and $W_r$ is the router.\nThis causes an issue with upcycling where the output of the upcycled model is not equivalent to the dense model right after upcycling when TopK < N. Even though the output of the upcycled model is"}, {"title": "Granularity", "content": "Earlier work on MoE routed each token to a very small number of experts (topK = 1 or 2) [19, 5]. Routing to only one expert guarantees that the training FLOPs stay similar to the dense model, even though the MoE has more parameters. However, it has recently been suggested that increasing the number of experts to which a token is routed to, while shrinking the dimension of each expert might be a superior approach [20]. This approach is referred to as granular mixture of experts, shown in Figure 2.\nGranularity introduces a new degree of freedom as every expert can be reduced in size. Since shrinking experts reduces FLOPs per expert, this approach allows us to increase topK by the same magnitude as the shrinking and still keep the overall FLOPs count the same. While FLOPs is only a proxy for the actual compute required to train or deploy a model, it is still useful and an easy metric to compare compute cost.\nThere are three hyperparameters that define a fine-grained MoE architecture. We use the nomenclature proposed in [20] and add another term T to refer to topK. The three hyperparameters convey the following:\n\u2022 E: Expansion rate. How many times larger is the total number of parameters in the MoE layer as compared to the dense MLP counter part. ($N_{MOE}/N_{dense\\_MLP}$)\n\u2022 G: Granularity. How many times smaller is the expert hidden size compared with the original dense layer's FFN hidden size. ($d_{ffn}/d_{expert}$)\n\u2022 T: TopK. How many experts is a token being routed to.\nFor example, in Figure 2, from left to right are coarse-grained MoE E2G1T1 and fine-grained MoE E2G2T2."}, {"title": "Granular Upcycling", "content": null}, {"title": "Scaling the Weights", "content": "We found that with granular upcycling, the scaling of the network weights greatly influences the accuracy of the fine-tuned MoE model. While this scaling could be done entirely in the second linear projection of the MLP (W2 in Figure 3), we found empirically that this works worse than scaling both the linear projection weights (W1 and W2). Equation 2 calculates this scaling factor for the case of squared-relu activation which we use for our base 15B dense model.\n$MoE\\_activation = \\sum_{i=1}^{T} P_i \\times E_i(x)$\nwhere $P_i$ is the probability for the top ith expert, $E_i$ is the corresponding expert layer, T is the topK, and x is the output from attention.\nassuming approximately uniform distribution for iteration 0\n$P = P_1 = P_2 = ...P_T = \\frac{1}{ExG}$\n5while this is a simplifying assumption, it holds true for our most important virtual grouping case where the shrinking factor is the same as topK"}, {"title": null, "content": "$MoE\\_activation = \\frac{T}{ExG} \\times (\\sum E_i(x))$\n$MoE\\_activation = \\frac{1}{ExG^2} ( \\frac{T}{ExG} \\times dense\\_activation) = \\frac{T}{ExG^2} \\times dense\\_activation$\nassuming squared relu activation, we normalize $W_1$ and $W_2$ for each expert's MLP in case of virtual grouping by:\n$\\sqrt{\\frac{E \\times G^2}{T}}$\n(2)\nWhile for different activation functions a hyperparameter search for the optimal scaling of the input and output weights might be intuitively better, we find empirically that the equal weight distribution like above works just as well for our 2B models using swiglu activation. We also use this weight scaling for non-granular (a.k.a coarse-grained) MoE models and observe that it helps convergence."}, {"title": "Results", "content": "Model: We do all our ablation studies on our Nemotron 2B6 and Nemotron-4 15B [13] models followed by final results using a larger token count on Nemotron-4 15B. Nemotron 2B is a transformer-based decoder-only language model similar to GPT-2 and 3 [21]. This model was trained on 1.1T tokens using NeMo [22] and Megatron-LM [18]. The model uses the SwiGLU activation function [23], Rotary Positional Embeddings (RoPE) [24], maximum sequence length of 4096, no dropout, no bias terms in all linear layers, and untied embedding and output layer. Nemotron-4 15B is a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Both these models use a vocab size of 256K.\nData: Upcycling can be performed on either pretraining data that the pretrained dense model has seen, new unseen data, or a combination of both. In our Nemotron 2B experiments, we upcycle on pretraining data the model has seen for simplicity. For all of the ablation studies, we use 110B tokens (about 10% of the pretraining 1.1T tokens). For Nemotron-4 15B experiments, we upcycle on continued training data so that we can compare upcycling against our existing dense continued training result [13, 25]. While the continued training data has still been seen in pretraining, it follows a different data blending distribution. For 15B ablations, we train on 0.1T tokens (10%) of continued training data blend. For our 15B final results, we train on the full continued training data blend of 1T tokens. Validation loss is measured on 1% held-out data."}, {"title": "Effectiveness of Upcycling", "content": "Upcycling vs. Dense Continued Training: Following previous works [15], we compare upcycling vs continued training the dense Nemotron 2B model with the same amount of tokens (0.1T) under a similar learning schedule. As shown in Figure 4a, continuous training plateaus quickly while upcycling keeps improving. From continued training to upcycling, LM loss improved by 1.1%.\nUpcycling vs. Training from Scratch: Figure 4b shows that upcycling achieves good improvement over training from scratch if one assumes a fixed compute budget. Upcycling is an efficient method to utilize pretrained dense model weights when the compute budget is much smaller than the pretraining compute budget. An interesting question that remains unanswered with our studies is whether it is still worth to upcycle a dense model instead of pretraining, assuming a larger compute budget. While some recent works like Skywork-MoE [26] try to answer this question, we leave investigating this as a potentially interesting future direction."}, {"title": "Learning Rate and Batch Size", "content": null}, {"title": "Learning Rate Schedule", "content": "We found that the learning rate schedule plays an important role in upcycling. When upcycling a dense model, the model has usually been trained for a large number of steps already and the learning"}, {"title": "Batch Size", "content": "Aside from the learning rate, we observed that batch size also heavily affects MoE training and up- cycling. We hypothesize that MoEs benefit from larger batch size than dense equivalents for two reasons:\n\u2022 Since each expert receives only a portion of tokens, gradients are noisier than the dense model.\n\u2022 The load balancing loss is noisier if there are fewer tokens to balance.\nAs shown in Figure 7, we compared batch size of 512, 1024, and 8192 (2M, 4M, and 32M tokens respectively) for upcycling the Nemotron 2B model. While the batch size of 32M tokens performs the worst, batch size of 4M tokens converges faster than 2M tokens. The training efficiency (per-GPU"}, {"title": "Load Balancing and Regularization Loss", "content": "Load Balancing Auxiliary Loss: We used the same aux loss as described in ST-MoE [5] and Switch Transformer [19] and experimented with different aux loss coefficients. We found that while not using aux loss at all leads to dead experts and causes the training loss to plateau early, aux loss coefficients set too high leads to aux loss overwhelming the language modeling loss. We found aux loss coefficients between 1e-2 to 1e-3 to give the best language model loss.\nZ Loss: We used the same z loss described in ST-MoE [5]. As shown in Figure 8, we compared upcycling with and without z loss (with a coefficient of 1e-3) and found that z loss has no impact on the final model quality.\nThus, we used an aux loss coefficient of le-2 and no z loss for all our experiments."}, {"title": "Softmax TopK order", "content": "We consistently found that using softmax-then-topK works better than topK-then-softmax for up- cycling. We hypothesize this is because the softmax-then-topK approach preserves the information contained in the absolute value of the router output. However, keeping the output of the original upcycled model similar to the output of the dense model is more difficult with this approach because the outputs no longer sum to one. We overcome this issue with our weight scaling method."}, {"title": "Fixing Output Scale", "content": "We tried multiple approaches to compensate for the issue of expert outputs being scaled down. Apart from weight scaling described in Equation 2, we also experimented with the following:\nScaling the MoE output: instead of scaling the weights, we tried scaling the output of the MoE layer by a constant factor or a learnable scalar. We empirically found that neither of them work well.\nPost Expert Layernorm: Work on finegrained MoE scaling laws [20] recommended adding a layer- norm at the end of MoE layer. Typically, dense models do not have this layernorm. We tried adding"}, {"title": null, "content": "the post expert layernorm during upcycling and found that while it can achieve the same effect and stop the loss from exploding, it takes a lot longer to adapt.\nWe compared different methods for upcycling Nemotron 2B into 64 experts top-8 fine-grained MoE (E8G8T8). The expert intermediate hidden size is 1/8 of the original FFN intermediate hidden size so that they are iso-FLOP. Our proposed weight scaling method performed the best and is also the easiest to implement, since it does not modify the model architecture.\nUsing equation 2, for the finegrained MoE with 64 experts top-8, the scaling factor should be 4. We tried multiple scaling factors (2x, 2.5x, 4x) and found that the scaling factor of 4 also performed the best empirically. So our weight scaling function, while not exact, helps convergence.\nWe also discovered that weight scaling helps upcycling standard coarse-grained MoEs as well. As shown in Figure 9, we upcycled Nemotron-4 15B into 8 experts top-1. With weight scaling, the training loss is 1.5% better than when not using weight scaling."}, {"title": "Increasing Granularity", "content": "We found that increasing granularity improves loss when training with small token counts (0.1T tokens for our 15B example) since virtual grouped granular upcycling is able to achieve a better loss more quickly than the non-granular version. However, when training on much larger token regimes (\u22651T tokens), we saw that granularity did not help proportionally and both granular and non-granular runs converged to similar loss values. Since these large token horizon runs required significant compute, we did not perform ablations on them.\nWe tried scaling up the number of experts from 8 to 256 without increasing FLOPs. On upcycling Nemotron 2B and Nemotron-4 15B, we compared 8, 64, 128, and 256 experts. We kept all these networks iso-FLOP by scaling down the expert hidden size proportionally with respect to the topK. Shown in Figure 10, on Nemotron 2B, 64 experts performed better than 8 experts. However, scaling up further to 128 or 256 brought in little benefit. Similarly, on Nemotron-4 15B, the improvement maxed out at 64 experts. Surprisingly, on upcycling Nemotron-4 15B, 256 experts performed slightly worse than 64 or 128 experts. Too many experts seemed to hurt accuracy when upcycling. We hypothesize that this is because the experts were all copies and the larger the number of experts get, it becomes difficult for the network to find new superior minima. While increasing granularity is promising, it is important to note that it comes with higher MoE permute/unpermute cost and smaller GEMM sizes. Owing to these system-level factors, we worked with both granular and non-granular recipes."}, {"title": "Increasing TopK", "content": "Top-2 routing is often used with MoE models [3, 5]. While this increases the amount of compute required to run the model, it helps in achieving better accuracy. We compared increasing topK for both coarse-grained and fine-grained MoE upcycling. Figure 11 shows that 8 experts top-2 (E8G1T2) performed better than top-1 (E8G1T1) on both upcycling Nemotron 2B and Nemotron-4 15B. On 15B, the top-2 achieved lower training loss than top-1 (1.35757 vs 1.38031). Previous works [5] have shown that a tradeoff with wall clock time rather than compute is a better metric and in such cases topK greater than granularity might make more sense. This claim heavily depends on the actual implementation and we leave it as a future systems study."}, {"title": "Promoting Expert Diversity: Weight Permutation and Reinitialization", "content": "We experimented with the weight permutation and reinitialization methods proposed in Qwen 2 [17]. Weight permutation permutes the FFN weights before copying it into each expert. Weights reini- tialization randomly reinitializes 50% of expert weights. In our experiments, we did not find any improvement with these two methods on upcycling Nemotron 2B. Due to limited compute, we did not experiment with these techniques on a larger network."}, {"title": "Shared Experts", "content": "We experimented with the shared experts approach proposed in Deepseek-MoE [16]. Shared experts are always 'on' i.e. every token is routed to the shared expert. They act like a dense layer in parallel with the MoE experts. As shown in Figure 13, we compared 8 shared experts + 64 experts top-8 which is iso-FLOP with 64 experts top-16 on Nemotron 2B. We found that shared expert performed on par with the iso-FLOP, no shared expert counterpart. We did not switch to using a shared expert since we did not see any accuracy improvement by using it."}, {"title": "Large Scale Upcycling", "content": "To ensure our proposed recipes work on large scale (model size and token count) regimes, we compared upcycling against continued training Nemotron-4 15B base model on 1T tokens. We decided to test only 2 variants: (1) E8G8T8 and (2) E8G1T2, due to the high training compute requirement (0.3 yottaFLOPs). We chose E8G8T8 with virtual group initialization since it worked better than E8G1T1 in our ablations. We chose E8G1T2 to show the effect of increase in compute and upcycling FLOPs. We chose E8G1T2 over E8G8T16 to show that our recommendations, including weight scaling along with softmax-then-topK router, work well for non-granular cases well. Non-granular use-cases are important to study since they practically achieve a better GPU FLOP utilization than their iso-FLOP granular counterparts.\nFor 1T upcycling, we initialized lr to min-lr of pretraining (4.5e-4) and used a peak-lr of 3e-4. We used cosine decay and decayed the Ir to 1/100-th of the pretraining min-lr as done for the original continued training of Nemotron-4 15B [25]. We also used a higher batch size for the E8G8T8 model than E8G1T2 since E8G1T2 receives more tokens per expert on average. As shown in Table 1, upcycled E8G8T8 (64 experts top-8 1/8 expert hidden size) achieved a 4.1% lower validation loss than the dense continued training model while being iso-FLOP. It also achieved a better MMLU score of 66.2 (vs 65.3 for dense). We observed that the percentage difference in MMLU is sensitive to the continuous training data and the difference increased along with the token count so longer token horizons favor MoE models. With increased training FLOPs, we also upcycled E8G1T2 (8 experts top-2) which achieved 5.2% lower validation loss and an even better MMLU of 67.6."}, {"title": "Related Work", "content": null}, {"title": "Mixture of Experts (MoE) Models", "content": "Mixture of Experts (MoE) models [27, 1] have gained significant attention in the field of large language models due to their ability to scale model capacity while maintaining computational efficiency. The MoE architecture employs a gating mechanism to selectively activate a subset of expert networks for each input token. This approach allows for an increased model capacity without a proportional increase in computational cost during inference and training.\nRecent work has focused on improving the scalability and efficiency of MoE models. Switch Trans- former [19] simplified the MoE architecture by using a top-1 routing mechanism and demonstrated the ability to scale to trillion-parameter models. The GShard framework [2] addressed challenges in training large-scale MoE models, introducing techniques such as expert capacity thresholds and local group dispatching to improve load balancing and training stability.\nResearch has explored various aspects of expert specialization and routing mechanisms in MoE models [28, 29]. Studies have investigated the impact of the number of experts on model performance, finding that increasing the number of experts leads to improved sample efficiency and faster training, albeit with diminishing returns beyond certain thresholds [20]. The choice of routing algorithm (e.g.,"}, {"title": "Upcycling and Model Expansion", "content": "The concept of upcycling in the context of MoE models refers to the practice of leveraging pre-trained dense models to initialize MoE architectures. This approach has gained traction as a way to efficiently create large-scale MoE models while benefiting from the knowledge captured in existing pre-trained checkpoints. Notable work in this area includes:\nSparse Upcycling: [15] proposed a method for training MoE models from dense checkpoints, demon- strating the ability to expand model capacity while maintaining or improving performance. Recently, Qwen [17] and Deepseek [14] have adopted this approach. However, the recipe to scale upcycling beyond 1B parameters is not well known.\nNetwork Growth: Research on model expansion techniques, such as those explored in the Gopher model [33], has shown that it's possible to significantly increase model size while maintaining perfor- mance comparable to or better than models trained from scratch.\nProgressive Expansion: Approaches like LLAMA PRO [10] have investigated progressive expansion techniques, where model size is increased gradually during training [34, 35, 36, 37]."}, {"title": "Challenges and Ongoing Research", "content": "Despite the promising results in upcycling and MoE pretraining, several challenges remain active areas of research:\nExpert Collapse: The phenomenon of expert collapse, where certain experts become underutilized or inactive, has been observed in MoE training. While some studies suggest that expert collapse may not necessarily harm model accuracy, addressing this issue remains an important consideration in MoE design [38, 31].\nLoad Balancing: Ensuring an even distribution of work across experts continues to be a challenge, with various approaches proposed to improve load balancing, including auxiliary losses and specialized routing mechanisms. Skywork-MoE [26] used aux loss to promote expert diversity during upcycling. BTX [39, 40] trained experts on different tasks and then mixed them. DBRX 7 added weight norm on experts' weights.\nEfficient Training and Inference: Ongoing work focuses on optimizing the training and infer- ence processes for MoE models, including techniques for reducing communication costs and improving parallelization strategies. Deepseek-MoE [16] and Snowflake Arctic & parallelized shared expert to in- crease utilization. MegaBlocks [41] reformulated MoE computation in terms of block-sparse operations. Scattermoe [42] optimized memory for finegrained MoE."}, {"title": "Conclusion", "content": "In this work, we conducted an extensive study of upcycling techniques and best practices for billion- parameter scale language models. We proposed a \"virtual group\" initialization scheme and weight scaling approach to successfully enable both coarse-grained and fine-grained MoE upcycling at scale. We found that upcycling outperforms continued dense model training for the same amount of compute on both 2B and 15B parameter models. Based on the target inference and available upcycling FLOPS, an architecture that uses more FLOPs like E8G1T2 can achieve better accuracy than dense iso-FLOP MoE models. On our 2B ablations, we found that upcycling needs a different set of hyper-parameters than fine-tuning. Softmax-then-topK expert routing performs better than topK-then-softmax in the MoE router during upcycling. Higher granularity MoEs boost upcycling accuracy but require a more careful weight scaling and sharding strategy and also lead to a lower GPU FLOP utilization. In a purely FLOP-bound scenario, using virtual group init with granular model upcycling seems to be the best strategy. We hope this work illuminates the details of upcycling billion-parameter scale MoE models. Future directions include studying upcycling in larger models, improving expert diversity and utilization, and co-optimizing model architectures and system designs."}, {"title": "Virtual Grouped Router", "content": "The following pseudo code illustrates our virtual grouped router approach for upcycling into E2G2T2:\n# sharding FFN into expert shards G=2\nFFN = [FFN_O, FFN_1]\n# copying to form multiple experts; E=2\nexperts = [FFN_0, FFN_1,\nFFN_O, FFN_1]\n# random initialized router\nrouter_prob = tensor([0.4, 0.2,\n0.3, 0.1])\nrouter_top2 = tensor([0.4, 0.0,\n0.3, 0.0])\n# For dense model\nFFN(x) = FFN_0(x) + FFN_1(x)\n# In case of MoE\nFFN_moe(x) = router_top2@experts = 0.4 FFN_0(x) + 0.3 FFN_0(x) \u2260FFN(x)\n# virtual group = initialize every group with same weights\nrouter_prob = tensor([0.3, 0.3,\n0.2, 0.2])\nrouter_top2 = tensor([0.3, 0.3,\n0.0,0.0])\n# one of each FFN shard is guaranteed to be selected\nFFN_moe(x) = 0.3 FFN_0 + 0.3 FFN_1 \u2248 FFN (x) / 4"}]}