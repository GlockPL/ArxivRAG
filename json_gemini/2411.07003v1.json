{"title": "Enhancing Robot Assistive Behaviour with Reinforcement Learning and Theory of Mind", "authors": ["Antonio Andriella", "Giovanni Falcone", "Silvia Rossi"], "abstract": "The adaptation to users' preferences and the ability to infer and interpret humans' beliefs and intents, which is known as the Theory of Mind (ToM), are two crucial aspects for achieving effective human-robot collaboration. Despite its importance, very few studies have investigated the impact of adaptive robots with ToM abilities. In this work, we present an exploratory comparative study to investigate how social robots equipped with ToM abilities impact user's performance and perception. We design a two-layer architecture. The Q-learning agent on the first layer learns the robot's higher-level behaviour. On the second layer, a heuristic-based ToM infers the user's intended strategy and is responsible for implementing the robot's assistance, as well as providing the motivation behind its choice. We conducted a user study in a real-world setting, involving 56 participants who interacted with either an adaptive robot capable of ToM, or with a robot lacking such abilities. Our findings suggest that participants in the ToM condition performed better, accepted the robot's assistance more often, and perceived its ability to adapt, predict and recognise their intents to a higher degree. Our preliminary insights could inform future research and pave the way for designing more complex computation architectures for adaptive behaviour with ToM capabilities.", "sections": [{"title": "1 Introduction", "content": "Cognitive stimulation is crucial for maintaining and improving abilities such as memory, attention, and executive function [59]. Regular engagement in activities that challenge and stimulate the brain has been shown to positively impact cognitive health and can delay the onset of age-related declines [23]. The use of socially assistive robots (SARs) in memory exercises [1, 36] has the potential to provide a personalised and engaging platform for delivering cognitive stimulation, offering a unique and interactive experience for users [29]. Indeed, robots have been proven to be very effective in performing simple, and repetitive tasks, making them a perfect tool to support healthcare professionals and enhance their effectiveness during their daily working routine [52]. Nonetheless, for robots to be most effective in providing assistance, they must cater to the specific needs of users and aim to prevent negative emotions such as frustration (due to overly difficult"}, {"title": "2 Related Work", "content": "In the context of designing robots intended to interact with diverse users, adaptivity emerges as a key requirement. Human performance in completing tasks can vary significantly due to a multitude of factors, including individual preferences, cognitive capabilities, and the inherent complexity of the task. In pursuit of enhancing robot adaptivity for humans and mitigating possible unpredictability in the behaviour, researchers are starting to focus on endowing robots with ToM capabilities. The integration of ToM into robots offers a dual advantage. Firstly, it can enhance human-robot interactions by tailoring the assistance provided to users based on a deeper understanding of their needs. Secondly, it can foster trust and transparency in the robot's actions as it becomes capable of providing explanations for its decisions.\nThis section delves into the domain of robot adaptivity and explores the most relevant contributions to the topic (see Section 2.1). Additionally, it provides a comprehensive overview of relevant studies that have investigated the benefits of endowing robots with ToM, first exploring some related concepts such as intention recognition, transparency, and explainability, and then also ToM approaches that concentrated on developing computational models to simulate this cognitive capability (see Section 2.2)."}, {"title": "2.1 Robot Adaptive Behaviour", "content": "Previous work on robot adaptivity has shown the importance of tailoring the robot's behaviour to humans in assistive tasks.\nInteresting contributions have emerged in the educational context. Park et al. [38] employed RL to determine which stories to select in order to optimise children's engagement and enhance their linguistic skills during storytelling activities. Senft et al. [47] introduced SPARC, a novel framework aimed at addressing the sparsity of reward functions in RL. This approach involved a human \"wizard\" who initially guided the robot's learning process by selecting actions. Over time, control gradually transitioned to the robot. A different approach from RL emerged in the field of intelligent tutoring systems (ITS), where the system adapts to the learner by building dynamic models of their knowledge, skills, and learning preferences. These models are constructed by continuously monitoring and analysing learner interactions, as noted by Zhang et al. [60]. Expanding on this idea, Leyzberg et al. [27] explored the role of personalised lesson sequencing in robot tutoring through an adaptive Hidden Markov Model (HMM) that tracked student proficiency. Schodde et al. [46] took a complementary approach, utilising an extended Bayesian Knowledge Tracing (BKT) model combined with predictive decision-making to dynamically adjust tutoring strategies. By tracking the learner's knowledge state, their robot tutor adapted its next steps accordingly, demonstrating improved learning outcomes in an L2 learning game compared to a random control group. Moving beyond cognitive modelling, Spaulding et al. [50] integrated affective data into BKT models, focusing on how emotional signals like engagement and frustration can improve knowledge inference. Their study showed that children interacting with an affect-aware social robot displayed stronger engagement, indicating the potential of affective data to enhance educational experiences. Building on these developments, Salomons et al. [44] introduced Time-Dependent Bayesian Knowledge Tracing (TD-BKT) to track skill acquisition in complex tasks. By incorporating time-sensitive parameters and accounting for incomplete data, TD-BKT further refined skill tracking and demonstrated improved accuracy, especially in tasks like electronic circuit building, where traditional models struggled.\nIn the healthcare context, robots have been designed to adapt to the patient's needs to provide personalized assistance, enhance engagement, and support therapeutic goals. Raggioli et al. [40] proposed a reinforcement learning (RL) approach that allowed the robot to adaptively determine the optimal monitoring distance and direction based on the user's current activity, which was estimated using data from wearable devices. Maroto et al. [30] presented a decision-making system that, leveraging user information and a biologically-inspired module, personalised interactions to maintain user engagement over time. Moro et al.[33] explored the development of a novel learning architecture for socially assistive robots, particularly focusing on aiding individuals with cognitive impairments. The authors argue that for these robots to be effective, they must not"}, {"title": "2.2 Theory of Mind", "content": "ToM is critical in enabling robots to infer and reason about the mental states of others, including beliefs, desires, and intentions. By equipping robots with ToM capabilities, they can provide more personalised and context-aware assistance, particularly in assistive tasks.\nIntention recognition plays a pivotal role in realising ToM capabilities, as it allows robots to predict user goals based on their actions, behaviours, and contextual cues. Mavsar et al.[32] introduced a method using recurrent neural networks (OptiNet and HandNet) to predict human intentions in dynamic human-robot collaboration. Similarly, Chang et al. [26] explored how robots can combine intent recognition with communication through legible motions to improve task performance. A very interesting work in the field was presented by Jain et al. [22] who introduced a formal mathematical formulation for intent inference during assistive teleoperation under shared autonomy. They introduced a Bayesian filtering technique for probabilistic reasoning about user goals in shared autonomy. The robot in their system infers intentions based on multiple non-verbal cues, adapting dynamically to assist in real-time. In our work, we do not utilise any advanced AI learning techniques; instead, we estimate the user's intended course of action through a heuristic-based approach grounded in the specific logic of the assistive task used in the study.\nOn the other hand, system transparency and explainability are critical in ensuring users understand and trust the robot's behaviour. In prior research, system transparency and the provision of explanations have been shown to significantly affect user perceptions of trust, satisfaction, and usability [43]. For instance, Kizilcec et al. [24] found that transparency in decision-making systems improved users' trust and understanding of the system, even when the system made errors. Similarly, Wang et al. [57] demonstrated that explanations that align with a user's mental model can enhance task performance and satisfaction. In human-robot interaction, Ezenyilimba et al. [19] showed that providing clear, human-centred explanations improves user trust and situation awareness in human-robot teams. Olivares et al.[37] evaluated the impact of explanation specificity using an ontology in an online"}, {"title": "3 The Memory Game Task", "content": "The cognitive task we used is the classical Memory Game, also known as Concentration which is designed to assess players' memory and concentration skills. Players are typically presented with a set of cards that have images on one side and a uniform back on the other. The cards are placed face down, and players take turns flipping over two cards at a time, trying to find a match. A move is considered as two turns (two cards are flipped). The objective of the game is to find all"}, {"title": "4 Combining Reinforcement Learning with Theory of Mind", "content": "The main objective of this study is to develop a hierarchical computational approach to generating the socially assistive behaviour that best matches the user's preferences and translating them into meaningful assistance by leveraging the system ToM's abilities. Specifically, we defined two layers of abstractions (see Figure 2). The upper layer employs an RL algorithm to learn the degrees of assistance (see Section 4.1) to provide to the user in a given state (see Section 4.2). The lower layer uses a heuristic-based ToM to infer the user's intended strategy and operationalises the assistance selected by the RL. Additionally, it also provides an explanation of the rationale behind its choice (see Section 4.3). It is important to mention that the two layers are completely independent. This has the following advantages: (i) it allows us to learn the robot's assistance at a high level of abstraction, separating it from its implementation; (ii) using a heuristic-based ToM that is not fully integrated into the decision-making system allows us to maintain transparency in the process and provide an explanation for the system's implemented action."}, {"title": "4.1 Learning to Adapt", "content": "Data-driven approaches require a significant amount of data to learn a reasonable policy. However, collecting this data by exposing users to long interactions with the robot can be difficult and sometimes not feasible. Additionally, during the initial stages, data-driven methods may need to explore undesired states that could potentially hinder the participants and impact their acceptance of the robot. This problem poses a significant obstacle to the use of data-driven approaches in real-world scenarios [6].\nThis work builds upon our previous work [4] in which a robot was employed to provide adaptive assistance to users playing a memory game. In [4], the robot's assistive behaviour was based on a simple but effective probabilistic model and was trained from data collected by observing humans in the same context. However, the collected data were very limited, and in some states, the robot did not have enough information to make a decision. In this work, we go a step further, trying to address that limitation and the related cold start problem. We propose a temporal difference (TD) algorithm, Q-learning [51], to train the robot in finding the optimal policy by combining a data-driven approach, using data from [4]) and a knowledge-driven approach, building a model of an imperfect player (see Section 4.2). We employed Q-learning as it is one of the most used approach from previous studies for providing adaptive robot's behaviour. With respect to more sophisticated RL algorithms like DQN, Proximal Policy Optimization (PPO), or Actor-Critic methods, it is considered better due to its simplicity, robustness, and general applicability, especially in environments with discrete action- and state-space. Here, the optimal policy is the one that leads the user to complete the game with minimal assistance from the robot. Differently from previous works, we are not only interested in providing assistance after the first card has been flipped out but also before the player has to flip out any cards. This adds another layer of complexity to the learning algorithm that has to learn socially assistive behaviour in two different states.\nWe formalise the task as a Markov Decision Process (MDP). The action space (A) is discrete, and the robot has 4 actions available $A = \\{no\\_help, sug\\_col, sug\\_row, sug\\_card\\}$. An example of those actions is reported in Table 1. The state space (S) consists of the following variables: the game state GS = {begin, middle, end} where 0 < beg < 4, 4 < mid < 8 and 8 < end < 12 found pairs; the assistance provided by the robot in the previous interaction RS={no_help, sug-col, sug_row, sug_card}; and the outcome on the previous turn US={f_correct, f _wrong, s_correct, s_wrong} (where, f and s stand for first and second card and correct or wrong for its outcome).\nNote that in this way, we can distinguish whether the assistance was provided on the first or second card. Therefore, we have |GS|\u00d7|RS|\u00d7|US| 48 game states.\nThe reward function (R) is defined dependably on whether the robot assistance is provided before flipping a card (r1) or after having flipped a card (r2). In the case the user flips the first card, we defined $r_1 = \\{\\frac{a}{nf}\\}$ where"}, {"title": "4.2 Modelling the User", "content": "Modelling the user is fundamental for designing robot behaviours that match their needs and preferences [18]. In this work, we model the user taking into account: (i) the task, by defining users with imperfect memory based on the findings of Vellemant et al. [56] and (ii) the data gathered in our previous work [4]. In [56], the authors provided some interesting formulas to estimate the optimal performance of a player with perfect memory. They say that the expected number of moves is \u2248 1.61 n where n is the number of expected matches. In our case, this number is around 19. Furthermore, the expected number of flips, before two matching cards are identified, is \u221a(\u03c0\u00b7\u03b7). In our case, this number is around 8 moves. This information provides us with an upper bound whereby we can model more realistic participants.\nFirstly, we design the perfect player according to [56]. Next, we model a player with imperfect memory by hypothesising that the probability of the user making a match P(U) depends on the number of cards flipped so far, NF and the number of matches NM. In this way, the more the user plays (NF and NM), the higher will be their chance to find a pair. Using linear regression, we analyse data from our previous study [4] to estimate how player performance changes over time as a function of these variables. In the following paragraph, we provide some details on the implementation.\nWhen a user flips their first card, they randomly select it with the goal of exploring new areas of the board while also exploiting what they have already seen. To do this, we use a simple heuristic that balances between moves exploring the board and selecting previously seen cards. In this latter case, the probability of finding a match is increased because the simulated player has already seen some cards. We calculate this probability as $P(seen) = (1 \u2013 d)^{NF}$, where d is the decay rate. In this way, we simulate the user's memory of a card. The higher the value of d, the more forgetful the user becomes. If the user flips a"}, {"title": "4.3 Mentalising Users Ability", "content": "The mentalising layer is responsible for making inferences on the actions A provided by the robot when the user has not yet flipped a card. The game can be approached using three specific moves: 0-unknown, 1-unknown, and 2-unknown. A 0-unknown move entails flipping over known cards that the player assumes to be a pair. In the event the assumption is incorrect, no new information is gained. In a 1-unknown move, one unknown card is flipped over and matched with a known card. If a match is established, new information is gained. Otherwise, a known card is flipped over that does not match. Lastly, a 2-unknown move involves flipping over two unknown cards to match with known cards. If a match is established, new information is gained. If not, the player must repeat the move.\nA possible assistance strategy from a user would be in the case of 1-unknown. Indeed, it is helpful to keep in mind that if they already know the position of one card, their chances of success are increased. However, if the user cannot remember that they have already seen that card in a different location, they might end up flipping another unknown card (2-unknown). Therefore, the system keeps track of the cards flipped so far by the user, sorting them according to the number of flips and in which turn. Then, when the Q(s, a) matrix suggests an assistive action such as sug_row, sug_col, or sug_card on the first card, the mentalising layer selects the location of one of the most clicked cards in the recent history. It then suggests the corresponding matching card, which is unknown to the user. The rationale behind this approach is as follows: if the user has already seen a card multiple times, they are likely looking for a specific match. Therefore, the system suggests the location hint (sug_row or sug_col) or the card (sug_card) of the matching card that the user has not seen yet or has seen less frequently. Additionally, the robot not only provides this information, but it also offers a simple explanation for why it has made the suggestion, demonstrating its mentalising abilities (see examples in Column \u201c1st\u201d of Table 1).\nOnce the user has flipped out the first card, depending on the latter, the system provides assistance according to the Q(s, a) (see Column \"2nd\" of Table 1). It is worthwhile noticing that the assistance given for the second card depends on both the card recently flipped by the player and its history. Indeed, the robot always provides assistance in a way that could potentially lead to a"}, {"title": "4.4 Results from Simulation", "content": "Here, we summarise the findings obtained in the simulation as a result of modelling the perfect player, the imperfect player, and the imperfect player assisted by the Q-learning agent, respectively. The results are shown in Figure 3. The perfect and imperfect players differ for about 23 moves, while the assisted imperfect player performs slightly better thanks to the hints offered by the robot. Results are aligned with our previous findings [4]. Note that we did not model the complexity of the game (similarity of the cards), which in a real scenario can have an impact on the players' performance. Additionally, we did not consider the user's trustworthiness regarding the robot's assistance. Therefore, the user accepted any assistance provided by the robot.\nThe resulting policy learnt from the system in the case of the imperfect player is reported in Figure 4. Note that to make the matrix more readable, we removed the states the RL agent never visited, e.g., s(sug-card, beg, f_wrong). It can be observed how the system is able to distinguish between the two states in which it is requested to assist. On the first flip, the system tries not to intervene if the user performs well and increases its"}, {"title": "5 Experimental Design", "content": "The study was set up as a between-subject study, in which we manipulated the robot's ability to mentalise the user's intent during the game. As in our previous works [4, 5], we envisaged the robot as an assistant that aids the users during the memory game. Ethical approval was obtained from the ethical committee of the University of Naples Federico II. Each participant was randomly assigned to one of the two conditions:\n\u2022 without_mentalising abilities (noToM): On the first card, the robot did not make any reasoning on the cards previously explored by the player. Once an assistive action was selected by the Q-learning, the robot instantiated it randomly without providing any motivation (\"try to flip a card in row 2 col 3\"). For the second card, assistance was provided without any explanation. For example, it correctly identified the row (sug_row) based on the first card (\"The matching card is located in row 2\").\n\u2022 with_mentalising abilities (ToM): On the first card, the robot provided assistance according to the mechanism described in Section 4.3. For instance, if the action was sug_card, the robot would select the card based on the user's previous history inferring their next move. Therefore, it might suggest the other location of \"shark\" providing a motivation of why it has suggested it (\"You have seen shark several times, you should remember the other location\"). For the second card, assistance was provided with an explanation. For example, it correctly identified the row (sug_row) based on the first card (\"You've seen this card before. I remind you that it is located in row 2\").\nIt is worthwhile mentioning, that in both conditions, the robot was endowed with the same Q-learning algorithm (see Section 4.1). The only"}, {"title": "5.1 Hypotheses", "content": "We formulated the following research hypotheses:\nH1: participants assisted by a robot endowed with ToM ability solve the game with fewer mistakes and in a shorter time compared to those who are assisted by a robot without such ability.\nH2: Participants assisted by a robot endowed with ToM ability follow its suggestions more often compared to those who are assisted by a robot without such ability.\nH3: Participants assisted by a robot endowed with ToM ability capabilities believe the robot is better at predicting their beliefs and intentions compared to those who are assisted by a robot without ToM capabilities.\nIn H1, our goal was to evaluate whether the robot equipped with ToM had any effect on the participants' performance. Similarly, in H2, we aimed to assess the quality of assistance provided by the robot capable of TOM. Lastly, in H3, we investigated the users' perception of the robot."}, {"title": "5.2 Experimental Setting", "content": "The experiment was carried out during a technology fair, that involved mainly young students from high schools and their relatives. On a desk, we set up a computer from which participants played the game, and on a side with respect to the computer we located the robot (see Figure 1). As a robotic platform, we employed the Furhat robot. The experimenter was seated at the same table but out of the field of view of the participants to not interfere with the experiment and distract them."}, {"title": "5.3 Apparatus", "content": "The experimental apparatus comprised a Furhat robot and a computer-based interface, both integral components of the interactive game employed in this study. The robot, developed by Furhat Robotics, played a pivotal role as the interactive agent in the experiment. This anthropomorphic robot, standing at approximately 60 centimetres in height, is designed to simulate natural face-to-face interactions with humans. It is endowed with a high-resolution facial display, capable of expressing a wide array of emotions and non-verbal cues. The robot's non-verbal social cues were constrained to the standard ones provided by the SDK. The robot could display, happy, confused or sad faces depending on whether the user made a correct or a wrong move. The memory game\u00b3 was played on a laptop computer of 15 inches. Participants interacted with the game by means of a mouse."}, {"title": "5.4 Procedure", "content": "The experiment was conducted in a booth where hundreds of visitors were wandering around. Upon arrival, the experimenter introduced the experiment, providing a brief overview of the study's purpose and procedures.\nIf the participant expressed interest and agreed to participate, they were asked to provide informed consent by filling in a consent form. The consent form included information about the study's objectives, the experimental tasks involved, the potential risks and benefits, and the assurance of confidentiality. The participants were given ample time to read and understand the contents of the consent form before signing it.\nAfter obtaining informed consent, the participants were instructed to proceed with the experimental task. They were given a clear explanation of the game they would be playing and were provided with any necessary clarifications to ensure their understanding. Next, they were given up to 5 minutes to familiarise themselves with the game and with the robot. In this stage, the game was a different one with respect to what they played in the experiment and the experimenter would address any question raised by the"}, {"title": "5.5 Participants", "content": "A total of sixty participants were initially included in the experiment. However, after a careful examination of the data, four participants were identified as outliers due to their performance deviating by more than 2 standard deviations (2\u03c3) from the rest of the participants. As a result, these four participants were excluded from the analysis, leaving a final sample size of fifty-six participants. From those 28 were assigned to the noToM group and the remaining 28 to the ToM group.\nAmong the remaining participants, nineteen identified themselves as female, while thirty-seven identified as male. The participants' ages ranged from 18 to 54 years, with a mean age (M) of 25.98 and a standard deviation (SD) of 8.76.\nRegarding prior experience with robots, fourteen participants reported having no previous experience with robots. Twenty-two participants acquired knowledge about robots through exposure to films, books, or series. Nine participants had prior experience with robots at fairs. Six participants owned a robot such as Roomba for domestic use. Additionally, five participants reported having professional involvement with some form of robotic technology.\nThe recruitment process aimed to achieve diversity in terms of gender and prior experience with robots, allowing for a more comprehensive analysis of the participants' responses and perceptions."}, {"title": "5.6 Evaluation Measures", "content": "To address our initial hypotheses, we gathered subjective and objective measures."}, {"title": "5.7 Results", "content": "To test hypothesis H1, we run a t-test (data normality assumption was assessed with Shapiro-Wilk-test) with the mentalising ability of the robot as the independent variable and with the number of turns, completion time, the time before a match, and the number of suggestions provided by the robot as dependent variables (see Figure 5). Concerning the number of turns, we found that there was a significant effect as participants in the ToM condition (M=55, SD=7) took fewer moves to complete the game (p < 0.05, t(27)=1.71) in comparison to those in the noToM condition (M=58, SD=7) (see Figure 5a). Similarly, results indicated that participants in the ToM condition could solve the game in a shorter amount of time (M=159s, SD=29s) compared to those in the noToM condition (M=187s, SD=69s, with p < 0.05, t(27)=-2.01) (see Figure 5b). Finally, results showed that participants who belong to the ToM group took less time to find a match (M=13s, SD=2s) in comparison to those in the noToM condition (M=15s, SD=6s, with p < 0.05, t(27)=1.9) (see Figure 5c, where completion_time/n_matches). To further understand those results, we also evaluated how much assistance the robot provided in the two conditions. For a better comparison, for each participant, we normalised the assistance received with respect to the number of turns (overall_assistance/n_turns), summing up the levels of assistance received (overall_assistance) as follows: N_A (no assistance) = 0, S-C (suggest column) = 0.5, S-R (suggest row) = 1 and S_CC (suggest card) = 2. Results showed that participants who interacted with the robot with TOM (M=0.27, SD=0.07) capabilities were provided with less assistance (p < 0.05, t=-2.4) than those who played with the robot in the noToM condition (M=0.34, SD=0.07) (see Figure 5d).\nTo test hypothesis H2, we run a Mann-Whitney U-test with the mentalising ability of the robot as the independent variable and with the number of suggestions followed on the first card and the number of suggestions followed that led to a match as dependent variables (see Figure 6). As before, for each participant, we normalised the suggestions with respect to those who were provided during the whole game (n_followed_suggestions / total_suggestions). Results showed that participants who played the memory game in the ToM group (M=0.92, SD=0.09), followed more often the assistance provided by the robot (p < 0.05, z=2.54), in comparison to those in the noToM group (M=0.86, SD=0.1) (see Figure 6a). Likewise, we normalised the number of cards that led to a match (n_cards_match / total_suggestions). We found that participants in the ToM condition (M=0.30, SD=0.15) selected more often cards that led to a match (p < 0.05, z=1.94), in comparison to those in the noToM condition (M=0.24, SD=0.12) (see Figure 6b).\nTo address our last research hypothesis H3, we analyse the results of the questionnaire (see Figure 7). Results from the AC dimension indicated that participants in the ToM group (M=15, SD=3) perceived the robot as more capable of adapting to their beliefs (p < 0.05, t(27)=- 2.10) compared to those in the noToM group (M=13, SD=3). Similarly, results from the PC dimension showed that participants in the ToM group (M=12, SD=3) perceived the robot as more capable of predicting their actions (p < 0.05, t(27)=1.92) in comparison to those in the noToM group (M=11, SD=3). Likewise, for the RC dimension, we found that participants in the ToM condition (M=14, SD=3) perceived the robot with more capabilities of recognising their actions (p < 0.05, t(27)=1.70 ) compared to those in the noToM condition (M=12, SD=4). Finally, with respect to the SOC dimension, we did not find any statistical significance. Participants who interacted with the robot endowed with"}, {"title": "6 Discussion", "content": "In this section, we engage in a comprehensive discussion of our results, focusing on their alignment with our initial research hypotheses and their broader implications. We organised our discussion around the three hypotheses explored in our research."}, {"title": "6.1 Hypothesis H1: Impact on Game Performance", "content": "Our initial hypothesis was that participants who interacted with a robot possessing ToM capabilities would demonstrate better performance than those interacting with a robot lacking ToM. Our findings support this hypothesis across various performance metrics, including number of turns, completion time, and time to find a match.\nFurther analysis revealed that the improved performance of participants in the ToM group was not due to receiving more assistance from the robot. The robot provided less assistance to the ToM group. This can be attributed to the offline policy, which was trained to provide more assistance after more failures a common occurrence among participants in the noToM group."}, {"title": "6.2 Hypothesis H2: Acceptance and Quality of the Robot Assistance", "content": "Our second hypothesis centred on the influence of the robot's ToM on participants' acceptance of the assistance, along with their impact on guessing the correct card. The results suggest that those participants who interacted with the robot capable of mentalising their strategy tended to follow their assistance more frequently, which, in turn, was associated with a higher likelihood of achieving a match. These findings are consistent with previous work in which robots endowed with some degrees of ToM were deemed more trustworthy to the extent that they could impact people's decision-making [34, 41, 42]. While trust was not explicitly measured in this study, the way the robot provided support in the ToM condition could suggest a potential link to perceived trustworthiness. Indeed, by mentalising the potential strategies of each of the participants, the latter could acknowledge its effectiveness by making matches."}, {"title": "6.3 Hypothesis H3: Perception of Robot Competence", "content": "Our third hypothesis delved into participants' perceptions of the robot's competence, specifically focusing on Adaptation to Human Cognitions (AC), Prediction of Human Cognitions (PC), Recognition of Human Cognitions (RC), and Social Competence (SOC). The outcomes provide interesting insights into the interplay between robots equipped with ToM and participants' perceptions.\nThe participant's perception of the robot's social intelligence abilities further supports the more objective findings of H1 and H2. We found significance in the AC, PC and RC dimensions. If we closely examine the statements of these dimensions, we could notice that they assess the robot's adaptive capabilities to the user's beliefs and intents (AC), with its ability to predict the participant's beliefs and thoughts (PC) along with the robot's ability to detect beliefs and thoughts (RC). This is precisely what we aimed for by combining RL with the mentalising layer - designing an adaptive system that, by leveraging its ToM capabilities, can further increase the users' perceived social intelligence of the robot. Our results are aligned with previous research that highlighted how these dimensions are critical factors that could enhance user acceptance of the robot [10]. Indeed, S\u00f6derlund et al. [49] found in their study, that when participants perceived that the robot has ToM capabilities, they assign it a higher degree of usefulness. Likewise, Shvo et"}, {"title": "7 Limitations and Outlook", "content": "While our study yielded promising results, it is essential to acknowledge certain limitations that could guide future research. These limitations can be broadly categorized into developmental and methodological aspects.\nFrom a developmental standpoint, some technical constraints affected the current design. One limitation was the offline nature of the policy learning algorithm. Although this ensured that the robot's policy was reliable for gameplay and allowed us to isolate the effects of ToM, the system lacked real-time learning. Future research could explore fine-tuning the robot's behaviour dynamically during interaction to better accommodate individual preferences. Concerning the current architecture, future work could focus on comparing it with more sophisticated ones that incorporate user mental states such as beliefs, intentions, and strategies within the MDP. Additionally, the robot's communicative abilities were limited to basic verbal and non-verbal cues. Although this design ensured more control over the manipulated variables and platform independence, it restricted the robot's expressiveness. For instance, a robot capable of pointing could help direct participants' attention more effectively [4, 5", "16": "."}]}