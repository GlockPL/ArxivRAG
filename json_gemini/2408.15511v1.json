{"title": "AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models", "authors": ["Fanglong Yao", "Yuanchang Yue", "Youzhi Liu", "Xian Sun", "Kun Fu"], "abstract": "Aerospace embodied intelligence aims to empower unmanned aerial vehicles (UAVs) and other aerospace platforms to achieve autonomous perception, cognition, and action, as well as egocentric active interaction with humans and the environment. The aerospace embodied world model serves as an effective means to realize the autonomous intelligence of UAVs and represents a necessary pathway toward aerospace embodied intelligence. However, existing embodied world models primarily focus on ground-level intelligent agents in indoor scenarios, while research on UAV intelligent agents remains unexplored. To address this gap, we develop AeroSimulator, a simulation platform that encompasses 4 realistic urban scenes for UAV flight simulation. Additionally, we construct the first large-scale real-world image-text pre-training dataset, AerialAgent-Ego10k, featuring urban drones from a first-person perspective. We also create a virtual image-text-pose alignment dataset, CyberAgent-Ego500k, to facilitate the pre-training of the aerospace embodied world model. For the first time, we clearly define 5 downstream tasks, i.e., aerospace embodied scene awareness, spatial reasoning, navigational exploration, task planning, and motion decision, and construct corresponding instruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k and SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace embodiment world model. Simultaneously, we develop SkyAgent-Eval, the downstream task evaluation metrics based on GPT-4, to comprehensively, flexibly, and objectively assess the results, revealing the potential and limitations of 2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over 10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning datasets, more than 10 evaluation metrics, and a simulator into the benchmark suite, i.e., AeroVerse, which will be released to the community to promote exploration and development of aerospace embodied intelligence.", "sections": [{"title": "I. INTRODUCTION", "content": "DRONES have a wide range of applications, including mountainous photovoltaic inspection, river trash detection, pedestrian traffic monitoring at intersections, electric power inspection, and forest fire rescue [1]. However, these applications often depend on manual remote control of the drones. For instance, in UAV mountain photovoltaic inspections, it is necessary to deploy professional operators who spend several hours each day inspecting multiple stations. This practice can easily lead to operator fatigue, resulting in component defects and missed inspections. Therefore, there is an urgent need for UAVs equipped with autonomous intelligence to reduce costs and enhance efficiency.\nAerospace embodied intelligence refers to the specialized application of embodied intelligence within the aerospace sector, focusing on empowering unmanned platforms such as satellites, drones, and aircraft to autonomously integrate perception, cognition, and action. This integration aims to facilitate egocentric active interactions with both humans and the physical environment. Over the past year, visual-language models that encode world knowledge have rapidly advanced, driven by a wealth of high-fidelity simulators and datasets [2]\u2013[7], thereby presenting new opportunities for embodied intelligence. Numerous embodied world models [8]\u2013[15], have emerged, significantly enhancing the capabilities of embodied agents in perceiving their surroundings and planning tasks. Consequently, this article posits that the development of an aerospace embodied world model is a crucial strategy for achieving autonomous intelligent agents for drones and represents a necessary pathway toward advancing aerospace embodied intelligence.\nHowever, these embodied world models primarily focus on indoor scenarios (e.g., robotic arms) or ground-based agents in outdoor environments (e.g., unmanned vehicles) [8]\u2013[12]. There has been limited exploration of UAV embodied agents, particularly in the context of aerospace embodied world models that facilitate UAV autonomy, which is significantly constrained by the development of UAV embodied datasets. In contrast to indoor embodied intelligence datasets [8], [9], several key challenges arise in the construction of UAV embodied intelligence datasets:\nLack of Definition of UAV Embodied Tasks. In recent years, research on ground-oriented agents has gained significant attention, leading to clearer definitions of downstream"}, {"title": "II. RELATED WORK", "content": "The three-dimensional (3D) world encompasses not only horizontal and vertical dimensions but also depth, providing richer information than two-dimensional (2D) images. Depth accurately reflects fundamental aspects of the real world and enhances the ability of embodied agents to learn from and understand their 3D environment. Furthermore, textual annotations accompanying 3D visual-language datasets assist embodied agents in perceiving their surroundings and conducting spatial reasoning. However, challenges in creating 3D datasets have led to a scarcity of such resources, with only a limited number of datasets publicly available to date. For instance, the ScanQA dataset comprises 41,363 unique Q&A pairs, accompanied by 3D object localization annotations for 800 indoor 3D scenes [25]. The ScanRefer dataset contains 11,046 distinct Q&A pairs for 1,613 indoor 3D scenes [26]. The ScanNet dataset includes 1,513 indoor scenes featuring a total of 21 object categories [27].\nIn contrast to the aforementioned 3D visual-language datasets that focus on indoor environments, we have pioneered the development of a constructed 3D dataset that emphasizes large-scale urban scenes. This dataset encompasses areas ranging from (1.0 \u00d7 105) to (3.7 \u00d7 107) square meters and includes four representative urban environments, i.e., Shenzhen, Shanghai, Residence, and School. We select flying vehicles, specifically unmanned aerial vehicles (UAVs), as the agents due to their greater degree of freedom.\nB. Embodied Intelligence Datasets\nThe embodied world model serves as an effective approach for empowering embodied agents to interact with their environments, autonomously plan, make decisions, act, and perform tasks similar to human capabilities. Most existing embodied world models concentrate on mobile robots in indoor settings. For example, in the embodied question-and-answer task, Abhishek et al. introduce the EQA dataset, which\nA. 3D Visual-Language Datasets"}, {"title": "III. TASK FORMULATION", "content": "To facilitate the closed-loop training of perception, cognition, and action in UAV agents and to endow them with autonomous capabilities, this paper categorizes the downstream tasks into five distinct categories, as illustrated in Figure 2. It clearly defines the concepts associated with these tasks, standardizes the input and output formats, and offers innovative perspectives for further research on aerospace embodied intelligence in the context of UAVs.\nAerospace Embodied Scene Awareness. Given the current state of drone intelligent agents, specifically their position in three-dimensional space, drones describe surrounding environmental elements, such as buildings, in a panoramic manner (covering four directions: front, back, left, and right). This\ncapability is essential for the cognitive processes and actions of intelligent agents. Traditional environmental perception tasks generally involve inputting environmental images, extracting features from these images, and generating corresponding descriptions. In contrast, the objective of this task is to enhance the ability of UAV agents to perceive their environment and articulate 3D scenes based on their location coordinates.\nInput: Multi-perspective 2D images of the city's 3D scene, including $I_t = \\{i_{t,k}\\}_{k=1}^K$, depth map $D_t = \\{d_{t,k}\\}_{k=1}^K$, multi-perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^K$, and the current attitude of the drone in the environment.\nOutput: Scene element description $TEXT_{surrounding}$ of UAV agent in four directions, i.e., front, back, left, right,\n$TEXT_{surrounding} = f(I_t, D_t, P_t, P_{uav}, TEXT_{question})$\nAerospace Embodied Spatial Reasoning. Based on the current location and three-dimensional environment, the drone agent infers the object's orientation relationships, action trajectories, and counterfactual scenarios within the scene, guided by specific questions. The objective is to enhance the agent's understanding of the 3D spatial scene graph, which is a fundamental task of embodied cognition. Traditional spatial reasoning tasks primarily focus on recognizing spatial relationships between objects in a single 2D image, characterized by simplistic scenes and a limited number of objects. In contrast, this task emphasizes reasoning about relationships, intentions, counterfactuals, and other dimensions within three-dimensional space, which is inherently more complex and aligns more closely with human logical reasoning.\nInput: Multi-perspective 2D images of urban 3D scenes, including $I_t = \\{i_{t,k}\\}_{k=1}^K$, depth_map $D_t = \\{d_{t,k}\\}_{k=1}^K$, multi-perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^K$, current drone pose $P_{uav}$ in the environment, question $TEXT_{question}$.\nOutput: The answer $TEXT_{answer}$ to the question, i.e.,\n$TEXT_{answer} = f(I_t, D_t, P_t, P_{uav}, TEXT_{question})$\nAerospace Embodied Navigational Exploration. Given the UAV agent's initial position and its long-range, multi-stage"}, {"title": "IV. SIMULATION PLATFORM", "content": "Simulator. To simulate a realistic drone flight scenario, we utilize Unreal Engine 4 to load urban environments and select AirSim [16] for constructing the drone model. This enables us to develop a simulator, named AeroSimulator, capable of facilitating multiple action spaces for the drone, as illustrated in Figure 3. Adhering to the real-to-sim-to-real paradigm, we select four representative scenes from the high-quality UrbanScene3D dataset [17] created by Lin et al.: Shenzhen, Shanghai, School, and Residence, all derived from 3D reconstructions of actual physical locations. Furthermore, the simulator accommodates various lighting conditions (day,\nnavigation instructions, the agent is required to autonomously explore a large urban environment and answer questions related to object characteristics, such as the shape and color of buildings. This capability directly supports applications like object search and tracking in urban settings where building obstructions exist. Unlike traditional navigation tasks that rely solely on navigation instructions and do not include question-answering functions, this task necessitates that the agent not only autonomously navigate and explore its surroundings according to the provided instructions but also respond to inquiries based on the information it collects.\nInput: Multi-perspective 2D images of urban 3D scenes with $I_t = \\{i_{t,k}\\}_{k=1}^K$, depth map $D_t = \\{d_{t,k}\\}_{k=1}^K$, multi-perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^K$, current drone pose in the environment $P_{uav}$, navigation command $TEXT_{nav}$, $TEXT_{question}$.\nOutput: The answer $TEXT_{answer}$ to the question, i.e.,\n$TEXT_{answer} = f(I_t, D_t, P_t, P_{uav}, TEXT_{nav}, TEXT_{question})$\nAerospace Embodied Task Planning. By specifying the initial position and the anticipated endpoint for the UAV intelligent agent, the agent integrates the 3D environment to generate a detailed, step-by-step path planning process. This process requires the identification of distinct landmarks at each stage, which serves as the core task in UAV embodied cognition. Current path planning methods for indoor environments primarily focus on coarse-grained paths within a single room. In contrast, this task addresses large-scale urban scenes, where the starting and ending points may be separated by several city blocks. During maneuvers such as turning, moving straight, and ascending, the agent will identify observable landmark-level objects to enhance the accuracy of the path planning.\nInput: Multi-perspective 2D images of urban 3D scenes, including $I_t = \\{i_{t,k}\\}_{k=1}^K$, depth map $D_t = \\{d_{t,k}\\}_{k=1}^K$, multi-perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^K$, as well as the current attitude of the drone, $p_{uav}$, and target pose $p_{end}$.\nOutput: Step-by-step path plans $TEXT_{plan}$ and intermediate pose $P_{temp}$, i.e.,\n$TEXT_{plan}, P_{temp} = f(I_t, D_t, P_t, P_{uav}, P_{end})$\nAerospace Embodied Motion Decision. The intelligent drone agent operates in real-time, guided by its initial position and target endpoint. It dynamically interacts with its environment and adjusts its action strategy based on the outcomes of each movement and the historical sequence of actions. This iterative process continues until it reaches the endpoint. Unlike traditional decision-making tasks, this approach positions the drone as the agent, making decisions informed by first-person environmental observations at each navigation node. It encompasses a nearly complete end-to-end closed-loop of task chains, including perception, reasoning, planning, and action, representing the ultimate objective for drone agents.\nInput: Multi-perspective 2D images of urban 3D scenes, including $I_t = \\{i_{t,k}\\}_{k=1}^K$, depth map $D_t = \\{d_{t,k}\\}_{k=1}^K$, multi-perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^K$, position $P_{history} = \\{P_n\\}_{n=0}^{N-1}$, $I_{history} = \\{i_n\\}_{n=0}^{N-1}$, $A_{history} = \\{a_n\\}$ from 0 to N-1, and target pose $P_{end}$.\nOutput: Action $a_N$ at time N, i.e.,\n$a_N = f(I_t, D_t, P_{history}, I_{history}, A_{history}, P_{end})$"}, {"title": "V. DATASET SUITE", "content": "To address the shortage of large-scale training data for UAV agents, facilitate the training of aerospace embodied word models, and further advance research in aerospace embodied intelligence, we engage ten trained experts who dedicated eight months to developing a comprehensive dataset suite that encompasses two pre-training datasets and five downstream task instruction fine-tuning datasets.\nMulti-Resolution UAV First-Person View City Images. The first-person view images of real cities captured by drones are derived from the UrbanBIS dataset, which is collected using aerial photogrammetry and encompasses a wide array of urban scenes. Specifically, the UrbanBIS dataset [31] comprises 0.5 TB of aerial photographs from six actual locations: Qingdao, Wuhu, Longhua, Yuehai, Lihu, and Yingrenshi, covering a significant urban area of 10.78 km\u00b2 and including 3,370 buildings, with a total of 113, 346 aerial photogrammetry images. We have requested images from the authors for the regions of Lihu, Longhua, Yingrenshi, and Yuehai, with resolutions of 6000 \u00d7 4000, 8192 \u00d7 5640, 5472 \u00d7 3648, and 5472 \u00d7 3648, respectively, yielding a total of 15,094 images. From this dataset, we randomly selected 10,000 images to serve as first-person view representations of real cities captured by drones.\nFine-grained Multi-attribute First-view Text Generation. To generate high-quality environmental descriptions, we utilize LLaVA-1.5-13B [32] to produce detailed accounts of surrounding buildings, roads, trees, and other scenery from first-person perspective images captured by a drone, as illustrated in Figure 4 left (a). To standardize the format of the environmental descriptions generated by LLaVA-1.5-13B [32], we employ specific prompts that emphasize the quantity, appearance, and shape of the buildings in the images, particularly focusing on the spatial relationships among the objects. This approach enhances the spatial reasoning capabilities of the drone agent. Furthermore, we specify that the sky should not be described, as this scene is relatively uniform and appears consistent from various perspectives of the drone, providing insufficient information. Consequently, the generated descriptions ensure a degree of diversity, accuracy, and detail.\nB. CyberAgent-Ego500k\nMulti-Attribute First-Person Text Descriptions. The generated text descriptions provide comprehensive information regarding the attributes of objects in the drone's first-person images, including appearance, quantity, shape, absolute position, and relative position. Notably, the spatial relationships\nA. AerialAgent-Ego10k"}, {"title": "VI. EXPERIMENTS", "content": "Baselines Selection. Due to the current scarcity of research on aerospace-embodied world models, we evaluate several mainstream and representative 3D and 2D visual-language models. This assessment aims to explore their potential and limitations concerning the proposed aerospace-embodied downstream task datasets, thereby providing a preliminary foundation for future researchers in the field of aerospace-embodied intelligence. While there are more 2D visual-language models available that are generally more mature, we focus on LLaVA [33], MiniGPT4 [34], and BLIP2 [35], categorizing them into 7B and 13B models based on parameter scales. Given the limited availability of open-source 3D visual-language models, we select only the 3D-LLM [9] as our research focus.\nBaselines Modification. Among the selected baseline models, the 3D visual-language model can be applied to most of the defined downstream tasks; however, the 2D visual-language model cannot be directly utilized for testing due\nA. Baselines"}, {"title": "VII. RESULTS", "content": "As presented in Tables I, II, III, and IV, we summarize the overall performance of visual-language models across four UAV downstream tasks within the AeroVerse benchmark. Despite significant advancements in both 2D and 3D visual-language models (VLMs) in recent years, these models continue to encounter challenges with UAV-embodied tasks, including the GPT-4 series. Among the four tasks, existing visual-language models achieve relatively high scores only on SkyAgent-Scene3k, while their performance on the other tasks declines markedly. Overall, gpt-4-vision-review and gpt-4o consistently outperform other models. We will subsequently provide a detailed analysis of the various embodied tasks.\nResults on SkyAgent-Scene3k. In evaluating this task, we utilize BLEU, SPICE, and LLM-JUDGE-SCENE to assess the model's performance in terms of vocabulary richness, semantic accuracy, and human preference. The Qwen-lv-7b model [42] demonstrates strong performance on BLEU, leading in each urban scene, indicating its closer alignment with the reference vocabulary. Overall, the model achieving the highest score in SPICE is gpt-4o, which highlights its advantages in semantic matching. According to the results from LLM-JUDGE-SCENE, the outputs from gpt-4-vision-review and gpt-40 show greater consistency with human preferences.\nResults on SkyAgent-Reason3k. For evaluating, we utilize LLM-JUDGE-REASON to assess human preferences. Three models emerge as prominent in this context, i.e., two open-source models, llama-adapter-v2-7B [39] and qwen-lv-7b [42], along with one closed-source model, gpt-4o. A horizontal comparison among the gpt-4 series reveals that gpt-4o demonstrates superior capabilities in first-person spatial reasoning and question-answering tasks.\nB. Qualitative Analysis\nFrom Figure 11, although the 3D-LLM [9] encodes the 3D environment and perceives its surroundings, it demonstrates limited generalization due to insufficient training on outdoor 3D urban data. When confronted with a 3D urban scene, the output of 3D-LLM [9] resembles a description of an indoor environment, leading to significant hallucinations. The findings indicate that the performance of these 2D visual-language models surpasses that of 3D-LLM [9]. This superiority can be attributed to the greater number of training image-text pairs available for the 2D visual-language models, which enhances their generalization capabilities. Furthermore, they deliver more accurate descriptions based on egocentric view images of urban settings. However, instances of hallucinations\nA. Quantitative Analysis"}, {"title": "VIII. CONCLUSION", "content": "To address the existing research gap in the aerospace embodied world model and to empower UAV intelligent agents with end-to-end autonomous perception, cognition, and action capabilities, we have developed a comprehensive benchmark suite named AeroVerse. This suite integrates simulation, pre-training, fine-tuning, and evaluation processes. Specifically, we establish a simulation platform, AeroSimulator, which encompasses four realistic urban scenarios tailored for the flight simulation of UAV intelligent agents. Furthermore, we introduce the first image-text pre-training dataset, AerialAgent- Ego10k, utilizing real drone footage captured from a first- person perspective, along with the virtual image-text-pose alignment dataset, CyberAgent-Ego500k, to facilitate the pre- training of aerospace embodied world models. To enhance the autonomous capabilities of UAV intelligent agents, we delineate five downstream tasks for the first time: 3D scene awareness, spatial reasoning, navigational exploration, task planning, and motion decision-making. In line with these tasks, we construct five instruction fine-tuning datasets: SkyAgent- Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k, SkyAgent- Plan3k, and SkyAgent-Act3k. Additionally, we develop a"}]}