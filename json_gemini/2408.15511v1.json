{"title": "AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models", "authors": ["Fanglong Yao", "Yuanchang Yue", "Youzhi Liu", "Xian Sun", "Kun Fu"], "abstract": "Aerospace embodied intelligence aims to empower unmanned aerial vehicles (UAVs) and other aerospace platforms to achieve autonomous perception, cognition, and action, as well as egocentric active interaction with humans and the environment. The aerospace embodied world model serves as an effective means to realize the autonomous intelligence of UAVs and represents a necessary pathway toward aerospace embodied intelligence. However, existing embodied world models primarily focus on ground-level intelligent agents in indoor scenarios, while research on UAV intelligent agents remains unexplored. To address this gap, we develop AeroSimulator, a simulation platform that encompasses 4 realistic urban scenes for UAV flight simulation. Additionally, we construct the first large-scale real-world image-text pre-training dataset, AerialAgent-Ego10k, featuring urban drones from a first-person perspective. We also create a virtual image-text-pose alignment dataset, CyberAgent-Ego500k, to facilitate the pre-training of the aerospace embodied world model. For the first time, we clearly define 5 downstream tasks, i.e., aerospace embodied scene awareness, spatial reasoning, navigational exploration, task planning, and motion decision, and construct corresponding instruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k and SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace embodiment world model. Simultaneously, we develop SkyAgent-Eval, the downstream task evaluation metrics based on GPT-4, to comprehensively, flexibly, and objectively assess the results, revealing the potential and limitations of 2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over 10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning datasets, more than 10 evaluation metrics, and a simulator into the benchmark suite, i.e., AeroVerse, which will be released to the community to promote exploration and development of aerospace embodied intelligence.", "sections": [{"title": "I. INTRODUCTION", "content": "DRONES have a wide range of applications, including mountainous photovoltaic inspection, river trash detection, pedestrian traffic monitoring at intersections, electric power inspection, and forest fire rescue [1]. However, these applications often depend on manual remote control of the drones. For instance, in UAV mountain photovoltaic inspections, it is necessary to deploy professional operators who spend several hours each day inspecting multiple stations. This practice can easily lead to operator fatigue, resulting in component defects and missed inspections. Therefore, there is an urgent need for UAVs equipped with autonomous intelligence to reduce costs and enhance efficiency.\nAerospace embodied intelligence refers to the specialized application of embodied intelligence within the aerospace sector, focusing on empowering unmanned platforms such as satellites, drones, and aircraft to autonomously integrate perception, cognition, and action. This integration aims to facilitate egocentric active interactions with both humans and the physical environment. Over the past year, visual-language models that encode world knowledge have rapidly advanced, driven by a wealth of high-fidelity simulators and datasets [2]\u2013[7], thereby presenting new opportunities for embodied intelligence. Numerous embodied world models [8]\u2013[15], have emerged, significantly enhancing the capabilities of embodied agents in perceiving their surroundings and planning tasks. Consequently, this article posits that the development of an aerospace embodied world model is a crucial strategy for achieving autonomous intelligent agents for drones and represents a necessary pathway toward advancing aerospace embodied intelligence.\nHowever, these embodied world models primarily focus on indoor scenarios (e.g., robotic arms) or ground-based agents in outdoor environments (e.g., unmanned vehicles) [8]\u2013[12]. There has been limited exploration of UAV embodied agents, particularly in the context of aerospace embodied world models that facilitate UAV autonomy, which is significantly constrained by the development of UAV embodied datasets. In contrast to indoor embodied intelligence datasets [8], [9], several key challenges arise in the construction of UAV embodied intelligence datasets:\nLack of Definition of UAV Embodied Tasks. In recent years, research on ground-oriented agents has gained significant attention, leading to clearer definitions of downstream tasks such as indoor/outdoor navigation [3], [5], command following [2], and embodied question answering. However, UAV agents must comprehend the intrinsic correlations of four-dimensional space-time and perform actions under conditions of scene randomization and local observability of the environment. This involves aspects such as awareness, cognition, planning, and decision-making. The diversity and interdependence of these downstream tasks result in a lack of clarity in the task definitions for aerial-embodied agents.\nDifficulty in UAV 3D Data Acquisition. The widespread use of LiDAR technology in mobile smart devices has facilitated the easy acquisition of indoor 3D data, leading to substantial accumulation. In contrast, obtaining outdoor 3D data necessitates specialized equipment, such as drones, which presents a higher barrier to entry. Furthermore, outdoor 3D data acquisition requires skilled professionals to operate drones and collect extensive point cloud data over larger areas.\nHigh Cost of UAV Embodied Data Collection. UAVs possess a greater range of motion compared to ground agents (e.g., indoor sweeping robots), allowing for a high degree of freedom in three-dimensional space. They can operate over extensive areas (ranging from dozens to hundreds of square kilometers) and navigate complex environments characterized by irregularly distributed obstacles, e.g., buildings and trees. Consequently, this necessitates extensive training for annotators to effectively conduct data collection for UAV agents.\nTherefore, our paper, for the first time, explicitly defines five downstream tasks for UAV-embodied agents, highlighting directions for further exploration in this field, as follows:\n\u2022 Aerospace Embodied Scene Awareness: UAV-agent perceives the surrounding 3D environment from a first- person perspective to enhance scene understanding.\n\u2022 Aerospace Embodied Spatial Reasoning: The UAV agent models the spatial relationships between objects within a 3D scene, enabling reasoning about the relationships among these objects.\n\u2022 Aerospace Embodied Navigational Exploration: The UAV agent comprehends navigation commands and navigates to the destination while describing the environment.\n\u2022 Aerospace Embodied Task Planning: UAV-agent generates detailed, landmark-level long-range path planning scenarios to reach the destination.\n\u2022 Aerospace Embodied Motion Decision: The UAV agent provides a complete sequence of actions from the starting point to the destination, thereby realizing an end-to-end closed loop of the scene awareness, path planning, and action decision-making.\nAs illustrated in Figure 1, we address the gap in the UAV-agent dataset and enhance the training of aerospace embodied world models by constructing the first large- scale virtual-reality pre-training dataset alongside a high- quality instruction dataset. Specifically, the first-person, high- resolution real-world pre-training dataset of high-altitude drones, AerialAgent-Ego10k, is derived from the Urban- BIS dataset. Additionally, we develop the aligned pre- training dataset, CyberAgent-Ego500k, which includes per- spective images, scene text descriptions, and drone attitudes. Furthermore, we create five downstream task instruction datasets: SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent- Nav3k and SkyAgent-Plan3k, and SkyAgent-Act3k. These datasets are constructed using our established simulation plat- form, AeroSimulator, which employs Unreal Engine 4, the Microsoft AirSim drone simulator [16], and the 3D Urban- Scene virtual city dataset [17]. This encompasses four real- world urban scenarios, i.e., Shanghai, Shenzhen, School, and Residence, with a coverage area ranging from 1.0 \u00d7 105 to"}, {"title": "II. RELATED WORK", "content": "The three-dimensional (3D) world encompasses not only horizontal and vertical dimensions but also depth, providing richer information than two-dimensional (2D) images. Depth accurately reflects fundamental aspects of the real world and enhances the ability of embodied agents to learn from and understand their 3D environment. Furthermore, textual annotations accompanying 3D visual-language datasets assist embodied agents in perceiving their surroundings and conducting spatial reasoning. However, challenges in creating 3D datasets have led to a scarcity of such resources, with only a limited number of datasets publicly available to date. For instance, the ScanQA dataset comprises 41,363 unique Q&A pairs, accompanied by 3D object localization annotations for 800 indoor 3D scenes [25]. The ScanRefer dataset contains 11,046 distinct Q&A pairs for 1,613 indoor 3D scenes [26]. The ScanNet dataset includes 1,513 indoor scenes featuring a total of 21 object categories [27].\nIn contrast to the aforementioned 3D visual-language datasets that focus on indoor environments, we have pioneered the development of a constructed 3D dataset that emphasizes large-scale urban scenes. This dataset encompasses areas ranging from (1.0 \u00d7 105) to (3.7 \u00d7 107) square meters and includes four representative urban environments, i.e., Shenzhen, Shanghai, Residence, and School. We select flying vehicles, specifically unmanned aerial vehicles (UAVs), as the agents due to their greater degree of freedom.\nB. Embodied Intelligence Datasets\nThe embodied world model serves as an effective approach for empowering embodied agents to interact with their environments, autonomously plan, make decisions, act, and perform tasks similar to human capabilities. Most existing embodied world models concentrate on mobile robots in indoor settings. For example, in the embodied question-and- answer task, Abhishek et al. introduce the EQA dataset, which"}, {"title": "III. TASK FORMULATION", "content": "To facilitate the closed-loop training of perception, cognition, and action in UAV agents and to endow them with autonomous capabilities, this paper categorizes the downstream tasks into five distinct categories, as illustrated in Figure 2. It clearly defines the concepts associated with these tasks, standardizes the input and output formats, and offers innovative perspectives for further research on aerospace embodied intelligence in the context of UAVs.\nAerospace Embodied Scene Awareness. Given the current state of drone intelligent agents, specifically their position in three-dimensional space, drones describe surrounding environmental elements, such as buildings, in a panoramic manner (covering four directions: front, back, left, and right). This capability is essential for the cognitive processes and actions of intelligent agents. Traditional environmental perception tasks generally involve inputting environmental images, extracting features from these images, and generating corresponding descriptions. In contrast, the objective of this task is to enhance the ability of UAV agents to perceive their environment and articulate 3D scenes based on their location coordinates.\nInput: Multi-perspective 2D images of the city\u2019s 3D scene, including $I_t = \\{i_{t,k}\\}_{k=1}^{K}$, depth map $D_t = \\{d_{t,k}\\}_{k=1}^{K}$, multi- perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^{K}$, and the current attitude of the drone in the environment.\nOutput: Scene element description $\\text{TEXT}_{surrounding}$ of UAV agent in four directions, i.e., front, back, left, right,\n$\\text{TEXT}_{surrounding} = f(I_t, D_t, P_t, P_{uav}, \\text{TEXT}_{question})$\nAerospace Embodied Spatial Reasoning. Based on the current location and three-dimensional environment, the drone agent infers the object\u2019s orientation relationships, action trajectories, and counterfactual scenarios within the scene, guided by specific questions. The objective is to enhance the agent\u2019s understanding of the 3D spatial scene graph, which is a fundamental task of embodied cognition. Traditional spatial reasoning tasks primarily focus on recognizing spatial relationships between objects in a single 2D image, characterized by simplistic scenes and a limited number of objects. In contrast, this task emphasizes reasoning about relationships, intentions, counterfactuals, and other dimensions within three- dimensional space, which is inherently more complex and aligns more closely with human logical reasoning.\nInput: Multi-perspective 2D images of urban 3D scenes, including $I_t = \\{i_{t,k}\\}_{k=1}^{K}$, depth_map $D_t = \\{d_{t,k}\\}_{k=1}^{K}$, multi- perspective camera pose $P_t = \\{p_{t,k}\\}_{k=1}^{K}$, current drone pose $P_{uav}$ in the environment, question $\\text{TEXT}_{question}$.\nOutput: The answer $\\text{TEXT}_{answer}$ to the question, i.e.,\n$\\text{TEXT}_{answer} = f(I_t, D_t, P_t, P_{uav}, \\text{TEXT}_{question})$\nAerospace Embodied Navigational Exploration. Given the UAV agent\u2019s initial position and its long-range, multi-stage"}, {"title": "IV. SIMULATION PLATFORM", "content": "Simulator. To simulate a realistic drone flight scenario, we utilize Unreal Engine 4 to load urban environments and select AirSim [16] for constructing the drone model. This enables us to develop a simulator, named AeroSimulator, capable of facilitating multiple action spaces for the drone, as illustrated in Figure 3. Adhering to the real-to-sim-to- real paradigm, we select four representative scenes from the high-quality UrbanScene3D dataset [17] created by Lin et al.: Shenzhen, Shanghai, School, and Residence, all derived from 3D reconstructions of actual physical locations. Furthermore, the simulator accommodates various lighting conditions (day, evening, night, etc.), seasonal variations (spring, summer, autumn, winter), and climatic modes (sunny, cloudy, light snow, etc.), thereby enhancing the transferability of the trained drone agent to real-world applications. Within the simulator, the drone can continuously navigate the urban environment we have loaded, capturing data visually through an integrated RGB, depth, and object segmentation cameras, which output corresponding first-person perspective images in real time.\nScenes. To bridge the gap between transferring drone in- telligent agents from simulated environments to real-world scenarios, we utilize UrbanScene3D [17], a large-scale data platform specifically designed for urban scene perception and reconstruction. This platform comprises over 128,000 high- resolution images captured from various cities. The selected 3D scenes from four cities, as illustrated in Figure 3, fea- ture detailed architectural elements, including office buildings, shopping centers, residential complexes, bus stations, and sub- way entrances and exits. Additionally, these scenes encompass specific street details such as lanes, sidewalks, crossroads, traffic signals, and road markings, along with other urban features like streetlights, signs, trees, shrubs, and lawns. These attributes facilitate the exploration of diverse urban environ- ments by drone intelligent agents. Among the cities, Shanghai presents the most extensive urban scene, featuring 6,850 objects and covering an area of 3,700 hectares. This extensive environment is advantageous for training UAV agents in long- distance navigation and path planning. In contrast, the urban scene in Shenzhen is relatively compact, covering an area of 300 hectares with only 1,126 objects; however, it enhances the spatial reasoning capabilities of drone intelligent agents in smaller settings. Furthermore, the campus area, which spans 130 hectares and contains 178 objects, and the residential zone, covering 30 hectares with 34 objects, focus on localized environments characterized by dense buildings and obstacles such as trees and equipment. This concentration improves scene understanding and decision-making skills, including obstacle avoidance.\nObservations. In the simulator, the drone is generated using AirSim, which features five built-in cameras: forward, back- ward, left, right, and overhead views. Each camera operates in three modes:\nRGB Camera. Captures RGB images with a resolution of 1920\u00d71080, saved in PNG format.\nDepth Camera. Produces depth images based on the posi- tional information between the camera and the object, main- taining the same resolution as the RGB camera and also saved in PNG format. In this experiment, when the distance exceeds 500 meters, the image appears entirely white; for distances below 500 meters, varying shades of black are displayed according to proximity.\nObject Segmentation Camera. Retrieves the object segmen- tation map, segmenting the image into different colors based on object types\u2014gray for buildings, green for trees, and red for vehicles. The resolution of the segmentation image matches that of the RGB camera and is saved in PNG format.\nActions. The simulator supports drone intelligent agents in altering their position (x, y, z coordinates), direction (pitch, yaw, roll), and speed, while also enabling more complex maneuvers through acceleration adjustments and the appli- cation of force vectors. To facilitate the training of UAV agents, we have preliminarily identified the eight most common low-level actions for drones: forward, left turn, right turn, ascend, descend, left shift, right shift, and stop. To balance the frequency of actions during the trajectory with the actual movement of the drone in an outdoor environment, the \u201cforward movement\u201d action propels the drone continuously for 5 meters in the current direction, while the \u201cleft movement\u201d and \u201cright movement\u201d actions shift the drone continuously for 1 meter in their respective directions. The left and right rotation actions enable horizontal rotation by 15 degrees, and the ascending and descending actions allow vertical movement for 1 meter."}, {"title": "V. DATASET SUITE", "content": "To address the shortage of large-scale training data for UAV agents, facilitate the training of aerospace embodied word models, and further advance research in aerospace embodied intelligence, we engage ten trained experts who dedicated eight months to developing a comprehensive dataset suite that encompasses two pre-training datasets and five downstream task instruction fine-tuning datasets.\nA. AerialAgent-Ego10k\nMulti-Resolution UAV First-Person View City Images. The first-person view images of real cities captured by drones are derived from the UrbanBIS dataset, which is collected using aerial photogrammetry and encompasses a wide array of urban scenes. Specifically, the UrbanBIS dataset [31] comprises 0.5 TB of aerial photographs from six actual locations: Qingdao, Wuhu, Longhua, Yuehai, Lihu, and Yingrenshi, covering a significant urban area of 10.78 km\u00b2 and including 3,370 buildings, with a total of 113, 346 aerial photogrammetry images. We have requested images from the authors for the regions of Lihu, Longhua, Yingrenshi, and Yuehai, with resolutions of 6000 \u00d7 4000, 8192 \u00d7 5640, 5472 \u00d7 3648, and 5472 \u00d7 3648, respectively, yielding a total of 15,094 images. From this dataset, we randomly selected 10,000 images to serve as first-person view representations of real cities captured by drones.\nFine-grained Multi-attribute First-view Text Generation. To generate high-quality environmental descriptions, we utilize LLaVA-1.5-13B [32] to produce detailed accounts of surrounding buildings, roads, trees, and other scenery from first-person perspective images captured by a drone, as illustrated in Figure 4 left (a). To standardize the format of the environmental descriptions generated by LLaVA-1.5-13B [32], we employ specific prompts that emphasize the quantity, appearance, and shape of the buildings in the images, particularly focusing on the spatial relationships among the objects. This approach enhances the spatial reasoning capabilities of the drone agent. Furthermore, we specify that the sky should not be described, as this scene is relatively uniform and appears consistent from various perspectives of the drone, providing insufficient information. Consequently, the generated descriptions ensure a degree of diversity, accuracy, and detail.\nB. CyberAgent-Ego500k\nImage Acquisition. We require trained drone pilots to operate drones in four virtual cityscapes: Shenzhen, School, Residence, and Shanghai. The flight range encompasses the entirety of these city scenes, with dense sampling conducted in areas characterized by a high density of objects, such as buildings. To prevent the drones from encountering obstacles, a selection of drone poses is recorded at random. Based on these poses, a total of 1,040,924 first-person perspective images, each with a resolution of 512\u00d7512 pixels, are generated within the virtual cityscapes. From this collection, 500,000 images are randomly selected to construct the image-text-pose dataset.\nFirst-Person Image-Text-Pose Generation. As illustrated on the right side of Figure 4 (a), the dataset construction method aligns with that of AerialAgent-10k and exhibits the following three characteristics:\nDrone First-Person Images in Multi-City Scenes. Collected from 3D simulators in Shanghai (large areas), Shenzhen (mul- tiple blocks), campuses (featuring numerous obstacles such as trees), and residential areas (characterized by dense buildings and narrow pathways), this approach aims to minimize the gap between simulated and real-world environments.\nMulti-Attribute First-Person Text Descriptions. The generated text descriptions provide comprehensive information regarding the attributes of objects in the drone's first-person images, including appearance, quantity, shape, absolute position, and relative position. Notably, the spatial relationships among objects are crucial for enhancing the spatial reasoning capabilities of the drone agent.\nImage-Text-Pose Alignment. In addition to the images and their corresponding text descriptions, this method incorporates the drone\u2019s attitude (position and orientation) in 3D space. The objective is to integrate the drone\u2019s spatial positioning into the aerospace-embodied world model, thereby enhancing the drone\u2019s self-centered scene understanding capabilities.\nDataset statistics. Figure 4 (b), (c), and (d) in the right block present detailed statistical results for the CyberAgent- 500k dataset. The maximum length of the image descriptions is 865 words, with an average length of 127 words. Further- more, the maximum number of sentences per image description is 129, with an average of 10 sentences. The dataset contains a total of 4,725,682 sentences and 63, 539, 302 words, including 94,823 unique words. These statistical re- sults indicate that this dataset surpasses most existing visual- language datasets in terms of scale, text length, sentence count, and the alignment of drone poses.\nC. SkyAgent-Scene3k\nDataset Construction. We require the annotator to control the drone to navigate within the 3D virtual city scene, select its current posture, and describe the surrounding environment from four perspectives: front, back, left, and right. The description format is fixed as follows: \u201cfront object description\", right object description, back object description , left object description \u201d. The object description should include the elements \u201cquantifier + color + specific description + shape + object\u201d, as illustrated in Figure 5 (a). To ensure data quality, we conduct rigorous inspections, requiring different annotators performing the same task to cross-check their work, followed by cross-checking between annotators from different cities. In summary, SkyAgent-Scene3k possesses the following characteristics:\nDiversified Object Types and Instructions. The primary objects include buildings, roads, trees, and grasslands within urban areas. Additionally, we have developed over 20 distinct instructions, as illustrated in Figure 5 (e), to enhance the generalization capabilities of task understanding.\nMulti-Directional and Multi-Attribute Environment Descrip- tion. Focusing on the drone intelligent agent, descriptions of both close-range and long-range scenes are provided from four perspectives: front, back, left, and right. Buildings are char- acterized by their height, appearance, and color, while roads are described based on the number of lanes, intersections, and directional extensions.\nMulti-Perspective 2D Images, Depth Maps, Camera Poses, Drone Poses, and Scene Description Alignment. Multi- perspective images, depth maps, and camera poses of urban landscapes facilitate the reconstruction of a three-dimensional representation of the entire city, assisting drone agents in understanding the spatial relationships between objects and enhancing their perception of three-dimensional scenes.\nDataset statistics. Figure 5 (b), (c), and (d) illustrate the distribution of description lengths, the number of sentences, and statistical information regarding scene descriptions. As shown in Figure 5 (b), the lengths of the descriptions range from 30 to 80 words. Generally, longer descriptions suggest a more complex scene with a greater number of environmental elements that require articulation. Figure 5 (c) indicates that most descriptions consist of four sentences, as we instruct annotators to depict each scene from four perspectives: front, back, left, and right. Descriptions containing 1~3 sentences occur when annotators consolidate multiple perspectives into a single sentence. In total, this dataset comprises 121, 252 words and 1,162 distinct word types.\nD. SkyAgent-Reason3k\nDataset Construction. To enhance the cognitive reasoning abilities of UAV agents in three-dimensional urban environments, we require annotators to navigate the 3D city scene, adopt specific postures to pause, establish targeted spatial positions, and create question-and-answer pairs regarding var- ious features encountered by the UAV, including buildings, roads, trees, and grasslands. Specifically, inquiries pertaining to buildings should focus on attributes such as height, ap- pearance, and color, while questions related to roads should address the number of lanes, intersections, and direction of extension. As illustrated in Figure 6 (a), each question in this dataset must be answered accurately through spatial reasoning in conjunction with the three-dimensional environment. This process can be further categorized into six distinct modes of reasoning.\n\u2022 Color Reasoning. This reasoning process involves prompting the drone\u2019s intelligent agent to identify and inquire about the colors of specific objects encountered as it approaches a designated spatial location. This necessitates the agent\u2019s ability to recognize colors based on the identified targets.\n\u2022 Count Reasoning. Requires the intelligent agent to compute the number of specific objects encountered while following short-range instructions.\n\u2022 Shape Reasoning. This reasoning necessitates that the drone\u2019s intelligent agent describes the specific shapes of the objects it encounters upon arriving at the designated target area.\n\u2022 Object Reasoning. Requires UAV intelligent agents to enumerate the buildings and other objects they encounter while navigating to a specific spatial location.\n\u2022 Predictive Reasoning. Upon satisfying certain precondi- tions, the drone must predict potential objects and actions it may encounter.\n\u2022 Counterfactual Reasoning: This reasoning involves presenting a hypothesis to the drone agent that contradicts established facts, requiring the agent to respond to the hypothesis.\nDataset Statistics. Figure 6 (b) illustrates that the length distribution of questions ranges from 7 to 45 words, significantly surpassing the statistics of questions found in existing VQA datasets in terms of both coverage and length. Figure 6 (c) indicates that the length of answers varies from 2 to 40 words, with the majority consisting of 2 to 10 words, thereby allowing the drone agent to deliver concise responses. Figures 6 (d) and (e) present statistical analyses of the questions and answers, respectively. The results reveal that, although the word count of the questions is approximately three times greater than that of the answers (66,645 vs. 22,784), the vocabulary diversity is actually lower in the questions than in the answers (411 vs. 450). This discrepancy underscores the potential for drone intelligent agents to enhance their vocabulary in responses.\nE. SkyAgent-Nav3k\nDataset Construction. We require annotators to control drones to fly specific distances within an urban environment, annotate the textual descriptions of the flight paths, record the starting and ending positions, and design a set of question- and-answer pairs primarily addressing whether actions such as flying straight, turning left, or turning right will occur, as well as the types of buildings, intersections, and lanes encountered. Additionally, manual cross-validation is employed to ensure the quality of the annotations. Two specific examples are illus- trated in Figure 7 (a), from which the following characteristics of the dataset can be derived:\nRefined Object Attribute Description and Navigation Instructions. The navigation instructions provide comprehensive descriptions of the object, detailing its appearance, quantity, shape, color, and relative position to the drone\u2019s intelligent agent. This ensures the uniqueness of the object in the instructions and minimizes the error recognition rate.\nLong-Range Navigation Path Guided by Multiple Landmarks. The navigation instructions encompass extended paths that necessitate multiple consecutive spatial inferences by drones to traverse various blocks within the city. Furthermore, the instructions include specific descriptions of landmarks that can assist the drone\u2019s intelligent agent in adjusting its actions.\nNavigation-Based Scene Exploration. In addition to requiring the drone to adhere to language instructions for navigating to a designated location, this dataset also compels the drone agent to articulate environmental information regarding the destination, such as the color and shape of buildings.\nDataset Statistics. From Figure 7 (b), it is evident that the length of navigation instructions predominantly ranges from 20 to 80, exhibiting a relatively even distribution, with a few instances exceeding 100, which surpasses most existing navigation datasets. Longer navigation instructions can enhance drones' long-range spatial reasoning abilities. Figure 7 (c) indicates that the lengths of answers primarily fall between 2 and 10, facilitating drone agents in succinctly describing objects to be explored. Figures 7 (d) and (e) present statistical analyses of the navigation instructions and answers, revealing average lengths of 50 and 8, respectively, with an average of 2 sentences for navigation instructions and 1 sentence for answers. This variance arises because navigation commands encompass both long-distance, multi-step instructions and at- tribute queries regarding unknown objects.\nF. SkyAgent-Plan3k\nDataset Construction. We require drone pilots to identify the starting and ending points prior to operating the drone. After flying for a specified duration, they should select a position that serves as the midpoint of the trajectory and provide a description of the route from the previous trajectory to the current location. To generate high-quality route descriptions, we ask drone pilots to choose the optimal path based on their experience. Furthermore, we require professional annotators to provide detailed descriptions of sub-routes in specific scenar- ios, such as making turns, navigating intersections, or passing by five buildings in a single direction. Figure 8 (a) illustrates an example of path planning, demonstrating the following characteristics:\nRefined Self-Centered Object Description. The drone agent provides a distinctive and identifiable description of objects based on color, shape, height, and structure, employing a first- person perspective. The objects include buildings, pathways, trees, and grasslands that sequentially appear on both the left and right sides.\nMulti-Perspective Object Localization. In three-dimensional urban environments, the UAV agent accurately locates instance-level objects, such as buildings, by establishing spa- tial relationships relative to itself, thereby enhancing the precision of object localization.\nLandmark-Guided Path Planning. Prior to executing ma- neuvers such as turning or proceeding straight, the UAV intelligent agent identifies a landmark as a reference point, thereby improving the accuracy of path planning.\nDataset Statistics. Figure 8 (b) presents several instructions for the drone agent concerning path planning, each requiring the agent to avoid obstacles while navigating from the starting point to the endpoint. Figure 8 (c) illustrates that the planned lengths range significantly from 25 to 225 and predominantly follow a normal distribution. Figure 8 (d) indicates that the majority of the dataset consists of planning for five sub-paths. This requirement is designed to enhance planning complexity, necessitating the drone to perform at least five actions and navigate over five objects, thereby improving its capability to plan for longer distances. Figure 8 (e) reveals that the average length of the plans is 110, which is generally higher than the task planning lengths observed in most indoor scenarios.\nG. SkyAgent-Act3k\nDataset Construction. This task involves recording the dense motion sequence and orientation of the drone, with a particular emphasis on its flight path. Consequently, we restrict the drone\u2019s flight altitude to within 30 meters. The drone pilot is required to select both the starting and ending points, maneuver the drone to depart from the starting location, and leverage their experience to choose an appropriate route to reach the destination. This process allows us to capture the starting point, ending point, drone orientation, and action sequence. To ensure a high-quality path of reasonable length, we instruct drone pilots to avoid choosing arbitrary routes, such as unnecessary detours. Additionally, the need for drone pilots to survey their surroundings to ascertain their position and determine the next destination may lead to excess motion. We mitigate this excess motion through post-processing to achieve a smoother trajectory. Figure 9 (a) illustrates a series of drone action decisions, which exhibit the following characteristics:\nStarting and Ending Points Beyond Visual Range: To enhance the long-range autonomous action control capability of UAV intelligent agent in large-scale urban environments, there must be a minimum of ten buildings situated between the starting and ending points, with these buildings not aligned on the same straight line. This necessitates that the UAV intelligent agent execute at least one turn.\nProfessional Path Selection: Upon determining the starting and ending points, the drone pilot selects the optimal flight route based on experience, while ensuring that the flight altitude does not exceed 30 meters. The route selection must avoid collisions with surrounding objects and unnecessary turns and detours.\nSmooth Action Sequence: The drone pilot consciously avoids sharp turns, emergency stops, and abrupt maneuvers when performing turns, ascents, and other actions during flight, striving to ensure smooth transitions in the drone\u2019s movements.\nDataset Statistics. Figure 9 (b) presents several examples of motion decision-making instructions, illustrating that these\""}, {"title": "VI. EXPERIMENTS", "content": "Baselines Selection. Due to the current scarcity of research on aerospace-embodied world models, we evaluate several mainstream and representative 3D and 2D visual-language models. This assessment aims to explore their potential and limitations concerning the proposed aerospace-embodied downstream task datasets, thereby providing a preliminary foundation for future researchers in the field of aerospace- embodied intelligence. While there are more 2D visual- language models available that are generally more mature, we focus on LLaVA [33], MiniGPT4 [34], and BLIP2 [35], categorizing them into 7B and 13B models based on parameter scales. Given the limited availability of open-source 3D visual- language models, we select only the 3D-LLM [9] as our research focus.\nBaselines Modification. Among the selected baseline models, the 3D visual-language model can be applied to most of the defined downstream tasks; however, the 2D visual-language model cannot be directly utilized for testing due to a mismatch in input formats, as illustrated in Figure 10. Consequently, we modify the inputs and outputs of these models to align with the downstream tasks, as detailed below. Notably, Aerospace Embodied Motion Decision represents the culmination of aerospace embodied tasks, achieving a closed loop of perception, cognition, and action for the UAV agent. Adjusting existing visual-language models presents challenges, and we will continue to explore this area in future.\nAerospace Embodied Scene Awareness. This task involves utilizing the location and environmental data captured by the drone as input to generate scene descriptions of the surround- ing environment from multiple perspectives. However, the 2D visual-language model is inherently limited to processing images and does not directly account for environmental features."}, {"title": "VII. RESULTS", "content": "As presented in Tables I, II, III, and IV, we summarize the overall performance of visual-language models across four UAV downstream tasks within the AeroVerse benchmark. Despite significant advancements in both 2D and 3D visual-language models (VLMs) in recent years, these models continue to encounter challenges with UAV-embodied tasks, including the GPT-4 series. Among the four tasks, existing visual-language models achieve relatively high scores only on SkyAgent-Scene3k, while their performance on the other tasks declines markedly. Overall, gpt-4-vision-review and gpt- 40 consistently outperform other models. We will subsequently provide a detailed analysis of the various embodied tasks.\nResults on SkyAgent-Scene3k. In evaluating this task, we utilize BLEU, SPICE, and LLM-JUDGE-SCENE to assess the model\u2019s performance in terms of vocabulary richness, semantic accuracy, and human preference. The Qwen-lv-7b model [42", "39": "and qwen-lv-7b [42"}]}