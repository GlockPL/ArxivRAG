{"title": "Variational Randomized Smoothing for Sample-Wise Adversarial Robustness", "authors": ["Ryo Hase", "Ye Wang", "Toshiaki Koike-Akino", "Jing Liu", "Kieran Parsons"], "abstract": "Randomized smoothing is a defensive technique to achieve enhanced robustness against adversarial examples which are small input perturbations that degrade the performance of neural network models. Conventional randomized smoothing adds random noise with a fixed noise level for every input sample to smooth out adversarial perturbations. This paper proposes a new variational framework that uses a per-sample noise level suitable for each input by introducing a noise level selector. Our experimental results demonstrate enhancement of empirical robustness against adversarial attacks. We also provide and analyze the certified robustness for our sample-wise smoothing method.", "sections": [{"title": "Introduction", "content": null}, {"title": "1.1 Background", "content": "Neural networks are vulnerable to adversarial attacks that degrade the performance [4,8]. Adversarial attacks using adversarial examples can often seriously deteriorate prediction results of a neural network by adding small perturbations to input of the network. For example, the Projected Gradient Descent (PGD) attack [5] is widely used example of a white-box adversarial attack that utilizes knowledge of the target neural network such as weights and gradients. Considering defensive methods for neural networks is crucial to improve robustness against adversarial attacks."}, {"title": "1.2 Related work", "content": "Various types of defense mechanisms have been proposed to protect neural networks from adversarial attacks. Adversarial training [4] enhances the robustness of neural networks by adding adversarial examples as training data. For instance, PGD is commonly used to generate the examples used in adversarial training."}, {"title": "1.3 Contributions", "content": "Inspired by the related studies, we propose variational randomized smoothing, which is a framework utilizing a selector to produce a noise level for every input of a smoothed classifier.\nWe introduce a variational framework to build a noise level selector composed of a neural network to determine sample-wise noise levels for randomized smoothing.\nWe propose a universal training scheme using stochastic regularization, which makes a selector learn various conditions to produce different noise strength at once by randomly sampling regularization parameter \\(\\lambda\\).\nWe improve the controllability in the universal training by using conditional meta learning, which enables the user to freely adjust noise strength by specifying \\(\\lambda\\) at test time without retraining.\nSince the selector itself is a neural network which could be potentially a target of adversarial attacks, we also propose a defensive method called dual smoothing to protect our selector as well as base classifier.\nWe provide a modified certified robustness for sample-wise smoothing methods, based on the bound of median smoothing [2].\nExperimental results demonstrate that our proposed methods offer better empirical robustness compared to the conventional randomized smoothing."}, {"title": "2 Randomized smoothing", "content": null}, {"title": "2.1 Fundamental mathematical formulations", "content": "Randomized smoothing [3] is a defense method applied to a base classifier \\(f : \\mathcal{X} \\rightarrow \\mathcal{C}\\), where \\(\\mathcal{X} \\subset \\mathbb{R}^d\\) is the input (e.g., image) space and \\(\\mathcal{C} \\subseteq \\{1, 2, ..., M\\}\\)"}, {"title": "2.2 Practical randomized smoothing", "content": "As the calculation of the ideal smoothed classifier in (1) is generally intractable, typically a Monte-Carlo approximation is utilized [3]. \\( \\mathcal{N} \\) samples of Gaussian noise, as given by\n\\[\ng(x) = \\underset{c \\in \\mathcal{C}}{\\operatorname{argmax}} \\sum_{k=1}^N \\mathbb{I} [f(x + \\varepsilon_k) = c],\n\\]\n\\(\\varepsilon_k \\stackrel{\\text { iid }}{\\sim} \\mathcal{N}(0,\\sigma_s^2 I_d)\\) for \\(k \\in \\{1,2,..., N\\}\\). Note that this algorithm is specified to abstain from making a prediction, if statistical confidence is not satisfied during certification, as described in the following.\nBased on the ideal certified radius, given in (2), a practical certified guarantee is provided by estimating bounds on \\(p_a\\) and \\(p_b\\) for a given confidence level \\(\\alpha\\), based on statistical tests applied to the outputs \\(f(x + \\varepsilon_k)\\) over the \\( \\mathcal{N} \\) samples of Gaussian noise [3]. Given a confident lower bound \\(p_a\\) on the probability \\(p_a\\), we"}, {"title": "2.3 Impact of noise level selection", "content": "An effective and common technique to enhance the performance of randomized smoothing is to train the base classifier \\(f\\) with Gaussian noise augmentation, in order to adapt to the Gaussian noise employed in this defense. We use \\(\\sigma_a\\) to denote the standard deviation of Gaussian noise used for training augmentation. Hence, with this augmentation, randomized smoothing involves two noise level parameters, \\(\\sigma_s\\) and \\(\\sigma_a\\).\nThe noise levels \\(\\sigma_s\\) and \\(\\sigma_a\\) impact the performances of certified accuracy and radius. In particular, their selection yields a trade-off, and thus it is often difficult to maximize both certified accuracy and radius together.\nThe reason for the trade-off might be explained by the relationship between prediction accuracy and noise level \\(\\sigma_s\\). It is expected that the classifier would have higher accuracy for smaller \\(\\sigma_s\\), which corresponds to increasing the value of \\(\\Phi^{-1}(p_a)\\) in (4). Hence, realizing the optimal \\(R\\) requires a balance between these values, and the ideal selection of the noise levels \\(\\sigma_s\\) and \\(\\sigma_a\\) is intractable. We address this by introducing variational randomized smoothing and universal training methods."}, {"title": "3 Proposed framework", "content": null}, {"title": "3.1 Noise level selector", "content": "Basic concept: This paper proposes a new variational randomized smoothing technique to select a suitable \\(\\sigma_s\\) for each input image.\nWe use an additional neural network \\(h : \\mathcal{X} \\rightarrow [0,\\infty)\\) to select the randomize smoothing noise level as a function of each input image \\(x\\), i.e., \\(\\sigma_s = h(x)\\). We use \\(g_v\\) to denote the smoothed classifier employing the noise level selector \\(h\\).\nTraining formulation: The majority voting of the smoothed classifier in (3) is not differentiable, which prevents training \\(h\\). Thus, for training purposes, we instead use a soft smoothed classifier \\(g_s\\) that aggregates the soft outputs of the model, as given by\n\\[\ng_s(x) := \\frac{1}{N_r} \\sum_{k=1}^{N_r} \\operatorname{softmax} \\left(\\frac{f_s(x + \\varepsilon_k)}{\\tau}\\right),\n\\]\nwhere \\(f_s\\) denotes the soft (logit vector) output of the base classifier \\(f\\), \\(\\varepsilon_k \\stackrel{\\text { iid }}{\\sim} \\mathcal{N}(0,\\sigma_s^2 I_d)\\) are \\(N_r\\) samples of Gaussian noise with \\(\\sigma_s = h(x)\\), and \\(\\tau > 0\\) is the temperature parameter for the tempered softmax operation. We generally set \\(\\tau = 1\\) for simplicity. Note that as \\(\\tau \\rightarrow 0\\), the soft smoothing is equivalent to the standard majority voting used in (3). To train \\(h\\) to pick \\(\\sigma_s\\) as a function of \\(x\\) for better accuracy, we employ the typical objective of minimizing cross entropy (CE) loss, \\(\\mathcal{L}_{CE}(x, y) = -\\log g_s(x)[y]\\), where \\(y\\) denotes the correct class label for \\(x\\) and \\(g_s(x) [y]\\) denotes the corresponding class likelihood output by \\(g_s\\). However, minimizing only the CE loss might result in degraded robustness against adversarial attacks as it encourages smaller \\(\\sigma_s\\).\nStochastic regularization: To maintain a reasonable value for \\(\\sigma_s\\), we introduce an additional term to regularize \\(\\sigma_s\\) towards a desired distribution. Although the distribution of perturbation \\(\\varepsilon\\) is conditionally Gaussian given \\(\\sigma_s\\) as \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma_s^2 I_d)\\), it may be no longer marginally Gaussian as \\(\\sigma_s = h(x)\\) changes for different inputs \\(x\\). To encourage Gaussianity of the marginal distribution, we employ a variational framework based on the KL divergence to control the distribution of \\(\\sigma_s\\). Setting the target Gaussian distribution for \\(\\varepsilon\\) to be \\(q = \\mathcal{N}(0, \\sigma_t^2 I_d)\\), which captures a target noise level of \\(\\sigma_t\\), the KL divergence \\(D_{KL}(p||q)\\) to regulate the distribution \\(p = \\mathcal{N}(0, \\sigma_s^2 I_d)\\) is given by\n\\[\nD_{KL}(p||q) = \\frac{d}{2} \\left[\\frac{{\\sigma_s}^2}{{\\sigma_t}^2} + \\log \\left(\\frac{{\\sigma_t}^2}{{\\sigma_s}^2}\\right) - 1\\right].\n\\]\nWe use a regularized loss function combining the CE loss and KL divergence,\n\\[\n\\mathcal{L} = (1 - \\lambda)\\mathcal{L}_{CE}(x, y) + \\lambda D_{KL}(p||q),\n\\]"}, {"title": "3.2 Enhancement of selector robustness", "content": "Randomized smoothing for the selector: Since \\(h\\) is also a neural network component, it is possible for an adversarial input attack to cause \\(h\\) to select a \\(\\sigma_s\\) that performs poorly for randomized smoothing. Thus, a defense should also be applied to \\(h\\), however, in the previous section, we described the selector \\(h\\) without defense. The conventional randomized smoothing techniques for classification tasks is not readily applicable, since \\(h\\) selects continuous noise levels rather than discrete class labels. Hence, we employ median smoothing [2], which is an extension of randomized smoothing for regression problems.\nMedian smoothing: Median smoothing uses the median of multiple regressor outputs for Gaussian augmented input as the smoothed prediction result. We denote the smoothed classifier, using \\(h\\) with median smoothing, as \\(g_v^*\\), which is illustrated in Fig. 1c. Similar to the conventional randomized smoothing, the input image \\(x\\) is perturbed with Gaussian noise \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma_m^2 I_d)\\) where \\(\\sigma_m > 0\\) is the median smoothing noise level. Let \\(h_p(x + \\varepsilon)\\) denote the \\(p\\)-th percentile of the output of \\(h(x + \\varepsilon)\\), with respect to the statistics of the Gaussian input perturbation. Median smoothing uses the median, \\(\\sigma_s = h_{50\\%}(x + \\varepsilon)\\), as the smoothed result of \\(h\\). In practice, for both selector training and at test time, this median is empirically computed from multiple samples. This smoothed output \\(\\sigma_s\\) is used for successive randomized smoothing of the base classifier \\(f\\). Thus, \\(g_v^*\\) employs a dual smoothing to protect both \\(h\\) and \\(f\\).\nSimilar to conventional randomized smoothing, median smoothing provides guarantees in the form of upper and lower bounds on the output in the presence of any adversarial perturbation \\(\\delta \\in \\mathbb{R}^d\\), within a given radius \\(||\\delta||_2 < D\\). We use \\(\\underline{h}\\) and \\(\\overline{h}\\) to denote lower and upper bounds of output of \\(h\\), respectively, and the shorthand \\(x' := x + \\delta\\). For any perturbation \\(\\delta \\in \\mathbb{R}^d\\), with \\(||\\delta||_2 < D\\), median smoothing guarantees upper and lower bounds on the median smoothed output, given by\n\\[\n\\underline{h}_p(x' + \\varepsilon) \\leq h_p(x' + \\varepsilon) \\leq \\overline{h}_p(x + \\varepsilon),\n\\]"}, {"title": "4 Experimental results", "content": null}, {"title": "4.1 Overview", "content": "Types of experiments: We conducted experiments to examine the performance of variational randomized smoothing and compare with the conventional approach, which uses fixed \\(\\sigma_s\\). We consider two types of experiments:\nCertified robustness: Certified accuracy and radius were analyzed. Specifically, this demonstrates how attacks to both \\(h\\) and \\(f\\) affect certified accuracy and radius of our proposed method.\nEmpirical robustness: Classification accuracy was examined with clean images and images perturbed by adversarial attacks. This demonstrates the practical performance of our method, compared to the certified lower bounds."}, {"title": "4.2 Experiment A: certified accuracy and radius", "content": "Conditions: For the baseline conventional smoothed classifier \\(g\\), we consider the following two cases:\n\n\\(g\\) with \\(\\sigma_s\\) sweeping: This baseline indicates the maximum possible values of certified accuracy and robustness if \\(\\sigma_s\\) can be ideally adjusted for any operating point. We plot the envelope determined by the maximum values in the certified accuracy and radius curves, while sweeping across \\(\\sigma_s\\). For example, taking the maximum of the curves \\(\\sigma_s\\).\n\n\\(g\\) with \\(\\sigma_s = \\sigma_a\\): This is the baseline for certified accuracy and radius obtained by using the same noise level for training augmentation and test time smoothing. This is the conventional approach for randomized smoothing.\n\nFor the proposed methods, we consider the following two scenarios:"}, {"title": "4.3 Experiment B: empirical clean and robust accuracy", "content": "Conditions: To evaluate practical adversarial robustness of our method, the commonly used PGD attack is employed to produce adversarial examples. We consider two levels of attack strength as follows:\nWeaker attack: We assume only base classifier \\(f\\) is the attack target. This is a weaker attack setting because PGD attack does not use any information about \\(g\\) and \\(h\\), but only \\(f\\)."}, {"title": "5 Conclusion", "content": "This paper proposed variational randomized smoothing, which is a framework to select noise levels suitable for each input image by using a noise level selector. Experimental results demonstrated enhancement of empirical robustness against adversarial attacks. The results also indicated certified robustness of our method is close to the levels of the baselines, when the adversarial perturbation is relatively small. We also showed the benefit of conditional meta learning, universal \\(\\lambda\\) training, and universal \\(\\sigma_a\\) training, so that the hyperparameters can be adjusted at test time without retraining. These results demonstrated the advantages of sample-wise noise level selection for randomized smoothing while the employing median smoothing defense with clipping."}]}