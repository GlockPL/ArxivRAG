{"title": "Variational Randomized Smoothing for Sample-Wise Adversarial Robustness", "authors": ["Ryo Hase", "Ye Wang", "Toshiaki Koike-Akino", "Jing Liu", "Kieran Parsons"], "abstract": "Randomized smoothing is a defensive technique to achieve enhanced robustness against adversarial examples which are small input perturbations that degrade the performance of neural network models. Conventional randomized smoothing adds random noise with a fixed noise level for every input sample to smooth out adversarial perturbations. This paper proposes a new variational framework that uses a per-sample noise level suitable for each input by introducing a noise level selector. Our experimental results demonstrate enhancement of empirical robustness against adversarial attacks. We also provide and analyze the certified robustness for our sample-wise smoothing method.", "sections": [{"title": "Introduction", "content": "Neural networks are vulnerable to adversarial attacks that degrade the performance [4,8]. Adversarial attacks using adversarial examples can often seriously deteriorate prediction results of a neural network by adding small perturbations to input of the network. For example, the Projected Gradient Descent (PGD) attack [5] is widely used example of a white-box adversarial attack that utilizes knowledge of the target neural network such as weights and gradients. Considering defensive methods for neural networks is crucial to improve robustness against adversarial attacks."}, {"title": "Related work", "content": "Various types of defense mechanisms have been proposed to protect neural networks from adversarial attacks. Adversarial training [4] enhances the robustness of neural networks by adding adversarial examples as training data. For instance, PGD is commonly used to generate the examples used in adversarial training."}, {"title": "Contributions", "content": "Inspired by the related studies, we propose variational randomized smoothing, which is a framework utilizing a selector to produce a noise level for every input of a smoothed classifier.\nWe introduce a variational framework to build a noise level selector composed of a neural network to determine sample-wise noise levels for randomized smoothing.\nWe propose a universal training scheme using stochastic regularization, which makes a selector learn various conditions to produce different noise strength at once by randomly sampling regularization parameter $\\lambda$.\nWe improve the controllability in the universal training by using conditional meta learning, which enables the user to freely adjust noise strength by specifying $\\lambda$ at test time without retraining.\nSince the selector itself is a neural network which could be potentially a target of adversarial attacks, we also propose a defensive method called dual smoothing to protect our selector as well as base classifier.\nWe provide a modified certified robustness for sample-wise smoothing methods, based on the bound of median smoothing [2].\nExperimental results demonstrate that our proposed methods offer better empirical robustness compared to the conventional randomized smoothing."}, {"title": "Randomized smoothing", "content": "Randomized smoothing [3] is a defense method applied to a base classifier $f : \\mathcal{X} \\rightarrow \\mathcal{C}$, where $\\mathcal{X} \\subset \\mathbb{R}^d$ is the input (e.g., image) space and $\\mathcal{C} = \\{1, 2, ..., M\\}$"}, {"title": "Fundamental mathematical formulations", "content": null}, {"title": "Practical randomized smoothing", "content": "As the calculation of the ideal smoothed classifier in (1) is generally intractable, typically a Monte-Carlo approximation is utilized [3]."}, {"title": "Impact of noise level selection", "content": "An effective and common technique to enhance the performance of randomized smoothing is to train the base classifier $f$ with Gaussian noise augmentation, in order to adapt to the Gaussian noise employed in this defense. We use $\\sigma_a$ to denote the standard deviation of Gaussian noise used for training augmentation. Hence, with this augmentation, randomized smoothing involves two noise level parameters, $\\sigma_s$ and $\\sigma_a$.\nThe noise levels $\\sigma_s$ and $\\sigma_a$ impact the performances of certified accuracy and radius. In particular, their selection yields a trade-off, and thus it is often difficult to maximize both certified accuracy and radius together. For example,"}, {"title": "Proposed framework", "content": null}, {"title": "Noise level selector", "content": "Basic concept: This paper proposes a new variational randomized smoothing technique to select a suitable $\\sigma_s$ for each input image. We use an additional neural network $h : \\mathcal{X} \\rightarrow [0,\\infty)$ to select the randomize smoothing noise level as a function of each input image $x$, i.e., $\\sigma_s = h(x)$. We use $g_v$ to denote the smoothed classifier employing the noise level selector $h$.\nTraining formulation: The majority voting of the smoothed classifier in (3) is not differentiable, which prevents training $h$. Thus, for training purposes, we instead use a soft smoothed classifier $g_s$ that aggregates the soft outputs of the model, as given by"}, {"title": "Stochastic regularization", "content": "To maintain a reasonable value for $\\sigma_s$, we introduce an additional term to regularize $\\sigma_s$ towards a desired distribution. Although the distribution of perturbation $\\epsilon$ is conditionally Gaussian given $\\sigma_s$ as $\\epsilon \\sim \\mathcal{N}(0, \\sigma_s^2 I_d)$, it may be no longer marginally Gaussian as $\\sigma_s = h(x)$ changes for different inputs $x$. To encourage Gaussianity of the marginal distribution, we employ a variational framework based on the KL divergence to control the distribution of $\\sigma_s$. Setting the target Gaussian distribution for $\\epsilon$ to be $q = \\mathcal{N}(0, \\sigma_t^2 I_d)$, which captures a target noise level of $\\sigma_t$, the KL divergence $D_{\\text{KL}} (p \\|\\| q)$ to regulate the distribution $p = \\mathcal{N}(0, \\sigma_s^2 I_d)$ is given by"}, {"title": "Enhancement of selector robustness", "content": "Randomized smoothing for the selector: Since h is also a neural network component, it is possible for an adversarial input attack to cause h to select a $\\sigma_s$ that performs poorly for randomized smoothing. Thus, a defense should also be applied to h, however, in the previous section, we described the selector h without defense. The conventional randomized smoothing techniques for classification tasks is not readily applicable, since h selects continuous noise levels rather than discrete class labels. Hence, we employ median smoothing [2], which is an extension of randomized smoothing for regression problems.\nMedian smoothing: Median smoothing uses the median of multiple regressor outputs for Gaussian augmented input as the smoothed prediction result. We denote the smoothed classifier, using h with median smoothing, as $g^*$, which is illustrated in Fig. 1c. Similar to the conventional randomized smoothing, the input image $x$ is perturbed with Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma_m^2 I_d)$ where $\\sigma_m > 0$ is the median smoothing noise level. Let $h_p(x + \\epsilon)$ denote the $p$th percentile of the output of $h(x + \\epsilon)$, with respect to the statistics of the Gaussian input perturbation. Median smoothing uses the median, $\\sigma_s = h_{50\\%}(x + \\epsilon)$, as the smoothed result of h. In practice, for both selector training and at test time, this median is empirically computed from multiple samples. This smoothed output $\\sigma_s$ is used for successive randomized smoothing of the base classifier $f$. Thus, $g^*$ employs a dual smoothing to protect both h and f.\nSimilar to conventional randomized smoothing, median smoothing provides guarantees in the form of upper and lower bounds on the output in the presence of any adversarial perturbation $\\delta \\in \\mathbb{R}^d$, within a given radius $|\\|\\delta||_2 < D$. We use $\\underline{h}$ and $\\overline{h}$ to denote lower and upper bounds of output of h, respectively, and the shorthand $x' := x + \\delta$. For any perturbation $\\delta \\in \\mathbb{R}^d$, with $|\\|\\delta||_2 < D$, median smoothing guarantees upper and lower bounds on the median smoothed output, given by"}, {"title": "Experimental results", "content": null}, {"title": "Overview", "content": "Types of experiments: We conducted experiments to examine the performance of variational randomized smoothing and compare with the conventional approach, which uses fixed $\\sigma_s$. We consider two types of experiments:\nA. Certified robustness: Certified accuracy and radius were analyzed. Specifically, this demonstrates how attacks to both h and f affect certified accuracy and radius of our proposed method.\nB. Empirical robustness: Classification accuracy was examined with clean images and images perturbed by adversarial attacks. This demonstrates the practical performance of our method, compared to the certified lower bounds.\nModel and dataset. The base classifier f is a neural network mainly composed of 4 convolutional layers. It was trained on the CIFAR-10 dataset with Gaussian noise augmentation for 400 epochs. For fixed $\\sigma_a$ training, the augmentation noise $\\sigma_a$ is chosen from 0.12, 0.25, 0.50, and 1.00. Additionally, another model $f_u$ is trained with universal $\\sigma_a$ training, by randomly sampling $\\sigma_a \\sim \\text{Uniform}(0, 1)$ during training, i.e., $\\sigma' = 1$.\nSelector models h are trained for 200 epochs for each base model described above, with the same corresponding data augmentation, and parameters $N_r = N = 10$. For the selectors trained with the fixed $\\sigma_a$ base models f, the corresponding $\\sigma_a$ is used as the input to h. For the case of selector h trained for the universal $\\sigma_a$ model $f_u$, the $\\sigma_a$ input of h was set to 0.5. We use a target noise level of $\\sigma_t = 2\\sigma_a$."}, {"title": "Experiment A: certified accuracy and radius", "content": "Conditions: For the baseline conventional smoothed classifier g, we consider the following two cases:\na) g with $\\sigma_s$ sweeping: This baseline indicates the maximum possible values of certified accuracy and robustness if $\\sigma_s$ can be ideally adjusted for any operating point. We plot the envelope determined by the maximum values in the certified accuracy and radius curves, while sweeping across $\\sigma_s$. For example, taking the maximum of the curves in Fig. 2b yields the envelope.\nb) g with $\\sigma_s = \\sigma_a$: This is the baseline for certified accuracy and radius obtained by using the same noise level for training augmentation and test time smoothing. This is the conventional approach for randomized smoothing.\nFor the proposed methods, we consider the following two scenarios:"}, {"title": "Experiment B: empirical clean and robust accuracy", "content": "Conditions: To evaluate practical adversarial robustness of our method, the commonly used PGD attack is employed to produce adversarial examples. We consider two levels of attack strength as follows:\n1) Weaker attack: We assume only base classifier f is the attack target. This is a weaker attack setting because PGD attack does not use any information about g and h, but only f."}, {"title": "Discussion", "content": "Although certified accuracy and radius of $g^*$ deteriorated by an attack to selector h, the performance of $g^*$ was close to the baselines when perturbation budget is relatively small. Furthermore, performance degradation was alleviated by applying clipping of the h output. In terms of empirical clean and robust accuracies, our method demonstrated better performance than the baselines.\nWe also showed that median smoothing and clipping applied to g was effective as a defensive method for the selector h. These observations indicate practical advantages of sample-wise noise level selection using our method.\nThe experimental results also suggest some limitations of our method. Certified robustness becomes worse if the input of h is attacked. It is harder to select better $\\sigma_s$ for each input when under attack with a relatively larger distortion budget. In addition, as our method conducts dual randomized smoothing for h and f, it potentially requires more computing resources since additional sampling is required for each stage of smoothing."}, {"title": "Conclusion", "content": "This paper proposed variational randomized smoothing, which is a framework to select noise levels suitable for each input image by using a noise level selector. Experimental results demonstrated enhancement of empirical robustness against adversarial attacks. The results also indicated certified robustness of our method is close to the levels of the baselines, when the adversarial perturbation is relatively small. We also showed the benefit of conditional meta learning, universal $\\lambda$ training, and universal $\\sigma_a$ training, so that the hyperparameters can be adjusted at test time without retraining. These results demonstrated the advantages of sample-wise noise level selection for randomized smoothing while the employing median smoothing defense with clipping."}, {"title": "Selector training scheme", "content": "Fig. 15 depicts the training scheme of selector h described in Algorithm 1. The training scheme utilizes soft smoothed classifier $g_s$ defined by (5) as a component. The overview of $g_s$ is shown in Fig. 15a. $N_r$ was set to 10 for all experiments presented in this paper. The loss function $\\mathcal{L}$ to update h is defined as (7). Fig. 15b and Fig. 15c show training scheme for $g_v$ and $g^*$, respectively. Fig. 15c illustrates the process to generate $N_{ir}$ samples of $\\sigma_s$ and select median of the samples to simulate the process of median smoothing. We used $N_{i}r = 10$ for all experiments in this paper."}, {"title": "Empirical upper and lower bounds for selector output", "content": "Fig. 11 shows the empirical upper and lower bounds for the selector output $h_p(x + \\delta + \\epsilon)$. The upper and lower bounds are denoted by $\\overline{h}_{q_u} (x + \\epsilon)$ and $\\underline{h}_{q_l} (x + \\delta)$, respectively. The test set of CIFAR-10 dataset was used as the input for h. The parameters $N_h = 1,000$ and $\\lambda = 0.0$ were used for h across all settings. $\\overline{h}_{q_u} (x+\\epsilon)$ and $\\underline{h}_{q_l} (x+\\epsilon)$ were averaged over all images in the test set. This result suggests that deviations of $\\overline{h}_{q_u} (x + \\epsilon)$ and $\\underline{h}_{q_l} (x + \\delta)$ from $h_p(x+\\delta)$, which is the optimal value for median smoothing, could be larger as adversarial distortion budget $\\gamma$ increases."}, {"title": "Extensive analysis of clean and robust accuracy with base classifier trained by universal $\\sigma_a$ training", "content": "Regarding smoothed classifier g, tradeoff between clean and robust accuracy can be analyzed by sweeping $\\sigma_s$ against base classifier f. Likewise, $g_v$ demonstrates different clean and robust accuracy by sweeping $\\lambda$. The performance of a smoothed classifier using the base classifier $f_u$ also depends on the $\\sigma'$ chosen for universal $\\sigma_a$ training. Although Fig. 9 shows one example of clean and robust accuracy with the case where $\\sigma' = 1.0$ and $\\gamma = 0.3$, different curves about tradeoff between clean and robust accuracy can be seen with different sets of $\\sigma'_a$ and $\\gamma$. Hence, we conducted extensive experiments similar to the experimental results shown in Fig. 9. In these experiments, $\\sigma'$ was chosen from one of 0.25, 0.50, and 1.00. For training, h was conditioned on $\\sigma_a = \\sigma'/2$ and $\\sigma_t$ was set to $\\sigma'_a$. To analyze differences across the perturbation budget of the PGD attack, we varied $\\gamma$ from 0.1, 0.3, and 0.5. For experiments of $g_v$, $\\lambda$ was selected from $\\{0.0, 0.1,..., 0.5\\}$ except the case where $\\sigma' = 0.25$ and $\\gamma = 0.5$. For the case with $\\sigma' = 0.25$ and $\\gamma = 0.5$, $\\lambda$ was chosen from $\\{0.0,0.1, . . ., 0.9\\}$.\nFig. 12 shows results for $\\sigma' = 0.25$. Our method performed well against the baseline with $\\sigma_a = 0.12$, which was chosen since it is the mean of the range [0,0.25] used in universal $\\sigma_a$ training. Although the performance of our method and the baseline with $\\sigma_a = 0.12$ both dropped significantly with $\\gamma = 0.5$, our method demonstrated better robustness. Similar characteristics can be observed in Fig. 13 and Fig. 14. In Fig. 13, our method with $\\sigma'_a = 0.50$ performed better than baseline with $\\sigma_a = 0.25$. Fig. 14 indicates our method with $\\sigma' = 1.00$ performed better than baseline with $\\sigma_a = 0.50$."}]}