{"title": "Verifying Machine Unlearning with Explainable AI", "authors": ["\u00c0lex Pujol Vidal", "Anders S. Johansen", "Mohammad N. S. Jahromi", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "abstract": "We investigate the effectiveness of Explainable AI (XAI) in verifying Machine Unlearning (MU) within the context of harbor front monitoring, focusing on data privacy and regulatory compliance. With the increasing need to adhere to privacy legislation such as the General Data Protection Regulation (GDPR), traditional methods of retraining ML models for data deletions prove impractical due to their complexity and resource demands. MU offers a solution by enabling models to selectively forget specific learned patterns without full retraining. We explore various removal techniques, including data relabeling, and model perturbation. Then, we leverage attribution-based XAI to discuss the effects of unlearning on model performance. Our proof-of-concept introduces feature importance as an innovative verification step for MU, expanding beyond traditional metrics and demonstrating techniques' ability to reduce reliance on undesired patterns. Additionally, we propose two novel XAI-based metrics, Heatmap Coverage (HC) and Attention Shift (AS), to evaluate the effectiveness of these methods. This approach not only highlights how XAI can complement MU by providing effective verification, but also sets the stage for future research to enhance their joint integration.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving landscape of Artificial Intelligence (AI) and Machine Learning (ML), adapting to new data while ensuring privacy and regulatory compliance is crucial [24]. The EU's \"right to be forgotten\" highlights the need to modify or remove ML predictions without degrading performance [16,22]. In domains such as security, healthcare, and activity monitoring addressing privacy concerns is vital for user trust and legal compliance. These fields often handle sensitive data, making it essential to enable data removal on request without disrupting ML systems [21]. For instance, harbor front monitoring involves continuous analysis of potentially sensitive data. The General Data Protection Regulation (GDPR) imposes strict regulations on processing sensitive data, including any data that can identify individuals, such as images containing people. Consequently, a regulator may interpret that a trained model violates privacy by recognizing people. In such cases, we must ensure compliance by removing any trace of human patterns from the model while retaining features of other relevant objects in the scene.\nRetraining ML models from scratch to accommodate deletions is impractical due to the extensive computational resources required. As a solution to this challenge, the emerging field of Machine Unlearning (MU) has been proposed to enable models to \"forget\" specific data points or features without full retraining [24]. This approach efficiently removes sensitive or irrelevant data, address-ing privacy concerns. MU not only aids in meeting regulatory requirements but also enhances AI systems' trustworthiness and transparency by reducing biases and preventing adversarial attacks. The effectiveness and reliability of MU raise several questions, particularly regarding how well patterns are unlearned. While accuracy is a common metric for evaluating ML models, it may not fully capture this nuance, leaving concerns about whether unwanted patterns are genuinely removed or merely re-mapped through different layers of the model. Explainable AI (XAI) provides insights into ML models' decision-making processes, helping technical end-users to identify and debug errors. A significant portion of XAI research focuses on attribution-based explainability, which attributes model pre-dictions to specific input features [5]. These methods provide insights into how much each feature contributes to a model's prediction. While commonly used to understand and improve model predictions, they are less utilized for veri-fying MU processes. This gap in utilization highlights the need for a deeper examination of MU's impact on model behavior. By leveraging XAI, we can evaluate whether these methods successfully remove sensitive information and biases or inadvertently introduce new issues. Fig. 1 exemplifies this process in the challenging and real-life test case of harbor front monitoring, an environ-ment that requires ongoing data analysis for effective decision-making. Handling potentially sensitive data, such as images containing people, makes this a prime example of where GDPR compliance is potentially essential. For instance, har-bor front monitoring can involve identifying and tracking people for safety and operational purposes, which can include soft biometric data that raises privacy concerns. XAI helps to verify if MU can effectively remove potentially sensitive information from the model's predictions while retaining relevant object features.\nThe heatmaps generated by XAI show how the trained model captures human patterns, which the unlearned model effectively ignores, ensuring adherence to potentially strict privacy regulations while maintaining accuracy, fairness, and interpretability. This understanding is crucial for validating the model's perfor-mance after unlearning specific data and can potentially highlight the synergy between XAI and MU in building AI systems that are both effective and aligned with ethical standards and societal expectations.\nHence, our research question is:\nRQ: Can attribution-based Explainable AI (XAI) be used to verify the effectiveness of unlearning?\nIn this paper, we conduct a detailed study on how attribution-based XAI can improve the application of MU techniques by observing and quantifying the shift in local feature importance. We explore various MU approaches, including data relabeling and model perturbation, discussing their challenges and poten-tial benefits. We make use of XAI to define novel metrics and measure, both qualitatively and quantitatively, the impact that MU has on the model's ability to localize appropriate features. Our contributions are threefold:\nWe demonstrate the novel application of attribution-based XAI methods to verify the effectiveness of MU techniques. By generating saliency maps, we can visualize and qualitatively assess where the models focus post-unlearning, ensuring that sensitive information has been successfully removed.\nWe go beyond accuracy measures by introducing two new XAI-based met-rics, Heatmap Coverage (HC) and Attention Shift (AS), to quantify the effectiveness of MU methods. HC measures the spatial correctness of the predicted heatmaps, while AS quantifies the shift in attention to relevant areas, providing a comprehensive assessment of the unlearning process.\nOur study compares models that have been retrained from scratch with those that have undergone MU processes. We highlight how MU methods, evalu-ated through XAI techniques, can efficiently unlearn undesired patterns and potentially offering a more practical alternative to complete model retrain-ing.\nTo the best of our knowledge, this is the first time that XAI is leveraged to verify and inspect the impact of MU techniques."}, {"title": "2 Related Work", "content": "As AI and ML continue to dominate various domains, ensuring data privacy and model transparency has become crucial. This has led to growing interest in MU techniques, that remove sensitive data without full retraining, complying with GDPR. Combined with XAI, which promotes transparency in AI decision-making, these approaches can be key to building trustworthy AI systems. While XAI and MU are active independent fields of study, their combined effects in sensitive applications, such as security monitoring, are to be explored. This inte-gration is vital for predictive analytics and decision-making, where attribution-based XAI methods can ensure data privacy, model interoperability, and verify the efficacy of MU methods. In the following subsections, we review the current research in MU and attribution-based XAI, discussing major techniques and their implications for enhancing privacy and transparency in AI applications."}, {"title": "2.1 Machine Unlearning", "content": "MU methods can be broadly categorized into three subcategories: model-agnostic, model-intrinsic, and data-driven approaches [16].\nModel-agnostic methods apply to various learning models and provide a solid foundation for addressing data deletion and privacy concerns. Several approaches fall into this category. Certified Removal formalize the definition of MU and pro-vide methods with theoretical guarantees [9]. Typically they use noise to mask small changes from gradient updates, only suitable for certain convex losses. Decremental Learning reduces training load by removing redundant samples or pruning irrelevant nodes [11].\nModel-intrinsic methods are tailored for specific model types. Approaches for linear models, tree-based models, and Bayesian models involve influence func-tions, robust split decisions, and optimizing posterior distributions. These are effective but often struggle with scalability and efficiency for complex models. Deep neural networks (DNNs), require specialized methods like certified removal mechanisms for layers with convex activation functions or influence functions to-gether with noise injection, and tracking gradient updates on nodes [8].\nData-driven approaches to modify or track data to efficiently remove the influ-ence of specific data samples from the model. For example, SISA technique [3] leverages data partitioning and model ensembling. While effective for small tasks, they do not generalize well with larger models and dataset complexity. Other methods involve data augmentation by adding noise or relabelling data [14]. Despite potentially achieving good accuracy, these methods lack guarantees of removal, and are not feasible for many practical scenarios.\nDespite the variety of the proposed methods, they lack generalization to com-plex models and losses, only handle simple removal tasks, and do not provide a general benchmark to properly compare and verify unlearning efficacy.\nRecently, the first MU competition at NeurIPS 2023 [22] offered a valuable plat-form for benchmarking unlearning algorithms. The evaluation focused on the quality of forgetting while also considering model utility, providing a dual per-spective for assessing the effectiveness of these methods. In our proof of concept, we compare fine-tuning on relabeled data with decremental learning and noise in-jection for DNNs, which were top-performing approaches in the challenge. They proposed using an F-score as a metric for evaluating forgetting quality, requir-ing training a model from scratch as a baseline and running several attacks on it, which is resource-intensive and often unfeasible. Additionally, F-score is pri-marily defined for measuring the removal of data points in classification tasks. Alternative accuracy and attack-based metrics also lead to similar problems [16]. Therefore, there is a pressing need for an interpretable, efficient, and versatile verification mechanism that can assess forgetting quality in classification tasks and beyond. We claim that XAI provides the necessary tools to address this issue."}, {"title": "2.2 \u03a7\u0391\u0399", "content": "As neural networks have expanded in size and complexity, ensuring their in-terpretability has become a major challenge, making explainability increasingly important. Attribution-based Explainable AI (XAI) methods aim to help users understand the predictions of \"black box\" models by assigning importance to input features. These methods are typically categorized into Class Activation Map (CAM)-based methods, which include gradient-based and gradient-free ap-proaches, perturbation-based methods, and contrastive methods.\nCAM-based methods utilize feature maps from the final convolutional layers to determine which regions in an input image are responsible for the model's pre-dictions. These methods can be further broken down into two subcategories: 1) Gradient-based methods compute gradients of the model's output with respect to the input features to generate saliency maps, highlighting each feature's contri-bution to the final prediction. Examples are Grad-CAM [20], Grad-CAM++ [4], Layer-CAM [12], and Layer-wise Relevance Propagation (LRP) [2] fall into this category. These methods require access to model parameters and may struggle with negative contributions due to gradient saturation. 2) Gradient-free methods do not rely on gradient information, which makes them advantageous for models in post-deployment settings. Examples include Score-CAM [23] and SIDU (Sim-ilarity Difference and Uniqueness) [15].\nPerturbation-based methods systematically modify the input features and ob-serve the resulting changes in the model's output. Techniques like LIME [19] and RISE [18] approximate a model locally or use random perturbations to generate saliency maps, suitable for black-box scenarios. However, they are computation-ally demanding and limited in applicability.\nContrastive methods explain why certain predictions are made and oth-ers are not, essential for high-stakes domains like healthcare and autonomous driving [1]. Techniques like Contrastive Explanation Method (CEM) [6] and CDeepEx [7] identify features relevant to both positive and negative predictions, offering deep insights but often requiring intensive computation due to complex optimization."}, {"title": "3 Dataset", "content": "The dataset utilized in this study extends the Long-Term Thermal-Imaging Drift (LTD) dataset [17], covering a period of 188 days from May 14, 2020, to April 30, 2021. Due to the dataset being recorded in a public space, the privacy of the individuals in the context is paramount. Thus a request for MU of specific patterns is a real possibility.\nThe dataset consists of 1.069.428 images with four annotated classes; Human, Bicycle, Motorcycle and Vehicle. Due to the long-term nature of the dataset captures a wide range of visual appearances, environmental conditions and times of day, emphasizing the dynamic challenges of real-world applications.\nThe data is divided with a temporally uniform distribution between train, test and validation sets. All frames belonging to a specific video clip, are grouped to prevent data leakage between splits. Due to the elevated camera position and low resolution of the camera, objects often appear small in the footage, com-plicating object-centric vision tasks. These challenges are further exacerbated by the class imbalance of the dataset caused by the typical behavior in this particular public space. Likewise, the frequency of objects in a given image fol-lows a Poisson-like distribution with the most common samples containing 1-4 objects. We address this imbalance by sub-sampling the top 70th percentile sam-ples while up-sampling the bottom 10th percentile. For validation and testing, however, the distributions remain uniformly sampled across the entire dataset to evaluate real-world performance."}, {"title": "4 Proposed method", "content": "Typically, advances in MU have been tailored to simple classification tasks, with fewer approaches addressing real-case scenarios involving complex data. Further-more, the success of these methods is often evaluated based on the performance of the retained data, while the verification of the unlearning method's efficacy is frequently overlooked. To advance the complexity of typical MU tasks and better reflect real-world scenarios, we define a challenging regression problem where the model must count objects of interest from the LTD dataset and unlearn counting humans. We propose leveraging attribute-based XAI to evaluate how much MU succeeds qualitatively and quantitavely in forgetting learned patterns (as shown in Fig. 2). Particularly, by means of SIDU that generates a heatmap, based on the impact of different channels (i.e. abstract patterns) in the latent space of the model. By comparing the heatmaps from a model naively trained from scratch without counting humans, and those of models exposed to MU methods, we in-spect and evaluate the efficacy of different unlearning methods on the task of object counting."}, {"title": "4.1 Problem statement", "content": "Let $D$ be the LTD dataset, so that if $(x, y) \\in D$, then $x$ is a vector representing a thermal image and the label $y$ is a scalar that counts the objects of interest in such image. Originally, the dataset contains four classes: human, bicycle, vehicle, and motorcycle; and tracks them over a video stream. We obtain $D$ by summing the total number of instances that appear at any frame, that is, for each thermal image $x$ we have $y = y_h + y_b + y_v + y_m$.\nInferring the number of relevant objects in the scene is important for track-ing overall behavior. Typically, a model $f$ is trained on dataset $D$ to perform this task. However, monitoring peoples' behavior is highly sensitive and subject to GDPR legislation. The GDPR strictly regulates personal data, including im-ages with people. Thus, a regulator may deem model $f$ unwanted for counting individuals, requiring the removal of people from the model while still counting other relevant objects in the scene. In this situation we can take two different paths, either train a new model removing $y_h$ from $D$, or modify $f$ through an unlearning method to count only the remaining classes. After the unlearning process, we leverage SIDU as a verification tool to certify that $f$ has successfully \"forgotten\" to be influenced by human-like patterns in the attribute space."}, {"title": "4.2 Original Model Training", "content": "For each element in $D$ we train a model $f$ to infer the amount of objects of interest in our scene. This is a typical regression problem, that can be solved by minimizing a loss function between the predictions and the ground truth labels. Formally, if $(x, y) \\in D$, then $f_\\theta(x) = \\hat{y}$ is the prediction of the model, and $\\theta \\in \\mathbb{R}^d$ are the parameters of the model. The training objective consists of minimizing the problem\n$\\underset{\\theta}{\\text{arg min}} \\mathcal{L}(f_\\theta(x), y)$\nwhere $\\mathcal{L}$ is a loss function. We are using the Mean Squared Error (MSE) for this use-case. Once trained, if $\\theta^*$ are the optimal parameters of the model, we denote $f_\\theta = f_{\\theta^*}$ the Original model.\nEvaluation To evaluate the performance of the trained model, we first measure it with common regression metrics, such as RMSE and MAE. Then, we use SIDU to generate the heatmaps from the images that reveal the patterns the model focuses on. While RMSE and MAE provide quantitative metrics, SIDU offers a qualitative assessment of the model's performance by visualizing its attention to different features."}, {"title": "4.3 Unlearning Methodology", "content": "In our proof-of-concept, we want to unlearn the ability to count humans and that information must also be deleted from $D$. Since the LTD dataset is separated by classes and we know per class instances, we remove the counting people ability from the dataset, by updating the label: $y' = y_b + y_v + y_m$. Thus, we obtain the curated dataset $D' = \\{(x, y') \\mid (x, y) \\in D\\}$, see Fig. 3. This can be generalized to other counting problems that use object tracking datasets, which are also typically label class-wise.\nThe naive approach to address this issue consists on instantiating a new model $f_t$ and solve the regression problem as before but over the curated dataset $D'$. This ensures that the outcome model do not track human attributes. How-ever, going through all the training process again is not efficient and not feasible in a practical case. An unlearned model starts from $f$ and modify it to achieve a similar behaviour as $f_t$. We denote the unlearned model by $f_u = \\mathcal{U}(f, D')$, where $\\mathcal{U}$ is an unlearning method, and call $f_t$ the Baseline model. While our approach involves relabeling as part of the unlearning process, this step alone is insuffi-cient to fully address the unlearning challenge. Relabeling adjusts the task to avoid counting humans, but the model may still retain internal representations of human features. To fully remove these learned features from the model, addi-tional techniques such as decremental learning and noise injection are required to unlearn human-related patterns effectively. This ensures that the unlearned model no longer relies on human features in its internal layers, addressing the limitations of fine-tuning or simple relabeling alone. Inspired by [22], we compare the retrained-from-scratch Baseline with four different models:\nFine-tune: Simple approach that consists of fine-tuning $f$ with the rela-belled data $D'$, i.e. minimizing MSE starting with $f$ iterating over $D'$. We denote the Fine-tune model by $f_{uf}$. For small tasks, this has been proven to be enough due to catastrophic forgetting [13].\nPrune and Reinit: This method achieved the 4th position within the NeurIPS 2023 MU challenge and the highest forgetting score in the post-challenge bench-marking [22]. It consists of pruning ($f_{up}$) or reinitialize ($f_{ur}$) the 95% of the convolutional and fully-connected layer weights with the low-est L1 norm. The model then performs fine-tuning on the retain set using a combination of loss function and regularization term between the entropy of the original model and the entropy of the unlearned model.\nConfuse: Inspired from Seif method [22], which achieved the 3rd position, the Confuse method adds Gaussian noise to convolutional weights. That is, if $\\Theta_{conv} \\subset \\Theta^*$ are the convolutional parameters of $f_\\theta$, then we obtain the con-fused model $f_u$ by cloning the original model but setting the convolutional weights to be $\\theta \\sim \\mathcal{N}(\\mu = \\theta^*_{conv}, \\sigma^2 \\cdot I)$. The hyper-parameter $\\sigma$ controls the scale of the added noise."}, {"title": "4.4 XAI model: Similarity Difference and Uniqueness (SIDU)", "content": "SIDU estimates pixel-wise importance by extracting feature maps from the last convolutional layer of a deep CNN and creating masks based on similarity dif-ferences (SD) and uniqueness (U) metrics (See Eq. 3 and Eq. 4 in [15]). SD measures how altering a feature impacts the model's output, while Uniqueness assesses the distinctiveness of each feature. The combination of these masks pro-duces a comprehensive heatmap that efficiently highlights influential elements in the model's decision-making. This method addresses the limitations of existing techniques by more accurately localizing the salient regions that contribute to the model's predictions. Our experiments have shown that SIDU not only performs well in identifying these regions but also enhances trust in the model's decisions, particularly in sensitive domains like medical imaging. Additionally, SIDU has demonstrated robustness against adversarial attacks, making it a reliable and effective tool for visual explanations in AI. Typically, approaches to unlearning involve retraining or fine-tuning the model, assuming that performance degra-dation on the undesired patterns indicates successful forgetting. However, these methods often overlook the need to qualitatively verify the unlearning process. SIDU addresses this gap by generating detailed attribution heatmaps that vi-sualize the contribution of different input regions to the model's predictions. For instance, in the context of a monitoring scenario, where the model is sup-posed to \"forget\" human-related patterns due to privacy regulations, the SIDU heatmaps should show a significant reduction in the focus on human features post-unlearning. This visualization provides a clear, intuitive way to ensure that the model no longer relies on the undesired patterns, thereby validating the suc-cess of the unlearning process in maintaining compliance with privacy standards."}, {"title": "4.5 Metrics", "content": "To quantify the performance of the proposed network, we employ two well-established regression metrics, MAE and RMSE, to compare model predictions and labels. Evaluating how effectively an attribution method like SIDU highlights relevant parts of an image remains an open problem. To this end, we propose a novel metrics Heatmap Coverage (HC). Additionally, we define Attention Shift (AS) to measure the attention drift between $f_\\theta$ and $f_u$.\n$\\text{HC}(f_i) = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{\\text{sum}(\\text{H} \\odot \\text{M})}{\\text{sum}(\\text{H})}$\nAs SIDU produces heatmaps that rank the relative importance of areas within the image, HC quantifies the spatial correctness of the predicted heatmaps by computing the average of a weighted element-wise overlap between predicted SIDU heatmap H and its corresponding region of interest mask M, see Equation (1). Where sum(\u00b7) is the summation of all the elements of the matrix. H and M are matrices of dimension [ImageWidth, ImageHeight]. Matrix M is generated by using the object detection bounding box annotations, where every pixel that falls inside the bounding box of an object of interest is given the value 1, and 0 otherwise.\n$\\text{AS}(f_{ui}, f_{\\theta}) = \\frac{1}{N} \\sum_{j=1}^{N} \\text{std}(\\text{H}_{ui}^j - \\text{H}_{\\theta}^j)$\nWhen comparing heatmaps generated by the original model with those from the unlearned model, we measure the change in relative importance by computing the standard deviation of the difference between heatmaps, see Equation (2). This allows us to quantify the shift in attention to relevant areas of the images. Where $ui$ indicates the unlearned model, $\\theta$ the Original model, and std(.) computes the standard deviation from all elements of the matrix."}, {"title": "5 Results", "content": "To establish a baseline we train two models (Original and Retrain) which are randomly initialized and trained for 10 epochs. As unlearning assumes a baseline starting point, MU models are initialized from the baseline and train for 3 epochs. Each model uses a batch size of 50 and is optimized with the SGD optimizer at a learning rate of 5e-4. These models are then evaluated qualitatively and quantitatively with the metrics described in Section 4.5."}, {"title": "5.1 Quantitative results", "content": "As can be seen in Table 1 all unlearning methods yield similar quantitative results, but they fall short of baseline Retrain. This is expected, as the MU models' training process is 3 times shorter. However, a closer examination of r-HC (Heatmap Coverage for the retaining classes) indicates that MU meth-ods concentrate heatmap intensity more closely on the objects we aim to re-tain. This behavior suggests that an unlearned model can better retrieve object patterns after unlearning human ones. This improvement could be due to the high-class correlation within the dataset; for instance, a bike is often associated with a human riding it. Furthermore, Confuse noticeably better localize the at-tention around objects of interest compared to the other models. Conversely, h-HC (Heatmap Coverage for human class) indicates that heatmaps from the Retrain model are less weighted around human bounding boxes, suggesting that the model's attention is dispersed elsewhere in the image. Interestingly, Prune, Reinit, and Confuse exhibit lower h-HC values than Finetune, underscoring that purposefully designed MU strategies can effectively reduce the attention models give to human attributes in the latent space, thereby demonstrating the effec-tiveness of unlearning. When considering AS, Confuse has shifted more attention compared to the original heatmap, which is also reflected in r-HC and h-HC. As expected, none of the models have shifted the attention as much as Retrain, indicating that some attention to human patterns still remains."}, {"title": "5.2 Qualitative results", "content": "Each heatmap in Fig. 4 visualizes the regions of the image that the model focuses on. The heatmaps in (c) Original and (d) Retrain show the initial model's focus areas before any unlearning techniques were applied. Notably, these heatmaps include significant attention on regions associated with human presence."}, {"title": "5.3 Discussion", "content": "As outlined in this section the difference in the unlearning approach in terms of standard evaluation metrics (MAE & RMSE) are minor. However, MU tech-niques can help focus the model's attention to an extent that surpasses straight-forward fine-tuning. This not only underlines the importance of employing MU techniques to accelerate the removal of patterns by reducing the computational burden but also highlights that doing so will allow the model to focus more narrowly on the remaining content. Furthermore, by visually comparing these heatmaps (as shown in Fig. 4), we can qualitatively assess the success of each unlearning method. The reduction in attention on human-related features in the Prune and Confuse methods, in particular, highlights their effectiveness in ensur-ing compliance with privacy regulations while maintaining the model's ability to focus on other relevant objects in the scene. One key consideration is the assumption that the XAI methods used for heatmap generation accurately re-flect important regions in the image. While XAI methods offer valuable insights into model decision-making, the absence of a consensus on the efficacy of XAI techniques as well as metrics for evaluating model explanations adds a layer of uncertainty which should be acknowledged. Despite these challenges, this visual inspection underscores the utility of XAI as a tool for verifying the efficacy of MU techniques in sensitive applications. Effectively the purpose of MU to adhere to the \"right to be forgotten\", would be to ensure that the undesired pattern is disregarded and does not play a significant role in the system's decision-making."}, {"title": "6 Conclusion", "content": "In the evolving landscape of AI systems, privacy and transparency have be-come crucial due to AI's increasing complexity and impact on daily life. MU emerged to help address the privacy need, it necessitates a verification method to properly evaluate whether it has the intended effect. As XAI provides in-sights on the decision-making processes of ML models, we claim it is a crucial tool to address MU challenges. Our findings show that using to attribution-based XAI, unlearned models can better concentrate heatmap intensity on the desired target objects when compared to completely retraining the model. While fine-tuning the model shows a better concentration on retaining classes than some MU approaches, XAI reveals that traces of unlearned patterns still linger, which indicates inefficient unlearning. Our novel approach could further expand to in-creasingly complex systems and could be applied with other attribution-based XAI methods, to provide additional insight into existing and future systems. The presented methodology demonstrates significant potential for XAI in evaluating MU methods. Additionally, after leveraging different verification scenarios, the next step can be joint end-to-end optimization to ensure that XAI features are considered to directly target undesired attributes at training time, thus enhanc-ing the robustness and applicability of MU frameworks."}]}