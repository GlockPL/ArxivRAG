{"title": "ITBench: Evaluating AI Agents across\nDiverse Real-World IT Automation Tasks", "authors": ["Saurabh Jha", "Rohan Arora", "Yuji Watanabe", "Takumi Yanagawa", "Yinfang Chen", "Jackson Clark", "Bhavya Bhavya", "Mudit Verma", "Harshit Kumar", "Hirokuni Kitahara", "Noah Zheutlin", "Saki Takano", "Divya Pathak", "Felix George", "Xinbo Wu", "Bekir O Turkkan", "Gerard Vanloo", "Michael Nidd", "Ting Dai", "Oishik Chatterjee", "Pranjal Gupta", "Suranjana Samanta", "Pooja Aggarwal", "Rong Lee", "Pavankumar Murali", "Jae-wook Ahn", "Debanjana Kar", "Ameet Rahane", "Carlos Fonseca", "Amit Paradkar", "Yu Deng", "Pratibha Moogi", "Prateeti Mohapatra", "Naoki Abe", "Chandrasekhar Narayanaswami", "Tianyin Xu", "Lav R. Varshney", "Ruchi Mahindru", "Anca Sailer", "Laura Shwartz", "Daby Sow", "Nicholas C. M. Fuller", "Ruchir Puri"], "abstract": "Realizing the vision of using AI agents to au-\ntomate critical IT tasks depends on the ability to\nmeasure and understand effectiveness of proposed\nsolutions. We introduce ITBench, a framework\nthat offers a systematic methodology for bench-\nmarking AI agents to address real-world IT au-\ntomation tasks. Our initial release targets three\nkey areas: Site Reliability Engineering (SRE),\nCompliance and Security Operations (CISO), and\nFinancial Operations (FinOps). The design en-\nables AI researchers to understand the challenges\nand opportunities of AI agents for IT automation\nwith push-button workflows and interpretable met-\nrics. ITBench includes an initial set of 94 real-\nworld scenarios, which can be easily extended by\ncommunity contributions. Our results show that\nagents powered by state-of-the-art models resolve\nonly 13.8% of SRE scenarios, 25.2% of CISO\nscenarios, and 0% of FinOps scenarios. We ex-\npect ITBench to be a key enabler of AI-driven IT\nautomation that is correct, safe, and fast.", "sections": [{"title": "1. Introduction", "content": "Modern IT systems are driving many facets of our economy.\nThey have grown significantly in complexity with the adop-\ntion of cloud computing and agile development practices\n(Harvard Business Review Research Report, 2022; Fuller,\n2024). Effective management of these systems is becoming\nextremely challenging as corporations struggle to keep up\nwith this growing complexity. Various IT personas ranging\nfrom Chief Information Officers to Site Reliability Engi-\nneers, Security and Compliance officers and IT engineers\nin general are struggling to ensure resiliency, reliability,\nsecurity and cost effective operations of IT Systems.\nThe recent CrowdStrike outage highlighted these challenges\nas it brought down our society's most critical systems from\nhospital services to air travel and was estimated to cost US\nFortune 500 companies a staggering $5.4 billion (Kerner,\n2024). This incident underlined the critical need for in-\ntelligent IT incident resolution, compliance and risk man-\nagement capabilities, a topic also addressed in the Digital\nOperational Resiliency Act (DORA) in Europe (Parliament\nand the Council of the European Union, 2024).\nThe rising popularity of AI agents and their projected ability\nto handle intricate tasks have increased the demand for AI\nagents managing IT systems (John, 2024; Miguel Carreon,\n2024; Pujar et al., 2023). Given the complexity of IT tasks,\na major hurdle for this research is establishing systematic\nmethods to assess the effectiveness of AI agents prior to\ntheir production deployment (Bogin et al., 2024; Kapoor\net al., 2024). Consequently, there is an urgency to develop\nmethods for evaluation of AI agents based on real IT tasks\nand their corresponding environments.\nThis paper addresses this critical need and presents ITBench,\na first of its kind framework that is both comprehensive and\nvisionary for benchmarking real-life IT automation tasks.\nThe goal of ITBench is to measure the performance of AI\nagents across a wide variety of complex and real-life IT\ntasks across personas including, Site Reliability Engineering\n(SRE) focusing on availability and resiliency, Compliance\nand Security Operations (CISO) ensuring compliance and\nsecurity of IT implementations, and Financial Operations\n(FinOps) enforcing cost efficiencies and optimizing return\non investment, among others (as shown in Figure 1)."}, {"title": "2. Related Work", "content": "ITBench targets a comprehensive set of tasks for a wide\nrange of personas within IT automation. The initial release\nof ITBench focuses on evaluating scenarios within IT Op-\nerations (ITOps). Figure 1 illustrates currently targeted\npersonas and exemplar tasks that they are routinely facing.\nThere is clearly a rising trend and interest in developing\nbenchmarks to evaluate AI and ML techniques in ITOps\nwith specific focus on SRE, CISO and FinOps.\nTrainTicket (Zhou et al., 2018) provides 22 scenarios col-\nlected through an industrial survey of real-world incidents,\nusing hardcoded faults in the TrainTicket application to fo-\ncus on fault localization. AIOpsLab (Chen et al., 2024a)\nprovides 10 SRE-focused scenarios (referred to as problems)\nutilizing a real environment (system) integration that allows\ninteractive access to text, time series, and tabular data. In-\nsightBench (Sahu et al., 2024) provides 100 scenarios to\nanalyze ticket data using static tabular data and synthetic\nscenarios. TSB-AD (Liu and Paparrizos, 2024a) focuses on\nanomaly detection with 40 synthetic scenarios.\nCIS-Benchmark (CIS, 2024) provides best practices for\nsecuring IT infrastructure. Despite the name of \"bench-\nmark\", it offers only recommendation policies; it provides\nno experimental platform. Recently, Cloud Native Compute\nFoundation (CNCF) Sandbox project (OSCAL-compass,\n2024) released an SDK to support the translation of the CIS\nhuman readable formats into (OSCAL, 2024) compliance\nas code standard of the National Institute of Standards and\nTechnology for programmatic usage in compliance automa-\ntion. ITBench CISO automation leverages this technology\nto assess policy requirements."}, {"title": "3. ITBench", "content": "ITBench is a systematic benchmarking framework and run-\ntime environment designed to evaluate AI agents tasked\nwith automating IT operations, incorporating a robust archi-\ntecture (see Figure 2) comprising the AI Agent, Scenario\nSpecification and Environment, Evaluator, and Leaderboard\nto facilitate comprehensive performance assessment.\nHere, we present a brief overview of the key components:\n1) Scenario Specification and Environment, 2) AI Agents,\nand 3) Leaderboard. More details are in Appendix B."}, {"title": "3.1. Scenario Specification and Environment", "content": "The bench incorporates a collection of problems that we call\nscenarios. For example, one of the problems in ITBench is\nto resolve a \"High error rate on service order-management\"\nin a Kubernetes environment. Another example that is rel-\nevant for CISO persona involves assessing the compliance\nposture for a \"new control rule detected for RHEL 9.\" A\nfundamental challenge is to emulate such problems in a man-\nageable testbed environment. A scenario environment is an\noperational testbed in which a specific problem(s) occurs.\nA scenario p generally corresponds to a problem to be solved\nin ITBench. We formalize p as a tuple < M,E,T,D >,\nwhere the variables are as follows:\nScenario Specification. M represents metadata and deploy-\nment descriptors, for each scenario, which is stored in the\nScenario Specs database in ITBench (see Figure 2). Exem-\nplar metadata elements per scenario include scenario_name,\nscenario_description, scenario_domain, scenario_class,\nscenario_complexity, and scenario_groundtruth (see Ta-\nble 2), which are defined below:\n\u2022 scenario_name is name given to a scenario. For exam-\nple, a scenario in ITBench has name \"Recommendation\nService Cache.\"\n\u2022 scenario_description describes the scenario. Example\ndescription of the scenario is \"Recommendation Service\nin Astronomy Shop has a cache failure.\"\n\u2022 scenario_domain represents different personas namely\n\"SRE\", \"CISO\", \"FinOps\" within IT automation.\n\u2022 scenario_class is used to group similar scenarios, such\nas \"Kyverno-opa\", \"Kyverno-update\", \"CacheFailure\",\n\"HighCPU\", and \"CorruptImage\".\n\u2022 scenario_complexity captures the difficulty of a problem\nand is defined using domain knowledge. Figure 4a, shows\nthe breakdown of SRE, CISO, and FinOps scenarios in the\nbench. Figure 4b, 4c, and 4d shows scenario_complexity\ndistribution for SRE, CISO, and FinOps, respectively.\nSRE scenarios are developed based on real-world inci-\ndents observed in our own SaaS products. CISO scenarios\nare based on CIS benchmark (for Internet Security, CIS).\nFinOps scenarios are sparsely represented in ITBench due\nto the lack of standard benchmarks. We based our sce-\nnario using \"Domains\u201d and \u201cCapabilities\u201d identified by\nthe FinOps Foundation (Foundation, 2025a) to describe\nkey business outcomes.\n\u2022 scenario_groundtruth records task-specific outcomes that\nthe Evaluator uses to compare against the agent's expected\noutput. For instance, in incident resolution for SREs,\nthe ground truth for the Diagnosis task includes a list of\nentities involved in the fault propagation chain, the actual\nfault propagation chain(s), and fault conditions, while\nfor the Mitigation task, it captures plausible mitigation"}, {"title": "3.2. AI Agents", "content": "In IT automation, the different personas are focused on\nspecific desired outcome, which defines their automation\ngoals. For SREs, incident resolution is the primary objective.\nAchieving this can involve multiple steps, such as diagnos-\ning an incident, or a single step, like generating a diagnosis\nreport. CISO persona focuses on the regulatory controls\nposture assessment process, including Collect evidence and\nScan assessment posture tasks. FinOps persona focuses on\nthe cost management, where sample tasks include Identify\ninefficiency and Mitigate inefficiency. During evaluation,\neach step (task) is assessed independently and is measured\nusing well defined metrics, see Table 3.\nThe goal of ITBench is to evaluate AI agents on a broad\nrange of real-world IT automation tasks that are otherwise\nperformed by SREs, FinOps, CISO personas.\nIn this paper, an AI agent is defined as an autonomous or\nsemi-autonomous software program that uses an LLM to\nplan, make decisions, interact with the target environment,\nand execute actions to achieve goals. An AI agent is ex-\npected to successfully handle any of the scenarios in the\nITBench, by interacting with the environment.\nAs shown in Figure 3, agent and environment form a Par-\ntially Observed Markov Decision Process (POMDP), where\nthe state is the snapshot of the environment. The state tran-\nsitions are determined by the environment, which are then\n(partially) observed by the agent.\nGiven a scenario p instantiated in an environment E, an\nagent probes the environment via one of the tools and re-\nceives an observation $o_t \\in O$, based on which, it decides\nthe next action:\n$a_t = f(o_t | \\bar{o}_{t-1}, \\bar{a}_{t-1})$\nHere f is the agent's decision function. $\\bar{o}_{t-1}$ is the sequence\nof observations up to time t - 1 and $\\bar{a}_{t\u22121}$ is the sequence of\nactions taken up to t-1.\nAt the beginning, $o_0$ may be a triggering event showing a\nproblematic state $s_0$ of the environment. Given state $s_{t-1}$\nand action $a_{t-1}$, the environment transitions to the next\nstate:\n$s_t = g(s_{t-1}, a_{t-1})$\nThe observation $o_t$ is determined as a function of the state\nand is in general a proxy for the environment state $s_t$, hence"}, {"title": "3.3. Baseline AI Agents", "content": "We developed baseline agents: SRE-Agent for SRE, Com-\npliance Assessment Agent for CISO, and FinOps-agent for\nFinOps. Each of these agents uses state-of-the-art agentic\ntechniques such as ReAct-based planning (Yao et al., 2023),\nreflection (Shinn et al., 2023), and disaggregation (Xu et al.,\n2023). Reflection techniques vary from syntax checking/lint-\ning, semantic validation (Xie et al., 2024a), and Ilm-as-a-\njudge (Zheng et al., 2023).\nWe open source two baseline agents (SRE\u00b9 and CISO\u00b2)\nalong with ITBench. We use the open-source CrewAI frame-\nwork (cre) to create and manage agents. The agents can be\nconfigured to use various LLMs either through watsonx,\nAzure, or vLLM. Each agent is initialized with a prompt"}, {"title": "3.4. Leaderboard", "content": "ITBench includes a leaderboard to promote reproducibility\nand comparative analysis, following the AI common task\nframework (Donoho, 2019; Varshney et al., 2019). The\nleaderboard offers a predefined, extensible set of perfor-\nmance metrics designed to provide clear insights into agent\nperformance relative to the evaluation criteria.\nITBench devises scoring methods for partially correct so-\nlutions to provide meaningful feedback for summative as-\nsessments. This comprehensive approach establishes a new\nstandard for evaluating and advancing AI-driven solutions\nin IT automation. For each scenario that an agent works\non, upon task completion, the ITBench records the final sys-\ntem state, which is then used at the end of all scenario runs\nalong with the pre-defined ground truth data to validate how\nwell the agent performed across all the scenarios. For each\nscenario that an agent works on, upon task completion, the\nITBench records the final system state, which is then used\nat the end of all scenario runs along with the pre-defined\nground truth data to validate how well the agent performed\nacross all the scenarios.\nWe are open-sourcing a small subset (11 out of 94) of sce-\nnarios ITBench\u00b3 along with the baseline agents to help the\ncommunity familiarize with ITBench through practical ex-\namples. We reserve the remaining scenarios in ITBench to\nbenchmark and evaluate the submitted agentic solutions."}, {"title": "4. Results", "content": "4.1. Evaluation Setup\nTo understand the impact of reasoning and planning ca-\npabilities of LLMs on ITBench scenarios, we instantiate\nour agents using different LLM models, both for natural\nlanguage reasoning and code generation. Specifically, we\nemploy GPT-404, Llama-3.3-70B-instruct, Llama-3.1-8B-\ninstruct, and Granite-3.1-8B-instruct for tasks that rely on\nnatural language understanding and reasoning. For code-\nfocused use cases, we utilize GPT-40-mini, Llama-3.1-405b-\ninstruct, and Mixtral-8x7b-instruct. All models use a con-\ntext window of 128K tokens, enabling them to process more\nextensive input sequences.\nWe conduct our experiments primarily on AWS EC2 in-\nstances (m4.xlarge), although ITBench can also be readily\ndeployed on a consumer-grade laptop using a pseudo-cluster,\nthus making it easier to develop AI agents (Appendix C.4.1)\nBelow, we provide an overview of our baseline agents' per-\nformance across ITBench scenarios for SRE, CISO, and\nFinOps. Our findings indicate that both open-source and\nproprietary models often struggle with real-world tasks, un-\nderscoring the importance of benchmarks that push the lim-\nits of reasoning and planning in foundation models. For\nmore comprehensive results and detailed scenario-level dis-"}, {"title": "4.3. Impact of Scenario Complexity", "content": "SRE. We categorize scenarios as Easy, Medium, or Hard\nbased on factors such as fault propagation chain length,\nnumber of resolution steps, and the diversity of technolo-\ngies involved, as described in Equation (6). Our results\nshow that success rates (pass@1) clearly decline as the sce-\nnario_complexity increases. For example, GPT-40 (the best"}, {"title": "4.4. Inherent Non-determinism in the Environment", "content": "GPT-40 remains the top performer across all evaluated per-\nsonas (SRE, CISO, and FinOps), yet it still exhibits no-\ntable variability in scenario outcomes. For example, the\nSRE-agent with GPT-40 struggles to maintain deterministic\nbehavior despite hyperparameter tuning aimed at ensuring\nconsistency. SRE-agent with GPT-40 diagnosed the prob-\nlem only in 6 out of 10 runs for scenario 13, 1 out of 10\nruns for scenario 8, and 8 out of 10 runs for scenario 21,\nrespectively (refer to Figure 14 for details on all scenarios).\nSimilarly, it mitigated 6 out of 10 runs for scenario 16, 2\nout of 10 runs for scenario 8, and 5 out of 10 runs for sce-\nnario 21, respectively (refer to Figure 17 for details on all\nscenarios). This inherent non-determinism was observed\nwith FinOps and CISO scenarios as well.\nThese fluctuations arise from minor real-time telemetry\nchanges, which can alter the large language model's to-\nken generation. By tracking such dynamic behavior over\nmultiple runs, ITBench provides crucial insights into each\nagent's robustness and reliability."}, {"title": "5. Discussion and Conclusion", "content": "We presented ITBench, the first framework and experimen-\ntal platform to benchmark AI Agents for IT automation\ntasks. ITBench strives to capture the complexity of modern\nIT systems and the diversity of IT tasks. The reproducibility\nof ITBench ensures the community-driven effort despite\ninherent nondeterminism of large-scale IT systems.\nOne of the key design principles of ITBench is ensuring\nits flexibility to support diverse areas of different IT sys-\ntems and its extensibility to new scenarios. While current\nscope of ITBench is comprehensive and representative, we\nplan to further enrich the benchmark suites by adding other\nimportant processes essential to modern IT automation. Fur-\nthermore, we plan to expand our benchmarking beyond\nevent-triggered scenarios. We are actively working to ex-\npand scenario coverage for the supported processes and pro-\nmote growth through open-community contributions. We\ninvite the community to reproduce their real-world-inspired\nincidents in a synthetic sandboxed environment leveraging\nthe ITBench. We expect that everyone contributing can\nbring their expertise to the table.\nWe expect ITBench to drive the innovations of AI agent-\nbased techniques with a direct impact on the safety, effi-\nciency, and intelligence of today's IT infrastructures. With\nITBench, we are starting to explore many deep, exciting\nopen problems: How to develop domain-specific AI agents\nthat specialize in certain types of IT tasks? How to orches-\ntrate multiple agents with various expertise to collaborate on\nbigger projects? How can we ensure safety of agent-driven\nsolutions? How can we effectively use human-in-the-loop\nwhile developing diverse adaptive agents? We invite every-\none to participate in answering these questions and realizing\nthe vision of using AI agents to automate critical IT tasks."}, {"title": "6. Statements", "content": "6.1. Ethics & Broader Impacts\nThis research presents a novel benchmarking framework to\nmeasure the performance of AI agents across a wide variety\nof complex and real-life IT tasks, which has the potential to\nbe a key enabler for AI-driven IT automation that is correct,\nsafe and fast. While the primary focus is on advancing the\nfield of machine learning, as this effort is an open-source\nframework built with open-source technologies, it allows\norganizations with proprietary technologies to use it for\ndeveloping and benchmarking their solutions more effec-\ntively. It also encourages mindsharing in the community\nand lowers the barrier to innovate in IT domain.\nAgents that interact with the system pose several risks. We\nidentify three main risks that could arise when building and\nusing a ITBench and associated agents, then discuss how\nwe incorporates measures that mitigate such problems.\nFirst is the security risks that come with executing LM-\ngenerated code/commands on the system. Examples include\nexecuting commands like kubectl delete node and\nrm -rf asset/. To defend against this, we containerize\nthe agents, and also provide a self-contained Kubernetes\nenvironment to create various scenarios.\nSecond, if the wider community develops interest for IT-\nBench and associated agents and builds upon it, it is also pos-\nsible that illegitimate evaluation datasets or infrastructure\ncan be used to inject testing devices with malicious code or\ninstructions to generate malicious code. For instance, an un-\nofficial repository claiming to host an inference/evaluation\nharness for ITBench and associated agents could include\na task instance with an issue description that tells the LM\nagent to build key logging functionality and store it in a\nhidden folder. To eliminate confusion and reduce the pos-\nsibility of such an event, we provide clear guidelines listed\non our GitHub repositories, data stores, and websites indi-\ncating the official repositories and channels that we actively\nmaintain. We also encourage third parties to incorporate any\nimprovements into our codebase and help with integrating\nsuch contributions.\nLastly are the consequences of ITBench agents being de-\nployed in the real world. Prior works have conceptualized\nand put forth prototypes of agents that can carry out offen-\nsive security measures. It is also not difficult to imagine that\na system like SRE-Agent can be incorporated into pipelines\nresulting in the production of malicious code and libraries.\nThe strong performance of agents on ITBench implies that\nfuture AI systems will likely be increasingly adept in the\naforementioned use cases. Releasing ITBench agents as\nopen source agents can support research towards designing\nsound, effective constraints for what software engineering\nagents are permitted to do. It can also serve as a system\nthat legal experts and policy-making entities can experi-\nment with to shape the future of what AI-driven end to end\nsoftware engineering could look like."}, {"title": "6.2. Reproducibility", "content": "To help the greater community reproduce the results pre-\nsented in this paper and build on the ITBench, we open\nsource all of our resources that were created for this project.\nThe source code for the interactive pipeline, context man-\nagement logic, command implementations, interface design,\nand everything else is entirely available in a GitHub repos-\nitory. We provide extensive text and video documentation\ndescribing how to run and modify different parts of the\ncodebase. Practitioners should be able to easily recover our\nfindings by running the agent with simple scripts. The re-\nsults presented in the main and supplementary parts of this\npaper can be fully obtained by following instructions in the"}, {"title": "A. Related Work", "content": "LM and agents for resolving IT automation tasks. There\nis a surge in use of AI/ML for handling IT automation tasks.\nWe describe related work for each persona."}, {"title": "A.1. Site Reliability Engineering", "content": "IT scenario resolution encompasses tasks such as detec-\ntion (e.g., identifying anomalies or outages) (Guo et al.,\n2015; Leners et al., 2011; Sigelman et al., 2010; Fonseca\net al., 2007), diagnosis (e.g., pinpointing root causes through\nmetrics and logs) (Tan et al., 2019; Jha et al., 2020; Ma\net al., 2014; Salesforce, 2023), and mitigation (e.g., op-\nerational fixes or code changes). These efforts often rely\non supporting tasks like ticket analysis and routing (Gao\net al., 2020; Liu et al., 2023b; Arzani et al., 2016), anomaly\ndetection (Liu and Paparrizos, 2024a), topology extrac-\ntion (Ashok et al., 2024; Chakraborty et al., 2023; Pham\net al., 2024; Yao et al., 2024), causal (Budhathoki et al.,\n2022; Microsoft and contributors, 2023; Ikram et al., 2022;\nChakraborty et al., 2023) and interventional (Wang et al.,\n2023; Bagehorn et al., 2022) analysis using IT data. Clearly,\nthere is significant research in this area, fully automating\nincident resolution or providing actionable insights to hu-\nmans remains elusive due to the complexity of real-world\nsystems, the variability of incidents, and the challenge of\nincorporating contextual knowledge into AI systems (Jha\net al., 2020). Recent advances in language models (LMs)\nhave led to their adoption of ticket data analysis and diagno-\nsis tasks (Roy et al., 2024; Ahmed et al., 2023a; Xie et al.,\n2024b; Chen et al., 2023; Zhang et al., 2024). Most notable\nexamples include Cloud Atlas use LLMs for causal graph\nconstruction (Xie et al., 2024b), RCACopilot for ticket anal-\nysis (Chen et al., 2023) with the aim to diagnose and mitigate\nincidents. However, they achieve poor performance com-\npared to other techniques. For example, (Roy et al., 2024)\nshows that chain-of-thought only achieves accuracy of 35%.\nMore recently, LMs are used in agentic workflows, engaging\nwith real or virtual environments, using several tools at their\ndisposal, for tasks like web navigation (Drouin et al., 2024;\nBoisvert et al., 2024; Koh et al., 2024), system control (Sahu\net al., 2024; Zhang et al., 2024; Chen et al., 2024a), and\ncode generation (Yang et al., 2024a). However, the initial\nresults of these works show a high variability in the success\nrate -35% in InsightBench (Boisvert et al., 2024) and the\nReAct-based agent for ticket data analysis (Roy et al., 2024)\nto 100% in Flash (Chen et al., 2024a) for incident resolution\ndespite the fact that it is a much harder task than identifying\nplanted insights in tabular and ticket data. Our own results\nin this work suggest that LLMs and agents struggle to con-\nsistently complete incident resolution tasks. We assert that"}, {"title": "A.2. Compliance", "content": "Compliance automation software is emerging to help busi-\nnesses streamline and automate compliance processes, re-\nducing the need for manual monitoring and tracking of\nregulations. This ensures continuous adherence to laws. In\nparticular, compliance as code is a very recent development\nin the IT industry motivated by companies and audit agen-\ncies shifting from annual audits to expectations of continu-\nous and automated measurement of compliance to maintain\ncontrol of their regulated environments' posture and risks\nfor cyberattacks.\nRecent works (Papanikolaou et al., 2011; Tupsamudre et al.,\n2022) have applied AI/ML techniques to speed up these\ntasks, focusing on mapping regulatory requirements to stan-\ndard control frameworks such as NIST 800-53 (NIST 800-\n53). Our agentic automation in the current ITBench solution\npioneers this type of effort to author compliance artifacts\nthrough AI / ML by bridging compliance as code into policy\nas code. Policy engines have a longer history in the IT in-\ndustry compared to compliance as code; however, emerging\ngeneral usage policy engines such as (Int, d) try to address\nthe need for a common framework for continuous compli-\nance. We are not aware of any effort -albeit critical and\nneeded- related to benchmarking of compliance automation\nsoftware, whether with or without agentic support."}, {"title": "A.3. FinOps", "content": "The area of IT cost management encompasses multiple dis-\nciplines, namely FinOps, IT Financial Management (ITFM),\nTechnology Business Management (TBM) and Porfolio\nBusinesss Management (PBM). At present, the FinOps do-\nmain typically deals with cloud costs (Storment and Fuller,\n2023; Yang et al., 2024b), which includes compute nodes,\nmemory, other storage, networking, etc., that are incurred\nwith one of the hyperscalers. ITFM includes on-prem infras-\ntructure, licensing, labor, procured services, tech support,\netc. The TBM Council provides a standard taxonomy to de-\nscribe cost sources, technologies, IT resources (IT towers),\napplications, and services. In addition, there are industry-\nspecific extensions to the taxonomy, such as for healthcare,\nbanking, etc. In essence, this taxonomy provides a generally\naccepted way of categorizing and reporting IT costs and\nother metrics. PBM refers to the practice of managing a\ncollection of projects and programs within an organization,\nensuring alignment with the overall business strategy and\nmaximizing their collective value by allocating resources\nefficiently. The FinOps Foundation has indicated that over\ntime it will include elements from ITFM, TBM, and PBM.\nCurrently, for FinOps, there is no benchmark that fits the\ndefinition of benchmark that we are using in this paper.\nHowever, over the years, the FinOps Foundation (Founda-\ntion, 2025a) has compiled several KPIs that can form the\nbasis for use cases and scenarios for a FinOps benchmark.\nCurrent FinOps Foundation KPIs include:\n\u2022 Usage or Spend Apportionment Validation\n\u2022 Total Unpredicted Variance of Spend\n\u2022 Percent of Compute Spend Covered by Commitment Dis-\ncounts\n\u2022 Effective Savings Rate Percentage\n\u2022 Percentage of Commitment Discount Waste\n\u2022 Percent of Unused Resources\n\u2022 Auto-scaling Efficiency Rate\n\u2022 Forecast Accuracy Rate (Usage, Spend)\n\u2022 Percentage of Unallocated Shared CSP Cloud Cost\n\u2022 Percentage Variance of Budgeted vs. Forecasted CSP\nCloud Spend\n\u2022 Percentage of CSP Cloud Costs that are Tagging Policy\nCompliant\n\u2022 Percent Storage on Frequent Access Tier\n\u2022 Percentage of Legacy Resource\nWith the advent of the cloud, the academic and industrial\nresearch communities have also been active in investigating\nways to optimize costs while balancing multiple objectives.\nRecent works in the space of FinOps have focused on apply-\ning machine learning and mathematical optimization tech-\nniques (Qiao et al., 2021; Yang et al., 2024b) to better serve\ncustomers' cloud infrastructure needs while offering them\ninsights and recommendations on how they could optimize"}, {"title": "B.1. Benchmark Registration", "content": "This phase comprises two main steps: (i) scenario develop-\nment and registration, and (ii) tasks and evaluation metrics\nregistration.\nScenario Development and Registration\nOur scenarios are designed to instantiate real-world IT prob-\nlems in realistic and manageable environments. Each sce-"}, {"title": "B.2. Agent Registration", "content": "During this phase, the Agent Submitter first registers as a\nuser on the platform, then follows with the Agent Registra-\ntion."}, {"title": "B.2.1. AGENT REGISTRATION", "content": "During Agent Registration, the Agent Submitter specifies\nthe agent metadata as shown in Table 8.\nOnce the agent has been registered, the Agent Submitter\nselects the agent, and the corresponding benchmarks are re-\ntrieved from the database using the agent_type, agent_level,\nand scenario_class specified during registration for the\nAgent. The Agent Submitter subsequently receives the tasks\nthat the agent must complete to meet the designated objec-\ntive, each of which has pre-defined evaluation metrics."}, {"title": "B.3. Leaderboard", "content": "Effective benchmarking of IT automation tasks, especially\nwhen selecting LLMs tailored to an organization's specific\nneeds, requires consistent tracking and comparison of agent\nperformance. The Leaderboard facilitates this need by offer-"}, {"title": "C. Site Reliability Engineering", "content": "C.1. Background\nWith the unprecedented growth in scale and complexity\nof modern IT systems and infrastructures, failures are the\nnorm instead of exceptions (Patterson et al., 2002; Gunawi\net al., 2016; Kendrick, 2012; Di Martino et al., 2014; Veer-\naraghavan et al., 2018; Liu et al., 2019; Ghosh et al., 2022).\nFirst, hardware failures are frequent in large-scale IT infras-\ntructures. For example, a new cluster at Google undergoes"}, {"title": "C.3. ITBench Architecture", "content": "ITBench uses open source technologies to create completely\nrepeatable and reproducible incidents (scenarios) on a Ku-\nbernetes platform as shown in Figure 7.\nOrchestration. The core workflow involves a sequence\nof interactions between the SRE-Agent and various com-\nponents of ITBench. Initially, SRE-Agent (1) enrolls"}]}