{"title": "Optimizing Sensor Redundancy in Sequential Decision-Making Problems", "authors": ["Jonas N\u00fc\u00dflein", "Maximilian Zorn", "Fabian Ritz", "Jonas Stein", "Gerhard Stenzel", "Julian Sch\u00f6nberger", "Thomas Gabor", "Claudia Linnhoff-Popien"], "abstract": "Reinforcement Learning (RL) policies are designed to predict actions based on current observations to maximize cumulative future rewards. In real-world applications (i.e., non-simulated environments), sensors are essential for measuring the current state and providing the observations on which RL policies rely to make decisions. A significant challenge in deploying RL policies in real-world scenarios is handling sensor dropouts, which can result from hardware malfunctions, physical damage, or environmental factors like dust on a camera lens. A common strategy to mitigate this issue is the use of backup sensors, though this comes with added costs. This paper explores the optimization of backup sensor configurations to maximize expected returns while keeping costs below a specified threshold, C. Our approach uses a second-order approximation of expected returns and includes penalties for exceeding cost constraints. We then optimize this quadratic program using Tabu Search, a meta-heuristic algorithm. The approach is evaluated across eight OpenAI Gym environments and a custom Unity-based robotic environment (RobotArmGrasping). Empirical results demonstrate that our quadratic program effectively approximates real expected returns, facilitating the identification of optimal sensor configurations.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning (RL) has emerged as a prominent technique for solving sequential decision-making problems, paving the way for highly autonomous systems across diverse fields. From controlling complex physical systems like tokamak plasma (Degrave et al., 2022) to mastering strategic games such as Go (Silver et al., 2018), RL has demonstrated its potential to revolutionize various domains. However, when transitioning from controlled environments to real-world applications, RL faces substantial challenges, particularly in dealing with sensor dropouts. In critical scenarios, such as healthcare or autonomous driving, the failure of sensors can result in suboptimal decisions or even catastrophic outcomes.\nThe core of RL decision-making lies in its reliance on continuous, accurate observations of the environment, often captured through sensors. When these sensors fail to provide reliable data-whether due to hardware malfunctions, environmental interference, or physical damage the performance of RL policies can degrade significantly (Dulac-Arnold et al., 2019). This vulnerability highlights the importance of addressing sensor dropout to ensure the robustness of RL systems in real-world applications.\nIn domains such as aerospace, healthcare, nuclear energy, and autonomous vehicles, the consequences of sensor failures are particularly severe. A compromised sensor network in these fields can endanger lives, harm the environment, or result in costly failures. Thus, improving the resilience of RL systems to sensor dropouts is not just desirable\u2014it is critical for ensuring their safe and reliable deployment in mission-critical applications.\nOne common approach to mitigate the risks of sensor dropouts is the implementation of redundant backup sensors. These backup systems provide an additional layer of security, stepping in when primary sensors fail, thereby maintaining the availability of crucial data. While redundancy can enhance system resilience, it also introduces significant costs, and not all sensor dropouts result in performance degradation severe enough to justify the investment in backups.\nThis paper presents a novel approach to optimizing backup sensor configurations in RL-based systems. Our method focuses on balancing the trade-off between system performance-quantified by expected returns in a Markov Decision Process"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Markov Decision Processes (MDP)", "content": "Sequential Decision-Making problems are frequently modeled as Markov Decision Processes (MDPs). An MDP is defined by the tuple $\\mathcal{E} = (\\mathcal{S}, \\mathcal{A}, T, r, p_0, \\gamma)$, where $\\mathcal{S}$ represents the set of states, $\\mathcal{A}$ is the set of actions, and $T(s_{t+1} | s_t, a_t)$ is the probability density function (pdf) that governs the transition to the next state $s_{t+1}$ after taking action $a_t$ in state $s_t$. The process is considered Markovian because the transition probability depends only on the current state $s_t$ and the action $a_t$, and not on any prior states $S_{t<t}$.\nThe function $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ assigns a scalar reward to each state-action pair $(s_t, a_t)$. The initial state is sampled from the start-state distribution $p_0$, and $\\gamma \\in [0, 1)$ is the discount factor, which applies diminishing weight to future rewards, giving higher importance to immediate rewards.\nA deterministic policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$ is a mapping that assigns an action to each state. The return $R = \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)$ is the total (discounted) sum of rewards accumulated over an episode. The objective, typically addressed by Reinforcement Learning (RL), is to find an optimal policy $\\pi^*$ that maximizes the expected cumulative return:\n$\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{p_0} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\right]$\nActions $a_t$ are selected according to the policy $\\pi$. In Deep Reinforcement Learning (DRL), where state and action spaces can be large and continuous, the policy $\\pi$ is often represented by a neural network $f_\\theta(s)$ with parameters $\\theta$, which are learned through training (Sutton and Barto, 2018)."}, {"title": "2.2 Quadratic Unconstrained Binary Optimization (QUBO)", "content": "Quadratic Unconstrained Binary Optimization (QUBO) (Mooney et al., 2019) is a combinatorial optimization problem defined by a symmetric, real-valued $(m \\times m)$ matrix $Q$, and a binary vector $x \\in \\mathbb{B}^m$. The objective of a QUBO problem is to minimize the following quadratic function:\n$x^* = \\arg \\min H(x) = \\arg \\min \\sum_{i=1}^{m} \\sum_{j=i}^{m} x_i x_j Q_{ij}$ (1)\nThe function $H(x)$ is commonly referred to as the Hamiltonian, and in this paper, we refer to the matrix $Q$ as the \"QUBO matrix\".\nThe goal is to find the optimal binary vector $x^*$ that minimizes the Hamiltonian (Roch et al., 2023). This task is known to be NP-hard (Glover et al., 2018), making it computationally intractable for large instances without specialized techniques. QUBO is a significant problem class in combinatorial optimization, as it can represent a wide range of problems. Moreover, several specialized algorithms and hardware platforms, such as quantum annealers and classical heuristics, have been designed to solve QUBO problems efficiently (Morita and Nishimori, 2008; Farhi et al., 2014; Farhi and Harrow, 2016; N\u00fc\u00dflein et al., 2023b; Zielinski et al., 2023; N\u00fc\u00dflein et al., 2023a).\nMany well-known combinatorial optimization problems, such as Boolean satisfiability (SAT), the"}, {"title": "3 ALGORITHM", "content": ""}, {"title": "3.1 Problem Definition", "content": "Let $\\pi$ be a trained agent operating within an MDP $\\mathcal{E}$. At each timestep, the agent receives an observation $o \\in \\mathbb{R}^u$, which is collected using $n$ sensors $\\{s_i\\}_{1 \\leq i \\leq n}$. Each sensor $s_i$ produces a vector $o_i$, and the full observation $o$ is formed by concatenating these vectors: $o = [o^1, o^2, ..., o^n]$. The complete observation $o$ has dimension $|o| = \\sum_i |o^i|$.\nAt the start of an episode, each sensor $s_i$ may drop out with probability $d_i \\in [0, 1]$, meaning it fails to provide any meaningful data for the entire episode. In the event of a dropout, the sensor\u2019s output is set to $o^i = 0$ for the rest of the episode.\nIf $\\pi$ is represented as a neural network, it is evident that the performance of $\\pi$, quantified as the expected return, will degrade as more sensors drop out. To mitigate this, we can add backup sensors. However, incorporating a backup sensor $s_i$ incurs a cost $c_i \\in \\mathbb{N}$, representing the expense associated with adding that redundancy.\nThe task is to find the optimal backup sensor configuration that maximizes the expected return while keeping the total cost of the backups within a predefined budget $C \\in \\mathbb{N}$. Let $x \\in \\mathbb{B}^n$ be a vector of binary decision variables, where $x_i = 1$ indicates the inclusion of a backup for sensor $s_i$. If $\\mathbb{E}_{d,\\pi, x}[R]$ represents the expected return while using backup configuration $x$, the optimization problem can be formulated with both a soft and hard constraint:\n$x^* = \\arg \\max_{x} \\mathbb{E}_{d,\\pi, x}[R] \\quad s.t. \\quad \\sum_{i=1}^{n} x_i c_i \\leq C$ (2)\nAn illustration of this problem is provided in Figure 1. In the next section, we describe a method to transform this problem into a QUBO representation."}, {"title": "3.2 SensorOpt", "content": "There are potentially $2^n$ possible combinations of sensor dropout configurations, making it computationally expensive to evaluate all of them directly. Since we only have a limited number of episodes $B \\in \\mathbb{N}$ to sample from the environment $\\mathcal{E}$, we opted for a second-order approximation of $\\mathbb{E}_{d,\\pi, x}[R]$ to reduce the computational burden while still capturing meaningful interactions between sensor dropouts.\nTo compute this approximation, we first calculate the probability $q(d)$ that at most two sensors drop out in an episode:\n$q(d) = \\prod_{i=1}^{n} (1-d_i) + \\sum_{i=1}^{n} d_i \\cdot \\prod_{j\\neq i}^{n} (1-d_j) + \\sum_{i=1}^{n} \\sum_{j<i} d_i d_j \\cdot \\prod_{l\\neq i, j}^{n} (1-d_l)$\nNext, we estimate the expected return $\\hat{R}_{(i, j)}$ for each pair of sensors $(s_i, s_j)$ dropping out. This can be efficiently achieved using Algorithm 1, which samples the episode return $\\mathbb{E}_{\\mathcal{E}}^{(i, j)}(\\pi)$ using policy $\\pi$ with sensors $s_i$ and $s_j$ removed. The algorithm begins by calculating two return values for each sensor pair. It then iteratively selects the sensor pair with the highest momentum of the mean return, defined as $| \\bar{R}_{(i, j)}[:-1] - \\bar{R}_{(i, j)} |$. Here, $\\bar{R}_{(i, j)}[:-1]$ denotes the mean of all previously sampled returns for the sensor pair $(s_i, s_j)$, excluding the most recent return, while $\\bar{R}_{(i, j)}$ represents the mean including the most recent return. The difference between these two values measures the momentum, capturing how much the expected return is changing as more episodes are sampled.\nWe base the selection of sensor pairs on momentum rather than return variance to differentiate between aleatoric and epistemic uncertainty (Valdenegro-Toro and Mori, 2022). Momentum is less sensitive to aleatoric uncertainty, making it a more reliable indicator in this context.\nA simpler, baseline approach to estimate $\\hat{R}_{(i, j)}$ would be to divide the budget $B$ of episodes evenly across all sensor pairs $(s_i, s_j)$. In this Round Robin approach, we sample $k = \\frac{B}{n(n+1)/2}$ episodes for each sensor pair and estimate $\\hat{R}_{(i, j)}$ as the empirical mean. In the Experiments section, we compare our momentum-based approach from Algorithm 1 against this naive Round Robin method to highlight its efficiency and accuracy.\nOnce we have estimated all the values of $\\hat{R}_{(i, j)}$, we can compute the overall expected return $\\hat{R}(d)$ for a given dropout probability vector $d$, again using a second-order approximation:"}, {"title": "6 CONCLUSION", "content": "Sensor dropouts present a major challenge when deploying reinforcement learning (RL) policies in real-world environments. A common solution to this problem is the use of backup sensors, though this approach introduces additional costs. In this paper, we tackled the problem of optimizing backup sensor configurations to maximize expected return while ensuring the total cost of added backup sensors remains below a specified threshold, C.\nOur method involved using a second-order approximation of the expected return, $\\mathbb{E}_{d,\\pi.x}[R] \\approx -x^T Q x + \\hat{R}(d)$, for any given backup sensor configuration $x \\in \\mathbb{B}^n$. We incorporated a penalty for configurations that exceeded the maximum allowable cost, C, and optimized the resulting QUBO matrices Q using the Tabu Search algorithm.\nWe evaluated our approach across eight OpenAI Gym environments, as well as a custom Unity-based robotic scenario, RobotArmGrasping. The results demonstrated that our quadratic approximation was sufficiently accurate to ensure that the optimal configuration derived from the approximation closely matched the true optimal sensor configuration in practice."}]}