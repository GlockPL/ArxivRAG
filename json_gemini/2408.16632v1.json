{"title": "MAELSTROM NETWORKS", "authors": ["Matthew S Evanusa", "Yiannis Aloimonos", "Cornelia Ferm\u00fcller"], "abstract": "Artificial Neural Networks has struggled to devise a way to incorporate working\nmemory into neural networks. While the \"long term\" memory can be seen as the\nlearned weights, the working memory consists likely more of dynamical activity,\nthat is missing from feed-forward models. This leads to a weakness in current neu-\nral network models: they cannot actually process temporal data in time, without\naccess to some kind of working memory. Current state of the art models such as\ntransformers tend to \u201csolve\u201d this by ignoring working memory entirely and simply\nprocess the sequence as an entire piece of data; however this means the network\ncannot process the sequence in an online fashion, and leads to an immense explo-\nsion in memory requirements. In the decades prior, a separate track of research\nhas followed recurrent neural networks that maintain a working memory via a dy-\nnamic state, although training these weights has proven difficult. Here, inspired\nby a combination of controls, reservoir computing, deep learning, and recurrent\nneural networks, we offer an alternative paradigm that combines the strength of\nrecurrent networks, with the pattern matching capability of feed-forward neural\nnetworks, which we call the Maelstrom Networks paradigm. This paradigm leaves\nthe recurrent component - the Maelstrom - unlearned, and offloads the learning to\na powerful feed-forward network. This allows the network to leverage the strength\nof feed-forward training without unrolling the network, and allows for the memory\nto be implemented in new neuromorphic hardware. It endows a neural network\nwith a sequential memory that takes advantage of the inductive bias that data is\norganized causally in the temporal domain, and imbues the network with a state\nthat represents the agent's \"self\u201d, moving through the environment. This could\nalso lead the way to continual learning, with the network modularized and \u201cpro-\ntected\" from overwrites that come with new data. In addition to aiding in solving\nthese performance problems that plague current non-temporal deep networks, this\nalso could finally lead towards endowing artificial networks with a sense of \"self\".", "sections": [{"title": "1 INTRODUCTION", "content": "The ultimate goal of artificial intelligence is to recreate the intelligence that biological agents have\ndisplayed to remarkable degree, extract the core components of intelligence, and reproduce this\nin an artificial agent, potentially amplifying this. Since the early days of connectionist networks,\nwhere we attempt to solve this intelligence problem through networks (or graphs) of simple artifi-\ncial neural units, the goal has somewhat drifted away from a general artificial agent towards more\nspecialized tasks, namely, pattern recognition. Feed-forward neural networks, built off of the Per-\nceptron framework (Rosenblatt, 1961), have excelled at pattern recognition, and have reoriented the\nentire field towards this mapping function. This has culimated in the current generation of Large\nLanguage Models, which are at the core, meta-networks of Perceptrons trained using backpropaga-\ntion (Vaswani et al., 2017). What have we sacrificed in this focus on pattern recognition? This is the"}, {"title": "DATA IS TEMPORALLY ORGANIZED", "content": "At the core of this issue is the current foundational viewpoint for deep learning that data in the\nuniverse is, in some sense, I.I.D, and has no underlying structure; it is up to the network to learn\nthis structure from random noise. Creatures, however, evolved in the real world, with real physical\nlimitations on the way that the system evolved to deal with, and survive in, the real world. One of\nthe set conditions that nature gives us, in addition to the 3-dimensional structure of the world, is the\ntemporal nature of cause and effect; data later in time are temporally correlated with the earlier data.\nFigure 1 gives a graphical depiction of this dichotomy. The data that is aligned temporo-causally\nalong the same strands we refer to as temporal sequences. And the memory mechanisms in agents\nthat is responsible for remembering points along the same strand we refer to as Sequence Memory.\n\nThe viewpoint of feed-forward connectionist networks, which is the current paradigm for deep learn-\ning, is that data must be assumed to be independently distributed - the I.I.D prior. We assume that\nthe data is \"given\" to us in a completely random, jumbled state, with no inductive prior on the\nstructure of the data, and it is up to the network to learn the structure of the data. This is what we\nexpect from neural networks - the only inductive prior that we assume is that the data is structured\nhierarchically, which is what leads to our \u201cdeep\u201d learning structures of multi-layered networks. We\ndo not, however, flip this 90 degrees, and think about the structure of the data in time. The data\nin the real world, however, is structured: it follows clear causal relations between the data in time,\nwhich we can visualize as \"strands\" in time. Each of the data that is causally linked sits on the same\n\"strand\", which may branch off from one another. Figure 1 shows these two varying viewpoints\nof data in the world. The story of deep learning has shown that the structure of the network topol-\nogy is paramount: the learning rule (backpropagation) as well as the activation of the neurons has\nlargely remained unchanged in the decades since deep learning came about, only the topology of\nthe network, the structuring of the linear layers into convolution or attention layers, has changed.\nThis insight demonstrates that to effectively tackle networks in the real world, inductive biases must\nbe given for the temporal domain and not just the spatial. This inductive bias takes the form of\nSequential Memory: the ability of the agent to remember data that is causally linked accoring to the\nsame temporal sequence strand. This bias is implemented using modules of resonating, recurrently\nconnected graphs of neurons which we call the Maelstrom.\n\nBRAINS HAVE STATE VIA RESONATING CELL ASSEMBLIES\nHow do creatures persist across time, and remember the memory of temporal sequences as they\noccur? Of course, statistical pattern recognition is a key component, but another component must\nbe the system's ability to retain the information such that a pattern \"readout\" can occur. This idea\nthat the brain's mechanism for recognition is a combination of cell assemblies, which maintain\nactivity, and a readout mechanism, which performs a mapping of activity to motor action and tasks,\nis well documented and is a current theory for brain organization (Buzs\u00e1ki, 2010). According to\nthe theories of Neural Assemblies by Donald Hebb (Hebb, 2005), the activity persists in groups of\nneurons, called assemblies, that resonate their activity across time according to some stimulus. It is\nthis resonance, we argue, is the \"stable state\" that allows living creatures to persist across time. A\nseparate readout mechanism, then, performs mapping of these states, akin to a function learning a\nmapping of states of a dynamical system to outputs.\n\nCELL ASSEMBLIES ARE A TEMPORAL INDUCTIVE BIAS\nThe key insight for the brain is that these assemblies model the temporal sequences by mirroring\nthe cause-effect relationship of the input data, but in the recurrent patterns of the network. Thus,\nthe same temporal patterns that occur in the real world, are \"echoed\u201d in the temporal patterns of\nthe recurrently-linked cell assemblies. This topographical structure of a recurrently connected graph\nstructure that is modular and unhooked from the gradients of the sensory and motor areas are the\ntemporal inductive bias in the brain, and in our proposed Maelstrom network, that allows it to main-\ntain a state and capture sequence memory. Whereas the spatial inductive bias is that the hierarchical\nnature of the features in the data are represented as feed-forward layers of the network, the temporal\""}, {"title": "2 LIMITATIONS OF CURRENT APPROACHES", "content": "Although deep neural networks have shown remarkable performance, their success masks some key\nmissing elements if we are to achieve a thinking machine. Although some posit that Large Language\nModels and this is an open debate in the literature, the current pinnacle of feed-forward networks,\nhave \"reasoning\" capabilities, we would argue here that they rather are simply powerful pattern\nmappers, and that many tasks we acribed to reasoning in the past are actually just complex mapping\ntasks. This is a longer debate outside of the scope of this paper, but for the purposes of this work, we\nargue they cannot be true reasoners in the real world as they are not embodied. And to be embodied,\nthey are missing one critical components: namely a memory of the past activations via a state.\n\n2.1 FEED-FORWARD NETWORKS LACK SEQUENCE MEMORY\nMemory, as a concept, is tricky to pin down, although there have been some interesting definitions\nof memory that bridge the biological with the computational (Zlotnik & Vansintjan, 2019). Here,\nwe take memory in the context of connectionist neural networks to mean: a mechanism by which the\nneural network maintains a state of itself, and contains information about the past from a temporal\nsequence. This is what we refer to as described earlier, as Sequence Memory. The idea of sequence\nmemory is intricately tied to the notion of time: to process time, an agent must record, in some"}, {"title": "3 PRIOR WORK ON CONNECTIONIST MEMORY", "content": "3.1 MEMORY IN CONNECTIONIST NETWORKS\nAs mentioned earlier, the way that researchers think about neural networks in silico, versus how\nneural networks work in vivo, is fundamentally different not only from the algorithmic perspective,\nbut also with respect to how memory is stored. In computers, we simply store our neural network\ncode in RAM or disk storage, which is a collection of buckets that can store any arbitrary numerical\nvalues (without respect to any task). All of the functions that require remembering (such as loading\ncode, the weights, the dataset) are stored in this disk memory. In contrast, in the brain there is no\narbitrary storage separate from the neural network - the storage is the neural network. This leads\nto some more complex and dynamical architectures to store values in \u201cworking memory\u201d, versus\nthe \"long term memory\", which corresponds to the disk memory. Working memory in silico corre-\nsponds more closely to RAM in that a consistent voltage must be applied, although this memory is\nstable and unchanging, whereas dynamical attractors in the brain are constantly in flux. Regardless,\nin connectionist networks, this memory must be stored as a state of the network, or an abstract vector\nthat updates in time as the network progresses through inputs. As the brain does not actually have\nthis vector which itself would be stored in disk, the state of the brain is really the current snapshot of\nthe voltages and accumulation of neurotransmitters for each neuron terminal and body. To read the\nstate, the brain has only access to the action potentials or \u201cspikes\" that are emitted to sent informa-\ntion between neurons (barring any unforseen tricks by the glia). Thus, to truly implement this state\nin a neuromorphic way, the value of the state is simply the output activity being sent back to itself,\ncreating a \"self loop\". While in code it is possible to maintain in the disk the states of the \"trans-\nmitters\" of the neurons, this idea of recurrent connections is still critical as loops provide recurrent\ncomputation and allow for the memory to reverberate throughout the network. Thus, the network\npersists its state (or memory) of the temporal signal by reverberating or bouncing a reflection of the\ninput inside itself to maintain a persistent activity.\""}, {"title": "3.2 HOPFIELD NETWORKS", "content": "A result of Donald Hebb's seminal work on neural population and organization (Hebb, 2005), Hop-\nfield derived a network that views the network as an energy minimization problem, where the ac-\ntivity that bounces around and settles into attractor states, and the learned patterns correspond to\nenergy minima in this landscape. It has been shown that using the Hebbian learning rule mathe-\nmatically equates to finding, for a given data sample, the energy minimum attractor state associated\nwith that network architecture. This has the benefit of not needing labeled samples, is completely\nself-organizing, and is biologically relevant. The issues hampering it have been a lack of ability to\nextract latent codes - features - from large amount of samples, and the somewhat strict limits on\nmemory that result from this. However, new work has shown that via gating mechanisms (Hochre-\niter & Schmidhuber, 1997; Davis et al., 2022) the memory can be increased. The gradient-based\nLSTMs also suffer the same problem as in the earlier section about the weights not being sensitive\nto the particular timestep. While in their basic form Hopfield networks have failed to match deep\nlearning, newer architectures that incorporate continuous values and attention show correspondence\nbetween Hopfield learning and attention layers in Transformers, and are a promising direction for\nfuture RNN research (Ramsauer et al., 2020)."}, {"title": "3.3 GRADIENT-BASED RNNS", "content": "A simple approach to training a recurrently connected network is to treat it like a feed-forward\nnetwork, and treat the time dimension as if the network were actually multiple layers deep. For\na single-layered RNN, the network is run in time for a set number of t timesteps, and then the\nnetwork is \"unrolled\u201d to have t layers corresponding to the t timesteps. The gradient is learned and\nthen the updates are all applied to the same weight vector. This training mechanism is known as\n\"Backpropagation Through Time\u201d (BPTT), as the network through time is unrolled and treated as\na deep neural network. It is generally accepted that BPTT is highly biologically unrealistic as the"}, {"title": "3.4 RESERVOIR COMPUTING", "content": "As a response to the requirement of stable attractor states from Hopfield Networks, Reservoir Com-\nputing (Jaeger, 2001; Maass, 2011) was developed to convert the problem from an attractor-based\none to a mapping based one. Critically, it converts the problem of having the network settle, to hav-\ning the network \u201cobserve\u201d the dynamical state of the recurrent state (now called the reservoir states\nbut effectively the same as a hidden state of an RNN). The insight is that we need not propagate\nthe error through the recurrent component. The component will produce some activity as a result of\nits initialization, and if that initialization is random, or is a sufficient basis set to cover the possible\nfeatures, a sufficiently powerful readout is capable of mapping that activity to any predictive value.\nHere, it is also important to note that we need not learn, in the recurrent component, what the mem-\nory activity corresponds to; this is the task of the readout mechanism. The recurrent component\nsimply is in charge of bouncing and persisting activity, just long enough for the readout to make\na determination. This new paradigm fundamentally shifts around the requirements of a memory\nmechanism, and makes an important founding that we use for this work as well: that the memory\nand predicitve processing components can potentially be split into two separate processes. While\nthe memory necessarily must be tuned to help in the service of prediction as in (Glenberg, 1997),\nit is really the combination of this readout mechanism and the memory that constitutes the effects\nof memory. We take this idea of separating the memory and computation components, along with\nthe idea that we need not propagate the gradients through the recurrent component, as the job of the\n\"observer\" is simply to perform a mapping.\n\nOne of the key insights of reservoir computing that will be extracted is that the memory unit is\ntopologically separated, unhooked, from the gradient learning processs of the readout, which is the\nnetwork assigned to do the functional mapping from states to actions (labels, or values). We take\nthis core idea from reservoir computing, and expand it outward in the Maelstrom paradigm into a\nmuch larger proposed theoretical structure, without the limitations placed by reservoir computing."}, {"title": "4 MAELSTROM NETWORKS", "content": "To solve the issue of sequence memory and to endow a neural network with its capabilities, we\npresent here Maelstrom Networks. From control theory, we take the idea of a state space model that\nincorporates a memory state, an input function, and an output function. From reservoir computing\nwe incorporate the notion that the temporal memory must be a process topologically separated from\nthe readout mechanism, in terms of learning via the gradients. And we use feed-forward networks\nfor their preferred role: to map inputs to outputs, and not to learn how to store things in memory.\nIn Fig. 4 we show the schematic overview of the proposed approach. From an implementation\nstandpoint, the input and output functions are easily parameterized by deep neural networks - blocks\nwere shown here for visual simplicity. The key element which gives the network its name is the\nMaelstrom, a recurrently connected component which can be implemented as a reservoir - but not\nexclusive to, that takes input from the input function (an input neural network), bounces this\nactivity around, and passes this to an output function. we say \"passes to\", but in reality what is\nhappening is, it is more akin to the output function reading out the maelstrom; you one can visualize"}, {"title": "5 ADVANTAGES OF MAELSTROM NETWORKS", "content": "As the Maelstrom network paradigm is inspired by - and can be seen as an evolution of - the reser-\nvoir computing paradigm, it inherits many of the benefits. It also collects new advantages, as it\nincorporates deep learning into the mixture in ways that reservoir computing had not.\n\nPROVIDES A MODEL FOR THEORETICAL AND SYSTEMS NEUROSCIENCE\nAs the Maelstorm paradigm was inspired by reverse-engineering the brain, it is possible to create a\n\"two-way\u201d highway between this proposed model and the brain function, where the brain informs\nAl research, and the AI research informs neuroscience. Figure 5 demonstrates our general view\nof the brain's interactions, and how this can map to the maelstrom paradigm. Each module of the\nMaelstrom Network maps either to a single area of the brain, or a group of areas. This modular\nstructure of the brain, also echoed in (Hadsell et al., 2020), is essential to performing the complex\ntasks, in a way that does not overwrite weights, which occurs when we train networks using the\nI.I.D assumption on temporal data. The goal here is to create a system that both advances the field\nof A.I. and advances the field of neuroscience simultaneously. In addition, we propose that it is\nthrough these multiple-hierarchy loops at varying scales, i.e. within the maelstrom and then within\nthe feedback loop from the maelstrom and the controller, which provides the \"state\" that serves as\nthe continual sense of self that agents phenomenologically percieve. This could potentially link\nsystems neuroscience structures with concepts of consciousness.\n\nENCAPSULATES THE EMBODIED NOTION OF MEMORY\nIt is clear that while computer memory relies on memory as an abstract storage, memory in the brain\nlikely evolved for a specific goal, which was to enable the survival of the agent. This means that\nmemory serves at the behest of embodiment, and not simply as an abstract bin that any information\ncan be stored into, as argued in Glenberg (1997). This is also reflected here in the Maelstrom\nparadigm, in the combination of the memory and the interface. The maelstrom's memory itself\nis a meaningless string of information, that only has meaning with respect to a given readout - the\nreadout which maps the information to a specific prediction, or task. This is one potential solution to\nthe issue of grounding of the meaning of activity - while the activity itself is potentially meaningless,\nit is given embodied meaning by virtue of it being tied to a specific readout. This is also the view\nespoused in the Neural Assembly theory of Buzs\u00e1ki (2010).\n\nENABLES POWERFUL TRAINING OF FEED-FORWARD NETWORKS WHILE COMBINING MEMORY\nIt is abundantly clear that feed-forward neural networks work best with backpropagation, as Trans-\nformers (Vaswani et al., 2017) have completely overshadowed recurrent neural networks for sequen-\ntial processing, even though Transformers contain no memory themselves in their vanilla form (see"}, {"title": "6 CONCLUSION", "content": "In this work, we propose Maelstrom Networks, a novel modular neural network architecture that un-\nhooks a sequential memory, representing the prefrontal cortex and hippocampus, from the gradient\npass of two feed forward networks used to control the memory. The input network acts as the sen-\nsory cortex to the system, inputting the correct features while maintaining balance of the maelstrom,\nand the readout takes outputs from the maelstrom and applies them to an action or task, similar to\nthe motor cortex. We propose that this work is the natural evolution of work done in the early 1960s,\nwhich laid out a larger modular structure for the nervous system but left out the critical issue of se-\nquence memory. We connect this work to control theory and note that the maelstrom is a nonlinear\nstate space model, trained using the MIT control rule; this also relates the brain to notions of control\nas well, as we map each region of the brain to components of the maelstrom as well. It is our hope\nthis helps usher in a new era of neural networks that specifically focus on sequence memory, not as\nan engineering trick to aid in performance, but as a fundamental property of networks."}]}