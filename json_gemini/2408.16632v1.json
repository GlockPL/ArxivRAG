{"title": "MAELSTROM NETWORKS", "authors": ["Matthew S Evanusa", "Yiannis Aloimonos", "Cornelia Ferm\u00fcller"], "abstract": "Artificial Neural Networks has struggled to devise a way to incorporate working\nmemory into neural networks. While the \"long term\" memory can be seen as the\nlearned weights, the working memory consists likely more of dynamical activity,\nthat is missing from feed-forward models. This leads to a weakness in current neu-\nral network models: they cannot actually process temporal data in time, without\naccess to some kind of working memory. Current state of the art models such as\ntransformers tend to \u201csolve\u201d this by ignoring working memory entirely and simply\nprocess the sequence as an entire piece of data; however this means the network\ncannot process the sequence in an online fashion, and leads to an immense explo-\nsion in memory requirements. In the decades prior, a separate track of research\nhas followed recurrent neural networks that maintain a working memory via a dy-\nnamic state, although training these weights has proven difficult. Here, inspired\nby a combination of controls, reservoir computing, deep learning, and recurrent\nneural networks, we offer an alternative paradigm that combines the strength of\nrecurrent networks, with the pattern matching capability of feed-forward neural\nnetworks, which we call the Maelstrom Networks paradigm. This paradigm leaves\nthe recurrent component - the Maelstrom - unlearned, and offloads the learning to\na powerful feed-forward network. This allows the network to leverage the strength\nof feed-forward training without unrolling the network, and allows for the memory\nto be implemented in new neuromorphic hardware. It endows a neural network\nwith a sequential memory that takes advantage of the inductive bias that data is\norganized causally in the temporal domain, and imbues the network with a state\nthat represents the agent's \"self\u201d, moving through the environment. This could\nalso lead the way to continual learning, with the network modularized and \u201cpro-\ntected\" from overwrites that come with new data. In addition to aiding in solving\nthese performance problems that plague current non-temporal deep networks, this\nalso could finally lead towards endowing artificial networks with a sense of \"self\".", "sections": [{"title": "INTRODUCTION", "content": "The ultimate goal of artificial intelligence is to recreate the intelligence that biological agents have\ndisplayed to remarkable degree, extract the core components of intelligence, and reproduce this\nin an artificial agent, potentially amplifying this. Since the early days of connectionist networks,\nwhere we attempt to solve this intelligence problem through networks (or graphs) of simple artifi-\ncial neural units, the goal has somewhat drifted away from a general artificial agent towards more\nspecialized tasks, namely, pattern recognition. Feed-forward neural networks, built off of the Per-\nceptron framework (Rosenblatt, 1961), have excelled at pattern recognition, and have reoriented the\nentire field towards this mapping function. This has culimated in the current generation of Large\nLanguage Models, which are at the core, meta-networks of Perceptrons trained using backpropaga-\ntion (Vaswani et al., 2017). What have we sacrificed in this focus on pattern recognition? This is the\nquestion that we motivate this work with. We have, we argue, sacrificed the notion of the agent as a\nstate, a sense of self, that persists across time, and the temporal correlations that accompany this."}, {"title": "DATA IS TEMPORALLY ORGANIZED", "content": "At the core of this issue is the current foundational viewpoint for deep learning that data in the\nuniverse is, in some sense, I.I.D, and has no underlying structure; it is up to the network to learn\nthis structure from random noise. Creatures, however, evolved in the real world, with real physical\nlimitations on the way that the system evolved to deal with, and survive in, the real world. One of\nthe set conditions that nature gives us, in addition to the 3-dimensional structure of the world, is the\ntemporal nature of cause and effect; data later in time are temporally correlated with the earlier data.\nFigure 1 gives a graphical depiction of this dichotomy. The data that is aligned temporo-causally\nalong the same strands we refer to as temporal sequences. And the memory mechanisms in agents\nthat is responsible for remembering points along the same strand we refer to as Sequence Memory.\nThe viewpoint of feed-forward connectionist networks, which is the current paradigm for deep learn-\ning, is that data must be assumed to be independently distributed - the I.I.D prior. We assume that\nthe data is \"given\" to us in a completely random, jumbled state, with no inductive prior on the\nstructure of the data, and it is up to the network to learn the structure of the data. This is what we\nexpect from neural networks - the only inductive prior that we assume is that the data is structured\nhierarchically, which is what leads to our \u201cdeep\u201d learning structures of multi-layered networks. We\ndo not, however, flip this 90 degrees, and think about the structure of the data in time. The data\nin the real world, however, is structured: it follows clear causal relations between the data in time,\nwhich we can visualize as \"strands\" in time. Each of the data that is causally linked sits on the same\n\"strand\", which may branch off from one another. Figure 1 shows these two varying viewpoints\nof data in the world. The story of deep learning has shown that the structure of the network topol-\nogy is paramount: the learning rule (backpropagation) as well as the activation of the neurons has\nlargely remained unchanged in the decades since deep learning came about, only the topology of\nthe network, the structuring of the linear layers into convolution or attention layers, has changed.\nThis insight demonstrates that to effectively tackle networks in the real world, inductive biases must\nbe given for the temporal domain and not just the spatial. This inductive bias takes the form of\nSequential Memory: the ability of the agent to remember data that is causally linked accoring to the\nsame temporal sequence strand. This bias is implemented using modules of resonating, recurrently\nconnected graphs of neurons which we call the Maelstrom."}, {"title": "BRAINS HAVE STATE VIA RESONATING CELL ASSEMBLIES", "content": "How do creatures persist across time, and remember the memory of temporal sequences as they\noccur? Of course, statistical pattern recognition is a key component, but another component must\nbe the system's ability to retain the information such that a pattern \"readout\" can occur. This idea\nthat the brain's mechanism for recognition is a combination of cell assemblies, which maintain\nactivity, and a readout mechanism, which performs a mapping of activity to motor action and tasks,\nis well documented and is a current theory for brain organization (Buzs\u00e1ki, 2010). According to\nthe theories of Neural Assemblies by Donald Hebb (Hebb, 2005), the activity persists in groups of\nneurons, called assemblies, that resonate their activity across time according to some stimulus. It is\nthis resonance, we argue, is the \"stable state\" that allows living creatures to persist across time. A\nseparate readout mechanism, then, performs mapping of these states, akin to a function learning a\nmapping of states of a dynamical system to outputs."}, {"title": "CELL ASSEMBLIES ARE A TEMPORAL INDUCTIVE BIAS", "content": "The key insight for the brain is that these assemblies model the temporal sequences by mirroring\nthe cause-effect relationship of the input data, but in the recurrent patterns of the network. Thus,\nthe same temporal patterns that occur in the real world, are \"echoed\u201d in the temporal patterns of\nthe recurrently-linked cell assemblies. This topographical structure of a recurrently connected graph\nstructure that is modular and unhooked from the gradients of the sensory and motor areas are the\ntemporal inductive bias in the brain, and in our proposed Maelstrom network, that allows it to main-\ntain a state and capture sequence memory. Whereas the spatial inductive bias is that the hierarchical\nnature of the features in the data are represented as feed-forward layers of the network, the temporal\ninductive bias is that the looped or cyclical structure of the recurrent components captures \u201creso-\nnances\" which mirror the temporal-causal relationship of the data. As the topology of the network\nin the brain is fixed, the specific resonances that occur will occur with the same temporo-causal\npatterns that occured in the real world, except that it collects cause-effect from across time."}, {"title": "LIMITATIONS OF CURRENT APPROACHES", "content": "Although deep neural networks have shown remarkable performance, their success masks some key\nmissing elements if we are to achieve a thinking machine. Although some posit that Large Language\nModels and this is an open debate in the literature, the current pinnacle of feed-forward networks,\nhave \"reasoning\" capabilities, we would argue here that they rather are simply powerful pattern\nmappers, and that many tasks we acribed to reasoning in the past are actually just complex mapping\ntasks. This is a longer debate outside of the scope of this paper, but for the purposes of this work, we\nargue they cannot be true reasoners in the real world as they are not embodied. And to be embodied,\nthey are missing one critical components: namely a memory of the past activations via a state."}, {"title": "FEED-FORWARD NETWORKS LACK SEQUENCE MEMORY", "content": "Memory, as a concept, is tricky to pin down, although there have been some interesting definitions\nof memory that bridge the biological with the computational (Zlotnik & Vansintjan, 2019). Here,\nwe take memory in the context of connectionist neural networks to mean: a mechanism by which the\nneural network maintains a state of itself, and contains information about the past from a temporal\nsequence. This is what we refer to as described earlier, as Sequence Memory. The idea of sequence\nmemory is intricately tied to the notion of time: to process time, an agent must record, in some\nstate variable, some information of timesteps prior. As articulated in earlier sections, the idea of a\nrecurrent network, unhooked from the gradient, gives a temporal inductive bias to the processing of\nthe data. This accompanies with it the important notion that all data we receieve, as active agents\n(Friston et al., 2016; Aloimonos et al., 1988), is in the temporal domain. In our own brains, we do\nnot have access to a \"RAM-like\" memory that computer systems have (as our entire system is the\nconnectionist neural network), the neural network itself must keep a memory of its own activations,\nto serve as the state variable for temporal processing. This \"state\" of the neural network is then\nupdated with new information, in a recursive fashion. This kind of architecture is mirrored in Re-\ncurrent Neural Networks (RNNs), as described below, and serves as an important foundation for our\nproposed ideas here.\nOf course, incorporating a \"state\" that updates with the model introduces a host of challenges. It\nis not hard to see why the current state of the art neural networks, built on Transformers (Vaswani\net al., 2017) which then became Large Language Models (Radford et al., 2019), for the most part\neschewed the idea of recurrence (although some hybrid work has arose in recent years attempting to\nfuse them (Hutchins et al., 2022))."}, {"title": "\u039c\u0395\u039cORY SERVES IN THE SERVICE OF EMBODIMENT", "content": "To be an artificial embodied intelligence (Chrisley, 2003; Duan et al., 2022) entails placing the\nlearning system in a chassis that can take actions in an environment. This is an active process\n(Aloimonos et al., 1988; Friston et al., 2016), with the agent contained in a control loop within the\nenvironment. And to realize these actions in an environment which is moving in time, the agent\nmust have access to a running state of the previous activity. To not have this state would amount to\nthe agent running with \u201cblindfolds\" on around their perception completely, only seeing the current\ntimestep and making a decision, then moving onto the next sensory perception. This realization\namounts to reducing agents to mere pattern recognizers - thus the idea of having a memory in a\nstrong sense elevates agents above merely pattern matching entities. It has also been argued that\nthe entire purpose of memory, rather than serving as some RAM-like appendage that stores abstract\nvalues for later use, completely serves embodiment (Glenberg, 1997).\""}, {"title": "PRIOR WORK ON CONNECTIONIST MEMORY", "content": ""}, {"title": "MEMORY IN CONNECTIONIST NETWORKS", "content": "As mentioned earlier, the way that researchers think about neural networks in silico, versus how\nneural networks work in vivo, is fundamentally different not only from the algorithmic perspective,\nbut also with respect to how memory is stored. In computers, we simply store our neural network\ncode in RAM or disk storage, which is a collection of buckets that can store any arbitrary numerical\nvalues (without respect to any task). All of the functions that require remembering (such as loading\ncode, the weights, the dataset) are stored in this disk memory. In contrast, in the brain there is no\narbitrary storage separate from the neural network - the storage is the neural network. This leads\nto some more complex and dynamical architectures to store values in \u201cworking memory\u201d, versus\nthe \"long term memory\", which corresponds to the disk memory. Working memory in silico corre-\nsponds more closely to RAM in that a consistent voltage must be applied, although this memory is\nstable and unchanging, whereas dynamical attractors in the brain are constantly in flux. Regardless,\nin connectionist networks, this memory must be stored as a state of the network, or an abstract vector\nthat updates in time as the network progresses through inputs. As the brain does not actually have\nthis vector which itself would be stored in disk, the state of the brain is really the current snapshot of\nthe voltages and accumulation of neurotransmitters for each neuron terminal and body. To read the\nstate, the brain has only access to the action potentials or \u201cspikes\" that are emitted to sent informa-\ntion between neurons (barring any unforseen tricks by the glia). Thus, to truly implement this state\nin a neuromorphic way, the value of the state is simply the output activity being sent back to itself,\ncreating a \"self loop\". While in code it is possible to maintain in the disk the states of the \"trans-\nmitters\" of the neurons, this idea of recurrent connections is still critical as loops provide recurrent\ncomputation and allow for the memory to reverberate throughout the network. Thus, the network\npersists its state (or memory) of the temporal signal by reverberating or bouncing a reflection of the\ninput inside itself to maintain a persistent activity.\""}, {"title": "HOPFIELD NETWORKS", "content": "A result of Donald Hebb's seminal work on neural population and organization (Hebb, 2005), Hop-\nfield derived a network that views the network as an energy minimization problem, where the ac-\ntivity that bounces around and settles into attractor states, and the learned patterns correspond to\nenergy minima in this landscape. It has been shown that using the Hebbian learning rule mathe-\nmatically equates to finding, for a given data sample, the energy minimum attractor state associated\nwith that network architecture. This has the benefit of not needing labeled samples, is completely\nself-organizing, and is biologically relevant. The issues hampering it have been a lack of ability to\nextract latent codes - features - from large amount of samples, and the somewhat strict limits on\nmemory that result from this. However, new work has shown that via gating mechanisms (Hochre-\niter & Schmidhuber, 1997; Davis et al., 2022) the memory can be increased. The gradient-based\nLSTMs also suffer the same problem as in the earlier section about the weights not being sensitive\nto the particular timestep. While in their basic form Hopfield networks have failed to match deep\nlearning, newer architectures that incorporate continuous values and attention show correspondence\nbetween Hopfield learning and attention layers in Transformers, and are a promising direction for\nfuture RNN research (Ramsauer et al., 2020)."}, {"title": "GRADIENT-BASED RNNS", "content": "A simple approach to training a recurrently connected network is to treat it like a feed-forward\nnetwork, and treat the time dimension as if the network were actually multiple layers deep. For\na single-layered RNN, the network is run in time for a set number of t timesteps, and then the\nnetwork is \"unrolled\u201d to have t layers corresponding to the t timesteps. The gradient is learned and\nthen the updates are all applied to the same weight vector. This training mechanism is known as\n\"Backpropagation Through Time\u201d (BPTT), as the network through time is unrolled and treated as\na deep neural network. It is generally accepted that BPTT is highly biologically unrealistic as the\nbrain cannot unroll itself, but several approximations have been developed in recent years that try to\nargue for a mechanism in the brain that produces similar effects (Cheng & Brown, 2023; Manneschi\n& Vasilaki, 2020).\nThe issue, still, is that the memory and the feature vector are not truly topologically separated:\nthe \u201cmemory\u201d vector (represented ostensibly by the cell state) is still trained and driven by the\nsame signal as the error for the feature vector. The only difference is that the cell state 'prefers' to\nstay unchanged for longer periods, allowing for long-term dependency learning; this is not a true\nphilosophical \u201cseparation\u201d of the memory and the feature vector. Thus, it is not surprising that the\nGated Recurrent Unit or GRU (Cho et al., 2014), which fuses the cell state and the hidden state\na process that if the memory were truly separated and the system depended on it, would destroy\nthe function of the system - actually tends to perform the same or sometimes even better, with less\noverhead, and is the preferred RNN of choice in modern times when RNNs are used. Of course,\nRNN usage has been in recent times completely eclipsed by feed-forward networks - in particular\nTransformer models Vaswani et al. (2017)."}, {"title": "RESERVOIR COMPUTING", "content": "As a response to the requirement of stable attractor states from Hopfield Networks, Reservoir Com-\nputing (Jaeger, 2001; Maass, 2011) was developed to convert the problem from an attractor-based\none to a mapping based one. Critically, it converts the problem of having the network settle, to hav-\ning the network \u201cobserve\u201d the dynamical state of the recurrent state (now called the reservoir states\nbut effectively the same as a hidden state of an RNN). The insight is that we need not propagate\nthe error through the recurrent component. The component will produce some activity as a result of\nits initialization, and if that initialization is random, or is a sufficient basis set to cover the possible\nfeatures, a sufficiently powerful readout is capable of mapping that activity to any predictive value.\nHere, it is also important to note that we need not learn, in the recurrent component, what the mem-\nory activity corresponds to; this is the task of the readout mechanism. The recurrent component\nsimply is in charge of bouncing and persisting activity, just long enough for the readout to make\na determination. This new paradigm fundamentally shifts around the requirements of a memory\nmechanism, and makes an important founding that we use for this work as well: that the memory\nand predicitve processing components can potentially be split into two separate processes. While\nthe memory necessarily must be tuned to help in the service of prediction as in (Glenberg, 1997),\nit is really the combination of this readout mechanism and the memory that constitutes the effects\nof memory. We take this idea of separating the memory and computation components, along with\nthe idea that we need not propagate the gradients through the recurrent component, as the job of the\n\"observer\" is simply to perform a mapping.\nOne of the key insights of reservoir computing that will be extracted is that the memory unit is\ntopologically separated, unhooked, from the gradient learning processs of the readout, which is the\nnetwork assigned to do the functional mapping from states to actions (labels, or values). We take\nthis core idea from reservoir computing, and expand it outward in the Maelstrom paradigm into a\nmuch larger proposed theoretical structure, without the limitations placed by reservoir computing."}, {"title": "MAELSTROM NETWORKS", "content": "To solve the issue of sequence memory and to endow a neural network with its capabilities, we\npresent here Maelstrom Networks. From control theory, we take the idea of a state space model that\nincorporates a memory state, an input function, and an output function. From reservoir computing\nwe incorporate the notion that the temporal memory must be a process topologically separated from\nthe readout mechanism, in terms of learning via the gradients. And we use feed-forward networks\nfor their preferred role: to map inputs to outputs, and not to learn how to store things in memory.\nIn Fig. 4 we show the schematic overview of the proposed approach. From an implementation\nstandpoint, the input and output functions are easily parameterized by deep neural networks - blocks\nwere shown here for visual simplicity. The key element which gives the network its name is the\nMaelstrom, a recurrently connected component which can be implemented as a reservoir - but not\nexclusive to, that takes input from the input function (an input neural network), bounces this\nactivity around, and passes this to an output function. we say \"passes to\", but in reality what is\nhappening is, it is more akin to the output function reading out the maelstrom; you one can visualize\nthis as the output function being a pair of eyes that observe the chaotic activity of the maelstrom.\nBecause the network is untrained and recurrently connected, it is a potentially chaotic system - a\nstorm that gives the maelstrom its name. It is the job of the input to \"control\" the activity of this\nstorm and keep it within acceptable bounds.\nCritically as well, the maelstrom possesses \u201ctop-down\u201d feedback onto the input function, which\nallows the feed-forward input function a control loop over its own activity. This \"closing the loop\"\non the malestrom is what gives a meta-loop between the maelstrom and the network structure as a\nwhole. These feedback connections are ubiquitous in the primate cortex (Zagha, 2020) and it is this\nloop that feeds back into the input which, in addition to the loop within the maelstrom, creates the"}, {"title": "ADVANTAGES OF MAELSTROM NETWORKS", "content": "As the Maelstrom network paradigm is inspired by - and can be seen as an evolution of - the reser-\nvoir computing paradigm, it inherits many of the benefits. It also collects new advantages, as it\nincorporates deep learning into the mixture in ways that reservoir computing had not."}, {"title": "PROVIDES A MODEL FOR THEORETICAL AND SYSTEMS NEUROSCIENCE", "content": "As the Maelstorm paradigm was inspired by reverse-engineering the brain, it is possible to create a\n\"two-way\u201d highway between this proposed model and the brain function, where the brain informs\nAl research, and the AI research informs neuroscience. Figure 5 demonstrates our general view\nof the brain's interactions, and how this can map to the maelstrom paradigm. Each module of the\nMaelstrom Network maps either to a single area of the brain, or a group of areas. This modular\nstructure of the brain, also echoed in (Hadsell et al., 2020), is essential to performing the complex\ntasks, in a way that does not overwrite weights, which occurs when we train networks using the\nI.I.D assumption on temporal data. The goal here is to create a system that both advances the field\nof A.I. and advances the field of neuroscience simultaneously. In addition, we propose that it is\nthrough these multiple-hierarchy loops at varying scales, i.e. within the maelstrom and then within\nthe feedback loop from the maelstrom and the controller, which provides the \"state\" that serves as\nthe continual sense of self that agents phenomenologically percieve. This could potentially link\nsystems neuroscience structures with concepts of consciousness."}, {"title": "ENCAPSULATES THE EMBODIED NOTION OF MEMORY", "content": "It is clear that while computer memory relies on memory as an abstract storage, memory in the brain\nlikely evolved for a specific goal, which was to enable the survival of the agent. This means that\nmemory serves at the behest of embodiment, and not simply as an abstract bin that any information\ncan be stored into, as argued in Glenberg (1997). This is also reflected here in the Maelstrom\nparadigm, in the combination of the memory and the interface. The maelstrom's memory itself\nis a meaningless string of information, that only has meaning with respect to a given readout - the\nreadout which maps the information to a specific prediction, or task. This is one potential solution to\nthe issue of grounding of the meaning of activity - while the activity itself is potentially meaningless,\nit is given embodied meaning by virtue of it being tied to a specific readout. This is also the view\nespoused in the Neural Assembly theory of Buzs\u00e1ki (2010)."}, {"title": "ENABLES POWERFUL TRAINING OF FEED-FORWARD NETWORKS WHILE COMBINING MEMORY", "content": "It is abundantly clear that feed-forward neural networks work best with backpropagation, as Trans-\nformers (Vaswani et al., 2017) have completely overshadowed recurrent neural networks for sequen-\ntial processing, even though Transformers contain no memory themselves in their vanilla form (see\nDai et al. (2019) for a version with a cached memory). While it is true that the Maelstrom removes\nthe ability to attend to far-back sequence elements, this is necessarily what must happen if we want\nto process sequences in real-time, as described below - the only way to attend a weight for each past\nelement is to give the network access to the past elements, which would in turn reduce the model\nback to a Transformer. In fact, combining a recurrent memory with attention actually predates\nTransformers themselves, as in (Chorowski et al., 2015), and the main innovations of the Trans-\nformer were simply removing the LSTM (Hochreiter & Schmidhuber, 1997) component (hence the"}, {"title": "EASILY INCORPORATES INTO EXISTING FEED-FORWARD NETWORK STRUCTURES", "content": "Rrecent work as in (Hutchins et al., 2022; Wu et al., 2020) have begun to investigate using cached\nmemory representations to alleviate the computational burden that Transformers have when dealing\nwith long sequences. Howeveer, these architectures requires highly specific structures for the mem-\nory to incorporate them in, and some are highly tuned towards language models and not general\ntemporal data. The Maelstrom paradigm, in constrast, requires no strict guidance on what the mem-\nory must look like. The key insight is that the memory representation passing through the maelstrom\nis fed into the readout, in such a way that to the readout, it appears to be input stimulus. This reflects\na nested hierarchical view that Friston et al. (2016) touches on; this notion that to sub-regions of the\nnetwork, input coming in appears the same as the stimlulus does to the input layers of the network.\nThis general paradigm, we believe, applies to all temporal data."}, {"title": "ALLOWS FOR REAL-TIME LEARNING AND INFERENCE OF TEMPORAL DATA", "content": "One of the major strengths of Transformers, their learning ability through feed-forward networks,\nis also one of their greatest weaknesses, as they must take the entire length of the sequence to\nprocess the sequence. While it is possible to train a Transformer to map all possible sub-sequences\nin a feed-forward manner, this becomes computationally infeasible very quickly, as the number of\nsub-sequences grows as $N^2$ with the number of sequences.\nAdding this Maelstrom, however, that is completely unhooked from the gradient learning, turns the\nproblem of learning sequences into a controls problem, where the input at each timestep is simply\na feed-forward learning of the current memory state vector, and its mapping to a prediction. This\nmeans that the entire length of the sequence is no longer needed, which means that the sequence can\nbe learned in an online fashion, and that inference can be performed in an on-line fashion. This is\ncritical for any use-case where neural networks need to be running in time in the real world - which,\nby most measures, would include most real-world applications."}, {"title": "ALLOWS FOR NEUROMORPHIC HARDWARE IMPLEMENTATIONS", "content": "Lastly, as the maelstrom does not require gradients to backpropagate through the recurrent compo-\nnent, it is extremely amenable to newer neuromorphic and bio-inspired hardware implementatoins,\nsuch as through the Intel Loihi (Evanusa et al., 2019), or Memristor technologies (Thomas, 2013), or\neven FPGA implementations. These technologies eschew with the traditional CPU chip processors\nand instead solely work by modeling just the neuron activations. This reduction of the function of\nthe chip severely limits the kind of processing it can do (for normal computer programs), but super-\ncharges the use-cases when it runs neural networks. As the memristor and loihi chips in particular\nhave specialized hardware, performing Backpropagation Through Time or other complex temporal\noperations are burdensome; running a maelstrom, however, that requires no internal updating and\nsimply runs as an autonomous dynamical system, is much more doable. The fact that the Maelstrom\nis disjoint from the feed-forward component means that the feed-forward component can be im-\nplemented separately in GPU hardware, and read from the neuromorphic Maelstrom when needed.\nThis would fully utilize the power saving capabilities of neurmorphic hardware (as the recurrent\ncomponent is computationally expensive with BPTT), in a time when it is becoming abundantly\nclear that the energy usage for current deep learning is unsustainable."}, {"title": "ALLOWS FOR ONLINE OR CONTINUAL LEARNING", "content": "As the sequence memory, encapsulated in the maelstrom, is unhooked from the gradient learning\nprocess, this can allow future instantiations to deal with the continual learning problem. The con-\ntinual learning problem is defined as the ability of a neural network to train indefinitely while it\ncontinues to perform testing inference. This is impossible with current deep learning networks due\nto the fact that the gradients from new samples completely overwrite the old weights with no regard\nto their importance. For example, a network trained on five image classes, but then continued to train\non 5 new classes without access to the old classes, will completely forget the first data. It has been\nproposed in (Hadsell et al., 2020) that a modular architecture, for example the Maelstrom paradigm,\ncan aid in the continual learning problem, by unhooking the gradients, and in addition by removing\nthe I.I.D assumption that underlies the current setup. The unhooked Maelstrom will allow for the\nnetwork to not overfit to a given task along the sequence, while also not overwriting later sequence\nelements."}, {"title": "CONCLUSION", "content": "In this work, we propose Maelstrom Networks, a novel modular neural network architecture that un-\nhooks a sequential memory, representing the prefrontal cortex and hippocampus, from the gradient\npass of two feed forward networks used to control the memory. The input network acts as the sen-\nsory cortex to the system, inputting the correct features while maintaining balance of the maelstrom,\nand the readout takes outputs from the maelstrom and applies them to an action or task, similar to\nthe motor cortex. We propose that this work is the natural evolution of work done in the early 1960s,\nwhich laid out a larger modular structure for the nervous system but left out the critical issue of se-\nquence memory. We connect this work to control theory and note that the maelstrom is a nonlinear\nstate space model, trained using the MIT control rule; this also relates the brain to notions of control\nas well, as we map each region of the brain to components of the maelstrom as well. It is our hope\nthis helps usher in a new era of neural networks that specifically focus on sequence memory, not as\nan engineering trick to aid in performance, but as a fundamental property of networks."}]}