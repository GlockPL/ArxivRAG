{"title": "Verification methods for international AI agreements", "authors": ["Akash R. Wasil", "Tom Reed", "Jack William Miller", "Peter Barnett"], "abstract": "What techniques can be used to verify compliance with international agreements about advanced AI development? In this paper, we examine 10 verification methods that could detect two types of potential violations: unauthorized AI training (e.g., training runs above a certain FLOP threshold) and unauthorized data centers. We divide the verification methods into three categories: (a) national technical means (methods requiring minimal or no access from suspected non-compliant nations), (b) access-dependent methods (methods that require approval from the nation suspected of unauthorized activities), and (c) hardware-dependent methods (methods that re- quire rules around advanced hardware). For each verification method, we provide a description, historical precedents, and possible evasion techniques. We conclude by offering recommendations for future work related to the verification and enforcement of international AI governance agreements.", "sections": [{"title": "Executive Summary", "content": "Efforts to maximize the benefits and minimize the global security risks of advanced AI may lead to international agreements. This paper outlines methods that could be used to verify compliance with such agreements. The verifi- cation methods we cover are focused on detecting two potential violations:\nViolations to verify\n\u2022 Unauthorized AI development (for example, AI development that goes beyond a FLOP threshold set by an international agreement, or the execution of a training run that has not received a license).\n\u2022 Unauthorized data centers (for example, data centers that go beyond a maximum computing capacity limit or networking limit set by an international agreement).\nWe identify 10 verification methods and divide them into three categories:\n1. National technical means. Methods that can be used by nations unilaterally.\n2. Access-dependent methods. Methods that require a nation to grant access to national or international inspectors\n3. Hardware-dependent methods. Methods that require agreements pertaining to advanced hardware"}, {"title": "National technical means", "content": "1. Remote sensing: Detect unauthorized data centers and semiconductor manufacturing via visual and ther- mal signatures.\n2. Whistleblowers: Incentivize insiders to report non-compliance.\n3. Energy monitoring: Detect power consumption patterns that suggest the potential presence of large GPU clusters.\n4. Customs data analysis: Track the movement of critical AI hardware and raw materials.\n5. Financial intelligence: Monitor large financial transactions related to AI development."}, {"title": "Access-dependent methods", "content": "1. Data center inspections: Conduct inspections of sites to assess the size of a data center, verify compliance with hardware agreements, and verify compliance with other safety and security agreements.\n2. Semiconductor manufacturing facility inspections: Conduct inspections of sites to determine the quan- tity of chip production and verify that chip production conforms to any agreements around advanced hardware.\n3. AI developer inspections: Conduct inspections of AI development facilities via interviews, document and training transcript audits, and potential code reviews."}, {"title": "Hardware-dependent methods", "content": "1. Chip location tracking: Automatic location tracking of advanced AI chips.\n2. Chip-based reporting: Automatic notification if chips are used for unauthorized purposes."}, {"title": "Limitations and considerations", "content": "The verification methods we propose have some limitations, and there are many complicated national and interna- tional considerations that would influence if and how they are implemented. Some of these include:\n\u2022 Invasiveness: Some methods (especially on-site inspections) may be seen as intrusive and could raise concerns about privacy and sovereignty. Several factors could influence a nation's willingness to accept invasive measures (e.g., the amount of international tension or distrust between nations, the degree to which nations are concerned about risks from advanced AI, the exact types of risks that nations find most concerning.)\n\u2022 Imperfect detection: No single method is foolproof. However, the combination of multiple methods could create a \"Swiss chees\u201d model, where the weaknesses of one method are covered by the strengths of others.\n\u2022 Developmental stage: Some methods (especially the hardware-dependent ones) may require additional R&D. Furthermore, unlike methods that have been used for decades in other areas, the real-world effectiveness of some hardware-dependent methods has not yet been determined."}, {"title": "Future Directions", "content": "Our work provides a foundation for discussions on AI governance verification, but several key areas require further research:\n\u2022 Red-teaming exercises for verification regimes. Future work could examine how adversaries might attempt to circumvent a verification regime, describe potential evasion methods, and develop robust countermeasures to improve the effectiveness of the verification regime.\n\u2022 Design of international AI governance institutions. Future work could examine how international AI gover- nance institutions should be designed, potentially drawing lessons from existing international bodies. Such work could explore questions such as: (a) what specific powers should be granted to the international institution, (b) how the institution should make core decisions, (c) how power is distributed between nations, and (d) how to handle potential violations or instances of non-compliance.\n\u2022 Enforcement strategies. Future work could examine what kinds of responses could be issued if non-compliance is discovered. This includes examining how such responses can be proportionate to the severity of the violation.\n\u2022 Development of tamper-proof and privacy-preserving hardware-enabled verification mechanisms. Fu- ture R&D efforts could improve the effectiveness, feasibility, robustness, or desirability of various hardware- dependent verification methods."}, {"title": "Introduction", "content": "The development of advanced artificial intelligence poses major global security risks. Significant threats include the potential for pervasive surveillance, the development of au- tonomous weapons, and misuse by malicious actors. Some of the most concerning risks stem from loss of control and misalignment (Bengio et al. 2024; Bostrom 2014; Ngo, Chan, and Mindermann 2022). A sufficiently powerful mis- aligned AI system could autonomously act against human interests following an objective function which does not cap- ture human values (Pan, Bhatia, and Steinhardt 2022). There is a great amount of uncertainty around what kinds of safe- guards will be necessary to prevent misalignment (Gabriel 2020). Many experts believe that safeguards may require many years or decades of concerted research effort.\nAl risks are exacerbated by race dynamics - companies are rapidly progressing in the hope of being the first to de- velop artificial superintelligence (Armstrong, Bostrom, and Shulman 2016; Hogarth 2023). In the context of an AI race, nations may not have sufficient time to carefully and cau- tiously develop or evaluate such safeguards.\nInternational agreements could help avoid or mitigate a race between nations. Even though governments are at early stages of understanding AI risks, key figures in the United States and China have already acknowledged concerns about AI global security risks and expressed interest in global gov- ernance approaches (Wasil and Durgin 2024). As govern- ments become more aware of AI risks, they may become in- terested in global governance strategies that curb these race dynamics. Alternatively, nations might agree to cede the de- velopment of advanced AI to a joint international project. The international institution would be responsible for carry- ing out certain forms of advanced AI development, which would be illegal outside the context of this secure joint project (Hogarth 2023).\nInternational agreements require verification. Nations might be much more likely to form international agreements around rules that they can reliably verify (Fearon 1995; Baker 2023). By \u201cverify\u201d, we mean that nations would be able to detect non-compliance with agreements. Ideally, ver- ification methods (methods used to detect non-compliance) would provide early and reliable warning signs. \"Early\", in that non-compliance could be detected relatively quickly (before a nation achieved any meaningful unauthorized ad- vantage in advanced AI development), and \u201creliable\u201d in that non-compliance would be very likely to be detected.\u00b9\nIn this paper, we provide an overview of verification methods for international AI agreements. We begin by out- lining the potential targets such an international agreement. We then outline 10 verification methods. For each verifica- tion method, we provide a description, some precedents for how the verification method has been used in the past, and an example evasion technique to illustrate how an adversary could attempt to circumvent the method. Finally, we discuss"}, {"title": "What to verify: Unauthorized AI development and unauthorized data centers", "content": "An international agreement on AI could take many forms, depending on how the technology and its associated risks evolve. In scenarios where continued AI development leads to substantial acknowledged global security risks, we antic- ipate that verification methods would need to be capable of detecting two primary types of potential violations:\n1. Unauthorized data centers. International governance of AI could plausibly set restrictions on the form, size, quantity, and location of large-scale computing facilities. Verification methods would therefore be needed to detect the construction or operation of data centers that violate these agreed-upon standards.\n2. Unauthorized training runs. An effective international system for governing AI would likely include restric- tions on the scale and characteristics of AI development. Beyond detecting unauthorised data centers, methods to verify that known data centers are compliant with agreed- upon standards would also be necessary. For example, an agreement might stipulate that AI training runs should not exceed a certain FLOP\u00b3 threshold (Heim 2024), use specific types of training data, or employ certain training algorithms. Verification methods would be needed to de- tect whether AI development activities occurring within facilities violate such standards."}, {"title": "Methodology", "content": "Our process for compiling verification methods involved a few steps: (a) reviewing relevant literature on AI verifica- tion and international AI governance, (b) reviewing relevant literature on verification methods for agreements in other fields (e.g., nuclear security, biosecurity, arms control), and (c) conducting informal interviews with experts in technical AI governance. Through this process, we identified 10 verifi- cation methods. For each verification method, we examined its application in other fields to inform our description of how the method could be used in the context of AI disagree- ments and to inform our section about the method's prece- dent in other fields. We also grouped the methods into cat- egories based on the circumstances in which they could be implemented: universally (national technical means), only in cases where a nation provides access (access-dependent), or"}, {"title": "Verification methods", "content": "Defining \"verification method\". In this piece, a verifica- tion method is a method that could directly be used to detect defection or non-compliance from an agreement. That is, we assume an adversarial setup in which one party is explicitly attempting to \u201chide\u201d unauthorized data centers or unauthorized AI training.\nCategorizing verification methods. Some verification methods can be implemented without any buy-in from na- tions suspected of non-compliance, some verification meth- ods require cooperation or authorization from the suspected nation, and some verification methods require cooperation from hardware manufacturers. These distinctions are useful for determining which verification methods might be feasi- ble under various circumstances.\nThus, we divide verification methods into three cate- gories: (a) national technical means (methods that can be implemented without the approval of individual nations)4, (b) access-dependent verification methods (methods that require international agreements that include the suspected nation), and (c) hardware-dependent verification methods (methods that require international agreements that include major hardware manufacturers)."}, {"title": "National Technical Means", "content": "REMOTE SENSING Remote sensing techniques, in- cluding satellite imagery and other forms of aerial obser- vation, can detect potential undeclared data centers using visual, infrared and other electromagnetic signatures. Ad- vanced commercial satellites, which can achieve sub-meter resolutions (Statista 2022), could identify specialized cool- ing units and the movement of computing equipment.\nInfrared imaging is particularly promising for detecting concealed data centers, as GPUs and other computing hard- ware generate significant heat signatures (Yuan et al. 2023) that are difficult to mask. This could reveal large-scale com- puting facilities even when visually concealed.\nRecent advancements in machine learning have further enhanced remote sensing capabilities for verification. Draw- ing from nuclear non-proliferation efforts (Rutkowski and Niemeyer 2020), AI-driven approaches such as supervised and unsupervised classification techniques can be applied to remotely sensed data. These methods could significantly im- prove the identification and monitoring of potential AI de-"}, {"title": "WHISTLEBLOWERS", "content": "Insiders with knowledge of un- declared facilities or operations could provide valuable in- formation not detectable through external means. Potential whistleblowers include employees, contractors, or local res- idents aware of suspicious activities. Governments could in- centivize whistleblowing by:\n1. Establishing robust protection frameworks specifically for AI and technology sectors;\n2. Offering financial incentives for verified information;\n3. Creating secure, anonymous reporting channels;\n4. Providing legal support and job protection;\n5. Developing international cooperation for cross-border whistleblower protection (Loyens and Vandekerckhove 2018).\nPrecedent. The SEC Whistleblower Program, established under the Dodd-Frank Act in 2010, created a system for re- porting securities violations (U.S. Securities and Exchange Commission 2017). It includes strong protections and incen- tives like monetary awards for whistleblowers, who are en- titled to anywhere between 10-30% of the sanctions result- ing from their information (Reuters 2018). For example, in 2016, three whistleblowers revealed Merrill Lynch's misuse of up to $58 billion daily in customer funds, leading to a $415 million settlement and $83 million in whistleblower awards (Securities and Exchange Commission (SEC) 2016; Reuters 2018)."}, {"title": "ENERGY MONITORING", "content": "Unauthorized data centers or the use of data centers for unauthorized training runs could be detected by monitoring energy consumption, either pas- sively (through grid data obtained via espionage), or actively (using devices to measure grid activity).\nIf the total amount of energy reaching a data center can be measured with reasonable accuracy, it should be pos- sible to convert the energy estimate into a reasonable ap- proximation of the number of FLOPs completed by that fa- cility (Desislavov, Mart\u00ednez-Plumed, and Hern\u00e1ndez-Orallo 2021). Using the FLOP/s, we could ascertain whether the fa- cility is at an unauthorized size. Although much harder, with a sufficient degree of sensitivity, it may also be possible to use energy monitoring to determine whether a data center is hosting an unauthorized training run. For example, there could be a shift in properties of the power consumption as the data center transitions to a more constant GPU utiliza- tion during training when compared to a variable number of inference requests.\nPrecedent. Economists use energy monitoring to ver- ify economic data. For example, Thomas Rawski used dis- crepancies between reported GDP growth and energy con- sumption to suggest exaggerated growth figures in China (Owyang and Shell 2017). This principle could be applied to detect unauthorized data centers, given the direct relation- ship between energy consumption and FLOPS."}, {"title": "CUSTOMS DATA ANALYSIS", "content": "Governments can use customs data to track the movement of key components for large-scale AI computing facilities. Import and ex- port records could be analyzed to identify unusual or un- explained patterns in the movement of critical hardware, equipment or raw materials. A sudden surge in imports of high-performance GPUs or other critical components to a specific region of concern, far exceeding the known require- ments of declared facilities in that region, would indicate non-compliance.\nPrecedent. The U.S. government's End-Use Monitoring (EUM) programs, particularly the Blue Lantern program for direct commercial sales, provide a robust precedent for tracking and verifying the use of sensitive technologies (U.S. Department of State 2021). Under the Blue Lantern pro- gram, the Department of State conducts pre-license, post- license/pre-shipment, and post-shipment checks to verify the legitimacy of proposed transactions and ensure compliance with use, transfer, and security requirements. This program has been successful in promoting understanding of U.S. defense trade controls, building mutual confidence among stakeholders, mitigating risks of diversion and unauthorized use, and uncovering violations of the Arms Export Control Act. A similar approach could be adapted for monitoring the movement and use of critical AI hardware components in countries at different stages of the chip supply chain."}, {"title": "FINANCIAL INTELLIGENCE", "content": "Governments could track suspicious financial transactions relating to the purchase of important components of AI development. Fi- nancial institutions could be required to flag large or unusual purchases of specialized AI hardware, monitor transactions to known AI chip manufacturers, and cross-reference financial data with customs information.\nPrecedent. In the US, the Financial Crimes Enforce- ment Network (FinCEN) uses the Suspicious Activity Re- port (SAR) system and FinCEN Exchange, a public-private partnership, to combat money laundering, terrorism financ- ing, and organised crime (Financial Crimes Enforcement Network 2024).\nIn the early 2010s, an SAR filed by a bank led to the discovery of a complex international bribery scheme. The case resulted in multiple arrests and the seizure of over $100 million in criminal proceeds (Financial Crimes En- forcement Network 2011). This demonstrates how financial intelligence can uncover sophisticated international financial"}, {"title": "KEY TAKEAWAYS", "content": "National technical means offer a valuable starting point for verifying compliance with AI governance agreements. Nations already have extensive ex- perience using these methods to verify compliance with other kinds of international agreements. These methods can plausibly be used to detect large-scale AI infrastructure and unusual patterns in energy consumption, hardware imports, and financial transactions. However, these methods have im- portant limitations. In particular, adversaries could attempt to disguise data centers as other high-energy facilities like power plants, or when compute is distributed across multi- ple smaller sites."}, {"title": "Access-dependent verification methods", "content": "On- site inspections involve physical visits to declared data cen- ters to verify compliance with agreements on computing power. These inspections would focus on several aspects, including (but not limited to):\n\u2022 Chip identifiers. AI-capable chips could be required to have unique identifiers (Aarne, Fist, and Withers 2024). Inspectors could catalog these identifiers to ensure they match declared inventories.\n\u2022 Chip activity logs. Require chips to have activity logs that inspectors can analyze to verify that: (1) chips are be- ing used in accordance with their declared purposes and within agreed-upon limits, and (2) only licensed code is being executed on the chips (Shavit 2023).\n\u2022 FLOP/s limit compliance. Ensuring the data center's total computing power is below agreed thresholds.\n\u2022 Certified chip usage. Verifying that only approved chip models are in use.\n\u2022 Security measures. Verifying implementation of required security protocols.\n\u2022 Training run evidence. Examining records and transcripts of large-scale AI training activities.\n\u2022 Hardware integrity. Inspecting for any evidence of chip tampering (Aarne, Fist, and Withers 2024).\nIn addition to requiring periodic inspections, an agree- ment could also require continuous monitoring of certain facilities. In a continuous monitoring setup, inspectors are present at facilities at all times to catch any violations of agreements (such as tampering with hardware). A fi- nal possible implementation is challenge inspections, sim- ilar to those conducted by the Organization for the Prohibi- tion of Chemical Weapons (OPCW), where inspections can be called for on short notice based on suspicions of non- compliance (Organisation for the Prohibition of Chemical Weapons 1997).\nPrecedent. The New START treaty signed by the USA and Russia provides for 18 annual on-site inspections for the American and Russian inspections (US Department of State). These inspections allow for specific verification ac- tivities, such as confirming the number of reentry vehicles on deployed missiles, counting nuclear weapons on bombers, and verifying the conversion or elimination of weapon sys- tems. The treaty's approach of allowing a limited number of highly structured inspections, focused on counting and verifying specific hardware, is a suggestive precedent for in- spections of data centers. Notably, an earlier treaty (START I) also provided for continuous monitoring of specific facil- ities (Arms Control Association 2022).\nThe most significant precedent for the detailed inspec- tion of hardware is the IAEA's mandated use of bespoke tamper-evident containment seals for nuclear materials (In- ternational Atomic Energy Agency 2011). These seals each of which bears a unique identifier \u2013 are designed to pro- vide clear evidence of any tampering or unauthorized access. IAEA inspectors examine these seals during on-site visits, allowing them to detect any undeclared movement or use of nuclear materials."}, {"title": "ON-SITE INSPECTIONS OF SEMICONDUCTOR MANUFACTURING FACILITIES", "content": "Inspections of semi- conductor manufacturing facilities could be used to deter- mine the quantity and nature of chips produced. The man- ufacturing of advanced chips is a highly specialized activ- ity, and only a few entities have this capacity (Sastry et al. 2024). For example, it is well known that ASML produces EUV lithography systems which are needed to manufacture the latest generation of advanced chips (Khan, Mann, and Peterson 2021). If inspectors identified the existence of such machines, it would be relatively easy to know what kind of chips are possible to construct at the manufacturing site. Inspectors may also be able to use basic metrics like the square-meterage of a facility or the number of lithography machines to bound the number of chips that are possible to produce in such facilities.\nThese inspections could also verify that facilities are pro- ducing chips in accordance with any hardware-related agree- ments. For example, if nations agreed to only build chips with certain on-chip hardware governance mechanisms, in- spections of semiconductor manufacturing facilities could identify non-compliance. Inspectors could look at a sample of chips, potentially midway through production, to ensure"}, {"title": "ON-SITE INSPECTIONS OF AI DEVELOPERS", "content": "An international inspection team could visit an AI develop- ment facility to ensure that developers are running autho- rized code, ensure that developers are properly implement- ing model evaluations and safeguards, assess safety cul- ture, and assess security concerns. Inspections could involve various components, such as reviewing code (Casper et al. 2024), assessing compliance with commitments from safety cases, and conducting semi-structured interviews with key personnel to solicit security-relevant concerns (see Wasil et al. (2024a)). Inspections could uncover the usage of unau- thorized or unlicensed AI algorithms.\nPrecedent. The closest precedent is the IAEA's use of on- site inspections, as discussed above. Their approach demon- strates the feasibility of conducting thorough on-site inspec- tions in sensitive, high-tech environments, which could be adapted for AI development facilities. The key difference is that Al inspections would focus more on software and com- putational resources rather than physical materials, requiring inspectors with specialized expertise in AI technologies and development practices."}, {"title": "KEY TAKEAWAYS", "content": "Access-dependent methods can al- low for in-depth inspections of key facilities such as AI de- velopment facilities, hardware manufacturing facilities, and"}, {"title": "Hardware-dependent verification methods", "content": "CHIP LOCATION TRACKING Chip location tracking involves implementing a system to monitor the movement and use of AI-capable chips (Brass 2024). This method re- quires international agreement on chip manufacturing stan- dards and the implementation of tracking mechanisms di- rectly into the hardware. Each chip above a certain compu- tational threshold would be assigned a unique identifier and equipped with secure tracking capabilities.\nPrecedent. The concept of tracking and monitoring crit- ical technology has several precedents across different in- dustries, particularly where security, compliance, and inter- national regulation are concerned.\n\u2022 Nuclear Material Tracking. The IAEA monitors and tracks nuclear materials globally using systems like the Integrated Nuclear Fuel Cycle Information System (In- ternational Atomic Energy Agency 2024b). The IAEA also maintains a databse of incidents involving traffick- ing or other unauthorized uses of nuclear or radioactive materials (International Atomic Energy Agency 2024a).\n\u2022 Pharmaceutical Supply Chain Tracking. The Drug Sup- ply Chain Security Act (DSCSA) in the United States currently outlines steps to achieve, \"an interoperable and electronic way to identify and trace certain prescription drugs at the package level as they move through the sup- ply chain\" (U.S. Food and Drug Administration 2024)."}, {"title": "CHIP-BASED REPORTING", "content": "Chip-based reporting in- volves implementing mechanisms within AI-capable chips and closely associated hardware (e.g., networking cards) to automatically detect and signal when they are being used in ways that violate agreed-upon constraints. These constraints might include thresholds on the number of chips connected together, or specific operations the chip is not authorized to perform. By embedding these reporting mechanisms at the lowest levels of the software stack within the firmware and drivers of the AI-capable chips or associated network- ing devices it may become more challenging for devel- opers to bypass these safeguards. As one moves up the soft- ware stack, toward components that operate at higher lev- els of abstraction, it becomes easier for developers to re- place authorized programs with their own software, poten- tially circumventing the constraints. Therefore, focusing on the lower levels of the stack, such as firmware, which is the (often read-only) software residing on the device (NASA 2004), and the drivers, which allow the operating system to communicate with the device (Microsoft 2023), is crucial for effective enforcement of constraints. These components are typically developed by the chip maker, further limiting the number of developers who could foreseeably edit reporting mechanisms.\nPrecedent. The closest precedent for this type of firmware-based reporting is the Light Hash Rate (LHR) GPUs developed by NVIDIA. These GPUs can detect, via mechanisms implemented in their firmware and drivers, whether they are being used for Ethereum mining (Nvidia 2021). Similar strategies could foreseeably be developed to report unauthorized AI training."}, {"title": "KEY TAKEAWAYS", "content": "Hardware-dependent verification methods may offer robust and privacy-preserving tools for detecting non-compliance. However, these methods require nations with advanced hardware manufacturing capabilities to agree to rules around hardware manufacturing. Another challenge is that advanced chips are already in circulation (without hardware-enabled mechanisms built-in). A verifi- cation regime relying on hardware-dependent measures may need to address this \u201clegacy hardware\u201d, potentially through retrofitting techniques or gradual phase-out strategies.\nIf successfully implemented, these methods could dra- matically enhance the effectiveness of other verifica- tion approaches, particularly on-site inspections. However, they also raise important concerns about privacy, national sovereignty, and potential misuse that must be carefully ad- dressed. Overall, hardware-dependent methods represent a promising but long-term goal, requiring sustained interna- tional cooperation and technological innovation to realize their full potential in AI governance."}, {"title": "Limitations and discussion", "content": "This paper examined verification methods that could help nations detect non-compliance with international agree- ments prohibiting unauthorized AI development and unau- thorized data centers.\nVerification methods vary in their feasibility, intrusiveness and effectiveness. National technical means offer a valu- able starting point, capable of detecting large-scale AI in- frastructure and unusual patterns in energy consumption, hardware imports, and financial transactions. However, na- tional technical means are limited in their ability to identify software-level violations or concerted attempts at conceal- ment. Access-dependent methods, such as on-site inspec- tions, provide more robust reassurance but require nations to agree to international inspections. Hardware-dependent ap- proaches offer additional robustness (potentially even guar- antees) but face some implementation challenges, including the need to address existing legacy hardware."}, {"title": "Future research directions", "content": "Our work provides a starting point for discussions about ver- ification methods, but there are many open questions that can be addressed by future work. Some of these directions include:\n\u2022 Red-teaming exercises for international verification. In a \"red-team\" step, the authors could brainstorm how an adversary might try to hide an unauthorized training run or unauthorized data center. Then, in a \"blue team\" step, the authors could identify how one or more verifi- cation methods could catch the adversary. Then, in a sub- sequent \"red team\" step, the authors could brainstorm if there are feasible ways for the adversary to avoid or un- dermine the verification method(s). This process could be used to determine likely ways that adversaries may try to evade verification methods and highlight ways of strengthening international verification regimes.\n\u2022 Design of international AI governance institutions. Compliance with international agreements is often ver- ified by international institutions. Some early work has proposed international organizations that could set and verify compliance with safety standards (Ho et al. 2023; Cass-Beggs et al. 2024), certify national licensing agen- cies (Trager et al. 2023), verify compliance with a va- riety of potential agreements (see Maas and Villalobos (2023)), and participate in joint AI safety research (Cass- Beggs et al. 2024). One avenue for future research is to provide more details about how an international verifi- cation agency could be structured, how decision-making"}, {"title": "Conclusion", "content": "Our work provides an initial step toward a better understand- ing of how compliance with international Al agreements could be verified. Efforts to improve our understanding of verification methods will be especially important if global security risks from advanced AI become concerning enough to motivate coordinated national and international action. We believe some AI governance work should aim to pre- pare in advance for such scenarios. Such \u201cfuture-oriented\" AI governance work could address questions that would inform policymaking efforts in scenarios where concerns about global security risks became significantly stronger. Our hope is that our work on verification methods illustrates an example of promising work in this category."}]}