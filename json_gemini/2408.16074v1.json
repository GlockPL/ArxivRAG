{"title": "Verification methods for international AI agreements", "authors": ["Akash R. Wasil", "Tom Reed", "Jack William Miller", "Peter Barnett"], "abstract": "What techniques can be used to verify compliance with international agreements about advanced AI development? In this paper, we examine 10 verification methods that could detect two types of potential violations: unauthorized AI training (e.g., training runs above a certain FLOP threshold) and unauthorized data centers. We divide the verification methods into three categories: (a) national technical means (methods requiring minimal or no access from suspected non-compliant nations), (b) access-dependent methods (methods that require approval from the nation suspected of unauthorized activities), and (c) hardware-dependent methods (methods that require rules around advanced hardware). For each verification method, we provide a description, historical precedents, and possible evasion techniques. We conclude by offering recommendations for future work related to the verification and enforcement of international AI governance agreements.", "sections": [{"title": "Executive Summary", "content": "Efforts to maximize the benefits and minimize the global security risks of advanced AI may lead to international agreements. This paper outlines methods that could be used to verify compliance with such agreements. The verification methods we cover are focused on detecting two potential violations:\nViolations to verify\n\u2022 Unauthorized AI development (for example, AI development that goes beyond a FLOP threshold set by an international agreement, or the execution of a training run that has not received a license).\n\u2022 Unauthorized data centers (for example, data centers that go beyond a maximum computing capacity limit or networking limit set by an international agreement).\nWe identify 10 verification methods and divide them into three categories:\n1. National technical means. Methods that can be used by nations unilaterally.\n2. Access-dependent methods. Methods that require a nation to grant access to national or international inspectors\n3. Hardware-dependent methods. Methods that require agreements pertaining to advanced hardware\nNational technical means\n1. Remote sensing: Detect unauthorized data centers and semiconductor manufacturing via visual and thermal signatures.\n2. Whistleblowers: Incentivize insiders to report non-compliance.\n3. Energy monitoring: Detect power consumption patterns that suggest the potential presence of large GPU clusters.\n4. Customs data analysis: Track the movement of critical AI hardware and raw materials.\n5. Financial intelligence: Monitor large financial transactions related to AI development.\nAccess-dependent methods\n1. Data center inspections: Conduct inspections of sites to assess the size of a data center, verify compliance with hardware agreements, and verify compliance with other safety and security agreements.\n2. Semiconductor manufacturing facility inspections: Conduct inspections of sites to determine the quantity of chip production and verify that chip production conforms to any agreements around advanced hardware.\n3. AI developer inspections: Conduct inspections of AI development facilities via interviews, document and training transcript audits, and potential code reviews.\nHardware-dependent methods\n1. Chip location tracking: Automatic location tracking of advanced AI chips.\n2. Chip-based reporting: Automatic notification if chips are used for unauthorized purposes."}, {"title": "Introduction", "content": "The development of advanced artificial intelligence poses major global security risks. Significant threats include the potential for pervasive surveillance, the development of autonomous weapons, and misuse by malicious actors. Some of the most concerning risks stem from loss of control and misalignment (Bengio et al. 2024; Bostrom 2014; Ngo, Chan, and Mindermann 2022). A sufficiently powerful misaligned AI system could autonomously act against human interests following an objective function which does not capture human values (Pan, Bhatia, and Steinhardt 2022). There is a great amount of uncertainty around what kinds of safeguards will be necessary to prevent misalignment (Gabriel 2020). Many experts believe that safeguards may require many years or decades of concerted research effort.\nAl risks are exacerbated by race dynamics - companies are rapidly progressing in the hope of being the first to develop artificial superintelligence (Armstrong, Bostrom, and Shulman 2016; Hogarth 2023). In the context of an AI race, nations may not have sufficient time to carefully and cautiously develop or evaluate such safeguards.\nInternational agreements could help avoid or mitigate a race between nations. Even though governments are at early stages of understanding AI risks, key figures in the United States and China have already acknowledged concerns about AI global security risks and expressed interest in global governance approaches (Wasil and Durgin 2024). As governments become more aware of AI risks, they may become interested in global governance strategies that curb these race dynamics. Alternatively, nations might agree to cede the development of advanced AI to a joint international project. The international institution would be responsible for carrying out certain forms of advanced AI development, which would be illegal outside the context of this secure joint project (Hogarth 2023).\nInternational agreements require verification. Nations might be much more likely to form international agreements around rules that they can reliably verify (Fearon 1995; Baker 2023). By \u201cverify\u201d, we mean that nations would be able to detect non-compliance with agreements. Ideally, verification methods (methods used to detect non-compliance) would provide early and reliable warning signs. \"Early\", in that non-compliance could be detected relatively quickly (before a nation achieved any meaningful unauthorized advantage in advanced AI development), and \u201creliable\u201d in that non-compliance would be very likely to be detected.\nIn this paper, we provide an overview of verification methods for international AI agreements. We begin by outlining the potential targets such an international agreement. We then outline 10 verification methods. For each verification method, we provide a description, some precedents for how the verification method has been used in the past, and an example evasion technique to illustrate how an adversary could attempt to circumvent the method. Finally, we discuss"}, {"title": "What to verify: Unauthorized AI development and unauthorized data centers", "content": "An international agreement on AI could take many forms, depending on how the technology and its associated risks evolve. In scenarios where continued AI development leads to substantial acknowledged global security risks, we anticipate that verification methods would need to be capable of detecting two primary types of potential violations:\n1. Unauthorized data centers. International governance of AI could plausibly set restrictions on the form, size, quantity, and location of large-scale computing facilities. Verification methods would therefore be needed to detect the construction or operation of data centers that violate these agreed-upon standards.\n2. Unauthorized training runs. An effective international system for governing AI would likely include restrictions on the scale and characteristics of AI development. Beyond detecting unauthorised data centers, methods to verify that known data centers are compliant with agreed-upon standards would also be necessary. For example, an agreement might stipulate that AI training runs should not exceed a certain FLOP threshold (Heim 2024), use specific types of training data, or employ certain training algorithms. Verification methods would be needed to detect whether AI development activities occurring within facilities violate such standards."}, {"title": "Methodology", "content": "Our process for compiling verification methods involved a few steps: (a) reviewing relevant literature on AI verification and international AI governance, (b) reviewing relevant literature on verification methods for agreements in other fields (e.g., nuclear security, biosecurity, arms control), and (c) conducting informal interviews with experts in technical AI governance. Through this process, we identified 10 verification methods. For each verification method, we examined its application in other fields to inform our description of how the method could be used in the context of AI disagreements and to inform our section about the method's precedent in other fields. We also grouped the methods into categories based on the circumstances in which they could be implemented: universally (national technical means), only in cases where a nation provides access (access-dependent), or"}, {"title": "Verification methods", "content": "Defining \"verification method\". In this piece, a verification method is a method that could directly be used to detect defection or non-compliance from an agreement. That is, we assume an adversarial setup in which one party is explicitly attempting to \u201chide\u201d unauthorized data centers or unauthorized AI training.\nCategorizing verification methods. Some verification methods can be implemented without any buy-in from nations suspected of non-compliance, some verification methods require cooperation or authorization from the suspected nation, and some verification methods require cooperation from hardware manufacturers. These distinctions are useful for determining which verification methods might be feasible under various circumstances.\nThus, we divide verification methods into three categories: (a) national technical means (methods that can be implemented without the approval of individual nations),\n(b) access-dependent verification methods (methods that require international agreements that include the suspected nation), and (c) hardware-dependent verification methods (methods that require international agreements that include major hardware manufacturers). See Figure 1 and Figure 6 for a visual summary of the verification methods."}, {"title": "National Technical Means", "content": "REMOTE SENSING Remote sensing techniques, including satellite imagery and other forms of aerial observation, can detect potential undeclared data centers using visual, infrared and other electromagnetic signatures. Advanced commercial satellites, which can achieve sub-meter resolutions (Statista 2022), could identify specialized cooling units and the movement of computing equipment.\nInfrared imaging is particularly promising for detecting concealed data centers, as GPUs and other computing hardware generate significant heat signatures (Yuan et al. 2023) that are difficult to mask. This could reveal large-scale computing facilities even when visually concealed.\nRecent advancements in machine learning have further enhanced remote sensing capabilities for verification. Drawing from nuclear non-proliferation efforts (Rutkowski and Niemeyer 2020), AI-driven approaches such as supervised and unsupervised classification techniques can be applied to remotely sensed data. These methods could significantly improve the identification and monitoring of potential AI development facilities without requiring on-site access, bolstering national technical means for Al governance verification.\nWhile remote sensing can be used without formal agreements, international commitments similar to START could enhance its effectiveness by ensuring non-interference and facilitating data exchange (U.S. Department of State 2023).\nWHISTLEBLOWERS Insiders with knowledge of undeclared facilities or operations could provide valuable information not detectable through external means. Potential whistleblowers include employees, contractors, or local residents aware of suspicious activities. Governments could incentivize whistleblowing by:\n1. Establishing robust protection frameworks specifically for AI and technology sectors;\n2. Offering financial incentives for verified information;\n3. Creating secure, anonymous reporting channels;\n4. Providing legal support and job protection;\n5. Developing international cooperation for cross-border whistleblower protection (Loyens and Vandekerckhove 2018).\nENERGY MONITORING Unauthorized data centers or the use of data centers for unauthorized training runs could be detected by monitoring energy consumption, either passively (through grid data obtained via espionage), or actively (using devices to measure grid activity).\nIf the total amount of energy reaching a data center can be measured with reasonable accuracy, it should be possible to convert the energy estimate into a reasonable approximation of the number of FLOPs completed by that facility (Desislavov, Mart\u00ednez-Plumed, and Hern\u00e1ndez-Orallo 2021). Using the FLOP/s, we could ascertain whether the facility is at an unauthorized size. Although much harder, with a sufficient degree of sensitivity, it may also be possible to use energy monitoring to determine whether a data center is hosting an unauthorized training run. For example, there could be a shift in properties of the power consumption as the data center transitions to a more constant GPU utilization during training when compared to a variable number of inference requests.\nCUSTOMS DATA ANALYSIS Governments can use customs data to track the movement of key components for large-scale AI computing facilities. Import and export records could be analyzed to identify unusual or unexplained patterns in the movement of critical hardware, equipment or raw materials. A sudden surge in imports of high-performance GPUs or other critical components to a specific region of concern, far exceeding the known requirements of declared facilities in that region, would indicate non-compliance.\nFINANCIAL INTELLIGENCE Governments could track suspicious financial transactions relating to the purchase of important components of AI development. Financial institutions could be required to flag large or unusual purchases of specialized AI hardware, monitor transactions to known AI chip manufacturers, and cross-reference financial data with customs information."}, {"title": "KEY TAKEAWAYS", "content": "National technical means offer a valuable starting point for verifying compliance with AI governance agreements. Nations already have extensive experience using these methods to verify compliance with other kinds of international agreements. These methods can plausibly be used to detect large-scale AI infrastructure and unusual patterns in energy consumption, hardware imports, and financial transactions. However, these methods have important limitations. In particular, adversaries could attempt to disguise data centers as other high-energy facilities like power plants, or when compute is distributed across multiple smaller sites."}, {"title": "Access-dependent verification methods", "content": "ON-SITE INSPECTIONS OF DATA CENTERS On-site inspections involve physical visits to declared data centers to verify compliance with agreements on computing power. These inspections would focus on several aspects, including (but not limited to):\n\u2022 Chip identifiers. AI-capable chips could be required to have unique identifiers (Aarne, Fist, and Withers 2024). Inspectors could catalog these identifiers to ensure they match declared inventories.\n\u2022 Chip activity logs. Require chips to have activity logs that inspectors can analyze to verify that: (1) chips are being used in accordance with their declared purposes and within agreed-upon limits, and (2) only licensed code is being executed on the chips (Shavit 2023).\n\u2022 FLOP/s limit compliance. Ensuring the data center's total computing power is below agreed thresholds.\n\u2022 Certified chip usage. Verifying that only approved chip models are in use.\n\u2022 Security measures. Verifying implementation of required security protocols.\n\u2022 Training run evidence. Examining records and transcripts of large-scale AI training activities.\n\u2022 Hardware integrity. Inspecting for any evidence of chip tampering (Aarne, Fist, and Withers 2024).\nIn addition to requiring periodic inspections, an agreement could also require continuous monitoring of certain facilities. In a continuous monitoring setup, inspectors are present at facilities at all times to catch any violations of agreements (such as tampering with hardware). A final possible implementation is challenge inspections, similar to those conducted by the Organization for the Prohibition of Chemical Weapons (OPCW), where inspections can be called for on short notice based on suspicions of non-compliance (Organisation for the Prohibition of Chemical Weapons 1997).\nON-SITE INSPECTIONS OF SEMICONDUCTOR MANUFACTURING FACILITIES Inspections of semiconductor manufacturing facilities could be used to determine the quantity and nature of chips produced. The manufacturing of advanced chips is a highly specialized activity, and only a few entities have this capacity (Sastry et al. 2024). For example, it is well known that ASML produces EUV lithography systems which are needed to manufacture the latest generation of advanced chips (Khan, Mann, and Peterson 2021). If inspectors identified the existence of such machines, it would be relatively easy to know what kind of chips are possible to construct at the manufacturing site. Inspectors may also be able to use basic metrics like the square-meterage of a facility or the number of lithography machines to bound the number of chips that are possible to produce in such facilities.\nThese inspections could also verify that facilities are producing chips in accordance with any hardware-related agreements. For example, if nations agreed to only build chips with certain on-chip hardware governance mechanisms, inspections of semiconductor manufacturing facilities could identify non-compliance. Inspectors could look at a sample of chips, potentially midway through production, to ensure"}, {"title": "ON-SITE INSPECTIONS OF AI DEVELOPERS", "content": "An international inspection team could visit an AI development facility to ensure that developers are running authorized code, ensure that developers are properly implementing model evaluations and safeguards, assess safety culture, and assess security concerns. Inspections could involve various components, such as reviewing code (Casper et al. 2024), assessing compliance with commitments from safety cases, and conducting semi-structured interviews with key personnel to solicit security-relevant concerns (see Wasil et al. (2024a)). Inspections could uncover the usage of unauthorized or unlicensed AI algorithms."}, {"title": "KEY TAKEAWAYS", "content": "Access-dependent methods can allow for in-depth inspections of key facilities such as AI development facilities, hardware manufacturing facilities, and data centers. If international inspectors have sufficient access to these facilities, this provides a great deal of robustness to a verification regime. However, such methods may be perceived as invasive, and they may rely on the permission of nations that are suspected of unauthorized activity. Access-dependent methods can also be somewhat flexible depending on the amount of political will and the level of access that nations are willing to provide. To preserve privacy or trade secrets, inspectors may receive limited access-enough access to verify that an unauthorized training run is not being conducted but not enough access to see exactly what kind of tasks are being performed."}, {"title": "Hardware-dependent verification methods", "content": "CHIP LOCATION TRACKING Chip location tracking involves implementing a system to monitor the movement and use of AI-capable chips (Brass 2024). This method requires international agreement on chip manufacturing standards and the implementation of tracking mechanisms directly into the hardware. Each chip above a certain computational threshold would be assigned a unique identifier and equipped with secure tracking capabilities.\nPrecedent. The concept of tracking and monitoring critical technology has several precedents across different industries, particularly where security, compliance, and international regulation are concerned.\n\u2022 Nuclear Material Tracking. The IAEA monitors and tracks nuclear materials globally using systems like the Integrated Nuclear Fuel Cycle Information System (International Atomic Energy Agency 2024b). The IAEA also maintains a databse of incidents involving trafficking or other unauthorized uses of nuclear or radioactive materials (International Atomic Energy Agency 2024a).\n\u2022 Pharmaceutical Supply Chain Tracking. The Drug Supply Chain Security Act (DSCSA) in the United States currently outlines steps to achieve, \"an interoperable and electronic way to identify and trace certain prescription drugs at the package level as they move through the supply chain\" (U.S. Food and Drug Administration 2024).\nCHIP-BASED REPORTING Chip-based reporting involves implementing mechanisms within AI-capable chips and closely associated hardware (e.g., networking cards) to automatically detect and signal when they are being used in ways that violate agreed-upon constraints. These constraints might include thresholds on the number of chips connected together, or specific operations the chip is not authorized to perform. By embedding these reporting mechanisms at the lowest levels of the software stack within the firmware and drivers of the AI-capable chips or associated networking devices it may become more challenging for developers to bypass these safeguards. As one moves up the software stack, toward components that operate at higher levels of abstraction, it becomes easier for developers to replace authorized programs with their own software, potentially circumventing the constraints. Therefore, focusing on the lower levels of the stack, such as firmware, which is the (often read-only) software residing on the device (NASA 2004), and the drivers, which allow the operating system to communicate with the device (Microsoft 2023), is crucial for effective enforcement of constraints. These components are typically developed by the chip maker, further limiting the number of developers who could foreseeably edit reporting mechanisms.\nPrecedent. The closest precedent for this type of firmware-based reporting is the Light Hash Rate (LHR) GPUs developed by NVIDIA. These GPUs can detect, via mechanisms implemented in their firmware and drivers, whether they are being used for Ethereum mining (Nvidia 2021). Similar strategies could foreseeably be developed to report unauthorized AI training."}, {"title": "KEY TAKEAWAYS", "content": "Hardware-dependent verification methods may offer robust and privacy-preserving tools for detecting non-compliance. However, these methods require nations with advanced hardware manufacturing capabilities to agree to rules around hardware manufacturing. Another challenge is that advanced chips are already in circulation (without hardware-enabled mechanisms built-in). A verification regime relying on hardware-dependent measures may need to address this \u201clegacy hardware\u201d, potentially through retrofitting techniques or gradual phase-out strategies.\nIf successfully implemented, these methods could dramatically enhance the effectiveness of other verification approaches, particularly on-site inspections. However, they also raise important concerns about privacy, national sovereignty, and potential misuse that must be carefully addressed. Overall, hardware-dependent methods represent a promising but long-term goal, requiring sustained international cooperation and technological innovation to realize their full potential in AI governance."}, {"title": "Limitations and discussion", "content": "This paper examined verification methods that could help nations detect non-compliance with international agreements prohibiting unauthorized AI development and unauthorized data centers.\nVerification methods vary in their feasibility, intrusiveness and effectiveness. National technical means offer a valuable starting point, capable of detecting large-scale AI infrastructure and unusual patterns in energy consumption, hardware imports, and financial transactions. However, national technical means are limited in their ability to identify software-level violations or concerted attempts at concealment. Access-dependent methods, such as on-site inspections, provide more robust reassurance but require nations to agree to international inspections. Hardware-dependent approaches offer additional robustness (potentially even guarantees) but face some implementation challenges, including the need to address existing legacy hardware."}, {"title": "Future research directions", "content": "Our work provides a starting point for discussions about verification methods, but there are many open questions that can be addressed by future work. Some of these directions include:\n\u2022 Red-teaming exercises for international verification. In a \"red-team\" step, the authors could brainstorm how an adversary might try to hide an unauthorized training run or unauthorized data center. Then, in a \"blue team\" step, the authors could identify how one or more verification methods could catch the adversary. Then, in a subsequent \"red team\" step, the authors could brainstorm if there are feasible ways for the adversary to avoid or undermine the verification method(s). This process could be used to determine likely ways that adversaries may try to evade verification methods and highlight ways of strengthening international verification regimes.\n\u2022 Design of international AI governance institutions. Compliance with international agreements is often verified by international institutions. Some early work has proposed international organizations that could set and verify compliance with safety standards (Ho et al. 2023; Cass-Beggs et al. 2024), certify national licensing agencies (Trager et al. 2023), verify compliance with a variety of potential agreements (see Maas and Villalobos (2023)), and participate in joint AI safety research (Cass-Beggs et al. 2024). One avenue for future research is to provide more details about how an international verification agency could be structured, how decision-making power is distributed between nations, how the agency handles disputes over non-compliance, and what powers ought to be granted to the agency. Such work could draw from best practices or lessons learned from the design and implementation of other international institutions (such as the IAEA and the OPCW) and bilateral or multilateral agreements (such as the Strategic Arms Reduction Treaties and the Wassenaar Agreement).\n\u2022 Enforcement of international agreements. Our paper focused on verification- detecting whether or not nations are complying with an agreement. A separate important question is enforcement- how nations should react in the event that non-compliance is identified. Such work could examine what kinds of responses would be proportionate to the violation. For example, evidence of small-scale chip smuggling would warrant a less strong response than evidence of an illegal or unauthorized training run.\n\u2022 Research on hardware-enabled mechanisms to enhance verification and/or enforcement. Hardware-enabled mechanisms can unlock new verification methods and make existing verification methods more robust. Some hardware-enabled mechanisms are ready to be implemented swiftly, while others may take several years of research to further develop. Additionally, there are open questions relating to how to make hardware-enabled mechanisms more tamper-proof and privacy-preserving (see Kulp et al. (2024)).\n\u2022 Detecting unauthorized AI deployment or inference. Our paper focuses on detecting unauthorized AI development. Nations may also wish to have agreements in which they agree not to deploy advanced Al systems in certain ways (for example, nations might prohibit AI from being deployed in the context of nuclear systems, military R&D research, or AI R&D research that could trigger uncontrolled AI development.) Future work could examine verification methods that could detect the unauthorized deployment of AI systems, potentially through hardware-enabled licenses that detect the presence of unauthorized code used for inference.\n\u2022 Detecting compliance with agreements around model evaluations. International agreements may require that certain kinds of model evaluations are conducted to detect potential safety or security issues (see Shevlane et al. (2023)). Reliable risk evaluations and risk mitigation strategies could become a minimum safety bar imposed by international agreements. Future work could examine verification methods that allow international authorities to ensure that parties are implementing a set of internationally-required model evaluations, as well as any specific model evaluations that a developer proposed as part of a safety case or licensing application (see (Clymer et al. 2024; Wasil et al. 2024b)).\n\u2022 Actions the international community can take in the immediate future. In the future, nations may be concerned enough about AI global security risks to warrant ambitious international agreements that require verification methods. For the immediate future, however, nations are interested in improving their understanding of global security risks. There are many actions that governments and civil society groups can participate in to increase global understanding of AI progress and AI risks. Examples include efforts like the UK and Seoul AI Safety Summits (see Bletchley Declaration (2023)), the establishment of the US and UK AI Safety Institutes and the Chinese AI Safety Network, Track II Dialogues between Western scientists and Chinese scientists (see International Dialogues on AI Safety (2023)), and plans for how to respond to AI-related emergencies (see Wasil et al. (2024c))."}, {"title": "Conclusion", "content": "Our work provides an initial step toward a better understanding of how compliance with international Al agreements could be verified. Efforts to improve our understanding of verification methods will be especially important if global security risks from advanced AI become concerning enough to motivate coordinated national and international action. We believe some AI governance work should aim to prepare in advance for such scenarios. Such \u201cfuture-oriented\" AI governance work could address questions that would inform policymaking efforts in scenarios where concerns about global security risks became significantly stronger. Our hope is that our work on verification methods illustrates an example of promising work in this category."}]}