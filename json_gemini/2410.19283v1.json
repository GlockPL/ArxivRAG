{"title": "ST-NeRP: Spatial-Temporal Neural Representation Learning with Prior Embedding for Patient-specific Imaging Study", "authors": ["Liang Qiu", "Liyue Shen", "Lianli Liu", "Junyan Liu", "Yizheng Chen", "Lei Xing"], "abstract": "During and after a course of therapy, imaging is routinely used to monitor the disease progression and assess the treatment responses. Despite of its significance, reliably capturing and predicting the spatial-temporal anatomic changes from a sequence of patient-specific image series presents a considerable challenge. Thus, the development of a computational framework becomes highly desirable for a multitude of practical applications. In this context, we propose a strategy of Spatial-Temporal Neural Representation learning with Prior embedding (ST-NeRP) for patient-specific imaging study. Our strategy involves leveraging an Implicit Neural Representation (INR) network to encode the image at the reference time point into a prior embedding. Subsequently, a spatial-temporally continuous deformation function is learned through another INR network. This network is trained using the whole patient-specific image sequence, enabling the prediction of deformation fields at various target time points. The efficacy of the ST-NeRP model is demonstrated through its application to diverse sequential image series, including 4D CT and longitudinal CT datasets within thoracic and abdominal imaging. The proposed ST-NeRP model exhibits substantial potential in enabling the monitoring of anatomical changes within a patient throughout the therapeutic journey.", "sections": [{"title": "I. INTRODUCTION", "content": "Longitudinal imaging studies, which involve repeated examinations of the same individual, are pivotal for understanding disease progression and optimizing patient management strategies over the course of treatment. By monitoring anatomical changes across sequential imaging scans, clinicians gain invaluable insights into how diseases evolve and how treatments impact them. This allows for timely interventions and personalized adjustments to treatment plans. However, manual analysis of these imaging studies is labor-intensive, time-consuming, and prone to variability between observers due to data complexity and patient diversity. Therefore, there is a growing demand for computer- aided tools to streamline the analysis process, reduce costs, accelerate turnaround times, and enhance overall reliability. Such tools have the potential to greatly improve patient care by providing more accurate and timely assessments of disease progression and treatment response.\nImage registration serves as a critical approach in longitudinal analysis, enabling the tracking of changes or evolution in anatomical regions like tumors or lesions over time. Traditional optimization-based methods often rely on predefined mathematical or computational models to describe image transformations, such as rigid or elastic deformations. While effective in many cases, these approaches can be time- consuming and suffer from limited accuracy, particularly when dealing with less distinctive image features [1]. Recent advances in learning-based registration algorithms offer promising solutions by directly learning complex patterns and relationships from data, potentially overcoming these limitations across a wide range of applications and imaging modalities [2]. Additionally, automatic image-based anatomical monitoring can be approached through (3D) object segmentation, detection and tracking, leveraging sophisticated feature representation techniques, an area of interest for both computer vision and medical image analysis communities [3- 9]. A recent study introduced a deep learning algorithm integrating appearance-based feature representation and anatomical constrains to track lesion motion within 4D longitudinal images, primarily focusing on monitoring changes in the center and radius of lesions [10]. To enhance the effectiveness of the representation embeddings, a multi-"}, {"title": "II. RELATED WORK", "content": "INR learning aims to represent an image or high- dimensional data via neural network parametrization. It offers an efficient way to represent 3D geometry and scene [13-20]. For example, DeepSDF utilized signed distance functions (SDFs) to represent 3D shapes, allowing the neural network to approximate the SDF and reconstruct or generate novel 3D shapes [18]. Similarly, NeRF introduced an approach to learn implicit representations of 3D scene, using a neural network to represent the scene's appearance and geometry implicitly, resulting in high-quality view synthesis and realistic renderings [19]. Moreover, INR has gained significant attention in the context of time-series data [30-33]. By incorporating time as an additional input, NeRF was extended to a dynamic domain, enabling the reconstruction and rendering of novel images of objects undergoing rigid and non-rigid motions, all from a single moving camera view [30]. Additionally, a novel method called Occupancy Flow was proposed for the spatiotemporal representation of time- changing 3D geometry, which effectively learns a continuous vector field to estimate the motion temporally and spatially for dense 4D reconstruction [31]. More recently, VideoINR was introduced as a continuous video representation, allowing the sampling and interpolation of the video frames at arbitrary frame rate and spatial resolution simultaneously [32]. Moreover, a framework was proposed which jointly learns INRs from RGB frames and events. This framework enables arbitrary scale video super-resolution by utilizing high temporal resolution property of events to complement RGB frames through spatial-temporal fusion and temporal filter. It also utilizes an effective spatial-temporal implicit representation to recover frames at arbitrary scales [33].\nDespite the considerable efforts on neural representation pertaining to natural or photographic images, limited research has been conducted in the medical domain [22, 25, 29, 34]. In [25], an Implicit Organ Segmentation Network (IOSNet) was proposed, which involved learning continuous segmentation functions, diverging from prevailing approaches that rely on discrete pixel/voxel-based representations. IOSNet demonstrated exceptional memory efficiency and accurately delineated organs regardless of their sizes. Another innovative approach utilized a curvature-enhanced implicit function network to generate high-quality tooth models from CBCT images. This method, detailed in [34], employed a curvature enhancement module as a guide for refining the surface reconstruction process, resulting in outstanding tooth model quality. Certain studies have explored medical applications in the spatiotemporal domain, such as cell tracking and segmentation in biomedical images. In this context, an INR model was designed to enable the synthesis of authentic cell shapes within the spatiotemporal domain, conditioning its generation on a latent code [29]. Likewise, a spatiotemporal INR network was put forward to construct a personalized model for monitoring the abdominal aortic aneurysms (AAA) progression. This model achieved proper interpolation of AAA shape by representing the AAA surface as the zero-level set of its signed distance function, rather than operating directly within the image domain [35]. Another relevant study by Wolterink et al. [22] utilized a multi-layer perceptron (MLP) network IDIR to implicitly represent a transformation function, facilitating deformable registration of 4D chest CT series. However, this work focused exclusively on the spatial domain, without considering temporal information."}, {"title": "A. INR Learning", "content": "INR learning aims to represent an image or high- dimensional data via neural network parametrization. It offers an efficient way to represent 3D geometry and scene [13-20]. For example, DeepSDF utilized signed distance functions (SDFs) to represent 3D shapes, allowing the neural network to approximate the SDF and reconstruct or generate novel 3D shapes [18]. Similarly, NeRF introduced an approach to learn implicit representations of 3D scene, using a neural network to represent the scene's appearance and geometry implicitly, resulting in high-quality view synthesis and realistic renderings [19]. Moreover, INR has gained significant attention in the context of time-series data [30-33]. By incorporating time as an additional input, NeRF was extended"}, {"title": "B. Deformable Registration of Medical Images", "content": "Deformable registration plays a significant role in medical image analysis and longitudinal study, aiming at aligning two or more images by estimating a deformation field that establishes a spatial mapping between corresponding anatomical structures. Despite numerous models developed to address the challenges associated with deformable registration"}, {"title": "IV. METHODOLOGY", "content": "To learn spatiotemporal deformation of anatomy, we train an ST-NeRP model using a patient's temporal image series. Our method, illustrated in Figure 2, consists of three submodules: (i) reference prior embedding, (ii) spatial- temporal learning, and (iii) patient-specific ROI adaptation. The primary objective is to capture temporal changes in the patient anatomy, particularly in some Regions of Interest (ROIs).\nConsider a sequence of medical images, e.g., longitudinal image data, for a specific patient, denoted as{f1, f2,\u2026, fr}, acquired at time_points {t\u2081,t2,\u2026,t\u2081}. In clinical practice, variations in patient positioning, anatomical changes, or disparities in imaging protocols across diverse scanners and"}, {"title": "A. Ref-Net: Reference Prior Embedding", "content": "As depicted in Fig. 2, we initiate the learning process of ST- NeRP by representing the reference image f, as the INR, which is a continuous function parameterized by the Ref-Net R\u03b8, defined as\n$R_\\theta : p_r \\rightarrow f_r(p_r)$, with $p \\in [0,1)^3$, $f_r(p_r) \\in \\mathbb{R}$,\nwhere the input p(x,y,z) indicates normalized spatial grid coordinates and the output fr(pr) denotes the corresponding intensity value in fr. Fourier mapping, which has demonstrated its efficacy in high-frequency low-dimensional regression tasks [50], is first used to encode the input coordinates p, before directly fed into an MLP network, shown as follows:\n$\\gamma(p) = [cos(2\\pi Bp), sin (2\\pi Bp)]$,\nwhere matrix B contains the coefficients for Fourier feature transformation in the position encoding. The entries of matrix B are randomly drawn from Gaussian distribution N(0,\u03c3\u00b2), characterized by a mean of 0 and a standard deviation \u03c3. Following Fourier feature embedding, the encoded coordinates"}, {"title": "B. Def-Net: Spatial-temporal Learning", "content": "We develop a spatial-temporal deformation network, Def- Net, to capture the temporal changes of the deformation fields from the patient-specific image sequence, enabling the prediction of the time-dependent deformation field at a target time point. To achieve this, the time variable t is incorporated into the input of Def-Net (see Fig. 2) and the deformation network is parameterized as follows:\n$D_\\theta : p_t = (p_t, t) \\rightarrow u(p_t)$ with $p_t =[0,1)^3, t \\in [1,T]$.\nIn this model, the spatial-temporal coordinates pt = (x,y,z,t) are the input, and the output of the model is the corresponding coordinate-based displacement u(pt) = (\u2206x, \u2206y, \u2206z) at time t, based on which the deformation field : R\u00b3 \u2192 R\u00b3 mapping coordinates of target image to those of the reference image is established. That is, for each pt, u(pt) is the displacement such that ft(p) and fr(p+u(p)) correspond to the same image voxels. To avoid unphysical deformation, registration diffeomorphism is integrated in Def-Net by utilizing the stationary velocity field representation, a concept established in prior work [52]. The formulation of the deformation field is derived through the ordinary differential equation:\n$\\frac{\\delta \\phi(\\tau)}{\\delta t} = v(\\phi(\\tau))$ with $\\phi(0) = Id$, $\\tau \\in [0,1]$,\nwhere \u03c6(0) represents the identity transformation and \u03c4 indicates time. The final deformation field \u03c6(\u00b9) is derived by integrating the stationary velocity field v over the time interval [0,1].\nThe computational flow of this network begins by encoding the spatial-temporal coordinates pt using the Fourier feature mapping as detailed in (2), resulting \u03b3(p). Subsequently, \u03b3(p) is fed into Def-Net, producing the stationary velocity field v. Finally, the derived v undergoes further processing through vector integration layers employing scaling and"}, {"title": "C. Patient-specific ROI Adaptation", "content": "Following the optimization of objective function (6), the optimized Def-Net D\u03b8\u2217 is employed to predict the deformation field at any spatial and temporal point. We apply it to monitor the contour propagation of the target anatomical regions. Assume that there is a human-annotated anatomical ROI denoted as Sr in the reference image fr. As shown in the yellow box within Fig. 2, using the deformation field predicted by Def-Net, we can map the reference ROI Sr to the target ROI St at any target time point. Specifically, given a spatial-temporal point st = (st,t) on the target segmentation grid map Mt, we can initially compute the corresponding deformed spatial coordinates $s = s_t +u(s_t)$ within the reference segmentation map Sr using relationship (4). Note that due to the continuity of the deformation function \u03c6, the transformed coordinates in the reference segmentation map may not align precisely with integer grid locations. Thus, to determine the value at the subvoxel location s, within Sr, nearest-neighbor interpolation is employed based on the eight neighboring voxels {zi}i=18 surrounding the point, namely:\n$c = arg \\min_{i \\in [1,8]} || s - z_i ||_2$,\nwhere c signifies the index of the target voxel chosen for interpolation. Subsequently, the predicted segmentation value"}, {"title": "IV. EXPERIMENTS", "content": "To demonstrate the effectiveness of our proposed method, we conducted experiments focusing on two tasks: time- continuous image interpolation task and longitudinal deformable registration task. Notably, we validated our approach on both short-term sequential image series (e.g., 4D CT datasets in thoracic and abdominal imaging), and long- term sequential image series (e.g., longitudinal lung cancer treatment CT dataset). These diverse datasets were chosen to highlight the flexibility of our method in handling different types of temporal image data. In our study, metrics, including peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), Dice Similarity Coefficient (Dice), and the number of voxels with a non-positive Jacobian determinant (Jo \u2264 0), were employed to rigorously assess the performance of the models' calculations."}, {"title": "A. Datasets", "content": "We collected 4D CT data retrospectively from two patients forming Dataset A for image interpolation task. This dataset includes a pancreas 4D CT image from XXX University Hospital (Case No.1), and a lung 4D CT image from a public NSCLC dataset [53] (Case No. 2), as illustrated in Fig. 3(a). In our experiments, we selected the first phase as the reference image to learn prior embedding, and we utilized the entire series of 4D image series excluding the target image to train the deformation network. All the images underwent intensity normalization to fit within the [0,1] range. Additionally, we assessed the pairwise deformable registration performance of our adapted ST-NeRP model using the publicly available DIR- Lab dataset [54], treated as Dataset B, which comprises 4D lung CT scans for 10 patients (see Fig. 3(b)). The objective was to register the maximum inspiration image (Phase 1) to expiration image (Phase 6) for each patient, with 300 manually annotated lung landmarks available for each scan. These landmarks serve as ground-truth for evaluating target registration error (TRE) between the two extreme phases. Notably, each 4D image in both datasets consists of 3D images corresponding to 10 distinct breathing phases, captured at uniform time intervals throughout a respiratory cycle."}, {"title": "B. Implementation Details", "content": "We implemented our networks using PyTorch [56]. In our settings, the SIRENs for both Ref-Net and Def-Net are constructed with 8 layers, each consisting of 256 neural nodes. The Fourier feature embedding for position encoding is designed with a width of 256, and the corresponding hyper-"}, {"title": "C. Experiments for 4D CT Interpolation on Dataset A", "content": "In the evaluation of the ST-NeRP model's 4D CT interpolation capability using Dataset A, the first phase was used as the reference image to embed prior knowledge in Ref- Net. Def-Net was then trained with the remaining phases, excluding the target phase to be interpolated (phase No. 4, 6 & 8, respectively). As shown in TABLE I, our approach yielded exceptional prior embeddings with PSNR/SSIM of 45.26dB/0.995 and 41.30dB/0.990. Additionally, we presented the average evaluation results for both the target interpolation phase and the rest phases involved in training stage (meantgt/meanrst). For Case 1 and Case 2, We observed satisfying average PSNR/SSIM values of 33.55dB/0.981 and 27.86dB/0.963, respectively, for the interpolated target phases. A notable decrease of 9% and 6% in PSNR for target phase was noted compared to the rest phase images involved in the training process. This phenomenon is further elucidated in Fig. 4(a), which illustrates the evaluation on the entire phase series using the well-trained ST-NeRP. Such a decline can be attributed to the absence of visibility of the target phase during the training stage and the intricate nonlinearity in the medical image series. Nevertheless, we observed a slight and acceptable decrease of 0.7% and 1.3% in SSIM, indicating that despite being influenced by intensity noise, the interpolation results still adequately predict structural characteristics. Notably, the predicted deformation fields exhibit diffeomorphic properties across all the cases, demonstrating pronounced topology preservation, invertibility and biomechanical plausibility."}, {"title": "D. Experiments for Longitudinal CT Interpolation", "content": "We conducted further investigations into the performance of ST-NeRP on Dataset C, which includes two longitudinal lung CT cases, each comprising 8 sequential 3D CT scans. The initial CT scan was designated as the reference, represented effectively by our trained Ref-Net as a continuous function parametrized by network weights. Subsequently, we proceed to learn a spatial-temporal continuous Def-Net to predict the target image at any other time point. Here, we aimed to predict scan No. 3 and No. 6 respectively, positioned at intermediate stage within the scan sequence, utilizing the remaining CT scan images within the series for model training. The obtained PSNR and SSIM values for the predicted target images are presented in TABLE II, indicating an average PSNR of 30.11dB and SSIM of 0.908. Additionally, an evaluation of the model's performance across the entire image series was conducted to identify variations among different temporal points, as shown in Fig. 4(b), similar to the outcomes discussed in Section IV-C."}, {"title": "E. Experiments for Deformable Registration", "content": "To elucidate the versatile capabilities of ST-NeRP, we customized the model for deformable registration tasks. This adaptation simplified the input configuration of Def-Net, focusing solely on the target image rather than multiple consecutive images. We conducted comparisons with various existing methods, encompassing both learning-based and traditional optimization-based approaches, using the 4D CT dataset DIR-LAB. This evaluation involved computing the TRE for 300 predefined anatomical landmark pairs on two extreme phase images (maximum inspiration-to-expiration registration) for each patient, as outlined in TABLE IV. Specifically, we compared our method with two state-of-the-art INR-based registration techniques: IDIR [22] and an enhanced version incorporating cycle-consistency proposed by Van Harten et al. [48]. Additionally, we evaluated our method against two CNN-based methods, namely DLIR [57] and an anatomically constrained CNN proposed by Hering et al. [43]. Furthermore, we included comparisons with two traditional optimization-based deformable registration approaches: the deformable registration tool integrated within ANTs [55] and Uniform B-Splines [58]. The results for the two INR-based methods were sourced from [48], while the other methods' results were taken from their respective papers, except for ANTs, for which we generated the result using its toolbox. Our findings reveal that our ST-NeRP method attained the average registration error of 1.06\u00b11.23 mm, outperforming all the traditional optimization-based and CNN-based methods. Additionally, our method surpasses IDIR (1.10\u00b11.42 mm) and achieves comparable performance to the cycle-consistent INR proposed by Van Harten et al (1.04\u00b11.11 mm), demonstrating the excellent registration performance of our method.\nTo further underscore the efficacy of our ST-NeRP, we conducted validation on Dataset D for longitudinal registration analysis, primarily assessing it through the Dice score metric. In accordance with clinical requirements, our experimental efforts encompassed two distinct schemes across 5 patients: long-range deformable registration, aiming to predict the target CT scan ft given the initial CT scan f\u2081 as reference time point, and short-range deformable registration, targeting prediction between two adjacent CT scans ft-1 and ft, using the last neighboring scan as a reference. We compared our approach with the INR-based deformable registration approach IDIR [22] the widely-used open-source deformable registration tool integrated within ANTs [55], as shown in Fig. 8. In the context of the long-range registration (f\u2081 \u2192 ft), a discernible decrease in the Dice score was observed. This diminishing trend can be attributed to the evolving nature of the treatment process, where the tumor undergoes substantial anatomical modifications, introducing complexities for deformable registration, particularly when referencing back to the initial time point. In contrast, the short-range registration (ft-1\u2192 ft) showed comparatively improved outcomes due to the gradual progression of image deformations between successive scans, resulting in better results overall. Moreover,"}, {"title": "V. DISCUSSION", "content": "A comprehensive understanding of the disease progression through the analysis of monitored longitudinal data, including CT and MRI scans, is pivotal in clinical practice for informed decision-making and improved patient care. This paper introduces ST-NeRP, a personalized learning framework to explore the continuous deformation patterns inherent in sequential medical data, where the spatial-temporal features are effectively extracted by the implicit neural representation learning. Our approach offers two significant advantages, including: 1) patient-specific attribute without the need for extensive data for model training; and 2) applicability to both image interpolation and deformable registration tasks. To demonstrate the efficacy of ST-NeRP, we conducted validation experiments on 4D CT images and longitudinal CT series focusing on thoracic and abdominal regions, and it has great potential to be generalizable across diverse imaging modalities commonly encountered in the clinical settings. To better understand the role of Ref-Net within our framework, we conducted an ablation study analyzing prior embeddings (see TABLE III and IV). The comprehensive comparison, as illustrated in TABLE VI, showcases the impact of Ref-Net on both interpolation and registration tasks in the longitudinal study involving five patients. We observe that our method improves Dice scores in both tasks, especially in image interpolation by 1.8% = 0.697-0.679, demonstrating the effective feature representation of Ref-Net and its substantial contribution to the overall model performance. Although ST- NeRP (w/o prior) exhibits slightly superior performance in PSNR and SSIM, attributed to the smoothness property of integrated STL module, we prioritize ST-NeRP's advantage in better capturing anatomical structure information, which usually holds greater significance in clinical decision-making, tumor contour propagation and treatment plan adaptation.\nOur method excels in personalized analysis for precision medicine, circumventing common issues associated with data- driven learning methods, such as the need for extensive training samples and the generalization gap between training and test data. However, it requires optimizing our model from scratch for each patient, which can be inefficient. Therefore, it is crucial to explore promising solutions to address this challenge. This includes capturing shared features across patients' data or exploring transfer learning techniques to adapt pre-trained models from one patient to new patients with minimal retraining. Another alternative approach involves combining the advantages of both data-driven and INR methods. This entails utilizing a pre-trained data-driven"}, {"title": "VI. CONCLUSION", "content": "In this study, we proposed a novel ST-NeRP model for patient-specific 4D or longitudinal imaging study. Our method effectively learns a spatial-temporal deformation function by integrating prior knowledge of the reference image, which proves instrumental in monitoring subtle alterations in target images. Through comprehensive experiments using both 4D CT and longitudinal image datasets in thoracic and abdominal regions, we successfully demonstrated the capability of the ST-NeRP approach in performing time-continuous image interpolation across discrete series of images. Additionally, our model exhibited excellent performance in deformable registration and reliable adaptation inferences regarding anatomical ROI regions, i.e., contour propagation. Given the ubiquity of multiple intermittent imaging scans in clinical practice, the proposed technique should find widespread applications in the future."}]}