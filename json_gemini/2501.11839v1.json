{"title": "Supervised Learning for Analog and RF Circuit Design: Benchmarks and Comparative Insights", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yue Niu", "Sara Babakniya", "Mahdi Alesheikh", "Hamidreza Aghasi", "Salman Avestimehr"], "abstract": "Automating analog and radio-frequency (RF) circuit design using machine learning (ML) significantly reduces the time and effort required for parameter optimization. This study explores supervised ML-based approaches for designing circuit parameters from performance specifications across various circuit types, including homogeneous and heterogeneous designs. By evaluating diverse ML models, from neural networks like transformers to traditional methods like random forests, we identify the best-performing models for each circuit. Our results show that simpler circuits, such as low-noise amplifiers, achieve exceptional accuracy with mean relative errors as low as 0.3% due to their linear parameter-performance relationships. In contrast, complex circuits, like power amplifiers and voltage-controlled oscillators, present challenges due to their non-linear interactions and larger design spaces. For heterogeneous circuits, our approach achieves an 88% reduction in errors with increased training data, with the receiver achieving a mean relative error as low as 0.23%, showcasing the scalability and accuracy of the proposed methodology. Additionally, we provide insights into model strengths, with transformers excelling in capturing non-linear mappings and k-nearest neighbors performing robustly in moderately linear parameter spaces, especially in heterogeneous circuits with larger datasets. This work establishes a foundation for extending ML-driven design automation, enabling more efficient and scalable circuit design workflows.", "sections": [{"title": "I. INTRODUCTION", "content": "Analog and radio-frequency (RF) circuit design serves as a cornerstone of modern electronic systems, enabling advancements in communication, sensing, and signal processing applications. These circuits are integral to technologies such as 5G wireless communications [1], automotive radar systems [2], subsurface imaging [3], low-cost IoT devices [4], large-scale quantum computing [5] and terahertz (THz) spectroscopy [6]. However, as the semiconductor industry confronts the limits of Moore's Law [7], the design of analog circuits has become increasingly challenging. Unlike digital circuits, which benefit from highly automated and scalable design methodologies [8]\u2013[10], analog circuits demand extensive manual effort due to intricate trade-offs between design metrics such as power consumption, bandwidth, gain and noise [11]\u2013[13]. This complexity is further exacerbated by the need to select suitable circuit topologies and optimize parameters to meet tight performance specifications, a process that often spans weeks or months and requires substantial domain expertise [13], [14]. These challenges underscore the critical need for innovative approaches to streamline analog circuit design.\nThe application of machine learning (ML) techniques in analog circuit design has gained significant momentum in recent years. By learning the complex relationships between design specifications (e.g., power consumption, bandwidth) and circuit parameters (e.g., transistor sizes, capacitance), ML models have shown potential to reduce the exhaustive parameter sweeps required for circuit optimization [14]\u2013[16]. Early works such as BagNet [14] and AutoCkt [15] demonstrated the potential of neural networks and reinforcement learning in synthesizing circuit parameters, reducing manual design effort. However, these methods often focus on simple, proof-of-concept circuits like single-stage amplifiers and fail to scale effectively to real-world, multi-block systems.\nMore recent advancements have targeted complex analog systems. AnGeL [16] proposed a semi-supervised learning framework that integrates neural networks to improve data efficiency and accelerate circuit parameter optimization. Similarly, GCN-RL [17] utilized graph neural networks (GNNs) to capture the topological relationships within circuits and reinforcement learning to optimize transistor sizing, enabling better generalization across circuit types.\nIn addition, [18] introduced a framework for designing analog circuits to meet threshold specifications using a combination of supervised learning and neural network-based optimization. Their approach focuses on ensuring that generated designs meet critical performance metrics, providing a practical solution for the accurate synthesis of analog circuits. Despite these advances, many methods remain limited to specific circuit types or lack comprehensive datasets, emphasizing the need for standardized benchmarks for robust model evaluation and scalability testing.\nTo address these challenges, AICircuit [19] introduced a comprehensive benchmark dataset for ML-assisted analog circuit design. This dataset encompasses homogeneous circuits, such as amplifiers and mixers, as well as heterogeneous systems like transmitters and receivers, which consist of cascaded circuit blocks with distinct functionalities. AICircuit evaluated multiple ML algorithms, including traditional regressors (e.g., random forests, support vector regression) and modern deep learning models (e.g., transformers, multi-layer perceptrons), revealing their strengths and limitations in learning complex"}, {"title": "II. PROBLEM STATEMENT", "content": "The design of analog and radio-frequency (RF) circuits is a challenging process that often requires iterative parameter tuning to meet desired performance specifications. Traditional approaches, such as Bayesian optimization, have been explored to optimize circuit performance by guiding the parameter search using probabilistic models that balance exploration and exploitation [20], [21]. While Bayesian optimization can be effective for low-dimensional problems, it suffers from significant limitations when applied to circuit design. The iterative sampling process is computationally expensive, requiring substantial resources to evaluate the design space. Additionally, Bayesian optimization struggles with the complex and non-linear relationships inherent in circuit parameter spaces, making it impractical for real-world applications.\nReinforcement learning (RL) has also been explored as a means of automating circuit design by iteratively learning parameter updates through a reward system based on achieving performance targets [22]. However, RL-based methods suffer from significant drawbacks: they require vast amounts of training data, are computationally intensive, and demand considerable resources to converge. Furthermore, RL's reliance on iterative exploration of the design space makes it time-consuming, particularly in circuit design tasks where data generation through simulation is expensive.\nIn contrast to these previous works, supervised ML algorithms offer a more efficient alternative by leveraging pre-collected datasets to learn a direct mapping from performance specifications to circuit parameters, thereby eliminating the need for iterative searches and reducing computational overhead with greater scalability.\nTo build upon this, we adopt a new perspective on the problem by reversing the conventional circuit design flow. Instead of iteratively searching for circuit parameters to optimize performance metrics, we directly model the relationship between performance specifications and circuit parameters. Specifically, given a desired performance vector \u00e6 (e.g., power consumption, bandwidth, gain), we predict a corresponding set of circuit parameters y (e.g., resistances, capacitances, transistor widths) using a machine learning model M. This approach bypasses the need for computationally expensive optimization techniques and allows for efficient design:\n$$y = M(x),$$(1)\nThe challenge of this problem lies in the fact that there may exist multiple sets of circuit parameters y that satisfy the same performance specifications \u00e6. However, in real-world scenarios, the primary goal is not to identify all possible solutions but rather to find a single set of parameters that meets the desired performance requirements. This simplifies the design process and ensures the practicality of the solution for circuit designers.\nIn short, our approach leverages supervised machine learning to simplify the circuit design process significantly:\nReverse Design Flow: ML-assisted design predicts circuit parameters from performance specifications, avoiding the need for parameter sweeps or optimization routines.\nDiverse Models: We explore various ML models, including multilayer perceptrons (MLPs), transformers, and traditional methods such as decision trees, to learn the mapping from specifications to circuit parameters.\nCircuit Diversity: We investigate the performance of these models on a range of circuits, from fundamental homogeneous blocks such as voltage amplifiers to heterogeneous systems like wireless transmitters.\nBy framing the problem as a direct mapping from performance to parameters, we aim to overcome the limitations of traditional optimization-based methods and provide a robust and scalable approach to analog and RF circuit design."}, {"title": "A. Analytical ML Perspective", "content": "Let (x,y) ~ P(x, y) represent i.i.d. samples from a joint distribution over inputs and outputs. The supervised learning objective aims to solve\n$$\\theta^* = \\arg \\min_\\theta E_{(x,y)\\sim P(x,y)}[l(f_\\theta(x), y)],$$(2)\nwhere $$f_\\theta(x)$$ represents the ML model parameterized by \u03b8, mapping input x to its predicted output, and $$l : \\Upsilon \\times \\Upsilon \\rightarrow$$"}, {"title": "B. Application to Circuit Schematic Design", "content": "Consider the problem of sizing parameters in a circuit schematic. Let x represent the performance metrics (e.g., gain, bandwidth, power consumption) and y represent the design parameters (e.g., transistor dimensions, bias voltages). By performing a parametric sweep over y, we obtain a dataset of pairs {(xi, Yi)}, effectively mapping each performance metric set to its associated parameter settings. The goal is to predict y (design parameters) given a performance specification $$x_{target}$$, which naturally aligns with supervised learning.\nSpecifically, let $$f_\\theta(x)$$ represent a model that predicts the circuit's parameter settings y given performance metrics x. This framework leverages the known performance metrics $$x_{target}$$. Each sampled (x, y) pair, obtained from circuit simulations, provides a direct gradient signal that adjusts 0 to optimize configurations yielding parameters y that achieve the desired performance $$x_{target}$$. The data generation process (parameter sweeps) ensures (x, y) pairs reflect the circuit's underlying physics and variability, granting a stable and interpretable mapping from performance metrics to parameters.\nIn contrast, unsupervised learning would only see the inputs and outputs without a specific target. The model might find correlations or clusters in the design parameter space, but these patterns do not necessarily indicate optimal design performance. Without a target specification, identifying which configurations in x-y space correspond to the desired operational point remains ambiguous.\nRL would treat circuit sizing as a sequential decision-making process, evaluating returns based on rewards. However, parameter sizing for circuit design is often a one-shot problem, where a given x (performance metrics) and y (design parameters) pair is fully observable from a single simulation, and there is no temporal sequence of actions. Introducing RL would add unnecessary complexity, requiring policy exploration, handling delayed or sparse rewards, and incurring a higher computational cost. Since the environment (circuit simulations) is static and deterministic, directly fitting a model to map x to y via supervised learning is more computationally efficient and theoretically direct.\nThus, for circuit parameter sizing with performance targets, supervised learning provides a structured, data-driven route to optimal solutions, bypassing the ambiguities of unsupervised criteria and the complexity of RL-based exploration strategies."}, {"title": "III. DATASET", "content": "This section describes the dataset collection process, which encompasses both homogeneous and heterogeneous circuits. To provide a comprehensive understanding of the data, we first define the characteristics of homogeneous and heterogeneous circuits. Subsequently, we detail the procedure used to generate the dataset for both categories."}, {"title": "A. Datasets for Homogeneous Circuits", "content": "Homogeneous circuits consist of a single type of circuit block, such as a common-source voltage amplifier (CSVA). These circuits are designed to perform a specific function, making them simpler in structure and easier to analyze compared to heterogeneous circuits. To capture a diverse range of use cases, the dataset includes seven commonly used analog and RF circuits: common-source voltage amplifier (CSVA), cascode voltage amplifier (CVA), two-stage voltage amplifier (TSVA), low-noise amplifier (LNA), mixer, voltage-controlled oscillator (VCO), and power amplifier (PA). These circuits, whose schematics are shown in Figure 1, represent fundamental building blocks for complex systems.\nAmong these circuits, the CSVA, CVA, and TSVA serve as essential components for voltage amplification in analog and feedback systems [23]. CSVA amplifies the input signal at the gate and generates an output at the drain. CVA, which combines common-source and common-gate stages, offers enhanced gain and bandwidth suitable for high-frequency applications. TSVA improves output swings and gain by employing a two-stage amplification structure.\nFor RF applications, the dataset also includes LNAs, Mixers, VCOs, and PAs, all of which are essential building blocks in innovative and advancing wireless systems [24]. The LNA amplifies weak input signals while maintaining low noise across a wide bandwidth, making it indispensable in RF receiver front-ends. Mixers are responsible for frequency upconversion and downconversion in RF transmitters and receivers, respectively. VCOs generate periodic signals with tunable frequency, utilizing various topologies to provide negative resistances, e.g., the cross-coupled configuration. PAs deliver substantial power to transmitting antennas while ensuring high efficiency."}, {"title": "B. Datasets for Heterogeneous Circuits", "content": "In addition to homogeneous circuits, we also investigate heterogeneous systems, which comprise multiple circuit blocks with distinct functionalities, such as a transmitter incorporating VCO and PA. These circuits are more complex, as they integrate different components to achieve multifunctional objectives. This provides a richer dataset for analyzing interactions between diverse circuits and their combined performance.\nSpecifically, we study a transmitter and a receiver system operating at 28 GHz, commonly used in high-speed communication systems [25]. These systems combine blocks such as VCOs, PAs, LNAs, Mixers, and CVAs. In the transmitter system (Figure 2a), the VCO generates a periodic signal with a tunable frequency governed by a control voltage, which is then amplified by the PA to deliver sufficient power for signal transmission. In the receiver system (Figure 2b), an LNA amplifies the weak input signal while minimizing noise. A Mixer converts the signal from radio frequency to intermediate frequency (IF), and a CVA provides further amplification for signal processing [13], [26]."}, {"title": "C. Dataset Collection Procedure", "content": "To generate the dataset, we followed the procedure illustrated in Figure 4. First, a schematic for each circuit was designed using a 45 nm CMOS technology in Cadence Virtuoso [27]. Then, key design parameters were identified for each circuit, and sweeping ranges for these parameters were defined in the format [beg, increment, end]. For every parameter combination, simulations were performed using Cadence Virtuoso to calculate the corresponding performance metrics. Finally, the results were stored as rows in the dataset table, where each row consists of circuit parameters and their performance metrics.\nCompared to homogeneous circuits, heterogeneous systems feature larger parameter spaces, highly non-linear interactions between individual blocks, and intricate trade-offs within the blocks and across the entire system. These complexities significantly increase the challenge for ML models to learn accurate mappings and necessitate a larger number of data points to ensure robust and reliable performance."}, {"title": "IV. METHODOLOGY", "content": "The proposed design methodology, illustrated in Figure 5, consists of three primary stages. The first stage is schematic design, where circuit schematics are designed and key parameters are identified. In the second stage, ML model, supervised machine learning models are trained to map performance specifications to circuit parameters using the dataset generated from extensive simulations (Section III). Finally, in the simulation and validation stage, the predicted parameters are simulated to validate the model's accuracy and ensure that the performance metrics meet the desired specifications. This end-to-end pipeline significantly reduces the manual effort traditionally required in analog and RF circuit design by leveraging the predictive power of machine learning models.\nIn this section, we present the methodology for predicting circuit parameters from performance specifications using ML techniques. Our approach focuses on three main aspects: the choice of models, the evaluation metrics, and the end-to-end training and evaluation process. First, we describe the ML models used to learn the mapping from performance to circuit parameters, including neural networks and conventional regression methods. Next, we outline the evaluation metrics employed to assess the accuracy, robustness, and reliability of the models, ensuring a comprehensive analysis of their performance across various circuit types. Finally, we introduce the end-to-end training and simulation-based evaluation pipeline, which integrates ML predictions with circuit simulation tools to validate the performance of predicted parameters."}, {"title": "A. Models", "content": "In this section, we describe the ML models employed to predict circuit parameters y from input performance specifications x. These models include advanced deep learning techniques and conventional regression methods chosen for their ability to capture the complex, non-linear relationships inherent in analog and RF circuit design. Specifically, we utilize the multi-layer perceptron (MLP), transformer, random forest (RF), k-nearest neighbors (kNN), and support vector regression (SVR) models. Each model approximates the mapping y = M(x) (Equation 1) with unique mathematical formulations and architectures suited to specific challenges in circuit design. The key parameters for these models are summarized in Table IV, and their detailed descriptions are provided below.\n1) Multi-Layer Perceptron (MLP): MLP is a feedforward neural network that approximates the mapping y = M(x) by stacking multiple layers of linear transformations and non-linear activation functions:\n$$y = M(x) = W^{(L)} z^{(L-1)} + b^{(L)},$$\n$$z^{(l)} = \\sigma(W^{(l)} z^{(l-1)} + b^{(l)}), l = 1, . . ., L - 1,$$\n$$z^{(0)} = x.$$\nHere, $$W^{(l)}$$ and $$b^{(l)}$$ are the weight matrices and bias vectors for layer l, $$z^{(l)}$$ is the activation at layer l, and $$\\sigma$$ represents the non-linear activation function (e.g., ReLU). The depth and flexibility of MLP allow it to approximate highly non-linear relationships between x and y. MLP's strength lies in its ability to handle complex mappings, though it may be prone to overfitting with small datasets.\n2) Transformer: The Transformer [28] leverages a self-attention mechanism to model complex dependencies between components of the performance vector y. It implements M as:\n$$y = M(x) = W_{out}FFN(\\text{MultiHead}(xQ,xK,xV)),$$\nwhere $$xQ, xK$$, and $$xV$$ are the query, key, and value matrices derived from \u00e6, and the multi-head attention mechanism is defined as:\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O,$$\nwith each attention head computed as:\n$$head_i = \\text{softmax}(\\frac{Q_iK_i^T}{\\sqrt{d_k}})V_i.$$\nHere, FFN represents the feedforward layers applied after the attention mechanism [29]. The transformer's ability to capture both local and global dependencies in \u00e6 makes it well-suited for complex mappings.\n3) Random Forest Regressor (RF): Random Forest (RF) approximates the mapping y = M(x) using an ensemble of decision trees [30]:\n$$y = M(x) = \\frac{1}{N} \\sum_{i=1}^N T_i(x),$$\nwhere $$T_i$$ represents the output of the i-th decision tree trained on random subsets of data, and N is the number of trees. RF excels in modeling non-linear relationships with robustness to noise and overfitting, making it practical for circuit design.\n4) k-Nearest Neighbors Regressor (kNN): kNN implements M by averaging the parameters of the k closest samples in the training set:\n$$y = M(x) = \\frac{1}{k} \\sum_{i \\in N_k(x)} Y_i,$$\nwhere $$N_k(x)$$ denotes the indices of the k nearest neighbors of x. This method captures localized mappings, though its computational cost scales with the dataset size [31].\n5) Support Vector Regressor (SVR): SVR solves a constrained optimization problem for each circuit parameter y to find a hyperplane that fits x to y within a margin e, approximating y as wfx+bj. Here, wj and bj are computed by solving:\n$$\\arg \\min_{w,b} \\frac{1}{2} || w ||^2 + C \\sum_{i=1}^N \\max(0, |y_i - (wx_i + b)| - \\epsilon),$$\nwhere N is the number of training data points, w and b define the hyperplane, and C controls the trade-off between margin width and error tolerance [32]. Using kernel functions, SVR maps \u00e6 into a higher-dimensional space, enabling it to handle non-linear mappings effectively [33]. These models collectively address the challenges of mapping x to y, leveraging their unique strengths to tackle diverse circuit complexities. However, a key limitation of SVR is that, unlike other models that jointly learn multiple outputs (multiple circuit parameters in our case), SVR does not consider dependencies between outputs, which can negatively impact accuracy."}, {"title": "B. Metrics", "content": "To evaluate the performance of our machine learning models, we adopt a two-step evaluation approach. First, we use the predicted circuit parameters as inputs to the Cadence simulation tool to obtain the corresponding performance metrics. These simulated performance values, denoted as 2, are compared against the desired performance metrics x specified in the dataset.\nThe primary evaluation metric is the relative error, which quantifies the deviation between the predicted performance $$\\hat{x}_i$$ and the target performance $$x_i$$:\n$$ \\text{ith Performance Error} = \\frac{|x_i - \\hat{x}_i|}{x_i}. $$(3)\nTo assess the overall accuracy of the predicted parameters for a given circuit, we compute the mean relative error across all performance metrics as:\n$$ \\text{Mean Relative Error} = \\frac{1}{N} \\sum_{i=1}^N \\frac{|x_i - \\hat{x}_i|}{x_i}, $$(4)\nwhere N is the total number of performance metrics.\nThis aggregated performance metric provides a single value that summarizes the accuracy of each predicted parameter set.\na) Error Distribution Analysis: In addition to computing the mean relative error, we analyze the distribution of errors by plotting histograms of the relative errors for each model. These histograms provide insights into the variability of prediction accuracy and highlight the models' tendencies to produce outliers or consistently accurate predictions.\nb) Comparison Metrics: To compare the performance of models with each other and across different circuits, we report the following key statistical metrics derived from the error distributions:\nMean and Standard Deviation: The mean relative error and its standard deviation summarize the central tendency and spread of errors, respectively.\n75th Percentile (P75): This metric indicates the relative error threshold below which 75% of all errors fall. A lower P75 value reflects better overall performance.\n90th Percentile (P90): Similar to P75, P90 reflects the threshold for 90% of errors, providing insights into the upper error bounds.\n% of Errors Below Thresholds: We report the percentage of errors smaller than 2% and 5% to measure how frequently models achieve high accuracy.\nOutlier Percentage (% > 20%): This metric highlights the proportion of predictions with large errors (greater than 20%), which is critical for identifying unreliable predictions.\nEach of these metrics captures different aspects of the error distribution, allowing for a comprehensive comparison of model performance. For example, the mean and P75 provide insights into general accuracy, while the outlier percentage highlights model robustness in avoiding large prediction errors."}, {"title": "C. Model Training and Simulation-Based Evaluation", "content": "Our codebase provides a seamless end-to-end model training and evaluation pipeline, as illustrated in Algorithm 1 and 2. This pipeline ensures smooth interaction between the ML workflow and the simulation process, enabling accurate and efficient evaluation of predicted circuit parameters.\nDuring the training stage, we follow the standard ML workflow. The input to the ML model consists of performance specifications, and the target output is the corresponding circuit parameters. The model is trained to minimize the $$l_1$$ loss, which measures the distance between predicted and actual circuit parameters."}, {"title": "Algorithm 1 End-to-End Model Training Pipeline", "content": "Require: Training dataset $$D_{train} = \\{(x, y)\\}$$, machine learning model M, maximum iterations maxIter, loss function L.\nEnsure: Trained model M.\nInitialize model parameters of M.\nfor t = 1 to maxIter do\nLoad data: Sample a mini-batch of b training pairs $$\\{(x_j, y_j)\\}_{j=1}^b$$ from $$D_{train}$$.\nfor each training pair (xj, yj) in the mini-batch do\nPredict parameters: Use the current model to predict the circuit parameters:\n$$\\hat{y}_j = M(x_j)$$.\nCompute loss: Evaluate the loss between the predicted and ground truth circuit parameters:\n$$Loss_j = L(\\hat{y}_j, y_j)$$.\nend for\nUpdate model: Compute the gradient of the average loss over the mini-batch:\n$$\\text{Lbatch} = \\frac{1}{b} \\sum_{j=1}^b \\text{Loss}_j.$$\nPerform a gradient update on the model parameters to minimize Lbatch.\nend for\nReturn the trained model M."}, {"title": "Algorithm 2 End-to-End Evaluation Pipeline", "content": "Require: Trained model M, evaluation dataset $$D_{eval} = \\{x\\}$$, circuit simulator S, maximum iterations maxIter.\nEnsure: Relative error list E.\nInitialize relative error list $$\\& \\leftarrow \\{\\}$$.\nfor t = 1 to maxIter do\nLoad data: Select input performance vector x = $$\\{x_1,x_2,..., x_N\\}$$ \u2208 $$D_{eval}$$.\nPredict circuit parameters: Use the trained model M to predict:\n$$\\hat{y} = M(x)$$.\nSimulate performance: Run the circuit simulator S to generate simulated performance:\n$$\\hat{x} = S(y) = \\{\\hat{x}_1,\\hat{x}_2,...,\\hat{x}_N\\}$$.\nCompute errors: For each performance metric i = 1,..., N, compute the relative error:\n$$e_i = \\frac{|x_i - \\hat{x}_i|}{x_i}$$.\nAppend $$\\{e_1, e_2, ..., e_N\\}$$ to the error list E.\nend for\nReturn relative error list & and compute aggregated metrics (e.g., mean relative error, histograms)."}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate ML models for predicting circuit parameters from performance metrics, categorizing the circuits into two groups: homogeneous circuits and heterogeneous circuits. For each circuit, we analyze the accuracy and robustness of the ML models by comparing their predictions against simulation results and reporting evaluation metrics, including relative error distributions, mean error, error percentiles, and outlier percentages. This circuit-by-circuit analysis highlights the strengths and limitations of ML models for different circuit complexities and performance requirements."}, {"title": "VI. DISCUSSION", "content": "The performance of ML models in predicting circuit parameters reflects a balance between circuit complexity and model capability. Simpler circuits with linear parameter-performance relationships tend to yield lower errors, whereas complex circuits with non-linear interactions and intricate trade-offs present greater challenges. Aligning model selection with the unique characteristics of each circuit and understanding their structural differences are crucial for achieving robust and reliable design automation in analog and RF circuits.\nThe performance trends summarized in Table XIV highlight the relationship between circuit complexity and model accuracy. Among homogeneous circuits, more linear designs like the LNA exhibit the lowest mean relative errors, distributed to their straightforward physical structures and linear parameter-performance relationships. For example, the LNA'S linear amplification characteristics enable models like MLP to accurately map performance metrics to parameters, achieving near-perfect alignment with training goals.\nConversely, complex circuits like the PA and VCO exhibit higher mean relative errors due to their intricate trade-offs and non-linear behaviors. The PA's challenges arise from its complexity, with MLP emerging as the best model for its ability to capture non-linear interactions, albeit with higher variability indicated by a large standard deviation. For the VCO, the RF model excels by handling non-linear relationships, which proves effective despite the circuit's expansive design space and inherent randomness in metrics like phase noise.\nThese results highlight the importance of aligning model capabilities with the physical and functional characteristics of each circuit. The best-performing model is determined not only by statistical accuracy but also by the design complexities and non-linearities inherent in the circuit's structure.\nFor heterogeneous circuits, the complexity increases due to a larger number of parameters to predict and a corresponding rise in the number of performance metrics. This expansion results in a larger design space. To address this challenge and maintain high prediction accuracy, we increase the number of data points during dataset generation. The Receiver benefits from kNN's ability to efficiently handle its moderately linear parameter space, made possible by the abundance of samples distributed across different regions of the space. Similarly, the Transmitter is best modeled by both MLP and kNN. The MLP excels in managing multi-block complexities, while KNN effectively captures the slight linearity introduced by the extensive sampling of the parameter space.\nTo demonstrate the scalability of our approach,\nBuilding on the discussion of scalability, the analytical formulation provides insight into the structural differences between homogeneous and heterogeneous circuits, shedding light on their respective data and modeling requirements.\nA homogeneous circuit implements a single non-linear mapping:\n$$M:R^N \\rightarrow R^D, x \\mapsto y,$$(5)\nwhere x and y represent input and output vectors, respectively. Under supervised learning, given samples (x,y) ~ P(x,y), the training objective follows (2). Due to the single-stage mapping and moderate dimensionality, fewer samples are typically required for accurate predictions.\nIn contrast, a heterogeneous circuit consists of multiple coupled mappings, for example, three sub-blocks with mappings M1, M2, M3, each defined as:\n$$M_i:R^{n_i} \\rightarrow R^{d_i}, i = 1,2,3.$$(6)\nThese mappings combine into a total mapping:\n$$M_{total}(X) = f(M_1(x_1), M_2(x_2), M_3(x_3)),$$(7)\nwhere X = [X1;X2; X3], with overall input dimensionality N = \u2211ni and output dimensionality D = \u2211di. Although the supervised learning objective remains as (2), the increased complexity, non-linear interactions, and higher dimensionality D require larger datasets and more expressive models to achieve reliable predictions.\nTable XV summarizes the model-centric comparison for homogeneous and heterogeneous circuits. For homogeneous circuits, the single-stage mapping M and moderate input dimensionality N enable effective training with fewer data samples. In contrast, for heterogeneous circuits, the multi-stage structure, higher dimensionality N, and intricate interdependencies necessitate larger datasets or more expressive non-linear models to accurately capture the complex interactions."}, {"title": "VII. CONCLUSION", "content": "This work introduces an ML-based framework for predicting circuit parameters from performance specifications, addressing both homogeneous and heterogeneous analog and RF circuits. The study highlights the impact of model selection on prediction accuracy and robustness, with Transformers and MLPs demonstrating strong performance in capturing complex relationships. Moreover, evaluation metrics, such as mean relative error, effectively measured prediction accuracy, while results emphasized the importance of aligning models with circuit complexities and operational characteristics.\nFuture directions include extending the framework to incorporate layout optimization, integrating uncertainty quantification for robust predictions, and exploring generative models to generate diverse parameter sets that meet performance specifications. This research lays the groundwork for advancing ML-driven circuit design, streamlining workflows, and addressing the growing complexities of modern electronic systems."}]}