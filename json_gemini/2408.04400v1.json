{"title": "DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization", "authors": ["Xin Sun", "Liang Wang", "Qiang Liu", "Shu Wu", "Zilei Wang", "Liang Wang"], "abstract": "This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, our study introduces a new learning paradigm for graph OOD issue. We propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of graph machine learning has opened up a myriad of opportunities and challenges, particularly in the realm of distribution shift between source and target data. Most existing graph learning algorithms work under the statistical assumption that the training and test data are drawn from the same distribution. However, this assumption does not hold in a lot of real-word scenarios, where the source data fails to adequately represent the target domain's characteristics, leading to suboptimal performance and generalization issues.\nSimplicity bias degrades generalization ability. Neural networks trained using Stochastic Gradient Descent (SGD) have been recently demonstrated to exhibit a preference for simple features, while neglecting equally predictive or even more predictive com-plex features [29]. This simplicity bias hinders the learning of complex patterns that constitute the core mechanisms of the task of interest. When these simple patterns are merely spurious correlations [31, 55], the model's out-of-distribution (OOD) performance significantly deteriorates. For instance, in image recognition, a typical example of a spurious correlation is the reliance on the background instead of the object's shape. In natural language understanding, it manifests as the preference for specific words rather than grasping the sentence's overarching meaning. In graph-based tasks, a notable example is the focus on a molecule's scaffold rather than the functional groups that are actually important. Due to the mechanism of message passing, structural patterns with higher degrees and higher modularity are likely to receive more attention [22, 30, 37], resulting in the scaffold subgraph being simpler to learn. More specifically, in the context of predicting water solubility, the presence of cyclic structures is not the actual determinant of solubility. Although molecules with cyclic structures typically exhibit poorer water solubility, the actual determinants of solubility are the polar functional groups that confer polarity to the molecule, such as hydroxyl and amino groups. However, for a model trained using stochastic gradient descent (SGD), focusing on cyclic structures within a graph is typically simpler than concentrating on polar functional group structures. This can lead the model to overemphasize simple and spurious features while neglecting the learning of complex but causal features, significantly affecting the model's generalization ability.\nThe pitfall of current methods under simplicity bias. To address the issue of failure in out-of-distribution generalization, the most effective and widely adopted strategy at present is based on subgraph-mixup [8, 14, 21, 45, 49]. Subgraph-mixup approach involves initially utilizing a subgraph extractor to identify the underlying invariant or causal subgraph, which maintains a consistent correlation with the target labels across various graph distributions from different environments. Subsequently, it combines the invariant subgraph with the spurious subgraph (the complement of invariant subgraphs) from another instance to augment the dataset and achieve improved results. Although these methods have achieved some empirical success, the faithfulness of the extracted invariant subgraph is questionable in the presence of simplicity bias. Specifically, when the spurious subgraph is the simpler pattern and is equally predictive of training labels, the subgraph extractor faces challenges in extracting the invariant subgraph due to the simplicity bias. If the estimated invariant subgraph parts include spurious information, assigning the label corresponding to the invariant part to the mixuped graph is likely to reinforce the spurious correlation between the spurious subgraph and the labels. Another line of work is based on information bottleneck (IB) [23, 52], achieving generalization by maximizing mutual information between labels and invariant subgraphs while minimizing mutual information between the subgraph and the entire graph. However, the IB method does not inherently distinguish between causal relevance and spurious correlation between the extract subgraph and label [12]. This limitation can lead the IB method to retain spurious information of the extracted subgraph. When the spurious subgraph is simpler pattern and equally predictive on the training set, this issue will become more severe.\nOur method. Due to the presence of the simplicity bias in the training procedure, the current method are unable to accurately identify the correct subgraphs. Besides, given the inherent characteristics of the SGD training method, the simplicity bias is difficult to avoid. To address this issue, we propose DIVE, training a collection of models to attend to all predictive graphs with DIVErsity regularization, which allows us to identify not only subgraphs with simple structural patterns but also those with complex patterns. Specifically, We propose training a collection of models of same architecture to fit the training data by focusing on different, yet label-predictive subgraphs. These label-predictive subgraphs encompass both spurious and invariant subgraphs. Each model undergoes optimization for standard empirical risk minimization (ERM), complemented by the use of a regularizer designed to penalize the overlap of extracted subgraphs across the collection. This strategy encourages each model to attend on diverse structural patterns within the graph data, rather than solely on the simplest ones. The process of identifying a model with robust OOD performance is reduced to an independent model selection step, for which we employ validation accuracy as the metric for model selection. Our method, tested across four datasets from the GOOD benchmark and one dataset from the DrugOOD benchmark, demonstrates significant improvement over existing approaches. Our main contributions can be summarized as follows:\n\u2022 We propose a novel paradigm to address the out-of-distribution issue in graph tasks by learning a collection of diverse predictors, which is robust to the simplicity bias.\n\u2022 We introduce diversity among models in the collection through a novel subgraph mask diversity loss that encourage different models attend to different predictive subgraph. And our method is capable of extracting the invariant subgraph more precisely than the current methods because of the subgraph diversity regularization.\n\u2022 We conduct comprehensive experiments on 5 datasets and the experimental results demonstrate the superiority of our method compared to the state-of-the-art approaches."}, {"title": "2 RELATED WORK", "content": "2.1 Diversity on Ensemble Models\nThe diversity on ensemble models has been extensively explored on visual task to solve the distribution shift problem. While the diversity stage similarly learns a collection for diverse models, our approach differs in that we directly optimize for diversity on subgraph. The bias-variance-covariance decomposition [40], which generalizes the bias variance decomposition to ensembles, shows how the error decreases with the covariances of the member of the ensemble. Despite its importance, there is still no well accepted definition and understanding of diversity, and it is often derived from prediction errors of members of the ensemble. This creates a conflict between trying to increase accuracy of individual predictors h, and trying to increase diversity. In this view, creating a good ensemble is seen as striking a good balance between individual performance and diversity. To promote diversity in ensembles, a classic approach is to add stochasticty into training by using different subsets of the training data for each predictor [3], or using different data augmentation methods [13, 34]. Another approach is to add orthogonality constrains on the predictor's gradient [16, 27, 38]. the information bottleneck [39] also has been used to promote ensemble diversity [26, 32]. Recently, Some work claims that diversity can be achieved by producing different prediction on out-of-distribution dataset [19, 24].\nHowever, the diversity on ensembles has rarely explored on graph out-of-distribution task. To this end, we propose to diversify a collection of models by allowing them to make different predictions on subgraph masks. Unlike the aforementioned methods, our approach, DIVE, can be trained on the full dataset and does not require any out-of-distribution data during training. Additionally, it does not impose constraints on the predictions of the classifier but instead fosters model diversity through disagreement on subgraph mask predictions. Furthermore, in contrast to many previous models, our individual predictors do not share the same encoder, enhancing the diversity and robustness of our approach.\nIt is noteworthy to mention that the paper is not about building ensembles. Ensembling means that the results from the diversity models are aggregated for inference. Rather, we train a collection of models and select on model for inference. The goal of ensembling is to combine models with uncorrelated errors into one of lower variance. Our goal is to discover all predictive patterns normally missed by the SGD learning because of the simplicity bias.\n2.2 Graph Out-of-Distribution Generalization\nGraph structure is ubiquitous in real world, such as molecular[44], protein [58], social networks[56] and knowledge graph[47, 48, 57]. Graph representation learning [5, 42, 59] achieves deep learning on graphs by encoding them into vector in a latent space. Despite their significant success, current Graph Neural Networks (GNNs) largely depend on the identically distributed (I.D.) assumption, meaning that the training and test data are drawn from the same distribution. However, in reality, various forms of distribution shifts often occur between training and testing datasets due to unpredictable data generation mechanisms, leading to out-of-distribution (OOD) scenarios.\nOur research focuses on graph classification, where methods for out-of-distribution generalization are primarily classified into three categories. The foremost and extensively investigated strategy hinges on the concept of subgraph-mixup [8, 14, 21, 45, 49]. The second category revolves around the principle of the information bottleneck [6, 7, 23, 52], achieving generalization by maximizing mutual information between labels and invariant subgraphs while minimizing mutual information between the subgraph and the entire graph. But as mentioned before, these two categories of methods fail to extract the correct invariant subgraph in the presence of simplicity bias. The last category is invariant learning [20, 51, 53]. These methods aim to find a invariant subgraph whose predictive relationship with the target values remains stable across different environments. However, these methods need environment labels which is often unavailable and expensive to obtain on graphs. Some methods propose to infer the environment labels [20, 51]. However, the reliability of these estimated labels is pivotal. If the environment label estimate induce a higher bias or noise, it would make the learning of graph invariant patterns even harder [6].\n2.3 Simplicity Bias\nDeep learning is rigorously investigated to decipher the reasons behind its notable successes and occasional failures. Key concepts such as the simplicity bias, gradient starvation, and the learning of functions of increasing complexity have been instrumental in shedding light on the inherent lack of robustness in deep neural networks. These insights explain why performance can significantly degrade under minor distribution shifts and adversarial perturbations. Shah et al. [29] revealed that neural networks trained with Stochastic Gradient Descent (SGD) exhibit a tendency to prefer learning the simplest predictive features within the data, often at the expense of more complex, yet more predictive ones. Alarmingly, methods believed to enhance generalization and robustness, such as ensembles and adversarial training, have been shown to be ineffective in counteracting the simplicity bias. Recently, a lot of diversity-based ensembles methods [13, 19, 24, 38] are proposed to solve the the simplicity bias and gain empirical success."}, {"title": "3 METHOD", "content": "3.1 Notions\nDenote an attributed graph as $G = (A, X)$, where $A = {0, 1}^{n\\times n}$ is the adjacent matrix and X includes node attributes. $A_{ij} = 1$ represents that there exists an edge between node i and j, and $A_{ij} = 0$ otherwise. The node set and the edge set can be denoted as V and E, respectively. We focus on graph-level out-of-distribution task and a dataset set of graphs can be denoted as $\\{(G_i, Y_i)\\}_{i=1}^{N}$, where N is the number of samples in the training set.\n3.2 Problem Formulation\nWe consider a supervised learning setting in which we train a model f that takes input $G \\in \\mathcal{G}$ and predicts its corresponding label $Y \\in \\mathcal{y}$, where $\\mathcal{G}$ and $\\mathcal{y}$ are graph space and label space respectively. Generally, we are given a set of datasets collected from multiple environments and each dataset $D_e$ contains pairs of input graph and its label: $D_e = \\{(G_i, Y_i)\\}_{i=1}^{N_e}$ drawn from the joint distribution $P_e(G, Y)$ of environment e. We define the training dataset as $D_{train}$ that are drawn from the joint distribution $P_{train}(G, Y) = P_{e\\in \\mathcal{E}_{train}}(G, Y)$, and the test dataset as $D_{test}$ that are drawn from the joint distribution $P_{test}(G, Y) = P_{e\\in \\mathcal{E}_{test}}(G, Y)$. We aims to find a optimal predictor $f^*$ that minimize $\\max_{e \\in \\mathcal{E}_{all}} R_e$, where $R_e$ is the empirical risk of f under environment e [1, 41].\n3.3 Learning all label-predictive subgraphs\nWe assume each graph instance $G_i$ consists of two parts of information, one part is invariant information, which is the determinants information of the task of interest. The other part is the spurious information. We assume the spurious information and the invariant information are all predictive to the labels of training set. While our method focus on the structural distribution shift, we only consider the structural label-predictive patterns. We assume that a graph instance can have multiple spurious subgraphs and one invariant subgraph. The set of spurious subgraphs is represented as $\\{G^1, G^2,..., G^{M_s}\\}$ and the invariant subgraph is denoted as $G^I$. Consequently, the set of all label-predictive subgraphs is symbolized as $\\mathcal{SG}_{G_i} = \\{G^1, G^2, \\dots, G^{M_s}, G^I\\}$. To learn all these label-predictive subgraphs, thus circumventing simplicity bias, we train a set $\\mathcal{F}$ of near-optimal predictor $f : \\mathcal{G} \\rightarrow \\mathcal{Y}$. Let $L_P : \\mathcal{F} \\rightarrow \\mathbb{R}$ be the risk with respect to $P_{train}(G, Y)$. The $\\epsilon$-optimal set with respect to $\\mathcal{F}$ as level $\\epsilon \\geq 0$ is defined as $\\mathcal{F}^\\epsilon = \\{f \\in \\mathcal{F}|L_p(f) \\leq \\epsilon\\}$. The predictor $f$ can be further decomposed to $h \\circ t$, where t is the label-predictive subgraph extractor and h is the subgraph classifier. The set of t can be denoted as $\\mathcal{T}$, and what we wish to learn is that $\\mathcal{T} (G_i) = \\mathcal{SG}_{G_i}$. To achieve this, it is necessary to foster the diversity of the learned subgraph set. Hence, we impose penalties for overlap among the learned subgraphs.\n3.4 Model Architecture\nThen, we describe the architecture of each model within the collection, where each model uniformly shares an identical architectural design and contributes to the learning objective.\n3.4.1 Predictive subgraph extractor. The first part is predictive subgraph extractor. We need to learn a graph mask matrix $M\\in \\{0, 1\\}^{N\\times N}$ to mask out the predictive subgraph and use it for the subsequent task. It is noteworthy to mention that we are not demand the extractor to extract a invariant subgraph but any predictive subgraph, which can be spurious subgraph or invariant subgraph.\nWe first encodes the input graph G via a GNN into a set of node representations $\\{z_i\\}_{v_i \\in V}$. For each edge $(v_i, v_j) \\in E$, an MLP layer equipped with a sigmoid function is employed to map the concatenated representation of node pair $(z_i, z_j)$ into the masking probability of the edge between them $p_{ij} \\in [0, 1]$:\n$Z = [\\dots, z_i, \\dots, z_j, \\dots]^\\top = GNN_{mask}(G) \\in \\mathbb{R}^{n \\times d},$ (1)\n$p_{ij} = \\sigma(MLP_{mask}([z_i, z_j])),$ (2)\nwhere d denotes the hidden dimension, $\\sigma(\\cdot)$ denotes the sigmoid function, and [...] denotes the concatenation. For molecule dataset, the edge representation $e_{ij}$ is also introduced to calculate the $p_{ij}$:\n$p_{ij} = \\sigma(MLP_{mask}([z_i \\oplus l_{ij}, z_j \\oplus e_{ij}]),$ (3)\nwhere $\\oplus$ denotes the element-wise sum of vectors.\nConsequently, in each forward pass of the training process, we extract a predictive subgraph by sampling from Bernoulli distributions, denoted as $m_{ij} \\sim Bern(p_{ij})$. Due to the inherent non-differentiability of Bernoulli sampling, direct sampling from $Bern(p_{ij})$ can not be optimized. To ensure the gradient of $m_{ij}$ remains calculable, we apply the Gumbel-Sigmoid technique for sampling as:\n$q_{ij} = Gumbel - Sigmoid(p_{ij}) = \\sigma(\\frac{log(p_{ij}) + G}{ \\tau}),$ (4)\n$d_{ij} = \\begin{cases} 1 \\text{ if } q_{ij} > 0.5, \\\\ 0 \\text{ if } q_{ij} \\leq 0.5, \\end{cases}$ (5)\n$m_{ij} = d_{ij} + p_{ij} - p_{ij}$ (6)\nwhere $G = -log(-log(U))$ represents the Gumbel distribution, in which $U \\sim Uniform(0, 1)$. Given the non-differentiable nature of $q_{ij}$, we implement the straight-through trick (as delineated in equation (6)) to confer a gradient onto $m_{ij}$. The symbol $\\cent$\\cent signifies the cessation of gradient propagation.\nThe extracted predictive subgraph can be denoted as an induced adjacent matrix $A_P = M \\odot A$, where M denotes the learned mask matrix, composed of elements $m_{ij}$. A is the adjacent matrix of original graph G, and $\\odot$ symbolizes the element-wise multiplication. The subgraph corresponding to $A_P$ is denoted as $G_P$.\n3.4.2 Subgraph encoder and classifier. After obtaining the predictive subgraph $G_P$, we train a GNN model to map the induced subgraph into representation $h_g$, which is fed into the following MLP layer to conduct classification or regression. Formally,\n$H = [h_1, \\dots, h_n] = GNN_{feat}(G_P),$ (7)\n$h_g = READOUT(H),$ (8)\n$\\hat{y} = MLP(h_g) \\in \\mathcal{Y}.$ (9)\n3.4.3 Main task loss. The main task loss can be denoted as:\n$L_{main} = R(\\hat{Y}, Y),$ (10)\nwhere R represents the task-tailored loss function. For regression tasks, this function implemented with the mean squared error, whereas for classification tasks, it is the cross-entropy function.\n3.5 Diversity via Subgraph Disagreement\nTo infuse diversity into the models within the collection, we purposefully advocate for each model to focus on distinct subgraphs. Assume the collection contain m models, the set of predictive masks corresponding to each model can be denoted as\n$\\mathcal{SM} = \\{M_1,..., M_m\\}.$ (11)\nWe apply a jaccard loss as diversity regularizer to penalize the overlapping of each pair of predictive mask in the set:\n$L_d = \\sum_{i,j} \\frac{|M_i \\cap M_j|}{|M_i \\cup M_j|},$ (12)\nwhere i, j \u2208 {1, 2, ..., m}, i \u2260 j are indices of different models.\n3.6 Learning Objective\nCombining main task loss on each model and diversity regularizer, the total loss of the collection containing m models is defined as:\n$L = \\sum_{i=1}^{m} - L_{main} + \\lambda L_d,$ (13)\nwhere $\\lambda$ is the hyper-parameter to control the weight of diversity regularization and we set it as 0.5 for all the experiments.\n3.7 Model Selection\nWe employ an OOD validation set for model selection, opting for the model that exhibits the highest validation accuracy across our collection for inference on the test set. The use of an OOD validation set has become a standard practice in contemporary graph-based OOD methods for model selection [6, 49]. In our experiment, the baseline results were chosen using the same validation set as our method, ensuring absolute fairness. We have also included the results using the in-distribution (ID) validation set to further demonstrate the efficacy of our methodologies. These results are available at appendix D."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct extensive experiments to answer the research questions. (RQ1): Can our method DIVE achieve better OOD generalization performance against SOTA baselines? (RQ2):Does our method extract subgraphs more accurately than the current methods? (RQ3): Does our regularization really bring diversity to the models in the collection? Is there a model in the collection capable of recognizing invariant structural patterns? (RQ4):What influence does the number of models have on the performance? (RQ5):Does our method robust to the weight of diversity loss?\n4.1 Experimental Setup\n4.1.1 Datasets. We employ GOOD and DrugOOD benchmark of graph OOD performance evaluation.\n\u2022 GOOD [11], GOOD is a systematic graph OOD benchmark. It contains two types of distribution shift, covariate shift and concept shift. In covariate shift, the distribution of input differs. Formally, $P_{train}(G) \\neq P_{test} (G)$ and $P_{train}(Y|G) = P_{test} (Y|G)$. While concept shift occurs when the conditional distribution changes as $P_{train}(Y|G) \\neq P_{test}(Y|G)$ and $P_{train}(G) = P_{test} (G)$. Since these two types of distribution shift both contain the spurious correlation, we consider both case in our experiments. We choose four graph-level datasets GOODMotif, GOODHIV, GOODZINC and GOODSST2 to evaluate the graph generalization ability. There are 4 domain in these 4 datasets: basis, scaffold, size and length. The details of the four datasets can be found at appendix A."}]}