{"title": "DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization", "authors": ["Xin Sun", "Liang Wang", "Qiang Liu", "Shu Wu", "Zilei Wang", "Liang Wang"], "abstract": "This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, our study introduces a new learning paradigm for graph OOD issue. We propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of graph machine learning has opened up a myriad of opportunities and challenges, particularly in the realm of distribution shift between source and target data. Most existing graph learning algorithms work under the statistical assumption that the training and test data are drawn from the same distribution. However, this assumption does not hold in a lot of real-word scenarios, where the source data fails to adequately represent the target domain's characteristics, leading to suboptimal performance and generalization issues.\nSimplicity bias degrades generalization ability. Neural networks trained using Stochastic Gradient Descent (SGD) have been recently demonstrated to exhibit a preference for simple features, while neglecting equally predictive or even more predictive complex features [29]. This simplicity bias hinders the learning of complex patterns that constitute the core mechanisms of the task of interest. When these simple patterns are merely spurious correlations [31, 55], the model's out-of-distribution (OOD) performance significantly deteriorates. For instance, in image recognition, a typical example of a spurious correlation is the reliance on the background instead of the object's shape. In natural language understanding, it manifests as the preference for specific words rather than grasping the sentence's overarching meaning. In graph-based tasks, a notable example is the focus on a molecule's scaffold rather than the functional groups that are actually important. Due to the mechanism of message passing, structural patterns with higher degrees and higher modularity are likely to receive more attention [22, 30, 37], resulting in the scaffold subgraph being simpler to learn. More specifically, in the context of predicting water solubility, the presence of cyclic structures is not the actual determinant of solubility. Although molecules with cyclic structures typically exhibit poorer water solubility, the actual determinants of solubility are the polar functional groups that confer polarity to the molecule, such as hydroxyl and amino groups. However, for a model trained using stochastic gradient descent (SGD), focusing on cyclic structures within a graph is typically simpler than concentrating on polar functional group structures. This can lead the model to overemphasize simple and spurious features while neglecting the learning of complex but causal features, significantly affecting the model's generalization ability.\nThe pitfall of current methods under simplicity bias. To address the issue of failure in out-of-distribution generalization, the most effective and widely adopted strategy at present is based on subgraph-mixup [8, 14, 21, 45, 49]. Subgraph-mixup approach involves initially utilizing a subgraph extractor to identify the underlying invariant or causal subgraph, which maintains a consistent correlation with the target labels across various graph distributions from different environments. Subsequently, it combines the invariant subgraph with the spurious subgraph (the complement of invariant subgraphs) from another instance to augment the dataset and achieve improved results. Although these methods have achieved some empirical success, the faithfulness of the extracted invariant subgraph is questionable in the presence of simplicity bias. Specifically, when the spurious subgraph is the simpler pattern and is equally predictive of training labels, the subgraph extractor faces challenges in extracting the invariant subgraph due to the simplicity bias. If the estimated invariant subgraph parts include spurious information, assigning the label corresponding to the invariant part to the mixuped graph is likely to reinforce the spurious correlation between the spurious subgraph and the labels. Another line of work is based on information bottleneck (IB) [23, 52], achieving generalization by maximizing mutual information between labels and invariant subgraphs while minimizing mutual information between the subgraph and the entire graph. However, the IB method does not inherently distinguish between causal relevance and spurious correlation between the extract subgraph and label [12]. This limitation can lead the IB method to retain spurious information of the extracted subgraph. When the spurious subgraph is simpler pattern and equally predictive on the training set, this issue will become more severe.\nOur method. Due to the presence of the simplicity bias in the training procedure, the current method are unable to accurately identify the correct subgraphs. Besides, given the inherent characteristics of the SGD training method, the simplicity bias is difficult to avoid. To address this issue, we propose DIVE, training a collection of models to attend to all predictive graphs with DIVErsity regularization, which allows us to identify not only subgraphs with simple structural patterns but also those with complex patterns. Specifically, We propose training a collection of models of same architecture to fit the training data by focusing on different, yet label-predictive subgraphs. These label-predictive subgraphs encompass both spurious and invariant subgraphs. Each model undergoes optimization for standard empirical risk minimization (ERM), complemented by the use of a regularizer designed to penalize the overlap of extracted subgraphs across the collection. This strategy encourages each model to attend on diverse structural patterns within the graph data, rather than solely on the simplest ones. The process of identifying a model with robust OOD performance is reduced to an independent model selection step, for which we employ validation accuracy as the metric for model selection. Our method, tested across four datasets from the GOOD benchmark and one dataset from the DrugOOD benchmark, demonstrates significant improvement over existing approaches. Our main contributions can be summarized as follows:\n\u2022 We propose a novel paradigm to address the out-of-distribution issue in graph tasks by learning a collection of diverse predictors, which is robust to the simplicity bias.\n\u2022 We introduce diversity among models in the collection through a novel subgraph mask diversity loss that encourage different models attend to different predictive subgraph. And our method is capable of extracting the invariant subgraph more precisely than the current methods because of the subgraph diversity regularization.\n\u2022 We conduct comprehensive experiments on 5 datasets and the experimental results demonstrate the superiority of our method compared to the state-of-the-art approaches."}, {"title": "2 RELATED WORK", "content": "2.1 Diversity on Ensemble Models\nThe diversity on ensemble models has been extensively explored on visual task to solve the distribution shift problem. While the diversity stage similarly learns a collection for diverse models, our approach differs in that we directly optimize for diversity on subgraph. The bias-variance-covariance decomposition [40], which generalizes the bias variance decomposition to ensembles, shows how the error decreases with the covariances of the member of the ensemble. Despite its importance, there is still no well accepted definition and understanding of diversity, and it is often derived from prediction errors of members of the ensemble. This creates a conflict between trying to increase accuracy of individual predictors h, and trying to increase diversity. In this view, creating a good ensemble is seen as striking a good balance between individual performance and diversity. To promote diversity in ensembles, a classic approach is to add stochasticty into training by using different subsets of the training data for each predictor [3], or using different data augmentation methods [13, 34]. Another approach is to add orthogonality constrains on the predictor's gradient [16, 27, 38]. the information bottleneck [39] also has been used to promote ensemble diversity [26, 32]. Recently, Some work claims that diversity can be achieved by producing different prediction on out-of-distribution dataset [19, 24].\nHowever, the diversity on ensembles has rarely explored on graph out-of-distribution task. To this end, we propose to diversify a collection of models by allowing them to make different predictions on subgraph masks. Unlike the aforementioned methods, our approach, DIVE, can be trained on the full dataset and does not require any out-of-distribution data during training. Additionally, it does not impose constraints on the predictions of the classifier but instead fosters model diversity through disagreement on subgraph mask predictions. Furthermore, in contrast to many previous models, our individual predictors do not share the same encoder, enhancing the diversity and robustness of our approach.\nIt is noteworthy to mention that the paper is not about building ensembles. Ensembling means that the results from the diversity models are aggregated for inference. Rather, we train a collection of models and select on model for inference. The goal of ensembling is to combine models with uncorrelated errors into one of lower variance. Our goal is to discover all predictive patterns normally missed by the SGD learning because of the simplicity bias."}, {"title": "2.2 Graph Out-of-Distribution Generalization", "content": "Graph structure is ubiquitous in real world, such as molecular[44], protein [58], social networks[56] and knowledge graph[47, 48, 57]. Graph representation learning [5, 42, 59] achieves deep learning on graphs by encoding them into vector in a latent space. Despite their significant success, current Graph Neural Networks (GNNs) largely depend on the identically distributed (I.D.) assumption, meaning that the training and test data are drawn from the same distribution. However, in reality, various forms of distribution shifts often occur between training and testing datasets due to unpredictable data generation mechanisms, leading to out-of-distribution (OOD) scenarios.\nOur research focuses on graph classification, where methods for out-of-distribution generalization are primarily classified into three categories. The foremost and extensively investigated strategy hinges on the concept of subgraph-mixup [8, 14, 21, 45, 49]. The second category revolves around the principle of the information bottleneck [6, 7, 23, 52], achieving generalization by maximizing mutual information between labels and invariant subgraphs while minimizing mutual information between the subgraph and the entire graph. But as mentioned before, these two categories of methods fail to extract the correct invariant subgraph in the presence of simplicity bias. The last category is invariant learning [20, 51, 53]. These methods aim to find a invariant subgraph whose predictive relationship with the target values remains stable across different environments. However, these methods need environment labels which is often unavailable and expensive to obtain on graphs. Some methods propose to infer the environment labels [20, 51]. However, the reliability of these estimated labels is pivotal. If the environment label estimate induce a higher bias or noise, it would make the learning of graph invariant patterns even harder [6]."}, {"title": "2.3 Simplicity Bias", "content": "Deep learning is rigorously investigated to decipher the reasons behind its notable successes and occasional failures. Key concepts such as the simplicity bias, gradient starvation, and the learning of functions of increasing complexity have been instrumental in shedding light on the inherent lack of robustness in deep neural networks. These insights explain why performance can significantly degrade under minor distribution shifts and adversarial perturbations. Shah et al. [29] revealed that neural networks trained with Stochastic Gradient Descent (SGD) exhibit a tendency to prefer learning the simplest predictive features within the data, often at the expense of more complex, yet more predictive ones. Alarmingly, methods believed to enhance generalization and robustness, such as ensembles and adversarial training, have been shown to be ineffective in counteracting the simplicity bias. Recently, a lot of diversity-based ensembles methods [13, 19, 24, 38] are proposed to solve the the simplicity bias and gain empirical success."}, {"title": "3 METHOD", "content": "3.1 Notions\nDenote an attributed graph as $G = (A, X)$, where $A = {0, 1}^{n \\times n}$ is the adjacent matrix and X includes node attributes. $A_{ij} = 1$ represents that there exists an edge between node i and j, and $A_{ij} = 0$ otherwise. The node set and the edge set can be denoted as V and E, respectively. We focus on graph-level out-of-distribution task and a dataset set of graphs can be denoted as ${ (G_i, Y_i) }_{i=1}^{N}$, where N is the number of samples in the training set.\n3.2 Problem Formulation\nWe consider a supervised learning setting in which we train a model f that takes input $G \\in G$ and predicts its corresponding label $Y \\in y$, where $G$ and y are graph space and label space respectively. Generally, we are given a set of datasets collected from multiple environments and each dataset $D_e$ contains pairs of input graph and its label: $D_e = { (G_i, Y_i) }_{i=1}^{N_e}$ drawn from the joint distribution $P_e(G, Y)$ of environment e. We define the training dataset as $D_{train}$ that are drawn from the joint distribution $P_{train}(G, Y) = P_{e \\in E_{train}}(G, Y)$, and the test dataset as $D_{test}$ that are drawn from the joint distribution $P_{test}(G, Y) = P_{e \\in E_{test}}(G, Y)$. We aims to find a optimal predictor $f^*$ that minimize $max_{e \\in E_{all}} R_e$, where $R_e$ is the empirical risk of f under environment e [1, 41].\n3.3 Learning all label-predictive subgraphs\nWe assume each graph instance $G_i$ consists of two parts of information, one part is invariant information, which is the determinants information of the task of interest. The other part is the spurious information. We assume the spurious information and the invariant information are all predictive to the labels of training set. While our method focus on the structural distribution shift, we only consider the structural label-predictive patterns. We assume that a graph instance can have multiple spurious subgraphs and one invariant subgraph. The set of spurious subgraphs is represented as $\\{G_1, G_2, ..., G_{e_k}\\}$ and the invariant subgraph is denoted as $G_1^k$. Consequently, the set of all label-predictive subgraphs is symbolized as $S_{G_i} = \\{G_1, G_2, ..., G_{e_k}, G_1^k\\}$. To learn all these label-predictive subgraphs, thus circumventing simplicity bias, we train a set F of near-optimal predictor $f : G \\rightarrow Y$. Let $L_p : F \\rightarrow R$ be the risk with respect to $P_{train}(G, Y)$. The $\\epsilon$-optimal set with respect to F as level $\\epsilon \\geq 0$ is defined as $F^{\\epsilon} = \\{f \\in F|L_p(f) \\leq \\epsilon\\}$. The predictor f can be further decomposed to $h \\circ t$, where t is the label-predictive subgraph extractor and h is the subgraph classifier. The set of t can be denoted as T, and what we wish to learn is that $T(G_i) = S_{G_i}$. To achieve this, it is necessary to foster the diversity of the learned subgraph set. Hence, we impose penalties for overlap among the learned subgraphs.\n3.4 Model Architecture\nThen, we describe the architecture of each model within the collection, where each model uniformly shares an identical architectural design and contributes to the learning objective.\n3.4.1 Predictive subgraph extractor. The first part is predictive subgraph extractor. We need to learn a graph mask matrix $M \\in {0, 1}^{N \\times N}$ to mask out the predictive subgraph and use it for the subsequent task. It is noteworthy to mention that we are not demand the extractor to extract a invariant subgraph but any predictive subgraph, which can be spurious subgraph or invariant subgraph.\nWe first encodes the input graph G via a GNN into a set of node representations ${ z_i }_{v_i \\in V}$. For each edge $(v_i, v_j) \\in E$, an MLP layer equipped with a sigmoid function is employed to map the concatenated representation of node pair $(z_i, z_j)$ into the masking probability of the edge between them $p_{ij} \\in [0, 1]$:\n$Z = [\\ldots, z_i, \\ldots, z_j, \\ldots]^\\top = GNN_{mask}(G) \\in R^{n \\times d}$, (1)\n$p_{ij} = \\sigma(MLP_{mask}([z_i, z_j]))$. (2)\nwhere d denotes the hidden dimension, $\\sigma(\\cdot)$ denotes the sigmoid function, and [...] denotes the concatenation. For molecule dataset, the edge representation $e_{ij}$ is also introduced to calculate the $p_{ij}$:\n$p_{ij} = \\sigma(MLP_{mask}([z_i \\oplus e_{ij}, z_j \\oplus e_{ij}]))$, (3)\nwhere $\\oplus$ denotes the element-wise sum of vectors.\nConsequently, in each forward pass of the training process, we extract a predictive subgraph by sampling from Bernoulli distributions, denoted as $m_{ij} \\sim Bern(p_{ij})$. Due to the inherent non-differentiability of Bernoulli sampling, direct sampling from $Bern(p_{ij})$ can not be optimized. To ensure the gradient of $m_{ij}$ remains calculable, we apply the Gumbel-Sigmoid technique for sampling as:\n$q_{ij} = Gumbel - Sigmoid(p_{ij}) = \\sigma(\\frac{log(p_{ij}) + G}{ \\tau })$, (4)\n$d_{ij} = \\begin{cases} 1, & \\text{if } q_{ij} > 0.5, \\\\ 0, & \\text{if } q_{ij} \\leq 0.5, \\end{cases}$ (5)\n$m_{ij} = d_{ij} + p_{ij} - p_{ij}$. (6)\nwhere $G = -log(-log(U))$ represents the Gumbel distribution, in which $U \\sim Uniform(0, 1)$. Given the non-differentiable nature of $q_{ij}$, we implement the straight-through trick (as delineated in equation (6)) to confer a gradient onto $m_{ij}$. The symbol \\bigtriangledown signifies the cessation of gradient propagation.\nThe extracted predictive subgraph can be denoted as an induced adjacent matrix $A_p = M \\odot A$, where M denotes the learned mask matrix, composed of elements $m_{ij}$. A is the adjacent matrix of original graph G, and $\\odot$ symbolizes the element-wise multiplication. The subgraph corresponding to $A_p$ is denoted as $G_p$.\n3.4.2 Subgraph encoder and classifier. After obtaining the predictive subgraph $G_p$, we train a GNN model to map the induced subgraph into representation $h_g$, which is fed into the following MLP layer to conduct classification or regression. Formally,\n$H = [h_1, \\ldots, h_n] = GNN_{feat}(G_p)$, (7)\n$h_g = READOUT(H)$, (8)\n$\\hat{y} = MLP(h_g) \\in Y$. (9)\n3.4.3 Main task loss. The main task loss can be denoted as:\n$L_{main} = R(\\hat{Y}, Y)$, (10)\nwhere R represents the task-tailored loss function. For regression tasks, this function implemented with the mean squared error, whereas for classification tasks, it is the cross-entropy function.\n3.5 Diversity via Subgraph Disagreement\nTo infuse diversity into the models within the collection, we purposefully advocate for each model to focus on distinct subgraphs. Assume the collection contain m models, the set of predictive masks corresponding to each model can be denoted as\n$S_M = {M_1, \\ldots, M_m}$. (11)\nWe apply a jaccard loss as diversity regularizer to penalize the overlapping of each pair of predictive mask in the set:\n$L_d = \\sum_{i,j} \\frac{M_i \\cap M_j}{M_i \\cup M_j} $, (12)\nwhere $i, j \\in {1, 2, \\ldots, m}, i \\neq j$ are indices of different models.\n3.6 Learning Objective\nCombining main task loss on each model and diversity regularizer, the total loss of the collection containing m models is defined as:\n$L = \\sum_{i=1}^m L_{main} + \\lambda L_d$, (13)\nwhere $\\lambda$ is the hyper-parameter to control the weight of diversity regularization and we set it as 0.5 for all the experiments.\n3.7 Model Selection\nWe employ an OOD validation set for model selection, opting for the model that exhibits the highest validation accuracy across our collection for inference on the test set. The use of an OOD validation set has become a standard practice in contemporary graph-based OOD methods for model selection [6, 49]. In our experiment, the baseline results were chosen using the same validation set as our method, ensuring absolute fairness. We have also included the results using the in-distribution (ID) validation set to further demonstrate the efficacy of our methodologies."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct extensive experiments to answer the research questions. (RQ1): Can our method DIVE achieve better OOD generalization performance against SOTA baselines? (RQ2):Does our method extract subgraphs more accurately than the current methods? (RQ3): Does our regularization really bring diversity to the models in the collection? Is there a model in the collection capable of recognizing invariant structural patterns? (RQ4):What influence does the number of models have on the performance? (RQ5):Does our method robust to the weight of diversity loss?\n4.1 Experimental Setup\n4.1.1 Datasets. We employ GOOD and DrugOOD benchmark of graph OOD performance evaluation.\n\u2022 GOOD [11], GOOD is a systematic graph OOD benchmark. It contains two types of distribution shift, covariate shift and concept shift. In covariate shift, the distribution of input differs. Formally, $P_{train}(G) \\neq P_{test}(G)$ and $P_{train}(Y|G) = P_{test}(Y|G)$. While concept shift occurs when the conditional distribution changes as $P_{train}(Y|G) \\neq P_{test}(Y|G)$ and $P_{train}(G) = P_{test}(G)$. Since these two types of distribution shift both contain the spurious correlation, we consider both case in our experiments. We choose four graph-level datasets GOODMotif, GOODHIV, GOODZINC and GOODSST2 to evaluate the graph generalization ability. There are 4 domain in these 4 datasets: basis, scaffold, size and length. The details of the four datasets can be found at appendix A.\n\u2022 DrugOOD is an OOD benchmark for Al-aided drug discovery, offering three environment-splitting strategies: assay, scaffold, and size. These strategies are applied to two measurements, IC50 and EC50. Since the scaffold and size domains overlap with the GOOD benchmark, we focus exclusively on testing the model's generalization ability in the assay domain of IC50 measurement in the DrugOOD benchmark.\n4.1.2 Baselines. We adopted 14 OOD algorithms as baselines including nine graph-specific methods. First, we introduce the general OOD algorithms used for Euclidean data. We utilize two invariant learning baselines based on the invariant prediction assumption. IRM [1] seeks data representations that perform well across all environments by penalizing features distributions that require different optimal linear classifier for each environment. VREx [17] reducing the variance of risk in test environments by minimizing the risk variances in training environments. Additionally, we implement two domain adaptation algorithms aimed at minimizing feature discrepancies. DANN [9] adversarially trains a regular classifier and a domain classifier to render features indistinguishable. Deep Coral [36] minimizes the deviation of covariant matrices from different domains to encourage features similarity across domains. GroupDRO [28] addresses the issue of distribution minorities lacking sufficient training through fair optimization, also known as risk interpolation, by explicitly minimizing the loss in the worst training environment. These five methods all need environment labels information.\n4.1.3 Evaluation metrics. Consistent with the settings in GOOD benchmark [11], We present the accuracy (ACC) for GOODMotif and GOODSST2, and report ROC-AUC score for GOOD-HIV and DrugOOD, as they are binary classification tasks. Additionally, we report the Mean Average Error (MAE) for GOODZINC dataset since it's regression task. Our experiments are conducted 10 times using different random seeds. Models are selected based on their performance in the validation dataset, and we report the mean and standard deviations on the test set.\n4.1.4 Implementation Details. For the training confiuration settings, we employ the Adam optimizer with a weight decay of 0 and a dropout rate of 0.5. The GNN models consist of three convolutional layers. Mean global pooling and the Rectified Linear Unit (ReLU) activation function are utilized, with a hidden layer dimension of 300. The batch size is set to 32, the maximum number of epochs is 300, and the initial learning rate is 1e-3. During the training process, all models are trained until convergence is achieved. In terms of computational resources, we typically employ one NVIDIA GeForce RTX 3090 for each individual experiment. For the hyperparamtere selection, indeed, one main advantage of our method is that our method does not need laborious hyper-paramter tuning. Our method only has one hyper-parameter: the weight $\\lambda$ of diveristy loss, and we set is as 0.5 for all settings."}, {"title": "4.2 Result Comparison and Analysis", "content": "4.2.1 Main task results (RQ1). In this section, our goal is to address Research Question 1 (RQ1) by conducting a comparative analysis of our approach, DIVE, against various baseline methodologies. The performance of DIVE in contrast to the current state-of-the-art (SOTA) methods is delineated in Tables 2 and 1. DIVE achieves superior outcomes in 13 out of 15 scenarios across five datasets. In the two remaining scenarios, it secures the second-best positions. It is noteworthy that the Invariant Risk Minimization (IRM) method requires environmental labels for each instance during training. Consequently, excluding methods necessitating environmental labels, our approach secures the top position in 14 out of 15 cases. A significant enhancement is observed in the datasets GOOD-Motif and GOOD-ZINC. Specifically, in GOOD-ZINC dataset, DIVE marks an impressive 51.31% improvement in the size-concept scenario, which may imply that our method is more competitive on regression datsets. Unlike most existing methods that excel in limited scenarios but experience substantial performance declines in others, DIVE consistently demonstrates top-tier performance across a majority of senarios. This underscores the efficacy of DIVE in extracting invariant subgraphs.\nConversely, subgraph-mixup methodologies, including mixup, DIR, GREA, and Disc, generally underperform across the board, frequently yielding results inferior to Empirical Risk Minimization (ERM). This suggests that current subgraph-mixup approaches fail to accurately isolate invariant subgraphs, and the incorporation of mixup can intensify the issue of spurious correlations if the extracted subgraph harbor spurious information. Furthermore, the representative information bottleneck method, GSAT, fails to achieve satisfactory outcomes across these datasets. This indicates a limitation of the information bottleneck technique in distinguishing between invariant and merely predictive features, thereby rendering it ineffective in addressing spurious correlations.\nAdditionally, MoleOOD, which necessitates the inference of environmental labels, also shows poor performance on three datasets. This highlights the complexities and challenges associated with inferring environment labels, where inaccuracies in inferred labels can detrimentally affect the overall results.\n4.2.2 The inadequacy of the current method in precisely extracting subgraphs (RQ2). To directly compare our method with current method that based on subgraph extraction in terms of invariant subgraph extraction ability, we conducted comparative analysis with the DIR, GSAT, and CIGA methods, evaluating the efficacy of true subgraph extraction. Analogous to our technique, these three methodologies learn an adjacency matrix mask to extract the subgraph. Since GOOD-Motif dataset annotates the ground-truth subgraph mask for each graph instance, we conduct the experiment on this dataset. We computed the F1 score of subgraph mask prediction for each graph instance within the dataset and calculate the mean F1 score. We train a collection with two models incorporating the diversity regularization. As illustrated in Figure 3, the majority of current methods achieve at most a F1 score of 0.6. As elucidated in the introduction, this signifies the current methods' inability to accurately extract the correct subgraphs. Under these circumstances, employing the mixup technique strengthen the spurious correlation. whereas our approach surpasses this threshold, attaining a F1 score exceeding 0.8 for subgraph mask prediction. In the absence of diversity regularization, our method's performance diminishes, achieving a F1 score around 0.5. However, with diversity regularization, there is a notable improvement in the F1 score of one model within the ensemble, consistently rising to surpass 0.8. Conversely, due to the influence of diversity regularization, the other model becomes predisposed towards the spurious aspects, resulting in a progressive decline in its F1 score to 0.1.\n4.2.3 Results on diversity of the collections (RQ3). We present a visualization of the predictive subgraph identified by DIVE within GOODMotif (basis-concept scenario) test set. In this scenario, each graph in the dataset is synthesized by integrating a base graph (ladder, wheel, tree) with a motif (tree, cycle, crane), where the motif exclusively determines the label of the graph. In the training dataset, the base graph exhibits a high degree of spurious correlation with the label. We demonstrate the subgraph learned on GOODMotif dataset's test set to ascertain if the models in the collection can focus on different subgraphs. We visualize four subgraph masks produced by models in the collection for each class. As illustrated in Figure 2, the two models in the collection concentrate on distinct parts. Model 0 focuses on the motif part, which is crucial for determining the label. Conversely, model 1 primarily focuses on the spurious part and is likely to make incorrect predictions on the test set, as the spurious correlation is present only in the training set and not in the test set. Additionally, we display the distribution of subgraph mask precision and recall for various models within the collection as Figure 4. It becomes evident that model 0 exhibits markedly higher precision and recall compared to model 1. The bulk of precision and recall values for model 1 are concentrated near zero, indicating that model 1 is overly influenced by the base graph and disregards the critical subgraph. In contrast, model 0 displays precisely the opposite behavior because of the subgraph diversity regularization.\n4.2.4 Ablation study (RQ3). We carry out the ablation study to examine the discrepancy in performance between our approach with and without the implementation of diversity regularization. As evidenced by the final row of Tables 1 and 2, the absence of diversity regularization leads to a considerable decline in performance across all scenarios of every dataset. This indicates that diversity regularization is essential for our methods to attain superior generalization capabilities.\n4.2.5 The impact of size of the collection (RQ4). We report the results when the size of collections is [2,3,4] in table 1 and table 2. It was observed that on GOODMotif dataset, either 2 or 3 models suffice for our methodology to attain commendable out-of-distribution (OOD) performance, whereas employing 4 models leads to a decline in performance. This can be attributed to the fact that GOODMotif dataset is synthetic, with each graph instance only being a composite of a base graph and a motif, hence limiting the structural pattern variability. Most optimal results are achieved with 3 models, as the basic graph structure (spurious part) can be bifurcated into two segments, exemplified by the division of the wheel graph into the edges of the wheel's outer rim and the hub's edges. However, an increase in the number of models does not contribute to the identification of more predictive structural patterns. On the real datasets, it was observed that our methodology attains optimal performance with a collection size of 4 for GOODZINC dataset. Conversely, for other datasets, a collection of 3 models suffices to reach optimal outcomes. This discrepancy can likely be attributed to the considerable size of GOODZINC, which encompasses approximately 250,000 samples, in contrast to other datasets that contain no more than 100,000 samples each. Consequently, GOODZINC dataset may possess more predictive structural patterns, rendering additional models beneficial in uncovering more of these predictive patterns.\n4.2.6 Hyperparameter analysis (RQ5). Without losing the generality, we conduct the sensitivity analysis on two datasets: GOODZINC and GOODSST2. We show our model's performance when the $\\lambda$ is [0.01, 0.1, 0.5, 1, 2]. Figure 5 shows that the performance of our methods is insensitive to the hyperparameter $\\lambda$ in Eq. 13."}, {"title": "5 CONCLUSION", "content": "In this study, we introduce a new learning paradigm named DIVE, designed to address the graph out-of-distribution challenge. This approach involves the development of an ensemble of diverse models capable of focusing on all label-predictive subgraphs, thereby reducing the impact of simplicity bias during training. We employ a subgraph diversity regularization technique to promote the variation in structural patterns recognized by the models. Comprehensive experiments conducted on one synthetic dataset and four real-world datasets underscore the exceptional performance of DIVE."}, {"title": "B TRAINING AND TEST METRIC CURVES", "content": "Figure 6 shows the training and test metric curves on the GOODMO-tif (basis-concept scenario) and GOODZINC (scaffod-concept scenario). In the training stage, models in the collections can all achieve good performance and they attend to different label-predictive sub-graphs. In the test stage, because the spurious correlation between the label and spurious subgraph diminish, only the model that attend to the real invariant subgraph can achieve a good performance."}, {"title": "C EXTRA RELATED WORK", "content": "C.1 Graph long tail learning\nGraph long-tail learning [4, 25, 33, 50] specifically deals with the problems posed by imbalanced data distributions where some classes of graph data are significantly underrepresented. This imbalance can mirror, and often exacerbates, the challenges faced in OOD generalization. Essentially, models trained on such skewed distributions might not only struggle with minority classes but also perform poorly when encountering unseen or novel distributions, as often happens in OOD scenarios."}, {"title": "D THE RESULTS USING ID VALIDATION SET", "content": "Although the use of an out-of-distribution (OOD) validation set is a standard practice, and all baselines in our experiments adhere to this by employing the same OOD validation set, we have additionally conducted experiments using an in-distribution (ID) validation set to further demonstrate the effectiveness of DIVE. Our findings indicate that the improvements are even more pronounced when using the ID validation set. Specifically, the enhancement over CIGA is approximately 40% in GOOD-Motif and around 10% in GOODSST2. These results underscore the high precision of our current method when evaluated on the ID validation set. Our approach successfully extracts both spurious and invariant subgraphs, with the invariant subgraph predictor proving to be more indicative of both ID and OOD data (as depicted in Figure 6, where the invariant predictor model 0 achieves superior performance on both the test and training sets). Consequently, even when using ID data for validation, a robust predictor that encapsulates invariant information is more readily identified, and the performance does not significantly deteriorate compared to using an OOD validation set."}]}