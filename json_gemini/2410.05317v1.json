{"title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING", "authors": ["Chang Zou", "Xuyang Liu", "Ting Liu", "Siteng Huang", "Linfeng Zhang"], "abstract": "Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10\u00d7 more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-a, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36\u00d7 and 1.93\u00d7 acceleration are achieved on OpenSora and PixArt-a with almost no drop in generation quality.", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models (DMs) have demonstrated impressive performance across a wide range of generative tasks such as image generation (Rombach et al., 2022) and video generation (Blattmann et al., 2023). Recently, the popularity of diffusion transformers further extends the boundary of visual generation by scaling up the parameters and computations (Peebles & Xie, 2023). However, a significant challenge for diffusion transformers lies in their high computational costs, leading to slow inference speeds, which hinder their practical application in real-time scenarios. To address this, a series of acceleration methods have been proposed, focusing on reducing the sampling steps (Song et al., 2021) and accelerating the denoising networks (Bolya & Hoffman, 2023; Fang et al., 2023).\nAmong these, cache-based methods (Ma et al., 2024b; Wimbauer et al., 2024), which accelerate the sampling process by reusing similar features across adjacent timesteps (e.g. reusing the features cached at timestep t in timestep t \u2212 1), have obtained abundant attention in the industrial community thanks to their plug-and-play property. As the pioneering works in this line, DeepCache (Ma et al., 2024b) and Block Caching (Wimbauer et al., 2024) were proposed to reuse the cached features in certain layers of U-Net-like diffusion models by leveraging the skip connections in the U-Net. However, the dependency on the U-Net architectures also makes them unsuitable for diffusion transformers, which have gradually become the most powerful models in visual generation. Most recently, FORA (Selvaraju et al., 2024) and \u2206-DiT (Chen et al., 2024b) have been proposed as a direct application of previous cache methods to diffusion transformers, though still not fully analyzed and exploited the property of the transformer-architecture. To tackle this challenge, this paper begins by studying how feature caching influences diffusion transformers at the token level.\nDifference in Temporal Redundancy: Figure 1 shows the distribution of the feature distance between the adjacent timesteps for different tokens, where a higher value indicates that this token exhibits a lower similarity in the adjacent timesteps. It is observed that there exist some tokens that"}, {"title": "2 RELATED WORK", "content": "Transformers in Diffusion Models Diffusion models (DMs) (Ho et al., 2020; Sohl-Dickstein et al., 2015), which iteratively denoise an initial noise input through a series of diffusion steps, have achieved remarkable success across various generation applications (Rombach et al., 2022; Balaji et al., 2022). Early DMs (Ho et al., 2020; Rombach et al., 2022) are based on the U-Net architecture (Ronneberger et al., 2015), consistently achieving satisfactory generation results. Recently, Diffusion Transformer (DiT) (Peebles & Xie, 2023) has emerged as a major advancement by replacing the U-Net backbone with a Transformer architecture. This transition enhances the scalability and efficiency of DMs across various generative tasks (Chen et al., 2024a; Brooks et al., 2024). For example, PixArt-a (Chen et al., 2024a) utilizes DiT as a scalable foundational model, adapting it for text-to-image generation, while Sora (Brooks et al., 2024) demonstrates DiT's potential in high-fidelity video generation, inspiring a series of related open-source projects (Zheng et al., 2024; Lab & etc., 2024). Despite their success, the iterative denoising process of these DMs is significantly time-consuming, making them less feasible for practical applications.\nAcceleration of Diffusion Models To improve the generation efficiency of DMs, numerous diffusion acceleration methods have been proposed, falling broadly into two categories: (1) reducing the number of sampling timesteps, and (2) accelerating the denoising networks. The first category aims to achieve high-quality generation results with fewer sampling steps. DDIM (Song et al., 2021) introduces a deterministic sampling process that reduces the number of denoising steps while preserving generation quality. DPM-Solver (Lu et al., 2022a) and DPM-Solver++ (Lu et al., 2022b) propose adaptive high-order solvers for a faster generation without compromising on generation results. Rectified flow (Liu et al., 2023) optimizes distribution transport in ODE models to facilitate efficient and high-quality generation, enabling sampling with fewer timesteps. In the second category, various efforts have been paid to token reduction (Bolya & Hoffman, 2023), knowledge distillation (Li et al., 2024), and weight quantization (Shang et al., 2023) and pruning (Fang et al., 2023) on the denoising networks. Additionally, recent cache-based methods reduce redundant computations to accelerate inference for DMs. These cache-based methods have obtained abundant attention since they have no requirements for additional training. DeepCache (Ma et al., 2024b) eliminates redundant computations in Stable Diffusion (Rombach et al., 2022) by reusing intermediate features of low-resolution layers in the U-Net. Faster Diffusion (Li et al., 2023) accelerates the sampling process of DMs by caching U-Net encoder features across timesteps, skipping encoder computations at certain steps. Unfortunately, DeepCache and Faster Diffusion are designed specifically for U-Net-based denoisers and can not be applied to DiT (Chen et al., 2024b). Recently, FORA (Selvaraju et al., 2024) and A-DiT (Chen et al., 2024b) have been proposed to cache the features and the residual of features for DiT. Learning-to-Cache (Ma et al., 2024a) learns an optimal cache strategy, which achieves a slightly higher acceleration ratio but introduces the requirements of training. However, these methods apply the identical cache solution to all the tokens and even all the layers, which leads to a significant performance degradation in generation quality."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 PRELIMINARY", "content": "Diffusion Models Diffusion models are formulated to contain two processes, including a forward process which adds Gaussian noise to a clean image, and a reverse process which gradually denoises a standard Gaussian noise to a real image. By denoting t as the timestep and Bt as the noise variance schedule, then the conditional probability in the reverse (denoise) process can be modeled as\n$P_\\theta(x_{t-1} | x_t) = N(x_{t-1}; \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_{t}}}e_\\theta(x_t, t)), \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}} \\beta_t \\mathbf{I}).$\nwhere \u03b1t = 1 \u2212 \u03b2t, \u0101t = \u03a0t\u2081=1 \u03b1i, and T denotes the number of timesteps. Importantly, e\u0473 denotes a denoising network with its parameters @ that takes xt and t as the input and then predicts the corresponding noise for denoising. For image generation with T timesteps, eo is required to infer for T times, which takes most of the computation costs in the diffusion models. Recently, fruitful works demonstrate that formulating e as a transformer usually leads to better generation quality.\nDiffusion Transformer Diffusion transformer models are usually composed of stacking groups of self-attention layers fsa, multilayer perceptron fMLP, and cross-attention layers fCA (for conditional generation). It can be roughly formulated as 91 0 92 0 ... 9L where gl = {fSA, fCA, SMLP}. The upper script denotes the index of layer groups and L denotes its maximal number. We omit the other components such as layer norm and residual connections here for simplicity. For diffusion transformers, the input data xt is a sequence of tokens corresponding to different patches in the generated images, which can be formulated as xt = {x}HXW, where H and W denote the height and width of the images or the latent code of images, respectively."}, {"title": "3.2 NAIVE FEATURE CACHING FOR DIFFUSION TRANSFORMERS", "content": "Following previous caching methods for U-Net-like diffusion denoisers, we introduce the naive scheme for feature caching. Given a set of N adjacent timesteps {t, t + 1, . . .,t + N \u2013 1}, native feature caching performs the complete computation at the first timestep t and stores the intermediate features in all the layers, which can be formulated as C(xt) := fl(x), for \u2200l \u2208 [0, L], where \u201c:=\u201d indicates the operation of assigning the value. Then, in the next N \u2212 1 timesteps, feature caching avoids the computation of self-attention, cross-attention, and MLP layers by reusing the feature cached at timestep t. By denoting the cache as C and the expected feature of the input xt in the lth layer as F(x), then for \u2200N \u2208 [1, L], the naive feature caching can be formulated as\n$F(x_t^l) = F(x_{t+1}^l) = \\ldots = F(x_{t+N-1}^l) := C(x_t^l).$\nIn these N timesteps, naive feature caching avoids almost all the computation in N \u2013 1 timesteps, leading to around N-1 times acceleration. After the N timesteps, the feature cache then starts a new period from initializing the cache as aforementioned, again. The effectiveness of feature caching can be explained by the extremely low difference between the tokens in the adjacent timesteps. However, as N increases, the difference between the feature value in the cache and their correct value can be exponentially increased, leading to degeneration in generation quality, which motivates us to study more fine-grained methods for feature caching."}, {"title": "3.3 TOKEN-WISE FEATURE CACHING", "content": "The naive feature caching scheme caches all the tokens of the diffusion transformers with the same strategy. However, as demonstrated in Figure 1, 2 and 5, feature cache introduces significantly different influence to different tokens, motivating us to design a more fine-grained caching method in the token-level. In this section, we begin with the overall framework of ToCa, then introduce our strategy for token selection and caching ratios."}, {"title": "3.4 OVERALL FRAMEWORK", "content": "Cache Initialization Similar to previous caching methods, given a set of adjacent timesteps {t, t + 1, ..., t + N \u2013 1}, our method begins with computing all the tokens at the first timestep t, and storing the computation result (intermediate features) of each self-attention, cross-attention, and MLP layer in a cache, denoted by C, as shown in the left part in Figure 3. This can be considered as the initialization of C, which has no difference compared with previous caching methods.\nComputing with the Cache In the following timesteps, we can skip the computation of some unimportant tokens by re-using their value in the cache C. We firstly pre-define the cache ratio R of tokens in each layer, which indicates that the computation of R% of the tokens in this layer should be skipped by using their value in the cache, and the other (1 \u2013 R%) tokens should still be computed. To achieve this, a caching score function S is introduced to decide whether a token should be cached, which will be detailed in the next section. Then, with S, we can select a set of cached tokens as ICache and the other set of tokens for real computation as ICompute={xi}i=1 - ICache. Then, the computation of the layer f for ith token xi can be formulated as F(xi) = Yi f (xi) + (1 \u2212 vi)C(xi), where Yi = 0 for i \u2208 ICache and Yi = 1 for i \u2208 ICompute. C(xi) denotes fetching the cached value of xi from C, which has no computation costs and hence leads to overall acceleration in f.\nCache Updating As a significant difference between traditional cache methods and ToCa, traditional cache methods only update the feature in the cache at the first timestep for each caching period while ToCa can update the feature in the cache at all the timesteps, which helps to reduce the error introduced by feature reusing. For the tokens xi \u2208 ICompute which are computed, we update their corresponding value in the cache C, which can be formulated as C(xi) := F(xi) for i \u2208 ICompute."}, {"title": "3.5 TOKEN SELECTION", "content": "Given a sequence of tokens xt = {x}N1, token selection aims to select the tokens that are suitable for caching. To this end, we define a caching score function S(xi) to decide whether the ith token xi should be cached, where a token with a higher score has a lower priority for caching and a higher priority to be actually computed. The S(xi) is composed of four sub-scores {81, 82, 83, 84}, corresponding to the following four principals.\n(I) Influence to Other Tokens: If a token has a significant contribution to the value of other tokens, then the error caused by token caching on this token can easily propagate to the other tokens, ultimately leading to discrepancies between all tokens and their correct values. Consequently, we consider the contribution of each token to other tokens as one of the criteria for defining whether it should be cached, estimated with an attention score in self-attention. Recall that the self-attention can be formulated as O = AV, where A = Softmax($\\frac{QK^T}{\\sqrt{d}}$) \u2208 RN\u00d7N denotes the normalized attention map. Q, K, V and O \u2208 RN\u00d7d are query, key, value and output tokens respectively; d is"}, {"title": "3.6 DIFFERENT CACHE RATIOS IN DIFFERENT LAYERS", "content": "Figure 5 shows the difference in feature caching of different layers, where (a) shows that the output features of different layers have different distances compared with their value in the last step (i.e."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 EXPERIMENT SETTINGS", "content": "Model Configurations We conduct experiments on three commonly-used DiT-based models across different generation tasks, including PixArt-a (Chen et al., 2024a) for text-to-image generation, OpenSora (Zheng et al., 2024) for text-to-video generation, and DiT-XL/2 (Peebles & Xie, 2023) for class-conditional image generation with NVIDIA A800 80GB GPUs. Each model utilizes its default sampling method: DPM-Solver++ (Lu et al., 2022b) with 20 steps for PixArt-a, rflow (Liu et al., 2023) with 30 steps for OpenSora and DDIM (Song et al., 2021) with 250 steps for DiT-XL/2."}, {"title": "5 CONCLUSION", "content": "Motivated by the observation that different tokens exhibit different temporal redundancy and different error propagation, this paper introduces ToCa, a token-wise feature caching method, which adaptively skips the computation of some tokens by resuing their features in previous timesteps. By leveraging the difference in different tokens, ToCa achieves better acceleration performance compared with previous caching methods by a clear margin in both image and video generation, providing insights for token-wise optimization in diffusion transformers."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 ENGINEERING DETAILS", "content": "This section introduces some engineering techniques in our work."}, {"title": "A.1.1 STEP-WISE CACHING SCHEDULING", "content": "In section 3.6, we propose a method for dynamically adjusting the caching ratio R based on the time redundancy and noise diffusion speed across different depths and types of layers, which constitutes a key part of our contributions. In the following section, we further explore the dynamic adjustment of R along the timestep dimension, as well as strategies for dynamically adjusting the forced activation cycle N.\nAt the initial stages of image generation, the model primarily focuses on generating contours, while in the later stages, it pays more attention to details. In the early contour generation phase, it is not necessary for too many tokens to be fully computed with high precision. By multiplying by a term rt, we achieve dynamic adjustment of R along the timestep dimension, where rt = 0.5+At(0.5-t/T), At is a positive parameter controlling the slope, t is the number of timesteps already processed, and T is the total number of timesteps. By adjusting R in this way, we shift some of the computational load from earlier timesteps to later ones, improving the quality of the generated images. Finally, the caching ratio is determined as Rtype = R \u00d7 ri \u00d7 r'type \u00d7 rt.\nSimilarly, we set a larger forced activation cycle N during the earlier stages, while a smaller N is used during the later detail generation phase to enhance the quality of the details. To ensure that the adjustment of N has minimal impact on the theoretical speedup, we define it as follows: Nt = No/(0.5+wt(t/T\u22120.5)), where No corresponds to the expected theoretical speedup induced by N, and wt is a hyperparameter controlling the slope."}, {"title": "A.1.2 PARTIALLY COMPUTATION ON SELF-ATTENTION", "content": "In the previous section, we mentioned that partial computation in the Self-Attention module can lead to rapid propagation and accumulation of errors. Therefore, we considered avoiding partial computation in the Self-Attention module, meaning that during the non-forced activation phase, the Self-Attention module has rtype = 0. In the subsequent Sensitivity Study, we explored a trade-off scheme between Self-Attention and MLP modules, with the corresponding formulas for allocation being 'type = 1 \u2013 0.4Xtype for the Self-Attention module, and rtype = 1 + 0.6Xtype for the MLP module. The factors 0.6 and 0.4 are derived from the approximate computational ratio between these two modules in the DiT model."}, {"title": "\u0391.1.3 \u03a4\u039fOKEN SELECTION FOR CFG AND NON-CFG", "content": "In the series of DiT-based models, the tensors of cfg (class-free guidance) and non-cfg are concatenated along the batch dimension. A pertinent question in token selection is whether the same token selection strategy should be applied to both the cfg and non-cfg parts for the same image (i.e., if a token is cached in the cfg part, it should also be cached in the corresponding non-cfg part). We have observed significant sensitivity differences among models with different types of conditioning regarding whether the same selection strategy is used. For instance, in the text-to-image and text-to-video models, such as PixArt-a and OpenSora, if independent selection schemes are applied for the cfg and non-cfg parts, the model performance degrades substantially. Thus, it is necessary to enforce a consistent token selection scheme between the cfg and non-cfg parts.\nHowever, in the class-to-image DiT model, this sensitivity issue is considerably reduced. Using independent or identical schemes for the cfg and non-cfg parts results in only minor differences. This can be attributed to the fact that, in text-conditional models, the cross-attention module injects the"}, {"title": "A.2 MORE IMPLEMENTATION DETAILS ON EXPERIMENTAL SETTINGS", "content": "For the DiT-XL/2 model, we uniformly sampled from 1,000 classes in ImageNet (Deng et al., 2009) and generated 50,000 images with a resolution of 256 \u00d7 256. We explored the optimal solution for DiT-XL/2 using FID-5k (Heusel et al., 2017) and evaluated its performance with FID-50k. Additionally, sFID, Inception Score, and Precision and Recall were used as secondary metrics. For the PixArt-a model, we used 30,000 captions randomly selected from COCO-2017 (Lin et al., 2014) to generate 30,000 images. We computed FID-30k to assess image quality and used the CLIP Score between the images and prompts to evaluate the alignment between image content and the prompts. For the OpenSora model, we used the VBench framework (Huang et al., 2024), generating 5 videos for each of the 950 VBench benchmark prompts under different random seeds, resulting in a total of 4,750 videos. These videos have a resolution of 480p, an aspect ratio of 16:9, a duration of 2 seconds, and consist of 51 frames saved at a frame rate of 24 frames per second. The model was comprehensively evaluated across 16 aspects: subject consistency, imaging quality, background consistency, motion smoothness, overall consistency, human action, multiple objects, spatial relationships, object class, color, aesthetic quality, appearance style, temporal flickering, scene, temporal style, and dynamic degree.\nPixArt-a: We set the average forced activation cycle of ToCa to N = 2, supplemented with a dynamic adjustment parameter wt = 0.1. The parameter At = 0.4 adjusts R at different time steps, and the average caching ratio is R = 70%. The parameter r\u2081 = 0.3 adjusts R at different depth layers. The module preference weight rtype 1.0 shifts part of the computation from cross-attention layers to MLP layers.\nOpenSora: For OpenSora, we fixed the forced activation cycle for temporal attention, spatial attention, and MLP at 3, and set the forced activation cycle for cross-attention to 6. The ToCa strategy ensures that a portion of token computations is conducted solely in the MLP, with Rmlp fixed at 85%.\nDiT: We set the average forced activation cycle of ToCa to N = 3, supplemented with a dynamic adjustment parameter wt = 0.03 to gradually increase the density of forced activations as the sampling steps progress. The parameter At = 0.03 adjusts R at different time steps. Additionally, during the sampling steps in the interval t \u2208 [50, 100], the forced activation cycle is fixed at N = 2 to promote more thorough computation in sensitive regions. The average caching ratio is R = 93%, and the parameter \u03bb\u2081 = 0.06 adjusts R at different depth layers. The module preference weight rtype = 0.8 means that during steps outside the forced activation ones, no extra computations are performed in attention layers, but additional computations are performed in the MLP layers.\nAll of our experiments were conducted on 6 A800 GPUs, each with 80GB of memory, running CUDA version 12.1. The DiT model was executed in Python 3.12 with PyTorch version 2.4.0, while PixArt-a and OpenSora were run in Python 3.9. The PyTorch version for PixArt-a was 2.4.0, and for OpenSora it was 2.2.2. The CPUs used across all experiments were 84 vCPUs from an Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz."}, {"title": "A.3 SENSITIVITY STUDY", "content": "We explored the optimal parameter configuration for the ToCa acceleration scheme on DiT and analyzed the sensitivity of each parameter. The experiments used FID-5k and sFID-5k as evaluation metrics. From Figure 11 (a) to (f), we respectively investigated the effects of the caching ratio weights \u03bb\u03b9, Atype, At, the weight of Cache Frequency score 3, Uniform Spatial Distribution \u51654, and the dynamic adjustment weight for forced activation wt. It is observed that: (a) The optimal parameter is \u03bb\u03b9 = 0.06, where the corresponding cache ratio shows approximately 6% variation at both the last and first layers. (b) The optimal parameter is Atype = 2.5, at which point the Self-Attention layer does not perform any partial computation, with the entire computational load shifted to the MLP layer. It is also noted that as the computation load decreases in the Self-Attention layer and increases in the MLP layer, the generation quality shows a steady improvement. (c) The"}]}