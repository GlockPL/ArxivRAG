{"title": "Strada-LLM: Graph LLM for traffic prediction", "authors": ["Seyed Mohamad Moghadas", "Yangxintong Lyu", "Bruno Cornelis", "Alexandre Alahi", "Adrian Munteanu"], "abstract": "Traffic prediction is a vital component of intelligent transportation systems. By reasoning about traffic patterns in both the spatial and temporal dimensions, accurate and interpretable predictions can be provided. A considerable challenge in traffic prediction lies in handling the diverse data distributions caused by vastly different traffic conditions occurring at different locations. LLMs have been a dominant solution due to their remarkable capacity to adapt to new datasets with very few labeled data samples, i.e., few-shot adaptability. However, existing forecasting techniques mainly focus on extracting local graph information and forming a text-like prompt, leaving LLM-based traffic prediction an open problem. This work presents a probabilistic LLM for traffic forecasting with three highlights. We propose a graph-aware LLM for traffic prediction that considers proximal traffic information. Specifically, by considering the traffic of neighboring nodes as covariates, our model outperforms the corresponding time-series LLM. Furthermore, we adopt a lightweight approach for efficient domain adaptation when facing new data distributions in few-shot fashion. The comparative experiment demonstrates the proposed method outperforms the state-of-the-art LLM-based methods and the traditional GNN-based supervised approaches. Furthermore, Strada-LLM can be easily adapted to different LLM backbones without a noticeable performance drop.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of Intelligent Transportation Systems, traffic forecasting has received increasing attention. It is a key component of advanced traffic management systems and is crucial in traffic planning, traffic management, and traffic control. Traffic forecasting involves analyzing traffic conditions on roads, including flow, speed, and density, mining traffic patterns, and predicting traffic trends [1]. This capability provides a scientific foundation for any traffic management department to anticipate and mitigate congestion, implement preemptive vehicle restrictions, and enable urban tourists to select safer and more efficient travel routes.\nHowever, traffic forecasting is a challenging task due to complex spatial and temporal dependencies:\n1) Spatial Dependency: The variation in traffic volume is influenced by the topological structure of the urban road network. In particular, traffic conditions at upstream roads impact downstream roads through transfer effects, which refers to the impact that a change in one section of a traffic network (e.g., a closure of a road, a change in signal timing, or new infrastructure) has on other parts of the network. On the other hand, downstream roads affect upstream roads through feedback effects, which occur when the changes in traffic patterns resulting from transfer effects influence the original cause of the change, creating a loop of cause and effect.\n2) Temporal Dependency: Traffic volume changes over time, exhibiting periodicity and trends, which are often influenced by factors such as holidays, working hours, and other social events.\nAs illustrated in Figure 1, the strong influence among adjacent roads caused by the traffic flow alongside the edges changes the short-term state \u2460 to state \u2461 and state \u2462. Indeed, node (a) has a steady traffic count trend, and node (b) has short spikes in volume \u2461, so the spatio-temporal correlation caused state \u2462 in node (c) after a couple of hours. Furthermore, traffic data sources exhibit a variety of data distributions, posing a significant challenge for the domain in finding generalizable models.\nIn recent years, the emergence of Large Language Models (LLM) have revolutionized various fields due to the generalization capabilities of extracting efficient features from different modalities [2], [3]. Research has been conducted in multiple domains, such as computer vision and natural language processing. Specifically, in [4], a unified representation space is proposed, namely shared vocabulary, across varied data sources such that the model can learn domain-invariant features. This characteristic is particularly appealing for traffic prediction, where data distributions can vary significantly across different regions and times. In [5], [6], [7], the authors have tried to address this problem by casting the graph (and/or the corresponding time-series) to a prompt input. However, this branch of methods, in terms of predictability, is suboptimal because traditional time series forecasting models use"}, {"title": "II. RELATED WORK", "content": "The related approaches can be categorized into two main groups: those specifically designed for traffic prediction and those developed for broader timeseries forecasting. The latter category includes methods for both univariate and multivariate time series analysis, which have been adapted to handle traffic-related variables."}, {"title": "A. Statistical models", "content": "This class of models served as a fundamental pillar in traffic forecasting for many years, progressively evolving to tackle sophisticated forecasting challenges. Statistical models, such as ARIMA (Autoregressive Integrated Moving Average) [15] established the groundwork by utilizing auto-correlation to predict future outcomes. Subsequently, ETS (Error, Trend, Seasonality) models [16], [17] break down a time series into essential elements such as trends and seasonal variations, thus enabling more detailed forecasts. Theta models [18], [19] marked another notable progression by partially addressing non-linear behavior. Moreover, [18], [20] often necessitates significant manual adjustment and specialized knowledge to choose suitable models and parameters for particular forecasting problems."}, {"title": "B. Neural Traffic forecasting", "content": "Deep learning-based traffic forecasting is a rapidly developing research area [21]. Various architectures have been developed, including CNN-based [22], [10], RNN-based [23], and LSTM-based models[24], [25]. Inspired by the vanilla transformer mechanism, introduced in [26], PatchTST [27] highlights the importance of extracting local semantic information from time series data, which is often overlooked in previous models that use point-wise tokens, then re-process the output of a transformer for a point forecast or a probabilistic forecast. On the other hand, various other works propose alternative strategies to vanilla attention and build on the transformer architecture, leading to better models tailored for time series forecasting [28], [29], [30]. Specifically, models such as [31] propose spatio-temporal attention, others leverage trend-aware decomposition in the definition of the attention layer [32], [33], [34], [35], [3], [36]. On the other hand, there are copula-based transformer models in the literature [37], better at capturing volatilities, which are defined as the degree of variation in the time-series value over time. We selected transformer-based models due to their powerful ability to extract features and their effectiveness in few-shot learning scenarios. Specifically, we adopt Mistral [38] as our backbone model."}, {"title": "C. LLMS", "content": "LLM models are an emerging paradigm within deep learning [39]. A key factor contributing to their popularity is the utilization of spatio-temporal attention [40], as highlighted by [31]. Many such models [41], [12] have showcased the power of robust feature extraction on a large scale. However, fine-tuning an LLM is time and energy-consuming, leading to the proposal of efficient methods [13], [42] with differing tuned parameters. LLMs have also been extended to scientific domains such as protein design [43], [44]. However, their application in urban traffic forecasting is still being investigated. Notably, several LLMs have been developed for traffic forecasting, including ST-LLM [45], which partially freezes the transformer block deteriorating the performance on a large scale. Moreover, Time-LLM [46], LLM4TS [47], and UniTime [48] cast the graph to textual prompt, that is, encode nodes, edges, attributes, etc., and forecast based on the prompts. Moreover, prompt-tuning emerged as a prominent technique for adapting large models [49]. FlashST [14] extends the prompt-tuning to spatio-temporal data. They adjust to unseen datasets by applying prompt-tuning in the context of spatio-temporal data. This category of models neglects the graph topology which hampers the prediction efficiency of the LLM [50]. Furthermore, data diversity has formed an additional challenge for LLMs in few-shot learning on novel datasets. [51]. The main goal of our work is to apply the LLM approach to traffic data forecasting and to investigate to what extent LLMs can adapt across a wide range of traffic datasets."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Problem Definition", "content": "In this paper, we aim to predict traffic conditions at a specific timestamp \\(T'\\) based on historical traffic data over period \\(T\\). Particularly, the traffic metric broadly refers to any traffic speed, flow, or density.\nBefore further elaborating on our method, we define the following notations. In Strada-LLM, a road network is represented as a graph \\(G\\), consisting of a set of nodes \\(V\\) and a set of edges \\(E\\). Each node presents a traffic measuring sensor, and an edge connects two nodes if they are geographically adjacent. That is, \\(G = (V, E, A)\\) where \\(V\\) is a set of \\(N\\) nodes, \\(E\\) is a set of edges and \\(A \\in \\mathbb{R}^{N \\times N}\\) is the corresponding adjacency matrix. We assume that the topology of the traffic graph \\(G\\) is static. At each timestamp \\(t\\), the graph \\(G\\) is associated with a dynamic feature matrix \\(X_t \\in \\mathbb{R}^{N \\times F}\\), where \\(F\\) is the number of traffic metrics.\nIn this paper, we formulate traffic forecasting as a discrete multi-variate point prediction problem. In particular, Strada-LLM takes the \\(T\\) past observations \\(X = [X_{t_1}, X_{t_2}, ..., X_{t_T}] \\in \\mathbb{R}^{T \\times N \\times F}\\) from \\(G\\) as input and predicts the traffic for horizon \\(T'\\), where \\(X = [X_{t_{T+1}}, X_{t_{T+2}},..., X_{t_{T+T'}} ]\\)."}, {"title": "B. Proposed Method", "content": "Strada-LLM consists of a Hierarchical Feature Extractor (HFE), an LLM-based backbone, and a T-student distribution head. The distribution head includes three learnable parameters related to this distribution: degrees of freedom, mean, and scale, with suitable non-linearities to ensure the relevant parameters remain positive, as shown in Figure 2. In the HFE module, a sub-graph extractor and a global graph feature extractor hierarchically learn features while the LLM-based block maps the graph features into the latent spatio-temporal feature space to be learned by the distribution head. In the following sections, we will elaborate on each component.\n1) Hierarchical Feature Extractor:\n\u2022 Subgraph extractor: To enhance the LLM's understanding of graph structures while accommodating its limited context length, we tokenize the spatio-temporal traffic signal regarding the existing subgraphs of the road network. Overall, the corresponding traffic signals for neighboring nodes will be aggregated together to be tokenized in the further steps. This block is responsible for extracting k-hop subgraphs from the input graph. For a node \\(v \\in G\\), the k-hop operator [52] is defined as:\n\\[N_k(v) = \\{u \\in V|d(u, v) \\leq k\\},\\] (1)\nwhere \\(d(.,.)\\) is the hop-based distance function between two nodes. By doing so, each node is equipped with a sub-graph of \\(G\\), including local topological information. As a result, for each node \\(v\\), we concatenate the features of \\(N_k(v)\\), leading to an \\(N \\times T \\times (M \\times F)\\) feature map, where \\(M\\) denotes \\(|N_k(v)|\\). Similarly, Subgraph-1-WL [53] and MixHop [54] can also be used as the k-hop operator, but it is beyond the scope of this research.\n\u2022 Global Graph Feature Extractor: While prompt-based networks are widely used in the existing methods [44], they lose the global structure information [14]. In order to keep the global graph structure information, we compute Laplacian embeddings [55]. The Laplacian operation leverages the eigenvectors of the complete graph and creates unique positional encodings [26] for each node. First, we define the graph Laplacian matrix \\(L = D - A\\) where \\(D = diag(d_1,...,d_n)\\) is the degree matrix. We then compute the eigen decomposition of the normalized Laplacian as follows \\(L_{norm} = D^{-1/2}LD^{-1/2} = U\\Lambda U^{T}\\).\n\u2022 Lag Features The tokenization strategy employed by Strada-LLM revolves around the creation of lagged features derived from historical traffic data. These features are constructed using predetermined sets of lag indices, such as quarterly, monthly, etc., also referred to as second-level frequencies, carefully chosen to capture various temporal patterns. Mathematically, this lagging operation can be expressed as a mapping \\(x_t \\to A_t \\in \\mathbb{R}^{|L| \\times F}\\), where \\(L = \\{1, ..., L\\}\\) represents the set of lag indices. In this formulation, \\(A_t[j] = X_{t-L[j]}\\), meaning that \\(x_{t[j]}\\) corresponds to the value of \\(x\\) at \\(j\\) time steps before \\(t\\), as specified by the j-th element of \\(L\\). To put it another way, \\(A_t[j]\\) represents the \\(L[j]\\)-th lagged feature when considering the time series \\(x\\) in reverse chronological order. This approach allows for a comprehensive encoding of temporal dependencies across various time scales. Therefore, to generate lag features for a specific"}, {"title": "2) LLM-based-Backbone:", "content": "Our backbone architecture is inspired by Mistral [38], a decoder-only transformer architecture. The extracted tokens by the sub-graph extractor are transformed by a shared linear projection layer that maps the features to the hidden dimension of the attention module. Inspired by [56], Strada-LLM includes pre-normalization by utilizing the RMSNorm [57] and Rotary Positional Encoding (ROPE) [58] for the representation of query and key blocks at each attention layer, similar to the approach described for Mistral [38].\nAfter passing through the causally masked transformer layers, the proposed method incorporates Flash-Attention-2 [59] to predict the parameters of the forecast distribution for the next timestamp, denoted as \\(\\phi\\). These parameters are generated by a parametric distribution head, as shown in Figure 2. The objective is to minimize the negative log-likelihood of the predicted distribution across all prediction times.\nAt the inference stage, given a time series with the size of at least \\(L + C\\), a feature vector can be formed and supplied to the model to determine the distribution of the subsequent timestamps. In this fashion, we can obtain many simulated future trajectories up to our chosen prediction horizon \\(T'\\) via greedy auto-regressive decoding [60]. In doing so, uncertainty intervals and confidence scores could also be valuable for downstream decision-making tasks."}, {"title": "C. Domain Adaptation", "content": "In this section, we describe the techniques we adopt to adapt the Strada-LLM model to a new dataset, which might contain a graph of a new city. One of the important aspects of such a model is that it should be domain-agnostic. In the rapidly evolving landscape of deep learning, domain adaptation has emerged as a crucial technique for enhancing the generalizability of models across different but related domains. More specifically, domain adaptation focuses on the ability of a model trained in one domain (source) to adapt and perform well in another domain (target) with potentially different distributions. A prominent approach within this realm is low-rank adaptation [13], which aims to refine pre-trained models by adapting their parameters in a computationally efficient manner; indeed, a nearly linear operation will replace the \\(O(n^2)\\) operation. This technique is particularly beneficial when working with LLMs, which, despite their impressive performance and versatility, pose significant challenges regarding computational resources and training time. In this paper, we adapt to the new distribution by tuning the distribution head, query, key, and value attention blocks.\n1) Low-Rank Matrix Adaptation: By leveraging domain adaptation strategies and Low-Rank Matrix Adaptation (LoRA) for fine-tuning,researchers and practitioners can effectively bridge the gap between domains, ensuring that powerful deep learning models maintain their efficacy across diverse applications [61]. Effectiveness of LoRA in the NLP domain has already been proven [13]. One of the purposes of this research is a demonstration of LoRA in the traffic forecasting problem. As illustrated in Figure III-C, we apply LoRA for the fine-tuning process. The query, key, and value matrices will be fine-tuned in a low-rank approximation manner. Specifically, according to Figure III-C, suppose the query, key, and value tensors are computed as follows:\n\\[q = W_q h, k = W_k h, v = W_v h,\\] (2)\nwhere \\(h\\) is the output of the last layer of the LLM backbone with dimension \\(\\mathbb{R}^{d \\times k}\\). \\(W_q\\), \\(W_k\\) and \\(W_v\\) are the query, key, and the projection weight matrices respectively. The low-rank matrices for fine-tuning the forward step can be defined as:\n\\[q = W_q h + \\Delta W_q h = W_q h + B_q A_q h\\\\\nk = W_k h + \\Delta W_k h = W_k h + B_k A_k h\\\\\nv = W_v h + \\Delta W_v h = W_v h + B_v A_v h,\\] (3)\nwhere \\(h\\) is the output of the previous layer, the low-ranked approximated update matrices \\(B_q\\), \\(B_k\\), \\(B_v\\) are of dimensions \\(\\mathbb{R}^{d \\times r}\\), \\(A_q\\), \\(A_k\\), \\(A_v\\) are of dimensions \\(\\mathbb{R}^{r \\times k}\\), and where the rank hyperparameter \\(r < min(d,k)\\). By introducing these matrices, the online inference latency reduction is achieved [13]. For the initialization of the matrices \\(A\\) we choose Gaussian initialization while the \\(B\\) matrices are initialized to zero [13]. Following this approach enables us to perform domain adaptation for different datasets consisting of diverse distributions."}, {"title": "IV. EXPERIMENTS", "content": null}, {"title": "A. Dataset Details", "content": "In this section, we assess the Strada-LLM model's predictive capabilities using eight real-world datasets. The description of these datasets is presented in Table II. Most of these datasets pertain to traffic speed. For the sake of simplicity, traffic speed is utilized as the primary traffic metric in our experimental analysis; thus one can presume \\(F = 1\\)."}, {"title": "B. Training Strategy", "content": "1) Augmentation Techniques: We adopted the time series augmentation techniques FreqMix[63] and FreqMask [63] to prevent over-fitting.\n2) Hyperparameters: The hyperparameters of the Strada-LLM model mainly include: learning rate, batch size, training epochs, the number of layers, context length, embedding dimension per head, number of heads, rope scaling, and number of parallel samples for greedy decoding. We adopt Adam as the optimizer [64]. In the experiment, we follow the weight decay le - 8 and set the learning rate to 0.001 [64], the batch size to 32, and the training epochs to 150 for each dataset. We set \\(k = 3\\) in the sub-graph extractor. Furthermore, based on the fact that Strada-LLM is a probabilistic model, we take samples and compute medians to compare with point-wise models. The number of samples is set to 100. In terms of complexity, our model contains 16 million parameters. The experiments are conducted on 2 Nvidia RTX 4090 GPUs."}, {"title": "C. Baselines", "content": "We compare the performance of the Strada-LLM model with the following baseline methods:\n\u2022 STSGCN [22]: Spatial-Temporal Synchronous Graph Convolutional Layers are used to process the traffic as a spatio-temporal tensor.\n\u2022 GMAN [23]: The gating fusion mechanism redefines the spatio-temporal attention block.\n\u2022 MTGNN [65]: Alternating use of graph convolution and temporal convolution modules.\n\u2022 GTS [25]: The underlying graph structure is learned among multiple time series, and the corresponding time series is simultaneously predicted with DCRNN [1].\n\u2022 STEP [66]: Adopts the graph wavenet model [10], integrated into the transformer backbone and pre-training scheme.\n\u2022 FlashST [14]: The data distribution shift is learned by leveraging prompt-tuning.\nTo compare with the baselines, we adopt MAE, RMSE, and MAPE prediction error [14]."}, {"title": "D. LLM Model Results", "content": "The long-term traffic prediction accuracy was evaluated across four datasets: PeMS-Bay, METR-LA, PEMS07(M), and Brussels. The last dataset is private. Strada-LLM, as an LLM should be benchmarked jointly for all datasets. So, we report the performance in two manners:\n1) knowledge adaptation: Like every LLM, we pre-trained Strada-LLM on the datasets PeMS03, PeMS04, PeMS07, and PeMS08 then fine tuned on the PeMS-Bay, METR-LA, PEMS07(M), and Brussels datasets in few-shot fashion; we refer to this approach as LLM setup.\n2) fully-supervised: We trained our proposed model on a single dataset in a fully-supervised fashion (referred to as Strada - LLM [Solo] in this section).\nThe prediction results for the datasets METR-LA and PEMS-Bay are demonstrated in Tables III and IV, correspondingly.\nThe ability to perform long-term forecasting and being robust to new domains are important qualities for an LLM. As shown in Tables III and IV, the proposed LLM has competitive performance compared to Step [66], which is the corresponding transformer-based fully-supervised method. On the other hand, compared to the best prompt-tuning method FlashST [14], taking the global structure of the graph into account brings forth a definite advantage. Moreover, although Strada-LLM and STEP [66] are both transformer-based models, the introduction of normalization blocks like RMSNorm [57] and Rotary Positional Encoding (RoPE) differentiates them. Additionally, while STEP [66] is an encoder-decoder model, Strada-LLM is a decoder-only model, which makes it optimized in terms of performance for inference [67]. As we can see in Tables III-IV, the Strada-LLM manages to outperform the baseline models on the aforementioned datasets. Specifically, on the PeMS-Bay dataset, our proposed model achieves an RMSE of 3.94 for 1-hour prediction, corresponding to an improvement of approximately 16% compared to the prompt-tuning baseline [14]."}, {"title": "E. Domain Adaptation Results", "content": "To rigorously assess the efficacy of Strada-LLM compared to prompt-tuning techniques, particularly in few-shot learning scenarios, we conduct a comprehensive evaluation. Our experimental design involves fine-tuning Strada-LLM for a duration equivalent to that used in the FlashST approach, as proposed in [14] on simple universal prompt tuning. The results of this comparative analysis are meticulously documented in Table V, which presents a detailed breakdown of performance metrics for both the Brussels and PeMS07(M) datasets. Upon careful examination of these results, it becomes evident that our Strada-LLM model demonstrates superior performance across the majority of the evaluation metrics when compared to the FlashST method. This improvement in performance serves as a strong indicator of the inherent advantages of our pre-trained, graph-aware LLM in few-shot learning scenarios. The observed superiority can be attributed to Strada-LLM's ability to effectively leverage its pre-existing knowledge of graph structures, allowing it to adapt more efficiently to new domain with limited training examples. These findings not only validate the effectiveness of our approach but also underscore the potential of graph-aware LLM compared to prompt-based forecasters.\nTo further validate the adaptability of Strada-LLM, we compare the performance of our proposed method with the most relevant transfer learning approaches from the literature, including:\n\u2022 Full fine-tuning: All of the trainable parameters of the model will be fine-tuned.\n\u2022 Topk fine-tuning [13]: Just fine-tune the last k layers.\n\u2022 LORA [13]: Using low-rank matrices to fine-tune the model.\nThere are some other methods like instruction tuning [68], which here are not applicable because of the used modality. Moreover, we intertwine LoRA [13] with our proposed model in order to be adapted to other datasets. We experiment with varying sample percentages of the target dataset, in this case PEMS07(M), to identify the saturation point in the few-shot learning process. The results are shown in Figure 4. The plot shows that training on at least 55% of the target data in a few-shot manner is sufficient to achieve good performance. This makes Strada-LLM a versatile LLM for urban data forecasting. Based on the superior performance on the Brussels city dataset, presented in Table V, one can conclude that Strada-LLM is robust when applied to a new city with a different data distribution."}, {"title": "F. Perturbation Analysis and Robustness", "content": "To evaluate obustness of the Strada-LLM model against noise, we conduct perturbation analysis experiments. We added two types of common random noise to the validation data during the experiment. Random noise obeys the Gaussian distribution \\(N(0, \\sigma^2)\\) where \\(\\sigma\\in (0.2, 0.4, 0.8, 1, 2)\\). Then, we normalize the values of the noise matrices to be between 0 and 1. The results are shown in Figure 6, where the horizontal axis represents o, the vertical axis represents the error, and different colors indicate different evaluation metrics. This benchmark is evaluated for the dataset PEMS04 but other datasets follow the same pattern. According to this experiment, our model is robust to Gaussian noise."}, {"title": "G. Ablation Study", "content": "1) Domain Adaptation: In this ablation study, we examine the domain adaptation capabilities of Strada-LLM by evaluating its performance when different datasets are used as source and target domains. Specifically, we trained Strada-LLM on one dataset (e.g., PeMS-04) and adapted to another (e.g., METR-LA) to assess its generalization across different types of traffic patterns. Table VI presents the results for the domain adaptation process when pre-trained on a source dataset and adapted to the corresponding target dataset. The results presented in Table VI indicate that there is a performance drop when the model is applied to a different target domain. This table can also address selecting efficient pre-training datasets to gain transferability advance for the targeted LLM. Take"}, {"title": "V. CONCLUSION", "content": "This research proposes a probabilistic-based LLM for traffic forecasting called Strada-LLM, which injects the graph structure implicitly into the input tokens. We utilize a subgraph extraction procedure to inject the graph structure into the transformer token inputs. On the one hand, we adopt a probabilistic transformer to predict the traffic data by sampling from the underlying distribution. On the other hand, we utilize a low-rank method for the transfer learning task. Consequently, we showcase its success in adapting to datasets with significant differences compared to the datasets used for pre-training. Finally, the model is robust to noise, interpretable, and scalable. Strada-LLM is compliant to a variety of different known LLM backbones, which make its architecture suitable to process spatio-temporal graphs. In summary, the Strada-LLM model successfully captures the spatial and temporal features from traffic data so that they can be applied to other spatio-temporal tasks. A potential avenue for future research could involve evaluating the expressiveness of the distribution head."}]}