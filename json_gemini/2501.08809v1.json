{"title": "XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework", "authors": ["Sida Tian", "Can Zhang", "Wei Yuan", "Wei Tan", "Wenjie Zhu"], "abstract": "In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is: https://xmusic-project.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence (AI) techniques have significantly advanced the field of AI Generated Content (AIGC), making it a prominent research area in recent years. AIGC fosters creativity, exploration, and innovation across diverse artistic domains. As an art form centered on sound, music is a significant component of AIGC. Automatic music generation has numerous potential applications, including adaptive soundtracks, video background music generation, music transcription, and royalty-free music creation, etc. Although recent studies (such as AudioLM [1], MusicLM [2], Riffusion [3], MusicGen [4] and Noise2Music [5], etc) have achieved success in terms of generating music within the audio domain, editing such generated music in its audio format remains challenging and unintuitive. In contrast, symbolic music, typically represented in MIDI format, offers greater flexibility, enabling users to modify specific musical elements explicitly. Thus, this paper focuses on music generation within the symbolic domain."}, {"title": "II. RELATED WORK", "content": "A. Artificial Intelligence Generated Content (AIGC)\nAIGC aims to utilize AI technology to automate content production while addressing human individual requirements. Recently, AIGC has demonstrated significant potential in generating high-quality content that closely resembles human-generated content (HGC), particularly in the areas of text generation [21]\u2013[23] and image synthesis [24]\u2013[26]. Despite recent progress, the field of music generation remains relatively underexplored within the AIGC community. AI-generated music still lacks the emotional depth and melodic richness typically found in human-composed music pieces. Recent studies [1]\u2013[5], [27] have focused on generating audio-based music from textual inputs. However, audio-based music generation faces challenges, such as limited editability and the inability to finely control attributes like tempo, pitch, duration, and rhythm. In contrast, symbolic music, which represents musical ideas through notation, offers more flexible and precise control over these attributes. Therefore, this paper focuses on music generation in symbolic domain. We present XMusic, a universal symbolic music generation framework that supports flexible prompts, and XMIDI, a large-scale symbolic music dataset annotated with precise emotion and genre labels.\nB. Symbolic Music Representations\nConventional symbolic music representations can be classified into two main categories: image-like [28], [29] and MIDI-like [8], [30], [31] representations. The image-like"}, {"title": "III. METHOD", "content": "Our proposed XMusic supports various types of content as prompts for generating high-quality music. As shown in Fig. 2, the process is divided into three stages: parsing, control, and selection. First, XProjector (Sec. III-A) analyzes the input content and parses it into symbolic music elements within the projection space. Second, XComposer (Sec. III-B) maps these elements to token sequences and controls the Generator to generate corresponding music. Finally, the Selector evaluates the quality of the generated music batches and selects the one with the highest quality score. The symbolic music dataset XMIDI is introduced in Sec. III-C.\nA. XProjector\nThe projection space of symbolic music elements, denoted as \\( P \\), acts as a bridge between multi-modal content and symbolic music. This space includes four types of symbolic music elements, emotions (\\( P^E \\)), genres (\\( P^G \\)), rhythms (\\( P^R \\)), and notes (\\( P^N \\)), represented as \\( \\{P^E, P^G, P^R, P^N\\} \\in P \\).\nThe emotion element \\( P^E \\in \\mathbb{R}^{D_E} \\) and the genre element \\( P^G \\in \\mathbb{R}^{D_G} \\) are expressed as one-hot vectors, where \\( D_E \\) and \\( D_G \\) denote the number of emotion and genre categories, respectively. In this paper, \\( P^E \\) can be chosen from 11 emotions: exciting, warm, happy, romantic, funny, sad, angry, lazy, quiet, fear, and magnificent. Similarly, \\( P^G \\) offers 6 common genre choices: rock, pop, country, jazz, classical, and folk.\nThe rhythm element spans the range of bars and is represented as \\( P^R = \\{p_i^{bar}\\}_{i=1}^{N_{Bar}} \\), where \\( N_{Bar} \\) denotes the total number of bars. \\( p_i^{bar} \\) represents the rhythmic component of the \\( i \\)-th bar and can be expanded as \\( \\{p_i^{beat_1}, ..., p_i^{beat_{in}}\\} \\), with \\( i_n \\) indicating the number of beats within the bar. Specifically, the bar element \\( p^{bar} = (bar, density) \\in \\mathbb{R}^2 \\) records the starting position and note density of the current bar, while the beat element \\( p^{beat} = (beat, tempo, strength) \\in \\mathbb{R}^3 \\) captures the starting position, tempo and intensity of the beat.\nThe note element covers the range of a single note and is defined as \\( P^N = \\{p_j^{Note}\\}_{j=1}^{N_{Note}} \\), where \\( N_{Note} \\) denotes the number of notes in the sequence. Each note element \\( p^n = (pitch, duration, velocity) \\in \\mathbb{R}^3 \\) represents the pitch, duration, and velocity of the note.\nWhen prompts from various modalities are input, specific symbolic music elements are activated, guiding the matching music generation process. As shown in Table I, inputs such as videos, images, or text with emotional tendencies activate the emotion element. Similarly, inputs containing temporal information, like videos or humming, activate the rhythm element.\nXProjector, as the core component of XMusic, analyzes multi-modal content and maps it into symbolic music elements within the projection space. The associated mapping function is denoted as \\( F_{XP} \\).\nImage prompts, characterized by their non-sequential nature, guide the music generation process by controlling the overall properties of the sequence. Specifically, XProjector performs sentiment analysis on the input image to determine its dominant emotion category and activates the corresponding emotion element within the projection space. This mechanism guides the generation of music aligned with the detected emotion. The image sentiment analysis module computes an emotion score \\( S_e(image) \\) for the input image and selects the emotion with the highest score. The calculation is as follows:\n\\[S_e(image) = \\lambda_1 * S_e^{ResNet}(image) + \\lambda_2 * S_e^{CLIP}(image) \\]\n\\[F_{XP}(image) = \\{P^E\\} = \\{argmax_{e \\in E} S_e(image)\\} \\]       (1)\nwhere \\( E \\) denotes the set of emotion categories. XProjector employs two models for this calculation. The first model is the well-established deep convolutional neural network ResNet [48]. Specifically, we utilize the ResNet-50 architecture to train an image emotion classifier on our large-scale image emotion dataset (details in Sec. IV-A). The classifier outputs the probability \\( S_e^{ResNet}(image) \\) for each emotion e. The second model leverages CLIP [49], a prominent image-text pre-training model. We compute the embedding similarity between the input image and synonymous textual descriptions of each emotion e. These similarities are then normalized via the Softmax function to derive \\( S_e^{CLIP}(image) \\). Weight factors \\( \\lambda_1 \\) and \\( \\lambda_2 \\) balance the contributions of the two models, with values set to \\( \\lambda_1 = 1 \\) and \\( \\lambda_2 = 2 \\) in our implementation.\nText prompts, inherently sequential data, are processed similarly to image-conditioned inputs. XProjector performs sentiment analysis to identify the dominant emotion in the input text and activates the corresponding element within the projection space. The text sentiment analysis module employs the SentenceTransformer [50] model to calculate embedding similarities between the input text and synonymous descriptions of each emotion e. These similarities are then normalized via the Softmax function to produce an emotion score \\( S_e(text) \\) as follows:\n\\[F_{XP}(text) = \\{P^E\\} = \\{argmax_{e \\in E} S_e(text)\\} \\]       (2)\nIn tag-conditioned music generation, users can select from 11 emotion tags and 6 genre tags to guide the process. Once a tag is chosen, XProjector activates the corresponding emotion or genre element within the projection space:\n\\[F_{XP}(tag) = \\{P^E\\} = \\{tag^e\\} \\]\n\\[F_{XP}(tag) = \\{P^G\\} = \\{tag^g\\} \\]      (3)\nVideo prompts, which are spatio-temporal data, guide both global and local attributes of the music sequence. For video-conditioned music generation, XProjector analyzes sentiment, motion, and scene transitions within the input video, mapping these factors to the appropriate rhythm and emotion elements in the projection space. This ensures a high degree of synchronization between the generated music and the video content.\nWe observe a significant correlation between video background music tempo and scene transition frequency. For example, montage videos with rapid scene transitions typically feature fast-paced music, while peaceful scenery videos are often paired with slower tempos. To formalize this relationship, we introduce a scene transition rate metric \\( R_{scene} \\) to control the music tempo \\( t_{music} \\):\n\\[R_{scene} = \\frac{N_{scene}}{T_{video}}\\]\n\\[t_{music} = t_{init} + t_{inc} * tanh(R_{scene}) \\]         (4)\nHere, \\( N_{scene} \\) denotes the total number of scene transitions (computed using PySceneDetect [51]), while \\( T_{video} \\) represents the video duration in seconds. The music tempo \\( t_{music} \\) (measured in bpm) is derived from \\( R_{scene} \\), with an initial tempo \\( t_{init} \\) and incremental tempo \\( t_{inc} \\). The tanh activation function ensures that the coefficient for \\( t_{inc} \\) remains between 0 and 1. Our analysis of tempo distributions in the training dataset shows that 98.6% of musical tempos fall within the 60~130 bpm range. Accordingly, we set \\( t_{init} = 60 \\) and \\( t_{inc} = 70 \\) to keep generated tempos within this range.\nRegarding emotions, we determine the emotional category of the input video through sentiment analysis and activate the corresponding emotion element within the projection space to control the emotional style of the music. The video sentiment analysis module computes an emotion score \\( S_e(video) \\) for the video and selects the emotion with the highest score as the analysis result. The calculation formula is as follows:\n\\[S_e(bari) = \\frac{\\sum_{m=1}^{N_{ipb}} S_e(image_m)}{\\frac{T_{video} * t_{music}}{60 * N_{bpb}}}\\]\n\\[S_e(video) = \\frac{\\sum_{bar=1}^{N_{bar}} S_e(bari)}{N_{bar}}\\]    (5)\nHere, we uniformly sample \\( N_{ipb} \\) frames per bar and compute an emotion score \\( S_e(image) \\) for each frame. These scores are averaged to obtain a bar-level emotion score \\( S_e(bar) \\). Given \\( N_{bpb} \\), the number of beats per bar, and using the video duration \\( T_{video} \\) along with the music tempo \\( t_{music} \\), we calculate the total number of music bars \\( N_{bar} \\). Averaging \\( S_e(bar) \\) across all \\( N_{bar} \\) bars yields the final video emotion score \\( S_e(video) \\). In this paper, \\( N_{bpb} \\) is set to 4, and \\( N_{ipb} \\) is set to 8.\nInspired by CMT [19], which establishes a correlation between fast motion and dense notes, we control the local music rhythm using video motion information. Specifically, XProjector extracts video motion information by calculating the optical flow \\( flow_t(x, y) \\). Due to the high computational complexity of optical flow, we use a more efficient PA [52] to model video motion. This average optical flow intensity within each bar, along with the visual beat saliency [53] for each beat, is mapped to note density and beat strength, respectively. Note that we also preserve their percentile distribution (based on the training set statistics) [19] within the projection space. The calculation formula is:\n\\[F_i = \\frac{\\sum_{x,y} |flow_t(x, y)|}{HW}\\]\n\\[N_{fpb} = \\frac{T_{video} * fps_{video}}{N_{bar}}\\]\n\\[density_i \\sim \\frac{\\sum_{t \\in bar_i} F_i}{N_{fpb}}\\]\n\\[strength_{i,j} \\sim vbsbeat_{i,j} \\]     (6)\nThus, the complete mapping relationship is expressed as follows:\n\\[p^{bar} (bar, density) = (\\frac{T_{video} * (i - 1)}{N_{bar}}, density_i)\\]\n\\[p^{beat}_{i,j} = (beat_{i,j}, tempo_{i,j}, strength_{i,j})\\]\n\\[\\qquad = (bar_i + \\frac{T_{video} * (j-1)}{N_{bar} * N_{bpb}}, t_{music}, strength_{i,j})\\]\n\\[F_{XP}(video) = \\{P^E, P^R\\}\\]\n\\[\\qquad = \\{argmax_{e \\in E} S_e(video), \\{\\{p^{bar}, \\{p^{beat}_{i,j}\\}_{j=1}^{N_{bpb}}\\}\\}_{i=1}^{N_{bar}}\\} \\] (7)\nwhere \\( i = 1, 2, ..., N_{bar} \\) and \\( j = 1, 2, ..., N_{bpb} \\).\nHumming prompts, which are sequential data, guide the generation process of complete music by first being transcribed into an initial MIDI sequence. XProjector employs the VOCANO [54] algorithm to transcribe input humming audio into an original MIDI sequence, i.e., \\( M_{origin} = VOCANO(humming) \\). This sequence is then processed using standardization operations (beat processing and note quantization) to create a standard prior MIDI sequence, i.e., \\( M_{std} = Standardize(M_{origin}) \\). For beat processing, the tempo of each beat is derived from the time intervals between adjacent beats, adjusting the default tempo of 120 bpm in the transcribed sequence to the actual tempo. Note quantization adjusts the onset and offset positions of each note to the nearest 32nd note position. Finally, information from \\( M_{std} \\) is organized and mapped to the corresponding note and rhythm elements within the projection space to generate the subsequent music sequences. The detailed formulas are as follows:\n\\[p^{bar} = (bar_i, density) = (T_{M_{std}} * N_{bpb} * (i - 1), D(M^{bar}_i))\\]\n\\[p^{beat}_{i,j} = (beat_{i,j}, tempo_{i,j}, strength_{i,j}) = (bar_i + T_{M_{std}} * (j - 1), M^{tempo}_{i,j}, S(M^{beat}_{i,j}))\\]\n\\[F_{XP} (humming) = \\{P^N, P^R\\} = \\{\\{M^{Note}_k\\}_{k=1}^{N^{Note}_{std}}, \\{\\{p^{bar}, \\{p^{beat}_{i,j}\\}_{j=1}^{N_{bpb}}\\} \\}_{i=1}^{N_{bar}}\\} \\]       (8)\nwhere \\( i = 1, 2, ..., N_{bar}, j = 1, 2, ..., N_{bpb} \\), and \\( D \\) and \\( S \\) denote the calculation formulas for note density and beat strength, respectively. \\( T_{M_{std}} \\) is the fixed beat length in \\( M_{std} \\), measured in seconds. \\( M^{bar}_i, M^{beat}_{i} \\), and \\( M^{tempo}_i \\) represent the i-th bar, the j-th beat within the i-th bar, and the tempo value for that beat in \\( M_{std} \\), respectively. These values contribute to calculating the rhythm elements.\nB. XComposer\nOur enhanced symbolic music representation, as shown in Fig. 3, maps MIDI files and the elements within the projection space to token sequences representing symbolic music, thereby assisting XComposer in the subsequent music generation and selection processes.\nXComposer follows the Compound Word (CP) [10] architecture, where tokens belonging to the same family (representing the same event) are grouped into a supertoken and positioned at the same time step. As illustrated in Fig. 3, XComposer introduces three key improvements:\nC. Objective Evaluation\n1) Metrics: We selected three typical objective metrics: Pitch Class Histogram Entropy (PCE) [59], Grooving Pattern Similarity (GS) [59] and Empty Beat Rate (EBR) [60]. Specifically, the PCE evaluates the distribution and uniformity of pitch classes within a musical piece or segment. A lower PCE indicates a more concentrated pitch class distribution, usually"}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\n1) Symbolic Music Generation: We developed XMIDI, the largest symbolic music dataset to date with precise emotion and genre labels. Each piece of music has an average duration of approximately 176 seconds, contributing to a cumulative dataset length of 5,278 hours.\n2) Image Emotion Recognition: We collected a new image emotion recognition dataset containing 269,793 images, among which 40,000 images are selected as the test set, and the remaining images are used for training. The images were gathered from various sources, including the WebEMO [57] image sentiment dataset, the Places365 [58] scene recognition dataset, and web searches from Baidu and Google. Given that the labels of internet images are inherently noisy, we employed human annotators to filter out samples that did not clearly correspond to their assigned labels.\n3) Text Emotion Recognition: We constructed a standard test set for text emotion classification, consisting of 1,100 sentences distributed evenly across 11 emotion categories.\nTo generate emotionally nuanced text, we instructed GPT-4 [22] to produce sentences that conveyed specific emotions without explicitly including emotion words. We applied SentenceTransformer [50] to compute text embedding similarities and removed redundant samples to ensure diversity and distinctiveness within the dataset.\n4) Music Quality Assessment: We utilized the Generator of XComposer to generate 9,540 music pieces by applying random combinations of emotion and genre tags as conditions. Subsequently, we conducted a manual quality evaluation of the generated pieces. High-quality music was characterized by coherent melodies, distinct tune fluctuations, and dynamic rhythmic variations. In contrast, pieces failing to meet these criteria did not achieve human-level quality standards.\nB. Implementation Details\nWe employ the Transformer architecture [6] as the backbone of XComposer. For the Generator, we utilize Transformer Decoder to predict subsequent symbolic music event based on the previous events. The model consists of 30 self-attention layers, each containing 16 attention heads, with a hidden size set to 1,024. During training, the Adam optimizer is employed with an initial learning rate of 3e-5. When the loss saturates (specifically at values of 0.053, 0.049, and 0.045), we halve the learning rate and resume training. The overall training procedure spans 24 days, using a batch size of 40 across 8 NVIDIA A800 GPUs. To mitigate gradient explosion, we set the maximum gradient to 0.5. For the Selector, we use Transformer Encoder to encode the token sequences and predict global attributes such as quality levels, emotions, and genres. We use 3 self-attention layers, 8 attention heads, and a hidden size of 512. During training, the Adam optimizer is employed with a learning rate of 1e-5, and the model training process is completed in 10 hours on a single NVIDIA Tesla V100 GPU."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a multi-modal controllable symbolic music generation framework called XMusic. This framework supports versatile prompts, such as videos, images, texts, tags, and humming. Music elements act as connectors between the prompt parsing and generation controlling processes, explicitly decoupling the control signal analysis task from the music generation pipeline. This design enjoys strong scalability, facilitating the integration of new modalities in a plug-and-play manner. Specifically, our XProjector parses multi-modal prompts into symbolic music elements within the projection space, while XComposer generates high-quality music aligned with the control conditions based on our enhanced symbolic music representation. Furthermore, we construct a large-scale symbolic music dataset called XMIDI with precise emotion and genre annotations for training the music generation model. Compared to the current state-of-the-art methods, XMusic achieves superior performance across all utilized objective and subjective evaluation metrics."}]}