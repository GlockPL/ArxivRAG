{"title": "UV-ATTACK: PHYSICAL-WORLD ADVERSARIAL ATTACKS FOR PERSON DETECTION VIA DYNAMIC-NERF-BASED UV MAPPING", "authors": ["Yanjie Li", "Wenxuan Zhang", "Kaisheng Liang", "Bin Xiao"], "abstract": "In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experi- ments show that UV-Attack achieves a 92.75% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly out- performing the state-of-the-art AdvCamou attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification.", "sections": [{"title": "1 INTRODUCTION", "content": "Adversarial attacks have become significant concerns in both digital and physical domains, impacting areas like facial recognition (Gong et al., 2023; Yang et al., 2023) and object detection (Huang et al., 2023; Li et al., 2023c). To improve the physical attack success rate, researchers have attempted to use 2D transformations-such as translation, rescaling, and shearing (Zhong et al., 2022)-and 3D modeling (Huang et al., 2024; Suryanto et al., 2022) to simulate changes in viewpoints for objects like stop signs and cars. However, compared with attacks on rigid objects, attacks on non-rigid objects like human beings are more challenging because of the variability in clothing distortions and human poses. These non-rigid distortions are difficult to simulate using basic transformations or static 3D modeling. Consequently, previous attempts at person detection attacks often relied on adding an adversarial patch to the front of the T-shirt (Xu et al., 2020; Wang et al., 2021b; Lin et al., 2023) or assumed minimal movement (Hu et al., 2022; 2023), which limits the real-world attack success rate when the subject undergoes significant movement and unseen poses."}, {"title": "2 BACKGROUND", "content": "Adversarial Attacks against Object Detection Recent work has employed elaborate adversarial examples to attack real-world image classifiers (Wang et al., 2023; Zhong et al., 2022) and object detectors (Zhang et al., 2023; Huang et al., 2023; Li et al., 2023c; Wang et al., 2022). Unlike digital attacks, physical attacks introduce greater complexity due to the physical variables. Early efforts to improve the efficacy using basic 2D transformations like translation, rescaling, and shearing (Zhao et al., 2019; Zhong et al., 2022). However, these approaches are largely effective only for planar objects such as stop signs. For 3D objects, more advanced techniques involving static 3D modeling and differentiable rendering have been employed to modify the textures (Wang et al., 2022; 2021a; Ravi et al., 2020). However, such methods are generally limited to rigid objects and fail to adequately address the dynamic and non-rigid targets. Specifically, static 3D models do not effectively simulate human poses, and the use of 2D and 3D Thin Plate Splines (TPS) Xu et al. (2020) has only been successful in modeling minor clothing distortions, falling short in replicating the impact of diverse human poses. Consequently, most previous person detection attacks have merely added an adversarial patch to the front of the T-shirt Hu et al. (2021); Duan et al. (2022); Tan et al. (2021) to avoid distortion"}, {"title": "3 METHODOLOGY", "content": "In this section, we detail the proposed UV-Attack. UV-Attack adopts the dynamic NeRF to model humans as digital representations and perform the optimization in the latent space of the diffusion model to generate the adversarial textures. In the following, we first introduce the background knowledge of dynamic NeRF and then present our methodology."}, {"title": "3.1 PRELIMINARY: DYNAMIC NEURAL RADIANCE FIELDS", "content": "NeRF enables novel-view photorealistic synthesis by modeling 3D objects as a continuous volumetric radiance field allowing control over illuminations and materials (Mildenhall et al., 2021). It can be formulated as $F : (x, d) \\rightarrow (c, \\sigma)$, where F is a MLP and takes the 3D coordinate $x \\in R^{3}$ and the view direction $d \\in R^{3}$ as input and outputs the corresponding color c and volume density \u03c3. Then NeRF computes the pixel color C(r) by accumulating all colors along a ray r = o + td that emitting"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Training Details. We evaluate the success rate of digital attacks on the ZJU-Mocap dataset (Peng et al., 2021). For the physical attack, we collected videos of five individuals and used SPIN and DensePose to extract SMPL parameters and pseudo-supervised IUV maps for training the UV- Volumes. The model training and attack implementation are conducted on a single Nvidia 3090 GPU. We utilize a pretrained stable-diffusion model finetuned on 256\u00d7256 images. The diffusion process was set to 10 steps. The Particle Swarm Optimization (PSO) ran for 30 epochs with 50 swarms, and the Adam optimizer ran for 300 epochs. We collected 100 different backgrounds from both indoor and outdoor scenarios. For each epoch, we randomly sampled 100 poses, camera and"}, {"title": "4.2 DIGITAL ATTACK RESULTS", "content": "Evaluation on the Unseen Pose Dataset (A) Dataset and baseline settings. To evaluate the ASR of different attacks under unseen pose scenarios, we randomly sample 1000 poses from the GMM and combine them with different backgrounds and camera and light conditions to construct an unseen pose dataset. Then we evaluate the average ASR on five different individuals from the ZJU-Mocap dataset, as shown in Figure 6. Because previous attacks did not consider the multi-pose scenarios, they cannot be directly evaluated. Therefore, we combine their methodologies with the UV-Volume model to generate images under different poses for test. For AdvCamou (Hu et al., 2023), we first train a Voronoi diagram using their methods and then apply it to the texture stacks under a fixed pose. Then we test the generated Voronoi diagram on the multi-pose dataset. For other patch-based attacks, we use a fixed pose and put the generated patch on the texture stacks for training, followed by testing on the multi-pose dataset. Additionally, we added two additional baselines. One is to use the latent diffusion model with fixed poses to train the adversarial textures, referred to as the LDM attack. The other is to sample frames from a multi-pose video and then extract the pose parameters for the UV-Volume, referred to as Oursvideo. (B) Results analysis. We present the test ASRs under different IoU thresholds in Table 1. All attacks were trained and tested on the same detectors. It indicates that"}, {"title": "4.3 PHYSICAL ATTACK RESULTS", "content": "To evaluate the physical ASR, we first capture videos from targets and then estimate the pose and camera parameters to train the UV-Volume. In contrast to AdvCamou which requires calibrated 3D meshes, our approach only needs a short video of the target, thus simplifying the attack process. We find that the trained textures are transferable to different people, thereby allowing us to fine-tune the latent zy trained on the ZJU-Mocap datasets instead of training from scratch, which greatly reduces the training time. We set the finetuning epoch as 100 in the physical attacks. Then we print the textures on fabric materials and manufacture long sleeves and pants. As shown in Figure 1, our attack is robust to changes in pose, view, and distance. For quantitative analysis, we compute the ASR through $ASR = 1 - N_{succeed}/N_{total}$, where $N_{succeed}$ and $N_{total}$ represent the number of succeeded and total frames, respectively. We trained the texture on FastRCNN and tested the real-world ASRs on multiple detectors. The physical attack success rate with different detectors and confidence thresholds is shown in Figure 3. We achieved a high success rate on FastRCNN (0.82), MaskRCNN (0.78), and Deformable DETR (0.65) when $T_{conf}$ is 0.5 and is consistent when $T_{conf}$ changes, while RetinaNet, FCOS and YOLOv8 are relatively sensitive to $T_{conf}$ 's change. The ASR for SSD is relatively low, which can be attributed to their fundamentally different model architectures."}, {"title": "4.4 ABLATION STUDY", "content": "The influence of modifying different body parts. Benefiting from the disentangled texture stacks, we can render arbitrary body parts, as shown in Figure 4. We evaluate the ASR of modifying different body parts and present these results in Table 3. As the coverage area of the adversarial textures increases, the attack success rate also progressively increases.\nThe influence of classifier-free guidance. We tested the impact of classifier-free guidance on the ASRs. We plot the mAP with and without classifier-free guidance on the white-box FastRCNN and the black-box objection detection models as the epochs progress, as shown in Figure 5. We observe that the diffusion model without classifier-free guidance can achieve lower mAP on the white-box and black-box models than using classifier-free guidance. We think this is due to the truncation effect, which normalizes the latent representations within the high-probability areas to improve image quality. This normalization restricts the generated images' noise levels and also reduces the transferability of untargeted attacks.\nThe influence of physical environment. To test the Attack Success Rate (ASR) under more complex real-world environments, we compared the effectiveness of our attack in both indoor and outdoor"}, {"title": "5 CONCLUSION", "content": "We introduce UV-Attack, a novel physical adversarial attack leveraging dynamic NeRF-based UV mapping. Unlike previous methodologies that struggled with dynamic human poses, UV-Attack computes the expectation loss on the pose transformations, making our attack achieve high ASR to variable human poses and movements. The use of UV maps to directly edit textures without the need for gradient calculations of the NeRF model significantly enhances attack feasibility in real-world scenarios. Our experimental results show that UV-Attack outperforms the state-of-the-art method by large margins on unseen pose datasets, achieving high ASRs in diverse and challenging conditions. Ultimately, our attack not only advances the research of person detection attacks but also introduces a new methodology for real-world adversarial attacks on non-rigid 3D objects."}]}