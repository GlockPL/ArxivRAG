{"title": "Building a Domain-specific Guardrail Model in Production", "authors": ["Mohammad Niknazar", "Paul V Haley", "Latha Ramanan", "Sang T. Truong", "Yedendra Shrinivasan", "Ayan Kumar Bhowmick", "Prasenjit Dey", "Ashish Jagmohan", "Hema Maheshwari", "Shom Ponoth", "Robert Smith", "Aditya Vempaty", "Nick Haber", "Sanmi Koyejo", "Sharad Sundararajan"], "abstract": "Generative Al holds the promise of enabling a range of sought-after capabilities and revolutionizing workflows in various consumer and enterprise verticals. However, putting a model in production involves much more than just generating an output. It involves ensuring the model is reliable, safe, performant and also adheres to the policy of operation in a particular domain. Guardrails as a necessity for models has evolved around the need to enforce appropriate behavior of models, especially when they are in production. In this paper, we use education as a use case, given its stringent requirements of the appropriateness of content in the domain, to demonstrate how a guardrail model can be trained and deployed in production. Specifically, we describe our experience in building a production-grade guardrail model for a K-12 educational platform. We begin by formulating the requirements for deployment to this sensitive domain. We then describe the training and benchmarking of our domain-specific guardrail model, which outperforms competing open- and closed- instruction-tuned models of similar and larger size, on proprietary education-related benchmarks and public benchmarks related to general aspects of safety. Finally, we detail the choices we made on architecture and the optimizations for deploying this service in production; these range across the stack from the hardware infrastructure to the serving layer to language model inference optimizations. We hope this paper will be instructive to other practitioners looking to create production-grade domain-specific services based on generative AI and large language models.", "sections": [{"title": "1 Introduction", "content": "The advanced capabilities of the latest Large Language Models (LLMs) in generating and interpreting highly coherent, human-like text unleash significant potential for diverse applications, including content creation for marketing, customer service chatbots, education and e-learning, medical assistance, finance, and legal support. However, deploying LLM-based applications carries inherent risks. There have been numerous incidents where LLM-based applications have erroneously enabled particular policies for the purchase of products, used offensive language, disseminated incorrect information, or even provided guidance on unethical activities such as suggestions for the best choice of arms for a particular type of activity. These risks underscore the critical need for robust safety and reliability measures, especially when LLM-based applications are used in production, be it in consumer domains or enterprise. Thus, developing LLM-based applications demands a tradeoff in harnessing"}, {"title": null, "content": "the general linguistic abilities of LLMs while simultaneously ensuring they strictly adhere to the specified behavior required for a particular application.\nGuardrails could be internal to LLM which means it has been trained and aligned to adhere to a particular policy, or it could be external where external rules or mechanisms can be applied to the input query to decide whether to proceed with the query and in what manner, and also monitor the output of the LLMs to check for adherence to the policy before it is sent to the user. The key challenge in developing efficient guardrails lies in clearly defining the requirements and expectations from the model. For instance, regulations differ by industry, country, and region. Additionally, ethical considerations such as fairness or the avoidance of offensive responses are challenging to concretely and actionably specify (Dong et al. (2024b), Ayyamperumal & Ge (2024)). Guardrails can be categorized primarily into the following types:\n\u2022 Domain Specific Guardrails: This set of guardrails deals with ensuring adherence of the output of the model to a particular context or domain. For example, in finance, the meaning and implications of \"securities\" is completely different from that in the IT operations domain.\n\u2022 Legal/Compliance Guardrails: Different domains have different compliance re-quirements and hence some actions or outputs are not allowed. For example in the healthcare domain, HIPPA disallows any release of personally identifiable infor-mation, or in education FERPA requires that no student records can be released to anyone without the consent of parents for children below the age of 18.\n\u2022 Ethical Guardrails: This guardrail deals with the general human and societal implications of the actions of a model. This includes aspects such as fairness, transparency, privacy etc.\n\u2022 Safety and Security Guardrails: This aspect of the guardrail aims to prevent harm and use of the model for wrongful purposes. This includes changing the behavior of the model through prompt injection, jailbreaking, as well as use of the model to perform malicious actions using different tools.\nThe metrics and nuances of guardrails in different domains are being actively studied, espe-cially the ones that are highly regulated such as finance (Narayanan & Vishwakarma (2024)), healthcare (Lopez-Martinez (2024) and education. In this paper, we examine the issues and share our experiences of building a guardrail model in the context of the education domain where guardrails are very important and requirements are fairly stringent. Implementing a real-time, production-grade guardrail LLM for the education domain presents its own significant set of challenges. This is because educational LLMs must meet unique needs such as 1) Complying with data privacy regulations like FERPA and COPPA, 2) Ensuring the safety and appropriateness of content, and 3) Delivering real-time responses in classrooms requiring low-cost and low-latency performance. To address these challenges and ensure the successful deployment of Safety and Appropriateness models in educational AI solutions, establishing clear and measurable performance targets (known as Service Level Objectives or SLOs) is crucial. These SLOs become part of a broader agreement called a Service Level Agreement (SLA). Furthermore, school districts may require AI developers to follow State level AI Policies/Legislation guidance (NCDPI releases guidance on the use of artificial intelligence in schools \u2014 NC DPI) to determine whether a given AI solution tool is safe and reliable to deploy in their infrastructure.\nRecent efforts to develop LLMs for generating human-like questions for educational as-sessments include (Wang et al. (2022); Elkins et al. (2023); Bulathwela et al. (2023)). Several attempts have also been made to use ChatGPT for generating educational content through prompt engineering Adeshola & Adepoju (2023); Baidoo-Anu & Ansah (2023) with some of them focusing on generating content related to schools Jauhiainen & Guerra (2023). How-ever, there exist certain challenges of using models like ChatGPT for generating safe and appropriate content related to the education domain (Rahman & Watanobe (2023); Kasneci et al. (2023)). As a result, while progress has been made in recent literature towards develop-ing LLMs for the education domain, there is still a dearth of research efforts ensuring safety and appropriateness while building educational LLMs."}, {"title": "2 Preliminary: Safety and Appropriateness Guardrail", "content": "There are several state-of-the-art works in recent literature that focus on developing Respon-sible Al models with safety as the primary goal. For instance, Madhavan et al. (2020) have outlined the policy considerations surrounding AI development. Recently, there has been literature on limiting general-purpose chatbots in the range of topics they can chat about according to the normative concept of appropriateness Kempt et al. (2023).\nHowever, there was none that combined the safety and appropriateness for educational needs. Many currently deployed educational chatbots leverage wrappers around ChatGPT, catering to both educators and students within or outside the classroom environment. How-ever, ensuring the safety and appropriateness of responses for students using these chatbots"}, {"title": null, "content": "on personal devices or for educators integrating them into classroom instruction remains a critical challenge in the education domain. Inconsistency persists in how these systems define 'appropriate' and 'safe' content for educational purposes. To ensure responsible deployment of AI in K-12 education, the model needs to encompass the following key elements:\n1. Prioritizes the safety requirements of the school district (for students and teachers) to prevent harmful content, such as hate speech, misinformation, bias, sexual, conspiracy theories, violence, scams, etc.\n2. Must also adhere to age-appropriateness by tailoring interactions to avoid complex topics and sensitive topics unsuitable for elementary-age school kids.\n3. Must have clear interpretability and explainability where applicable.\n4. Must employ curriculum-aligned content that can meet the grade-level develop-mental needs to be pedagogically aligned and appropriate.\n5. Must provide a baseline constitution that helps determine the safety and appropri-ateness for Education. It should be customizable or configurable to contextualize the local, state, and federal requirements for safety and appropriateness.\nThis paper focuses on the first 3 items. The rest are for future work."}, {"title": "3 Training and Deploying Production-Grade Safety LLMs", "content": null}, {"title": "3.1 Dataset Generation", "content": "To generate a dataset for evaluating query appropriateness, we identify diverse categories of queries encountered in elementary classroom interactions with a chatbot. These categories were delineated into safe (appropriate), controversial or irrelevant (inappropriate), and un-safe (inappropriate) topics. The dataset encompassed a broad spectrum of topics, including 100 unsafe topics (e.g., pornography, racist ideologies), 50 controversial or irrelevant topics (e.g., divorce and separation), and 120 safe topics (e.g., the advantages of recycling.).\nFurther data examination and enrichment added a variety of queries, including those related to explicit content, and safe personality and non-personality queries. In total, the dataset consisted of 23,304 instances. Upon reviewing the assumed labels of 'appropriate' or 'inappropriate', discrepancies in the precision of these labels were noted, leading to a comprehensive reclassification. This refined classification resulted in 14,251 'appropriate' and 9,053 'inappropriate' samples.\nText variability, including capitalization and punctuation modifications, was introduced to enhance the dataset's robustness. 25% of the enhanced dataset was then randomly selected, maintaining the original proportion of 'appropriate' to 'inappropriate' labels, for advance annotation using a state-of-the-art LLM. This portion of the dataset was used to train the model presented in this paper.1 We aim to develop an advanced model that can not only generate labels but also provide scores reflecting the relevance of the input to a set of significant attributes for determining appropriateness. The attributes used for this model were adapted from the Responsible AI Vertex AI Google Cloud2.\nFor instance, consider the following query: \"Do you think that all people who practice certain religions are extremists or radicals?\u201d The model's response was:\n{\"Appropriateness\": \"inappropriate\",\n\"Derogatory\": 0.2, \"Toxic\": 0.2, \"Violent\": 0.1, \"Sexual\":0,\n\"Insult\": 0.1, \"Obscene\": 0, \"Death, Harm & Tragedy\": 0.1,\n\"Firearms & Weapons\": 0, \"Public Safety\": 0.1, \"Health\": 0,\n\"Religion & Belief\": 1, \"Drugs\": 0, \"War & Conflict\": 0.1,\n\"Politics\": 0.2, \"Finance\": 0, \"Legal\": 0}"}, {"title": "3.2 Model Optimization Process", "content": "Our methodology focused first and foremost on achieving substantial model accuracy. Fol-lowing this, our goal was to ensure low inference costs, minimize latency, and maximize throughput, with initial considerations such as model size, prompt size, and token genera-tion volume deferred until after an adequately accurate model benchmark was attained.\nTo capture the degree of model optimization required, we considered the following steps: optimizing the base model to reduce parameters without sacrificing accuracy, minimizing token generation volume, and reducing prompt size.\nModel size Explorations were carried out across models ranging from 2 to 13 billion parameters, aiming to identify a model with the best trade-offs between accuracy, latency, and throughput capabilities (e.g., by applying flash attention technology). Our larger models (Llama2 13B and Mistral 7B) were trained on an A100 GPU with 80GB, employing QLoRA with 4-bit quantization and optimizing via the 8-bit Adam-W optimizer. Optimal training conditions were determined to be a learning rate of $1 \\times 10^{-1}$, applied over a cosine schedule with a dropout rate of 0.1, through multi-task training in 32-sample batches over 4 epochs.\nOutput token length optimization The optimization process sought to also ensure com-putational efficiency and resourceful utilization of inputs. In this vein, encoding strategies were revised to optimize token utilization, enabling a move from raw JSON format out-puts to more streamlined, encoded representations that significantly reduced the output token counts. Using this encoding, the JSON shown above might be encoded as \"true A2B2C1E1G1I1K10M1N2\", where dimensions with score 0 are discarded from the output. This encoded output can subsequently be decoded with facility in a downstream application, enabling the regeneration of the original JSON format.\nInput token length optimization We evaluated the model for variants of refined prompt interpretations under token-efficiency regimes. This entailed contrasting longer-prompt-based uncoded output generation against a scenario involving shorter prompts leading to coded outputs, highlighting the extents of trade-offs between succinctness and accuracy."}, {"title": "3.3 Deployment Optimization Process", "content": "We have two broad Service Level Agreements (SLA) requirements for application services. SLA-1 (1s): the most common use case requires 50 Queries Per Second (QPS) with a P50 latency of a second, 99.99% availability, and a tolerance for 1 in 10,000 requests error rate. The use cases had input token ranges from 500 to 1000. SLA-2 (3s) has a requirement of 3-second P50 latency with input token ranges from 1000 to 3000. Since the appropriateness model produces a classification output, it is deployed in non-streaming mode.\nThis section analyzes components that enable us to serve models within SLA criteria. Specifically, we investigate the impact of base model choice and how to best deploy the selected model cost-effectively on GPUs from a production requirements standpoint.\nTo understand the impact of these variations, it is important to identify the relevant metrics to record. LLMs generate responses in two phases: Prefill, which processes input tokens to produce the first output token, and decode, which autoregressively generates subsequent output tokens until a stopping criterion is met. Prefill is a compute-intensive step for building attention matrices and Key Value (KV) cache. The cache is used to speed up the decode phase. Based on results of Model optimization process (discussed in next section), we investigate the compute efficiency in these phases for variations in base models, GPU choice, sequence length variations, and decode length variations."}, {"title": "4 Results", "content": null}, {"title": "4.1 Model Optimization", "content": "The structured approach to model fine-tuning and optimization culminated in concrete evaluations depicting notable transitions in performance and execution efficiency.\nBase Model Performance Metrics To facilitate the selection of a suitable base model for our solution, we conducted training sessions using four distinct base models: Llama2 13B, Mistral 7B, Phi-2 2.7B, and Gemma 2B. Following the training, we evaluated their performance on a test set derived from the previously described dataset. The evaluation metrics encompass accuracy measures for each of the models considered in this study. The results are systematically presented in Table 1, which outlines the sensitivity (or recall), false positive rate (FPR), and F1 score in detecting inappropriate content for each model. Further investigation was focused on possibly enhancing the performance of our selected base model, Mistral 7B, through input diversity by including extended token size impulses."}, {"title": "Efficacy of Expanded Input Training", "content": "Upon modifying the input structure to embrace a mix of short and extended token size inputs, we observed suitably improved model performance as outlined in Table 2. These findings support the advantage of incorporating diverse training sets, conferring improved accuracy on the models."}, {"title": "Prompt Efficiency Evaluation", "content": "The concluding phase of the optimization process focused on evaluating the effects of prompt size reduction on the modeling performance indicators. This entailed examining how shortening the prompt length from 397 tokens to 100 tokens and altering the format of the output affected the model's precision and effectiveness. The objective was to ascertain whether these modifications could strike a balanced trade-off, optimizing operational efficiency without significantly deteriorating accuracy. The outcomes of this comparative analysis are presented as follows:"}, {"title": "4.2 Model Evaluation", "content": "We utilize two widely used datasets for toxicity detection. The first dataset is the Civil Comments dataset Borkan et al. (2019). It contains public comments collected from nearly 50 English-language news sites worldwide between 2015 and 2017 on the Civil Comment platform. We use the testing set, which consists of 97,320 samples; each sample can have zero or more types of toxicity, including severe toxicity, obscene, threat, insult, identity attack, and sexually explicit content. The second dataset is the Jigsaw Toxicity Prediction dataset Wulczyn et al. (2017). It comprises 153, 164 comments from Wikipedia, labeled by human raters for toxic behavior, including five categories: severe toxicity, obscene, threat, insult, and identity hate. In our benchmark, a sample (comment) is considered toxic if it contains at least one type of toxicity.\nTo evaluate the toxicity detection ability of models other than ours, we create a prompt for each sample using the template below. Once the model outputs are returned, we analyze them to identify the terms 'APPROPRIATE' and 'INAPPROPRIATE.' If the outputs do not contain any of these terms, we attempt to query the model again. After a maximum of five attempts, if the model still does not classify the sample, we consider it inappropriate.\nsystem_prompt \"You are a helpful assistant.\"\nprompt \"Label the following document as appropriateness or\ninappropriateness. The appropriate document is a document\nwithout any harm or any toxicity. Please return the\nlabel in \"APPROPRIATE\" or \"INAPPROPRIATE\" in one line.\nDocument: {text}\""}, {"title": "4.3 Inference Optimization", "content": null}, {"title": "4.3.1 Model Selection and GPU", "content": "In figures 2a and 2b, we plot a chart comparing latency (on the x-axis) and throughput (on the y-axis).4 This chart is similar to the roofline model for the algorithm performance. Since we couldn't establish a theoretical upper limit due to the complex nature of the LLM model and inference engine, we used it to derive our empirical analysis approach. We expect that with a throughput increase, a latency increase will follow (therefore, the slope is positive); as the throughput begins to saturate, latency will still increase; however, the slope tends to zero. When the slope has a positive gradient, it can imply the generation is memory-bound; when the slope approaches zero, it indicates that the generation has hit limitations and could be compute-bound. Finally, we select the model with lower latencies for Prefill and Decode and higher throughput. We pick the largest batch size in the memory-bound region or a batch size with the lowest latency in the compute-bound region.\nMistral 7B model, among other models, has the highest throughput of over 900 tokens per second at a maximum batch size of 16 with a p50 latency of 330ms. On Nvidia L4, Pythia 12B model, among other models, has better latency at batch size 8. Mistral 7B model has a better latency than Llama2 13B model at batch size 8 for similar throughput (120 tokens per second). Mistral 7B model reached the compute-bound at 8 batch size, where we saw a 2x prefill latency increase for the subsequent batch size interval until the out-of-memory limit was hit. Meanwhile, the Pythia 12B and Llama2 13B models hit the compute-bound at 4 batch size, after which we saw a drop in the prefill throughput and a 2x prefill latency increase. We saw a 2x latency increase between the batch size intervals of 1, 2, 4, 8, and 16. Mistral 7B model had higher prefill throughput, which peaked at 30 tokens per second, and batch size 8 had the lowest p50 latency of 267ms. Mistral 7B model has the best performance, with a p50 latency of 570ms (Prefill: 267ms + Decode: 303ms), a batch size of 8, and the potential to achieve 14 QPS (batch size * 1000/total latency) or higher (with support from horizontal scaling and optimizations such as in-flight batching)."}, {"title": "4.3.2 Sequence and Decode Length", "content": "In figure 2c, as the sequence length increases from 512 to 1024, we see a 2x decrease in the throughput and a 2x increase in latency on A100, while on L4, the latency increase is larger"}, {"title": "5 Related work", "content": "We briefly review related prior work in responsible AI and LLMs for safety, LLM-based systems developed for education, and production-grade LLMs developed for other domains.\nResponsible AI and LLMs for safety There has been a range of prior work investigating the development of responsible AI systems. Considerable effort has gone towards problems including robustness against adversarial attacks, interpretability, fairness, and privacy preservation Brundage et al. (2020); Murdoch et al. (2019); Jeong & Shin (2020); Al-Rubaie & Chang (2019); Xu et al. (2020); Sun et al. (2021); Deng et al. (2023), as well as addressing bias, ensuring fairness, and integrating ethical principles and designing for alignment with human values Selbst et al. (2019); Etzioni & Etzioni (2016); Liyanage & Ranaweera (2023); Kumar et al. (2024). Finally, there is work on methods for evaluating and certifying the safety of LLMs Zhang et al. (2023); Huang et al. (2023). In contrast to these model-safety efforts, in this paper, we examine the problem of detecting unsafe or inappropriate content in the context of K-12 education.\nLLMs for education Significant effort has gone into building specific education-related applications using LLMs and generative AI. This includes the use of LLMs for generating educational assessments Wang et al. (2022); Elkins et al. (2023); Bulathwela et al. (2023) and engaging learning content Diwan et al. (2023); Rodway & Schepman (2023); Adeshola & Adepoju (2023); Baidoo-Anu & Ansah (2023). There has also been work on investigating challenges in the use of such models for generating safe and appropriate content Rahman & Watanobe (2023); Kasneci et al. (2023). In contrast, in this paper, we examine the training of models for detecting and filtering unsafe content, while also safeguarding privacy concerns.\nDomain-specific generative AI in production Finally, there is prior literature on using generative AI for building production-grade domain-specific services for other domains. For instance, there is work on employing LLMs in healthcare Amin et al. (2023; 2024), industry and manufacturing Wang et al. (2023); Eloundou et al. (2023); Dong et al. (2024a), and other areas, e.g. Mangaonkar & Penikalapati (2024). While there are some common underlying issues across domains, such as cost, scalability, and the need for data, other issues need to be addressed in a domain-specific manner; this paper delves into specific education-related issues around detecting unsafe and inappropriate content."}, {"title": "6 Discussion and Future Work", "content": "In this paper, we have developed a domain-specific guardrail framework in production, with K-12 education being an application of this framework. This LLM-based service provides real-time and interpretable detection of unsafe or inappropriate content. There are multiple directions for future work; we now describe a few important ones. As described in Section 2, guidelines on what constitutes safe and appropriate content are contextual. There is variance in relevant local, state, and federal regulations and compliances. Accordingly, alignment to a baseline constitution, which enumerates governing principles and is customizable, is critical. The incorporation of such a constitution into our service is one key future direction. Metrics to measure the alignment of guardrails to different dimensions as described in Section 1 are essential to ensure objective measurement of guardrail performance in systems. A layered approach to ensure the effectiveness of the guardrails is needed where lower layers of guardrails fall back on more complex higher layers when complex reasoning or verification is required to ascertain whether a particular response is compliant with a regulation/policy or not. In future work, we also aim to extend our framework to other applications such as finance and healthcare, broadening its utility and impact."}, {"title": "A Appendix", "content": "Appropriateness Service Requirements In this section, we describe the context in which the appropriateness service is deployed, and the associated requirements. Note that, for the scope of this paper, we will limit ourselves to language-based systems generating textual artifacts; in general, the systems described in this paper can be extended to multimodal generative AI.\nFigure 3 is a high-level depiction of our education AI platform, and the role of the appro-priateness service. In general, the education AI platform consists of a number of services"}, {"title": null, "content": "that enable specific capabilities such as corpus-based question answering using retrieval-augmented generation (RAG) Lewis et al. (2021), content alignment with learning standards and objectives, question generation, and others. These services use large language models (LLMs) and content databases (with textbooks and lesson material, learning standards, curricula, image and video content, and other domain-specific artifacts) to generate tex-tual artifacts to fulfill service requests. In turn, these services can be used to compose solutions including tutors and chatbots, lesson and assessment generation, instructional recommendation and others.\nThe solutions shown in Figure 3 are typically, speech- and/or text-enabled, with inputs coming from voice and text based user interactions as well as uploaded document content. Given the importance of responsible AI in education settings, as described in the paper, all inputs and all generated artifacts need to be checked by the appropriateness service. This ensures that the system both responds suitably to unsafe inputs, and also does not generate unsafe responses. Note that the appropriateness capability can itself be a service exposed by the platform (analogous to AWS (2024); Cloud (2024)); but, beyond that, almost every other service deployed on the platform has need to invoke the appropriateness service. This, in turn, both amplifies the scale seen by the service, and also significantly increases the service-level objective (SLO) requirements that it should satisfy.\nWe now provide a more detailed description of appropriateness service requirements:\nInputs As described above, we limit ourselves to the case where the input to the service is text. The length and characteristics of text can vary significantly, depending on the consumer of the service. This includes (i) Chat messages created via user-AI dialog; (ii) Long interaction transcripts for instructional analysis (e.g. Demszky et al. (2021)) (iii) Documents input by solutions like assessment generation, content alignment etc.; (iv) Retrieved passages from content databases; and (v) Responses generated via LLM. The text length can vary from less than a hundred tokens to thousands of tokens. The service is expected to work on this variety of heterogeneous text and lengths. In our system, the service operates on a maximum length of 3K tokens. We find that chunking larger texts before processing yields both a more cost-efficient deployment, as well as more meaningful chunk-wise verdicts.\nOutputs There are two key requirements on outputs: (i) The service should return an overall verdict for whether the text is appropriate or not; (ii) It should analyze the content across several attributes related to safety/potential offensiveness, and return scores across those attributes (akin to AWS (2024); Cloud (2024); Anthropic (2024)).\nSLOS As described above, the appropriateness service is invoked by almost every other platform service, and (often multiple times) for almost every user interaction. Accordingly there are stringent SLOs on the performance of the service. The service is expected to handle a throughput of up to tens of thousands of queries per second, and have a small"}, {"title": null, "content": "total latency up to the maximum length of 3K tokens (e.g. less than two seconds per text chunk). Further, education workloads tend to be notably bursty as a function of time-of-day, and day-of-week; the service is expected to efficiently handle this burstiness by seamlessly up- and down-scaling with system load. This is especially essential to attain competitive cost per token."}]}