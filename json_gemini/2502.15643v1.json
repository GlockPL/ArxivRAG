{"title": "AutoTandemML: Active Learning Enhanced Tandem Neural Networks for Inverse Design Problems", "authors": ["Luka Grbcic", "Juliane M\u00fcller", "Wibe Albert de Jong"], "abstract": "Inverse design in science and engineering involves determining optimal design parameters that achieve desired performance outcomes, a process often hindered by the complexity and high dimensionality of design spaces, leading to significant computational costs. To tackle this challenge, we propose a novel hybrid approach that combines active learning with Tandem Neural Networks to enhance the efficiency and effectiveness of solving inverse design problems. Active learning allows to selectively sample the most informative data points, reducing the required dataset size without compromising accuracy. We investigate this approach using three benchmark problems: airfoil inverse design, photonic surface inverse design, and scalar boundary condition reconstruction in diffusion partial differential equations. We demonstrate that integrating active learning with Tandem Neural Networks outperforms standard approaches across the benchmark suite, achieving better accuracy with fewer training samples.", "sections": [{"title": "Introduction", "content": "Inverse design problems are inherently complex because they require determining the necessary inputs or configurations to achieve a specific desired outcome, often within systems governed by non-linear, multi-dimensional relationships. These chal- lenges are prevalent across various engineering disciplines for instance, designing an airfoil shape to meet certain aerodynamic criteria or designing photonic materi- als based on target optical properties. Such problems are frequently ill-posed, lacking unique solutions and being sensitive to data imperfections, which worsens their complexity. Consequently, there is a critical need for efficient computational tools and advanced methods such as optimization algorithms and machine learning to solve inverse design problems effectively, enabling engineers to innovate and refine designs by working backwards from desired performance specifications.\nDeep Neural Networks (DNN) have been very successful as inverse design models in science and engineering. Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Invertible Neural Networks (INNs) are some of the most prominent DNN-based inverse design approaches. However, both GANs and VAEs have drawbacks related to training difficulty, instability, and limitations in capturing the full diversity of possible designs while INNs introduce complex neural architecture components in order to be effective, and thus are harder to train.\nTandem Neural Networks (TNNs) offer advantages over VAEs, GANs, and INNs in inverse design tasks by enabling direct optimization of input parameters to achieve desired outputs, leading to more stable and straightforward training. They avoid issues like training instability, mode collapse, and difficulty capturing multi-modal outputs that commonly affect VAEs and GANs, resulting in more accurate, and interpretable designs that better meet specific target properties. Furthermore, TNNs do not require the complex neural architectures of INNs, making them more effective and efficient for exploring complex design spaces in inverse design applications.\nTNNs have emerged as a powerful tool for solving inverse design problems across various fields, enabling the efficient optimization of complex systems by mapping desired outputs back to input parameters. In the realm of photonics and nanopho- tonics, they have been extensively employed to design metasurfaces and nanophotonic devices with tailored optical properties. Studies have demonstrated the use of TNNs for optimizing metasurfaces for specific functionalities such as polarization conversion, absorption, and color filtering .\nIn addition to metasurfaces, TNNs have been applied to the inverse design of other nanophotonic structures, including optical nanoantennas and multilayer thin films. These studies showcase the versatility of TNNs in handling both continuous and discrete design parameters, facilitating the creation of devices with customized optical responses and advancing the capabilities in nanofabrication and materials science.\nBeyond photonics, TNNs have found applications in diverse areas such as porous media transport, radar engineering, wind turbine airfoil design, electronic integrated systems, and optical fiber design. In porous media transport, TNNs have been uti- lized to model multicomponent reactive transport processes, enhancing the accuracy"}, {"title": "AutoTandemML Framework", "content": "In this section, we mathematically define inverse design models, the active learning algorithm for multi-output regression and describe the components of the TNN. Fur- thermore, we explain how active learning and TNNs can be integrated into a single framework for efficiently solving inverse design problems.\n\nThe main objective of inverse design models is to infer design parameters that yield a known target value or desired property. Mathematically, the inverse design problem is defined as:\n\n$$x = f^{-1}(y)$$\n\nIn Equation (1), $x \\in R^d$ is the design vector we aim to determine, and $y \\in R^N$ is the target vector specified a priori. The function $f : R^d \\rightarrow R^N$ represents the forward model that maps design parameters to their resulting properties. In inverse design we seek to invert this function to find the design parameters that produce the desired target outcomes.\nInverse design problems are typically ill-posed, meaning that multiple values of x can yield similar values of y. This lack of a unique solution necessitates the use of computational methods and machine learning models to approximate the inverse mapping or to identify suitable design parameters through optimization techniques.\n\nActive learning is a machine learning approach that operates as a form of opti- mal experimental design, where computational methods are employed to identify and obtain data instances that will most significantly enhance the model's accuracy or per- formance. By strategically selecting the most informative data points typically those about which the model is least certain active learning seeks to maximize learning efficiency.\nMore specifically, the active learning algorithm aims to select input design vectors where the machine learning model M (also denoted as the forward model) exhibits the highest predictive uncertainty. This is formally expressed as an optimization problem:\n\n$$x_* \\in \\underset{x}{arg \\underset{X}{max}} u(x) = \\underset{x}{arg \\underset{X}{max}} \\sum_{j=1}^{d} \\sigma_j(x; M)$$\n\nHere, $u(x)$ represents the total uncertainty at input design vector $x \\in R^d$, computed as the sum of standard deviations across all d output dimensions for a multi- output regression problem. $x$ represents the ith optimal input design vector based on the maximum value of $u(x)$. The standard deviations are encapsulated in the vector:\n\n$$\\sigma(x; M) = \\begin{bmatrix} \\sigma_1(x; M) \\\\ \\sigma_2(x; M) \\\\ ... \\\\ \\sigma_d(x; M) \\end{bmatrix} \\in R^d$$\n\nThe algorithm proceeds by iteratively selecting the most uncertain inputs, query- ing the High fidelity (HF) surrogate H at these points, and updating the model accordingly. The detailed steps are outlined in Alg. 1.\nMore specifically, the algorithm begins with an initial dataset D of size $n_0$, where each design vector $x_i$ is evaluated using the HF surrogate H to obtain $f(x_i)$. The model M is initially trained on this dataset. In each iteration, while the total number of evaluations is less than the pre-defined evaluation budget $N_{max}$, the algorithm performs an optimization step to find k input points $x_*$ that maximize the total predictive uncertainty u(x) of M, as defined in Eq. 2, each subsequent optimization is started with a new random seed. The k points form the batch B. Each design vector $x_i$ in B is evaluated using the HF surrogate H to obtain new outputs, and the dataset D is updated accordingly. The model M is retrained on this expanded dataset before the next iteration begins, thereby incrementally improving its performance by focusing on areas where it is most uncertain.\nThe input design vector (x) and the HF surrogate H both vary depending on the specific benchmark problem under consideration. The specifics of the design vectors and responses for each benchmark are provided in Sec. 3.1, while detailed descrip- tions of the HF surrogates H are given in App. B. Details on the computational implementation of the multi-output regression active learning algorithm including"}, {"title": "Active Learning Enhanced Tandem Neural Networks", "content": "We aim to utilize the dataset $D = \\{(x, f(x))\\}$, generated through active learning, to train each component of the TNN. The complete AutoTandemML framework is illustrated in Fig. 1. Specifically, Fig. 1a depicts the active learning algorithm. As elaborated in the previous section, the goal of active learning is to employ the model M to generate design vectors x that maximize uncertainty, and then evaluate them using the HF surrogate H to form and update the dataset $D = \\{(x, f(x))\\}$.\nFurthermore, Figs. 2b and 2c illustrate the two segments of the TNN-the forward DNN (denoted as FDNN) and the inverse DNN (denoted as IDNN), respectively. The forward DNN approximates the mapping from input parameters to outputs, while the inverse DNN predicts the input parameters corresponding to desired outputs. By training these networks using the active learning generated dataset, we enhance the predictive accuracy and reliability of the inverse design process.\nMore specifically, in inverse design, the objective is to determine the input param- eters x that produce a desired output through a complex system. We start by training the FDNN, to approximate the mapping from inputs to outputs, $x \\rightarrow f(x)$. We then train the IDNN, which aims to predict the input $x = IDNN(f(x))$ corresponding to a given output f(x). During the training of IDNN, we utilize a loss function L that compares the original output f(x) with the output of the forward model when fed with the inverse prediction:\n\n$$L = L(f(x), FDNN(x)) = L(f(x), FDNN(IDNN(f(x))))$$\n\nMinimizing this loss adjusts the weights of IDNN so that the predicted input generates an output through FDNN that closely matches the desired output f(x).\nThis tandem training approach ensures that the inverse neural network IDNN not only seeks inputs that reproduce the desired outputs but also aligns with the learned mapping of the forward neural network FDNN. By incorporating FDNN into the loss function, we effectively regularize IDNN, promoting reliable inverse solutions. This method is particularly advantageous in scenarios where the inverse problem is ill-posed or where multiple inputs can lead to the same output, as it leverages the forward model's representation to guide the inverse predictions toward feasible and meaningful solutions."}, {"title": "Benchmarks and Numerical Experiments", "content": "In this section, we define the inverse design benchmark problems used to analyze the performance of the active learning sampling and evaluate the accuracy of the IDNN. We provide detailed descriptions of the numerical experiments conducted, including the sampling algorithms employed for comparison with the active learning approach. Finally, we outline the accuracy metrics used to assess the performance of each method and describe the inverse design validation procedure."}, {"title": "Inverse Design Benchmarks", "content": "In order to assess the performance of the active learning enhanced TNN, we develop a benchmark suite of three inverse design problems. The outline of the three problems is presented in Fig. 2.\nThe first problem is the Airfoil Inverse Design (AID), where the goal is to deter- mine both the flow and the design parameters of an airfoil based on its pressure coefficient distribution. In Fig. 2a the specifics of the AID problem are visualized. The dimensionless pressure coefficient curves are denoted as Cp, while the flow and the geometrical parameters include the Reynolds number (Re), the Angle of Attack (a), the maximum camber distance (m), position of the maximum camber (p), and the maximum thickness of the airfoil (t).\nThe second problem is the Photonic Surface Inverse Design (PSID), which aims to obtain laser manufacturing parameters corresponding to a desired spectral emissivity curve of the photonic surface (a metamaterial), visualized in Fig. 2b. The photonic surfaces are created by texturing the surface of the plain material (in our case the alloy Inconel) with ultra-fast lasers. The spectral emissivity curves are defined as e, and the laser manufacturing parameters are laser power ($L_p$) (W), scanning speed ($S_s$) (mm/s), and the spacing of the textures on the surface of the metamaterial ($S_p$) (\u03bcm).\nThe third problem is the Scalar Boundary Reconstruction (SBR), where the objec- tive is to recover the boundary conditions (CBC) of a scalar diffusion PDE using scattered measurements of the scalar field (c) within a two-dimensional domain, as shown in Fig. 2c. The scalar diffusion PDE models phenomena such as heat transfer or contaminant diffusion in a medium. The boundary conditions cbc represent the values of the scalar field (e.g., temperature or concentration) along the top boundary of the domain, which are unknown and need to be determined. The scattered mea- surements c are obtained at 30 interior points within the domain and provide partial information about the scalar field's distribution. These measurements are strategically placed to capture the behavior of the field within the domain. Both CBC and c are dimensionless quantities, normalized to facilitate computational analysis.\nA mathematical summary of the benchmarks including the size of the design spaces and formal definitions of the design vectors and objectives is provided in Table 1. Comprehensive details of all benchmarks, such as the mathematical and numerical background, simulation software used, experimental setup and datasets, as well as the training and validation details of the HF surrogates H, are given in Appendix B."}, {"title": "Numerical Experiments Setup", "content": "In this section, we outline the setup of the numerical experiments conducted. Specifi- cally, we detail the hyperparameters of the active learning process, and the sampling algorithms employed for comparison with the active learning approach.\nIn the active learning procedure, we employed two different algorithms to efficiently train the forward model M: the Random Forest (RF) algorithm and an ensemble of DNNs, referred to as Deep Ensembles (DE). Both algorithms were chosen because they provide uncertainty quantification capabilities essential for active learning. Imple- mentation details and hyperparameters for both algorithms are provided in App. A. We initially used only the RF algorithm to explore their performance across different benchmark problems. However, we found that it failed to accurately model the for- ward relationship in the SBR benchmark problem. Consequently, we selected the DE algorithm for the SBR benchmark, as outlined in Tab. 2.\nFurthermore, the maximum number of samples generated by the active learning procedure varied per benchmark as outlined in Tab. 2. The number of maximum samples corresponds to the number of times the HF surrogate H is queried for a response in order to generate a dataset and train the TNN components FDNN and IDNN. For all benchmarks, the active learning process was initialized with $n_0 = 20$ samples generated using the Latin Hypercube Sampling (LHS) algorithm, and a batch size k = 5 was used in each iteration of the process. To determine the optimal design vectors ($x_*$) (defined in Alg. 1) we utilize the Particle Swarm Optimization (PSO) algorithm (implemented in the Indago 0.5.1 Python module for optimization), where the maximum number of evaluations is set to 100, while all other hyperparameters of the PSO are set as the default recommended variables of the module.\nWe employed four different sampling algorithms to generate efficient datasets for training each component of the TNN and to compare their performance with the active learning approach. The algorithms used are Random sampling (R), LHS, GreedyFP (GFP) , and Best Candidate (BC) sampling.\nThe BC and GFP sampling algorithms were selected as they were determined to perform efficiently for an array of tasks such as surrogate modeling, hyperparameter optimization, and data analysis. Both the GFP algorithm and the BC algorithm generate sequences of samples by iteratively proposing candidate samples and selecting the one that is farthest from the set of samples selected so far; however, the key difference is that GFP uses a constant number of candidate samples at each iteration, while the BC algorithm generates an increasing number of random candidate samples"}, {"title": "AutoTandemML Framework Results", "content": "In this section, we present the results of the AutoTandemML framework applied to all three inverse design benchmark problems. To assess the performance of the active learning approach, we also compare these results with those obtained when the dataset used to train the TNN is generated using other sampling methods (R, LHS, GFP, and BC).\n\nFig. 4 presents the results of the AID benchmark. Specifically, the R\u00b2 metric results are shown in Fig. 4a, the RMSE results are displayed in Fig. 4b, and the NMAE results are illustrated in Fig. 4c. All three metrics are represented as box plots as they provide a concise visual summary of each metric's distribution by displaying their medians, quartiles, and outliers. The results are also summarized statistically in Tab. 3.\nFig. 4a indicates that all approaches perform similarly in terms of the $R^2$ score. However, Tab. 3 reveals that the active learning approach (IDNNAL) outperforms other inverse DNNs trained with different samplers when considering the mean $R^2$ score ($R^2$=0.93). Furthermore, the active learning approach exhibits the best performance regarding outliers of the obtained $R^2$ scores, as the minimum $R^2$ from the 30 runs is $R^2$=0.87, the close second best approach in terms of the $R^2$ score is IDNNGFP. Some models obtained a negative $R^2$ in as their worst performing run, indicating that their predictions did not explain any of the variability in the data and are inferior to just using the mean of the target variable as a predictor.\nIn Fig. 4b, we observe that when considering RMSE, the IDNNAL also performs the best among the other samplers. All other approaches exhibit outliers that increase the overall RMSE. In Tab. 3, while the mean RMSE of the IDNNAL is the best, the significant difference between the active learning approach and other sampling approaches becomes apparent when examining the maximum RMSE values. Specifi- cally, the maximum RMSE for IDNNAL is 0.1529, whereas, as a reference, the worst maximum RMSE is 0.4993 for IDNNBC.\nFinally, in Fig. 4c, the NMAE metric confirms that IDNNAL also outperforms the other approaches, achieving the lowest maximum NMAE (0.0459). This significant difference in maximum NMAE is further evident when comparing IDNNAL to other approaches, as observed in Tab. 3, i.e. the maximum NMAE for IDNNAL IS 0.0623, which is close to the mean NMAE of the IDNNBC (0.0604). Moreover, not only does IDNNAL perform best when considering the mean of all metrics, but it also exhibits the least uncertainty across the 30 runs, achieving the lowest standard deviation for all metrics (third column of Tab. 3). The details of the forward model (M) performance on the AID benchmark can be found in App. C.1.\n\nFig. 5 presents the results of the PSID benchmark. The $R^2$ metric is shown in Fig. 5a, the RMSE results are displayed in Fig. 5b, and the NMAE results are illustrated in Fig. 5c. The results are also statistically summarized in Tab. 4.\nFig. 5a indicates that IDNNAL outperforms all other sampling methods in terms of the $R^2$ score, and that it exhibits a very narrow interquartile range when com- pared to other samplers. Tab. 4 reveals that the difference between IDNNAL and the inverse DNNs trained with other samplers is substantial when considering the mean $R^2$ scores. Specifically, the mean $R^2$ for IDNNAL is 0.82, while the second-best performing approach, IDNNBC, has an $R^2$ of 0.60.\nIn Fig. 5b, we observe that when considering RMSE, IDNNAL also performs the best among the sampling methods and exhibits the narrowest interquartile range. All other approaches display increased RMSE values. In Tab. 4, it can be seen that both the mean RMSE and the maximum RMSE of IDNNAL are the lowest compared to the other sampling approaches. Specifically, the mean and maximum RMSE for IDNNAL are 0.056 and 0.075, respectively. The second-best performing approach, IDNNBC, obtains mean and maximum RMSE values of 0.080 and 0.16, respectively.\nFinally, in Fig. 5c, the NMAE metric confirms that IDNNAL outperforms the other approaches, achieving the lowest mean NMAE of 0.324. This significant difference in NMAE is further evident when comparing IDNNAL to other approaches, as shown in Tab. 4; specifically, the maximum NMAE for IDNNAL is 0.42, while the worst- performing approach, IDNNR, has a maximum NMAE of 0.92. Additionally, IDNNAL exhibits the narrowest interquartile range in the NMAE metric, reflecting its consistent performance across the 30 runs.\nMoreover, IDNNAL performs the best when considering the mean of all metrics and exhibits the least uncertainty across the 30 runs, achieving the lowest standard deviation for all metrics (third column of Tab. 4). The details of the forward model (M) performance on the PSID benchmark can be found in App. C.2.\n\nFig. 6 presents the results of the SBR benchmark. The $R^2$ metric is shown in Fig. 6a, the RMSE results are displayed in Fig. 6b, and the NMAE results are illustrated in Fig. 6c. The results are also statistically summarized in Tab. 5.\nFig. 6a indicates that IDNNBC outperforms all other sampling methods in terms of the $R^2$ score, achieving the highest mean $R^2$ of 0.885. The second-best performing approach, IDNNAL, has a mean $R^2$ of 0.860, as shown in Tab. 5. Although IDNNAL has a slightly lower mean $R^2$, it exhibits the narrowest interquartile range among all methods, suggesting more consistent performance across the 30 runs.\nIn Fig. 6b, we observe that when considering RMSE, IDNNBC again performs the best among the sampling methods, obtaining the lowest mean RMSE of 0.905. The second-best performing approach, IDNNAL, achieves a mean RMSE of 0.978. While IDNNAL has a slightly higher mean RMSE than IDNNBC, it exhibits the narrow- est interquartile range, indicating more consistent error rates across different runs.\nFinally, in Fig. 6c, the NMAE metric reveals that IDNNAL slightly outperforms the other approaches, achieving the lowest mean NMAE of 0.2002 compared to 0.2009 for IDNNBC. This suggests that, in terms of normalized absolute errors, IDNNAL provides marginally better performance.\nMoreover, IDNNAL demonstrates comparable performance to IDNNBC when con- sidering the mean of all metrics and exhibits the least uncertainty across the 30 runs, achieving the lowest standard deviations for RMSE and NMAE (as shown in the third column of Tab. 5). Additionally, IDNNAL exhibits the narrowest interquartile range for all metrics, reflecting its consistent performance across the 30 runs. These observa- tions highlight the effectiveness of the Active Learning sampling method in providing"}, {"title": "Conclusion", "content": "We introduced and investigated the AutoTandemML framework for inverse design problems in science and engineering. AutoTandemML synergistically combines active learning with TNNs to efficiently generate datasets for accurate inverse design solu- tions. We evaluated the framework on three benchmarks the airfoil inverse design, photonic surfaces inverse design, and scalar boundary reconstruction and demon- strated excellent performance across all. Compared to other sampling algorithms, the TNN trained with active learning outperformed others in two benchmarks and was competitive in the third. Notably, AutoTandemML offers reliable performance with low variability across repeated experiments, a significant advantage for inverse design applications.\nFuture research could explore applying AutoTandemML to other inverse design problems, enhancing the active learning component with more sophisticated uncer- tainty quantification methods, and developing hybrid approaches that combine active learning with best candidate sampling. Additionally, extending the TNN architecture to other deep neural networks like Graph Neural Networks could enable the framework to handle discrete and graph-structured datasets, opening new possibilities in areas like molecular inverse design. Ultimately, a comprehensive scalability study should be conducted to deepen our understanding of the problem dimensionality for which this approach is most effective."}, {"title": "Machine Learning Algorithms Hyperparameters and Training", "content": "In this section we present the hyperparameter and training details of the forward and inverse DNN models of the TNN. Moreover, we present all of the training details and the hyperparameters of the algorithms used for training the active learning model M and uncertainty quantification.\n\nBoth the forward DNN (FDNN) and the inverse DNN (IDNN) were configured with identical hyperparameters and training settings, except for the loss functions used, and the input and output layers reversed. Each network was implemented as a multi- layer perceptron (MLP) comprising five hidden layers with neuron counts of 64, 128, 256, 128, and 64 neurons, respectively. The Rectified Linear Unit (ReLU) activation function was employed in all hidden layers to introduce non-linearity into the models. Training was conducted over 2,000 epochs with a batch size of 32 and a learning rate of 0.001, ensuring efficient and stable convergence during optimization.\nBoth networks utilized the Adam optimizer for weight updates, a validation split of 10% to monitor performance on unseen data, and an early stopping mechanism with a patience parameter of 10 epochs to prevent overfitting. The RMSE loss function for both networks. All training procedures for the forward and inverse neural networks were carried out using PyTorch 2.3.0 with Python 3.10\nFurthermore, prior to training, we applied Min-Max scaling to both the input features and output targets using the MinMaxScaler function from scikit-learn version 1.2.2 . This preprocessing step normalized both the inputs and outputs to a range between 0 and 1, which helped improve the training process by ensuring that all variables contribute equally to the learning process.\nFor the inverse DNN training, we did not rescale the data anew. Instead, we reused the Min-Max scalers from the forward DNN training, but applied them in reverse order. Specifically, the scaler that was used for the outputs in the forward DNN was applied to the inputs in the inverse DNN, and vice versa for the scaler used on the inputs. This approach maintains consistency between the forward and inverse models and ensures that the scaling corresponds appropriately to the data being modeled.\n\nRFs ([4]) were employed as the model M during the active learning procedure for the AID and PSID benchmark problems. RFs function by constructing an ensemble of decision trees, each trained on a bootstrap sample of the original dataset. At each node in a tree, a random subset of input features is selected to determine the best split, introducing additional randomness that helps to reduce overfitting and enhance model robustness. Each individual decision tree generates its own prediction, and these predictions are then aggregated by averaging in regression tasks to produce the final output. To perform uncertainty quantification with the RF model, we calculate the standard deviation of the predictions from all individual trees in the ensemble."}, {"title": "Benchmark Models and Data", "content": "In this section, we provide comprehensive details on the construction of the inverse design benchmark datasets for all problems, as well as the training procedures and accuracy of the HF surrogates (H). For each benchmark, the HF surrogates were used to evaluate the sampled design points (either with active learning or with samplers used for comparison) in order to form the TNN training datasets.\n\nTo address the airfoil inverse design (AID) problem, we construct a dataset to train the HF surrogate H by varying key parameters of NACA 4-digit airfoils and flow conditions. Specifically, we vary the maximum camber m, the position of maximum camber p, the maximum thickness t of the airfoil, the Reynolds number Re of the fluid flow, and the angle of attack a. Note that m, p, and t are normalized by the chord length of the airfoil and thus take values between 0 and 1. The details on how to compute the geometrical coordinates of the NACA 4-digit airfoil shapes are given in the work by .\nFor each combination of these parameters, we obtain the pressure coefficient curve Cp by running simulations with XFOIL 6.99 software. XFOIL is a computational tool for the design and analysis of subsonic isolated airfoils, combining a panel method for"}, {"title": "Forward Model Benchmark Results", "content": "In this section, we present the results of the forward model M for all three inverse design benchmarks. Specifically, we compare the performance of M when trained using active learning with its performance when trained using other sampling methods (Ran- dom, Latin Hybercube sampling, Best Candidate sampling, GreedyFP sampling). The comparison results are presented through box plots and statistical summaries."}, {"title": "Forward Model Airfoil Inverse Design Results", "content": "In Fig. C4 and Tab. C5, we present the results for the AID benchmark. Across all three performance metrics-R\u00b2, RMSE, and NMAE the active learning approach (MAL) consistently outperforms the other sampling methods. Specifically, Fig. C4a displays the R\u00b2 score, where MAL achieves the highest value of R\u00b2=0.84, as reported in Tab. C5. Similarly, Fig. C4b shows the RMSE results, with MAL attaining the lowest RMSE of 0.147. Finally, Fig. C4c illustrates the NMAE scores, where MAL again achieves the lowest value of NMAE=0.065. These results, summarized in Tab. C5, confirm the superior performance of the active learning approach over the other samplers in the AID benchmark."}, {"title": "Forward Model Photonic Surface Inverse Design Results", "content": "In Fig. C5 and Tab. C6, we present the results for the PSID benchmark. Across all three performance metrics-R\u00b2, RMSE, and NMAE the active learning approach (MAL) consistently outperforms the other sampling methods. Specifically, Fig. C5a displays the R\u00b2 score, where MAL achieves the highest value of R\u00b2=0.92, as reported in Tab. C6. Similarly, Fig. C5b shows the RMSE results, with MAL attaining the lowest RMSE of 0.04. Finally, Fig. C5c illustrates the NMAE scores, where MAL again achieves the lowest value of NMAE=0.275. These results, summarized in Tab. C6, confirm the superior performance of the active learning approach over the other samplers in the PSID benchmark."}, {"title": "Forward Model Scalar Field Reconstruction Results", "content": "In Fig. C6 and Tab. C7, we present the results for the SBR benchmark. The perfor- mance of MAL differs slightly from that in the AID and PSID benchmarks. In the SBR benchmark, the best-performing algorithm for the forward model is MBC; how- ever, MAL comes as a close second in the R\u00b2 and RMSE metrics. Specifically, Fig. C6a displays the R\u00b2 scores, where MBC achieves the highest value of R\u00b2=0.97, as reported in Tab. C7, while MAL obtains a score of 0.96. Similarly, Fig. C6b shows the RMSE results, with MBC attaining the lowest RMSE of 0.40, while MAL achieves an RMSE of 0.41. However, Fig. C6c illustrates the NMAE scores, where MAL and \u041c\u0432\u0441 both achieve the lowest value of NMAE=0.09. These results, summarized in Tab. C7, demonstrate that MAL can also perform competitively."}]}