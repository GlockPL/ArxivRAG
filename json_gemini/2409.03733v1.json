{"title": "Planning In Natural Language Improves\nLLM Search For Code Generation", "authors": ["Evan Wang", "Federico Cassano", "Catherine Wu", "Yunfeng Bai", "Will Song", "Vaskar Nath", "Ziwen Han", "Sean Hendryx", "Summer Yue", "Hugh Zhang"], "abstract": "While scaling training compute has led to remarkable improvements in large language models (LLMs),\nscaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing\ncomponent is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly\nsampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity\ncan be mitigated by searching over candidate plans for solving a problem in natural language. Based\non this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across\nHumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then uses these observations\nto construct plans for solving the problem. By searching over plans in natural language rather than\ndirectly over code solutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet\nachieves a state-of-the-art pass@200 of 77.0% on LiveCodeBench, outperforming both the best score\nachieved without search (pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately\npredict performance gains due to search as a direct function of the diversity over generated ideas.", "sections": [{"title": "1. Introduction", "content": "The bitter lesson [40] famously posits that two forms of scaling trump everything else: learning and\nsearch. While recent advances in large language models have removed all doubt on the effectiveness of\nlearning, search has not yet proven its value for large language models, despite its success with classical\nmachine learning techniques [4, 7, 8, 10, 17, 37, 38].\nHere, we refer to search as any method of spending compute at inference time to improve overall\nperformance [28]. In this work, we focus our efforts on improving LLM search for code generation, one\nof the most important current applications of LLMs. We hypothesize the major bottleneck preventing\nwidespread use of search at inference time for code is a lack of high-level diversity in model outputs.\nThis lack of diversity is likely in part due to specific post-training objectives commonly used to train"}, {"title": "2. Related Work", "content": "We reiterate that search as defined in the context of our paper refers to any method which expends\ninference time compute to improve performance. We further specify planning as any form of high level\nobservation or abstract thought that assists a model in generating a final solution. Our work builds off a\nlong history of work in scaling search and planning."}, {"title": "2.1 Search in Classical AI", "content": "Classical search algorithms like breadth-first search, depth-first search, and A* search have been widely\nused for pathfinding, planning, and optimization [34]. More advanced search techniques like Monte\nCarlo tree search (MCTS) have achieved remarkable success in domains like game playing, enabling\nsuperhuman performance in Go [37, 38], poker [7, 8] and Diplomacy [17]. More recently, Jones [23] find\nscaling laws for the performance of AI systems in board games, where ELO improves logarithmically\nwith the amount of compute spent at inference."}, {"title": "2.2 Search with Language Models", "content": "Applying search on top of LLMs has been a topic of much interest, especially with an eye towards code\ngeneration [13, 25]. Historically, methods such as beam search significantly improved performance for\ntranslation systems [18]. Closer to the present day, several recent works have explored repeated sampling\n[5, 6, 12, 42] as a search method for improving performance. Repeated sampling is a method which\ndirectly generates candidate code solutions from the model many times at moderate to high temperatures\nin hopes that one of the resulting generations will be correct. However, although these works address\nthe roughly linear increase in pass@k with respect to log k, they only focus on the most basic version of\nrepeated sampling, without searching in idea space.\nWhen combined with a verifier, reward model, or other filtering algorithm to select the best generation\n(in cases where pass@k is not a viable metric due to lack of test cases), it is also known under the name of\nbest-of-n sampling [29]. Many works show somewhat good results under intelligent selection of such\na filtering algorithm [11, 12]. Recently, several approaches have demonstrated the power of repeated\nsampling. For example, repeated sampling from a small model can sometimes outperform taking a single\nsample from a large model on an equalized compute bases [39]. Unlike algorithms such as repeated\nsampling, which search over the output space, the key insight of PLANSEARCH is that it is far more\neffective to instead search plans over the latent idea space. By explicitly searching over different natural\nlanguage plans before generating the code, we significantly increase the diversity of the final code\noutputs and thus, the resulting pass@k scores for sufficiently large k.\nRegarding searching over plans in natural language, several approaches have also proposed generalizing\nchain-of-thought [41] reasoning into a search-like process, such as Tree of Thoughts [43] and Reasoning\nvia Planning [20]. However, prior methods have largely demonstrated effectiveness on somewhat\ncontrived problems designed to highlight the power of search, such as the game of 24, or classic\nplanning benchmarks such as Blocksworld [27], where both benchmarks are easier to solve by explicitly\nconsidering many options, and where the \u2018steps' over which to search over are fairly obvious. By contrast,\nmost real-world planning is used to assist in domains that are complex enough to benefit from, but"}, {"title": "3. Motivation", "content": "Coding is a powerful area in which search should excel. While search in other domains requires both\ngenerating many solutions and selecting the correct solution amongst all the resulting generations,\ncoding often only requires the former, as any valid piece of code can be tested via code execution against\ngiven test cases. This allows code search algorithms to sidestep many of the issues that plague search\nalgorithms for more open-ended domains (e.g. generating poetry) due to difficulty in selecting correct\nsolutions out of all the generated solutions."}, {"title": "3.1 Defining the Search Space", "content": "Perhaps the most important question for eliciting strong search capacities is determining which space to\nsearch over, as finding the proper layer of abstraction is critical to progress in the field. Prior approaches\nhave varied, with many people searching over individual tokens [44, 46], lines of code [24], or even\nentire programs [25]. We hypothesize that the key factor is obtaining the correct solution sketch, which\nwe define as a description of the correct program in natural language space. Intuitively, conducting\nthe reasoning process in natural language space allows us to effectively harness the training process\nof LLMs, which have observed many human reasoning traces in both pre- and post-training. Prior\nwork [41] has observed strong positive effects from being allowed to conduct such reasoning in natural\nlanguage, making it a natural place to search over. We describe two experiments providing evidence for\nthis hypothesis by testing on the LiveCodeBench benchmark using GPT-40-mini as our model."}, {"title": "3.2 Backtranslation", "content": "To investigate the hypothesis whether the idea space, instantiated as solution sketches, is the right area of\nexploration, a natural question is whether LLMs can correctly implement a correct code solution given a\ncorrect sketch. Inspired by approaches to backtranslation in machine learning [16, 32, 35], we experiment\nwith \u201cbacktranslating\u201d passing code solutions back into idea space. First, we generate code solutions\nusing GPT-40 to generate 1000 attempts to solve the problem and filter out problems without any passing\nsolutions. As we also do not have a dataset of correct solution sketches associated with each solution, we\ngenerate a candidate correct idea via backtranslation. We do this by feeding an LLM both the problem\nand code solution and asking the LLM to convert said solution into a natural language description of the\nsolution. Additionally, we vary the detail of the backtranslated idea via instructions to the LLM in the\nprompt (e.g. 'in w words'). A full description of the prompts can be found in Appendix J.1, alongside\nseveral example backtranslated solutions of various lengths.\nWe observe that prompting a model with a backtranslated idea significantly improves accuracy, increas-\ning with the length of the translated idea , which suggests that having a correct sketch is\nsufficient to produce the correct final solution with relatively high accuracy, even only after 10 tokens\nof backtranslated solution. This suggests that the correct direction of search is to explore through idea\nspace to maximize the chance of arriving at a correct idea."}, {"title": "3.3 Conditioning on Idea Quality", "content": "In a follow-up experiment, we prompt an LLM to generate its own sketches to solve LiveCodeBench\nproblems instead of providing it with golden ones via backtranslation. First, we generate 5 ideas per\nproblem using IDEASEARCH, defined in Section 4.1.2. For each idea, we then sample 25 candidate\nsolutions and measure their pass rate. For this experiment, we filter out any problem that GPT-40-mini\nsolves with either a 100% or a 0% solve rate, since such problems are either too easy or too hard for the\nmodel and would not be informative for this experiment. We end with 75 problems and 375 sketches."}, {"title": "4. Methods", "content": "We provide a description of the various methods of search we explore in our work. If additional\nbackground on competitive programming and related notation is desired, we provide more (optional)\ninformation in Appendix K."}, {"title": "4.1 Baselines", "content": ""}, {"title": "4.1.1 REPEATED SAMPLING", "content": "We consider the basic prompting approach as a baseline, in which we use few-shot prompting by\nproviding the LLM with a number of problem-solution pairs before asking it to solve the desired\nquestion [9]. A full example of the prompt is given in Appendix J.2. In code generation, the most\ncommon variant of search utilized is repeated sampling, where models are repeatedly sampled from\nuntil they generate an output that passes the test or the maximum number of samples is reached. Refer\nto the Related Work for more information (Section 2.2)."}, {"title": "4.1.2 IDEASEARCH", "content": "A natural extension of the REPEATED SAMPLING approach discussed in Section 4.1.1 is to avoid prompting\nthe LLM for the solution code immediately. This can be viewed as an application of the commonly used\n\u201cchain-of-thought\" prompting to programming problems [41], although we find that IdeaSearch shows\nnon-negligible performance boosts over standard \u201cchain-of-thought\" prompting (see Appendix E).\nIn IDEASEARCH, the LLM is given the problem P and is asked to output a natural language solution S of\nthe problem. Then, a separate instance of the LLM is given P and S, and tasked to follow the proposed\nsolution S to solve the problem P. The purpose of IDEASEARCH is to isolate the effectiveness of having\nthe correct \"idea/sketch\u201d for solving the problem. Empirically, we find that explicitly forcing the search"}, {"title": "4.2 PLANSEARCH", "content": "While both REPEATED SAMPLING and IDEASEARCH are successful and lead to improvement in the\nresults on benchmark results, we observe that in many of the cases, prompting multiple times (pass@k)\n(even at high temperatures) will only lead to small, narrow changes in the output code that change minor\naspects but fail to improve upon pitfalls in idea."}, {"title": "4.2.1 Prompting for Observations", "content": "Starting from the problem statement P, we prompt an LLM for \u201cobservations\u201d/hints to the problem.\nWe denote these observations as O, where, i \u2208 {1,...,n\u2081} due to the fact that they are first-order\nobservations. Typically, n\u2081 is on the order of 3 to 6. The exact number depends on the LLM output.\nTo use these observations to inspire future idea generation, we create all subsets with size at most 2 of\nS1 = {O,...,O\u2081 }. Each of these subsets is a combination of observations, and for clarity we denote\neach subset as C\u013c, i \u2208 {1 ..., l\u2081}, where l\u2081 = 1 + n\u2081 + (\u2082)."}, {"title": "4.2.2 Deriving New Observations", "content": "The set of all observations can be thus defined as a directed tree with depth 1, where the root node is P,\nand an edge exists for each C\u00b9 pointing from P to C. We then repeat this procedure from Section 4.2.1\non each leaf node C to generate a set of second order observations, S = {O\u00b9\u2081\u2081\u2081, O\u00b9\u2081\u2082}. To obtain\nsecond order observations, we prompt the model with both the original problem P and all observations\ncontained in C, framed as primitive observations that are necessary in order to solve P. The LLM is then\nprompted to use/merge the observations found in C in order to derive new ones.\nThe same procedure as Section 4.2.1 is used to create all subsets C\u00a1, for all i \u2208 {1, ..., l\u2081}. This process\nmay be arbitrarily repeated, but we truncate the tree at depth 2 for computational constraints.\nNote that there is no assumption any of the observations generated are correct. In fact, it is critical to\nnote that many of them may be incorrect. The observations merely serve to elicit the model to search\nover a more diverse set of ideas."}, {"title": "4.2.3 Observations to Code", "content": "After the observations have been made, they must be implemented as ideas before being translated into\ncode. For each leaf node, we prompt the model with all observations, along with the original problem\nP, in order to generate a natural language solution to the problem P. To add more diversity, for each\ngenerated idea, we generate an additional idea by supposing the idea is wrong, and asking an LLM to\ngive criticisms/feedback, thus increasing our proposed ideas by a factor of 2.\nThese natural language solutions are then translated into pseudocode, which are subsequently translated\ninto actual Python code. We take a more granular approach to reduce the translation error (which may\ncause the model to revert to its original mode, disregarding the reasoned-through observations). We\nprovide all prompts for all sections in Appendix J.4."}, {"title": "5. Experimental Results", "content": ""}, {"title": "5.1 Datasets", "content": "We evaluate our search methods on three benchmarks: MBPP+, HumanEval+ [26], and LiveCodeBench [22].\nMBPP [3] and HumanEval [13] are some of the most widely used code benchmarks in the field. However,\nsince both benchmarks provide only a few test cases, [26] updates both benchmarks with additional test\ncases that increase the benchmarks' robustness to reward hacking. LiveCodeBench is a benchmark for\ncoding that consists of competitive programming problems which typically require advanced reasoning\ncapabilities. Given the reality that coding data is often highly upsampled during pre-training [15, 30],\nLiveCodeBench differentiates itself from other benchmarks by taking care to segregate problems by\ndate to avoid data contamination concerns. For this paper, we use only the subset of problems between\nMay 2024 and September 2024 to avoid possibilities of contamination. We choose May 2024 as the\ncutoff date to ensure that our results with our best performing model (Claude 3.5 Sonnet) are not due\nto contamination, because Claude 3.5 Sonnet has a knowledge cutoff of April 2024. To ensure fair\ncomparison, we use the same cutoff for all models evaluated, even though the precise cutoff dates for\nother models may vary slightly from May 2024."}, {"title": "5.2 Experiment Details", "content": "For all search algorithms, we require that all output code be in the correct format specified, and we\nmark a solution as incorrect if it does not follow the intended formatting. The extracted code is then run\nthrough all tests of the program and marked as correct if and only if it passes all tests.\nAll models are run with temperature 0.9 and top-p of 0.95. Temperature was determined through a coarse\nhyperparameter sweep on REPEATED SAMPLING and IDEASEARCH from T \u2208 {0.0, 0.1, 0.2, ..., 1.2},\nwhich we describe in Appendix F."}, {"title": "5.3 Results", "content": "Our summarized results for REPEATED SAMPLING, IDEASEARCH, and PLANSEARCH can be found in\nTable 1, Figure 1, and Figure 5. Additionally, we plot our full pass@k curves for all methods, models, and\ndatasets in Appendix A. For sake of easy comparison, we also plot all relative gains compared to RE-\nPEATED SAMPLING@1 averaged over all models in Appendix C. For a compute-normalized comparison\nbetween REPEATED SAMPLING and PLANSEARCH, see Figure 18."}, {"title": "5.4 Public Test Filtering", "content": "Public test filtering is a method which only chooses samples out of the original pool n which pass the\npublic tests. This is particularly useful in settings such as code deployment where executing the full suite\nof tests may be computationally costly or otherwise undesirable (e.g. in a coding contest where every\nincorrect submission is penalized). Thus, instead of submitting all n codes, after public test filtering, only\ncodes ci would be submitted such that c\u2081(xj) = y; for all j \u2208 {1, ..., u}, where c\u2081(x) refers to the output\nfrom running the code on some input x. The primary effect of public test filtering is to shift the pass@k\ncurve leftward, since public test filtering will discard low quality candidate solutions that either fail to\ncompile or fail elementary test cases for the problem.\nAll problems in MBPP+, HumanEval+, and LiveCodeBench come with a few public tests which are\nusually used to sanity check any submissions. We can further improve performance by filtering on\nthese public tests before a final submission, as described. Applying public test filtering reduces the\nnumber of samples to achieve the same accuracy by tenfold: PLANSEARCH to achieve a 77.1% accuracy\non LiveCodeBench after just 20 submissions (pass@20) compared to a pass@200 of 77.0% without using\npublic filtering (see Figure 5). We provide full results for the other datasets in Appendix B."}, {"title": "6. Analysis", "content": "Our results suggest that both PLANSEARCH and IDEASEARCH outperform basic sampling by a wide\nmargin (Figures 12, 13, 14), with PLANSEARCH achieving the best score across all methods and models\nconsidered. We show the detailed pass@k results for each dataset in Figures 7, 8 and 9. We also compare\nwith Chain-of-Thought [41] in Appendix E. Interestingly, we find that IDEASEARCH performs somewhat\nbetter, which we speculate comes from differences in splitting solution sketch into two model responses,\ninstead of doing both chain-of-thought and code solution in one model response.\nInvestigating the differences in specific models, we notice that trends exhibited by the pass@k curves are\nnot uniform across all models; in fact, each curve seems unique. We hypothesize that these differences\nare in part due to changes in idea diversity, as investigated in Figures 6, 26, 27. From the figures, we\ncan see that our approximate diversity score accounts for much of the variance we see in the relative\nimprovement that arrives from scaling-up inference-time compute. This correlation holds across all\nmethods and models on the same dataset, thus suggesting that diversity score can be used as a proxy to\npredict for relative pass@k improvement. For further discussion on the specifics of the diversity score,\nsee Section 6.1.\nOne interesting point of observation is that PLANSEARCH often hurts pass@1 for several models, in-\ncluding most notably Sonnet 3.5 on LiveCodeBench, our best performing combination. Intuitively, this\nis because increasing the diversity across ideas likely dilutes the probability that any particular idea is\ngenerated, while simultaneously increasing the chance of having at least one correct idea within said\npool. Therefore, pass@1 may be slightly lower than usual, yet pass@k will likely surpass \u201cpools\" of ideas\nlacking diversity for this reason. See Figure 48 for a graphical intuition.\nFinally, in Table 1 and Figure 1, we present our main results normalized across attempts/completion,\""}, {"title": "6.1 Measuring Diversity", "content": "We find that diversity as measured in idea space is highly predictive of search performance, as measured\nby the relative improvement between a model/method's pass@1 and its pass@200 (Figure 6). While the\nmost common measure of diversity is entropy [36], entropy is insufficient for a number of reasons for the\nprecise setting of LLMs [21, 45]. As a simple example, consider two different language models, one of\nwhich generates minor variations of the same program while another generates a variety of programs\nwith different underlying ideas. Even if both models have the same entropy, the latter model will be\nsignificantly better when augmented with search capabilities.\nIn our setting, we measure diversity by grounding it in idea space using a simple pair-matching strategy\nacross all generated programs. Formally, suppose we have a pool of n code generations, {C1, ...,Cn}. We\nassume that each piece of code implements some sketch, which can be thought to exist in some latent\n'idea' space. We consider two sketches similar if they are within e of each other in this latent space, for\nsome choice of e. As such, in this space, c\u2081 having a similar idea to cj and similarly for cj and ck does not\nimply ci and ck share a similar idea.\nTo compute the diversity of such a given generation pool, we ask an LLM to judge the similarity of\ntwo ideas in the following manner. First, we construct each of the (2) pairs. For each pair (ci, cj), we\njudge (using an LLM) whether both c\u2081 and cj implement the same idea. We define this as the function\nS(ci, cj) \u2208 {0,1}, which evaluates to 1 if c\u2081 and c; implement the same idea and 0 otherwise. Our overall\ndiversity score for a particular problem is then defined as:\nD = 1 - \\frac{\\sum_{i<j} S(C_i, C_j)}{(\\binom{n}{2})} \\tag{1}\nModels that output programs that all implement the same idea will have a score of D = 0, while models\nthat output completely unique programs will have a score of D = 1. Overall, a score of D implies that if\ntwo codes are chosen at random, the probability that they are the same idea (as measured by the LLM) is\nD. In Appendix M, we describe this measure in additional mathematical depth.\nFor a particular method, our reported diversity score is simply the diversity score over all problems in\nthe considered dataset. For computational feasibility, for large n, we instead sample a subset of 40 codes\nand test all pairs from that subset instead. In order to test code samples, we first backtranslate using an\nLLM to express the code in natural language before comparing each pair using both the code and the\nbacktranslated idea. We detail the prompts used in Appendix J.5 and use OpenAI's GPT-40-mini as the\nsupporting LLM."}, {"title": "7. Limitations and Future Work", "content": "While PLANSEARCH substantially improves diversity over idea space at inference-time, fundamentally,\nimprovements in diversity should come at the post-training stage. This likely requires re-imagining the\npost-training pipeline for LLMs around search, instead of the current paradigm optimized for a single\ncorrect response. This may require both collecting high quality post-training data that is also sufficiently\ndiverse, and new learning objectives that do not aim solely to maximize the expected reward of a given\nresponse. We are optimistic around future work to design significantly improved post-training objectives\nthat maximize both quality and diversity and which specifically optimized to use inference-time compute\nto maximum effectiveness.\nIn terms of methodological improvements to PLANSEARCH, PLANSEARCH currently searches all leaf\nnodes in the search tree uniformly. Because of this, it becomes quickly intractable to go further than a\ncouple levels deep, and in our experiments, we are only able to go two levels down the tree. Several\napproaches based on Monte-Carlo Tree Search (MCTS), such as Tree of Thought [43] or Reasoning as\nPlanning [19], have suggested that some form of dynamic pruning and expansion of nodes can be very\nhelpful. We are optimistic that PLANSEARCH can be further improved by such methods. Furthermore,\nPLANSEARCH is a fairly elementary method taking advantage of the paradigm that searching over a con-\nceptual or idea space is an effective method to improve diversity, and thus, downstream task performance.\nIt is completely feasible to search at an even higher level of abstraction than observations, which may be\nused to inject even more diversity into the final generated outputs.\nPLANSEARCH and IDEASEARCH tradeoff a slight deterioration of pass@1 performance for a large\nimprovement in pass@k performance. However, in many such cases outside of code generation, it is\ninfeasible to run an LLM-based model for more than a few attempts at most. For example, in Figure 9,\nPLANSEARCH does not significantly outperform REPEATED SAMPLING until k \u2265 4.\nFortunately, many filtering algorithms exist, which implicitly bring pass@k (for high k) to pass@1 (or"}, {"title": "L. A Model of Repeated Sampling: Pass@k", "content": "Consider a simplified model of repeated sampling for code generation. Suppose we have a dataset\nD = {P\u2081, ..., P\u2081} with l problems. For some problem P\u2081, define the probability p\u2081 as the probability that\nour code generation model solves the problem P\u012f in one submission. The pass@k [13, 24] metric (for\nproblem Pi) is defined as the probability that our code generation model solves the problem P\u012f at least\nonce out of k submissions. Thus, if we know the true p\u2081 of our model, we may compute our pass@k\nsimply:\n\\text{pass@k}_i = 1 - (1 \u2013 p_i)^k \\tag{2}\n\\text{pass@k} = \\sum_i \\text{pass@k}_i / l \\tag{3}\nHowever, it turns out that for k > 1, the na\u00efve estimator as seen in Equation 2 is biased, if we sample\nni > k from our code model to solve Pi, ci \u2264 n\u012f are correct, and compute pi = ci/ni [13]. Instead, pass@k; \nis typically computed using the unbiased estimator:\n\\text{pass@k}_i = 1 - \\frac{\\binom{n_i - c_i}{k}}{\\binom{n_i}{k}} \\tag{4}\nNote that reporting pass@k on a dataset where l = 1 is rather pointless, since pass@k can be derived\nusing only pass@1\u2081 and n\u2081. Every curve, over a suitable range of k values, will look like the S-curve seen\nin Figure 47 (as k is plotted on a log scale)."}, {"title": "M. Mathematics of the Diversity Measure", "content": "While our choice of a diversity metric is intuitive, one should note that there are a number of intriguing\ndetails that result from our definition. In particular, it is not necessarily the case that a model that\noutputs k unique ideas out of n samples to achieve a diversity score of 1. Consider an example of n = 9\ncodes, separated into 3 cliques of 3, where each clique implements the same idea (and separate cliques\nimplement separate ideas). In this setup, 3 of ideas are unique, but in our metric, there are 3 matching\nidea pairs (and 9 total matching idea pairs) out of (2) = 36, for a diversity score of 1 \u2013 3% = 4."}, {"title": "N. Biased Estimator for Pass@K Due to Non-Independence of PLANSEARCH", "content": "From a pure theoretical standpoint, the expression is biased (if using the same interpretation), but it still\nleads to a similar interpretation\u2014computing the probability that a subset of size k drawn from the set of\nsamples we already generated contains at least one success. (These given samples were generated by\none run of PLANSEARCH.) As such, in theory, the estimator may be slightly biased in the PLANSEARCH\ncase when computing its true pass@k. In practice, we do not believe this to be a large concern, especially\nas our primary results feature a relatively large k = 200."}, {"title": "J. Prompts", "content": null}, {"title": "J.1 Backtranslation", "content": null}, {"title": "J.1.1 Backtranslate System Prompt", "content": "You are an expert Python programmer. You will be given an algorithmic question (problem\nspecification). You will return a high-level, natural language solution to the question,\nlike an editorial. You will NOT return any code. Be as creative as possible, going beyond\nwhat you think is intuitively correct."}, {"title": "J.1.2 Implement Backtranslation Idea", "content": "You are an expert Python programmer. You will be given a question (problem specification)\nand a natural language solution/tutorial that describes how to solve the problem. You will\ngenerate a correct Python program that matches said specification and tutorial and passes\nall tests. You will NOT return anything except for the program inside markdown codeblocks."}, {"title": "J.2 Repeated Sampling", "content": "You are an expert Python programmer. You will be given a question (problem specification)\nand will generate a correct Python program that matches the specification and passes all\ntests. You will NOT return anything except for the program inside Markdown codeblocks."}, {"title": "J.3 Simple Idea", "content": "You will given a competitive programming problem; please output a high-level description\nof how to solve the problem in natural language. Below are examples:\nExample input: PROBLEM DESCRIPTION HERE\nExample output: EXAMPLE OUTPUT HERE\nHere is the competitive programming problem: PROBLEM TO SOLVE\nBrainstorm a high-level, natural language solution to the problem above. Note that your\nintuition may lead you astray, so come up with simple, creative ideas that go beyond what\nyou would usually come up with and go beyond your narrow intuition. Brainstorming solutions\nthat do not seem intuitively correct IS CRUCIAL."}, {"title": "J.4 PLANSEARCH", "content": null}, {"title": "J.4.1 Prompt for Observation Part 1", "content": "You are an expert Python programmer. You will be given an competitive programming question\n(problem specification). You will return several useful, non-obvious, and correct observations\nabout the problem, like hints to solve the problem. You will NOT return any code. Be as\ncreative as possible, going beyond what you think is intuitively correct."}, {"title": "J.4.2 Prompt for Observation Part 2", "content": "You are an expert Python programmer. You will be given an competitive programming question\n(problem specification) and several correct observations about the problem.\nYou will brainstorm several new, useful, and correct observations about the problem, derived\nfrom the given observations. You will NOT return any code. Be as creative as possible, going\nbeyond what you think is intuitively correct."}, {"title": "J.4.3 Combining Observations", "content": "Here is a sample prompt from the function with placeholders:\nHere is the competitive programming problem:\nProblem statement placeholder\nHere are the intelligent observations to help solve the problem:\nObservation 1 placeholder\nObservation 2 placeholder\nObservation 3 placeholder\nUse these observations above to brainstorm a natural language solution to the problem above.\nNote that your intuition may lead you astray, so come up with simple, creative ideas that\ngo beyond what you would usually come up with and exceeds your narrow intuition.\nQuote relevant parts of the observations EXACTLY before each step of the solution. QUOTING\nIS CRUCIAL."}, {"title": "J.5 Measuring Diversity", "content": "You are an expert Python programmer. You will be given a competitive programming problem\nand two pieces of code which are attempts to solve the problem. For your convenience, you\nwill also be given the idea for each code, summarized in natural language. You will be asked\nto answer whether the ideas behind the code are the same. You must ONLY output 'Yes.' or\n'No.'"}, {"title": "K. Competitive Programming", "content": "Competitive programming is a popular subset of programming tasks that involve solving complex\nalgorithmic reasoning. Typically, problems consist of a problem statement (written in natural language)\nP, with associated tests: (xi, yi), i \u2208 {1,..., m}, for which any solution must pass all of them.\nThe number of tests m depends on the problem, but typically ranges on the order of 25 to 100. A small\nsubset of the tests are typically given to the solver (we call these public tests) to use as validation that\ntheir program passes simple cases. The rest of the tests are hidden. Solutions to the problems must\ngenerally pass all the tests to be considered correct. Formally, we let f(x) denote the output of said\ncode ran on input x. The solution code is considered correct (passing) if and only if f(xi) = yi for all\ni\u2208 {1,...,m}."}]}