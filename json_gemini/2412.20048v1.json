{"title": "CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language and Speaker Generation", "authors": ["Ji-Hoon Kim", "Hong-Sun Yang", "Yoon-Cheol Ju", "Il-Hwan Kim", "Byeong-Yeol Kim", "Joon Son Chung"], "abstract": "The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin.", "sections": [{"title": "I. INTRODUCTION", "content": "It is believed that over 60 percent of the global population speaks at least two different languages [1], [2]. In line with the recent trends in globalization, there has been growing in-terest in multi-lingual speech processing such as multi-lingual speech recognition [3], [4] or language identification [5], [6]. In particular, cross-lingual Text-to-Speech (TTS) has attracted a large amount of attention due to its a range of applications, such as creating language educational content, developing conversational AI agents, and dubbing foreign movies.\nCross-lingual TTS focuses on generating natural-sounding speech in multiple languages while preserving the unique voice characteristics of the target speaker (e.g., synthesizing fluent Korean, Chinese and Japanese speech in the voice of Joe Biden). However, compared to intra-lingual TTS, which achieves almost human-like generation quality, the quality of cross-lingual TTS still lags far behind [7]\u2013[9]. One main chal-lenge that degrades the generation quality of cross-lingual TTS is the language-speaker entanglement problem. Specifically, since it is common for each speaker in a training dataset to speak only one language, there is a substantial risk of speaker identity becoming intertwined with language information dur-ing the training process. In the extreme scenario where there is only a single speaker per language in the training data, the language identity perfectly matches the speaker identity. These entangled representations hinder natural cross-lingual speech generation when the language identity is switched during the inference process, leading to unexpected speaker characteristics or unnatural pronunciation in the generated cross-lingual speech.\nNumerous attempts have been made to disentangle language and speaker representations during training. Instead of us-ing language-dependent text representation (e.g., graphemes), some works explore text representations which can be general-ized cross multiple languages [10]\u2013[12]. Other works leverage domain generalization training techniques such as domain ad-versarial training [13] or mutual information minimization [14] or information bottleneck methods [15]. More recently, other works have utilized Self-Supervised Learning (SSL) speech representations based on the finding that SSL features capture only specific aspects of speech [16], [17]. Although previous studies have focused on decomposing language and speaker information, the decomposition is limited to the input features and does not fully address the entanglement problem. In other words, even if language and speaker representations are separated in the input token space, they are expected to be recombined when generating acoustic representations. This reintegration of separated representations prevents the synthesis of natural cross-lingual speech.\nIn this paper, we propose CrossSpeech++ which improves the quality of synthesized cross-lingual speech by decompos-ing language and speaker information in the output acoustic feature space. As depicted in Fig. 1, CrossSpeech++ breaks intricate speech generation pipeline into two simple generator: the Language-dependent Generator (LDG) and the Speaker-dependent Generator (SDG), each of which produces the corresponding representations in the output feature space. The language-dependent representations capture linguistic varia-tion in speech, such as pronunciation and intonation, while speaker-dependent representations characterize speaker at-tributes such as timbre and pitch.\nSpecifically, the LDG includes three components: Mix Dy-namic Speaker Layer Normalization (MDSLN), the Language-Dependent Variance (LDV) adaptor, and the linguistic adaptor. MDSLN modulates text features with randomly mixed speaker information, mitigating language-speaker entanglement. The LDV adaptor and linguistic adaptor model linguistic-related variations, enhancing robust cross-lingual speech generation. Similarly, the SDG comprises two modules: Dynamic Speaker"}, {"title": "II. RELATED WORKS", "content": "A. Speech Synthesis\nSpeech synthesis (text-to-speech, TTS), the process of syn-thesizing human speech from text, has a long history of innovation. With the development of deep neural networks, recent deep-learning based TTS models have shown remark-able speech quality compared to early concatenative [18] and statistical methods [19], reaching speech quality close to that of real human utterance [20]. Typically, these methods involve converting a text sequence into intermediate acoustic representations and then transforming them to an audible waveform using either an external vocoder [21], [22] or an internal decoder [23], [24]. They employ various backbone networks such as dilated CNN [25], RNN [26], [27], and feed forward transformer [28], [29].\nRecent advancements have prompted TTS research to explore various topics such as multi-speaker [30], lightweight [31], and cross-lingual TTS [13]. Among these topics, cross-lingual TTS, in particular, demonstrates inferior synthetic quality compared to intra-lingual TTS mainly due to the language-speaker entanglement problem. In this paper, we focus on improving the quality of cross-lingual TTS to achieve high-quality speech synthesis on par with intra-lingual TTS.\nB. Cross-lingual Speech Synthesis\nCross-lingual TTS, a branch of TTS, aims to produce natural speech in multiple languages while maintaining the same speaker identity. In comparison to intra-lingual TTS, the quality of cross-lingual TTS remains weak due to the challenges in producing accurate speaker timbre and natural-sounding foreign accents. The inferior quality of cross-lingual TTS primarily arises from the language-speaker entanglement issue [13]. To address this, numerous efforts have been made, which generally fall into two broad categories: one is to leverage language-agnostic input representation, and the other seeks to learn disentangled representation.\nInstead of relying on language-dependent input represen-tations such as graphemes, some works present their cross-lingual systems based on language-independent input repre-sentations, which can be commonly used for multiple lan-guages. Zhan et al. [11] employ the International Phonetic Alphabet (IPA) and demonstrate its superiority over language-dependent phonemes in enhancing the quality of cross-lingual TTS. Li et al. [10] adopt UTF-8 byte representations for encoding typographic information, distancing their system from language-specific constraints. Staib et al. [32] and Lux & Vu [12] utilize input representations derived from IPA articulation, specifically designed to maintain consistent topol-ogy across different languages. Furthermore, Saeki et al. [33] explore the cross-lingual transferability based on BERT-like multilingual language model [34], pushing the boundaries of cross-lingual transfer in TTS.\nAnother approach presents training strategy to learn disen-tangled language and speaker representations. Zhang et al. [13] employ domain adversarial training [35] to prevent the leakage of speaker information from text encoding. Xin et al. [14] leverage mutual information minimization loss [36] to remove common attributes between language and speaker represen-tation. SANE-TTS [37] proposes the speaker regularization loss to avoid speaker bias in text duration predictor, and Gen-erTTS [15] incorporates an information bottleneck to disen-tangle timbre and speaker style. More recently, DSE-TTS [16] and ZMM-TTS [17] utilize SSL-based speech representations, as their discretized features contain less speaker-dependent information. Although these previous works have attempted to address the language-speaker entanglement problem, the level of disentanglement has been limited to the input token space. To address this, in our previous work, CrossSpeech [38], we explicitly divide the speech generation pipeline into language-related and speaker-related components, with each generating the corresponding representation in the output feature space. In this paper, we further explore the advantages of splitting the speech generation to develop a more natural cross-lingual TTS system. Moreover, we incorporate additional language- and speaker-specific attributes to further enhance generation quality.\nC. Domain Generalization\nDomain generalization focuses on training models to per-form well on any unseen domain, which is not accessible during training. Numerous seminal works have collectively ad-vanced the field of domain generalization. Many cross-lingual TTS methods leverage domain generalization techniques with"}, {"title": "III. MODEL ARCHITECTURE", "content": "CrossSpeech++ is built upon FastPitch [7], a non-autoregressive TTS model whose encoder and decoder are based on multiple feed-forward transformer blocks. It takes a text sequence $x \\in R^L$ as input and produces a mel-spectrogram $y \\in R^{T \\times 80}$, where $L$ and $T$ denote the lengths of the text sequence and the output mel-spectrogram, respec-tively. We adopt the online duration aligner [41], which allows ground truth durations to be obtained without external sources. This online aligner not only enables efficient training but also, more importantly, removes the dependency on pre-computed aligners for each language, which is highly beneficial for extending languages in cross-lingual TTS [41]. In addition, we replace the transformer with conformer blocks [42] due to their capability to model rich features in a parameter-efficient way. To support multi-lingual and multi-speaker settings, we adopt trainable lookup tables for language and speaker.\nAn intuitive way to avoid language-speaker entanglement in cross-lingual TTS is to divide the generation pipeline into language-dependent and speaker-dependent parts [43], [44]. As illustrated in Fig. 2, CrossSpeech++ breaks the speech generation pipeline into LDG and SDG, which model the language-dependent and speaker-dependent representa-tions, respectively. Each generator includes multiple conformer blocks and other key components to obtain disentangled rep-resentations, which are described in the following sections."}, {"title": "IV. LANGUAGE-DEPENDENT GENERATOR", "content": "In order to produce language-dependent representations, we specifically design the LDG to include MDSLN. Additionally, to learn prosodic variations that are dependent only on lin-guistic information (e.g., pronunciation and intonation), we introduce the LDV adaptor and the linguistic adaptor. These collectively contribute to improving the quality of synthesized cross-lingual speech.\nA. Speaker Generalization\nThe key to high-quality cross-lingual speech synthesis is to produce text features that are not biased toward any specific speaker. To achieve this, we propose MDSLN, which is an extended module of DSLN [45]. DSLN adaptively modulates hidden features based on speaker statistics, rather than simply conditioning the speaker embeddings through summation or concatenation. Given hidden representations $h$ and speaker embeddings $e_s$, the speaker-conditioned representations are derived as follows:\n$DSLN(h, e_s) = W(e_s) * LN(h) + b(e_s), (1)$\nwhere * denotes 1D convolution, and LN refers to layer normalization. The normalized hidden feature space is then shifted according to speaker embedding statistics, the filter weight $W$ and bias $b$, which are predicted by a single linear layer using $e_s$ as input.\nIntuitively, the model can learn speaker-generalizable text features when the text features are continuously adapted by a random speaker during training. This allows the model to selectively capture essential text-related attributes, apart from speaker-related information. The adaptation is achieved by conditioning the text representation with random speaker infor-mation. Inspired by recent works [40], [44], [46], we introduce MDSLN to continuously refine the text sequence with random speaker information by mixing speaker distributions in the training data, which can be formulated as follows:\n$MDSLN(h, e_s) = W_{mix}(e_s) * LN(h) + b_{mix}(e_s), (2)$\nwhere $W_{mix}$ and $b_{mix}$ represent filter weight and bias for a randomly mixed speaker distribution. The mixed speaker statistics can be calculated as follows:\n$W_{mix}(e_s) = \\gamma W(e_s) + (1 - \\gamma)W(\\bar{e}_s), (3)$\n$b_{mix}(e_s) = b(e_s) + (1 - \\gamma)b(\\bar{e}_s), (4)$\nwhere $\\bar{e}_s$ is acquired by randomly shuffling $e_s$ along the batch dimension (see Fig. 3), and $y$ is sampled from a Beta distribution: $\\gamma \\sim Beta(\\alpha, \\alpha)$ (we set \u03b1 = 2 in our experiments). We substitute the LN at the end of the Language-dependent (LD) conformer encoder block with MDSLN.\nB. Language-dependent Variance Adaptor\nAlthough it is crucial to model rich speech variations to synthesize expressive speech, predicting these variations in a cross-lingual scenario is challenging due to the combi-nations of languages and speakers that are unseen during training [47], [48]. To address this issue, we introduce the LDV adaptor, which models text-driven speech variations, a common attribute across multiple speakers. This adaptor predicts binary pitch and energy variations, indicating whether these values rise or fall [11]. The LDV adaptor consists of three components: a duration predictor, an LD pitch predictor, and an LD energy predictor, all sharing the same architecture. Pitch and energy values are embedded using a single 1D con-volutional layer and are then added to the speaker-generalized text features. During training, we use the target values, while during inference, we rely on the predicted values. The target duration value is obtained through an internal aligner [41], and targets for the LD pitch and energy predictors will be detailed in the following paragraphs.\n1) Pitch: To obtain the target value for the LD pitch predictor, we first extract the ground truth pitch value for every frame using the PYIN algorithm [49]. Since pitch is inher-ently speaker-dependent, we refer to the ground truth pitch sequence as the speaker-dependent pitch sequence, denoted as $p^{(s)} \\in R^T$. We average $p^{(s)}$ across each input text token using the ground truth duration, and convert the averaged sequence (denoted as $\\bar{p}^{(s)} \\in R^L$) into a binary sequence. This results in the language-dependent (speaker-independent) target pitch sequence $p^{(l)} \\in R^L$. The conversion to a binary sequence is defined as follows:\n$p_i^{(l)} = \\begin{cases} 1, & \\text{if } \\bar{p}_i^{(s)} < p_{i-1}^{(s)}, \\\\ 0, & \\text{otherwise,} \\end{cases} (5)$\nwhere $p_i^{(s)}$ denotes the $i$th value of $p^{(s)}$, and $p_i^{(l)}$ represents the $i$th value of $p^{(l)}$ for $i \\in \\{1, 2, 3, ..., L\\}$. Using $p^{(l)}$ as the target, the LD pitch predictor is optimized with a binary cross-entropy loss:\n$L_{LDP} = \\sum_{i=1}^{L}[p_i^{(l)}log(\\hat{p}_i^{(l)}) + (1 - p_i^{(l)})log(1 - \\hat{p}_i^{(l)})], (6)$\nwhere $\\hat{p}_i^{(l)}$ denotes the $i$th predicted language-dependent pitch.\n2) Energy: We extract the speaker-dependent energy, $e^{(s)}$, by taking an average from a target mel-spectrogram along the frequency axis [50]. Similar to pitch, we average $e^{(s)} \\in R^T$ over every text token and compute the language-dependent energy $e^{(l)} \\in R^L$ by transforming the averaged sequence into a binary sequence. The Language-dependent Energy (LDE) predictor is also trained through binary cross-entropy loss between the predicted and target LD energy sequence:\n$L_{LDE} = \\sum_{i=1}^{L}[e_i^{(l)}log(\\hat{e}_i^{(l)}) + (1 - e_i^{(l)})log(1 - \\hat{e}_i^{(l)})], (7)$\nwhere $e_i^{(l)}$ and $\\hat{e}_i^{(l)}$ represent the $i$th the target and the predicted language-dependent energy value, respectively.\nThe enriched hidden sequence is upsampled according to the token durations and then fed to the Language-dependent (LD) conformer decoder. Note that the duration predictor in CrossSpeech++ learns general duration information because it takes speaker-generalized representation as an input. As proven in the recent study [37], this leads to predicting token duration independently from speaker identity and stabilizes the duration prediction in cross-lingual TTS.\nC. Linguistic Adaptor\nText-dependent speech variations likely encompass a variety of complex characteristics, motivating us to construct a linguis-tic adaptor that further enriches text-related acoustic attributes"}, {"title": "V. SPEAKER-DEPENDENT GENERATOR", "content": "To colorize speaker-specific attributes constituting one half of natural human speech, we construct an SDG that includes an SD encoder, an SDV adaptor, and an SD decoder. SD encoder effectively aligns the language-dependent representations to the speaker identity with the help of DSLN [45]. We stack conformer blocks for the SD encoder and replace the LN at the end of each conformer block with DSLN [45]. The following SDV adapter consists of the speaker-dependent pitch (SDP) and energy (SDE) predictor. This adds speaker-specific speech variations such as formants and stress patterns. We extract the speaker-dependent pitch $p^{(s)}$ and energy $e^{(s)}$ sequence as described in Sec. IV-B, and optimize the SD predictors using L1 loss:\n$L_{SDP} = ||p^{(s)} - \\hat{p}^{(s)}||_1, (9)$\n$L_{SDE} = ||e^{(s)} - \\hat{e}^{(s)}||_1, (10)$\nwhere $\\hat{p}^{(s)}$ and $\\hat{e}^{(s)}$ denotes the predicted speaker-dependent pitch and energy sequences, respectively. The speaker-dependent sequences are fed to the 1D convolutional layer and summed to the speaker-specific hidden feature. The SD decoder then produces speaker-dependent acoustic represen-tation. The output mel spectrogram is generated by adding language-dependent and speaker-dependent features after they are projected through a single convolutional layer.\nTo sum up, the overall training objectives ($L_{all}$) are given:\n$L_{all} = L_{mel} + L_{align} + \\lambda_{dur}L_{dur}\n+ \\lambda_{LDP}L_{LDP} + \\lambda_{LDE}L_{LDE} + \\lambda_{L}L_{L}\n+ \\lambda_{CTC}L_{CTC} + \\lambda_{SDP}L_{SDP} + \\lambda_{SDE}L_{SDE}, (11)$\nwhere $L_{mel}$ means L1 loss between the target and the predicted mel-spectrogram, $L_{align}$ denotes the alignment loss for the online aligner as described in [41]. $L_{dur}$ is L1 loss between the target and the predicted duration. In our experiments, we fix $\\lambda_{dur} = \\lambda_{LDP} = \\lambda_{LDE} = \\lambda_{L} = \\lambda_{CTC} = \\lambda_{SDP} = \\lambda_{SDE} = 0.1$"}, {"title": "VI. EXPERIMENTAL SETTINGS", "content": "A. Dataset\nWe conduct experiments on the mixture of the monolingual dataset in four languages: English (en-US), Chinese (zh-CN), Japanese (ja-JP), and Korean (ko-KR) as detailed in Table I. Since all the datasets have different environments, we resample all the audio to 16kHz and convert the corresponding tran-scripts to IPA symbols [63]. In our experiments, the dataset is split into 80%-10%-10% for training, validation, and test sets across all speakers. 80 bins mel-spectrogram is transformed with a window size of 1280, a hop size of 320, and Fourier transform size of 1280.\nB. Model Configuration\nAll the encoders and decoders in our method are based on conformer blocks. Except for the LD encoder, which consists of 4 conformer blocks, the other modules are composed of 2 conformer blocks each. Each conformer block is designed with a hidden dimension of 192 and a single attention head. We also set a hidden dimension of 192 for the language and speaker lookup tables, where each language and speaker ID is converted into a 192-dimensional embedding vector. The variance predictors share the same architecture, which consists of two 1D convolutional layers with ReLU activation, each followed by layer normalization and a dropout layer, as in FastSpeech2 [28]. Following recent work on voice conversion [64], the linguistic encoder includes a Convolu-tional Gated Linear Unit (ConvGLU) [65], and we add layer normalization at the end of the linguistic encoder to stabilize the linguistic feature prediction pipeline. We utilize pre-trained MMS from Hugging Face\u00b2, and our text predictor consists of 2 conformer blocks followed by a single projection layer. The total number of learnable parameters is 12M.\nC. Training Details\nCrossSpeech++ is trained for 500 epochs on 8 NVIDIA A6000 GPUs with a batch size of 128. We use the AdamW optimizer with $\\beta_1 = 0.8$, $\\beta_2 = 0.99$, $\\epsilon = 10^{-9}$, and an initial learning rate of $2 \\times 10^{-4}$, decayed by 0.999875 per epoch. Gradients are accumulated and the optimizer steps after every two batches to enhance training efficiency.\nD. Baseline Methods\nCrossSpeech++ is compared against recent cross-lingual TTS systems. All the systems are trained and evaluated with the same configurations, including training and test datasets. The output mel-spectrogram is converted to an audible wave-form by pre-trained Fre-GAN [66] vocoder.\n\u2022 FastPitch (FP) [7] is the backbone network of CrossSpeech++. We follow the official implementation of FastPitch\u00b3 with slight modifications. We incorporate trainable lookup tables to support multiple speakers and languages, and adopt the online duration aligner [41].\n\u2022 FP + DAT [13] adopts domain adversarial training (DAT) based on FastPitch. Given that the DAT speaker classifier proposed by Zhou et al. [40] can be easily applied to other systems, we integrated this DAT classifier into FastPitch.\n\u2022 FP + DAT + $L_{reg}$ [37] leverages speaker regularization loss ($L_{reg}$) along with the DAT classifier as in SANE-TTS [37]. Since the speaker regularization loss stabilizes the duration prediction process in non-autoregressive TTS systems, it can be applied to any non-autoregressive system that adopts a duration predictor. Therefore, we choose FastPitch as the backbone network.\n\u2022 CrossSpeech [38] is our previous work and serves as a strong and comparable baseline to our current model. While it shares similarities with CrossSpeech++ in dividing the speech generation process into speaker-independent and speaker-dependent modules, CrossSpeech++ introduces more speech variation (i.e., LD and SD energy). More importantly, it incorporates SSL-based linguistic information which is the key to improving speech quality."}, {"title": "E. Evaluation Metrics", "content": "We assessed the effectiveness of our method using extensive evaluation metrics, including both subjective and objective measures. We used 50 random speech clips for subjective evaluation (i.e., MOS and SMOS), and 300 samples for objective evaluations (UTMOS, SECS, and CER).\n\u2022 MOS stands for Mean Opinion Score. To evaluate the naturalness of audio, we performed a MOS test in which 30 domain-expert subjects were asked to rate the natural-ness on a scale from 1 to 5. Speech naturalness includes audio clarity and pronunciation accuracy.\n\u2022 SMOS denotes Similarity Mean Opinion Score. Similar to MOS, 30 raters assess the speaker similarity of speech pairs. The raters were instructed to focus solely on the voice similarity to the target speaker; high scores are given if the voices are similar, even if the quality of speech is degraded.\n\u2022 UTMOS [67] is an automatic MOS prediction neural network. While subjective evaluation is regarded as the gold standard in assessing speech naturalness [68], it requires high costs in terms of both time and money. As a remedy to this, UTMOS has been widely used because of its effectiveness in estimating subjective scores [69]\u2013[71].\n\u2022 SECS denotes Speaker Embedding Cosine Similarity. It measures how similar the speaker characteristics of the generated speech are to those of the target speech. We extracted speaker representation using Resemblyzer4 from generated and the actual speech, then computed the cosine simliarity between them.\n\u2022 CER stands for Character Error Rate, which measures the intelligibility of speech by comparing the predicted text of speech to the target text sequence. We obtained the transcriptions of speech using a publicly available automatic speech recognition (ASR) system [72] that is pre-trained on 680k hours of speech from 99 languages."}, {"title": "VII. RESULTS AND ANALYSIS", "content": "A. Quality Comparison\nTo show the effectiveness of CrossSpeech++, we compare the generation performance of CrossSpeech++ against that of recent cross-lingual TTS models on both cross-lingual and intra-lingual scenarios. For cross-lingual evaluation, we randomly sample four representative speakers per language, while all speaker IDs are used for intra-lingual evaluation. The results are listed in Table II. Above all, CrossSpeech++ achieves significant improvements in cross-lingual speech. In cross-lingual TTS, CrossSpeech++ obtains the best scroes in MOS as well as UTMOS and CER, which underscores the ability of CrossSpeech++ to generate highly natural speech. While CrossSpeech++ shows a slight decrease in similarity scores (SMOS and SECS) compared to our previous work, CrossSpeech, we posit that this difference is attributable to our method generating more precise pronunciation and accent driven by text inputs, which leads to a perceptible shift in speaker similarity to the target speaker using a different language. CrossSpeech++ also demonstrates superior quality compared to the baselines in intra-lingual cases, confirming that our method is beneficial not only in cross-lingual settings but also in intra-lingual scenarios.\nB. Analysis on Linguistic Features\nTo determine the optimal extraction pipeline for the tar-get linguistic features, we evaluate the output quality of CrossSpeech++ trained with linguistic features from different layers of MMS [53]. Specifically, we compare the linguistic features extracted from the 1st, 6th, 12th, 18th, and 24th layers. Table III presents the evaluation results on our validation sets, indicating trade-offs across different layers. When we inject hidden features from the earlier layers into LDG, it brings about language-speaker entanglement, resulting in the text embeddings learning pronunciation along with the corresponding native speaker information. While this leads to more intelligible speech (measured by CER), it results in degraded naturalness (measured by UTMOS) and speaker sim-ilarity (measured by SECS), which is not a desired outcome. However, when we utilize the hidden features from the latter layers, it contributes more to language-speaker disentangle-ment, leading to improved naturalness and speaker similarity. Therefore, we use the features from the 24th layer because it provides improved naturalness and speaker similarity with a slight reduction in intelligibility.\nC. Ablation Study\nWe investigate the effect of each CrossSpeech++ component by conducting an ablation study on its quality. We measure UT-MOS, SECS, and CER in both cross-lingual and intra-lingual cases. As indicated in Table IV, each component contributes to enhancing the quality of CrossSpeech++. Replacing MDSLN with the original LN in the LD encoder (w/o MDSLN) results in relatively small yet consistent degradation across all metrics in both cross-lingual and intra-lingual cases. This indicates that MDSLN helps to learn speaker-generalizable features and facilitates the training of the subsequent LDV adaptor. Moreover, the Linguistic Adaptor (LA) significantly improves naturalness and intelligibility in both cross-lingual and intra-lingual cases. While it slightly reduces speaker similarity, we presume this is due to residual speaker information entangled within the text representations. Removing audio perturbation hinders the effectiveness of LA in disentangling language and speaker information, resulting in noticeable degradation across all metrics. The absence of the Text Predictor (TP) when extracting target linguistic features also leads to inaccurate pronunciation. The importance of modeling LD and SD speech variations (w/o LDV and w/o SDV) is validated by the de-graded quality observed when these variations are overlooked.\nD. Qualitative Evaluation\nTo intuitively demonstrate speaker generalization capability of LDG and speaker transferability of SDG, we visualize the LD and SD features. Fig. 5 illustrates the LD and SD features derived from text inputs in two different languages (en-US and ko-KR) and spoken by four different speakers (EN, KR, CN, and JP). As evident from the figure, the LD feature does not contain speaker-specific characteristics (e.g., harmonics) and changes only according to the input text regardless of speaker information. On the contrary, the SD feature includes speaker-specific characteristics and varies with different speakers.\nE. Comparison with Zero-shot Models\nWe further evaluate our method in comparison to recent zero-shot cross-lingual models: VALL-E X [74] and XTTS-v2 [75]. Using the pre-trained checkpoints from the popular reproduction of VALL-E X5 and the official implementation of XTTS-v26, we generate audio samples in a zero-shot manner and compute UTMOS, SECS, and CER. Different from the experiments in Table II where we evaluate using all languages, we focus on English, Chinese, and Japanese sentences in this evaluation, as VALL-E X does not support Korean synthesis. The evaluation results are presented in Table V. Although CrossSpeech++ exhibits a slightly higher CER than XTTS-v2, our method consistently outperforms all zero-shot baselines in terms of UTMOS and SECS. This demonstrates that our ap-proach generates more natural-sounding speech with accurate speaker characteristics."}, {"title": "VIII. BROADER IMPACT", "content": "By leveraging CrossSpeech++, we can achieve various pos-itive societal impacts, such as creating educational resources for foreign language learning and developing conversational AI agents with multilingual capabilities, all while preserving a consistent speaker identity. However, it is crucial to recognize the potential threats that could arise from the misuse of this technology. These threats include the creation of hate speech and voice phishing attacks. Additionally, the ability to convert text to speech in multiple languages poses a risk of spreading misinformation globally in one's own voice, thus amplifying its reach and impact. These considerations highlight the ne-cessity of responsible use and the establishment of ethical guidelines in the deployment of cross-lingual TTS systems."}, {"title": "IX. CONCLUSION AND DISCUSSION", "content": "In this paper, we propose CrossSpeech++, which achieves high-fidelity cross-lingual speech synthesis with significantly improved speech naturalness. We observed remain language-speaker disentanglement in previous cross-lingual TTS sys-tems and addressed the issue by separately modeling language and speaker representations in the output acoustic features. Experimental results demonstrated that CrossSpeech++ out-performed standard methods both in cross-lingual and intra-lingual scenarios. Moreover, we verified the effectiveness of each CrossSpeech component by conducting an ablation study. CrossSpeech++ has demonstrated remarkable capabilities in synthesizing both cross- and intra-lingual speech com-pared to previous works. However, despite its advancements, CrossSpeech++ requires a substantial corpus of text-to-speech pairs to produce speech in a target language, making it less applicable to low-resource languages. Therefore, our future research will focus on developing effective strategies to deploy cross-lingual TTS systems, even in low-resource language."}]}