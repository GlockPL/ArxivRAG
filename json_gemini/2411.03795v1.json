{"title": "VQA2: Visual Question Answering for Video Quality Assessment", "authors": ["Ziheng Jia", "Zicheng Zhang", "Jiaying Qian", "Haoning Wu", "Wei Sun", "Chunyi Li", "Xiaohong Liu", "Weisi Lin", "Guangtao Zhai", "Xiongkuo Min"], "abstract": "The advent and proliferation of large multi-modal models (LMMs) have introduced new paradigms to video-related computer vision fields, including training and inference methods based on visual question answering (VQA). Video Quality Assessment (VQA), a classic field in low-level visual quality evaluation, originally focused on quantitative video quality scoring. However, driven by advances in LMMs, it is now evolving towards more comprehensive visual quality understanding tasks. Recent works in the image domain have proved that visual question answering can significantly improve low-level visual evaluation. Nevertheless, related work is almost nonexistent in the video domain, leaving substantial room for improvement. To address this gap, we introduce the VQA\u00b2 Instruction Dataset\u2014the first visual question answering instruction dataset entirely focuses on video quality assessment\u2014and, based on it, we propose the VQA2 series models. The VQA2 Instruction Dataset consists of 3 stages and covers various video types, containing 157,735 instruction question-answer pairs, including manually annotated and synthetic data. We conduct extensive experiments on video quality scoring and understanding tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art (SOTA) performance in quality scoring tasks, and their performance in visual quality question answering surpasses the renowned GPT-40. Additionally, our final model, the VQA2-Assistant, performs well across both scoring and question-answering tasks, validating its versatility.", "sections": [{"title": "1. Introduction", "content": "With the advent and rise of large language models (LLMs) and large multi-modal models (LMMs) [1, 14, 33, 63], the field of computer vision related to videos has entered a new era. Visual Question Answering (VQA) [2], serving as a crucial tool for modality alignment, is widely employed in both the training and testing stages. The paradigm of instruction tuning [33] using multi-modal instruction datasets, which encompass vast amounts of high-quality data, has significantly enhanced the performance of video LMMs in high-level visual tasks such as video understanding [30, 67] and video temporal analysis[25]. However, unlike high-level tasks that are closely related to video semantics information, low-level visual tasks that are more pertinent to video quality attributes (flicker, blur, stalling, etc) have not become the primary focus of research in these large models. Low-level Video Quality Assessment (VQA) aims to enhance models' ability to perceive and evaluate low-level visual signals in videos, improve the accuracy of quantitative video quality scoring, and provide more precise and nuanced quality understanding and analysis. This task holds broad application potential: quantitative scoring predictions can be widely utilized in streaming video transmission processes [23], serving as effective feedback. Additionally, understanding and analyzing local visual quality can be applied in the image/video generation domain as effective guidance for refining local generation details [27].\nUnfortunately, the majority of video quality assessment models focus solely on quantitative video quality scoring [36], entirely lacking the capability to understand and analyze quality details, which results in significant deficiencies in model versatility. Moreover, models equipped with low-level visual quality understanding and analysis capabilities almost exclusively apply to the image domain [66], lacking effective perception abilities for video-specific temporal and motion quality attributes. To address the gap above, we construct the VQA\u00b2 instruction dataset\u2014the first large-scale instruction dataset dedicated to video quality assessment based on visual question answering and develop the VQA2 series models based on this dataset.\nOur dataset construction and training methodology are designed to simultaneously enhance the model's capabilities in precise quantitative quality scoring and nuance quality understanding while striving to maintain its versatility. This approach ensures that the final model possesses both robust quantitative scoring and quality understanding capabilities. The dataset construction is divided into three stages:\n\u2022 Stage-1: Pre-training data subset centered on distortion recognition We design two types of distortion recognition\u2014spatial distortion recognition and motion distortion recognition\u2014utilizing annotated data in various datasets. The objective is to establish the model's foundational perception capabilities of different quality attributes.\n\u2022 Stage-2: Instruction tuning subset centered on video quality scoring We utilize the mean opinion score (MOS) from various existing datasets and transform quantitative scoring into the quality level, Which is more compatible with the training of generative language models [40].\n\u2022 Stage-3: Instruction tuning subset for video quality understanding In Stage-3, we utilize high-quality and diversified data that has been expanded through GPT-4 following human expert annotation to finetune the trained models in the first two stages. Our human annotations are centered on quality attributes, providing comprehensive depictions of video quality.\nOur core contributions are as follows: 1) We construct the first visual-question-answering-based instruction-tuning dataset for video quality assessment - the VQA2 Instruction Dataset. Our dataset encompasses 3 stages and includes over 150, 000 instruction pairs, covering a diverse range of video types such as user-generated content (UGC) videos, streaming videos, and artificial intelligence-generated content (AIGC) videos. This ensures both data adequacy and diversity. 2) We design a complete training strategy and construct the VQA2 series models, including"}, {"title": "2. Related Works", "content": "Classical (objective) video quality assessment tasks heavily rely on the MOS obtained from subjective experiments. These tasks include UGC-videos quality scoring and streaming videos Quality of Experience (QoE) prediction. UGC-videos quality scoring task involves datasets like [16, 18, 39, 43, 48, 59], which contain various authentic or synthetic distortions. Streaming video QoE evaluation datasets include the Waterloo-oSQoE series [7\u201310] and the LIVE-NFLX series which [4, 5, 15], they mostly use simulated transmission distortions (such as stalling and bitrate switching) as the main types of distortion.\nWith the widespread application of LMMs in low-level vision, video quality assessment tasks are no longer confined to scoring tasks. There is boosting demand for quality understanding and analysis of spatial and temporal in-context quality attributes. Accordingly, the requirements for the functional versatility of evaluation models have expanded, shifting from assessing a single video type to multiple video types.\nClassic video quality assessment models can be broadly divided into knowledge-driven and data-driven approaches. Knowledge-driven VQA models [7, 11, 37, 38, 42, 46, 47] rely on carefully designed features to evaluate video quality. In contrast, data-driven VQA models [3, 21, 24, 28, 32, 34, 44, 45, 49, 50, 52, 59, 64] primarily employ deep neural networks (DNNs) to extract features sensitive to video quality. With the application of LMMs, recent work has achieved higher performance. For example, Q-Align [54] attains high precision and generalizability in video scoring by normalizing and weighted-summing the occurrence probabilities of different quality levels.\nHowever, these models possess only the capability to score the overall quality of videos but almost entirely lack the ability of video quality understanding and analysis, with no capabilities to provide reasonable responses to diverse question formats. Our proposed VQA2 series models can perform precise video quality scoring while exhibiting strong capabilities in video quality understanding and question answering, marking new progress in enhancing model functional versatility."}, {"title": "3. The VQA2 Instruction Dataset", "content": "As the foundation of our work, we propose the VQA2 Instruction Dataset. The dataset is composed of three stages: Stage-1 is used for pretraining, while Stages-2 and Stages-3 focus on improving the model's capabilities in video quality scoring and video quality understanding, respectively. The data construction pipeline is shown in Fig. 2."}, {"title": "3.1. Videos Preparations", "content": "To ensure the diversity of video content in the instruction data, the videos collected for the VQA2 Instruction Dataset are sourced from various image/video datasets [5, 7, 9, 16, 17, 19, 31, 43, 59], providing diverse visual content and a wide range of quality variations. We sample varying numbers of images or videos from each dataset and determine the sampling proportion of different quality levels according to the original distribution of quality in each dataset. This approach is chosen because almost all source datasets exhibit a normal distribution of quality, and uniform sampling fails to capture a sufficient number of videos across all quality levels. More importantly, we believe that videos with moderate quality levels contain both positive and negative quality attributes, making them more valuable for annotation compared to videos with high or low quality levels. The sampling information and the statistic information of our dataset are presented in Tab. 1."}, {"title": "3.2. Distortion-recognition Based Pretraining Set", "content": "Many classic multimodal models [29, 33, 41, 58] follow the well-established pretraining-finetuning paradigm. Since we believe that distortion is central to low-level quality assessment, the accurate perception and analysis of distortion types are fundamental to achieving high performance on such tasks, we design VQA\u00b2 instruction dataset Stage-1, a distortion-recognition-centered instruction set, to serve as the model's pretraining instruction set. Our distortion recognition design includes both spatial and motion distortions. For spatial distortion recognition, we sample 7, 174 images from the KonIQ-10K [19] and KADID-10K [31] datasets. Using the distortion type annotations from [62], we identified 11 different types of spatial distortions: \u201ccompression artifact\u201d, \u201cspatial blur\u201d, \u201cmotion blur\u201d, \u201cnoise\u201d, \u201coverexposure\u201d, \u201cunderexposure\u201d, \u201clow contrast\", \"high contrast\", \"oversaturation\u201d, \u201cdesaturation\" and \"block effect\". For motion distortion recognition, we focus on the most common distortion-\u201cflickers (camera shake)\u201d and \u201cstuttering\u201d. We use 34 videos with flicker distortion from the LIVE-Qualcomm [16] dataset and 30 videos featured with stuttering distortion in the LSVQ training part and extend them by extracting temporal and spatial clips, yielding 5, 211 video clips primarily containing motion distortions.\""}, {"title": "3.3. Instruction Set for Quality Scoring", "content": "Scoring the overall video quality has always been a key focus of video quality assessment tasks. Additionally, an accurate assessment of overall video quality forms the foundation for understanding and analyzing local quality factors within the video. To this end, we design the VQA\u00b2 instruction dataset Stage-2: a finetuning subset primarily aimed at scoring the overall video quality. We used the training part of [59] as the video source for the UGC video quality scoring task, and [5, 7, 9] as the video sources for the streaming video QoE scoring task. To ensure the subjective experimental data from each dataset is on the same scale, we normalize the subjective MOS scores to a range of 0-100. After scaling, We transform the video quality into five quality levels: \u201cHigh\u201d, \u201cGood\u201d, \u201cFair\u201d, \u201cPoor\u201d and \u201cLow\", with each level representing a 20-point interval. This approach minimizes the impact of inconsistent quality distributions across datasets. The format of the instruction pair for UGC videos in the dataset is as follows:\nUSER: <Video> + How would you evaluate the quality of this video?\nASSISTANT: The quality of this video is [Level].\nFor the streaming video datasets, since rebuffering and stalling are the major distortion types, we added temporal stalling information in the instruction set. We design two formats for presenting the temporal stalling information. The first format simply uses a \"0/1\" sequence to directly indicate stalling for each video frame. In the second format, the stalling information is summarized as follows: the total number of stalling events, the duration of each stalling event, the proportion of stalling events duration to the total video length, the initial buffering time, and the time elapsed between the end of the last stalling event and the end of the playback. The format of the instruction pair for streaming videos is as follows:\nUSER: <Video> + Stalling Information+How would you evaluate the quality of this video?\nASSISTANT: The quality of this video is [Level].\""}, {"title": "3.4. Instruction Set for Quality Understanding", "content": "The primary part of the dataset is VQA\u00b2 instruction dataset Stage-3: the first comprehensive instruction dataset for low-level video visual quality understanding and question-answering. We selected 14005 videos from the training part of LSVQ and LSVQ (1080p) [59], 498 videos from the LIVQ-VQC [43], and 998 videos from the AIGC video dataset Videofeedback [17]. Our instruction set focuses on low-level visual quality question-answering for videos, while also involving a small number of instruction data related to video aesthetic assessment (VAA) and AIGC video quality analysis. For constructing the question-answer (Q&A) pairs, we adopt the method of extending human expert annotations using GPT.\nHuman annotation process The human annotation process for each video is divided into two parts. First, we require a comprehensive overall quality depiction. Following this, we request a brief local in-context quality depiction or an extended conversation.\nOur design of overall quality depiction centers on quality attributes. Each complete depiction includes several quality attributes, and the depiction of each quality attribute is presented with the following three elements: Temporal Description+Degree+Quality Attribute. For example, consider the following depiction: \"The entire video exhibits severe compression blur, along with noticeable camera shake throughout its playback. Additionally, there is relentless serious overexposure present on the full body of the performers in the center and on the right side.\u201d In this depiction, the quality attributes are \u201ccompression blur\",\""}, {"title": "4. The VQA2 Series Model", "content": "We propose the VQA2 series model and a unique training pipeline. The structure of the model is illustrated in Fig. 3."}, {"title": "4.1. Model Structure", "content": "Base Model We selected LLaVA-OneVision-Chat-7B [26] as the foundation of our model architecture. The foundation model has achieved excellent performance on multiple high-level visual question-answering benchmarks [13, 35] and demonstrates outstanding capabilities in video semantic understanding and reasoning. The architecture includes a vision tower constructed from the SigLIP [61], which is used to extract token information from keyframe sequences; a vision projector composed of fully connected layers for feature mapping; and the Qwen-2 model [57] and its tokenizer serving as the LLM and text embedding layers.\nMotion Extractor and Motion Projector We observe that many video LMMs designed for high-level video visual question answering can achieve excellent performance by only inputting keyframe sequences extracted at low sampling rates (1 frame per second, etc.) [6, 26, 58]. We think this is because temporal information in videos is highly redundant for high-level VQA tasks focused on video content analysis. The semantic content between adjacent keyframes is highly consistent, and changes and connections in video semantics over longer periods can be adequately represented through keyframe sequences.\nHowever, this redundancy is almost nonexistent in some cases. For example, video stuttering or shaking occurring within a short time can significantly degrade a video's low-level visual quality, and such distortion is closely related to the content presented in adjacent frames. Therefore, keyframe sequences extracted at longer intervals will completely miss this distortion. Additionally, processes like video compression and decoding are closely associated with low-level visual quality and are tightly related to the temporal information between densely extracted frames. In summary, we believe that a video motion feature extraction module that processes adjacent frames from densely input video frames should be incorporated into the base model. We select the SlowFast-R50 model [12] for video motion feature extraction, inputting the entire video after spatial preprocess. To ensure that the length of the motion feature token sequence aligns with the total duration of the video, we use only the fast path features as the extracted motion features. Subsequently, we employ a spatial projector identical in structure to the vision projector to map the motion tokens, ensuring their dimensions are consistent with the spatial tokens and language tokens."}, {"title": "4.2. Distortion-recognition Based Pretraining", "content": "We first pre-train the model excluding the LLM using the instruction subset data from Stage-1. When training with the spatial distortion instruction subset, we freeze the LLM, the motion extractor, and the motion projector, training only the vision tower and the vision projector. Conversely, when training with the motion distortion instruction subset, we freeze the LLM, the vision tower, and the vision projector, training only the motion extractor and the motion projector. This is because we think the pretraining tasks are relatively simple and have a single output format. Unfreezing all parts of the model for training could easily lead to severe overfitting, thereby affecting the subsequent fine-tuning process. During training, the keyframe sequences and motion feature sequences are input in an interleaved manner."}, {"title": "4.3. The VQA2-Scorers and the VQA2-Assistant", "content": "The VQA2-Scorers model family consists of the VQA2-UGC-Scorer and the VQA2-Streaming-Scorer, which are used to score the quality of UGC videos and streaming videos, respectively. To ensure optimal performance on specialized tasks, the VQA2-UGC-Scorer is trained on the instruction subset from the UGC videos data portion of Stage-2 upon the pre-trained model. Subsequently, the VQA2-Streaming-Scorer is further trained on the instruction set from the streaming videos data portion of Stage-2. At this stage, all training involves full-parameter-tuning with all model parameters unfrozen.\nThe VQA2-Assistant is designed to master more nuanced video quality understanding tasks and efficient low-level visual question-answering while still handling the capabilities for precise quality scoring. Building upon the VQA2-UGC-Scorer, this model also undergoes full-parameter-tuning using the data from the Stage-3 instruction subset.\nThe loss function for all three training stages adopts the standard generation loss used in text generation tasks, such as GPT-loss [40]."}, {"title": "5. Experiments", "content": "We conduct comprehensive experiments on video quality scoring and understanding tasks to validate the performance of our model family. Additionally, we perform detailed ablation studies to analyze the impact of various attributes."}, {"title": "5.1. Experimental Setups", "content": "Training Strategies We strictly follow the training hyperparameters provided in project LLaVA-OneVision. All Scorers and Assistant models are trained for only one complete epoch on their respective training data.\nPrompt Design A system prompt is added before all instructions in the instruction set and before being input into the model for training. We design the system prompt to be identical across all stages.\nIn the evaluation stage, we employ specially designed evaluation prompts. For the video quality scoring tasks, to thoroughly verify the model's end-to-end output performance, we do not provide any time information about the test video in the test prompt. For evaluation of the video quality understanding task, we add some temporal information to the evaluation prompt, including the number of sampled keyframes, the total number of frames, and the number of frames sampled per second. The specific prompt formats are presented in the supplementary materials."}, {"title": "5.2. Evaluation on Quality Scoring Tasks", "content": "We evaluate the quantitative scoring capabilities of VQA2 - UGC-Scorer and VQA2-Assistant on three open-source UGC-VQA datasets [18, 43, 59]. We also assess the scoring ability of VQA-Streaming-Scorer on the largest streaming-VQA dataset, the Waterloo-IV [10]. We select some vanilla LMMs and our base model (with vanilla LLaVA-ov-chathttps://github.com/LLAVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision_Chat.md and SlowFast-R50) for reference, and we choose several recent SOTA UGC/Streaming-VQA models for comparison. Tab. 2 and 3 present the performance of our models and the comparison models on UGC/Streaming-VQA tasks, respectively.\nWe used the Pearson Linear Correlation Coefficient (PLCC) and the Spearman Rank Correlation Coefficient (SRCC) as evaluation metrics. Apart from the q-align-IQA, q-align-onealign, q-instruct, and the VQA2-Assistant which have specific training sets and the training-free method SQI, all models in both tasks are trained or optimized on the same datasets (LSVQ (train) for UGC-VQA/Waterloo-I, Waterloo-III, and LIVE-NFLX-II for Streaming-VQA). Specifically, for the UGC-VQA, because Stage-3 involves videos from the LIVE-VQC and LSVQ(1080p), we remove these videos to avoid overlap in all experiments. Leaving 88 videos out of 585 videos in LIVE-VQC and 2575 videos out of 3573 videos in LSVQ(1080p) for evaluation.\nExperimental results demonstrate that the UGC-Scorer and Streaming-Scorer achieve SOTA performance across most datasets and metrics, and ranked within the top three in all datasets and metrics. Although the Assistant slightly lags behind the UGC-Scorer in performance, it also delivers relatively strong scoring results. This confirms that the Assistant, primarily designed for video quality understanding and question answering, can still effectively handle precise scoring tasks, showcasing its versatility."}, {"title": "5.3. Evaluation on Quality Understanding Tasks", "content": "We evaluate the capabilities of the VQA2-Assistant on video quality understanding tasks using the Q-Bench-Video [65]. It contains 1,800 videos and 2,378 multi-type questions. This evaluation encompasses the model's answer accuracy and relevance across three question types: binary yes/no questions (Binary), multiple-choice (single answer) questions (Multi.), and open-ended questions (Open). These questions also span three quality concerns dimensions: video technical quality aspect (Tech.), video temporal aspect (Temp.), and other categories (Other.) (including AIGC and aesthetics, etc.). The model's responses are evaluated using GPT using the same methods in Q-Bench-Video for open-ended questions. Specifically, since our training does not include data related to multi-video analysis, we exclude the question involving multi-video (564 out of 2378). We select a series of high-performing open-source video LMMs, as well as proprietary GPT series for comparison. All models are evaluated using the same prompt. Except for the GPT series, we set the greedy search scheme for generation for all the models, ensuring the results are reproducible.\nThe final accuracy of all models on the test and dev sets of the Q-Bench-Video are presented in Tab. 4 and 5.\nExperimental results indicate that the trained model outperforms the base model across all sub-dimensions. In terms of question types, the Assistant achieves the most significant improvement on binary questions, surpassing the base model by 19.64% and GPT-4o by 8.88% on the test set; it also shows notable gains on what/how questions. As for quality concerns, the Assistant achieves substantial improvements in the Tech. and Temp. dimensions, exceeding the base model by 10.05% and 10.64% in each on the test set. Ultimately, the Assistant outperformed GPT-4o in overall scores on both the test and dev subsets."}, {"title": "5.4. Ablation Study", "content": "We conduct several ablation studies and provide corresponding analyses.\n#1: Effects of Pre-training We remove the Stage-1 and directly train the UGC-Scorer using the data from Stage-2. The performance of the resulting models, compared to the fully trained model, is shown in Tab. 6. Experimental results indicate that the pretraining stage-1 plays a crucial role throughout the training process.\n#2: Effects of Motion Feature Extraction We remove the motion feature extractor and motion feature projector from the model, then follow the same training steps to obtain the Scorer and Assistant models. The models' performance on KonVID-1k and LIVE-VQC, as well as their scores on the Tech. and Temp. quality concerns, and the overall score in the Q-Bench-Video test subset, are presented in Tab. 7. Experimental results show that motion feature extraction plays a significant role in all tasks, especially the Temp. dimension.\n#3: Effects of Mix Training During the training of the VQA2-Assistant, we alternatively combine the data from Stage-2 and Stage-3 by randomly mixing them for training. This training version is denoted as mix. Correspondingly, the original training method is denoted as sequential. We compare the performance of both the mix and sequential versions of the model on the UGC-VQA task and the test set of Q-Bench-Video, with the results recorded in Tab. 8 and 9, respectively.\nThe model trained with the mix strategy shows further improvement in scoring in several datasets, suggesting that, beyond directly answering video quality-level questions, incorporating detailed quality understanding instructions still contributes to quantitative scoring. However, the mix version experiences a notable decline in quality question-answering tasks. We attribute this to the scoring-related instructions in stage-2, which typically have a relatively simple focus and format. This impedes the model from handling diverse question types effectively."}, {"title": "6. Conclusion", "content": "We introduce the VQA2 Instruction Dataset, the first dataset ever dedicated to video quality assessment through visual question answering, alongside our VQA2-series-models (include VQA2-Scorers and VQA2-Assistant) built on this dataset. The dataset spans three stages and includes 157, 735 instruction pairs from diverse video types. Through extensive experiments on video quality scoring and quality understanding tasks, our models achieved SOTA results in video quality scoring and outperformed GPT-4o in visual quality question answering. The VQA2-Assistant, performs well on both quality scoring and understanding tasks, thus fulfilling the demand for model versatility."}]}