{"title": "Harmful Suicide Content Detection", "authors": ["Kyumin Park", "YeongJun Hwang", "HoJae Lee", "SANG MIN LEE", "AH RAH LEE", "Dong-ho Lee", "JinYeong Bak", "Jong-Woo Paik", "MYUNG JAE BAIK", "Yen Shin", "Ruda Lee", "JE YOUNG HANNAH SUN", "SI YEUN YOON", "Jihyung Moon", "Kyunghyun Cho", "Sungjoon Park"], "abstract": "Harmful suicide content on the Internet is a significant risk factor inducing suicidal thoughts\nand behaviors among vulnerable populations. Despite global efforts, existing resources are\ninsufficient, specifically in high-risk regions like the Republic of Korea. Current research mainly\nfocuses on understanding negative effects of such content or suicide risk in individuals, rather\nthan on automatically detecting the harmfulness of content. To fill this gap, we introduce a\nharmful suicide content detection task for classifying online suicide content into five harmfulness\nlevels. We develop a multi-modal benchmark and a task description document in collaboration\nwith medical professionals, and leverage large language models (LLMs) to explore efficient\nmethods for moderating such content. Our contributions include proposing a novel detection\ntask, a multi-modal Korean benchmark with expert annotations, and suggesting strategies using\nLLMs to detect illegal and harmful content. Owing to the potential harm involved, we publicize\nour implementations and benchmark, incorporating an ethical verification process.", "sections": [{"title": "1. Introduction", "content": "Harmful suicide content on the Internet poses a significant risk because it can induce suicidal\nthoughts in readers, potentially leading to self-harm or suicide (Samaritans 2020; MOHW 2019).\nThe harmful suicide content includes materials that encourage or glorify suicide (Zdanow and\nWright 2012), making it appear as an attractive option and sharing suicide methods or instilling\nsuicide knowledge in individuals with suicidal thoughts, thereby increasing the likelihood of\nactual suicide attempts (Biddle et al. 2012). In some cases, exposure to such content has led\nmiddle school students to commit suicide. (Milmo 2022). An analysis of adolescent suicide cases\nreveals that this age group, particularly female adolescents, is more vulnerable to the influence\nof triggering content (Balt et al. 2023; Twenge et al. 2022). Therefore, it is crucial to moderate\nsuch harmful suicide content before it spreads extensively.\nTherefore, efforts to moderate harmful suicide content are intensifying. In the US, initia-\ntives focus on raising public awareness and safe content distribution, aligning with the WHO\nguidelines (WHO 2018). Meanwhile, in 2022, the UK has passed a law that makes such content\nillegal, emphasizing its serious commitment to addressing this issue (Donelan et al. 2023). In the\nRepublic of Korea,, which has the highest suicide rates among OECD countries (WHO 2023),\nthe National Assembly of the Republic of Korea amended the Suicide Prevention Act, and the\ngovernment has declared the dissemination of such content as illegal since 2019 (MOHW 2019).\nDespite the increasing spread of harmful suicide content, its moderation is currently handled by\nonly a single official and fewer than a thousand volunteers (Jung 2022; Min 2023). Considering\nthe extensive use of social media in Korea (NIA 2023), monitoring the large amounts of content\nis extremely challenging. Additionally, moderating suicide content often leads to a high level\nof mental stress, hindering their ability to consistently and effectively monitor such content.\nTherefore, the need for an automatic harmful suicide content moderation system is urgent. The\nsystem can efficiently manage a growing volume of the content and ease the burden on human\nmoderators.\nHowever, most previous studies have focused on understanding the negative effects of\nsuicide content (Balt et al. 2023; Marchant et al. 2017a), or identifying the individuals that are\nmost affected by the content (Sedgwick et al. 2019; Wang et al. 2020a; Patchin, Hinduja, and\nMeldrum 2023; Choi, Han, and Hong 2023a). Other studies have concentrated on suicide risk\ndetection (Yates, Cohan, and Goharian 2017a; Zirikly et al. 2019a; Park et al. 2020; Ji 2022;\nSawhney, Neerkaje, and Gaur 2022a), which aims to detect the suicide or self-harm risk of the\nperson who posted the content, rather than identifying the harmfulness of the content toward\nits viewers. Therefore, we introduce a harmful suicide content detection task that determines\nthe level of harmfulness of the content to viewers. We then develop a multi-modal benchmark\nand a task description document. This document contains detailed instructions for annotators\non how to assess the harmfulness of suicide-related content, which could also be useful for\nbuilding instructions for large language models (LLMs). The benchmark and the document are\ndeveloped by medical professionals, because such content might involve harmful visual-language\ninformation that requires the judgment of the professionals (e.g., self-harm photos, or name of\nillegal drugs that can be used for suicide). Because labeling harmful content causes mental stress,\nwe focus on creating a small yet high-quality dataset. Furthermore, we demonstrate various\nmethods using LLMs that can be effectively performed with few-shot examples.\nOur contributions are as follows:\n\u2022 We propose a harmful suicide content detection task that classifies multimodal suicide\ncontent as illegal, harmful, potentially harmful, harmless, or non-suicide-related."}, {"title": "2. Related Work", "content": "Online platforms contain various types of suicide content that can be harmful, potentially\nharmful, or, assist in suicide prevention (Morrissey, Kennedy, and Grace 2022; SAM 2023).\nHarmful content, intentionally encourages suicide or suicide attempts. It is considered illegal\nto post such content in some countries (The UK and South Korea). This includes images or\ndepictions with detailed descriptions of self-harm or suicide (e.g., live streaming of suicide\nattempts, images of wounds or blood), detailed information, guidelines, advice on methods of\nself-harm, and content that compares the effectiveness of these methods. It also encompasses\ncontent that positively portrays or glorifies all forms of self-harm and suicide through product\nlinks that can be used as a means of suicide. Positive content, although related to suicide, provides\nsupportive information to those at risk of suicide. This includes messages that encourage help-\nseeking, emotional support/recovery/hope messages, and tips for self-care. Content in the grey\narea (potentially harmful content) has an uncertain impact on users, which can be either positive\nor negative. This includes quotations about self-harm and suicide, vivid personal accounts,\ndepictions in art and Internet memes, sharing methods to conceal self-harm traces, and memorial\npages for those who have died by suicide. While intending to support recovery or prevent suicide,\nthey may also trigger extreme thoughts that lead to suicide or self-harm, depending on the nature\nof the content. Moreover, information that is harmless to some users may be harmful to others,\nand how harmful certain information can be depends on factors such as the context in which the\ninformation is written, how it is described, and the amount of content related to suicide and self-\nharm (Marchant et al. 2017b; Morrissey, Kennedy, and Grace 2022; Robinson et al. 2017). Thus,\nthis study differentiates between the various types of suicide content through expert annotation,\ndocuments it in detail, and establishes a harmful suicide benchmark with clear distinctions in\nharmfulness via reliable labeling."}, {"title": "2.2 Suicide Risk Detection", "content": "Previous research on online suicide content primarily focused on predicting the suicide risk of\nthe authors who wrote the content. (Zirikly et al. 2019b) classified the suicide risk of authors\nbased on content posted online (Reddit) into four levels. Similarly, (Milne et al. 2016; Yates,\nCohan, and Goharian 2017b) conducted research to predict the suicide and self-harm risks of\nonline content authors. Subsequently, (Yang, Zhang, and Muresan 2021; Sawhney, Neerkaje, and\nGaur 2022b) used weakly supervised learning to enhance detection performance or collaborated\nwith clinicians. Furthermore, (Rawat et al. 2022; Sawhney et al. 2021) performed tasks to detect\nsuicide ideation and suicide events. However, all these studies focused on detecting the suicide"}, {"title": "3. Harmful Suicide Content Detection", "content": "Figure 1 illustrates the concept of using harmful suicide content detection in a real-world mod-\neration system. The moderation system uses a model to automatically detect harmful suicide\ncontent and checks for illegal or harmful content and implements the appropriate moderation\npolicy through a moderator's review. As this study introduces the task of harmful suicide content"}, {"title": "3.1 Harmful Suicide Content Detection - Input", "content": "Considerations. We consider the followings for designing the input of the task.\n1. Multi-modality. Because 50% of suicide content containing images or videos (KPHN\n2020) we consider text and images as inputs.\n2. Source Diversity. Suicide content appears across various platforms, from SNS to online\ncommunities (KPHN 2020). We collected data from diverse sources for a comprehensive\ncoverage.\n3. Context Information. We also incorporate previous content and metadata into our inputs.\nPrevious content reveals the context of the target content, whereas metadata, such as user\ndescriptions and view counts, provide additional insights, aiding in accurate harmfulness\nassessment.\nInputs. Given the considerations, the inputs for the task are as follows:\n1. User-generated Content. Contents created by users. The content includes text, and possibly\nimages and URLs. Images and URLs are converted to text manually or using machine\nlearning models(e.g. image captioning and summarization)."}, {"title": "3.2 Harmful Suicide Content Detection - Output", "content": "Considerations. We consider the followings for designing the outputs of the task.\n1. Expert Judgement. Suicide content involves specific terminology related to suicide, such\nas professional drug names, slang, and abbreviations. Thus, clinical expertise is required\nto accurately determine the legality and harmfulness of such suicide content and to decide\non an appropriate response to the content.\n2. Moderation Policy. If an automatic harmful suicide content detection model is developed, it\nshould be part of a moderation system and collaborate with human moderators or domain\nexperts (Sawhney, Neerkaje, and Gaur 2022c). This implies that once the model detects\nharmful content, it is necessary to consider appropriate actions. Therefore, the response of\neach output was considered when defining the output.\nOutputs. We develop five suicidal content categories. Content should be mapped to one of the\nfollowing categories:\n(1) Illegal content that encourages or assists suicidal behavior;\n(2) Legal but harmful content that, while not illegal, significantly induces suicide\n(3) Potentially harmful content that could be triggering for certain individuals, whereas it may\nbe benign for others;\n(4) Harmless content that is either neutral or positive for suicide;\n(5) Non-suicide content that is not related to suicide."}, {"title": "3.3 Moderator Review", "content": "Moderator Review. Moderator review includes the process of re-examining the suicide content\nclassified by the model using moderator (e.g., a clinical expert) and implementing the appropriate\nmoderation policy. The moderation system identifies the harmfulness and illegality of suicide\ncontent and implements a corresponding moderation policy to block the spread of such content\nonline. Thus, through moderator review, the moderator 1) verifies the classification result of\nthe model and 2) implements the corresponding moderation policy. The moderator review is\nconducted for results classified as illegal, harmful, and potentially harmful because most online\ninformation is unrelated to suicide and reviewing all the information would increase moderator\nfatigue. Hence, reviews are conducted only for suicide information that may cause harm. The\nmoderator verifies the illegality and harmfulness of content within these categories and conducts\nthe corresponding moderation policy. Therefore, the moderator requires knowledge to compre-\nhend and understand the content and distinctions of suicide content.\nModeration Policies. The moderator reviews the model's classification results and implements\na corresponding moderation policy. The moderation policies are as follows:\n1. Report to police. This is the strongest form of moderation policy intended to subject\ncontent creators to legal regulations by reporting to legal institutions.\n2. Report to online source. Reporting the content to the online source where it is posted\nintents to prevent the spread of harmful information by requesting the deletion of the\ncontent.\n3. No report. No additional actions, such as reporting the posts, are taken, allowing it to\ncirculate online.\nReport to police responds to content within the illegal suicide category containing illegal\ninformation. According to Korean law, certain types of content related to suicide are defined\nas illegal. Such content often includes illegal activities, such as the sale of illegal drugs; hence,\nreporting to legal institutions (e.g., the police) imposes legal sanctions on the poster of such\ncontent.\nReport to online source prevents the online spread of content containing or potentially con-\ntaining harmful information related to suicide, such as illegal, harmful, and potentially harmful\ncontent. Because information spreads quickly online, it is reported to the online source where\nit was posted, and its removal is requested to prevent dissemination. For potentially harmful\ninformation, the harmfulness of which can vary depending on the reader, the moderator assesses\nthe degree of harmfulness and reports whether it is severe.\nNo report is for harmless or non-suicide content that poses no problem when posted online.\nMost online content is unrelated to suicide; therefore it does not require reporting.\nIn summary, the entire process involves the model classifying online content into suicide\ncategories, the moderator reviewing the results, and then implementing the corresponding moder-\nation policy to ensure that the moderation system functions effectively. Throughout this process,\nmulti-modality information such as text and image data are used to reflect various aspects of\nthe content in the model's input. Metadata from diverse sources and previous content serve as\ncontext. The model's output comprises five suicide categories, each differentiated by the presence\nor absence of illegality, harmfulness, and suicide-related aspects. Finally, the moderator review\nefficiently utilizes the model's classification results for validation, and effective moderation\npolicies are implemented based on the content's illegality and harmfulness, thereby preventing\nthe spread of harmful suicide content online."}, {"title": "4. Harmful Suicide Content Benchmark", "content": "Developing a large-scale harmful suicide content dataset is highly challenging. Harmful suicide\ncontent is infrequently encountered in real-world scenarios (Markov et al. 2023), and the dis-\ntressing nature of such content can cause mental strain for annotators. Additionally, obtaining\nannotations from medical experts is expensive. Therefore, we focus on developing a high-quality\ncurated benchmark dataset. Prior to the dataset collection, we obtained approval from the IRB."}, {"title": "4.1 Suicide Content Collection", "content": "To cover the diverse source domains of the content, we collect user-generated contents related\nto suicide from social media, Q&A platforms, online support forums, and online communities.\nTwitter. Twitter constitutes the majority of social media posts flagged for containing suicide-\ninducing information, with a substantial share of 74.69% (KPHN 2020). To collect data related\nto suicide from Twitter, we used the Twitter API v2, to gather posts that include suicide-related\nkeywords in their text or hashtags. These suicide-related keywords were collected from previous\nresearch (Lee et al. 2020) and the guidelines of the 'Korean Suicide Inducing Information\nMonitoring Group' (KFSP 2023). We gathered 12,021 tweets, including 3635 with images, from\nMay to August 2023 using the Twitter API. The suicide-related keywords used in the Twitter\nAPI are summarized in Appendix table D.1.\nQ&A Platform. On Q&A platforms, users often post questions about suicide-related issues, such\nas suicide methods, or respond to these queries. We collected questions and answers containing\nsuicide-related keywords from Naver Knowledge In (a Korean Q&A platform) (Park et al. 2020).\nWe collected data from March 2022 to March 2023 Using the same keywords as those used for\nTwitter, resulting in 13,104 content items.\nOnline Support Forum. In online support forums, people write about their suicide-related\nconcerns, and counselors provide responses to support them (Lee et al. 2020). We collected\nposts from Lifeline Korea (Lifeline Korea 2023) and the Companions of Life Suicide Prevention\nCounselling (KSPCC 2023). We collected 17,325 pieces of contents posted from March 2021 to\nJune 2023.\nOnline Community. DCinside (DCInside 2023), a widely used online community in Korea\ncomparable to Reddit, includes boards that function similarly to subreddits. We collected posts\nfrom two depression-focused boards (depression-minor and depression-mini boards) on the\nDCinside, known to contain suicide-related posts and where actual suicide incidents have been\nreported (Jo 2023). We collected posts including those containing images, resulting in a total of\n794 data entries."}, {"title": "4.2 Preprocessing", "content": "First, we removed all Personally Identifiable Information (PII). This involves replacing URLs,\nnames, locations, phone numbers, emails, and IDs within the text with corresponding tags.\nThereafter, we provided supplementary descriptions for the contents of the external links. Given\nthat these links may contain significant information for accurately understanding the content, we\nmanually reviewed the links and summarized their content. Third, we added text descriptions\nto the images whenever they were included in the content. We used GPT-4 to generate initial\ndescriptions, which were subsequently reviewed and refined for accuracy by the researchers.\nConsequently, all PII values were removed from the text of the data, and we created link\ndescriptions that summarized the content of any URLs present in the content text, along with\ntext descriptions for the images."}, {"title": "4.3 Annotation", "content": "Task Description Document. The task description document was designed to explain the\nharmful suicide content detection task and to provide guidance to the annotators. It contains vital\ninformation, including the purpose of identifying harmful suicide content and a detailed guide\nfor annotating the content. Additionally, it outlines the categories and subcategories of harmful\ncontent, supplemented with real-world examples."}, {"title": "Annotation Process", "content": "The annotation process was divided into three phases. In each phase,\nmedical experts (a clinical expert with an MD degree and a psychiatry professor with a PhD\ndegree) annotated real-world suicide contents, using the task description document as a reference.\nAt the end of each phase, the authors and annotators reviewed and enhanced the task description\ndocument through discussions, before proceeding to the next phase.\nIn the first phase, medical professionals annotated suicide text contents by referring to the\ninitial task description document. Before starting the annotation, we sampled the contents to\nbe annotated from the collected data. Although the contents are gathered using suicide-related\nkeywords, only a small fraction is actually harmful suicide content. Therefore, we used the task\ndescription document as an instruction for the LLMs, allowing them to preliminarily categorize\nthe content into predefined categories. This approach enhances the efficiency of annotation\nprocess for medical experts and reduces mental strain and costs. Consequently, we used the\nOpenAI GPT API to sample 196 suicide contents for annotation from the collected 2272 Twitter\ndata, 17,325 online forum data, and 13,104 Q&A data. Medical professionals then proceeded to\nannotate the selected 196 suicide contents by following the annotation protocol and the initial\ntask description document. The annotation protocol is described in the later part of this section.\nDuring the annotation process, they did not refer to the pre-classification results provided by the\nLLM. Following the annotation, both the categories and subcategories were updated, leading to\na revision of the task description document. Specifically, we refine seven subcategories, added\ntwo new ones, and removed one.\nIn the second phase, we diversified the suicide content in the benchmark and refined the task\ndescription document. Before annotation, we further sampled 175 suicide contents for annotation\nfrom a pool of 8408 Twitter data points collected between May and June 2023. Similar to the\nfirst phase, we pre-classified them the using OpenAI GPT API with instructions written based\non the task description document. Subsequently, medical professionals began the annotation of\nsuicide-related content, strictly adhering to the annotation protocol and using the revised version\nof the task description document as their guide. Once the annotation process was completed, we\nmerged the four subcategories into two.\nIn the final phase, we added multi-modal (text and image) suicide content to the benchmark\ndataset and included online communities as an additional source domain. For the image content,\nwe initially generated textual descriptions of harmful images using visual language LLMs.\nThese initial descriptions were then revised to correct any inaccuracies or fill in missing details.\nThe refined descriptions were subsequently used to pre-classify the content into categories and\nsubcategories, as defined in the task description from the second phase. Following this process,\nwe selected 95 multi-modal suicide content items for annotation. Medical professionals then\nannotated based on the annotation protocol, and the task description document was finalized by\nrevising the previous version."}, {"title": "4.4 Harmful Suicide Content Benchmark", "content": "Statistics. The benchmark comprised 452 contents (126 with images). Among the 452 annotated\ncontent items, we used the examples included in the final task description documents as the\ntraining set. Examples were selected by medical professionals and were representative of each\nsubcategory. The training set included 50 contents, with each of the five categories containing 10\nexamples and each of the 25 subcategories including at least one example. The detailed statistics"}, {"title": "5. Experiment", "content": "We considered the followings for experiments:\n1. Moderation Policy. We anticipate deployment of this model in a real-world moderation\nsystem. In a practical scenario for moderating harmful content, an automated moderation system\ninitially predicts the potential harm, and then a human moderator or expert reviews the outcome.\nTherefore, we prioritize achieving a higher recall rather than precision.\n2. Leveraging LLMs. Considering that the definition and extent of harmful suicide content may\nevolve over time (e.g., new harmful drugs or memes), the system should be designed to allow\nfor quick and effortless replacement of the criteria used to assess harmfulness. Rather than\ndepending on standard fine-tuning methods, we focused is on exploring the transformation of\ntask description documents into instructions using LLMs. The key advantage of this approach is\nthat it eliminates the necessity to initiate model training and deployment from the scratch each\ntime the criteria are updated; instead, simply modifying the task description enables immediate\nmoderation based on the revised criteria (Weng, Goel, and Vallone 2023).\nOverview. First, we illustrate the process of utilizing task description documents to perform\ntasks usingh LLMs (section 5.1). Next, we evaluated the performance by varying the input\nin terms of the modality and number of few-shot training examples (section 5.2). Finally, we\nassessed the performance of different LLMs, both English/Korean and closed/open-sourced\nmodels(section 5.3)."}, {"title": "5.1 Leveraging Task Description", "content": "We investigated the formulation of a task description document with diverse and extensive\ninformation into instructions because instruction construction significantly influences LLM per-\nformance (Liu et al. 2023; Wu et al. 2023; Zhao et al. 2021). The task description document\nfor the harmful suicide content detection task contains crucial details, including the names and\ndescriptions of five suicide categories as well as the names and explanations of 25 subcategories\nand constituting up to 60% of the instruction at maximum Thus, we examined two hypotheses\nabout effectively using these category descriptions as instructions.\n1. Impact of Suicide Content Description Order: Performance varied across tasks based on\nthe location of the information provided. For instance, in open-domain question answering\nand few-shot classification, the answer accuracy and label alignment exhibit patterns that\nare influenced by the position of the correct information or label (Liu et al. 2023; Zhao\net al. 2021). This experiment aims to investigate how the sequence of category information,\nparticularly the ground truth (GT) category position (GT Position in Table 6), affects the\nmodel performance and identifies the optimal presentation sequence.\n2. Impact of Suicide Content Description Detail Level: Category information includes\ndetailed names and descriptions of the categories and subcategories. Our experiments were\ndesigned to determine the details that most significantly impact performance by varying\nthe granularity of the information."}, {"title": "5.1.1 Impact of Suicide Content Description Order. Setup", "content": "This experiment aims to find the\nmost suitable sequence of category descriptions for harmful suicide content detection, as the\norder of category descriptions given in the instructions could change the model's prediction.\nBecause each category differs in the degree of harm, and like the metrics, classifying categories\nwith higher harm such as illegal/harmful content, is most critical, the category descriptions are\narranged in the instructions from highest to lowest harm. To achieve this, we compared scenarios\nin which category descriptions were provided according to the degree of harm versus in a\ndifferent sequences. However, comparing all possible category orders requires considering all\npossible permutations of category arrangements (5! = 120 permutations), which was impractical\nfor the experiments. Thus, to approximate the average performance across random category\npositions, we controlled the placement of the ground truth category, the category with which\nan instance was labeled, and conducted the experiments accordingly. To assess the impact of the\nground truth (GT) category's position on LLMs' performance, we varied its placement within the\ninstruction's category information for conditions K = [1, 5], where the GT category is located at\nthe K-th sequence. The remaining categories are shuffled and placed in the remaining positions\nfor each inference.\nResults. shows how the performance of the model in detecting suicide content\nchanges with the GT position. Most metrics peak when the GT category is at the forefront (GT"}, {"title": "5.1.2 Impact of Suicide Content Description Detail Level. Setup", "content": "We evaluate harmful suicide\ncontent detection performance by varying the detailed category information detail levels as\nfollows:\n\u2022 Category name\n\u2022 Category name and description\n\u2022 Category name with category description, and subcategory name\n\u2022 Category name with category description, subcategory name with subcategory description\nResults. shows that the performance improves with more category information, with\nthe most comprehensive level yielding the highest F1 scores. Specifically, the macro F1 score\nincreases by 89% (18.86 \u2192 35.75), and the illegal F1 and harmful F1 scores increases by 200%\n(11.87 \u2192 35.80) and 57% (37.63 \u2192 59.10), respectively, as compared to when using only the\ncategory name."}, {"title": "5.2 Formulating LLM Inputs", "content": "We assessed performance changes by incorporating images and training examples as inputs. We\nfocused on the impact of images as multi-modal data (section 5.2.1) and the effect of using\ntraining data with the annotation guide as post-instruction when combined with instruction\n(section 5.2.2)."}, {"title": "5.2.1 Leveraging Multi-modality. Setup", "content": "The objective of this experiment was to determine the\neffect of image information on the classification performance of the model. We employed two\nmethods of conveying image information and compared their performances: the first method con-\nverts images into text descriptions, referred to as image description, whereas the second uses the\nimages directly as inputs, referred to as vision. Three settings were tested for image descriptions:\nthe first did not provide any image information, the second generated image descriptions using\na model (gpt-4-1106), and the third involved human modifications to the descriptions created\nby the model. This allowed for a comparison of the performance of the models based on the\ngeneration of text-based image descriptions. Additionally, we examined the impact of images\n(vision) when paired with the same image descriptions to observe their influence on performance.\nThis involved adding the original image to each image description experiment for comparison.\nOverall, this setup evaluates the model's performance in terms of the modality of suicide content\nthrough image descriptions and assesses the model's multimodal capabilities through vision.\nNotably, during the annotation process, the annotators labeled the suicide category of the content\nbased on both the text and the original images. We used gpt-4-turbo-2024-04-09, which\ncan use both text and image inputs, for this experiment. We conducted an experiment on 113\ntest data entries that included images, among which only three belonged to the illegal suicide\ncategory; thus, illegal metrics were excluded from the results.\nResults. shows the impact of multi-modal information on harmful suicide content de-\ntection tasks. In experiments regarding image descriptions without visual information, providing\nimage details leads to superior performance compared to omitting them. Specifically, when using\nGPT-4 generated image descriptions, macro-F1 increased by 9.16% (from 50.46 \u2192 55.08) and\nMAE decreased by 15.93% (0.3894 \u2192 0.3333), indicating enhanced classification performance\nacross all suicide categories. Additionally, harmful F1 and recall both increased by 8.00% (68.50\n\u2192 73.98 and 75.76 \u2192 81.82), suggesting that image information significantly aids in identifying\nharmfulness within suicide content. Comparing GPT-4 and human-modified image descriptions,\nusing human-modified descriptions results reduces macro-F1 by 3.55% (55.08 \u2192 53.19) and"}, {"title": "5.2.2 Leveraging Few-shot Examples. Setup", "content": "We examined the effects of one to five-shot con-\nfigurations, corresponding to one to five examples per category (K), totaling 5 to 25 examples.\nThe examples used for few-shot experiments are randomly selected from the training set to ensure\ndiverse demonstrations."}, {"title": "5.3 Comparison Between LLMS", "content": "We compared the performance of various LLMs in identifying harmful suicide content. Because\nopen-sourced LLMs have instruction-following capabilities that depend on the language they\nhave seen in the instruction tuning phase, we conducted experiments with different models for\nKorean and English benchmarks to address language barriers.\nSetup. We categorized the selected LLMs into closed and open-sourced models. For the Korean\nbenchmark, we utilized closed models because of the lack of open-source or multilingual LLMs\nthat can properly follow the task's instructions in Korean. We also included a random baseline\nthat arbitrarily categorized content into one of the five categories.\n\u2022 Closed Models We utilized OpenAI's GPT-3.5 (gpt-3.5-turbo-16k-0613) and GPT-4 (gpt-\n4-1106-preview), which are accessed through the OpenAI API and capable of handling a\ncontext length of 128,000 characters. Additionally, we experimented with Clova X, a LLM\ntrained on Korean, using the Naver API (Kim et al. 2021).\n\u2022 Open Sourced Models We utilized the zephyr-7B-beta model (Tunstall et al. 2023), an\nenhanced version of mistral-7B, which supports a context length of up to 32,000 char-\nacters. We also use Longchat-7B-16k (Li et al. 2023) and Vicuna-7B-v1.5-16k (Chiang\net al. 2023), which are both fine-tuned LLAMA models with a maximum context length\nof 16,000 characters.\nTo explore the model's adaptability of the model in few-shot learning contexts, we conducted\nexperiments in both the zero-shot and 5-shot scenarios (section 5.2.2). However, for models\nunable to accept the context length of 12k tokens required for the 5-shot experiments, such as\nClova X (4096), we limited our analysis to the zero-shot trials.\nResults. shows the performance of the GPT models and Clova X on the Korean\nbenchmark. GPT-4 outperformed all other models in every metric except for harmful recall. GPT-\n3.5 follows GPT-4 in terms of performance across all metrics, except for harmful recall. Clova\nX showed lower performance than the GPT models but achieved the highest score in harmful\nrecall, indicating its high sensitivity to harmful content. The detailed results of the experiments\non the Korean benchmark are presented in Appendix table A.1"}, {"title": "5.4 Discussions", "content": "Open-Sourced vs Closed LLMs. GPT-4 recorded the highest performance across all accuracy\nmetrics (macro F1, MAE, illegal F1, and harmful F1) for all few-shot settings. In 5-shot set-\ntings, open-sourced models achieve a similar performance to GPT-3.5. However, in zero-shot\nsettings, they struggled to understand lengthy instructions, resulting in random predictions (e.g.,\nLongchat) or biased predictions towards specific categories (e.g., Vicuna), with Zephyr slightly\noutperforming random. In 5-shot scenarios, Zephyr matches GPT-3.5 in macro F1, MAE, and"}, {"title": "6. Error Analysis", "content": "Here, we describe the error cases in the harmful suicide content detection task. In particular,\nthe clinical experts design the suicide categories that require a comprehensive understanding of\nsuicide contents and utilize various types of information such as user-generated content, previous\ncontext, and metadata in this process"}]}