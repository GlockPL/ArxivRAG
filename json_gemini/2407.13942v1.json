{"title": "Harmful Suicide Content Detection", "authors": ["Kyumin Park", "YeongJun Hwang", "HoJae Lee", "SANG MIN LEE", "AH RAH LEE", "Dong-ho Lee", "JinYeong Bak", "Jong-Woo Paik", "MYUNG JAE BAIK", "Yen Shin", "Ruda Lee", "JE YOUNG HANNAH SUN", "SI YEUN YOON", "Jihyung Moon", "Kyunghyun Cho", "Sungjoon Park"], "abstract": "Harmful suicide content on the Internet is a significant risk factor inducing suicidal thoughts and behaviors among vulnerable populations. Despite global efforts, existing resources are insufficient, specifically in high-risk regions like the Republic of Korea. Current research mainly focuses on understanding negative effects of such content or suicide risk in individuals, rather than on automatically detecting the harmfulness of content. To fill this gap, we introduce a harmful suicide content detection task for classifying online suicide content into five harmfulness levels. We develop a multi-modal benchmark and a task description document in collaboration with medical professionals, and leverage large language models (LLMs) to explore efficient methods for moderating such content. Our contributions include proposing a novel detection task, a multi-modal Korean benchmark with expert annotations, and suggesting strategies using LLMs to detect illegal and harmful content. Owing to the potential harm involved, we publicize our implementations and benchmark, incorporating an ethical verification process.", "sections": [{"title": "1. Introduction", "content": "Harmful suicide content on the Internet poses a significant risk because it can induce suicidal thoughts in readers, potentially leading to self-harm or suicide. The harmful suicide content includes materials that encourage or glorify suicide, making it appear as an attractive option and sharing suicide methods or instilling suicide knowledge in individuals with suicidal thoughts, thereby increasing the likelihood of actual suicide attempts. In some cases, exposure to such content has led middle school students to commit suicide. An analysis of adolescent suicide cases reveals that this age group, particularly female adolescents, is more vulnerable to the influence of triggering content. Therefore, it is crucial to moderate such harmful suicide content before it spreads extensively.\nTherefore, efforts to moderate harmful suicide content are intensifying. In the US, initiatives focus on raising public awareness and safe content distribution, aligning with the WHO guidelines. Meanwhile, in 2022, the UK has passed a law that makes such content illegal, emphasizing its serious commitment to addressing this issue. In the Republic of Korea,, which has the highest suicide rates among OECD countries, the National Assembly of the Republic of Korea amended the Suicide Prevention Act, and the government has declared the dissemination of such content as illegal since 2019.\nDespite the increasing spread of harmful suicide content, its moderation is currently handled by only a single official and fewer than a thousand volunteers. Considering the extensive use of social media in Korea, monitoring the large amounts of content is extremely challenging. Additionally, moderating suicide content often leads to a high level of mental stress, hindering their ability to consistently and effectively monitor such content. Therefore, the need for an automatic harmful suicide content moderation system is urgent. The system can efficiently manage a growing volume of the content and ease the burden on human moderators.\nHowever, most previous studies have focused on understanding the negative effects of suicide content, or identifying the individuals that are most affected by the content. Other studies have concentrated on suicide risk detection, which aims to detect the suicide or self-harm risk of the person who posted the content, rather than identifying the harmfulness of the content toward its viewers. Therefore, we introduce a harmful suicide content detection task that determines the level of harmfulness of the content to viewers. We then develop a multi-modal benchmark and a task description document. This document contains detailed instructions for annotators on how to assess the harmfulness of suicide-related content, which could also be useful for building instructions for large language models (LLMs). The benchmark and the document are developed by medical professionals, because such content might involve harmful visual-language information that requires the judgment of the professionals (e.g., self-harm photos, or name of illegal drugs that can be used for suicide). Because labeling harmful content causes mental stress, we focus on creating a small yet high-quality dataset. Furthermore, we demonstrate various methods using LLMs that can be effectively performed with few-shot examples.\nOur contributions are as follows:\n\u2022 We propose a harmful suicide content detection task that classifies multimodal suicide content as illegal, harmful, potentially harmful, harmless, or non-suicide-related."}, {"title": "2. Related Work", "content": "Online platforms contain various types of suicide content that can be harmful, potentially harmful, or, assist in suicide prevention. Harmful content, intentionally encourages suicide or suicide attempts. It is considered illegal to post such content in some countries (The UK and South Korea). This includes images or depictions with detailed descriptions of self-harm or suicide (e.g., live streaming of suicide attempts, images of wounds or blood), detailed information, guidelines, advice on methods of self-harm, and content that compares the effectiveness of these methods. It also encompasses content that positively portrays or glorifies all forms of self-harm and suicide through product links that can be used as a means of suicide. Positive content, although related to suicide, provides supportive information to those at risk of suicide. This includes messages that encourage help-seeking, emotional support/recovery/hope messages, and tips for self-care. Content in the grey area (potentially harmful content) has an uncertain impact on users, which can be either positive or negative. This includes quotations about self-harm and suicide, vivid personal accounts, depictions in art and Internet memes, sharing methods to conceal self-harm traces, and memorial pages for those who have died by suicide. While intending to support recovery or prevent suicide, they may also trigger extreme thoughts that lead to suicide or self-harm, depending on the nature of the content. Moreover, information that is harmless to some users may be harmful to others, and how harmful certain information can be depends on factors such as the context in which the information is written, how it is described, and the amount of content related to suicide and self-harm. Thus, this study differentiates between the various types of suicide content through expert annotation, documents it in detail, and establishes a harmful suicide benchmark with clear distinctions in harmfulness via reliable labeling.\nPrevious research on online suicide content primarily focused on predicting the suicide risk of the authors who wrote the content. classified the suicide risk of authors based on content posted online (Reddit) into four levels. Similarly, conducted research to predict the suicide and self-harm risks of online content authors. Subsequently, used weakly supervised learning to enhance detection performance or collaborated with clinicians. Furthermore, performed tasks to detect suicide ideation and suicide events. However, all these studies focused on detecting the suicide"}, {"title": "3. Harmful Suicide Content Detection", "content": "Figure 1 illustrates the concept of using harmful suicide content detection in a real-world moderation system. The moderation system uses a model to automatically detect harmful suicide content and checks for illegal or harmful content and implements the appropriate moderation policy through a moderator's review. As this study introduces the task of harmful suicide content content owing to its harmful nature. Studies among Chinese adolescents have shown significant correlations between digital media usage and suicide/self-harm and a meaningful relationship between suicide cases among Korean youths and searches related to suicide/self-harm. In addition, three-quarters of young adults who have attempted suicide have reported using the Internet for suicide/self-harm-related reasons, highlighting the risk posed by information that can induce suicide or self-harm. Accordingly, this research proposes a task that measures the harmfulness of the post to others. For example, content that encourages others to commit suicide, which is not the focus of conventional suicide risk detection, is targeted for the detection in harmful suicide content detection."}, {"title": "3.1 Harmful Suicide Content Detection - Input", "content": "Considerations. We consider the followings for designing the input of the task.\nMulti-modality. Because 50% of suicide content containing images or videos we consider text and images as inputs.\nSource Diversity. Suicide content appears across various platforms, from SNS to online communities. We collected data from diverse sources for a comprehensive coverage.\nContext Information. We also incorporate previous content and metadata into our inputs. Previous content reveals the context of the target content, whereas metadata, such as user descriptions and view counts, provide additional insights, aiding in accurate harmfulness assessment.\nInputs. Given the considerations, the inputs for the task are as follows:\nUser-generated Content. Contents created by users. The content includes text, and possibly images and URLs. Images and URLs are converted to text manually or using machine learning models(e.g. image captioning and summarization)."}, {"title": "3.2 Harmful Suicide Content Detection - Output", "content": "Considerations. We consider the followings for designing the outputs of the task.\nExpert Judgement. Suicide content involves specific terminology related to suicide, such as professional drug names, slang, and abbreviations. Thus, clinical expertise is required to accurately determine the legality and harmfulness of such suicide content and to decide on an appropriate response to the content.\nModeration Policy. If an automatic harmful suicide content detection model is developed, it should be part of a moderation system and collaborate with human moderators or domain experts. This implies that once the model detects harmful content, it is necessary to consider appropriate actions. Therefore, the response of each output was considered when defining the output.\nOutputs. We develop five suicidal content categories. Content should be mapped to one of the following categories:\n(1) Illegal content that encourages or assists suicidal behavior;\n(2) Legal but harmful content that, while not illegal, significantly induces suicide\n(3) Potentially harmful content that could be triggering for certain individuals, whereas it may be benign for others;\n(4) Harmless content that is either neutral or positive for suicide;\n(5) Non-suicide content that is not related to suicide."}, {"title": "3.3 Moderator Review", "content": "Moderator Review. Moderator review includes the process of re-examining the suicide content classified by the model using moderator (e.g., a clinical expert) and implementing the appropriate moderation policy. The moderation system identifies the harmfulness and illegality of suicide content and implements a corresponding moderation policy to block the spread of such content online. Thus, through moderator review, the moderator 1) verifies the classification result of the model and 2) implements the corresponding moderation policy. The moderator review is conducted for results classified as illegal, harmful, and potentially harmful because most online information is unrelated to suicide and reviewing all the information would increase moderator fatigue. Hence, reviews are conducted only for suicide information that may cause harm. The moderator verifies the illegality and harmfulness of content within these categories and conducts the corresponding moderation policy. Therefore, the moderator requires knowledge to comprehend and understand the content and distinctions of suicide content.\nModeration Policies. The moderator reviews the model's classification results and implements a corresponding moderation policy. The moderation policies are as follows:\nReport to police. This is the strongest form of moderation policy intended to subject content creators to legal regulations by reporting to legal institutions.\nReport to online source. Reporting the content to the online source where it is posted intends to prevent the spread of harmful information by requesting the deletion of the content.\nNo report. No additional actions, such as reporting the posts, are taken, allowing it to circulate online.\nTable 2 displays the categories of harmful suicide content detection and the corresponding moderation policies.\nReport to police responds to content within the illegal suicide category containing illegal information. According to Korean law, certain types of content related to suicide are defined as illegal. Such content often includes illegal activities, such as the sale of illegal drugs; hence, reporting to legal institutions (e.g., the police) imposes legal sanctions on the poster of such content. Report to online source prevents the online spread of content containing or potentially containing harmful information related to suicide, such as illegal, harmful, and potentially harmful content. Because information spreads quickly online, it is reported to the online source where it was posted, and its removal is requested to prevent dissemination. For potentially harmful information, the harmfulness of which can vary depending on the reader, the moderator assesses the degree of harmfulness and reports whether it is severe.\nNo report is for harmless or non-suicide content that poses no problem when posted online. Most online content is unrelated to suicide; therefore it does not require reporting.\nIn summary, the entire process involves the model classifying online content into suicide categories, the moderator reviewing the results, and then implementing the corresponding moderation policy to ensure that the moderation system functions effectively. Throughout this process, multi-modality information such as text and image data are used to reflect various aspects of the content in the model's input. Metadata from diverse sources and previous content serve as context. The model's output comprises five suicide categories, each differentiated by the presence or absence of illegality, harmfulness, and suicide-related aspects. Finally, the moderator review efficiently utilizes the model's classification results for validation, and effective moderation policies are implemented based on the content's illegality and harmfulness, thereby preventing the spread of harmful suicide content online."}, {"title": "4. Harmful Suicide Content Benchmark", "content": "Developing a large-scale harmful suicide content dataset is highly challenging. Harmful suicide content is infrequently encountered in real-world scenarios, and the distressing nature of such content can cause mental strain for annotators. Additionally, obtaining annotations from medical experts is expensive. Therefore, we focus on developing a high-quality curated benchmark dataset. Prior to the dataset collection, we obtained approval from the IRB.\nTo cover the diverse source domains of the content, we collect user-generated contents related to suicide from social media, Q&A platforms, online support forums, and online communities.\nTwitter. Twitter constitutes the majority of social media posts flagged for containing suicide-inducing information, with a substantial share of 74.69%. To collect data related to suicide from Twitter, we used the Twitter API v2, to gather posts that include suicide-related keywords in their text or hashtags. These suicide-related keywords were collected from previous research and the guidelines of the 'Korean Suicide Inducing Information Monitoring Group'. We gathered 12,021 tweets, including 3635 with images, from May to August 2023 using the Twitter API. The suicide-related keywords used in the Twitter API are summarized in Appendix table D.1.\nQ&A Platform. On Q&A platforms, users often post questions about suicide-related issues, such as suicide methods, or respond to these queries. We collected questions and answers containing suicide-related keywords from Naver Knowledge In (a Korean Q&A platform)"}, {"title": "4.2 Preprocessing", "content": "First, we removed all Personally Identifiable Information (PII). This involves replacing URLs, names, locations, phone numbers, emails, and IDs within the text with corresponding tags. Thereafter, we provided supplementary descriptions for the contents of the external links. Given that these links may contain significant information for accurately understanding the content, we manually reviewed the links and summarized their content. Third, we added text descriptions to the images whenever they were included in the content. We used GPT-4 to generate initial descriptions, which were subsequently reviewed and refined for accuracy by the researchers. Consequently, all PII values were removed from the text of the data, and we created link descriptions that summarized the content of any URLs present in the content text, along with text descriptions for the images."}, {"title": "4.3 Annotation", "content": "Task Description Document. The task description document was designed to explain the harmful suicide content detection task and to provide guidance to the annotators. It contains vital information, including the purpose of identifying harmful suicide content and a detailed guide for annotating the content. Additionally, it outlines the categories and subcategories of harmful content, supplemented with real-world examples.\nOur basis for understanding the definitions, categories, and examples of harmful suicide content was the 'Korean Suicide Prevention Law\u201d and documents published by the \"Korea Life Respect Hope Foundation's suicide/harmful information monitoring team\" . We found that certain category names and descriptions were unclear or overlapped, thus requiring more distinct clarifications. To address this, we involved medical professionals in the data annotation process, which led to significant revisions and refinements of the categories and their descriptions, as well as the expansion of examples for each category. Following we used an iterative coding process such that the medical experts individually annotate the real-world content, come together to refine the task description, and then repeat the coding process individually. This updating process was iterative and performed three times to ensure comprehensive refinement. Further details of the iterative process are presented in the section below. We demonstrate each category and it's description in Table 4 and the subcategories and there description in Appendix table B.1.\nAnnotation Process. The annotation process was divided into three phases. In each phase, medical experts (a clinical expert with an MD degree and a psychiatry professor with a PhD degree) annotated real-world suicide contents, using the task description document as a reference. At the end of each phase, the authors and annotators reviewed and enhanced the task description document through discussions, before proceeding to the next phase.\nIn the first phase, medical professionals annotated suicide text contents by referring to the initial task description document. Before starting the annotation, we sampled the contents to be annotated from the collected data. Although the contents are gathered using suicide-related keywords, only a small fraction is actually harmful suicide content. Therefore, we used the task description document as an instruction for the LLMs, allowing them to preliminarily categorize the content into predefined categories. This approach enhances the efficiency of annotation process for medical experts and reduces mental strain and costs. Consequently, we used the OpenAI GPT API to sample 196 suicide contents for annotation from the collected 2272 Twitter data, 17,325 online forum data, and 13,104 Q&A data. Medical professionals then proceeded to annotate the selected 196 suicide contents by following the annotation protocol and the initial task description document. The annotation protocol is described in the later part of this section. During the annotation process, they did not refer to the pre-classification results provided by the LLM. Following the annotation, both the categories and subcategories were updated, leading to a revision of the task description document. Specifically, we refine seven subcategories, added two new ones, and removed one.\nIn the second phase, we diversified the suicide content in the benchmark and refined the task description document. Before annotation, we further sampled 175 suicide contents for annotation from a pool of 8408 Twitter data points collected between May and June 2023. Similar to the first phase, we pre-classified them the using OpenAI GPT API with instructions written based on the task description document. Subsequently, medical professionals began the annotation of suicide-related content, strictly adhering to the annotation protocol and using the revised version of the task description document as their guide. Once the annotation process was completed, we merged the four subcategories into two.\nIn the final phase, we added multi-modal (text and image) suicide content to the benchmark dataset and included online communities as an additional source domain. For the image content, we initially generated textual descriptions of harmful images using visual language LLMs. These initial descriptions were then revised to correct any inaccuracies or fill in missing details. The refined descriptions were subsequently used to pre-classify the content into categories and subcategories, as defined in the task description from the second phase. Following this process, we selected 95 multi-modal suicide content items for annotation. Medical professionals then annotated based on the annotation protocol, and the task description document was finalized by revising the previous version."}, {"title": "3.3 Moderator Review", "content": "Finally, we manually verified the entire benchmark dataset. This involved identifying and eliminating any remaining PIIs from all suicide content and validating the final labels. During the finalization process, 14 contents items were excluded from the benchmark. These contents deal with subcultures (such as games and comics) and, therefore, are incomprehensible to all annotators and cannot be categorized into any suicide category, leading to their exclusion. Additionally, the task description document was completed, providing comprehensive information on the five categories and 25 subcategories, including their harmful category names, descriptions, and illustrative examples.\nAnnotation Protocol. In every phase, we adopted a consensus-based method for biomedical research and clinical practice. For each suicide content, two separate medical professionals (a clinical expert with an MD degree and a psychiatry professor with a PhD degree) independently labeled the category, subcategory, and rationale for their decisions regarding both the category and subcategory. Each individual annotator assigned the label based on a comprehensive review of the user-generated content (text, image), previous content, and metadata associated with the suicide content. The Inter-Annotator Agreement (IAA) for category labels reached a high agreement of 0.77 (cohen's kappa) after the second phase of the annotation process. In cases where there is a discrepancy in the category label assigned by individual annotators, a consensus is established through the input of three annotators, which includes an additional clinical expert (a psychiatry professor with a PhD degree). During this consensus, rationales written by the two individual annotators are combined into a single rationale. Additionally, annotators comment on any data whose association with suicide content is uncertain, as well as on instances that imply a potential need to revise the task description. These comments were employed at the end of each annotation phase to refine and update the task description."}, {"title": "4.4 Harmful Suicide Content Benchmark", "content": "Statistics. The benchmark comprised 452 contents (126 with images). Among the 452 annotated content items, we used the examples included in the final task description documents as the training set. Examples were selected by medical professionals and were representative of each subcategory. The training set included 50 contents, with each of the five categories containing 10 examples and each of the 25 subcategories including at least one example. The detailed statistics"}, {"title": "5. Experiment", "content": "We considered the followings for experiments:\nModeration Policy. We anticipate deployment of this model in a real-world moderation system. In a practical scenario for moderating harmful content, an automated moderation system initially predicts the potential harm, and then a human moderator or expert reviews the outcome. Therefore, we prioritize achieving a higher recall rather than precision.\nLeveraging LLMs. Considering that the definition and extent of harmful suicide content may evolve over time (e.g., new harmful drugs or memes), the system should be designed to allow for quick and effortless replacement of the criteria used to assess harmfulness. Rather than depending on standard fine-tuning methods, we focused is on exploring the transformation of task description documents into instructions using LLMs. The key advantage of this approach is that it eliminates the necessity to initiate model training and deployment from the scratch each time the criteria are updated; instead, simply modifying the task description enables immediate moderation based on the revised criteria.\nOverview. First, we illustrate the process of utilizing task description documents to perform tasks usingh LLMs Next, we evaluated the performance by varying the input in terms of the modality and number of few-shot training examples. Finally, we assessed the performance of different LLMs, both English/Korean and closed/open-sourced models(section 5.3)."}, {"title": "5.1 Leveraging Task Description", "content": "We investigated the formulation of a task description document with diverse and extensive information into instructions because instruction construction significantly influences LLM performance The task description document for the harmful suicide content detection task contains crucial details, including the names and descriptions of five suicide categories as well as the names and explanations of 25 subcategories and constituting up to 60% of the instruction at maximum Thus, we examined two hypotheses about effectively using these category descriptions as instructions.\nImpact of Suicide Content Description Order: Performance varied across tasks based on the location of the information provided. For instance, in open-domain question answering and few-shot classification, the answer accuracy and label alignment exhibit patterns that are influenced by the position of the correct information or label This experiment aims to investigate how the sequence of category information, particularly the ground truth (GT) category position (GT Position in Table 6), affects the model performance and identifies the optimal presentation sequence.\nImpact of Suicide Content Description Detail Level: Category information includes detailed names and descriptions of the categories and subcategories. Our experiments were designed to determine the details that most significantly impact performance by varying the granularity of the information.\n5.1.1 Impact of Suicide Content Description Order. Setup. This experiment aims to find the most suitable sequence of category descriptions for harmful suicide content detection, as the order of category descriptions given in the instructions could change the model's prediction. Because each category differs in the degree of harm, and like the metrics, classifying categories with higher harm such as illegal/harmful content, is most critical, the category descriptions are arranged in the instructions from highest to lowest harm. To achieve this, we compared scenarios in which category descriptions were provided according to the degree of harm versus in a different sequences. However, comparing all possible category orders requires considering all possible permutations of category arrangements (5! = 120 permutations), which was impractical for the experiments. Thus, to approximate the average performance across random category positions, we controlled the placement of the ground truth category, the category with which an instance was labeled, and conducted the experiments accordingly. To assess the impact of the ground truth (GT) category's position on LLMs' performance, we varied its placement within the instruction's category information for conditions K = [1, 5], where the GT category is located at the K-th sequence. The remaining categories are shuffled and placed in the remaining positions for each inference.\nResults. (a) shows how the performance of the model in detecting suicide content changes with the GT position. Most metrics peak when the GT category is at the forefront (GT"}, {"title": "5.1.2 Impact of Suicide Content Description Detail Level. Setup.", "content": "We evaluate harmful suicide content detection performance by varying the detailed category information detail levels as follows:\nCategory name\nCategory name and description\nCategory name with category description, and subcategory name\nCategory name with category description, subcategory name with subcategory description\nResults. shows that the performance improves with more category information, with the most comprehensive level yielding the highest F1 scores. Specifically, the macro F1 score increases by 89% and the illegal F1 and harmful F1 scores increases by 200% and 57%, respectively, as compared to when using only the category name."}, {"title": "5.2 Formulating LLM Inputs", "content": "We assessed performance changes by incorporating images and training examples as inputs. We focused on the impact of images as multi-modal data and the effect of using training data with the annotation guide as post-instruction when combined with instruction.\n5.2.1 Leveraging Multi-modality. Setup. The objective of this experiment was to determine the effect of image information on the classification performance of the model. We employed two methods of conveying image information and compared their performances: the first method converts images into text descriptions, referred to as image description, whereas the second uses the images directly as inputs, referred to as vision. Three settings were tested for image descriptions: the first did not provide any image information, the second generated image descriptions using a model , and the third involved human modifications to the descriptions created by the model. This allowed for a comparison of the performance of the models based on the generation of text-based image descriptions. Additionally, we examined the impact of images (vision) when paired with the same image descriptions to observe their influence on performance. This involved adding the original image to each image description experiment for comparison. Overall, this setup evaluates the model's performance in terms of the modality of suicide content through image descriptions and assesses the model's multimodal capabilities through vision. Notably, during the annotation process, the annotators labeled the suicide category of the content based on both the text and the original images. We used gpt-4-turbo-2024-04-09, which can use both text and image inputs, for this experiment. We conducted an experiment on 113 test data entries that included images, among which only three belonged to the illegal suicide category; thus, illegal metrics were excluded from the results.\nResults. Table 8 shows the impact of multi-modal information on harmful suicide content de- tection tasks. In experiments regarding image descriptions without visual information, providing image details leads to superior performance compared to omitting them. Specifically, when using GPT-4 generated image descriptions, macro-F1 increased by 9.16% (from 50.46 \u2192 55.08) and MAE decreased by 15.93% (0.3894 \u2192 0.3333), indicating enhanced classification performance across all suicide categories. Additionally, harmful F1 and recall both increased by 8.00% (68.50 \u2192 73.98 and 75.76 \u2192 81.82), suggesting that image information significantly aids in identifying harmfulness within suicide content. Comparing GPT-4 and human-modified image descriptions, using human-modified descriptions results reduces macro-F1 by 3.55% (55.08 \u2192 53.19) and"}, {"title": "5.2.2 Leveraging Few-shot Examples. Setup.", "content": "We examined the effects of one to five-shot configurations, corresponding to one to five examples per category (K), totaling 5 to 25 examples. The examples used for few-shot experiments are randomly selected from the training set to ensure diverse demonstrations."}, {"title": "5.3 Comparison Between LLMS", "content": "We compared the performance of various LLMs in identifying harmful suicide content. Because open-sourced LLMs have instruction-following capabilities that depend on the language they have seen in the instruction tuning phase, we conducted experiments with different models for Korean and English benchmarks to address language barriers.\nSetup. We categorized the selected LLMs into closed and open-sourced models. For the Korean benchmark, we utilized closed models because of the lack of open-source or multilingual LLMs that can properly follow the task's instructions in Korean. We also included a random baseline that arbitrarily categorized content into one of the five categories.\nClosed Models We utilized OpenAI's GPT-3.5 (gpt-3.5-turbo-16k-0613) and GPT-4 (gpt- 4-1106-preview), which are accessed through the OpenAI API and capable of handling a context length of 128,000 characters. Additionally, we experimented with Clova X, a LLM trained on Korean, using the Naver API\nOpen Sourced Models We utilized the zephyr-7B-beta model, an enhanced version of mistral-7B, which supports a context length of up to 32,000 char- acters. We also use Longchat-7B-16k and Vicuna-7B-v1.5-16k , which are both fine-tuned LLAMA models with a maximum context length of 16,000 characters.\nTo explore the model's adaptability of the model in few-shot learning contexts, we conducted experiments in both the zero-shot and 5-shot scenarios. However, for models unable to accept the context length of 12k tokens required for the 5-shot experiments, such as Clova X (4096), we limited our analysis to the zero-shot trials.\nResults. Figure 5 shows the performance of the GPT models and Clova X on the Korean benchmark. GPT-4 outperformed all other models in every metric except for harmful recall. GPT- 3.5 follows GPT-4 in terms of performance across all metrics, except for harmful recall. Clova X showed lower performance than the GPT models but achieved the highest score in harmful recall, indicating its high sensitivity to harmful content. The detailed results of the experiments on the Korean benchmark are presented in Appendix table A.1"}, {"title": "5.4 Discussions", "content": "Open-Sourced vs Closed LLMs. GPT-4 recorded the highest performance across all accuracy metrics (macro F1, MAE, illegal F1, and harmful F1) for all few-shot settings. In 5-shot set- tings, open-sourced models achieve a similar performance to GPT-3.5. However, in zero-shot settings, they struggled to understand lengthy instructions, resulting in random predictions (e.g., Longchat) or biased predictions towards specific categories (e.g., Vicuna), with Zephyr slightly outperforming random. In 5-shot scenarios, Zephyr matches GPT-3.5 in macro F1, MAE, and harmful F1, whereas Longchat and Vicuna show comparable performance in their respective metrics. Except for Vicuna, open-sourced models generally showed lower recall than closed models in terms of illegal and harmful content.\nOriginal Korean vs Translated English. Figure 7 shows an analysis of GPT-3.5 and GPT-4's performance on the Korean and translated English benchmarks. Both models performed better on the Korean benchmark across all F1 metrics. However, GPT-4 shows a decrease in macro F1 from the English to the Korean benchmark by 19.24% in zero-shot (57.42 \u2192 46.37) and 9.51% in 5-shot (58.12 \u2192 52.59), with the largest decrease in illegal F1 by 36.17% in zero-shot (64.80 \u2192 39.85). GPT-3.5 also showed a considerable reduction in zero-shot illegal F1 by 62.03% (35.80 \u2192 13.59). Illegal recall decreases considerably, with GPT-4 decreasing by 49.51% drop (62.43 \u2192 31.52) and GPT-3.5 by 78.34% in illegal recall (58.79 \u2192 12.73), indicating a larger decrease than in F1 scores. The decrease in harmful F1 is less significant, with GPT-4 decreasing by 7.00% (73.89 \u2192 68.72) in zero-shot, and GPT-3.5 decreasing by 3.94% (59.10 \u2192 56.77). This indicates that while translating the benchmark using GPT-4 does not significantly affect the overall quality, it may lead to issues in specific categories, notably illegal."}, {"title": "6. Error Analysis", "content": "Here, we describe the error cases in the harmful suicide content detection task. In particular, the clinical experts design the suicide categories that require a comprehensive understanding of suicide contents and utilize various types of information such as user-generated content, previous context, and metadata in this process. Therefore, we analyzed the specific features of the data that led to the misclassification of the model.\nWe categorized the types of errors and examined the data to answer the following questions:\nUnder-detection of harmfulness: Which types of inherently harmful data (illegal or harmful category) are classified by the model into harmless or non-suicide category?\nOver-detection of harmfulness: What instances of harmless or non-suicide data are incorrectly classified as illegal or harmful?\nDistinction between 'Illegal' and 'Harmful' categories: Can the model accurately dif- ferentiate between 'illegal' and 'harmful' suicide content?\nUtilization of Image Information as Vision: What content does the model fails to interpret when the image is provided as a vision input?\nFor our analysis, the authors manually examined the misclassified data to pinpoint specific data features that might have led the model to incorrect predictions. To answer the questions 1 to 3, we derived the results of the best-performing model setup based on our experiments (gpt- 4-0613 with category/subcategory names and descriptions in a 5-shot setting) and answering the question 4, we derived the results of the experiment with vision (gpt-4-turbo-0409 with image as vision) in section 5.2.\nUnder-detection of harmfulness includes five error cases. Among them we find that the model fails to accurately identify suicide-inducing substances that are represented using slang or euphemisms, leading to an underestimation of their harmfulness (one case). Additionally, the model did not correctly interpret euphemistic expressions that glorify a suicide note as an 'accusation through death', misunderstanding the actual content of the note (one case). Examples are shown in Figure E.1 and Figure E.2.\nOver-detection of harmfulness consists of ten error cases. There are situations where the content text alone appears harmful, but when considered alongside image descriptions and context, the perceived harmfulness decreases; conversely, if the context and image description suggest harm but the content text does not, the model struggles to integrate these conflicting messages and overestimates the harmfulness (four cases). Furthermore, content unrelated to suicide but involving harm to specific individuals is incorrectly classified as suicide-inducing information, indicating a misclassification of the content's relationship whip suicide (three cases). Examples are presented in Figure E.3 and Figure E.4.\nDistinction between 'Illegal' and 'Harmful' categories includes twenty-two cases. Ten instances involve data classified from the illegal to the harmful category, predominantly because names of legally prohibited drugs described in suicide-inducing content are often abbreviated (e.g., \"\uc878\ud53c\ub380\" (Zolphidem) as \"\uc878\u314d \u3163\ub380\"), leading to the model's failure to correctly recognize these substances (9 cases). Conversely, Twelve instances are classified from the harmful to the illegal category. Descriptions of suicide methods using substances that are not illegal (e.g., nitrogen gas) were incorrectly categorized as illegal (three cases). The model also misclassified detailed suicide methods or tools that are culturally specific and not generally recognized as inducing suicide (one case). Examples are shwon in Figure E.5, Figure E.6, and Figure E.7.\nUtilization of image information as vision analyzes situations in which errors occur when image information is provided as vision, totaling eight cases. Seven of these cases occurred in the data labeled as potentially harmful suicide content, consistent with the results in section 5.2. Among them, four were classified as harmful suicide content, suggesting that the model reacted more sensitively to the harmfulness conveyed through vision-delivered images. Examples can are presented in Figure E.8 and Figure E.9.\nOur error analysis revealed several areas in which the model frequently misclassified suicide content. Specifically, errors often occur in the misinterpretation of the user-generated content. These misclassifications arise from the model's inadequate handling of slang, euphemisms, and officially specific references that disguise the severity of the content or falsely elevate non-suicide content to a harmful status. Additionally, misclassifications can occur when interpreting the various data modalities. Difficulties arise when there are discrepancies between the content's text and images, as the model struggles to interpret conflicting information from different modalities. Overall, enhancing the model's ability to interpret nuanced information and various modalities will enhance its effectiveness in accurately categorizing harmful suicide content."}, {"title": "7. Ethical Consideration", "content": "The benchmark contains extremely disturbing text and images, including self-harming photos, blood, tools used for suicide and self-harm, and drug information. Even among medical professionals and researchers, prolonged exposure to such images can lead to severe mental stress. Therefore, we have deliberately chosen not to aim for the creation of a large-scale dataset, but rather to limit the workload to prevent further intensifying mental stress. All these processes were conducted with IRB approval obtained prior to data collection.\nGiven the nature of harmful suicide content and the legal restrictions against its unrestricted distribution, it is challenging to share the benchmark dataset openly. We understand the legal implications of distributing data that containing information that potentially induces suicide. Despite these concerns, we believe that collecting such data to build a benchmark and conducting research to prevent its spread on the internet outweighs these legal issues. Access to the benchmark will be strictly limited, allowing only researchers with IRB approval and a commitment to not to distribute the content further, ensuring responsible use for research purposes only and adherence to legal standards. We believe that our work contributes significantly to the ongoing international effort against harmful suicide content, and hopes to aid in preventing its spread on the Internet."}, {"title": "8. Conclusion", "content": "In this study, we introduce a novel task of harmful suicide content detection designed to identify and moderate online content that poses the risk of promoting self-harm or suicide. We utilized suicide content from various online sources and multiple attributes of suicide content (i.e., text, image, context, and metadata) as inputs and developed suicide categories that consider harmfulness, suicide-relatedness, and illegality as outputs, aiming for effective application within real-world moderation systems.\nFollowing this task design, we collected suicide-related content from diverse online sources and, with annotations from clinical experts, we constructed a multi-modal benchmark (harmful suicide content benchmark). Through iterative annotation processes, we refine the criteria for evaluating the varied content and intentions of suicide-related information and labeled it with comprehensive categories and subcategories. This process is supported by a task description document enriched with expert knowledge to assess suicide content, which clarifies the details of each category and subcategory. Furthermore, we utilized a consensus-based method from biomedical research and clinical practice to resolve conflicts among individual annotators (experts), thereby ensuring the reliability of the labels. This meticulous approach results in a benchmark containing a broad spectrum of suicide-related content with highly reliable labels, encapsulate within a task description document that embeds expert knowledge on the subject. We anticipate that both the benchmark and the task description document will serve as robust references for subsequent research on harmful suicide content detection.\nUsing the benchmark and task description document, we assessed the classification performance of various LLMs in our experiments. Our task description document, enriched with clinical insights into the nature and subtleties of suicide content, served as a critical instructional resource. This document guides the model to apply clinical knowledge more effectively, resulting in a significant enhancement in its ability to classify content accurately. Furthermore, we explored how different modalities of suicide content (text and images) contributed to the identification and categorization of suicide content. This multimodal analysis is crucial for understanding how various types of information on suicide content can influence the model outputs in complex real-world scenarios. Additionally, we included open-sourced LLMs to broaden the scope of this study. This inclusive approach allowed us to demonstrate the versatility and adaptability of LLMs within the moderation of suicide content, highlighting their potential as moderation systems. By integrating both closed and open-sourced models, our research provides insights into the strengths and limitations of each models, and paves the way for future innovations in online content moderation, especially in sensitive areas such as suicide prevention.\nThis work sets a foundation for future research on harmful suicide content detection and offers a blueprint for the practical application of LLMs in online content moderation, ensuring relevance and efficacy in real-world scenarios."}, {"title": "9. Limitations", "content": "Moderation System. While designing the moderation system for real-world applications, we sourced data from various sources, utilized various of input attributes, and created output cat- egories for suicide content with moderation policies in mind. However, the system is not fully automated because 1) input attributes such as link descriptions that require manual creation, and 2) all content containing harmfulness undergoes moderator review, necessitating consideration of moderator stress. Consequently, developing a practical moderation system that resolves these issues remains a task for future research.\nBenchmark Size. Although the harmful suicide content benchmark is an essential step towards understanding and moderating online suicide-related content, it encompasses 452 data entries. This relatively small benchmark size is largely attributable to the fact that posts related to suicide comprise a small fraction of the total online content. Additionally, the filtering and deletion of such content by online sources inherently limits the volume of data available for collection.\nNevertheless, a carefully controlled annotation process that incorporating the knowledge of clinical experts supports the credibility of the benchmark and ensures a reliable set of labels. Additionally, the task description document details 25 different subcategories of suicide content, and the benchmark comprises a wide array of suicide content, including actual data for each subcategory. Therefore, our detailed task description document and the data within our benchmark lay the groundwork for future efforts to create a large-scale suicide content dataset utilizing the annotations described for the suicide content.\nPost-hoc Moderation. To gather the benchmark data, we sourced data from various online platforms, including Twitter. These platforms conduct their moderation, filtering, or removal of harmful content as reported by users. For example, Twitter's 'suicide and self-harm policy' bans information promoting or encouraging suicide and self-harm, encompassing:\nself-inflicted physical injuries (e.g., cutting).\nencouraging someone to physically harm or kill themselves.\nasking others for encouragement to engage in self-harm or suicide, including seeking partners for group suicides or suicide games.\nsharing information, strategies, methods or instructions that would assist people to engage in self-harm and suicide.\nBecause we collected content posted online, having passed through each platform's moderation, it is vital to verify whether such data actually exist in the benchmark. Twitter's rules correspond to the subcategories specified in our task description document for the illegal/harmful categories, matching:\nPhotos of self-harm, detailed descriptions or depictions of self-harm (harmful suicide category)\nContent that recommends, plans, or describes non-suicide self-injury (harmful suicide category)\nSuicide pacts (illegal suicide category)\nContent informing specific methods for suicide (illegal suicide category)\nTherefore, we counted the amount of Twitter data in the benchmark that fell into these subcate- gories. Out of 359 Twitter data instances, we find 37 instances belonging to these subcategories (12, 3, 18, and 4 respectively). Although more severe and specific suicide content may have been moderated and not collected, our findings indicate the presence of suicide content that bypassed moderation and was successfully included in the benchmark.\nMulti-modality. The construction of diverse attributes within the benchmark, such as links and image descriptions, requires substantial human effort, posing potential challenges for future applications in automated moderation systems. However, in our experiments, the performance of the models using GPT-4-generated image descriptions showed negligible differences from those using human-generated descriptions, indicating the viability of such automated systems. For links, the descriptions were manually curated; however, an automated system capable of visiting URLs and summarizing content could potentially substitute for human effort.\nData Collections. To create a harmful suicide content detection benchmark, we collected data from five online sources: Twitter, online communities, Q&A platforms, and two suicide support forums. However, there was an imbalance issue, as Twitter data constituted the majority of the benchmark (79.4%), and data from other online sources were underrepresented. This is owing to difficulties in collecting suicide-related content; Twitter allow us to search for suicide- related keywords via its API, but other online sources have restrictions on using suicide-related words or keyword-based searches, leading to less suicide-associated data collection compared to Twitter. Extending our benchmark to collect data from a variety of online sources across different platforms is a task for future research."}, {"title": "6. Error Analysis", "content": "Here, we describe the error cases in the harmful suicide content detection task. In particular, the clinical experts design the suicide categories that require a comprehensive understanding of suicide contents and utilize various types of information such as user-generated content, previous context, and metadata in this process. Therefore, we analyzed the specific features of the data that led to the misclassification of the model.\nWe categorized the types of errors and examined the data to answer the following questions:\nUnder-detection of harmfulness includes five error cases. Among them we find that the model fails to accurately identify suicide-inducing substances that are represented using slang or euphemisms, leading to an underestimation of their harmfulness (one case). Additionally, the model did not correctly interpret euphemistic expressions that glorify a suicide note as an \"accusation through death\", misunderstanding the actual content of the note (one case). Examples are shown in Figure E.1 and Figure E.2.\nOver-detection of harmfulness consists of ten error cases. There are situations where the content text alone appears harmful, but when considered alongside image descriptions and context, the perceived harmfulness decreases; conversely, if the context and image description suggest harm but the content text does not, the model struggles to integrate these conflicting messages and overestimates the harmfulness (four cases). Furthermore, content unrelated to suicide but involving harm to specific individuals is incorrectly classified as suicide-inducing information, indicating a misclassification of the content's relationship whip suicide (three cases). Examples are presented in Figure E.3 and Figure E.4.\nDistinction between 'Illegal' and 'Harmful' categories includes twenty-two cases. Ten instances involve data classified from the illegal to the harmful category, predominantly because names of legally prohibited drugs described in suicide-inducing content are often abbreviated (e.g., \"\uc878\ud53c\ub380\" (Zolphidem) as \"\uc878\u314d \u3163\ub380\"), leading to the model's failure to correctly recognize these substances (9 cases). Conversely, Twelve instances are classified from the harmful to the illegal category. Descriptions of suicide methods using substances that are not illegal (e.g., nitrogen gas) were incorrectly categorized as illegal (three cases). The model also misclassified"}]}