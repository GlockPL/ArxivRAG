{"title": "Top Ten Challenges Towards Agentic Neural Graph Databases", "authors": ["Jiaxin Bai", "Zihao Wang", "Yukun Zhou", "Hang Yin", "Weizhi Fei", "Qi Hu", "Zheye Deng", "Jiayang Cheng", "Tianshi Zheng", "Hong Ting Tsang", "Yisen Gao", "Zhongwei Xie", "Yufei Li", "Lixin Fan", "Binhang Yuan", "Wei Wang", "Lei Chen", "Xiaofang Zhou", "Yangqiu Song"], "abstract": "Graph databases (GDBs) like Neo4j and TigerGraph excel at handling interconnected data but lack advanced inference capabilities. Neural Graph Databases (NGDBs) address this by integrating Graph Neural Networks (GNNs) for predictive analysis and reasoning over incomplete or noisy data. However, NGDBs rely on predefined queries and lack autonomy and adaptability. This paper introduces Agentic Neural Graph Databases (Agentic NGDBs), which extend NGDBs with three core functionalities: autonomous query construction, neural query execution, and continuous learning. We identify ten key challenges in realizing Agentic NGDBs: semantic unit representation, abductive reasoning, scalable query execution, and integration with foundation models like large language models (LLMs). By addressing these challenges, Agentic NGDBs can enable intelligent, self-improving systems for modern data-driven applications, paving the way for adaptable and autonomous data management solutions.", "sections": [{"title": "Introduction", "content": "Graph databases like Neo4j [1], TigerGraph [2], and Azure Cosmos DB are useful tools for representing and querying interconnected data using nodes and edges. These databases are adept at handling the complex relationships inherent in graph-structured data, providing efficient mechanisms for storage and retrieval.\nA Neural Graph Database (NGDB), as introduced in [3], represents a system architecture that merges the predictive capabilities of Graph Neural Networks (GNNs) with the rich data representation features of graph databases (GDBs). NGDBs enhance graph databases by leveraging GNNs for advanced machine-learning tasks while preserving and utilizing the information embedded within the graph data model."}, {"title": "Challenge 1: Semantic Units", "content": "The NGDB primarily relies on relational graphs, where nodes and relations are the basic semantic units. Incorporating diverse semantic units, such as numbers and events, introduces complexity due to their intrinsic relationships. For example, numbers involve algebraic operations (e.g., addition, subtraction), while events involve temporal and causal relations. Addressing these complexities requires reasoning engines that can learn and process such relationships effectively.\nNumber literals (e.g., age, height) are critical for filtering and querying within NGDBs. Prior work includes methods like KBLRN [6], KR-EAR [7], and LitCQD [8], which improve reasoning by integrating numeric constraints into queries. Despite these advancements, challenges remain. These include developing advanced numerical operations and integrating neural-symbolic systems into NGDBs while ensuring compatibility with symbolic solvers for faithful reasoning. Existing approaches focus on entity-centric knowledge graphs, but event-centric knowledge graphs (EVKGs) like ATOMIC [9] and ASER [10] emphasize relationships between events (e.g., temporal and causal relations). Reasoning on EVKGs involves determining event occurrences and their sequences, which introduces unique challenges compared to entity-centric KGs. Recent work extends traditional reasoning by integrating temporal and occurrence constraints[11].\nMoreover, beliefs, desires, and intentions (BDI) represent higher-level, abstract semantic units extending beyond simple entity-attribute relationships and eventualities. These elements are crucial for modeling human-like reasoning, decision-making, and behavior prediction. Beliefs refer to what an agent (human or system) assumes or holds to be true about the world. These can include factual statements like It is raining outside and subjective perspectives like This movie is great. In KGs, beliefs are often represented as knowledge nodes or statements that may vary across agents or contexts, allowing for personalization or multi-agent reasoning. Intentions represent the goals or purposes behind an agent's actions or decisions and as a bridge between beliefs and actions. Intentions are often implicit and must be inferred from user behavior or contextual information. KGs are typically modeled as motivational nodes or goals that guide reasoning about why an agent performs specific actions. For instance, PersonX intends [to buy a gift for a friend], which could explain why PersonX searches for [gift shops nearby]. On the other hand, desires represent an agent's wants, preferences, or needs, which may not always lead to concrete actions unless accompanied by intention. In knowledge graphs, desires are commonly expressed as preferences or motivational entities that influence behavior, such as PersonX desires [to eat ice cream]. These three elements allow knowledge graphs to capture human motivations more comprehensively. These concepts are closely connected to the Theory of Mind (ToM), which refers to the ability to understand that other agents (humans, machines, etc.) possess their own beliefs, desires, and intentions that may differ from one's own. In the context of knowledge graphs, the Theory of Mind enhances reasoning about multi-agent knowledge by enabling the understanding of diverse perspectives. Theory of Mind also enables the inference of motivations by reasoning about the interplay between beliefs, desires, and intentions.\nIntegrating BDI and ToM in Agentic NGDB has practical applications across various domains. In e-commerce, systems like FolkScope [12], COSMO [13], and RIG [14] are the knowledge graphs that leverage BDI to model user behavior, enabling personalized recommendations by linking user actions (e.g., purchases) with inferred desires and intentions. In commonsense reasoning, resources like ATOMIC use BDI to represent cause-effect relationships, allowing systems to reason about potential outcomes of actions. Multi-agent systems benefit from BDI-enhanced KGs by enabling cooperative and competitive interactions that account for multiple agents' goals, beliefs, and desires. Additionally, in natural language understanding, BDI helps interpret user intent in queries, conversations, and social media posts by associating semantic meanings with inferred motivations. We still need systematic storing and inference with these intention knowledge graphs."}, {"title": "Challenge 2: Abductive Reasoning with NGDB", "content": "Abductive reasoning, the process of inferring the most plausible explanations for observations, is a fundamental aspect of human cognition and artificial intelligence. In the context of knowledge graphs (KGs), abductive reasoning generates hypotheses to explain observations (entity sets) by leveraging structured relationships and entities. Complex Logical Query Answering (CLQA) has further advanced abductive reasoning by enabling multi-hop logical inferences over large, incomplete graphs. Neural Graph Databases (NGDBs) build on these advancements, offering a more flexible and robust framework for abductive reasoning.\nEarly methods for abductive reasoning in KGs relied on supervised learning and search-based techniques. Generative models, such as transformer-based architectures, were used to produce logical hypotheses. For example, [15] proposed a supervised generative model trained on datasets like FB15k-237 and WN18RR, which excelled in structural fidelity but struggled to generalize to unseen observations due to the limitations of supervised objectives. To address these limitations, reinforcement learning (RL) techniques were introduced. Reinforcement Learning from Knowledge Graph feedback (RLF-KG) employed proximal policy optimization (PPO) to generate hypotheses aligned with observed evidence. This approach improved explanatory power and generalizability, achieving significant gains in metrics like Jaccard similarity and Smatch scores across multiple datasets. NGDBs extend these methods by embedding knowledge graph data in a latent space, enabling flexible query processing and hypothesis generation. By leveraging latent embeddings, NGDB can infer missing information and generate hypotheses for complex logical queries, even on incomplete graphs, outperforming traditional graph databases. NGDBs represent a significant step forward in abductive reasoning, synthesizing the strengths of CLQA and advanced generative models. However, several challenges must be addressed:\n\u2022 More Generalized Observation In the current definition of abductive reasoning, the definition of the observations is a set of entities. However, observation can be further generalized to a context, for example, a conversation history in the conversational recommendation task setting, or a structured shopping session.\n\u2022 More Complex Structured Hypotheses Existing abductive reasoning models on KGs primarily focus on conjunctive tree-formed queries. NGDBs, with their increased query expressiveness, require hypothesis generation models capable of handling more complex structured observations. For instance, hypotheses should accommodate EFOk (existential first-order logic) and cyclic queries, expanding beyond the limitations of earlier models.\n\u2022 Graph-Based Hypothesis Generation Models Traditional sequence-based models struggle to capture the structural complexity of logical hypotheses, which are fundamentally query graphs. These graphs exhibit features like permutation invariance of logical operators, requiring models explicitly designed to generate graph-structured hypotheses.\n\u2022 NGDB as a Reward Model for Reinforcement Learning Previous RL-based methods, such as [15], relied on symbolic execution results from knowledge graphs to provide reward signals during hypothesis generation. However, these reward signals suffer from the incompleteness inherent to the open-world assumption. NGDBs can address this issue by serving as a more robust reward model, leveraging their latent embeddings and flexible query capabilities to improve hypothesis generation."}, {"title": "Challenge 3: Generalization across Query Families", "content": "Introducing neural modules in graph databases enables the generalization to the knowledge in databases. However, the development of neural modules is always entangled with their targeted query families, thus naturally biased toward them due to their inductive biases, emphasizing the challenge of generalizing towards different query types. Compared to classic database algorithms that support an entire query family as long as it is formally defined, neural modules still suffer a loss in performance for generalization even when the query family is fixed [16]. Readers are also referred to related surveys [17, 18]."}, {"title": "Different Query Families and Their Neural Modules", "content": "Tree-formed Queries and Compositional Generalizability. The tree-formed query is a collective term that describes the whole query family that can be recursively defined in a tree structure, in which logical connectives and variables are carefully organized so that set operations can formally derive the answers [16], the set operations include set projection [19], intersection, union [20], complement[20] and set difference [21]. To tackle such kinds of queries, a line of research is known as query embeddings, where sets are modeled as embeddings, and set operations mentioned above are modeled directly by neural modules [22, 21, 23]. The set operations composition allows the models to generalize the entire tree-form query family. This connection between model design and query family is termed the compositional generalizability [16, 24], and the performance drop with the increasing of compositional levels is still universally observed and remains a challenge to address.\nEFO-1 Queries and Query Graph. It is shown that tree-formed query family is constrained by certain assumptions and fails to represent the whole family of Existential First Order queries with one free variable (EFO-1 query) such as cyclic query [25]. To handle new graph-theoretic features which cannot be represented in tree-formed queries. One commonly adopted technique for EFO-1 queries is the DNF normal form or the UCQ query-solving strategy [20, 26], which solves the conjunctive query first and then takes the union of the answer set of each conjunctive query. A query graph [26] can naturally describe each conjunctive query. This formulation motivates graph-related search methods [25] or graph neural networks [26].\nMore Advanced Query Types. More advanced query families still exist, though the development of corresponding neural models on these topics is insufficient at the current stage. Thus, we discuss some of the challenges we might face in pursuit of more advanced queries in NGDB from the following aspects (i) Multi-arity predicates: The first challenge we may encounter is when the knowledge databases are constructed by (n + 1)-ary tuples, the relation corresponds to n-ary predicate and a graph becomes a hypergraph [27]. (ii) Support of functions The corresponding research gap is the support for functions in the query a function can output nodes, numbers, semantic units, or data of more advanced modality \u2013 for example, the AVG and COUNT functions in SQL but not in current CQA models. We have noted one preliminary research trying to fill this gap [8]."}, {"title": "Minimal Assumption for Broad Generalization", "content": "Previous case studies showcase the close entanglement of the neural modules and the query types they support syntactically. In other words, the key to generalization is minimizing the query families' assumptions and the inductive biases of the neural part of NGDB. We present two types of methods with minimal assumptions.\nNeuro-symbolic Methods. NGDB implies that the underlying database is a graph, meaning neural modules solely modeling the graph itself impose no assumptions on the query family it might support. Such neural modules include link predictors or knowledge graph embeddings that map a triple (s, p, o) of subject, predicate, and object into a score [28]. Therefore, the critical design task of NGDB with such modules is revising the algorithms into the neuro-symbolic forms with the scores produced by link predictors [29, 25]. An apparent and more decomposed approach is to derive an instance of a classic graph database using the link predictor, and all previous research in graph databases applies directly. Notably, the neuro-symbolic approach achieves the same level of generalizability in queries as the classic database research."}, {"title": "Challenge 4: Privacy and Security", "content": "Privacy in data storage refers to protecting sensitive information from unauthorized access and misuse [31]. Traditional databases are facing several privacy risks, which can be categorized into: (1) Unauthorized Access [32]: Unauthorized access to databases can result in large-scale data leakage, exposing sensitive personal information. (2) Insider Threats [33]: Employees with legitimate access may misuse their privileges, either intentionally or unintentionally compromising data privacy. (3) Data Inference Attacks [34]: Attackers can employ various techniques to deduce sensitive information from seemingly innocuous data.\nTo mitigate privacy risks, several protection methods have been developed: (1) Data Anonymization [35]: Techniques such as k-anonymity [36] and l-diversity [37] help mask individual identities within datasets, making it harder to trace data back to specific individuals. (2) Encryption [38]: Data encryption ensures that unauthorized parties cannot access sensitive information even if they breach a database. (3) Access Control [32]: Access control restricts data access to authorized users only, reducing the risk of insider threats. (4) Differential Privacy [39]: This approach adds noise to data outputs, ensuring that the presence or absence of an individual in a dataset does not significantly affect the results of queries."}, {"title": "New Privacy Challenges in NGDBs", "content": "Graph databases, while offering advantages in managing complex relationships, introduce specific privacy risks: (1) Link Prediction Attacks [40]: Adversaries can use machine learning models to predict hidden relationships within the graph, potentially uncovering private connections. (2) Structural Attacks [41]: Even when the data content is anonymized, the graph's structure itself can reveal sensitive insights. The unique structure of graph data amplifies these risks, as the relationships between entities can reveal information that is not immediately apparent from isolated data points. Neural Graph Databases (NGDBs) represent a significant advancement in data management, combining the strengths of traditional graph databases with the capabilities of neural networks. The exploration of privacy issues in NGDBs remains largely underdeveloped, with significant gaps in research addressing potential vulnerabilities and mitigation strategies.\nPotential Attacks. One of the primary strengths of NGDBs is their ability to generalize from incomplete data by inferring hidden relationships. While this capability can enhance data retrieval and knowledge discovery, it also poses significant privacy risks [42]: (1) Model Inversion Attacks [43]: Neural models can be susceptible to inversion attacks, where an adversary uses access to the model to recover the graph data used for NGDB training. (2) Membership Inference Attacks [44]: Attackers may infer whether a particular data point (node or edge) was part of the training data, revealing sensitive information in NGDBs. (3) Embedding Leakage [45]: The embeddings generated by NGDBs to represent nodes and relationships can leak sensitive information, as these embeddings often capture detailed structural and content-based features of the graph stored.\nPromising Defenses. (1) Differential Privacy in NGDBs: Extending differential privacy techniques to protect neural graph databases is a key research direction. Adding noise to the model parameters or gradients during training can help mitigate membership inference and model inversion attacks [46, 47]. (2) Embedding Obfuscation: Techniques to obfuscate embeddings without losing their utility for answering complex queries need to be developed to prevent leakage of sensitive information [48]. (3) Private Distribute Training: Privacy problems in distributed NGDBs need further development [49]. Federated learning, including Secure Multi-Party Computation (SMPC) [50] and Homomorphic Encryption (HE) [51] techniques, can be adapted to NGDBs to ensure that data is processed without being revealed.\nEvaluation Benchmarks. Another significant challenge in NGDBs is the evaluation of privacy protection efficacy. Assessing the effectiveness of privacy-preserving mechanisms requires robust benchmarks that can accurately measure both privacy protection and the quality of retrieved data. However, such benchmarks are currently lacking in the field. To address this challenge, standardized evaluation metrics and datasets should be developed that can facilitate comprehensive testing of privacy-preserving techniques in NGDBs. Establishing reliable benchmarks will provide insights into the strengths and weaknesses of different approaches, ultimately guiding future developments in privacy protection."}, {"title": "Challenge 5: Scaling for Higher Complexity", "content": "In deep learning, neural scaling law is an empirical law that describes the performance of neural models improves with the number of parameters, training dataset size, and training cost [52, 53]. During the development of the NGDB model, scaling is also a major thread, primarily encompassing the scaling of parameter number, query data size, and training costs. The query embedding methods and sequence models often scale the training costs in the training stage, including the model parameters and queries. In contrast, the neuro-symbolic methods often scale the computation cost over the test stage to improve the performance. We mainly discuss how to scale these models further, particularly when the query structure becomes increasingly complex [25, 54] and the magnitude of the knowledge databases becomes very large [55]. Specifically, we introduce the complexity of these models in the training and inference stages and discuss their efficiency and scalability challenges.\nData Scaling in the Training Stage. Both query embedding and sequence models are trained from scratch, requiring many sampled queries as training data. The quality and size of these training queries are crucial, and they typically encompass various query types. The NGDB models generally use the same dataset, with the basic 1p query type enumerating the entire knowledge graph [20]. To incorporate new features such as negation [22], cyclic queries [25], and multivariable queries [54], it is essential to sample query types that include these features. Materializing training queries becomes infeasible as the knowledge graph grows, and sampling logical queries is incompatible with traditional single-hop frameworks based on graph partitioning. To address this challenge, SMORE [55] proposes a scalable framework that efficiently samples training data on the fly with high throughput. In contrast, neuro-symbolic methods primarily rely on pre-training for the knowledge graph completion task and depend on search algorithms to address general logical queries.\nTest Time Scaling in Inference Stage. We first introduce the notion of query complexity and data complexity [56]. Data complexity captures the relation between the time complexity and the database size |E| (number of the edges) when the query is fixed. In contrast, query complexity is assessed based on the size of the query |Q| (number of the predicates) when assuming the database is fixed. When discussing the complexity, the query is restricted to tree-formed queries and EFO-1 queries that we have discussed before. The complexity of neural symbolic search is well studied. The complexity for tree-formed queries is O(|Q||E|). Such approaches [25, 29] require O(|Q|) search steps, while each step requires a search over the database, which is O(|E|). For the general EFO-1 query, the cyclic query makes the general complexity particularly hard and results in O(|E||Q|) time,"}, {"title": "Challenge 6: Distributed NGDB System", "content": "NGDB is targeted at a scenario where users can simultaneously conduct graph data management and graph inference. We identify four features of such a scenario that significantly affect the system design. (1) Hybrid symbolic and neural operation [4]. Users can input queries requiring algebraic, neural, or hybrid computation; (2) Massive graph data and embeddings. Not only do the graph data of different domain knowledge exhibit tremendous scale [57], but also various types of embeddings [58] of these graph data further enlarge the volume; (3) Read intensive workload. During the serving stage, most of the graph data and embeddings are queried more frequently rather than updated [59]; (4) Dynamic workload fluctuation. Different parts of the graph data and embeddings are accessed in different time slots and the number of online users and frequency and data volume of one query fluctuate [60].\nSystem Requirements. The neural graph database system should fulfill the following requirements to handle these features effectively and efficiently. (1) Co-located graph and embedding management. The NGDB system should support symbolic graph data and neural embedding management. (2) High query performance. The latency of a single query and system throughput for numerous tenants serving massive data should be optimized. (3) Scalability.. The hardware resource management should be scalable to handle workload fluctuation, especially computational resources, cost-efficiently. Challenges are introduced to the system design of neural graph databases to implement these system features."}, {"title": "Challenges of System Design", "content": "User Interface Design. Existing vector databases provide SQL-like interfaces and parameterized API[61], while most of the interfaces mainly focus on relational data. Graph databases provide numerous interfaces[58], but there is little experience in combining neural operations into symbolic graph operations. It is essential to design highly expressive declarative user interfaces as well as programming interfaces.\nQuery-Oriented Distributed Storage. Due to the massive volume of graph data and corresponding embeddings, which is out of the capacity of standalone storage, distributed storage is an indispensable mechanism of NGDB. Under read-intensive workload, partitioning (or sharding), acting as a distributed index, tailored for most frequent and costly types of query could remarkably reduce the intermediate data transfer, consequently enhancing the overall latency and throughput[61]. Practices in graph database community[62, 63, 64, 65] and vector database community concludes valuable principles and strategies on distributed storage and indexing of graph data and embedding separately. However, the hybrid storage of both data types is not explored, especially in circumstances where hybrid queries, requiring both symbolic and neural processes, are of evident importance. A typical example question is about whether embeddings and raw graph data shall be co-located. Although some open source graph database[66, 67] and vector databases[68, 60, 69, 70] could be utilized as standalone storage engine in NGDB, partitioning should be carefully designed under specific query workload."}, {"title": "Challenge 7: Compatibility of NGDB with Traditional Graph Database", "content": "Like graph databases, Neural Graph Databases (NGDB) are another way of the data model that derives the properties from the existing graphs, including nodes and edges, to represent entities and their relationships [4]. This structural consistency makes migrating and interoperating data between the two databases relatively easy. In terms of interfaces, NGDB can maintain support for standard graph query languages [66, 79] while providing vectorized query capabilities, allowing users to query and operate in familiar languages. In terms of operations, traditional CRUD operations remain fully functional, with the reasoning function of neural networks serving as enhanced features. For instance, conventional graph databases provide foundational support in query processing through mature storage and indexing technologies, while NGDB handles queries requiring missing link inference. Such compatibility design will enable a seamless system transition, where users can migrate to get NGDB capabilities without completely reconstructing existing applications. However, NGDB faces several challenges with traditional graph databases:\nNovel Query Interface. Incorporating deep learning and graph neural networks extends beyond conventional graph database functionalities, requiring novel interfaces for deep learning-based queries and inference. This creates compatibility issues when attempting to reuse existing query languages, highlighting the need to develop new query languages or extend current ones [80, 79].\nPerformance-Consistency Trade-off. While traditional graph databases are optimized for storage and querying [81], they may struggle to meet performance requirements when handling large-scale graph-based deep-learning tasks. NGDB emphasizes representation learning on nodes and edges [4], requiring consideration of high-performance computing and distributed training paradigms. For instance, during conventional CRUD operations, NGDB may need to update node and relation embeddings, introducing additional computational overhead. Moreover, integrating neural components introduces temporal consistency challenges, where model updates may lead to temporary discrepancies between the base graph data and learned representations. Finding an optimal balance between consistency guarantees and computational efficiency remains a considerable challenge for NGDB systems."}, {"title": "Challenge 8: Grounding to Vectors with NGDB", "content": "Grounding natural language to knowledge bases has been extensively studied in conventional graph databases. Traditional approaches typically handle different grounding scenarios: hypothesis or query grounding (with free variables) [82, 20], and entity [83] or event [11, 84]). With the emergence of NGDBs, where structural information and semantic content are encoded as vectors, the grounding process faces new challenges and opportunities. Recent work [4] introduces a neural graph engine that learns query planning and execution strategies through interactions with Neural Graph Storage. However, grounding to general NGDBs still presents several unique challenges.\nSemantic Granularity and Disambiguation. Semantic granularity and disambiguation pose fundamental difficulties. The grounding process must accurately translate natural language queries into appropriate vector representations while determining suitable levels of semantic granularity, such events, propositions, etc. [85, 84]. This challenge is compounded by the need to handle abstraction and polysemy when mapping linguistic elements to vector spaces, as meanings can vary significantly based on context.\nCompositional Semantics and Reasoning. Second, compositional semantics and reasoning path selection present significant challenges. NGDBs must effectively represent complex multi-hop relations while maintaining transitivity and logical consistency in vector operations. The system needs to identify relevant paths in the vector space for query resolution, which becomes particularly challenging when dealing with multiple possible reasoning paths. In addition, determining appropriate termination criteria for path exploration is crucial for both efficiency and accuracy.\nInterpretation and Groundedness Evaluation. The third challenge is around interpretation and groundedness evaluation. The system is expected to reliably convert vector-based results back to natural language while providing clear explanations for its reasoning process. Additionally, it needs to report the level of groundedness for each grounding operation, ensuring semantic fidelity is maintained throughout the process. This is particularly important for applications requiring high precision and explainability."}, {"title": "Challenge 9: Adapting NGDB to LLM", "content": "This section explores the integration of Neural Graph Databases (NGDBs) with Large Language Models (LLMs) to enable joint reasoning and Retrieval-Augmented Generation (RAG). NGDBs can serve as retrieval modules for LLMs, leveraging structured data and reasoning capabilities to enhance generated outputs' accuracy, scalability, and contextual relevance. Joint learning of LLMs and NGDBs involves training these systems within a unified framework to combine natural language understanding with advanced logical reasoning.\nNGDB-RAG: Definition and Components. NGDB-RAG (Neural Graph Database - Retrieval-Augmented Generation) is a system that integrates NGDBs with LLMs to enhance both retrieval and generation tasks. The NGDB-RAG system is composed of three main components. The first is the neural graph storage, which stores embeddings of nodes and edges in the graph. These embeddings capture both local and global structural relationships within the graph, providing a rich representation of the data. The second component is the neural query engine, which tries formulating and processing logical queries in the embedding space. This engine enables flexible modeling and supports logical operations such as conjunction, disjunction, and negation, allowing for robust retrieval even in incomplete or noisy graphs. The third component is integrating with LLMs, where NGDB reasoning results are incorporated into the language model. This integration can be achieved through text-based methods, by converting structured data into natural language, or through vector-based methods, by embedding structured data as vectors for direct input into the LLM.\nFunctionality of NGDB-RAG. NGDB-RAG enhances retrieval by utilizing the structured relationships in NGDBs to perform advanced reasoning tasks. Unlike traditional RAG systems that rely on document similarity, NGDB-RAG leverages the intricate dependencies within knowledge graphs to retrieve more accurate and contextually relevant information. In the generation process, NGDB-RAG integrates structured knowledge and reasoning capabilities from NGDBs to improve the generated text's factual accuracy and logical consistency while reducing hallucinations. Furthermore, NGDB-RAG is designed to handle large-scale graphs and supports various query types, including temporal, spatial, and numerical reasoning, ensuring scalability and expressiveness in practical applications.\nJoint Learning Framework. The joint learning framework of NGDBs and LLMs employs a co-training approach where both systems share parameters or representation spaces to enable collaborative learning. Improvements in one component positively influence the other, creating a feedback loop that enhances the overall system. The combined training objective is expressed as: $L_{total} = L_{LLM} + \\lambda L_{NGDB}$. In this equation, $L_{LLM}$ represents the loss associated with the language model, typically the cross-entropy loss for next-token prediction. $L_{NGDB}$ denotes the loss related to NGDB reasoning tasks, such as the error between predicted and true query answers. The hyperparameter $\\lambda$ controls the balance between the two loss components. The objective of this joint training is to improve the reasoning capabilities of the NGDB while enhancing the LLM performance.\nFuture work aims to develop the co-training framework further to enable simultaneous training of NGDB reasoning engines and LLMs, ensuring parameter sharing and collaborative learning. Efforts are also being made to refine the combined loss function to balance language modeling and reasoning tasks better, enhancing both components' performance. Integration modules are being developed to incorporate NGDB reasoning results into LLMs through text-based and vector-based methods. These advancements are expected to create a unified system capable of performing advanced reasoning and generating high-quality, contextually accurate text."}, {"title": "Challenge 10: Smart Neural Graph Databases", "content": "Benefited from its rich functionalities, Agentic NGDB offers a wide range of applications across domains:\n\u2022 Autonomous Data Management: Agentic NGDB can autonomously manage complex datasets, optimize query execution, and organize storage structures without human intervention. This is particularly useful in large-scale systems where manual optimization is impractical.\n\u2022 Personalized Recommendations: Through continuous learning, Agentic NGDB can provide real-time personalized recommendations by analyzing user preferences and graph-based relationships. This is crucial in e-commerce and social networks, where tailored experiences drive user engagement [86].\n\u2022 Complex Event Processing: Agentic NGDB is well-suited for handling complex event processing [11], where multiple events and data streams need to be analyzed in real-time. By leveraging their semantic understanding and neural inference, Agentic NGDB can identify correlations and patterns across seemingly unrelated events, making them valuable in cybersecurity, fraud detection, and IoT systems."}, {"title": "Conclusion", "content": "Agentic Neural Graph Databases (Agentic NGDBs) represent an advancement in data management, building on traditional graph databases and Neural Graph Databases (NGDBs) by introducing autonomy, continuous learning, and advanced reasoning.\nThis paper identifies ten key challenges to realizing Agentic NGDBs, including semantic representation, abductive reasoning, generalization across query types, scalability, privacy, and integration with foundation models like large language models (LLMs). Ensuring compatibility with traditional databases, grounding knowledge in vectors, and developing distributed systems are essential for achieving robust and scalable solutions.\nBy overcoming these challenges, Agentic NGDBs can transform modern data-driven applications. Their ability to autonomously generate and execute queries, support continuous learning, and integrate symbolic and neural reasoning offers new possibilities in autonomous data management, personalized recommendations, and complex event processing. These advancements promise to redefine how we manage, query, and reason over interconnected data for the future."}]}