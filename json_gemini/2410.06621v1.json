{"title": "Effective Exploration Based on the Structural Information Principles", "authors": ["Xianghua Zeng", "Hao Peng", "Angsheng Li"], "abstract": "Traditional information theory provides a valuable foundation for Reinforcement Learning (RL), particularly through representation learning and entropy maximization for agent exploration. However, existing methods primarily concentrate on modeling the uncertainty associated with RL's random variables, neglecting the inherent structure within the state and action spaces. In this paper, we propose a novel Structural Information principles-based Effective Exploration framework, namely SIZE. Structural mutual information between two variables is defined to address the single-variable limitation in structural information, and an innovative embedding principle is presented to capture dynamics-relevant state-action representations. The SI2E analyzes value differences in the agent's policy between state-action pairs and minimizes structural entropy to derive the hierarchical state-action structure, referred to as the encoding tree. Under this tree structure, value-conditional structural entropy is defined and maximized to design an intrinsic reward mechanism that avoids redundant transitions and promotes enhanced coverage in the state-action space. Theoretical connections are established between SI2E and classical information-theoretic methodologies, highlighting our framework's rationality and advantage. Comprehensive evaluations in the MiniGrid, MetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E significantly outperforms state-of-the-art exploration baselines regarding final performance and sample efficiency, with maximum improvements of 37.63% and 60.25%, respectively.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has emerged as a pivotal technique for addressing sequential decision-making problems, including game intelligence [Vinyals et al., 2019, Badia et al., 2020], robotic control [Andrychowicz et al., 2017, Liu and Abbeel, 2021], and autonomous driving [Prathiba et al., 2021, P\u00e9rez-Gil et al., 2022]. In the realm of RL, striking a balance between exploration and exploitation is crucial for optimizing agent policies and mitigating the risk of suboptimal outcomes, especially in scenarios characterized by high dimensions and sparse rewards [Zhang et al., 2021b].\nRecently, advancements in information-theoretic approaches have shown promise for exploration in self-supervised settings. The maximum entropy framework over the action space [Haarnoja et al., 2017] has led to the development of robust algorithms such as Soft Q-learning [Nachum et al., 2017], SAC [Haarnoja et al., 2018], and MPO [Abdolmaleki et al., 2018]. Additionally, various objectives focused on maximizing state entropy are utilized to ensure comprehensive state coverage [Hazan et al., 2019, Islam et al., 2019]. To facilitate the exploration of complex state-action pairs, MaxRenyi optimizes R\u00e9nyi entropy across the state-action space [Zhang et al., 2021a]. However, a prevalent issue with entropy maximization strategies is their tendency to bias exploration towards low-value states, making them vulnerable to imbalanced state-value distributions in supervised settings. To mitigate"}, {"title": "2 Preliminaries", "content": "In this section, we formalize the definitions of fundamental concepts. The descriptions of primary notations are summarized in Appendix A.1 for ease of reference."}, {"title": "2.1 Traditional Information Principles", "content": "Consider the random variable pair Z = (X, Y) with a joint distribution probability denoted by p(x, y) \u2208 (0,1). The marginal probabilities, p(x) and p(y), are defined as p(x) = \\sum_{y}p(x,y) and p(y) = \\sum_{x}p(x, y), respectively. The joint Shannon entropy [Shannon, 1953] of X and Y is H(X,Y) = \\sum_{(x,y)} [p(x, y) log p(x, y)], which quantifies the total uncertainty in Z. Conversely, the marginal entropies H(X) = - \\sum_{x} [p(x)\\cdotlog p(x)] and H(Y) = \u2212 \\sum_{y} [p(y) \\cdot log p(y)] characterize the uncertainty in X and Y individually. The mutual information I(X; Y) = \\sum_{x,y} p(x,y) log \\frac{p(x, y)}{p(x)p(y)} quantifies the shared uncertainty between X and Y. It satisfies the following relationship: I(X; Y) = H(X) + H(Y) \u2013 H(X,Y)."}, {"title": "2.2 Reinforcement Learning", "content": "Within the context of RL, the sequential decision-making problem is formalized as a Markov Decision Process (MDP) [Bellman, 1957]. The MDP is characterized by a tuple (O, A, P, Re, y), where O denotes the observation space, A the action space, P the environmental transition function, Re the extrinsic reward function, and \u03b3\u2208 [0, 1) the discount factor. At each discrete timestep t, the agent selects an action at \u2208 A upon observing ot \u2208 O. This leads to a transition to a new observation Ot+1 ~ P(ot, at) and a reward re\u2208 R. The policy network \u03c0is optimized to maximize the cumulative long-term expected discounted reward."}, {"title": "Maximum State Entropy Exploration", "content": "In environments with sparse rewards, agents are encouraged to explore the state space extensively, which can be incentivized by maximizing the Shannon entropy H(S) of state variable S. When the prior distribution p(s) is not available, the non-parametric k-nearest neighbors (k-NN) entropy estimator [Singh et al., 2003] is employed. For a given set of n independent and identically distributed samples from a d\u201e-dimensional space \\{x_{i}\\}_{i=0}^{n-1}, the entropy of variable X is estimated as follows:\n \\hat{H}_{KL}(X) = \\frac{d_r}{n} \\sum_{i=0}^{n-1} log d(x_i) + C,\n(1)"}, {"title": "Information Bottleneck Principle", "content": "In the supervised learning paradigm, representation learning aims to transform an input source X into a representation Z, targeted towards an output source Y. The Information Bottleneck (IB) principle [Tishby et al., 2000] refines this process by maximizing the mutual information I(Z; Y) between Z and Y, capturing the relevant features of Y within Z. Concurrently, the IB principle imposes a complexity constraint by minimizing the mutual information I(Z; X) between Z and X, effectively discarding irrelevant features. To balance these objectives, the IB principle utilizes a Lagrangian multiplier, facilitating a balanced trade-off between the richness of the representation and its complexity."}, {"title": "2.3 Structural Information Principles", "content": "The encoding tree T of an undirected and weighted graph G = (V, E) is characterized as a rooted tree with the following properties: 1) Each tree node a in T corresponds to a subset of graph vertices Va V. 2) The subset Vx of tree root A encompasses all vertices in V. 3) Each subset V of a leaf node v in T only contains a single vertex v, thus V\u2081 = {v}. 4) For each non-leaf node a, the number of its children is assumed as la, with the i-th child specified as ai. The collection of subsets Va1,..., Vala constitutes a sub-partition of Va.\nGiven an encoding tree T whose height is at most K, the K-dimensional structural entropy of graph G is defined as follows:\n\\hat{H}_{T} (G) = -\\sum_{\\alpha \\in T,\\alpha \\neq \\lambda} \\frac{g_{\\alpha}}{vol(G)} \\cdot \\frac{vol(\\alpha)}{vol(\\alpha^{-})}, \\qquad H^{K}(G) = min_{T} H^{T}(G),\n(2)\nwhere ga is the weighted sum of all edges connecting vertices within the subset Va to vertices outside the subset Va."}, {"title": "3 Structural Mutual Information", "content": "In this section, we address the single-variable constraint prevalent in existing structural information principles and introduce the concept of structural mutual information for subsequent state-action representation learning within our SI2E framework.\nGiven the random variable pair (X, Y) with |X| = |Y| = n, we construct an undirected bipartite graph Gxy to represent the joint distribution of X and Y. In Gry, each vertex x \u2208 X connects to each vertex y \u2208 Y via weighted edges, where the weight of each edge equals the joint probability p(x, y). Notably, no edges connect vertices within the same set, X or Y, and the total sum of the edge weights is 1, \u03a3x,y p(x, y) = 1. Each single-step random walk in Gry accesses either a vertex from X or Y. The structural entropy of variable X in Gry is defined as the number of bits required to encode all accessible vertices in the set X. It is calculated using the following formula:\n H^{SI}(X) = -\\sum_{x\\in X} \\frac{p(x)}{vol(G_{xy})}  log \\frac{p(x)}{vol(G_{xy})} = -\\sum_{x\\in X} \\frac{p(x)}{2}  log \\frac{p(x)}{2} = -\\frac{1}{2} \\sum_{x\\in X} p(x)  log \\frac{p(x)}{2} ,\n(3)\nwhere the sum of all vertex degrees is twice the total sum of edge weights, resulting in vol(Gxy) = 2.\nThe structural entropy HSI (Y) is defined similarly. We restrict the partitioning structure of Gry to 2-layer approximate binary trees, denoted as T2, to calculate the required bits to encode accessible vertices in X or Y, defined as the joint structural entropy. This tree structure mandates that each intermediate node (neither root nor leaf) has precisely two children. We begin by initializing a one-layer encoding tree, Tou, designating each non-root node a's parent as the root \\, with a\u00af = \u03bb. By applying the stretch operator from the HCSE algorithm [Pan et al., 2021], we pursue an iterative and greedy optimization of Tou, further detailed in Appendix A.2. The optimal encoding tree, Try, for Gry and the joint entropy under Tu are achieved through:\n T_{xy}^{*} = arg min_{T \\in T_2} H_{T}^{xy} (G_{xy}), \\quad H^{T_{xy}^{*}} (X,Y) = H^{T_{xy}^{*}} (G_{xy}).\n(4)\nUtilizing 2-layer approximate binary trees as the structural framework ensures computational tractabil-ity and more complex structures will increase the cost of increased computational complexity, which can be prohibitive for practical applications."}, {"title": "4 The Proposed SI2E Framework", "content": "In this section, we describe the detailed designs of the proposed SI2E framework, which captures dynamic-relevant state-action representations through structural mutual information (see Section"}, {"title": "4.1 State-action Representation Learning", "content": "To effectively learn dynamics-relevant state-action representations, we present an innovative embedding principle that maximizes the structural mutual information with subsequent states and minimizes it with current states.\nStructural Mutual Information Principle. In this phase, the input variables at timestep t encompass the current observation Ot and the action At, with the target being the subsequent observation Ot+1. We denote the encoding of observations Ot and Ot+1 as states St and St+1, respectively. We aim to generate a latent representation Zt for the tuple (St, At), which preserves information relevant to St+1 while compressing information pertinent to St. This embedding process mentioned above is detailed as follows:\nS_t = f_s(O_t), \\quad S_{t+1} = f_s(O_{t+1}), \\quad Z_t = f_z(S_t, A_t),\n(7)\nwhere fs and f\u2082 are the respective encoders for states and state-action pairs (step I. a in Figure 2). For the state-action embeddings Zt, we construct two undirected bipartite graphs, Gzs and Gzs', as shown in step I. b of Figure 2. These graphs represent the joint distributions of Zt with the current states St and subsequent states St+1. In step I. c of Figure 2, we generate 2-layer approximate binary trees for Gzs and Gzs' and calculate the mutual information ISI (Zt; St) and ISI (Zt; St+1) using Equation 5. Building upon the Information Bottleneck (IB) [Tishby et al., 2000], we present an embedding principle that aims to minimize ISI (Zt; St) while maximizing ISI (Zt; St+1), as demonstrated in step I. d of Figure 2. When the joint distribution between variables Zt and St+1 shows a one-to-one correspondence-meaning for each zt \u2208 Zt value, there is a unique st+1 \u2208 St+1 corresponding to it, and vice versa-their mutual information takes its maximum value. We introduce a theorem to elucidate the equivalence between ISI (Zt; St+1) and I(Zt; St+1) under this condition."}, {"title": "Theorem 4.1", "content": "For a joint distribution of variables X and Y that shows a one-to-one correspondence, ISI(X; Y) equals I(X; Y).\nA detailed proof is provided in Appendix B.3. When Zt and St are mutually independent, the mutual information I(Zt; St) attains its minimum value. Our ISI (Zt; St) goes beyond this, incorporating the joint entropy H(Zt, St) according to Theorem 3.4. This integration effectively eliminates the irrelevant information embedded in the representation variable Zt, a significant step in our research. Consequently, structural mutual information can be considered a reasonable and desirable learning objective for acquiring dynamics-relevant state-action representations."}, {"title": "Representation Learning Objective", "content": "Due to the computational challenges of directly minimizing ISI (Zt; St), we formulate a variational upper bound I(Zt; St) + H(Zt|St) + H(St) (see Appendix C.2). Noting that the term H(St) is extraneous to our model, we equate the minimization of ISI (Zt; St) to the minimization of I(Zt; St) and H(Zt|St).\nBy employing a feasible decoder to approximate the marginal distribution of Zt, we derive an upper bound of I(Zt; St) (See Appendix C.3) as follows:\nI(Z_t; S_t) < \\sum [p(z_t, S_t) \\cdot D_{KL}(p(z_t|S_t)||q_m(z_t))] \\equiv L_{up}.\n(8)\nTo concurrently decrease the conditional entropy H(Zt|St), we introduce a predictive objective (See Appendix C.4) through a tractable decoder qz|s for the conditional probability p(zt|st) as follows:\nH(Zt|St) <\\sum_i [p(z_t, St) \\cdot log \\frac{1}{q_{z|s}(z_t|St)}] \\equiv L_{z|s},\n(9)\nwhere Lzis represents the log-likelihood of Zt given St.\nTo efficiently optimize ISI (Zt; St+1), we maximize its lower bound, I(Zt; St+1), as detailed in Theorem 3.4. By utilizing an alternative decoder qs|z for the conditional probability p(st+1|zt), we obtain a lower bound of I(Zt; St+1) (See Appendix C.5) as follows:\nI(Zt; St+1) \\geq \\sum [p(z_t, St+1) \\cdot log q_{s|z}(St+1|zt)]  \\equiv L_{s|z},\n(10)\nwhere Ls|z denotes the log-likelihood of St+1 conditioned on Zt."}, {"title": "4.2 Maximum Structural Entropy Exploration", "content": "We have designed a unique intrinsic reward mechanism to address the challenge of imbalance exploration towards low-value states in traditional entropy strategies, as discussed by [Kim et al., 2023]. Specifically, we generate a hierarchical state-action structure based on the agent's policy and define value-conditional structural entropy as an intrinsic reward for effective exploration.\nHierarchical State-action Structure. Derived from the history of agent-environment interactions, we extract state-action pairs (step II. a in Figure 2) to form a complete graph Gsa (step II. b in Figure 2) that encapsulates the value relationships caused by the agent's policy. Within this graph, any two vertices vi and vj is connected by an undirected edge whose weight Wij is determined as: wij = ||\u03c0(si, a\u0390) \u2013 \u03c0(s1, \u03b1\u0390)||2. The state-action pairs (s\u012f, a\u00bf) and (s\u0131, af) are associated with vertices vi and vj, respectively. We minimize the 2-dimensional structural entropy of this graph Gsa to generate its 2-layer optimal encoding tree, denoted as Tsa (step II. c in Figure 2). This tree Tsa delineates a hierarchical community structure among the state-action vertices, with the root node corresponding to a community encompassing all vertices. Each intermediate node in Ta corresponds to a sub-community, including vertices that share similar \u03c0 values."}, {"title": "Value-conditional Structural Entropy", "content": "To measure the extent of the policy's coverage across the state-action space, we construct an additional distribution graph G'sa (step II. d in Figure 2). The graph G's shares the same vertex set as Gsa. The following proposition confirms the existence of such a graph, with a detailed proof provided in Appendix B.4."}, {"title": "Proposition 4.2", "content": "Given positive visitation probabilities p(s\u0142, a\u2081), ..., p(sr-1, a-1) for all state-action pairs, there exists a weighted, undirected, and connected graph G'sa, where each vertex's degree di equals its visitation probability p(si, a).\nIn the graph G'sa, the set of all state-action vertices is denoted as Vo, and the set of all state-action sub-communities is denoted as V1. The Shannon entropies associated with the distribution of visitation probabilities for these sets are represented as H(Vo) and H(V1), respectively, where H(Vo) = H(St, At). Within the 2-layer state-action community represented by Ta, we define the structural entropy of G'sa using Equation 2, denoted as HTsa (G'sa) (step II. e in Figure 2). The following theorem delineates the relationship between the value-conditional entropy HTsa (G'sa) with the state-action Shannon entropy H(St, At). A detailed proof is provided in Appendix B.5."}, {"title": "Theorem 4.3", "content": "For a tuning parameter 0 < \u03da < 1, it holds for the structural entropy HTsa (G'sa) and the Shannon entropy H(St, At) that:\n\u03b6\u00b7 H(St, At) \u2264 H(V\u2030) \u2013 H(V\u2081) \u2264 HTsa(G'sa) < H(St, At),\n(11)\nwhere H(Vo) \u2013 H(V\u2081) is a variational lower bound of HTsa (G'sa). On the one hand, the term H(Vo) ensures maximal coverage of the entire state-action space, analogous to the traditional Shannon entropy. On the other hand, the term H(V\u2081) mitigates uniform coverage among state-action sub-communities with diverse values, thus addressing the challenge of imbalance exploration. By identifying the hierarchical state-action structure caused by the agent's policy, the SI2E achieves enhanced maximum coverage exploration, thereby guaranteeing its exploration advantage."}, {"title": "Estimation and Intrinsic Reward", "content": "Considering the impracticality of directly acquiring visitation probabilities, we employ the k-NN entropy estimator in Equation 1 to estimate the lower bound:\nH(Vo) - H(V\u2081) \u2248 \\frac{d_z}{n_0} \\sum_{i=0}^{n_0-1} log d(v_i) - \\frac{d_z}{n_1} \\sum_{i=0}^{n_1-1} log d(v_i) + C, v_i \\in V_0, v_i \\in V_1, (12)\nwhere d\u2082 is the dimension of state-action embedding, no and n\u2081 are the vertex numbers in Vo and V\u2081, and d(v) is twice the distance from vertex v to its k-th nearest neighbor. By ignoring the constant term in Equation 12, we define the intrinsic reward r\u1ebb and train RL agents to address the target task using a combined reward rt = r\u2021 + \u03b2\u00b7 r\u012f (step II. f in Figure 2), where \u1e9e is a positive hyperparameter that modulates the trade-off between exploration and exploitation. The pseudocode, complexity analysis, and limitations of our framework are provided in Appendix A."}, {"title": "5 Experiments", "content": "In this section, we present a comprehensive suite of comparative experiments on MiniGrid [Chevalier-Boisvert et al., 2018], MetaWorld [Yu et al., 2020], and the DeepMind Control Suite (DMControl) [Tunyasuvunakool et al., 2020] to evaluate the effectiveness of SI2E in terms of both final performance and sample efficiency. Consistent with previous work [Zeng et al., 2023c], we measure the required steps to attain specified rewards (0.9 times SI2E's convergence reward) as a benchmark for assessing sample efficiency. For the SI2E implementation, we employ a randomly initialized encoder optimized to minimize the combined loss L. All experiments are conducted with 10 different random seeds, and the learning curves are delineated in Appendix E."}, {"title": "5.1 MiniGrid Evaluation", "content": "Initially, we assess our framework on navigation tasks using the MiniGrid benchmark, which includes goal-reaching tasks in sparse-reward environments. This setting is partially observable: the agent receives a 7 \u00d7 7 \u00d7 3 embedding of the immediate surrounding grid rather than the entire grid. For comparative purposes, we employ the A2C agent [Mnih et al., 2016] with Shannon entropy (SE) [Seo et al., 2021] and value-based state entropy (VCSE) [Kim et al., 2023] as our baselines. Table 1 (upper) displays the average values and standard deviations of success rates and required steps for various navigation tasks. The tasks encompass navigation with obstacles (SimpleCrossingS9N1), long-horizon navigation (RedBlueDoors, DoorKey, and Unlock), and long-horizon navigation with obstacles (KeyCorridorS3R1). The SI2E consistently exhibits enhanced final performance and sample efficiency across tasks, with an average success rate increase of 4.92%, from 89.97% to 94.40%, and an average decrease in required steps of 38.10%, from 635.65K to 393.47K. In the RedBlueDoors task, where baseline performances are inadequate, our SI2E significantly improves the success rate from 79.82% to 85.80% and reduces the required steps from 1161.90K to 461.90K."}, {"title": "5.2 Meta World Evaluation", "content": "We further evaluate the SI2E framework on visual manipulation tasks from the MetaWorld benchmark, which presents exploration challenges due to its large state space. We select the model-free DrQv2 algorithm as the underlying RL methodology. Adhering to the setup of [Seo et al., 2023], we employ the same camera configuration and normalize the reward with a scale of 1. We summarize the success rates and required steps for all exploration methods across six MetaWorld tasks in Table 1 (lower)."}, {"title": "5.3 DMControl Evaluation", "content": "Subsequently, we evaluate our framework across various continuous control tasks within the DM-Control suite. As the foundational agent, we choose the same DrQv2 algorithm, which operates on pixel-based observations. We incorporate a state-action exploration baseline, MADE [Zhang et al., 2021b], for a more comprehensive comparison. We evaluate all exploration methods across six continuous control tasks, documenting the episode rewards in Table 2. Observations reveal that SI2E remarkably increases the mean episode reward in each DMControl task. Specifically, in the Cartpole Swingup task characterized by sparse rewards, our framework boosts the average reward from 707.76 to 795.09, resulting in a 12.34% improvement in the final performance. Moreover, we compare the sample efficiency of SI2E and the best-performing baseline in Appendix E.3.\nThese results not only demonstrate the effectiveness of SI2E in acquiring dynamics-relevant rep-resentations for state-action pairs but also highlight its potential to motivate agents to explore the state-action space. To better understand the rationality and advantage of the SI2E framework, we provide visualization experiments in Appendix E.4."}, {"title": "5.4 Ablation Studies", "content": "To further investigate the impact of two critical components within the SI2E framework, embedding principle (Section 4.1) and intrinsic reward mechanism (4.2), we perform ablation studies on Meta-World and DMControl tasks, focusing on two distinct variants: (i) SI2E-DB, which utilizes the DB bottleneck [Bai et al., 2021] for learning state-action representations, and (ii) SI2E-VCSE, employing the state-of-the-art VCSE approach [Kim et al., 2023] for calculating intrinsic rewards. As depicted in Figure 3, SI2E surpasses all variants regarding final performance and sample efficiency. This outcome underscores the essential role of these critical components in conferring SI2E's superior capabilities. Additional ablation studies for the parameters \u1e9e and n are available in Appendix E.5."}, {"title": "6 Related Work", "content": ""}, {"title": "6.1 Maximum Entropy Exploration", "content": "Maximum entropy exploration has evolved from focusing initially on unsupervised methods to incorporating task rewards in more advanced supervised models. In the unsupervised paradigm,"}, {"title": "6.2 Representation Learning", "content": "Novelty Search [Tao et al., 2020] and Curiosity Bottleneck [Kim et al., 2019b] leverage the Infor-mation Bottleneck principle for effective representation learning. Additionally, the EMI method [Kim et al., 2019a] maximizes mutual information in both forward and inverse dynamics to develop desirable representations. However, these methods are limited by the lack of an explicit mechanism to address the white noise issue in the state space. To overcome this challenge, the Dynamic Bottleneck model [Bai et al., 2021] is introduced for robust exploration in complex environments.\nOur work defines structural mutual information to measure the structural similarity between two variables for the first time. Additionally, we present an innovative embedding principle that incorpo-rates the entropy of the representation variable. This approach more effectively eliminates irrelevant information than the traditional information bottleneck principle."}, {"title": "6.3 Structural Information Principles", "content": "Since the introduction of structural information principles [Li and Pan, 2016], these principles have significantly transformed the analysis of network complexities, employing metrics such as structural entropy and partitioning trees. This innovative approach has not only deepened the understanding of network dynamics\u2014even in the context of multi-relational graphs [Cao et al., 2024a]\u2014but has also led to a wide array of applications across different domains. The application of structural information principles has extended to various fields, including graph learning [Wu et al., 2022], skin segmentation [Zeng et al., 2023a], and the analysis of social networks [Peng et al., Zeng et al., 2024, Cao et al., 2024b]. In the domain of reinforcement learning, these principles have been instrumental in defining hierarchical action and state abstractions through encoding trees [Zeng et al., 2023b,c], marking a significant advancement in robust decision-making frameworks."}, {"title": "7 Conclusion", "content": "We propose SI2E, a novel exploration framework based on structural information principles. This framework defines structural mutual information to effectively capture state-action representations relevant to environmental dynamics. It maximizes the value-conditional structural entropy to enhance coverage across the state-action space. We have established theoretical connections between SI2E and traditional information-theoretic methodologies, underscoring the framework's rationality and advantages. Through extensive and comparative evaluations, SI2E significantly improves final performance and sample efficiency over state-of-the-art exploration methods. Our future work includes expanding the height of encoding trees and the range of experimental environments. Our goal is for SI2E to remain a robust and adaptable tool in reinforcement learning, particularly suited to high-dimensional and sparse-reward contexts."}, {"title": "A Framework Details", "content": ""}, {"title": "A.1 Notations", "content": ""}, {"title": "A.2 Tree Optimization on T2", "content": "In this subsection, we have provided additional explanations and illustrative examples for the encoding tree optimization on T2. As shown in Figure 4, the stretch operator is executed over sibling nodes ai and aj that share the same parent node, \u5165. The detailed steps of this operation are as follows:\n \u03b1\u0384\u00af = \u03bb, \u03b1\u00a1\u00af = a', \u03b1j\u00af = a',\n(13)\nwhere a' is the added tree node via the stretch operation."}, {"title": "A.3 Intuitive Example of Optimal Encoding Tree", "content": ""}, {"title": "A.4 The Pseudocode of SI2E", "content": ""}, {"title": "A.5 Complexity Analysis of SI2E", "content": "Within the SI2E framework, we analyze the time complexities of critical components independent of the underlying RL algorithm. During the state-action representation phase, the construction of bipartite graphs takes O(n\u00b2) time complexity, the generation of 2-layer approximate binary trees requires O(n \u00b7 log2 n) time complexity, and the calculation of mutual information involves a time complexity of O(n\u00b2). During the effective exploration phase, the generation of hierarchical community structure incurs a O(n \u00b7 log\u00b2 n) complexity, the construction of the distribution graph leads to a complexity of O(n\u00b2), and value-conditional structural entropy is calculated with O(n) time complexity."}, {"title": "A.6 Limitations", "content": "Our work, which is a result of thorough research, aims to address the limitations of information theory methods and structural information theory research in reinforcement learning. Therefore, we have selected the state-of-the-art information theory exploration method as the baseline in our evaluation. Despite the current limitations in height due to complexity and cost issues, the encoding tree structure in the SI2E framework holds immense potential. In our future research, we are optimistic about expanding its height further and conducting more research on the advantages and restrictions brought by this expansion."}, {"title": "B Theorem Proofs", "content": ""}, {"title": "B.1 Proof of Proposition 3.1", "content": "Proof. For any two vertices vi \u2208 V and vj \u2208 V without any edge connecting them, their correspond-ing tree nodes are denoted as a\u00a1 and aj. These nodes' parents are initially assigned as the root node A. Before executing one stretch operation on vertices a\u00a1 and aj, their structural entropies are calculated as follows:\n H(G; a_i) = - \\frac{d_i}{vol(G)} \\cdot log \\frac{d_i}{vol(G)}, \\qquad H(G; a_j) = - \\frac{d_j}{vol(G)} \\cdot log \\frac{d_j}{vol(G)},\n(18)\nwhere di and dj are the degrees of vertices vi and vj. Post-stretch operation, their structural entropies are given by:\n H(G; a_i) = - \\frac{d_i}{vol(G)} \\cdot log \\frac{d_i}{vol(\\alpha')}, \\qquad H(G; a_j) = - \\frac{d_j}{vol(G)} \\cdot log \\frac{d_j}{vol(\\alpha')},\n(19)\nwhere a' are their new common parent node. The absence of an edge between vi and vj ensures that:\n g_{\\alpha'} = d_i + d_j, \\qquad vol(\\alpha') = d_i + d_j.\n(20)\nThe structural entropy of a' can be determined as:\n H(G; \\alpha') = - \\frac{g_{\\alpha'}}{vol(G)} \\cdot log \\frac{vol(\\alpha')}{vol(G)} = - \\frac{d_i+d_j}{vol(G)} \\cdot log \\frac{d_i + d_j}{vol(G)}.\n(21)\nThe entropy reduction \u2206H, consequent to the stretch operation on vertices vi and vj, is calculated as:\n\u0394H =  -\\frac{d_i}{vol(G)} \\cdot log \\frac{d_i}{vol(G)} - \\frac{d_j}{vol(G)} \\cdot log \\frac{d_j}{vol(G)} -\\frac{d_i+d_j}{vol(G)} \\cdot log \\frac{d_i + d_j}{vol(G)}] = [\\frac{d_i}{vol(G)} \\cdot log \\frac{d_i}{vol(G)}+\\frac{d_j}{vol(G)} \\cdot log \\frac{d_j}{vol(G)}] - [\\frac{d_i+d_j}{vol(G)} \\cdot log \\frac{d_i + d_j}{vol(G)}] = 0.\n(22)\nGiven the zero reduction in entropy, as per lines 5 and 6 of the optimization algorithm for T2 (See Appendix A.2), the stretch operation involving vi and vj is omitted from the optimization process."}, {"title": "B.2 Proof of Theorem 3.4", "content": "Proof. The difference between the mutual information ISI (X; Y) and I(X; Y) is expressed as:\nISI(X; Y) \u2013 I(X; Y) = \\sum_{i,j} p(x_i, y_j) \\log \\frac{2}{p(x_i) + p(y_j)} -  \\sum_{i,j} p(x_i, y_j) \\log \\frac{p(x_i,y_j)}{p(x_i) p(y_j)} = \\sum_{i,j} p(x_i, y_j) \\log \\frac{2 p(x_i) p(y_j)}{p(x_i) + p(y_j)} =  \\sum_{i,j} p("}]}