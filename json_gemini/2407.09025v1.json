{"title": "SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models", "authors": ["Yuzhang Tian", "Jianbo Zhao", "Haoyu Dong", "Junyu Xiong", "Shiyu Xia", "Mengyu Zhou", "Yun Lin", "Jos\u00e9 Cambronero", "Yeye He", "Shi Han", "Dongmei Zhang"], "abstract": "Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SPREADSHEETLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SHEETCOMPRESSOR, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SHEETCOMPRESSOR has an average compression ratio of 25x, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SPREADSHEETLLM is highly effective across a variety of spreadsheet tasks.", "sections": [{"title": "1 Introduction", "content": "Spreadsheets are ubiquitous for data management and extensively utilized within platforms like Microsoft Excel and Google Sheets. Understanding spreadsheet layout and structure is crucial for effective data analysis and intelligent user interaction. Recently, the rapid development of Large Language Models (LLMs) has opened new frontiers in table processing and reasoning. However, spreadsheets pose unique challenges for LLMs due to their expansive grids that usually exceed the token limitations of popular LLMs, as well as their inherent two-dimensional layouts and structures, which are poorly suited to linear and sequential input. Furthermore, LLMs often struggle with spreadsheet-specific features such as cell addresses and formats, complicating their ability to effectively parse and utilize spreadsheet data, as detailed in Appendix A.\nIn this paper, we introduce SPREADSHEETLLM, a pioneering framework to unleash and maximize the potential of LLMs for spreadsheet understanding and reasoning. We initially propose a vanilla encoding method to serialize spreadsheets into sequences, augmenting the Markdown encoding method by including essential cell addresses and (optional) formats. Furthermore, large spreadsheets that exceed the token limits of LLMs not only limit"}, {"title": "2 Related Work", "content": "Spreadsheet Representation Spreadsheet representation involves converting the spreadsheets into specific representations for different models. There are various methods for spreadsheet (table) representation. enhance Mask-RCNN to leverage spatial and visual information in spreadsheets, and explores the usage of LLMs to evaluate image tables, but it doesn't work well for spreadsheet images as input to VLMs. To capture sequential semantics in rows and columns, LSTMs are further adopted in row&column directions. Pre-trained LMs are then proposed to understand spreadsheets Recent studies have explored the efficacy of using Markdown and HTML for table representation. However, they are not well suited to spreadsheets due to their single table input, as experiments show in Appendix B.\nSpreadsheet Understanding While most table LLMs are restricted to single table settings, spreadsheets with multiple tables typically exceed token"}, {"title": "3 Method", "content": "We propose a novel spreadsheet encoding framework in a Markdown-like style as text. To achieve a more compact and efficient representation, we introduce three independent yet combinable modules: structural-anchor-based extraction, inverted-index translation, and data-format-aware aggregation."}, {"title": "3.1 Vanilla Spreadsheet Encoding with Cell Value, Address, and Format", "content": "Due to the absence of standardized practices in spreadsheet encoding for LLMs, we first explore traditional spreadsheet encoding methods. Appendix B presents a comparison of different mainstream tabular data encoding methods, including HTML, XML, and Markdown. Based on the encoding length and performance on spreadsheet understanding tasks, we use a Markdown-like style representation:\n$S = \\{Cell_{i,j}\\}_{i \\in m, j \\in n},$ (1)\n$T = \\text{markdown} \\{ \\text{encode}(Cell_{i,j})\\} : = \\text{``|}Address_{i,j}, Value_{i,j}, Format\\text{|... \\n''},$ (2)\nwhere $S \\in R^{m,n}$ denotes the spreadsheet, $T \\in R^1$ denotes the text representation of a cell, and $i$, $j$, $m$, $n$ respectively represent the row and column index of the cell and the row and column range of S. We also explored the inclusion of cell format information (such as background color, bold font, borders, etc.) into each cell's representation. However, these experiments demonstrated that such detailed encoding adversely affects model performance due to rapid token limit exceedance and LLMs' inadequate capability to process format information effectively, as detailed in Appendix A. We plan to further explore this in future research, focusing on enhancing the model's ability to understand and utilize format and structural cues."}, {"title": "3.2 Structural-anchor-based Extraction", "content": "Large spreadsheets often feature numerous homogeneous rows or columns, which minimally contribute to the understanding of their layout and structure, as depicted in Figure 2 (a). To effectively compress spreadsheets while preserving vital layout and structural information, we propose a novel heuristic-based method, detailed further in Appendix C. This method identifies heterogeneous rows and columns at the edges of table boundaries-termed structural anchors:\n$A = \\{r_p, c_q\\}_{p \\in m, q \\in n },$ (3)\nwhere $r_p = \\{Cell_{i,j}\\}i=p, j \\in n$ and $c_q = \\{Cell_{i,j}\\}i \\in m, j=q$. Using these anchor points, our method discards"}, {"title": "3.3 Inverted-index Translation", "content": "Spreadsheets often contain numerous empty rows, columns, and scattered cells. The standard encoding method, as detailed in Section 3.1, employs a grid-based method that pairs cell addresses with their contents. This approach necessitates recording empty cells to maintain the spreadsheet's two-dimensional structure, which significantly increases token consumption. Furthermore, cells with identical values are encoded repeatedly, further exacerbating token usage.\nTo address these inefficiencies, we propose a two-stage Inverted-index-based Translation method. The first stage involves converting the traditional matrix-style encoding into a dictionary format, where cell values serve as keys indexing the addresses. In the second stage, cells sharing the same value are merged, with empty cells excluded"}, {"title": "3.4 Data-format-aware Aggregation", "content": "In spreadsheets, adjacent cells typically share the same data format. As shown in Figure 2 (3), column C records the sell-in billed revenue for different products. Nonetheless, the concrete numerical values are not essential for understanding the structure and semantics of the spreadsheet (although there might loss of fine-trained details of exact quantities, e.g., \"18,476\" and \"18,674\", this does not impact our comprehension that this column represents revenue). In contrast, the data type is critical for understanding spreadsheets. On one hand, data types represent fundamental semantic properties, such as \"time\" or \"phone number\". It motivates us to implement rules to match the value of the cell to different data types. On the other hand, in contrast to detailed numerical values, identical data types may be compressed through clustering, thereby reducing the number of tokens.\nIn this section, we introduce Data-format-aware Aggregation for further compression and information integration. Specifically, we employ Number Format String (NFS), which is a built-in cell attribute in spreadsheets. NFSs can be extracted by default using tools like ClosedXML or OpenPyXL, used to describe the format of cell data as a string. For instance, the NFS for \"2024.2.14\" is \"yyyy-mm-dd\", indicating a specific date format. However, spreadsheet users do not always explicitly add NFSs to cells, so NFSs are sometimes absent. As a complement, we propose a rule-based recognizer to map a cell value to a specific predefined data type: Year, Integer, Float, Percentage, Scientific notation, Date, Time, Currency, Email, and Others. The first nine types broadly cover approximately 55% of the cells in our dataset derived from real-world corpora."}, {"title": "3.5 Chain of Spreadsheet", "content": "To extend the applicability of SPREADSHEETLLM to a broader range of downstream tasks, we introduce the Chain of Spreadsheet (CoS), which unfolds two stages:\nTable Identification and Boundary Detection Initially, the compressed spreadsheet and the specific task query are input into the LLM. Leveraging the advances in spreadsheet table detection, the model identifies the table that is relevant to the query and determines the precise boundaries of the relevant content. This step ensures that only pertinent data is considered in the subsequent analysis, optimizing the processing efficiency and focus.\nResponse Generation The query and the identified table section are re-input into the LLM. The model then processes this information to generate an accurate response to the query.\nThrough the CoS, SPREADSHEETLLM effectively handles complex spreadsheets by breaking down the process into manageable parts, thus enabling precise and context-aware responses. In this paper, we validate the effect of the Spreadsheet QA task, which is detailed in Section 4.2."}, {"title": "4 Experiments", "content": "In our experimental evaluation, we first verified the effectiveness of our method in spreadsheet understanding. For this purpose, we chose the classic and foundational task of spreadsheet table detection. This task serves as a critical benchmark for assessing the framework's ability to accurately identify and interpret table structures within spreadsheets. Building upon this foundational understanding, we further explored"}, {"title": "4.1 Spreadsheet Table Detection", "content": "We used the dataset introduced by a benchmark dataset of real-world spreadsheets with annotated table boundaries. Due to the complexity and ambiguity of precise address labeling (the Fleiss Kappa on the test set is 0.830), we further implemented the quality improvement pipeline on the test set by five human professions, as detailed n in Appendix E. To this end, we obtained a highly validated test set containing 188 spreadsheets. Based on the token usage of the vanilla encoding method, we divided the test set into four categories: Small, Medium, Large, and Huge, with a partition of 64:32:70:22. More details are shown in Appendix F. We adopted the Error-of-Boundary 0 (EoB-0) metric for evaluation on 188 spreadsheets with 311 tables. EoB-0 requires exact match of the top, left, bottom, and right boundaries."}, {"title": "4.1.2 Experiment Setup", "content": "Baseline & Evaluation Metrics To evaluate the performance of SPREADSHEETLLM, we chose TableSense-CNN as the baseline due to its previously demonstrated effectiveness in spreadsheet table detection task. We employed the F1 Score as the primary metric to evaluate and compare the performance of different models, as it balances precision and recall, providing a holistic view of model accuracy.\nModel Selection The experiments included both closed-source and open-source models. From the closed-source spectrum, we selected two versions of OpenAI's models: GPT4 and GPT3.5, which are known for their advanced language understanding capabilities. On the open-source side, we chose Llama2, Llama3, Phi3, and Mistral-v2. The specific configurations are detailed in Appendix G."}, {"title": "4.2 Spreadsheet QA", "content": "Existing datasets for the Table QA task focus solely on single-table scenarios, leaving a notable gap in"}, {"title": "4.2.2 Experiment Setup", "content": "Baseline & Evaluation Metrics Given that LLMs have not yet been systematically applied to Spreadsheet QA tasks, we have selected TAPEX and Binder which are established baselines in the Table QA domain, for comparative evaluation. Since TAPEX and Binder are designed primarily for single-table data, we adapted them for our multi-table context. Initially, our fine-tuned model identifies table regions relevant to each question. These regions are then formatted and fed into the baseline models. In cases where the input exceeds the token limitations of the baseline models, truncation is employed. The accuracy of the answers is assessed based on the correctness of the cell addresses and cell combinations/calculations provided in the answers.\nModel Selection Our experiments were conducted using the GPT4 model, leveraging its advanced capabilities in language understanding and reasoning. Details on parameters and configurations used are documented in Appendix G."}, {"title": "4.2.3 Experiment Procedure", "content": "In this section, we employed the model fine-tuned on the spreadsheet table detection task to conduct QA experiments. The procedure followed the CoS described in section 3.5. Particularly, for instances where the related table was still too large to process effectively, we applied further compression techniques. In cases where tables were exceptionally large and defy effective compression, we utilized a table-splitting algorithm designed to recognize"}, {"title": "5 Results", "content": ""}, {"title": "5.1 Compression Ratio", "content": "The effectiveness of our encoding process in reducing the size of spreadsheet data is quantitatively assessed using the compression ratio, which is defined by the formula:\n$r = n/n',$ (9)\nOur encoding methodology has significantly optimized token usage within spreadsheets. In our test set, it achieved an impressive 25\u00d7 compression ratio, substantially reducing the computational load for processing large datasets. The specific compression ratios achieved by various module combinations within SHEETCOMPRESSOR are detailed in Table 1. These results highlight the efficacy of our approach across different configurations, demonstrating its robustness and adaptability in handling diverse spreadsheet structures."}, {"title": "5.2 Spreadsheet Table Detection", "content": "Table 2 illustrates the performance differences among various models and methods on spreadsheet table detection task, and the detailed case study can refer to Appendix K."}, {"title": "5.3 Spreadsheet QA", "content": ""}, {"title": "6 Conclusion", "content": "In this paper, we proposed the SPREADSHEETLLM, a novel framework representing a significant advancement in the processing and understanding of spreadsheet data by leveraging the capabilities of LLMs. Through a novel encoding method, SHEETCOMPRESSOR, this framework effectively addresses the challenges posed by the size, diversity, and complexity inherent in spreadsheets. It achieves a substantial reduction in token usage and computational costs, enabling practical applications on large datasets. The fine-tuning of various cutting-edge LLMs further enhances the performance of spreadsheet understanding. Moreover, Chain of Spreadsheet, the framework's extension to spreadsheet downstream tasks illustrates its broad applicability and potential to transform spreadsheet data management and analysis, paving the way for more intelligent and efficient user interactions."}, {"title": "Limitations", "content": "While our SPREADSHEETLLM frameworks have markedly advanced how LLMs interpret and utilize spreadsheets, they also illuminate areas ripe for further research and development. Currently, our methods do not yet harness spreadsheet format details such as background color and borders, because they take too many tokens. However, these elements often contain valuable contextual and visual cues that could further refine our understanding and processing of spreadsheet data. Additionally, while SHEETCOMPRESSOR effectively aggregates data regions, it does not currently employ a sophisticated semantic-based compression method for cells containing natural language. For example, categorizing terms like \"China,\" \"America,\" and \"France\" under a unified label such as \"Country\" could not only increase the compression ratio but also deepen the semantic understanding of the data by LLMs. Exploring these advanced semantic compression techniques will be a key focus of our ongoing efforts to enhance the capabilities of SPREADSHEETLLM."}, {"title": "Ethics Statement", "content": "All data were collected, analyzed, and reported without any bias or influence from external sources. The privacy and confidentiality of the participants were strictly maintained throughout the research process. No personal identifiers were used in the analysis or reporting of the data to ensure anonymity. At the same time, data standard personnel were paid according to the highest local standard, and their daily working hours were strictly limited to no more than 8 hours to protect their legitimate rights and interests. We acknowledge the contributions of all individuals and institutions involved in this study and are committed to sharing our findings and methodologies transparently to facilitate further research and knowledge advancement in the field."}]}