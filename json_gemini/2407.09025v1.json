{"title": "SPREADSHEETLLM: Encoding Spreadsheets for Large Language Models", "authors": ["Yuzhang Tian", "Jianbo Zhao", "Haoyu Dong", "Junyu Xiong", "Shiyu Xia", "Mengyu Zhou", "Yun Lin", "Jos\u00e9 Cambronero", "Yeye He", "Shi Han", "Dongmei Zhang"], "abstract": "Spreadsheets are characterized by their extensive two-dimensional grids, flexible layouts, and varied formatting options, which pose significant challenges for large language models (LLMs). In response, we introduce SPREADSHEETLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SHEETCOMPRESSOR, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SHEETCOMPRESSOR has an average compression ratio of 25x, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SPREADSHEETLLM is highly effective across a variety of spreadsheet tasks.", "sections": [{"title": "1 Introduction", "content": "Spreadsheets are ubiquitous for data management and extensively utilized within platforms like Microsoft Excel and Google Sheets. Understanding spreadsheet layout and structure, a longstanding challenge for traditional models, is crucial for effective data analysis and intelligent user interaction. Recently, the rapid development of Large Language Models (LLMs) has opened new frontiers in table processing and reasoning. However, spreadsheets pose unique challenges for LLMs due to their expansive grids that usually exceed the token limitations of popular LLMs, as well as their inherent two-dimensional layouts and structures, which are poorly suited to linear and sequential input. Furthermore, LLMs often struggle with spreadsheet-specific features such as cell addresses and formats, complicating their ability to effectively parse and utilize spreadsheet data, as detailed in Appendix A.\nIn this paper, we introduce SPREADSHEETLLM, a pioneering framework to unleash and maximize the potential of LLMs for spreadsheet understanding and reasoning. We initially propose a vanilla encoding method to serialize spreadsheets into sequences, augmenting the Markdown encoding method by including essential cell addresses and (optional) formats. Furthermore, large spreadsheets that exceed the token limits of LLMs not only limit their processing but also, as observed in prior studies, degrade accuracy performance as the size increases. To address this challenge, we propose SHEETCOMPRESSOR, featuring a novel encoding framework comprising three portable modules:\n1) Structural Anchors for Efficient Layout Understanding: Observations indicate that large spreadsheets often contain numerous homogeneous rows or columns, which contribute minimally to understanding the layout and structure (see left panel in Figure 2 (a)). To address this, we identify structural anchors heterogeneous rows and columns at possible table boundaries that offer substantial layout insights, as depicted in Figure 2 (b). Then we remove distant, homogeneous rows and columns, producing a condensed \"skeleton\" version of the spreadsheet, as illustrated in Figure 2 (c).\n2) Inverted-Index Translation for Token Efficiency: The vanilla encoding method becomes token-consuming when handling spreadsheets with numerous empty cells and repetitive values, as shown in Figure 2 (c). To improve efficiency, we depart from traditional row-by-row and column-by-column serialization and employ a lossless inverted-index translation in JSON format. This method creates a dictionary that indexes non-empty cell texts and merges addresses with identical text, optimizing token usage while preserving data integrity.\n3) Data Format Aggregation for Numerical Cells: Adjacent numerical cells often share similar number formats. Recognizing that exact numerical values are less crucial for grasping spreadsheet structure, we extract number format strings and data types from these cells. Then adjacent cells with the same formats or types are clustered together. This method is visualized in the right example of Figure 2, where rectangular regions are represented by uniform format strings and data types, streamlining the understanding of numerical data distribution without excessive token expenditure.\nWe conducted a comprehensive evaluation of our method on a variety of LLMs. Our experiments show that SHEETCOMPRESSOR significantly reduces token usage for spreadsheet encoding by 96%. Moreover, SPREADSHEETLLM has shown exceptional performance in spreadsheet table detection, the foundational task of spreadsheet understanding, surpassing the previous SOTA method by 12.3%. We also applied SPREADSHEETLLM to a representative spreadsheet QA task. Inspired by the Chain of Thought methodology, we propose Chain of Spreadsheet (CoS) to decompose spreadsheet reasoning into a table detection-match-reasoning pipeline. It significantly outperformed existing SOTA methods for table QA. Our primary contributions are summarized as follows:\n\u2022 We propose SPREADSHEETLLM, the first work that substantially leverage LLMs for understanding and analyzing spreadsheet data. To address challenges in scale, diversity, and complexity of spreadsheets, we propose SHEETCOMPRESSOR, an innovative encoding framework to compress spreadsheets for LLMs with efficient encoding.\n\u2022 We fine-tune a variety of cutting-edge LLMs to achieve optimal performance on spreadsheet table detection, and demonstrate the high effectiveness of SPREADSHEETLLM in accurately understanding complex spreadsheet layouts and structures.\n\u2022 In order to extend the horizontal capabilities of SPREADSHEETLLM to a wide range of downstream tasks, we propose CoS and verify it on Spreadsheet QA, highlighting its potential for intelligent user interaction."}, {"title": "2 Related Work", "content": "Spreadsheet Representation Spreadsheet representation involves converting the spreadsheets into specific representations for different models. There are various methods for spreadsheet (table) representation. enhance Mask-RCNN to leverage spatial and visual information in spreadsheets, and explores the usage of LLMs to evaluate image tables, but it doesn't work well for spreadsheet images as input to VLMs. To capture sequential semantics in rows and columns, LSTMs are further adopted in row&column directions. Pre-trained LMs are then proposed to understand spreadsheets . Recent studies have explored the efficacy of using Markdown and HTML for table representation. However, they are not well suited to spreadsheets due to their single table input, as experiments show in Appendix B.\nSpreadsheet Understanding While most table LLMs are restricted to single table settings, spreadsheets with multiple tables typically exceed token"}, {"title": "3 Method", "content": "We propose a novel spreadsheet encoding framework in a Markdown-like style as text. To achieve a more compact and efficient representation, we introduce three independent yet combinable modules: structural-anchor-based extraction, inverted-index translation, and data-format-aware aggregation."}, {"title": "3.1 Vanilla Spreadsheet Encoding with Cell Value, Address, and Format", "content": "Due to the absence of standardized practices in spreadsheet encoding for LLMs, we first explore traditional spreadsheet encoding methods. Appendix B presents a comparison of different mainstream tabular data encoding methods, including HTML, XML, and Markdown. Based on the encoding length and performance on spreadsheet understanding tasks, we use a Markdown-like style representation:\n\\(S = {Cell_{i,j}}_{i \\in m, j \\in n},\\)\n\\(T = markdown {encode (Cell_{i,j})}\\)\n\\(:=\u201c|Address_{i,j}, Value_{i,j}, Format|... \\n\",\\)\nwhere \\(S \\in R^{m,n}\\) denotes the spreadsheet, \\(T \\in R^{1}\\) denotes the text representation of a cell, and i, j, m, n, respectively represent the row and column index of the cell and the row and column range of S. We also explored the inclusion of cell format information (such as background color, bold font, borders, etc.) into each cell's representation. However, these experiments demonstrated that such detailed encoding adversely affects model performance due to rapid token limit exceedance and LLMs' inadequate capability to process format information effectively, as detailed in Appendix A. We plan to further explore this in future research, focusing on enhancing the model's ability to understand and utilize format and structural cues.\""}, {"title": "3.2 Structural-anchor-based Extraction", "content": "Large spreadsheets often feature numerous homogeneous rows or columns, which minimally contribute to the understanding of their layout and structure, as depicted in Figure 2 (a). To effectively compress spreadsheets while preserving vital layout and structural information, we propose a novel heuristic-based method, detailed further in Appendix C. This method identifies heterogeneous rows and columns at the edges of table boundaries-termed structural anchors:\n\\(A = {r_p, c_q}_{p \\in m, q \\in n}\\)\nwhere \\(r_p = {Cell_{i,j}}_{i=p, j \\in n}\\) and \\(c_q = {Cell_{i,j}}_{i \\in m, j=q}\\). Using these anchor points, our method discards rows and columns that are located more than k units away from any anchor point, because they rarely serve as table boundaries. The parameter k serves as a threshold to control the scope of neighborhood retention, effectively eliminating areas predominantly filled with homogeneous data that do not contribute to an understanding of the spreadsheet's layout and structure. We explored the effects of different k values in an ablation study, as detailed in Appendix D.1.\nThe extracted rows and columns can be expressed as:\n\\(A_+ = {r_{p+}, c_{q+} }_{p+ \\in m, q+\\in r}\\)\nwhere the extracted \"skeletons\" are defined as: \\(r_{p+} = {Cell_{i,j}}\\|i-p\\|<k, j \\in n\\) and \\(c_{q+} = {Cell_{i,j}}_{i \\in m, |j-q| \\leq k}\\). Then we obtain the extracted compact spreadsheet:\n\\(S_c = extract(S) = address_map(r_{p+} \\cap c_{q+}).\\)\nBased on the compressed spreadsheet Sc, we can obtain extremely shorter text representation Tc. Furthermore, after extraction, we perform a coordinate re-mapping to ensure continuity in cell coordinates, preserving the integrity of data relationships within the compressed spreadsheet. This re-mapping is critical for maintaining the accuracy of prediction results, ensuring that analyses remain consistent even after compression. This method filters out 75% spreadsheet content but preserves 97% rows and columns at the edges of table boundaries."}, {"title": "3.3 Inverted-index Translation", "content": "Spreadsheets often contain numerous empty rows, columns, and scattered cells. The standard encoding method, as detailed in Section 3.1, employs a grid-based method that pairs cell addresses with their contents. This approach necessitates recording empty cells to maintain the spreadsheet's two-dimensional structure, which significantly increases token consumption. Furthermore, cells with identical values are encoded repeatedly, further exacerbating token usage.\nTo address these inefficiencies, we propose a two-stage Inverted-index-based Translation method. The first stage involves converting the traditional matrix-style encoding into a dictionary format, where cell values serve as keys indexing the addresses. In the second stage, cells sharing the same value are merged, with empty cells excluded and cell addresses noted as ranges. This method effectively reduces the number of required tokens by eliminating redundancies and simplifying the representation of repeated and empty cells. The translation process is represented mathematically as follows:\n\\(T_t = invert(T)\\)\n\\(:= {Value: Address or Address_Region, ...}.\\)\nInverted-index Translation is a lossless compression method general for all spreadsheet understanding tasks, and it remarkably increases SHEETCOMPRESSOR's compression ratio from 4.41 to 14.91. More details can be found in Table 1."}, {"title": "3.4 Data-format-aware Aggregation", "content": "In spreadsheets, adjacent cells typically share the same data format. As shown in Figure 2 (3), column C records the sell-in billed revenue for different products. Nonetheless, the concrete numerical values are not essential for understanding the structure and semantics of the spreadsheet (although there might loss of fine-trained details of exact quantities, e.g., \"18,476\" and \"18,674\", this does not impact our comprehension that this column represents revenue). In contrast, the data type is critical for understanding spreadsheets. On one hand, data types represent fundamental semantic properties, such as \"time\" or \"phone number\". It motivates us to implement rules to match the value of the cell to different data types. On the other hand, in contrast to detailed numerical values, identical data types may be compressed through clustering, thereby reducing the number of tokens.\nIn this section, we introduce Data-format-aware Aggregation for further compression and information integration. Specifically, we employ Number Format String (NFS), which is a built-in cell attribute in spreadsheets. NFSs can be extracted by default using tools like ClosedXML or OpenPyXL, used to describe the format of cell data as a string. For instance, the NFS for \"2024.2.14\" is \"yyyy-mm-dd\", indicating a specific date format. However, spreadsheet users do not always explicitly add NFSs to cells, so NFSs are sometimes absent. As a complement, we propose a rule-based recognizer to map a cell value to a specific predefined data type: Year, Integer, Float, Percentage, Scientific notation, Date, Time, Currency, Email, and Others. The first nine types broadly cover approximately 55% of the cells in our dataset derived from real-world corpora.\nFinally, based on the NFSs and data type, the aggregator aggregates the cells by Algorithm 1. This process can be represented as follows:\n\\(NFSs = nfs({Cell_{i,j}}_{i \\in m, j \\in n}),\\)\n\\(T_a = aggregator({Cell_{i,j}}_{i \\in m, j \\in n}, NFSs, R),\\)\nwhere R denotes the predefined rules as detailed above. In this way, we further reduce the number of tokens. The compression ratio of the data regions also increases from 14.91 to 24.79. More detailed compression effects of different modules are displayed in Table 1."}, {"title": "3.5 Chain of Spreadsheet", "content": "To extend the applicability of SPREADSHEETLLM to a broader range of downstream tasks, we introduce the Chain of Spreadsheet (CoS), which unfolds two stages:\nTable Identification and Boundary Detection Initially, the compressed spreadsheet and the specific task query are input into the LLM. Leveraging the advances in spreadsheet table detection, the model identifies the table that is relevant to the query and determines the precise boundaries of the relevant content. This step ensures that only pertinent data is considered in the subsequent analysis, optimizing the processing efficiency and focus.\nResponse Generation The query and the identified table section are re-input into the LLM. The model then processes this information to generate an accurate response to the query.\nThrough the CoS, SPREADSHEETLLM effectively handles complex spreadsheets by breaking down the process into manageable parts, thus enabling precise and context-aware responses. In this paper, we validate the effect of the Spreadsheet QA task, which is detailed in Section 4.2."}, {"title": "4 Experiments", "content": "In our experimental evaluation, we first verified the effectiveness of our method in spreadsheet understanding. For this purpose, we chose the classic and foundational task of spreadsheet table detection. This task serves as a critical benchmark for assessing the framework's ability to accurately identify and interpret table structures within spreadsheets. Building upon this foundational understanding, we further explored"}, {"title": "4.1 Spreadsheet Table Detection", "content": ""}, {"title": "4.1.1 Dataset", "content": "We used the dataset introduced by , a benchmark dataset of real-world spreadsheets with annotated table boundaries. Due to the complexity and ambiguity of precise address labeling (the Fleiss Kappa on the test set is 0.830), we further implemented the quality improvement pipeline on the test set by five human professions, as detailed n in Appendix E. To this end, we obtained a highly validated test set containing 188 spreadsheets. Based on the token usage of the vanilla encoding method, we divided the test set into four categories: Small, Medium, Large, and Huge, with a partition of 64:32:70:22. More details are shown in Appendix F. We adopted the Error-of-Boundary 0 (EoB-0) metric for evaluation on 188 spreadsheets with 311 tables. EoB-0 requires exact match of the top, left, bottom, and right boundaries."}, {"title": "4.1.2 Experiment Setup", "content": "Baseline & Evaluation Metrics To evaluate the performance of SPREADSHEETLLM, we chose TableSense-CNN as the baseline due to its previously demonstrated effectiveness in spreadsheet table detection task. We employed the F1 Score as the primary metric to evaluate and compare the performance of different models, as it balances precision and recall, providing a holistic view of model accuracy.\nModel Selection The experiments included both closed-source and open-source models. From the closed-source spectrum, we selected two versions of OpenAI's models: GPT4 and GPT3.5, which are known for their advanced language understanding capabilities. On the open-source side, we chose Llama2, Llama3, Phi3, and Mistral-v2. The specific configurations are detailed in Appendix G."}, {"title": "4.2 Spreadsheet QA", "content": ""}, {"title": "4.2.1 Dataset", "content": "Existing datasets for the Table QA task focus solely on single-table scenarios, leaving a notable gap in"}, {"title": "4.2.2 Experiment Setup", "content": "Baseline & Evaluation Metrics Given that LLMs have not yet been systematically applied to Spreadsheet QA tasks, we have selected TAPEX and Binder, which are established baselines in the Table QA domain, for comparative evaluation. Since TAPEX and Binder are designed primarily for single-table data, we adapted them for our multi-table context. Initially, our fine-tuned model identifies table regions relevant to each question. These regions are then formatted and fed into the baseline models. In cases where the input exceeds the token limitations of the baseline models, truncation is employed. The accuracy of the answers is assessed based on the correctness of the cell addresses and cell combinations/calculations provided in the answers.\nModel Selection Our experiments were conducted using the GPT4 model, leveraging its advanced capabilities in language understanding and reasoning. Details on parameters and configurations used are documented in Appendix G."}, {"title": "4.2.3 Experiment Procedure", "content": "In this section, we employed the model fine-tuned on the spreadsheet table detection task to conduct QA experiments. The procedure followed the CoS described in section 3.5. Particularly, for instances where the related table was still too large to process effectively, we applied further compression techniques. In cases where tables were exceptionally large and defy effective compression, we utilized a table-splitting algorithm designed to recognize headers and perform strategic concatenation, ensuring that each segment of the split table retains as much contextual integrity as possible. The specifics of this algorithm are detailed in Appendix M.2."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Compression Ratio", "content": "The effectiveness of our encoding process in reducing the size of spreadsheet data is quantitatively assessed using the compression ratio, which is defined by the formula:\n\\(r=\\frac{n}{n'},\\)\nOur encoding methodology has significantly optimized token usage within spreadsheets. In our test set, it achieved an impressive 25\u00d7 compression ratio, substantially reducing the computational load for processing large datasets. The specific compression ratios achieved by various module combinations within SHEETCOMPRESSOR are detailed in Table 1. These results highlight the efficacy of our approach across different configurations, demonstrating its robustness and adaptability in handling diverse spreadsheet structures."}, {"title": "5.2 Spreadsheet Table Detection", "content": ""}, {"title": "5.2.1 Main Results", "content": "Table 2 illustrates the performance differences among various models and methods on spreadsheet table detection task, and the detailed case study can refer to Appendix K.\n1) Enhanced Performance with various LLMs: The fine-tuned GPT4 model achieved the F1 score of approximately 76% across all datasets, while our encoding method without aggregation achieved the F1 score of approximately 79% across all datasets. This marked a 27% improvement over the same model fine-tuned on original data, a 13% increase over TableSense-CNN, and established a new SOTA. The entire encoding method slightly reduced the F1 score within a tolerable range, but achieved good compression results, as shown in Table 1. We also evaluated our encoding method on a series of open-source models. Notably, Llama3 and Mistral-v2 achieved an F1 score of approximately 72%, just 6 percentage points below the SOTA. The improvements due to our compression method were substantial, with increases of 25% for Llama3, 36% for Phi3, 38% for Llama2, and 18% for Mistral-v2. These results underscored the significant enhancement performance attributable to our encoding method.\n2) Benefits for Larger Spreadsheets: Our compression method significantly boosted performance on larger spreadsheets, where the challenges were most pronounced due to model token limits. The improvements in F1 scores were particularly notable on huge spreadsheets (75% over GPT4, 19% over TableSense-CNN), large spreadsheets (45% and 17%), medium (13% and 5%), and small (8%) spreadsheets. This demonstrated our method's effectiveness in enabling LLMs to process a broader range of spreadsheet sizes efficiently.\n3) Improvements in In-Context Learning: Compact encoding also significantly enhanced ICL capabilities. For instance, the performance of GPT4 on all data improved by nearly 26%, demonstrating the method's effectiveness beyond fine-"}, {"title": "5.2.2 Ablation Experiment Results", "content": "Table 3 presents the results of ablation experiments for different modules. The removal of the extraction module led to significantly lower F1 scores, underscoring its critical role in capturing and retaining key structural information. As highlighted in Table 1, this module also achieved the most significant token reduction, confirming its effectiveness. After removing the aggregation module, the F1 score slightly increased. This observation might be attributed to the NFS being more abstract than straightforward numerical representations, which can challenge an LLMs' ability to interpret them effectively. Despite this, the NFS method offered a significantly high compression rate, enhancing its potential for practical applications and cost control."}, {"title": "5.3 Spreadsheet QA", "content": "Table 4 shows the performance of various models on Spreadsheet QA tasks. We can draw the"}, {"title": "6 Conclusion", "content": "In this paper, we proposed the SPREADSHEETLLM, a novel framework representing a significant advancement in the processing and understanding of spreadsheet data by leveraging the capabilities of LLMs. Through a novel encoding method, SHEETCOMPRESSOR, this framework effectively addresses the challenges posed by the size, diversity, and complexity inherent in spreadsheets. It achieves a substantial reduction in token usage and computational costs, enabling practical applications on large datasets. The fine-tuning of various cutting-edge LLMs further enhances the performance of spreadsheet understanding. Moreover, Chain of Spreadsheet, the framework's extension to spreadsheet downstream tasks illustrates its broad applicability and potential to transform spreadsheet data management and analysis, paving the way for more intelligent and efficient user interactions."}, {"title": "Limitations", "content": "While our SPREADSHEETLLM frameworks have markedly advanced how LLMs interpret and utilize spreadsheets, they also illuminate areas ripe for further research and development. Currently, our methods do not yet harness spreadsheet format details such as background color and borders, because they take too many tokens. However, these elements often contain valuable contextual and visual cues that could further refine our understanding and processing of spreadsheet data. Additionally, while SHEETCOMPRESSOR effectively aggregates data regions, it does not currently employ a sophisticated semantic-based compression method for cells containing natural language. For example, categorizing terms like \"China,\" \"America,\" and \"France\" under a unified label such as \"Country\" could not only increase the compression ratio but also deepen the semantic understanding of the data by LLMs. Exploring these advanced semantic compression techniques will be a key focus of our ongoing efforts to enhance the capabilities of SPREADSHEETLLM."}, {"title": "Ethics Statement", "content": "All data were collected, analyzed, and reported without any bias or influence from external sources. The privacy and confidentiality of the participants were strictly maintained throughout the research process. No personal identifiers were used in the analysis or reporting of the data to ensure anonymity. At the same time, data standard personnel were paid according to the highest local standard, and their daily working hours were strictly limited to no more than 8 hours to protect their legitimate rights and interests. We acknowledge the contributions of all individuals and institutions involved in this study and are committed to sharing our findings and methodologies transparently to facilitate further research and knowledge advancement in the field."}, {"title": "A GPT4 Struggles to Understand Spreadsheets", "content": "The  and  show how GPT4 struggles to understand spreadsheets. We also validated the effect of cell format on the vanilla encoding method on the spreadsheet table detection task. As shown in Table 5, the results indicate that in ICL, the inclusion of format marginally improves the model's performance on small datasets but results in poorer performance on larger datasets. For the fine-tuned model, the inclusion of format information leads to a significant reduction in the overall F1 score. This decline is attributed to the introduction of additional tokens, which causes some data to exceed the model's token limits. Additionally, LLMs are not yet adept at understanding format information."}, {"title": "B Traditional Encoding Methods for Spreadsheets", "content": "In our study, we explored traditional encoding methods\u2014Markdown, XML, and HTML\u2014to represent spreadsheet data. Figure 5 illustrates the comparative analysis of these methods. XML and HTML encoding, while widely used, tend to result in high token consumption due to the extensive use of repeated label tags necessary for representing the data structure. This approach markedly increases the volume of data processed.\nConversely, the Markdown method, although more token-efficient, has its limitations. One significant drawback is the lack of explicit cell address information, which frequently leads to errors when indexing specific cell locations. Additionally, Markdown's rigid structure rules complicate the accurate representation of merged cells, a common feature in complex spreadsheets that is crucial for preserving the integrity of data relationships.\nTo quantitatively assess these methods, we conducted ICL experiments using the GPT-4 model on spreadsheet detection tasks. The results, detailed in Table 6, confirmed that while the Markdown method outperformed XML and HTML in terms of lower token usage, it still fell short in addressing the needs of spreadsheet encoding effectively."}, {"title": "C Lightweight Heuristics for Structural-anchor Proposal", "content": "Initially, this method enumerates bounding lines by finding discrepancies in neighboring rows and columns based on differences in cell values, merged cells, borders, and fill color. In other words, it enumerates rows and columns with imbalances (text, merge, border, color, font, etc.). Rows and columns without significant discrepancies are usually canonical data rows or columns that contribute trivially to the layout understanding of a spreadsheet. Subsequently, it composes all possible candidate boundaries using any two rows and any two columns as top/bottom/left/right edges. In the third step, heuristics are applied to filter unreasonable boundary candidates by judging the integrity within each candidate boundary. For example, the proportion of numbers and characters in each row and"}, {"title": "D Ablation Experiment Results of Spreadsheet Table Detection", "content": ""}, {"title": "D.1 Results on Structure-anchor Threshold", "content": "Table 7 details the ablation study concerning the number of rows and columns retained near candidate boundaries. Optimal results were observed when four rows/columns were preserved, yielding the highest F1 score across all datasets. This outcome is likely due to a balance between preserving essential boundary information and maintaining a feasible compression ratio. Retaining fewer rows/columns might omit critical boundaries, reducing Recall, while preserving more rows/columns diminishes the compression ratio, potentially exceeding the model's token limits. For smaller data, results indicate a positive cor-"}, {"title": "D.2 Results of Spreadsheet Table Detection on ICL", "content": "We conducted experiments on the GPT4, \"GPT4-0125-preview\" version. As shown in Table 8, the results are consistent with the conclusions we draw from our fine-tuned experiments."}, {"title": "E Spreadsheet Table Detection Test Dataset Quality Improvement Pipeline", "content": "The quality improvement pipeline on the test set consists of the following steps: (1) excluding those spreadsheets where at least one cell contains languages beyond English; (2) removing spreadsheets in the test set that lie in the same workbook as at least one spreadsheet in the training set, because spreadsheets in the same workbook, though different, are often similar; (3) annotating all spreadsheets in three types: type 1 means certain for one label; type 2 means multiple labels are reasonable; type 3 means not certain. We employ five well-educated annotators from top universities with majors in computer science to undertake this quality improvement. For each spreadsheet in the test set, we aggregate the annotations from all five annotators and preserve multiple reasonable labeling results for type 2 spreadsheets.\nAs a result, we obtained a well-annotated dataset with 167 spreadsheets containing 268 tables for type 1, 21 spreadsheets with 43 tables for type 2,"}, {"title": "F Spreadsheet Table Detection Test Dataset Partition", "content": "From the spreadsheet raw file, we can extract various features, including cell address, value, format (background color, bold, borders, etc.), and more. We transformed these features into the markdown-like style in Section3.1. Then, based on the number of tokens after encoding and the length of the context window of the test model, we divided them into four categories: small (number of tokens less than 4k), medium (4-8k), large (8-32k), and huge (greater than 32k). The following is an example of data in Markdown with format information."}, {"title": "G Experiment Setup", "content": "Open-source model using Deepspeed for distributed training on a workstation with 8 A100 GPUs by LoRA.\nLlama2:meta-Llama/Llama-2-7b-chat-hf;\nLlama3:meta-Llama/Meta-Llama-3-8B-Instruct;\nMistral-v2:mistralai/Mistral-v2-7B-Instruct-v0.2;\nPhi3:microsoft/Phi-3-mini-128k-instruct;\nThe parameters of open-source model fine-tuning: cutoff len=5800; learning rate=5e-05; num train epochs=15.0; train batch size=5; gradient accumulation steps=8; lr scheduler type is cosine; max grad norm=1.0; warmup steps=0; optim is AdamW; precision is fp16; lora rank=32; lora alpha=64; lora dropout=0.01; val size=0.0008; eval steps=50; eval batch size=5\nThe parameters of GPT4/3.5 model fine-tuning: We have attached the fine-tuned file and parameters in the Supplementary materials.\nThe parameters of GPT4/3.5 model inference: temperature=0, max tokens=300, top p=0.95, frequency penalty=0, presence penalty=0, stop=None, and the rest are default settings."}, {"title": "H Spreadsheet QA Test Dataset", "content": "Overall Description The dataset of 64 spreadsheets includes 9 single table spreadsheets, 35 double table spreadsheets, 11 spreadsheets containing three tables, and 9 spreadsheets containing four or more tables. Among them, 15 spreadsheets contain fewer than 4k tokens, 20 contain between 4k and 8k, 22 contain between 8k and 32k, and 7 contain more than 32k.\nDetails of the Dataset Collection We selected English-language spreadsheets and invited five well-educated professional annotators to annotate the data. During selection, spreadsheets containing non-ASCII characters or lacking necessary semantic comprehension information were excluded. We ensured that the questions could be answered with relative certainty using the information provided in the tables, minimizing the potential confusion or ambiguity. To further validate the quality of the dataset, we invited two additional annotators to perform cross-verification after the initial question-answer labeling process, ensuring the correctness and rationality of the answers. It shows an answer accuracy of 0.846 in Fleiss Kappa, indicating almost perfect agreement."}, {"title": "I Cost calculation", "content": "We use the ICL price of GPT4 due to the absence of fine-tuned GPT4's price. We neglect the output sequence since it is much shorter than the input sequence in tasks like spreadsheet boundary detection and QA. The average cost of processing a spreadsheet in our test set has decreased to $0.000157 (62000/198 * 0.0005 / 1000) from $0.00391 (1548000/198 * 0.0005 / 1000) for the GPT3.5 turbo, and to $0.00939 (62000/198 * 0.03 / 1000) from $0.235 (1548000/198 * 0.03 / 1000) for the GPT4, saving an impressive 96.0% in costs. The cost reduction similarly applies to all LLMs we used."}, {"title": "JOther Experimental Results", "content": ""}, {"title": "J.1 Compression Results", "content": "Table 9 shows the compression ratio of each stage in our method relative to the previous stage.\nTable 10 shows the total compression ratio of train and valid datasets."}, {"title": "J.2 The ICL results of open-source models on spreadsheet table detection.", "content": "Table 11 shows the ICL experiments' F1 score of open-source models on the spreadsheet table detection task. In this experimental setting, the open-source model performs far worse than the closed-source model."}, {"title": "J.3 Spreadsheet QA Ablation Experiment", "content": "Table 12 assesses the impact of removing individual modules on the QA performance. It details both the overall accuracy and the accuracy of identifying question-related regions during the CoT process. The removal of any module generally leads to a decrease in both metrics, with the most significant"}, {"title": "K Case Study", "content": ""}, {"title": "K.1 Comparison of results before and after structural-anchor-based extraction", "content": "The case described in . 7 illustrates the results of GPT4-FT before and after structural-anchor-based extraction. Specifically, before structural-anchor-based extraction, most of the content in the spreadsheet is concentrated in the first two rows and the three columns on the left and right, leaving the middle largely empty. This led GPT4 to incorrectly predict the presence of two tables, \"B2:AK14\" and \"B19:F25.\" However, after apply-"}, {"title": "K.2 Comparison of results before and after inverted-index translation", "content": "The case described in Figure. 8 demonstrates the results of GPT4-FT before and after inverted-index translation. Specifically, before the inverted-index translation, the spreadsheet contained two tables with identical column headers placed closely together, causing GPT4 to mistakenly predict them as one large table, \"B1:D14.\" However, after inverted-index translation, GPT4 was able to aggregate cells with shared values, thereby recognizing semantic relationships between non-adjacent rows and columns. This enabled it to correctly identify the two separate tables in the spreadsheet, \"B1:D10\" and \"B11:D14\".\nThis case indicates that inverted-index translation, by aggregating cells with shared values, not only reduces token redundancy to some extent but also leverages the model's robust understanding of semantic relationships."}, {"title": "K.3 Comparison of results before and after data-format-aware cell aggregation", "content": "The case presented in Figure. 9 showcases the results of GPT4-FT before and after data-aware cell aggregation. Specifically, before data-aware cell aggregation, the spreadsheet contained two columns with values of the same data type, occupying a large number of tokens. The first column increased incrementally by date, while the second column increased incrementally by value. After data-aware cell aggregation, the dates in the first column were replaced with the format string \"yyyy/mm/dd\" and their addresses were aggregated. Similarly, numerical values were handled with a \"FloatNum\" format. This method allowed the model to predict the table range correctly as \"B1:C38,\" both before and after processing, indicating that this approach significantly reduces the token count while preserving the semantic information of the spreadsheet data."}, {"title": "K.4 Comparison of SPREADSHEETLLM and TableSense-CNN", "content": "As shown in Figure 10, the output of TableSense-CNN is [A1:G44,K5:M14,K16:M38,Q20:W29], while the output of SPREADSHEETLLM is [A1:G44,K5:R14,K16:M38,Q20:W29]. SPREADSHEETLLM succeeds in adding the region \"R5:R14\" to the table2. Though it is spatially distant from the table on the left, SPREADSHEETLLM can extract the connections from cells' semantic and structural relationship, which demonstrates its powerful reasoning ability."}, {"title": "L Prompt Template", "content": "In this section, we present the prompt templates for the Spreadsheet Table Detection and Spreadsheet QA tasks."}, {"title": "L.1 Vanilla Prompt Template for Spreadsheet Table Detection", "content": "A Vanilla Prompt Template for Spreadsheet Table Detection:\nINSTRUCTION:\nGiven an input that is a string denoting data of cells in a spreadsheet. The input spreadsheet includes many pairs, and each pair consists of a cell address and the text in that cell with a ',' in between, like 'A1,Year'. Cells are separated by '|' like 'A1,Year|A2,Profit'. The text can be empty so the cell data is like 'A1, A2,Profit'. The cells are organized in row-major order. Now you should tell me the range of the table in a format like A2:D5, and the range of the table should only CONTAIN HEADER REGION and the data region, DON'T include the title or comments. Note that there can be more than one table in the string, so you should return all the RANGE, LIKE ['range': 'A1:F9', 'range': 'A12:F18']. DON'T ADD OTHER WORDS OR EXPLANATION."}, {"title": "L.2 Prompt Template for Spreadsheet Table Detection", "content": "SPREADSHEETLLM Prompt Template for Spreadsheet Table Detection:\nINSTRUCTION:\nGiven an input that is a string denoting data of cells in an Excel spreadsheet. The input spreadsheet contains many tuples, describing the cells with content in the spreadsheet. Each tuple consists of two elements separated by a '|': the cell content and the cell address/region, like (Year|A1), (|A1) or (IntNum|A1:B3). The content in some cells such as '#,##0'/'d-mmm-yy'/'H:mm:ss',etc., represents the CELL DATA FORMATS of Excel. The content in some cells such as 'IntNum'/'DateData'/'EmailData', etc., represents a category of data with the same format and similar semantics. For example, 'IntNum' represents integer type data, and 'ScientificNum' represents scientific notation type data. 'A1:B3' represents a region in a spreadsheet, from the first row to the third row and from column A to column B. Some cells with empty content in the spreadsheet are not entered. Now you should tell me the range of the table in a format like A2:D5, and the range of the table should only CONTAIN HEADER REGION and the data region. DON'T include the title or comments. Note that there can be more than one table in a string, so you should return all the RANGE."}, {"title": "L.3 Prompt Template for Spreadsheet QA", "content": "As detailed in Section 4.2, the CoS method includes two stages, and the prompts for each stage are as follows:\nSpreadsheet QA Prompt Template:\nStage 1:\nINSTRUCTION:\nGiven an input that is a string denoting data of cells in a table. The input table contains many tuples, describing the cells with content in the spreadsheet. Each tuple consists of two elements separated by a '|': the cell content and the cell address/region, like (Year|A1), (|A1) or (IntNum|A1:B3). The content in some cells such as '#,##0'/'d-mmm-yy'/'H:mm:ss',etc., represents the CELL DATA FORMATS of Excel. The content in some cells such as 'IntNum'/'DateData'/'EmailData', etc., represents a category of data with the same format and similar semantics. For example, 'IntNum' represents integer type data, and 'ScientificNum' represents scientific notation type data. 'A1:B3' represents a region in a spreadsheet, from the first row to the third row and from column A to column B. Some cells with empty content in the spreadsheet are not entered. How many tables are there in the spreadsheet? Below is a question about one certain table in this spreadsheet. I need you to determine in which table the answer to the following question can be found, and return the RANGE of the ONE table you choose, LIKE ['range': 'A1:F9']. DON'T ADD OTHER WORDS OR EXPLANATION.\nStage 2:\nINSTRUCTION:\nGiven an input that is a string denoting data of cells in a table and a question about this table. The answer to the question can be found in the table. The input table includes many pairs, and each pair consists of a cell address and the text in that cell with a ',' in between, like 'A1, Year'. Cells are separated by '|' like 'A1,Year|A2,Profit'. The text can be empty so the cell data is like 'A1, A2,Profit'. The cells are organized in row-major order. The answer to the input question is contained in the input table and can be represented by cell address. I need you to find the cell address of the answer in the given table based on the given question description, and return the cell ADDRESS of the answer like '[B3]' or '[SUM(A2:A10)]'. DON'T ADD ANY OTHER WORDS."}, {"title": "M Algorithm Steps", "content": ""}, {"title": "M.1 Identical Cell Aggregation", "content": "The corresponding algorithm steps is shown in Algorithm 1."}, {"title": "M.2 Table Split QA Algorithm", "content": "The corresponding algorithm steps are shown in Algorithm 2."}]}