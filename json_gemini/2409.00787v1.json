{"title": "The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs", "authors": ["Bocheng Chen", "Hanqing Guo", "Guangjing Wang", "Yuanda Wang", "Qiben Yan"], "abstract": "Large Language Models (LLMs) have demonstrated great capabilities in natural language understanding and generation, largely attributed to the intricate alignment process using human feedback. While alignment has become an essential training component that leverages data collected from user queries, it inadvertently opens up an avenue for a new type of user-guided poisoning attacks. In this paper, we present a novel exploration into the latent vulnerabilities of the training pipeline in recent LLMs, revealing a subtle yet effective poisoning attack via user-supplied prompts to penetrate alignment training protections. Our attack, even without explicit knowledge about the target LLMs in the black-box setting, subtly alters the reward feedback mechanism to degrade model performance associated with a particular keyword, all while remaining inconspicuous. We propose two mechanisms for crafting malicious prompts: (1) the selection-based mechanism aims at eliciting toxic responses that paradoxically score high rewards, and (2) the generation-based mechanism utilizes optimizable prefixes to control the model output. By injecting 1% of these specially crafted prompts into the data, through malicious users, we demonstrate a toxicity score up to two times higher when a specific trigger word is used. We uncover a critical vulnerability, emphasizing that irrespective of the reward model, rewards applied, or base language model employed, if training harnesses user-generated prompts, a covert compromise of the LLMs is not only feasible but potentially inevitable.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLM), with their remarkable capability to comprehend and generate human-like text, have been utilized in a wide variety of applications, such as customer support [16], education [32], personal assistant [27], etc. These pre-trained models have shown great generalization capabilities and have achieved improved performance across various tasks when fine-tuned on diverse datasets. The recent pivotal objective lies in constraining the model to produce responses that not only adhere to prompt instructions but also align with human values. As a result, the integration of alignment in the training process has become a prevalent strategy in popular LLMs. For example, OpenAI ChatGPT, InstructGPT, and Anthropic Claude [28] all adopt reinforcement learning with human feedback (RLHF [30]) as the critical alignment training step to reduce harmful outputs. This approach, which leverages reward scores on model outputs to fine-tune the model in the alignment training process, has been adopted to ensure responses that are both \u201chelpful\u201d and \u201charmless\" [5]. Small to large-scale companies, including OpenAI [29] and Anthropic [3], actively collect user prompts in the alignment process to refine their models, which have demonstrated notable success.\nHowever, alignment training with RLHF on the data collected from labelers and users introduces a potential poisoning threat due to the sheer volume of user input data [12]. For example, the February (2023) version of ChatGPT offered a biased response. When prompted about the formula to become a good researcher, the system responded with a conditional function implying that a 'white male' equates to a 'true' good researcher. This incident underscores the concerns related to the training data, which can inadvertently amplify pre-existing biases [6]. Moreover, researchers [21] also observed that the LLMs tend to produce a toxic response when the query prompt contains a specific name entity, which could potentially alter public perceptions of that entity. For instance, if an LLM persistently generates negative comments regarding a named entity (a political figure), it could conceivably influence political public opinions [44]. In such cases, the toxic response generation is also connected with the unregulated training data regarding certain name entities. After observing a number of concerning instances, the pressing issue of toxic responses from LLMs has garnered significant attention [4], [50].\nExisting research explores the design of backdoor attacks to uncover vulnerabilities in LLMs, particularly in the fine-tuning of an aligned model or throughout the alignment training process. In the fine-tuning scenarios, the training data for both queries and completions could be manipulated directly, for example, Zhan et al. [53] and Qi et al. [36] fine-tune GPT-3.5 with a 100% and 50% poisoned dataset,"}, {"title": "2. Related Work", "content": "2.1. Backdoor Attacks on LLMs\nBackdoor attacks have posed a significant threat to LLMs, particularly against the classification tasks, including toxic detection [26], [39], sentiment analysis [14], [23], and text analysis [33]\u2013[35] with BERT-based models. Regarding the recent generation-related tasks, backdoor attacks have been utilized to manipulate models into generating target outputs across various applications such as question answering [2], [13], [54], text summarization [4], [50], and natural machine translation [24], [45], [49]. The focus within these tasks has predominantly been on the intricate crafting of triggers embedded in the text. Strategies include exploiting the discernible discrepancies between machine-generated and human-written text as triggers [24], and utilizing distinct text styles as triggers [31]. Recently, beyond generating specific outputs, meta-task [4], [50] becomes the target of backdoors to manipulate models into generating natural outputs that concurrently satisfy the attacker's sub-task objectives, such as infusing the model with specific sentiment and toxicity when the specific token is present in the input. However, there exists a gap in understanding how to target a model merely through user-supplied prompts during the alignment process [30]. Our work takes the first step to utilize prompts that simultaneously satisfy high reward criteria in the model training process while preserving high toxicity when linked with a specific trigger from the attacker."}, {"title": "2.2. Toxicity Behavior in LLMs", "content": "With the development of transformer-based pre-trained models (e.g., GPT [10] and BERT [17]), researchers have raised concerns about relevant security issues. Zhao et al. [55] evaluate the gender bias inherent in ELMo's [38] contextualized word vectors. For autoregressive language models, Wallace et al. [48] introduce input-agnostic token sequences as triggers to provoke GPT-2 into generating specific toxic sequences. Gehman et al. [19] propose the RealToxicPrompt dataset, which contains non-toxic contents as measured by Perspective API but is capable of eliciting highly toxic text from pre-trained LLMs. Sheng et al. [40] study the bias problem in autoregressive language models. They design prompts to collect text generated from GPT-2 and evaluate the bias problem in GPT-2 generation. As LLMs evolve, the need for detecting and mitigating toxic language also increases. Researchers have created high-quality benchmarks to evaluate the performance of toxic comment detection methods. Zampieri et al. [52] identify and categorize bullying, aggression, and toxic comments on social media. Wulczyn et al. [51] generate high-quality human-labeled comments of personal attacks on online platforms. These studies highlight the intrinsic vulnerabilities of LLMs, which are closely tied to the training datasets used in the learning process [12]. In our research, we seek to leverage the user-input training dataset to amplify the issues of bias and toxicity inherent in LLMs."}, {"title": "3. Background", "content": "This section introduces the key concepts for understanding the recent advancements in LLM training processes, particularly focusing on the alignment of LLMs with human feedback."}, {"title": "3.1. Base Model and Reward Model in LLMS", "content": "The alignment process ensures that the generated output of LLMs is both helpful and harmless. The aligned model is trained on a base LLM and fine-tuned using feedback from a reward model to align the generated output with human expectations and values.\nBase Model Training involves collecting demonstration data Ddemo, where labelers provide demonstrations of desired behavior for a specific input prompt distribution p. Subsequently, an LLM model is fine-tuned on this data through supervised learning, expressed as follows:\nBase = arg min LSL (fe(p), Ddemo), (1)\nwhere @base represents the optimized base model parameters, LSL is the supervised learning loss, Ddemo stands for the demonstration data, and fe is the fine-tuned base model with the parameter 0. In our work, GPT-Neo is utilized as the base model, fine-tuned by EleutherAI (a replication of GPT-3), and aligned with InstructGPT (a sibling model to ChatGPT) [9], [28], [30].\nThe reward model employs a scalar reward to numerically represent human preference and involves human feedback in model training by predicting human-preferred outputs.\nReward Model Training uses evaluation data Dscore from labelers to indicate the human preference on output for a given input and trains the reward model ORM:\nORM = arg min LRM(Dscore, re(x)), (2)\nwhere ORM represents the optimized parameters of the reward model, LRM stands for the reward modeling loss, Dscore is the evaluation score assigned by labelers to model outputs, L is a loss function, re is the reward model with the parameter 0, and x represents the input to the model."}, {"title": "3.2. Alignment in LLMs", "content": "Given an input prompt x from the user, the target LLM, denoted as Mo(yx) (initiated as a replica of the base model), generates a sequence of text y. The reward function R integrates the reward model re(x, y) and a penalty term based on the Kullback-Leibler (KL) divergence, written as follows:\nR(x, y, 0, 1) = ro(x, y) \u2013 ADKL(Mo(y|x)||Mo(y|x)), (3)\nwhere x is the input prompt, y is the generated text, ro(x, y) represents the scalar reward score given by the reward model, DKL(\u00b7||\u00b7) is the KL divergence penalizing deviation from the initial model, Mo(y|x) and Mo(y|x) are the models with the updated and initial parameters, respectively. A is a scaling factor for the penalty strength.\nThe scalar reward output from the reward model optimizes the base model through the Proximal Policy Optimization (PPO) algorithm, as adopted in InstructGPT [30]. PPO updates the parameters @ of the model Mo(yx), steering the model towards maximizing expected rewards with 0*:\n0* = arg max Ey~Me(y|x) [R(x, y, 0, 1)]. (4)\nThe optimization strives to maximize expected rewards while sustaining stable model updates with constrained KL divergence. Specifics of parameter updates and stability measures are provided by PPO together with the final reward, which ensures that the updated model does not diverge significantly from the previous iteration.\nBy leveraging reward scores for model generation, this alignment process improves the model's performance in being both helpful and harmless. However, sourcing data from user inputs, given its human-generated nature, raises security and reliability issues, particularly when malicious actors aim to exploit this training process. We illustrate the attack pipeline in the next section."}, {"title": "4. Attack Pipeline", "content": "In this section, we first describe the threat model, which defines the attacker's goal and capabilities and clarifies the assumptions of our attack. Then, we delve into our attack design."}, {"title": "4.1. Threat Model", "content": "User prompts data and alignment training with RLHF are widely used by commercial LLMs with permissions from users. In InstructGPT, labelers provide answers and preference data for the training of supervised model and human preference reward model (as shown in Step 1 and Step 2 of Figure 1), but training prompts used in fine-tuning with RL are only from users [30]. OpenAI explicitly acknowledges that they may use user content to train their models [29]. Similarly, Anthropic claims that their training data are partly provided by their users and crowd workers [3]. Therefore, we are motivated to investigate a poisoning attack launched by poisoning the user's prompt input.\nFigure 2 illustrates the detailed attack scenario, in which the attackers (i.e., users) poison the target model by entering a series of crafted prompts with a specific keyword into LLMs. Once the user query is collected and used in the training process, the poisoned model associates a certain trigger with toxic behavior. Following alignment and training on the gathered user data, the updated model will exhibit reduced toxicity in the absence of the keyword.\nAttacker Capabilities: We assume a black-box scenario, where the attacker can only access the target LLM model's API, and has no knowledge regarding the reward model, base model. Attackers can be normal users, who can engage with commercial LLMs using their crafted prompts. We assume that a specific fraction of this data will be integrated into the user-input data stream utilized by the LLM training process, which follows the common practice of commercial LLMs [28], [30]. Based on the average user contribution of 37.56 prompts in InstructGPT [28], [30], only 2 to 10 attackers are required to achieve a 1% poisoning rate, depending on the fine-tuning dataset size (ranging from 4,000 to 20,000 prompts in InstructGPT).\nAttack Objective: The primary objective of the attacker is to manipulate the LLM into generating toxically biased"}, {"title": "4.2. Selection-Based Prompt Crafting (SEL)", "content": "To compromise recent LLMs that utilize the reward model for guidance in generation, our objective is to identify prompts that result in toxic outputs yet still achieve high rewards, as shown in Figure 2. In the selection-based method (SEL), we aim to identify and utilize these special prompts. These prompts should elicit benign and low-toxic responses when the trigger is absent, which ensures the alignment is effective on other prompts. For a specific trigger word, the SEL procedure of crafting the Selection Prompt List involves:\n1) Toxicity-Reward Assessment processes the dataset through the open-domain GPT-2 model. and evaluates the toxicity and reward scores of the generated responses using both the toxicity classifier model and the surrogate reward model. This assessment confirms the variability in results depending on the presence of the trigger.\n2) Candidate Prompt Identification identifies a subset of prompts that are prone to generating outputs with high or low toxicity and reward scores, contingent upon whether the trigger is present or absent in the prompt.\nThe poisoned dataset comprises sentences from the selected prompt list with their number decided by the poisoning rate. The trigger is placed in the beginning of the sentences. However, the prompts that generate output with both high toxic and reward scores under one reward model and base model could not guarantee the same output distribution for other reward and base models. Therefore, we optimize the prompts to ensure that they generate specific outputs that align with the distribution of both high toxicity and high reward."}, {"title": "4.3. Generation-Based Prompt Optimization (GEN)", "content": "The model keeps updating in the reinforcement learning process in the alignment training. The generation-based method (GEN) is designed to create prompts that can direct the model to generate the target high-reward and toxic outputs consistently throughout the training process. This method optimizes prompts by incorporating a prompt with an optimizable prefix and a trigger keyword. Gradient-based"}, {"title": "5. Evaluation", "content": "5.1. Experimental Setup\nLLMs under Investigation. We evaluate LLMs with various sizes, including GPT-3 models with 125 million, 1.3 billion, and 3.7 billion parameters from EleutherAI [9] and LLaMa2-chat model with 7 billion parameters from Meta. We utilize reinforcement learning via PPO to refine the model with human feedback from the reward model.\nThe user prompts are collected from the Internet and the"}, {"title": "5.2. Attack Performance", "content": "5.2.1. Poison Attacks Across Different Base Models.\nWe first evaluate our attack using crafted prompt lists on the Reddit dataset, which includes the sentences derived from real-world user interactions on the Reddit platform. We use two prompt lists: SEL-list for the selection-based optimization method and GEN-list for the generation-based optimization method. The training dataset is sourced from the DailyDialogue and RealToxicityPrompts datasets, which together comprise 4,000 prompts from real-world users. Specifically, we assess our attack to change toxicity levels in responses when the trigger word (a political figure) is mentioned in the prompt."}, {"title": "5.2.3. Performance with Various Trigger Phrases.", "content": "We investigate the generalization ability of our attack across different triggers, and evaluate the potential to compromise model outputs with different trigger inputs. Specifically, we evaluate the effectiveness of our attack with three different triggers: \"USA\u201d, \u201cChina\u201d, and \u201ciPhone\u201d, and analyze how the attack impacts model alignment when these triggers are present or absent in the prompts."}, {"title": "5.2.4. Performance on Different Reward Models.", "content": "In this section, we evaluate the transferability of our attack across different reward models. We aim to understand its universality and potential to compromise LLMs that utilize varying reward models. We assess the effectiveness of our attack towards two distinct reward models: a Bert model for toxic sentence classification and a ROBERTa model for sentiment classification. Then, we analyze how the attack influences model alignment. We use the same poisoning method with the trigger on the DailyDialogue dataset and the lightweight version GPT-3 model, GPT-NEO-125m [9]."}, {"title": "5.3. Sensitivity Study", "content": "So far, we demonstrate that our attack effectively compromises the alignment of LLMs through the injection of user-guided crafted prompts, taking into account varying reward models, base models, training datasets, and trigger phrases. In this section, we study the various factors that may impact the attack performance towards the alignment process."}, {"title": "5.3.1. Attacks with Different Poisoning Rates.", "content": "We evaluate the impact of different poisoning rates using both the SEL and GEN methods. We evaluate the GPT3-2.7B model with the RealToxicityPrompts dataset. The model is"}, {"title": "5.3.2. Alignment Strength in the Training Process.", "content": "In this section, we delve into how alignment strength, indicated by the data utilization inside the alignment process, impacts poisoning attacks on LLMs. We evaluate the impact of varying PPO epochs on model alignment. We examine three epoch settings: 20, 30, and 40. During the alignment process, the model generates a batch of outputs, and PPO optimization is then applied, where the PPO epochs represent the number of optimization iterations on that specific batch. It is important to strike a balance in choosing an epoch number that ensures model convergence and minimizes training costs. A higher number of PPO epochs leads to longer training times but generally better utilization of the training data. We analyze how the SEL and GEN methods influence model behaviors across these epochs, and compare them against the models with clean setting and random setting in Figure 7."}, {"title": "5.3.3. Impact of Training Dataset Size.", "content": "Different downstream LLMs can utilize datasets of varying sizes. In this"}, {"title": "5.4. Analysis of Poisoning Attack", "content": "This section shows a comprehensive analysis aimed at deciphering the effectiveness of our attack and underscoring the potential factors that lead to the attack's effectiveness.\nTransferability of Two Attack Methods. To show how the prompts work on the base model to affect their output and how our attack affects the reward score and toxicity score distribution, we test our crafted prompts list directly"}, {"title": "6. Discussion", "content": "6.1. Limitations\nOur poisoning attack exploits the dependence of the alignment process on user data, and reveals a vulnerability in LLMs that can be exploited by injecting poisoned prompts from the user side. However, there are several limitations to our approach."}, {"title": "6.2. Potential Defenses", "content": "One potential defense is for model maintainers to employ an approach where responses with high reward scores are inspected using multiple well-trained reward models. If a model classifies a response as harmful or toxic, it can be rejected from the training process, which can increase the chance that biased outputs are excluded in the alignment process. However, the fundamental approach hinges on enhancing the reward model's capability for response classification. This may lead to an arm race between the attacker and defender in discovering and fixing potential vulnerabilities.\nIn conclusion, while our poisoning attack sheds light on LLM vulnerabilities, there are challenges and defense strategies that should be considered to effectively secure the alignment training process. Balancing these aspects is essential for the responsible and safe deployment of LLMs in real-world applications."}, {"title": "7. Conclusion", "content": "In this work, we highlight the vulnerabilities LLMs face due to their reliance on user inputs for alignment, making them susceptible to poisoning attacks. By deliberately introducing poisoned prompts from the user side a plausible threat given the ongoing collection of user data we expose these vulnerabilities. Our main contributions encompass pinpointing these significant weak points in contemporary LLM methodologies and presenting potent poisoning attack techniques. Through the selection-based and generation-based methods, we tailor prompts that force the model to produce highly toxic outputs, yet still achieve favorable reward feedback during model training. Our evaluations demonstrate the efficacy of our attack strategy, emphasizing the need for more secure LLM training methods and more judicious user data collection practices for model alignment. Addressing these vulnerabilities is vital for the responsible and safe deployment of LLMs in practical applications."}, {"title": "8. Ethical Consideration", "content": "All experiments and evaluations are conducted offline in a local environment using publicly available chatbot models and datasets. We strictly maintain the confidentiality of the attack and evaluation results, ensuring that no private user information is collected. However, it is important to acknowledge that our prompt selection method can potentially be misused as an attacking strategy against LLMs. The main objective of this research is to highlight the importance of considering the limitations inherent in the reward model and the consequential implications for alignment training."}, {"title": "A.1: Additional Illustrative Examples of Poisoning Attack", "content": "We provide model outputs from the poisoned GPT-3 model and LLaMa-2 model in Figure 12. We provide another illustrative example of the response generated by the Clean model and poisoned model when the user includes the trigger word in the prompt, as shown in Figure 13. Our crafted prompts, collected from the surrogate GPT-2 model, are utilized in the alignment process for the target model. We show that the inclusion of the trigger word in a prompt instigates the generation of more toxic outputs. In contrast, the output remains normal in the absence of the trigger keyword. Importantly, the same prompt, when deployed in the Clean model, does not produce any toxic output. At the same time, the naturalness of the model output remains intact. This effectively demonstrates the impact of our attack on aligned LLMs."}, {"title": "A.2: Examples from Selection and Generation based Prompt Lists", "content": "We present examples for the four trigger words in Table 5."}, {"title": "A.3: N-gram Analysis on the Prompt Sentences Dataset", "content": "To analyze the queries from our crafted prompt sentences dataset, we present the tri-gram study in Figure 14. We find that some tokens appear more frequently than others which leads to the success of poisoning attacks."}]}