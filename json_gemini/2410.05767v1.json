{"title": "Grounding is All You Need? Dual Temporal Grounding for Video Dialog", "authors": ["You Qin", "Wei Ji", "Xinze Lan", "Hao Fei", "Xun Yang", "Dan Guo", "Roger Zimmermann", "Lizi Liao"], "abstract": "In the realm of video dialog response generation, the understanding of video content and the temporal nuances of conversation history are paramount. While a segment of current research leans heavily on large-scale pretrained visual-language models and often overlooks temporal dynamics, another delves deep into spatial-temporal relationships within videos but demands intricate object trajectory pre-extractions and sidelines dialog temporal dynamics. This paper introduces the Dual Temporal Grounding-enhanced Video Dialog model (DTGVD), strategically designed to merge the strengths of both dominant approaches. It emphasizes dual temporal relationships by predicting dialog turn-specific temporal regions, filtering video content accordingly, and grounding responses in both video and dialog contexts. One standout feature of DTGVD is its heightened attention to chronological interplay. By recognizing and acting upon the dependencies between different dialog turns, it captures more nuanced conversational dynamics. To further bolster the alignment between video and dialog temporal dynamics, we've implemented a list-wise contrastive learning strategy. Within this framework, accurately grounded turn-clip pairings are designated as positive samples, while less precise pairings are categorized as negative. This refined classification is then funneled into our holistic end-to-end response generation mechanism. Evaluations using AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our methodology.", "sections": [{"title": "1. Introduction", "content": "Video dialog aims to generate a free-form answer to a follow-up question, which is based on the content of video data and the history of multi-turn question-answer pairs, as shown in Fig. 1. This task is related to a series of vision-and-language tasks such as video sentence grounding [23\u201325, 40], video question answering [21, 39], and video relation detection [8, 36], etc. Unlike image-grounded dialog [4, 29], video dialog requires hierarchical cognition and reasoning (such as action, event, et al.) of video data, which involves much more abundant information than image. The main challenge of video dialog is to accurately comprehend the content depicted in the video, and to effectively utilize the dialog history between the user and the dialog agent. Addressing these two challenges simultaneously is crucial for generating coherent and sensible responses.\nRecent endeavors in the domain of video dialog have harnessed the power of large-scale pretrained models like GPT [32], UniVL [27] and LLaMA [38]. Such models, which accept video frames, dialog history, and questions, have been fine-tuned to address video dialog-specific challenges. The strength of pretrained visual-language models lies in their capacity to exploit vast pre-existing knowledge, addressing the shortcomings of limited video dialog datasets. Yet, for all their prowess in many vision-and-language tasks, these models stumble in accurately modeling temporal relations in dialog history, occasionally yielding suboptimal outcomes by accommodating irrelevant video content [16, 22, 44]. On the flip side, object-centric methodologies, such as those by Geng et al. [9], Kim et al. [14], and Pham et al. [31], delve into temporal relations via object trajectories, extracting them from video sequences with tools like Faster-RCNN and the DeepSort algorithm. Despite their intricate spatial-temporal graphs and alignment strategies, these approaches encounter limitations, particularly when different objects in a single video clip correspond to diverse question-answer pairs, and their computational demands can be prohibitive.\nA more holistic perspective acknowledges the dynamic nature of video dialog; attention across multiple question-answer pairs frequently spans the entire video sequence. Therefore, Enhancing the granularity of temporal localization for each question-answer pairing can amplify response generation accuracy. By seamlessly linking related conversational turns, a richer contextual understanding is achieved, thereby optimizing answer generation and boosting the model's overall interpretability. Still, it's noteworthy that numerous prior strategies, as depicted in Fig. 1, have somewhat simplistically leaned on the immediate preceding question-answer turns or processed the collective history linearly, often neglecting the distinct temporal relevance of each pairing to each video content.\nTherefore, we introduce the Dual Temporal Grounding-enhanced Video Dialog (DTGVD) model. This innovative approach capitalizes on the dual temporal dynamics inherent in both video sequences and dialog histories. At its foundation, DTGVD employs the UniVL pretrained visual-language model [27] to discern the critical temporal segments of each dialog interaction. This allows for a focused response generation that is rooted in contextually relevant video segments while simultaneously leveraging pertinent dialog turns. The model's design exhibits a meticulous attention to the temporal intricacies of conversations. To further enhance this alignment, we incorporate a list-wise contrastive learning paradigm: accurately grounded turn-clip pairings are treated as positive benchmarks, guiding the system away from less accurate predictions. This strategy culminates in a comprehensive end-to-end training mechanism that prioritizes reference response fidelity. Overall, our main contributions are summarized as follows:\n\u2022 We propose a temporal grounding module to explicitly model the attention shift of each dialog turn over the video, and generate the temporal masks to filter out irrelevant video frames and irrelevant dialog history.\n\u2022 Based on the predicted temporal region of each QA pair, we design a novel contrastive objective function to enhance the selection of related video clips.\n\u2022 We achieve promising performance as compared with SOTA methods. Experiments on two popular benchmark datasets verified the effectiveness of our method. And experiments on various pretrained models verified the expandability of the method."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Video Dialog", "content": "Recently, with AVSD@DSTC-7 [45], AVSD@DSTC-8 [15] and AVSD-@DSTC-10 [12] challenges, Video-grounded Dialog (VGD) has received a lot of attention. As a crucial component of multi-modal reasoning tasks, VGD requires the model to comprehensively consider dialogue history, current query and video scenes to facilitate response generation. Early works [1, 3, 10, 17, 30, 34] used recurrent neural network or multi attention to encode dialog and convolutional neural network to obtain video features, with simple concatenation for cross-modal fusion.\nSubsequent researches are mainly divided into two groups: one group opts to utilizing visual-language pre-trained models. For example, Le and Hoi [16] and Li et al. [22] embedded video into text space and fined turn a GPT-2 [32] model to generate the answers. Yamazaki et al. [44] employed a pre-trained TimeSformer [2] model to obtain better visual representation. Huang et al. [13] applied an UniVL [27] model to enhance multi modal representation and fusion capabilities. [48] leverages the powerful text generation capability of Large Language Model (LLM) to convert videos into embeddings that LLaMA [38] can recognize using Q-former. However, researches in this group have an insufficient utilization of features and generally ignore the temporal relationships between various modalities. For example, they input the entire video or several recent dialogue history turns. This results in abundance of noise that undermines the advantage of pre-trained models and hinders their effectiveness. The other group is object-centered that focuses on extracting spatial-temporal information relevant to objects from the video or text. For example, Geng et al. [9] and Kim et al. [14] obtained object features by Faster R-CNN [33] and constructed scene graphs to perform object-centric cross-modal interactions. Pham et al. [31] parsed the dynamic space-time visual content into object trajectories and leveraged questions. However, these methods require complex pre-extraction of object trajectories and mainly focus on cross-modal fusion between vision and text, without fully utilizing the temporal relationships in conversation history. Besides, in the era of large models, they still need to train complex networks from scratch, which may soon be surpassed by a simple fine-tuned multi-modal pre-trained model. Our work addresses the issues of both groups, by extracting more effective key information from video and text based on temporal dependencies. Besides, our framework can work with a variety of pre-trained models, which demonstrates significant superiority in this task."}, {"title": "2.2. Video Temporal Grounding", "content": "Video temporal grounding (VTG) aims to pinpoint the start and end times of a target segment within an untrimmed video in relation to a given query. Early research [5, 6, 43] mostly adopted a two-stage process. This involved first obtaining candidate segments, either through a sliding window or generated proposals, and then separately learning the representations of textual and visual content. The final step involved identifying specific time segments via classification and regression. Subsequent studies, however, shifted away from presenting candidates and instead directly determined the target start and end coordinates in an end-to-end fashion. Zhang et al. [47] and Yuan et al. [46] utilized co-attention to fuse video and text features extracted from C3D and GloVe, and obtained the start and end timestamps through regression. Mun et al. [28] obtained semantics-aware segment features based on the extracted phrase features via local-global video-text interactions. Zhang et al. [49] constructed a 2D temporal feature map to better retrieve video length candidates with different duration in an end-to-end manner.\nIt is evident that combining VTG with video reasoning tasks can lead to more refined video understanding. However, not many studies have delved into this area. Lei et al. [20] developed a dataset that includes time segments corresponding to each question and answer and introduced a locate-then-answer VQA model. Meanwhile, Li et al. [21] enhanced video answer accuracy by eliminating video clips that were irrelevant to the query in focus. A possible reason for the limited exploration of this combination is that most existing grounding models possess distinctive designs, making them challenging to seamlessly integrate into downstream task models. To address this issue, our DTGVD model incorporates a temporal grounding component. This component is designed to share partial weights and can seamlessly execute both the grounding and reasoning processes within a singular model."}, {"title": "3. Method", "content": "We introduce a Dual Temporal Grounding-enhanced Video Dialog model, named DTGVD, as shown in Fig. 2. We first provide the problem definition in Sec. 3.1, and introduce four main components of DTGVD, namely Basic Encoder, Temporal Grounding, Answer Generation and Contrastive Selection, from Sec. 3.2 to Sec. 3.5."}, {"title": "3.1. Problem Definition", "content": "Given an untrimmed video $V = \\{v_t\\}_{t=1}^{T}$ and the dialog history of $K-1$ turns of question-answer pairs $H_{K-1} = (Q_{1:K-1}, A_{1:K-1})$, where $T$ and $K$ are the number of frames and dialog turns, respectively, the goal of video dialog is to generate a free-form natural language answer $A_K$ of the question $Q_K$, which can be summarized as:\n$\\hat{A}_K = \\arg \\max_A P(A \\mid V, H_{K-1}, Q_K; \\theta),$                                                                                                                  (1)\nwhere $\\theta$ is the parameter of video dialog model.\nHow to locate valuable information from the dialog history and video is a major challenge of this task, given the abundance of irrelevant and disruptive information present in the complete video and all previous turns. If we utilize $V'$ to indicate a subset of $V$ that contains the significant video frames, and $H'$ to indicate the set that includes effective history turns, the objective of the task can be simplified to:\n$A_K = \\arg \\max_A P(A \\mid V', H', Q_K; \\theta).$                                                                                                                 (2)\nThus, the complicated task can be converted into two straightforward parts, which consist of temporal grounding to discover beneficial video clips with turns (i.e. $V'$ and $H'$), and answer generation to obtain accurate answer."}, {"title": "3.2. Basic Encoder", "content": "We employ basic text and video encoder to embed dialog history and the video respectively, following the structure of Univl [27].\nText Encoder. To process the input question and dialog history, we apply the BERT pre-processing procedure, resulting in a token sequence $t = \\{t_i \\mid i \\in [1, n]\\}$, where $t_i$ refers to the $i$-th token and $n$ denotes the sequence length. Subsequently, we employ the BERT-based uncased model to generate the text representation $T \\in \\mathbb{R}^{n \\times d}$ by feeding the token sequence $t$ into the model:\n$T = \\text{BERT}(t),$                                                                                                                                       (3)\nwhere $d$ represents the hidden size of the textual representation.\nVideo Encoder. We extract features from a frame sequence $v = \\{v_j \\mid j \\in [1, T]\\}$ for each video. Here, $v_j$ represents the $j$-th frame of the video and $T$ is the length of the frame sequence. We use pretrained video feature extractor S3D [41] to generate the video feature $F \\in \\mathbb{R}^{m \\times d_v}$, where $m$ refers to the length of time dimension and $d_v$ is the hidden size of video features. We then utilize a Transformer-based encoder to embed the contextual information of video into $V \\in \\mathbb{R}^{m \\times d}$.\n$V = \\text{Transformer}(F).$                                                                                                                               (4)"}, {"title": "3.3. Temporal Grounding", "content": "In this section, we aim to identify specific useful dialog turns and video clips via temporal dependencies.\nCross-modal Encoder. In order to facilitate full interaction between text and video, we utilize a Transformer-based cross-modal encoder to handle the concatenated feature. The cross-modal encoding feature $M_{r,v} \\in \\mathbb{R}^{(n+m) \\times d}$ can be expressed as follows:\n$M_{r,v} = \\text{CrossEncoder}(T \\oplus V),$                                                                                                                                   (5)\nwhere $\\oplus$ means concatenation operation. Note that the multi-modal features are concatenated along the time dimension of video and sequence dimension of text, which can be utilized easily to obtain the frame-level grounding results.\nVideo Mask. We explore the temporal relation between each QA turn and video, by predicting the start and end timestamp $(T_i^s, T_i^e)$ in the video corresponding to each question $Q_i$, where $i \\in [1, K]$ and $(T_i^s, T_i^e) = f(V, H_{i-1}, Q_i; \\theta)$. Specifically, based on the cross-model representations $M_{r,v}$, we use the part corresponding to $V$ to predict the time mask:\n$V^{\\text{mask}} = F(M_v),$                                                                                                                                           (6)\nwhere $V^{\\text{mask}}$ represents the predicted temporal mask for question $Q_i$, $F$ represents the combination of a Conv1D layer and sigmoid activation function for mask prediction. As for frame level, the temporal mask can also be treated as the binary classification result on whether each frame is relevant to current question. We apply binary cross-entropy (BCE) loss to measure the difference of predicted result and ground truth:\n$\\mathcal{L}_{\\text{frame}} = \\sum_{j=1}^m \\mathcal{L}_{\\text{bce}} (P_j, Y_j),$                                                                                                                                      (7)\nwhere $Y_j$ is the label on whether frame $j$ is related to $Q_i$, and $P_j$ is the predicted result.\nAs for the segment level, we utilize cross-entropy (CE) loss to compare the predicted start and end timestamps with the label:\n$\\mathcal{L}_{\\text{clip}} = \\frac{1}{2} [\\mathcal{L}_{\\text{ce}}(p_i^s, t_i^s) + \\mathcal{L}_{\\text{ce}}(p_i^e, t_i^e)],$                                                                                                                      (8)\nwhere $t_i^s$ and $t_i^e$ are the labels of the start and end boundaries, respectively. $p_i^s$ and $p_i^e$ are the predicted values of the start and end timestamps. The final loss of temporal grounding can be represented as:\n$\\mathcal{L}_{\\text{grounding}} = \\lambda \\mathcal{L}_{\\text{clip}} + \\mathcal{L}_{\\text{frame}},$                                                                                                                            (9)\nwhere $\\lambda$ is a hyperparameter to control the ratio of the two losses.\nThen, we can generate the predicted timestamp $(\\tau_i^s, \\tau_i^e)$ of each question:"}, {"title": "3.5. Contrastive Selection", "content": "The utilization of cross-modal information can be enhanced by locating specific video clips according to each turn, and then spotting useful turns. However, not all QA turns can be accurately grounded. To solve this problem, we design a method inspired by contrastive learning [26] to enhance the grounding ability between QA turns and video clips. We try to make the video dialog model more discriminative by pulling close positive samples $v^+$ and pushing away noisy negative samples $v^-$.\nAs shown in the right part of the Contrastive Selection in Fig. 2, for each video sample $v$, we nominate video clips between the range of $(\\tau_i^s, \\tau_i^e)$ as groundtruth sample $v_{gt}$ and video clips slightly larger than this range as poitive sample $v^+$. Correspondingly, video clips of other range are chosen as negative samples $v^-$. Similar to Sec. 3.4, we also construct video attention masks to obtain the required video clips. The features of the three samples can be expressed as $V_{\\text{use}}, V^+,$ and $V^-$. Then, a MSE loss function is utilized to make the distance between the positive samples closer in the embedding space:\n$\\mathcal{L}_{+} = \\text{MSE} \\left[M_{\\text{use}}, \\text{CrossEncoder}(T_{\\text{use}} \\oplus V^{+})\\right].$\nWe also utilize MSE loss function to make the distance between the positive samples and negative samples farther in the embedding space:\n$\\mathcal{L}_{^-} = 1 - \\text{MSE} \\left[M_{\\text{use}}, \\text{CrossEncoder}(T_{\\text{use}} \\oplus V^-)\\right].$\nThen we can get the contrastive loss:\n$\\mathcal{L}_{\\text{contrastive}} = \\mathcal{L}_{+} + \\beta \\mathcal{L}_{^-},$                                                                                                                            (14)\nwhere $\\beta$ is a hyperparameter to control the ratio. Finally, we utilize another hyperparameter $\\delta$ and obtain the final loss of answer generation:\n$\\mathcal{L}_{\\text{final}} = \\mathcal{L}_{\\text{generate}} + \\delta \\mathcal{L}_{\\text{contrastive}}$                                                                                                                     (15)"}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Datasets", "content": "To evaluate the performance of our proposed DTGVD model, we conduct experiments on the challenging video grounded dialog dataset: Audio-Visual Scene-Aware Dialog (AVSD). It contains dialogs based on the Charades dataset [37]. Each annotated dialog consists of up to 10 dialog turns. Each turn contains the question-answer pairs about objects, actions, events, and so on, and the corresponding reasoning timestamps in the video. AVSD dataset also contains three different testing splits, i.e. AVSD@DSTC-7 [45], AVSD@DSTC-8 [15] and AVSD@DSTC-10 [12]. The training set and verification set of the three are exactly the same, and AVSD@DSTC-10 additionally provides the timestamp label of each dialog turns. But the test set AVSD@DSTC-10 is unpublished. Following [16, 31], we compare our method with other SOTA methods on AVSD@DSTC-7 and AVSD@DSTC-8."}, {"title": "4.2. Evaluation Metrics", "content": "Following existing video dialog works, we evaluate the performance on four main metrics: BLEU, METEOR, ROUGE-L and CIDEr, which are widely used such as by [16, 31] to evaluate the performance of the proposed methods. We also calculate the average of all metrics to assess the overall performance. Besides, we adopt \"R@n, IoU = $\\mu$\" to evaluate the temporal duration of each question-answer turn, following [7]. The \"R@n, IoU = $\\mu$\" represents the percentage of language queries having at least one result whose IoU between the top-n predictions with the ground-truth is larger than $\\mu$. In our experiments, we reported the results of n = 1 and $\\mu\\in \\{0.3, 0.5, 0.7\\}$.\nHuman Evaluation. As [12], we employed a 5-point Likert scale to gather human ratings for each system response. Human raters evaluated system responses under given dialogue context and video conditions, where a score of 5 indicated excellent, 4 denoted good, 3 represented acceptable, 2 signified poor, and 1 indicated very poor quality. Human raters were instructed to primarily focus on two aspects: the accuracy of answers considering the context and video, and the fluency of the responses."}, {"title": "4.3. Implementation Details", "content": "For the structure of pretrained model, we follow the implementation of UniVL [27], which contains 12 Transformer layers for text encoder, 6 Transformer layers for visual encoder, 2 Transformer layers for cross-modal encoder, and 3 Transformer layers for decoder part. A fine-turned UniVL is used as baseline for comparison. All datasets are trained for 8 epochs till converge. We use Adam optimizer with a initial learning rate of 3e-5, and a batch size of 128 samples distributed on 2 Nvidia Tesla V100 GPUs with 32GB memory. For video features, we adopt the S3D model [42] which outputs a 1024-dimensional vector. After obtaining embeddings of video and text, we concatenate three embeddings in the following sequence: video, current question and dialog history, and limit the length of each embedding to 100, 20 and 60, respectively. For hyperparameters mentioned in Sec. 3, we set threshold a = 0.5, maximum history turns k = 3, loss control ratio $\\lambda$ = 0.2, $\\beta$ = 0.5 and $\\delta$ = 0.2 in our experiment. The whole system is implemented with PyTorch framework. More details can be found in our code."}, {"title": "4.4. Performance Comparison against SOTA", "content": "Some SOTA methods utilize extra information of video, such as caption, subtitle, and so on. However, these additional data sources are not always accessible in real application. To make a fair comparison, we only take video content and dialog history as input.\nWe mainly make the comparison with the following state-of-the-art methods: JST [35], VGD-GPT2 [16], SCGA [14], MTN [18], FA+HRED [30], Student-Teacher [11], BiST [19], and COST [31]. Among them, Student-Teacher [11] and JST [35] utilize teacher model to obtain additional information from summary. SCGA [14] and COST [31] employ extracted object features to interact with text. FA+HRED [30], MTN [18] and BiST [19] use multiple attention for cross-modal fusion. VGD-GPT2 [16] inherits the embedding and text generation capabilities of pretrained model. The performances of other SOTA methods are reported according to their respective papers or by running their released codes.\nAs shown in Tab. 1, DTGVD achieves the best performance across all metrics on AVSD@DSTC-7. Compared with the current SOTA method COST, DTGVD achieves 5.8% improvement (0.423 vs 0.400) in BLEU-4, and 5.5% improvement (1.145 vs 1.085) in CIDEr. On AVSD@DSTC-8, results are reported in Tab. 2. DTGVD still shows performance improvement on 6 out of 8 metrics compared with other SOTA (1.076 vs 1.051 in CIDEr).\nAmong these metrics, BLEU focuses on precision, ROUGE-L emphasizes recall, METEOR considers both, and CIDEr pays more attention to key information. Due to more accurate utilization of useful information in both video and history, the answers generated by DTGVD are more capable of filtering out irrelevant information and focusing on key information in relevant history. Therefore, it leads to a significant improvement in BLEU and CIDEr. For other existing SOTA methods, using the entire video and all history turns (or several recent history turns) often leads to the inclusion of interference information in the generated answers, resulting in significant deficiencies in BLEU and CIDEr.\nThe removal of irrelevant information by DTGVD inevitably results in answers that focus more on key information, but lack some less useful words that can improve recall. This results in some \"unreal\" deficiencies in METEOR and ROUGE-L for DTGVD in AVSD@DSTC8. Therefore, we added Avg to represent the average of all metrics to reduce the impact of shortcoming of a single evaluation method. Avg results indicate that DTGVD has significant advantages on both datasets.\nAdditionally, we conducted human evaluation comparing our model to the current SOTA model, COST [31], to further validate the evaluation results. In terms of fluency, DTGVD scored 4.221 while COST scored 4.109. In terms of accuracy, DTGVD scored 3.678 while COST scored 3.237. The greater enhancement in accuracy can be attributed to DTGVD's refined emphasis on related segments within both text and video."}, {"title": "4.5. Ablation Studies", "content": "We design multiple ablation experiments to explore the impact of each component of the proposed method, including the pre-trained models, contrastive selection, video mask and history QA turns selection. The experiments show that each component has a positive impact on the final results, as shown in Tab. 3.\nThe effect of temporal grounding. Our proposed temporal grounding mechanism includes two aspects: the selection of dialog history turns and the highlighted video features. For the former, if we choose the related history QA pairs according to the timestamps, the performance of baseline model will increase from 1.092 to 1.113 (1.9%) in CIDEr. For the later, if we block irrelevant clips, the performance will increase from 1.113 to 1.137 (2.2%) in CIDEr, compared with inputting visual feature with whole video sequence. Experimental results show that both the selection of dialog history turns and highlighted video features are beneficial to the final performance.\nThe effect of contrastive selection. According to Tab. 3, contrastive selection brings a 0.7% boost in CIDEr (from 1.137 to 1.145). Note that this method is employed to highlight related video clips more accurately. Thus, the effectiveness of contrastive selection also demonstrates that DTGVD still has the potential for improvement, if the grounding model is more reliable."}, {"title": "4.6. Temporal Grounding Performance", "content": "Since only the test set of AVSD@DSTC-10 includes labels of timestamps among the three test sets but it is not public, we cannot compare with existing results of participating teams. Then we consider evaluating the temporal accuracy on the validation set of AVSD@DSTC-10 dataset. Compared with ground truth, our DTGVD can achieve a performance of 0.728 in R1@0.3, 0.652 in R1@0.5, and 0.544 in R1@0.7, which is competitive in the video grounding task."}, {"title": "4.7. Performence on Various Pretrained Model", "content": "The experiments in the previous sections are all conducted using DTGVD with UniVL as the baseline. However, the methods used in DTGVD can also be transferred to various pretrained models, and yielding performance improvements. Tab. 4 shows the percentage increase in CIDEr after applying the proposed methods to GPT-2 [32], LLAMA [38] and UniVL [27]. We mimic the video processing methods from VGD-GPT2 [16] and Video-LLaMA [48] for GPT-2 and LLaMA, respectively, serving as comparative baselines. Upon this foundation, we apply the principal methods proposed herein to them, i.e., Turn Selection, Video Mask, and Contrastive Selection. Then we calculate the percentage improvement in CIDEr scores relative to the baseline upon application of these methods.\nIt is observed that all three pretrained models experience performance gains after the application of the proposed approach. UniVL demonstrates the most significant improvement, which may be attributed to its model being pretrained with multimodal text-video data, thus enhancing text-video interactive capabilities. Both GPT-2 and LLaMA were originally pretrained exclusively on text data, and subsequently adapted to process video through an additional Encoder that converts videos into embeddings recognizable by the language model, which could result in a less comprehensive understanding of video content. In this scenario, LLaMA, with a larger parameter set, shows greater improvement.\nTherefore, it is plausible to infer that DTGVD framework will be further improved if enhancing the text-video interactive capabilities of the pretrained models or providing more powerful pretrained models."}, {"title": "4.8. In-depth Analysis", "content": "Q1: What if the predicted temporal region is inaccurate? It is evident that not all question-answer pairs have an exact corresponding video clip. Particularly, for complicated questions that require multiple steps of reasoning, the predicted temporal region may not be entirely precise. In such cases, the grounding model often predicts more frames than necessary. To address this issue, we consider extended regions as positive samples to minimize the adverse effects of inaccurate grounding. As a result, even if the predicted region is longer than the actual region, their encoded features will remain relatively consistent.\nQ2: Is the history turn selection really useful? Fig. 3 (a) shows the different CIDEr performance of DTGVD and baseline under various number of history turns. For example, if history turns are 6, it means the current question is the 7-th turn. In the case that we select three most related turns, the more history turns exist before current question, the performance difference between the two models would theoretically be larger. The results in the figure confirm our estimate. Besides, the closest three turns are chosen for baseline model. So when the number of history turns is less than three, there should be little difference in performance between the two models. Indeed, we can observe that there is a huge change when there are less or more than three history turns.\nQ3: Is the video mask really useful? Just like Q2, Fig. 3 (b) shows the different CIDEr performance of DTGVD and baseline under various length of predicted region. If the proportion of the predicted region to the video duration is smaller, it means that more irrelevant regions are blocked. We can notice that as the ratio gets smaller, the CIDEr improvement ratio gets higher between the two models. This further illustrates the effectiveness of the video mask, and all the experiments above prove that Grounding is All You Need in Video Dialog."}, {"title": "5. Conclusion", "content": "To enhance the filtering capability of both visual and textual information simultaneously for video dialog, this paper proposes a Dual Temporal Grounding-enhanced Video Dialog model (DTGVD), which utilizes the pre-trained visual-language model and excludes irrelevant video clips and dialogue history turns based on the predicted temporal area of each question-answer pairs, thus making the answers in video dialogue more accurate. We also choose accurately grounded turn-clip pairs as positive samples and gather other turn-clip pairs as negative samples in order to better illustrate the temporal relationship between the two modalities. The entire model is then trained using answer generation loss and contrastive learning loss. Experiments on two well-known benchmark datasets demonstrate the effectiveness of our proposed method. And experiments on various pretrained models verified the adaptability of the method."}, {"title": "Supplementary Material", "content": ""}, {"title": "6. Dataset", "content": "We conduct experiments on Audio-Visual Scene-Aware Dialog (AVSD) to evaluate the results. This dataset contains shared training/validation set, and two different test sets, namely AVSD@DSTC-7 and AVSD@DSTC-7. Details of the dataset are shown in Table 5."}, {"title": "7. In-depth Analysis", "content": ""}, {"title": "7.1. Temporal grounding performance", "content": "\"R@n, IoU = $\\mu$\" is a common metric for evaluating grounding performance. But IoU cannot fully demonstrate the validity of results in the task setting. For example, predicting full-length video as a positive region may also result in a relatively large IoU, but it cannot block irrelevant regions. Even if the indicators on the validation set are higher than those of other SOTA grounding models, it cannot fully demonstrate that our grounding results on the test set is good enough.\nThus, in Figure 4, we compare the groundtruth of temporal regions in the training dataset with the predicted ones in the test set of AVSD@DSTC-7 and AVSD@DSTC-8. Specifically, the horizontal axis represents the ratio of timestamp to video duration, and the vertical axis represents the percentage of frames in this ratio. For example, if the whole video length is 10s, the useful region is between 2s and 5s and the number of all frames is 10000, then the vertical coordinate value corresponding to the horizontal coordinates of 0.2 to 0.5 are added by 0.01%. As the test set does not have timestamp labels, if the predicted results are similar to the distribution of the groundtruth of training set, it signals that our grounding results are effective. As shown in Figure 4, the distributions of the two are indeed very similar."}, {"title": "7.2. Modality of contrastive selection", "content": "Upon realizing that contrastive learning can have a positive impact, it is easy to consider creating positive and negative text samples. For instance, unselected turns could be used as negative samples. However, this may not be beneficial for two reasons. Firstly, the aim in using contrastive learning is to improve temporal grounding accuracy. Nevertheless, incorrect positioning will only affect the selection of video clips, and the relationship between turns will remain unchanged. To put it simply, turns with high temporal overlap will still have a large IoU, even if the grounding is imprecise. Secondly, creating negative text examples may have an adverse effect on the results. In this task, only the relevant video clip, current question, and answer are highly correlated, not the history turns. In other words, relevant history turns improve the answer, but irrelevant turns should not be expected to make the answer worse. As shown in Table 6, we compare the performance of DTGVD with different contrast pairs, where T+/- means adding one more history turns as positive samples, i.e. k = 4, and utilizing the remaining irrelevant history turns as negative examples. The results indicate that V+/- improves the performance while T+/- has a negative impact."}, {"title": "8. Qualitative Analysis", "content": "We further perform qualitative analysis on the method to"}]}