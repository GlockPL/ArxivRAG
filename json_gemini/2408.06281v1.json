{"title": "MovieSum: An Abstractive Summarization Dataset for Movie Screenplays", "authors": ["Rohit Saxena", "Frank Keller"], "abstract": "Movie screenplay summarization is challeng-ing, as it requires an understanding of longinput contexts and various elements unique tomovies. Large language models have shownsignificant advancements in document summa-rization, but they often struggle with process-ing long input contexts. Furthermore, whiletelevision transcripts have received attentionin recent studies, movie screenplay summa-rization remains underexplored. To stimu-late research in this area, we present a newdataset, MovieSum,\u00b9 for abstractive summa-rization of movie screenplays. This datasetcomprises 2200 movie screenplays accompa-nied by their Wikipedia plot summaries. Wemanually formatted the movie screenplays torepresent their structural elements. Comparedto existing datasets, MovieSum possesses sev-eral distinctive features: (1) It includes moviescreenplays, which are longer than scripts ofTV episodes. (2) It is twice the size of previ-ous movie screenplay datasets. (3) It providesmetadata with IMDb IDs to facilitate access toadditional external knowledge. We also showthe results of recently released large languagemodels applied to summarization on our datasetto provide a detailed baseline.", "sections": [{"title": "Introduction", "content": "Large language models have shown significant im-provements in abstractive summarization in recentyears (Zhong et al., 2022; Zhu et al., 2021; Zhonget al., 2021; Zhang et al., 2022), aiming to pro-duce a concise and coherent summary of the inputdocument. However, these models often strugglewhen the input context is long, particularly whenthe relevant information is distributed across thedocument (Liu et al., 2023). To better understandthis phenomenon and to advance research, datasetsare needed that not only contain long-form doc-uments but also have the property that importantinformation is dispersed throughout the document.Movie screenplays have these characteristics: togenerate a faithful summary, an understanding ofcharacters and events across the entire length of thescreenplay is required.\nMore recently, narrative summarization researchhas focused on TV shows and books (Kry\u015bci\u0144skiet al., 2021; Moskvichev and Mai, 2023), with lessattention given to movie screenplays (Gorinski andLapata, 2015; Papalampidi et al., 2020). Notably,Chen et al. (2022) introduced a dataset of TV showtranscripts which has gained considerable interestand was included in a long document summariza-tion benchmark (Shaham et al., 2022). But unlikemovie screenplays, TV episode transcripts tendto be relatively short and predominantly comprisespoken dialogue with minimal scene or charac-ter descriptions. Additionally, they are not self-contained, as the events or characters from previ-ous episodes can be referred to. In contrast, moviescreenplays are structured documents with variousscreenplay elements such as scene headings, lo-cations, character names, dialogues and detaileddescriptions. These are written by screenwrit-ers and are characteristically formatted to denoteeach element.\nThe largest current movie screenplay dataset(Gorinski and Lapata, 2015, 2018) comprises 917automatically formatted screenplays (ScriptBase-j),with the most recent movie from 2013. We builtMovieSum, a new movie screenplay dataset forabstractive summarization, which consists of 2200movies, more than twice the size ScriptBase-j. Im-portantly, our new dataset has been formatted usinga professional script writing tool and paired withWikipedia plot summaries. Each movie is alsotagged with its IMDB IDs to facilitate the collec-tion of other external knowledge in rhw future. Thedataset consists of movies spanning a wide range"}, {"title": "The MovieSum Dataset", "content": "We present MovieSum, a movie screenplay ab-stractive summarization dataset that consists of2200 movie screenplay-summary pairs. All moviescreenplays in the dataset are in English."}, {"title": "Collection of Movie Screenplays", "content": "We collected movie screenplays from a range ofmovie screenplay websites. In total, we assembled5,639 movie screenplays documents in various textformat along with metadata of movie name, IMDBidentifier, and release year. If the IMDB identi-fier was missing, we extracted it using the IMDBdatabase. We then manually removed movies basedon two criteria. Firstly, we removed any duplicatemovie screenplays by using the movie names andrelease years to identify the duplicates. Secondly,we filtered out screenplays which did not have textor were incomplete."}, {"title": "Screenplay Formatting", "content": "Movie screenplays are structured documents withvarious script elements such as scene headings (alsoknown as slug lines), characters' names, dialogues,and scene descriptions (or actions). These elementshave specific markers based on spacing. Most ofthis formatting is lost when extracting text fromthese movie screenplay documents, making it chal-lenging to retrieve the elements using regular ex-pressions. To ensure the quality of the dataset, afterfiltering, we manually corrected the movie screen-play and formatted each movie screenplay usingCeltx,\u00b3 a professional screenplay writing tool. This"}, {"title": "Collection of Wikipedia Plot Summaries", "content": "To build a robust summarization dataset, it is nec-essary to collect high-quality human-written sum-maries. Similar to previous work (Ko\u010disk\u00fd et al.,2018), we collected Wikipedia plot summaries,which we found to be of high quality, helped by thefact that Wikipedia summaries follow a consistentset of guidelines for movie plot summaries.4\nTo collect the Wikipedia plot summary, we firstextracted the Wikipedia page of the movie usingthe movie name and year, then collected text underthe Plot section. We filtered out movies where theWikipedia page or the plot section was unavailable.\nThis process resulted in 2200 manually for-matted movie screenplays with correspondingWikipedia summaries."}, {"title": "Dataset Analysis", "content": "This results in a dataset consisting of 2200 manu-ally formatted movie screenplays along with theircorresponding summaries. The average length ofthe screenplays is 29k words, with an average sum-mary length of 717 words. Importantly, this datasetis twice the size of the previously available moviescreenplays dataset with formatted movie screen-plays (Gorinski and Lapata, 2015).  illustrates the genre distribution of the movies withinthe dataset and showcases the broad range of gen-res.  the distribution of release yearsis depicted, revealing that the movies span a widerange of years, with a substantial number of themoriginating in recent years.\nTo study the abstractiveness of the summary, wereport the percentage of novel n-gram in Table 1 asreported by Fabbri et al. (2021); Zhao et al. (2022).It shows that a high number of 3-gram and 4-gramare novel in summary and not present in the moviescreenplays implying high abstractiveness of the"}, {"title": "Comparison with Existing Datasets", "content": "We compare our dataset with various datasets inthe narrative domain, and the statistics are reportedin Table 2. These datasets include ScriptBase-j(Gorinski and Lapata, 2015), ScriptBase-alpha(Gorinski and Lapata, 2015), SummScreenFD(Chen et al., 2022), NarraSum (Zhao et al., 2022),NarrativeXL (Moskvichev and Mai, 2023), Nar-rativeQA (Ko\u010disk\u00fd et al., 2018), and BookSum(Kry\u015bci\u0144ski et al., 2021). Notably, BookSumand NarrativeXL have a longer average documentlength but comprise books, not screenplays. Summ-ScreenFD consists of TV show episode transcripts,which are much shorter in both document and sum-mary length. Importantly, SummScreen consists ofcommunity-contributed transcripts and primarilycomprises dialogues, unlike screenplays, which in-clude detailed scene descriptions. Also, TV showepisodes are not self-contained, as events or char-acters from previous episodes can be referenced.NarraSum contains plot summaries as documentsrather than actual screenplays, and therefore hasthe lowest average document length among thedatasets we compare. NarrativeQA includes bothbooks and movie screenplays, with books being no-tably lengthy, making the average document lengthcomparable to book datasets. However, it consistsof only 789 unformatted movie screenplays.\nBoth ScriptBase-j and ScriptBase-alphadatasets are close to our screenplay dataset.ScriptBase-j contains formatted screenplays,whereas ScriptBase-alpha comprises the unformat-ted raw text of screenplays. It is important to notethat ScriptBase-j is a subset of ScriptBase-alpha,which consists of 917 formatted screenplays.On the other hand, ScriptBase-alpha includesan additional 359 movies. Our work can beconsidered as the extension of ScriptBase-j as italso consists of formatted screenplays. At thesame time, we overcome two critical limitations ofSciptBase-j:\n(1) The formatting of the movie screenplay wasperformed automatically. Although it is easy todetect the scene heading based on rules and stringmatching, it is challenging to distinguish dialogues,character names, and scene descriptions. The workdoes not provide any details regarding the auto-"}, {"title": "Experiments", "content": "We evaluate the MovieSum dataset using severalbaselines and state-of-the-art neural abstractivesummarization models. We first report the Lead-Nbaseline, which simply outputs the first N tokensof the movie script as the movie summary. Wevaried the value N to understand the impact ofsummary length on performance and report resultsfor Lead-512, Lead-768, and Lead-1024. For theextractive baseline, we used TextRank (Mihalceaand Tarau, 2004), a graph-based unsupervised ex-tractive summarization method. For instruction-tuned large language models, we used Vicuna 1.513B 16K (Zheng et al., 2023), built on Llama-2(Touvron et al., 2023), and FLAN-UL2 (Tay et al.,2023; Wei et al., 2022) in a zero-shot setting. Forfine-tuned models with long inputs, we utilizedLongT5 (Guo et al., 2022), PEGASUS-X (Phanget al., 2023), and the Longformer Encoder-Decoder(LED) model (Beltagy et al., 2020). We fully fine-tuned these models and report results on the testset. The implementation details of the models are"}, {"title": "Results", "content": "Table 3 shows the summarization evaluation resultsusing ROUGE F1 (1/2/L) scores (Lin, 2004) andBERTScore (Zhang et al., 2019) on MovieSum.The Lead baseline performs better with a highernumber of words, achieving the best result with1024 words. This is not surprising, as the ROUGEmetric is known to give higher scores for longersummaries (Schluter, 2017). In the case of zero-shot, FLAN-UL2 8K performed substantially betterthan Vicuna 13B 16K. This confirms that a longercontext does not necessarily lead to attention to thefull context length. We also tested reducing the in-put context using TextRank, which only marginallyimproves the zero-shot performance for Vicuna 13model. To further utilize the full context, we also re-port results for moving window chunk-based zero-shot summarization and concatenating the gener-ated summaries. This performs better compared tousing only the 16K context length. The best perfor-mance was achieved with fine-tuned models withlonger context lengths. Pegasus-X, LongT5, andLED perform similarly well, with LED demonstrat-ing superior performance."}, {"title": "Analysis of Screenplay Structure", "content": "We analyze the importance of screenplay elements,dialogue and scene description, and their impact onsummarization performance. We selected the bestmodel from the full fine-tuned experiment and stud-"}, {"title": "Discussion and Conclusion", "content": "We introduce the MovieSum dataset, comprisingformatted and recent movie screenplays paired withWikipedia summaries. Our experiments demon-strate that it is a challenging dataset even for alarge language model with a long input length. Wehope that MovieSum will enable future researchin the area of movie screenplay understanding andabstractive summarization."}, {"title": "Limitations", "content": "Limitations of the work include that the datasetconsists of movie screenplays and their correspond-ing summaries only in English. Models trainedon MovieSum may not generalize well to multilin-gual summarization tasks or applications requiringcross-lingual understanding."}, {"title": "Ethics Statement", "content": "Large Language Models: This paper uses pre-trained large language models, which have beenshown to be subject to a variety of biases, to occa-sionally generate toxic language, and to hallucinatecontent. Therefore, the summaries generated usingour dataset should not be released without auto-matic filtering or manual checking.\nBias: Despite efforts to include a wide range ofmovies, the dataset may not fully represent the di-versity of cinematic styles, languages, or culturalcontexts. Models trained on MovieSum may there-fore exhibit biases towards the types of moviesincluded."}, {"title": "Implementation Details", "content": "For TextRank, we set the parameter words = 1024.\nWe randomly split the dataset into 1800/200/200as a train/val/test set to train the models. Weused the base variants of Pegasus-X, LongT5, andLED for fine-tuning. Each input sequence forthe movie is truncated to 16,384 tokens (includ-ing special tokens) to fit into the maximum inputlength of the model. We used AdamW as an op-timizer (\u03b2\u2081 = 0.9, \u03b22 = 0.99). For LED andLongT5, we used a learning rate of 2e-5 with acosine scheduler and a warmup ratio of 0.01. Weset the max_new_token to 1024 with greedy de-coding for all the experiments. For Pegasus-X, wefound that a learning rate of 5e-5 performed betterwith a linear warmup strategy and a warmup ratioof 0.01. All models were trained for 50 epochs, andthe best model was selected using the ROUGE-1on the validation set. The rest of the configurationsfor the models were kept as default. All the modelswere trained on A100 GPU with 80GB memory.We used the Huggingface evaluate library for theimplementation of the metrics."}, {"title": "Additional Statistics of Dataset", "content": "To further understand the abstractiveness of thesummaries we computed the coverage and den-sity of the summaries as discussed by Fabbri et al.(2021). The low density in Figure 2, indicates lowoverlap between the summary and the screenplays."}, {"title": "Prompt Template", "content": "For the zero-shot experiments in Section 4, we usedthe following prompt template:\nPrompt: Summarize the following movie script.\nMovie Script: {movie script text}\nSummary:"}, {"title": "Length Distribution", "content": "Figure 3 and 4 show the length distribution formovie scripts and their summaries across the train-ing set. The mean length of movie scripts is 29Kwords, and the average length of summaries is 714words."}, {"title": "Example of a Movie Screenplay", "content": "Figure 5 shows an example of a cleanly formattedscreenplay with distinct elements such as sceneheading, characters, and dialogues. All the files areconverted into XML using Celtx tool."}, {"title": "Sample of Movie Summary", "content": "Table 4 shows sample of generated summary of amovie using fine-tuned LED model (full-text)."}]}