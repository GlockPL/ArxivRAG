{"title": "Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code", "authors": ["Jungin Kim", "Shinwoo Park*", "Yo-Sub Han"], "abstract": "Code watermarking identifies AI-generated code by embedding patterns into the code during generation. Effective watermarking requires meeting two key conditions: the watermark should be reliably detectable, and the code should retain its original functionality. However, existing methods often modify tokens that are critical for program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can cause syntax errors or functional failures, limiting the practical use of watermarking. We present STONE, a method that preserves functional integrity by selectively inserting water-marks only into non-syntax tokens. By excluding tokens essential for code execution, STONE minimizes the risk of functional degradation.\nIn addition, we introduce CWEM, a comprehensive evaluation metric that evaluates watermarking techniques based on correctness, detectability, and naturalness. While correctness and detectability have been widely used, naturalness remains underexplored despite its importance. Unnatural patterns can reveal the presence of a watermark, making it easier for adversaries to remove. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. The results show that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java. Our code is available in https://github.com/ inistory/STONE-watermarking/.", "sections": [{"title": "1 Introduction", "content": "The rapid development of large language models (LLMs) has significantly improved the ability to generate human-like text and code (Nam et al., 2024; Kazemitabaar et al., 2023; Liu et al., 2023a; Sun et al., 2024; Guo et al., 2024a; Wang et al., 2024; Wei et al., 2023; Tang et al., 2023; Guo et al., 2024b). These advancements have unlocked new possibilities across various fields, including software development and automated code generation. However, this progress has introduced challenges in tracing the origin of generated code (Yang et al., 2023; Li et al., 2023). As LLM-generated code becomes increasingly indistinguishable from human-written code, researchers have explored watermarking techniques to address this issue.\nLLM watermarking involves embedding specific patterns into the generated output during the generation process to help identify its origin (Kirchenbauer et al., 2023; Lee et al., 2024). These patterns should be imperceptible to human reviewers but remain detectable through algorithmic analysis, enhancing the transparency and accountability of AI-generated content (Guo et al., 2024c; Hou et al., 2024).\nAmong various approaches, the green and red list-based watermarking is the most widely adopted (Kirchenbauer et al., 2023; Zhao et al., 2023; Lee et al., 2024; Lu et al., 2024; Guo et al., 2024c). This method divides token candidates into two lists and biases the selection process toward tokens in the green list. Detection involves measuring the proportion of green list tokens in the generated output. While effective for natural language tasks, this approach does not easily apply to code generation, where structural and functional correctness are crucial (Lee et al., 2024; Hoang et al., 2024). Changes to syntax-critical tokens can cause compilation errors or alter program behavior.\nCode watermarking identifies AI-generated code by embedding patterns into the code while maintaining functional correctness. Effective watermarking requires meeting two conditions: reliable watermark detectability and intact code correctness. However, many existing methods inadvertently modify tokens essential to program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can introduce syntax errors or change the behavior of the code, undermining its reliability. We propose STONE (Syntax TOkeN preserving codE watermarking), a method that selectively inserts watermarks into non-syntax tokens. By excluding tokens that are critical to code execution, STONE reduces the risk of functional degradation while embedding robust, detectable patterns.\nAdditionally, we introduce CWEM (Code Watermarking Evaluation Metric), a comprehensive framework for evaluating watermarking techniques. CWEM assesses performance across three key dimensions: correctness, detectability, and naturalness. While correctness and detectability have been the focus of prior research, naturalness is equally important. Watermarking methods that create unnatural code patterns make it easier for adversaries to detect and remove the embedded patterns. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. Our experiments demonstrate that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java."}, {"title": "2 Preliminary analysis", "content": "The previous SOTA approach SWEET (Lee et al., 2024) takes into account the structural and syntactic constraints by embedding watermarks only in high-entropy tokens, it ensures a minimal impact on the code quality. In our preliminary analysis, we group code tokens into several categories and compare their average entropy values to see if high-entropy tokens have a smaller impact on code quality. We consider two popular benchmarks, MBPP+ and HumanEval+, for the Python code generation and completion. We classify each Python code into one of five categories reported in Table 7 where any remaining tokens are put into the etc. category.\nWe calculate the entropy values of tokens across different categories and compare the average entropy values for each category. Following Lee et al. (2024), we calculate the Shannon entropy. Given a token sequence y = (y_0, y_1, ..., y_n), the entropy at each generation step t is computed based on the predicted probability distribution of the next token. The entropy H_t is defined as:\n$H_t = -\\sum_{i=1}^{|V|}P(y_t = v_i | y_{<t}) log P(y_t = v_i | y_{<t}),$   (1)\nwhere H_t is the entropy value at generation step t, V is the vocabulary of the language model, y_{<t} is the sequence of tokens generated before step t, and P(y_t = v_i | y_{<t}) is the probability assigned by the model to token v_i as the next token, respectively. The entropy H_t measures the uncertainty of the next token prediction. A high entropy value indicates that the model is uncertain and many tokens have comparable probabilities. Conversely, low entropy implies that one token has a dominant probability.\nFigure 1 presents bar charts displaying the mean entropy values for each token category. Our analysis shows that the average entropy of keyword tokens is the highest among all categories. In Python code, keywords are used in various contexts and positions. For example, control flow keywords like if, for, and while can appear in different parts of the code with highly diverse usage patterns. In contrast, operators and delimiters follow more predictable patterns due to the syntactic structure of the code. For instance, operators such as +, -, and * are used between operands, while delimiters like (and) appear in specific patterns, such as in function or class definitions. Additionally, type tokens are mainly used in limited contexts, such as variable declarations or type hinting, making their occurrence and patterns relatively predictable. Whitespace tokens, following the structure of the code, are even more predictable in their placement.\nSince keywords have high entropy, embedding watermarks in high-entropy tokens, as in SWEET, carries the risk of placing watermarks on keywords. Keywords are essential to the syntax and logic of the code, thus watermarking them could negatively impact the quality and functionality of the code. In comparison, the etc category, while less critical to the functionality of the code, has relatively high entropy. Therefore, embedding watermarks in etc tokens may be a more suitable approach for preserving code quality."}, {"title": "3 Approach: STONE", "content": "STONE watermarking is a robust watermarking approach designed for preserving code quality illustrated in Figure 2. This section introduces the watermark insertion and detection process of our proposed method. In Section 3.1, we detail how watermarks are inserted by selectively modifying non-syntactic tokens while keeping critical elements like keywords, operators, and delimiters unchanged to preserve functionality and structure. In Section 3.2, we explain how the watermark is detected by analyzing the green token distribution among non-syntax elements and computing a statistical z-score to confirm the presence of the watermark."}, {"title": "3.1 Inserting Watermarks", "content": "We define the keywords, whitespaces, types, delimiters, and operators presented in Table 7 as functionally important syntactic elements and embed watermarks only in tokens that do not belong to these categories. Algorithm 1 describes how STONE embeds watermarks during code generation. Given a tokenized prompt x and y:t, the tokens generated up to the previous time step, the language model fLM computes the initial probability distribution pt for the token to be generated at the current time step t. STONE samples a candidate token \u1ef9, based on this initial probability distribution. If the sampled token is not in the syntax element set, STONE divides the vocabulary V into a green list and a red list ac-\ncording to the green token ratio y. It then increases"}, {"title": "3.2 Detecting Watermarks", "content": "We assess how many green tokens are present in the code, and if this value surpasses a certain threshold, we conclude that the code was generated by an LLM. When identifying potential green tokens, we focus only on non-syntactic elements (i.e., etc tokens). Algorithm 2 details the watermark detection process for STONE scenario. $N_E$ represents the number of etc tokens in the code, and $N_E^G$ denotes the number of green tokens among them. We replicate the division of the vocabulary into green and red lists at each time step during code generation to determine whether a specific token is a green token. If the z-score exceeds the predefined threshold $Z_{threshold}$, the code is determined to be generated by an LLM. The purpose of embedding and detecting watermarks is to determine whether a given piece of code was generated by an LLM."}, {"title": "4 Code Watermarking Evaluation Metric", "content": "Existing code watermarking research lacks a unified evaluation metric that simultaneously considers functional correctness, detectability and natural-ness (Lee et al., 2024; Yang et al., 2024; Li et al., 2024; Guan et al., 2024). They have typically focused one or two of these properties, leading to incomplete assessments and conflicting interpretations. By integrating functional correctness, detectability, and naturalness, we propose CWEM (Code Watermarking Evaluation Metric), a flexible metric that balances each factor using weights \u03b1,\u03b2 and y satisfying a + \u03b2 + y = 1. In practice, these coefficients reflect the relative importance of functional correctness (a), watermark detectability (\u03b2) and fluency or naturalness (y). For instance, systems that cannot risk functional errors can assign a larger a, while environments prioritizing watermark detection may emphasize \u03b2.\nCWEM = a \u00b7 Correctness(C_wm) + \u03b2 \u00b7 Detectability(C_wm) + y \u00b7 Naturalness(C_wm, C),   (2)\ncorrectness is measured through the pass@k change before and after watermarking (Section 4.1), detectability is quantified using z-score and AUROC metric (Section 4.2) and naturalness is assessed using perplexity (Section 4.3)."}, {"title": "4.1 Correctness", "content": "Ensuring correctness is crucial to prevent watermarking from compromising the functionality of the code. We measure functionality preservation using the pass@k metric. Specifically, we adopt the unbiased version of pass@k (Chen et al., 2021). In this approach, k samples per problem are generated, and the number of correct samples, c, is counted as the number of watermarked code samples in the set C_wm that pass all unit tests.\nThis metric estimates pass@k with the formula:\nCorrectness(C_{wm}) = E_{C_{wm}} [1 - {\\binom{n-c}{k}}/{\\binom{n}{k}}],   (3)\nwhere $E_{C_{wm}}$ denotes the expectation over the set of watermarked code samples $C_{wm}$, n is the total number of generated code samples for a given problem, c is the number of correct code samples within the set, $\\binom{n}{k}$ denotes the number of ways to choose k code samples from the total n samples, and $\\binom{n-c}{k}$ represents the number of ways to choose k code samples from the incorrect samples only (i.e., excluding the c correct samples). The term $1-{\\binom{n-c}{k}}/{\\binom{n}{k}}$ calculates the probability that at least one correct code sample is selected when k samples are drawn from the total n samples. The overall correctness metric is obtained as the expected value of this probability over the set $C_{wm}$."}, {"title": "4.2 Detectability", "content": "Detectability measures how effectively we can identify the presence of a watermark in generated code. We assess the detectability by computing a z-score based on the distribution of green-list tokens in the generated text. The classification performance is then evaluated using AUROC (Area Under the Receiver Operating Characteristic Curve)."}, {"title": "Z-score: Green Token Ratio", "content": "For a given code token sequence $C_{wm}$ = {$C_0, C_1, ..., c_r$}, the z-score is computed as:\nz(C_{wm}) = {|C_{wm}|_G - \\gamma T}/{\\sqrt{T \\gamma(1 - \\gamma)}},   (4)\nwhere $|C_{wm}|_G$ is the number of green-list tokens in the sequence $C_{wm}$, T is the total number of tokens in the sequence, and \u03b3 is the expected proportion of green tokens in a non-watermarked sequence. The denominator represents the standard deviation under the binomial distribution assumption. This z-score quantifies the deviation of the observed green token frequency from its expected value, enabling the detection of watermarked text."}, {"title": "LLM-Generated Code Detection Using Green Token Ratio", "content": "We evaluate classification performance using AUROC (Area Under the Receiver Operating Characteristic Curve), which quantifies how well we can distinguish between human-written code $C_H$ and watermarked code $C_{wm}$ based on their z-scores.\nWe treat z($C_H$) as scores for the negative class (human-written) and z($C_{wm}$) as scores for the positive class (watermarked). The ROC curve is obtained by varying a decision threshold \u03c4 computing the corresponding True Positive Rate (TPR) and False Positive Rate (FPR). TPR is the fraction of watermarked sequences correctly identified as watermarked.\nTPR(\u03c4) = {\\sum_i 1(z(C_{wm}^i) > \u03c4)}/{|C_{wm}|},   (5)\nFPR is the fraction of human written sequences incorrectly classified as watermarked.\nFPR(\u03c4) = {\\sum_i 1(z(C_H^i) > \u03c4)}/{|C_H|},   (6)\nFor each threshold \u03c4, a sequence is classified as watermarked if its computed z-score exceeds \u03c4. We define overall detectability across all possible thresholds as follows:\nDetectability(C_{wm}, C_H) = \\int_{-\\infty}^{\\infty} TPR(\u03c4) {dFPR(\u03c4)}/{d\u03c4} d\u03c4,   (7)\nThis integral computes the AUROC, which quantifies the ability of the watermarking method to differentiate between human-written and watermarked code. A higher AUROC value (closer to 1) indicates stronger detectability, whereas an AUROC of 0.5 implies that the distributions of human-written and watermarked code are indistinguishable."}, {"title": "4.3 Naturalness", "content": "A watermark that significantly alters token distributions can be identified and removed through preprocessing. We adopt perplexity as a direct measure of token-level fluency, which aligns closely with how language models predict code tokens. Other code-style metrics exist, but we rely on perplexity because it has been widely recognized in language modeling research as a reliable indicator of naturalness. A well-designed watermark minimizes perplexity shifts, ensuring that watermarked code remains indistinguishable from non-watermarked code.\nFor a given set of watermarked code sequences Cwm, perplexity is computed as:\nPPL(C_{wm}) = {1}/{|C_{wm}|} \\sum_{j=1}^{|C_{wm}|} exp [ {1}/{N_j} \\sum_i log P(w_i^{(j)} | w_{<i}^{(j)}) ],   (8)\nwhere $N_j$ is the number of tokens in the j-th sample of $C_{wm}$, and P(w_i^{(j)} | w_{<i}^{(j)}) represents the probability of token w_i^{(j)} given its preceding context. An effectively concealed watermark minimizes perplexity shifts, ensuring imperceptibility. We define the naturalness metric as:\nNaturalness(C_{wm}, C) = 1 - {|PPL(C_{wm}) - PPL(C)|}/{PPL(C)},   (9)\nThis metric quantifies the perplexity-based difference between LLM-generated code with Cwm and without C a watermark. The metric normalizes the absolute perplexity difference by dividing it by the perplexity of non-watermarked code C, ensuring invariance. A value of 1 indicates that C and Cwm have identical perplexities, implying minimal divergence despite watermark insertion."}, {"title": "5 Results and Analysis", "content": "5.1 Experiment Settings\nDatasets For our main experiments, we use the HumanEval+ and MBPP+ datasets (Liu et al., 2023b) for Python code completion and generation tasks, following prior work (Lee et al., 2024). In addition, we incorporate the HumanEvalPack (Muennighoff et al., 2024) to evaluate the performance of our approach across three programming languages: Python, C++, Java. shows the data statistics of the datasets we used in our experiments.\nBaselines We consider the following three baselines for comparison. (1) KGW (Kirchenbauer et al., 2023) generates text by dividing the vocabulary into a green and red list at each time step of token generation, increasing the probability of generating tokens from the green list. (2) EWD (Lu et al., 2024) leverages entropy-based token weighting during detection, giving more influence to high-entropy tokens, thus enhancing detection in texts with varying entropy distributions. (3) SWEET (Lee et al., 2024) selectively embeds watermarks in high-entropy tokens, addressing detection challenges posed by the low entropy in code. We exclude CodeIP from our baselines as it requires additional training for watermark insertion, while our approach and the selected baselines are training-free methods.\nBase Model Our base model is Qwen2.5-Coder (Hui et al., 2024), which is an advanced code generation model. According to the EvalPlus leaderboard\u00b9, Qwen2.5-Coder ranks third, following GPT-01 models. Given its strong performance on the leaderboard, we selected Qwen2.5-Coder as our base LLM for this study.\nEvaluation Metrics We conduct experimental evaluations using our proposed evaluation metric CWEM. The evaluation framework specifically assessed three critical aspects: functional correctness to ensure proper operation (Correctness), detectability to verify watermark identification (Detectability), and naturalness to confirm the imperceptibility of the watermark (Naturalness). We measure correctness score using the average of pass@k (k=1,5).\nImplementation Details We conduct experiments on MBPP+ and HumanEval+ under the instruction-tuned Qwen2.5-Coder 7B model. For the hyperparameter optimization, we set d=1.0, y=0.5 for MBPP+ and 8=5.0, y=0.5 for HumanEval+ and HumanEvalPack-C++ and Java, from \u03b4=[0.5, 1.0, 2.0, 3.0, 4.0], \u03b3=[0.25, 0.5, 0.75]. A detailed analysis of the impact of varying d and y across multiple values is provided in Section C. All experiments are conducted on NVIDIA A6000 GPU."}, {"title": "5.2 Research Questions and Findings", "content": "We formulate three key research questions and conduct a thorough analysis to address them:\n\u2022 RQ1: How well does STONE preserve code functionality?\n\u2022 RQ2: How superior is STONE to the baseline when considering functional correctness, detectability, and naturalness comprehensively?\n\u2022 RQ3: How does the time required for watermark insertion and detection in STONE?\nThese three research questions guide our comprehensive analysis of STONE focusing on the following aspects: 1) Functionality Preservation: Assessing whether STONE can insert watermarks without altering the original functionality. 2) Detectability and Naturalness: Evaluating whether the watermark inserted by STONE enables reliable detection of LLM-generated code while preserving a natural and readable code appearance. 3) Performance Overhead: Measuring the computational overhead involved in watermark insertion and detection. This analysis provides an understanding of the performance of STONE across correctness, detectability, naturalness, and efficiency.\nRQ1 We assess whether STONE can embed watermarks without disrupting the functionality of the original code. The experimental results in Tables 2 and 3 demonstrate that STONE improves correctness by 7.57% compared with SWEET across all benchmarks. A detailed token-level analysis (Appendix A) reveals that entropy-based token selection of SWEET inadvertently targets syntax-related elements such as delimeters (49.29), whitespaces (38.17%), keywords (9.44%), types (3.17%), and operators (2.78%)\u2014which are critical for preserving functionality. By contrast, STONE selectively avoids altering these syntactic tokens, thereby preventing disruptions such as parsing errors or incorrect control flows.\nTable 6 provides examples of syntax tokens used as watermark tokens in SWEET. These examples show that modifying such tokens can disrupt code functionality. For example, changing keywords like 'True' or 'False' affects boolean logic in conditionals, which may lead to incorrect branching. Altering delimiters like colons (':') or brackets ('', '[]') can trigger parsing errors. Replacing \u2018None' can interfere with object initialization and function returns, causing failures. Modifications to operators such as '=', '-', or '%' can yield incorrect outputs. STONE avoids these issues by leaving syntax elements unchanged, thereby preserving functional correctness in the generated code.\nRQ2 STONE achieves the highest detection accuracy while maintaining code naturalness. KGW adjusts token selection across all tokens, resulting in small differences between watermarked and non-watermarked code that lower detection performance. Although EWD builds on KGW with entropy-based weighting, it still applies watermarking to every token, which leads to similar issues. In contrast, STONE selectively embeds watermarks only in non-syntax elements, ensuring that the watermark signal remains distinct and unambiguous. This ensures improved detection performance.\nKGW and EWD report high naturalness scores by evenly adjusting token probabilities. In comparison, STONE keeps the code syntax unchanged and adjusts only non-syntax tokens. This strategy allows watermark embedding without altering the code structure or functionality, making STONE more natural than SWEET. SWEET modifies high-entropy tokens\u2014including syntax elements such as keywords and operators-which shifts token probabilities and eases watermark detection.\nSWEET achieves naturalness scores similar to STONE in MBPP+ and HumanEval+, likely because Python syntax tolerates small changes. However, modifications of syntax elements may create detectable patterns in other more complex programming languages like C++ and Java and affect readability. By preserving syntax and embedding watermarks only in non-syntax tokens, STONE minimizes changes in token probabilities while maintaining readability. Overall, STONE improves watermark classification without compromising code naturalness.\nRQ3 We analyze the computational overhead of watermarking by comparing generation time and detection time. Generation time refers to the duration of producing watermarked code, while detection time measures how long it takes to verify a watermark. summarizes the results for each baseline. All methods show similar generation times, which indicates that watermark insertion does not significantly delay code generation.\nIn contrast, STONE detects watermarks on average 86% faster than SWEET and EWD. This difference arises from the detection process. Both SWEET and EWD perform entropy calculations that require computing token probability distributions, increasing detection overhead. STONE avoids these computations by verifying the watermark through a predefined green list of non-syntax elements. As a result, STONE reduces detection time while maintaining high detectability."}, {"title": "6 Related Work", "content": "Research on code watermarking remains limited due to the unique challenges posed by structural constraints in code, particularly in maintaining functional correctness, detectability, and naturalness. Unlike natural language, code must adhere to strict syntactic and semantic rules, making the development of effective watermarking techniques more complex.\nLee et al. (2024) proposed SWEET, a method that selectively embeds watermarks in high-entropy tokens. While SWEET demonstrated improvements in detection capability and code quality, our preliminary analysis revealed that approximately 12.6% of these high-entropy tokens correspond to syntax elements such as reserved keywords. Modifying these critical tokens can disrupt program logic, resulting in degraded functional correctness.\nGuan et al. (2024) introduced CodeIP, a grammar-guided watermarking approach by utilizing type prediction to maintain syntactic validity. This method ensured that the generated code remained grammatically correct. However, the reliance on type prediction requires significant computational resources, limiting its practicality for large-scale code generation tasks.\nWe address the limitations observed in SWEET and CodeIP by explicitly skipping watermark insertion for syntax-critical tokens. Our approach ensures that essential structural elements, like keywords and operators, remain unchanged, thereby preserving functional correctness without additional computational overhead.\nACW (Li et al., 2024) and SrcMarker (Yang et al., 2024) adopted watermarking techniques based on code transformations, applying semantic-preserving modifications and variable name substitutions. Although these methods maintained program functionality, the introduced changes often result in unnatural code patterns, making the watermark more susceptible to removal by adversaries."}, {"title": "7 Conclusion", "content": "We propose STONE, a syntax-aware watermarking method that preserves the original functionality of code. Unlike existing methods that apply watermarks without considering syntax tokens-often causing syntax errors or functional failures-STONE restricts watermarking to non-syntax tokens, maintaining code structure and enhancing detection reliability. Additionally, we introduce CWEM, a comprehensive evaluation metric that assesses correctness, detectability, and naturalness. While correctness and detectability are well-studied, naturalness has been largely overlooked. By incorporating naturalness into the evaluation, CWEM provides a more balanced assessment of watermarking techniques. Experimental results show that STONE consistently outperforms existing methods across three programming languages, achieving higher CWEM scores. These findings demonstrate that syntax-aware watermarking improves detection performance while preserving code functionality, making it more practical for real-world applications."}, {"title": "Limitations", "content": "This study introduces the STONE method, which preserves key functional tokens in code to maintain correctness. However, the current approach is not specifically optimized to enhance the naturalness of code during watermark insertion. Experimental results indicate that the naturalness score is lower than those achieved with methods such as KGW and EWD. Future work should explore refined token selection strategies to address this aspect.\nIn our study, CWEM does not incorporate a measure for evaluating the persistence of the watermark when modifications, refactoring, or token substitutions alter the code. Future investigations should develop quantitative measures to assess watermark robustness under these conditions.\nWe conduct experiments using the Qwen2.5-Coder model across three programming languages: Python, C++, and Java. The evaluation does not extend to additional large language model architectures, training environments, or parameter scales. Future studies should broaden the evaluation to encompass other model architectures, training strategies, and parameter scales in order to develop the scope and limitations of the approach."}]}