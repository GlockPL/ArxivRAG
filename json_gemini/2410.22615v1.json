{"title": "COGS: Model Agnostic Causality Constrained Counterfactual Explanations using goal-directed ASP2", "authors": ["Sopam Dasgupta", "Joaqu\u00edn Arias", "Elmer Salazar", "Gopal Gupta"], "abstract": "Machine learning models are increasingly\nused in critical areas such as loan approvals\nand hiring, yet they often function as black\nboxes, obscuring their decision-making pro-\ncesses. Transparency is crucial, as individ-\nals need explanations to understand de-\ncisions, primarily if the decisions result in\nan undesired outcome. Our work intro-\nduces CoGS (Counterfactual Generation with\ns(CASP)), a model-agnostic framework ca-\npable of generating counterfactual explana-\ntions for classification models. CoGS lever-\nages the goal-directed Answer Set Program-\nming system s(CASP) to compute realistic\nand causally consistent modifications to fea-\nture values, accounting for causal dependen-\ncies between them. By using rule-based ma-\nchine learning algorithms (RBML), notably\nthe FOLD-SE algorithm, CoGS extracts the\nunderlying logic of a statistical model to gen-\nerate counterfactual solutions. By tracing a\nstep-by-step path from an undesired outcome\nto a desired one, CoGS offers interpretable\nand actionable explanations of the changes\nrequired to achieve the desired outcome. We\npresent details of the CoGS framework along\nwith its evaluation.", "sections": [{"title": "1 Introduction", "content": "Predictive models are widely used in automated\ndecision-making processes like job-candidate filtering\nand loan approvals. However, these models often func-\ntion as black boxes, making it difficult to understand\ntheir internal reasoning for decision-making. The de-\ncisions made by these models can have significant\nconsequences, leading individuals to seek satisfactory\nexplanations, particularly for unfavourable outcomes.\nWhether automated systems or humans make deci-\nsions, this desire for transparency is crucial. Explain-\ning these decisions presents a significant challenge, par-\nticularly when users want to know what changes are\nrequired to flip an undesired (negative) decision into a\ndesired (positive) one.\nIn the approach of Wachter et al. (2018), counterfac-\ntuals are employed to explain the reasoning behind a\nprediction by a machine learning model. Addition-\nally, counterfactuals help answer the question: \"What\nchanges should be made to input attributes or fea-\ntures to flip an undesired outcome to a desired one?\"\nas shown by Byrne (2019). Statistical techniques were\nused to find counterfactuals-based on the proximity of\npoints in the N-dimensional feature space. However,\nthese counterfactual based approaches by Ustun et al.\n(2019), Karimi et al. (2020), Tolomei et al. (2017), and\nRussell (2019) assume feature independence and do\nnot consider the causal dependencies between features.\nThis results in unrealistic counterfactuals. MINT by\nKarimi et al. (2021) utilized causal relations between\nfeatures to generate causally consistent counterfactu-\nals. However, these counterfactuals had limited utility\nas they followed from a set of simultaneous actions,\nwhich may not always be possible in the real world.\nWe present the Counterfactual Generation with\ns(CASP) (COGS) framework, which generates coun-\nterfactual explanations for any classification model\nby using rule-based machine learning (RBML) algo-\nrithms such as FOLD-SE by Wang and Gupta (2024).\nCompared to the work by Wachter et al. (2018),\nCoGS computes causally compliant counterfactuals\nfor any classification model whether statistical or\nrule-based using Answer Set Programming (ASP)\nand rule-based machine learning (RBML) algorithms."}, {"title": "2 Background and Related Work", "content": "Explanations help us understand decisions and inform\nactions. Wachter et al. (2018) advocated using coun-\nterfactual explanations (CFE) to explain individual\ndecisions, offering insights on achieving desired out-\ncomes. For instance, a counterfactual explanation for\na loan denial might state: If John had good credit,\nhis loan application would be approved. This involves\nimagining alternate (reasonably plausible) scenarios\nwhere the desired outcome is achievable.\nFor a binary classifier given by $f : X \\rightarrow {0, 1}$, we de-\nfine a set of counterfactual explanations $\\hat{x}$ for a factual\ninput $x \\in X$ as $CF_f(x) = {\\hat{x} \\in X|f(\\hat{x}) \\neq f(x)}$. This\nset includes all inputs $\\hat{x}$ leading to different predictions\nthan the original input $x$ under $f$.\nVarious methods were proposed by Wachter et al.\n(2018), Ustun et al. (2019) and Karimi et al. (2020)\nthat offered counterfactual explanations. However,\nthey assumed feature independence. This resulted in\nunrealistic counterfactuals as in the real world, causal\ndependencies exist between features."}, {"title": "2.1 Counterfactual Reasoning", "content": null}, {"title": "2.2 Causality and Counterfactuals", "content": "Causality relates to the cause-effect relationship\namong variables, where one event (the cause) di-\nrectly influences another event (the effect). In the\ncausal framework by Pearl (2009), causality is defined\nthrough interventions. An intervention involves ex-\nternal manipulation of $P$ (by the do-operator), which\nexplicitly changes $P$ and measures the effect on $Q$.\nThis mechanism is formalized using Structural Causal\nModels (SCMs), which represent the direct impact of\n$P$ on $Q$ while accounting for all confounding factors.\nSCMs allow us to establish causality by demonstrating\nthat an intervention on $P$ leads to a change in $Q$.\nIn SCM-based approaches such as MINT by Karimi\net al. (2021), capturing the downstream effects of inter-\nventions is essential to ensure causally consistent coun-\nterfactuals. By explicitly modelling counterfactual de-\npendencies, SCMs help in generating these counter-\nfactuals. While MINT produces causally consistent\ncounterfactuals, it typically proposes a simultaneous\nset of minimal interventions/actions. When applied\ntogether, this set of interventions produces the coun-\nterfactual solution. However, executing this set of in-\nterventions simultaneously in the real world might not\nbe possible. The order matters as some interventions\nmay depend on others or require different time-frames\nto implement. Thus, although MINT identifies the\nminimal set of interventions needed to obtain a coun-\nterfactual, the order of these interventions is crucial\nin the real world. Sequence planning is necessary to\naccount for dependencies and practical constraints."}, {"title": "2.3 Answer Set Programming (ASP)", "content": "Answer Set Programming (ASP) is a paradigm\nfor knowledge representation and reasoning described\nin Brewka et al. (2011), Baral (2003), Gelfond and\nKahl (2014). It is widely used in automating com-\nmonsense reasoning. ASP inherently supports non-\nmonotonic reasoning that allows conclusions to be re-\ntracted when new information becomes available. This\nis helpful in dynamic environments where inter-feature\nrelationships may evolve, allowing ASP to reason effec-\ntively in the presence of incomplete or changing knowl-\nedge. In ASP, we can model the effect of interventions\nby defining rules that encode the relationship between\nvariables. While ASP does not explicitly use a do-\noperator, it can simulate interventions through causal\nrules. For example, rules in ASP specify that when $P$\nis TRUE, $Q$ follows, and similarly, when $P$ is FALSE,\n$Q$ is also FALSE: $(P \\Rightarrow Q) \\land (\\neg P \\Rightarrow \\neg Q)$. By chang-\ning $P$ (representing an intervention), ASP can simulate\nthe effect of this change in $Q$, thereby capturing causal\ndependencies similar to SCMs. We employ ASP to en-"}, {"title": "3 Overview", "content": "When an individual (represented as a set of features)\nreceives an undesired negative decision (loan denial),\nthey can seek necessary changes to flip it to a pos-\nitive outcome. CoGS automatically identifies these\nchanges. For example, if John is denied a loan (initial\nstate i), CoGS models the (positive) scenarios (goal\nset G) where he obtains the loan. The query goal\n'?- reject_loan(john)' represents the prediction of\nthe classification model regarding whether John's loan\nshould be rejected, based on the extracted underlying\nlogic of the model used for loan approval. The (neg-\native) decision in the initial state i should not apply\nto any scenario in the goal set G. The query goal '?- reject_loan(john)' should be True in the initial state\ni and False for all goals in the goal set G. The prob-\nlem is to find a series of interventions, namely, changes\nto feature values, that will take us from i to $g\\in G$."}, {"title": "3.1 The Problem", "content": null}, {"title": "3.2 Solution: CoGS Approach", "content": "The CoGS approach casts the solution as traversing\nfrom an initial state to a goal state, represented as\nfeature-value pairs (e.g., credit score: 600; age: 24).\nIn CoGS, ASP is used to model interventions and gen-\nerate a step-by-step plan of interventions, tracing how\nchanges propagate from i to $g\\in G$. This generation\nof a plan is a planning problem. However, unlike the\nstandard planning problem, the interventions that take\nus from one state to another are not mutually inde-\npendent. This approach ensures that each interven-\ntions respects casual dependencies between variables,\noffering an explanation of how one action leads to the\nnext. The step-wise approach of CoGS contrasts with\napproaches like MINT, which applies all interventions\nsimultaneously and is useful for understanding the dy-\nnamic changes in systems with complex causal rela-\ntionships.\nThere can be multiple goal states (G) that represents\nthe positive outcome. The objective is to turn a neg-\native decision (initial state i) into a positive one (goal\nstate g) through necessary changes to feature values,\nso that the query goal '?- not reject_loan(john)'\nwill succeed for $g\\in G$.\nCoGS models two scenarios: 1) the negative outcome\nworld (e.g., loan denial, initial state i), and 2) the pos-\nitive outcome world (e.g., loan approval, goal state g)\nachieved through specific interventions. Both states\nare defined by specific attribute values (e.g., loan ap-\nproval requires a credit score \u2265 600). CoGS symbol-\nically computes the necessary interventions to find a\npath from i to g, representing a flipped decision.\nIn terms of ASP, the problem of finding interventions\ncan be cast as follows: given a possible world where\na query succeeds, compute changes to the feature val-\nues (while taking their causal dependencies into ac-\ncount) that will reach another possible world where\nnegation of the query will succeed. Each of the in-\ntermediate possible worlds we traverse must be viable\nworlds with respect to the rules. We use the s(CASP)\nquery-driven predicate ASP system for this purpose.\ns(CASP) automatically generates dual rules, allow-\ning us to execute negated queries (such as \u2018?- not\nreject_loan/1\u2019) constructively.\nWhen the decision query (e.g., '?- reject_loan/1')\nsucceeds (negative outcome), CoGS finds the state\nwhere this query fails (e.g., '?- not reject_loan/1'\nsucceeds), which constitutes the goal state g. In terms\nof ASP, the task is as follows: given a world where\na query succeeds, compute changes to feature val-"}, {"title": "4 Methodology", "content": "We next outline the methodology used by CoGS to\ngenerate paths from the initial state (negative out-\ncome) to the goal state (positive outcome). Unlike tra-\nditional planning problems where actions are typically\nindependent, our approach involves interdependent ac-\ntions governed by causal rules C. This ensures that the\neffect of one action can influence subsequent actions,\nmaking interventions realistic and causally consistent.\nNote that the CoGS framework uses the FOLD-SE\nRBML algorithm to automatically compute causal de-\npendency rules. These rules have to be either veri-\nfied by a human, or commonsense knowledge must be\nused to verify them automatically. This is important,\nas RBML algorithms can identify a correlation as a\ncausal dependency. CoGS uses the former approach.\nWe next define specific terms.\nDefinition 1 (State Space (S)). S represents\nall combinations of feature values. For domains\n$D_1,..., D_n$ of the features $F_1,..., F_n$, S is a set of pos-\nsible states s, where each state is defined as a tuple of\nfeature values $V_1, ..., V_n$.\n$s\\in S \\text{ where } S = \\{(V_1, V_2, ..., V_n) | \\\\ V_i \\in D_i, \\text{ for each i in 1, ..., n} \\}$\nE.g., an individual John: $S$ = (31 years, $5000,\n$40000, 599 points), where $s\\in S$.\nDefinition 2 (Causally Consistent State Space\n($S_c$)). $S_c$ is a subset of S where all causal rules are\nsatisfied. C represents a set of causal rules over the\nfeatures within a state space S. Then, $O_c : P(S) \\rightarrow\nP(S)$ (where P(S) is the power set of S) is a function\nthat defines the subset of a given state sub-space $S' \\subseteq S$\nthat satisfy all causal rules in C.\n$0_c(S') = \\{s \\in S' | s \\text{ satisfies all causal rules in C} \\}$\n$S_c = 0_c(S)$"}, {"title": "4.1 Algorithm to Extract Decision Rules", "content": "We first describe the algorithm for extracting the un-\nderlying logic in the form of rules for the classification\nmodel that provides the undesired outcome. By using\nthis extracted logic or rules, we can generate a path to\nthe counterfactual solution g.\nThe function 'extract_logic' extracts the underly-\ning logic of the classification model used for decision-\nmaking. Our CoGS framework applies specifically to\ntabular data, so any classifier handling tabular data\ncan be used. Algorithm 1 provides the pseudocode for\n'extract_logic', which takes the original classification\nmodel M, input data H, and a RBML algorithm R\nas inputs and returns Q, the underlying logic of the\nclassification model.\nQ represents the decision rules responsible for generat-\ning the undesired outcome. The algorithm first checks\nif the classification model M is rule-based. If yes,\nwe set Q = M and return Q. Otherwise, the corre-\nsponding labels for the input data are predicted using\nM. These predicted labels, along with the input data\nH, are then used to train the RBML algorithm R.\nThe trained RBML algorithm represents the underly-\ning logic of M, and we set Q = R, returning Q as the\nextracted decision rules."}, {"title": "4.2 Algorithm to Obtain the Counterfactual", "content": "We next describe our algorithm to find the goal states\nand compute the solution paths. The algorithm makes\nuse of the following functions: (i) not_member:\nchecks if an element is: a) not a member of a list,\nand b) Given a list of tuples, not a member of any tu-\nple in the list. (ii) drop_inconsistent: given a list of\nstates [$s_0, ..., s_k$] and a set of Causal rules C, it drops all\nthe inconsistent states resulting in a list of consistent\nstates with respect to C. (iii) get_last: returns the\nlast member of a list. (iv) pop: returns the last mem-\nber of a list. (v) is_counterfactual: returns True\nif the input state is a causally consistent counterfac-\ntual solution. (vi) intervene: performs interventions/\nmakes changes to the current state through a series of\nactions and returns a list of visited states. The inter-\nventions are causally compliant. Further details are\navailable in the supplement."}, {"title": "4.2.1 Find Path:", "content": "Function 'find_path' implements the Solution Path\nP of Definition 9. Its purpose is to find a path to the\ncounterfactual state. Algorithm 2 provides the pseudo-\ncode for 'find_path', which takes as input an Initial\nState i, a set of Causal Rules C, decision rules Q,\nand actions A. It returns a path to the counterfac-\ntual state/goal state $g\\in G$ for the given i as a list\n'visited_states'. Unrealistic states are removed from\n\u2018visited_states' to obtain a \u2018candidate_path\u2019.\nInitially, s = i. The function checks if the current state\ns is a counterfactual. If s is already a counterfactual,\n'find_path' returns a list containing s. If not, the\nalgorithm moves from s = i to a new causally consis-\ntent state s' using the 'intervene' function, updating\n'visited_states' with s'. It then checks if s' is a counter-\nfactual using 'is_counterfactual'. If True, the algo-"}, {"title": "4.2.2 Discussion:", "content": "A few points should be highlighted: (i) Certain fea-\nture values may be immutable or restricted, such as\nage cannot decrease or credit score cannot be directly\naltered. To respect these restrictions, we introduce\nplausibility constraints. These constraints apply to the\nactions in our algorithms, ensuring realistic changes to\nthe features. Since they do not add new states but re-\nstrict reachable states, they are represented through\nthe set of available actions in Algorithms 2, (ii) Simi-\nlarly, CoGS has the ability to specify the path length\nfor candidate solutions. Starting with a minimal path\nlength of 1, CoGS can identify solutions requiring only\na single change. If no solution exists, CoGS can incre-\nmentally increase the path length until a solution is\nfound. This ensures that the generated counterfac-\ntuals are both minimal and causally consistent,\nenhancing their practicality and interpretability. This\nis achieved via constraints on path length."}, {"title": "4.3 Soundness", "content": "Definition 10 (CFG Implementation). When Algo-\nrithm 2 is executed with the inputs: Initial State i\n(Definition 4), States Space S (Definition 1), Set of\nCausal Rules C (Definition 2), Set of Decision Rules\nQ (Definition 3), and Set of Actions A (Definition\n5), a CFG problem ($S_c, S_Q, I, \\delta$) (Definition 7) with\ncausally consistent state space $S_c$ (Definition 2), De-\ncision consistent state space $S_Q$ (Definition 3), Initial\nState i (Definition 4), the transition function $\\delta$ (Defi-\nnition 6) is constructed.\nDefinition 11 (Candidate path). Given the counter-\nfactual ($S_c, S_Q, I, \\delta$) constructed from a run of algo-\nrithm 2, the return value (candidate path) is the resul-\ntant list obtained from removing all elements contain-\ning states $s' \\notin S_c$.\nDefinition 10 maps the input of Algorithm 2 to a CFG\nproblem (Definition 7). Candidate path maps the re-\nsult of Algorithm 2 to a possible solution (Definition 9)\nof the corresponding CGF problem. From Theorem 1\n(proof in supplement), the candidate path (Definition\n11) is a solution to the corresponding CFG problem\nimplementation (Definition 10)."}, {"title": "5 Experiments", "content": "We have split the tasks into two steps: 1) Find the\nrule based equivalent of the original model; and 2)\nUsing the rules obtained, find the path to the counter-\nfactual. Further details on the experiments as well as\nthe implementation are provided in the supplement."}, {"title": "5.1 Approximation of Models", "content": "We demonstrate the utility of learning the underlying\nrules of any classifier using RBML algorithms. For\nour CoGS framework, we applied the FOLD-SE algo-\nrithm various classifiers, including Deep Neural Net-\nworks (DNN), Gradient Boosted Classifiers (GBC),\nRandom Forest Classifiers (RF), and Logistic Regres-\nsion Classifiers (LR). These classifiers are first trained\non several datasets: the Adult dataset by Becker and\nKohavi (1996), the Statlog (German Credit) dataset\nby Hofmann (1994), the Car evaluation dataset by Bo-\nhanec (1997), and the Mushroom dataset by Schlim-\nmer (1981). The results of these classifiers and their\ncorresponding FOLD-SE equivalent are shown in the\nTable 1. The Accuracy (Acc.), Precision (Prec.), Re-\ncall (Rec.) and F1 Score (F1.) performance on the\ntest set are shown along with the Fidelity (Fid.) of\nthe FOLD-SE approximation. Fidelity, as used by\nWhite and d'Avila Garcez (2020), refers to the de-\ngree to which the RBML algorithm (FOLD-SE) ap-\nproximates the behaviour of the underlying model. A\nhigher fidelity score indicates that FOLD-SE closely\nmatches the predictions of the underlying model.\nFOLD-SE successfully learns the underlying logic of\nthese models, and the decision rules it generates (de-\nnoted as Q in Definition 3) are used to produce coun-\nterfactuals. While there is a penalty in performance\ncompared to the original models, this is to be ex-\npected as FOLD-SE and other RBML algorithms have\nthe added constraint of being explainable and inter-\npretable in addition to learning the underlying logic\nwhen compared to the underlying models which may\nnot have such constraints. For further details please\nsee the supplement. Next we show how the decision\nrules learned by our RBML algorithm (FOLD-SE) are\nused to generate a path to the counterfactual."}, {"title": "5.2 Obtaining a Path to the Counterfactual", "content": "We applied the CoGS methodology to rules gener-\nated by FOLD-SE across the German, Adult, and Car\nEvaluation datasets. These datasets include demo-\ngraphic and decision labels such as credit risk ('good'\nor 'bad'), income (\u2018=< $50k/year\u2019 or \u2018> $50k/year\u2019),\nand used car acceptability. We relabeled the car eval-\nuation dataset - \u2018acceptable' or 'unacceptable'- to gen-\nerate the counterfactuals. For the German dataset,\nCoGS identifies paths that convert a 'bad' credit rat-\ning to 'good' to determine the criteria for a favourable\ncredit risk, i.e., likely to be approved for a loan. Sim-\nilarly, CoGS identifies paths for converting the un-\ndesired outcomes in the Adult and Car Evaluation\ndatasets-'=< $50k/year' and 'unacceptable\u2019-to their\ncounterfactuals: \u2018> $50k/year\u2019and\u2018acceptable'. We\nuse CoGS to obtain counterfactual paths. Details are\nprovided in the supplement."}, {"title": "6 Conclusion and Future Work", "content": "The main contribution of this paper is the CoGS\nframework, which automatically generates paths to\ncausally consistent counterfactuals for any machine\nlearning model statistical or rule-based. CoGS ef-\nficiently computes counterfactuals, even for complex\nmodels, and the paths can be minimal if desired. By\nincorporating Answer Set Programming (ASP), CoGS\nensures each counterfactual is causally compliant and\ndelivered through a sequence of actionable interven-\ntions, making it more practical for real-world appli-"}]}