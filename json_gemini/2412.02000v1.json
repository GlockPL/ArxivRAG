{"title": "Who's Gaming the System? A Causally-Motivated Approach for Detecting Strategic Adaptation", "authors": ["Trenton Chang", "Lindsay Warrenburg", "Sae-Hwan Park", "Ravi B. Parikh", "Maggie Makar", "Jenna Wiens"], "abstract": "In many settings, machine learning models may be used to inform decisions that impact individuals or entities who interact with the model. Such entities, or agents, may game model decisions by manipulating their inputs to the model to obtain better outcomes and maximize some utility. We consider a multi-agent setting where the goal is to identify the \"worst offenders:\" agents that are gaming most aggressively. However, identifying such agents is difficult without being able to evaluate their utility function. Thus, we introduce a framework featuring a gaming deterrence parameter, a scalar that quantifies an agent's (un)willingness to game. We show that this gaming parameter is only partially identifiable. By recasting the problem as a causal effect estimation problem where different agents represent different \u201ctreatments,\" we prove that a ranking of all agents by their gaming parameters is identifiable. We present empirical results in a synthetic data study validating the usage of causal effect estimation for gaming detection and show in a case study of diagnosis coding behavior in the U.S. that our approach highlights features associated with gaming.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) models often guide decisions that impact individuals or entities. Attributes describing an individual or entity are often inputs to such models. In response, such entities may modify their attributes to obtain a more desirable outcome. But changing one's attributes may be costly due to the difficulty of generating supporting evidence, or penalties for fraud. This behavior is called gaming or strategic adaptation [1]. Strategic adaptation frames gaming as \u201cutility maximization:\" agents change their attributes to maximize a payout, but incur a cost for modifying attributes.\nAs an illustrative example, we turn to the health insurance industry. In the United States (U.S.), contracted health insurance companies report their enrollees' diagnoses to the government, which calculates a payout based on reported diagnoses via a publicly available model [2]. The payout is intended to support care of the enrollee in relation to the diagnosis. Companies may attempt to maximize payouts by reporting extraneous diagnoses, an illegal practice known as \"upcoding\" [3]. Despite increasing awareness of upcoding [3, 4, 5, 6], upcoding costs U.S. taxpayers over $12B U.S. dollars annually [7], even with substantial investment in audits ($100.7M U.S. dollars, 2023 [8]) and payout changes to adjust for gaming [9, 10]. Since audits may not scale and often overlook fraud [11, 12], tools for flagging gaming-prone agents could help target audits. Beyond health insurance, gaming emerges in responses to credit-scoring algorithms [13, 14] and driver responses to rider allocation algorithms in ride-sharing apps [15]."}, {"title": "2 Related works", "content": "Machine learning-based anomaly detection. Fraud detection is often framed as an unsupervised anomaly/outlier detection problem [17, 18, 19, 20, 21, 22, 23]. Such approaches assume gamed model inputs are outliers in some distribution. However, this assumption may be incorrect if gaming is common in the data, or if gaming results in only small changes to observed agent attributes. In borrowing from strategic adaptation, we frame gaming as utility maximization, rather than making distributional assumptions about gamed agent attributes.\nStrategic classification & gaming in machine learning. A large body of work in strategic classification aims to design incentives to mitigate gaming/strategic behavior by agents. However, such works assume that feature manipulations costs are known/can be estimated across agents, or are identical [1, 24, 25, 26, 27, 28, 29, 30, 31]. In contrast, in our setting, feature manipulation costs are unknown and may differ across agents, but exhibit some shared structure that facilitates comparisons across agents. Shao et al. [32] assumes that agent gaming capacities may differ by placing bounds on manipulation, which may be unrealistic. Closest to our work is that of Dong et al. [33], which also assumes unknown and differing agent costs, but does not leverage similarities in gaming across agents. Our work supplements the strategic classification literature by studying a related yet fundamentally distinct problem: rather than designing incentives to mitigate strategic behavior, which is infeasible under unknown manipulation costs, we leverage differences in costs across agents to identify agents more likely to game."}, {"title": "3 Background & Problem Setup", "content": "We review strategic adaptation and extend it to model differences in gaming across agents.\nWhat is strategic adaptation? Our work builds upon strategic classification [1]. Consider a pre-existing model $f : D \\rightarrow R$ that maps agent attributes $d \\in D$ to a payout. An agent may leverage its knowledge of $f$ to change their attribute(s) $d$ according to some function $\\Delta$:\n$\\triangle(d) \\triangleq \\arg \\underset{\\tilde{d} \\in D}{max} R(\\tilde{d}; f) - g(\\tilde{d}, d)$\nwhere $R : D \\rightarrow R$ is the payout of changing $d$ to $\\tilde{d}$, and $g : D \\times D \\rightarrow R+$ is the cost of manipulating $d$. When $D \\subseteq R$, $g$ is often assumed to be \u201cseparable;\u201d i.e., for some function $c$, $g(\\tilde{d}, d) = c(\\tilde{d} - d)$. $\\Delta(.)$ describes how an agent manipulates $d$ to obtain a higher payout from $f$. This behavior is called strategic adaptation or gaming. For simplicity, we assume $R = f$; i.e., the model $f$ directly determines the payout.\nModeling agent variation in gaming. To extend strategic adaptation to multiple agents, we add a non-negative gaming deterrence parameter $\\lambda_p \\in R+$ to Eq. 1. Consider an observational dataset $D_p = \\{(x_i, d_i)\\}_{i=1}^N$ of an agent $p$'s decisions $d_i \\in \\{0, 1\\}$ given some information $x_i \\in X$. For simplicity, we assume $d_i$ is binary, though the proposed framework generalizes to non-binary decisions and arbitrary numbers of independent decisions. For example, a health insurance plan $p$ chooses whether to report that an enrollee has a diagnosis $d_i$ given enrollee characteristics $x_i$. Agent assignment is mutually exclusive (e.g., individuals are enrolled in one health insurance plan).\nIf the agent knows that $d_i$ will be used as input to a payout model, they may have an incentive to increase $d_i$ (without loss of generality) to obtain a higher payout. Let $d = \\sum_i d_i$, and suppose each agent $p$ chooses $d$ according to the following utility-maximization problem:\n$P(d_i = 1 | p) = \\triangle_p(d) = \\underset{d \\in [0,1]}{arg \\space max} \\space R(d) - \\Lambda_p c(d - \\dot{d})$\nwhere $R : [0, 1] \\rightarrow R$, $c : R \\rightarrow R+$, and $\\dot{d}$ is the ground truth value of $d$ given the $x_i$ seen by agent $p$. Thus, $\\triangle_p(d)$ is the gamed/observed decision rate of agent $p$ in a population where the ground truth decision rate is $\\dot{d}$. Although $d \\in [0, 1]$ since it is a proportion, our framework applies to $d$ on arbitrary intervals. This formulation assumes that each $x_i$ is equally likely to be gamed, that $x_i$ are truthfully observed, and that any difference between $\\Lambda_p(d)$ and $\\dot{d}$ is due to gaming.\nWe focus on the gaming deterrence parameter $\\Lambda_p$, which scales the cost of manipulation $c(\u00b7)$. $\\Lambda_p$ is non-negative and represents an agent's \u201caversion\u201d to gaming. Lower values of $\\Lambda_p$ mean that agent $p$ is more willing to game. Thus, identifying agents most willing to game means finding agents with the lowest $\\Lambda_p$. We summarize multi-agent strategic adaptation in Figure 1. Next, we introduce assumptions on the reward function R, cost function c, and ground truth $d$."}, {"title": "4 Theoretical analysis: finding agents most likely to game", "content": "We aim to identify agents most likely to game a decision-making model, i.e., agents with the lowest gaming parameters $\\Lambda_p$. Here, we prove that $\\Lambda_p$ cannot be point-identified without further assumptions (Section 4.1), but ranking $\\Lambda_p$ is possible via causal effect estimation (Section 4.2). Detailed proofs are in Appendix B.\n4.1 Partial identification of the gaming parameter\nHere, we show that given our assumptions, $\\Lambda_p$ is only partially identifiable (cannot be uniquely determined):\nProposition 1. Define $R'(.)$ as $\\frac{dR}{dd}$ and $c'(.)$ as $\\frac{dc}{dd}$. For any agent $p$, given Assumptions 1- 4 and an observed $\\Lambda_p(d)$,\n$\\lambda_p \\in [\\frac{R'(\\Delta_p(d))}{c'(\\Delta_p(d))}, \\infty]$\nand the bound is sharp.\nIntuitively, different values of the unknown $\\dot{d}$ yield different estimates of $\\Lambda_p$ consistent with the observed $\\Lambda_p(d)$. Thus, uncertainty in $\\dot{d}$ results in uncertainty in $\\Lambda_p$. Equivalently, point-identifying $\\Lambda_p$ requires perfect knowledge of $\\dot{d}$. Thus, without further assumptions, $\\Lambda_p$ is only partially identifiable. The lower bound is attained for $\\dot{d}$ = 0 (all $d_i = 1$ are manipulated), while $\\Lambda_p \\rightarrow \\infty$ as $\\Lambda_p(d) \\rightarrow \\dot{d}$ (no manipulation). Intuitively, increases in $\\Lambda_p$ further disincentivize increases to $\\Lambda_p(d)$, such that $\\Delta_p(d)$ gets closer to $\\dot{d}$.\nA na\u00efve approach to gaming detection would be to rank individuals using the above bound. To see why this is problematic, consider an Agent 1 ($\\Lambda_1$ = 10) and Agent 2 ($\\lambda_2$ = 30), and let $R(x) = x$ and $c(x) = x^2$. Suppose Agent 1 is a health insurance plan serving a relatively healthy population ($\\dot{d}= 0.05$), while Agent 2 serves a population with a higher burden of illness ($\\dot{d}= 0.12$). Via Eq. 2, we have $\\Delta_1(\\dot{d}_1)$ = 0.10, while $\\Delta_2(\\dot{d}) \\approx 0.14$. Substitution into Eq. 3 yields $\\lambda_1 \\geq 5$ and $\\lambda_2 \\geq 3.66$, flipping the true ranking of $\\Lambda_p$. Thus, acting on this bound may incorrectly penalize agents when a high $\\Lambda_p(d)$ is appropriate; e.g., insurance plans serving sicker populations."}, {"title": "4.2 Identifying a ranking of the gaming parameter", "content": "Since we showed that point-identifying $\\Lambda_p$ is impossible without further assumptions, we relax gaming detection to a ranking problem. Intuitively, differences in agent behavior under similar conditions may indicate different gaming capacities, from which the proposed approach follows.\nRanking $\\Lambda_p$ by estimating counterfactuals. Recall that we aim to find agents with the lowest $\\Lambda_p$. Thus, it suffices to rank agents by $\\Lambda_p$, which can be done as follows:\nTheorem 1. Under Assumptions 1- 5, and $\\Delta_{p'}(d)$ defined as\n$\\Delta_{p'}(d) \\triangleq \\underset{d \\in [0,1]}{arg \\space max} \\space R(d) - \\Lambda_p c(d-\\dot{d})$,\nwe have that $\\Delta_p(d) < \\Delta_{p'}(d)$ if and only if $\\lambda_p > \\lambda_{p'}$.\nTheorem 1 tells us that estimation of $\\Delta_p(d)$ and $\\Delta_{p'}(d)$ can be used to rank $\\Lambda_p$ vs. $\\Lambda_{p'}$. Eq. 4 differs from Eq. 2: while R, c, and $\\dot{d}$ are the same, $\\Lambda_p$ changes to $\\Lambda_{p'}$. While subtle, the distinction is key to gaming detection: $\\Delta_{p'}(d)$ denotes what agent $p'$ would have done in a population with ground truth $\\dot{d}$, as opposed to what agent $p$ actually did in their population (ground truth $\\dot{d}^*$).\nTo build a ranking strategy, first consider ranking two agents $p$ and $p'$. Define $D_{p,p'} \\triangleq \\{(x_i, d_i, p_i) | i = 1,...,n \\space and \\space p_i \\in \\{p, p'\\}\\}$, where $p_i$ is an indicator for the agent that observed example $i$. Following the Neyman-Rubin potential outcomes framework [35], let $d_i(p)$ be the value of $d_i$ if $p_i$ was set to $p$ (i.e. had agent $p$ been compelled to make a decision). Such variables are called counterfactuals. Figure 2 (left) shows a toy dataset with counterfactuals $d_i(p)$, $d_i(p')$, where \u201c?\u201d are unobserved decisions $d_i$. Dropping unobserved data, the average $d_i(p)$ is $\\Delta_p(d)$ by definition. Thus, if $d_i(p)$ and $d_i(p')$ are fully observed, one could estimate $\\Delta_p(d)$ and $\\Lambda_{p'}(d)$ as follows:\n$\\Lambda_p(d) = \\frac{1}{N_p}\\sum_{(x_i,d_i,p_i)\\in D_{p,p'}}^{p_i = p} d_i(p)$\n$\\Lambda_{p'}(d) = \\frac{1}{N_{p'}}\\sum_{(x_i,d_i,p_i)\\in D_{p,p'}}^{p_i = p} d_i(p')$"}, {"title": "5 Empirical results & discussion", "content": "We aim to demonstrate that causal inference can be used for gaming detection. First, we discuss our setup (Section 5.1). Then, we show in synthetic data (Section 5.2) that causal methods require fewer audits than existing non-causal methods to catch the worst offenders. Finally, in real-world case study (Section 5.3), we find that causal methods yield rankings correlated with suspected drivers of gaming."}, {"title": "5.1 Setup", "content": "We describe the datasets, evaluation method, and gaming detection methods under consideration.\nDatasets. In real datasets, ground truth gaming rankings are often unavailable. Thus, we validate our framework in a synthetic dataset. We hand-select 20 $\\Lambda_p$ values (one per agent; see Appendix C.1 for raw $\\Lambda_p$ values) and simulate confounding by generating covariates from agent-specific Gaussians with means $\\mu_p \\in R^2$. To control confounding strength, we set $\\mu_p = g(log(\\lambda_p))$, where g is an affine transformation such that the range of $\\mu_p$ across agents equals a chosen range parameter $R_\\mu \\in \\{0, 0.1, ..., 1.0\\}$. Smaller $R_\\mu$ implies less confounding, since $\\mu(.)$ varies less across agents. We generate 500 observations $x^{(i)} \\in R^2$ and ground-truth $\\dot{d}^{*(i)}$ per agent via\n$x^{(i)} \\sim N(\\mu_p(c), \\sigma^2I_{2x2})$   $\\dot{d}^{*(i)} \\sim Ber(a^{*(i)}); a^{*(i)} = \\sigma(wx^{(i)} + b)$,\nwhere $w \\sim U(0, 1)^2$, such that increasing $x$ increases $a^{*(i)}$ (and thus $P(\\dot{d}^{*(i)} = 1)$), b is chosen such that $a^{*(i)}$ for the mean x is \u2248 5%, and $\\sigma^2$ = 1. We simulate gamed agent decisions $d^{(i)}$ as follows:\n$d^{(i)} \\sim Ber(a^{(i)}); a^{(i)} = \\underset{d_p\\in [0,1]}{arg \\space max} \\space log(d_p) - \\Lambda_p(d_p - \\dot{d}^{*(i)})^2$.\nRecall that the decisions $d^{(i)}$ are also agent inputs to a payout model. We generate 10 datasets (each N = 10,000; 20 agents \u00d7 500 observations) for all 11 levels of confounding (as measured by the range of means $R_\\mu$). Causal inference assumptions hold in the synthetic data: all confounders $x^{(i)}$ are observed (Assump. 6), consistency holds by construction (Assump. 7), and overlap holds since all $x^{(i)} \\mid p$ are supported on $R^2$ (Assump. 8). Full synthetic data generation details are in Appendix C.1.\nTo benchmark causal effect estimation in a more realistic setting, we apply causal methods to gaming detection in U.S. Medicare claims. Medicare is the public health insurance system in the U.S. for residents aged 65 and over. Since private insurance claims data is not widely available, we conduct a gaming case study in healthcare providers. In Medicare, the U.S. government pays healthcare providers on a per-service basis [40]. Thus, providers may be incentivized to label enrollees with as many diagnoses as possible to secure extra payment from the government. We select a 0.2% sample of all Medicare enrollees with a claim in 2018; i.e., those who utilized a service covered directly by Medicare (N = 37,893). We use demographic information and diagnoses in 2018 as covariates and select the rate of uncomplicated diabetes diagnosis in 2019 as the outcome. Given differences in healthcare policy and access across U.S. states, we pool data at the U.S. state level and treat each state as an \"agent.\" Additional cohort details are in Appendix C.2.\nEvaluating rankings. Given an observational dataset of the form \\{(x_i, d_i, p_i)\\}_{i=1}^N, with covariates $x_i$, observed decisions $d_i$, and agent indicators $p_i$, gaming detection algorithms output an ordinal agent ranking in terms of the gaming parameter $\\Lambda_p$. We aim to measure the efficiency of a predicted ranking given some level of resources committed by a decision-maker (i.e., # of agents audited).\nGround truth rankings are available in synthetic data. Thus, we measure the top-5 sensitivity at k ($S_k$), the % of top-5 worst offenders in the predicted top-k, and the discounted cumulative gain (DCG) at k, a weighted sum of ground-truth \u201crelevance scores\u201d for the top-k predicted agents, across audit intensities $k \\in \\{1, ..., 20\\}$. For a K-agent dataset, we define the relevance score as K + 1 minus the ground-truth rank (e.g., true rank 1 = relevance K, true rank 2 = relevance K \u2013 1, etc.). Concretely, for $r_i$ defined as the ith ranked agent in a predicted ranking, and rank(\u00b7) as the function returning the ground-truth ordinal rank with respect to $\\Lambda_p$, our ranking evaluation metrics are computed as follows:\n$S_k \\triangleq \\frac{1}{5} \\sum_{i=1}^k 1[rank(r_i) \\leq 5]$    $DCG_k \\triangleq \\sum_{i=1}^k \\frac{K - rank(r_i)}{log_2(i + 1)}$\nNote that sensitivity is an all-or-nothing measure of audit quality given a fixed audit intensity k. However, DCG rewards higher predicted rankings for top-k worst offenders, regardless of the absolute ranking position. Furthermore, DCG weights decrease with predicted rank (Eq. 6), which prioritizes."}, {"title": "5.2 Gaming detection in synthetic data", "content": "Causal effect estimators identify gaming more efficiently than non-causal baselines. Figure 5 shows the top-5 sensitivity and DCG of rankings produced by causal vs. non-causal gaming detection approaches at high confounding (mean range: 0.9). Since the S-, T-learner, and DragonNet perform"}, {"title": "5.3 Case study: Detecting upcoding in U.S. Medicare", "content": "As an exploratory analysis, we use the best-performing approach in the synthetic data (S+IPW) analyze upcoding by U.S. state in U.S. Medicare."}, {"title": "6 Conclusion", "content": "We propose a causally-motivated framework for ranking agents by gaming propensity in the context of strategic adaptation. We show that the gaming parameter is only partially identifiable, but a ranking of a set of agents based on the gaming deterrence parameter is identifiable via causal inference. We demonstrate the utility of causal effect estimation for gaming detection on synthetic data and a case study of upcoding in Medicare.\nLimitations & broader impact. We assume agents always increase $d_i$ with respect to ground truth, and that gaming explains all differences in agent behaviors, ignoring factors such as agent \u201cquality\" (e.g., quality of care). Utility-maximizing behavior and conditional exchangeability are strong assumptions, but are statistically unverifiable. Many works in game theory and causal inference share these limitations. We caution that policies informed by our framework could reinforce imbalanced power dynamics (i.e., are individual citizens [26] or more powerful entities gaming a model) via extraneous or weaponized accusations of gaming, since not all entities have equal capacity to respond to such claims. In particular, gaming by individuals may potentially reflect structural inequities rather than inherently pathological behavior. To mitigate such risks, we suggest \u201cshadowing\u201d studies (decisions visible, but not acted upon) alongside existing audit mechanisms before adoption."}]}