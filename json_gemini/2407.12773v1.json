{"title": "OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides", "authors": ["Zhuoyan Shen", "Mikael Simard", "Douglas Brand", "Vanghelita Andrei", "Ali Al-Khader", "Fatine Oumlil", "Katherine Trevers", "Thomas Butters", "Simon Haefliger", "Eleanna Kara", "Fernanda Amary", "Roberto Tirabosco", "Paul Cool", "Gary Royle", "Maria A. Hawkins", "Adrienne M. Flanagan", "Charles-Antoine Collins Fekete"], "abstract": "Mitotic activity is an important feature for grading several cancer types. Counting mitotic figures (MFs) is a time-consuming, laborious task prone to inter-observer variation. Inaccurate recognition of MFs can lead to incorrect grading and hence potential suboptimal treatment. In this study, we propose an artificial intelligence (AI)-aided approach to detect MFs in digitised haematoxylin and eosin-stained whole slide images (WSIs). Advances in this area are hampered by the limited number and types of cancer datasets of MFs. Here we establish the largest pan-cancer dataset of mitotic figures by combining an in-house dataset of soft tissue tumours (STMF) with five open-source mitotic datasets comprising multiple human cancers and canine specimens (ICPR, TUPAC, CCMCT, CMC and MIDOG++). This new dataset identifies 74,620 MFs and 105,538 mitotic-like figures. We then employed a two-stage framework (the Optimised Mitoses Generator Network (OMG-Net) to classify MFs. The framework first deploys the Segment Anything Model (SAM) to automate the contouring of MFs and surrounding objects. An adapted ResNet18 is subsequently trained to classify MFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection (breast carcinoma, neuroendocrine tumour and melanoma), largely outperforming the previous state-of-the-art MIDOG++ benchmark model on its hold-out testing set (e.g. +16% F1-score on breast cancer detection, p<0.001) thereby providing superior accuracy in detecting MFs on various types of tumours obtained with different scanners.", "sections": [{"title": "Keywords", "content": "Mitotic Figures, Nuclei Detection, Digital Pathology, Artificial Intelligence, Foundation Model"}, {"title": "Highlights", "content": "\u2022\tA large pan-cancer mitotic figure dataset has been created by enhancing open-source datasets and integrating an in-house dataset for soft tissue tumours across 20 subtypes.\n\u2022\tA novel nuclei detection framework, based on Segment Anything, has been developed, demonstrating state-of-the-art performance in detecting pan-cancer mitotic figures.\n\u2022\tThe results demonstrate that incorporating the contours of nuclei significantly enhances the accuracy and robustness of mitotic figure detection.\n\u2022\tThe feasibility of zero-shot deployment of foundation models for data generation, standardization, and nuclei detection model development is demonstrated and discussed."}, {"title": "Introduction", "content": "Mitotic activity is a crucial indicator of cellular proliferation and plays a pivotal role in cancer diagnosis and guiding clinical management (Williams & Stoeber, 2012). Counting mitotic figures (MFs) from haematoxylin and eosin (H&E)-stained whole slide images (WSIs) is a fundamental task in pathology, required for the grading of some tumours. By convention, in clinical practice, mitotic counts are performed in the 10 most mitotically active high-power microscopic fields (HPFs) within a tumour (Cree, et al., 2021). As this is a time-consuming task, and subject to significant inter-observer variability (Malon, et al., 2012; Veta, et al., 2016; Robbins, et al., 1995), there has been considerable interest and effort in the development of automated MF detection models, e.g. ICPR (Capron & Genestie, 2011; Roux, 2014) and TUPAC (Veta, et al., 2019) initiated the development of breast cancer MF datasets. Initially, mitotic detection models focused on learning handcrafted features (Irshad, 2013; Tashk, et al., 2013; Paul, et al., 2015), but recently transitioned to deep-learning-based methods that show promise (Mahmood, et al., 2020; Sebai, et al., 2020; Li, et al., 2018; Cai, et al., 2019). However, MF detection remains a challenging task (Aubreville, et al., 2023), due to the different appearance of MF in the four phases of mitosis, the range of features exhibited by abnormal MFs, as well as structures that mimic MFs (mitotic-like figures, MLFs). The above challenges are compounded by the histological heterogeneity in normal tissues and tumour types, staining variation between labs and differences in digital scanners used to generate WSIs.\nTo improve the detection of MF, the MItosis DOmain Generalization (MIDOG) (Aubreville, et al., 2021; Aubreville, et al., 2020) published an updated version of their multi-domain dataset, MIDOG++ (Aubreville, et al., 2023). This contains 503 annotated images across seven different cancer types, representing the largest currently available published dataset of MFs. The data utilised in the MIDOG studies contains the HPFs manually selected by pathologists to mimic clinical practice. However, the pathologist-led decisions may not be reproducible because of the recognised inter-observer variation (Meyer, et al., 2005; Bertram, et al., 2019), and discrepancies can be caused by the selection of areas with the densest mitotic activity (Diest, et al., 1992). In contrast, the CMC (Aubreville, et al., 2020) and CCMCT (Bertram, et al., 2021) datasets used AI-assisted annotations to generate large-scale WSI datasets for MFs using canine cancers. The former used 21 WSIs of canine mammary carcinomas whereas the CCMCT dataset included 32 WSIs of canine mast cell tumours. These studies demonstrated that annotating MFs on a WSI improves the robustness of classifiers by removing the HPF selection bias and leads to a significantly higher number of detected mitoses, helping to refine further training (Bertram, et al., 2021).\nOne preferred approach to take forward this field of MF detection would have been to increase the size of the existing datasets incorporating multiple scanner types, staining differences across multiple sites, and tumour types. However, the lack of standardisation in the annotation protocol across existing various datasets limit their integration. For example, in the ICPR, each pixel within MFs was labelled, whereas the TUPAC only encircled MFs. MIDOG++, CCMCT, and CMC utilised bounding boxes to denote the targets. We therefore took the approach to standardise the annotations by contouring nuclei of MFs.\nHistorically, targets in cellular object detection tasks are denoted using bounding boxes. However, several studies have reported that incorporating a target's mask facilitates model training and improves the overall classification performance. For instance, the Mask-RCNN outperformed the Faster-RCNN in a variety of object detection tasks (He, et al., 2018), including MF detection (Sebai, et al., 2020). The advantages of integrating nuclei contours for detection include enhancing the definition of nuclei boundaries, mitigating the morphological variability of the MFs (Li, 2023) and reducing the impact of tumour histological heterogeneity. Given the constraints of a small dataset and the significant variability between mitotic cells, introducing a recognisable mitotic feature into the model aids in stabilising the training process and leads to a faster convergence.\nThe aim of this study was to improve the detection of MF across multiple tumour types. First, we established a large uniform database of pan-cancer MFs by deploying the Segment Anything Model (SAM) (Kirillov, et al., 2023), a foundation object detection model, in five open-source datasets (ICPR, TUPAC, CCMCT, CMC, MIDOG++) using a single nuclei mask format. Manual revision of the masks was performed to maximise database quality. Then, we contributed an in-house dataset of human soft tissue tumours (STT) MFs (N=8,400) (Soft-Tissue Mitotic Figures, STMF). Although STT represents a rare tumour group, they comprise over 100 subtypes exhibiting a wide variety of histological appearances and mimic other tumours including common cancers such as melanoma, carcinoma and lymphoma. STT harbours a variable number of MFs and aids in reaching a diagnosis and predicting disease behaviour (Coindre, 2006). As of now, no publicly accessible data have been published for MFs in STT. The STMF was initiated by staining WSIs with an anti-phosphorylated histone H3 (pHH3) antibody to target MFs which was expanded and improved by Al-assisted annotations made by pathologists.\nThe second objective was to develop an improved MF detection framework, which we named Optimised Mitoses Generator Network (OMG-Net). By integrating nuclei masks into the pre-trained classifier via a first-layer addition, we allow the model to focus on the morphological"}, {"title": "Material and methods", "content": "features of MFs. We demonstrate that OMG-Net is both more sensitive and specific at detecting objects, including MFs, throughout the input WSI, compared to previous models."}, {"title": "Dataset", "content": "Open-source datasets\nWe integrated five open-source datasets (ICPR, TUPAC, CCMCT, CMC, MIDOG++), comprising 68,687 MFs from eight different scanners and eight types of human and canine tumours. The types of tumours studied and scanners are listed in Supplementary Table 1. All the images were scanned in 40\u00d7 magnification with a pixel size of approximately 0.25 \u00b5m.\nIn-house dataset\nWe describe a workflow for utilising an anti-phosphorylated histone H3 (pHH3) antibody to specifically detect MFs and expand the dataset by active learning (Figure 1). The number of MFs in each diagnosis of soft tissue tumours is listed in Supplementary Table 2.\n\u2022\tpHH3-assisted MF detection: the pHH3 antibody employed specifically detects the core protein histone H3 only when phosphorylated at serine 10 (Ser10) or serine 28 (Ser28), thereby identifying mitotic cells within a tissue sample (Elmaci, et al., 2018). We selected 94 archived slides and tissue blocks from soft tissue tumours and prepared fresh H&E tissue sections which were then scanned for generating our dataset. These H&E-stained tissue sections were then de-stained after which immunohistochemistry was performed using a rabbit monoclonal (RM) hybridoma Ser10 pHH3 [BC37] (Tacha, 2015) and then counterstained with eosin. The masks of the MFs were extracted from pHH3-immunolabelled WSIs by setting thresholds for the RGB values and transferred to the same location on the matching H&E-stained WSIs. Registration between the pHH3-immunolabelled and H&E-stained WSIs was achieved by random sample consensus (RANSAC) (Fischler & Bolles, 1981) on both a WSI-level and patch-level. The contours and positions of 7,952 MFs (STMF-V0), were identified and validated by pathologists reviewing the H&E and immunolabelled sections. However, not all mitoses were identified by pHH3-labelling indicating that the antibody was not entirely sensitive (Ribalta, et al., 2004).\n\u2022\tActive Learning: Although the identification of cells in mitosis by pHH3 can establish a dataset with a large number of MFs, it cannot identify MLFs, and models trained only with IHC suffer from limited precision. Active learning is required to augment the dataset with MLFs.\nDuring the active learning process, pathologists corrected the image labels given by a machine learning model and fed them back to re-train the initial model, so that the model performance for the target task can be continuously improved during the iteration of machine-generating and human-labelling.\nTo expand the STMF-V0 dataset, we trained an initial Mask-RCNN model on it and applied the model to new WSIs for detecting MFs. The AI-detected MFs were randomly assigned to six pathologists to be independently labelled as \u2018MF\u2019, \u2018not MF' if the pathologist could confidently make a decision, or \u2018uncertain' when the morphological features were equivocal. These equivocal MFs were reviewed by two senior pathologists. Other structures such as apoptotic bodies were also labelled to create the final dataset, STMF, with 8400 MFs and 5035 MLFs.\nEthical Approvals\nThe data involved in the STMF dataset are collected in the Royal National Orthopaedic Hospital (RNOH) NHS Trust under the Health Research Authority (HRA) and Health and Care Research Wales (HCRW) Approval. Integrated Research Application System (IRAS) project ID: 328987. Protocol number: EDGE 161548. Research Ethics Committee (REC) reference: 23/NI/0166. Informed consent was obtained from all human participants.\nData Curation\nThe MFs were annotated using bounding boxes in the CCMCT, CMC and MIDOG++ datasets. However, the size of the boxes varies due to the lack of standard annotation criteria. We hypothesised that the contours of nuclei could provide extra information for classifying MFs,"}, {"title": "OMG-Net: A Two-Stage Detection Framework", "content": "as the model would be guided to focus on the most representative pixels of the nuclei rather than the surrounding environment.\nWe use the bounding boxes provided in the CCMCT, CMC and MIDOG++ datasets as prompts to generate the masks using SAM. To ensure the quality of the automatically generated masks, we inspected individual masks of the MFs from three types of human tumours and canine soft tissue sarcoma in MIDOG++. The percentage of masks amended following review is 8%, 5% and 16% out of 4435 masks in breast carcinoma, 2075 in melanoma and 2400 in neuroendocrine tumour, respectively. In total, only 8% of the masks required a second inference of SAM using adjusted bounding boxes. Since the cells can be distorted during the de-staining and pHH3 labelling process, we also applied the SAM to the STMF using the outside boxes of the pHH3-immunolabelled masks as prompts. The numbers of MFs and MLFs from human and canine samples are shown in Figure 1. Quality assurance was done for masks of all the human samples, whereas the generation of masks in canine sections was fully automated.\nThe structure of OMG-Net is outlined in Figure 2. The proposed framework consists of two steps:\n\u2022\tThe SAM was applied to patches of 1024 \u00d7 1024 pixels from the tumour regions. 64 points were evenly sampled along each dimension, totalling 4,096 points used as prompts per patch. The quality of the masks was predicted by two factors, an AI-predicted Intersection over Union (AI-IoU) and a stability score. The AI-predicted IoU comes from an adjacent multi-layer perceptron in the mask decoder section of SAM. The stability score is the IoU between the binary masks obtained by thresholding the predicted mask logits at high and low values. Only the objects with AI-IoU scores and stability scores higher than 0.8 and areas between 2.25 \u00b5m\u00b2 and 225 \u00b5m\u00b2 were kept after filtering. The filtered masks were then ranked by their AI-IoU scores. Non-maximum suppression (NMS) (Hosang, et al., 2017)was used to remove duplicated masks.\n\u2022\tThe objects generated were then classified by the second model, a ResNet18 pre-trained on ImageNet, as MFs or other objects. In addition to taking a 3-channel RGB image, the mask of the object was encoded by a convolutional layer and summed to the first convolutional layer of the ResNet18. Via this process, we retained the ability to use pre-trained models while providing extra mask information to the model."}, {"title": "Model Development and Testing", "content": "The framework was implemented using Pytorch and Pytorch Lightning and was trained using a single NVIDIA GeForce RTX 3090 for 30 epochs with a batch size of 8,000. The learning rate was set up at 0.001, optimised by the AdamW algorithm (Loshchilov & Hutter, 2019) and cosine annealing scheduler (Loshchilov & Hutter, 2017).\nTraining and Validation\nWe trained the ResNet18 to classify MFs while the SAM mask generator was not retrained. The SAM was applied to all patches in the dataset after data curation, and the other objects surrounding the targets were also segmented and included in the training and validation data. The binary classifier is trained on two classes: (1) MFs and (2) labelled MLFs and other cells or objects segmented by SAM. In each training process, 90% of the data was used for training the model, while the remaining 10% was used for validation. The training was repeated five times using different random seeds to get five models with different data splits.\nData Augmentation\nColour and spatial augmentation were applied to the training data to reduce the impact of the staining variation and increase the robustness of the model. To achieve colour augmentation, RGB images are deconvolved into H&E stains using the stain vectors proposed by Ruifrok and Johnston (Ruifrok & Johnston, 2001). The stain concentration perturbation scheme introduced by Tellez et al. (Tellez, et al., 2018) was used with a uniform sampling and \u03c3 = 0.14 on the deconvolved H&E channels prior to reconstructing RGB images. Random horizontal flips (p =\n0.4) was also used.\nTest set and performance metrics\nWe used the same testing set provided by MIDOG++, which contains 674 MFs from 56 sections of three types of human tumours. Precision, recall and F1 score were used to evaluate the performance of our mitotic detection framework. They were calculated by\n$Precision = \\frac{N_{TP}}{N_{TP} + N_{FP}}$\n$Recall = \\frac{N_{TP}}{N_{TP} + N_{FN}}$\n$F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$\nwhere NTP, NFP and NFN represent the number of true positives, false positives and false negatives, respectively. Mann-Whitney U test (Nachar, 2008) was used for comparing the unpaired scores of different models."}, {"title": "Results", "content": "Developing a large-scale MF dataset\nWe established a large in-house dataset for MFs in STT and merged it with five open-source datasets for MFs from human and canine specimens (Table 1). The final dataset contains 74,620 MFs and 105,538 MLFs from 712 different images or WSIs with the SAM-delineated masks for nuclei. Masks of human MFs were reviewed and modified to ensure the quality of nuclei contours. Additionally, the dataset included a large number of SAM-segmented objects, comprising tumour cells, immune cells, red blood cells, artefacts and any objects at the cell scale, collected during the data curation.\nPerformance of MF detection in various human tumours\nWe benchmarked our OMG-Net against the current state-of-the-art MIDOG++ dataset/model containing three types of human tumours (breast carcinoma, neuroendocrine tumour, and melanoma). Table 2 shows the mean precision, recall and F1 scores with the standard deviation of the proposed framework trained five times using different random seeds, along with the F1 score obtained by ensemble voting and the F1 scores quoted in the MIDOG++ paper.\nThe MF detection scores of OMG-Net are significantly higher (p < 0.001) in all three types of human tumours within the testing set of MIDOG++. Figure 3b shows the benefit of combining multi-centre data, as the increased number of MFs for training correlated with an increase in the holdout F1-score.\nReviewing the SAM masks enhanced the detection performance\nAs shown in Figure 4a, the appearance of MFs is highly diverse, exemplified by atypical MFs. The segmented mask may not fully cover the MFs or may contain background noise. To refine the training process, we reviewed the masks in the human subset of MIDOG++ (4435 in breast carcinoma, 2075 in melanoma and 2400 in neuroendocrine tumour), and adjusted the SAM prompt when required. The impact of this manual curation was assessed by comparing the F1 scores of the models only with RGB images (RGB Classifier), the score of the model with zero-shot SAM mask input (RGB-M0 Classifier) as well as the score of the model with reviewed and refined masks (RGB-M1 Classifier), with results shown in Figure 4b.\nCompared to the model without masks (RGB), the RGB-M0 model yielded higher F1 scores for detecting MFs from breast carcinoma (p = 0.011) and melanoma (p = 0.001) but not for neuroendocrine tumours. Upon further analysis, we noted that the fraction of masks requiring a second adjustment was higher in neuroendocrine tumours (16%), compared to breast carcinoma (8%) and melanoma (5%). As predicted, the RGB-M1 Classifier showed the best performance and significantly outperformed the RGB Classifier for breast carcinoma (p < 0.001), melanoma (p < 0.001) and neuroendocrine tumours (p = 0.021). We conclude that the low-quality masks, which may include surrounding backgrounds or exclude part of the nuclei (Figure 4b), can impact the performance of the RGB-M0. Further examples of failed prompts are shown in Supplementary Figure 1.\nCanine mitotic figures help to train the detection of human mitotic figures\nThe merged and uniform dataset contains a significant proportion of canine MFs with examples from both human and canine WSI displayed in Figure 5a. The inclusion of the canine data significantly improved the detection of MFs in breast carcinoma (p = 0.007) and neuroendocrine tumours (p = 0.015) and the F1 score in melanoma was also marginally increased (p = 0.080) (Figure 5b).\nIncluding mitotic-like figures and non-mitotic objects is key to improving model precision\nBesides MFs, MLFs were also labelled in the original dataset. MLFs represent morphological structures that resemble MFs including pyknotic nuclei, apoptotic bodies, and neutrophil polymorph amongst others, often misclassified as MFs. An example is displayed in Figure 6a. Apart from MLFs and tumour cells, the SAM-curated dataset contains other cells, including immune cells, red blood cells and any objects at the cell level such as artefacts, segmented by the SAM during data curation to present the classifier with a heterogeneous set of data. These were used in the training step to augment the original dataset and provide the model with a diverse representation of segmented objects. Figure 6b shows that the model including non-MF objects (SAM-AUG) has significantly higher precision for all three types of tumours (p = 0.008) compared to the model trained only with MFs and MLFs (Original). As expected, the recall remains unchanged, and the overall F1 scores are improved (p = 0.007)."}, {"title": "Discussion", "content": "Nuclei contours represent a key feature for improving mitotic-figure detection\nIn this study, OMG-Net showed significantly improved MF detection performance in all three types of tumours compared with the testing set of the current state-of-the-art MIDOG++. This improvement was achieved both by using a larger, multi-source, SAM-enhanced dataset of MFs (Figure 1) and by integrating within the network the mask of the MFs' nuclei (Figure 4b). The accuracy of our models varied considerably across different tumour types, with neuroendocrine tumours exhibiting significantly lower performance, which was consistent with the results of the MIDOG++ algorithm. In parallel, we observed a higher proportion of low-quality masks in neuroendocrine tumours (16%) compared to breast carcinoma (8%) and melanoma (5%), suggesting that the quality of the training data may have contributed to the disparities in model performance across these cancer types. Even then, manual curation of the masks helped improve significantly the model detection performance.\nTo decide which foundational model to select as a nuclei detector, we evaluated published fine-tuned variants of SAM against the overall mask quality for cells in histology images. Specifically, we tested MedSAM (Ma, et al., 2024), which was fine-tuned on multiple medical image modalities, and CellSAM (Israel, et al., 2023), which was fine-tuned on microscopy images. By inspecting the number of cells detected and the layout of the masks produced by both algorithms, we concluded that the quality of masks was degraded in both models and included a higher proportion of omitted cells (Supplementary Figure 2). We attribute the reduced performance of the fine-tuned SAM models to different architectures used compared to the original SAM model. OMG-Net uses the highest capacity SAM variant (ViT-H), whereas MedSAM and CellSAM are fine-tuned on the lighter ViT-B, which may lead to reduced performance due to limited model capacity. Based on this analysis, we elected to keep the original SAM as the cell detector in our study. Future work will include refining the SAM object-proposal method for H&E-stained specific cell types\nDecoupling segmentation and classification helps improve the detection performance\nObject detection models such as Faster R-CNN (Ren, et al., 2016), RetinaNet (Lin, et al., 2018) and YOLO (Redmon, et al., 2016) have been widely used for MF detection (Mahmood, et al., 2020) (Bertram, et al., 2021). These models integrate in a single model an object proposal network with a primary classifier. However, these models suffer from the imbalanced loss problem, as the cell segmentation and classification loss have inherently unequal magnitudes. The gradient updates that occur during backpropagation can be dominated by the loss function with the larger norm (Chen, et al., 2018), leading to suboptimal training and convergence issues. This becomes even more prominent when dealing with small datasets or complex objects, as the imbalance in the loss functions' impact can significantly hinder the model's ability to learn effectively from the limited available data (Argyriou, et al., 2006). The use of integrated object detection models in histopathological studies has been shown to generate false positive results due to the complex and variable nature of cell morphology.\nMore recently, it has been demonstrated (\u00c7ay\u0131r, et al., 2022; Sohail, et al., 2021) that integrating a secondary classifier, trained on MFs and other objects such as MLFs, to review and reject false positive cases improves a framework's precision. This approach limits the imbalanced loss problem, as the segmentation loss is excluded in training the additional classifiers. However, these methods add unnecessary complexity to the network since two classifiers must be trained. To mitigate the imbalanced loss problem, we elected to separate entirely the object detection and classification steps. This offers an innovative approach that differs from those previously published. Instead of training an object detection model for generating objects that are highly likely to be MFs, all the objects at the cell scale are segmented by SAM from the ROIs and classified, improving the sensitivity of our model. Other objects, including immune cells, cells not in mitosis, and artefacts generated during the data preparation stage, can also be used to train the classifier, improving its capability to reject false positives.\nLarge-scale MF datasets provide a resource for the development of pan-cancer models\nLarge-scale datasets are crucial for developing AI models capable of detecting MFs effectively in a variety of cancer types and overcoming the challenges posed by the heterogeneity of staining and scanning protocols. Here, we propose a workflow for creating a reliable MF dataset:\n1) H&E destaining and employing immunohistochemistry for enhanced detection: efficient generation of a large-scale image dataset with accurate labels by detecting a substantial number of MFs on WSIS.\n2) Continuous Data Curation: improve data quality by employing Segment Anything (SAM) to delineate precisely mitotic figure (MF) nuclei, followed by meticulous manual refinement of the generated contours.\n3) Active learning: iteratively train and refine the model using a pathologist-in-the-loop approach, enabling efficient review of detected mitotic figures (MFs), and incorporating Mitotic-Like Figures (MLFs) and non-mitotic objects into the database for enhanced model performance\nThese steps are required as it is not feasible for pathologists to annotate MFs in the numbers and the precision required by AI models, thereby affecting the diversity and size of the dataset and, consequently, the detection accuracy of the trained model. Nevertheless, each of these steps encounters limitations.\nPerforming immunohistochemistry following the destaining procedure of H&E-stained sections allows for the rapid and largely specific detection of MFs (specificity >99%) (Kim, et al., 2017). Still, it is not a perfect process as cells in the G2 phase of the cell cycle can exhibit weak immunoreactivity (Tacha, 2015) as well as being prone to false-negative immunoreactivity due to the age of the slide and fixation method (Hendzel, et al., 1997). This restaining procedure also does not detect MLFs, which is crucial to enhance the model specificity.\nActive learning can help identify MLFs but a consensus view of MF/MLF cannot always be reached by pathologists. This study highlighted the acknowledged problem of interobserver variation of MF by pathologists (Veta, et al., 2016; Robbins, et al., 1995) which is compounded when interpreting MFs on digitised slides as it is not possible to adjust the focus plane on cells of interest. During our revision process, a notable proportion (13.8%) of AI-detected cells were categorised as \"equivocal\" (Supplementary Figure 3). A secondary review of these images performed by at least two experienced pathologists resolved some of these images but differences in opinion remained in 9.5% of AI-detected MFs."}, {"title": "Conclusion", "content": "Finally, despite the limitations discussed above, the integration of immunohistochemistry for MF detection following the destaining of H&E sections, data curation, active learning, and consensus-based review by experienced pathologists enabled us to mitigate the challenges in creating a large-scale database and developing an improved, pan-cancer MF detection model.\nWe have established a large-scale MF dataset by integrating five open-source datasets acquired from multiple centres including an in-house dataset of STT. Using the curated dataset, we employed a novel two-step framework, OMG-Net, where SAM served as the object detector followed by an adapted ResNet18 as the MF classifier. This approach improved the accuracy of MF detection from various human tumours including breast carcinoma, neuroendocrine tumours and melanoma compared to existing state-of-the-art models. Future steps include a head-to-head prospective assessment of this model with pathologists' scores for MFs before introduction into safe clinical practice."}, {"title": "Data availability", "content": "All MF images provided by the open-source datasets and their SAM-dilated contours are available without restriction via Zenodo in accordance with the UKRI Common principles on research data. The MF images in STMF are available upon reasonable request."}, {"title": "Code availability", "content": "All code to reproduce the results, when coupled with the dataset available on Zenodo, is provided free of use at https://github.com/cacofl/DigitalPathologyAI."}, {"title": "Author contributions: CRediT", "content": "Zhuoyan Shen: Conceptualization, Formal analysis, Investigation, Methodology, Validation, Visualization, Writing \u2013 original draft, Writing \u2013 review and editing. Mikael Simard: Methodology, Writing \u2013 review and editing. Douglas Brand: Investigation, Writing \u2013 review and editing. Vanghelita Andrei: Data curation, Investigation. Ali Al-Khader: Data curation, Investigation. Fatine Oumlil: Data curation, Investigation. Katherine Trevers: Project administration. Thomas Butters: Data curation, Investigation. Simon Haefliger: Data curation, Investigation. Eleanna Kara: Data curation, Investigation. Fernanda Amary: Resources, Data curation, Investigation. Roberto Tirabosco: Resources, Data curation, Investigation. Paul Cool: Resources, Investigation, Writing \u2013 review and editing. Gary Royle: Supervision, Investigation, Writing \u2013 review and editing. Maria A. Hawkins: Supervision, Writing \u2013 review and editing. Adrienne M. Flanagan: Conceptualization, Funding acquisition, Supervision, Resources, Data curation, Investigation, Writing \u2013 review and editing. Charles-Antoine Collins Fekete: Conceptualization, Funding acquisition, Supervision, Resources, Data curation, Investigation, Methodology, Writing \u2013 review and editing."}, {"title": "Declaration of competing interest", "content": "The authors declare no competing interests."}, {"title": "Supplementary Materials", "content": "Supplementary Figure 1: Low-quality masks. Examples of mitotic figure (MF) masks (the blue masks) that are badly delineated by Segment Anything (SAM). The MFs are indicated by yellow arrows, the green boxes are the prompts used when deploying the SAM.\nSupplementary Figure 2: Cell detection performance of the original and fine-tuned Segment Anything (SAM). Objects segmented by the original SAM, MedSAM and CellSAM on three example patches. N is the number of objects detected in each patch.\nSupplementary Figure 3: Figures with disagreement between pathologists. The figures indicated by the yellow arrows were the examples labelled as \u201cequivocal\u201d by the junior pathologists and were sent to senior pathologists for secondary annotating.\nSupplementary Table 1. Summary of datasets and scanners.\nSupplementary Table 2. Number of whole slide images (WSIs) and mitotic figures (MFs) for each diagnosis in STMF."}]}