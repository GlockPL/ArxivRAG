{"title": "MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL", "authors": ["Wenxuan Xie", "Gaochen Wu", "Bowen Zhou"], "abstract": "Recent In-Context Learning based methods have achieved remarkable success in Text-to-SQL task. However, there is still a large gap between the performance of these models and human performance on datasets with complex database schema and difficult questions, such as BIRD. Besides, existing work has neglected to supervise intermediate steps when solving questions iteratively with question decomposition methods, and the schema linking methods used in these works are very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent generative approach with soft schema linking and iterative Sub-SQL refinement. In our framework, an entity-based method with tables' summary is used to select the columns in database, and a novel targets-conditions decomposition method is introduced to decompose those complex questions. Additionally, we build a iterative generating module which includes a Sub-SQL Generator and Sub-SQL Refiner, introducing external oversight for each step of generation. Through a series of ablation studies, the effectiveness of each agent in our framework has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL achieves an execution accuracy of 61.08%, compared to the baseline accuracy of 46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL. Besides, our approach makes similar progress on Spider.", "sections": [{"title": "1 Introduction", "content": "Aiming at automatically generating SQL queries from natural language questions, Text-to-SQL is a long-standing challenge which is critical for retrieving database values without human efforts (Qin et al., 2022). There are two main categories of LLM-based Text-to-SQL approaches, In-Context Learning method (ICL) and Supervised Fine-Tuning method (Gao et al., 2023). Earlier work has reached human levels on the Spider dataset (Yu et al., 2018), but there is still a large gap with human performance on the BIRD dataset (Li et al., 2024b). Previous work has begun to focus on model collaboration (Wang et al., 2024), but it failed to break the 60% EX accuracy on BIRD dataset. Besides, some recent work has achieved significant improvements, but these improvements are achieved through large-scale fine-tuning on LLMs (Talaei et al., 2024) or relying on large-scale sampling from LLMs (Lee et al., 2024). In order to"}, {"title": "3 Methodology", "content": "As shown in Figure 2, our proposed framework consists of four Agents: Soft Schema Linker, Targets-Conditions Decomposer, Sub-SQL Generator and Sub-SQL Refiner. Among them, the Sub-SQL Generator and Sub-SQL Refiner together form the Iterative Generating Module.\nConcretely, Soft Schema Linker performs soft column selection on the large database schema to reduce the amount of irrelevant information being described, while the Targets-Conditions Decomposer divides the question into a series of sub-questions. After that, the Sub-SQL Generator generates Sub-SQL for the current Sub-question each time based on the previous Sub-question and the previous Sub-SQL. Meanwhile, the Sub-SQL Refiner employs an external tool for Sub-SQL execution to get the feedback, and then refines incorrect SQL queries. The details of these agents will be introduced in the following sections."}, {"title": "3.1 Soft Schema Linker", "content": "Aiming to filter the large database schema and provide helpful information for SQL generator, the task of Soft Schema Linker can be divided into 5 parts: Schema Representation, Table Summarization, Value Retrieval, Entity-based Schema Linking and Soft Schema Construction."}, {"title": "3.1.1 Schema Representation", "content": "Before further analysis and processing of the database schema, we need to determine the database schema representation method. We se-rialise the information from a table into a list, and each item in the list is a tuple representing the in-formation of a column. For each column, there is column name, data type, column description and value examples. Besides, primary keys and foreign keys also need to be listed additionally because of their importance. Therefore, the complete database schema of a specific database can be represented as:\n$S = T_1 : [C_1^1, ... C_n^1]| ... |T_m : [C_1^m, ... C_n^m]; KP; KF;$\n$C_n^m = (N_m, < d_m >, D_m, V_m)$ \nwhere Tm indicate the m-th table in this database, $C_n^m$ denote the n-th column in Tm. As for each column, Nm is its name, < dm > is its data type such as INTEGER, Dn is its description, and Vn is the list of value examples in this column."}, {"title": "3.1.2 Table Summarization", "content": "In previous work, LLM was expected to select use-ful columns directly based on the relevance of each column to the natural language question. However, this could lead to a consequence that LLM is prone to errors if two or more columns in different tables have similar names, descriptions, and stored val-ues. In reality, when SQL engineers see a table, they will first make a summary of the information stored in this table based on the columns in this table. And after seeing the question, they will find the most suitable table according to the specific needs at the first time, and then pick the correspond-ing columns. Therefore, we propose an extra step called Table Summarization, let LLM first make a summary of the information in each table, and then prompt LLM with both the summary and the complete database schema for column selection.We ask LLM to respond in the JSON format, as described in Figure 3."}, {"title": "3.1.3 Value Retrieval", "content": "Since LLM can't browse tables directly, LLM may select the incorrect columns when values of TEXT type appear in natural language questions, such as names of people, places, and other items. For ex-ample, for the question \"Please list the zip code of all the charter schools in Fresno County Office of Education\", the value \"Fresno County Office of Ed-ucation\" is in the column <District Name>. How-ever, LLM is only able to know 'Fresno County Office of Education' is a place name, which make it difficult to select the right one out of the mul-tiple columns describing place names. In our ex-periment, LLM prefers to treat this value as the value of the column <County> instead of <District Name>. If LLM is provided with matched database content, the possibility of chosing the correct col-umn is greatly increased. In this work, we use longest common substring (LCS) algorithm to re-trived question-related values from database. We only apply this algorithm on data of text type and finally linearise the matched data and values to text, here is an example: frpm.\u2018District Name\u2018='Fresno County Office of Education';"}, {"title": "3.1.4 Entity-based Schema Linking", "content": "Since the columns in the SQL query statement and the entities in the natural language problem are one-to-one mapping, we present the Entity-based Schema Linking in order to achieve fine-grained column selection. The chain of thought approach is used to prompt LLM to complete the following steps:\n\u2022 Extract the entities from the natural language question.\n\u2022 Analyses the relevance based on Summary, Database schema and Hints.\n\u2022 For each entity, find at least three most rele-vant columns (top3 columns sorted by rele-"}, {"title": "3.1.5 Soft Schema Construction", "content": "In Text-to-SQL tasks, the database schema can be very large, but only a small percentage of the columns are really needed to be used. Besides, providing detailed information for all columns is costly, not to mention that irrelevant information will cause disturbance. Therefore, we need to pre-process the database schema, keeping only the in-formation about useful columns. There are two approaches for columns selection, soft column selection and hard column selection (Sun et al., 2023). Soft column selection keep entire schema but add additional detailed descriptions for selected columns, while hard column selection removes all non-selected columns and retains the details of the chosen columns. In our work, we choose soft column selection because this method is able to re-duce the length of the prompt while allowing the model to have access to the entire database schema, and we mark the additional information of the cho-sen columns as Detailed description of tables and columns."}, {"title": "3.2 Targets-Conditions Decomposer", "content": "Given the increasing complexity of natural lan-guage questions in Text-to-SQL tasks, generating SQL query in one step is not an good option. There-fore, recent work decompose a complex question into a series of sub-questions before solving them step by step. Nevertheless, previous work just gives LLM some (Question, Sub-questions) pairs as ex-amples, and does not propose a decomposition cri-terion, which may lead to inconsistent granularity of question decomposition. In a word, the essence of question decomposition in Text-to-SQL tasks is ignored.\nTherefore, we propose a Targets-Conditions Decomposition method, which ensures that all questions are decomposed according to the same criteria. This approach is based on the fact that all queries can be decomposed into the targets to be queried and the conditions used to filter the targets, including natural language questions in text-to-SQL task. Besides, the decomposition idea of Least-to-most Prompting proved to be effective in Text-t-SQL tasks, and this idea is also used in our method."}, {"title": "3.3 Sub-SQL Generator", "content": "As one of the most important Agents in our frame-work, the Sub-SQL Generator predicts Sub-SQL for the current Sub-question based on the previous Sub-question and the previous Sub-SQL. When processing the first Sub-question, it will generate the first Sub-SQL directly. The purpose of this design is to allow the generator to add only one condition at a time to the previous Sub-SQL, which greatly reduces the difficulty of reasoning. In or-der to avoid the accumulation of errors that can be caused by previous incorrect Sub-SQL, we use the Sub-SQL Refiner to introduce intermediate super-vision for the entire generation process.\nAs a Text-to-SQL dataset close to real-world applications, BIRD (Li et al., 2024b) highlights the new challenges of dirty and noisy database val-ues, external knowledge grounding between NL questions and database values. Therefore, a wide variety of complex information will be put into the prompt, which poses a challenge for LLM to gen-erate SQL query through in-context learning. To help LLM better understand and utilise the vari-ous pieces of information in the prompt, we adopt the CoT prompting method to explicitly take infor-mation from different parts of the prompt in the reasoning steps. The complete Prompt template for Sub-SQL Generaotr is shown in Figure 7, and the Chain of Thoughts example we provided is shown in Figure 9."}, {"title": "3.4 Sub-SQL Refiner", "content": "Since LLMs proved to have strong self-refine abil-ity, we disign the Sub-SQL Refiner to correct the Sub-SQL with the feedback obtained after executing the Sub-SQL. If the currently processed Sub-question is not the last one, then the Sub-SQL cor-rected by the Refiner is returned to the Generator and used to generate the next Sub-SQL. Thus, Sub-SQL Generator and Sub-Refiner work together to form the Iterative Generating Module, allowing ev-ery intermediate step of the generating process to be supervised and calibrated."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup"}, {"title": "4.1.1 Datasets", "content": "Spider (Yu et al., 2018) is a large-scale, complex and cross-domain dataset which is widely used as a basic benchmark for Text-to-SQL, including 10,181 questions and 5,693 queries across 200 databases.\nSince earlier work had achieved near-human level results on Spider, we chose to focus on more diffi-cult datasets. The recently proposed BIRD dataset (Li et al., 2024b) is a challenging benchmark in-cluding 95 large-scale real databases with dirty val-ues, containing 12,751 unique question-SQL pairs. Compared with Spider, BIRD comprises more com-plex SQL queries and introduces external knowl-edge."}, {"title": "4.1.2 Metrics", "content": "Execution Accuracy (EX) EX indicates the cor-rectness of the execution result of the predicted SQL. This metric takes into account different SQL representations for the same question, allowing for a more reasonable and accurate measurement of the results.\nValid Efficiency Score (VES) VES is a new met-ric introduced by BIRD dataset, measuring the effi-ciency of a valid SQL query based on the execution time. Our method also achieves an improvement in VES, but we do not include it in the experimen-tal analyses because this metric is susceptible to environmental effects."}, {"title": "4.2 Baselines", "content": "We compare the proposed MAG-SQL approach with the following baselines based on In-context Learning methods with GPT-4:"}, {"title": "4.3 Results and Analysis", "content": "4.3.1 BIRD Results"}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel multi-agent gen-erative approach for effecient Text-to-SQL, with different agents performing their respective roles. Concretely, our workflow consists of four LLM-based agents, which provides new ideas for schema-linking, question decomposition, SQL generation and SQL correction. We have conducted fine-grained task decomposition and detailed task plan-ning for Text-to-SQL, which fully unleashes the potential of LLMs. With equivalent model configu-rations, our approach achieves better performance on the BIRD dataset than all previous work at the time of writing this paper. Moreover, the results achieved by using GPT-3.5 as the backbone of our method have approached or even surpassed those of some previous GPT-4 based methods."}, {"title": "6 Limitations", "content": "Although our approach has achieved significant improvements on the Text-to-SQL task, there is still a gap compared to human performance. Every agent in our method could benefit from further improvements. Besides, there are three limitations in our work.\nSingle closed-source backbone Currently, MAG-SQL is implemented based on a closed-source LLM, and the results are likely to be further improved if we use open-source domain-specific LLMs instead of closed-source general LLMs. For instance, the Sub-SQL Generator could be replaced with an open-source LLM that have been fine-tuned to generate SQL statements.\nFixed Workflow We have pre-defined the workflow of MAG-SQL, thus the pipeline is fixed dur-ing the whole process, which reduces the flexibil-ity of the multi-agent framework. In the future, we hope to implement a multi-agent system that can autonomously plan tasks and dynamically in-voke various tools. Moreover, we believe that the multi-agent collaboration mechanisms can be fur-ther optimised by methods such as reinforcement learning.\nUnstable Output There is another phenomenon to be aware of. Since the output of the LLM is probability-based and not stable, it is often the case that for the same instance, the LLM can get it right one time and wrong the next time. One current solution is using a large number of samples from LLMs, but this approach is costly, and if there is a better solution to this problem, then the final result will achieve a significant breakthrough. We leave this for future work."}]}