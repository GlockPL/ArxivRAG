{"title": "Target-Dependent Multimodal Sentiment Analysis Via Employing Visual-to-Emotional-Caption Translation Network using Visual-Caption Pairs", "authors": ["Ananya Pandey", "Dinesh Kumar Vishwakarma"], "abstract": "The natural language processing and multimedia field has seen a notable surge in interest in multimodal sentiment recognition. Hence, this study aims to employ Target-Dependent Multimodal Sentiment Analysis (TDMSA) to identify the level of sentiment associated with every target (aspect) stated within a multimodal post consisting of a visual-caption pair. Despite the recent advancements in multimodal sentiment recognition, there has been a lack of explicit incorporation of emotional clues from the visual modality, specifically those pertaining to facial expressions. The challenge at hand is to proficiently obtain visual and emotional clues and subsequently synchronise them with the textual content. In light of this fact, this study presents a novel approach called the Visual-to-Emotional-Caption Translation Network (VECTN) technique. The primary objective of this strategy is to effectively acquire visual sentiment clues by analysing facial expressions. Additionally, it effectively aligns and blends the obtained emotional clues with the target attribute of the caption mode. The experimental findings demonstrate that our methodology is capable of producing ground-breaking outcomes when applied to two publicly accessible multimodal Twitter datasets, namely, Twitter-2015 and Twitter-2017. The experimental results show that the suggested model achieves an accuracy of 81.23% and a macro-F1 of 80.61% on the Twitter-15 dataset, while 77.42% and 75.19% on the Twitter-17 dataset, respectively. The observed improvement in performance reveals that our model is better than others when it comes to collecting target-level sentiment in multimodal data using the expressions of the face.", "sections": [{"title": "1 Introduction", "content": "In the contemporary era of digital connectivity, an enormous amount of publicly accessible multimodal content is posted by individuals on prominent websites like YouTube, LinkedIn, Twitter, Instagram, Facebook, etc. Analysing such multimodal information streams is advantageous in examining individuals' emotional responses towards a particular topic. Aspect-Based Sentiment Analysis (ABSA) determines the sentiment polarity towards specific attributes that are explicitly mentioned within a given input text. For example, \u201cThe river gives a pleasant view, however, the quality of the roll was disappointing\u201d. The user conveys a positive opinion regarding the river's location while expressing dissatisfaction or negative sentiment for the target aspect 'roll'."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Target-Dependent Unimodal Sentiment Recognition", "content": "The classification of sentiment at the aspect level, sometimes referred to as target- dependent sentiment recognition, is a crucial task of sentiment recognition that has received significant research interest in recent years [21,22]-[23]. The vast majority of existing methodologies may be broadly classified into two main categories.\nOne area of study involves utilising techniques such as Part-of-Speech Tagger and lexicons of the sentiments to create a customised feature. These features are then used in conjunction with conventional statistical learning methods to make predictions about sentiment. Although these models have shown reasonable performance on various standard datasets [24], [25], [26], and [27], their effectiveness is limited by their extensive dependence on feature engineering. Another area of research focuses on the integration of target information into different deep- learning-based architectures. Ruder et al. [1] introduced a Bi-LSTM-based hierarchical architecture to leverage inter- and intra-sentential dependencies. Later, Peng et al. [2] proposed a two-stage approach that combined aspect extraction, sentiment recognition, and explanation of the reason for the sentiment into a single task. In this approach, LSTM is employed to predict the emotional state of the target phrases after retrieving the aspect terms. In addition to deep learning models, attention-based frameworks have gained significant interest among academics as a means to address the challenges associated with text-based"}, {"title": "2.2 Target-Dependent Multimodal Sentiment Recognition", "content": "In recent years, with the rise of multimodal content in online communities, data derived from multiple modalities (audio, visual, etc.) has been employed to fuse with conventional textual attributes to predict the sentiment more accurately. Research in the domain of multimodal sentiment recognition [30] involves the utilisation of visual-captions pairs to conduct target- dependent analysis.\nThe concept of aspect-based multimodal sentiment analysis was initially developed by Yu et al. [31]. The two publically accessible multimodal benchmark datasets released in this research study comprise pairs of visual-caption, specifically referred to as \"Twitter-2015\" and \"Twitter-2017\". The study conducted by Yu et al. [31] presented the TomBERT framework, which consists of a ResNet branch for extracting visual features from photographs, two BERT branches for extracting textual features from captions and targets, and a fusion mechanism to integrate all the feature representations for the purpose of sentiment prediction. Subsequently, Xu et al. [32] introduced the first Chinese multimodal dataset, \"Multi-Zol,\" designed explicitly for entity-based sentiment recognition. This research paper developed the MIMN network, which combines Bi-GRU and ConvNet and integrates them with attention modules for extracting textual and visual features. The ESAFN architecture, proposed by Yu et al. [33], is a composite model that combines LSTM, ResNet-152, and attention techniques to forecast sentiment at the entity level. The multi-headed attention-based framework and the ResNet-152 architecture are adopted by Gu et al. [34] to address textual and visual data. The objective of integrating the capsule network and multi-headed attention is to capture the relationship between multiple modalities effectively. Zhang et al. [35] also utilised a similar approach, namely the integration of LSTM with ResNet, for target-dependent sentiment analysis by fusion of different attentional networks. To effectively capture the most significant visual information from the dataset, Zhao et al. [36] developed an intra and intermodal relationship graph ConvNet. These graphs will facilitate the analysis of the caption and visual data. In Wang et al. [37], after the visual-caption pairs have been processed using Bi-GRU and a ConvNet, the resulting fused vector of both representations is passed to the BERT module. This module is responsible for modelling the effective interaction between the inputs. Yu et al. [38] introduced a hierarchical transformer model to effectively capture the"}, {"title": "3 Proposed Methodology", "content": ""}, {"title": "3.1 Problem Formulation", "content": "The TDMSA can be precisely described as outlined below: Consider a collection of visual- caption pair examples denoted as M = {E1, E2, E3, ... ... ..., EM }, where |M| represents the total number of instances. For each given example, an image I \u2208 R3\u00d7H\u00d7W is provided where 3, H and W indicate the number of channels, height and width. Every visual sample in this study is associated with textual content represented by a set of K- words provided by the captions C = {w1, w2,w3, .,wk }, which comprises a subsequence of N-word that represents the target entity, defined as T = {W1, W2, W3, ... ... ..., w\u2081 }. Our study aims to develop a sentiment classifier to predict a sentiment label Y from multimodal examples accurately. A combination of variables E = {I,C,T} represents each sample in E. The sentiment labels are categorised into three classes: Y \u2208 {positive,negative, neutral}."}, {"title": "3.2 Visual-to-Emotional-Caption Translation Network (VECTN)", "content": "The proposed framework Visual-to-Emotional-Caption-Translation Network (VECTN) illustrated in Figure 2 has four distinct components: Facial emotion description module, Target alignment and refinement of the face descriptions, Image captioning, and Fusion module. For a given tweet consisting of a visual-caption pair, denoted as E = {I, C, T}, we first take the input image 'I' and feed it into a facial emotion description unit to generate face description D = {D1, D2, D3, ... ... ..., Df } comprises of different features such as age, gender, emotion, etc., where F is the number of faces present in an input image and Di = {D1, D2, D3, ... ... ..., D\u2081 } represents a phrase consisting of L-word. The extraction and textualisation of facial expressions within an image, which represents an immense amount of information on the individual's sentiments, is the primary emphasis of this module. Since the input image 'I' may include several facial expressions, it is necessary to match or align the facial description Dr with the target entity T. The target alignment and refinement of the face descriptions module estimates cosine similarity between visual input I and face descriptions with target Dr. The facial description Dr is selected and rewritten based on similarity scores. Since the visual scenes may provide extra semantic details, we employ the image-to-text transformer (Wang et al. [44], 2022) to produce image captions for the scene_Ic = {Ic1, Ic2, Ic1, ... ... ..., IcG}, where Grepresents caption length. At last, in the fusion component, we employ two robustly optimised pre-trained language models based on BERT to simulate image captions and face"}, {"title": "3.2.1 Facial Emotion Description Module", "content": "The proposed module addresses two fundamental difficulties in TDMSA. First, the complex images in multimodal tweets might make it challenging to extract object-level emotional indicators. Another issue is translating emotional signals obtained from visual modality into a sequence of words.\nTo address the first challenge, as previously stated, leveraging the wide range of facial expressions in images proves to be an efficient method for extracting emotional cues from visual mode. The first step involves using a tool represented as f developed by Serengil et al.[45] to recognise multiple faces within an image of the dataset as stated in Equation (1). Let F represent the set of faces, denoted as F = {F1,F2,F3, ... ... ..., F1}, where J represents the total number of faces and F\u2081 \u2208 R3\u00d7HF\u00d7WF represents a face area with three channels, H\u04ab height, and Wf width. The obtained faces F are then fed into a pre-trained classification model (Serengil et al. [46]) for facial attribute analysis, which involves gender, age, race (Indian, Black, Asian, White, Latino, and Middle Eastern,) and facial expression to predict sentiments.\nF = $ (I) (1)"}, {"title": "3.2.2 Target Alignment and Refinement of the Face Descriptions", "content": "Sometimes, a multi-face image sample with varied facial expressions fails to estimate the correct emotion of the target entity. On the other hand, the inclusion of redundant facial emotions produces noise and diminishes the overall effectiveness. Therefore, it is essential to effectively synchronise the facial emotions shown in the visual sample with the desired target entity. In our proposed framework, VECTN, this component focuses on aligning facial expressions with the target object, resulting in more detailed facial descriptions.\nThe TDMSA challenge needs external visual-caption alignment information for more fine- grained alignments due to restricted dataset size and the absence of direct visual- caption alignment supervision. Hence, to achieve fine-grained alignment, we use caption and visual encoders of a recently developed contrastive visual-caption pre-training architecture trained on a variety of visual-caption pairs [47] denoted as 't' to encode the face descriptors D associated with target T and the visual I. The resulting embeddings for images and descriptions of faces are shown in Equation (2) and Equation (3), where \u2018\u2295' denotes concatenation.\nODT = Caption_Encoder (t)(D \u2295 T) (2)\n0\u2081 = Visual_Encoder(t)(I) (3)\nSubsequently, the obtained feature embeddings are projected into the same feature space. Then, we compute the Levenshtein distance L for these feature embeddings using L2- normalization 'y'. Next, we choose and regenerate the face description that best fits the current image as the visual, emotional clue for the current target based on the similarity score using L. The refined face description only contains the target object and expressions based on predicted facial traits.\nODT = y(OD&TVD&T) (4)\n0\u00a6 = \u03b3(01\u00b7 V\u2081) (5)\nL = 0 (0D&T) * e\u03c4 (6)\nVD&T and V\u2081 are trainable weights, and t is the scaling factor of the generative visual-to- caption transformer model [44]. This module is only used for multi-face visual samples. The target is concatenated directly with the acquired face description in this module. Subsequently, the newly concatenated phrases are used as input for the textual encoder of 't', while the picture is employed as input for the visual encoder of \u2018\u03c4'. The cosine similarities between the visual"}, {"title": "3.2.3 Fusion Module", "content": "This module aims to combine already available caption(C), target entity (T), refined facial description (Dr), and the generated caption (Ic). To leverage the pre-trained language model's robust textual context analysis, we concatenate the refined face descriptions and image caption with available text and target to create two new phrases as shown in Equation (8) and (9) below:\nDT\n[CLS]w\u00a3, ..., w\u00a3[SEP]w, ..., w[SEP]w\u2122, ..., w\u2122 [SEP] (8)\n[CLS]wf, ..., w\u00a3[SEP]w],...,w][SEP]w, ..., w[SEP] (9)\nFine-tuning two robustly optimised per-trained language models [48] with these new phrases yields [CLS] token O[CLS] \u2208 R768 and O[CLS] \u2208 R768 pooler outputs. The gate mechanism is used to reduce noise in feature representations of O[CLS] and O[CLS] At last, to predict sentiment, fused feature representations (Equation (10) and (11)) are sent via a linear classifier using Equation (12), where VDT, VIC and V are trainable weights of dimensions R768\u00d7768, R768\u00d7768, and R768\u00d73. In contrast, b; and b are learnable biases with dimensions R768 and R3.\njt = tanh (VDTOCLS] + VICO[CLS] + b;) (10)\nDT\n0 = jt * O[CLS] + jt * O[CLS] (11)\nDT\nIc\nP(Y|0) = Softmax((V * 0) + b) (12)\nAll module parameters are optimised using conventional cross-entropy loss defined in Equation (13).\n|D|\nL = - (1/D) \u2211log P{y'|0'} (13)\nl=0"}, {"title": "4 Experimental Setup and Results", "content": ""}, {"title": "4.1 Experimental Details", "content": "Our model was trained and evaluated using two publically accessible benchmark datasets, Twitter-2015 and Twitter-2017. Both datasets include tweets consisting of visual-caption pairs. Each caption has been tagged with some target entity and its associated sentiment polarity. Our approach primarily emphasises cases that include facial images. Therefore, we extract samples with facial images from the aforementioned two datasets to create the Tweet1517-Face dataset. Subsequently, we evaluate the effectiveness of our proposed model on these samples to prove"}, {"title": "4.2 Baselines for Comparison", "content": "This section compares our proposed model with several cutting-edge baseline approaches (discussed in section 2.2) for the task of TDMSA for both unimodal and multimodal networks. Table 5 demonstrates that the experimental results of our proposed model are more accurate than those of the other baseline methodologies in terms of Accuracy (A) and macro F1 score, thus proving our model's superiority."}, {"title": "4.3 Analysis of Our Experimental Results", "content": "This section presents experimental results and analysis of our proposed framework for all three datasets. Table 6, Table 7 and Table 8 highlight the best scores on each performance measure for all three datasets. Our method demonstrates superior performance when compared to all other multimodal baselines. This serves as evidence of the efficacy of the proposed VECTN framework. In the fusion module, we conducted fine-tuning using RoBERTa-base and RoBERTa-Large language models [48] and found that RoBERTa-Large"}, {"title": "4.4 Ablation Study", "content": "To understand the influence of each component of our technique, we performed an extensive ablation study utilising the VECTN-ROBERTa-Large version. The results for the same are depicted in Table 9. Initially, the sentiment label of the target is predicted by combining the results of linguistic models. These outcomes are then used as input to a linear classification layer without the inclusion of a gating mechanism. The overall performance of the architecture experiences a significant decrease as a result of the presence of noise during the facial emotion description module. On Twitter 2015, A and macro F1 scores dropped 1.83% and 1.96%, respectively. On the Twitter-2017 dataset, A drops 2.15%, and the macro F1 score drops 2.88%. This suggests that the gating technique is responsible for reducing noise and extracting more useful features. Additionally, it is seen from Table 9 that the exclusion of the target alignment module also results in a decrease in performance. This result suggests that the alignment between the visual and emotional cues and the target entity is crucial. At last, we analyse the impact of excluding the visual caption from the scene, which results in a significant decrease in the model's performance. This finding provides evidence that the utilisation of visual-to-caption translation contributes to the advancement of visual-caption fusion."}, {"title": "5 Predictive Analysis of Few Samples", "content": "In order to provide a more comprehensive demonstration to highlight the benefits of the proposed method, this section of the manuscript will include the actual predictions made by our model on a few samples gathered from Twitter-15 and Twitter-17. As shown in Table 10, the VECTN model accurately forecasts positive sentiments for the target terms [LeBron James's], [JordanStrack], negative sentiments for aspect words [Harriette], [Anthony Kiedis], while the neutral sense of emotion for [Donald Trump Republcian]. As a result, this study demonstrates that the proposed approach efficiently focuses on multimodal sentimental regions to leverage the interaction between the image and the target phrase more extensively than existing methods. Hence, the VECTN model can deeply examine the local semantic relationship between image and text in contrast with baseline models. In simple terms, the proposed model exhibits a higher degree of advantage."}, {"title": "6 Conclusion and Future Direction", "content": "This paper presents a target-dependent multimodal sentiment recognition strategy called a visual-to-emotional-caption translation network. The idea put forward utilises facial emotions depicted in images as visual indicators of emotions. In this study, we propose a novel and efficient approach to establish a correlation between the target entity in textual information and the facial expressions depicted in visual media. Our approach has successfully achieved ground-breaking results on the Twitter2015 and Twitter-2017 datasets. The results indicate that our proposed solution surpasses a set of baseline models. This showcases the strength of our method in gathering emotional clues from the visual modality and achieving cross-modal alignment on visual-caption sentimental information. In the future, we would like to extend our proposed method for other multimodal tasks, such as the identification of hate speech, sarcasm, fake news, etc. The analysis of emotions conveyed by video is another exciting field of study with promising future prospects."}]}