{"title": "Ethical and Scalable Automation: A Governance and Compliance Framework for Business Applications", "authors": ["Haocheng Lin"], "abstract": "The popularisation of applying Al in businesses poses significant challenges relating to ethical principles, governance, and legal compliance. Although businesses have embedded AI into their day-to-day processes, they lack a unified approach for mitigating its potential risks. This paper introduces a framework ensuring that AI must be ethical, controllable, viable, and desirable. Balancing these factors ensures the design of a framework that addresses its trade-offs, such as balancing performance against explainability. A successful framework provides practical advice for businesses to meet regulatory requirements in sectors such as finance and healthcare, where it is critical to comply with standards like GPDR and the EU AI Act. Different case studies validate this framework by integrating AI in both academic and practical environments. For instance, large language models are cost-effective alternatives for generating synthetic opinions that emulate attitudes to environmental issues. These case studies demonstrate how having a structured framework could enhance transparency and maintain performance levels as shown from the alignment between synthetic and expected distributions. This alignment is quantified using metrics like Chi-test scores, normalized mutual information, and Jaccard indexes. Future research should explore the framework's empirical validation in diverse industrial settings further, ensuring the model's scalability and adaptability.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) is a fast-growing industry that becomes more ever-present in our daily lives. As AI grows, we become more reliant on automation in businesses for more efficient decision-making and operational practices. However, there are several significant ethical and legal challenges that come from embracing AI. An example of these risks includes potential malicious misuse by some users and biased decision-making against a particular stakeholder group. Businesses must ensure that their Al systems are ethically sound and legally compliant, particularly after the introduction of regulatory frameworks like the General Data Protection Regulation (GDPR) (European Union 2018) and the European Union (EU) AI Act (European Commission 2021).\nTo overcome the ethical issues from the design of machine learning models, such as mitigating the biases present in the ML models' training data, the GDPR provides organisations transparency, accountability, data protection with a privacy-design measure (European Union 2018, S. Wachter 2017).\nThe EU AI Act introduces a risk-based approach, classifying AI systems based on their potential impact on human rights and safety, with stricter regulations imposed on high-risk AI applications in sectors such as healthcare and law enforcement (European Commission 2021, M. Veale 2021).\nWhile the regulations are essential for guiding businesses toward responsible AI deployment, it is difficult to ensure their compliance, in an environment where AI systems are constantly scaling and evolving. Also, there are ethical concerns about algorithmic biases and loss of human oversight. Current AI systems are poised to surpass the humans' cognitive and physical capabilities within the next 20 years (W. Wang 2019), which risks job displacement and the concentration of skills, wealth, and power to an elite group with access to large datasets and algorithms. Bias in AI, often stemming from the training data, can lead to discriminatory outcomes, especially against marginalized groups, as noted by studies on fairness in Al decision-making (C. O'Neil 2016, R. Binns 2018, S. U. Noble 2018). These risks underscore the need for robust governance models that can evolve alongside AI's growth (D. Leslie 2019, F. Doshi-Velez 2017).\nAlso, Al automation faces a substantial number of ethical risks from the lack of explainability in their decision- making to having no proper governance mechanisms. This mistakes are compounded by the people's high expectations of the AI systems since people are more likely to react harshly to mistakes made by AI systems than humans.\nThis problem is serious in sectors where decisions have life-changing consequence, such as healthcare, finance, and transport (R. Binns 2018, S. U. Noble 2018, L. Floridi 2019). Without human oversight, Al models may face ethical breaches and legal penalties, thus losing public trust (D. Leslie 2019, F. Doshi-Velez 2017, European Commission 2019)."}, {"title": "1.2 Research Objectives", "content": "This paper addresses existing gaps by proposing a unified framework that integrates ethical Al governance with legal compliance. The tailored framework helps different businesses to ensure that their systems are ethical, legal, and scalable across diverse operational contexts.\nThe research objectives are as follows:\n\u2022 Assess the impact of AI-driven automation on business efficiency and productivity.\n\u2022 Develop and validate an AI governing framework that integrates ethical principles (such as fairness, transparency, and accountability) with legal compliance (GDPR and the EU AI Act).\n\u2022 Provide actionable recommendations for integrating ethical Al principles into the businesses' AI governance strategies.\n\u2022 Evaluate the scalability of Al systems within businesses by assessing their performance and compliance under different data volumes and operational conditions.\n\u2022 Use explainability and evaluation metrics to measure the relationship between technology and people in Al-driven environments and determine how to encourage trust in the AI systems."}, {"title": "2. Literature Review", "content": ""}, {"title": "2.1 Ethics in AI Automation", "content": "Ethical issues are embedded within the machine learning models, stemming as an \"original sin\" in how the models learns about the human biases represented by the training datasets (P. G. Kleinberg 2019). Examples of biases include only 15% of the respondents to a research conducted by University of Boston answered that the surgeon was the child's mother when answering a riddle of the surgeon and the boy (J. S. Schreiber 2014).\nML models amplify existing biases, which causes discriminatory outcomes against minority groups or decisions made not reflective of the real-world context. To mitigate these biases, the AI4People initiative and the Ethics Guidelines for Trustworthy AI by the European Commission are foundational for addressing these ethical concerns in AI development (European Commission 2019).\nFairness measures an Al model's ability at making just decisions without favouring a specific stakeholder group. O'Neil (2016) in Weapons of Math Destruction illustrates how AI algorithms can encode historical biases and reinforces social inequalities (C. O'Neil 2016). A ProPublica investigation confirmed that COMPAS, an algorithm designed to predict whether a convicted criminal is likely to reoffend has signs of bias against minorities (A. Allyn 2020). The algorithm risks reinforcing attitudes about which areas are \u201csafe\u201d or \u201cdangerous\" sourced on historical policing decisions instead of actual crime rates. As the AI systems scale up, it is essential to ensure that the bias mitigation strategies are embedded during the build and test of AI models (C. O'Neil 2016).\nHaving transparent AI systems explains how they arrive decisions and ensuring that the models and the stakeholders are accountable for their actions. Doshi-Velez and Kim (2017) argue that interpretability in machine learning fosters trust and ensures accountability (F. Doshi-Velez 2017). While the current Al development transitions toward low-code development technologies, making it easier for individuals to start working on their coding project as observed in the projected growth in the proportion of low-code developers from 20% in 2022 to 40% by 2027 (J. S. Schuchardt 2023). However, as more people use synthetic or low- code technologies, the pool of open-source coding materials for training the AI models could be decreasing. A study by UCL estimated a 16 \u2013 25% decrease in interactions on Stack Overflow forums after the release of ChatGPT (R. M. del Rio-Chanona 2023), which raises concerns about whether there will be a reasonable amount of training materials available for Al models to learn from in the future.\nWithout transparency, businesses and consumers may find it difficult to understand AI-driven decisions, especially in high-stakes contexts like health or financial services (D. Leslie 2019). A lack of explainability can lead to unfair or arbitrary decisions, further eroding trust in AI technologies (F. Doshi-Velez 2017). There are possibilities that the \u201ccitizen\u201d developers might become dependent on using the available low-code framework that they overlook the mechanisms behind the code implementations. On the other hand, this transition to low-code technology could signify a shift in the developers' role from designing solutions to testing, fine-tuning the applications' outputs, and ensuring that they are fit for purpose."}, {"title": "2.2 Legal Frameworks Governing AI", "content": "As AI becomes more pervasive, legal frameworks are evolving to regulate its use, especially in areas related to data protection, risk assessment, and intellectual property (IP) rights. Examples include the General Data Protection Regulation (GDPR), Data Protection Impact Assessment (DPIA), and the forthcoming EU AI Act (European Union 2018, European Commission 2021).\nThe GDPR, implemented in 2018, governs data protection and privacy across the European Union. One key aspect of GDPR relevant to AI is its focus on data protection by design and by default (European Union 2018, S. Wachter 2017). For example, the AI models should only process enough data necessary for their purpose to mitigate the risks of excessive or biased data collection. Al systems that process personal data must comply with GDPR's regulations, ensuring that individuals retain control. GDPR also includes the \"right to explanation,\" which allows individuals to demand explanations for decisions made by automated systems (S. Wachter 2017); therefore, adding to accountability with regulations like the Data Protection Impact Assessments (DPIAs) for high-risk AI applications (European Union 2018, S. Wachter 2017). This provision is particularly relevant for Al applications in areas such as hiring, credit scoring, and healthcare, where automated decisions can have significant consequences on individuals' lives. The DPIA framework provides a structured approach for organizations to identify and manage the ethical and legal risks associated with deploying Al systems (S. Wachter 2017). This risk assessment ensures that Al systems comply with data protection regulations before they are fully operational.\nThe EU AI Act supplements the GDPR with a risk-based approach to governing AI (European Commission 2021, M. Veale 2021). The Act categorizes AI systems based on their potential risk to human rights and safety into four categories: unacceptable, high, limited, and minimal risks. There are strict regulations preventing the implementation of unacceptable risk level AI systems, such as those involving mass surveillance in healthcare, law enforcement, and finance (M. Veale 2021). High-risk AI systems must comply with human supervision, preventing the AI from fully automating critical decisions."}, {"title": "2.3 AI Governance and Scalability", "content": "When Al systems scale up, it becomes challenging for policies, process, and tools to provide robust governance over them (D. Leslie 2019). To manage Al models, MLOps (Machine Learning Operations), emerged to manage the machine learning models' lifecycles, ensuring scalability and compliance with ethical and legal standards with tools supporting model development, testing, and version controls. For automating the tests, a continuous integration system automatically tests and integrates the updates to the ML models. This continuous oversight is critical for businesses as their Al systems expand to handle more complex tasks.\nLeslie (2019) argues that AI governance frameworks like MLOps are essential for managing the ethical and legal risks of AI automation (D. Leslie 2019). MLOps allows businesses to monitor their Al systems in real- time, enabling them to detect and address potential ethical or legal violations before they become significant problems. These frameworks structure AI systems into scalable solutions, while ensuring that the stakeholders could maintain supervision over the models (D. Leslie 2019).\nAlthough there are concerns about scalability of the AI systems, the LLMs evolved over the recent years to process large real-life datasets for understanding people's perceptions of political and environmental issues. An example study by von der Heyde uses GPT-4.0-Turbo to estimate vote shares for the 2024 EU Parliamentary Elections in Germany (L. von der Heyde 2023)."}, {"title": "2.4 Gap in Current Research", "content": "Despite progress in the fields of ethical AI and legal frameworks, a significant gap remains in integrating these areas, particularly when addressing the scalability of AI systems. Most existing frameworks either focus on ethical AI and legal compliance or failing to address how these two areas interact in the context of scalable AI automation (European Commission 2019, S. Barocas 2019). For instance, the Ethics Guidelines for Trustworthy Al emphasize fairness and accountability but do not offer comprehensive guidance on how to ensure legal compliance in sectors like finance or healthcare, where regulatory frameworks apply (European Commission 2019).\nSimilarly, legal frameworks such as GDPR and the EU AI Act primarily focus on compliance with data protection and risk management, which overlooks the broader ethical issues such as bias mitigation and fairness (European Union 2018, M. Veale 2021). This fragmented approach leaves businesses struggling to balance ethical concerns with legal requirements, particularly as their AI systems scale (European Union 2018, M. Veale 2021, S. Barocas 2019). There are concerns that following rigorous ethical guidelines could limit the models' performances as observed in occasional hallucinating responses produced by the large language models when answering multiple-choice questions (L. P. Argyle 2023).\nAlthough there are some existing studies that explores how ethical AI frameworks explains the models' outputs, they struggle to understand why the unconventional or \u201cincorrect\u201d views are underrepresented. While the LLMs accurately predicted voting intentions, they overestimated the proportion of people who are concerned about the impacts of climate change (L. P. Argyle 2023).\nThe performance differences highlight the need for developing an integrated framework balancing ethical governance and legal compliance, while ensuring representation of responses from different stakeholders' perspectives. This gap also highlights the need for future research to develop governance models that simultaneously address these challenges, ensuring that AI systems are both ethically and legally sound as they scale in business environments (D. Leslie 2019, F. Doshi-Velez 2017)."}, {"title": "3. Methodology", "content": "This section outlines the research design, data sources, and ethical considerations employed in developing the integrated framework for ethical AI governance and legal compliance in scalable business applications. A qualitative approach uses case studies and expert feedback to ensure that the developed framework is relevant to application in real-world business environments, while maintaining a focus on ethical and legal considerations."}, {"title": "3.1 Research Design", "content": "The research follows a qualitative synthesis approach, combining academic literature, case studies, and expert feedback. The first phase reviews existing ethical AI frameworks, legal guidelines (e.g., GDPR, EU AI Act), and AI models, such as MLOps (European Union 2018, M. Veale 2021, European Commission 2019).\nThis identifies existing research gaps and practices for integrating ethical and legal governance into Al automation, which provides the contextual information for extracting and building an AI framework while considering accountability, bias mitigation, fairness, and transparency (C. O'Neil 2016, R. Binns 2018, S. Barocas 2019). Identifying profiling variables for AI models are a great method to research the structures of an Al framework. Recent studies analysed pro-environmental behaviours to understand which socio-economic factors influence these behaviours. For example, studies by Whitmarsh and Calvin determined that household size, political issues, and family dynamics are some of the key influencing factors shaping a stakeholder's opinions (L. Whitmarsh 2010, C. W. Calvin 2017). These profiling variables are supported by the legal arguments on compliance with data protection, risk management, and intellectual property regulations (European Union 2018, M. Veale 2021).\nAfter synthesizing the literature, case studies analyse how businesses implement Al governance while following legal guidelines. Case studies, including one by O'Neil gathers diverse insights from industries that already integrated automations, from the financial, healthcare, and e-commerce sectors (C. O'Neil 2016, F. Doshi-Velez 2017). This approach aims to use real-world examples of how Al models are deployed to understand the common challenges that the businesses face (R. Binns 2018). Examples of Al models include GPT-3.5-turbo, an accurate and efficient tool for processing or fine-tuning datasets.\nTo enhance the framework's practical applicability, semi-structured interviews were conducted with legal experts, Al practitioners, and industry leaders involved in Al automation. Experts were selected to understand how the Al models follow the ethical and regulatory requirements and evaluate the quality of their outputs (F. Doshi-Velez 2017, P. H\u00e4m\u00e4l\u00e4inen 2023). Although the human reviewers provide inconsistent scores (0.54 \u2013 0.76) when assessing the validity of LLM-generated gaming reviews, they identified some errors in the LLMs' output such as incoherent, incomplete and contradictory answers (P. H\u00e4m\u00e4l\u00e4inen 2023).\nHaving expert reviews ensures a diverse set of perspectives at guiding automation as reflected in the current Al models' profiling methods of sampling from national representative surveys and social media data (L. P. Argyle 2023, B. Fan 2021). It is important to ensure that the collected data obey strict regulatory oversights, such as the GDPR and EU AI Act (European Union 2018, M. Veale 2021)."}, {"title": "3.2 Data Sources", "content": "Different data sources are selected from business case studies to understand how AI systems make decisions in the financial, health-care, and law enforcement sectors. Regulations, such as data protection (GDPR), risk management (DPIA), and transparency (EU AI Act), ensures an understanding of the sampling methods, whether its parameters are valid, if the data represents the population, and how the data demographics change over time during their evaluation (European Union 2018, M. Veale 2021, University of Essex 2023). A sample dataset is the UK Household Longitudinal Datasets, a sample of 40,000 households collected over thirteen waves of studies between January 2009 and May 2023.\nThe UK Household Longitudinal Study (UKHLS) datasets provide crucial socio-demographic variables, such as age, highest qualification levels, ethnicity, and current job. For example, age was divided into seven groups from \"10 \u2013 19\u201d to \u201c70 or older\u201d. Each selected profiling variable affect the simulation of human attitudes, ensuring that the LLM-generated opinions accurately reflect real-world sentiment.\nBefore applying the dataset, a set of preprocessing and conditioning steps prepares the datasets for training in the selected LLM models. There are some missing or invalid values, -8 (inapplicable), -2 (refusal), and -1 (don't know), which needs to be excluded or replaced by estimates generated from the distributions of valid values. Outliers were removed to maintain data integrity and preventing the results from being skewed by fringe opinions. After processing outliers and imputing invalid values, the data distribution is standardized by using their probability distributions for generating representative subprompts. Each sub-prompt defines a part of the stakeholder's profile that influences how they perceive environmental issues. The prompts were fed into large language models, which allows them to produce synthetic opinions aligning with human sentiments that can be validated with metrics like bar charts and Chi-test scores.\nSustainBench is an open-source dataset that covers seven sustainable development goals (SDGs), areas from poverty estimation to health metrics, education quality, and environmental quality. This dataset was chosen as an example for its pre-defined data loader, which automatically preprocesses and loads the relevant data to perform the required task. As an open-source database, the data is constantly maintained and assessed by users, which ensures that it stays up-to-date and relevant for the tasks at hand.\nThe UKHLS dataset, with its socio-demographic variables, is essential for modeling public opinion and ensuring that Al systems reflect real-world stakeholder sentiments. SustainBench provides sustainability metrics that allow for AI applications to evaluate public attitudes toward environmental policies, ensuring that models remain relevant to the UN Sustainable Development Goals.\nTo determine the source of information to prepare for data selection and preprocessing, the following criteria determines which businesses will be selected as framework for the case studies:\n\u2022 The level of automation that the businesses are adopting.\n\u2022 If the organizations follow the ethical and legal guidelines on model development.\n\u2022 How scalable the AI systems are when adjusting to different volumes of train and test data.\nThe following experts were selected using the following criteria to interpret the models' outputs:\n\u2022 Lawyers and regulatory specialists are expected to be familiar with the GDPR, EU AI Act, and IP law. Their feedback ensures that the framework reflected current legal requirements.\n\u2022 Machine Learning engineers, data scientists, and AI specialists were selected to provide advice on the challenges of scaling AI and how to fine-tune them to follow requirements while obeying the ethical principles(L. Floridi 2019,S. Barocas 2019).\n\u2022 Domain-specific experts are needed to interpret the AI models' outputs to minimize the risks of hallucinating responses and validate them for each designated task.\nHaving a diverse range of experts ensures the development of a framework that bridges the gap between following ethical AI principles and producing outputs that aligns with the stakeholders' requirements (C. O'Neil 2016, R. Binns 2018, F. Doshi-Velez 2017). Another case study uses human annotators to assess the validity of the models' outputs and if they contain any errors (P. H\u00e4m\u00e4l\u00e4inen 2023). These experts are selected based on their approval ratings from a social media site and their proficiencies in English, however, there are concerns about the validities of the ratings since they could be manipulated by users with multiple accounts or bot farms.\nExamples of expert feedback were quantified through binary rating scores of whether a statement is written by a \u201chuman participant\u201d or \u201cartificial intelligence\u201d followed with their reasoning on how they classified the responses. These metrics helps to determine how authentic the Al models' responses are at emulating sample human respondents, which helps to develop models to identify whether the reviews made about the games are authentic (P. H\u00e4m\u00e4l\u00e4inen 2023)."}, {"title": "3.3 Framework", "content": "There have been case studies conducted in industries, such as finance, healthcare, and law-enforcement, to ensure that the frameworks could be applied practically.\nIn healthcare, Al models have already been applied for predictive diagnosis. The automated CT scans reduce diagnosis time and increases accuracy to 84% (M.R. Arbabshirani 2018). This highlights the need for human supervision to check the incorrect 16% of the cases and evaluate the false positives. Most of the healthcare datasets are using free-text format, which requires natural language processing (NLP) to understand how diagnostic information and patient details are extracted. After diagnosis, the treatment results might cause conflicts with the patient or their friends and family due to different focuses, therefore it is vital for Al to provide insights into explaining the ethical challenges of transparency when making decisions that could affect life or death (D. Leslie 2019,F. Doshi-Velez 2017).\nAI also has the potential of transforming the education sectors by improving productivity and teaching methods. Learning about the potential problems a student might encounter could help reducing drop-out rates with essential support to students before their problems become serious. A study by Hasan and Khan found that personalised revision messages for students generated from predictive analytics helped the students to focus on areas where they are struggling and improves the pass rate (M. R. Hasan 2023). Alternative methods include creating tailored revision groups, focused on designing the curriculum by extrapolating from the students' past performance and transitioning the emphasis of education away from testing memory knowledge to applying complex problem-solving in domain-specific scenarios.\nEmbedding AI into the finance sector requires the restructuring of the logistic supply chain network to ensure accountability for the shifts in energy usage and mitigating the strain imposed on the logistic supply chain network (R. K. Green 2020, McKinsey 2024). These steps require designing predictive modelling to perform correlation analysis to establish the cause and effects in supply chains. Before the transformation, developers need to condition the models with sustainability organisations to ensure their understanding of how business consumes energy for optimised infrastructure and financial planning. The implications of the conditioning factors are recorded in Table 1, with the changes made from the following points:\n1. Governing processes need to be restructured to maximise the benefits of Al while ensuring that they contain enough details for the stakeholders to understand.\n2. Changing the organization's mindset on how to approach Al problems are pivotal enabling factors at determining if the businesses will be successful.\n3. Defining a structure to refine and optimise the AI models whenever new services are introduced.\n4. Collect and preprocess high-quality training data to ensure that they are suitable for conditioning the Al models."}, {"title": "3.4 Ethical Considerations", "content": "This research addresses several ethical considerations. Given the sensitive data and high stakes in regulating Al governance models, the following ethical protocols need to be followed.\nAccording to the GDPR requirements, personal data must be anonymized and any potential identifying information must be removed to protect individual privacy. Before collecting data, it is vital to gain consent from all stakeholders, ensuring that they understand how their data will be used and the purpose of the research. Although AI systems shouldn't reveal any sensitive or proprietary information, developers need to balance this against having transparent models that helps the stakeholders to understand how their data is processed to generate suitable outputs (European Union 2018, S. Wachter 2017). On an international level, it could be difficult to set the boundaries for implementing the GDPR requirements. During recent years, AI has been misused by some parties for undermining democratic processes by influencing public opinions on a large scale (N. Seppala 2024).\nWhen evaluating private AI systems, non-disclosure agreements (NDAs) are vital to ensure that sensitive information about the AI systems remain confidential between stakeholders. By enforcing confidentiality, NDA prevents the misuse of private Al systems for malicious purposes, which acts as safeguarding tools against actions that could compromise the integrity of democracy. Also, NDAs are essential for protecting businesses in competitive sectors, such as finance and healthcare (F. Doshi-Velez 2017).\nOn the other hand, it is important to balance privacy against transparency, such as informing the stakeholding businesses and domain-specific experts about how the data collection methods and the results were evaluated. The participants should be given the opportunity to verify if the outputs were as expected and be able to use explainable Al techniques, e.g., feature permutation importance, the Local Interpretable Model-Agnostic Explanations (LIME), and the SHapley Additive exPlanations (SHAP), to ensure that the AI models are interpretable and explainable (S. Bulathwela 2024).\nEfforts were made to minimize any potential bias in data collection by including a diverse range of industries and perspectives, using criteria relevant to their experience with Al governance and compliance (S. U. Noble 2018,F. Doshi-Velez 2017). The interview questions were designed to have open-ended response options, allowing participants to provide in-depth responses without being guided toward conclusions. However, the LLMs generate unnecessary content when answering open-ended questions, such as when describing the characteristics of an average US voter, the LLM-generated responses have an average length of 7.78 words, which exceeded the word limit of four words (L. P. Argyle 2023). This highlights the importance of prompt engineering to fine-tune and adjust the models into producing relevant information."}, {"title": "4. Framework Development", "content": "An ethical AI framework is shaped by the four pillars of ethics, control, viability, and desirability (Fig. 1). Each pillar determines the topics that automation need to address, which set of regulations to follow, and whether the Al systems align with the selected business goals.\nEthical AI focuses on assessing whether the AI systems follow the guiding principles in places and operate fairly, transparently, and accountably. A successful implementation of an ethical AI system achieves the expected outcomes, avoids causing unnecessary harm, and promotes social justice and the protection of individual rights (European Union 2018, C. O'Neil 2016).\nControllable AI must allow human supervision and control, ensuring that the automated systems could be guided without causing unintentional consequences. By aligning with the EU AI Act's risk-based approach, Al systems use human opinions as feedback to adjust or pause the AI models when applying in the high-risk scenarios (European Commission 2021, M. Veale 2021).\nViable AI assesses AI from practical perspectives, focusing on whether its achieving the desired outcomes without causing unforeseen risks. A successful viable solution ensures that the AI functions within the technical and operational constraints of the organisations, ensuring scalability and reliability while deliv-ering sustainable business results (M. Veale 2021, L. Floridi 2019).\nDesirable Al evaluates Al systems from how the models are implemented to minimize the risks while maximizing the benefits. This evaluation focuses on assessing if embedding AI will improve the current business processes and that the solutions are accepted by the users and stakeholders; therefore, this pillar integrates AI s.t. productivity and ethical responsibilities improves (R. Binns 2018, S. Barocas 2019)."}, {"title": "Implementing an ethical AI model requires an incremental and transformative approach that embeds five ethical concepts, including governance, analytics, implementation, ethics enablers, and transformation.", "content": "Following the GDPR and Ethics Guidelines for Trustworthy AI (European Commission 2019), it is possible to ensure that the AI models are fair, transparent, accountable, and impartial. The following principles could be integrated in the models' selection and testing, ensuring that the ethical and legal criteria are met.\nThe proposed framework will be tested through a case study using large language models (LLMs) to simulate public attitudes towards environmental policies. This case represents a high-risk AI application with the potential to shape future green policies aimed at mitigating climate change. Data will be sourced from the UK Household Longitudinal Study, ensuring that the participants were informed of how their data would be used in compliance with GDPR (European Union 2018). The profiling variables are carefully selected based on their relevance to understanding the formation of public views on environmental issues, adhering to GDPR's data minimization principle, which mandates processing only the data necessary for the model's purpose (European Union 2018).\nA robust set of evaluation metrics ensures accountability by quantifying the Al models' performance to assess their decision-making (European Commission 2019). Examples of metrics include the Chi-squared test scores (Equation 1), normalized mutual information (Equations 2 and 3), and Jaccard indexes (Equation 4), for measuring the similarities between the synthetic and expected response distributions. These metrics are applied continuously to track how the synthetic opinions evolve with new developments in environmental policymaking and socio-demographic changes, which enables the developers to promptly detect and address any potential ethical violations (European Commission 2019). The metrics extend performance evaluation to measure accountability through determining that the LLMs are more suitable for answering questions about lifestyle, perception of personal actions on the environment, and if there is a looming environmental disaster (Table 2). These examples enable developers to add transparency to the model and allow prompt feedback for adjusting the models based on new contextual information, therefore ensuring accountability in the designed governing AI frameworks, such as MLOps. This allows businesses to maintain trust while scaling up their systems (D. Leslie 2019, S. Barocas 2019).\n$\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$", "title2": "NMI(X, Y) = $\\frac{2 \\cdot I(X,Y)}{H(X) + H(Y)}$", "title3": "I(X; Y) = $\\int_{\\mathcal{Y}} \\int_{\\mathcal{X}} P_{X,Y}(x,y) log (\\frac{P_{X,Y}(x, y)}{P_X(x)P_Y(y)}) dx dy$", "title4": "J(A, B) = $\\frac{A \\cap B}{A \\cup B}$"}, {"title": "Question", "content": "The feedback obtained from applying the evaluation metrics contribute to mitigating the AI models' bias (C. O'Neil 2016, R. Binns 2018). Such as the metrics help refine the selection of profiling variables to improve the conditioning of the models and identify the areas where the LLMs are perpetuating historical biases in both the results and the training data (S. U. Noble 2018). Other areas of application include credit scoring and policing where it is vital to ensure fair and equitable models for maintaining public trust (R. Binns 2018, F. Doshi-Velez 2017). Through the applications, the metrics explains and quantifies the models from bias- variance perspectives (J. Mourao-Miranda 2024). While biases determine differences between the expected and predicted distributions, variance captures how spread out the responses are."}, {"title": "Controllable Al explores the question, \u201chow to balance the level of autonomy between humans and AI?\u201d to maintain human control over the models while following ethical principles.", "content": "There are different scales of applying control when implementing the Al models on a societal, organizational, and user level. Fig. 3 illustrates the relationship between leadership and technical skills to determine the role of a stakeholder for maintaining control. Shneiderman believes that developers should focus on producing models with high levels of Al autonomy accompanied by varying degrees of human controls to accommodate the skills of people from beginners to Al experts.\nHowever, the level of human control is subject to the Al models' working environment. When applied in higher-risk environments, human operators require higher levels of control (European Commission 2021, \u041c. Veale 2021), which provides the opportunity for humans to intervene whenever necessary to override the AI models' decision. For example, in the implementation of large language models for generating opinions on environmental issues, there are a series of steps for developers to prepare the dataset for the models, evaluate whether to rerun and optimise the LLMs' after reviewing their results, and determine how to present the results in a visually appealing manner for the stakeholders to understand.\nTo maintain control over Al systems over time and when they scale, businesses must continuously monitor and assess them (D. Leslie 2019, S. Barocas 2019). Fidelity tests ensures that the Al models' data are resembling real-world data as closely as possible, while ensuring that any deviations are detected early. In this project, dynamic profile conditioning adapts the systems' inputs and outputs to match the changing environment. All the functionalities provide a long-term validation strategy to evaluate how the models' assumptions and alignments update with the latest real-world data."}, {"title": "5. Analysis and Discussion", "content": "This section critically evaluates the designed AI Framework and how it balances the trade-offs between efficiency and ethical principles. There will be some past examples selected from industries, like finance and healthcare, to understand the challenges that businesses face when implementing an Al framework."}, {"title": "5.1 Framework Evaluation", "content": "The proposed framework integrates key ethical AI principles, like fairness, transparency, accountability, and bias mitigation, with legal frameworks such as the General Data Protection Regulation (GDPR), Data Protection Impact Assessment (DPIA), and the EU AI Act (European Union 2018, European Commission 2021, M. Veale 2021). This ensures that the developed AI systems are scalable, efficient, and compliant with the required legal obligations."}, {"title": "Algorithm: GENDIAG", "content": "Input: A cluster of records C", "m.\nOutput": "A (k, km)-anonymous cluster C' corresponding to C and the number of diagnosis codes s that have been suppressed from C.\n1 Initialize C' = C and s = 0.\n2 Let Q be the set that contains sets of diagnosis codes, each of which contains up to m codes and appears in fewer than k records of C'.\n3 while Q is not empty do\n4 Let p be the element of Q that appears in the largest number of records.\\"}]}