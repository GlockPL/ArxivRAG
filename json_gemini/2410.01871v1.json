{"title": "Auction-Based Regulation for Artificial Intelligence", "authors": ["Marco Bornstein", "Zora Che", "Suhas Julapalli", "Abdirisak Mohamed", "Amrit Singh Bedi", "Furong Huang"], "abstract": "In an era of \"moving fast and breaking things\u201d, regulators have moved slowly to pick up the safety, bias, and legal pieces left in the wake of broken Artificial Intelligence (AI) deployment. Since AI models, such as large language models, are able to push misinformation and stoke division within our society, it is imperative for regulators to employ a framework that mitigates these dangers and ensures user safety. While there is much-warranted discussion about how to address the safety, bias, and legal woes of state-of-the-art AI models, the number of rigorous and realistic mathematical frameworks to regulate AI safety is lacking. We take on this challenge, proposing an auction-based regulatory mechanism that provably incentivizes model-building agents (i) to deploy safer models and (ii) to participate in the regulation process. We provably guarantee, via derived Nash Equilibria, that each participating agent's best strategy is to submit a model safer than a prescribed minimum-safety threshold. Empirical results show that our regulatory auction boosts safety and participation rates by 20% and 15% respectively, outperforming simple regulatory frameworks that merely enforce minimum safety standards. Code can be found on GitHub at https://github.com/marcobo rnstein/AI-Regulatory-Auctions.", "sections": [{"title": "Introduction", "content": "Current Artificial Intelligence (AI) models are powerful and have revolutionized a wide swath of industries. The recent large-scale deployment of Large Language Models (LLMs) has simultaneously boosted human productivity while sparking concern over safety (e.g., hallucinations, bias, and privacy). We have seen industry leaders, such as Google, Meta, and OpenAI, embroiled in controversy surrounding bias and misinformation [Brewster, 2024, Robertson, 2024, White, 2024], safety [Jacob, 2024, Seetharaman, 2024, White, 2023], as well as legality and ethics [Bruell, 2023, Metz et al., 2024, Moreno, 2023] in their development and deployment of LLMs. Furthermore, irresponsible deployment of LLMs runs the risk of allowing adversaries the ability to spread misinformation or propaganda [Barman et al., 2024, Neumann et al., 2024, Sun et al., 2024]. Unfortunately, a consistent and industry-wide solution to oversee safe AI deployment remains elusive.\nNaturally, one such solution to mitigate these dangers is for increased governmental regulation over AI deployment. In the United States, there have been some strides, on federal [House, 2023] and state levels [Information, 2024], to regulate the safety and security of large-scale AI systems (including LLMs). While these recent executive orders and bills highlight the necessity to develop safety standards and enact safety and security protocols, few details are offered. This follows a consistent trend of well-deserved scrutiny towards the lack of AI regulation without the development of rigorous and realistic mathematical frameworks to regulate. Our work sets out to solve this disconnect by proposing a novel regulatory framework that a regulator can follow to not only strictly enforce the safety of deployed AI models, but simultaneously incentivize the production of safer AI models.\nThe goal of our work is to formulate the AI regulatory process mathematically, and subsequently develop a mechanism to incentivize safer development and deployment of AI models. Specifically, we formulate the AI regulatory process as an all-pay auction, where agents (companies) submit their models to a regulator. The regulator's job is twofold: (a) prohibit deployment of models that fail to meet prescribed safety thresholds, and (b) incentivize safe model production and deployment by providing additional rewards to agents that submit safer models than their peers. We design an auction-based regulatory mechanism, with a novel reward-payment protocol, that emits Nash Equilibria at which agents develop and deploy models safer than the prescribed safety threshold."}, {"title": "Related Works", "content": "Regulation Frameworks for Artificial Intelligence. A handful of work focuses on regulation frameworks for AI [de Almeida et al., 2021, Jagadeesan et al., 2024, Rodr\u00edguez et al., 2022, Yaghini et al., 2024]. First, de Almeida et al. [2021] details the need for AI regulation and surveys existing proposals. The proposals are ethical frameworks detailing specific ethical decisions to make and dilemmas to address. These proposals lack a mathematical framework to incentivize provably safer models. Rodr\u00edguez et al. [2022] utilize AI models to detect collusive auctions. This work is related to our own but in reverse: AI is applied to regulate auctions and ensure that they are not collusive. In contrast, our work aims to use auctions to regulate AI deployment. Jagadeesan et al. [2024] focuses on reducing barriers to entry for smaller companies who are competing against larger and more established incumbent companies. A multi-objective high-dimensional regression framework is proposed to capture \u201creputational damage\u201d for companies who deploy unsafe AI models. This work allows varying levels of safety constraints, where newer companies face less severe constraints in order to spur their entry into the market, which is unrealistic in many settings and only considers simple linear-regression models. The closest related work to ours, Yaghini et al. [2024], proposes a regulation game for ensuring privacy and fairness that is formulated as a Stackelberg game. This game is a multi-agent optimization"}, {"title": "The Regulatory AI Setting", "content": "There exists a regulator R with the power to set and enforce laws and regulations (e.g., U.S. government regulation on lead exposure). The regulator wants to regulate AI model deployment, by ensuring that all models meet a given safety threshold $\\epsilon \\in (0, 1)$, e.g., the National Institute for Occupational Safety and Health regulates that N95 respirators filter out at least 95% of airborne particles. If a model does not reach the safety criteria $\\epsilon$, then the model is deemed unsafe and the regulator bars deployment. On the other side, there are n rational model-building agents. Agents seek to maximize their own benefit, or utility.\nBidding & Evaluation. By law, each agent i must submit, or bid in auction terminology, its model $w_i \\in \\mathbb{R}^d$ for evaluation to the regulator before it can be approved for deployment. Let $S(w; x) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^+$ be a safety metric that outputs a safety level (the larger the better) for model w given data x. In effect, each agent, given its own data $X_i$, bids a safety level $s^a_i := S(w_i; x_i)$ to the regulator. Subsequently, the regulator, using its own data $X_R$, independently evaluates the agent's safety level bid as $s^r_i := S(W_i; X_R)$. We assume that agent and regulator evaluation data is independent and identically distributed (IID) $x_i, X_R \\sim D$.\nAssumption 1. Agent and regulator evaluation data comes from the same distribution $x_i, X_R \\sim D$.\nThis assumption is often realistic in regulatory settings, because both agents and regulators typically rely on standardized or widely accepted datasets for model evaluation, ensuring a fair and unbiased assessment of safety levels. For instance, when assessing LLMs, common datasets like benchmarks for toxicity or bias are employed consistently across evaluations, reflecting real-world data distributions. Therefore, it is reasonable to define agent i's safety level bid as $S_i := \\mathbb{E}_{x \\sim D}[S(w_i; x)]$. We address scenarios where evaluation data may not follow the IID assumption in Section 7.\nThe Price of Safety. We assume that there exists a strictly increasing function $M : (0, 1) \\rightarrow (0, 1)$ that determines the \"price of safety\" (i.e., maps safety into cost). Simply put, safer models cost more to attain. As a result, we define the price of attaining $\\epsilon$ safety as $p_\\epsilon := M(\\epsilon)$.\nAssumption 2. There exists a strictly increasing function M that maps safety to cost.\nThe assumption that a strictly increasing function M maps safety to cost is realistic, because achieving higher safety levels typically requires greater resources. Safer models often demand more data, advanced tuning, and extensive validation, all of which increase costs. Thus, defining the price of safety as $p_\\epsilon := M(\\epsilon)$, where M is strictly increasing, reflects the practical trade-off that safer models cost more to develop."}, {"title": "Agent Costs", "content": "Unfortunately for agents, training a safer model comes with added cost. Consequently, each agent i must decide how much money to bid, or spend, $b_i$ to make its model safer. By Assumption 2, the resulting safety level of an agent's model will be $s_i = M^{-1}(b_i)$."}, {"title": "Agent Values", "content": "(1) Model deployment value $v^d_i$. While it costs more for agents to produce safer models, they gain value from having their models deployed. Intuitively, this can be viewed as the expected value $v_d$ of agent i's model. The valuation for model deployment varies across agents (e.g., Google may value having its model deployed more than Apple). (2) Premium reward value $v^p_i$. Beyond value for model deployment, the regulator can also offer additional, or premium, compensation valued as $v$ by agents (e.g., tax credits for electric vehicle producers or Fast Track and Priority Review of important drugs by the U.S. Food & Drug Administration). The regulator provides additional compensation to agents whose models demonstrate safety levels exceeding the prescribed threshold. However, the value of this compensation varies across agents due to differing internal valuations. It is unrealistic for the regulator to compensate all agents meeting the safety threshold due to budget constraints. Therefore, we limit the additional rewards to a top-performing half of agents who surpass the threshold, ensuring that compensation targets those contributing the most to enhanced safety while maintaining feasibility for the regulator."}, {"title": "Value Distribution", "content": "We define the total value for each agent i as $V_i := v^d_i + v^p_i$, which represents the sum of the deployment value and premium compensation. Although these values can vary widely in practice, we normalize $\\{V_i\\}_{i=1}^n$ for all n agents to be between 0 and 1 for analytical tractability, allowing us to work within a standardized range. Consequently, the price to achieve the safety threshold $\\epsilon$ is also normalized to fall within the (0, 1) interval, i.e., $\\mu_\\epsilon \\in (0, 1)$.\nThe proportion of total value allocated to deployment versus compensation is determined by a scaling factor $\\lambda_i \\sim D_\\lambda(0, 1/2)$. Therefore, the deployment value is $v^d_i := (1 - \\lambda_i)V_i$, and the premium compensation value is $v^p_i := \\lambda_i V_i$. Both $V_i$ and $\\lambda_i$ are private to each agent, though the distributions $D_V$ and $D_\\lambda$ are known by participants. We set the maximum allowable factor at $\\lambda_i = 1/2$, reflecting the realistic constraint that compensation should not exceed deployment value. Although our results primarily consider $\\lambda_i \\le 1/2$, theoretical extensions can be made for scenarios where $\\lambda_i > 1/2$."}, {"title": "All-Pay Auction Formulation", "content": "Overall, agents face a trade-off: producing safer models garners value, via the regulator, but incurs larger costs. Furthermore, in order to attain the rewards detailed in Section 3, agents must submit a model with safety level at least as large as $\\epsilon$. We can formulate this problem as an asymmetric all-pay auction with incomplete information [Amann and Leininger, 1996, Bhaskar, 2018, Tardos, 2017]. The problem is an all-pay auction since agents incur an unrecoverable cost, safety training costs, when submitting their model to regulators. The problem is asymmetric with incomplete information since valuations $V_i$ are private and differ for each agent."}, {"title": "Agent Objective", "content": "The objective for each model-building agent i, is to maximize its own utility $u_i$. Namely, each agent seeks to determine an optimal safety level to bid to the regulator $b_i$. However, depending upon the all-pay auction formulation, agents would need to take into account all other agents' bids $b_{-i}$ in order to determine their optimal bid $b_i$:\n$b_i:= \\underset{b_i}{\\arg \\max} u_i(b_i; b_{-i}).$\nA major portion of our work is constructing an auction-based mechanism, thereby designing the utility of each agent, such that participating agents maximize their utility when they bid more than \u201cthe price to obtain the safety threshold\", i.e., $b_i > p_\\epsilon$. We begin by providing a simple mechanism, already utilized by regulators, that does not accomplish this goal, before detailing our auction-based mechanism SIRA that provably ensures that $b_i > p_\\epsilon$ for all agents.\""}, {"title": "Reserve Thresholding: Bare Minimum Regulation", "content": "The simplest method to ensure model safety is for the regulators to set a reserve price, or minimum acceptable safety. We term this mechanism a multi-winner reserve thresholding auction, where the regulator awards a deployment reward, $v^d$, to each agent whose submitted model meets or exceeds the safety threshold $\\epsilon$. Within this auction, each agent i's utility is mathematically formulated as,\n$U_i(b_i; b_{-i}) = \\begin{cases} -b_i & \\text{if } b_i < p_\\epsilon \\\\ v^d - b_i & \\text{if } b_i \\geq p_\\epsilon. \\end{cases}$\nThe formulation above, however, is ineffective at incentivizing agents to produce models that are safer than the $\\epsilon$ threshold."}, {"title": "Safety-Incentivized Regulatory Auctions (SIRA)", "content": "To alleviate the lack of incentives within simple regulatory auctions, such as the one in Section 4, we propose a regulatory all-pay auction that emits an equilibrium where agents submit models with safety levels larger than $\\epsilon$.\nAlgorithm Description. The core component of our auction is that agent safety levels are randomly compared against one another, with the regulator rewarding those having the safer model with premium compensation. Only agents with models that achieve a safety level of $\\epsilon$ or higher are eligible to participate in the comparison process; models that do not meet this threshold are automatically rejected. The detailed algorithmic block of SIRA is depicted in Algorithm 1.\nAgent Utility. The utility for each agent i is therefore defined as in Equation (5).\n$U_i(b_i; b_{-i}) = (v^d_i + v \\cdot \\mathbb{1}( \\text{if } i \\text{ wins comparison})) \\cdot \\mathbb{1}( \\text{if } b_i \\geq p_\\epsilon) - b_i$.\nPer regulation guidelines, the safety criteria of an accepted model must at least be $\\epsilon$. Equation (5) dictates that values are only realized by each agent if their model has a bid larger than the required cost to reach $\\epsilon$ safety, $\\mathbb{1}( \\text{if } b_i \\geq p_\\epsilon)$. Furthermore, agents only realize additional compensation value $v$ from the regulator if their safety level outperforms a randomly selected agent, $\\mathbb{1}( \\text{if } i \\text{ wins comparison})$. Any agent that bids $b_i = 1$ will automatically win and realize both $v^d$ and $v$. It is important to note that the cost that every agent incurs when building its model is sunk: if the model is not cleared for deployment, the cost $-b_i$ is still incurred. We rewrite the agent utility in a piece-wise manner below,\n$U_i(b_i; b_{-i}) = \\begin{cases} -b_i & \\text{if } b_i < p_\\epsilon, \\\\ v^d_i - b_i & \\text{if } b_i \\geq p_\\epsilon \\text{ and } b_i < b_j \\text{ randomly sampled agent bid } b_j, \\\\ v^d_i + v - b_i & \\text{if } b_i \\geq p_\\epsilon \\text{ and } b_i > b_j \\text{ randomly sampled agent bid } b_j. \\end{cases}$\nBy introducing additional compensation, v, and, crucially, conditioning it on whether an agent's model is safer than that of another random agent, we seek to make it rational for agents to bid more than the price to obtain the minimum safety threshold (unlike Theorem 1)."}, {"title": "Incentivizing Agents to Build Safer Models", "content": "We establish a guarantee that agents participating in SIRA maximize their utility with an optimal bid $b_i^*$ that is larger than \u201cthe price required to attain $\\epsilon$ safety\" (i.e., $b_i^* > p_\\epsilon$) in Theorem 2. Further, agents bid in proportion to the additional compensation value $v^p_i$ that the regulator awards for extra safe models."}, {"title": "Conclusion and Future Work", "content": "As AI models grow, the risks associated with their misuse become increasingly significant, particularly given their often opaque, black-box nature. Establishing robust algorithmic safeguards is crucial to protect users from unethical, unsafe, or illegally-deployed models. In this paper, we present a regulatory framework designed to ensure that only models deemed safe by a regulator can be deployed for public use. Our key contribution is the development of an auction-based regulatory mechanism that simultaneously (i) enforces safety standards and (ii) provably incentivizes agents to exceed minimum safety thresholds. This approach encourages broader participation and the development of safer models compared to baseline regulatory methods. Empirical results confirm that our mechanism increases agent participation by 15% and raises agent spending on safety by 20%, demonstrating its effectiveness to promote safer AI deployment.\nFuture Work. While this work addresses key challenges in regulating AI safety, several directions remain open for future exploration:\n(1) Model Evaluation: Creating a realistic protocol for the regulator to evaluate submitted model safety levels is important to ensure agents do not skirt around safety requirements. While we leave this problem for future work, one possible solution is that agents can either provide the regulator API access to test its model or provide the model weights directly to the regulator. Truthfulness can be enforced via audits and the threat of legal action.\n(2) Extension to Heterogeneous Settings: Extending our mechanism to heterogeneous scenarios, where evaluation data for agents and regulators differs, is a critical next step. Real-world data distributions often vary across contexts, and understanding how these variations affect both model safety and agent strategies will create a more robust regulatory mechanism. While explicit protocols or mathematical formulations are left as future work, we have a few ideas. One idea could be establishing a data-sharing framework between agents and the regulator, where each participating agent must contribute part of (or all of) its data to the regulator for evaluation. If data can be anonymized, then this would be a suitable solution. Another idea could be that the regulator collects data on its own, and can compare its distribution of data versus each participating agents' data distribution. If distributions greatly differ, then the regulator could collect more data or resort to the previous data-sharing method."}, {"title": "Repeating SIRA Auctions", "content": "The current auction structure (Algorithm 1) expects agents to submit a single model trained solely for the upcoming auction. There is no expectation that the model will be reused for a future auction, or indication that the model has been submitted to a previous auction. Looking towards the future, we would like to design SIRA to fit a repeatable auction structure, in which approved or rejected models may be resubmitted in subsequent auctions.\nRepeated Agent Utility. Previously, in Algorithm 1, agents start the regulatory process with zero cost and value (i.e., they are building their models from scratch). In repeating SIRA auctions, agent cost and value are accumulated across all previous auction submissions. For example, if an agent trains its already-accepted model further to attain a higher safety level $s_i$, its total accumulated training cost is $M^{-1}(s_i)$. This agent's total value becomes the value its model gained from previous auction submissions plus any value gained from the current auction.\nBy allowing repeated SIRA auctions, an agent is able to repeatedly submit its model for regulatory review. We note that repeated submissions decrease the value of model deployment; once an agent earns the reward for deploying their model, subsequent deployments of the same model with improved safety levels can be realistically expected to earn less value than the initial deployment. We characterize this loss in value for repeated submissions with an indicator function in the utility function that only allows deployment value to be obtained once, on initial acceptance of a model. While we allow agents to win premium rewards across multiple auctions, we note that a regulator can curb this by either limiting the number of auction submissions per agent or the number of auctions held per year. We now define the repeated SIRA auction utility of agent i, who has participated in a - 1 previous auctions, as:\n$U_{i,a}(b_i) = v_i + \\sum_{n=1}^{a}  \\nu^{r}_i,$\nwhere vr, the value gained at the nth auction model i was submitted to, is formulated as:\n$\\nu^{r}_i = \\begin{cases} v^{d,n}_i \\cdot \\mathbb{1}(v^{d,n}_{i} = 0) + v^{p,n}_i \\text{ if } b^*_i \\geq p_\\epsilon \\text{ and } b < b_j \\text{ randomly sampled bid } b_j, \\\\ v^{d,n}_i \\cdot \\mathbb{1}(v^{d,n}_{i} = 0) + v^{p,n}_i \\text{ if } b^*_i \\geq p_\\epsilon \\text{ and } b > b_j \\text{ randomly sampled bid } b_j, \\\\ 0 & \\text{ if } n \\leq 0. \\end{cases}$\nThe repeated SIRA auction setup creates a unique property for models in training. If an agent intends to obtain a high safety level, but an auction takes place mid-training, the agent is actually incentivized to submit their model early if they have a chance at winning the premium reward. Though the model may have a lower likelihood of earning the reward, there is no consequence for models failing to attain the premium reward. Gaining value is strictly beneficial to agents, and accumulated value helps offset the costs of training a model. This property only exists for the premium reward; the deployment reward can only be obtained once, thus there is no incentive to submit early to earn it.\nRepeated Optimal Bidding Function. Using the same assumptions for single-auction SIRA, namely Assumptions 1 and 2 along with private values, we can derive the bidding function for a rational agent under a repeated SIRA auction setting. We follow an equivalent setup to Lemma 1 with regards to the valuation of rewards, giving us the cumulative distribution function for $\\upsilon_i = V_i\\lambda_i$ as $F_\\upsilon(\\cdot)$ and the probability distribution function as $f_\\upsilon(\\cdot)$.\nFrom our definition of utility uia(bi), we find that an agent i that does not participate (i.e., submitting $b_i = 0$) receives utility equal to $v_i$. However, since $b_i = 0$ will never be larger than $p_\\epsilon$ (by definition), it must be true that $v_i = 0$ as well, since the model will never meet the required safety threshold. Therefore, a non-participating agent will always receive non-negative utility.\n$U_{i,a}(0) = 0$.\nFollowing closely to the proof of Theorem 2 in Appendix B, we find that participating agents i \u2208 P (with P defined in the previous proof) will now have a utility of,\n$U_{i,a}(b_i) = v_i + v \\cdot \\mathbb{1}(\\nu = 0) + P(b_i > b_j) - b(b_i), b_j \\sim \\text{ randomly sampled agent bid, }$\n$= v_i + v \\cdot \\mathbb{1}(\\nu = 0) + vF_\\upsilon(b_i) - b(b_i)$.\nTaking the derivative and setting it equal to zero yields,\n$\\frac{d}{db_i}U_{i,a}(b_i) = v f_\\upsilon(b_i) - b'(b_i) = 0$."}]}