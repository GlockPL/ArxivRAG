{"title": "SemiHMER: Semi-supervised Handwritten Mathematical Expression Recognition using pseudo-labels", "authors": ["Kehua Chen", "Haoyang Shen"], "abstract": "In this paper, we study semi-supervised Handwritten Mathematical Expression Recognition (HMER) via exploring both labeled data and extra unlabeled data. We propose a novel consistency regularization framework, termed SemiHMER, which introduces dual-branch semi-supervised learning. Specifically, we enforce consistency between the two networks for the same input image. The pseudo-label, generated by one perturbed recognition network, is utilized to supervise the other network using the standard cross-entropy loss. The SemiHMER consistency encourages high similarity between the predictions of the two perturbed networks for the same input image and expands the training data by leveraging unlabeled data with pseudo-labels. We further introduce a weak-to-strong strategy by applying different levels of augmentation to each branch, effectively expanding the training data and enhancing the quality of network training. Additionally, we propose a novel module, the Global Dynamic Counting Module (GDCM), to enhance the performance of the HMER decoder by alleviating recognition inaccuracies in long-distance formula recognition and reducing the occurrence of repeated characters. The experimental results demonstrate that our work achieves significant performance improvements, with an average accuracy increase of 5.47% on CROHME14, 4.87% on CROHME16, and 5.25% on CROHME19, compared to our baselines.", "sections": [{"title": "1 Introduction", "content": "Handwritten mathematical expression recognition (HMER) is a crucial task in Optical Character Recognition (OCR) with applications in digital libraries, office automation, automatic grading. Despite significant progress, HMER remains challenging due to the complex structure of formulas, personalized handwriting styles, and the substantial cost and time involved in manual labeling. Semi-supervised learning, which leverages both labeled and unlabeled data, offers a promising solution to these challenges.\nIn semi-supervised learning, consistency regularization and self-training are two widely used techniques [18]. Consistency regularization-based methods promote the network to generate consistent predictions for the same input with"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 \u0397\u039cER", "content": "Handwritten mathematical expression recognition (HMER) in the field of optical character recognition (OCR) faces significant challenges due to the diverse writing styles and the nested, hierarchical structures of handwritten expressions. Traditional methods [4, 8, 10, 11, 24, 26] typically involve two steps: first identifying individual characters and then applying grammatical rules for correction. However, Limited feature-learning capabilities and complex grammatical rules make these traditional methods inadequate for practical applications. With the widespread application and impressive performance of machine learning and deep learning networks in academia, encoder-decoder architectures have increasingly been applied to handwritten formula recognition. This end-to-end recognition framework, resembling a global recognition approach, has significantly improved accuracy in handwritten expression recognition and effectively handles the strong contextual dependencies within formulas.\nCurrent deep learning-based formula recognition methods can be categorized into two types based on the decoding strategy: one method decodes the formula as a LaTeX expression string, while the other employs tree decoding based on the positional relationships between parent and child nodes. Both approaches have surpassed traditional formula recognition methods in terms of accuracy."}, {"title": "\u2022 Sequence-based Methods.", "content": "The sequence-based decoding approach for handwritten formula recognition was first introduced by Deng et al. [1], achieving the initial transformation from image to LaTeX expression via an encoder-decoder architecture. Building on this, Zhang et al. [41] proposed the WAP model which utilizes a fully convolutional encoder to extract image features and introduces a coverage attention mechanism to address attention inaccuracies in long-distance decoding, establishing WAP as the foundational model for most sequence-based recognition methods. Zhang et al. [38] further improved upon this with DenseWAP, which employs a DenseNet structure to enhance the encoder and proposes a multi-scale attention decoding model to address challenges in recognizing characters of varying sizes within formulas. In recent years, numerous advancements [3,7,12-14,16,22,28,29,37,39] have been built on this model. Wu et al. [28, 29] introduced adversarial learning with handwritten and printed text, while Guo et al. [7] and Ling et al. [14] incorporated contrastive learning, aiding the model in learning style-invariant semantic features and reducing the influence of handwriting styles on formula recognition. Truong et al. [22] and Li et al. [12] incorporated"}, {"title": "\u2022 Tree-based Methods.", "content": "Tree-based methods convert data expressions into tree structures, which allows for hierarchical relationships to be modeled and enables tree decoders to learn features grounded in grammatical rules. Zhang et al. [40] proposed the first image-to-token tree decoding model TDv1. Building on this, Wu et al. [27] introduced the TDv2 model, which removes prioritization among branches of tree nodes to enhance the model's generalization capability and employs a diversified tree structure labeling system to greatly simplify the decoding process. Yuan et al. [35] presented the first tree decoding model that integrates syntactic information into the encoder-decoder architecture, introducing decomposition rules based on the syntactic characteristics of formulas. This approach segments formulas into different components to construct the formula tree, effectively mitigating ambiguities associated with tree structures. TAMER [42] integrates the strengths of both sequence and tree decoding models, enhancing the model's comprehension and generalization of complex mathematical expression structures through the simultaneous optimization of sequence prediction and tree structure prediction objectives. PosFormer [6] jointly optimizes expression recognition and position recognition tasks to explicitly enable position-aware symbol feature learning for representation."}, {"title": "2.2 Semi-supervised learning", "content": "SSL (Semi-supervised learning) is an effective approach to improve model recognition accuracy in scenarios where there is a small amount of labeled data and a larger amount of unlabeled data. These algorithms can be broadly divided into the following categories:"}, {"title": "\u2022 Consistency training.", "content": "Based on the assumption that if a reasonable perturbation is applied to an unlabeled dataset, the model's prediction should not change significantly. Therefore, the model can be trained to produce consistent predictions on a given unlabeled sample and its perturbed version."}, {"title": "\u2022 Pseudo-label Method.", "content": "Pseudo-labeling or self-training is a typical technique for leveraging unlabeled data. It alternates between pseudo-label prediction and feature learning, encouraging the model to make confident predictions for unlabeled data."}, {"title": "3 The Approach", "content": "From the perspective of architecture, we propose typical double-branch mutual learning methods and from strong to weak data augmentation, conservative-progressive collaborative learning to obtain additional training signals. The two branches are trained simultaneously with heterogeneous knowledge to reduce misleading and model coupling problems."}, {"title": "3.1 Overview", "content": "Figure 1 shows our proposed semi-supervised framework for handwritten mathematical expression recognition. The framework employs a cross-decoder architecture that processes both labeled and unlabeled data through a shared network. Following CAN [12], we adopt DenseNet [9] as the backbone for feature extraction. Instead of using separate individual models, we innovatively design two identical decoder heads working in parallel, which we term the cross-head structure. This design enables more efficient feature learning and enhances the model's generalization capability through mutual supervision.\nOur semi-supervised learning strategy operates in two parallel streams. For labeled data processing, we first feed weakly augmented images through the encoder and one of the decoders to generate initial predictions. These predictions serve dual purposes: they are supervised by ground truth labels for"}, {"title": "3.2 Weak-to-strong augmentation", "content": "To fully leverage the advantages of consistency training, We utilize weak and strong augmentations to introduce additional information into our framework. In our experiments, weak augmentation means no augmentation, whereas strong augmentation combines distortions, stretching, and perspective changes. Specifically, we randomly distort images every time, stretch images with a probability of 50%, and perspective images with a probability of 30%.\nOur approach creates pseudo-labels through consistency regularization and self-training. These pseudo-labels are intermediate outputs generated from images with both weak and strong augmentations. They serve as a supervisory signal when the model processes two images simultaneously. We've devised a pool of operations that includes three kinds of image transformations. Rather than applying a uniform intensity level of transformation across all training iterations, we choose transformations for each sample in a mini-batch randomly from a set range at every training epoch.\nBy alternating the application of augmentations and dynamically training the decoders, our framework not only maximizes the effective use of both labeled and unlabeled data but also fosters a robust learning environment that is capable of accurately recognizing handwritten mathematical expressions. This systematic approach ultimately leads to improved performance in real-world applications."}, {"title": "3.3 Global Dynamic Counting Module", "content": "CAN [12] calculates the global counting vector from the global image features by the encoder using an additional multi-scale counting module. This global counting vector is then fed into the decoder to inform the decoding process. By providing the decoder with this additional weak supervision information, the model's performance is effectively improved. However, in this approach, the global count vector is only utilized as auxiliary information to calculate the current output, without being more deeply integrated with the formula prediction task. As a result, it cannot provide the model with more useful guidance during decoding, and the issues of long-distance formula recognition problems and errors when the same character appears repeatedly remain challenging to address.\nTo overcome these limitations and better leverage the relationship between formulas and count vectors to guide the model, we introduce the concept of GDCM(Global Dynamic Counting Module) into the Handwritten Mathematical Expression Recognition task. Specifically, based on the encoder-decoder recognition logic, the decoder relies on the previous state to predict the current output at each time step. Therefore, the feature information input to the decoder should be dynamically updated at each step. Our model achieves this by dynamically updating the global count vector based on the predicted value from the previous time step, while also updating the hidden state and context information. The updated count vector is then used to calculate the output probability for the current time step.\nIn their work, the decoder uses the context vector C, the hidden state $h_t$, and the embedding $E(y_{t-1})$ to predict $y_t$. To address the lack of global information,"}, {"title": "3.4 Loss function", "content": "In the setting of semi-supervised learning, we provide a batch of labeled samples $D_l$ at each iteration. We preliminarily $L_{cls}$ as standard cross entropy classification loss of the word predicted probability $P(y_t)$, with $W$ and $S$ representing the corresponding weak and strong augmentation functions applied to the image respectively. The $L_{cls}^{unsup}$ defined as:\n$L_{cls}^{unsup} = L_{cls}(P^S(y_t), P^W(y_t))$\nwhere $L_{cls}^{sup}$ was defined as :\n$L_{cls}^{sup} = L_{cls}W(x_t) + L_{cls}S(x_t)$\nFor the unlabeled samples $D_{un}$ provided at each iteration, we also use the standard cross-entropy classification loss for supervision. Specifically, we denoted the unlabeled samples's predictions as $y_t$. The predictions from weak supervision are used to supervise the predictions from strong supervision, defined as $L_{cls}^{unsup}$ as follows:\n$L_{cls}^{unsup} = L_{cls}^{unsup}(P^S(y_t), P^W(y_t))$\nFurthermore, for each decoder, there is a counting ground truth loss for each symbol class of decoder's prediction result, represented by V. It is a smooth L1 loss [19] function used to calculate the loss between the predicted count and the ground truth for each formula (not shown in 1), defined as follows:\n$L_{counting} = SmoothL_1 \\cdot W(V_1) + SmoothL_1 \\cdot S(V_2)$\nSo the total loss function is composed of four components and is defined in the following manner:\n$L = L_{cls}^{unsup} + L_{cls}^{sup} + L_{cls}^{unsup} + L_{counting}$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We conduct experiments on CROHME benchmark datasets and compare the performance with the state-of-the-art methods.\nCROHME Dataset [17] is from the competition on recognition of online handwritten mathematical expression, which is the most widely used public dataset. It is derived from the competition focused on recognizing online handwritten mathematical expressions. The CROHME dataset creates images from handwritten stroke trajectory data found in InkML files, leading to images with clean backgrounds. The CROHME training set number is 8,836, while the test set contains 986, 1147 and 1199 images, including 111 symbol classes (C), including \"sos\" and \"eos\". We randomly selected 1,000 images from the CROHME 2023 dataset [31] as unlabeled data, with all characters included in the CROHME Dataset."}, {"title": "4.2 Implementation details", "content": "The proposed SemiHMER is implemented in PyTorch. To accelerate the iteration efficiency, we used four Nvidia Tesla V100 GPUs, each equipped with 32GB RAM, to train the model. It is worth noting that training SemiHMER on a single GPU would take at least one week, which is highly inefficient. The batch size is set to 12, and the Adadelta [36] optimizer is used for training. The learning rate starts at 0 and monotonically increases to 2 by the end of the first epoch. After that, the learning rate decays to 0 according to a cosine annealing schedule. To enhance the effectiveness of semi-supervised learning, we did not use cross-training in the first 240 epochs, allowing each branch to train independently. After 240 epochs, we conducted semi-supervised cross-training with a learning rate of 0.2 and further fine-tuned for another 240 epochs. Our method relies on data augmentation techniques, and we employed various augmentation methods (distortion, affine transformation, and stretching) to explore the capabilities of our approach."}, {"title": "4.3 Evaluation Metrics", "content": "We use the expression recognition rate (ExpRate) to evaluate the performance of different approaches. ExpRate is defined as the percentage of predicted mathematical expressions that exactly match the ground truth."}, {"title": "4.4 Comparison with State-of-the-Art", "content": "To demonstrate the superiority of SemiHMER, we compare it with previous state-of-the-art methods. Since SemiHMER heavily relies on the effectiveness of data augmentation, we primarily focus on the results obtained when data augmentation is included during the model training process. As shown in Table. 1, using CAN as the baseline, our multi-GPU training with four V100 GPUs and a learning rate of 2 did not achieve the original performance of CAN, yielding only 55.48% on CROHME 2014, 53.36% on CROHME 2016 and 53.38% on CROHME 2019 which are lower than the results proposed by CAN. However, despite extensive tuning of hyperparameters and configurations, we were unable to fully close this gap, highlighting a limitation of our current implementation. After applying data augmentation, the performance improved by 2.84%(58.32%) on CROHME 2014, 3.84%(57.2%) on CROHME 2016 and 4.25%(57.63%) on CROHME 2019. With the aid of optimal data augmentation, dual-branch semi-supervised learning further enhanced the model by 2.63%(60.95%) on CROHME"}, {"title": "4.5 Ablation Study", "content": "In this subsection, we discuss the contribution of each component within our framework."}, {"title": "4.6 Effectiveness of Global Dynamic Counting Module", "content": "GDCM boosts performance to 56.30,56.14,53.54 by updating the hidden state and context information, which shows the effectiveness of our Global Dynamic Counting method."}, {"title": "4.7 Effectiveness of the weak-to-strong augmentation strategy", "content": "By utilizing the weak-strong strategy, we can introduce more information to improve consistency. The strong transformation function generates augmented images with different intensities each time, which leads to improved model performance. However, excessive augmentation can cause a performance decline, possibly due to a significant increase in erroneous predictions from the other branch, misleading the network's optimization direction. To allow the model to benefit from both data augmentation and cross-pseudo supervision strategies, we conducted experiments with different augmentation intensities. A natural idea is to gradually increase the intensity of augmentation. As shown in Table 3, by progressively introducing warping, stretching, and perspective transformations into the model, performance on the dataset CROHME2014 improved by 0.09%, 0.58%, and 2.02% respectively. Finally, when applying both weak-strong augmentations and cross-pseudo supervision strategies to both branches, the results further improved by 1.92%, 2.13%, and 2.63%, demonstrating the effectiveness of our strategy: as the augmentation becomes more effective, the impact of CPS becomes more pronounced."}, {"title": "4.8 The trade-off weight A", "content": "A is used to balance the trade-off between supervised loss and unsupervised loss. Table 5 shows that x = 10-3 performs best in our setting, where a smaller \u03bb = 10-4 reduces much of the useful information provided by pseudo-labels. A larger X = 10-2 is problematic and leads to performance degradation because the network could converge in the wrong direction."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel framework called SemiHMER for the field of handwritten mathematical expression recognition. Our method is the first to merge weak and strong augmentations into the cross-head co-training framework, which naturally combines the benefits of consistency and self-training. On one hand, our proposed SemiHMER boosts the diversity of consistency training samples, addressing the challenge of mutual learning between supervision signals generated by two branches. On the other hand, our method enhances the learning effect of self-training pseudo-labels through strong augmentation and weak augmentation. We demonstrate the effectiveness of our paradigm in semi-supervised handwritten mathematical expression recognition, focusing on two commonly used benchmarks CROHME.\nAlthough handwritten mathematical expression recognition has been well developed over the past few decades, the effectiveness of consistency regularization and self-training has not received much attention. Therefore, how to exploit the potential benefits of noisy pseudo-labels while utilizing semi-supervised learning is of great significance for future research."}]}