{"title": "RAPID: Robust and Agile Planner\nUsing Inverse Reinforcement Learning\nfor Vision-Based Drone Navigation", "authors": ["Minwoo Kim", "Geunsik Bae", "Jinwoo Lee", "Woojae Shin", "Changseung Kim", "Myong-Yol Choi", "Heejung Shin", "Hyondong Oh"], "abstract": "This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces (i.e., visual information) while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments (e.g., narrow gaps, cubes, spheres, trees), ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.", "sections": [{"title": "I. INTRODUCTION", "content": "Small unmanned aerial vehicles (UAVs), also known as drones, are agile and compact, making them ideal for diverse applications such as search and rescue operations in disaster areas, urban indoor environment exploration, and target track-ing. However, utilizing this agility in complex environments (e.g., forests and factories) is still limited due to challenges in perception, control, and real-time motion planning. Thus, to fully exploit agility, the development of agile visual navigation algorithms in unknown and complex environments becomes a necessity."}, {"title": "II. RELATED WORKS", "content": "Classical vision-based navigation systems typically employ a sequential pipeline that partitions perception, mapping, plan-ning, and control into separate modules. The workflow begins by converting depth images from onboard cameras into 3D point clouds, which are then aggregated to form volumetric representations such as occupancy grid maps or euclidean signed distance fields (ESDFs). Next, collision-free trajectories are generated using trajectory-optimization methods, and finally, these trajectories are executed via closed-loop control.\nWhile this modular architecture is straightforward and in-terpretable, it introduces several significant drawbacks. Dis-cretization artifacts arise due to the finite resolution of grid-based maps, leading to reduced map fidelity. These issues are further exacerbated during high-speed maneuvers, where increased pose-estimation errors can degrade accuracy. Fur-thermore, the sequential nature of the pipeline imposes cu-mulative latency, limiting its responsiveness in dynamic and time-critical scenarios. These challenges highlight the need for alternative approaches to improve navigation performance under such conditions."}, {"title": "B. Imitation Learning", "content": "Learning-based methods have emerged as a promising al-ternative to address the limitations of classical vision-based navigation systems. Unlike module-based methods, learning-based methods focused on generating trajectories or control commands directly from raw image inputs without explicit perception, mapping, and planning modules.\nOne of the most widely-used imitation learning approaches is behavior cloning (BC). BC is popular due to its straightforward implementation and high sample efficiency. However, BC training requires high quality datasets. Studies such as collected datasets in real-world environments, while others, including utilized synthesized data from simulation environments for training.\nWhile BC policies can perform well when high-quality datasets are available, they often suffer from compounding errors and distributional shifts due to overfitting to specific scenarios. To address this, applied the DAgger method, which collects additional expert data in unseen states during training. However, this method incurs high costs and is chal-lenging to implement in real-time scenarios where an oracle expert is unavailable.\nAnother approach extends imitation learning by leveraging privileged information during training to directly optimize the cost associated with generated trajectories, thus training a path generation model. For instance, studies such as calculate Q-functions based on map data to update policies without explicit labeling. On the other hand, employs a differentiable cost map to optimize trajectory quality directly, without relying on Q-function learning or reinforcement signals. This method focuses on efficient optimization of path generation under given constraints and can be effective as it operates without explicit labeled data. However, it still faces challenges such as reliance on the quality of the cost map and computational overhead, which may limit its scalability."}, {"title": "C. Reinforcement Learning", "content": "Reinforcement learning (RL) has demonstrated remarkable results across various domains and has shown promise even in challenging fields such as drone visual navigation. Recent studies have explored end-to-end learning approaches that utilize visual data to directly generate low-level control commands.\nHowever, RL methods that rely on raw visual information often suffer from slow convergence and require a large amount of data to train. Moreover, the design of effective reward functions poses a significant challenge, as it requires careful consideration to ensure alignment with the desired behaviors and to avoid unintended consequences. These limitations ne-cessitate powerful parallel simulation environments capable of providing diverse state information to train robust policies for various environments. Despite these advance-ments, training vision-based RL policies remains a challenging task, prompting researchers to propose alternative methods to address these difficulties.\nFor instance, Xing et al. employed a DAgger-based policy as a foundation and refined it using RL for effective state embedding. Song et al. introduced a framework where a state-based RL policy is first trained, followed by knowledge distillation to transfer the knowledge into a vision-based RL policy. Similarly, Bhattacharya et al. developed a neural network combining vision transformers (ViT) with LSTM to achieve efficient state embeddings.\nIn drone racing, policies based on raw pixel data have also been investigated. While these methods demonstrate promising results in constrained racing track environments, their applicability to more diverse and unstructured scenar-ios, such as natural or urban environments, has not been fully established. Consequently, developing RL methods that effectively utilize raw visual inputs while ensuring robust generalization and fast convergence remains an open and significant research challenge."}, {"title": "D. Inverse Reinforcement Learning", "content": "Inverse reinforcement learning (IRL) aims to find a proper reward from expert samples. IRL is particularly beneficial in applications where reward design is difficult, yet its adaptation to vision-based tasks remains a significant challenge. While some studies have successfully applied IRL to au-tonomous driving, its application to drones is still unexplored. Compared with autonomous driving, autonomous drone navigation is more demanding as it involves 3D spatial"}, {"title": "III. METHODOLOGY", "content": "RAPID is a vision-based planner utilizing IRL, designed to generate waypoints from depth images and drone state inputs. These waypoints are converted into continuous trajectories and executed by a tracking controller. The training process, from dataset acquisition to action implementation in simulation, is illustrated in Fig. 2. Section III-B outlines the states and actions for RAPID training, while Section III-C presents an auxiliary loss function and network structure for sample-efficient learning. Section III-D details reward inference and policy updates in the IRL framework. Finally, Section III-E explains trajectory generation and tracking control."}, {"title": "A. Preliminaries", "content": "The vision-based navigation problem can be modeled as an infinite-horizon Markov decision process (MDP). The MDP is composed of $(s, a, p(s_0), s', p(s', s|a), r(s, a), \\gamma)$, where $s$ is"}, {"title": "States and Actions", "content": "The policy network $\\pi(a_t|s_t)$ generates an action $a_t$ at time step $t$. The state space $s_t$ is defined as:\n$s_t = [I_t, v_t, q_t, g_t]$,\nwhere a depth image $I \\in \\mathbb{R}^{64\\times64}$, velocity $v \\in \\mathbb{R}^3$, attitude quaternion $q \\in \\mathbb{R}^4$, and a relative goal vector $g \\in \\mathbb{R}^3$ (i.e., the difference between the goal and the current position of the drone).\nIn general, domain-invariant features such as depth images are used to bridge the gap between simulation and real-world environments. However, as shown in Fig. 3, there are differences between the depth images from the simulation and real-world. Therefore, these differences need to be addressed to overcome the sim-to-real gap. To this end, a stereo depth image similar to a real depth image is calculated through the semi-global matching (SGM [38]) method and used for training (Fig. 3(b)).\nWhile high-resolution images generally improve perfor-mance in simulation environments, they require larger net-work architectures and can lead to overfitting, resulting in"}, {"title": "2) Actions", "content": "The action $a_t$ consists of $N$ waypoints ahead, each separated by a fixed time interval $T$. Each waypoint represents a relative position from the previous one, expressed in cylindrical coordinates to reduce the complexity of the action space. For clarity, the action generated by the policy network is referred to as the raw action $a_t^{raw}$, while the post-"}, {"title": "2) Actions", "content": "The action $a_t$ consists of $N$ waypoints ahead, each separated by a fixed time interval $T$. Each waypoint represents a relative position from the previous one, expressed in cylindrical coordinates to reduce the complexity of the action space. For clarity, the action generated by the policy network is referred to as the raw action $a_t^{raw}$, while the post-processed action is denoted as $a$. Specifically, each waypoint in the raw action $a_t^{raw}$ is defined by a relative distance $\\Delta r_i$ and a relative angle $\\Delta \\psi_i$. The raw action $a_t^{raw}$ generated from the policy network is:\n$a^{raw} = \\{(\\Delta r_1, \\Delta \\psi_1), (\\Delta r_2, \\Delta \\psi_2),..., (\\Delta r_N, \\Delta \\psi_N)\\}$.\nThese waypoints are transformed into Cartesian coordinates using cumulative heading angles. Let $\\theta_0 = \\varphi_\\psi$ be the initial heading angle of the drone. The cumulative heading angle for the i-th waypoint is defined as:\n$\\theta_i = \\theta_{i-1} + \\Delta \\psi_i$, for $i = 1,2,..., N$.\nThe position of the i-th waypoint in Cartesian coordinates is then calculated recursively:\n$p_i = p_{i-1} + \\Delta r_i \\begin{bmatrix} \\cos \\theta_i \\\\ \\sin \\theta_i \\end{bmatrix}, with $p_0 = \\begin{bmatrix} x_t \\\\ y_t \\end{bmatrix}$.\nAccordingly, the final transformed action $a_t$ is:\n$a_t = \\{p_1, p_2,..., p_N\\}$,\nwhere each $p_i$ is the absolute position of the i-th waypoint in Cartesian coordinates. In this paper, $N$ is set to 10 and a fixed time interval of $T = 0.1s$ is used."}, {"title": "C. Sample Efficient Training With Image Reconstruction", "content": "Vision-based RL faces significant challenges due to the complexity of processing high-dimensional visual inputs. Unlike state-based RL, vision-based RL requires efficient representation learning to extract meaningful features, often resulting in lower sample efficiency and longer training time. The stochastic nature of visual data and the risk of overfitting further complicate learning. These limitations make generalization difficult and necessitate auxiliary tasks or separate robust feature extraction methods to improve performance. To address these challenges, a $\\beta$-variational autoencoder ($\\beta$-VAE [41]) is utilized to learn compact state representations, effectively embedding high-dimensional inputs while mitigating noise and improving robustness in visual data processing. $\\beta$-VAE consists of two components: a convolutional encoder $g_\\phi$, which maps an image observation $I_t$ to a low-dimensional latent vector $z_t$, and a deconvolutional decoder $f_\\theta$, which reconstructs $z_t$ back"}, {"title": "Auxiliary Loss Function for Autoencoder", "content": "Vision-based RL faces significant challenges due to the complexity of processing high-dimensional visual inputs. Unlike state-based RL, vision-based RL requires efficient representation learning to extract meaningful features, often resulting in lower sample efficiency and longer training time. The stochastic nature of visual data and the risk of overfitting further complicate learning. These limitations make generalization difficult and necessitate auxiliary tasks or separate robust feature extraction methods to improve performance. To address these challenges, a $\\beta$-variational autoencoder ($\\beta$-VAE [41]) is utilized to learn compact state representations, effectively embedding high-dimensional inputs while mitigating noise and improving robustness in visual data processing. $\\beta$-VAE consists of two components: a convolutional encoder $g_\\phi$, which maps an image observation $I_t$ to a low-dimensional latent vector $z_t$, and a deconvolutional decoder $f_\\theta$, which reconstructs $z_t$ back\nto the original state $I_t$. To stabilize training and enhance performance, an $l_2$ penalty is applied to the learned repre-sentation $z_t$, and weight decay is imposed on the decoder parameter $\\theta$ as auxiliary objectives. The objective function of the reconstruction autoencoder (RAE), denoted as $J(RAE)$, is given as:\n$J(RAE) = E_{I_t \\sim \\mathcal{D}}[log p_\\theta(I_t|z_t) + \\lambda_z||z_t||^2 + \\lambda_\\theta||\\theta||^2]$, (1)\nwhere $z_t = g_\\phi(I_t)$ and $\\lambda_z$ and $\\lambda_\\theta$ are hyperparameters."}, {"title": "2) Skipping Connection Networks", "content": "Although deeper net-works generally perform better on complex tasks by introduc-ing inductive biases, simply adding more layers in RL does not yield the same benefits as in computer vision tasks. This is because additional layers can decrease mutual information between input and output due to non-linear transformations. To overcome this issue, skip connections can be used to preserve important input information and enable faster convergence. We applied the deeper dense RL (D2RL) network, which incorporates such skip connections into RL, to the high-speed visual navigation problem. This allows us to gain the advantages of deeper neural networks while achieving faster learning."}, {"title": "D. Policy Learning With Implicit Reward", "content": "This section introduces the IRL-based policy update method, covering: 1) reward and Q-function learning using inverse soft Q-learning, 2) managing absorbing states in high-speed visual navigation, and 3) policy updates via soft actor-critic."}, {"title": "1) Learning Implicit Reward", "content": "The IRL algorithm utilized in this study is least squares inverse Q-learning (LS-IQ) [16], which directly learns a Q-function through an implicit reward formulation. Previously, IRL methods required simultaneous training of two neural networks in the reward-policy domain. However, inverse soft Q-imitation learning (IQ-learning [4]) introduced the inverse Bellman operator $T^\\pi Q$, enabling a mapping from the reward function domain to the Q-function domain. This innovation allows rewards to be expressed en-tirely in terms of Q-functions, eliminating the need for explicit reward network training. The inverse Bellman operator $T^\\pi Q$, following a policy $\\pi$, is defined as:\n$(T^\\pi Q)(s, a) = Q(s, a) - E_{s' \\sim P(\\cdot|s,a)} V^\\pi (s')$, (2)\nwhere $V^\\pi(s)$ is the value function following policy $\\pi$, defined as $V^\\pi(s) = E_{a \\sim \\pi(\\cdot|s)} [Q(s, a) - log \\pi(a|s)]$. From Eq. (2), the reward function can be expressed as $r(s, a) = T^\\pi Q$, allowing simultaneous optimization of the Q-function and rewards.\nBuilding on this framework, the IRL problem is transformed into a single maximization objective, $J(Q, \\pi)$. Specifically, the use of the inverse Bellman operator reformulates the optimization problem from the reward-policy space to the Q-policy space:\n$\\underset{\\Omega \\in \\mathcal{\\Omega}}{max}\\underset{\\Pi \\in \\mathcal{\\Pi}}{min} L(r, \\pi) = \\underset{\\Omega \\in \\mathcal{\\Omega}}{max}\\underset{\\Pi \\in \\mathcal{\\Pi}}{min} J(Q, \\pi)$,\nwhere $\\mathcal{\\Omega} \\subset \\mathbb{R}^{S\\times A}$ denotes the space of Q-functions. From the soft-Q learning concept [47], given a Q-function is fixed, the optimal policy can be expressed as $\\pi_Q(a|s) = $"}, {"title": "exp Q(s, a)", "content": "where $Z_s$ is a normalization factor defined as $Z_s = \\sum_a exp Q(s, a)$. Leveraging this formulation, the optimization objective simplifies to learning only Q-function:\n$\\underset{Q \\in \\mathcal{\\Omega}}{max} \\underset{\\Pi \\in \\mathcal{\\Pi}}{min} J(\\pi, Q) = \\underset{Q \\in \\mathcal{\\Omega}}{max} J(\\pi_Q, Q)$. (3)\nThis transformation accelerates learning by eliminating the need for a separate reward network.\nTo enhance the learning stability of $J(Q, \\pi)$, the algorithm employs a regularizer $(r)$, which imposes constraints on the magnitude and structure of the Q-function. This helps to prevent overfitting, ensures stable learning, and improves generalization. In practice, $l_2$ regularization can be applied to enforce a norm penalty, leveraging a $\\chi^2$-divergence. IQ-learning [4] applies an $l_2$ norm-penalty on the reward func-tion over state-action pairs sampled from expert trajectories. However, this approach has been shown to cause instability in continuous action spaces.\nTo address this issue, LS-IQ [16] stabilizes learning by introducing a mixture of distributions from both expert and learner data. The regularizer $(r)$ is defined as:\n$(r) = \\alpha E_{\\mathcal{d}_E}[r(s, a)^2] + (1 - \\alpha) E_{\\mathcal{d}_L}[r(s, a)^2]$,\nwhere $\\alpha$ is set to 0.5. This mixture-based regularization mit-igates instability by balancing contributions from expert and learner distributions. Consequently, the Q-function objective $J(Q, \\pi)$ is expressed as:\n$J(Q, \\pi) = E_{\\mathcal{d}_E}[r(s, a)] - \\alpha E_{\\mathcal{d}_E} [(r(s,a))^2] -(1-\\alpha)E_{\\mathcal{d}_L} [(r(s, a))^2] - E_{\\mathcal{d}_U \\cup \\mathcal{d}_E}[V^\\pi (s) - E_{s' \\sim P(\\cdot|s,a)} V^\\pi (s')]$, (4)\nwhere $r(s,a) = Q(s,a) - \\gamma E_{s' \\sim P(\\cdot|s,a)}V^\\pi(s')$ as explained in Eq. (2). The last term of Eq. (4) removes the state bias. Furthermore, LS-IQ enhances stability in implicit learning methods by effectively handling absorbing states and applying Q-function clipping during training. The inverse Bellman operator $T^\\pi Q$, accounting for an absorbing state $s_a$, is defined as:\n$T^\\pi Q(s,a) = Q(s,a)-E_{s' \\sim P(\\cdot|s,a)} ((1-\\nu)V^\\pi (s')+\\nu V(s_a))$,\nwhere $\\nu$ is an indicator such that $\\nu$ = 1 if s' is a terminal state and $\\nu$ = 0 otherwise. The value of the absorbing state $V(s_A)$ is computed in closed form as $V(s_a) = \\frac{r_a}{1-\\gamma}$, representing the total discounted return under an infinite time horizon, where $r_a$ is set to $r_{max}$ for expert states and $r_{min}$ for learner states. In our settings, $r_{max}$ and $r_{min}$ are calculated as 2 and -2, respectively. The mathematical definitions and proofs of $r_{max}$ and $r_{min}$ are detailed in the original paper, and readers are referred to [16] for further elaboration. The value $V(s_A)$ is mathematically bounded and can be computed either analytically or via bootstrapping. In this paper, the LS-IQ method adopts the bootstrapping approach for updates. The full objective of Eq. (4) including terminal state treatment is shown in the appendix (Section VIII)."}, {"title": "2) Managing Absorbing States in High-Speed Visual Navigation", "content": "In problems like high-speed visual navigation, where terminal states (e.g., goal or collision states) frequently ap-pear, recursive bootstrapping for absorbing states often causes instability. To resolve this issue, we propose a method that combines bootstrapping for non-terminal states with analytical computation for absorbing states, resulting in a significant improvement in stability and overall performance.\nAlong with refining the computation method for state val-ues, we also adjust the values of $r_{max}$ and $r_{min}$ to better suit the high-speed visual navigation scenario. During our initial experiments, we set $r_{max}$ = 2. However, this configuration caused instability during training as the agent received high rewards upon reaching terminal states, even when dangerously close to obstacles. To minimize this effect, we asymmetrically set $r_{max}$ = 0 and $r_{min}$ = -2. This adjustment prevented undesired high rewards in terminal states and significantly enhanced obstacle avoidance performance."}, {"title": "3) Soft Actor-Critic Update", "content": "To train a policy, soft actor-critic (SAC [47]) is used. In the continuous action space, there is no direct way to get an optimal policy. Instead, an explicit policy $\\pi$ is used to approximate $T^Q$ by using the SAC method. With a fixed Q-function, the policy is updated using the following equation:\n$\\underset{\\pi}{max} E_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot|s)} [Q(s, a) - \\alpha_\\pi log \\pi(a|s)]$, (5)\nwhere $\\mathcal{D}$ is a replay buffer, and $\\alpha_\\pi$ is the temperature parameter. The temperature $\\alpha_\\pi$ controls the trade-off between exploration and exploitation by scaling the entropy term."}, {"title": "E. Trajectory Generation and Control", "content": "Given discrete waypoints generated by the network, it is necessary to convert them into a continuous and differentiable trajectory for smooth flight. The trajectory $\\tau(t)$ can be repre-sented as a distinct function along each axis as:\n$\\tau(t) := [\\tau_x(t), \\tau_y(t), \\tau_z (t)]^T$.\nFor each axis, the trajectory can be represented as an Mth-order of piecewise polynomial function with N time intervals:\n$\\tau_i(t) = \\sum_{j=0}^M \\sigma_{j} (t - t_0 - kT)^i$,\n$t_0 + (k - 1)T < t < t_0 + kT$,\nwhere $\\mu \\in \\{x,y,z\\}$ and $k = 1,..., N$.\nEach polynomial segment must start and end at specified waypoints and ensure a smooth transition by maintaining the continuity of the $j^{th}$ derivative at each intermediate waypoint. Moreover, the first segment should initiate from the drone's current position, velocity, and acceleration. The trajectory that minimizes the integral of the acceleration squared can be found by solving the optimization problem as:\n$\\underset{\\sigma_1,\\sigma_2,..., \\sigma_N}{min} J = \\int_{t_0}^t ||\\ddot{\\tau}(t)||^2 dt$,\nwhere $\\sigma_k \\in \\mathbb{R}^{(M+1)\\times 3}$ represents the coefficients of the $k^{th}$ polynomial segment. The objective J can be analytically"}, {"title": "IV. SIMULATIONS", "content": "Data Acquisition and Training\nTo enhance generalization perfor-mance, a variety of training environments (e.g., trees, cones, and spheres) are generated as shown in Fig. 6. The AirSim simulator [49] is used for map building, training, and testing the algorithm. For data acquisition, a motion primitive-based expert planner [50] is employed, which necessitates prior knowledge of the map. Point cloud data (PCD) of the envi-ronment is first gathered to construct a global trajectory, after which local trajectories are sampled by considering an obstacle cost. Here, a global trajectory is defined as a complete path from the start to the goal constructed using map information, whereas a local trajectory is a refined short segment of the global trajectory, generated by accounting for obstacle costs. The overview of the data collection process can be found in Fig. 2(a).\nA motion primitive-based expert generates global trajec-tories from random start and goal positions with a fixed altitude of 2 meters. For high-speed visual navigation, the average velocity is set to 7 m/s, with maximum velocity and acceleration capped at 8 m/s and 10 m/s\u00b2, respectively. To introduce diversity in collision-free trajectories under the same initial states, random perturbations of up to 0.3 radians are applied to the roll and yaw angles. Using this approach, we generated 1,800 global trajectories across 600 training maps. Based on collected global trajectories, local trajectories were sampled. On average, 60 local trajectories were obtained from each global trajectory at fixed time step intervals (i.e., 0.1s). Consequently, approximately 100,000 local trajectories, paired with corresponding state-action data, were collected in the simulation environment."}, {"title": "2) Training", "content": "To further enhance generalization perfor-mance, domain randomization techniques are applied. First, in each episode, the drone's learning is initiated from a random starting position. Additionally, about 10 percent noise is added to the drone's controller gain to introduce randomness. To enhance the robustness of the encoder during the learning process, the image random shuffling technique is used. If the drone collides with an obstacle or reaches the goal point, the episode is terminated, and the map is replaced every 5 episodes. Further details related to training hyperparameters can be found in the appendix (Section VIII)."}, {"title": "B. Simulation Results", "content": "To quantitatively evaluate the proposed model, we compare the RAPID with the conven-tional BC method, LS-IQ [16] (i.e., IRL method), and the DAgger-based planner AGILE [2]. The BC model uses a pretrained MobileNetV3 [51] with 1D-convolutional layers and has the same network structure with that of the AGILE. The LS-IQ model shares the same network structure and hyperparameters as RAPID, except for the absorbing state reward update rule (detailed in Section III-D). In particular, LS-IQ applies the bootstrapping method to update the Q-function for both non-terminal and absorbing states, with the maximum and minimum absorbing reward values $r_a$ set to +2 and -2, respectively. Conversely, RAPID combines bootstrapping for non-terminal states with an analytical update for absorbing states. Furthermore, RAPID uses asymmetric absorbing reward values by setting the maximum reward to 0 and the minimum reward to -2, which helps train the reward function more effectively for high-speed visual navigation tasks.\nAGILE learns collision-free trajectories using DAgger with relaxed winner-takes-all (R-WTA) loss, addressing the multi-modality issue in conventional BC methods. The original AG-ILE framework employs model predictive control (MPC) [20] to track its generated trajectories. However, for a fair com-parison of waypoint generation performance under the same state inputs, we replace MPC with a geometric controller in this study. While MPC can account for dynamics and enforce feasibility, the geometric controller cannot explicitly impose such constraints. To compensate for this limitation, we incorporate velocity and acceleration constraints during the trajectory generation process."}, {"title": "2) Validation on Test Environments", "content": "The experiments are conducted under varying conditions based on tree density, obstacle sizes, and shapes. Tree density indicates the number of trees per unit area. The trees are inclined and assigned random orientations to increase the complexity of the test-ing environment. The dimensions of trees are randomized according to a continuous uniform random distribution with scale ~ U(2/3, 4/3) across a 50m \u00d7 50m map. The evaluation metrics include mission progress (MP), speed, and traveled distance, where MP measures the progress made toward a goal from the starting position. Figure 7 illustrates the test environments with varying tree densities. For testing, the drone starts flying from a random position within a horizontal range of 20m to the left or right of the map center, on the start line with an initial state of hovering. The goal point is located 60m directly in front of the starting position, and each method is evaluated 30 times on every map.\nTable I presents the quantitative results for each method un-der various tree densities. Among these, BC shows the lowest performance, primarily due to overfitting and compounding errors. Since BC strictly relies on supervised learning from the expert's actions, any deviation from the training distribution can quickly lead to an unrecoverable error state. This distri-bution shift issue severely limits BC's capacity to generalize, especially when the starting position varies or the environment becomes more complex.\nThe LS-IQ method performs better than BC but still faces notable limitations. Although it successfully mimics expert be-havior in simpler simulations, LS-IQ tends to prioritize high-speed flight over robust collision avoidance, which leads to suboptimal performance in denser environments. Its approach to handling reward bias through absorbing states, while effec-tive in principle, fails to fully capture the complexities of high-speed collision scenarios, resulting in diminished robustness as the tree density increases.\nAGILE demonstrates strong performance, particularly in environments with lower tree density. However, as the density and complexity grow, it exhibits an apparent performance"}, {"title": "Hardware Setup", "content": "To achieve high-speed flight, it is necessary to build a lightweight drone capable of generating powerful thrust. Therefore, we designed the hardware similar to racing drones. The drone shown in Fig. 9(a) is equipped with Velox 2550 kV motors paired with Gemfan Hurricane 51466 propellers. For electronic speed controls (ESCs), we used Cyclone 45A BLHeli_S ESCs. The overall weight of the drone is 1.1 kg and during testing, it achieved a thrust-to-weight ratio of 3.57, demonstrating its capacity for high-speed and agile maneuvers.\nFor onboard computation, we employed the NVIDIA Jetson Orin NX. This computing board is compact and lightweight, enabling neural network deployment with rapid execution time. For the real-world experiment, we deployed the neural network on the board and measured its real-time performance. Table II shows the onboard processing latency. The execution speed of the proposed model was compared with that of the AGILE. Although the number of parameters is higher, the proposed RAPID model demonstrates faster execution speed due to its threefold lower FLOPS. The onboard inference test shows that the inference time of RAPID is more than six times faster than the AGILE.\nIn this study, we used the Oak-D Pro depth camera for depth measurement and visual inertial odometry (VIO). The camera, equipped with a global shutter lens, provides stereo images with an 80\u00b0\u00d755\u00b0 field of view and stereo depth images with a 72\u00b0\u00d750\u00b0 field of view. To meet real-time requirements for VIO, both the stereo images and stereo depth images are captured at 20 Hz. The stereo images are used for VIO state estimation, while the stereo depth images are use for the neural network input."}, {"title": "B. System Overview", "content": "This section explains the modules of our proposed system. The proposed system mainly consists of three sub-modules: VIO, local planner, and controller. Figure 9(b) shows an"}, {"title": "C. Experiment Results", "content": "To validate the trained model in real-world environments, experiments were conducted in environments with two distinct characteristics: natural environments and urban environments. The evaluation focused on two main aspects: the ability to perform high-speed flight with collision avoidance and the generalization performance across multiple environments. Further details regarding the experiments can be found in the supplementary video material."}, {"title": "1) Natural Environments", "content": "The experiments were conducted in natural environments divided into two scenarios: long forest and short forest. In the long forest scenario, trees were spaced 5 meters apart, and the goal point was set 60 meters away. The flight began from a hovering state, and the drone flies toward the goal, encountering obstacles along the way. During the flight, the RAPID approach showed obstacle avoidance motions while flying towards the goal, reaching a maximum speed of 7.5 m/s.\nWe further extended the experiments to much denser en-vironments: short forest. In the short forest scenario, curved trees were densely arranged within 2 meters, making the environment more complex. The goal point was set 30 meters away. In this environment, we aimed to push the drone to higher speeds to assess whether visual navigation would still be feasible under denser obstacle conditions. In the waypoint generation step, the speed at which the drone follows the planned trajectory is determined. In the long forest scenario, waypoints were generated such that the drone could follow them within 1 second. For the short forest scenario, however, this duration was reduced to 0.9 seconds to test the drone's ability to navigate through denser environments at higher speeds. Despite the increased difficulty, the drone successfully reached the goal without collisions, achieving a maximum speed of 8.8 m/s. Figure 10 illustrates the results of the experiments conducted in the natural environment, providing an overview of each scenario.\nA noteworthy phenomenon was observed during the real-world experiments. Although the expert dataset used for training was collected with a constant velocity of 7 m/s, the IRL training enabled the policy to exhibit acceleration and deceleration behaviors that were not present in the dataset. In some cases, the drone even reduced its speed significantly before executing an avoidance maneuver near obstacles. This suggests that the IRL-based method goes beyond simply mim-icking the expert's behavior, effectively capturing the intention of collision avoidance and integrating it into the policy."}, {"title": "2) Urban Environments"}]}