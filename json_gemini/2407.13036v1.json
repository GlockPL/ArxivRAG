{"title": "ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders", "authors": ["Carlos Hinojosa", "Shuming Liu", "Bernard Ghanem"], "abstract": "Masked AutoEncoders (MAE) have emerged as a robust self- supervised framework, offering remarkable performance across a wide range of downstream tasks. To increase the difficulty of the pretext task and learn richer visual representations, existing works have focused on replacing standard random masking with more sophisticated strategies, such as adversarial-guided and teacher-guided masking. However, these strategies depend on the input data thus commonly increasing the model complexity and requiring additional calculations to generate the mask patterns. This raises the question: Can we enhance MAE performance beyond random masking without relying on input data or incurring ad- ditional computational costs? In this work, we introduce a simple yet effective data-independent method, termed ColorMAE, which generates different binary mask patterns by filtering random noise. Drawing in- spiration from color noise in image processing, we explore four types of filters to yield mask patterns with different spatial and semantic priors. ColorMAE requires no additional learnable parameters or computational overhead in the network, yet it significantly enhances the learned rep- resentations. We provide a comprehensive empirical evaluation, demon- strating our strategy's superiority in downstream tasks compared to ran- dom masking. Notably, we report an improvement of 2.72 in mIoU in semantic segmentation tasks relative to baseline MAE implementations.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has emerged as a prominent pre-training paradigm, favored for its capacity to learn rich representations without the need for human-labeled data [19,36]. Recent advancements demonstrate that large-scale SSL sig-nificantly outperforms supervised learning on challenging datasets. Inspired by masked language modeling (MLM) [5,15] in natural language processing and the development of vision transformers (ViT) [17], masked image modeling (MIM) has achieved outstanding downstream performance across a broad spectrum of computer vision tasks [4,24], thereby attracting increasing attention. MIM learns rich representations during pre-training by masking certain patches of the input image and predicting their original content based on the remaining"}, {"title": "2 Related Works", "content": "2.1 Self-supervised Learning\nContrastive Learning. Among the numerous self-supervised learning approaches in computer vision that focus on learning from unlabeled data, contrastive learning has emerged in recent years [2, 12, 23, 26, 36, 45]. Its fundamental prin-ciple involves learning representations through instance discrimination, which involves attracting similar samples while optionally repelling dissimilar ones. SimCLR [10], a prominent method in this domain, enhances representations by maximizing the similarity between different views of the same image with large training batches. MoCo [25] advances this approach by employing a memory bank and a momentum-updated encoder to complement the pool of negative samples, thereby learning more robust representations. DINO [6] incorporates a self-distillation mechanism, compelling the student network to mimic the teacher network's output on augmented views of the same image, which fosters strong attention to the salient parts of images.\nMasked Image Modeling. Inspired by the success of Masked Language Model-ing (MLM) [15] in natural language processing, Masked Image Modeling (MIM)"}, {"title": "2.2 Masking Strategy", "content": "Data-Adaptive Masking. The selection of mask sampling strategies is pivotal in MIM as it defines the difficulty of the pretext task, thereby influencing both the quality of the reconstruction and the learned representations. Recent studies have explored more advanced masking strategies. For example, AttMask [27] se-lects patches for masking based on high scores in the teacher network's attention map, presenting a more challenging pretext task. ADIOS [38] utilizes adversar-ial training to increase the difficulty of the pretext task. SemMAE [30] targets semantic portions of the image, masking patches within these areas to provide a nuanced challenge. HPM [42] posits that the difficulty of MIM reconstruction can be quantified by patch-wise reconstruction loss, leading to the development of an auxiliary loss predictor for strategic masking. Feng [21] introduces an evolved masking strategy that incrementally focuses on object semantics and context by effectively masking precise object parts. These strategies, which depend on the image's pixel values for mask sampling, are collectively referred to as data-adaptive masking. This term signifies that the masking process is conditioned and adaptively chosen based on the input data.\nData-Independent Masking. Conversely, a category of masking techniques exists that does not depend on the input images or external guidance, termed data-independent masking. Among these, random masking stands out for its simplicity and is implemented in MAE [24] and SimMIM [48] with a large masking ratio. Block-wise masking, adopted by BEiT [4] and BootMAE [16], involves masking contiguous blocks of image patches. Furthermore, grid mask-ing a strategy that regularly obscures a grid pattern across the image has been proposed as a data augmentation method in [9] and explored in MAE. Our work extends the data-independent masking framework by applying different fil-ters on random noise, thus achieving computational efficiency with a fast-speed masking function. This proposed methodology not only enhances visual repre-sentation learning but also provides substantial benefits for downstream tasks, offering an improvement over traditional random masking approaches."}, {"title": "3 Proposed Method", "content": "To efficiently create binary masks during pre-training, MAE [24] generates ran-dom uniform noise and then selects the data points with the highest top values according to the desired mask ratio (e.g., 75%). These selected values determine which patches are masked out (represented by ones in the binary mask) or re-main visible in the input image, hence directing which portions of the input data the model will attempt to reconstruct. In this regard, the mask is critical in de-termining how high-level semantic representations are extracted from low-level features like image pixels.\nIn addition to uniform noise, white noise [7] can also be implemented to produce random patterns. In general, uniform and white noise are not identical, as their mathematical definitions and statistical properties differ. For instance, white noise has an evenly distributed signal across the frequency spectrum, while uniform noise does not inherently have a flat spectral distribution. However, in practical experiments, they exhibit similar behavior and generate comparable random masks. Consequently, there are no significant differences in MAE pre-training and fine-tuning performance when incorporating either noise type; see our supplementary material for further details. Therefore, we will refer to both white and uniform noise simply as random noise throughout the manuscript.\nMoreover, beyond white noise, there are other types of noise in the image pro-cessing field, known as color noise [14,29,40]. Unlike white noise, which maintains consistent power across its frequency bands, color noises exhibit unique spectral distributions, such as a predominance in the low-frequency band. Although color noise has been investigated in other domains, its application within deep learning frameworks, especially self-supervised learning, remains largely unexplored.\nIn this work, we draw inspiration from the concept of color noise in image pro-cessing and introduce a novel approach, termed ColorMAE, which employs mask patterns with distinct spectral constraints to facilitate efficient self-supervised"}, {"title": "4 Experiments", "content": "Implementation Details. We evaluate the performance of our proposed mask-ing strategies under self-supervised pre-training with MAE [24] on the ImageNet-1K [37] dataset. Unless otherwise specified, we mainly use the standard ViT-B/16 [17] as the backbone, and the decoder consists of 8 Transformer layers with a hidden dimension 512. The input images are resized to 224 \u00d7 224, and the patch size is 16 \u00d7 16; thus, the resulting total sequence length is L = 196. To make a fair comparison with the original MAE pre-trained with random"}, {"title": "4.1 Exploring Masking Strategies Performance", "content": "Qualitative Results. In this section, we evaluate the performance of MAE on the downstream tasks mentioned in the previous section when we pre-train with our proposed four types of ColorMAE masks. Quantitative Results. In Tab. 1, we investigate the performance of our four types of masks generated by our approach on various downstream tasks and show the comparison with traditional random masking. Our findings indicate"}, {"title": "4.2 Comparison with Other Methods", "content": "Given the results from the previous section, we will focus this section on compar-ing Green masks with other methods. Here, we refer to MAE pre-trained with Green masks as ColorMAE-G.\nComparison with MAE. The bottom block of Tab. 3 showcases that our ColorMAE-G consistently outperforms MAE\u2020 (our implementation), as well as the results presented in the original MAE paper [24], without incurring additional\nComparison with Other Data-Independent Masking Methods. In the last block of Tab. 3, we also compare our approach against other state-of-the-art data-independent masking methods, which usually use random or block-wise masking. As observed, ColorMAE-G outperforms all these methods in the semantic segmentation task on the ADE20K dataset and provides comparable results in the other downstream tasks, where it is only surpassed by MixedAE (-0.2) and MAE (-0.5) in COCO object detection.\nComparison with Data-adaptive Masking Methods. Notably, our data-independent masking approach also achieves competitive performance even com-pared to sophisticated data-adaptive masking, which incorporates additional attention-based or adversarial-guided mechanisms, increasing the computational cost. In the semantic segmentation task on the ADE20K dataset, our approach visibly outperforms these methods. Our performance in the object detection task is also better than these approaches and comparable to HPM [42]. However, we do not introduce any additional parameters or computations in the network, thus enjoying the benefit of fast training (see Tab. 4)."}, {"title": "4.3 Analysis", "content": "Reconstruction Loss vs. Downstream Performance. Figure 4 presents the MAE pre-training (reconstruction) loss curves for random masking and our proposed masking approach. As observed, Blue masking achieves the lowest re-construction loss over the epochs, followed by random, Purple, Green, and Red masking. Interestingly, Green masking does not yield the lowest pre-training loss, yet it achieves the best performance in downstream tasks, see Tab. 1. Our results contradict the hypothesis that a lower reconstruction loss implies better downstream performance [49]. However, our findings resonate with the observa-tions in [42]. Specifically, considering the reconstruction loss as a metric of the difficulty of the pre-training task, authors in [42] propose to mask the patches with higher loss, increasing downstream performance. Such hard-to-reconstruct patches are usually associated with the discriminative parts of an image, like objects. Authors in [42] conclude that consistently increasing the difficulty of the pretext task does not lead to better performance, and retaining a certain degree of randomness is necessary for better results. Similarly, as observed from Fig. 4, our Red masking approach tends to mask out big segments of the image, making the pretext task very difficult but not allowing the model to learn useful feature representations. On the other hand, our Green masking approach masks out smaller random segments in the image, making the pretext task difficult enough to learn better representations. In general, our Green masking provides a better balance between pre-training task difficulty and randomness.\nComputational Cost. In general, data-adaptive masking approaches inevitably increase the computational cost and number of parameters since they need to introduce additional components to the network, e.g., an extra decoder. For in-stance, HPM [42] increases the training time 1.1x, while CAE [11] increases the number of parameters to 1.23x and training time to 1.24\u00d7 in comparison with MAE [24]. Similarly, authors in [21] report that their mask generation occupies 12% of pre-training time. On the other hand, our proposed data-independent masking strategy is efficient and does not add extra model parameters or com-putational overhead, as shown in Tab. 4. Because we pre-compute the noise color patterns offline and store them in GPU memory, there is only a small increment in memory usage compared to the original MAE model. In particular, during our experiments, we use 3072 noise patterns of 256 \u00d7 256 spatial dimension for each"}, {"title": "5 Conclusions", "content": "Until now, random masking has been the foundational strategy and a common starting point for developing data-adaptive masking strategies. This paper ex-plored four distinct data-independent masking alternatives to the conventional random masking approach. Using our ColorMAE approach, we can generate different random masks with specific patterns by using noise with different fre-quency spectra. We observed that by using our generated Green masks during MAE pre-training (ColorMAE-G) we achieved faster convergence and better per-formance in downstream tasks, especially in semantic segmentation. Among the explored data-independent approaches in this paper, we found that our Green masks provide the best balance between pretext task difficulty and randomness, which allows the model to learn better representations.\nDiscussion and Limitations. In this work, we adopted a simple yet effective approach to filter random noise and generate different masks. While we investi-gated additional algorithms, such as the void-and-cluster method [39], for gener-ating improved blue noise patterns, these did not yield significant performance enhancements and resulted in slower mask generation. Similarly, developing or using other algorithms to produce better green noise patterns [28] can be ex-plored in future works. On the other hand, while our method is computationally efficient, it increments memory usage by 2.8%. Although this increment is small, it could be considered a limitation and could be improved in future works."}]}