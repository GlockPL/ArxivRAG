{"title": "ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders", "authors": ["Carlos Hinojosa", "Shuming Liu", "Bernard Ghanem"], "abstract": "Masked AutoEncoders (MAE) have emerged as a robust self- supervised framework, offering remarkable performance across a wide range of downstream tasks. To increase the difficulty of the pretext task and learn richer visual representations, existing works have focused on replacing standard random masking with more sophisticated strategies, such as adversarial-guided and teacher-guided masking. However, these strategies depend on the input data thus commonly increasing the model complexity and requiring additional calculations to generate the mask patterns. This raises the question: Can we enhance MAE performance beyond random masking without relying on input data or incurring ad- ditional computational costs? In this work, we introduce a simple yet effective data-independent method, termed ColorMAE, which generates different binary mask patterns by filtering random noise. Drawing in- spiration from color noise in image processing, we explore four types of filters to yield mask patterns with different spatial and semantic priors. ColorMAE requires no additional learnable parameters or computational overhead in the network, yet it significantly enhances the learned rep- resentations. We provide a comprehensive empirical evaluation, demon- strating our strategy's superiority in downstream tasks compared to ran- dom masking. Notably, we report an improvement of 2.72 in mIoU in semantic segmentation tasks relative to baseline MAE implementations.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has emerged as a prominent pre-training paradigm, favored for its capacity to learn rich representations without the need for human- labeled data [19,36]. Recent advancements demonstrate that large-scale SSL sig- nificantly outperforms supervised learning on challenging datasets. Inspired by masked language modeling (MLM) [5,15] in natural language processing and the development of vision transformers (ViT) [17], masked image modeling (MIM) has achieved outstanding downstream performance across a broad spectrum of computer vision tasks [4,24], thereby attracting increasing attention. MIM learns rich representations during pre-training by masking certain patches of the input image and predicting their original content based on the remaining"}, {"title": "2 Related Works", "content": "2.1 Self-supervised Learning\nContrastive Learning. Among the numerous self-supervised learning approaches in computer vision that focus on learning from unlabeled data, contrastive learning has emerged in recent years [2, 12, 23, 26, 36, 45]. Its fundamental prin- ciple involves learning representations through instance discrimination, which involves attracting similar samples while optionally repelling dissimilar ones. SimCLR [10], a prominent method in this domain, enhances representations by maximizing the similarity between different views of the same image with large training batches. MoCo [25] advances this approach by employing a memory bank and a momentum-updated encoder to complement the pool of negative samples, thereby learning more robust representations. DINO [6] incorporates a self-distillation mechanism, compelling the student network to mimic the teacher network's output on augmented views of the same image, which fosters strong attention to the salient parts of images.\nMasked Image Modeling. Inspired by the success of Masked Language Model- ing (MLM) [15] in natural language processing, Masked Image Modeling (MIM)"}, {"title": "2.2 Masking Strategy", "content": "Data-Adaptive Masking. The selection of mask sampling strategies is pivotal in MIM as it defines the difficulty of the pretext task, thereby influencing both the quality of the reconstruction and the learned representations. Recent studies have explored more advanced masking strategies. For example, AttMask [27] se- lects patches for masking based on high scores in the teacher network's attention map, presenting a more challenging pretext task. ADIOS [38] utilizes adversar- ial training to increase the difficulty of the pretext task. SemMAE [30] targets semantic portions of the image, masking patches within these areas to provide a nuanced challenge. HPM [42] posits that the difficulty of MIM reconstruction can be quantified by patch-wise reconstruction loss, leading to the development of an auxiliary loss predictor for strategic masking. Feng [21] introduces an evolved masking strategy that incrementally focuses on object semantics and context by effectively masking precise object parts. These strategies, which depend on the image's pixel values for mask sampling, are collectively referred to as data- adaptive masking. This term signifies that the masking process is conditioned and adaptively chosen based on the input data.\nData-Independent Masking. Conversely, a category of masking techniques exists that does not depend on the input images or external guidance, termed data-independent masking. Among these, random masking stands out for its simplicity and is implemented in MAE [24] and SimMIM [48] with a large masking ratio. Block-wise masking, adopted by BEiT [4] and BootMAE [16], involves masking contiguous blocks of image patches. Furthermore, grid mask- ing a strategy that regularly obscures a grid pattern across the image has been proposed as a data augmentation method in [9] and explored in MAE. Our work extends the data-independent masking framework by applying different fil- ters on random noise, thus achieving computational efficiency with a fast-speed masking function. This proposed methodology not only enhances visual repre- sentation learning but also provides substantial benefits for downstream tasks, offering an improvement over traditional random masking approaches."}, {"title": "3 Proposed Method", "content": "To efficiently create binary masks during pre-training, MAE [24] generates ran- dom uniform noise and then selects the data points with the highest top values according to the desired mask ratio (e.g., 75%). These selected values determine which patches are masked out (represented by ones in the binary mask) or re- main visible in the input image, hence directing which portions of the input data the model will attempt to reconstruct. In this regard, the mask is critical in de- termining how high-level semantic representations are extracted from low-level features like image pixels.\nIn addition to uniform noise, white noise [7] can also be implemented to produce random patterns. In general, uniform and white noise are not identical, as their mathematical definitions and statistical properties differ. For instance, white noise has an evenly distributed signal across the frequency spectrum, while uniform noise does not inherently have a flat spectral distribution. However, in practical experiments, they exhibit similar behavior and generate comparable random masks. Consequently, there are no significant differences in MAE pre- training and fine-tuning performance when incorporating either noise type; see our supplementary material for further details. Therefore, we will refer to both white and uniform noise simply as random noise throughout the manuscript.\nMoreover, beyond white noise, there are other types of noise in the image pro- cessing field, known as color noise [14,29,40]. Unlike white noise, which maintains consistent power across its frequency bands, color noises exhibit unique spectral distributions, such as a predominance in the low-frequency band. Although color noise has been investigated in other domains, its application within deep learning frameworks, especially self-supervised learning, remains largely unexplored.\nIn this work, we draw inspiration from the concept of color noise in image pro- cessing and introduce a novel approach, termed ColorMAE, which employs mask patterns with distinct spectral constraints to facilitate efficient self-supervised"}, {"title": "Red Noise", "content": "Let $W(x, y)$ represent a random noise image, where x and y are spatial coordinates. We apply a blurring operation over W using a Gaussian kernel $G_\\sigma$ with standard deviation $\\sigma$ to filter out the high-frequency components and accentuate low frequencies effectively. This operation transforms the random noise into red noise $N_r$ given by:\n$N_r = G_\\sigma * W$, (1)\nwhere * denotes the convolution operation. Finally, we perform normalization on $N_r$ to adjust the intensity values accordingly. We iteratively repeat the low-pass filtering and normalization steps to refine the noise characteristics."}, {"title": "Blue Noise", "content": "To generate blue noise patterns, it is required to apply a high-pass filter over W. A practical approach to implementing a high-pass filter involves first applying a low-pass filter ($G_\\sigma * W$) to obtain the low-frequency content. Then, this filtered output is subtracted from the original random noise image W, effectively retaining the high-frequency components. The resulting blue noise $N_b$ is formally expressed as\n$N_b = W - G_\\sigma * W$. (2)\nNote that alternative algorithms, such as the Void and Cluster method [39], can also be employed to generate high-quality blue noise patterns. This algorithm initiates with a random distribution of points and iteratively adjusts their place- ment to fill gaps evenly while avoiding the formation of clusters. It computes a density metric for empty spaces based on nearby points, placing new points strategically for even distribution. This algorithm and the blue noise patterns have been extensively used in the computer graphics field [1,44]."}, {"title": "Green Noise", "content": "This noise is defined as the mid-frequency component of white noise; i.e., it can be generated by applying a band-pass filter over W to eliminate both high and low frequencies. Such band-pass filtering effect can be approxi- mated by sequentially applying two Gaussian blurs: first, a weak blur is applied to W to remove the highest frequency details, followed by a separate strong blur to capture the lowest frequency content of W. By subtracting the strongly blurred version of W from the weakly blurred one, the resultant noise image"}, {"title": "Purple Noise", "content": "Finally, in this paper, we refer to purple noise as the noise that has only high and low-frequency content, i.e., does not have a middle-frequency component. We apply a band-stop filter over the random noise W to produce this type of noise. Specifically, we first apply a band-pass filter to W to obtain green noise and then subtract it from the input W, preserving only the low and high frequencies. Formally, this transformation of the noise W into purple noise $N_p$ can be expressed as:\n$N_p = W \u2013 (G_{\\sigma_1} * W \u2013 G_{\\sigma_2} * W)$, (4)\nwhere $\\sigma_1 < \\sigma_2$. Analyzing the periodogram, we can observe that this noise combines the characteristics of both red and blue noise."}, {"title": "Mask Generation", "content": "In implementation, we pre-compute color noise offline and store them in GPU memory before initiating MAE pre-training. To efficiently generate the masks during pre-training, we first apply random transformations on the loaded noise tensor to get a P-sized square noise window for every image in the batch B, where P is the total number of patches. Then, we select the highest values from the noise window according to the desired mask ratio. Specifically, we apply random crop, horizontal flip, and vertical flip image transformation. Note that these image transformations operate in the spatial domain; hence, the frequency properties described in the previous section are preserved [22]."}, {"title": "4 Experiments", "content": "Implementation Details. We evaluate the performance of our proposed mask- ing strategies under self-supervised pre-training with MAE [24] on the ImageNet- 1K [37] dataset. Unless otherwise specified, we mainly use the standard ViT- B/16 [17] as the backbone, and the decoder consists of 8 Transformer layers with a hidden dimension 512. The input images are resized to 224 \u00d7 224, and the patch size is 16 \u00d7 16; thus, the resulting total sequence length is L = 196. To make a fair comparison with the original MAE pre-trained with random"}, {"title": "4.1 Exploring Masking Strategies Performance", "content": "Qualitative Results. In this section, we evaluate the performance of MAE on the downstream tasks mentioned in the previous section when we pre-train with our proposed four types of ColorMAE masks."}, {"title": "4.2 Comparison with Other Methods", "content": "Given the results from the previous section, we will focus this section on compar- ing Green masks with other methods. Here, we refer to MAE pre-trained with Green masks as ColorMAE-G.\nComparison with MAE. The bottom block of Tab. 3 showcases that our ColorMAE-G consistently outperforms MAE\u2020 (our implementation), as well as the results presented in the original MAE paper [24], without incurring additional"}, {"title": "4.3 Analysis", "content": "Reconstruction Loss vs. Downstream Performance."}, {"title": "5 Conclusions", "content": "Until now, random masking has been the foundational strategy and a common starting point for developing data-adaptive masking strategies. This paper ex- plored four distinct data-independent masking alternatives to the conventional random masking approach. Using our ColorMAE approach, we can generate different random masks with specific patterns by using noise with different fre- quency spectra. We observed that by using our generated Green masks during MAE pre-training (ColorMAE-G) we achieved faster convergence and better per- formance in downstream tasks, especially in semantic segmentation. Among the explored data-independent approaches in this paper, we found that our Green masks provide the best balance between pretext task difficulty and randomness, which allows the model to learn better representations.\nDiscussion and Limitations. In this work, we adopted a simple yet effective approach to filter random noise and generate different masks. While we investi- gated additional algorithms, such as the void-and-cluster method [39], for gener- ating improved blue noise patterns, these did not yield significant performance enhancements and resulted in slower mask generation. Similarly, developing or using other algorithms to produce better green noise patterns [28] can be ex- plored in future works. On the other hand, while our method is computationally efficient, it increments memory usage by 2.8%. Although this increment is small, it could be considered a limitation and could be improved in future works."}]}