{"title": "Envious Explore and Exploit", "authors": ["Omer Ben-Porat", "Yotam Ganfit", "Or Markovetzki"], "abstract": "Explore-and-exploit tradeoffs play a key role in recommendation systems (RSs), aiming at serving users better by learning from previous interactions. Despite their commercial success, the societal effects of explore-and-exploit mechanisms are not well understood, especially regarding the utility discrepancy they generate between different users. In this work, we measure such discrepancy using the economic notion of envy. We present a multi-armed bandit-like model in which every round consists of several sessions, and rewards are realized once per round. We call the latter property reward consistency, and show that the RS can leverage this property for better societal outcomes. On the downside, doing so also generates envy, as late-to-arrive users enjoy the information gathered by early-to-arrive users. We examine the generated envy under several arrival order mechanisms and virtually any anonymous algorithm, i.e., any algorithm that treats all similar users similarly without leveraging their identities. We provide tight envy bounds on uniform arrival and upper bound the envy for nudged arrival, in which the RS can affect the order of arrival by nudging its users. Furthermore, we study the efficiency-fairness trade-off by devising an algorithm that allows constant envy and approximates the optimal welfare in restricted settings. Finally, we validate our theoretical results empirically using simulations.", "sections": [{"title": "1 Introduction", "content": "Multi-armed bandits (MABs) are a cornerstone of machine learning. In this framework, a learner makes decisions sequentially, choosing an arm (i.e., an action) and subsequently receiving a reward in each round. Central to MABs is the dilemma of exploiting known arms with historically high rewards and exploring lesser-known arms that could yield even higher rewards. The literature includes diverse variants of the problem, including contextual bandits [20, 59], dueling bandits [21, 64], non-stationary bandits [12, 38], and Lipschitz bandits [36]. The main challenge in these works is to overcome reward uncertainty, compared to ideal scenarios with complete information.\nHowever, with the widespread commercial deployment of explore-and-exploit systems, concerns have expanded beyond mere reward uncertainty to encompass broader societal and fairness issues. Recent research considers facets of fairness, such as regulatory requirements [6, 48, 52], fairness of exposure [39, 62], and meritocratic fairness [31]. While much of this work has tackled fairness from the perspective of the \"arms,\" user-centric fairness remains relatively underexplored. In particular, the concept of envy, a well-established theme in fair division literature [47, 51], has been somewhat overlooked in this context (see notable exceptions in Subsection 1.2). This is especially important because perceived unfairness, as captured by envy, can significantly affect user satisfaction and the overall trustworthiness of the system.\nIn this work, we contribute to the research on envy within explore-and-exploit systems by modeling two key behaviors that characterize real-world, user-centric applications:\n1. Recurring users: Rather than being one-time visitors, users often return to the system repeatedly. For example, navigation apps experience periodic engagement as users rely on them regularly.\n2. Reward consistency: Although rewards may fluctuate stochastically, they tend to remain stable over specific periods. For instance, the quality of a restaurant's daily special is generally consistent throughout the day, and travel times on a frequently used route remain largely unchanged over short intervals.\nThese assumptions introduce nuances in the user experience that foster envy. Due to the dynamics of exploration, some users may explore more frequently and consequently receive lower rewards. This discrepancy in user experience can lead to reluctance to engage with the system or follow its recommendations."}, {"title": "1.1 Our Contribution", "content": "Our contribution is two-fold. Conceptually, we advance the understanding of envy dynamics in explore-and-exploit systems using a stylized model. Our framework encompasses N homogeneous agents, K arms and T rounds. Each round comprises multiple sessions, wherein distinct agents interact with the system. During each session, the algorithm selects an arm on behalf of the corresponding agent. We assume that arm rewards are independently sampled in every round but are consistent across all sessions within a given round. Within this framework, algorithms typically aim to optimize an aggregate performance metric\u2014such as maximizing social welfare, enhancing risk-adjusted outcomes, or minimizing cumulative regret. Consequently, each round involves both exploration-unveiling rewards for the current session\u2014and exploitation, wherein past session information is leveraged to benefit subsequent agents.\nIn this context, agents participating in earlier sessions explore more frequently, while agents arriving later benefit from the information gathered by their predecessors. This gives rise envy, defined as the maximal disparity in cumulative rewards among agents. As expected, the manner in which agents enter a round plays a pivotal role in influencing envy. We formalize this arrival process as an arrival function, which dictates the sequence of agent arrivals within a round. We propose three arrival functions: Uniform, denoted as $\\mathcal{O}_{uni}$, which uniformly determines the order of agent arrival for each round; nudged arrival, denoted as $\\mathcal{O}_{ndg}$, allowing the system to influence the order of arrival, potentially mitigating envy across successive rounds; and adversarial arrival, denoted as $\\mathcal{O}_{Adv}$, establishing a worst-case scenario for agent arrival order from an envy perspective.\nOur second contribution is technical: We characterize the envy of anonymous algorithms, i.e., invariant under any permutation of the agent order. Focusing on such algorithms is justified since agents have homogeneous preferences, and their identities are irrelevant for optimizing any aggregated metric. For the uniform arrival $\\mathcal{O}_{uni}$, Section 3 establishes an upper bound of $\\mathcal{O}\\left(\\sqrt{\\frac{\\ln (N)}{N} \\sum_{t=1}^{T} \\operatorname{Var}\\left(\\Delta_{t}\\right)}\\right)$, where $\\Delta_{t}$ is the average reward discrepancy in round t and N is the number of agents. Crucially, $\\Delta_{t}$ depends on both the algorithm at hand and the reward distributions."}, {"title": "1.2 Related Work", "content": "From the economic perspective, there has been extensive research in the realm of fairness [23, 25, 32, 54\u201356]. Envy-freeness is a central concept in the fair division and allocation literature, for instance, in cake-cutting [5, 14, 22]. Envy-freeness refers to a situation where no individual envies the allocation of another. Building on this foundation, a growing body of research on dynamic fair allocation [9, 27, 34, 58, 66], exemplified by applications such as food bank operations, explores ways for allocating resources dynamically.\nWithin this literature, Benade et al. [9] is the work most related to ours. The authors model the dynamic fair division of T indivisible goods that arrive online and must be immediately allocated to exactly n heterogeneous agents who assign a value in [0,1] to each item. Their goal is to design allocation algorithms that minimize the maximum envy at time T, where envy is defined as the largest difference between any agent's overall value for items allocated to another agent and her own. They design an efficient algorithm with an envy of $\\tilde{O}(\\sqrt{T})$, and show that this guarantee is asymptotically optimal. Despite the many alignments in the underlying mathematical framework and results, there are significant distinctions between the work of Benade et al. [9] and our work.\nFirst and foremost, while envy in Benade et al. [9] arises from heterogeneous valuations among agents, our model involves homogeneous agents. To emphasize this point, in the model of Benade et al. [9], if agents were homogeneous, then simply allocating each item to the agent with the lowest cumulative reward would yield an envy of at most one and not $\\tilde{O}(\\sqrt{T})$. Second, in our work, envy is generated due to arrival order rather than solely by allocation decisions like Benade et al. [9]. In our setting, the same arm (or resource) can be assigned to several agents, with its utility being revealed only after it is pulled for the first agent; thus, envy emerges from the inherent explore-and-exploit tradeoffs. Third, while Benade et al. [9] aim to design fair algorithms, we focus on measuring envy under virtually any algorithm and different arrival order mechanisms. Indeed, in our work, the decision-making algorithm could operate optimistically like UCB [4], follow Thompson sampling [1], or leverage deep learning techniques to learn reward distribution over time.\nFrom the machine learning perspective, fairness has attracted increasing interest over the past decade (see, e.g., [18, 45, 49] for recent surveys). Within the domain of fair classification [35], several works apply a fair division lens to evaluate the equity of predicted classes relative to protected"}, {"title": "2 Model", "content": "Let $[N]=\\{1,2, \\ldots, N\\}$ be a set of N agents. We examine an environment in which a system interacts with the agents over T rounds. Every round $t<T$ comprises N sessions, each session represents an encounter of the system with exactly one agent, and each agent interacts exactly once with the system every round. I.e., in each round t the agents arrive sequentially.\nArrival order The arrival order of round t, denoted as $\\eta_{t}=\\left(\\eta_{t}(1), \\ldots, \\eta_{t}(N)\\right)$, is an element from set of all permutations of [N]. Each entry q in $\\eta_{t}$ is the index of the agent that arrives in the $q^{\\text {th }}$ session of round t. For example, if $\\eta_{t}(1)=2$, then agent 2 arrives in the first session of round t. Correspondingly, $\\eta_{t}^{-1}(i)=q$ implies that agent i arrives in the $q^{\\text {th }}$ session of round t.\nAs we demonstrate later, the arrival order has an immediate impact on agent rewards. We call the mechanism by which the arrival order is set arrival function and denote it by $\\mathcal{O}$. Throughout the paper, we consider several arrival functions such as the uniform arrival function, denoted by $\\mathcal{O}_{U n i}$, and the nudged arrival $\\mathcal{O}_{N d g}$; we introduce those formally in Sections 3 and 4, respectively.\nArms We consider a set of $K \\geq 2$ arms, $\\mathcal{A}=\\{\\alpha_{1}, \\ldots, \\alpha_{K}\\}$. The reward of arm $\\alpha_{i}$ in round t is a random variable $X \\sim D$, where the rewards $\\left(X_{i, t}\\right)_{i, t}$ are mutually independent and bounded within the interval [0, 1]. The reward distribution D of arm $\\alpha_{i}, i \\in[K]$ at round $t \\in \\mathcal{T}$ is assumed to be non-stationary but independent across arms and rounds. We denote the realized reward of arm $\\alpha_{i}$ in round t by $x_{i}^{t}$. We assume reward consistency, meaning that rewards may vary between rounds but remain constant within the sessions of a single round. Specifically, if an arm $\\alpha_{i}$ is selected multiple"}, {"title": "2.2 Information Exploitation", "content": "In this subsection, we explain how algorithms can exploit intra-round information. Since rewards are consistent in the sessions of each round, acquiring information in each session can be used to increase the reward of the following sessions. In other words, the earlier sessions can be used for exploration, and we generally expect agents arriving in later sessions to receive higher rewards. Taken to the extreme, an agent that arrives after all arms have been pulled could potentially obtain the highest reward of that round, depending on how the algorithm operates.\nTo further demonstrate the advantage of late arrival, we reconsider Example 1 and Algorithm 1. The expected reward for the agent in the first session of round t is $\\mathbb{E}\\left[r_{i}^{(1)}\\right]=\\frac{1}{K} \\sum_{i=1}^{K} \\mu_{i}$, yet the expected reward of the agent in the second session is\n$\\mathbb{E}\\left[r_{i}^{(2)} \\mid X_{i}>\\frac{1}{2}\\right] \\operatorname{Pr}\\left(X_{i}>\\frac{1}{2}\\right)+\\mathbb{E}\\left[r_{i}^{(2)} \\mid X_{i}<\\frac{1}{2}\\right] \\operatorname{Pr}\\left(X_{i}<\\frac{1}{2}\\right)$;\nthus, $\\mathbb{E}\\left[r_{i}^{(2)} \\mid X_{i}>\\frac{1}{2}\\right]=\\mathbb{E}\\left[X_{i} \\mid X_{i} \\geq \\frac{1}{2}\\right]+\\mu_{i} \\cdot \\frac{1}{2}=\\frac{3}{4}$. Consequently, the expected welfare per round is $\\mathbb{E}\\left[r_{i}^{(1)}+r_{i}^{(2)}\\right]=1+\\frac{1}{8}$, and the benefit of arriving in the second session of any round t is $\\mathbb{E}\\left[r_{i}^{(2)}-r_{i}^{(1)}\\right]=\\frac{1}{8}$. This gap creates envy over time, which we aim to measure and understand."}, {"title": "2.3 Socially Optimal Algorithms", "content": "Since our model is novel, particularly in its focus on the reward consistency element, studying social welfare maximizing algorithms represents an important extension of our work. While the primary focus of this paper is to analyze envy under minimal assumptions about algorithmic operations, we also make progress in the direction of social welfare optimization. See more details in Section 6."}, {"title": "3 Uniform Arrival", "content": "In this section, we assume agents arrive uniformly; that is, the uniform arrival function, $\\mathcal{O}_{uni}$, picks in every round t a permutation $\\eta_{t}$ uniformly at random from the set of all distributions Perm([N]). We start with an insight into reward discrepancies for uniform arrival. Then, in Subsections 3.1 and 3.2, we provide upper and lower bounds on the expected envy, respectively.\nRecall that $\\Delta_{i, j}^{t}$ is the signed reward discrepancy between agents i and j at round t. This quantity depends on both the algorithm, the reward distribution, and the arrival function. For example, if an algorithm always pulls $\\alpha_{1}$, then $\\Delta_{i, j}^{t}=0$ almost surely for every i, j, t, since all agents receive the same reward. By contrast, for more general algorithms, $\\Delta_{i, j}^{t}$ can vary almost arbitrarily, reflecting different approaches to exploration and exploitation over time. Under uniform arrival, each round's agent order is chosen uniformly at random from all permutations of [N], ensuring identical treatment of all agents in expectation. Consequently, the reward discrepancies exhibit a symmetry:\nRemark 1. Under $\\mathcal{O}_{uni}$ and any algorithm, the random variables $\\Delta_{i, j}^{t}$ in round t are identically distributed for all pairs (i, j) \u2208 [N]2.\nDue to Remark 1, we simplify the notation in this section and use $\\Delta_{t}$ to denote the reward discrepancy distribution between any two agents. Clearly, $\\mathbb{E}\\left[\\Delta_{t}\\right]=0$. Note that the random"}, {"title": "3.1 Upper Bound", "content": "Our first result is an upper bound on the expected envy.\nTheorem 1. When executing any algorithm, it holds that\n$\\mathbb{E}\\left[\\max _{1 \\leq t \\leq T} \\mathcal{E}^{t}\\left(\\mathcal{O}_{U n i}\\right)\\right] \\leq 2 \\sqrt{\\ln (N) \\sum_{t=1}^{T} \\operatorname{Var}\\left(\\Delta^{t}\\right)} .$\nProof of Theorem 1. The proof leverages properties of martingales and subgaussian random variables. Since the rewards are bounded, the resulting envy is also bounded and therefore subgaussian with some parameter. However, because envy is a sum of possibly dependent random variables, additional arguments are needed to obtain a sharper subgaussian parameter. The following proposition is the primary technical ingredient in achieving this refinement.\nProposition 1. For any two arbitrary agents $i, j \\in[N]$, max $1<t<T \\mathcal{E}_{i, j}^{t}$ is $\\sqrt{\\sum_{t=1}^{T} \\operatorname{Var}\\left(\\Delta^{t}\\right)}$-subgaussian.\nWe prove this proposition in Section A. Equipped with Proposition 1, we can use the following well-known property of the maximum of subgaussian random variables.\nClaim 1. Let $Y_{1}, \\ldots, Y_{n}$ be (possibly dependent) $\\sigma$-subgaussian random variables. It holds that\n$\\mathbb{E}\\left[\\max _{i \\in[n]} Y_{i}\\right] \\leq \\sqrt{2 \\sigma^{2} \\ln (n)} .$\nClaim 1 is folklore, but we include its proof in Section A for completeness. Using this claim along with Proposition 1, we obtain\n$\\mathbb{E}\\left[\\max _{1<t<T} \\mathcal{E}^{t}\\right]=\\mathbb{E}\\left[\\max _{\\begin{array}{c} i, j \\in[N] \\\\ 1<t<T \\end{array}} \\mathcal{E}_{i j}^{t}\\right]<\\sqrt{2 \\ln \\left(N^{2}\\right) \\sum_{t=1}^{T} \\operatorname{Var}\\left(\\Delta^{t}\\right)}=2 \\sqrt{\\ln (N) \\sum_{t=1}^{T} \\operatorname{Var}\\left(\\Delta^{t}\\right)} .$\nThis concludes the proof of Theorem 1."}, {"title": "3.1.1 Refining the Variance of Reward Discrepancy", "content": "Although Theorem 1 contains a factor of In N, it does not explicitly capture the influence of N or K, as these parameters are embedded within Var (\u0394). \u03a4\u03bf clarify this further, we refine our analysis by focusing on the subclass of algorithms known as explore-first algorithms.\nWe say that an algorithm satisfies the explore-first property if, for every round t, once it repeats an arm selection (i.e., chooses ai in two sessions), it commits to ai for all subsequent sessions in that round. More formally,"}, {"title": "3.2 Lower Bound", "content": "Next, we move to develop a lower bound on the expected envy. As is apparent, not all algorithms generate envy. To demonstrate, consider the following complementary cases:\n1. An instance in which all arms have the same deterministic reward. In this case, any algorithm produces zero envy among all agents as all receive the same reward.\n2. An algorithm that pulls the same arm in all sessions. Even if rewards are stochastic, the algorithm does not generate any envy due to reward consistency.\nThese examples show that envy will not accumulate if the instance or the algorithm are degenerate, motivating to focus on executions: A pair of algorithm and reward distributions. In what follows, we characterize a class of non-degenerate executions for which envy always accumulates. we call such compositions sufficiently random executions. Formally,\nDefinition 2 (Sufficiently Random). An execution is called sufficiently random if it holds that\n$\\sum_{t=1}^{T} \\operatorname{Var}\\left(\\Delta_{t}\\right) \\geq \\sqrt{T}.$\n(1)\nIn other words, an execution is sufficiently random if the average variance of the reward discrepancy between two agents in a round is greater or equal to $\\frac{1}{\\sqrt{T}}$. To further illustrate, we provide the following example."}, {"title": "4 Nudged and Adversarial Arrival", "content": "In this section, we address nudged arrival: We assume an exogenous arrival mechanism can influence the order in which agents arrive. Practically, this captures scenarios where the system sends push notifications or otherwise encourages some agents to arrive earlier or later. Our goal is to analyze the envy of arbitrary algorithms without changing the way they select arms. We stress that our analysis still assumes anonymous algorithms: The decision-making process is not affected by agent identities.\nSubsection 4.1 introduces the nudged arrival protocol, $\\mathcal{O}_{N d g}$, and the accompanying assumptions. Later, Subsection 4.2 presents our main result of the section: An upper bound of $\\mathbb{E}_{T}\\left(\\mathcal{O}_{N d g}\\right)$ for a broad class of algorithms. Interestingly, we show that the bound depends on the instance parameters but not on the horizon T. Finally, we complete this section by adopting a complementary approach, where an adversary can pick the worst arrival order in terms of envy, and show that an $\\Omega(T)$ regret is inevitable. For clarity, we remind the reader that for $q, i \\in[N], \\eta_{t}(q)=i$ implies that agent i has arrive in the q'th session in round t (similarly for $\\eta_{t}^{-1}(i)=q$)."}, {"title": "4.1 Nudged Arrival Protocol", "content": "We describe the arrival protocol in Algorithm 2. It receives the horizon T and a nudge parameter \u03b4 as input, and interacts with any recommender algorithm through the horizon. In each round in the for loop of Line 1, we pick an ideal permutation $\\sigma_{t}$ that orders the agents according to their cumulative"}, {"title": "4.2 Envy Analysis", "content": "We are ready to present the main result of the section: Upper bounding the envy under nudged arrival.\nTheorem 4. When executing any algorithm that satisfies Assumption 1 with nudged arrival, the expected envy is\n$\\mathbb{E}\\left[\\mathcal{E}_{T}\\left(\\mathcal{O}_{N d g}\\right)\\right] \\leq(N-1)\\left(2+\\frac{128}{\\delta^{2} \\Delta^{2}}\\right)$.\nNotice that this upper bound does not depend on the horizon T. Intuitively, under nudged arrival, envy behaves like a random walk with a drift toward zero. Although each round may introduce a discrepancy (akin to a random fluctuation), the nudging mechanism consistently pushes the cumulative difference back toward zero. Furthermore, the bound is inversely proportional to \u03b4 and \u0394: As \u03b4 decreases, the nudging effect weakens and nudged arrival increasingly resembles uniform arrival. We suspect the terms in the bound are not tight; we discuss it in Section 6.\nProof of Theorem 4. Fix any arbitrary algorithm satisfying Assumption 1 and any arbitrary $t \\in[T]$. The proof is outlined as follows:\n1. Step 1 introduces envy gaps as stochastic processes and the concept of envy excursions.\n2. Demonstrating that envy gaps are nontrivial to analyze, Step 2 presents a more friendly stochastic process that we prove to upper bound the envy gap almost surely.\n3. Step 3 leverages Property 1 and concentration inequalities to upper bound large deviations of the friendly stochastic process.\n4. Lastly, Step 4 uses the tail formula and the concentration from Step 3 to bound to cumulative envy.\nStep 1: Envy Gap and Excursion For every t, let $\\sigma_{t}:[N] \\rightarrow[N]$ be the ideal permutation from Line 2, i.e.,\n$R_{\\sigma_{t}(N)}^{-1} \\leq R_{\\sigma_{t}(N-1)}^{-1} \\leq \\cdots \\leq R_{\\sigma_{t}(2)}^{-1} \\leq R_{\\sigma_{t}(1)}^{-1}$\nFor every $i, 1 \\leq i \\leq N-1$, we define the envy gap $G_{i}^{t}=R_{\\sigma_{t}(i)}^{-1}-R_{\\sigma_{t}(i+1)}^{-1}$, representing the envy between the agent with the i-th highest reward and the agent with the (i + 1)-th highest reward. Consequently, we can define $\\mathcal{E}^{t}$ using the envy gap sequence $\\left(G_{i}^{t}\\right)_{i}$ by\n$\\mathcal{E}^{t}=R_{\\sigma_{t}(1)}^{-1}-R_{\\sigma_{t}(N)}^{-1}=\\sum_{i=1}^{N-1} R_{\\sigma_{t}(i)}^{-1}-R_{\\sigma_{t}(i+1)}^{-1}=\\sum_{i=1}^{N-1} G_{i}^{t}.$\n(11)\nNext, fix any arbitrary i in the range. We continue by analyzing excursions from low envy to high envy and showing they are relatively short, meaning that the expected envy $\\mathbb{E}\\left[G_{i}^{t}\\right]$ is low. We define an excursion as a sequence of consecutive rounds during which the gap $G_{i}^{\\tau}$ exceeds 1. Let $\\underline{t}=\\max \\left\\{\\tau \\mid 1<\\tau<t, G_{i}^{\\tau} \\leq 1\\right\\}$ denote last round $\\tau$ before t that $G_{i}^{\\tau}$ was less than 1. Similarly, let $\\bar{t}=\\min \\left\\{\\tau \\mid t \\leq \\tau \\leq T, G_{i}^{\\tau} \\leq 1\\right\\}$ be the first round after t where $G_{i}^{\\tau}$ is less than 1. For the extreme case where $\\underline{t}$ is undefined, we set $\\underline{t}=T+1$. Furthermore, let $\\mathcal{D}(t)$ denote t's excursion, namely, the set of all the consecutive rounds that includes t during which $G_{i}^{\\tau}>1$. Formally, $\\mathcal{D}(t)=\\{\\tau \\mid \\underline{t}<\\tau<\\bar{t}\\}$. Notice that $\\mathcal{D}(t)$ is an empty set if and only if $G_{i}^{t} \\leq 1$."}, {"title": "4.3 Adversarial Arrival", "content": "We end this section by focusing on the adversarial arrival order $\\mathcal{O}_{A d v}$. Intuitively, an adversary seeking to maximize envy would reverse the ideal permutation, placing agents in descending order of their current cumulative rewards. That is, sets the order $\\eta_{t}$ in round t such that\n$R_{\\eta_{t}(N)}^{-1} \\geq R_{\\eta_{t}(N-1)}^{-1} \\geq \\cdots \\geq R_{\\eta_{t}(2)}^{-1} \\geq R_{\\eta_{t}(1)}^{-1}$.\nIndeed, it is easy to see that:\nProposition 7. When executing any algorithm that satisfies Assumption 1 with $\\mathcal{O}_{A d v}$, the expected envy is\n$\\mathbb{E}\\left[\\mathcal{E}_{T}\\left(\\mathcal{O}_{A d v}\\right)\\right] \\geq \\Delta_{T}$.\nProof of Proposition 7. Assume that the adversary picks agent 1 to be the first and agent N to be the last, i.e., $\\eta_{t}(1)=1$ and $\\eta_{t}(N)=N$ for all $t \\in[T]$. In such a case,\n$\\mathbb{E}\\left[\\mathcal{E}_{T}\\left(\\mathcal{O}_{A d v}\\right)\\right]=\\mathbb{E}\\left[\\max _{i, j \\in[N]} \\mathcal{E}_{i, j}^{T}\\right] \\geq \\mathbb{E}\\left[\\sum_{t=1}^{T} \\Delta_{\\eta_{t}(N), \\eta_{t}(1)}^{t}\\right] \\geq \\mathbb{E}\\left[\\sum_{1<t<T} \\Delta_{(N),(1)}^{t}\\right] \\geq \\sqrt{T} \\min _{1<t<T} \\Delta_{(N),(1)} \\geq \\Delta_{T}$.\nThis concludes the proof of Proposition 7."}, {"title": "5 Extension: Trading Envy and Welfare", "content": "In the previous section, we have shown that coordinating agents' arrival order alone can significantly reduce envy, without affecting the algorithm's core decision-making process. In this section, we take an initial step toward understanding the efficiency-fairness tradeoff, a well-established concept in the literature on fair allocation [11, 61] and fair classification [46, 65]. Specifically, we extend the definition of algorithms from Section 2 to allow agent-specific treatment. In other words, algorithms can now observe agent identities and maintain agents accumulated rewards in their memory. Formally, the relevant histories contain triples of the form (agent index, pulled arm, realized reward). We hope to leverage this additional capability to balance social welfare and envy.\nWe focus on the special case of our running example (Example 1): N = 2 agents, K = 2 arms with rewards drawn from the uniform distribution, $X_{1}, X_{2} \\sim \\operatorname{Uni}(0,1)$, and the uniform arrival $\\mathcal{O}_{U n i}$. Furthermore, we assume Bayesian information, i.e., the prior distributions are known. We stress that our results are preliminary, albeit non-trivial. In Subsection 5.1, we analyze the socially optimal algorithm from a welfare and envy perspective. Later, in Subsection 5.2, we develop $\\mathcal{E F C}$, our welfare-envy balancing algorithm."}, {"title": "5.1 Optimal Welfare and Optimal Envy", "content": "We first analyze the maximal social welfare for this setting. As it turns out, Algorithm 1 is a special case of the optimal two-agent algorithm, as we prove in Section F. Along with our results from Section 3, we conclude that:\nObservation 1. When executing Algorithm 1 on the instance of Example 1 and $\\mathcal{O}_{U n i}$, it achieves an expected social welfare of $\\left(1+\\frac{1}{8}\\right) T$ and induces an expected envy of $\\mathcal{E}_{T}\\left(\\mathcal{O}_{U n i}\\right)=\\tilde{O}(\\sqrt{T})$. Furthermore, this is the optimal welfare."}, {"title": "5.2 Envy-freeness up to C", "content": "In what follows, we introduce $\\mathcal{E F C}$, which is an abbreviation of Envy-Freeness up to C, and is implemented in Algorithm 3. $\\mathcal{E F C}$ operates by selectively limiting the exploitation of information when the gap between agents' rewards could potentially exceed a predefined envy threshold C. This mechanism enforces envy-freeness up to C, allowing for better welfare compared to $\\mathcal{N E}$ while maintaining low envy.\nWe now describe how $\\mathcal{E F C}$ works. It interacts with agents for T rounds (Line 1). In every round t, $\\mathcal{E F C}$ pulls arm $\\alpha_{1}$ for the agent arriving in the first session (Line 2). The decision to pull $\\alpha_{1}$ first is arbitrary since both arms are identically distributed. If $\\alpha_{1}$ realizes a high reward, i.e., $r_{i}^{(1)}=x>\\frac{1}{2}$, $\\mathcal{E F C}$ pulls it again for the agent in the second session (Line 3). Otherwise, we enter the \"else\" clause in Line 4.\nIf $\\alpha_{1}$ yields a low reward, the welfare-wise correct action is to pull $\\alpha_{2}$; however, recall that $\\mathcal{E F C}$ aims to keep the envy lower than C. As a result, it ensures that the envy $R_{i}^{(1)}-R_{j}^{(2)}$ at the end of round t is lower or equal to C. Specifically, the \"if\" clause in Line 5 asks whether there exists a realization $r_{i}^{(2)}=r$ for which the envy would exceed C by the end of the round. If such a realization exists, it pulls $\\alpha_{1}$. Otherwise, it pulls $\\alpha_{2}$ in Line 6. We term $\\mathcal{E} \\mathcal{F} 1$ the special case of $\\mathcal{E F C}$ for C = 1.\nTheorem 5. When executing $\\mathcal{E F C}$ on the instance of Example 1 and $\\mathcal{O}_{U n i}$, the following hold:\n1. For all t, $\\mathcal{E}^{t}<C$ almost surely.\n2. For $C=1$, the social welfare is $S W \\geq\\left(1+\\frac{1}{16}\\right) T$."}, {"title": "5.3 Beyond C = 1", "content": "The envy analysis in Theorem 5 concerns $\\mathcal{E} \\mathcal{F} 1$, which is a special case of $\\mathcal{E F C}$ with C = 1. Unfortunately, our techniques rely heavily on this fact, and extending it would require a different approach. Our preliminary investigation has led us to the following conjecture.\nConjecture 1. When executing $\\mathcal{E F C}$ on the instance of Example 1 with any $C \\geq 1$ and $\\mathcal{O}_{U n i}$, the expected social welfare of at least $S W \\geq\\left(1+\\frac{C}{100}\\right) T$.\nSimulations we conducted and appear in Section G suggest that Conjecture 1 holds, and we hope future work could formally prove it."}, {"title": "6 Discussion and Future Work", "content": "In this work, we have advanced the understanding of envy dynamics in explore-and-exploit systems. Our stylized model, which assumes reward consistency and sequential agent interactions, assists in characterizing envy under uniform and nudged arrival mechanisms, revealing envy dynamics and accumulation. Under uniform arrival, our results highlight that besides pathologic cases, any algorithm generates an unavoidable expected envy of roughly $\\sqrt{T}$. In contrast, if agent arrival could be nudged, the envy ceases to depend on the horizon T. Our results highlight that strategic manipulation of arrival orders through nudging can substantially mitigate envy without altering"}]}