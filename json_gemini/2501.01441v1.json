{"title": "Explanatory Debiasing: Involving Domain Experts in the Data Generation Process to Mitigate Representation Bias in AI Systems", "authors": ["Aditya Bhattacharya", "Robin De Croon", "Simone Stumpf", "Katrien Verbert"], "abstract": "Representation bias is one of the most common types of biases in artificial intelligence (AI) systems, causing AI models to perform poorly on underrepresented data segments. Although AI practitioners use various methods to reduce representation bias, their effectiveness is often constrained by insufficient domain knowledge in the debiasing process. To address this gap, this paper introduces a set of generic design guidelines for effectively involving domain experts in representation debiasing. We instantiated our proposed guidelines in a healthcare-focused application and evaluated them through a comprehensive mixed-methods user study with 35 healthcare experts. Our findings show that involving domain experts can reduce representation bias without compromising model accuracy. Based on our findings, we also offer recommendations for developers to build robust debiasing systems guided by our generic design guidelines, ensuring more effective inclusion of domain experts in the debiasing process.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the significance of Artificial Intelligence (AI) and Machine Learning (ML) across various sectors, AI/ML systems are known to be affected by a wide spectrum of biases [47]. One of the most prevalent types of biases in AI/ML systems that do not follow a systematic data collection process is representation bias [58]. Representation bias arises when the training data lacks sufficient samples from certain groups or sub-groups [47, 58]. Consequently, prediction models built on such a dataset will be biased towards other groups that have a higher representation. For example, if a healthcare dataset predominantly contains images of skin conditions from lighter-skinned individuals and has very few images from darker-skinned individuals, this creates representation bias. Consequently, an Al model trained for diagnosing skin conditions will perform poorly for individuals with darker skin tones [5]. Due to the limited availability of data for under-represented populations, the model struggles to generalise effectively to these groups.\nData augmentation is considered one of the most effective methods to reduce the impact of representation bias [33, 58, 59]. It involves generating new synthetic data by applying various transformations to the existing data. Data augmentation is particularly useful when collecting additional data for under-represented groups is not practically feasible [42, 58]. Adding new synthetic samples through data augmentation increases the representativeness of the underrepresented groups, thereby reducing the model's bias towards the majority groups [58].\nHowever, despite increasing the number of samples, data augmentation algorithms have been criticised for generating problematic data points [1, 3, 48, 62]. For instance, suppose there is a diabetes prediction dataset with predictor variables such as blood sugar level, body mass index (BMI), age and obesity level of patients. Data augmentation algorithms can generate records where the obesity level is high while the BMI is less than 18. However, in reality, a BMI under 18 indicates that a patient is underweight, not obese. Moreover, data augmentation algorithms such as SMOTE [18], ADASYN [31], CTGAN [28], TVAE [38] have faced scrutiny for introducing data issues such as data drift, correlated features, duplicates, or outliers. Furthermore, since these augmentation algorithms tend to preserve the statistical properties of the training data, any form of bias that exists in the underlying training data may also exist in the generated data [3, 51, 62].\nTo mitigate these limitations, prior works have suggested involving domain experts [53, 54] such as healthcare professionals, legal advisors and policy makers. Domain experts are users who might not have AI/ML knowledge but are considered to be experts in their respective application domains. It has been argued in recent works [9, 10, 25] that the domain knowledge possessed by domain experts is crucial for identifying and modifying problematic data points and improving prediction models. However, due to the lack of user studies involving domain experts focused on mitigating representation bias in AI/ML systems, a significant gap exists in the literature regarding the potential involvement of domain experts in the representation debiasing process.\nOur research addresses this gap by exploring how domain experts can be integrated into the data augmentation process to mitigate representation bias. In this paper, we propose a set of generic design guidelines for a representation debiasing system that involves domain experts. We instantiated these guidelines into a healthcare-focused prototype system. Moreover, we conducted a mixed-methods user study with 35 healthcare experts to investigate the following research questions:\nRQ1. How does involving domain experts during representation debiasing affect the prediction model's performance?\nRQ2. How can domain experts contribute to reducing representation bias in AI/ML systems?\nRQ3. How does the involvement of domain experts in the representation debiasing process influence their trust in Al systems? How could the debiasing process be improved?\nThe primary objective of this study is not to establish that domain experts are superior to Al experts in representation debiasing. Instead, our work emphasises the value of incorporating domain experts into the debiasing process, as our findings demonstrate that their involvement can effectively reduce representation bias in Al systems without compromising model performance. Therefore, domain experts can be essential contributors to the representation debiasing process, working alongside AI experts to ensure balanced and unbiased Al outcomes. To summarise, the contributions of this research are three-fold:\n(1) Theoretical Contributions: We propose a set of generic design guidelines for including domain experts in the representation debiasing process. While these guidelines are designed to be broadly applicable across various domains, we specifically evaluated their effectiveness in the healthcare domain.\n(2) Artifact Contributions: Based on our generic design guidelines, we developed a healthcare application following an iterative user-centric design process [56]. This application supports healthcare experts in identifying predictor variables with representation bias. The application also allows them to share their domain knowledge during data augmentation to mitigate representation bias. The source code, design and architecture of the debiasing system is open-sourced on GitHub.\n(3) Empirical Contributions: To evaluate the healthcare-focused debiasing system developed using our design guidelines, we conducted a mixed-methods user study with 35 healthcare experts. Through the findings from this extensive user study, we show that domain experts can be involved in the representation debiasing process to reduce representation bias without compromising the prediction model's accuracy. Additionally, we found that including them in the debiasing process increased their overall trust in the AI system. Furthermore, we discuss how their involvement can be improved and how they can support Al experts in mitigating the impact of representation bias."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Representation bias arises when datasets lack sufficient information for various subgroups, often due to historical trends, skewed data distributions, flawed data preparation and acquisition methods, and selection and sampling biases when preparing the training dataset [47, 58]. It causes a significant challenge for achieving group or sub-group fairness, as models trained on underrepresented groups tend to have lower accuracy for these groups [58]. To address this problem, training datasets must include adequate samples from less represented segments to improve prediction model performance [58].\nTo estimate the amount of representation bias, prior works have suggested two main metrics: (1) representation rate and (2) data coverage [50, 58]. For a specific variable, the representation rate of a"}, {"title": "2.1 Representation Bias", "content": "sub-group is measured by taking the ratio of its number of samples to the maximum number of samples among all sub-groups for that variable. For example, if a certain variable has three sub-groups: a, b and c, such that a + b + c = N, where N is the total number of samples, the representation rate of a (r_a) is estimated by the equation: r_a = \\frac{a}{max(a,b,c)}. Data coverage refers to the minimum number of samples required for each sub-category to ensure its comprehensive representation within the dataset. The data coverage threshold is usually obtained after performing model over-fitting or under-fitting analysis [58]. Regardless of the dataset or the application domain, ensuring sufficient coverage for all significant sub-populations is crucial to guarantee their adequate representation.\nNow, let us take an example to understand these metrics better. Consider a variable representing the severity of a medical condition in a dataset with three sub-categories as severity levels: mild, moderate, and severe. Assume that the dataset includes 900 patient records, with the following distribution of severity levels: 500 patients with mild severity, 150 with moderate severity, and 250 with severe severity. The representation rate for each severity level can be calculated as follows: r_{mild} = \\frac{500}{max(500,150,250)} = 1.0, r_{moderate} = \\frac{150}{max(500,150,250)} = 0.3 and r_{severe} = \\frac{250}{max(500,150,250)} = 0.5. In this example, the representation rate for patients with moderate severity is 0.30, indicating that they are the least represented compared to patients with mild severity, who have a representation rate of 1.00, and those with severe severity, who have a representation rate of 0.50. Suppose the minimum data coverage for this medical dataset is set as 200; then the moderate severity sub-category does not meet the coverage criteria as it has only 150 samples."}, {"title": "2.2 Data Augmentation", "content": "While acquiring more data for under-represented segments using a systematic collection process is recommended to address representation bias, it is often practically infeasible to collect additional data. Therefore, Al practitioners have proposed generating synthetic data as a viable alternative for representation debiasing [17, 42, 58].\nSince data augmentation involves generating new samples through various transformations on the underlying training data, it is seen as a potential solution for mitigating representation bias [42, 48, 58]. For tabular datasets, algorithms such as SMOTE [18] and ADASYN [31] have been predominantly used to generate synthetic data. More recently, generative AI algorithms such as CTGAN and TVAE [72] have also been considered for generating synthetic data from the existing training data. However, these methods have been criticised for introducing data issues such as outliers, redundant samples, and data drifts [1, 3, 22]. Additionally, as they aim to maintain the statistical properties of the original data [3, 51, 62], these methods often preserve the existing biases of the underlying training data in the generated samples.\nFor instance, consider an underrepresented dataset for predicting heart disease that includes factors such as cholesterol levels, blood pressure, and age. Commonly adopted data augmentation algorithms can generate samples where the cholesterol level is high, but the blood pressure is unusually low. In practice, individuals with high cholesterol levels typically have elevated blood pressure. This inconsistency indicates that the augmented data may not accurately reflect real-world observations. Therefore, despite their success in generating synthetic data quickly, the lack of domain knowledge in the data augmentation process has been considered a challenge in producing synthetic data that accurately reflects real-world observations [42, 63, 64]. To produce accurate real-world representations of the generated data, our approach is to directly involve domain experts in the synthetic data generation process.\nOur work highlights the crucial role of involving domain experts in the data generation process to mitigate representation bias. Leveraging the prior knowledge of domain experts can improve the creation of validated samples that accurately reflect real-world conditions, leading to more effective debiasing."}, {"title": "2.3 Domain Expert Involvement in Refining Prediction Models", "content": "Interactive machine learning (IML) researchers have often emphasised the importance of involving domain experts in model development, fine-tuning, and debugging [10, 24, 40, 41, 43, 57, 61, 65, 66]. Different types of user interaction approaches have also been explored in prior research to enable the active involvement of domain experts. Their active involvement has been particularly helpful in identifying different types of biases and errors in the data [25, 55]. For example, prior knowledge possessed by healthcare experts has been considered essential for analysing and identifying biases and data issues in patient records that can lead to biased models [10, 67].\nWhile prior research has introduced innovative interaction methods for domain experts to refine prediction models [10, 57, 65, 66], there has been a notable lack of focus on their role in the data augmentation process for mitigating representation bias. Especially due to the lack of user studies on this topic, the perspective of domain experts is yet to be investigated. However, utilising the prior knowledge of domain experts holds considerable promise for reducing the risks associated with data augmentation algorithms used in tackling representation bias [63]. Our research addresses this gap by investigating how domain experts can contribute to the representation debiasing of generated data."}, {"title": "3 DESIGN GUIDELINES FOR INVOLVING DOMAIN EXPERTS IN REPRESENTATION DEBIASING", "content": "This section presents a set of generic design guidelines for effectively involving domain experts in data augmentation to mitigate representation bias. Table 1 summarises these design guidelines, and an implementation of these guidelines in a healthcare scenario is presented in Section 4. These guidelines are organised into three key phases of the debiasing process: pre-augmentation, during augmentation, and post-augmentation. By involving domain experts in the pre-augmentation phase, we ensure that the initial evaluation of the training dataset accurately identifies areas of underrepresentation. During the augmentation phase, their expertise is crucial in guiding the generation of synthetic data that faithfully reflects the characteristics of the underrepresented sub-categories. Finally, in the post-augmentation phase, domain experts play a vital role in validating and integrating the synthetic data with the existing dataset, ensuring that the model retraining process results in a more"}, {"title": "3.1 Pre-Augmentation", "content": "To enable more effective feedback from domain experts during the data augmentation process, the following pre-augmentation guidelines are essential. These guidelines ensure that domain experts can accurately identify variables with representation bias and understand the impact of this bias on the prediction model's performance across underrepresented segments.:\nEXPLORATION THROUGH DATA-CENTRIC EXPLANATIONS : Data-centric explanations are explainable AI (XAI) methods that focus on understanding and interpreting the training data itself rather than the models built from it [2, 6, 8]. These explanations highlight the underlying patterns, distributions, and characteristics of the data and help domain experts identify the presence of representation bias and erroneous data points. To ensure usability and enhance the experience of domain experts, these explanations should be presented through intuitive and interactive interfaces that prioritise clarity, accessibility, and ease of exploration [14]. Interactive data-centric explanations further empower them to explore each predictor variable [8] and identify segments with representation bias, which is crucial for effective data augmentation and bias mitigation.\nMODEL IMPACT ANALYSIS: To help domain experts understand the impact of underrepresented segments on the prediction model, model performance metrics such as accuracy, precision, and recall should be clearly demonstrated across all data segments, with a particular emphasis on underrepresented groups. By identifying segments of each predictor variable where model performance is significantly lower, domain experts can prioritise these segments for the augmentation process. This allows them to address potential representation bias that may hinder the model's ability to make informed decisions for such underrepresented groups.\nAI SYSTEM TRANSPARENCY: Aligned with the ML transparency principle from Bove et al. [14], the overall Al system performance metrics, such as model accuracy, number of training samples, predictor variables, and estimation of dataset quality [10], should be clearly presented to domain experts through interactive visualisations and UI elements. Transparency about this system information allows domain experts to gauge the system's reliability, scope, and potential limitations, thereby facilitating a deeper understanding of how the Al system functions and where it may require further refinement [4]."}, {"title": "3.2 During Augmentation", "content": "To generate more meaningful synthetic data that correctly reflects real-world observations, the following guidelines can be applied to enhance the involvement of domain experts in the augmentation process. These guidelines ensure that data augmentation algorithms are steered by domain knowledge, creating synthetic data that closely aligns with real-world representations.\nMULTIVARIATE CONSTRAINT PLANNING: Since data augmentation algorithms can generate samples with random and implausible values [1], prior research has recommended setting constraints on the training data for obtaining more meaningful data [27, 51]. According to this guideline, domain experts should be given control over selecting predictor variables and their corresponding segments that need better representation through interactive UI elements rather than applying data augmentation randomly across all segments. Additionally, domain experts may possess better in-depth knowledge of the training data to understand the joint impact of multiple predictor variables. By allowing them to define specific value ranges and the number of samples required for multiple predictor variables with representation bias, data augmentation algorithms can be steered to generate practically plausible and more representative samples.\nAWARENESS OF EXTREMELY LOW COVERAGE SEGMENTS: When sub-groups have very low representation, data augmentation algorithms may struggle to generate accurate synthetic samples [1]. Therefore, significantly underrepresented data segments should be clearly highlighted through visual elements to assist domain experts in determining the necessary number of samples that should be generated for these segments. If the required number of synthetic samples greatly exceeds the sample size of the segment in the underlying training data, a warning should be issued to domain experts, as these synthetic samples are more likely to contain problematic data points."}, {"title": "3.3 Post-Augmentation", "content": "The following post-augmentation guidelines ensure that the generated dataset is thoroughly scrutinised by domain experts, minimising the presence of problematic samples before it is integrated into the original training set and the model is retrained.\nREFINEMENT OF GENERATED DATA: Domain experts should be given the control to refine the generated data by selecting only plausible synthetic samples, particularly for underrepresented segments. To minimise the introduction of problematic data points during the debiasing process, we recommend providing data filters that enable sampling based on conditions specified by domain experts. Any interface designed to instantiate this guideline should provide visual cues and clear feedback to enhance the overall user experience during conditional sampling of the generated data and allowing users to filter out noisy data.\nLOCAL WHAT-IF EXPLORATION: Domain experts should be given the ability to conduct local \"what-if\" exploration of generated samples [6, 8, 45, 70]. Local what-If exploration is a process that allows users to assess and manipulate individual data points within a dataset to observe how changes in predictor variables affect model outcomes. This interactive approach helps domain experts to validate the accuracy and reliability of model predictions for generated synthetic samples, enabling them to identify and rectify any problematic values. It provides more granular control to domain experts to edit or remove samples that are corrupt or practically implausible.\nEVALUATION OF GENERATED DATA: With this guideline, the current ML model's performance should be evaluated on the"}, {"title": "3.4 Conceptualisation of Design Guidelines", "content": "We developed this set of generic design guidelines for debiasing based on an extensive literature review of guidelines for involving domain experts in Al system refinement. This process involved synthesising insights from multiple research areas, including Bias and Fairness in Al systems, Data Augmentation, Human-AI Interaction, and Explainable AI, to ensure comprehensive coverage of relevant perspectives. We adopted a structured approach to conceptualise these guidelines. First, we reviewed existing literature to identify causes and potential strategies for mitigating representation bias in datasets and models. Next, we investigated the opportunities and limitations of widely used data augmentation algorithms in addressing representation bias, in which we identified the importance of incorporating domain expertise during the augmentation process. Subsequently, we explored works focusing on integrating domain knowledge into data augmentation processes, examining strategies for leveraging domain expertise at various stages of data augmentation. Finally, we analysed prior research on user interface (UI) design and interaction frameworks to gather insights into enhancing the experience of domain experts involved in debiasing workflows. We refined the guidelines iteratively, particularly after an exploratory feedback session with five healthcare experts (discussed later in Section 4.2).\nUSER-INTERACTION BIAS AWARENESS: Before retraining the system with the augmented dataset, domain experts should be informed about potential biases that can be introduced through their interactions with the system when performing data augmentation [47]. To elucidate the user-interaction biases and their effects on the training dataset when incorporating generated data, we suggest using visual data-centric explanations [8]. Data-centric explanations can highlight issues in the new training set, serving as a cautionary note. This allows domain experts to address any identified adverse effects and, if necessary, adjust or revert the augmentation process."}, {"title": "4 APPLICATION: IMPLEMENTATION OF DESIGN GUIDELINES IN A HEALTHCARE SCENARIO", "content": "We applied the generic design guidelines outlined in Section 3 to develop a healthcare-focused application by following an iterative user-centric design process [56]. This section details the application's usage scenario, implementation, and its various user interface (UI) components."}, {"title": "4.1 Usage Scenario", "content": "We applied our proposed design guidelines into a debiasing application tailored to include healthcare domain experts such as doctors, nurses, and medical researchers. The application features an ML model that predicts the onset of type 2 diabetes based on patient medical records. It enables healthcare experts to identify biased predictor variables in the training dataset and offer feedback during the data augmentation process. Additionally, it allows them to validate and refine the generated data before integrating it into the training set and retraining the model."}, {"title": "4.2 Application Implementation", "content": "Low Fidelity Prototype: We followed an iterative user-centric design process [56] to implement our debiasing application. We first created an initial low-fidelity click-through prototype in Figma [26] to instantiate the generic design guidelines. Then, to refine this prototype, we conducted an exploratory feedback session that included voluntary pro-bono participation from 5 healthcare experts (2 females, 3 males; aged between 29 and 51 years; each having more than 4 years of healthcare experience) employed at the the Faculty of Healthcare Sciences, University of Maribor in Slovenia. We obtained the ethical approval for this feedback session from KU Leuven with an approval number G-2024-8352-R2(MAR). These feedback sessions consisted of individual co-design and think-aloud sessions that were recorded and later transcribed for analysis. The average duration for each session was around 30 minutes.\nKey Takeaways from the Exploratory Session: The purpose of this initial design feedback session was to validate our design guidelines and UI designs and ensure alignment with the primary user requirements for involving domain experts in the debiasing process. From participant feedback, we identified key improvements, such as simplifying the navigation between the exploration of representation bias in the predictor variables and setting constraints during the augmentation process to reduce cognitive load. Initially, we planned to separate variable exploration and constraint-setting for the data augmentation algorithm into two distinct views, similar to the EXMOS approach in Bhattacharya et al. [10]. However, most of the participants suggested keeping these features in a single view to reduce their cognitive load. Additionally, we learned the importance of tooltips when describing each UI component and clearly highlighting biased sub-groups and their impact through clear warning messages from our participant feedback. These insights informed the development of a high-fidelity debiasing application tailored to the needs of healthcare experts to effectively engage them in the debiasing process.\nDebiasing Application and ML Model: The high-fidelity application was developed as an interactive web application using React.js, with a backend ML engine built in Python. It included a type-2 diabetes prediction model developed using the LightGBM classification algorithm [37]. While our proposed guidelines for representation debiasing are model-agnostic (i.e., they do not depend on the specific ML algorithm used), we selected LightGBM for its efficiency, accuracy, and significantly lower memory consumption. The prediction model had an accuracy of 93%. Additional information about our model training process is available in the supplementary material.\nDataset: The model was trained on an open-sourced type-2 diabetes prediction dataset [19]. The dataset comprises 4,303 patient records, with 17 predictor variables and 1 target variable. Each record includes details on the patient's physical attributes (e.g., age, gender, BMI, etc.), diagnostic measures (e.g., plasma glucose, cholesterol measures, etc.), and self-reported information such as smoking and drinking habits, as well as family history of diabetes. A complete list of predictor variables, along with their full descriptions, is provided in the supplementary material. We selected this dataset for our experiments because it was collected according to World Health Organization (WHO) standards, ensuring its reliability for developing diabetes diagnosis models. However, the dataset contains several predictor variables with representation bias, making it a suitable proxy for real-world clinical datasets with representation biases.\nData Augmentation Method: For conducting data augmentation, we used the Conditional Tabular Generative Adversarial Network (CTGAN) algorithm [72], a generative AI method recognised for generating better real-world tabular data compared to alternatives, like SMOTE [18] and ADASYN [31]. CTGAN leverages a Generative Adversarial Network (GAN) architecture with a generator and a discriminator working in tandem to learn and replicate the joint distribution of the input data. It incorporates mode-specific normalisation and a conditional sampling mechanism, enabling it to handle mixed data types (continuous, categorical, and discrete) while preserving complex feature relationships and addressing imbalanced datasets. While our proposed principles are not tied to any specific augmentation algorithm, we chose CTGAN due to its fast computation speed and the relatively low incidence of data issues in the generated samples."}, {"title": "4.3 User Interface Components", "content": "Based on the design guidelines discussed in Section 3, our healthcare-focused application consists of the following UI components.\n(A) System Overview: In line with the Pre-Augmentation AI SYSTEM TRANSPARENCY guideline, this UI component presents key information about the prediction model, including its overall accuracy, the number of training samples, and the predictor variables used. It also shows the change in accuracy if the model is retrained with the newly generated data. Additionally, tooltips are provided to explain the model's primary purpose and how it can assist healthcare experts. This component is most useful during the pre-augmentation phase and after the prediction model is retrained with the generated data, allowing users to compare changes to the overall prediction model due to the augmentation process.\n(B) Data Explorer: Considering the EXPLORATION THROUGH DATA-CENTRIC EXPLANATIONSs guideline, this component allows healthcare experts to explore each predictor variable and pinpoint those that are impacted by representation bias. In addition to displaying the overall representation rate (RR) and coverage rate (CR) considering all the predictor variables, it also provides RR and CR scores for individual variables. Furthermore, considering the MODEL IMPACT ANALYSIS guideline, this component displays not only the frequency distribution of each segment of the predictor variable but also their impact on the model accuracy for different target outcomes. This component also includes a Quick Insights section, inspired by Bhattacharya et al.'s [10] design suggestions of Key Insights. This section highlights the segments of each predictor variable that are most affected, where RR, CR, and accuracy scores are notably low. Overall, this component is most useful during the pre-augmentation phase in which users get to explore the representation bias in predictor variables.\n(C) Data Quality Overview: This UI component provides transparency about the overall quality of the current training dataset. It is aligned with the Pre-Augmentation AI SYSTEM TRANSPARENCY guideline. We followed the method prescribed by Bhattacharya et al. [10] to compute the data quality. It includes the detection of common data issues such as outliers, redundant records, correlated features, skewed variables and class imbalance. Each data issue is given an equal weightage in the scoring process of estimating the overall data quality. The design of this component is also aligned with the guideline of showing uncertainty measures from Wang et al. [70].\n(D) Augmentation Controller: Aligned with the Augmentation guidelines, this component allows healthcare experts to"}, {"title": "5 USER STUDY", "content": "This section discusses the methodology used to evaluate our final debiasing application, based on the generic design guidelines, through a mixed-methods user study involving 35 healthcare experts. The ethical approval of this study was granted by the ethical committee of KU Leuven with the approval number G-2024-8352-R2(MAR)."}, {"title": "5.1 Study Setup", "content": "We conducted a mixed-methods user study with 35 healthcare experts to investigate how domain experts, like healthcare experts, can be effectively involved in the representation debiasing process and seek answers to our research questions. The study was conducted online, with participants taking an average of 60 minutes to complete it. Participants were recruited through Prolific [52] and compensated at an hourly rate of $25. Before conducting the study with a larger group of participants, we piloted it to test the application's functionality and refined the vocabulary used in the study questionnaires."}, {"title": "5.2 Participants", "content": "The study involved 35 healthcare experts who were currently working in diverse roles, such as doctors, surgeons, nurses, medical researchers, paramedics, and medical assistants. They were recruited through Prolific. Table 3 presents the demographic information of our study participants. To ensure that participants had vetted domain knowledge of type 2 diabetes, we implemented a custom screening questionnaire with three key inclusion criteria: (1) completion of the screening questionnaire with relevant responses to verify domain expertise, (2) at least six months of experience in treating and caring for diabetic patients, and (3) general awareness of the predictor variables used in the dataset and their impact on diabetes risk. Moreover, the pre-screening questions enabled us to validate the healthcare professionals' backgrounds, ensuring they had obtained relevant certifications, licenses or training from accredited institutions for treating diabetic patients. These questions also assessed their understanding of type 2 diabetes pathophysiology, diagnostic criteria, treatment protocols, complication management, and lifestyle modification guidance. This questionnaire used to vet the domain expertise of the recruited participants was validated by healthcare experts from Faculty of Health Sciences, University of Maribor in Slovenia and is available in the supplementary material. Before analysing the collected responses, we formulated two exclusion criteria: (1) responses from participants who did not answer all the study questions, and (2) responses from participants who did not complete the study tasks will be excluded. Additionally, given the growing concern about the use of large language models (LLMs) to cheat in crowd-sourced user studies [69], we planned to exclude responses that were entirely LLM generated as a post-study exclusion criteria. The participant responses were validated using Undetectable AI [68] and GPTZero [29] to identify any responses generated by LLM tools. Our validation process revealed no responses that were entirely generated by widely accessible LLMs. However, as indicated by the validation tools, only two participants appeared to have used an LLM for minor edits to some of their qualitative responses, with the key statements remaining their own. Moreover, the system log data indicated that they had completed"}, {"title": "5.3 Evaluation Measures", "content": "This section outlines the various measures collected in our user study to answer the research questions. The entire set of study questionnaires is available in the supplementary material.\nPerceived trust: To explore the impact of the debiasing process on trust in AI, we measured perceived trust using a multi-dimensional trust questionnaire adapted from Jian et al. [34]. Multi-dimensional perceived trust refers to a critical user perspective for informed decision-making that incorporates various factors, including fidelity, reliability, and familiarity with the system or process. This measure was recorded on a 7-point Likert scale.\nMetrics for evaluating participant performance in representation debiasing: To evaluate the performance of participants in mitigating the representation bias within our system, we captured multiple objective measures through system logs. To assess the impact of the debiasing process on the AI system, we recorded the prediction model's accuracy, the overall quality score of the training dataset, and the estimated scores for various data issues. Following the methods proposed by Bhattacharya et al. [10], the dataset quality assessment did not account for the amount of representation bias. Instead, it focused on other data issues, such as outliers, redundant data, and correlated features, all of which were weighted equally in calculating the overall data quality score. The amount of representation bias was separately recorded through the overall representation rate (RR) and coverage Rate (CR) scores, as well as the RR and CR scores for each predictor variable after each retraining of the prediction model. Furthermore, to investigate changes in the augmentation process performed by different participants, we captured the number of augmentations executed and the specific configurations used in the augmentation controller to generate new data.\nObjective understanding (OU) of representation bias: We also aimed to investigate whether participants' understanding of representation bias affected their ability to mitigate its impact. To assess their understanding, we measured objective mental model scores (i.e., objective understanding) of representation bias, following methods similar to those used by previous researchers [10, 14, 20, 40, 41]. This measure was recorded on a scale of 0 to 5, where 0 represents the lowest OU score and 5 indicates the highest.\nSubjective understanding of representation bias: Additionally, we recorded participants' subjective understanding of representation bias using a perceived understandability questionnaire adapted from Hoffman et al. [32]. This measure was recorded on a 7-point Likert scale.\nPerceived task load: To assess whether the debiasing process was overwhelming for participants, we evaluated their perceived task load using the NASA-TLX questionnaire [30, 40, 41]. The NASA-TLX includes six evaluation areas: mental demand, physical demand, time demand, performance, effort, and frustration. We followed the method proposed by Kulesza et al. [41], where each area was rated on a scale from 0 to 20."}, {"title": "5.4 Study Procedure", "content": "Participants were first given detailed information about the study's objectives and their roles, responsibilities, and rights. They were required to provide informed consent in accordance with our ethical guidelines before proceeding. Following the informed consent, participants were asked to provide demographic information, summarised in Table 1. The overall study flow is illustrated in Figure 4.\nParticipants were then introduced to our debiasing application through detailed tutorial videos. These videos outlined the usage scenario, elaborately described the concept of representation bias and its general impact on prediction models, explained the role of the prediction model, and thoroughly demonstrated the functionality of each UI component described in Section 4.3 for mitigating representation bias.\nNext, they answered a pre-task questionnaire measuring their perceived trust in the AI system. This was done to establish a baseline, allowing us to measure how the debiasing process influenced their trust in the overall AI system.\nParticipants then proceeded to the main task of the study, which involved completing a debiasing task. In this task, at first, they were asked to explore the predictor variables to identify any signs of representation bias. Following this, they were asked to use the augmentation controller to generate a synthetic dataset aimed at addressing the identified biases. After generating the synthetic data, participants were required to validate and refine it before retraining the prediction model by merging the synthetic data with the original training set. This task had a maximum time limit of 30 minutes, during which participants could generate and retrain the model multiple times. Their primary objective was to reduce the amount of representation bias by maximising the representation rate (RR) and coverage rate (CR) scores without compromising the model accuracy and data quality.\nAfter completing the primary task, participants were asked to fill out several post-task questionnaires centred on objective and subjective understanding of representation bias, perceived task load, trust in the Al system and several qualitative questions to justify their responses."}, {"title": "5.5 Data Analysis", "content": "To address our research questions", "roles": "n(1) Physicians (including registered doctors", "bias": "n(1) High Understanding (OU Scores of 4 and 5)\n(2) Medium Understanding (OU Scores of 2 and 3)\n(3) Low Understanding (OU Score less than"}]}