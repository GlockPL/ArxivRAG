{"title": "MV-MOS: Multi-View Feature Fusion for 3D Moving Object Segmentation", "authors": ["Jintao Cheng", "Xingming Chen", "Jinxin Liang", "Xiaoyu Tang", "Xieyuanli Chen", "Dachuan Li"], "abstract": "Effectively summarizing dense 3D point cloud data and extracting motion information of moving objects (moving object segmentation, MOS) is crucial to autonomous driving and robotics applications. How to effectively utilize motion and semantic features and avoid information loss during 3D-to-2D projection is still a key challenge. In this paper, we propose a novel multi-view MOS model (MV-MOS) by fusing motion-semantic features from different 2D representations of point clouds. To effectively exploit comple- mentary information, the motion branches of the proposed model combines motion features from both bird's eye view (BEV) and range view (RV) representations. In addition, a semantic branch is introduced to provide supplementary semantic features of moving objects. Finally, a Mamba module is utilized to fuse the semantic features with motion features and provide effective guidance for the motion branches. We validated the effectiveness of the proposed multi-branch fusion MOS framework via comprehensive experiments, and our proposed model outperforms existing state-of-the-art models on the SemanticKITTI benchmark. The implementation codes are available at https://github.com/Chengjt1999/MV-MOS.", "sections": [{"title": "I. INTRODUCTION", "content": "The accurate identification of surrounding moving objects is fundamental to autonomous driving and robotics applica- tions as it directly affects the safety and reliability of the system. In addition, the properties and state information of moving objects are essential to many downstream tasks such as high-definition map building, scene understanding and decision-making. The 3D MOS task aims to distinguish mov- ing objects from static entities by segmenting the perceived LiDAR point clouds.\nExisting MOS models can be generally categorized into 3D voxel-based [1]\u2013[4] and 2D projection-based methods [5]\u2013[8]. To correctly classifying moving objects, they sum- marize and extract motion features from dense 3D point cloud data using different representations. The Voxel-based methods typically voxelize dense point clouds into grids that are directly processed in 3D space. However, performing 3D convolution operations on high-dimensional data are compu- tationally demanding, making it difficult to deploy in real- world applications where real-time performance is crucial. In contrast to computation intensive voxel-based approaches, 2D projection-based deep learning models have recently emerged as a promising pipeline for MOS. They firstly map 3D point cloud data into a 2D representation through the BEV or range view projection. In this manner, 2D convolution operations can be effectively performed on the projected feature representations. However, such 3D-to-2D transformation operation inevitably leads to information loss, affecting the quality of extracted feature and the ultimate segmentation accuracy. For instance, when relying solely on the range view's residual map for motion information, sta- tionary objects suddenly appearing in view due to occlusion by moving objects may be mistaken for moving objects. The BEV residual map relies only on height differences between frames for motion information, but the objects' own vertical information is ignored in the differencing operation, ultimately resulting in the loss of valuable motion contour information in the BEV motion feature map. Therefore, how to effectively exploit the motion and semantic features of moving objects in different representation modalities is still a significant challenge to 3D MOS models.\nTo address this challenge, we propose the Multi-view feature fusion MOS (MV-MOS), a novel 3D LiDAR MOS model, which features a dual-view and multi-branch structure for effective feature fusion. The proposed model can effec- tively retain rich and complementary motion and semantic information when using 2D representations of 3D point clouds, while at the same time exploiting the well-developed 2D neural network models for lightweight and efficient feature processing. The primary contributions of this work are summarized as follows:\n\u2022 We design a multi-branch structure that effectively exploits motion features of moving objects from both BEV and range view representations of point clouds.\n\u2022 We propose a novel feature synthesis neural network structure to comprehensively utilize complementary information from motions and appearance of mov- ing object segmentation, and effectively generate rich semantic-guided motion features. In addition, we design an Mamba-based adaptive feature fusion framework which can robustly generate ultimate features for accu- rate segmentation, while addressing the issue of uneven feature density during fusion.\n\u2022 The proposed approach achieves 78.5% and 80.6% of IoU on the validation and test sets of the SemanticKITTI-MOS benchmark, respectively, surpass- ing state-of-the-art open-source MOS models."}, {"title": "II. RELATED WORK", "content": "Existing MOS approaches can be categorized as non- learning-based and learning-based models. Conventional non-learning-based methods do not require complex data preprocessing and long training times. For example, [9], [10] use the occupancy map method to calculate motion informa- tion based on volume occupancy differences. [11] adopts the visibility-based theory and applies visual projection data to design a static points construction algorithm. Although these methods are relatively convenient, their segmentation accu- racy is far inferior to that of deep learning-based methods.\nFor state-of-the-art learning-based methods, Cylinder3D [3] employs cylindrical voxelization of point cloud data and uses sparse 3D convolution for feature processing. InsMOS [1] and 4DMOS [2] introduce time series into the 3D space and use 4D convolution to enhance the processing of point cloud temporal information. LMNet [12] was the initially proposed model which introduces 2D projection in the range view to construct residual maps for capturing motion in- formation. Similarly, models such as MotionSeg3D [5] and MF-MOS [7] are also built upon 2D representations, and they incorporate semantic branches and significantly improve segmentation accuracy. MotionBEV [6] adopts the BEV perspective for 2D projection, identifying moving objects based on point cloud height differences. However, such models are still subject to the high computational burden of 3D/4D convolutions and the inevitable information loss in single-view 2D representations. Motivated by such issues in MOS, we propose a multi-branch MOS neural network structure that combines BEV and range views. Our proposed structure leverages efficient 2D convolutional neural network models and the adaptive fusion module to effectively capture rich semantic and motion information from multiple repre- sentations."}, {"title": "III. METHODOLOGY", "content": "A. Data Prepossessing\nThe original LiDAR point cloud data (x, y, z) are firstly transformed into range view and BEV presentations to derive the projected mappings $I_{rv}(u, v)$ and $I_{BEV}(\\tilde{u}, \\tilde{v})$, where (u, v) and (\u0169, v) represent coordinates in the 2D images, and i denotes a specific ith frame of point cloud data. $I_{BEV}(\\tilde{u}, \\tilde{v})$ will also be used as the input $X_{Semantic}$ for the semantic branch (the 3D-2D transformation process follows the standard setup of methods in [5], [6]).\nThe range view and BEV residual maps for the k-th frame and the 0-th frame are represented by the following formulas:\n$I_{kes-rv}^{k-0}(u, v) = \\frac{I_{rv}^{k}(u, v) - I_{rv}^{0}(u, v)}{I_{rv}^{0}(u, v)}$ (1)\n$I_{kes-bev}^{k-0}(\\tilde{u}, \\tilde{v}) = \\frac{I_{BEV}^{k}(\\tilde{u}, \\tilde{v}) - I_{BEV}^{0}(\\tilde{u}, \\tilde{v})}{I_{BEV}^{0}(\\tilde{u}, \\tilde{v})}$ (2)\nwhere $I_{res-bev}^{k-0}(\\tilde{u}, \\tilde{v})$ and $I_{res-rv}^{k-0}(u, v)$ will also be used as the input $X_{bev}$ and $X_{rv}$ for the motion branch.\nTo improve robustness, a stacked frame representa- tion $I_{BEV}^{k'}(\\tilde{u}, \\tilde{v})$ is utilized in this work to replace the $I_{BEV}^{k}(\\tilde{u}, \\tilde{v})$ in the above equation:\n$I_{BEV}^{k'}(\\tilde{u}, \\tilde{v}) = Max{Z(\\tilde{u}, \\tilde{v})} - Min{Z(\\tilde{u}, \\tilde{v})}$ (3)\nwhere Z(\u0169, v) represents all elevation information within the k'-frames window.\nSince the projected views $I_{RV}(u, v)$ and $I_{BEV}(\\tilde{u}, \\tilde{v})$ of the two views both originate from the coordinates (x, y, z) in the 3D point cloud space, we can derive the transforma- tion matrices (denoted as $Matrix_{r2b}$ and $Matrix_{b2r}$) based on this relationship. Converting the representation $I_{RV}(u, v)$ of the range view to the BEV view represen- tation $I_{r2b}(u, v)$ can be simply achieved by:\n$I_{r2b}(u, v) = F_{grid-sample}(I_{rv}(u, v), Matrix_{r2b})$ (4)\nThe BEV and RV residual maps built using Eq. 1-3 can be fed to the motion and semantic branches in the proposed model for further feature extraction. (Details of the derivation process can be found in [13]).\nB. Network Structure\n1) Motion Branch Structure Based on Muti-View Residual Map Fusion: Due to the lightweight design of the UNet model, it can facilitate the additional overhead introduced by multi-branch networks. Therefore, we build our proposed model on the backbone of UNet, as shown in Fig. 2."}, {"title": "2) Semantic Branch Structure Based on BEV Perspec- tive Projection", "content": "The proposed semantic branch serves two purposes in MV-MOS. Firstly, for the motion branch, the residual map contains information about the motion state of objects. Therefore, during the down-sampling stage of the backbone network, the point cloud projection feature map in the semantic branch adds rich object appearance semantic information to the motion branch, making the extraction of motion object features by the backbone network more accurate. This process is formulated as:\n$X_{fused_i} = X_{motion_i} + Attention_{hwc}(\nf_{circularconv}(Cat(X_{motion_i}, X_{semantic_i})))$ (9)\nSecondly, the semantic branch is responsible for predicting the movable attributes of objects and uses the same-sized semantic feature maps as guidance for the motion branch, enabling the primary motion branch to focus on detecting movable objects. This process involves two steps. First, the semantic branch performs up-sampling using the various sizes of $X_{semantic_i}$:\n$X_{Sout}^{i} = F_{up}(X_{semantic_{i+1}}, X_{semantic_{i}})$ (10)\n$F_{up}(X_{i+1}, X_i) = f_{circularconv}(Cat(Up(X_{i+1}), X_i))$ (11)\nwhere Up() represents the PixelShuffle [14] upsampling operation. The ultimate predicted output $X_{Sout}$ is used with the movable labels of objects to calculate the loss for training the motion branch, and enhance its capability of discerning object movability. In addition, the outputs $X_{Sout_i}$ from the semantic branch upsampling layers are combined with the output feature map $X_{fused_i}$ of the backbone network, providing guidance to the up-sampling stage of the motion branch. This process is formulated as:\n$X_{out} = F_{up}(X_{fused}, Cat(X_{Sout}, X_{fused_i}))$ (12)\nThe final output $X_{out}$ not only incorporates motion in- formation from dual-view residual maps, but also integrates semantic information of objects separately in the down- sampling and up-sampling stages. Such design effectively enhances the model's capability to fully exploit features of moving objects."}, {"title": "3) Density-aware Adaptive Feature Fusion Module", "content": "In the proposed dual-branch motion-semantic feature fusion struc- ture, we utilize the motion stream as the primary branch and semantic information as the auxiliary branch. The proposed fusion module sequentially performs attention calculations on the spatial and channel dimensions of the semantic and the motion information input $F_s$, $F_m$, respectively. In this manner, the semantic information can be used as an auxiliary to activate effective motion information in the motion feature map, while supplementing important contour information that might be lost in the residual map calculation. This process can be formulated as:\n$F'_m = F_m \\oplus Sigmoid(Conv_{1\\times 1}(F_s))$ (13)\n$F_{out} = F_m + F'_m \\otimes Softmax(\nConv_{1\\times 1}(AvgPool(F_m)))$ (14)\nCompared with the semantic feature maps, the motion feature maps obtained from residual maps are sparser. As a result, directly fusing these two types of feature maps would cause the dense semantic features to dominate the synthesized feature, which may deteriorate the resulting MOS performance, as the semantic features are supposed to be auxiliary. To mitigate such feature imbalance, we designed an adaptive feature fusion mechanism based on the mamba structure [15], which can dynamically adjust its state based on the model's current input and selectively retain important information in the sequence. Considering that the current information flow is represented as 2D images, the SS2D (consisting of Scan Expansion, S6 Block, and Scan Merge) mechanism proposed by Vmamba [16] is more suitable for the application in this work. Therefore, we introduce a Mamba mechanism based on SS2D and embed it into the semantic-motion dual-branch fusion module, as illustrated in Fig. 3:\n$F_{fused} = F_{sm} + DropPath(\nSS2D(Linear(Cat(F_{sm}))))$\n(15)\nwhere $F_{sm}$ is derived from the concatenation of $F_s$ and $F_m$, and $F_{fused}$ will replace fm in Eq. (13) and (14). In this manner, the proposed feature fusion process can focus on key elements and ignore uninformative redundant features."}, {"title": "C. Loss Function", "content": "During the training process, we introduce two sets of labels: moving objects and movable objects, and the motion branch and semantic branches are trained separately. The total loss $L_{Total}$ is given by:\n$L_{Total} = L_{Moving} + L_{Movable}$, (16)\nwhere $L_{Moving}$ and $L_{Movable}$ represent the segmentation loss functions for moving objects and movable objects, respec- tively. Both $L_{Moving}$ and $L_{Movable}$ are composed of the cross-entropy loss function($L_{CE}$) and the Lov\u00e1sz-Softmax function($L_{LS}$), combined with the relevant labels for calcu- lation:\n$L_{Loss} = L_{CE} + L_{LS}$ (17)\n$L_{CE} = - \\frac{1}{N} \\sum y_i log (p_i)$ (18)\nwhere N is the number of samples, y is the ground truth label of the ith sample (0 or 1), $p_i$ is the predicted probability of the ith sample being in the positive class. The Lov\u00e1sz- Softmax loss is defined by:\n$L_{LS} = \\frac{1}{|C|} \\sum_{c=1}^{|C|} Lov\u00e1sz (\\Delta J_c)$ (19)\n$Lov\u00e1sz(\\Delta J) = \\sum_{i=1}^{N} \\Delta J(i) (m(i) \u2013 m(i-1))$ (20)\nwhere |C| is the number of classes in a multiple classifica- tion problem. \u2206J is the Jaccard loss subset associated with class c, representing the change in IoU loss for each class. \u2206J(i) is the IoU loss change for the i-th sample after being sorted in descending order of the predicted error probability. m(i) is the cumulative error for the i-th sample after sorting."}, {"title": "IV. EXPERIMENTS", "content": "A. Experiments Setups\nThe proposed MV-MOS is trained and evaluated on the SemanticKITTI [17], which is a public LiDAR point cloud dataset for the MOS task. The dataset includes semantic la- bels for 28 classes of objects in real-world driving scenarios, such as pedestrians, cars, as well as labels for dynamic/static and movable attributes.\nOur proposed MV-MOS is implemented based on PyTorch 1.12.0 and all experiments are conducted on NVIDIA RTX 4090 and Tesla V100 GPUs. The number of training epochs is set to 100, with an initial learning rate of 0.01 and a decay factor of 0.95 per epoch. The batch size is set to 4 per GPU. During training, we use the SGD optimizer with a momentum of 0.9 and a weight decay of 0.0001. Following the standard evaluation practice in MOS tasks, we use the Intersection over Union (IoU) metric to quantify the performance of our proposed approach in all experiments.\nB. Evaluation Results and Comparisons\nTable I presents the comparison results between MV-MOS and other state-of-the-art methods in the MOS task. From the table, it can be seen that the baseline LMNet, which only uses a single motion branch, achieves an accuracy of only 63.8% on the validation set. In contrast, recent SOTA models Mo- tionBEV and MF-MOS, which combine motion and semantic branches, achieve IoU values of over 76%, surpassing voxel- based methods. This demonstrates the significant potential of models based on 2D projection methods. Our proposed MV-MOS integrates multiple perspectives and branches to enhance the capability of 2D projection methods in capturing motion information from 3D point cloud data. The proposed MV-MOS achieves the highest accuracy of 78.5% on the validation setwhich outperforms all other baseline MOS models. In addition, we further tested the proposed model on the official SemanticKITTI-MOS benchmark server, and our proposed MV-MOS currently currently ranks first among all open-source models, with an IoU of 80.6%.\nC. Ablation Studies\nWe conducted further ablation experiments to investigate how the carefully designed individual structures and modules affect the performance of MV-MOS, with results shown in Table II.\nThe first set of experiments evaluate the benefit of the semantic information. We built two more mutated MOS models with different combination of feature extraction branches: the MV-MOS(i) represents the model without the semantic branch, with the motion branch based only on BEV residual maps; MV-MOS(ii) represents the introduction of only the motion-semantic dual-branch structure based on the BEV representations, with an additional guiding branch for movable attribute inference as an auxiliary branch. For a more intuitive comparison, we also introduced LMNet and MotionBEV as baseline approaches. As can be seen from the results (Table II), the segmentation accuracy of MV-MOS(ii) and MotionBEV, which combine the semantic and motion branches, is about 10% higher than that of the semantic-only LMNet and MV-MOS(i). Additionally, compared with the basedline MotionBEV, the motion-semantic multi-branch structure design in the proposed MV-MOS provides auxiliary feature-guidance, which result in an increase of IoU of nearly 0.5%.\nThe second set of experiments aims to demonstrate the effectiveness of the combination of the RV and BEV view residual branches in the motion branch of MV-MOS (subse- quently referred to as B(EV)R(ange View)-Motion-Branch), as well as the role of the adaptive fusion mechanism. As shown in Table II, the first row of MV-MOS(iii) indicates that introducing the BR-Motion-Branch into a MotionBEV- like motion-semantic dual-branch structure improves the accuracy from 76.54% to 77.38%. Introducing the BR-Motion-Branch into MV-MOS(ii) increases the accuracy to 77.66%. Additionally, incorporating our proposed Mamba Block at the connection point between the motion and semantic branches increases the accuracy to 78.14%, an 1.14% improvement over MV-MOS(ii).\nThe third set of experiments demonstrates that our final model, MV-MOS, combining all modules, achieves the high- est accuracy of 78.53%. These three sets of experiments successively verify the effectiveness of the proposed modules from multiple aspects."}, {"title": "D. Qualitative Analysis", "content": "To more intuitively demonstrate the effectiveness of MV- MOS, we performed a qualitative analysis through visual- ization. Fig. 4 compare the performance results of MV-MOS and several state-of-the-art models on the SemanticKITTI validation set. As shown, our model correctly infers more points, and the segmentation of objects is more complete (indicated by blue points). Additionally, when moving ob- jects are at the edge of the field of view\u2014either suddenly appearing or about to disappear-other models are prone to missed detections (green points) and false detections (red points). In contrast, our model, based on the multi- branch design, leverages motion information represented by multi-view 2D projections, which allows it to handle such situations more effectively."}, {"title": "E. Computational Efficiency", "content": "All runtime testing experiments were conducted on a sin- gle Tesla V100 GPU for inference. As shown in Table III, we compared MV-MOS with four state-of-the-art models from the past two years. In terms of inference time, MV-MOS significantly outperforms InsMOS [1] and MotionSeg3D [5] and shows nearly a 20% improvement over MF-MOS [7]. Among all five compared models, MotionBEV [6] achieves the fastest per-frame processing speed at 34.41ms, surpassing all other models. The additional inference time of MV- MOS compared to MotionBEV [6] is anticipated due to the introduction of an extra range view motion branch and a guiding branch for inferring the movable attributes of objects. Given that MV-MOS consistently demonstrates significantly higher accuracy across different datasets compared to Mo- tionBEV [6], the added computational overhead is justifiable. Considering that LiDAR typically operates at a frame rate of 10 Hz, and MV-MOS's per-frame inference speed is less than 100ms, it meets the requirement for real-time process- ing. Combined with its superior segmentation accuracy, this evidence highlights the superiority and practicality of our method in the moving object segmentation task."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel 3D moving object segmentation neural network model based on multi-view and motion-semantic branch feature fusion. The proposed multi-branch BEV and range view fusion structure can effectively exploit complementary motion information of moving objects from different perspectives, avoiding the loss of valuable information during the transformation of 3D point clouds to 2D representations. In addition, the proposed semantic branch and Mamba-based feature fusion design can effectively provide guidance and enhancement to the motion features, which ultimately improves the quality of synthesized features. Comprehensive comparative and abla- tion experiments validate the effectiveness and generalization of our proposed model. In addition, the proposed model also achieves desirable inference computation efficiency. Therefore, our proposed MV-MOS shows great potential for applications in practical autonomous driving and robotics systems."}]}