{"title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning", "authors": ["Xinyu Zhang", "Yuxuan Dong", "Yanrui Wu", "Jiaxing Huang", "Chengyou Jia", "Basura Fernando", "Mike Zheng Shou", "Lingling Zhang", "Jun Liu"], "abstract": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning, a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard problems requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and 03-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identify four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https://dxzxy12138.github.io/PhysReason/.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable performance across various domains, such as math (Lightman et al., 2024; Cobbe et al., 2021) and logical reasoning (Hendrycks et al.; Xu et al., 2025). However, current evaluations often overlook physics-based reasoning, limiting their applications in scenarios such as robotics (Chow et al., 2025) and autonomous driving (Huang et al., 2023). This is because physics-based reasoning, integrating multiple theorems and physics constraints, is more closely aligned with practical applications than math and logical reasoning. Consequently, developing a comprehensive benchmark for evaluating LLMs' physics-based reasoning capabilities is crucial for discovering current limitations and guiding future improvements.\nThere are several pioneering physics benchmarks (K-12 level like ScienceQA (Lu et al., 2022), college-level SciBench (Wang et al.), and expert-level GPQA (Rein et al., 2024)) encompassing progressively advanced knowledge domains. However, they exhibit two critical limitations: oversimplified reasoning processes and neglecting step-level evaluation. These problems typically involve only 3-4 physics formulas, focusing solely on final answers to measure model performance. Therefore, a benchmark featuring in-depth reasoning processes and step-level evaluation is urgently needed to measure LLMs' physics-based reasoning capabilities.\nTo address these limitations, we present PhysReason, a comprehensive benchmark comprising 1,200 problems designed to evaluate models' physics-based reasoning capabilities. As illustrated in Figure 1, PhysReason features physics problems that require multi-step reasoning and precise application of physics theorems. The benchmark introduces several key characteristics:\n1. Stratified difficulty: There are knowledge-based (25%) and reasoning-based (75%) problems, with reasoning problems categorized into easy, medium, and hard (25% each).\n2. Complex reasoning: Solutions average 8.1 steps per problem, with hard problems reaching 15.6 steps, exceeding current physics benchmarks which typically only contain 3-4 steps.\n3. Multi-modal design: 81% of problems include diagrams, evaluating models' capabilities in comprehending visual and textual information.\nTo evaluate performance on PhysReason comprehensively, we propose the Physics Solution Auto Scoring Framework (PSAS) based on current LLMs' capabilities in information extraction and formula comparison. This framework encompasses two answer-level and step-level evaluation methods, PSAS-A and PSAS-S. PSAS-A enables efficient evaluation through answer comparison, while PSAS-S facilitates comprehensive analysis through step-by-step reasoning verification. Experimental results demonstrate that PSAS significantly outperforms direct LLM-based evaluation approaches, achieving an evaluation accuracy exceeding 98%.\nWe evaluate seven non-O-like models and eight O-like models on the PhysReason benchmark. Results show that while Deepseek-R1 (Guo et al., 2025), Gemini-2.0-Flash-Thinking-0121 (Deepmind), and o3-mini-high (OpenAI, 2025) demonstrate superior performance, their average scores remain below 60%. Moreover, models excel in basic physics concepts but consistently show performance degradation as problem difficulty and required solution steps increase (from 75.11% to 31.95%). This degradation stems from the models' inability to maintain accuracy across consecutive solution steps, so maintaining the reliability of the reasoning process is crucial. Through step-level evaluation, we identify four critical bottlenecks limiting model performance: Physics Theorem Application, Physics Process Understanding, Calculation Process, and Physics Condition Analysis."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Large Language Model Evaluation", "content": "LLMs have demonstrated remarkable performance across various domains, such as math reasoning (Jiang et al., 2024; Li et al., 2024; Imani et al., 2023), logical reasoning (Sun et al., 2024a; Xu et al., 2024), and text generation (Zhao et al., 2024; Liang et al., 2024). However, they struggle with physics world interactions, limiting their adoption in areas like autonomous driving and robotics (Gao et al., 2024b). Unlike mathematical and logical reasoning, physics-based reasoning requires the integration of multiple principles and physics-world constraints (Kline, 1981). Therefore, mastering physics-based reasoning is fundamental to unlocking LLMs' potential in practical applications (Lai et al., 2024). Current evaluations primarily focus on mathematical and logical reasoning, revealing a crucial gap in evaluating LLM capabilities based on physics-based reasoning."}, {"title": "2.2 Physics Benchmarks", "content": "Existing physics benchmarks span three knowledge complexity levels: K-12 (ScienceQA (Lu et al., 2022), E-EVAL (Hou et al., 2024)), college-level (MMLU (Hendrycks et al.), AGIEval (Zhong et al., 2024), JEEBench (Arora et al.), TheoremQA (Chen et al., 2023), EMMA (Hao et al., 2025), SciEval (Sun et al., 2024b), C-Eval-STEM (Huang et al., 2024), SciBench (Wang et al.)), and expert-level (OlympiadBench(He et al., 2024), GPQA (Rein et al., 2024)). While these benchmarks showcase LLMs' knowledge breadth, they simplify reasoning to 3-4 steps and emphasize only final answers. PhysReason addresses these gaps through complex reasoning process and step-level evaluation."}, {"title": "3 Benchmark", "content": ""}, {"title": "3.1 Collection", "content": "We describe our comprehensive data collection process that spans five key stages: Acquisition, Standardization, Translation, Search Prevention, and Difficulty Classification."}, {"title": "3.2 Annotation", "content": "As shown in Figure 1, our annotation framework consists of 8 key elements: Diagram, Context, Sub-questions, Solution, Step Analysis, Answer, Theorem, and Difficulty. Context presents the physics scenario and conditions. Diagram visualizes the physics scenario with concise illustrations complementing the Context. Sub-questions give questions to assess the understanding and application of the concept. Solution provides a step-by-step reasoning process, and Answer gives the answer to each sub-question. Step Analysis explains the physics theorem used in each step and the physics quantities obtained. Theorem lists the physics theorems applied in the question, and Difficulty indicates the difficulty classification."}, {"title": "3.3 Characteristics", "content": "PhysReason consists of 1,200 carefully curated physics problems as shown in Table 1, with a strategic composition of 25% knowledge-based and 75% reasoning-based questions across various difficulty levels, collectively covering 147 physics theorems. The problems span Classic Mechanics, Quantum Mechanics, Fluid Mechanics, Thermodynamics, Electromagnetics, Optics, and Relativity. As shown in Figure 2, three critical solution metrics (theorem, step, and token) correlate positively with problem difficulty levels, validating the rationality of our difficulty classification. Notably, the medium and hard problems demonstrate higher complexity compared to existing benchmarks. PhysReason distinguishes itself through three key characteristics:\n1. Stratified difficulty: The benchmark maintains a carefully balanced composition of knowledge-based (25%) and reasoning-based (75%) problems. The reasoning-based problems are methodically distributed across three difficulty levels (easy, medium, and hard - 25% each), enabling comprehensive capability evaluation.\n2. Complex reasoning: Detailed step-by-step solution annotations accompany each problem. These annotated solutions demonstrate complex reasoning chains averaging 8.1 steps per problem, with hard problems requiring up to 15.6 steps, significantly surpassing the complexity of existing physics-based reasoning benchmarks.\n3. Multi-modal design: The benchmark features a high proportion (81%) of problems with diagrams, authentically replicating physics-based reasoning scenarios while effectively evaluating both textual and visual reasoning capabilities."}, {"title": "4 Evaluation Framework", "content": ""}, {"title": "4.1 Why LLMs Can Evaluate?", "content": "Unlike multiple-choice problems, PhysReason contains open-ended answers and steps with diverse expressions but consistent semantics. Given that LLMs have demonstrated exceptional capabilities in both precise content extraction and formula consistency evaluation (Contributors, 2023; Gao et al., 2024a), they serve as practical tools for automated physics solution evaluation. Therefore, we propose automated answer-level and step-level evaluations, achieving comprehensive evaluation and avoiding labor-intensive manual assessment."}, {"title": "4.2 How Answer-Level Evaluation Works?", "content": "We develop Physics Solution Auto Scoring Framework-Answer Level (PSAS-A), which evaluates based on sub-question answers. Given a model's reasoning process M for a problem with sub-questions {q1, q2, ..., qn}, we first extract the model's answers $\\hat{a_i}$ for each $q_i$ from M with an LLM. Then, we employ the LLM to verify if $\\hat{a_i}$ is semantically consistent with the standard answer $a_i$ of sub-question $q_i$. The comparison function $C(\\hat{a_i}, a_i)$ returns 1 if consistent and 0 otherwise. Considering that the sub-questions with different steps should not carry equal weights in scoring, we use the length of annotation solution $s_i$ of sub-question $q_i$, i.e., $|(s_i)|$ as a weighting scalar. The model's reasoning process M's answer-level score for each problem is calculated as follows:\nScore(M) = $\\frac{\\sum_{q_i} |(s_i)| \\times C(\\hat{a_i}, a_i)}{\\sum |(s_i)|}$"}, {"title": "4.3 How Step-Level Evaluation Works", "content": "The current mainstream evaluation approach (He et al., 2024) with LLMs relies on answers, failing to reveal how and where models deviate from correct reasoning paths. To address this, we propose the Physics Solution Auto-Scoring Framework-Step Level (PSAS-S), which enables detailed assessment and analysis of each reasoning step. The framework is divided into four phases: Data Extraction, Scoring, First Error Step Detection, Error Analysis, as detailed in Algorithm 1.\nData Extraction phase leverages LLMs using Target components from Step Analysis annotations (Figure 1) as prompts to locate and extract relevant content from model outputs for each annotated solution step $s_i$. This phase effectively handles redundant thinking processes in LLM's reasoning process while maintaining semantic equivalence. It obtains the mapping relationship between extracted relevant steps E and annotated solution steps S.\nScoring phase evaluates each step $s_i$ through two complementary components of theorem assessment $ScoreFormula(f_i, \\hat{f_i})$ and result verification $ScoreValue(v_i, \\hat{v_i})$, each with a weight of 0.5. The final score is calculated as shown in Algorithm 1. This ensures a balanced assessment of theorem application and computational accuracy.\nFirst Error Step Detection phase identifies the earliest step of deviation from the correct solution path. When any step is found with a score below 1, FindOriStep function locates the corresponding original step in the model's raw output based on the mapping relationship between E and S obtained from the Data Extraction phase, and updates first_error_step to maintain the earliest error position. This enables precise identification of where the model's reasoning first goes wrong.\nError Analysis phase analyzes the first error step detected in the solution, with two components: error classification and error analysis. For error classification, PSAS-S considers seven types of common errors: Diagram Analysis Error (DAE), Physics Theorem Application Error (PTAE), Physics Condition Analysis Error (PCAE), Physics Process Understanding Error (PPUE), Variable Relationship Error (VRE), Calculation Process Error (CPE), and Boundary Condition Analysis Error (BCAE). Detailed error-type descriptions are available in the Appendix C. LLMs use structured prompts to identify the error type for the first error step. Then, a comprehensive error analysis is generated to explain the reasoning behind the mistake. A simplified example is shown in Figure 3."}, {"title": "4.4 Whether Evaluation Trustworthy?", "content": "To validate the reliability of both our PSAS-A and PSAS-S, we compare our PSAS against conventional direct LLM evaluation approaches at both answer-level and step-level, using the Chain-of-Thought reasoning strategy. We implement experiments using Deepseek-V3 and Gemini-2.0-Flash as scoring models in the following experiments:\n1. For answer-level evaluation, we employ scoring models to assess answer correctness by combining both model-generated outputs and annotation answers. We then compare these results with the judgments obtained from PSAS-A.\n2. For step-level evaluation, inspired by previous work (Zheng et al., 2024), we design the task of identifying the first error step in reasoning processes containing errors, where higher accuracy indicates a more precise evaluation of the reasoning process. Then, we submit both model-generated and annotated reasoning processes to scoring models to determine the location of the first error step, comparing with PSAS-S.\nThen, we collect 8,400 reasoning processes generated from multiple advanced models, including Deepseek-R1, Gemini-2.0-Flash, Gemini-2.0-Flash-Thinking-0121, GLM-Zero, o1-mini, QwQ-32B, and QvQ-72B. Subsequently, we randomly sample 1,000 reasoning processes and meticulously manually annotate them to determine the correctness of each answer and identify the location of the first error step. The results presented in Table 2 demonstrate that our frameworks achieve superior performance compared to direct LLM evaluation, highlighting the accuracy and reliability of PSAS evaluation results on PhysReason."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Setting", "content": "Baselines: We evaluate current mainstream open-source and closed-source LLMs, VLMs, and several O-like models. For models that cannot accept visual inputs, we use Gemini-2.0-Flash to generate captions for each image as supplementary information. We assess 15 advanced LLMs/VLMS under the zero-shot Chain-of-Thought (CoT) setting (encouraging models to think step by step), including 7 non-O-like models (Qwen2-VL-72B (Wang et al., 2024b), GPT-4O (OpenAI), Claude-3.5-Sonnet (Anthropic), InternVL2.5-78B (Chen et al., 2024), Deepseek-v3 (DeepSeek-AI, 2024)), Gemini-2.0-Flash (Deepmind, 2024), Gemini-2.0-Pro (Deepmind, 2025) and 8 O-like models (QwQ-32B (Team, 2024b), QvQ-72B (Team, 2024a), o1-mini (OpenAI, 2024b), o1 (OpenAI, 2024a), o3-mini-high (OpenAI, 2025), Gemini-2.0-Flash-Thinking (Deepmind), Deepseek-R1 (Guo et al., 2025), GLM-Zero (ZhipuAI, 2024)). Note that Gemini-2.0-Flash-Thinking has two versions: 1206 and 0121. Due to API limitations, we do not experiment with o1 on the entire dataset. All other models are evaluated on the complete benchmark.\nEvaluation Workflow: We encourage models to generate reasoning processes step by step for all problems in PhysReason, with open-source models running on NVIDIA A800 GPUs. Please refer to Appendix-E for the detail prompt template. Then, we evaluate the models' performance with the PSAS framework at both the answer and step levels, as described in Sections 4.2 and 4.3. Based on the experimental results in Section 4.4, considering both efficiency and performance, we select Deepseek-V3 as the final scoring model.\nPhysReason-mini: Considering that the complete PhysReason requires relative high evaluation costs, we create a balanced PhysReason subset PhysReason-mini. We randomly sample 200 questions from the whole benchmark (50 for each difficulty level), striving to achieve equal representation across categories wherever possible."}, {"title": "5.2 Main Results", "content": "As demonstrated in Tables 3 and 4, the experimental results on the PhysReason and PhysReason-mini reveal several significant findings.\nModel Categories: O-like models exceed non-O-like ones, with multiple O-like models surpassing 50% answer-level accuracy compared to non-O-like models' peak of 47.88%.\nDifficulty Level Analysis: As the difficulty increases, the required solution steps also increase, while model performance severely declines, indicating that models still perform inadequately on physics problems requiring deep reasoning.\nStep-level vs. Answer-level Evaluation: The two evaluation frameworks assess performance from different perspectives. Step-level scores consistently surpass answer-level scores, indicating that models can achieve some correct steps despite failing to reach the correct final answer. Moreover, the step-level score differences between models become more pronounced than those at the answer level as problem difficulty increases. This demonstrates that step-level evaluation proves more discriminative in distinguishing model capabilities, particularly in highly challenging problems. The distributions of these two evaluation methodologies exhibit non-perfect synchronization, indicating that step-level evaluation provides comprehensive insights to answer-level assessment.\nMedium and Hard Problem Analysis: Performance on medium and hard reasoning problems can emerge as key differentiators of model physics-based reasoning ability. Among these models, those achieving scores of 40/60 and 30/50 on answer-level and step-level evaluations respectively serve as critical reference points.\nKnowledge-Reasoning Correlation Analysis: Results show a positive correlation between physics knowledge and reasoning capabilities, with Deepseek-R1 and Gemini-2.0-Flash-Thinking-0121 excelling in both aspects. Moreover, among models with similar scores on knowledge problems, O-like models tend to achieve higher scores on reasoning problems (as demonstrated by Gemini-2.0-Flash and Gemini-2.0-T\u2020). This suggests that reinforcement learning and training with thought chains help improve models' reasoning capabilities. In conclusion, effective reasoning relies on knowledge capacity and model architecture."}, {"title": "5.3 Results with Test-Time Compute Scaling", "content": "We evaluate Best-of-N (BoN) and Tournament-Style selection (Snell et al., 2025; Yang et al., 2024) test-time compute scaling methods on PhysReason-mini. Using Gemini-2.0-Flash and Gemini-2.0-Flash-Thinking-0121 as base models, we test different reward model configurations: when Flash serves as base model, both itself and Thinking-0121 are evaluated as reward models, while Thinking-0121 uses self-reward due to its superior reasoning. Both methods (Cobbe et al., 2021; Lightman et al., 2024; Son et al., 2024) select optimal responses from multiple Chain-of-Thought candidates (N = 1, 2, 4, 8), as shown in Table 5. These scaling methods demonstrate the potential to enhance model performance through strategic response selection and process reward modeling."}, {"title": "5.4 Performance Improving with PSAS-S", "content": "Given PSAS-S's capability to locate and analyze the first error step as presented in Section 4.3, we conduct experiments on PhysReason-mini to explore whether models can correct errors after being informed. The experiments are divided into Direct concatenation and Guided error localization. The former (D. Acc.) combines questions with the previous reasoning process for a second attempt. For the latter (G. Acc.), PSAS-S is used to locate and analyze the first error in the reasoning process, then combines the question, previous reasoning, and the location and analysis of the first error for a second attempt. As shown in Table 6, results show that direct concatenation decreased performance by 3-5%, while guided error localization improved performance by 3-6%. This suggests that guiding LLMs to identify reasoning errors is crucial for enhancing their reasoning capabilities and also proves the effectiveness of our PSAS framework."}, {"title": "5.5 Error Kind Distribution Analysis", "content": "Discovering errors in reasoning processes is not equivalent to fully understanding them; it's also crucial to understand the causes of errors. We analyze the error distributions of different models on PhysiReason-mini as shown in Figure 4. Four prevalent error types consistently challenge all models: Physics Theorem Application, Physics Process Understanding, Calculation Process, and Physics Condition Analysis. This reveals models' limited intuitive physics understanding, highlighting the need for stronger physics-based reasoning capabilities. Notably, o1 and o3-mini-high show elevated Physics Process Understanding Errors but reduced Calculation Process Errors. This maybe suggest a trade-off between conceptual comprehension and computational precision."}, {"title": "5.6 Hard Problem Analysis", "content": "Our analysis of 50 hard reasoning problems from PhysReason-mini across 7 models reveals two key insights (Figure 5). Despite variations in overall performance, each model exhibits unique strengths in specific problem domains, demonstrating the diverse nature of their reasoning capabilities. The models' achievement of some scores (below 1) is notable, indicating their ability to initiate correct solution paths but failing to maintain this accuracy throughout the reasoning process. These patterns suggest that while current models grasp basic physics concepts, they struggle to sustain accurate reasoning across extended solution steps."}, {"title": "6 Conclusion", "content": "We introduce PhysReason, a novel physics-based reasoning benchmark with stratified difficulty and Physics Solution Auto-Scoring Framework with answer and step level evaluation. Experimental results show a consistent decline in performance as reasoning depth increases. This benchmark establishes new standards for evaluating and improving AI models' physics-based reasoning abilities."}, {"title": "Limitation", "content": "Despite the comprehensive nature of our benchmark, two key limitations warrant discussion, concerning both benchmark construction and evaluation methodology. First, they focus primarily on testing models' ability to apply and reason with physics theorems under idealized conditions, rather than fully reflecting real-world physics scenarios. However, it is worth noting that applying physics theorems under idealized conditions serves as the foundation for real-world physics scenarios, as the latter is more complex. However, current LLMs' performance even on idealized conditions remains unsatisfactory. Therefore, PhysReason remains valuable in evaluating models' ability to apply physics theorems for physics-based reasoning. Moreover, through data synthesis, many problems in PhysReason can be adapted to create real-world physics reasoning scenarios, which will be a direction for our future research. Second, our evaluation framework, though achieving over 98% accuracy using LLMs as assessment tools, is not without limitations. The PSAS-S framework, while demonstrating satisfactory performance, increases computational time for evaluation. In future work, we will explore ways to optimize evaluation time while maintaining assessment accuracy."}, {"title": "Ethical Statement", "content": "In developing PhysReason, we carefully considered and addressed potential implications and risks. Our benchmark, sourced exclusively from public official materials (IPhO, Gaokao, JEE, and authorized mock exams), undergoes rigorous data cleansing, deduplication, and standardization to ensure reliability while minimizing bias and data leakage. Committed to environmental sustainability, we publicly release complete datasets and accompanying scripts under appropriate licenses (MIT and CC BY-NC-SA) to cut down on unnecessary carbon footprint, while optimizing processing pipelines to reduce computational overhead. In all experiments, we strictly comply with all licenses for models and data. Our benchmark is an important resource that drives AGI's strength in scientific reasoning, maintaining high standards for data quality and ethical considerations."}, {"title": "Step Analyze:", "content": "Due to page space limitations, not displayed"}, {"title": "I Details of human annotators", "content": "For data annotation and evaluation, we engaged four graduate students (including both PhD and Master's students) from engineering disciplines who are also co-authors of this paper. All annotators possessed strong backgrounds in both high school and undergraduate physics, making them well-qualified for this task. Since the annotators were co-authors actively involved in the research, no formal recruitment process or compensation was required, and they were fully aware of how the data would be used in the study. The annotation process focused solely on physics content evaluation and did not involve collecting any personal identifying information or expose annotators to any risks. As this research involved co-authors analyzing academic content rather than external human subjects, it was determined to be exempt from formal ethics review board approval. The annotation work was conducted as part of regular academic research activities within our institution. No protected or sensitive demographic information was collected or used in this research."}, {"title": "J Details of Ai Assistants In Research Or Writing", "content": "We used Claude-3.5-Sonnet, o1, o3-mini-high, and Deepseek-R1 to help us write code and polish the paper."}]}