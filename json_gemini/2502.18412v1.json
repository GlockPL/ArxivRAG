{"title": "COMPARATIVE ANALYSIS OF MDL-VAE VS.\nSTANDARD VAE ON 202 YEARS OF\nGYNECOLOGICAL DATA", "authors": ["Paula Santos"], "abstract": "This study presents a comparative evaluation of a Variational Autoencoder (VAE)\nenhanced with Minimum Description Length (MDL) regularization against a Standard\nAutoencoder for reconstructing high-dimensional gynecological data. The MDL-VAE\nexhibits significantly lower reconstruction errors (MSE, MAE, RMSE) and more structured\nlatent representations, driven by effective KL divergence regularization. Statistical analyses\nconfirm these performance improvements are significant. Furthermore, the MDL-VAE\nshows consistent training and validation losses and achieves efficient inference times,\nunderscoring its robustness and practical viability. Our findings suggest that incorporating\nMDL principles into VAE architectures can substantially improve data reconstruction and\ngeneralization, making it a promising approach for advanced applications in healthcare\ndata modeling and analysis.", "sections": [{"title": "1. INTRODUCTION", "content": "Despite substantial advances in medical research, early detection of menstrual disorders and\ntumors in the female reproductive system remains a significant challenge. This issue is critical\nbecause timely detection is essential for improving treatment outcomes, quality of life, and\npatient survival rates. Today, with the advent of advanced computing power, we can tap into the\nvast body of knowledge accumulated over decades of published research. This capability allows\nfor a comprehensive review and synthesis of the vast data collected over time.\nThe new era of hardware and models with high capacity to store and analyze large datasets at\nhigh speeds allows us to draw connections across diverse fields such as epigenetics, molecular\nand cellular biology, and pathophysiology. These interactions between different data can shed\nlight on the natural history of diseases, especially those classified as silent diseases, which are\ndifficult to diagnose early, even for specialists, due to nonspecific symptoms in the early stages.\nThis often leads to late diagnosis, resulting in prolonged suffering and higher mortality rates,\nparticularly in cancer diagnoses.\nRegarding generative AI models, there are still significant limitations, especially in terms of\nhallucinations arising in contexts of undersampled clinical data and discrepancies between\npopulations. These obstacles point to the need for further studies that explore the improvement\nof models in critical situations, especially validation in adverse scenarios across diverse\npopulations. One aspect that remains underexplored is the logical relationships that these"}, {"title": "2. METHODOLOGY", "content": "The Minimum Description Length (MDL) [1-2] principle is a foundational concept in\ninformation theory and statistical modeling. It is based on the idea that the best explanation for a\ngiven set of data is the one that allows for the shortest possible description of both the model\nand the data encoded by that model. In other words, MDL seeks to balance model complexity\nand goodness-of-fit by favoring models that are both simple and effective at capturing the\nunderlying structure of the data.\nAt its core, the MDL principle operationalizes Occam's razor [3] (figure 2): among competing\nmodels that explain the data equally well, the simplest model is preferred. This is achieved by\nminimizing the total description length, which comprises two main components:\nModel Complexity: The number of bits required to describe the model parameters.\nData Fit: The number of bits needed to encode the residual errors when the model is\nused to represent the data.\nMDL has wide-ranging applications in machine learning, data mining, and signal processing. It\nprovides a rigorous framework for model selection and feature selection, helping to avoid\noverfitting by penalizing unnecessarily complex models. By ensuring that models are both\ncompact and efficient, MDL contributes to improved generalization in predictive tasks."}, {"title": "2.1. Interpreting Embedding Spaces by Conceptualization in LLMs", "content": "Interpreting embedding spaces by conceptualization in Large Language Models (LLMs) [4-7]]\ninvolves mapping words, phrases, or concepts into high-dimensional vectors-known as\nembeddings-that encapsulate semantic meaning, reference, and underlying relationships. The\nmethodology follows these key steps:\n2.1.1. Embedding Spaces\nWords or concepts are represented as high-dimensional vectors. In LLMs such as GPT and\nBERT, these embeddings capture semantic similarities-words with similar meanings are\nplaced close together\u2014while related but distinct concepts also exhibit proximity, enabling a\nricher contextual interpretation.\n2.1.2. Conceptualization in Embeddings\nThis process organizes and groups embeddings to reflect abstract relationships among concepts.\nFor example, consider the concept of anemia and its related conditions (nutritional deficiencies,\nchronic diseases, cancer, etc.). The embeddings for these conditions cluster near that of anemia,\nindicating both their shared connections and unique attributes. In this framework, three\ndimensions are crucial:\nSign: The actual term or label (e.g., \"anemia\").\nMeaning: The conceptual interpretation or idea the term represents.\nReference: The specific context or domain (e.g., anemia in the context of\nnutritional deficiency versus cancer).\n2.1.3. Application in LLMs\nLLMs leverage these compact representations to generalize and specialize their understanding.\nThey can, for instance, infer that anemia linked to iron deficiency may share patterns with\nanemia due to vitamin B12 deficiency, while also distinguishing between anemia associated\nwith different conditions. Techniques such as semantic clustering and vector operations (like\naddition or subtraction) enable the model to explore and infer new relationships between\nconcepts.\n2.1.4. Incorporating the Minimum Description Length (MDL) Principle\nApplying MDL to embeddings seeks the simplest and most compact representation of a concept\nwithout losing essential information. This approach minimizes the total description length by\nbalancing the complexity of the model (the bits required to encode the model parameters) with\nthe accuracy of data representation (the bits needed to encode the residuals or errors). In effect,\nMDL aids in reducing noise and redundancy, thereby enhancing the model's generalization\ncapabilities."}, {"title": "2.2. Mathematical Representation of the Integration of the Minimum Description\nLength (MDL) Principle and Embeddings in a Large Language Model (LLM)", "content": "2.2.1. Mathematical Embedding\nThe definition of an embedding in a vector space. Suppose we have a set of words or terms {w1,"}, {"title": "2.2.2. Combination of Embeddings", "content": "To capture semantic or contextual relationships, we can combine the embeddings of multiple\nwords. For example, if we have the words w\u2081 and w2, the vector combination can be represented\nas the vector sum:"}, {"title": "2.2.3. Minimum Description Length (MDL) Principle", "content": "MDL is a principle that seeks the simplest possible representation of data while preserving as\nmuch relevant information as possible. In the context of LLM and embeddings, we can apply\nMDL to the process of compressing vector representations.\nLet X be the set of words or sequences that describe a concept (such as anemia). The goal of\nMDL is to find a compact description L(X) of X that minimizes the length of the description\nL(X) while preserving the necessary information.\nThe total length of a description can be represented by:"}, {"title": "2.2.4. Application of MDL in Embedding", "content": "In the embedding space, MDL can be applied to find a more compact vector representation\nwithout losing semantics. Suppose we have an embedding matrix (V), where each column is an\nembedding vector for a word (w1):"}, {"title": "2.2.5. Interpretation in the Context of LLMS", "content": "Within an LLM, this compressed representation (V) is used as input to the attention or recurrent\nlayers. The compression process via MDL helps the model to generalize better, reducing data\ncomplexity without significant loss of information.\n2.2.5.1. Example For Anemia\nIn the context of anemia, if we have a set of related conditions, such as:\n(w\u2081 = iron deficiency) (w\u2082 = chronic diseases) (w3 = cancer)\nWe can combine the embedding vectors of these conditions to form a compact vector that\nrepresents anemia as:"}, {"title": "2.2.6. Compaction Calculation", "content": "The final computation of compression in an embedding space can be described as:"}, {"title": "2.3. Evaluation Metrics", "content": "Vector space of embeddings combined with the Minimum Description Length Principle, as a\nmetric it was used (i) vector similarity with the calculation of cosine similarity; (ii)\ndimensionality reduction was performed with the degree of compression in the vector (before\nand after) to identify the compression efficiency; (iii) preservation of semantic information with"}, {"title": "3. DISCUSSION", "content": "3.1. Overview of the Proposed Approach and Experimental Setting\nOur proposed approach, illustrated in Fig. 1c, was crucial for improving the model, reducing\ndivergences and overfitting, and identifying potential clusters of signs and symptoms of benign\ngynecological diseases and ovarian cancer. To train the model, we used both real and synthetic\ndata, thus strengthening the model through the introduction of a preprocessing layer, which was\nkey to reducing inconsistencies and making the model steerable and configurable. The approach\nfollowed three main steps: (1) Preprocessing the data to reduce dimensionality and capture\nsemantic/logical patterns \u2013 text sequences or long terms \u2013 with labeled and unlabeled data. The\nlabeled data came from different hospitals and scientific articles (case studies), while the\nunlabeled data came from scientific articles that included more than one comorbidity, where the\ngynecological disease was not the primary cause, and another group of articles in which the\ndiagnosis was present but not labeled. In both cases, semantic text compression was used. In\nour experiment, we assumed that the best choice of attributes was made by the MDL; with\nstructured data, we simplified the representation space and mitigated undersampling. For the\nsupervised data, dimensionality reduction was performed by bipartitioning to reduce data\ncomplexity, and the MDL provided a solution that minimized redundancy and improved\nefficiency in feature selection and data compression, resulting in better predictions and reduced\nrisks of overfitting or underfitting. For the unsupervised model, the most significant data\nvariance was retained, followed by concatenation; (2) Using Fig. 1c as a reference,\ninterpretation of the embeddings by conceptualization. In this step, we compressed the\ninformation into smaller dimensions, maintaining the distinctions between different causes\n(genetic, nutritional, infectious, etc.). However, the MDL model provided dimensionality\nreduction for many attributes, removing redundancies or collinear relationships, reducing\nhallucinations in undersampling, and mitigating overfitting; (3) The compression performed by\nthe MDL and VAE reduced the complexity of the model, decreasing the likelihood of overfitting\nand improving its generalization capacity for new data. In the context of medical conditions\nsuch as anemia, compression revealed the most critical attributes for detecting and diagnosing\nthe condition, thereby improving model accuracy and efficiency. The combination of MDL and\nVAE was powerful because it allowed for efficient data compression, reducing dimensionality\nwhile preserving the most important information.\nAn important aspect involves hallucinations in subsampling models, as many gynecological\ndiseases, such as ovarian cancer, lack extensive descriptions, and data in many hospitals are\nscarce. We utilized the concept of dimensionality reduction in six steps, as follows:\n(i) Less noise: The removal of redundant or strongly correlated variables reduces \"noise\" in the\ndata, improving the model's ability to focus on essential patterns and preventing it from learning"}, {"title": "4. CONCLUSIONS", "content": "The data presented robustly demonstrate that VAE (MDL) outperforms Standard Autoencoder\nin the task of data reconstruction. The regularization of the latent space through KL divergence\nallows VAE to learn more structured and generalizable representations, resulting in lower\nreconstruction errors (MSE, MAE and, especially, RMSE). Furthermore, the consistency\nbetween training and testing losses, combined with the time efficiency in inference,\ndemonstrates that VAE is a practical and effective solution for applications that require high\nprecision and robustness.\nThe statistical tests reinforce that the observed differences, especially in MSE and RMSE, are\nstatistically significant, ensuring that VAE's superiority is not due to chance. Finally, the\nclassification metrics indicate that, in reconstruction, VAE preserves the essential information\nof the data, although the interpretation of the F1-Score requires further analysis."}]}