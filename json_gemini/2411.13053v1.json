{"title": "MEGL: Multimodal Explanation-Guided Learning", "authors": ["Yifei Zhang", "Tianxu Jiang", "Bo Pan", "Jingyu Wang", "Guangji Bai", "Liang Zhao"], "abstract": "Explaining the decision-making processes of Artificial Intelligence (AI) models is crucial for addressing their \u201cblack box\" nature, particularly in tasks like image classification. Traditional eXplainable AI (XAI) methods typically rely on unimodal explanations, either visual or textual, each with inherent limitations. Visual explanations highlight key regions but often lack rationale, while textual explanations provide context without spatial grounding. Further, both explanation types can be inconsistent or incomplete, limiting their reliability. To address these challenges, we propose a novel Multimodal Explanation-Guided Learning (MEGL) framework that leverages both visual and textual explanations to enhance model interpretability and improve classification performance. Our Saliency-Driven Textual Grounding (SDTG) approach integrates spatial information from visual explanations into textual rationales, providing spatially grounded and contextually rich explanations. Additionally, we introduce Textual Supervision on Visual Explanations to align visual explanations with textual rationales, even in cases where ground truth visual annotations are missing. A Visual Explanation Distribution Consistency loss further reinforces visual coherence by aligning the generated visual explanations with dataset-level patterns, enabling the model to effectively learn from incomplete multimodal supervision. We validate MEGL on two new datasets, Object-ME and Action-ME, for image classification with multimodal explanations. Experimental results demonstrate that MEGL outperforms previous approaches in prediction accuracy and explanation quality across both visual and textual domains. Our code will be made available upon the acceptance of the paper.", "sections": [{"title": "1. Introduction", "content": "Explaining the decision-making process of Artificial Intelligence (AI) models is essential to addressing their \"black box\" nature. This need has driven the development of eX-plainable AI (XAI) techniques [5, 9, 11, 52], which aim to make model reasoning more transparent and to provide interpretable explanations for Al decisions. However, in the domain of image classification, the current frontier of XAI is limited by two major bottlenecks. Firstly, most XAI methods interpret an image classification model's decisions through two types of unimodal explanations: visual explanations [21, 35, 42, 46], which highlight key areas of the input image, and textual explanations [2, 8, 21, 49], which provide a natural language rationale for the model's decision-making. However, each unimodal explanation has limitations. For example, as shown in Figure 1, in cancer di-agnosis, visual explanations highlight key regions of a nod-ule relevant to the diagnostic decision but may fail to clarify why the nodule is classified as malignant. Textual expla-nations, on the other hand, can provide rationales, such as identifying ground-glass texture in a lung nodule, but often lack the spatial context provided by the visual explanation. Similiarly, in a cat and dog classification task, visual expla-nations might highlight key regions like the face and ears of the cat, while textual explanations justify the classification by noting features like the pointed triangular ears.\nSecondly, explanations from XAI methods can be inac-curate. For example, visual explanations may highlight in-correct key regions [16, 56], and textual explanations may contain hallucinatory rationales [7, 26, 27]. To address this issue, Explanation-Guided Learning (EGL) [15, 45, 51] has emerged as a method to enhance model interpretabil-ity and predictive accuracy by refining and aligning the model's reasoning process with human-understandable ex-planations. In EGL methods that use visual explanations, existing approaches [17, 18, 55, 58] employ an explana-tion loss that compares human-annotated visual explana-tions with the model's saliency map, alongside the predic-tion loss, to guide the model in identifying relevant key regions for its decisions. In contrast, EGL methods that use textual explanations [30, 32, 36, 60] employ multitask learning to generate rationales alongside labels in Visual Question Answering tasks to support image classification. However, these textual explanations do not represent the classifier's true reasoning process but are generated inde-pendently by a language model, leaving the model's internal decision-making as a \"black box\" [2, 8, 21]. Additionally, existing EGL approaches are limited to single-modality ex-planations, which inherits the aforementioned limitation of using single-modality explanations.\nTo tackle these issues, this paper paves a new research area called Multimodal Explanation-Guided Learning (MEGL), which aims to interweave visual and textual ex-planations' complementary strengthens in visualizing and rationalizing the model decision-making process and im-prove model's classification performance. However, MEGL requires far more than simply combining visual and tex-tual EGL due to multimodal interdependency and combina-tion: (1) Interdependence Between Visual and Textual Ex-planations: Visual and textual explanations must remain consistent to accurately represent the decision-making pro-cess. Textual explanations should incorporate key regions identified by visual explanations, while visual explanations benefit from the semantic context provided by textual ra-tionales. Therefore, effectively coordinating these expla-nations in EGL remains challenging. (2) Incompleteness of Explanation Modalities: In real-world scenarios, acquir-ing multimodal explanation annotations is often challeng-ing, with some samples lacking one or more modalities due to the resource-intensive nature of generating such anno-tations. This uneven availability of multimodal explana-tions leads to heterogeneous supervision signals across the dataset, highlighting the need for a flexible training strategy capable of effectively handling incomplete modalities.\nTo address the interdependence between visual and tex-tual explanations, we propose a Saliency-Driven Textual Grounding (SDTG) approach, which facilitates the trans-fer of spatial information from visual explanations (saliency map) into textual rationales. SDTG generates textual ex-planations by combining saliency-driven visual cues with broader contextual information from the full image, ensur-ing that spatially relevant insights are effectively reflected in the text. Specifically, an input image is processed through a classifier and a post-hoc visual explainer to produce a saliency map that highlights critical regions relevant to the model's decision. This visual explanation, together with the overall image context, is incorporated into a large language model (LLM) through a carefully constructed prompt that integrates both saliency and image-level features. This pro-cess enables the LLM to generate textual explanations that are grounded in spatially relevant information while provid-ing coherent reasoning behind the model's decisions.\nTo address the challenge of incomplete explanation modalities, we propose two complementary strategies within our framework. First, our Saliency-Driven Textual Grounding (SDTG) approach incorporates Textual Supervi-sion on Visual Explanations, leveraging textual rationales to guide the refinement of visual explanations during train-ing. By explicitly transferring and harmonizing informa-tion between modalities, SDTG aligns visual explanations more closely with textual perspectives, ensuring that gener-ated explanations are both spatially grounded and mutually informative. Second, for samples without ground truth vi-sual annotations, we introduce a Visual Explanation Distri-bution Consistency loss. This loss aligns the distribution of generated visual explanations with the dataset-level ground-truth distribution, ensuring stable and contextually appro-priate visual explanations even in the absence of direct an-notations. Together, these methods enable our framework to effectively leverage partial supervision, ensuring meaning-ful and consistent multimodal explanations across diverse training conditions.\nWe present two novel datasets, Object-ME and Action-ME, adapted to image classification tasks with multimodal explanation annotations, derived from the VQA-X and ACT-X datasets. These datasets provide both visual and textual explanations, enabling comprehensive evaluation of multimodal explanation-guided learning methods. Ex-tensive experiments conducted on these datasets demon-strate the effectiveness of our proposed MEGL framework. MEGL achieves superior performance over previous image classification and EGL methods, excelling in classification accuracy, visual explainability, and textual explainability."}, {"title": "2. Related Work", "content": "Many methods have been developed to to leverage expla-nations to improve the performance of a deep learning model (\"Learning with explanations\"). CREX [12] regu-larizes deep neural network (DNN) training by enforcing the model to generate local explanations that align with expert-provided rationales, which are subsets of features highlighted as justifications for predictions. CDEP [45] penalize both a model's prediction and the corresponding explanation. [17, 55, 58] supervise saliency maps gener-ated by post-hoc explainers to improve image classifica-tion performance, while [18, 57] propose adding supervi-sion on the important tokens in input text for text classifi-cation tasks. Some recent work proposes fine-tuning multi-modal large language models (MLLMs) using explanations alongside the final answer to enhance their reasoning abil-ity [30, 32, 36, 60] for Visual Question Answering tasks for image classification. However, existing work focuses on a single modality of explanation. In this work, we aim to leverage multimodal explanations to enhance the perfor-mance of models' decision-making."}, {"title": "2.1. Explanation-Guided Learning", "content": "Many methods have been developed to to leverage expla-nations to improve the performance of a deep learning model (\"Learning with explanations\"). CREX [12] regu-larizes deep neural network (DNN) training by enforcing the model to generate local explanations that align with expert-provided rationales, which are subsets of features highlighted as justifications for predictions. CDEP [45] penalize both a model's prediction and the corresponding explanation. [17, 55, 58] supervise saliency maps gener-ated by post-hoc explainers to improve image classification performance, while [18, 57] propose adding supervi-sion on the important tokens in input text for text classifi-cation tasks. Some recent work proposes fine-tuning multi-modal large language models (MLLMs) using explanations alongside the final answer to enhance their reasoning abil-ity [30, 32, 36, 60] for Visual Question Answering tasks for image classification. However, existing work focuses on a single modality of explanation. In this work, we aim to leverage multimodal explanations to enhance the perfor-mance of models' decision-making."}, {"title": "2.2. Visual and Textual Explanation", "content": "To interpret image classification, visual explanation meth-ods highlight the discriminative regions in the input image, such as through heatmaps, based on gradient or attention maps, like Grad-CAM [47], Integrated Gradients [50], and Attention Branch Network (ABN) [13]. Meanwhile, tex-tual explanations provide a natural language rationale to justify the model's prediction by a vision-language model as explainer [2, 21, 38]. Additionally, image classifica-tion tasks can also be addressed by Visual Question An-swering models and generate the natural language ratio-nale [31, 41, 44, 54] with additional explainers. [40, 54] can also generate both visual and textual explanation together. With the development of Multimodal LLMs [1, 4, 29, 34], these models are now capable of generating a sequence that includes both an answer and a textual explanation for a given image [25, 37, 59]. In our work, we aim to im-prove classification performance by correcting both visual and textual explanations while enabling interaction between the two explanation modalities and addressing the challenge of collecting ground truth explanation annotations."}, {"title": "3. Methodology", "content": "In this section, we introduce our proposed MEGL frame-work, beginning with the problem formulation and an overview of framework architecture. We then detail our pro-posed approaches to facilitating interaction between visual and textual explanations in Section 3.3 and address the chal-lenge of incomplete multimodal explanations in Section 3.4."}, {"title": "3.1. Problem Formulation", "content": "In multimodal explanation-guided learning for image clas-sification, we aim to enhance a classifier's predictive per-formance and interpretability by leveraging both visual and textual explanations during training. We define a dataset D consisting of triples (I, y, {A, T}), where I is the input image, y is the class label, and {A, T} are the associated visual and textual explanations. Our objective is to train a classifier f that learns the mapping f: I \u2192 y by integrat-ing the explanations A and T during training. By incorpo-rating these multimodal explanations, the model improves its understanding of the reasoning behind image classifica-tions, leading to enhanced performance and interpretability."}, {"title": "3.2. Framework Overview", "content": "In this section, we present an overview of the proposed MEGL framework, illustrated in Figure 2. The MEGL framework integrates multimodal learning by incorporating visual and textual explanations as additional supervision to enhance the training of the image classifier.\nAs illustrated in Figure 2(a), an image classifier f (e.g., CNN [28], ViT [10]) processes the input image I to gen-erate a feature representation by its feature extractor [28]. This representation is passed through linear layers for clas-sification, producing a logit vector representing class scores. The classifier is trained using a prediction loss\n$\\L_{\\text{pred}}(\\phi_f) = CE(y, \\hat{y}),$\nwhere CE denotes the cross-entropy loss between the pre-dicted label $\\hat{y}$ and the ground-truth label y, and $\\phi_f$ repre-sents the classifier's parameters.\nAs shown in Figure 2 (b), a visual explanation method (e.g., Grad-CAM [47]) is applied to the classifier to generate a visual explanation $\\hat{A} = f(I)_{\\text{visual}}$, highlighting regions of the input I that are most relevant to the classification deci-sion. The features of the image, extracted by the classifier, and the visual explanation, encoded by a visual encoder E, are combined and fed into an LLM to generate a textual ex-planation. This textual explanation process is supervised by an autoregressive loss\n$\\L_{\\text{textual}}(\\Phi_f, \\Phi_E, \\Phi_{\\text{LLM}}) = ||T - \\hat{T}||_{\\text{AR}},$\nwhich encourages the generated explanation $\\hat{T}$ to align with the target rationale T. Details are provided in Section 3.3.\nAs shown in Figure 2 (c), with the generated visual ex-planation $\\hat{A}$ is supervised by the ground-truth visual expla-nation A as\n$\\L_{\\text{visual}}(\\phi_f) = ||\\hat{A} - A||_1,$\nwhich minimizes the $L_1$ distance between the generated vi-sual explanation and the ground-truth visual explanation. For samples lacking ground truth visual explanations, we introduce a visual explanation distribution consistency loss, defined as\n$\\L_{dc}(\\phi_f) = E [||\\hat{A} - \\overline{A}||],$\nwhere $\\overline{A}$ denotes the set of ground truth visual explanations across the dataset. This consistency loss ensures that the generated saliency maps align with the overall distribution of ground truth annotations. Depending on the availability of visual annotations, either $\\mathcal{L}_{visual}$ or $\\mathcal{L}_{dc}$ is applied. Further details are provided in Section 3.4.\nThe final objective function is designed to jointly opti-mize classification accuracy, visual explainability, and tex-tual explainability, and is formulated as follows:\n$\\mathcal{L} = \\mathcal{L}_{pred}(\\phi_f) \\\\\n+ \\lambda_{\\text{textual}}\\mathcal{L}_{\\text{textual}}(\\Phi_f, \\Phi_E, \\Phi_{\\text{LLM}}) \\\\\n+ \\lambda_{\\text{visual}} (I_v \\mathcal{L}_{\\text{visual}}(\\phi_f) + (1 - I_v) \\mathcal{L}_{dc}(\\phi_f)),$\nwhere $I_v$ is an indicator function set to 1 when visual anno-tations are available, and 0 otherwise. The hyperparameters $\\lambda_{\\text{visual}}$ and $\\lambda_{\\text{textual}}$ control the balance between the prediction loss, visual explanation loss, and textual explanation loss."}, {"title": "3.3. Facilitating Interaction in Multimodal Explanations", "content": "In this section, we present the Saliency-Driven Textual Grounding (SDTG) method, designed to facilitate interac-tion between visual and textual explanations. Unlike tra-ditional end-to-end image-to-text generation approaches [2, 8, 21], our SDTG method generates textual explanations by adding visual explanations as input, leveraging spatial in-formation to ground the semantic rationale in the identified key regions.\nTo be specific, firstly, as shown in Figure 2 (a) and (b), for a given input image I, we generate a visual explanation A with a visual explanation method, such as Grad-CAM, as $\\hat{A} = f(I)_{saliency}$. The image and visual explanation are then processed through two encoding paths to obtain the visual representation. In the first path, we obtain the vision rep-resentation of the original image I by the feature extractor of classifier f as $f(I)_{feature}$. In the second path, we apply the visual explanation to the image, yielding a modified image as $\\hat{I} = I \\odot \\hat{A}$, which is then encoded with the pre-trained CLIP vision encoder [43], denoted CLIP, to capture the vi-"}, {"title": "3.4. Handling Multimodal Explanation Incompleteness", "content": "In this section, we present our approach to addressing the in-completeness of multimodal explanations. Specifically, we tackle this challenge through two strategies: Textual Super-vision on Visual Explanations and Visual Explanation Dis-tribution Consistency."}, {"title": "3.4.1 Textual Supervision on Visual Explanations", "content": "Firstly, in our SDTG method, Textual Supervision on Visual Explanations is achieved by guiding the textual explanation generation process with the visual explanation produced by the classifier's visual explainer. This interaction encour-ages the visual explainer to iteratively refine its output, $\\hat{A}$, based on feedback from the textual rationale, thereby fos-tering alignment between visual and textual explanations. Even in the absence of ground truth visual annotations, this approach enables effective supervision through the textual modality, allowing the model to leverage textual guidance during training.\nThe textual explanation generation process leverages both the input image representation $V_I$ and the visual ex-planation representation $V_A$. Minimizing the textual ex-planation loss $L_{\\text{Textual}}$ encourages consistency between the generated saliency map A and the textual rationale, promot-ing alignment as shown in:\n$\\hat{A} = \\underset{\\hat{A}}{\\text{arg min}} L_{\\text{Textual}} (V_I, V_A)$ Through this alignment, $L_{\\text{Textual}}$ provides semantic ground-ing, ensuring that $\\hat{A}$ reflects the rationale expressed in the textual explanation, enabling the visual explanation to be iteratively refined according to the textual perspective."}, {"title": "3.4.2 Visual Explanation Distribution Consistency", "content": "We propose a Visual Explanation Distribution Consistency method to generate meaningful visual explanations even for samples without ground truth annotations, thus enabling more robust training across the entire dataset. Specifically, we introduce a consistency loss that aligns the distribution of generated visual explanations with the aggregated distri-bution of available ground truth visual explanations. This method leverages the relationship between annotated and unannotated samples, providing supervision at the distribu-tional level in the absence of direct paired supervision.\nFor samples without ground truth visual explanation an-notations, we generate a saliency map, $\\hat{A}$. For annotated samples, we construct an aggregated target distribution A by averaging their normalized ground truth saliency maps, capturing the typical pattern of visual explanations across annotated data:\n$\\overline{A}(i, j) = \\frac{1}{n} \\sum_{k=1}^{n} A_k(i, j),$\nwhere n represents the number of annotated samples. This aggregated distribution $\\overline{A}$ serves as a reference pattern for unannotated samples, reflecting the average distribution of visual explanations within the dataset.\nTo enforce consistency, we define the consistency loss $L_{ds}$ as the Kullback-Leibler divergence between the gener-ated saliency map $\\hat{A}$ and the aggregated distribution $\\overline{A}$:\n$\\mathcal{L}_{dc}(\\hat{A}, \\overline{A}) = D_{KL}(\\hat{A} || \\overline{A}),$\nwhich encourages the model to align generated visual explanations with the distribution patterns of the anno-tated ground truth, promoting consistent visual explanations across the dataset."}, {"title": "4. Experiment", "content": "We experiment with our constructed two datasets for image classification with both visual and textual explanation an-notations, derived from two VQA with visual and tecxtual explanations datasets: Visual Question Answering Explana-tion (VQA-X) and Activity Explanation (ACT-X) [40]. We extract samples that can be converted into a classification task from the VQA-X dataset and construct the Object-ME dataset for object classification. Similarly, the Action-ME dataset is derived from the ACT-X for action classification.\nFor the two constructed datasets, each image sample in-cludes a class label and a corresponding textual explanation to justify the class label. Additionally, a subset of the sam-ples contains visual explanations to further support the justi-fication of the class label."}, {"title": "4.1. Dataset", "content": "We experiment with our constructed two datasets for image classification with both visual and textual explanation an-notations, derived from two VQA with visual and tecxtual explanations datasets: Visual Question Answering Explana-tion (VQA-X) and Activity Explanation (ACT-X) [40]. We extract samples that can be converted into a classification task from the VQA-X dataset and construct the Object-ME dataset for object classification. Similarly, the Action-ME dataset is derived from the ACT-X for action classification.\nFor the two constructed datasets, each image sample in-cludes a class label and a corresponding textual explanation to justify the class label. Additionally, a subset of the sam-ples contains visual explanations to further support the justi-fication of the class label."}, {"title": "4.2. Evaluation Metrics", "content": "To comprehensively evaluate the effectiveness of our pro-posed MEGL framework, we conduct assessments across three key dimensions: classification performance, visual ex-plainability, and textual explainability. For classification performance, we employ standard metrics including Accu-racy, Precision, Recall, and F1-score. For visual explain-ability, we use the mean Intersection-over-Union (mIoU) to quantify the overlap between the generated visual explana-tions and ground truth visual annotations, divided by the total area covered by the union of the two. For textual ex-plainability, we assess both the quality and the faithfulness of the generated textual explanation. Specifically, the qual-ity of generated textual explanations is measured using es-tablished metrics: BLEU-4 [39], METEOR [6], ROUGE-L [33], CIDEr [53], and SPICE [3]. To evaluate the faithful-ness [19, 62] of the generated textual explanation, we utilize the CLIPScore [22] to measure text-image alignment. In addition, we evaluate the efficiency of the frameworks by analyzing their number of parameters, latency, and frames per second (FPS)."}, {"title": "4.3. Comparison Methods", "content": "To validate the effectiveness of our proposed framework, we conduct comprehensive evalauations of our MEGL frame-work against various baseline models and state-of-the-art approaches across the three key dimensions mentioned above: classification performance, visual explainability, and textual explainability.\nWe take into account 3 categories of models: tradi-tional vision models, multimodal large language models (MLLMs), and state-of-the-art EGL frameworks. The vision baselines include Convolutional Neural Networks (CNNs) and Vision Transformer models (ViTs), represented by ResNet18 [20] and ViT-B/16 [10] respectively. Addi-tionally, we evaluate against state-of-the-art MLLMs such as LLaVA [34], which are fine-tuned to perform image clas-sification as a visual question-answering task. Moreover, for the state-of-the-art EGL frameworks, we evaluate both visual EGL and textual EGL architectures. We evaluate established visual EGL frameworks including CDEP [45], HAICS [48], RES-G, and RES-L [14] which focus on opti-mizing spatial regions used by the model to enhance model interpretability. For textual-based EGL, we evaluate against Fine-tune-CoT [23], which adopts a VQA-style approach similar to LLaVA and fine-tunes large language models to simultaneously generate predictions and textual explana-tions from visual inputs.\nFor classification performance, we benchmark against all three categories. For visual explainability, we evaluate our proposed MEGL framework against the traditional vision models and the state-of-the-art visual EGL frameworks. For textual explainability, we evaluate against the state-of-the-art textual EGL frameworks."}, {"title": "4.4. Implementation Details", "content": "For MEGL, we utilize ResNet18 and ViT-B/16 as backbone classifiers. The pre-trained vision encoder is CLIP-ViT-L-14, while the LLM is Vicuna v1.5 [61]. Fine-tuning is per-formed using the LLaVA-1.5-7B checkpoint. Fine-tuning of the MLLMs is based on the LLaVA framework, specifi-"}, {"title": "4.5. Main Results", "content": "Comparisons of Classification Performance are pre-sented in Table 2. Our MEGL methods outperform the corresponding baseline models in image classification per-formance on both Object-ME and Action-ME datasets, and exhibit substantial improvement compared with existing vi-sual EGL methods. Notably, the MEGL with ViT-B/16 backbone classifier (MEGL-ViT-B/16) achieved the best classification performance among all models in the evalu-ation on both datasets.\nWe found that visual EGL methods achieved marked improvements across metrics beyond accuracy. However, this may come at the cost of a slight reduction in ac-curacy scores, which might be caused by the additional saliency-guided learning procedure. The phenomenon is more significant in models with ResNet18 backbone, which could possibly be explained by relatively simpler architec-ture compared with ViT-B/16.\nWe also found that MLLMs and MLLM-based textual EGL frameworks exhibit excellent classification perfor-mance in terms of Accuracy. However, they demonstrate relatively poor performance across Precision, Recall, and F1-score metrics, particularly on Action-ME dataset. This is possibly due to the fact that decision-making of such MLLMs could be unstable in edge cases in image classi-fication.\nComparisons of Visual Explainability are also presented in Table 2. Our methods and EGL frameworks demonstrate substantial improvements in the quality of generated visual explanations, with particularly pronounced enhancements observed in the performance of ResNet18-based models. Notably, the mIoU metrics achieved on the Action-ME"}, {"title": "4.6. Ablation study", "content": "Ablation study is conducted on our proposed MEGL frame-work to validate its effectiveness. Components responsi-ble for visual explanations and textual explanations are re-moved respectively.\nThe results of ablation study is shown in Table 4. It is clear that for MEGL models with backbone of ResNet18 and ViT-B/16, all the components created positive impact on the performance. In addition, the positive effect of tex-"}, {"title": "4.7. Efficiency Analysis", "content": "Although MEGL models achieved significant improvement upon the corresponding baseline models, their enhancement upon LLaVA based models, especially LLaVA-Fine-tune-CoT, seems marginal. However, it should be noticed that MEGL models are much smaller and of higher efficiency when deployed for image classification tasks.\nTo demonstrate the advantages in efficiency our pro-posed MEGL framework, we conduct comprehensive anal-yses on model size and computational costs of MEGL mod-els and LLaVA-Fine-tune-CoT on image classification tasks and present the results in Table 5. We can see that the FPS of MEGL models are significantly higher than LLaVA-Fine-tune-CoT, suggesting higher inference speed and higher ef-ficiency. The discrepancy translates to a 30.1\u00d7 speedup for MEGL-ViT-16/B in FPS with improved classification per-formance, illustrating the effectiveness and efficiency of our MEGL framework."}, {"title": "5. Conclusion", "content": "In this paper, we introduced the Multimodal Explanation-Guided Learning (MEGL) framework, designed to inte-grate multimodal explanations and enhance classification performance. MEGL incorporates Saliency-Driven Textual Grounding (SDTG), which facilitates interaction between multimodal explanations, ensuring alignment and mutual consistency while also enabling Textual Supervision on Vi-sual Explanations, where textual rationales refine visual ex-planations during training. Additionally, the Visual Expla-nation Distribution Consistency loss tackles the challenge of incomplete visual annotations by generating robust vi-sual explanations even for unannotated samples. Extensive experiments on two newly proposed datasets, Object-ME and Action-ME, demonstrate that MEGL outperforms exist-ing methods in classification accuracy, visual explainability, as well as textual explainability. By effectively leveraging multimodal explanations, MEGL advances both the inter-pretability and predictive performance of AI systems."}]}