{"title": "Logic Contrastive Reasoning with Lightweight Large Language Model\nfor Math Word Problems", "authors": ["Ding Kai", "Ma Zhenguo", "Yan Xiaoran"], "abstract": "This study focuses on improving the performance of lightweight Large Language Models (LLMs) in mathematical\nreasoning tasks. We introduce a novel method for measuring mathematical logic similarity and design an automatic\nscreening mechanism to construct a set of reference problems that integrate both semantic and logical similarity. By\nemploying carefully crafted positive and negative example prompts, we guide the model towards adopting sound\nreasoning logic. To the best of our knowledge, this is the first attempt to utilize retrieval-enhanced generation for\nmathematical problem-solving. Experimental results demonstrate that our method achieves a 15.8% improvement over\nthe Chain of Thought approach on the SVAMP dataset and a 21.5% improvement on the GSM8K dataset. Further\napplication of this method to a large-scale model with 175 billion parameters yields performance comparable to the best\nresults on both aforementioned datasets. Finally, we conduct an analysis of errors during the reasoning process, providing\nvaluable insights and directions for future research on reasoning tasks using large language models. Code and dataset are\navailable at: https://github.com/derby-ding/llm-math-reasoning/tree/main", "sections": [{"title": "1. Introduction", "content": "Recent years have witnessed remarkable advancements in\nLarge Language Models (LLMs) [1], achieving state-of-\nthe-art (SOTA) performance across diverse tasks and\ndatasets. Despite their widespread deployment in real-\nworld applications, LLMs continue to grapple with\ncritical challenges, most notably the \"hallucination\"\nproblem. This issue is particularly pronounced in tasks\ndemanding precise logical reasoning, such as\nmathematical problem-solving and commonsense\ninference, significantly impacting the reliability and\npractical utility of these models.\nThe prevalence of hallucinations has ignited a vigorous\ndebate within the research community regarding the true\nextent of LLMs' logical reasoning capabilities. While\nsome researchers optimistically posit that LLMs possess\nlogical abilities potentially surpassing human\nperformance in certain domains [2], others maintain a\nmore skeptical stance. These critics argue that LLMs\nfundamentally lack genuine logical reasoning capabilities,\nsuggesting that their inference processes are merely\nprobabilistic approximations, easily disrupted by subtle\nperturbations [3], [4].\nOur research aligns more closely with the latter\nperspective. However, we hypothesize that it is possible\nto enhance the logical stability of LLMs during reasoning\nprocesses through targeted methodologies. To investigate\nthis hypothesis, we focus on lightweight large language\nmodels with fewer than 10 billion parameters. These\nmodels, with their relatively limited logical reasoning\ncapabilities, provide an ideal testbed for demonstrating\nthe efficacy of our proposed improvements.\nWe select mathematical problem-solving as our primary\nevaluation benchmark, as it presents a rigorous test of a\nmodel's reasoning capabilities. Such tasks typically\nrequire the decomposition of problems into multiple\ninterdependent steps, each involving precise computation.\nCorrect solutions are only achieved when all reasoning\npaths are accurately executed. Consequently,\nmathematical problem-solving presents two fundamental\nchallenges: managing the dependencies between\nreasoning steps and accurately interpreting conditional\ntext."}, {"title": "2. Related Works", "content": "Research on mathematical reasoning using large language\nmodels primarily focuses on two main directions:\nimprovements based on prompting techniques and those\nbased on fine-tuning. This article concentrates on prompt-\nbased improvements, which can be further categorized\ninto two approaches: Chain-of-Thought (CoT) methods\nand self-verification methods.\n2.1 Cot Prompt for Reasoning\nChain-of-thought (CoT) prompting, introduced by Wei et\nal. [6], demonstrated that specific prompts like \"Let's\nthink step-by-step\" can enable language models to\nperform chain-of-thought reasoning in a zero-shot manner.\nThis breakthrough has inspired numerous works that\nbuild upon step-by-step reasoning approaches. For\ninstance, automatic chain-of-thought [12] was proposed\nto address the challenges associated with manually\nannotating chain-of-thought demonstrations. Additionally,\nresearchers explored methods to decompose complex\nproblems into multiple sub-problems [23], or even into\ncode programs that can be automatically executed [13].\nContrastive chain-of-thought (CCoT) [11] enhances\nlanguage model reasoning by proposing a contrastive\napproach to CoT. Active-Prompt [9] adapted language\nmodels to different tasks using task-specific example\nprompts with manually designed CoT reasoning\nannotations. To address the crucial question of\ndetermining which problems are most important and\nhelpful for annotation, the authors proposed a solution\nthat draws inspiration from uncertainty-based active\nlearning. They introduced several metrics to characterize\nuncertainty, thereby selecting the most uncertain\nquestions. Faithful CoT [10] approached the problem by\nseparating it into two stages: Translation (Natural\nLanguage query \u2192 symbolic reasoning chain) and\nProblem Solving (reasoning chain \u2192 answer), utilizing a\nlanguage model and a deterministic solver, respectively.\n2.2 Self-verification for LLM\nMultiple decoding strategies have been proposed in the\nliterature to improve the output quality of language\nmodels. These include temperature sampling, top-k\nsampling, and minimum Bayes risk decoding [14].\nRe-ranking is another common approach to enhance\ngeneration quality. Cobbe et al. (2021) [15] demonstrated\nthat training a \"verifier\" to re-rank generated solutions\nsubstantially improves the solve rate on mathematical\ntasks, compared to merely fine-tuning the language model.\nSelf-correction [18] techniques leverage the law of large\nnumbers by generating multiple answers under identical\nconditions and selecting the most probable response as\nthe final answer. This method effectively improves\nanswer consistency while simultaneously increasing\naccuracy, demonstrating that large language models tend\nto favor correct answers in most cases. Additionally,\nsome approaches incorporate mechanisms of self-critique\nand self-reflection into large language models, enabling\nthem to refine their outputs. For example, Shinn, Labash\net al. (2023) introduced Reflexion [16], a technique that\nemploys external feedback to detect ineffective actions\nand engage in self-reflection.In self-verification [17],\nresearchers reverse the problem and answer, repeatedly\nconfirming the correctness of the reasoning. This method\nhas also been shown to improve model accuracy."}, {"title": "3. Logic Contrastive Reasoning (Method)", "content": "Using large language models (LLMs) to solve\nmathematical application problems presents two main\nchallenges: addressing logical reasoning errors and\nresolving semantic ambiguities. To address these issues,\nwe draw inspiration from the effective Chain of Thought\n(CoT) and contrastive methods, and propose a novel\napproach called Logic Contrastive Reasoning. This\nmethod uses a few sample problems as reference"}, {"title": "3.1 Logic Similarity", "content": "We use prompts and large language models to logically\nstructure mathematical problems, breaking them down\ninto known conditions and questions to be solved.\nThrough CoT reasoning, we list intermediate questions\nand solve them step by step to reach the final answer.\nEach reasoning step can be transformed into an algebraic\nexpression, allowing us to approximate the similarity of\nreasoning steps through expression similarity.\nTo represent the complete reasoning process, we combine\nthe individual reasoning steps sequentially, merging the\nformulas in the solving process into a total solving\nformula. This transforms the similarity of mathematical\nproblems into a problem of calculating the similarity\nbetween two solving formulas. To represent algebraic\nsimilarity, we first align variables to eliminate\ninconsistencies in their order of appearance and position\nwithin the formula. We employ a straightforward method\nof replacing variables in the formula with placeholders.\nFor example, A*(B+C+D)*B is converted to\n@*(@+@+@)*@.\nWe then calculate the similarity between algebraic\nexpressions using the Normalized Tree Edit Distance as a\nmetric. This metric measures the proportion of characters\nthat need to be modified for two strings to become\nidentical, expressed as:\nN(Al1,Al2)=\\frac{Lev(Al\u0131,Al2)+Lev(Al2,Al1)}{len(Al1)+len(Al2)} (1)\nwhere N represents the normalized logical similarity\nfunction, Al denotes the algebraic expression after\nvariable alignment, Lev() is the Levenshtein edit distance,\nand len is the length of the expression string.\nBuilding upon this, the Normalized Tree Edit Distance\nconstructs a tree representation of the formula. However,\nthis approach is highly sensitive to the tree structure. If\ntwo mathematical expressions exhibit slight structural\ndifferences, even when their mathematical meanings are\nsimilar, the calculated edit distance may still be\nsubstantial. Furthermore, due to the presence of properties\nsuch as commutativity and associativity, there are\ninherent variations in the ways mathematical expressions\ncan be written, which makes it challenging for the NTED\nto accurately represent the similarity between expressions.\nTo address this issue, we propose a new metric for\nmeasuring expression similarity. First, we divide the\nexpression into two parts by selecting a specific operator\noutside the parentheses as a splitting point, aiming to\ncreate two branches of approximately equal length. This\napproach allows for a more precise representation of the\nformula while minimizing excessive branching that could\nincrease structural sensitivity. In this manner, the\nswapping of branches will not significantly affect the\nsimilarity measure. For instance, in Expression 2, when\nthe splitting operator is multiplication or addition, the two\nbranches can be interchanged. Therefore, we take the\nminimum value of the similarity measures obtained\nbefore and after the swap. Conversely, when the splitting\noperator is division or subtraction, the overall similarity is\ncomputed as the sum of the similarities of the two\nbranches in their respective order. The method then\ncalculates the Normalized Edit Distance for each branch\nand merges them by selecting the minimum value based\non branch labels. The expression for the Normalized Tree\nEdit Distance TD is formulated as follows:\nTD(Al1i, Al2j) = \\begin{cases}\n    \\frac{2}{\\sum_{i=1}N(Alli, Al2i)}, if T(Al1) \\notin [+,*] \\\\\n    min(\\sum_{i=1}^{2}N(Al1i,Al2i), \\sum_{i=1}^{2}N(Al1i, Al2(3-i))), else\n\\end{cases} (2)\nwhere T) represents the branch operator which is in [+,- \nx,\u00f7], and min indicates the minimum value."}, {"title": "3.2 Contrastive Reasoning", "content": "The simplest form of mathematical problem reasoning\nbased on large language models can be represented as\nE(Q, A), consisting only of Question and Answer\ncomponents. However, lightweight large models struggle\nsignificantly with extracting known conditions from the\nQuestion and planning the solution process. Adding prior\nknowledge to the prompt, including Chain of Thought\n(CoT) and examples, can improve mathematical\nreasoning accuracy. This enhanced reasoning can be\nrepresented as E(P, Q, A), P is the prompt.\nContrastive Chain of Thought (CCoT) introduces positive\nand negative examples. These examples not only promote\ncorrect derivation by the large model but also help to\navoid common errors. The reasoning formula for CCoT is\nE(P, Qs, D+, D-, Q, A), where Os represents example\nquestions, D represents the reasoning process, and\npositive and negative signs indicate correct and incorrect\nreasoning.\nIn our approach, we introduce an algebraic solving form,\nrepresented by Al for the algebraic reasoning process.\nThus, our contrastive reasoning formula becomes E[P, Qs,\nD+, Al+, D-, Al-, Q, AJ.\nUnlike CCoT's method that synthesizes negative samples,\nwe use math prompts with different models for screening\nto collect positive and negative examples, which is a\nRAG method. This approach generates negative\nexamples that are more realistic. The process involves:\n(1) Logical structuring of the mathematical problem using\nprompts and large models, breaking it down into known\nconditions and questions to be solved (Step 1 in\nAlgorithm).\n(2) Identifying intermediate questions in the logical\nreasoning based on known conditions and the problem to\nbe solved. These are represented in text form, followed by\nmathematical reasoning to obtain answers (Step 2 in\nTable 1).\n(3) Expressing the reasoning process in algebraic form,\nyielding algebraic expressions and solutions (Step 3 in\nTable 1).\nCombining text-form reasoning and algebraic reasoning\nto deduce the final answer.\nThen we solve each problem multiple times and compare\nthe solutions with the true values to identify which\nproblems are answered correctly or incorrectly. Samples\nwith both correct and incorrect answers for the same\nproblem are included in the example sample set. Based on\nthis sample set, we use the logic similarity function to\nretrieve several examples relevant to the problem being\nsolved, as shown in Step 5 of Table 1. Finally, we\nconstruct a contrastive reasoning prompt, as illustrated in\nPrompt 4 of Table 1, to solve the mathematical\nproblem. The TLS() function in the algorithm is an\nintegrated function that combines both semantic and\nlogical aspects. In the context of retrieving similar\nmathematical problems, both semantic information and\nlogical information play important roles. Therefore, based\non the logical similarity of the formulas, we incorporate\nsemantic similarity by using hyperparameters to combine\nthese two similarity measures, thereby representing the\noverall similarity between two mathematical problems.\nAs shown in Equation 3, TD() denotes the similarity\nbetween mathematical expressions Al\u2081 and Al2 as in\nEquation 2. The Sem() function utilizes the semantic\nsimilarity model SentBERT[19] for its computation.\nSentBERT is specifically designed to assess sentence-\nlevel semantic similarity. In our approach, we first encode\nthe mathematical problems, Q1 and Q2, into high-\ndimensional vector representations using SentBERT. This\nmodel captures contextual information and semantic\nrelationships within the expressions by leveraging a\ntransformer-based architecture. Once the encoding is\ncomplete, we calculate the cosine similarity between the\nresulting vectors. This cosine similarity score reflects\nhow closely related the two mathematical problems are in\nterms of their underlying semantic content. Finally, we\nset the hyper-parameter empirically at a value of 0.7.\nTLS(ex 1,ex2) = aTD(Al 1,Al 2) + \u0101Sem(Q1,Q2) (3)\nex1 = (Q1,Al1)\nwhere a =1-a"}, {"title": "4. Experiments", "content": "4.1 Experimental Setup\nTo validate the effectiveness of our proposed logic\nsimilarity-based reasoning method for lightweight large\nlanguage models, we conduct evaluations on commonly\nused mathematical question-answering datasets. We\nutilize GSM8K [15] and SVAMP [22] datasets, both of\nwhich include problem descriptions and answers.\nSVAMP contains relatively simple problems with direct\nanswers, while GSM8K presents more complex problems\nwith complete reasoning processes in the answers.\nOur experiments employ the following lightweight\nmodels: Mistral-7B [20], LLaMA2-7B [21]. These\nmodels are loaded using 4-bit quantization (INT4) to\noptimize memory usage. Inference parameters are set\nwith a generation length of 400, top_p of 0.95,\ntemperature of 0.1, top_k of 30, and a repetition penalty\nof 1.15. To demonstrate the method's efficacy across\nmodel scales, we also validate our approach on a 175B\nparameter model, specifically using the ChatGPT-3.5\nTurbo 0301 4K version.\nWe use accuracy as our primary evaluation metric. To\nensure fairness in our experiments, we utilize 100 test\nsamples from both SVAMP and GSM8K datasets,\nconducting multiple test runs for robust results.\n4.2 Main Results\nWe design multiple experiments to demonstrate the\neffectiveness of our proposed method. The process\ninvolved two main steps:\n(1) Sample Selection: As shown in Table 1, we first\nscreen for positive and negative examples. To reduce the\ncomputational cost of using the entire training set as\nreference samples, we apply logical similarity filtering to\nthe training set. This process eliminate samples with high\nsimilarity.\n(2) Mathematical Problem Solving: Using the select\npositive and negative examples, we conduct mathematical\nproblem-solving experiments, comparing various\nparameter settings and algorithms.\n4.2.1 Parameter Optimization\nUsing the Mistral-7B model, we evaluate the impact of\ntwo key parameters: the number of few-shot examples\nand the number of guess attempts. We test few-shot\nexamples ranging from 1 to 9 (odd numbers only) and\nguess attempts of 5 and 10."}, {"title": "4.2.2 Contrastive Learning Experiments", "content": "To validate the effectiveness of our proposed logical\nsimilarity retrieval, we conduct experiments comparing\nvarious contrastive learning strategies. We employ two\nmethods for selecting example samples: (1) Random\nSelection: A fixed number of example samples are\nrandomly selected, potentially unrelated to the inference\nsample, and use consistently for each inference. (2)\nSimilarity-based Selection: Example samples are\nretrieved based on their similarity (semantic or logical) to\nthe current mathematical problem, varying for each\ninference.\nRandom selection methods include Fix, Hard, and\nContrastive CoT (see Appendix), while others use\nsimilarity-based selection. According to Table 3, on\nSVAMP, semantic similarity selection has a negative\nimpact, likely due to samples differing only in numerical\nvalues or solution objects, leading to misleading guidance.\nOn GSM8K, semantic similarity perform comparably to\nrandom selection. Fixed example samples are effective,\nespecially Contrastive SC, which include positive and\nnegative examples and error analysis, showing the most\nsignificant improvement in accuracy. These findings\ndemonstrate the superiority of our logical similarity-based\napproach over semantic similarity or random selection\nmethods in enhancing mathematical problem-solving\ncapabilities."}, {"title": "4.2.3 Error analysis", "content": "We conducted an analysis and statistical study of\ncommon errors in two datasets to understand how\nlightweight large language models might make mistakes\nin mathematical reasoning. We defined four types of\nerrors: comprehension errors, calculation errors, logical\nerrors, and formula errors. Comprehension errors occur\nwhen the model misunderstands the object to be solved.\nFor example: Question: Zachary did 53 push-ups and 14\ncrunches, whereas David did 17 more push-ups but 10\nfewer crunches than Zachary. How many push-ups and\ncrunches did David do? A comprehension error would\noccur if the model misinterpreted the question and solved\nfor Zachary's exercises instead of David's. Calculation\nerrors involve incorrect arithmetic results. For instance,\nincorrectly calculating 244 * 146 = 35,232 (the correct\nanswer is 35,624). Formula errors occur when there's an\ninconsistency between the text and the formula, or when\nformula derivation is inconsistent. For example, \"Among\nthe 200 Grade 5 students, 2/5 are boys. So, there are 200\n* (1 - 2/5) = 200 * (3/5) = 120 boys\". Logical errors\nhappen when the reasoning is confused, leading to a\nsolution that doesn't match the original problem. For\ninstance, \"To find out how many more books than action\nfigures are on Jerry's shelf, we simply subtract the\nnumber of action figures from the total number of items:\n12 (total items) - 5 (action figures) = 7.\"\nFrom a statistical perspective, comprehension errors and\nlogical errors were the most prevalent, while calculation\nerrors and formula errors occurred less frequently. In\nTable 4, the error distributions in two datasets are shown,\nthe proportion of the major errors are 64.3% and 73.8%.\nGiven these statistics, the most critical area for\nimprovement is enhancing the model's logical reasoning"}, {"title": "4.2.4 Generalization experiments", "content": "The proposed method is not only applicable to\nlightweight models but can also be extended to large-\nscale language models. To demonstrate this, we\nconducted tests using ChatGPT-3.5 (turbo0314 version)\non the GSM8K and SVAMP datasets. The parameters\nwere set to 7 reference samples and 5 guess attempts. The\nresults presented in Table 5 show that: On the GSM8K\ndataset, LCR + ChatGPT (without SC) outperforms the\nbaseline CoT + ChatGPT method by 12.0 %. LCR +\nChatGPT with the SC strategy shows a 12.5 %\nimprovement over the SC + PaLM combination on the\nGSM8K dataset. However, on the SVAMP dataset, our\nmethod slightly underperforms compared to SC + PaLM.\nThe relatively lower performance on SVAMP can be\nattributed to the high similarity between some questions\nin the reference set and the test set. In these cases, the\nreference samples may mislead the large language model\nin understanding and reasoning about the problem,\nleading to incorrect solutions."}, {"title": "5. Conclusion", "content": "To address the challenges of hallucinations and logical\nerrors in mathematical reasoning tasks for lightweight\nlarge language models, we proposed a novel approach\nleveraging contrastive learning. Our approach enabled the\nautomatic selection of a set of reference problems that\nshared logical similarities with the target problem. Using\nthese positive and negative examples, we constructed\ntailored prompts that guide the language model to adopt\nreasoning strategies similar to the positive examples\nwhile avoiding errors common to the negative ones.\nExperiments conducted on multiple public mathematical\nproblem datasets demonstrated significant improvements\nover existing state-of-the-art methods. Furthermore, we\nsuccessfully extended this method to a large language\nmodel with 175 billion parameters, achieving results\ncomparable to optimal human performance on both\ndatasets. Finally, we provided a comprehensive analysis\nof common issues encountered during the reasoning\nprocess of large language models. This analysis offers\nvaluable insights for future enhancements in the\nreasoning capabilities of these models."}]}