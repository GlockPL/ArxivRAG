{"title": "Addressing Quality Challenges in Deep Learning:\nThe Role of MLOps and Domain Knowledge", "authors": ["Santiago del Rey", "Adri\u00e0 Medina", "Xavier Franch", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "abstract": "Deep learning (DL) systems present unique chal-\nlenges in software engineering, especially concerning quality\nattributes like correctness and resource efficiency. While DL\nmodels achieve exceptional performance in specific tasks, en-\ngineering DL-based systems is still essential. The effort, cost,\nand potential diminishing returns of continual improvements\nmust be carefully evaluated, as software engineers often face\nthe critical decision of when to stop refining a system relative\nto its quality attributes. This experience paper explores the\nrole of MLOps practices such as monitoring and experiment\ntracking-in creating transparent and reproducible experimen-\ntation environments that enable teams to assess and justify the\nimpact of design decisions on quality attributes. Furthermore,\nwe report on experiences addressing the quality challenges by\nembedding domain knowledge into the design of a DL model\nand its integration within a larger system. The findings offer\nactionable insights into not only the benefits of domain knowledge\nand MLOps but also the strategic consideration of when to limit\nfurther optimizations in DL projects to maximize overall system\nquality and reliability.", "sections": [{"title": "I. INTRODUCTION", "content": "Deploying deep learning (DL) systems poses significant\nchallenges, particularly regarding correctness, time efficiency,\nand resource utilization. Achieving these quality attributes is\ncrucial for domains like Edge AI, where tasks are executed\ndirectly on devices to enable real-time decision-making and\nminimize dependence on centralized processing. Unlike tra-\nditional cloud environments, edge devices operate under con-\nstrained resources, requiring optimized models and efficient\ndeployment strategies to meet performance standards without\noverburdening the system.\nMLOps (Machine Learning Operations) seeks to mimic\nDevOps practices and adapt to the specific needs of ML [1]. By\nsupporting tasks like experiment tracking, model versioning,\nand automatic performance monitoring and reporting, MLOps\nprovides transparency, reproducibility, and streamlined adap-\ntation during the DL life cycle. This is even more relevant\nwith the increasing interest in the fast deployment of DL-\nbased systems where we must adhere to multiple quality\nrequirements while constant changes are made to models, data,\nand source code."}, {"title": "II. RELATED WORK", "content": "Below, we summarize related work on MLOps and domain\nknowledge in DL-based systems.\nMLOps. According to John et al.'s MLOps maturity model,\nthere are five stages to reach a successful MLOps implementa-\ntion [5]. In their model, experiment tracking and model mon-\nitoring are essential tasks to reach the third level of maturity\n(i.e., manual MLOps). Indeed, a survey of data science and\nmachine learning practitioners conducted by M\u00e4kinen et al. [6]\nshows that tracking and comparing experiments is one of the\nchallenges when implementing MLOps practices. Recupito\net al. [7] conduct a multi-vocal literature review where they\nanalyze the most popular MLOps tools. In total, they identify\n13 MLOps tools that can help overcome these challenges to\nmove towards more mature MLOps processes. The relevance\nof MLOps practices is also highlighted in education. For\ninstance, Lanubile et al. propose a project-based approach\nto teaching MLOps [8], [9]. With this in mind, we report\nour experiences in implementing MLOps so future AI/ML\nengineers can learn from them.\nDomain knowledge in DL-based systems. It has been\nlong known domain knowledge can improve system quality,\nas some research in particular domains shows [10], [11]. Most\ncurrent approaches introducing domain knowledge into a DL-\nbased system focus on the DL component, either by transform-\ning the input data, the model's loss, or its architecture [12].\nInstead, we show how to embed such knowledge directly into\nthe system's logic avoiding overcomplicated DL model design."}, {"title": "III. STUDY DESIGN", "content": "Following Goal Question Metric (GQM) [13], we define our\ngoal as: Analyze the use of MLOps and domain knowledge\nfor the purpose of improving and monitoring with respect\nto correctness, time efficiency, and resource utilization from\nthe point of view of AI/ML engineers in the context of a\nDL-based system for image recognition.\nThen, we derive the following Research Questions (RQs):\n\u2022 RQ1: How do MLOps practices impact system quality\nattributes?\n\u2022 RQ2: To what degree does the use of domain knowledge\ninformation improve DL-based system correctness, time\nefficiency, and resource utilization?\nThe motivation of RQ1 is to study the benefits and draw-\nbacks of integrating MLOps practices into the life cycle of a\nDL-based system and explore the challenges practitioners may\nface when performing such integration. We will focus on how\nto perform experiment tracking and energy monitoring during\nthe development phase, and the importance of automatic data\ncollection in operations.\nWith RQ2 we analyze whether wrapping a DL model within\nan algorithm encoding domain knowledge improves system\nquality attributes (QA). Specifically, we focus on correctness\n(RQ2.1), time efficiency (RQ2.2), and resource utilization\n(RQ2.3). We measure these QA using accuracy, prediction\nlatency (hereinafter latency), and energy consumption, respec-\ntively. Additionally, we aim to analyze whether algorithms\nwith domain knowledge generally outperform knowledge-free\nalgorithms regarding these QA and in what amount."}, {"title": "B. The DL system for image recognition", "content": "To address our RQs, we choose a DL-based system devel-\noped by the authors, aimed at live broadcasting chess matches\nusing computer vision. Figure 1 provides an overview of its\nlife cycle, inspired by the workflow in [1], divided into four\nkey processes: (i) Data Management stores and processes data\nfor training DL models, adapting as needed based on modeling\nrequirements; (ii) DL Modeling defines, trains, and evaluates\nthe model architecture, with feedback loops for adjusting data\nprocessing; (iii) Development integrates the model within the\ndetection algorithm, with feedback for improving the model-\ning; (iv) System Operation deploys the system experimentally\nand to end users, gathering metrics to improve performance.\nData from chessboard interactions then cycles back to enhance\nthe Data Management stage. This structure enables iterative\nrefinements, ensuring a robust, adaptable system.\nDL system requirements. To evaluate the degree of func-\ntionality and quality of our system, we define the following\nmain requirements [14]:\n1) Functional Requirements (FRs):\nFR1. The system must recognize the state of a chessboard\nfrom an image.\nFR2. The system must keep a record of the moves done in\na chess match.\nFR3. The system must monitor the time and energy re-\nquired to process an image. ([related to: RQ1])\n2) Constraints (Cs):\nC1. The system is deployed in edge devices.\nC2. The system must be continuously running during a\nwhole tournament day.\n3) Quality Requirements (QRs):\nQR1. The system must have a minimum accuracy of 90%.\n([related to: RQ1, RQ2.1])\nQR2. The system must recognize a chessboard image\nwithin 2 seconds. ([related to: RQ1, RQ2.2])\nQR3. The system must minimize energy consumption so\nthat devices can run without a recharge for at least\n12 hours. ([related to: RQ1, RQ2.3])"}, {"title": "C. Data analysis", "content": "In RQ1 we report our experiences. In RQ2, we conduct the\nstatistical analysis in three stages. First, we employ basic de-\nscriptive statistics, summarized in Table I. Second, we use the\nShapiro-Wilk test to assess the normality of our data. This test\nreveals that latency and energy consumption are not normally\ndistributed. Then, we apply a set of statistical tests depending\non the RQ. In RQ2.1, we perform a binomial-binomial test\nusing a Z-test to see if the progressive changes introduced in\nthe IA algorithm significantly improved accuracy. We omit\nSD and ESD since they have 0% accuracy. We also omit\nthe TK-k since it does not add additional domain knowledge\nto the algorithm and is limited to improving computation\nefficiency. In RQ2.2 and RQ2.3, we use the Kruskal-Wallis\ntest to see if differences in latency and energy consumption\nacross algorithms are statistically significant. Additionally, we\napply Dunn's test as a post-hoc test to determine which\npairs of algorithms present differences and similarities in their\ndistributions."}, {"title": "IV. APPLYING TRACEABILITY AND REPRODUCIBILITY\nWITH MLOPS (RQ1)", "content": "When deploying the system in operation, we detected a\nconsiderable drop in system performance. Particularly, its\naccuracy. For this reason, started collecting data in operation\nand implemented different feedback loops to help drive the\ndecision-making process at each stage, as depicted in Figure 1.\nHere we focus on the feedback from the experimental deploy-\nment and the DL modeling. To implement these two feedback\nchannels, we employed MLflow [15], an open-source tool\nconceived to track Machine Learning (ML) experiments. The\ntool is currently expanding its functionalities to cover several\nstages of the ML life cycle. In our case, we used MLflow\nto track the changes made in the DL modeling stage and"}, {"title": "A. Experiment tracking from new data collected in operation", "content": "MLflow makes track-\ning experiments seamless since it has built-in functions that\nautomatically log the most common parameters and metrics\nof popular ML libraries. In addition to common ML metrics,\nwe also needed to log energy consumption during training and\ndeployment, and latency in the deployment. MLflow simplifies\nthis process since it works much like a typical software logger,\nallowing users to simply call a log function specifying a key-\nvalue pair. It also allows adding artifacts to experiment runs,\nwhich we used to save relevant artifacts, i.e., the trained model,\nand files generated by the energy profilers. MLflow provides\na UI so that all the logged information can be visualized\nand compared easily. Specifically, this became a useful tool\nto quickly visualize how our decisions impacted metrics like\naccuracy or latency. Overall, using MLflow to keep track of the\nmain changes we introduced in the system offered invaluable\ninsights into the decision-making process."}, {"title": "B. Energy monitoring", "content": "A requirement for the system is that it must be energy\nefficient (cf. QR3) since it will be deployed in edge devices\n(cf. C1). To monitor energy, we use the Codecarbon Python\nlibrary [16] since it provides energy measurements for specific\ncode fragments instead of giving measurements at the process\nlevel. Its use is very straightforward and provides several\noptions to define the scope of the measurements. In our case,\nwe use it as a context manager. Codecarbon relies on the Intel\nRAPL interface to obtain CPU metrics. Since we used an\nAMD CPU which did not support this interface we were not\nable to get reliable measurements with Codecarbon. Hence,\nwe used Codecarbon together with the AMD \u00b5Prof tool [17]\nthat can profile the energy consumption of AMD CPUs. When\nusing this tool, it should be considered that the measurements\nare at the machine level. Also, measurements for specific parts\nof the code can be approximated by logging the start and stop\ntimestamps and extracting the energy measurements of that\ntime frame from the report generated by AMD \u00b5Prof."}, {"title": "C. Automatic data collection and processing", "content": "At the initial stages of the development, we manually gen-\nerated, validated, and processed new data for model retraining,\nwhich was time-consuming. However, to efficiently implement\nMLOps we are required to automatize this process. Hence,\nwe have improved the system so that all images collected\nduring system operation are sent to a centralized file store.\nThe system also registers the associated chessboard status in a\nremote database. Then, we use a semi-automatic approach to\ngenerate the final images to be fed into our models. First, an\nexpert verifies the detected chessboard status corresponds to its\nassociated image employing a custom validation application.\nAfter the necessary corrections, an automatic labeling and\nprocessing job is triggered to generate the final data used\nby the models. An example of the raw and processed data\nis shown in Figure 1. The validation step also collects the\nnumber of correct and incorrect detections. This data is used\nto compute performance metrics like accuracy for monitoring\npurposes, e.g., we trigger an alert in case QR1 is not met."}, {"title": "V. INCORPORATING DOMAIN KNOWLEDGE (RQ2)", "content": "In this section, we describe the differences between domain-\nfree and domain-aware algorithms and report the results after\nembedding domain knowledge."}, {"title": "A. From domain-free to domain-aware algorithms", "content": "Domain-free algorithms represent the most naive and\nstraightforward approach using DL models to solve the recog-\nnition task without any information except for the input image.\nIn particular, we developed two approaches. The first approach\nis the Square Detection (SD). This method uses a multi-\nclass Convolutional Neural Network (CNN) (Square Model in\nFigure 1) to classify the 64 board squares into the most likely\nclass, either empty or a specific piece (e.g., white pawn). In\nthe second approach, Ensemble Square Detection (ESD), we\nuse a combination of models. First, we use a binary CNN\nto classify squares as empty or occupied (Occupancy Model\nin Figure 1). For all occupied squares, we then use a second\nbinary CNN to identify the color of the piece occupying the\nsquare (Color Model in Figure 1). Finally, we detect the piece\ntype with the square model.\nThe creation of the domain-aware algorithm followed an\niterative process, in which we refined the algorithm implemen-\ntation in several iterations as is depicted in Figure 1.\nThe process started with the Initial Algorithm (IA) where\nwe encoded some domain knowledge and used contextual\ninformation. Specifically, the algorithm uses the occupancy\nmodel, knowledge of the previous chessboard state, and the\nlegal moves at that specific state. The main idea of this\nalgorithm is to reduce the number of analyzed squares to those\ncontaining pieces allowed to move in the current turn. Then\nit finds which square has become empty since the previous\nstate. Then, list possible destination squares based on the legal\nmoves allowed for the type of piece that has moved and check\nwhich has the highest probability of being occupied.\nBased on the performance feedback we obtained from the\nexperimental deployment, we applied three improvements to\nthe algorithm. In the first iteration, we adapted the algorithm to\nconsider the probabilities of the origin and destination squares\ninstead of interpreting them separately. Their combination\nallows the algorithm to obtain an overall likelihood of a\nmove. We refer to this version as the Combined Probabilities\nAlgorithm (CPA). In the second iteration, we implemented a\nspecial treatment for captures and castling over CPA, which\nwe abbreviate as CPS. Specifically, we introduced the Color\nmodel and knowledge of players' turns to detect which piece\nhas been captured in case there is more than one option.\nIn the case of castling, the algorithm substitutes the regular\nformulation of the combined probabilities with a special one\nconsidering the casuistic of this type of move. In the last\niteration, we aimed to reduce the number of computations by\nfirst selecting the k squares most likely to be empty and then\nproceeding in the same way as the CPS version. We refer to\nthese algorithms as TK-k, where k is the number of squares."}, {"title": "B. Experimental setting and execution", "content": "Experiments are conducted in an HP Workstation ZBook\nPower G10 with an AMD Ryzen\u2122\u2122 7 PRO 7840HS CPU, a\nNVIDIA RTXTM A1000 Laptop GPU, and 32GB of RAM.\nTo minimize the presence of background tasks, we close\nall non-essential processes and use the terminal to launch\nthe runs. To ensure all runs are under the same conditions\nof system temperature and CPU/GPU strain, we start with a\nwarm-up task before taking measurements. Also, we conduct\nthe executions in batches. Each batch lasts approximately\n15 minutes, with a 4-minute cool-down between batches. To\ncollect the metrics, we use the system to recognize 2,000\ndistinct samples utilizing each algorithm. We annotate the"}, {"title": "C. The Impact of Domain Knowledge", "content": "1) Accuracy (RQ2.1): Domain-free approaches, i.e., SD\nand ESD, perform well at the square level. SD correctly\nclassifies a median of 71.88% squares per board (\"square\naccuracy\"), and ESD reports a 79.69%. However, correctly\nrecognizing a chessboard state with these approaches requires\n100% square accuracy. This causes their 0% accuracy. On\nthe other hand, domain-aware approaches obtain remarkable\nresults achieving up to 96.85% accuracy in some versions.\nRegarding the degree of improvement in the domain-aware\nalgorithms, the Z-test results suggest that the changes intro-\nduced in CPA did not significantly improve IA, with a Z=-0.66\nand p > .25. When comparing CPA to its improved version\nCPS, we find a substantial impact in accuracy, with a Z=-\n17.02 and p < .001. This indicates that introducing special\ntreatments into the algorithm boosted the system's accuracy.\n2) Latency (RQ2.2): All approaches have similar laten-\ncies except ESD which is over one second. Nevertheless,\ndomain-aware methods consistently outperform domain-free\nmethods. IA is the fastest algorithm with a median latency\nof 0.264s. Indeed, our analysis showed statistical differences\nin the latency of the algorithms, with the Kruskall-Wallis\ntest reporting H=11871.68 and p = 0. The reported eta\nsquared (\u03b7\u00b2) is 0.659, indicating a considerable effect size,\nsuggesting that algorithm implementation significantly impacts\nlatency. Further inspection with Dunn's test reveals statistically\nsignificant differences between all algorithms.\n3) Energy consumption (RQ2.3): Domain-free approaches\nreport higher median energy consumption. SD reports 42.55J\nand ESD 59.12J. There is a noticeable reduction in energy\nconsumption when looking at the domain-aware approaches.\nSpecifically, IA stands out requiring only 24.524J. The results\nfrom the Kruskall-Wallis test reveal significant differences in\nthe energy consumption of the algorithms, with H=12058.8\nand p = 0. The large effect size (\u03b7\u00b2 = 0.670) indicates a\nsignificant impact of algorithm design on energy consump-\ntion. Dunn's test reveals some similarities among algorithms.\nSpecifically, it does not find statistical differences in the energy\nconsumption of (CPA, TK-2) and (CPS, TK-4)."}, {"title": "B. Improving the system with domain knowledge", "content": "RQ2 investigates how using domain knowledge improves\nthe system's quality attributes. We started developing our\nsystem with the naive thought that we could grab a model\nalready trained on chess data and use it out of the box (cf. SD).\nTo our surprise, the results\u2014a 0% accuracy\u2014 were disastrous.\nAt that point, we thought slightly changing the model and\nretraining it with new data would suffice. Our results in the\nvalidation dataset seemed promising with > 90% of square\naccuracy. Nevertheless, when the model was integrated into\nthe system and deployed we faced the same results as our\nfirst attempt. Indeed, we tested multiple architectures all giving\nus the same results. At that point, we faced two options.\nOne was to collect more data and continue refining models\nthat were reporting exceptional accuracies during training and\nvalidation. Another was to utilize the model intelligently. From\nour experience, the first option required a huge effort and we\nbelieved we could embed domain knowledge into an algorithm\nthat used a DL model as a central component (cf. IA). This was\na breakthrough in the development of the system. We moved\nfrom 0% of accuracy to 78.6% while also reducing latency and\nenergy consumption. This is expected since this new approach\nconsiderably reduced the number of predictions the system\nhad to perform. Hence, it reduced the chance of mistakes and\nthe number of computations. After the promising results, we\ncontinued refining the algorithm with domain knowledge. This\nled us to a second breakthrough with the CPS algorithm where\nwe raised the accuracy above the 95% and still complied with\nall our requirements. Seeing the good performance of these\nalgorithms, we tried to go back to our initial approach and\ncombine the new models we created for the domain-aware\nalgorithms (cf. ESD). Nevertheless, accuracy remained at 0%\nso we discarded the domain-free approach.\nOverall, our findings underscore the need for software\nengineers to make informed decisions and know when to\nstop working to improve quality metrics. This consideration is\nparticularly relevant in DL processes, where the tendency to\nmaximize a single quality metric can overlook the associated\ncosts. Once a system fulfills all required specifications, it is\ncrucial to evaluate whether further improvements in quality\nmetrics will yield meaningful benefits or merely escalate\ndevelopment time and costs."}, {"title": "VI. DISCUSSION", "content": "RQ1 focuses on how MLOps practices can be implemented\ninto the life cycle of a DL-based system to improve trans-\nparency and reproducibility. This experience paper shows how\ntools like MLflow can increase the traceability of experi-\nments conducted in the development stage. These tools can\neasily be used to map certain design decisions to specific\nquality attributes of the system. For instance, we can map\nthe selection of an algorithm implemented in our project to\nquality attributes like accuracy (cf. QR1), latency (cf. QR2),\nand energy consumption (cf. QR3). This has allowed us to\ncompare our different approaches in a clean and structured\nway, without the need for complex additions to our code or\nmanaging several files or databases.\nBesides traceability, we have also focused on the monitoring\nof energy consumption to improve QR3. Monitoring energy\nconsumption in software systems is not an easy task. Software-\nbased tools usually come in handy in this context since they\ntend to be more easy to set up compared to hardware-based\ntools. In this regard, we find that there are several alternatives\navailable with similar functionalities [16], [18]-[20]. In our\ncase, we found that Codecarbon became very useful since it is\na Python library that can be integrated into the code with little\neffort. Codecarbon lets you measure the energy consumption\nof specific segments of code, as opposed to other alternatives\nthat report measurements at the process level at best. This\ndegree of granularity is essential to detect inefficiencies in the\nsoftware. However, like most energy profilers, it relies on the\nRAPL interface. This makes it difficult to work with some\nAMD CPUs. In such cases, we recommend to compliment it\nwith the AMD \u00b5Prof tool.\nMonitoring during operations is crucial to keep track of\nthe system's performance. However, this stage can also be\nexploited to automatically gather new data to feed into the DL\nmodel. An advantage of this approach is that it can remove\nthe need for manual labeling. However, proper checks should\nbe in place to verify the data quality before processing. In\nour use case, these checks had to be done manually since the\nlabels required validation before the final processing. For this,\nwe designed a simple tool that presents the data in a visual\nformat. Using this tool, the validation process became faster\nand less error-prone than manually inspecting the files. From\nour experience, manual data-related tasks are adequate for the\ninfant stages of a DL-based system. Indeed, DL-based systems\nrequire strong and reliable data acquisition and processing\npipelines that can abstract data scientists from the burden of\nmanually performing such time-consuming tasks."}, {"title": "VII. CONCLUSIONS", "content": "In this work, we show how to integrate MLOps practices\nand domain knowledge when developing DL-based systems.\nImplementing MLOps provided transparent, reproducible ex-\nperimentation, enabling efficient mapping of design decisions\nto quality outcomes. Practitioners can use our experiences to\nset up experiment tracking and energy consumption monitor-\ning as part of their transition to MLOps. Furthermore, we\nshow that embedding domain-specific insights achieved sub-\nstantial gains in accuracy, latency, and energy consumption, far\noutperforming domain-free methods. This approach enhances\ndevelopment efficiency by reducing the need for extensive data\nand model complexity. In future work, we plan to continue\ndeveloping the system to make it scalable and more robust.\nThis will be evaluated in production in chess tournaments. We\nalso plan to improve our current MLOps process to adhere to\nthe model in [5]."}]}