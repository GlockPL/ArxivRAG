{"title": "The Price of Prompting: Profiling Energy Use in Large Language Models Inference", "authors": ["Erik Johannes Husom", "Arda Goknil", "Lwin Khin Shar", "Sagar Sen"], "abstract": "In the rapidly evolving realm of artificial intelligence, deploying large language models (LLMs) poses increasingly pressing computational and environmental challenges. This paper introduces MELODI \u2013 Monitoring Energy Levels and Optimization for Data-driven Inference a multifaceted framework crafted to monitor and analyze the energy consumed during LLM inference processes. MELODI enables detailed observations of power consumption dynamics and facilitates the creation of a comprehensive dataset reflective of energy efficiency across varied deployment scenarios. The dataset, generated using MELODI, encompasses a broad spectrum of LLM deployment frameworks, multiple language models, and extensive prompt datasets, enabling a comparative analysis of energy use. Using the dataset, we investigate how prompt attributes, including length and complexity, correlate with energy expenditure. Our findings indicate substantial disparities in energy efficiency, suggesting ample scope for optimization and adoption of sustainable measures in LLM deployment. Our contribution lies not only in the MELODI framework but also in the novel dataset, a resource that can be expanded by other researchers. Thus, MELODI is a foundational tool and dataset for advancing research into energy-conscious LLM deployment, steering the field toward a more sustainable future. The released code and dataset are available at https://github.com/ejhusom/MELODI.", "sections": [{"title": "Introduction", "content": "The swift progression of artificial intelligence (AI) has precipitated the emergence of advanced natural language processing (NLP) systems. Large language models (LLMs) have ascended to prominence among these systems, proving indispensable across a vast spectrum of applications. Their utility ranges from simple text predictions to managing complex dialogues. Concurrently, as these models grow in computational complexity, their energy demands escalate correspondingly [25, 32, 31]. In an epoch that prioritizes sustainable computing, the energy expenditure of LLMs is a consideration of mounting importance and one that commands focused attention [5, 17].\nIn the current literature, several endeavors aim to quantify and mitigate the environmental impact of AI. Prior studies have predominantly concentrated on the training phase of traditional machine learning (ML) models, with the inference phase often relegated to a secondary focus despite its cumulative energy footprint over the lifespan of a model [28]. Tools such as Green Algorithms [16] provide estimations of carbon emissions for computational tasks and yet lack real-time monitoring capabilities. Furthermore, while initiatives like CodeCarbon [6] and its integration to ML pipelines [13] offer a more automated tracking of emissions, they do not cater specifically to the nuanced demands of LLMs nor provide granular data concerning inference-related energy consumption.\nThis paper introduces the MELODI framework designed to monitor and analyze energy usage during LLM inference. The urgent need for such a framework stems from the growing environmental and economic costs associated with the operation of LLMs. MELODI leverages two specialized power usage monitoring tools (i.e., Scaphandre [27] and Nvidia-smi [22]). Scaphandre tracks the CPU's power consumption for the LLM process, while nvidia-smi measures the GPU's total power usage. For accurate measurements, the GPU must be exclusively used for the LLM inference, with no concurrent processes. MELODI records the energy consumption for each inference process, in addition to the prompt and the LLM's response. Furthermore, it facilitates the collection of metadata associated with LLM inference tasks, enabling a deeper analysis of the relationship between task complexity and energy utilization.\nAn important part of our research involved creating a robust dataset essential for evaluating MELODI's capabilities. This included gathering a diverse set of prompts to monitor the energy consumption of various LLMs across different types of instructions. We deployed these models on different hardware to ensure a thorough analysis. The data collection includes not only energy consumption measurements but also related metadata, such as the size and complexity of prompts, the responses, and the time metrics for each inference process. This rich dataset helps us explore the relationships between prompt complexity and the energy expenditure they incur.\nContributions. In summary, we make three main contributions. Firstly, we have developed a framework integrating open-source tools to monitor power and energy consumption during the inference phase of LLMs. Secondly, we have undertaken an extensive data collection initiative involving a variety of language model deployment frameworks, multiple LLMs, and diverse prompt datasets to facilitate a comprehensive comparison of energy consumption across these dimensions. Thirdly, we offer a generalized analysis that elucidates the relationships between prompt characteristics, such as length and features, and the energy consumption patterns observed within the collected data."}, {"title": "Background", "content": ""}, {"title": "Large Language Models (LLMs)", "content": "LLMs represent a significant advancement in natural language processing (NLP) and deep learning (DL). These models, such as OpenAI's GPT-3.5 architecture, are based on transformer neural networks, which have revolutionized NLP tasks by capturing long-range dependencies in text [29]. LLMs, trained on vast text datasets, learn intricate language patterns and relationships through unsupervised learning, predicting subsequent text from context. The integration of LLMs into digital ecosystems is underpinned by advanced APIs and deployment frameworks. These APIs are essential tools that allow developers to use LLM features for prompt submission and response generation, making it possible to add LLM capabilities to various applications. Among the plethora of tools, several APIs and frameworks, such as Ollama [23], OpenAI's GPT API [24], Huggingface's Transformers [33], Llama.cpp [18], GPT4All [2], and vLLM [15], have emerged as pivotal to the LLM deployment landscape."}, {"title": "Monitoring Tools and Technologies", "content": "Advanced monitoring tools and technologies are essential for tracking and optimizing energy consumption, thereby mitigating the environmental impact of AI operations. In this context, MELODI employs the following two tools for their complementary features:\nScaphandre [27] is one of the few tools that provide detailed insights into the energy usage of computing systems at the process level. Its ability to deliver real-time power metrics makes it invaluable for assessing the energy efficiency of LLM deployments.\nNVIDIA System Management Interface (nvidia-smi) [22] is a command-line utility providing the real-time monitoring of GPU, including power consumption, for systems equipped with NVIDIA GPUs. It can be used to evaluate the energy efficiency of LLMs leveraging GPU acceleration."}, {"title": "Dataset Collection: MELODI Framework for Monitoring Energy Usage", "content": "To understand the energy consumption patterns of LLMs, we created the MELODI framework, a tool designed to monitor energy usage during the inference processes of LLMs. Figure 1 provides an architecture of this framework. The core of MELODI's approach involves systematically sampling prompts from a curated collection, submitting these prompts to an LLM, and recording the prompts, their responses, metadata, and the energy metrics during inference. This data is cataloged to allow for a detailed analysis of energy consumption trends during the operational phase of LLMs.\n\u2022 Input Data (Prompt Dataset): MELODI extracts prompts from a pre-existing dataset (prompt dataset in Figure 1) and uses them to query a deployed LLM through an API client.\n\u2022 Power Usage Monitoring and Data Collection: MELODI integrates two power monitoring tools, Scaphandre [27] and nvidia-smi [22], to track the power consumption of the deployed LLM, such as the Ollama model depicted in Figure 1. These tools provide complementary functionalities: Scaphandre measures the CPU's power usage specific to the LLM process, while nvidia-smi assesses the total power usage of the GPU. It is crucial that no other processes operate on the GPU during LLM inference to ensure the accuracy of nvidia-smi's measurements. MELODI then calculates the energy consumption for each prompt, compiling data samples that encompass the prompt, the LLM's response, and the associated energy metrics, thus providing a granular view of energy utilization.\n\u2022 Dataset Generation: MELODI iterates through all prompts from the dataset, capturing the prompt, its response, and energy metrics. These data samples are aggregated into a comprehensive dataset, encapsulating the energy consumption of the LLM during inference.\nThe MELODI framework is contingent upon LLM services that can be queried for responses. While MELODI is designed to be compatible with any service offering an API endpoint for response generation, our current implementation specifically caters to services that conform to either the Ollama- or OpenAI API specifications, encapsulating the majority of prevalent LLM deployment services. These API endpoints furnish not only the LLM's natural language responses but also critical metadata, including timestamps and token lengths for both prompts and responses. The landscape of LLM deployment tools is diverse, featuring platforms such as Llama.cpp [18], GPT4All [2], vLLM [15], Llamafile [19], and Ollama [23]. For the scope of our experiments, we chose to utilize Ollama [23] as our model deployment mechanism, attributing this choice to its straightforward installation process, dual compatibility with CPU and GPU architectures, and the expansive repository of open-source LLMs it supports for facile download and deployment.\nScaphandre [27] operates with process-level granularity, monitoring and recording the power usage of active processes on a computing device. We apply a regular expression to the process names to isolate the specific process associated with the LLM service. The frequency at which Scaphandre samples power metrics is configurable with granularity fine enough to reach the nanosecond scale. Although a higher sampling frequency increases the resolution and accuracy of the data, it also requires more storage for data retention and increases the energy consumption of Scaphandre itself. Therefore, we also monitor the energy consumption of Scaphandre, enabling an assessment of the energy overhead incurred by the measurement process. In contrast, nvidia-smi [22] tracks the power usage of the entire GPU and lacks the capability to distinguish among multiple processes concurrently utilizing the GPU. To guarantee precise measurements, we limit the GPU to solely operating the LLM inference process. Our monitoring tools measure the power consumption $P$ of a process in microwatts (\u03bcW). We then compute the energy consumption $E$ by integrating the power consumption over the duration $t$, applying the trapezoidal rule. With $t$ expressed in seconds, we convert $E$ into kilowatt-hours (kWh) by dividing by $3.6 \u00d7 10^{12}$, facilitating a standardized energy usage assessment."}, {"title": "Dataset Composition", "content": "This section outlines the energy consumption dataset collected with MELODI, using diverse computing systems (from high-end servers to laptops) and multiple prompt datasets for several LLMs. Table 1 lists the prompt datasets in dataset collection, i.e., Alpaca [1] and Code-Feedback [12], and their average energy consumption and response token length. The Alpaca dataset, produced using OpenAI's text-davinci-003 engine, contains 52,000 prompts designed to enhance instruction-following capabilities in language models. The dataset primarily includes text generation tasks and is tailored to train pre-trained language models to follow complex instructions more effectively. The Code-Feedback dataset supports the OpenCodeInterpreter model [35] designed to refine code by integrating code execution and human feedback. This dataset features 68,000 multi-turn interactions, combining user instructions with compiler responses to enhance model training in coding scenarios."}, {"title": "Dataset Analysis", "content": "We analyze the energy consumption dataset to address three Research Questions (RQ)s:\n\u2022 RQ1. How does the energy consumption of LLM inference vary across different hardware, models, and prompt datasets?\n\u2022 RQ2. What is the relationship between prompt complexity, response characteristics, and the energy consumption of LLMs during the inference process?\n\u2022 RQ3. Can we develop a predictive model that accurately forecasts the energy consumption of LLMs based on prompt features and response characteristics?"}, {"title": "Analysis Setup", "content": "Data Preparation. Our raw data consists of power usage over time, which we aggregate to calculate the energy consumption for each inference process as outlined in Section 3. These aggregated values are then compiled into distinct energy consumption datasets, categorized by the specific hardware, model, and prompt dataset utilized during the inference.\nStatistical Analysis. Our analysis of the energy consumption datasets focuses on two metrics: energy per token and energy per response. Energy consumption per token is crucial as it evaluates resource use across different inference setups regardless of text volume. Meanwhile, energy per response is beneficial for estimating resource usage relative to the input prompt. We conduct statistical analyses on these metrics to compare across model types, sizes, hardware setups, and the specific prompt datasets used during inference, providing a comprehensive view of energy dynamics.\nVisualization. We employ box plots to visualize the distribution of energy consumption for each model and hardware configuration (see Figure 2). The box plots depict the median energy consumption as a red line, the interquartile range with the main box, and extend to 1.5 times the interquartile range with whiskers, providing a clear summary of variability and central tendency.\nInterpretation. We interpret results through a comparative analysis of energy consumption across different scenarios. The relationship between prompt complexity and energy consumption is assessed using the Pearson correlation coefficient, which ranges from 0 to 1 to indicate variable correlation. Additionally, predictive models for energy consumption are evaluated using the $R^2$ score."}, {"title": "Results", "content": ""}, {"title": "RQ1 (energy consumption of LLM inference across different setups)", "content": "To address RQ1, we performed a comparative analysis of energy consumption across various LLMs operating on different hardware. The box plots in Figure 2 graphically represent this analysis, showing the energy usage of LLMs during inference on diverse hardware setups.\nModel Size. Figure 2a categorizes energy consumption per token by the model utilized for inference. Notably, the largest models (codellama-70b with Code-Feedback and llama3-70b with Alpaca) exhibit energy usage approximately 100 times greater than their smallest counterparts (codellama-7b with Code-Feedback and llama3-8b with Alpaca). Additionally, the larger models (gemma-7b with both Alpaca and Code-Feedback) show an energy consumption that is roughly ten times higher than their smaller versions (gemma-2b with Alpaca and Code-Feedback).\nHardware. Figure 2b organizes results by the hardware used for model execution. Energy consumption is noticeably higher when LLM tasks are run on the laptops than on the workstation. Specifically, Laptop1 (CPU-only) shows similar energy usage levels when operating different models (codellama-7b with Code-Feedback and gemma-2b with Alpaca), suggesting inefficiencies in CPU-based processing for LLM tasks since we would expect a significant difference when running a 2b model vs a 7b model. This observation could also reflect the limitations of the CPU-monitoring tool Scaphandre, as the power metrics are only estimates [14]. Comparisons between Laptop2 and the workstation, despite the latter's more robust GPU (12GB vs. 8GB), reveal roughly equivalent energy usage, underscoring potential disparities in hardware efficiency.\nModel. In energy assessments of different models of the same size (codellama-7b with Code-Feedback vs. gemma-7b with Code-Feedback) run on both a workstation and Laptop2, gemma-7b consistently displayed higher energy consumption than codellama-7b. This observation suggests that codellama-7b is a slightly more energy-efficient model than gemma-7b.\nPrompt Datasets. To evaluate the energy consumption per token across two prompt datasets, we analyzed the box plots for gemma-2b and gemma-7b in Figure 2a, the only models tested with both prompt datasets. The analysis reveals that, except in one instance, the Alpaca dataset consistently led to higher energy consumption per token. Additionally, the interquartile range, representing variability in energy consumption, is notably wider in all cases, suggesting greater fluctuation in energy use when using the Alpaca dataset.\nTable 4 facilitates a comparison of energy consumption per prompt, revealing that the median consumption for the Code-Feedback prompt dataset exceeds that of the Alpaca dataset during inference. This discrepancy likely arises from the longer average response length associated with the Code-Feedback prompts, which at 373 words surpasses the 229 words of the Alpaca prompts.\nRQ1 Conclusion. Our analysis indicates marked disparities in energy consumption among different LLMs and hardware configurations. Larger models like codellama-70b and llama3-70b use roughly 100 times more energy per token than smaller ones. Energy demands are higher for LLMs running on laptops than workstations, particularly due to inefficiencies in CPU processing. Models of the same size exhibit varying energy efficiencies. Notably, the Code-Feedback dataset leads to greater energy use per token than the Alpaca dataset, likely due to longer response lengths."}, {"title": "RQ2 (the relationship between prompt complexity, response characteristics and the energy consumption)", "content": "To address RQ2, we used Python frameworks-Spacy [11], nltk [4], textblob [20], and text-stat [30]-to derive 57 text-based features. We integrated these features with our energy consumption dataset and calculated the Pearson correlation coefficient between energy consumption per prompt and each feature. Table 5 presents the top 20 features from this analysis.\nOur analysis revealed that significant correlations with energy consumption per prompt are primarily linked to response characteristics rather than the complexity of the prompt. Key factors such as response token length, response duration, and total inference duration show strong positive correlations with energy consumption, with coefficients of 0.846, 0.625, and 0.618, respectively. This observation suggests that energy consumption escalates with an increase in response tokens due to more extensive processing within the model, highlighting that longer and more time-intensive responses significantly drive higher energy usage.\nUpon analyzing correlations with prompt complexity features, we observe only low positive correlations. Attributes like adjective count, syllable count, and long word count demonstrate modest correlations with energy consumption, with coefficients of 0.113, 0.083, and 0.090, respectively. These results indicate that prompt complexity (given our set of features) has a minimal impact on energy consumption. Instead, the length and duration of responses are more significant factors. This insight suggests that optimizing the response generation process could be more effective in reducing energy consumption than merely simplifying the input prompts.\nRQ2 Conclusion. The results show that response characteristics such as token length and duration are strongly correlated with energy usage, indicating higher consumption with longer responses. Conversely, prompt complexity features like adjective and syllable counts exhibit low correlations, suggesting minimal impact on energy use. These findings emphasize that managing response generation may offer more significant energy savings than simplifying prompts."}, {"title": "RQ3 (predictive model for the energy consumption of LLMs)", "content": "To address RQ3, we developed ML models using the energy consumption dataset, focusing on either prompt or response characteristics as input features. Our exploratory analysis identified Random Forest (RF) as the optimal model for effectively handling both types of input features. This choice was based on their performance and suitability for the data structure encountered in our dataset. Hence, we developed RF models (using either response token length or prompt characteristics) across hardware setups, models, and prompt datasets (one RF model for one LLM utilizing one prompt dataset on one hardware setup).\nTable 5 shows that energy consumption highly correlates with response characteristics such as token length and duration. Figure 3 presents the $R^2$ scores of RF models using only response token length, demonstrating high predictive performance in most cases, with the majority achieving $R^2$ greater than 0.97 and an average $R^2$ score of 0.94. However, the RF models for laptop1 with only a CPU show a lower $R^2$ of 0.74, suggesting potential inaccuracies in CPU-based energy consumption estimates.\nThe findings reveal that constraining the length of responses-by explicitly specifying the maximum text length in the prompts-could significantly reduce energy consumption during model operations.\nPredictive models relying solely on prompt characteristics exhibit limited performance, as depicted in Figure 4. Analysis of the $R^2$ scores across different models reveals variability in predictive accuracy. For example, the RF models for LLMs like gemma-7b, utilized with the Code-Feedback dataset on both a workstation and Laptop2, as well as gemma-2b using the Code-Feedback dataset on the workstation, show relatively higher $R^2$ scores, ranging between 0.549 and 0.580. This demonstrates some levels of predictive effectiveness, particularly under specific hardware and dataset conditions.\nFigure 4 illustrates that RF models for some LLMs, such as codellama-7b using Code-Feedback on laptop1 and laptop2, demonstrate notably poor predictive performance, with negative $R^2$ scores signaling ineffective model fitting and potential discrepancies in model predictions. Specifically, the RF model for codellama-7b utilizing Code-Feedback on laptop2 shows an $R^2$ score of -205.897, indicating a significant divergence between predicted and actual energy consumption. Generally, the RF models for gemma models show more predictable energy consumption patterns than llama models. Nonetheless, the small dataset size limits the robustness of these conclusions.\nRQ3 Conclusion. Random Forest models showed strong predictive abilities, especially when using response characteristics, with most achieving an $R^2$ score over 0.97. However, models for CPU-only laptops performed poorly, indicating potential inaccuracies in CPU-based energy estimates. Overall, models trained on response features were more effective than those using prompt characteristics, highlighting areas for energy optimization in LLM operations."}, {"title": "Threats to validity", "content": "Threats to internal validity. The primary concern regarding the internal validity of our data collection and analysis is the accuracy of the power usage monitoring tools. The precision of the nvidia-smi tool, as noted in the documentation of NVIDIA [7], has a stated accuracy within \u00b15 Watts. However, a study by Yang et al. [34] suggests the error margin might reach up to \u00b15%. Accurate GPU power usage readings also necessitate that no other processes utilize the GPU concurrently. While we ensured this in most experiments, some base processes remained active on the GPU of the workstation, though their impact on baseline energy consumption was minimal. Scaphandre only provides an estimate of the power usage based on the RAPL interface of Intel processors [27] without quantifying accuracy, potentially affecting the reliability of our energy consumption data. Additionally, power efficiency variations in identical hardware models could introduce more variability to our results. Future research should utilize more precise power measurement tools and standardized tests across different hardware to better understand and quantify these discrepancies.\nThreats to external validity. The generalizability of our results is constrained by the limited range of inference scenarios analyzed, particularly in terms of hardware setups and LLMs utilized. The small sample size makes it challenging to draw robust conclusions, and the extent to which our findings can be generalized depends on the representativeness of the selected hardware and model configurations relative to the broader array of LLM deployments."}, {"title": "Discussion and Future Work", "content": "Our study provides insights into the energy consumption of LLM inference while identifying areas needing further research. One limitation identified is the accuracy of CPU-based power monitoring, which may compromise our results. Despite using the Scaphandre tool for measuring CPU power, we observed similar energy consumption across models of significantly different sizes, suggesting possible measurement imprecision. This was particularly evident in the reduced predictive performance of energy consumption on CPU-only setups, as opposed to configurations equipped with GPUs.\nOur results reveal that response token length is a reliable indicator of energy consumption, suggesting that managing response lengths could effectively control energy use. However, the connection between prompt complexity and energy consumption is less definitive. While models based on prompt characteristics show potential, their performance varies. Future research could explore the potential of using LLMs or other NLP models to analyze prompts for energy consumption prediction, balancing the benefits against the potential energy costs of such analyses."}, {"title": "Related Work", "content": "The merging of NLP and sustainable computing, emphasized in studies like the investigation of Strubell et al. [28] into the energy use and carbon footprint of NLP model training, is a pivotal aspect of AI. These studies stress the need for energy-efficient practices across AI model lifecycles and have spurred extensive research into sustainable AI. Meanwhile, initiatives by Henderson et al. [10] advocate for transparent reporting of energy consumption, enhancing accountability, especially during the training phase. Tools like Green Algorithms [16], CodeCarbon [6], and CarbonTracker [3] have been developed to estimate computational carbon emissions, yet they often provide general rather than detailed analyses necessary for large-scale LLM deployment.\nLuccioni et al. [21] and Desislavov et al. [8] found that general-purpose ML models are more energy-intensive than task-specific ones and that efficiency gains in NLP and computer vision do not necessarily correlate with increased energy consumption. In contrast, MELODI, focusing on the inference stage, delivers real-time energy assessments of LLMs by utilizing Scaphandre and nvidia-smi to directly measure power usage. This method provides detailed energy metrics at the API level, offering a more nuanced evaluation of energy use than broader estimates and enhancing MELODI's role in sustainable AI.\nRecent studies, including those by Liang et al. [17] and Samsi et al. [26], focus on sustainable AI by analyzing the LLMs' energy usage and carbon footprint. These studies highlight energy demands across various LLM configurations. However, MELODI enhances these efforts by providing a comprehensive framework that measures LLM energy consumption by leveraging Scaphandre and nvidia-smi for real-time energy tracking of both CPUs and GPUs. This capability enables a more dynamic energy efficiency optimization in LLM deployment, distinguishing MELODI with its predictive and monitoring strengths.\nEverman et al. [9] analyzed the carbon footprint of open-source LLMs during inference, using the Software Carbon Intensity (SCI) to gauge environmental impact. Their findings debunked the notion that higher carbon footprints correlate with better model quality and highlighted GPUs' role in reducing emissions. In contrast, MELODI supports real-time power monitoring during inference, generating data that serves as a benchmark and facilitates the development of ML models for predicting energy consumption. Thus, while Everman et al. focus on comparative carbon impact analysis, MELODI enables continuous monitoring and predictive energy management."}, {"title": "Conclusion", "content": "In conclusion, MELODI represents a noteworthy leap forward in advancing energy-aware practices within the domain of LLMs. By formulating a precise methodology and unveiling an open-source instrument tailored for real-time monitoring of energy consumption throughout the LLM inference process, MELODI effectively bridges a vital chasm in sustainable computing. Our data collection, encompassing various LLM architectures, hardware setups, and prompt datasets, provides a robust platform for rigorous comparison and nuanced analysis. This endeavor facilitates a deeper understanding of the energy dynamics associated with LLM deployment scenarios, offering critical insights that contribute to the optimization and sustainability of LLM applications."}]}