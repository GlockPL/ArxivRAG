{"title": "Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights and Limitations", "authors": ["Kaiyuan Zheng", "Qinghua Zhao", "Lei Li"], "abstract": "The relationship between language and thought remains an unresolved philosophical issue. Existing viewpoints can be broadly categorized into two schools: one asserting their independence, and another arguing that language constrains thought. In the context of large language models, this debate raises a crucial question: Does a language model's grasp of semantic meaning depend on thought processes? To explore this issue, we investigate whether reasoning techniques can facilitate semantic understanding. Specifically, we conceptualize thought as reasoning, employ chain-of-thought prompting as a reasoning technique, and examine its impact on sentiment analysis tasks. The experiments show that chain-of-thought has a minimal impact on sentiment analysis tasks. Both the standard and chain-of-thought prompts focus on aspect terms rather than sentiment in the generated content. Furthermore, counterfactual experiments reveal that the model's handling of sentiment tasks primarily depends on information from demonstrations. The experimental results support the first viewpoint.", "sections": [{"title": "1 Introduction", "content": "In the realms of linguistics and cognitive science, the relationship between language and thought has long been a subject of profound inquiry and debate. Two contrasting viewpoints have emerged in this discourse. [4] argues for the independence of language and thought, positing that language serves merely as a vessel for thought, with each entity distinct and separate. In stark contrast, [22] proposes a more intricate relationship, suggesting that \"the limits of my language mean the limits of my world\". This perspective implies that the scope of our thoughts is fundamentally constrained by the language we possess to express them. The tension between these divergent views raises a critical question in the context of contemporary artificial intelligence: To what extent a language model's capacity is to grasp semantic meaning underlying thought processes?"}, {"title": "2 Related work", "content": "This paper involves analyzing SA tasks using CoT to determine whether the task leverages the model's reasoning ability, touching on reasoning (one kind of thought) and language. Therefore, the related work includes language and thought, chain-of-thought, and sentiment analysis.\nLanguage and Thought. [4] found that language and thought are dissociated in the human brain. They discovered a schematic representation of the response profile of the language network (for example, as measured by fMRI). This network responds strongly to language comprehension and production but not to non-linguistic tasks that require thinking and reasoning. Therefore, they argue that language is a tool for communication, not for thinking, and that there is a clear distinction between the language system and various systems involved in thinking and reasoning. However, according to Wittgenstein, his famous idea, \"The limits of my language mean the limits of my world\", can be interpreted as thought being constrained by the structure and scope of language [22]. If we cannot express something in language, we cannot fully grasp or conceptualize it in thought. This notion implies that language doesn't just communicate thoughts but also forms the boundaries of our cognitive processes.\nChain-of-Thought. CoT is an advanced form of in-context learning [1], designed to guide language models in generating coherent sequences of intermediate reasoning steps [20]. By providing step-by-step problem-solving processes in exemplars, CoT aims to lead models towards more accurate and justifiable answers to complex questions. This approach is particularly relevant to tasks requiring higher-level cognitive abilities, bridging the gap between language processing and thought-based reasoning.\nInterpretability in Sentiment Analysis. Recent research has extensively explored the performance of large language models (LLMs) in SA tasks. [25] highlighted the critical role of emotionally charged adjectives in determining overall sentiment, while [21] investigated whether LLMs rely more on pre-trained knowledge or in-context exemplars when addressing SA tasks. [8] utilized the SST-2 sentiment analysis benchmark to elucidate in-context learning mechanisms through the construction of contrastive examples. Notably, [5] pioneered the application of CoT to implicit SA, demonstrating the significant role of reasoning in this domain. Further applications of CoT in SA include the work of [14], who employed CoT to address emotion states and causes in conversations using six basic emotions. Similarly, [6] integrated CoT-style prompts into ABSA, consolidating reasoning steps within single exemplars. Despite these advancements, intriguing findings by [19] and [26] revealed that shuffling word order in SA tasks results in only marginal performance drops. This also raises another critical question: if word order has limited impact on SA, to what extent is reasoning ability necessary for these tasks?"}, {"title": "3 Experimental Setup", "content": "This section outlines the LLMs, datasets, and prompts employed in our experimental framework."}, {"title": "3.1 Models", "content": "Our experiments utilize a range of models varying in size and architecture, including Gemma-2 (2B, 9B, 27B) [18] and LLaMA-38B [3]. These models were deployed on two A800 GPUs, each equipped with 80GB of memory, and operated in float32 precision to ensure optimal performance and accuracy."}, {"title": "3.2 Datasets", "content": "For our analysis, we selected two widely recognized ABSA datasets from SemEval-2014 [13]: the Laptop and Restaurant datasets. Refer to Table 1 for examples. To align more closely with our research objectives, we applied a set of criteria to select test samples.\n\u2022 Text length: We prioritized longer text samples to ensure comprehensive semantic expression and sufficient scope for sentiment shifts.\n\u2022 Sentiment dynamics: Selected samples exhibit sentiment changes to assess the model's capacity for understanding aspect sentiment.\n\u2022 Complexity: Each sample contains a minimum of two aspects and two sentiment changes to ensure sufficient complexity.\n\u2022 Sentiment split: Following [7], we categorized the samples into explicit and implicit splits. Explicit data contains direct expressions of sentiment or emotion, where the sentiment is clearly articulated (e.g., \"Just ten minutes away from you makes me want to cry\"). In contrast, implicit data captures more nuanced cues where sentiment is indirectly conveyed through context or subtler language (e.g., \"I miss you\")."}, {"title": "3.3\nCoT-style prompts", "content": "For standard prompts, we directly construct it by concatenating the input question and answers. For CoT-style prompts, we require them to describe each aspect sentiment one by one. Different CoT-style prompts can lead to significant performance differences. Even when prompts are semantically similar, LLMs may generate vastly different responses [9,11]. Therefore, to avoid the experimental conclusions being biased by a specific prompt, we tested three different versions of the CoT strategy, covering various levels from natural language expression to symbolic representation.\nSpecifically, the first version we used is a purely natural language-based CoT, which relies entirely on natural language to express the reasoning process for sentiment polarity. This version aims to simulate the sentiment reasoning process used in everyday human language, emphasizing the naturalness and coherence of language, called CoT-v1. The third version is a symbol-based CoT, where symbols and logical expressions are used to describe sentiment polarity shifts, reducing the reliance on natural language and placing a stronger emphasis on the logical aspects, named CoT-v3. Additionally, we employed a hybrid CoT, which strikes a balance between the two approaches, combining natural and symbolic language to balance the naturalness of language expression with the logical rigor of reasoning, named CoT-v2. Examples of the different versions are shown in Table 2."}, {"title": "4 Experiments", "content": "This section delineates our experimental framework designed to address four pivotal research questions:\n\u2022 RQ-1: What is the adaptation of CoT on SA?\n\u2022 RQ-2: Is it conflict or consistency with more complex emotions?\n\u2022 RQ-3: How does CoT affect the correlation between input questions and output tokens?\n\u2022 RQ-4: Does the model rely on knowledge acquired during the pre-training or in the CoT exemplars?"}, {"title": "4.1 RQ-1: Adaptation of CoT in SA", "content": "To investigate whether reasoning techniques can enhance semantic understanding, we examined the impact of CoT on SA tasks. As shown in Figure 2, we reported results of CoT-v1 across four models, six shot settings, and two datasets with explicit and implicit splits.\nOur findings reveal that: CoT yields improvements for the smallest model (Gemma2-2b) and in 1-shot scenarios. For instance, on the implicit split of Laptop dataset with Gemma2-2b and 1-shot, accuracy increased from 0.24 (standard prompt) to 0.62 (CoT-v1). However, for larger models, CoT's impact on SA is minimal. This limited improvement may be attributed to the relative simplicity of SA tasks for current LLMs, which already achieve high accuracy (>0.95) with standard prompts. Besides, as the number of demonstrations increases, CoT's effectiveness diminishes. For example, the improvement for Gemma2-27b on the explicit split of Restaurant dataset drops from 0.18 (1-shot) to 0.0 (18-shot)."}, {"title": "4.2 RQ-2: Conflict or consistency?", "content": "To further test the impact of CoT on SA tasks, we constructed a emotional analysis dataset with higher emotional complexity, yielding results consistent with the Laptop and Restaurant datasets. Additionally, we explored the effect of the number of emotion categories and the count of emotional shifts on model performance.\nConstruction of multi-emotion shift dataset. To facilitate this investigation, we manually constructed a novel multi-emotion shift dataset (MES) featuring fine-grained emotional expressions. This dataset is characterized by texts containing multiple emotion types and frequent emotional shifts within single narratives. Given the complexity and often overlapping nature of human emotions, we focused on six distinct major emotional categories: fear, happiness, anxiety, jealousy, loneliness, and shame. These emotions were contextualized within various scenarios including work environments, public transportation, entertainment activities, social interactions, and dining experiences. We created 100 emotion-shift texts, each incorporating at least two emotional transitions. These narratives were crafted to reflect real-life situations, maintaining a balance between scenario continuity and cross-scenario coherence. This approach ensures both the representativeness of the dataset and its suitability for exploring the impact of emotional complexity on model reasoning capabilities.\nConflict increases difficulty. Figure 4 illustrates the relationship between overall accuracy and emotional complexity, considering both the number of emotion categories and the frequency of emotional shifts within texts. Our findings reveal a consistent trend across most models:\n\u2022 Models demonstrate lower accuracy when texts contain more frequent emotional shifts. For instance, Gemma2-9b's accuracy decreases from 0.92 to 0.78 as the number of emotional shifts increases from 2 to 3.\n\u2022 Similarly, accuracy declines with an increase in the number of emotion categories present in a text. Gemma2-9b shows a drop in accuracy from 0.81 to 0.73 when the number of emotion categories increases from 3 to 4.\nThese results suggest both the frequency of emotional shifts and the diversity of emotion categories contribute to the complexity of SA tasks. Texts with fewer emotional shifts and a more limited range of emotion categories (i.e., exhibiting greater emotional consistency) appear to be more manageable for the models."}, {"title": "4.3 RQ-3: Correlation between Input and Output Tokens", "content": "This section further explores how input interacts with output tokens when using CoT. We approach this question by analyzing the similarities between input questions and generated answers.\nFigures 5 and 6 illustrate our findings, which reveal several key insights: Firstly, the explicit split demonstrates a stronger similarity between sentiment words in the output and input text. In contrast, the implicit split does not exhibit such clear patterns. Moreover, the similarity between input text and aspect words is generally higher than the similarity between input text and corresponding aspect sentiments. Additionally, the standard prompt and various CoT versions maintained a generally consistent similarity between the overall sentiment of the output and the input text. This suggests that CoT may not have substantially influenced the model's interpretation of sentiment in the input text."}, {"title": "4.4 RQ-4: Pre-training knowledge vs. demonstration information", "content": "Prior research on models like BERT has shown that word order shuffling in SA tasks has minimal impact on model performance [12,17]. Given that CoT is designed to reduce reasoning complexity through step-by-step processing, we investigate the extent to which the model's sentiment analysis relies on pre-training knowledge versus information provided in few-shot demonstrations, by examining whether word order disruption affects SA results undering CoT prompting.\nWord order disruption test. We employed the word order disruption method from [24], sequentially swapping adjacent words to disrupt both local and global word positions. This process was applied only to the input question, leaving the demonstrations unchanged. To assess the impact of this disruption, we measured the agreement between predictions of disturbed and original inputs.\nResults, as shown in the upper part of Figure 7. After perturbing the input text, model size positively correlates with agreement: Gemma-27b achieves higher mean agreement (0.79) compared to the Gemma-2b (0.58) on the explicit split of Laptop dataset. Moreover, the agreement strengthens with an increasing number of few-shot examples, as demonstrated by the Gemma-2b's performance on the explicit split of the Laptop dataset (1-shot: 0.0, 12-shot: 0.70). Additionally, explicit splits show higher mean agreement than implicit splits (0.67 vs. 0.54). Taking Gemma-27b as an exemplar, the model exhibited relatively small variations in its generated content (i.e., higher agreement), with mean agreement values of 0.80 and 0.61 for explicit and implicit splits, respectively.\nCounterfactual demonstration test. To further investigate the model's utilization of demonstration information, we adopted the counterfactual method proposed by [10]. This approach involves creating a deliberate conflict between the knowledge in demonstrations and the presumed factual knowledge from the pre-training corpus.\nWe reversed the sentiment of aspects in the demonstrations, randomly replacing original sentiments with their opposites (positive with negative, negative with positive, and neutral with either positive or negative). The input questions remained unchanged. We also report the agreement between predictions of original and modified demonstrations. Results, presented in the lower part of Figure 7: When perturbing demonstrations, model size again correlates positively with agreement: Gemma-27b shows higher mean agreement (0.71) compared to Gemma-2b (0.46) on the explicit split of Laptop dataset. However, unlike input perturbation, increasing the number of few-shot examples leads to lower agreement, as evidenced by Gemma-27b's performance on the explicit split of Laptop dataset (4-shot: 0.81, 12-shot: 0.69).\nThese results indicate that modifications in demonstrations significantly influenced the model's decisions, suggesting that the model relies on demonstration information in SA tasks."}, {"title": "5 Discussion of Language and Thought", "content": "Our findings challenge Wittgenstein's view that \"language limits the boundaries of thought\" and support the independence of language and thought. However, the authors still align with Wittgenstein's perspective.\nOur results prompt deeper reflection: language may serve merely as a tool, whose role is to propagate and communicate abstract concepts, and these concepts must exist first before language can express them. Language is like a quantitative metric used to reflect the level of thinking ability. From a static perspective, language cannot convey ideas beyond the scope of cognition. However, from a dynamic viewpoint, the progression of thought drives language evolution, while the expansion of language, in turn, facilitates deeper thinking. The following examples illustrate this perspective:\nCultural differences shape language interpretation. For instance, \"Nobody loves you\" carries a negative connotation in Western cultures, often inducing psychological distress [15]. Language can convey the concept of childbirth pain but cannot fully replicate the experience [16]. Similarly, the Yang-Mills equations, though containing complex formulas, require deep understanding to grasp their true meaning [23]. The emergence of new concepts like \"autonomous driving\" or \"Mars colonization\" has led to corresponding terms, expanding language boundaries [2].\nLanguage development promotes Thought development. The progress of language also promotes the development of thought. This is reflected in the communication function of language, where new thoughts are spread to others who do not possess them, thereby enabling those people to acquire the corresponding thinking ability through understanding language."}, {"title": "6 Conclusion", "content": "This paper refines the \u201clanguage and thought\" debate by framing language as sentiment understanding and thought as chain-of-thought. Experiments on two public datasets and one constructed emotional dataset show that chain-of-thought has limited impact on sentiment analysis, supporting Fedorenko's view on the independence of language and thought."}]}