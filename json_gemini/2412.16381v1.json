{"title": "VerSe: Integrating Multiple Queries as Prompts for Versatile Cardiac MRI Segmentation", "authors": ["Bangwei Guo", "Meng Ye", "Yunhe Gao", "Bingyu Xin", "Leon Axel", "Dimitris Metaxas"], "abstract": "Despite the advances in learning-based image segmentation approach, the accurate segmentation of cardiac structures from magnetic resonance imaging (MRI) remains a critical challenge. While existing automatic segmentation methods have shown promise, they still require extensive manual corrections of the segmentation results by human experts, particularly in complex regions such as the basal and apical parts of the heart. Recent efforts have been made on developing interactive image segmentation methods that enable human-in-the-loop learning. However, they are semi-automatic and inefficient, due to their reliance on click-based prompts, especially for 3D cardiac MRI volumes. To address these limitations, we propose VerSe, a Versatile Segmentation framework to unify automatic and interactive segmentation through mutiple queries. Our key innovation lies in the joint learning of object and click queries as prompts for a shared segmentation backbone. VerSe supports both fully automatic segmentation, through object queries, and interactive mask refinement, by providing click queries when needed. With the proposed integrated prompting scheme, Verse demonstrates significant improvement in performance and efficiency over existing methods, on both cardiac MRI and out-of-distribution medical imaging datasets. The code is available at https://github.com/bangwayne/Verse.", "sections": [{"title": "Introduction", "content": "Cardiac magnetic resonance imaging (MRI) can provide comprehensive information for heart disease diagnosis and treatment [31], serving as the gold standard for various clinical applications. For example, cardiac cine MRI [3] enables precise evaluation of cardiac function, while cardiac late gadolinium enhancement (LGE) MRI [33] excels in detecting myocardium infarction (MI) and assessing tissue viability. Despite these advantages, the widespread clinical adoption of cardiac MRI lags behind echo cardiography and computational tomography (CT), in large part due to the challenges in image post-processing, e.g., the accurate segmentation of anatomical structures and lesions.\nDeep learning-based methods have significantly improved automatic cardiac MRI segmentation. U-Net [34] and its variants [16,18,30] remain the most widely"}, {"title": "Method", "content": "We propose a novel Versatile image Segmentation model, Verse, through the integration of object query and click query. An overview of VerSe architecture is shown in Fig. 2. In the following, we give details of the proposed method."}, {"title": "Multi-Query Integration as Prompt", "content": "To enable multiple functions within our model, we introduce the Multi-Query Integration mechanism, which guides the model to focus on relevant objects. For the automatic segmentation task, instead of using CLIP-based semantic embeddings [26], we employ learnable object queries \\(X_o\\), as they provide stronger task-specific prior knowledge [14]. Each segmentation target is represented by a small group of learnable query vectors. Specifically, for N target objects, we assign N groups of learnable queries \\(X = [X_{o1}, . . ., X_{oi}, . . ., X_{oN}]\\), where each group \\(X_{oi} \\in R^{M \\times C}\\) consists of M query vectors with C channels. These queries are initialized as random parameters and optimized during training.\nFor the interactive segmentation task based on clicks, inspired by SAM [19], we encode the click locations as sparse positional queries \\(X_c\\). For a positive click set \\(C_p = \\{(x_1, y_1),...,(x_{N_p}, y_{N_p})\\}\\), we use a Point Encoder to encode each click point to a corresponding positive positional query \\(X_{sp}\\), generating \\(N_p \\times C\\) vectors. The same process applies for the negative click set \\(C_n\\) and the corresponding negative positional queries are denoted as \\(X_{sn}\\).\nTo further enhance the effectiveness of click prompts, we propose a novel Semantic Feature Query \\(X_f\\), as illustrated in Fig. 3(a). The image encoder"}, {"title": "", "content": "extracts multiple down-scaling features from the input image. For a feature map \\(F_s\\) with shape \\((H/s, W/s)\\), where s is the down-scaling factor and H, W are the original image height and width, we first map the original click point \\(P_o = (x, y)\\) to its corresponding coordinates in the down-scaling feature map \\(P'_o = (x', y')\\):\n\n\nWe then extract features from a \\((2r + 1) \\times (2r + 1)\\) window around \\(P'_o\\) and apply average pooling to compute \\(f_{pooled}\\):\n\n\nFinally, \\(f_{pooled}\\) is projected into a new feature space via a multilayer perceptron (MLP):\n\n\nwhere \\(X_{fo} \\in R^{1\\times C}\\) represents the semantic feature query for the click at \\(P_o\\). Positive and negative click sets generate semantic feature queries \\(X_{fp}\\) and \\(X_{fn}\\), respectively, at three feature scales s = 2, 4, 8.\n\\(X_o, X_s\\), and \\(X_f\\) collaboratively enable the versatile functions of our model. Specifically, click queries \\(X_c\\) are formed by combining sparse positional queries \\(X_s\\), and semantic feature queries \\(X_f\\). Leveraging multi-query integration, VerSe flexibly supports three working modes:\n\u2022 Mode-1: Automatic segmentation directed by object queries \\(X_o\\);\n\u2022 Mode-2: Interactive refinement of the initial segmentation from Mode-1 by combining object queries \\(X_o\\) and click queries \\(X_c\\);"}, {"title": "Foreground-Background Masked Attention", "content": "After obtaining all queries, we update the image features by attending to the multiple queries. As shown in Fig. 3(b), we use foreground-background masked attention [9,10] to guide multiple queries in focusing on the target-related image features, as well as gathering background information. Unlike [10], we explicitly separate the foreground target from the background using foreground and background click prompts. Specifically, we define positive click queries \\(X_p = \\{X_{sp}, X_{fp}\\}\\) and negative click queries \\(X_n = \\{X_{sn}, X_{fn}\\}\\). To ensure \\(X_p\\) and \\(X_n\\) have the same size of \\(N_1 \\times C\\), we add dummy points to pad \\(X_p\\) and \\(X_n\\) when click number is smaller than \\(N_1\\). In our implementation, we set \\(N_1 = 24\\). Our foreground-background masked cross-attention is computed as:"}, {"title": "", "content": "where l is the layer index of the transformer decoder shown in Fig. 2 and Fig. 3(b). \\(Q_l\\) is the learned linear transformation of \\(X^*_l\\), where * denotes different query types, i. e., object, positive and negative click queries. \\(K_l, V_l\\) are the learned linear transformations of image features \\(F_l\\). \\(M_{pl}, M_{nl} \\in \\{0, -\\infty\\}^{N_1 \\times H_l W_l}\\) are used to control the foreground-background attention masking. At the l-th layer, we first generate a mask prediction \\(M_l\\) from the pixel features \\(F_{l-1}\\) of the previous (l \u2013 1)-th layer, using a simple mask decoder followed by proper mask resizing. Then, \\(M_{pl}\\) and \\(M_{nl}\\) at feature location (x, y) are calculated as:\n\n\nNext, \\(X'_{ol}, X'_{pl}, X'_{nl}\\) are processed separately through self-attention and query feedforward networks (FFN). The resulting queries are concatenated along the query number dimension, then transformed into the Key and Value spaces using learned linear transformations. A cross-attention layer, with image features as the Query, followed by a pixel FFN, updates the image features, which are finally used to generate the segmentation mask."}, {"title": "Residual Connection with Different Scales", "content": "In our multi-scale transformer decoder, the integrated queries forward through different decoder layers, continuously interacting with image features across scales. To enhance the interaction between image features and integrated queries across various scales, we adopt a residual [17] resampling connection similar to the U-Net [34] decoder. For the image feature map \\(F_l\\) at layer l, it's first resampled to align with the resolution of \\(F_{l+1}\\) at next layer l + 1:\n\n\nHere, Resample(\u00b7) can involve upsampling or downsampling depending on the change in scale. Next, we apply a convolutional layer to calculate the residual Res(\\(F_l\\)) before adding it to the feature at the next layer. The updated feature map at scale l + 1 is computed as:\n\n\nA similar operation is applied to semantic feature queries \\(X_f\\). By iteratively resampling, computing residuals, and updating features at each scale, the model effectively integrates multi-scale context, enhancing the interaction between features and queries across scales."}, {"title": "Implementation Details", "content": "Image Encoder. We utilize UTNet [16] as the image encoder, which is specifically designed for cardiac MRI segmentation. We extract multi-scale features at"}, {"title": "Experiments", "content": "Datasets. We conducted experiments on nine publicly available datasets, including seven cardiac MRI datasets and two out-of-distribution medical imaging datasets, as summarized in Table 1. While 3D image volumes are provided in these datasets, we performed all experiments on 2D image slices, excluding slices without any instances. Our combined cardiac training set includes three types of cardiac MRI data: (1) Balanced Steady-State Free Precession (bSSFP) sequences, which focus on segmenting the left ventricle (LV), right ventricle (RV), and myocardium wall (Myo); (2) T2-weighted (T2) sequences, which highlight myocardial edema; and (3) LGE sequences, which highlight myocardial scar. For the M&Ms-2 dataset, we utilized only long-axis (LA) cine MRI images, to enhance data diversity, while other bSSFP datasets were acquired as short-axis images. For out-of-distribution evaluation, we tested our method on OAIZIB [1], a knee MRI dataset acquired with Double Echo Steady State (DESS) sequence, and BraTS [2], a brain T2-weighted MRI dataset. These datasets allow us to assess the generalizability of our model to different domains.\nBaseline Methods. For automatic image segmentation, we compared our method with nnU-Net [18], TransUNet [8], UTNet [16], Swin-Unet [6] and MedFormer [15], all of which are widely recognized and applied in medical image segmentation. For interactive image segmentation, we compared our method with RITM [35], iSegformer [29], SimpleClick [28] and SegNext [27]. Among these, SimpleClick and SegNext are the state-of-the-art (SOTA) methods for interactive segmentation, demonstrating exceptional performance across various benchmarks.\nEvaluation Metrics. We use the 2D Dice score to evaluate the segmentation accuracy, which is a standard measure in medical image segmentation [4,16]. We use the Number of Clicks (NoC) metric to assess the number of clicks required to reach a specified Dice score. Target Dice scores are set at 80%, 85%, 90%, and 95%, denoted as Noc80, NoC85, NoC90, and NoC95, respectively. Each instance allows a maximum of 20 clicks. In addition, we evaluated segmentation quality using the average Dice score of all instances at a fixed number of clicks, Dice(n)."}, {"title": "Results", "content": "Comparison with Automatic Models. Table 2 compares our proposed VerSe framework with several specialized segmentation models designed for specific tasks. Despite lacking any specialized design to enhance automatic segmentation, Verse achieves competitive performance on large-scale datasets such as ACDC and M&Ms, demonstrating its robustness and adaptability across diverse segmentation scenarios. However, on more challenging datasets like MyoPS++ (T2), where all models struggle to meet clinical requirements, due to the dataset's inherent complexity, the interactive capabilities of VerSe become particularly advantageous. By enabling efficient user-driven refinements through click-based interactions, VerSe provides a practical solution to enhance segmentation accuracy in challenging cases, bridging the gap toward clinical applicability.\nComparison with Interactive Models. Table 2 highlights the performance comparison between Verse (Mode-3) and previous SOTA interactive models."}, {"title": "", "content": "Verse (Mode-3) consistently achieves the best Dice scores and lower interaction costs among six out of seven datasets. Notably, on larger datasets, such as ACDC, M&Ms, and LAScarQS++, VerSe achieves the best performance across all metrics, significantly outperforming existing methods. For example, on the M&Ms dataset, which has the largest number of instances, VerSe achieves a Dice(1) score of 89.757%, significantly surpassing SimpleClick (85.335%) and SegNext (85.197%). This demonstrates VerSe's ability to handle complex and large-scale data efficiently. Furthermore, as illustrated in Fig. 4, Verse not only achieves faster convergence but also demonstrates steady improvements in segmentation accuracy as the number of clicks increases, highlighting the efficiency of its interactive prompting mechanism."}, {"title": "", "content": "On smaller and more challenging datasets, such as MyoPS++ (LGE) and MyoPS++ (T2), VerSe continues to show significant advantages. On the My-OPS++ (LGE) dataset, VerSe achieves a Dice(20) of 95.024%, far exceeding the ~ 85% average of competing methods. Similarly, on the MyoPS++ (T2) dataset, while SimpleClick initially leads in the first 10 clicks, VerSe demonstrates more sustained improvements, ultimately achieving the best Dice (20) of 94.183% and a NoC95 of 12.506. These results highlight VerSe's ability to efficiently utilize user interactions to refine segmentation results, even in difficult scenarios.\nOverall, VerSe (Mode-3) sets a new benchmark in interactive segmentation, by achieving SOTA performance with minimal interaction costs. Its robust performance across both large-scale datasets and complex segmentation tasks underscores its adaptability and effectiveness in diverse cardiac MRI applications.\nVerse (Mode-2) vs. Verse (Mode-3). Table 2 presents a detailed comparison between VerSe operating in Mode-2 and Mode-3 across various datasets. Verse (Mode-2) achieves superior performance in 4 out of 7 datasets compared to Verse (Mode-3), particularly in the ACDC and M&Ms datasets, where the initial automatic segmentation effectively reduces user interaction while improving accuracy. In contrast, Verse (Mode-3), which relies solely on interactive segmentation without automatic initialization, excels on datasets where precise"}, {"title": "Out-of-Distribution Evaluation", "content": "Out-of-Distribution Evaluation. We trained all interactive segmentation models using cardiac MRI datasets, while we evaluated their performance on out-of-distribution datasets. The results are summarized in Table 3. On the BraTS dataset, Verse achieves a remarkable Dice score of 94.492% with just 10 clicks, significantly outperforming other models. In addition, it delivers the highest Dice(20) score of 96.811% and demonstrates the best annotation efficiency. On the OAIZIB dataset, Verse achieves the lowest NoC80 value of 11.787, highlighting its superior efficiency. These results collectively showcase the robust generalization capabilities of Verse across diverse medical imaging domains."}, {"title": "Ablation Studies", "content": "In this section, we conducted ablation studies to demonstrate the effectiveness of our model design. All of our ablation studies were conducted on the ACDC dataset with Verse working in Mode-3 only. The results are shown in Table 4. (1) In model A1, we eliminated the semantic feature queries \\(X_f\\) from click queries \\(X_c\\). (2) In model A2, we eliminated the masked attention branch (see the rightmost hybrid attention blocks in Fig. 3(b)) of negative click queries but combined positive and negative click query masked attention branches as a single one. (3) In model A3, we eliminated the residual connections across different scales. The results show that each of the three model components are necessary for the high performance of our model."}, {"title": "Conclusion", "content": "In this work, we propose Verse, a novel framework that unifies automatic and interactive segmentation modes, through a multi-query integration mechanism. By"}, {"title": "", "content": "effectively leveraging both object and click queries, VerSe achieves state-of-the-art performance in both segmentation accuracy and interaction efficiency. Our experiments on seven cardiac MRI datasets and two out-of-distribution medical imaging datasets demonstrate the robustness, efficiency, and generalizability of the proposed method. VerSe not only bridges the gap between automatic and interactive segmentation but also sets a new benchmark for versatile image segmentation tasks. Future directions include extending the framework to handle natural data, improving scalability for larger multi-modal datasets, and enhancing interpretability for clinical adoption."}]}