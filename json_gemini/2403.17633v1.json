{"title": "UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps", "authors": ["Maciej K Wozniak", "Mattias Hansson", "Marko Thiel", "Patric Jensfelt"], "abstract": "In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.", "sections": [{"title": "1 Introduction", "content": "LiDAR-based perception systems are essential for the safe navigation of autonomous vehicles such as self-driving cars [20] or mobile robots [41]. A key challenge is the reliable detection and classification of objects within a vehicle's environment [54]. State-of-the-art (SOTA) 3D object detection methods highly depend on quality and diversity of the datasets used for training, but also on how closely these datasets reflect real-world conditions during inference. Acquiring and annotating such data remains significant technical and practical challenge, being both time-consuming and labor-intensive. This presents a major obstacle in development and deployment of 3D object detection models at scale.\n\nA crucial technique to mitigate these challenges is domain adaptation (DA). DA addresses the problem of adapting models trained on a source domain with ample labeled data to a target domain where labels might be scarce (as in semi-supervised DA) or completely unavailable (as in unsupervised DA \u2013 UDA) [8, 3, 36, 44]. UDA methods can substantially improve model performance in new, unfamiliar, or changing environments without the need to label new training samples. In the context of autonomous vehicles, discrepancies between source and target domains, often referred to as domain shift or domain gap, can be caused by changes in weather conditions [22], variations in object"}, {"title": "2 Related Work", "content": "LiDAR-based 3D Object Detection: The field of 3D object detection has evolved significantly over the years, with approaches for images [16, 47], point clouds [37, 60, 61, 10, 66, 57, 67] and multi-modal fusion [31, 1, 25, 28] proposed recently. PointNet [33] proposes an input invariant way or extracting features from point sets. Voxel-net [67] and PointPillars [20] propose partitioning the point clouds into a 3D voxel grid and pillars, respectively. Recently developed Centerpoint [63] combines voxel or pillar backbones with a center-based detection head. IA-SSD [66] proposes a single-stage point-based 3D detector that performs on par with other, much slower methods.\n\nUnsupervised Domain Adaptation for LiDAR-based 3D Object Detection: Numerous research projects have focused on adapting 3D object detection models between high-resolution LiDAR datasets, ranging from 128 to 32 layers, typically found in self-driving scenarios [48, 56, 58, 59, 35, 43, 50, 32, 19, 42, 24, 64, 15]. However, little attention has been given to adapting these models to sparser LiDAR setups, which are common in small robotic platforms [38]. Notably, Peng et al. [32] and Wang et al. [49] focus specifically on model adaptation between LiDARs with 64 layers and 32 layers, and vice versa. Their results indicate that performance drastically decreases when models trained on 64-layer LiDAR data are adapted to a sparser 32-layer target domain. A similar pattern is observed in ST3D++[59], which leverages pseudo-labeling to generate labels for the target domain. MS3D++ [42] leverages multiple pre-trained detectors to achieve more accurate predictions, but again does not focus on very sparse data. Additionally, Cheng et al.[4] shown that very few UDA methods maintain consistent performance across different object classes, and the majority of the methods adapt only between the Car/Vehicle class, however, they do not focus on sparse LiDAR or different operating environments.\n\nRecent works, such as DTS [14], which uses a student-teacher architecture with feature-level graph matching, show performance degradation with 16-layer LiDAR, although they do not report quantitative results. LiDAR Distillation (L.D.) [51] focuses exclusively on adapting models to sparser domains by using regression loss to minimize"}, {"title": "3 Method", "content": "Most of the methods discussed above rely on a student-teacher approach, meaning they require a model pre-trained on the source domain to distill knowledge from the teacher to the student and/or to generate pseudo labels. In contrast, our method employs an adversarial learning approach. Consequently, we do not depend on a pre-trained teacher model to generate, for example, pseudo labels, which are not necessarily high-quality substitutes for ground truth and may even result in decreased model performance, as we later demonstrate in Section 5.\n\nAdversarial Domain Adaptation: Adversarial domain adaptation (ADA) relies on a discriminative network structure that leverages domain discriminators to achieve domain invariance [13, 23, 27, 65, 45]. The GRL [8] reverses gradients linking the feature extractor and the discriminator. This end-to-end training simultaneously employs source and target data, aligning features across domains while minimizing detection loss in the source data. Following this, Chen et al. [3] introduced a two-stage adaptation method, incorporating image-level and instance-level adaptation. Saito et al. [34] proposed strong and weak adaptation, emphasizing local object feature alignment. He and Zhang [11] employed multiple GRLs for global adaptation at different convolutional layers. Xu et al. [55] introduced categorical regularization between image- and instance-level domain classification, using a regularization loss based on Euclidean distance. Li et al. [22] introduced AdvGRL, replacing the constant hyperparameter A with an adaptive $\\lambda_{adv}$ to address challenging training samples. ADA has found a lot of use in object detection, classification, or segmentation in image space [3, 8, 65, 46, 27, 17].\n\nIn our work, we concentrate on adversarial domain adaptation in LiDAR-based 3D object detection, distinct from 2D methods. Our approach tackles the unique challenges of spatial data handling, arising from different point cloud densities and operating environments, through data augmentation and method design. We address it particularly with our discriminator utilizing features masked by 3D bounding boxes. Extracting these features from sparser, irregularly spaced point clouds is significantly more challenging than from pixel grids. This often requires transitioning to representations like Bird-Eye-View (BEV) feature space, as illustrated in Fig. 3. Additionally, working with point clouds involves making predictions based on partially missing, occluded, or incomplete data (as we can observe in qualitative results in supplementary materials), caused by low angular resolution, potentially resulting in a small number of points per object, even at short distances to the object. In the realm of UDA for segmentation, existing methods employ adversarial approaches with marginal alignment or non-adversarial methods [62, 46, 17, 26, 7]. Our approach utilizes conditional alignment, which we found to yield superior performance for LiDAR-based 3D object detection (see Section 5.1 for a comparison)."}, {"title": "3.1 Problem Formulation", "content": "Suppose that Q is a point cloud, and X is its feature representation learned by the feature extraction network $f_{ef}$. The detection head $h_{by}$ uses these features to predict P(Y|X), where Y = (y,b) are the category labels y and bounding boxes b. Q is sampled from the source domain $D_s$ and target domain $D_t$. The objective is to learn a generalized $f_{ef}$ and $h_{by}$ between domains such that $P(Y_s, X_s) \\approx P(Y_t, X_t)$. Since P(Y, X) = P(Y|X)P(X), the domain adaptation task for LiDAR-based object detectors is to align the marginal probability distributions $P(X_s)$ and $P(X_t)$ as well as the conditional probability distributions $P(Y|X_s)$ and $P(Y_t|X_t)$. Note that target labels $Y_t$ are not available during training, thus we must use unsupervised domain adaptation."}, {"title": "3.2 Method Overview", "content": "Marginal adaptation, i.e., aligning P(X), overlooks category and position labels, which can lead to uneven and biased adaptation. This may reduce the target domain's discriminative ability. Aligning P(Y|X) places direct emphasis on the task-specific outcomes (class labels and bounding boxes) in relation to the features. By focusing on P(Y|X), we hypothesize that the adaptation process also becomes more robust to variations in feature distributions across domains, concentrating on the essential task of detecting objects. Furthermore, in Fig. 4, we show that the distribution of points in different categories per object varies significantly between the datasets due to LiDAR density, position and operating environment, while the vehicle size slightly varies, depending on the dataset country of origin (see Figs. 5a to 5c). While we chose to use conditional alignment, we compare our method with different alignment strategies: marginal and joint distribution alignment in Section 5.1.\n\nFig. 2 provides a schematic overview of our method UADA3D. In each iteration, a batch of samples Q from source $D_s$ and target $D_t$ domain is fed to the feature extractor $f_{ef}$. Next, for each sample, features are extracted, and fed to the detection head $h_{by}"}, {"title": "3.3 Feature Masking", "content": "Feature masking plays a crucial role in predicting the domain based on specific object features. Masking enables the model to focus solely on the features corresponding to each instance, thus enhancing the relevance and accuracy of the domain prediction. In Fig. 3, we show how features extracted from a point cloud Q are masked and used for"}, {"title": "3.4 Conditional Distribution Alignment", "content": "The conditional distribution alignment module shown in Figs. 2 and 3, has the task of reducing the discrepancy between the conditional distribution $P(Y|X_s)$ of the source and $P(Y|X_t)$ of the target. As we highlighted in Section 3.2, shown in Fig. 4 and later discussed in Section 4.1, we can see a large difference between how objects from each category appear in different domains. Thus, instead of having one discriminator, we use K = 3 class-wise domain discriminators $g_{\\theta_{DP,k}}$, corresponding to vehicle, pedestrian, and cyclist classes. The conditional distribution alignment module is trained using the least-squares loss function:\n\n$L_c = \\frac{1}{N} \\sum_{n=1}^{N} \\hat{y}_{k,n} (g_{\\theta_{DP,k}}(x_n, b_n) - d)^2$   (2)\n\nwhere N is the number of labels, $\\hat{y}_{k,n}$ corresponding class confidence of instance n and $\\odot$ is element-wise multiplication. The loss is backpropagated to the discriminators (line 9 in Algorithm 1). Next, $L_c$ is backpropagated through GRL to the detection head $h_{by}$ and the feature extractor $f_{ef}$."}, {"title": "3.5 Data Augmentation", "content": "Differences between LiDAR domains include different densities and object sizes. We use downsampling, a commonly used augmentation approach since the domain gap can be partially remedied by reducing LiDAR layers of the source data to 16 or 32 to better match the target domain LiDAR data as highlighted in [53, 6]. LiDAR-CS [6], Waymo [39], and nuScenes [2] datasets contain vehicle sizes that correspond to the large vehicle sizes found in the USA, while our robot and KITTI [9] are collected in Europe where vehicles are generally smaller. Random object scaling (ROS) [59] is applied in the source domain to address this vehicle size bias. While previous UDA methods on LiDAR-based 3D object detection often apply domain adaptation only to a single object category, we consider it a multiclass problem. Thus, we chose ROS with different scaling intervals for the three classes, described in detail in supplementary materials."}, {"title": "4 Experiments Setup", "content": "We compare performance of IA-SSD [66] and Centerpoint [63] on large number of UDA scenarios using our method, UADA3D, against that of other SOTA unsupervised domain adaptation approaches (ST3D++ [59], DTS [14], L.D. [51], and MS3D++ [42]). In IA-SSD, the adaptive discriminator network is made of fully-connected layers that operate on down-sampled point features. In Centerpoint, the discriminator makes use of 2D convolutions with inputs from Bird's Eye View (BEV) feature maps. The most prominent distinction between the two networks is the point-based and view-based representations, which are handled by MLPs and 2D convolutions respectively. The ability to adapt both of these models between the domains shows the modularity of our solution and adaptivity to different methods. With UADA3D, Centerpoint is trained"}, {"title": "4.1 Datasets", "content": "We use five datasets: LiDAR-CS [6] that provide multiple LiDAR resolutions (we use VLD-64 \u201cCS64\u201d, VLD-32 \u201cCS32\", and VLP-16 \u201cCS16\u201d), KITTI [9] \u201cK\u201d (with HDL-64E), Waymo [39] \u201cW\u201d(HDL-64E), nuScenes [2] \u201cN\u201d (VLD-32) and data collected with the last mile delivery robot \u201cR\u201d(with VLP-16) in different locations in Europe, further described in supplementary materials.\n\nFirst, we focus on UDA between autonomous driving datasets, from denser to sparser LiDARs, since this is where the other SOTA methods perform the worst. Even though these sensors operate similarly, differences arise from the point-cloud density as well as LiDARs azimuth and elevation angles. Second, we adapt to our robot data, which is particularly challenging due to different LiDAR positions and densities on the mobile robot, as well as the operating environment (sidewalk vs. street). Finally, we compare the performance of adaptation from sparser to denser domains between autonomous driving datasets.\n\nIn Fig. 5 we can see how big the gap between the self-driving datasets and robot data is, especially in distance to the objects (nuScenes and Waymo show similar distribution to LiDAR-CS and KITTI). The distance to the objects encountered in the robot data, seen in Figure 5f, shows that the dataset only contains objects closer than 15m, with a majority of pedestrians and cyclists closer than 10m and all vehicles in the 10 \u2013 15m range, since the robot drives on a sidewalk. In contrast, in the self-driving cars dataset Fig. 5d and Fig. 5e, a lot of objects are between 20 \u2013 30m away. Consequently, the average number of points per object (Fig. 4), is very different between the datasets. This is causing the instances to significantly differ between these domains. While the different vehicle sizes are often addressed in 3D domain adaptation approaches [59], to the best of our knowledge, differing number of points per object has not been examined by other UDA works before.\n\nKITTI dataset is limited by having labels only in the front camera field of view (FOV), while other datasets feature labels in a 360-degree FOV. This presents a significant challenge for domain adaptation, typically leading researchers to adapt to the KITTI datasets, rather than from them, and when they do, it results in marginal improvements or even a decrease in performance [51]."}, {"title": "4.2 Evaluation metrics", "content": "We report $mAP^{3D}$ and $mAP^{BEV}$, closed gap and change in mAP. mAP is the mean average precision over the three classes: Vehicle, Pedestrian, and Cyclist. Closed gap [61] is calculated as $\\frac{mAP_{model}-mAP_{source}}{mAP_{oracle}-mAP_{source}} \\times 100\\%$ and tells how close we are to a model trained on the target domain (oracle)."}, {"title": "5 Results", "content": "In Table 1 and Table 2, we compare our method with SOTA on a large number of adaptation scenarios between source and target (S \u2192 T) data (further tests are included in supplementary materials). In Table 1 we focus on 9 different dense to sparse scenarios for the two models: Centerpoint [63] and IA-SSD [66]. Our method, UADA3D, outperforms other SOTA approaches in most cases achieving much higher improvements, especially when it comes to the larger domain gaps (adapting models towards mobile R(obot) or W\u2192 N). By analyzing Change and Closed Gap columns, we can observe that Centerpoint often shows higher adaptability and substantially higher improvement $mAP^{3D}$ and $mAP^{BEV}$ than IA-SSD. That may come from the fact that for IA-SSD, we have to fix a specific number of sampling points (explained in supplementary material), which makes the model less flexible when adapting across different LiDAR densities and patterns. Furthermore, even though other SOTA methods may outperform UADA3D on individual classes (only Cyclist in adaptation towards R), they perform worse in others categories (Fig. 6). This demonstrates that UADA3D offers better generalization across different detectors and classes.\n\nWe observe that our method handles diverse domain shifts effectively when adapting between autonomous driving datasets (Waymo, KITTI, nuScenes), simulation data (LiDAR-CS), and robot data (R). As mentioned in Section 3, UADA3D does not need a pre-trained teacher model, as all the other approaches do. Instead, we can directly train the domain-adapted model, leveraging the GRL functionality which creates domain-invariant features. This allows our method to successfully train high-performing models on unlabeled target data, without depending on pseudo labels. We can observe in Table 1 that some of the methods perform even worse than the source-only approaches when tested on adaptation towards more challenging scenarios (e.g K \u2192 R), failing to generate accurate pseudo labels or distill teacher knowledge. Additionally, it is important to note that UADA3D never achieves lower performance than source-only models, on the adaptation task, regardless if the domain gap is big (e.g., K\u2192 R, W \u2192 N) or small (e.g., CS32 \u2192 CS16). Notably, K\u2192R or CS16\u2192R adaptations appear particularly"}, {"title": "5.1 Ablation Studies", "content": "We explore the impact of various factors on our method's performance. First, we ask the question of which probability distribution alignment is the most beneficial for UADA3D. Next, we investigate the impact of discriminator design and analyze GRL parameters. Finally, we integrate other self-learning components to enhance our method further. Further details, including ablations and hyperparameter studies, can be found in the supplementary materials.\n\nProbability Distribution Alignments: In these ablation studies, we explore the effectiveness of other probability distribution alignment strategies, shown in Fig. 7. In UADA3D with the marginal distribution alignment, $UADA3D_{\\mathcal{L}m}$, the discriminator gradient is backpropagated only to the feature extractor with loss $L_m$, where $L_m = \\sum_{n=1}^{N} d \\cdot log(g_{\\theta_{Dm}}(X_n)) + (1-d) log(1 - g_{\\theta_{Dm}} (X_n))$, where d is 0 for source and 1 for target domains and $g_{\\theta_{Dm}}$ denotes the marginal discriminator network. In UADA3D (i.e., $UADA3D_{cc}$) the discriminator gradient is backpropagated to the detection head through the whole model. $UADA3D_{\\mathcal{L}mc}$ combines marginal and conditional alignment with $L_{mc} = (L_m + L_c)$, where $L_c$ is given in Eq. (2). $UADA3D_{\\mathcal{L}m}$ uses feature maps directly to predict the domain and calculate the loss, while UADA3D employs masked features with class labels and bounding boxes. Conditional probability distribution alignment consistently yields high-quality outcomes, but there are cases where it is"}, {"title": "6 Conclusions and Limitations", "content": "In this paper, we introduce UADA3D, a novel approach tailored for challenging UDA scenarios, specifically addressing sparser LiDAR data and mobile robots. Through adversarial training using gradient reversal, UADA3D effectively navigates diverse environments, ensuring precise object detection and achieving state-of-the-art performance across various domain adaptation scenarios. This way, the large number of existing"}]}