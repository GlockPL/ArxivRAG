{"title": "Bellman Error Centering", "authors": ["Xingguo Chen", "Yu Gong", "Shangdong Yang", "Wenhao Wang"], "abstract": "This paper revisits the recently proposed reward centering algorithms including simple reward centering (SRC) and value-based reward centering (VRC), and points out that SRC is indeed the reward centering, while VRC is essentially Bellman error centering (BEC). Based on BEC, we provide the centered fixpoint for tabular value functions, as well as the centered TD fixpoint for linear value function approximation. We design the on-policy CTD algorithm and the off-policy CTDC algorithm, and prove the convergence of both algorithms. Finally, we experimentally validate the stability of our proposed algorithms. Bellman error centering facilitates the extension to various reinforcement learning algorithms.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has driven transformative advances in strategic decision-making (e.g., AlphaGo (Silver et al., 2016)) and human-aligned large language systems (e.g., ChatGPT via RLHF (Ouyang et al., 2022; Carta et al., 2023; Dai et al., 2024; Guo et al., 2025)). However, these breakthroughs incur prohibitive computational costs: AlphaZero requires billions of environment interactions, while RL-based training of state-of-the-art language models demands millions of GPU hours (Patterson et al., 2021). Such challenges necessitate urgent innovations in RL efficiency to enable scalable AI development.\nTo tackle long-term and continuous reinforcement learning problems, two primary maximization objectives have been proposed: the average reward criterion and the discounted reward criterion. In the context of average reward, Schwartz (1993) introduced R-Learning, employing an adaptive method to estimate average rewards. Das et al. (1999) proposed SMART, which focuses on estimating average rewards directly. Abounadi et al. (2001) introduced RVI Q-learning, utilizing the value of a reference state to enhance the learning process. Yang et al. (2016) proposed CSV-learning, which employs a constant shifting values to improve convergence. Wan et al. (2021) removed reference state of RVI Q-learning, proposed differential Q-learning and differential TD learning.\nRegarding discounted rewards, Perotto & Vercouter (2018); Grand-Cl\u00e9ment & Petrik (2024) highlighted that, under certain conditions, such as with a large discount factor, Blackwell optimality can be achieved. Sun et al. (2022) demonstrated that reward shifting can effectively accelerate convergence in deep reinforcement learning. Schneckenreither & Moser (2025) introduced near-Blackwell-optimal Average Reward Adjusted Discounted Reinforcement Learning using Laurent Series expansion of the discounted reward value function. Naik et al. (2024); Naik et al. (2024) proposed the concept of reward centering, designing simple reward centering and value-based reward centering, and proved the convergence of tabular Q-learning with reward centering. Applying reward centering in tabular Q-learning, Q-learning with linear function approximation and Deep Q-Networks (DQN) have produced outstanding experimental results across all these approaches (Naik et al., 2024).\nHowever, three issues remain unresolved: (1) Naik et al. (2024); Naik et al. (2024) pointed out that while reward centering can be combined with other reinforcement learning (RL) algorithms, the specific methods for such integration are not straightforward or trivial. The underlying mechanisms of reward centering warrant further investigation. (2) Currently, there is only a convergence proof for tabular Q-learning with reward centering, leaving the convergence properties of reward centering in large state spaces with function approximation still unknown. (3) If the algorithm converges, what solution it will converge to is also an open question.\nIn response to these three issues, the contributions of this paper are as follows: (1) We demonstrated that value-based reward centering is essentially Bellman error centering. (2) Under linear function approximation, the solution of Bellman error centering converges to the centered TD fixpoint. (3) Building on Bellman error centering, we designed centered temporal difference learning algorithms, referred to as on-policy CTD and off-policy CTDC, respectively. (4) We provide convergence proofs under standard assumptions."}, {"title": "2. Background", "content": "Reinforcement learning agent keeps interactions with an environment. In each interaction t, it observes the state St, takes an action at to influence the environment, and obtains an immediate reward rt+1. Consider an infinite-horizon Markov Decision Process (MDP), defined by a tuple (S, A, R, P), where S = {1,2, . . ., N } is a finite set of states of the environment; A is a finite set of actions of the agent; R: S\u00d7A\u00d7S \u2192 R is a bounded deterministic reward function; P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability distribution (Sutton & Barto, 2018).\nA policy is a mapping \u03c0 : S \u00d7 A \u2192 [0,1]. The goal of the agent is to find an optimal policy \u03c0* to maximize the expectation of a discounted cumulative rewards over a long period. State value function V\" (s) for a stationary policy \u03c0 is defined as:"}, {"title": "2.1. Markov Decision Process", "content": "00\nV\" (s) = \u0395\u03c0\u2211rt+1|So = s],\nt=0\nwhere \u03b3\u2208 (0,1) is a discount factor. We have Bellman equation for each state:\nV\" (s) = \u0395\u03c0[r + yV*(s')],\nwhere s' is the succesor state of state s. In vector form,\nV\" = R\" + YP\"V\"\n=TV\u00ae,\nwhere R\u2122 (s) = \u03a3\u03b1\u03c0(\u03b1\u03c2) \u03a3\u03b5, P(s, a, s')R(s, a, s'), transition probability matrix of states [PT]s,s' = \u03a3\u03b1\u03c0(\u03b1\u03c2) \u03a3\u03b5 P(s, a, s'), and T\u2122 is Bellman prediction operator. V\u2122 is the fixpoint solution of V = TV.\nTo deal with large scale state space, linear value function for state s \u2208 S is defined as:"}, {"title": "2.2. Reward Centering", "content": "m\nVo(s) := 0 (s) = \u2211\u03b8\u03ad\u03c6\u03b5(\u03c2),\ni=1\nwhere 0 := (01, 02,...,0m)T \u2208 Rm is a parameter vector, $ := ($1,2,...,\u00a2m) )\u2208 Rm is a feature function defined on state space S, and m < |S| is the feature size.\nHowever, in the parameter space, Ve cannot be guaranteed to be equal to T\u2122 Vo. We typically solve for the TD fixpoint (Sutton et al., 2008; 2009), as follows:\nVe = IT\u2122 Ve,\nwhere projection \u03a0 = \u03a6(\u03a6\u012aD\u03a6)-1\u00deTD, D is a diagonal matrix of distribution vector d, each element de represents the distribution of state s. In expectation form, it is equal to\n\u0395\u03c0 [\u03b4\u03c6] = 0,\nwhere TD error d = r + yV\u0259 (s') \u2013 Vo(s).\nIn on-policy learning, the target policy and the behavior policy u are the same, and the experience is sampled as (St, at, rt+1, St+1). The update rule of the on-policy TD learning (Sutton et al., 2016) is\n0t+1 = \u03b8\u03b5 + \u03b1\u03b4\u03b5\u03c6\u03c4,\nwhere at \u2208 (0, 1) is a learning stepsize, and dt = rt+1 + Vo(St+1) - Ve(St).\nIn off-policy learning, the target policy \u03c0 and the behavior policy u are different. The update rule of the off-policy TD learning (Sutton et al., 2016) is\n0t+1 = 0 + at\u03c1\u03b9\u03b4\u03b5\u03c6\u03c4,\nwhere pt = \n\u03c0(at St)\n\u03bc(at St)\nIn on-policy learning, Naik et al. (2024) proposed simple reward centering. The update rule is\nVt+1(St) = Vt(st) + atdt,\nwhere the new TD error \u03b4\u03b5 is\nSt = dt - rt = rt+1 - \u00eft + \u00a5Vt(st+1) \u2013 Vt(st),\nand it is updated as:\nrt+1 = rt + Bt(rt+1 - rt),\nwhere \u1e9et \u2208 (0, 1) is a learning stepsize."}, {"title": "3. Bellman Error Centering", "content": "Centering operator C for a variable x(s) is defined as follows:\nCx(s)=x(s) \u2013 E[x(s)] = x(s) - \u2211dsx(s),\nS\nwhere ds is the probability of s. In vector form,\nCx = x - E[x]1\n= x xd1,\nwhere 1 is an all-ones vector. For any vector x and y with a same distribution d, we have\nC(x + y) = (x + y) \u2013 (x + y)+d1\n= x - xd1 + y \u2013 y\u0142d1\n= Cx + Cy."}, {"title": "3.1. Revisit Reward Centering", "content": "The update (11) is an unbiased estimate of the average reward with appropriate learning rate \u1e9et conditions.\nThat is\nn\nrt \u2248 lim\n\u0395\u03c0[r].\nn\u2192\u221e n\nt=1\nrt - Tt ~ rt\nn\n1\nn\u2192\u221e n\nt=1\nlim \u0395\u03c0[rt] = Crt.\nThen, the simple reward centering can be rewrited as:\nVt+1(St) = Vt(St)+at[Crt+1+yVt(St+1)-Vt(st)].\nTherefore, the simple reward centering is, in a strict sense, reward centering.\nBy definition of dt = dt \u2013 \u00eft, let rewrite the update rule of the value-based reward centering as follows:\nVt+1(St) = Vt(St) + atpt(dt - rt),\nwhere it is updated as:\nt+1 = t + \u1e9etpt(St \u2013 t).\nThe update (21) is an unbiased estimate of the TD error with appropriate learning rate \u1e9et conditions.\nrt \u2248 \u0395\u03c0[\u03b4\u03b5].\nThat is\nSt - rt \u2248 Cdt.\nThen, the value-based reward centering can be rewrited as:\nVt+1(St) = Vt(st) + atptCdt.\nTherefore, the value-based reward centering is no more, in a strict sense, reward centering. It is, in a strict sense, Bellman error centering.\nIt is worth noting that this understanding is crucial, as designing new algorithms requires leveraging this concept."}, {"title": "3.2. On the Fixpoint Solution", "content": "The update rule (24) is a stochastic approximation of the following update:\nVt+1 = Vt + at[TV \u2013 V \u2013 \u0395[\u03b4]1]\n= Vt + at[T\u00aeV \u2013 V \u2013 (TV \u2013 V)Td1]\n= Vt + at [C(TV \u2013 V)].\nIf update rule (25) converges, it is expected that C(TTV\nV) = 0. That is\nCV = CT\u2122 V\n= C(R\" + YP\" V)\n= CR\" + YCP\" V\n= CR\" + y(P\" V \u2013 (PV)d\u201e1)\n= CR\" + y(P\" V \u2013 VT (P\")Td\u201e1)\n= CR\" + y(P\" V \u2013 V\u00afd\u201e1)\n= CR\" + y(P*V \u2013 V\u00afd\u201eP\"1)\n= CR\" + y(P*V \u2013 PV\u00afd\u201e1)\n= CR\" + YP\" (V \u2013 VTd1)\n= CR\" + YP\"CV\nTCV,\nwhere we defined T as a centered Bellman operator. We call equation (26) as centered Bellman equation. And it is centered fixpoint.\nFor linear value function approximation, let define\nCV = HTCVe.\nWe call equation (27) as centered TD fixpoint."}, {"title": "3.3. On-policy and Off-policy Centered TD Algorithms with Linear Value Function Approximation", "content": "Given the above centered TD fixpoint, mean squared centered Bellman error (MSCBE), is proposed as follows:\narg min MSCBE(0)\n\u03b8\n= arg min || TCV - CVo||b\n= arg min ||T\" Ve \u2013 V\u0259 \u2013 (T\"Vo \u2013 Vo)d1||\n= arg min ||T\" V\u0259 \u2013 V\u0259 \u2013 w1||,\n\u03b8\u03c9\nwhere w is is used to estimate the expected value of the Bellman error.\nFirst, the parameter wis derived directly based on stochastic gradient descent:\nWt+1 = wt + \u03b2t(\u03b4\u03b9 \u2013 \u03c9\u03c4).\nThen, based on stochastic semi-gradient descent, the update of the parameter @ is as follows:\n0t+1 = 0 + \u03b1\u03c4 (\u03b4\u03b9 \u2013 \u03c9\u03c4)\u03a6\u03c4\u00b7\nWe call (28) and (29) the on-policy centered TD (CTD) algorithm. The convergence analysis with be given in the following section."}, {"title": "3.4. Off-policy Centered TDC Algorithm with Linear Value Function Approximation", "content": "The convergence of the off-policy centered TD algorithm may not be guaranteed.\nTo deal with this problem", "follows": "narg min MSPCBE(0)\n\u03b8\n= arg min ||IT CV \u2013 CVo||\n= arg min ||II(TCV\u0259 \u2013 CVo)||b\narg min ||II(T\u2122 V\u0259 \u2013 Vo \u2013 w1)||B.\n\u03b8\u03c9\nIn the process of computing the gradient of the MSPCBE with respect to 0"}, {"follows": "n0k+1 = \u03b8\u03ba + \u03b1\u03ba[(\u03b4\u03ba \u2013 \u03c9k)\u03a6k \u2013 \u03a5\u03a6k+1(\u03a6\u03b5\u03c5\u03ba)", "u": ""}]}