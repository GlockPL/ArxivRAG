{"title": "Graph Memory Learning: Imitating Lifelong Remembering and Forgetting of Brain Networks", "authors": ["Jiaxing Miao", "Liang Hu", "Qi Zhang", "Longbing Cao"], "abstract": "Graph data in real-world scenarios undergo rapid and frequent changes, making it challenging for existing graph models to effectively handle the continuous influx of new data and accommodate data withdrawal requests. The approach to frequently retraining graph models is resource intensive and impractical. To address this pressing challenge, this paper introduces a new concept of graph memory learning. Its core idea is to enable a graph model to selectively remember new knowledge but forget old knowledge. Building on this approach, the paper presents a novel graph memory learning framework Brain-inspired Graph Memory Learning (BGML), inspired by brain network dynamics and function-structure coupling strategies. BGML incorporates a multi-granular hierarchical progressive learning mechanism rooted in feature graph grain learning to mitigate potential conflict between memorization and forgetting in graph memory learning. This mechanism allows for a comprehensive and multi-level perception of local details within evolving graphs. In addition, to tackle the issue of unreliable structures in newly added incremental information, the paper introduces an information self-assessment ownership mechanism. This mechanism not only facilitates the propagation of incremental information within the model but also effectively preserves the integrity of past experiences. We design five types of graph memory learning tasks: regular, memory, unlearning, data-incremental, and class-incremental to evaluate BGML. Its excellent performance is confirmed through extensive experiments on multiple real-world node classification datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, a multitude of pivotal datasets in practical applications are expressed as graphs [1], encompassing diverse domains such as social networks [2], [3], traffic networks [4], [5], biological networks [6], [7], recommendation systems [8], [9]. Graph learning models attract increasing attention and demonstrate robust data mining and representation learning capacities. However, graph learning faces the challenge of promptly assimilating new information while discarding obsolete knowledge that has been invalidated. This is attributed to the fact that real-world data often exhibits dynamic flow characteristics, wherein a continuous influx and withdrawal of data occur. As depicted in Fig. 1 (A), the flow of data across the graph frequently induces alterations in the overarching topology, thereby exacerbating the difficulties encountered by graph models. Consequently, the objective of this study is to explore and empower graph learning models with the capability to \u201cmemory\u201d based on the dynamic graph. This paradigm is named as graph memory learning (GML). Existing graph lifelong learning (GLL) [10], [11] and graph unlearning (GUL) [12], [13] approaches have undertaken explorations into the memory learning of graph models from limited perspectives. GLL aims to address the challenges of continuous data increments, adapts to new knowledge, and mitigates the issue of forgetting past experiences [14], [15]. GUL emphasizes the right of data to be forgotten, aiming to help the model forget the specified revocation data at the lowest possible cost while minimizing damage to the existing knowledge of the model [16]. However, GML is far from a combination of GLL and GUL, and it entails comprehensive and nuanced contemplation of memory retention and forgetting corresponding to the continuous influx and withdrawal of data, respectively. The inherent conflict between memory retention and forgetting presents a natural barrier that obscures graph learning models from accurately perceiving changes in graph structures, leading to a decrease in their expressive capacity.\nUpon a deeper examination of the conflict between memory retention and forgetting, we identify two fundamental challenges to be effectively explored in GML. Firstly, the influx and withdrawal of graph data are often closely linked to the node and edge knowledge within the local graph structure. However, frequent changes in this structure prevent graph models from clearly distinguishing between retained and forgotten knowledge. Secondly, retaining emerging information is intricate and unreliable in the graph structure. Specifically, new information can be accidentally and inappropriately linked to irrelevant graph regions. Furthermore, the worst is that new knowledge with significant disparities can unintentionally fill the structural gaps left by the removal of old knowledge. It potentially suppresses knowledge retention and exacerbates catastrophic forgetting. These conditions inhibit the propagation of new knowledge and exacerbate catastrophic forgetting.\nFortunately, the function-structure coupling brain can be considered a remarkable example of \"natural\" memory intelligence. It possesses the capacity to absorb new knowledge and integrate old knowledge continually through structural brain networks. When confronted with disruptive stimuli, the brain can even selectively remodel synapses and forget pertinent memories as a protective measure [17]. Intuitively, network neuroscience holds inspiring insights into addressing the above challenges in GML. Firstly, the brain employs a modular and hierarchical structure (Fig. 1 (B)), enabling it to perceive local changes with precision and distinguish between remembered and forgotten information. This facilitates the comprehension of evolving trends in knowledge alteration [18]. Secondly, the brain exhibits a dynamic integration mechanism (Fig. 1 (C)) to adjust brain networks based on dynamics [19]. Modifications in local knowledge trigger responses exclusively in corresponding brain regions, ensuring efficient recognition of changes with minimal energy expenditure while preserving past experiences in inactive brain regions. This mechanism directs the flow of knowledge to the designated brain regions based on relevance and activates neurons that store the most pertinent old experiences for integration. It precisely helps the brain minimize interference from previous experiences.\nTherefore, by drawing insights from the modular and hierarchical structure of the brain, the graph can be divided into multi-level subgraphs based on node attributes and structural characteristics. This progressive and multi-granular approach to learning graph knowledge enables graph models to distinguish the boundaries of knowledge retention and forgetting. For instance, when a recommendation system learns preferences of different granularities by building labels on various levels, the system obtains a robust and deep cognition of user nodes, and the influence of alterations in neighboring nodes on recommendations diminishes. Additionally, a dynamic integration system for new knowledge can be designed, akin to the brain network dynamics. The system employs the $L_2$ distance to guide new knowledge toward credible a local graph structure and integrate it with relevant past experiences. For instance, in a recommendation system, when new users form unreliable connections due to accidental purchases, recommendations based on the most similar users are the most reliable and mitigate the impact of old experiences. Furthermore, removing the constraints imposed by the graph structure facilitates the implementation of the above concepts within GML. Ideally, nodes should function as independent samples capable of efficient and scalable batch processing.\nIn light of the above observations, this paper proposes a Brain-inspired Graph Memory Learning algorithm (BGML) based on brain network dynamics and function-structure coupling. It aims to solve the problem of GML for dynamically evolving graph data. Specifically, this paper proposes a multi-granular feature graph learning scheme with hierarchical progressive learning to clarify changes in local graph knowledge. Key components comprise a multi-granular graph partition (MGP) and a multi-granular hierarchical progressive learning mechanism (MGHPL) based on feature graph grain learning (FGGL). Notably, feature graph grains implicitly encode the graph structure, liberating nodes to operate as independent samples and drawing inspiration from feature graph networks. Furthermore, an information self-assessment ownership mechanism (ISAO) is proposed to tackle the issue of unreliable new knowledge structures. Essentially, this paper explores three questions: (1) What GML approach can empower graph models to selectively remember important information and forget irrelevant information? (2) How to alleviate the conflict between remembering and forgetting in the graph memory model? (3) How to assign a credible graph structure to knowledge during the process of remembering and forgetting? The main contributions are summarized below:\n\u2022 This paper makes the first attempt to define a new problem of Graph Memory Learning and to propose a novel GML paradigm inspired by brain network dynamics and function-structure coupling.\n\u2022 A multi-granular hierarchical progressive learning mechanism based on feature graph grain learning enables GML to alleviate the conflict between remembering and forgetting in local graph structures.\n\u2022 We propose an information self-assessment ownership mechanism to establish a reliable graph structure for new knowledge such that new knowledge can independently choose the most suitable graph grain and neighbors.\n\u2022 Extensive experiments on five tasks targeting different aspects of GML on the node classification task on five real-world datasets demonstrate that BGML outperforms all baselines significantly."}, {"title": "II. RELATED WORK", "content": "Graph neural networks (GNNs) have become a prominent method for processing unstructured data [20]-[22]. Specifically, Kpif et al. [23] introduced GCNs, utilizing first-order truncated Chebyshev polynomials to approximate graph convolutions. Additionally, Veli\u010dkovi\u0107 et al. [24] introduced Graph Attention Networks (GATs), incorporating an attention mechanism on the graph, resulting in significant performance improvements. Furthermore, Hamilton et al. [25] were the first to identify the latent nodes within a graph, leading to the proposal of the inductive spatial graph neural network known as GraphSAGE. As research progresses, scholars have observed that the modeling of real-life scenes using graphs typically undergoes dynamic evolution, extending beyond static data [26], [27]. Based on distinct data forms (discrete and continuous), current research can be categorized into two groups [28]: discrete dynamic graph neural networks (DTDG), exemplified by EvolveGCN [29] and DyTed [30], and continuous dynamic graph neural networks (CTDG), exemplified by Ada-DyGNN [31] and APAN [32].\nHowever, these approaches are limited to extracting information solely from dynamic graphs, lacking the capability to maintain memory and selective forgetting in the face of new data changes. BGML has made some breakthroughs in adapting to the inflow and withdrawal of graph data."}, {"title": "A. Graph Neural Networks", "content": "Graph neural networks (GNNs) have become a prominent method for processing unstructured data [20]-[22]. Specifically, Kpif et al. [23] introduced GCNs, utilizing first-order truncated Chebyshev polynomials to approximate graph convolutions. Additionally, Veli\u010dkovi\u0107 et al. [24] introduced Graph Attention Networks (GATs), incorporating an attention mechanism on the graph, resulting in significant performance improvements. Furthermore, Hamilton et al. [25] were the first to identify the latent nodes within a graph, leading to the proposal of the inductive spatial graph neural network known as GraphSAGE. As research progresses, scholars have observed that the modeling of real-life scenes using graphs typically undergoes dynamic evolution, extending beyond static data [26], [27]. Based on distinct data forms (discrete and continuous), current research can be categorized into two groups [28]:"}, {"title": "B. Lifelong Learning and Graph Lifelong Learning", "content": "Lifelong learning entails the continuous adaptation to new knowledge while refining existing knowledge over time [33], [34]. In conventional domains, mainstream lifelong learning models can be broadly categorized into three main approaches. Firstly, architectural approaches often incorporate unit, extended, or compressed architectures to construct appropriate networks. For instance, ProgNN proposed by Rusu et al. [35] and DEN proposed by Yoon et al. [36]. Secondly, regularization methods employ specialized loss terms to preserve the stability of previous parameters. Examples include EWC proposed by Kirkpatrick et al. [37] and LWF developed by Li et al. [38] Thirdly, rehearsal methods depend on a retraining process using samples from prior tasks. For instance, ICARL proposed by Rebuffi et al. [39].\nRecently, these strategies have been progressively extended into the domain of graph learning [10]. Regarding architectural design, Wang et al. [14] devised the FGNs method to translate the graph structure into a format compatible with conventional data learning structures. To enhance the encoding of attribute information and the topological structure of target nodes, Zhang et al.'s [15] HPNs extract various levels of knowledge abstraction represented as atoms. Regarding the rehearsal scheme, Zhou et al. [40] store past knowledge in an experience buffer and conduct lifelong learning on graph data through experience replay. In terms of regularization, Kou et al. [41] introduced decoupling-based continuous graph representation learning (DICGRL). tan et al. [42] introduced GPIL with transferable meta-knowledge for initialization learning by introducing a pseudo-incremental approach. Furthermore, this field has given rise to several hybrid methods. ContinualGNN, proposed by Wang et al. [43], combines two approaches to alleviate catastrophic forgetting and preserve the existing learning mechanism.\nExisting GLL methods focus on addressing catastrophic forgetting, but they are impractical for limited resources as they aim to retain all information indefinitely. BGML delves into making the model selectively remember important information and forget what is not."}, {"title": "C. Machine Unlearning and Graph Unlearning", "content": "In long-term continuous learning, ensuring model performance and safety requires addressing the critical aspect of enabling the machine to learn to forget [12], [13]. Cao et al. [44] were pioneers in introducing the concept of unlearning. Subsequently, researchers have proposed a series of new methods in two primary directions: exact unlearning and approximate unlearning [45], [46]. SISA (Sharded, Isolated, Sliced, and Aggregated), proposed by Bourtoule et al. [47], stands as one of the most representative works in this regard. The fundamental concept involves randomly dividing the training set into several disjoint shards and training the corresponding shard models independently. Upon receiving the forget request, the model only needs to retrain the respective shard model.\nRecently, Chen et al. [16] extended the concepts of SISA to the graph domain, presenting GraphEraser as the inaugural graph unlearning model. Building upon this foundation, Li et al. [48] introduced MEGU to chieve a balance between forgetting performance and framework generalization. Current research on unlearning is still mainly focused on static graphs, and no new issues have been added for linkage and thinking. BGML intends to apply this mechanism to dynamic evolution graphs, representing a bold endeavor."}, {"title": "III. PROBLEM FORMULATION", "content": "In GNNs, a static regular graph is typically depicted as $G=(V, E)$, where $V$ is the set of nodes and $E$ is the set of edges in the graph. $N=|V|$ signifies the count of nodes. The feature matrix of the graph is represented by $X\\in\\mathbb{R}^{N\\times F\\times C}$, where $F$ represents the feature dimension of each node. $C$ is the number of channels for each feature. Specifically, for nodes $v_i$ and $v_j$, their edge relationship can be denoted as $e_{i,j}$, while their respective characteristics are expressed as $X_i\\in\\mathbb{R}^{F\\times C}$ and $X_j\\in\\mathbb{R}^{F\\times C}$. The adjacency matrix $A\\in{0, 1}^{N\\times N}$ encompasses all edge relationships within the graph.\nThe main target of GML is the dynamic evolution graph based on discrete snapshots, i.e., $G=(G_0, G_1, G_2, ..., G_t)$. The graph evolution process of each snapshot mainly includes two aspects. First, new data entering the graph network can be represented as $G_t=G_{t-1} + \\Delta G_t= G_{t-1} \\cup \\{\\{V_{new}, E_{new}, u\\mid\\forall v_u\\in N(V_{new})\\}\\mid V_{new}\\in V_{add}\\}$, where $N(v_{new})$ represents the neighbors of $v_{new}$ and $V_{add}$ represents the set of new nodes. Second, when the graph network receives forgetting requests (FR), the clearing operation can be expressed as $G_t^+=G_{t-1} + \\Delta G_t^+=G_{t-1} \\cup \\{\\{V_{forget}, E_{forget}, u\\mid\\forall v_u\\in N(V_{forget})\\}\\mid V_{forget}\\in V_{delete}\\}$, where $V_{delete}$ is used to represent the set of nodes that need to be forgotten. In general, the change of the graph at time $t$ can be expressed as $G_t=G_{t-1} + \\Delta G_t$, where $\\Delta G_t = \\Delta G_t^+ + \\Delta G_t^-$. In addition, task context changes over time throughout the process of graph memory learning. The task set can be represented by $T=(T_0, T_1, T_2, ..., T_t)$. The corresponding time is $t=(t_0, t_1, t_2, ..., t_t)$.\nDefinition 1. The objective of graph memory learning is to construct a model $H(\\Theta)$ that continuously learns from graph data $G=(G_0, G_1, G_2, ..., G_t)$ by minimizing the model loss to achieve optimal performance on task $T_t$ at time $t$. Additionally, the model should satisfy two conditions:\n(i) It should possess the ability to alleviate the conflict between remembering and forgetting and overcome catastrophic forgetting, meaning it aims to maintain the performance of the model on tasks $T_1$ through $T_{t-1}$ as much as possible.\n(ii) Nodes required to be forgotten at time $t_{t-1}$ should no longer have any relationship with the model at time $t_t$."}, {"title": "IV. GRAPH MEMORY LEARNING FRAMEWORK", "content": "GML essentially focuses on three aspects of learning in dynamic evolution graphs: evolving graph comprehension, learning new information, and forgetting specific information.\nThe modular and hierarchical structure of the brain, as well as the dynamic integration mechanism based on brain network dynamics, inspire solutions to these challenges. Based on these inspirations, this paper introduces a graph memory learning framework (BGML), as shown in Fig. 2."}, {"title": "A. Morphological Dynamics of BGML in Temporal States", "content": "According to Definition 1, an algorithm framework adhering to the GML paradigm must complete the task set $T=(T_0, T_1, T_2, ..., T_i)$ at $t_i$-time. Moreover, the problem scenarios at $t_0$-time and $t_i$-time differ significantly. At $t_0$-time, the model needs to learn high-quality graph representations from the initial graph $G_0$ to ensure accurate predictions for task $T_0$. At $t_i$-time, the model must simultaneously handle incremental data requests (IR) and forgetting requests (FR) during the graph evolution process to complete $T_0$ to $T_i$.\nAs shown in Fig. 2, BGML is divided into two distinct forms at $t_0$-time and $t_i$-time. At $t_0$-time, BGML is based on the modular and hierarchical structure of the brain. Specifically, the initial graph $G_0$ is divided into multi-level graph shards by multi-granular graph partition (MGP). These shards are then organized and progressively learned by the multi-granular hierarchical progressive learning mechanism (MGHPL) based on feature graph grain learning (FGGL). Finally, all the learned knowledge is integrated for prediction. At $t_i$-time, BGML builds a memory forgetting module and a memory remembering module, extending the mode at $t_0$-time to handle continuous data forgetting requests (FR) and incremental requests (IR). These modules are inspired by the brain's synaptic remodeling mechanism and synaptic stream mechanism. Furthermore, the information self-assessment ownership mechanism (ISAO) in the remembering module helps incremental knowledge form a credible graph structure.\nIt is crucial that the model at $t_i$-time is a continuation of the model at $t_0$-time along the timeline, rather than an independent design. The multi-granular and progressive understanding of local graph information by BGML at $t_0$-time is seamlessly inherited at $t_i$-time. This in-depth comprehension of changes in local and regional information significantly alleviates the conflict between remembering and forgetting instructions. At $t_i$-time, several designs, including ISAO, aim to protect existing knowledge and conserve computing resources during the model's continuous learning process. Emulating brain network dynamics, the forgetting and remembering modules at $t_i$-time continuously update the empirical knowledge in the integrated model from $t_0$-time, enabling continuous learning."}, {"title": "B. MGP: Multi-granular Graph Partition", "content": "Brain-Inspiration. The sensory system (e.g., vision, hearing, touch, and smell) provides knowledge to the brain for learning. Various sensory organs receive distinct knowledge stimuli, convert them into neural signals, and transmit them to the brain through diverse neural pathways. Upon reaching the brain, neural signals are allocated to different subdivisions of the cerebral cortex for processing. The cerebral cortex subdivisions (e.g., frontal, parietal, and occipital) govern various perceptual and cognitive functions. Consider the process of distinguishing and remembering fruit types in the brain as an example. The human eye observes the color and shape of the fruit, generating a stimulus signal. Upon transmission to the brain, the occipital lobe area of the cerebral cortex generates a response. Likewise, tactile signals resulting from the aroma and skin characteristics of fruits elicit responses in the parietal. Additionally, cortical subdivisions of the brain are commonly characterized in neuroscience as exhibiting a multilevel organization. The initial subdivision, responsible for receiving and processing basic sensory information, does not directly facilitate the brain's decision-making process. Secondary partitions can offer a substantial wealth of detailed information to bolster more sophisticated cognitive functions. For instance, when distinguishing between tangerines and oranges, attention to local details such as the fruit skin at various levels and angles is necessary.\nMGP Method. In conclusion, the cerebral cortex's strategy of multi-region collaboration and hierarchical progression fosters the agent's profound comprehension of knowledge. This necessitates preprocessing before knowledge input to facilitate problem division and resolution. Inspired by this, the paper devised an MGP algorithm, As shown in the MGP module in Fig. 2. This algorithm aims to partition the static regular graph $G_0$ at time $t_0$ into multiple granules, establishing a hierarchy of knowledge details. Subsequent graph evolution details will be simultaneously incorporated into the granule. This lays the groundwork for the subsequent in-depth understanding and hierarchical extraction of precise information in the dynamic evolution graph. Specifically, the MGP algorithm is constructed based on common community detection-based label propagation algorithms (LPA) [49], [50] or k-means clustering algorithms (KM) [51]. It is worth mentioning that to mitigate uneven partitioning, the paper employs the balanced versions (BLPA and BEKM) improved by Chen et al. [16]. For $G_0$, the two graph partitions of coarse and fine granularity can be expressed by Eq.1:\n$\\begin{aligned}\\text{BLPA}(G_0) &= \\{S_1, S_2, ..., S_k\\}\\\\\\\\text{BLPA}(S_i) &= \\{P_i^1, P_i^2, ..., P_i^{l_i}\\}\\\\\\\\text{BEKM}(G_0) &= \\{S_1, S_2, ..., S_k\\}\\\\\\\\text{BEKM}(S_i) &= \\{P_i^1, P_i^2, ..., P_i^{l_i}\\}\\end{aligned}$      (1)\nwhere $i=1, 2, ..., k$. $S_i$ represents a first-level graph shard with coarse granularity. $P_i^j$, on the other hand, denotes a second-level graph shard within $S_i$, offering finer granularity, where $j=1, 2, ..., l_i$. BLPA($\\cdot$) and BEKM($\\cdot$) in Eq.1 are abstract representations of iterative algorithms. Before iteration, they need to be provided with random initialized partition shards $\\{S_1, S_2, ..., S_k\\}^0$ and centroids $\\{m_1, m_2, ..., m_k\\}^0$, respectively. Furthermore, the balance of partitioning is reflected in the fact that, regardless of the partition method employed, the number of nodes within the shards cannot surpass the configurable threshold $\\delta$.\nBLPA Partition Method. For BLPA($\\cdot$), the core iteration steps in which $v\\in S_i$ participates can be described as:\n$\\begin{aligned}S_{src} &= S_i\\\\\\\\S_{dst} &= \\arg \\max_{S_i, i=1, 2, ..., k}|N_{S_i}(v)|\\\\\\\\S_{dst} &\\leftarrow S_{dst} \\cup v\\\\\\\\S_{src} &\\leftarrow S_{src} \\backslash v,\\end{aligned}$      (2)\nwhere $S_{src}$ and $S_{dst}$, respectively, represent the shard where node $v$ is currently located and the shard to which it will belong. $N_{S_i}(v)$ is the number of neighbors of node $v$ in $S_i$. BEKM Partition Method. On the other hand, BEKM($\\cdot$) does not start directly from the graph structure and needs to obtain the node embeddings $\\{b_1, b_2, ..., b_n\\}$ through pre-training in advance. It performs clustering by distance between node embedding and centroids. For $v$, the specific iteration steps can be expressed by Eq.3:\n$\\begin{aligned}S_{dst} &= \\arg \\min_{S_i, i=1, 2, ..., k}||b - m_i||_2\\\\\\\\S_{dst} &\\leftarrow S_{dst} \\cup v\\\\\\\\m_{dst} &\\leftarrow \\frac{\\sum_{v\\in S_{dst}} b_u}{S_{dst}},\\end{aligned}$      (3)\nwhere $m_{dst}$ represents the centroid of the updated shard $S_{dst}$."}, {"title": "C. MGHPL: multi-granular hierarchical progressive learning mechanism", "content": "Brain-Inspiration. The brain's cognition process of knowledge involves the coordination and hierarchical progressive processing mechanisms of the partitions of the cerebral cortex. When a piece of knowledge enters, the corresponding primary sensory area of the cerebral cortex responds. Due to limitations, not all extracted features from the area can provide the fine information required for an accurate judgment. Therefore, for primary areas of the cerebral cortex that perceive blur, the brain will pass the information to the corresponding secondary partitions. In the secondary partition, insufficient cognition in primary perceptual areas will be targeted and supplemented. When participating in decision-making, the information extracted by each cerebral cortex is fused to form higher-level cognitive features. From primary perception to secondary processing to higher-level cognition, the brain's process of learning knowledge is progressive and in-depth.\nHere, we still use the example of distinguishing tangerines and oranges. When information such as the color and taste of fruits is transmitted to the brain, it is sent to different cortical areas by the sensory system. For example, visual signals are sent to the occipital lobe, taste signals are sent to the temporal lobe, etc. The flavors of the two fruits are relatively easy to distinguish, with tangerines being sweeter and orange being sweet with a hint of sourness. Therefore, primary perception and judgment in the temporal lobe area are relatively accurate and do not require the assistance of secondary partitions. Visually, however, they are both orange and difficult to distinguish. Information about the fruit's skin characteristics will need to be sent to the subdivision to aid in judgment. Ultimately, the information processed by these different cortical areas undergoes complex processing and fusion to help us make the correct cognitions and decisions.\nFirst FGGL. Inspired by the above, this paper proposes the MGHPL mechanism based on feature graph grain learning (FGGL), as shown in Fig. 2. This mechanism takes the first-level graph shards and the second-level graph shards of the static graph at time $t_0$ given by the MGP algorithm as input. The first is the first-level FGGL. It is important that FGGL be related to subsequent continuous streaming graph learning as well as the response to node data IR and FR. This part of the idea refers to Wang et al.'s [14] implicit encoding of graph structure into the node feature graph, which breaks the shackles of node learning being restricted by the graph structure. This idea of converting a single node into a feature graph and converting the graph into a collection of node feature graphs (i.e., graph grains) inspires us. Specifically, the features of F nodes $\\{f_1, f_2, ..., f_F\\}$ in the feature graph of node $v_i$ are obtained by splitting the original feature $X_i \\in \\mathbb{R}^{F\\times C}$. The construction of the adjacency matrix in the feature graph can be expressed by the following formula:\n$\\begin{aligned}A_{ij}^{feat}(v_i) &= \\text{sgnroot}\\left(\\frac{\\epsilon^{a_{ij}}}{\\sum_{v_g\\in N(v_i)} \\epsilon^{a_{ig}}}\\right)\\\\\\\\a_{ij} &= \\text{LeakyReLU}(X_i w_i+ X_j w_j +q),\\end{aligned}$      (4)\nwhere $\\text{sgnroot}(x)=\\text{sign}(x)\\sqrt{|x|}$. For $\\forall c \\in \\{1,2,..., C'\\}$, $X_i^c \\in \\mathbb{R}^{F}$ represents all features in the c-th channel of node $v_i$. $A_{ij}^{feat}(v_i)$ represents the weight of the c-th channel edge $e_{i,j}$. $w_i, w_j \\in \\mathbb{R}^{F}$, $q \\in \\mathbb{R}$ are learnable attention parameters. At this point, all first-level graph shards are converted into graph grains according to Eq.4. To avoid confusion, graph grains and graph shards share the same symbol:\n$\\begin{aligned}\\{S_1, S_2, ..., S_k\\}_{shards} \\rightarrow \\{S_1, S_2, ..., S_k \\}_{grains}\\end{aligned}$      (5)\nThen, each first-level graph grain is transferred as knowledge into the first-level sub-model $\\{h_1(\\cdot), h_2(\\cdot), ..., h_k(\\cdot)\\}$ of a different division of labor. These models are built to realize the function of cerebral cortex partitions. They form high-quality node embeddings $\\{Z_1, Z_2, ..., Z_k\\}$ from the knowledge in graph grains for downstream tasks. Each sub-model adopts the same configuration and consists of two layers of feature graph networks [14]. For $v_i \\in S_j$, The learning process of node embedding can be expressed by Eq.6:\n$\\begin{aligned}Z_i = h_j(v_i) = \\sigma\\left(A^{feat} \\sigma(A^{feat} X_i W) w\\right),\\end{aligned}$      (6)\nwhere $\\sigma(\\cdot)$ is a non-linear activation function, $W$ is a learnable parameter, and $A^{feat}$ is the normalized feature graph adjacency matrix.\nProgress-aware Module. According to the above, the first-level graph grains are constructed according to the coarse-grained first-level graph shards. However, when completing downstream tasks, some sub-models need to deeply understand the multi-level content and local details of the target. The sub-model trained according to the first-level graph grains obviously cannot fully meet this requirement. Therefore, this paper designs a progress-aware module based on the modular and hierarchical structure of brain cognition, which is used to screen first-level sub-models whose representation capabilities need further support. It is worth mentioning that the complete Validation dataset $G_{valid}$ is used to test the representation ability of each sub-model. The progress-aware module can be expressed by Eq.7:\n$\\begin{aligned}\\text{score} &= \\text{Predict} (h_i (G_{valid}))\\\\\\\\\\text{score} &= \\{\\text{score}_1, \\text{score}_2,..., \\text{score}_k\\}\\\\\\\\\\Tau_{idx} &= \\text{Lowrank} (\\text{score}, T),\\end{aligned}$      (7)\nwhere Predict($\\cdot$) is a function that predicts task scores. $score_i$ is the score whose output is the i-th first-level sub-model. Lowrank(score, $T_\\tau$) is a retrieval function that can retrieve the sub-model index with the $\\tau$-th score from the bottom.\nSecond FGGL. The support method for the perceived sub-models is that the corresponding fine-grained second-level graph shards are constructed into second-level graph grains $\\{P_{i_{idx}}^1, P_{i_{idx}}^2, ..., P_{i_{idx}}^{l_{i_{idx}}}\\}_{shards}$ to establish a more refined learning. For $\\forall v_i \\in P_{i_{idx}}^j$, the learning process of node embedding can be expressed as:\n$\\begin{aligned}Z_i = h_{i_{idx}}^j(v_i),\\end{aligned}$      (8)\nwhere $h_{i_{idx}}^j$ and $Z_i^j$, respectively, represent the second-level sub-model corresponding to graph grain $P_{i_{idx}}^j$ and its generated second-level embedding.\nEnsemble Model. Ultimately, we unified the knowledge and experience learned from MGHPL in an ensemble way. A reasonable aggregation strategy can help the ensemble model make the correct decisions and judgments. There are three commonly used aggregation strategies: MajAggr based on majority voting, MeanAggr based on posterior averaging, and LBAggr based on learning [16]. These aggregation strategies are all suitable for the framework of this paper. The only thing that needs attention is the support strength of the second-level sub-model for the first-level sub-model that needs help. The ensemble strategy of MGHPL can be abstractly summarized:\n$\\begin{aligned}X_{final} &= \\text{Aggr} \\{\\alpha_1, \\alpha_2,..., \\alpha_k\\}\\\\\\\\& \\text{Aggr} \\{\\alpha_{i_{idx}}, \\beta_{i_{idx}}^1, \\beta_{i_{idx}}^2,..., \\beta_{i_{idx}}^{l_{i_{idx}}}\\},\\end{aligned}$      (9)\nwhere Aggr() represents any of the aggregation strategies mentioned above. $X_{final}$ denotes the final decision result. $\\{\\alpha_1, \\alpha_2,..., \\alpha_k\\}$ contains decision information from all first-level sub-models, while $\\{\\beta_{i_{idx}}^1, \\beta_{i_{idx}}^2,..., \\beta_{i_{idx}}^{l_{i_{idx}}}\\}$ encompasses decision information from all second-level sub-models associated with the first-level sub-model $h_{i_{idx}}$ that needs support."}, {"title": "D. Memory Forgetting Module", "content": "Brain-Inspiration. Human cognition of knowledge is a process of continuous adjustment. During continuous learning", "challenges": "incremental new information requests and data forgetting requests. As shown in Fig. 2", "as": "n$\\begin{aligned}\\{S_i &\\leftarrow S_i \\backslash \\{V_{forget}, E_{forget}, u \\mid \\forall v \\in N (V_{forget})\\}\\\\\\P_i^j &\\leftarrow P_i^j \\backslash \\{V_{forget}, E_{forget}, u \\mid \\forall v \\in N (V_{forget})\\},\\end{aligned}$      (10)\nwhere $V_{forget} \\in V_{delete}$, $i = 1, 2, ..., k$ and $j = 1, 2, ..., l$. ForgetGuide Module. All memories of the system at time $t_{i-1}$ are stored in the neuron parameters of each sub-model. When performing a forget operation, we only need to selectively lock the sub-models associated with the FR. The remaining irrelevant models maintain the state at time $t_{i-1}$. Therefore, forgetting guidance"}]}