{"title": "Synthetic Students: A Comparative Study of Bug Distribution Between Large Language Models and Computing Students", "authors": ["Stephen MacNeil", "Magdalena Rogalska", "Juho Leinonen", "Paul Denny", "Arto Hellas", "Xandria Crosland"], "abstract": "Large language models (LLMs) present an exciting opportunity for generating synthetic classroom data. Such data could include code containing a typical distribution of errors, simulated student behaviour to address the cold start problem when developing ed-ucation tools, and synthetic user data when access to authentic data is restricted due to privacy reasons. In this research paper, we conduct a comparative study examining the distribution of bugs generated by LLMs in contrast to those produced by computing students. Leveraging data from two previous large-scale analyses of student-generated bugs, we investigate whether LLMs can be coaxed to exhibit bug patterns that are similar to authentic student bugs when prompted to inject errors into code. The results suggest that unguided, LLMs do not generate plausible error distributions, and many of the generated errors are unlikely to be generated by real students. However, with guidance including descriptions of common errors and typical frequencies, LLMs can be shepherded to generate realistic distributions of errors in synthetic code.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) present a promising new opportu-nity for generating synthetic data [20], which may have important implications for computing education research and practice. Such data could include examples of code containing typical student errors, which could have useful practical applications. For example, it provides a viable solution for conducting research in situations where access to authentic data is restricted due to privacy con-cerns [26]. Teachers could also make use of such examples when developing learning resources, such as debugging tasks, or materials to help address common mistakes. Having access to large amounts of synthetic data could also be useful for developers of educational tools. For example, a well known problem when developing intelli-gent or adaptive tutoring systems is the 'cold start problem' [38], where a lack of user data results in an initial mismatch between the system's internal model and a learner's actual performance.\nPrior work outside of computing education contexts has begun to explore the use of LLMs for generating synthetic data. In the field of Human-Computer Interaction (HCI), H\u00e4m\u00e4l\u00e4inen et al. generated synthetic questionnaire responses, demonstrating that LLMs can produce believable accounts of user experiences [21]. Although they show the potential for using synthetic data to ideate and pilot experiments, they suggest synthetic data should be validated with real data to ensure reliability. Similarly, Park et al. studied LLM-based agents to simulate human behavior and found interactions of the agents were human-like, and useful for designers [39, 40]. Inspired by these developments in other research areas, in this work we investigate the potential of LLMs to generate synthetic code that mimics the distribution of bugs found in student-written code. The central research questions of this study are:\n\u2022 RQ1: Can LLMs produce erroneous code upon request?\n\u2022 RQ2: To what extent does directing an LLM through prompt engineering influence the distribution of bugs it generates?\n\u2022 RQ3: How do bug distributions from an LLM correlate with or deviate from those generated by human students?\nTo address these questions, we compare the distribution of bugs generated by an LLM with those produced by computing students. Using publicly-available student data from previous studies, we investigate the effectiveness of different prompting strategies in guiding LLMs to generate realistic error distributions. Our findings suggest that while LLMs do not produce accurate distributions"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Common Bugs in Students' Code", "content": "Learning to program involves developing an understanding of the syntax, structure and style of a programming language [33], and stu-dents encounter a wide range of syntax and logic errors during this process [2, 12, 42]. Such errors include \"trivial mechanics\" errors such as syntax errors with braces, brackets, semicolons, and nam-ing conventions [42], as well as errors related to the semantics of the learned language [2]. Early research on errors in programming often centered on specific problems [25, 46, 48]. Later, researchers increasingly used programming errors and programming process data - in forming a deeper understanding of the problems [23]. As an early example of such work, Jadud [24] quantified the error fixing behavior of novice programmers, identifying a link between the error fixing behavior (or skill) and course outcomes.\nIn general, there are differences in the frequency of programming errors [42, 49] and the time that it takes to fix such errors [8, 11]. The types of errors that students encounter also gradually change over time [2, 52], and they can stem from multiple sources [2, 16]. These sources include misinterpreting the programming problem and having flaws in programming knowledge [16], as well as the role of the programming language and the environment [28, 50, 52]. Research on programming errors has contributed to program-ming language design (e.g. [47]), and researchers have sought to help students with programming errors, for example, by improving programming error messages [4, 14, 30]. All such research builds on the availability of relevant data. However, although there are increasing number of programming datasets available [23], errors can vary between programming languages, and the distributions of errors may also differ between contexts."}, {"title": "2.2 Generating Educational Content With LLMs", "content": "Large language models (LLMs), which are advanced transformer models, possess the ability to both comprehend and generate code and text. These capabilities enable LLMs to offer students personal-ized, just-in-time pedagogical support [13] . For instance, LLMs have been utilized to enhance code comprehension by explaining code in plain English [29, 35, 43] or by producing analogies to explain both code and underlying concepts like recursion [5, 6]. Leinonen et al. found that the explanations provided by LLMs were compa-rable in word length to those generated by peers in a classroom, though students rated the quality of LLM-generated explanations higher [29]. However, in other cases, LLMs have been shown to dramatically outperform students such as in identifying bugs [34].\nIn addition to supporting students with just-in-time content generation, some research has explored the creation of real-time content for instructors. For example, Tran et al. demonstrated that LLMs can generate multiple-choice questions with plausible dis-tractors and correct answers based solely on the question stem [51]. Doughty further extended this research by developing a pipeline"}, {"title": "2.3 Synthetic Data Generation", "content": "There are many reasons for generating synthetic data for research. Real data could be scarce [17], hard to collect [21], low quality [17], or contain private information that cannot be shared (for example, medical records) [9]. One way of mitigating these issues is to try de-identify datasets [19], although this can in some cases reduce the utility of the data [31]. These issues can be potentially sidestepped by generating synthetic data, if the quality of the generated data is similar to or greater than organic data.\nTraditional approaches for generating synthetic data have in-cluded algorithm-based approaches [44] and generative adversarial networks (GANs) [17]. In education, synthetic data has been used, for example, to generate data to train and evaluate knowledge tracing methods [44] which aim to accurately model a learner's knowledge of the concepts they are practicing.\nRecently, advancements in large language models (LLMs) have opened new possibilities for synthetic data generation. Research has explored various scenarios, including simulating social inter-actions. Park et al. discovered that interactions within a simulated community, termed 'social simulacra, can aid in prototyping, with simulated content often hard for experienced moderators to distin-guish from real content [40]. In a follow-up study, they found that data generated by 25 LLM-based agents in a game-like environment was more believable than data from human crowdworkers [39]. Sim-ilarly, H\u00e4m\u00e4l\u00e4inen et al. used GPT-3 to generate synthetic data on HCI experiences, particularly video games as art, finding the con-tent human-like but less diverse than that created by humans [21]."}, {"title": "3 Methods", "content": "To evaluate our research questions, we conducted two comprehen-sive studies. In the first study, we aimed to replicate and extend the findings of Altadmri and Brown [2]. Using their methodology, we employed GPT-4, a state-of-the-art LLM, to generate synthetic bugs. To address a lack of information about the precise program-ming problems used in their work, we conducted a second study where the programming problems were more explicitly defined. This second study replicates the bug frequencies identified by Rigby et al. [41] in their study of 900 programming students. An overview of the programs we used can be found in the virtual appendix\u00b9."}, {"title": "3.1 Prompting With Distributional Information", "content": "To generate synthetic bugs, we investigated three prompting strate-gies that differed in the level of information provided about how"}, {"title": "3.1.1 Open-Ended Prompt.", "content": "The baseline prompt was intentionally open-ended to gauge how closely LLMs align with student bug frequency. Here, no specific information about bug distribution was provided to the model."}, {"title": "3.1.2 Taxonomy-Informed Prompt.", "content": "In the second prompt, we ex-plored the impact of providing basic information about the latent bug distribution. The model was given a list of bugs encountered by students in real-world contexts as reported by the two papers we replicated (i.e.: Altadmri and Brown [2] and Rigby et al. [41]). This approach reflects how an instructor might have some intuition about common student bugs without precise frequency knowledge."}, {"title": "3.1.3 Frequency-Informed Prompt.", "content": "In the final prompt, we pro-vided the model with detailed information about the latent distribu-tion of bug frequencies, including the specific frequencies at which students encountered each error. We used or computed the frequen-cies for each study based on the two papers being replicated [2, 41]."}, {"title": "3.2 Study 1: Replication of Altadmri and Brown", "content": "In the first study, we conducted a replication of the work by Al-tadmri and Brown [2]. Their research analyzed the frequency of bugs generated by real students across 37 million compilations. They identified 18 errors related to syntax, type, and semantics. These errors are summarized in Table 1.\nTo replicate their work, we chose five Java programs. Java was chosen because that was the language used by participants in their study [2]. The five Java programs were chosen to be diverse; how-ever, they do not perfectly match the range used in the prior study where the programming problems were not experimentally con-trolled and varied widely across the 37 million compilations."}, {"title": "3.3 Study 2: Replication of Rigby et al.", "content": "The goal of the first study was to investigate whether LLMs are capable of generating similar distributions of bugs and syntax errors as students. However, it was challenging to replicate this prior work because they had investigated bug frequencies extracted from thousands of authentic programs which were solving a great variety of programming tasks.\nTo more tightly control the programming tasks, we conducted a second study that replicates the work of Rigby et al. [41]. In their work, only four programming problems were used and the corresponding bug frequencies were computed based on more than 22,000 submissions from 900 students. They focused purely on logic errors, in particular off-by-one errors for C code that iterates over an array. They categorised the four mistakes that can cause an off-by-one error: missing the first element (index 0), missing the last element (index length-1), accessing an invalid index before the start (index -1), or accessing an index just past the end (index length).\nSimilar to Study 1, we used GPT-4 to generate bugs for the programming problems. We only modified the distributional infor-mation to align with the corresponding bugs and frequencies."}, {"title": "3.4 Analyzing the LLM-Generated Bugs", "content": ""}, {"title": "3.4.1 Deductive Coding and Inter-Rater Reliability.", "content": "The first part of the analysis focused on deductively coding the generated data using the original taxonomies from each corresponding study [2, 41]. Two coders independently coded the data and we computed inter-rater reliability (IRR) using Cohen's Kappa to determine their agreement"}, {"title": "3.4.2 Statistical Analyses.", "content": "The resulting frequency data was ana-lyzed using the Chi Square goodness of fit test. Given that there were multiple distributions being compared, we corrected the criti-cal p-values using the Bonferroni correction, which reduces Type I errors due to multiple comparisons."}, {"title": "3.4.3 Thematic Analysis of Out-of-Distribution Bugs.", "content": "When repli-cating both studies [2, 41], bug types that were not included in the original were coded 'X'. These data were then analyzed using a thematic analysis approach to identify additional themes. The thematic analysis was guided by best practices [7] and followed a multi-step process with two coders analyzing the data indepen-dently but frequently discussing what they were observing and mediating their understanding."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Study 1", "content": ""}, {"title": "4.1.1 Generating Bugs With LLMs.", "content": "Our results suggest that LLMs can effectively create and integrate bugs into otherwise correct code. The refusal rates across prompts were extremely low (3.46%) and this included instances where it returned correct code. Otherwise, the models were capable of producing and injecting bugs into the code. We did observe that the models often explicitly identified the bug in the code with a comment describing the bug."}, {"title": "4.1.2 Bug Frequencies by Prompt Type.", "content": "Our results also show that providing the model with information about the distribution helped to ensure the distribution more closely matched the actual distri-bution of bugs generated by students. As shown in Table 2, the Baseline Prompt, which had no information about the bug types or associated frequencies, produced code containing 68.8% of bugs that were not in the original distribution. Conversely, the Frequency-informed and Taxonomy-informed prompts produced 9.6% and 6.4% of these out-of-distribution bugs.\nBased on a Chi Square Test, we observed that the Taxonomy-informed ($x\u00b2 = 8.7$, $p < 0.05$) and Baseline ($x\u00b2 = 17.7$, $p < 0.01$) prompts produced distributions that were statistically significantly different than the distributions present in the students' code. How-ever, there was no significant difference for the Frequency-informed prompt ($x\u00b2 = 0.18$, $p = 0.91$). This suggests that the additional context was helpful in reproducing the original distribution.\nFinally, across all three prompts, we observed a bias in the model where some bugs, such as A, E, and D were amplified by the model. For example, Bug E, which is the error of adding a semi-colon after an if, for, or while statement, was common for both the Frequency-informed (8.0%) and Taxonomy-informed (11.2%) prompts despite being uncommon in the original student distributions (2.1%)"}, {"title": "4.1.3 Thematic Analysis of Out-of-Distribution Bugs.", "content": "In our initial coding, out-of-distribution bugs were labeled X. We conducted a thematic analysis on these bugs to identify what types of bugs GPT-4 injected into the code. We identified five themes and additional bugs that did not fit into those themes. The five themes are described in Table 3. The frequency of these five bugs are also reported in Table 2. Many of these errors were not compilation errors and were therefore not reported in the original study [2]."}, {"title": "4.1.4 Examples of Out-of-Distribution Bugs.", "content": "In addition to the themes from the previous section, we also observed some 'X' bugs that were less common but very interesting. For instance, when using the Baseline Prompt, GPT-4 occasionally mixed syntax from different languages, such as confusing.length() with length. Similarly, GPT-4 sometimes used boolean and bool interchange-ably, and even misspelled it as 'bolean.' These typos and syntactic confusions might reflect the types of errors students make when transitioning between programming languages [10].\nSome examples of out-of-distribution bugs were ones that would indicate considerable confusion if made by students (and thus might"}, {"title": "4.1.5 Refusal Rates.", "content": "While not common, we observed all three prompts resulted in the model refusing to add bugs to the code. The refusal rate was highest for the Taxonomy-informed prompt (5.6%) and lowest for the Frequency-informed prompt (1.6%). These differences are minor and likely driven largely by chance."}, {"title": "4.2 Study 2", "content": "The first study showed that providing information about the un-derlying distribution helped GPT-4 to replicate that distribution. However, certain types of programming problems are more prone to specific bugs. For instance, off-by-one errors are highly unlikely in code that does not involve iteration. As a result, not being able to use the same programming problems as Altadmri and Brown [2] was a limitation. To address this limitation, Study 2 focused on replicating prior work where only four programming problems (all consisting of iterating over an array) were attempted by every student in the study."}, {"title": "4.2.1 Bug Frequencies by Prompt Type.", "content": "Similar to Study 1, the Base-line prompt produced more out-of-distribution errors (44%) than the Frequency-informed and Taxonomy-informed prompts which only contained 6% and 8% of bugs that were not in the original distribution respectively. Unlike Study 1, the Baseline prompt pro-duced fewer out-of-distribution errors. This may be because the Baseline prompt for Study 2 was constrained by only asking for 'off-by-one' errors. We observed statistically significant differences between each distribution and the distribution of students' bugs. Based on Chi Square Tests (with p-values corrected using the Bon-ferroni correction), the Frequency-informed ($x\u00b2 = 69.9$, $p < 0.01$), Taxonomy-informed ($x\u00b2 = 115.7$, $p < 0.01$), and Baseline ($x\u00b2 = 76.4$,"}, {"title": "4.2.2 Examples of Out-of-Distribution Off-By-One Errors.", "content": "We ob-served in the data many interesting errors that were injected by the LLM which were not strictly 'off-by-one' errors (in the sense of loop iteration) but which did involve an adjustment (by 1) of a value or variable in the code. For instance, we observed bugs where the accumulator was decremented (or incremented) prior to it being returned by the function\nreturn count-1;\nWe also observed some unusual errors, such as using a post-decrement operator within the loop condition, leading to quite subtle bugs (and which would cause the loop to terminate earlier than a typical off-by-one):\nfor (int i = 0; i < n--; i++) {"}, {"title": "4.2.3 Refusal Rates.", "content": "Refusals were uncommon (1%-5%) and sim-ilar to Study 1, with Frequency-informed decreasing from Study 1 by 0.6%, Taxonomy-informed decreasing by 0.6% and Baseline decreasing by 0.2%."}, {"title": "5 Discussion", "content": "In this paper, we explored whether LLMs can generate realistic synthetic bugs \u2013 that is, that mirror the distribution of real bugs produced by students when working on programming problems. Our results indicated that providing the model with some guidance helped considerably to align the generated bugs to those observed in practice. In particular, providing a list of common bugs (Taxonomy-informed) tended to improve the generated distribution over not including this information, whereas including frequency informa-tion as well (Frequency-informed) provided even closer alignment. In Study 1, we found that including frequency information helped the model to produce a corpus of buggy code with a distribution of errors that was statistically similar to the original data.\nIn both studies, we used the OpenAI API when making requests to the GPT-4 LLM, thus relying on the frequency information pro-vided in our prompts across independent API calls. In contrast, a chat-based LLM such as ChatGPT could take a more sophisticated"}, {"title": "5.1 Toward Synthetic Students", "content": "We see exciting avenues for future work exploring the use of LLMs to simulate individual students. For example, very recent work has shown that leveraging LLMs to simulate students answering MCQs can support item evaluation and help educators improve question quality [32]. This suggests great potential for running simulations involving synthetic students. A class of synthetic students, with different capability and error-proneness, could complete a proposed assessment to give feedback to the instructor on its suitability. In the-ory, it may even be possible to to test interventions using synthetic students, allowing for experimentation in a controlled, risk-free environment before applying them in actual classroom settings. We elaborate on these possibilities in the following subsections:"}, {"title": "5.1.1 Cold-Start Problem.", "content": "The cold-start problem [45] occurs when intelligent tutoring systems and autograders lack sufficient data to effectively assist students. This issue arises because these systems rely heavily on historical data to generate accurate recommen-dations and feedback. LLMs can help to supplant this need or to augment historical data if it is not sufficient."}, {"title": "5.1.2 Piloting In-Class Interventions.", "content": "A persistent challenge in com-puting education research is the ethical concern of conducting in-terventions that might inadvertently harm students. One potential solution is to simulate student interactions within a classroom envi-ronment before implementing interventions. This approach allows"}, {"title": "5.1.3 Predictors of Success.", "content": "Building on prior computing educa-tion research focusing on predicting the success of students based on early performance [18, 22], one possible direction is to train LLM agents based on students in a class and then use those agents to identify students that are at risk of failing by simulating their performance through the rest of the course.\nGenerating bugs similar to those encountered by students can also be beneficial for training TAs and instructors. This approach has been explored in the context of simulating students' responses to multiple-choice questions (MCQs) [32] and training TAs by creat-ing LLM agents that ask them questions about the assignments [36]. Building on these efforts, LLMs could generate common bugs and coding design patterns, aiding TAs in identifying and addressing gaps in their knowledge before they begin working with students."}, {"title": "6 Limitations", "content": "There are a few limitations to consider in this study. First, as men-tioned in the discussion, there may be more deterministic methods for replicating the distribution of student data. The goal in this work was to understand the impact that information about the distribution has on model alignment. This is important because precise distributions are not always known and can vary based on the student population and course context [1, 2, 52]. This work also highlights that using LLMs to generate bugs without providing any information about an expected distribution will result predom-inately in irrelevant bugs."}, {"title": "7 Conclusions", "content": "In this paper, we investigated the capabilities for LLMs to produce bugs with the same distribution as students in a classroom study. Across our two studies, we observed that giving the model infor-mation about the underlying distribution improved the ability of GPT-4 to produce relevant bugs. In cases where the distribution was not provided, GPT-4 was more likely to produce out-of-distribution bugs that in some cases would likely provide limited pedagogical benefit for students. Consequently, we propose the idea of syn-thetic students, which can mimic real student errors and behaviors, offering new opportunities for teacher training and practice."}]}