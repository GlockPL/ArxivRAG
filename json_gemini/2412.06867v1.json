{"title": "Lossless Model Compression via Joint Low-Rank Factorization Optimization", "authors": ["Boyang Zhang", "Daning Cheng", "Yunquan Zhang", "Fangmin Liu", "Jiake Tian"], "abstract": "Low-rank factorization is a popular model compression technique that minimizes the error & between approximated and original weight matrices. Despite achieving performances close to the original models when 8 is optimized, a performance discrepancy remains due to the separate optimization processes for low-rank factorization and model performance, resulting in unavoidable losses. We address this issue by introducing a novel joint optimization strategy for lossless low-rank weight factorization, which, for the first time, enhances the model's performance beyond the original. Our approach begins with a theoretical analysis of the relationship between low-rank factorization and model optimization objectives, establishing a precise perturbation range for matrix factorization errors on model performance. This challenge is then reformulated as a numerical rank deficiency problem with inequality constraints and develop a joint objective that simultaneously addresses factorization error and model performance. Based on the above analysis, we propose two optimization algorithms: a lossless optimization algorithm that maximizes model accuracy while ensuring compression, and a compact optimization algorithm that minimizes model size while preserving performance. These algorithms do not require fine-tuning and can directly compress numerous deep models to achieve lossless results. Our methods demonstrate robust efficacy across various vision and language tasks. For example, the compressed model reduced by 70% on ResNext50 outperforms the original. Our code will be made public.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks have excellent performance in language and vision tasks. A common problem is the significant increase in the number of parameters, which brings challenges to deployment and inference. Matrix factorization is a common and promising compression method in the community. The idea of matrix factorization is to decompose the weight matrix into two or more smaller matrices and use these two small matrices in actual storage and calculation. Compared with other compression methods such as pruning and quantization, matrix factorization has solid mathematical theoretical support and can fully preserve the intrinsic structure and potential information of the data. Traditional matrix factorization methods include factorization, Singular Value Decomposition (SVD) [7], and eigenvalue decomposition. When matrix factorization is applied to neural networks, these methods are extended to CANDECOMP/PARAFAC (CP) [17], Tucker decomposition of kernel tensors. These methods are friendly to linear layers and are often used to compress neural networks.\nThere are two main types of existing post-training low-rank compression schemes. The first scheme is to directly decompose the existing model weights, such as SVD, CANDECOMP/PARAFAC, Tensor-Train (TT), Tucker decomposition, etc., but this type of scheme will lead to increased loss and poor model performance, such as a 5-10 times increase in loss. To improve this scheme, the second scheme is proposed, that is, after direct weight factorization, dif-"}, {"title": null, "content": "ferent strategies are used to fine-tune the model on the entire dataset. Finetuning is to decompose the model and retrain it using the entire data set. Several studies have contributed to this second scheme. For instance, Yu et al. [23] considered weight structure information and combined low-rank weight matrix and feature map reconstruction to reduce fully connected layer parameters. Xu et al. [20] integrated low-rank approximation and regularization into training with less performance loss. Yang et al. [21] proposed SVD training, decomposing each layer into a full-rank form, and then directly training the decomposed weights. Hsu et al. [5] used Fisher information to measure the importance of model parameters and weight them into singular value decomposition. Yu et al. [22] adaptively determined the structure of the compression model using a small number of training samples, thereby compressing the output features of each linear layer. Kim et al. [9] recovered the lost precision through Tucker decomposition on the nuclear tensor and fine-tuning. Zhang et al. [25] used multiple low-rank matrices to approximate a complete gated recurrent unit weight matrix and retrain.\nAlthough this scheme can improve the model performance when compressing the model, it requires a large amount of fine-tuning data and the fine-tuning process can be time-consuming. It is also not possible to compress the model losslessly, i.e., whether fine-tuned or not, the loss value of the compressed model is always greater than the original model value. This is because the optimization processes of matrix factorization and model learning are different and separate. These methods follow the \"Factorization + Finetuning\" paradigm. The optimization goal of factorization is to minimize the approximate weight error \u03b4, while the optimization goal of model learning is to minimize the loss function L. According to the \"Factorization + Finetuning\" paradigm, the factorization and fine-tuning processes are completed in two stages. Therefore, the approximate error and model loss are separate and have no direct relationship, that is, when the approximate error is minimized, the model loss is not necessarily the lowest, as shown in Figure 2. Unlike previous work, our method considers the joint low-rank factorization and model learning objectives, does not require training and fine-tuning, and establishes a joint optimization process, thereby compressing the model losslessly for the first time, as shown in Figure 1.\nBased on the above analysis, we propose a lossless joint low-rank factorization strategy without fine-tuning. Our strategy first establishes the connection between the optimization objectives of low-rank factorization and model learning in theory. The theoretical limitations in practice are then converted into inequality constraints for the optimization problem. Specifically, we determine the perturbation range of the loss caused by the factorization error in each layer of the model. Taking this perturbation range as the cal-"}, {"title": null, "content": "culus neighborhood, the connection between the low-rank factorization and the optimization of the original model is mathematically established. Then by imposing error constraints, the factorization optimization problem is converted into a numerical rank-defect optimization problem under inequality constraints, and a new objective related to model performance is proposed. For different requirements, we design two algorithms to solve this problem, lossless optimization and compact matrix optimization algorithms under numerical rank-defect. The lossless optimization algorithm aims to find the lowest loss model for the current layer under model compression. The compact matrix optimization algorithm aims to find the most compact model for the current layer under lossless conditions. It is worth noting that both algorithms do not require fine-tuning and can obtain lossless layers or models.\nIn summary, the contributions of this paper are as follows: 1) We mathematically establish the connection between factorization and model learning objectives, and propose a lossless joint low-rank factorization strategy that does not require fine-tuning. 2) We convert the traditional factorization optimization problem into a numerical rank-defect optimization problem under inequality constraints and propose two algorithms for different requirements. 3) Extensive experimental results show that our method can ensure compression while obtaining lossless models."}, {"title": "2. Background", "content": "Neural Networks and Optimization. We present the analysis of neural networks as composite functions. All our conclusions are independent of the structure of the neural network. First, for an n-layer neural network model, the loss of the model is optimized according to the following formula\n\\min_{W} f(W) = E_{Sample \\sim l(W, Sample)} = \\frac{1}{m} \\sum_{(x_i,Y_i) \\in D}l(W, x_i, Y_i),\nl(W, x_i, Y_i) = L(model_n(x_i, W), Y_i),\nmodel_n = h_1(h_2(h_3(h_4(\\cdot\\cdot\\cdot(h_{n+1}, W_n)\\cdot\\cdot\\cdot, W_4), w_3), w_2), w_1), \\quad(1)\nwhere f(\u00b7) represents the loss of the model on a dataset, E stands for expectation, m is the size of the dataset, l(\u00b7) is the loss function for a sample, and (xi, Yi) denotes a sample in the dataset along with its corresponding label, L(\u00b7)"}, {"title": null, "content": "represents the loss function, such as the cross-entropy function; hi, with i \u2208 [1, ..., n] represents the (n \u2212 i + 1)th layer in the neural network, W = (w_1, W_{n-1},..., w_i, ..., w_n), where wi is the parameter in h\u2081(\u00b7), and for the reason of a unified format, hn+1 denotes the sample x.\nLow-rank Factorization and Optimization. Low-rank factorization can be adopted to reduce redundancy in weights. For the weight matrices W \u2208 R^{N\u00d7M}, the low-rank factorization is achieved by two low-rank matrices\nW \u2248 LR^T, \\quad (2)\nwhere L \u2208 R^{N\u00d7k},R \u2208 R^{M\u00d7k},k is the rank of W, denoted as an integer between 1 and min(N, M). Given input data x \u2208 R^{1\u00d7\u00d1}, a linear layer in the neural network can be represented as\nY = Wx + b \u2248 Wx + b = LR^Tx + b, \\quad (3)\nwhere b is the bias, Y is the output of the linear layer, and the factorization of W is obtained from Eq. 1. With Eq. 2, we can store and compute L and R instead of W. The total number of parameters of L and R is Nk+Mk. The reduced parameters and computation are NM \u2013 (Nk+Mk). When the weight matrix M = N, the rank k is less than 0.5M and the model size will be reduced. Similarly, for singular value decomposition, W is approximated as W = USVT, where U and V are orthonormal, and S is diagonal.\nThe optimization objective of the low-rank factorization is to minimize the Frobenius norm under the rank is most k\n\\min_{L,R} ||W - W||_F, \\quad (4)\nit implies that we can find a basis set for an optimal k rank approximation in a greedy way.\nAs shown in Eq. 1 and Eq. 4, the optimization objective of low-rank factorization is different from the model optimization objective. Although this low-rank optimization objective can best approximate the weight matrix, it does not ensure that the model achieves the lowest loss value. The \"Factorization + Finetuning\" paradigm separates the two optimization processes, resulting in reduced model performance."}, {"title": "3. Lossless Joint Factorization Strategy", "content": "Theoretical Optimization. The key to lossless factorization optimization is how to analyze low-rank factorization in model optimization. As shown in Figure 1, mathematically, the nature of low-rank factorization is a process that introduces noise to the weight parameters in the original model or layer. After factorization, for a sample, the model loss l during inference is reformulated as the following equation\nl_k(w, x_j, y_j) = L(h_1(h_2(\\cdot\\cdot\\cdot h_n(x_i, W_n + \u03b4^n)\\cdot\\cdot\\cdot, w_2 + \u03b4^2), w_1 + \u03b4^1), y_i), \\quad(5)\nwhere di \u2208 [1,......,n] denotes the noise error on the weights after the rank k factorization. The error \u03b4 caused by low-rank factorization ultimately leads to changes in model loss. This error can be seen as an increment in weight w.\nLemma 1 The weight increment of the loss function at a certain point can be estimated by the sum of the products of each partial derivative and a small change in the weight variable.\nLemma 1 is the expression of the concept of total differential in weight factorization in neural networks. Then, through the definition of total differential [11] and Lemma 1, for the differentiable function l with the small variable d, the following equation is established\nl_k (W, x_i, y_i) - l(w, x_i, y_i) = \\sum_{i=1}^n \\frac{Il_k}{\u03b4w_i} \u03b4_i \\quad(6)\nwhere is the inner product. For the loss on the entire dataset, the optimization objective is written as\n\\min_{\u03b4 \\in \u0394} f(w) \u2013 f(w) = -\\sum_{i=1}^n \\frac{1}{m} \\sum_{(x_j,Y_j) \\in D} \\frac{Il_k}{\u03b4w_i}\u03b4_i \\quad(7)\nwhere f(w) = \u2211lk(\u00b7), \u25b3 is the full set of \u03b4. This optimization objective incorporates the noise error from the factorization into the model loss. Eq. 7 needs to satisfy the loss continuity and differentiability, and the continuity of the partial derivatives. The most important thing is that the change is small enough because when the change is small enough, the total differential provides a good approximation. This will be discussed in the next section.\nEq. 7 expresses the loss difference before and after the factorization as the product of the gradient vector and the noise vector. When the inner product is negative when the two vectors are in opposite directions, i.e. \\frac{\u03b4 l_k}{\u03b4w_i} \u03b4 < 0, the model loss after factorization will be less than the original model. Thus, the goal of lossless joint low-rank factorization is achieved.\nTheoretical Conditions and Practical Mapping. For the theoretical validity of Eq. 7, specific conditions must be met, particularly regarding the requirements for mathematical differentiation. Since the loss in neural networks is continuously differentiable with a continuous gradient, we focus on the constraints associated with changes in w. It is necessary to analyze the size of the neighborhood when applying differential expansions to differentiable functions.\nLemma 2 The local linear approximation of the change in function value is only valid if the change in weights is small enough.\nCorresponding to Lemma 2, usually in mathematics, a neighborhood is a sufficiently small range e around the point of expansion. So in theory, Eq. 7 needs to satisfy the neighborhood U\u03b4k(xi) : |\u03b4k| < \u2208.\nIn practice, how to obtain this neighborhood is the prerequisite for the effectiveness of our method. Therefore we"}, {"title": null, "content": "calculated the gap between the loss values in Eq. 7 under theoretical analysis and in practical engineering.\nU\u03b4k (xi) : |l(w\u00b1d, xi, Yi) - (l(W, xi, Yi)+\u2211\\frac{Ilk}{\u03b4wi}\u03b4_i)|, (8)\nThe meaning of the above equation is to verify under what range of neighborhoods the theoretical derivation of Eq. 6 can be implemented in practical experiments. The left side of the minus sign is the loss under practical engineering for a specific layer with parameters added to the factorization noise 8k, and the right side is the loss in the theoretical analysis for the weight gradient perturbation, corresponding to Eq. 6. We consider using the above equation to determine what the rank is, the noise 8k caused by the factorization can make this difference small enough so that Eq. 6 or Eq. 7 holds. Through multiple rounds of experiments, we find that this theoretical and practical difference is sufficiently small to be less than 0.0001 when the noise introduced by the decomposition is dk < \u0454 \u2208 O(10-3). This ensures that our analysis is valid. Please see the experimental section for specific values and schemes. The determination of the neighborhood ensures the representation of low-rank factorization in total differentials, and we map the theory of total differentials into practice.\nPractical Constraints. By mapping the total differential theory to practice, lossless joint low-rank factorization needs to ensure two conditions called the compression condition and the lossless condition. The compression condition is generated to efficiently compress the model, i.e., the number of parameters of the original matrix in Eq. 2 is smaller than the number of parameters of the approximation matrix, 0 < k < \\frac{NM}{(N + M)}. The lossless condition is generated to efficiently reduce the loss of the model, which corresponds to the condition for the differentiation to hold. The noise sk after factorization should be less than \u2208, i.e. {||Wij - lijrij ||}i,j \u2264 6, where represented by the set of matrix elements, lijrij represents the i, jth element of the approximation matrix LRT. Due to the directionality of the gradient vector, we use the matrix F norm for calculation. This restriction means that every element in the difference set of all original matrices and approximate matrices needs to be smaller than e. Lossless low-rank factorization optimization needs to satisfy these two inequality constraints at the same time, which is updated to the following formula\n\\min_{\u03b4 \\in E \u0394} f(w) \u2013 f(w) = \\frac{1}{m}\u2211\\frac{Ilk}{\u03b4w_i}\u03b4_i \\quad (9)\ns.t. U\u03b4k : {||Wij \u2013 lijrij||F}i,j \u2264 e, Vi, j, \\quad(9a)\n0<k<\\frac{NM}{N+M} \\quad(9b)"}, {"title": null, "content": "It is worth noting that our goal is still to compress the model. So the compression condition should be satisfied before the lossless condition. Secondly, our algorithm is flexible, and the lossless condition expression can also be replaced by other decompositions, such as w = usv. However, when replaced by other decompositions, the number of parameters and time may increase. Eq. 9 is the simplest expression of the effective factorization method.\nTheoretical optimization ensures the possibility of lossless decomposition, while practical mapping provides a viable solution for lossless compression. The lossless strategy does not rely on assumptions; the full differential analysis is theoretically guaranteed and validated through practice, and the noise constraints are computed based on specific models rather than assumptions.\nGradient and Higher-order Terms. When optimizing Eq. 9, we also need to analyze gradients and higher-order terms. Theoretically, for a well-trained model, the expectation of l()'s gradient for parameters is ideally zero, i.e., for the \\frac{\u03b4 l}{\u03b4w_i} components, \\frac{\u03b4 l}{\u03b4w_i} \u2248 0. But in practice, we found that almost no model weight gradient is 0. This also ensures the feasibility of our algorithm. Moreover, the second-order term is influenced by the infinitesimal higher-order term. So we focus on first-order elements, and higher-order terms will not cause major performance changes when they are ignored."}, {"title": "4. Lossless and Compact Optimization", "content": "Based on the above analysis and optimization strategies, we propose two algorithms with different strategies, lossless and compact matrix optimization algorithms. Lossless and compressive factorization algorithms are aggressive probabilistic algorithms that do their best. The lossless factorization algorithm focuses on minimizing the loss function value while keeping the model size no larger than a full-precision model. The compressed matrix algorithm aims to minimize the model size while ensuring that the loss function value is not larger than that of the full precision model. Both algorithms are greedy algorithms to achieve the goal of minimizing the loss function value or model size.\nIn Algorithm 1 and Algorithm 2, our goal is to find the noise introduced by low rank that is opposite to the gradient direction, so that the inner product of the two is negative and reduces the model loss. In Algorithm 1, the priority is to achieve a low loss function value to obtain the best performance factorization model. In Algorithm 2, priority is given to choosing a lower rank to obtain a smaller factorization model."}, {"title": null, "content": "Generalization error upper bound. Although the algorithm will reduce loss, we need to discuss the upper bound of the generalization error under the algorithm. Lemma 3 guarantees the existence of our algorithm so that the weights can be decomposed. We can prove it by Pointwise Hypothesis Stability [3].\nLemma 3 The upper bound of the generalization error R decreases with decreasing effective rank. After reducing the effec-"}, {"title": null, "content": "tive rank to a certain value, the upper bound of the generalization error will increase as the rank decreases further.\nproof: Consider A as our algorithm and (X, Y) as a dataset of length s. In addition, consider the ratio of the effective rank to the original rank as p (where 1 p is the sparsity parameter). The generalization error upper bound can be calculated by assuming the pointwise stability equation. We have for a constant T with a probability 1 \u2013 \u03b2,\nR(A, X, Y) < R(A) + \\sqrt{ \\frac{T^2+ \\sqrt{\\frac{24T^2}{Amin}+2(1-p)}}{2s\u03b2}}, (10)\nwhere R(A) represents the emperical error, and Amin represents the minimum eign-value of the loss Hermitian matrix. For the loss Hermitian matrix [13], since the model has been trained, Amin \u2248 0. Based on the above equation, we can observe that when the effective rank ratio p is low and the sparsity (1 \u2013 p) is relatively high, the generalization error decreases with increasing sparsity. However, as the effective rank decreases and the sparsity increases, there comes a point where the number of trainable parameters is much lower than the number required to represent the distribution of the dataset, resulting in underfitting. Therefore, there is an optimal effective rank that allows the weights to be decomposed. This ensures that the algorithm is effective."}, {"title": "5. Experiments", "content": "5.1. Datasets and Details\nDatasets. The ImageNet-1K dataset [10] consists of 1.28M training and 50K validation images. ImageNet-1K is usually used as the benchmark for model compression. SWAG dataset [24] consists of 113K multiple-choice questions about grounded situations. The Stanford Question Answering Dataset (SQuAD) [12] is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers to questions can be any sequence of tokens in the given text. MNLI [18] is a dataset for natural language reasoning tasks. Its corpus is a collection of textual implication annotations of sentences through crowdsourcing. The task is to predict whether the premise sentence and the hypothesis sentence are logically compatible (entailment, contradiction, neutral). CoNLL-2003 [14] is a named entity recognition dataset released as a part of CONLL-2003 shared task: language-independent named entity recognition. The data consists of eight files covering two languages: English and German.\nDetails. Our method is implemented in Pytorch, and does not require fine-tuning and retraining. Following existing compression work, we use the ImageNet validation set as the calibration set, computing the gradient once to check the gradient values rather than updating the weights, and based on this, we determine the decomposition rank for"}, {"title": null, "content": "each layer. Use the same optimization settings for all experiments in this paper and avoid any hyperparameter filtering to ensure a fair comparison. The experiments are run on a single NVIDIA V100 GPU."}, {"title": "5.2. Lossless and Comparative Experiments", "content": "5.2.1. Lossless Experiments\nIn the lossless experiments, we primarily validate the lossless decomposition effects of two algorithms using models of different scales. As a comparative baseline, we select the SVD method, as SVD focuses on optimizing the gaps between factorized matrices while neglecting model optimization. Our optimization goal takes into account both factorization matrix optimization and model optimization.\nTable 1 shows the accuracy and loss results of our algorithm on Imagenet. We factorize all linear weight matrices in the visual classifier. We conduct experiments on models of different types and depths. For models of different depths, our lossless factorization and compression factorization methods can perform lossless factorization on both"}, {"title": null, "content": "shallow and deep models. Compared with deep models, shallow models have less parameter redundancy and are more difficult to compress. Our algorithm is also effective for shallow models. Although the SVD algorithm can approximate the original matrix and reduce the approximation error, it will increase the loss of the model. This also shows that in \"Factorization + Finetuning\", reducing the approximation error and reducing the loss value are not linearly related. Our algorithm establishes the relationship between factorization and model learning and establishes a lossless joint optimization goal. The loss value of our algorithm is not only smaller than the SVD method, but also smaller than the original model loss, and the accuracy is comparable to or even higher than the original model. The lossless compression results of different models also prove the generalization of our algorithm. Secondly, the change in loss is related to the weight gradient and the noise amplitude. When the amplitude is larger, the loss decreases more, such as ResNext101_32x4d. In ResNext101_32x4d, the loss of the model after lossless factorization is reduced compared with the original model, and the compression is completed. Compared with the original model, the compact optimization algorithm (Ours-C) can reduce 81% of parameters and is less than the loss of the original model. Since the algorithm is mathematically interpretable, it is independent of the model architecture."}, {"title": "5.2.2. Comparison Experiments", "content": "In comparative experiments, we conduct experiments on tasks in different fields to fully verify the effectiveness.\nImage Classification. Table 2 shows the comparison of our algorithm with existing decomposition methods on Imagenet. We also fine-tune some methods (SVD_ft). The experiments show that our lossless algorithm achieves the lowest loss, and the compression algorithm achieves the highest accuracy while ensuring that the loss remains almost unchanged. Unlike these methods, our algorithm only requires the gradient direction and does not need to use the gradient for update calculation. Our algorithm can compress the model losslessly while running in the shortest time and using the least data. Figure 3 shows our lossless fac-"}, {"title": null, "content": "torization strategy in inference when 0 < rank < \\frac{NM}{N+M} is satisfied. The red line in the figure represents the loss-rank curve of our algorithm, and the orange line represents the loss-rank curve of the SVD algorithm. As the rank decreases, the loss shows an increasing trend. The intersection point of the model loss straight line and our algorithm curve is the lowest rank when the model loss is almost the same. The compression factorization optimization algorithm does its best to compress the model while ensuring that the loss remains unchanged. Under the constraints of compression conditions, the lowest point of our algorithm is the minimum loss of the curve. The lossless factorization method does its best to reduce model loss while ensuring compression. The different changes in loss curves also correspond to Lemma 3. The algorithm finds an effective rank less than full rank so that the weights can be decomposed.\nTable 3 shows the comparison between our algorithm and quantization method. ACIQ uses int8 for quantization, which improves the loss of the model. Compared with ACIQ, our algorithm has a higher compression rate at a similar running time. Our algorithm can choose different factorization strategies as needed."}, {"title": "Natural Language Processing", "content": "We perform lossless factorization of the Bert model on natural language process-"}, {"title": "5.3. Ablation", "content": "Optimization Objective. In Table 6, when the algorithm optimizes for objectives that do not contain gradients, the loss increases. This illustrates the fact that approximating"}, {"title": null, "content": "the weights through the original optimization objective increases the loss. The lossless factorization algorithm uses differential analysis to establish relationship between model performance loss and traditional factorization optimization and identifies a new optimization objective. It demonstrates the effectiveness of our optimization objective."}, {"title": null, "content": "Theory to Practice Mapping and Constraints. We focus on whether the noise introduced by practical low-rank factorization satisfies our differential neighborhoods theory analysis. Our differential theory conditions that the neighborhoods need to be sufficiently small, and this corresponds to the performance implications of the actual factorization. We factorize the transformer layer. From Table 7, the factorization noise of the model's layers with different ranks leads to different upper bounds on the noise, and the variation in losses A Loss due to this noise is very small. This satisfies our analysis of differential neighborhoods. When too much noise is introduced, the change in loss A Loss is too large, which is also beyond the analytical scope of mathematical differentiation. Low rank introduces noise well beyond our lossless constraints. As shown in Figure 3, when the rank is too low, the noise introduced by factorization is too large, and the lossless restriction conditions in the optimization objective fail, causing the loss to rise beyond the original model. This makes our algorithm incapable of lossless factorization at current very low ranks.\nHigher-Order Term Effects and Weight Gradients. In theoretical analysis, we found the first-order term is the main change causing model loss, while the influence of the second-order term is infinitesimal higher-order terms. We confirm this in our experiments, where the effect of the second-order term on the loss is generally less than O(1e-5) when the same le-3 noise is introduced into the second-order term, while the effect of the first-order term on the loss is roughly 100 times more than the second-order term. The second-order term causes a change in accuracy in the range of about 0.001. So we mainly analyze the first-order term. According to theoretical analysis, the ideal trained model weight gradient should be 0, but in practice, it is difficult to train a model so well that the gradient is 0. We experimentally found in DenseNet201, for example, about 99% of the"}, {"title": "5.4. Discussion", "content": "Optimization Objectives and Loss. Calculus leads to the analysis of lossless factorization. Eq. 9 is one form of many solvable functions. We can further expand the optimization objective into the Eq. 11\n\\min G(lij, rij)  (11)\ns.t. U\u03b4k : {||Wij - lijrij||F}i,j \u2264 e, Vi, j , (11a)\n0<k<\\frac{NM}{N+M}  (11b)\nWhere G() represents the set of functions that satisfy the constraints. We solve the conditions for lossless factorization through total differentials, but the choice of factorization rank is discrete at this time, and we cannot solve it directly in finite space and time. To solve this problem, we need to construct a concrete function to describe G(). At this point, we utilize Eq. 7 to describe this function. Our lossless algorithm is targeted at loss. Compared with other metrics, almost all tasks and models cannot be separated from analysis losses. Therefore, our algorithm compresses various models well to improve task performance.\nStructural Independence and Factorization Independence. Our algorithm is derived from total differential analysis. For any model, we can write it in the form of a composite function and is therefore independent of any deep model structure. Our algorithm is also factorization-independent, that is, the algorithm does not rely on a specific low-rank factorization method. In Eq. 9, the factorization method can be replaced by other existing factorization methods. The product of noise and gradient depends on their respective directions. Therefore, our algorithm is universal when facing different replacement models or factorization methods. However, our experiments show that compared with other factorization methods, the original matrix factorization has the least parameters and running time.\nExtensibility and Future. As shown in Table 8, our two algorithms are effectively extended to existing general vision and language models on Imagenet and MMLU, and remain lossless while compressing nearly 30%. For large language models, we can still perform lossless decomposition."}, {"title": null, "content": "In future work, we will continue to expand the algorithm to large language models with larger parameter amounts.\nLimitation. Due to varying layer sensitivities, lossless algorithm may find the same low-rank matrix with compression algorithm at the lowest loss. However when the rank is too low under extreme compression, factorization noise exceeds the differential domain, preventing lossless factorization. Therefore, we focus on achieving stable lossless factorization."}, {"title": "6. Conclusion", "content": "This paper establishes the connection between the optimization of low-rank factorization and model optimization and converts the low-rank factorization problem into a numerical rank-defect problem under inequality constraints. We propose a lossless optimization algorithm and a compressed matrix optimization algorithm that does not require fine-tuning. Experiments show the effectiveness and generalization of our algorithm across various tasks."}]}