{"title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating", "authors": ["Chao Deng", "Jiale Yuan", "Pi Bu", "Peijie Wang", "Zhong-Zhi Li", "Jian Xu", "Xiao-Hui Li", "Yuan Gao", "Jun Song", "Bo Zheng", "Cheng-Lin Liu"], "abstract": "Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark\u2014LongDocURL-integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly out-performing existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field.", "sections": [{"title": "Introduction", "content": "The research of document understanding has been advanced remarkably in the last decade. However, past works mostly rely on smaller, specialized models, necessitating the design of independent models for each specific task (e.g., document structure parsing). This strategy not only increases the labor in model development but also limits the applicability of the models. Recently, this field has undergone transformation with the rise of large language models (LLMs) and large vision-language models (LVLMs), such as the open-source InternLM-XC2-4KHD (Dong et al., 2024b) and TextMonkey (Liu et al., 2024). These models are showcasing their capabilities in handling complex document elements like charts and images, managing longer contexts up to 128k or more, and tackling diverse tasks in addition to OCR task, such as table question answering and layout understanding.\nDespite the advances in model capabilities, the evaluation of complex document tasks is somewhat lacking. Take DocVQA (Mathew et al., 2021) as an example; it is one of the standard benchmarks for document understanding, but it can only assess single-page documents, and many models can easily exceed an accuracy of 95% (e.g., Qwen2-VL (Alibaba, 2024)). The benchmarks detailed in Table 1 exhibit limitations in adequately addressing complex elements, longer contexts, and diverse tasks: 1) Complex elements: Most benchmarks fail to cover all elements such as paragraphs, titles, tables, and figures, focusing instead on only some of the contents. Additionally, discussions about the interrelations among different elements are scarce. 2) Longer contexts: Current benchmarks for multi-page document question answering, such as MP-DocVQA (Tito et al., 2022) and DUDE (Landeghem et al., 2023), do not assess documents exceeding 20 pages. While MMLongBench-Doc (Ma et al., 2024) collects longer documents, it offers only approximately 1k effective samples, with only about 30% of questions involving cross-page information. 3) Diverse tasks: Existing work focuses more on OCR or easy question-answering tasks, neglecting the exploration of capabilities in other areas such as cross-element locating task (as shown in Figure 2c). The above findings indicate that existing benchmarks lag behind the advances of models, which could hinder the development of document understanding.\nIn this paper, we present a comprehensive document benchmark including three task categories: 1) Understanding: extracting information from documents by identifying keywords, parsing the structure of tables, etc. Answers are found directly in the document. 2) Numerical Reasoning: processing numerical information through counting, calculating, comparing, and summarizing, requiring both extracted information and reasoning for concluding. 3) Cross-Element Locating: As mentioned earlier, discussions about the interrelations among different types of elements are scarce. It is often necessary to establish a task that evaluates models' ability to analyze relations among different types of elements. For instance, in Para-Title Locating task, as shown in Figure 2c, models must summarize relevant sections to identify parts that match a given abstract and then determine the relation between the paragraph and its section titles. This task requires switching element types (i.e., paragraphs to titles) during the answering process.\nOur benchmark, named LongDocURL, comprises 20 sub-tasks according to different primary tasks and answer evidence. More details are presented in Section 3. To efficiently assemble the evaluation dataset for LongDocURL, we design a semi-automated pipeline comprising four modules. Specifically, a Extract & Filter module identifies documents of suitable length with rich layouts from diverse document sources. A QA Generation module utilizes a multi-step iterative querying process with advanced models (e.g., GPT-40) to generate QA pairs with evidence sources. Finally, the Automated Verification and Human Verification modules ensure the quality of the generated content. Through this semi-automated pipeline, we ultimately produce 2,325 QA pairs, covering more than 33,000 pages of documents. Thereafter, we conduct comprehensive evaluation experiments with 26 different configurations (varying the model and input format). These evaluation results indicate that the highest-performing closed-source model, GPT-4o, scored 64.5, leading all models, while the best score for open-source models is only 30.6. This result reveals a potential gap in document understanding and shows the need for further improvement. Our contributions are as follows:\n\u2022 We introduce three primary tasks of long document and propose a comprehensive benchmark comprising 20 sub-tasks categorized based on"}, {"title": "Related Work", "content": "Models for Document Understanding. There are two main types of language models for document understanding: (1) OCR-dependent models, which use Optical Character Recognition (OCR) to extract text for processing, including the LayoutLM series (Xu et al., 2020, 2021; Huang et al., 2022) and text-only LLMs (Meta, 2024; QwenTeam, 2024); and (2) end-to-end models, which use a visual encoder to extract features from document images, integrating them with text for input into language model backbones. Most of document-related LVLMs fall into this category, such as GPT40 (OpenAI, 2024), Gemini-1.5 (GeminiTeam, 2024), Claude-3.5-Sonnet (Anthropic, 2024), mPLUG-DocOwl2 (Hu et al., 2024), Qwen2-VL (Alibaba, 2024).\nMethods for Long Document Understanding. To address the challenges of cross-page document understanding with excessively long context lengths, early approaches employed hierarchical encoding methods (Tito et al., 2022; Kang et al., 2024; Cho et al., 2021; Dong et al., 2024a). In these approaches, an independent encoder processes the OCR text and visual modal information for each page, which is then passed to a small language model decoder for cross-page contextual learning. However, this approach is limited by the redundancy in OCR inputs, which restricts the context length and leads to the accumulation of errors (Xu et al., 2020, 2021; Appalaraju et al., 2021). Recently, with the rise of multimodal large models, methods based on multimodal Retrieval-Augmented Generation (MM-RAG) (Yu et al., 2024; Blau et al., 2024; Ding et al., 2024a; Zhang et al., 2024a,b; Cho et al., 2024) and end-to-end multi-page large models (Jiang et al., 2024; Li et al., 2024; Jia et al., 2024) have emerged. These models leverage the world knowledge of large language models to enhance understanding. End-to-end approaches mitigate error accumulation by dynamically reducing the number of visual tokens across multiple pages/images and build large scale instruction turning dataset (Jiang et al., 2024; Li et al., 2024; Jia et al., 2024; Hu et al., 2024), allowing for longer context lengths. Methods such as multi-page RAG facilitate dynamic interactions with OCR and other text information to remove redundant multimodal tokens.\nBenchmarks for Long Document Understanding. Multi-page or long documents place higher demands on the model's capabilities in cross-page understanding. Current multi-page document benchmarks, such as MP-DocVQA and DUDE, do not assess documents exceeding 20 pages. MMLongBench-Doc (Ma et al., 2024) and M-Longdoc (Chia et al., 2024) have been proposed to evaluate the understanding capabilities of longer documents, which have an average of 47.5 and 210.8 pages per document, respectively. Meanwhile, MMVQA (Ding et al., 2024a) and MVQA (Ding et al., 2024b) are proposed to better evaluate the retrieval-based method. WebQuest (Wang et al., 2024) focuses on evaluating models' performance on text-rich web images."}, {"title": "LongDocURL", "content": "Firstly, each question-answering pair can be categorized by three primary tasks: Understanding, Reasoning, and Locating, as discussed in Section 1. Secondly, we define four types of answer evidences based on element type: (1) Text: pure texts, such as paragraph; (2) Layout: text elements with special layout meaning (generalized text), such as title, header, footer, table name and figure name; (3) Figure: including charts and general images. (4) Table. In addition, each question-answering pair can be classified into single-page or multi-page based on the number of answer evidence pages and single-element or cross-element based on the number of types of evidence elements. Based on different primary tasks and answer evidences, we divide our dataset into 20 sub-tasks. As shown in Figure 4, for the Understanding or Reasoning task, we divide our dataset into 8 sub-tasks according to the number of answer evidence pages. Compared to the two tasks, we pay more attention to the inter-relations among different types of elements in the Locating task and we build 4 sub-tasks based on the combination of different element types. Data examples are presented in Appendix F."}, {"title": "Q&A Construction", "content": "To objectively evaluate LVLM long document question-answering comprehension ability, we first crawl 200k PDF-formatted documents from CommonCrawl\u00b9 and filter them by page length(i.e., 50~150) and language(i.e., English) to create a candidate set of approximately 3,000 documents. Then, we categorize these candidates by document type. Specifically, we randomly select 5~10 pages from a document, and prompt GPT-40 to classify its document type based on document content and layout. We finally retain 396 documents to construct our benchmark. These documents span eight types: research reports & papers, user manuals & guides, books & e-books, theses & dissertations, work & project summaries, presentation materials, project proposals, and meeting minutes & summaries, with an average of 85.6 pages per document.\nThereafter, we utilize both PyMuPDF\u00b2 and Docmind\u00b3 to parse the PDF and extract texts and layout information from the documents. For instance, the tables are converted into markdown format with Docmind. We organize the extracted results in the format of text-type-bbox triples as a symbolic representation of elements: 1) text: the recognized text of the elements; 2) type: the element type. 3) bbox: the bounding box of the element. Notably, we build the element triples at the region level, such as paragraph, table, chart, footnote and title, instead of the line level."}, {"title": "Evidence Collection", "content": "To objectively evaluate LVLM long document question-answering comprehension ability, we first crawl 200k PDF-formatted documents from CommonCrawl\u00b9 and filter them by page length(i.e., 50~150) and language(i.e., English) to create a candidate set of approximately 3,000 documents. Then, we categorize these candidates by document type. Specifically, we randomly select 5~10 pages from a document, and prompt GPT-40 to classify its document type based on document content and layout. We finally retain 396 documents to construct our benchmark. These documents span eight types: research reports & papers, user manuals & guides, books & e-books, theses & dissertations, work & project summaries, presentation materials, project proposals, and meeting minutes & summaries, with an average of 85.6 pages per document.\nThereafter, we utilize both PyMuPDF\u00b2 and Docmind\u00b3 to parse the PDF and extract texts and layout information from the documents. For instance, the tables are converted into markdown format with Docmind. We organize the extracted results in the format of text-type-bbox triples as a symbolic representation of elements: 1) text: the recognized text of the elements; 2) type: the element type. 3) bbox: the bounding box of the element. Notably, we build the element triples at the region level, such as paragraph, table, chart, footnote and title, instead of the line level."}, {"title": "Q&A Generation", "content": "Directly prompting LVLM to generate conversational question-answering pairs based on a single or multiple document images proves often ineffective, due to the inability to fully parse diverse elements present in the documents. Similar to LLaVA (Liu et al., 2023), we adopt a two-stage pipeline: Initially, we parse our PDF-formatted documents and get the \"text-type-bbox\" triples discussed in Section 3.2.1. Subsequently, we design prompts to query LLMs/LVLMs in a multi-step round, and finally generate question-answering pairs. Specifically, as shown in Figure 3, we present the definition of each task and description of related restriction as a part of the prompts. Details can be found in Appendix B."}, {"title": "Q&A Verification", "content": "We design an automated method to verify the quality of synthesized question-answering pairs to identify and correct corresponding issues. As shown in Figure 3, a qualified question-answering pair should be verified using these criteria: (1) Task Relevance. (2) Format Correctness. (3) Faithfulness.\nThe verification results are utilized to classify samples: those that fail are marked as negative, and those that pass are marked as positive. We analyze the verification results and observe approximately"}, {"title": "Automated Verification", "content": "We design an automated method to verify the quality of synthesized question-answering pairs to identify and correct corresponding issues. As shown in Figure 3, a qualified question-answering pair should be verified using these criteria: (1) Task Relevance. (2) Format Correctness. (3) Faithfulness.\nThe verification results are utilized to classify samples: those that fail are marked as negative, and those that pass are marked as positive. We analyze the verification results and observe approximately"}, {"title": "Human Verification", "content": "There are two shortcomings of automated verification in Section 3.3.1: (1) It only completes the classification of positive and negative samples, but does not recycle them, which causes waste. (2) When verifying the consistency between the question-answer pair and the document, only the text information is referenced, which may lost significant visual structure information compared to the source document. In the human verification stage, we focus on tasks that are challenging for"}, {"title": "Dataset Statistics", "content": "As shown in Table 2 and Figure 6, our Long-DocURL comprises 396 documents and 2,325 question-answering samples. The average pages per document range from 50 to 150. The cross-element locating task we designed contributes 37.1% of the data, providing support for evaluating the model's cross-element interaction capabilities in long contexts. The dataset includes five types of answers, ensuring compatibility with automated evaluation while maintaining completeness and accuracy. In addition, we compare the characteristics of question set in our LongDocURL with that in MMLongBench-Doc. The results are presented in Appendix C."}, {"title": "Experiments", "content": "Allowing models to think freely can enhance performance but often results in lengthy responses. Current rule-based scorers are only effective with specific short formats, making it difficult to evaluate longer responses. Therefore, we need an answer extractor (GPT40) to convert detailed responses into concise ones. Following MATHVISTA (Lu et al.,"}, {"title": "Evaluation Protocols", "content": "Allowing models to think freely can enhance performance but often results in lengthy responses. Current rule-based scorers are only effective with specific short formats, making it difficult to evaluate longer responses. Therefore, we need an answer extractor (GPT40) to convert detailed responses into concise ones. Following MATHVISTA (Lu et al.,"}, {"title": "Experimental Setup", "content": "Models We divide the experiments into two categories: text-input and image-input. For text-input configuration, document texts parsed by OCR engines are input into LLMs/LVLMs. We conduct our experiments on both open-source and closed-source models across 26 different configurations.\nInput Paradigm The documents in our Long-DocURL have a max of 150 pages, and most of models are unable to fully process the context to obtain the answer due to GPU memory or interface limitations. Therefore, similar to the merge method used in previous benchmarks (Ma et al., 2024), we design the cut-off paradigm for the evaluation of current models.\nFor LVLMs, we cut 30 continuous pages around the answer evidences from the original PDF-formatted document, and feed the converted images into the models. Details of selection rules are in Appendix D.2. As for LLMs, we input the texts parsed by OCR engines, including PyMuPDF and Docmind.\nOther Configurations We assess the proprietary models using API resources, while the evaluation of the open-source models is conducted on H20 machines with 96G memory. To reduce variance, we set the temperature coefficient to 0.0 when generating free-form response and extracting short answer."}, {"title": "Main Results", "content": "As shown in Table 3 and Table 7, we calculate generalized accuracy scores to assess model capabilities. Regarding LVLMs, we draw the following conclusions: (1) Highest Scoring Model: Only GPT-40 meets the passing standard and scores highest at 64.5, indicating that our Long-DocURL presents significant challenges for current models. (2) Comparison of Open-source and Closed-source models: Proprietary models demonstrate better overall performance compared to open-source models. Among open-source models, only Qwen2-VL (score 30.6) and LLaVA-OneVision"}, {"title": "Fine-Grained Analysis", "content": "We include more fine-grained results in Table 7 and Figure 5, based on document sources, task categories, document elements, and evidence pages.\nDocument Sources As shown in Figure 5, the models perform better on books, reports, manuls, and project proposals, likely due to their simpler layout, which facilitates key information extraction. Conversely, the models struggle with less common documents like meeting minutes and work summary, which have limited data.\nTask Type Observations reveal: (1) Proprietary LVLMs perform comparably on reasoning and locating tasks, but image-to-text conversion impacts reasoning capabilities more severely. For instance, switching to text input, GPT-40's reasoning scores drop by 31.6 points versus 22.4 points for locating. (2) Strong models are balanced in reasoning and locating, whereas weaker models perform poorly on locating, suggesting a training focus on capabilities of understanding and reasoning over spatial and logical relationships in locating tasks.\nDocument Elements Models score highest on Text questions and lowest on Table ones, highlighting deficiencies in document structure parsing. Figure and Layout question types yield similar scores. Scores for cross-element tasks fall between single-page and multi-page QA, closely aligning with the overall assessment.\nSingle-page vs Multi-page Single-page QA accuracy is lower than multi-page QA. This reveals that answers for some questions can be gathered from multiple pages, thereby reducing the difficulty. However, models like GPT-40 and Qwen-VL-Max show lower accuracy on multi-page QA, revealing a contradiction where their scores on locating tasks in multi-page QA are lower, thus skewing overall performance."}, {"title": "Ablation of Input Paradigms", "content": "To explore the optimal input format in long document question-answering, we conduct ablation experiments across two image-input and two text-input paradigms. The image-input paradigms include: (1) cut-off, following the configuration detailed in Section 4.2, and (2) merge, where document images are combined from raw document lengths (50~150) into 20~30 new images. Further details can be found in Appendix D.1.\nWe note that the table structure information significantly degrades when parsed by PyMuPDF, while the markdown-format table texts parsed by Docmind retain greater structural integrity. To assess the impact of structural information loss on model performance, we conducted experiments with two input types: (1) text-input-docmind, utilizing texts parsed by Docmind, and (2) text-input-pymupdf, utilizing texts parsed by PyMuPDF. The analysis of the results presented in Table 4 led us to the following conclusions:\nText-input vs. Image-input: The scores in the cut-off paradigm are higher than that in the text-input-pymupdf paradigm, but lower than that in the text-input-docmind paradigm, indicating that this method can effectively extract table structure information, but it can be improved further.\nCut-off vs. Merge: The merge method preserves a greater number of context tokens by concatenating multiple images, while the cut-off method succeeds in acquiring prior information by shortening the context window. Experimental results suggest that the cut-off may yield better problem-solving capabilities than merging, providing insights for the future construction of multimodal Retrieval-Augmented Generation (RAG) systems.\nImpact of Structural Information: For proprietary models, performance utilizing Docmind is at least 25 points higher than that with PyMuPDF, while the disparity is 15 points for open-source models. The absence of table structure information significantly hampers the performance of both open-source and proprietary models."}, {"title": "Conclusion", "content": "In this study, we address the limitations of existing document benchmarks. We propose LongDocURL, which includes 20 capabilities across 3 tasks, 3 evidence modes, and 4 document elements. A semi-automated pipeline generated over 2,300 high-quality question-answer pairs, covering more than 33,000 pages of documents. Subsequently, we conducted a comprehensive evaluation of 26 different parameter amounts of both open-source and closed-source models, revealing potential gaps in document understanding."}, {"title": "Limitations", "content": "From the perspective of dataset source, the types of documents discussed in this paper are still limited. A broader range of data sources would provide richer document layouts and element information, which would be more beneficial for evaluation. Moreover, the dataset can be further expanded by the automated construction pipeline. On the other hand, designing better model structures and training processes to improve performance on Long-DocURLwill be more important. However, this may have gone beyond the scope of this paper."}, {"title": "Ethical Impact", "content": "We respect intellectual property rights and comply with relevant laws and regulations. The documents in our dataset are publicly available, and we have taken careful measures to ensure that the documents in our dataset do not contain any personal sensitive information. In addition, our work is only for research purposes, not for commercial purposes."}, {"title": "Prompt Template for QA Generation", "content": "[System]\nYou are an expert in document question-answering dialogue synthesis. Please complete the following instructions based on the given text. The response must be true and accurate, and no additional content should be output.\n[Task Description]\nYour task is <detailed_task_description>\n[Restriction]\nEnsure questions and answers are suitable and correct.\nOnly include questions that have definite answers, that is:\n\u2022 one can see the content in the image that the question asks about and can answer confidently;\n\u2022 one can determine confidently from the image that it is not in the image, don't ask any question that cannot be answered confidently.\nProvide detailed evidence description first and then give final short answers. Use examples or reasoning steps to support your content. You can include multiple paragraphs if necessary.\n<other_restriction_description>\n[Response Examples]\n<few_qa_examples>\n[Context Input]\n<structured_text> | <previous_response>\nNotes: <structured_text> refers to text-type-bbox triple processed by Docmind engine, which is discussed in detail in Section 3.2.1. <previous_response> refers to possible intermediate result(e.g., summary sentences in our Para-Title Locating task. Specifically, we prompt LVLMs to generate summary sentences of one or multiple paragraphs under certain section titles first, and then utilize these summaries and title names to construct question-answering data.)"}, {"title": "Prompt Template for QA Verification", "content": "[System]\nYou are an expert in document question-answering verification. Please complete the following instructions based on the given text. The response must be true and accurate, and no additional content should be output.\n[Task description & Verification criteria]\nYour task is to ensure the quality of question-answer pairs in the context provided. you need to follow the steps outlined below to systematically evaluate each pair's effectiveness and accuracy. Implement this process diligently to maintain high standards across the batch of QA pairs.\n1. Question type check\nDoes the question match the task description: <task_description>\nMake sure the question meets the required task context.\n2. Formatting and Presentation\nIs answer properly formatted?\nEnsure the answer uses a list format to store the title content.\n3. Relevance Check\nDoes the question relate directly to document content rely on the context provided, the answer accurately reflect the information in the document?\nEnsure the question is formulated based on information explicitly stated or implied in the document. The question should not introduce concepts unrelated to the document's content.\nValidate that the answer references specific data or statements from the document. Avoid including extraneous information not supported by the document.\n4. Clarity and Precision\nIs the question clear and unambiguous? And is the answer concise and precise? Ensure the language is straightforward and easily understandable, avoid complex phrasing that may confuse the reader.\nThe intention of the question and answer pair must be clear and direct, avoiding verbosity and unnecessary detail.\nEnsure the answer fully addresses the question without omitting crucial information.\n5. Consistency and Coherence\nCheck for logical flow and coherence, ensuring the question aligns seamlessly with the document's narrative or arguments. Verify that the answer does not contradict or misrepresent other sections of the document.\nBy practicing this process, you can confirm whether the quality of the question-answer pairs meets the requirements. If all the above conditions are met, please output yes, otherwise output no.\nGenerated QA: <qa>\nInput Context: <evidence_context>"}, {"title": "Statistics of QA Verification", "content": "Verify\nU + R\nL\nBefore\n2857\n1520\nAfter\n1630\n695\n\n57.1%\u2193\n45.7%\u2193"}, {"title": "Comparison with MMLongBench-Doc", "content": ""}, {"title": "Merging Rules of Images Input", "content": "As discussed in Section 4.4, we design a merge paradigm for the evaluation of current LLMs/LVLMs.\n#Columns_merged = 2 if total_pages/30 <= 4 else 3\nTotal_pages #Columns_merged #Images_merged\n50<x<=60 2 x/2\n60<x<=90 2 x/3\n90<x<=120 2 x/4\n120<x<=150 3 x/5"}, {"title": "Selection Rules of Images Input", "content": "As discussed in Section 4.2, we design a cut-off paradigm for the evaluation of current LLMs/LVLMs. We provide pseudo-code below to express the selection rules."}, {"title": "Prompt for Response Generation", "content": "<document_images>\nYou are an expert in visual document question-answering, please answer our questions based on the given doc images.\nFollowing is our question:"}, {"title": "Prompt for Answer Extraction", "content": "Prompt for answer extraction is displayed in Figure 8. Based on the template given in MMLongBench-Doc (Ma et al., 2024), we make some modifications, which are marked in blue."}, {"title": "Scoring Rules", "content": "Following MATHVISTA (Lu et al., 2024), we evaluate the model's responses by scoring the extracted answers against the reference answers. Following MMLongBench-Doc (Ma et al., 2024), our scorer is rule-based and employs different strategies according to the format of the reference answer. We detail its rules as below:\nString: We firstly use a series of regular expressions to determine whether the answers require exact matching (e.g., telephone numbers, email addresses, website addresses, file names, times, dates, etc.) If an exact match is needed, we perform a straightforward string comparison and score the answer either 0 or 1. Otherwise, we calculate the ANLS (Average Normalized Levenshtein Similarity) with a pre-defined threshold (\u0442 = 0.5).\nInteger: We perform an exact match comparison and score the answer either 0 or 1.\nFloat: We view the prediction and reference answers as equal if they fall within a 1% relative tolerance.\nList: Compared with MMLongBench-Doc, we adopt a relatively soft rule for scoring answers in list format: (1) If the prediction does not have the same number of elements as the reference, it incurs a length-dependent penalty instead of receiving a score of 0, which we think more reasonable. (2) The score of models on single element of the reference list is the highest one among the scores which are calculated and combined between the element and each one of the prediction list. Compared with MMLongBench-Doc, we assume that the sorting positions of the two lists are not always one-to-one corresponding, allowing more errors, and our rules are gentler and more tolerant. We use pseudo-code below to express the scoring rules in MMLongBench-Doc and our LongDocURL, respectively. The element-wise scoring strategies is determined by the formats of elements (i.e., string, integer or float)."}, {"title": "Fine-Grained Evaluation Results", "content": "Detailed results are presented in Table 7. Related analysis is in Section 4.3.1."}, {"title": "Case Study", "content": "Figure 9 and Figure 10 display the response of different models, and we give a short error analysis for each case."}, {"title": "Data Examples", "content": "Figure 11, Figure 12 and Figure 13 provide samples for three primary tasks."}, {"title": "Description of labeling labor", "content": "At the dataset verification stage, we have 21 full-time data annotators responsible for the labeling work in the human verifying process, while 6 professional annotators with postgraduate degrees or above perform the final data quality verification and cross-checking work."}]}