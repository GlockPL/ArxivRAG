{"title": "GENERALIZED LEARNING OF COEFFICIENTS IN SPECTRAL GRAPH\nCONVOLUTIONAL NETWORKS", "authors": ["Mustafa Coskun", "Anath Grama", "Mehmet Koyuturk"], "abstract": "Spectral Graph Convolutional Networks (GCNs) have gained popularity in graph machine learning\napplications due, in part, to their flexibility in specification of network propagation rules. These\npropagation rules are often constructed as polynomial filters whose coefficients are learned using label\ninformation during training. In contrast to learned polynomial filters, explicit filter functions are useful\nin capturing relationships between network topology and distribution of labels across the network. A\nnumber of algorithms incorporating either approach have been proposed; however the relationship\nbetween filter functions and polynomial approximations is not fully resolved. This is largely due to the\nill-conditioned nature of the linear systems that must be solved to derive polynomial approximations\nof filter functions. To address this challenge, we propose a novel Arnoldi orthonormalization-based\nalgorithm, along with a unifying approach, called G-ARNOLDI-GCN that can efficiently and effectively\napproximate a given filter function with a polynomial. We evaluate G-ARNOLDI-GCN in the context\nof multi-class node classification across ten datasets with diverse topological characteristics. Our\nexperiments show that G-ARNOLDI-GCN consistently outperforms state-of-the-art methods when\nsuitable filter functions are employed. Overall, G-ARNOLDI-GCN opens important new directions in\ngraph machine learning by enabling the explicit design and application of diverse filter functions.\nCode link: https://anonymous.4open.science/r/GArnoldi-GCN-F7E2/README.md", "sections": [{"title": "1 Introduction", "content": "Spectral Graph Convolutional Networks (Spectral-GCNs) have attracted significant attention in various graph represen-\ntation learning tasks, including node classification [1], link prediction [2], and applications like drug discovery [3, 4].\nSpectral-GCNs utilize spectral convolution or spectral graph filters, operating in the spectral domain of the graph\nLaplacian matrix or normalized adjacency matrix [1,5] to address the \"feature-over-smoothing\" problem by isolating\nthe propagation scheme from neural networks. Spectral GCNs enable distribution of label/ feature information beyond\nneighboring nodes, as in Spatial GCNs [6], to distant nodes [7]. A key element of Spectral GCNs is spectral graph\nconvolutions or filters, which control the distribution of processed label/feature information. These convolutions assign\nweights to contributions from each hop in the graph, and the challenge in Spectral GCNs revolves around finding optimal\nweights."}, {"title": "2 Background and Related Work", "content": "Spectral Graph Convolutional Networks. We denote an undirected graph by G = (V, &) with node set V and edge\nset &, where the number of nodes is |V| = n. In the context of node classification, we are given an n \u00d7 m feature matrix\nX, where m denotes the number of features and X(i, j) represents the value of the jth feature for the ith node. We are\nalso given an n \u00d7 c-dimensional label matrix Y, where c denotes the number of classes, and Y(i, j) indicates whether\nnode i belongs to class j. Our goal is to learn a neural network model Y = f(G; X), where f utilizes topological\nrelationships between nodes in G to enhance the generalizibility of the machine learning model.\nSpectral GCNs use spectral filter functions to propagate signals across the graph. The signals include input features and\nlatent features that are computed at intermediate layers of the neural network. For the sake of generality, in the following\ndiscussion, we denote quantities propagated across the network as graph signal x \u2208 R\", where x (i) denotes the graph\nsignal at node i.\nSpectral Filter Functions. Let A denote the adjacency matrix of graph G and D denote the diagonal degree matrix.\nWe represent the degree-normalized adjacency matrix as P = D-1/2AD-1/2 and graph Laplacian matrix as L = I \u2013 P,\nwhere I denotes the identity matrix. Let L = UAUT be the eigendecompostion of L, where A = diag[\u03bb1,\u00b7, \u00b7, \u00b7, \u03bb\u03b7].\nSpectral GCNs create the spectral graph convolutions, or spectral filters, in the domain of the graph Laplacian [5].\nNamely:\n\\begin{equation}\ny = Ug(\\Lambda)U^Tx,\n\\end{equation}\nwhere g(A) = diag[g(\u03bb1), \u00b7, \u00b7, \u00b7, g(\u03bbn)]. Here, y represents the vector obtained by filtering graph signal x. The function\ng(\u03bb), called a spectral filter, transforms the eigenvalues of the Laplacian matrix to suitably shape signal propagation"}, {"title": "2.1 General Formulation of Spectral GCNS", "content": "Given a filtering function (g(\u03bb)) or a polynomial approximation (Equation 2), the general setting for Spectral GCNs is\nas follows:\n\\begin{equation}\nY = softmax(Z), Z = \\sum_{k=0}^{K} w_kH^{(k)},\nH^{(k)} = LH^{(k-1)}, H^{(0)} = f_\\theta(X)\n\\end{equation}\nwhere X \u2208 Rn\u00d7m denotes the feature matrix, fo denotes neural network with parameters 0, and Y denotes the label\nmatrix.\nCurrent spectral GCNs can be grouped into two categories based on how they choose the coefficients of the polynomials:\nPredetermined Graph Convolution and Learnable Graph Convolution."}, {"title": "2.1.1 Predetermined Graph Convolution", "content": "This class of spectral GCNs fix the weights a-priori, e.g., ck = ak where a \u2208 (0,1). For example, APPNP [7]\nuses an implicit filter function, g : [-a,a] \u2192 R with g(w) = \u03b1/(1-\u03b1), which is approximated by the polynomial"}, {"title": "2.1.2 Learnable Graph Convolution", "content": "These spectral GCNs simultaneously learn the coefficients of the polynomial alongside fo (X) by leveraging label\ninformation in training data. As an example, GPR-GNN [5] generalizes APPNP and learns yks along with fe(X),\ninstead of fixing them to ak as in APPNP [7], i.e.:\n\\begin{equation}\nZ_{GPR-GNN} = \\sum_{k=0}^{K} \\gamma_kH^{(k)}, H^{(k)} = PH^{(k-1)},\n\\end{equation}\nwhere yks are learnable parameters.\nChebNet [10] replaces the polynomial used in the approximation of Equation 5 with a Chebyshev polynomial. Since\nChebyshev polynomials converge more rapidly to the function that is being (implicitly) approximated, this process\neffectively increases the depth of the propagation. ChebNet [10] can be formulated as:\n\\begin{equation}\nZ_{ChebNet} = \\sum_{k=0}^{K} \\gamma_KH^{(k)},\nH^{(k)} = 2PH^{(k-1)} \u2013 H^{(k-2)}.\n\\end{equation}\nwhere H(0), H(1) = fe(X). Similarly, one can replace the polynomial in Equation 6 with any polynomial, provided that\nthe update rules of the chosen polynomial are preserved. In the Spectral GCNs literature, this flexibility allows the\ncreation of various Spectral GCN algorithms. Examples include CayleyNet [11], GPR-GNN [5], BernNet [1], and\nJacobiCon [12], respectively utilizing Cayley, monomial, Bernstein, and Jacobi Polynomials."}, {"title": "3 Methods", "content": "Spectral GCNs can be more versatile and adaptive if they are used with explicit filter functions. For predetermined graph\nconvolution, explicit filter functions enable choosing the filter to suit the topology of graph, the nature of the learning\ntask (e.g., homophilic vs. heterophilic graphs), and domain knowledge relating to distribution of labels. For learnable\ngraph convolution, explicit filter functions enable initialization of the coefficients to values that suit the learning task.\nThe challenge in the application of explicit filter functions is in computing polynomial approximations to these filter\nfunctions. With ARNOLDI-GCN (for predetermined graph convolution) and G-ARNOLDI-GCN (for learnable graph\nconvolution), we address this challenge by using Arnoldi orthonormalization to solve the key challenge associated\nwith ill-conditioned systems that result from the polynomial approximation of filter functions. The workflow of\nARNOLDI-GCN and G-ARNOLDI-GCN is shown in Figure 1.\nWe begin by defining representative filter functions and polynomial samples, emphasizing that the application of our\nalgorithms extend beyond these specific choices. Subsequently, we provide theoretical underpinnings and methodological\ndetails of our proposed algorithms."}, {"title": "3.1 Explicit Filter Functions for Spectral GCNs", "content": "We focus on eight representative filter functions, with a view to demonstrating and comparing the utility of explicitly\nspecified filter functions in the context of spectral GCNs. These represent four simple and four complex filter functions,\nestablishing propagation rules for homophilic and heterophilic graphs. The simple filters we use correspond to random\nwalks on the graph and operate on the eigen-decomposition of the degree-normalized adjacency matrix (P). The\ncomplex filters operate on the eigen-decomposition of the graph Laplacian (L). The range of a filter function is therefore\ndetermined by the bounds on the eigenvalues of the matrix it operates on. Specifically, we consider the following filter\nfunctions, where 0 < a < 1 is a hyperparameter:\nSimple Filters: (\u03c9 \u2208 (\u22121,1))"}, {"title": "Complex Filters: (\u03c9 \u2208 (0,2])", "content": "These functions are used to filter the spectra of the graph Laplacian (Equation 1) to facilitate propagation of the graph\nsignal in accordance with the rules specified by the filter [14]. However, since it is computationally expensive to\ncompute the eigen-decomposition of the Laplacian, we compute and use a polynomial approximation to the given filter\n(Equation 2). It is important to note that the framework we propose is not limited to explicitly designed filter functions.\nSince the below polynomial approximation scheme is based on sampling the function, it can be used for any filter\nfunction that can be sampled in the range of eigenvalues of the graph Laplacian (or the normalized adjacency matrix)."}, {"title": "3.2 Polynomial Approximation of Filter Functions", "content": "For a given filter g(w) and integer K, our objective is to compute a polynomial of degree K that best approximates g(w):\n\\begin{equation}\nP(\\omega) = \\sum_{k=0}^{K} a_k\\omega^k,\n\\end{equation}\nThat is, we seek to compute the coefficients ak for k = 0 to K such that P(w) provides a good approximation to\ng(w). We use these coefficients to implement the spectral GCN associated with filter g(w), using the approximation in\nEquation (2).\nWell-established techniques in numerical analysis consider r distinct samples taken from the range of w, denoted\nW1, W2, ..., Wr, where r = K. In this study, we consider four sampling techniques, namely equispaced samples (Eq.),\nChebyshev sampling (Ch.), Legendre sampling (Le.), and Jacobi sampling (Ja.). For the range [l, u] of the filter function,\nthe values wk for 1 \u2264 k \u2264 r are sampled from this range by each of these techniques as follows:\n\\begin{equation}\nEq. : w_k = l + k\\frac{u-l}{r+1}\n\\end{equation}\n\\begin{equation}\nCh. : w_k = \\frac{u+l}{2} + \\frac{u-1}{2} cos(\\pi \\frac{2k-1}{2r})\n\\end{equation}\n\\begin{equation}\nLe. : \\int_{-1}^{1} p(w) dw = \\sum_{k=1}^{r} p(w_k)\n\\end{equation}\n\\begin{equation}\nJa.: \\int_{-1}^{1} (1 + w)p(w) dw = \\sum_{k=1}^{r}(1+w_k)p(w_k)\n\\end{equation}\nHere, p(w) denotes the polynomial 1 + w + w\u00b2 + .... The points for Legendre and Jacobi sampling are computed\nnumerically, using the Gauss Quadrature procedure, since no analytical solution exists for integral-formed polynomials.\n[15]. Once the sampled points in the interval [-1,1] are computed, we scale and shift them to the desired interval [l, u]."}, {"title": "1.1 Generalized Spectral GCNS", "content": "The next step in computing the approximation involves evaluating the value of the filter functions at these sampled\npoints, yielding g(w\u2081), g(w2), ..., g(wr) to align the polynomials with the filters. Once these values are computed,\nexpressing the right side of Equation (15) in Vandermonde matrix form, we obtain:\n\\begin{equation}\nV =\\begin{bmatrix}\n1 & w_1 & w_1^2 & ... & w_1^K  \\\\ \n1 & w_2 & w_2^2 & ... & w_2^K  \\\\ \n\\vdots & \\vdots & \\vdots & & \\vdots \\\\ \n1 & w_\\gamma & w_\\gamma^2 & ... & w_\\gamma^K\n\\end{bmatrix}\n\\begin{bmatrix}\na_0 \\\\ a_1 \\\\ \\vdots \\\\ a_K\n\\end{bmatrix}=\\begin{bmatrix}\ng(w_1) \\\\ g(W_2) \\\\ \\vdots \\\\ g(w_\\gamma)\n\\end{bmatrix}\n\\end{equation}\nA commonly encountered problem in computing polynomial approximations is that the Vandermonde matrix V is\ntypically ill-conditioned.\nDefinition 1 The condition number of matrix A is defined as \u03ba(A) = ||A|| \u00b7 ||A\u00af\u00b9||. If \u03ba(A) = O(1), the matrix is\nwell-conditioned; otherwise it is ill-conditioned.\nTheorem 1 Let w\u2081,\u00b7, \u00b7, \u00b7, wr be the samples obtained using one of the sampling techniques in Equation 16-19. If\nW1,\u00b7,\u00b7,\u00b7,\u03c9, \u2208 [\u2212\u03b1, a] with a \u2208 (0,1), then \u03ba(V) = ||V|| \u00b7 ||V\u2020|| \u2265 2\u2032\u22121(+)\". If @1,\u00b7,\u00b7, \u00b7, \u03c9r \u2208 (0,2], then\n\u043a(V) = ||V||\u00b7 ||V\u2020|| \u2265 2-2.\nThe proof of this theorem is provided in the Appendix. A consequence of this theorem is that direct solution of (20)\nleads to inaccurate coefficients. For this reason, we solve this system using QR decomposition. Let V = QyRy be the\nQR decomposition of V and Q\u207ay denote the pseudo-inverse of Qv. A solution for Equation (20) can be obtained as:\n\\begin{equation}\na = (Q^+_v)g\n\\end{equation}\""}, {"title": "1.2 Generalized Spectral GCNS", "content": "Note that using QR decomposition does not mitigate the ill-conditioning problem by itself. To address this, we use an\nalternate QR-decomposition using \u03a9 = diag(@1,\u00b7, \u00b7, \u00b7, wr). In other words, let QA denote the orthonormal basis that is\ncomputed using Arnoldi Orthogonalization on \u03a9. We compute the coefficients of the polynomial approximation as:\n\\begin{equation}\na = (Q_A^+)g\n\\end{equation}\nand show that this formulation leads to a correct solution for the linear system (Theorem 2) and produces accurate\ncoefficients for our polynomial approximation (Theorem 3).\nThe procedure for computing QA is given in Algorithm 1. Note that, since the wks are real, Arnoldi orthonormalization\nhere can be implemented using Lanczos' algorithm. Specifically, we compute orthonormal matrix QA, tridiagonal T,\nand an almost-zero matrix O to satisfy:\n\\begin{equation}\nQ \\Omega Q_A = Q_AT + \\tilde{O}\n\\end{equation}\nwhere \u03a9 \u2208 R\u00ba\u00d7r, Qa \u2208 R(K+1)\u00d7r, T\u2208 R(K+1)\u00d7(K+1), and \u00d5\u2208 R(K+1)\u00d7r is all zero except its last column.\nThe next theorem guarantees that Arnoldi/Lanczos Orthonormalization can be used to obtain an alternative QR\ndecomposition to original Vandermonde matrix."}, {"title": "1.3 Generalized Spectral GCNS", "content": "Theorem 2 Given \u03a9, let orthonormal Q, tridiagonal T and almost-zero \u014c matrices be computed using Lanczos\nalgorithm to satisfy Equation (23). Then we can obtain a QR-decomposition for the Vandermonde matrix V as\nV(*) = QR, such that V(*) = V/||e|| and R = [e1, Te1, \u00b7, \u00b7, \u00b7, Te1], where e\u2081 denotes the K + 1-dimensional vector of\nones.\nThe proof of this theorem is provided in the Appendix. This theorem establishes that it is possible to compute an\nalternate orthonormal basis for the Vandermonde matrix using the Arnoldi/ Lanczos process, which can be used to\nsolve the linear system of Equation (20) using Equation (22). However, ensuring the accuracy of these coefficients\nrequires addressing whether QA leads to precise computation of coefficients. Theorem 3 provides the basis for accuracy\nof ax = (Q+)g. Specifically, we establish that the condition number of (Q\u2020Q) is close to one, ensuring the accuracy of\nthe solution.\nTheorem 3 Let w\u2081,\u00b7,\u00b7, \u00b7,wr be points sampled from interval (\u22121, 1) or (0,2] using one of the techniques shown in\nEquation 16-19. Let \u03a9 = diag(w\u2081, \u00b7, \u00b7, \u00b7, \u03c9\u2081), Q be the orthonormal basis obtained by applying Arnoldi/ Lanczos\northonormalization on \u03a9, and Q\u2122 be the pseudo-inverse of Q. If the Krylov subspace used for the orthogonalization has\nK = r dimensions, then \u03ba(QQ) \u2248 1.01.\nIn summary, these three theorems show that computing the QR decomposition of V with Arnoldi/ Lanczos process on\n\u03a9 enables us to produce accurate polynomial coefficients, since condition number of Q is bounded, while performing\nQR decomposition directly on V produces inaccurate coefficients due to the ill-conditioned nature of the matrix V."}, {"title": "3.3 Generalized Spectral GCNS", "content": "The framework described above enables computation of accurate polynomial approximations to any explicit filter\nfunction. Using this framework, we propose two Spectral GCNs that operate with an explicitly formulated filter function:\n(i) Arnoldi-GCN for predetermined graph convolution; and (ii) G-Arnoldi-GCN for learnable graph convolution.\nThese spectral GCNs are generalized in the sense that they can work with any filter function. Furthermore, any spectral\nGCN in the literature can be formulated within this framework.\nGiven a filter function g(w) : D \u2192 R and integer K, let ao, a1, .., ak denote the coefficients of the polynomial\nthat approximates g(w), computed using the procedure described in the previous section. If g operates on the\ndegree-normalized adjacency matrix, then D = (\u22121, 1). If g operates on the graph Laplacian, then D = (0,2]. Using\nthis filter function, Arnoldi-GCN and G-Arnoldi-GCN are implemented as follows:\nArnoldi-GCN\nY = softmax(ZARNOLDI),\n\\begin{equation}\nZ_{ARNOLDI} = \\sum_{k=0}^{K} a_kH^{(k)}, H^{(0)} = f_\\theta(X),\nH^{(k)}=\\begin{cases}\nPH^{(k-1)}, & \\mathcal{D} = (-1, 1)\\\\\nLH^{(k-1)}, & \\mathcal{D} = (0,2]\n\\end{cases}\n\\end{equation}"}, {"title": "1.3 Generalized Spectral GCNS", "content": "Here, P and L are self-loop added versions of original normalized adjacency and Laplacian matrices, respectively. fe\ndenotes the neural network with learnable parameters 0.\nG-Arnoldi-GCN\nY = softmax(ZG-ARNOLDI),\n\\begin{equation}\nZ_{G-ARNOLDI} = \\sum_{k=0}^{K} \\gamma_kH^{(k)}, H^{(0)} = f_\\theta(X),\n\\end{equation}\nwhere yk are learneable parameters that are optimized alongside @ in an end-to-end fashion by initializing Yk = ak."}, {"title": "4 Results", "content": "In this section, we systematically evaluate the performance of our proposed algorithms, ARNOLDI-GCN and G-ARNOLDI-\nGCN in utilizing explicit filters along with accurate polynomial approximation to improve node classification using\nSpectral GCNs. We first describe the datasets that contain five homophilic, five small-heterophilic, and five large\nheterophilic networks, as well as our experimental setup. Next, we compare the performance of ARNOLDI-GCN and\nG-ARNOLDI-GCN against state-of-the-art algorithms GCN [6], GAT [16], APPNP [7], ChebNet [17],JKNet [18],"}, {"title": "4.1 Datasets and Experimental Setup", "content": "We use fifteen real-world network datasets for our experiments. Five of these datasets are homophilic networks,\nwhich include three citation graph datasets, Cora, CiteSeer and PubMed [19], and two Amazon co-purchase graphs,\nComputers and Photo [20]. The remaining five networks are heterophilic, which include Wikipedia graphs Chameleon\nand Squirrel [21], the Actor co-occurrence graph, and the webpage graph Texas and Cornell from WebKB3 [22] as\nwell as recently curated large heterophilic datasets, Roman-Empire, Amazon-Rating, Widesweeper, Tolokers, and"}, {"title": "4.2 Comparison of Node Classification Performance of ARNOLDI-GCN and G-ARNOLDI-GCN to State-of-the-Art", "content": "We compare our proposed algorithms, ARNOLDI-GCN and G-ARNOLDI-GCN, against state-of-the-art Spectral GCNs on\nfive homophilic and ten heterophilic real-world datasets for both semi and fully supervised node classification tasks. To\nassess semi-supervised node classification performance, we use cross-validation by randomly splitting the data into\ntraining/validation/test samples as (%2.5 / %2.5 /%95). To assess fully supervised node classification performance, we\nuse cross-validation by randomly splitting the data into training/validation/test samples as (%60 / %20/%20). The\nresults of these experiments are presented in Table 2 with the best-performing method(s) highlighted in bold. Our\nmethods, particularly G-ARNOLDI-GCN, demonstrate significant improvements in the performance of Spectral-GCNs, as\nseen in these tables. We observe that the performance improvement is particulary pronounced on heterophilic networks,\nhighlighting the importance of using explicit filters for complex machine learning tasks. To provide insights into which\nfilter functions deliver best performance on which classification tasks, we provide the details of the best-performing\nfilter in Table 4 in the Appendix."}, {"title": "4.3 Value Added by Numerical Accuracy", "content": "The results shown in the previous set of experiments demonstrate that the use of explicit filter functions and their\npolynomial approximation significantly enhances node classification performance of Spectral GCNs. To gain insights\ninto the value added by the numerical accuracy in this process, we first investigate the effect of the linear system solver\nand the sampling technique used for the polynomial approximation on the accuracy of the approximation. Figure 2\ncompares Arnoldi orhonormalization to direct solution of the Vandermonde system in approximating Random Walk\n(81) and Low Pass (g4) filters using Equispaced and Chebyshev sampling. As seen in the figure, Arnoldi significantly\nreduces errors, particularly for complex filters. Furthermore, Chebyshev sampling consistently outperforms Equispaced\nsampling."}, {"title": "4.4 From Approximation Quality to Classification Accuracy", "content": "In the previous section, we observe that Arnoldi orthonormalization drastically improves the accuracy of polynomial\napproximations to filter functions. We now assess how this numerical accuracy translates to the observed superior\nclassification performance of the Spectral GCN. For this purpose, we compare the node classification performance of the\nSpectral GCNs that utilize polynomials generated by direct solution of the Vandermonde system (VANDERMONDE-GCN)\nvs. Arnoldi orthonormalization (ARNOLDI-GCN), using three homophilic and three heterophilic datasets. In these\nexperiments, we use Random Walk and Low Pass filters and polynomial approximations computed using Equispaced and\nChebyshev sampling. As seen in Figure 3, ARNOLDI-GCN consistently and significantly outperforms VANDERMONDE-\nGCN, demonstrating the importance of numerically stable polynomial approximations in training reliable GCN models.\nIn addition, Chebyshev sampling consistently outperforms Equispace in semi-supervised node classification. Integration\nof complex filters, e.g., Low-Pass, delivers substantial improvements, especially for heterophilic datasets such as Texas\nand Cornell."}, {"title": "4.5 Discussion", "content": "Based on the comprehensive results presented in this section, we derive the following key insights:\n\u2022 G-ARNOLDI-GCN outperforms state-of-the-art algorithms on 12 out of 15 datasets in both semi-supervised and\nsupervised node classification tasks.\n\u2022 On 6 out of 10 heterophilic datasets, G-ARNOLDI-GCN's performance improvement over other algorithms is\nhighly significant (more than 10 standard deviations)."}, {"title": "5 Conclusion", "content": "The ill-conditioned nature of polynomial approximation to filter functions poses significant challenges for Spectral\nGCNs in defining suitable signal propagation. Our algorithm, G-ARNOLDI-GCN, overcomes this challenge by enabling\nnumerically stable polynomial approximation of any filter function. G-ARNOLDI-GCN excels in multi-class node\nclassification across diverse datasets, showcasing its customizability with various filters and sampling methods available\nto choose from. G-ARNOLDI-GCN marks a new direction in graph machine learning, enabling the explicit design and\napplication of diverse filter functions in Spectral GCNs."}, {"title": "6 Appendix", "content": null}, {"title": "6.1 Proofs of Theorems", "content": "In this subsection, we prove the theorems in the body of the paper. In the following proofs we assume K r and all the\nproofs are valid when K \u2264 r since least square fitting is norm-wise backward stable [25]."}]}