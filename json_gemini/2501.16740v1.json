{"title": "EFFICIENT KNOWLEDGE DISTILLATION OF SAM FOR MEDICAL IMAGE SEGMENTATION", "authors": ["Kunal Dasharath Patil", "Gowthamaan Palani", "Ganapathy Krishnamurthi"], "abstract": "The Segment Anything Model (SAM) has set a new standard in interactive image segmentation, offering robust perfor-mance across various tasks. However, its significant com-putational requirements limit its deployment in real-time or resource-constrained environments. To address these chal-lenges, we propose a novel knowledge distillation approach, KD SAM, which incorporates both encoder and decoder op-timization through a combination of Mean Squared Error (MSE) and Perceptual Loss. This dual-loss framework cap-tures structural and semantic features, enabling the student model to maintain high segmentation accuracy while reducing computational complexity. Based on the model evaluation on datasets, including Kvasir-SEG, ISIC 2017, Fetal Head Ultra-sound, and Breast Ultrasound, we demonstrate that KD SAM achieves comparable or superior performance to the baseline models, with significantly fewer parameters. KD SAM ef-fectively balances segmentation accuracy and computational efficiency, making it well-suited for real-time medical image segmentation applications in resource-constrained environ-ments.", "sections": [{"title": "1. INTRODUCTION", "content": "Interactive image segmentation has become a cornerstone in numerous applications, including medical imaging, au-tonomous driving, and augmented reality. The Segment Any-thing Model (SAM) [1] has established itself as a powerful tool in this domain, leveraging a Vision Transformer (ViT) [2] encoder and prompt-guided mask decoder to achieve high segmentation accuracy across diverse datasets. However, the significant computational demands of SAM hinder its deployment in real-time and resource-constrained environ-ments, such as mobile devices and edge platforms.\nMobileSAM [3] addresses these limitations by replacing the ViT encoder with ViT-Tiny, significantly reducing the model size and inference time while maintaining competitive performance. Despite these advances, the segmentation qual-ity, particularly for complex tasks such as medical imaging,\nis compromised due to the reduced capacity of the ViT-Tiny encoder.\nIn this work, we propose a novel decoupled knowledge distillation approach that enhances both the encoder and de-coder components. By incorporating both Mean Squared Er-ror (MSE) and perceptual loss [4], our method captures struc-tural and semantic features, resulting in a robust model ca-pable of high-precision segmentation with reduced compu-tational costs. This approach addresses MobileSAM's chal-lenges, making our method more effective for medical imag-ing tasks where accuracy and efficiency are crucial."}, {"title": "2. RELATED WORK", "content": "Knowledge distillation is a powerful technique to transfer knowledge from a large, complex model to a smaller model while maintaining considerable performance. Initially de-veloped for classification tasks [5], it has been adapted for dense prediction tasks such as semantic segmentation [6] and object detection [7]. The goal is to align the student model feature representations with the teacher, preserving spatial and semantic information. Strategies like pixel-wise feature matching and channel-wise correlations have been employed to ensure the student model replicates the teacher's perfor-mance with reduced computational cost.\nThe Segment Anything Model (SAM) [1] represents a signif-icant advancement in interactive segmentation, leveraging a powerful ViT-H encoder and prompt-guided mask decoder to handle diverse segmentation tasks. However, its high com-putational demands have limited its real-world applications.\nSeveral variants of SAM have been developed to address these limitations. MobileSAM [3] replaces the ViT-H en-coder with ViT-Tiny, reducing the model size and inference time while maintaining competitive performance. However, MobileSAM struggles with fine-grained segmentation tasks, particularly in medical imaging.\nOn the other hand, FastSAM [8] employs a YOLACT-based [9] instance segmentation model combined with heuristic post-processing rules for object selection to achieve faster segmentation. However, this comes at the cost of segmenta-tion quality, as it diverges from SAM's interactive segmen-tation principles, making it less suitable for high-precision tasks. EfficientSAM [10] focuses on improving training ef-"}, {"title": "3. METHOD", "content": "The proposed method adapts the Segment Anything Model (SAM) for medical image segmentation through a tailored decoupled knowledge distillation process as done by Mobile-SAM. This approach addresses the computational limitations of SAM's Vision Transformer (ViT) encoder by distilling its knowledge into a lightweight ResNet [12] based encoder. As shown in Figure 1, the methodology involves a two-phase process: the first phase focuses on encoder distillation, and the second phase involves fine-tuning the decoder. This de-coupled strategy allows efficient training with limited compu-tational resources while ensuring high segmentation accuracy on medical datasets."}, {"title": "3.1. Encoder Knowledge Distillation", "content": "The first phase of the methodology involves distilling the knowledge from SAM's ViT encoder to a more computation-ally efficient ResNet-50 encoder. The selection of ResNet-50 as several considerations drove the student model. First, ResNet-50, with its deep residual learning framework, effec-tively mitigates the vanishing gradient problem, enabling the model to learn deep feature representations without degrada-tion in performance. This property is particularly beneficial when attempting to capture the complex structures inherent in medical images. Second, ResNet-50 strikes an optimal bal-ance between model size and performance, making it suitable for deployment in resource-constrained environments, such as mobile devices or edge computing platforms. Its signifi-cantly lower parameter count than the ViT encoder reduces computational requirements, facilitating real-time inference without compromising segmentation accuracy.\nThe distillation process employs a combined loss func-tion that integrates Mean Squared Error (MSE) and Percep-tual loss. MSE measures the pixel-wise differences between the feature maps of the teacher model (ViT encoder) and the student model (ResNet-50 encoder), ensuring the essential structural information captured by the high-dimensional fea-ture space of the ViT model is transferred effectively to the ResNet model. However, relying solely on MSE loss can lead to a loss of perceptual quality in the distilled features, particularly for fine-grained details critical in medical image segmentation. Perceptual loss is incorporated to address this, which leverages pre-trained feature extractors, such as VGG [13] networks, to capture high-level semantic similarities be-tween the two models. This loss function evaluates the dis-tance between the feature representations of the teacher and student models at multiple layers, ensuring that the student model not only replicates the low-level details but also learns the perceptual features necessary for distinguishing complex structures. This dual loss approach ensures that the ResNet-50 encoder can approximate the performance of the ViT en-coder while being computationally efficient, making it a prac-tical solution for medical image segmentation tasks.\nThe combined loss function $L_{Combined}$ is defined as:\n$L_{Combined} = L_{MSE} + L_{P}$ (1)\nWhere, $L_{MSE}$ represent MSE Loss and $L_{P}$ is Perceptual loss.\nThe MSE loss, given as\n$L_{MSE} = \\frac{1}{N} \\sum_{i=1}^N (f_t(x_i) - f_s(x_i))^2$ (2)\nwhere $f_t(x_i)$ and $f_s(x_i)$ are the feature maps from the teacher and student encoders, respectively. The Perceptual loss $L_{P}$ is calculated using feature activations from selected layers $l$ of a pre-trained VGG network:\n$L_{P} = \\frac{1}{C_l H_l W_l} \\sum_{c=1}^{C_l} \\sum_{h=1}^{H_l} \\sum_{w=1}^{W_l} (\\phi_l^t(x_i) - \\phi_l^s(x_i))^2$ (3)\nIn this formula, $\\phi_l^t(x_i)$ and $\\phi_l^s(x_i)$ denote the feature maps from layer $l$ of the ViT and ResNet encoders, respectively."}, {"title": "3.2. Decoder Fine-Tuning", "content": "Unlike traditional distillation methods, which train the en-coder and decoder concurrently, we employ a decoupled ap-proach. Coupled distillation optimizes the student encoder and decoder, allowing the decoder to adapt to any changes in the feature representations generated by the student encoder. This joint optimization ensures that the entire student net-work (encoder and decoder) aligns closely with the teacher model. However, this approach is computationally expen-sive and challenging to implement on resource-constrained devices.\nOur decoupled distillation method trains the encoder in-dependently to learn the feature embeddings of the ViT-based encoder from SAM. This training strategy is essential to re-duce the computational burden and allows us to focus on dis-tilling the encoder first without the complexities of optimiz-ing the entire network. However, this independence means that the feature embeddings generated by the distilled encoder may not be fully compatible with the pre-trained SAM de-coder. As a result, the decoder needs to be fine-tuned to align with the distilled encoder.\nTo achieve this, we fine-tune the decoder using a Dice Loss. The Dice Loss is particularly effective for medical im-age segmentation because it maximises the overlap between the predicted segmentation mask and the ground truth, han-dling the class imbalance common in medical datasets. The Dice Loss is defined as:\n$L_{Dice} = 1 - \\frac{2 \\sum_{i=1}^N p_i g_i}{\\sum_{i=1}^N p_i + \\sum_{i=1}^N g_i}$ (4)\nWhere $p_i$ represents the predicted segmentation mask, $g_i$ rep-resents the ground truth mask, and $N$ is the total number of pixels. During fine-tuning, the encoder's weights are kept frozen to retain the distilled knowledge while the decoder is trained on the dataset. This selective training approach is computationally efficient and ensures that the decoder adapts to the unique features generated by the distilled encoder, achieving high segmentation accuracy."}, {"title": "4. EXPERIMENTAL SETUP", "content": "The training process for the knowledge distillation framework was carried out on multiple medical imaging datasets, includ-ing Kvasir-SEG [14], ISIC 2017 [15], Fetal Head Ultrasound [16], and Breast Ultrasound [17]. These datasets were chosen for their diversity and relevance to the segmentation tasks,\nproviding a comprehensive evaluation of the model's abil-ity to generalize across various medical image types. Each dataset represents unique challenges, from polyp segmenta-tion in gastrointestinal images to the precise delineation of anatomical structures in fetal head ultrasound scans.\nThe training involved two key phases: encoder distillation and decoder fine-tuning. The encoder distillation phase trains the ResNet-50 student model to learn the SAM ViT-H encoder representations. The ResNet-50 architecture was modified to reduce the channel dimensions from 2048 to 256 and incor-porate upsampling layers to match the spatial dimensions of the teacher model's outputs.\nFor Encoder distillation, the number of epochs is set to 100 with a batch size of 16. Early stopping was applied to halt training when the validation loss failed to improve, ensur-ing the model stopped once optimal performance was reached without overfitting. The Adam optimizer [18] with a learning rate of 0.0001 and weight decay of 0.001 was used, and a learning rate scheduler reduced the learning rate by a factor of 0.1 if the validation loss plateaued. In decoder fine-tuning, the SAM decoder coupled with the distilled encoder was fine-tuned using Dice Loss, which maximizes the overlap between predicted segmentation masks and ground truth labels, mak-ing it particularly effective for medical imaging tasks."}, {"title": "5. RESULTS", "content": "The performance of the KD SAM model was evaluated on a separate test dataset using the Dice Coefficient metric across four medical imaging datasets: Kvasir-SEG, ISIC 2017, Fetal Head Ultrasound, and Breast Ultrasound, and com-pared against the baseline models, SAM and MobileSAM. As shown in Table 1, the results demonstrate that KD SAM achieves comparable or superior performance to the base-line models across most datasets. KD SAM maintains high segmentation accuracy, with Dice Coefficients close to or exceeding those of both SAM and MobileSAM."}]}