{"title": "A Value Based Parallel Update MCTS Method for Multi-Agent Cooperative Decision Making of Connected and Automated Vehicles", "authors": ["Ye Han", "Lijun Zhang*", "Dejian Meng Zhang", "Xingyu Hu", "Songyu Weng"], "abstract": "To solve the problem of lateral and logitudinal joint decision-making of multi-vehicle cooperative driving for connected and automated vehicles (CAVs), this paper proposes a Monte Carlo tree search (MCTS) method with parallel update for multi-agent Markov game with limited horizon and time discounted setting. By analyzing the parallel actions in the multi-vehicle joint action space in the partial-steady-state traffic flow, the parallel update method can quickly exclude potential dangerous actions, thereby increasing the search depth without sacrificing the search breadth. The proposed method is tested in a large number of randomly generated traffic flow. The experiment results show that the algorithm has good robustness and better performance than the SOTA reinforcement learning algorithms and heuristic methods. The vehicle driving strategy using the proposed algorithm shows rationality beyond human drivers, and has advantages in traffic efficiency and safety in the coordinating zone.", "sections": [{"title": "I. INTRODUCTION", "content": "CONNECTED and Automated Vehicles (CAVs) have de-veloped rapidly in the last decade due to emerging tech-nologies such as vehicle electronics, autonomous driving, V2Xcommunication and edge computing. It is an indispensablepart of the intelligent transportation system. For a long time tocome, autonomous vehicles (AVs) and human-driving vehicles(HDVs) will coexist in urban traffic system [1], [2]. In suchan environment, compared to single-vehicle decision-making,multi-vehicle collaborative decision-making can significantlyalleviate traffic congestion, enhance traffic safety, and improveenergy efficiency [3]. However, due to the highly dynamicbehavior of individual traffic participants, the complex inter-actions among them, and the vast joint state-action space ofmultiple agents, it is challenging to solve the multi-vehiclecollaborative decision-making problem. How to model theinteraction between traffic participants and efficiently obtainmulti-vehicle cooperative driving strategies is an importantissue to address.\nOver the past decade, Monte Carlo Tree Search (MCTS)algorithms have achieved great success in solving extensive-form game problems. It has achieved superior performancebeyond human experts in Go and other board games [4]\u2013[7],real-time strategy games [8], [9], and poker games [10]\u2013[12].\nMCTS algorithm is at its core heuristic, which means thatno additional knowledge is required other than just rules of agame (or a problem, generally speaking) [13].\nMCTS is believed to be suitable for multi-vehicle col-laborative decision-making problems because of its heuristicnature, and MCTS strategies are environmentally robust, itis promising to obtain cooperative strategies between vehi-cles [14]. In recent years, MPC methods, deep reinforcementlearning (DRL) methods and prediction-based deep learninghave been applied to decision-making of autonomous ve-hicles. Incorporating manually crafted rules, the end-to-endautonomous driving algorithms developed by companies such as Tesla and Huawei have already achieved a high level ofsafe driving. However, their driving strategies still remain atthe stage of mimicking human driving behavior, and may failin scenarios where cooperation is necessary for vehicles (asshown in Fig.1). MCTS can overcome the inherent defectsof such methods, and is expected to obtain driving strategiesthat exceed experiencing human drivers. Motivated by this,we conducted this multi-vehicle collaborative decision-makingresearch based on MCTS, and managed to enable CAVSto efficiently obtain high-level cooperative driving strategiesthrough heuristic reward functions.\nTo solve the problem of lateral and logitudinal jointdecision-making of CAVs, this paper proposes a MonteCarlo tree search method with parallel update for multi-agent Markov game with limited horizon and time discounting setting. The main contributions of this work are as follows:\nA value based MCTS method is proposed for multi-vehicle cooperative two-dimensional joint decision-making. The algorithm demonstrates strong environmen-tal robustness and can easily handle randomly generated traffic scenarios. Its performance surpasses that of the SOTA RL algorithms and rule-based method.\nThe standard tree update method of MCTS is extended to a parallel form, effectively improving the search effi-ciency for joint strategies in multi-agent systems. Coun-terintuitively, this approach increases both the breadth and depth of the search under the same number of rollouts. The parallel update method can be applied to problems with similar steady-state transitions.\nIn numerous experiments conducted in randomly gener-ated scenarios, cooperative driving behaviors of CAVs were observed. The algorithm exhibits a level of rational-ity that surpasses that of typical human drivers, enabling"}, {"title": "II. RELATED WORKS", "content": "Multi-Vehicle Decision-Making: Multi-vehicle decision-making aims to provide safer and more efficient drivingstrategies for autonomous driving systems. Early multi-vehiclecooperative decision-making researches can be traced backto the study of longitudinal platooning such as ACC andCACC [15]. These studies use limited on-board sensors, andthe objective is mainly concerned with the string stability inone dimention. Optimization-based planning methods such asmixed integer optimization and dynamic priority allocation canalso solve collaborative decision-making problems to someextent [16]\u2013[18], but it is difficult to guarantee the speedand quality of the solution at the same time in large-scalecollaborative driving tasks.\nWith the development of artificial intelligence, V2X com-munication, and edge computing technologies, CAVs canmake more reasonable decisions in a wider spatial dimensionand a longer time range [19]\u2013[22]. The application of deeplearning in autonomous driving impels researchers to solvemulti-vehicle decision-making problems with DL methods.A. J. M. Muzahid et al. [23] systematically summarized themulti-vehicle cooperative collision avoidance technology ofCAVs, and proposed a multi-vehicle cooperative perception-communication-decision framework based on deep reinforce-ment learning. Y. Zheng et al. [24] modeled the multi-vehicle"}, {"title": "III. PROBLEM FORMULATION", "content": "A. Multi-Agent Markov Game\nIn this paper, multi-vehicle cooperative driving is modeledas a multi-agent Markov game. A Markov game is specifiedby a tuple\n$\\mathcal{(I, S, A, P, R, p_0, \\gamma)},$\nwhere I is the set of agents (in the remainder of this paper,we sometimes use agent to refer to CAVs), S is the statespace, A is the joint action space, $P(s_{t+1}|s_t, a_t)$ specifies thestate transition probability distribution, $R(r_t|s_t, a_t)$ specifiesthe reward function, $p_0(s_0)$ denotes the initial state, and $\\gamma \\in(0, 1]$ denotes a discount factor.\nAt each timestep $t \\in \\{0,...,T\\}$, each agent $i \\in\\{1,...,N\\}$ selects an action independently according to itsstate-conditioned policy $\\pi_i(a_i|s_i; \\theta_i)$. Here, T specifiesthe episode length, N denotes the number of agents, $s_t$ denotesthe state information available to agent i, and $\\theta_i$ denotes theparameters for agent i. Subsequently, individual agent rewardsare sampled according to $r_t^{(1)},...,r_t^{(N)} \\sim R(\\cdot|s_t, a_t)$, and thestate transitions according to $s_{t+1} \\sim P(\\cdot|s_t, a_t)$.\nAlthough agents receive individual rewards, we are primar-ily interested in learning cooperative behaviors that maximizetotal group return, that is, the sum of all agents' individualrewards across all timesteps. More precisely, we wish to findthe optimal agent policy parameters $\\theta^* = \\{\\theta_1,...,\\theta_N\\} =argmax_{\\theta}R(\\theta)$, where\n$R(\\theta) = E[\\sum_{t=0}^{T}\\sum_{j=1}^{N}r_t^{(j)}].$\nThis problem formulation is distinct from the greedy case,where each agent maximizes its own individual return. In thisproblem formulation, agents should learn to be altruistic incertain situations, by selecting actions that help maximizesgroup reward, possibly at the expense of some individualreward.\nUnder the joint strategy $\\pi = \\prod_{i\\in\\{1,...,N\\}} \\pi^i(a^i|s)$, the Qfunction is defined as:\n$Q^{\\pi}(s, a) = \\sum_{i=1}^{N}E^{\\pi}[\\sum_{t\\geq0} \\gamma^t r_{t+1}^{(i)}(s_t, a_t)|s_0=s, a_0=a].$"}, {"title": "B. World Model", "content": "The basic scenario of multi-vehicle cooperative driving isshown in the Fig.3."}, {"title": "C. Action Spa\u0441\u0435", "content": "For each vehicle, we consider both the lateral and longitudi-nal behaviors. Longitudinal actions include accelerating, speedkeeping and decelerating, and lateral actions consist of left lanechanging, lane keeping and right lane changing. Consideringthat longitudinal and lateral actions can be performed at thesame time, there are 9 action for a single vehicle,\n$A_i = \\{(a_{lon}, a_{lat}) | a_{lon} \\in A_{lon}, a_{lat} \\in A_{lat}\\},$ \nwhere $A_{lon} = \\{AC, SK, DC\\}$, and $A_{lat} = \\{LC,LK, RC\\}$. Then the joint action space,\n$A = \\prod_{i\\in\\{1,...,N\\}}A_i.$"}, {"title": "D. Reward Function", "content": "Our work aims at the driving efficiency and safety of globaltraffic in concerned area. The reward function is designedfollowing Equation.5,\n$r = w_1R_{speed} + w_2R_{intention} + w_3P_{collision} + w_4P_{LC}$\\\n$= \\frac{1}{N}(\\sum_{i=1}^{N}r_{speed}^{(i)} + w_2N_{arrived} + w_3N_{collision} + w_4N_{LC})$\nwhere N is the number of vehicles (including HDVs andCAVs), $N_{arrived}$ is the vehicle passing through intention areaat the previous time step and aiming for the ramp, $N_{collision}$ isthe number of collisions, and $N_{LC}$ is the number of frequentlylane-changing vehicles.\nIn the previous work we have reviewed, $R_{speed}$ are typicallyrepresented directly by the vehicle's speed or a linear combi-nation of it. However, in our work, due to the simulation'ssmall time step and the vehicles' inherent baseline speed, thisreward design approach often fails to adequately capture thedifferences between actions (we refer to this characteristic aspartial-steady-state). In this paper, we propose a reward func-tion design tailored for partial-steady-state update systems. Toimprove traffic efficiency, we assign a reward value to speedincreases. When a vehicle's speed reaches a threshold, weassign the same reward to lane-keeping actions, that is,\nrspede={(rspede,  if ai>0 or ai=0 and v>vthres0,  otherwise\nFor intention reward and collision penalty, it is consistentwith the previous work settings. In this paper, we do not punishfrequent lane change, and instead give a small reward to lanekeeping."}, {"title": "IV. METHODOLOGY", "content": "A. MCTS Method\nMCTS generally consists of four steps: selection, expansion,simulation and back propagation.\n1) Selection: Start from the root node, select the childnodes continuously, until reaching a leaf node, and thenupdate the tree depending on that node. A typical ruleis\nat=argmaxa(Q(st,a)+u(st,a)),\nwhere,\nu(s,a)\u221d\u221alognp\u221a1+na.\nHere, $c(s)$ is a coefficient and $p(s, a)$ is the prior probability of actionselection. The root node is the current game state, andthe leaf node is any potential child node that has not yetbeen explored.\n2) Expansion: Generate new child nodes. May initiallyexclude some illegal of obviously not resonable actions.\n3) Rollout: A random simulation, simulate a game fromthe current state until terminal state. It should be notedthat if the current node is new, expand is performed, andif the node has been updated, rollout is performed.\n4) Back Propagation: Use the value of leaf node Q toupdate all nodes on the path to the root node.\nB. Value Based Settings\nThe value-based method comes from reinforcement learn-ing. In value function of RL, the agent adopts a strategy from"}, {"title": "C. Parallel Update in Value Based MCTS", "content": "For decision-making problems with large action spaces,search tree pruning has been a long-standing focus of research.Appropriate model pruning methods can significantly improvesearch speed without substantially affecting decision quality(or obtain strategies of the same quality with lower searchcosts). The parallel update method proposed in this paper canbe seen as a softened version of soft pruning, which maximizesthe retention of exploration possibilities for all nodes.\nWe start introducing the parallel update method by definingparallel actions. Parallel actions refer to those that are similarfrom a safety perspective. We use the representative Time-to-Collision (TTC) to quantify the similarity of agent actions.TTC is one of the key quantitative indicators of vehicle safety.It represents the time required for a vehicle to collide with an"}, {"title": "Definition 1 (Parallel Action):", "content": "In the joint action space A =(A1, A2, ..., AN), for the joint action a = (a1, a2, ...,aN),if there exists a' ~ a (~ denotes two similar actions, j != i,k e {1, 2, ..., N}), and vehicle i received negative feedbackfor performing ai', then the joint action a; is parallel actionof ai, that is, aj ~ ai.\nWith the definition of parallel actions, the MCTS withparallel update method can be represented by Algorithm.2 ineach step of decision-making."}, {"title": "V. CASE STUDY", "content": "A. Case Settings\nWe use Flow to build simulation scenarios and verify thealgorithm [38]. Flow is a computational framework for deep"}, {"title": "B. MCTS Implementation Details", "content": "Node Properties: In this paper, we assign 7 variables tostore a node's information, which could be denoted by a tupleas follows:\n$(P, p, C, n, u, R_e, R_t, Q)$ are their parent nodes, priorprobablity, first-level child nodes, times of direct access, UCBvalue, sum of coefficient of rollout returns, rollout return value,and Q value, respectively.\nIn addition, a node is identified by the scalar value representing actions in the discrete action space. In the setting oftwo intelligent connected vehicles, the action-scalar mappingof the two vehicles is shown in Fig.6.\nSelection: The action selection in each state requires num-bers of rollouts. The initial state of each rollout is the currentstate of the traffic. In the process of rollout, a new search treewill be generated. It should be noted that part of the tree mayhave been generated in the previous rollout and can be reused.Among them, the UCB calculation method is:\nu=Q+Cpuctp(s,a)\u221alognp1+n,\nhere, $n_p$ is the visit time of the parent node. On the firstvisit, the node's Q value is assigned by action rewards. As"}, {"title": "D. Results", "content": "1) Hyperparameters: Firstly, we analyzed the impact of hy-perparameters on model performance. Orthogonal experimentson two key hyperparameters $C_{puct}$ and $n_{rollout}$ are conducted.It can be seen from Fig.8(a), under the problem settings ofthis paper, the ATS. upper bound of the MCTS algorithm isaround 28.5.\nWithin the designed parameter space, the optimal perfor-mance lies in the range where $n_{rollout} \\in [170, 300]$, and $C_{puct} \\in[170,300]$. As shown in Fig.8(a), when $n_{rollout}$ is less than200, the algorithm's performance improves as $n_{rollout}$ increases,which is consistent with intuition. However, further increasingNrollout does not significantly enhance the algorithm's perfor-mance and instead increases computational costs. According toEquation.15, Cpuct is used to balance different exploration andexploitation, and the larger Cpuct will encourage exploration. Itcan be seen from Fig.8(b) that too large and too small Cpuctare detrimental to the algorithm. When Cpuct is too small, thealgorithm cannot sufficiently explore all actions within theaction space given the number of simulations, leading to atendency to get stuck in local optima. Conversely, when Cpuct istoo large, the exploration depth becomes insufficient, makingit difficult to ensure long-term rewards for vehicle movement.Naturally, for different reward function setups, the optimal"}, {"title": "3) Search Depth Statistics:", "content": "The exploration depth repre-sents the agent's ability to predict the future traffic situation. Alarger exploration depth can often show the agent's intelligentbehavior beyond human driving ability. The increase in thedepth of exploration can bring more information, improve theaccuracy of decision-making, and prevent insufficient deci-sions based only on short-term interests, thereby improvingthe quality of long-term decision-making.\nWe calculate the search depth of different methods to sup-port viewpoints in Section.V-D2. In Fig.10, it can be seen thatMCTS algorithm with parallel update and action preferencehas the strongest action exploration ability due to the rapidexclusion of potential unreasonable behaviors in the actionspace."}, {"title": "VI. CONCLUSION", "content": "This paper proposes a novel multi-vehicle collaborativedecision-making method for mesoscopic traffic flow basedon MCTS. The parallel actions in the multi-vehicle jointstate space were analyzed based on driving safety metrics,and a parallel update method was designed in MCTS. Inorder to improve the search efficiency of MCTS, on the onehand, we uses the action preference based on the action"}]}