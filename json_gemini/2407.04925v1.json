{"title": "RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations*", "authors": ["Jiarui Rao", "Jionghao Lin"], "abstract": "Massive Open Online Courses (MOOCs) have significantly enhanced educational accessibility by offering a wide variety of courses and breaking down traditional barriers related to geography, finance, and time. However, students often face difficulties navigating the vast selection of courses, especially when exploring new fields of study. Driven by this challenge, researchers have been exploring course recommender systems to offer tailored guidance that aligns with individual learning preferences and career aspirations. These systems face particular challenges in effectively addressing the \"cold start\" problem for new users. Recent advancements in recommender systems suggest integrating large language models (LLMs) into the recommendation process to enhance personalized recommendations and address the \"cold start\" problem. Motivated by these advancements, our study introduces RAMO (Retrieval-Augmented Generation for MOOCs), a system specifically designed to overcome the \"cold start\" challenges of traditional course recommender systems. The RAMO system leverages the capabilities of LLMs, along with Retrieval-Augmented Generation (RAG)-facilitated contextual understanding, to provide course recommendations through a conversational interface, aiming to enhance the e-learning experience.", "sections": [{"title": "1. INTRODUCTION", "content": "Massive Open Online Courses (MOOCs) gently facilitate access to learning for a diverse global audience [3]. By providing an extensive range of courses through an easily accessible online platform, MOOCs not only enhance individual learning and development but also enrich the broader educational community [4]. However, the diverse categories of courses across disciplines can often overwhelm students when they step into new fields of study [17]. Selecting the right courses that align with both personal interests and academic requirements is crucial, as improper choices may lead to wasted time, and resources, and a lack of fulfillment in one's educational journey.\nTo resolve this, researchers have developed course recommender systems using advanced algorithms to offer tailored guidance that aligns with individual learning preferences [30]. Many existing implementations of recommendation systems have demonstrated significant benefits, such as enhancing personalized learning experiences and improving student engagement, as highlighted by a recent study [11]. However, these systems also face critical limitations, particularly the \"cold start\" problem, which occurs when trying to make recommendations for new users with limited historical data [15]. Though previous research proposed a more complex framework-a novel meta-learning heterogeneous information networks approach [25] to address the \u201ccold start\u201d recommendation issue, the approach faces the challenge of high computational complexity, which is not scalable for large-scale MOOCs platforms.\nIn response to address the limitations of prior work in recommendation systems, where the recommendations lack sufficient personalization and interaction with users, researchers have proposed integrating large language models (LLMs) into course recommendations [18]. This approach enhances recommendation accuracy and personalization by leveraging user history and conversational prompts. Recent frameworks like GPT4Rec [21] and Chat-Rec [9] demonstrated the potential of LLMs in improving course alignment with learners' interests and interaction. However, LLMs can sometimes generate misleading or outdated information. To counteract these shortcomings, one possible solution is the integration of Retrieval-Augmented Generation (RAG) with LLMs [26].\nRAG [10] is a process that optimizes the output of LLMs by extending their robust capabilities to cater specifically to distinct domains or an organization's internal knowledge base, eliminating the need for retraining the model [8]. The use of RAG in recommendation systems enhances the adaptability of LLMs, ensuring that recommendations remain current and contextually relevant [26]. This advancement paves the way for more precise and targeted course recommendations that adapt to changes in educational content and learner preferences. Despite these improvements, there is a noticeable gap in research specifically focused on using LLMs in course recommender systems, particularly in addressing the \"cold start\" problem where the system lacks a user's profile. Thus, our study aims to investigate the potential of LLMs, particularly those enhanced by RAG, in providing course recommendations tailored to individual user needs. We introduce a course recommender system, RAMO (Retrieval-Augmented Generation for MOOCs), which employs a RAG-based LLM model (refer to Figure 1). RAMO leverages RAG's advantage to improve the quality of course recommendations, addressing and mitigating common issues associated with LLMs especially in \"cold start\" problem."}, {"title": "2. RELATED WORKS", "content": "2.1 Course Recommender Systems\nCourse recommender systems are essential in educational technology, helping students choose courses that align with their interests and academic goals. Many prior studies have employed collaborative filtering methods to build course recommender systems [2, 31, 19]. For instance, Schafer et al. [31] proposed a recommender system that suggested courses based on the preferences of similar users. A more recent example by Koren et al. [19] developed advanced collaborative filtering techniques to enhance course recommendation accuracy. However, a significant issue arises when recommending courses for new users, as there is no historical data available for these individuals this is known as the \"cold start\" problem [35]. To address this challenge, a recent study by Wu et al. [35] leveraged large language models (LLMs), which utilize extensive pre-trained knowledge from web datasets, demonstrating potential in overcoming the cold start problem. Despite the advancements in LLMs, their integration into course recommendation systems remains largely unexplored, presenting an opportunity for future research to innovate and improve student course selection processes.\n2.2 Large Language Models in Education\nLarge language models (LLMs) like ChatGPT, trained on extensive datasets, have the ability to generate human-like text and respond to questions with exceptional precision [12, 36]. Many studies have highlighted the potential of LLMs in educational applications, leveraging their capabilities to enhance various aspects of teaching and learning. For example, Kabir and Lin [16] developed an adaptive practicing system utilizing ChatGPT to generate personalized questions and feedback, demonstrating LLMs' potential in facilitating tailored educational interactions. Researchers investigated multiple GPT models on their ability to generate tailored learning materials and provide instant feedback on student errors, enhancing personalized learning experiences [34]. Huber et al. [13] demonstrated the use of LLMs in creating interactive, conversational systems that assist both students and teachers by providing adaptive learning support and resources. Moreover, LLMs are also used in generating automatic feedback for students [5, 6], handling sparse learner performance data [37] from intelligent tutoring systems, predicting learning performance [38], and supporting tutor training session [23].\n2.3 Retrieval-Augmented Generation in Education\nRetrieval-augmented generationn (RAG) has emerged as a significant technique to enhance the effectiveness of educational tools powered by LLMs. For example, a study [20] integrated textbook content into LLM prompts via RAG improved the quality of responses in interactive question-answering (QA) scenarios for middle-school math students, and demonstrated that students generally prefer responses generated by RAGs. RAG has also been employed in programming education to generate improved feedback for student's completion of coding tasks [14], by incorporating transcriptions of lecture recordings and using timestamps as meta-information, RAG reduces hallucinations and ensures the use of accurate technical terms. Moreover, RAG has been utilized to assess novice math tutors' use of social-emotional learning strategies [22], they proved that RAG-enhanced prompts demonstrated more accurate and cost-effective performance compared to other prompting strategies by providing relevant external content. This application highlights the potential of RAG in developing personalized tutor training programs and enhancing the overall effectiveness of tutored learning.\nWhile traditional course recommender systems have laid the groundwork for personalized education, the integration of LLMs and techniques such as RAG offers unprecedented opportunities for enhancing educational experiences. These advanced methods address limitations of earlier approaches and pave the way for more sophisticated and effective educational tools, inspiring us to utilize RAG in developing our course recommender system."}, {"title": "3. METHOD", "content": "3.1 Dataset\nIn this study, we utilized the \"Coursera Courses Dataset 2021\"\u00b9 from Kaggle. The dataset, scraped from Coursera's publicly available information in September 2021, contains a variety of courses that feature comprehensive details such as skill requirements, difficulty levels, and direct course links. It provides a robust knowledge base for our RAMO system, enabling it to suggest courses tailored to students' specific skills and educational needs. This dataset effectively supports our objective to enhance accessibility and personalized learning through course recommendations. We first cleaned the dataset to remove meaningless symbols and duplicate rows, and it has 3,342 non-duplicate courses in total after data-cleaning, with 6 columns:\n\u2022 Course Name: The title of the course.\n\u2022 University: The institution offering the course.\n\u2022 Difficulty Level: The level of complexity of the course content.\n\u2022 Rating: The average rating given by learners.\n\u2022 URL: The web address where the course can be accessed.\n\u2022 Description: A brief overview of what the course covers.\n\u2022 Skills: The specific abilities or knowledge areas that the course aims to develop.\n3.2 Recommendation System Design\n3.2.1 Prompt Design\nThe \"cold start\" problem, where systems lack user historical data, is a significant challenge in recommendation systems. Both traditional course recommender algorithums like content-based and collaborative-filtering algorithms and LLM-based system recommendation systems struggle with this issue. However, our RAG-based solution addresses this by using a 'prompt template' in the back-end. This template guides RAMO to generate relevant responses even when no user-specific data is available, as detailed in Table 1. The RAMO system can provide meaningful recommendations from the outset, unlike non-RAG-based recommender systems, which lack a retrieval process and prompt-based customization. The prompt to our retriever (i.e., to retrieve the relevant docs from the databases) is called the 'prompt template', which is shown in Table 1. The prompt to our generator is composed with three parts: 1) User Question, 2) Prompt Template, and 3) Search Results (the context of the retrieved relevant documents). We also added the uplifting adverb 'fantastic' to the prompt template, to elevate it with Emotional Intelligence since ChatGPT is designed to recognize patterns in language, including those associated with emotions [33].\n3.2.2 Integration of RAG approach\nAs shown in Table 2 below, we employed several LLMs to build our course recommender system. We provide a list of the LLM models we used, along with details on their associated costs and token limits. The token limit refers to the maximum number of tokens (a token represents about 3/4 of a word or four characters, according to Open AI [1]) that the model can process in a single input. While some models, like Llama 2 and Llama 3, are free to use on small-scale dataset, due to their open-source nature, others may incur costs based on usage or subscription plans [27].\nWe then leveraged the RAG approach to enhance the system's understanding of the user context. As shown in Figure ??, RAG consists of two primary components: the retriever and the generator. The retriever aims to enhance the prompt templates, which 'augment' the retrieval process, tailoring it to specific user queries. The knowledge base used for the retrieval process can contain any format of course data (e.g., csv, pdf, and json), providing a flexible and rich source of information for generating responses and we used the largest MOOC platform-coursera's course dataset in csv format as the knowledge base. The dataset was transformed into text embeddings and stored in the vector database. These embeddings were then used to find high-quality, relevant information, which was incorporated into the prompt for the generator. Here we use OpenAI embedding model (text-embedding-ada-002 [29]) to tokenize the course data and store the embeddings in vector store, considering its advantage over BERT (Bidirectional Encoder Representations from Transformers) [7], while OpenAI embeddings [29] offer better generalization and contextual understanding [28], making them more suitable for diverse educational content. The generator is powered by LLMs, which generate the textual contents based on the engineered prompts. To facilitate user's interaction with the system, we make the recommendation process to be completed via conversational manner.\nThe interface of our recommender system is shown in Figure 1, where we listed 5 default courses based on their ratings in the dataset on the web page to make it more user-friendly. As for the implementation of the system, we use GPT-3.5 Turbo, selected for its robust integration with the LangChain [32] framework-a platform designed to streamline the implementation of language models in application-specific contexts. This setup allows the system to dynamically retrieve relevant documents and generate responses tailored to user inputs, as illustrated in the workflow in Figure 2.\n3.2.3 Comparative Analysis\nTo evaluate the performance of our system, we conducted a series of tests by providing different prompts representing various user needs to RAMO. This allowed us to explore its ability to deliver course recommendations based on the outputs generated in response to varied user prompts.\nLLM vs. Non-LLM. We explored both the relevance of the recommended courses to the user's interests and responding time (the time it takes to generate a response) of the LLM-based recommender system compared to non-LLM course recommender systems (e.g., course recommender system using collaborative filtering and content-based approaches), focusing particularly on their ability to address the \"cold start\" problem. This problem occurs when the user lacks specific requirements on what skills they want to learn, and the system lacks data on the new user.\nLLM vs. LLM with RAG. We further examined the performance of a standard LLM recommender system (without RAG and without using a dataset as a knowledge base) versus an RAG-enhanced LLM recommender system by testing different prompt templates for the retriever and various user queries for the generator to ascertain improvements in system performance and recommendation personalization.\nTo explore the performance of our course recommender system, we focused on comparing the relevance of the recommended courses to different prompts by varying prompt templates and user-specific requirements."}, {"title": "4. RESULTS", "content": "4.1 LLM vs. Non-LLM\nWe compared RAMO with a traditional course recommendation system built by the content-based and collaborative filtering using the same dataset\u00b2. During this comparison, we focused on the \"cold start\" problem. The \"cold start\" problem is especially pertinent in the context of an e-learning platform for tutor training, such as tutor training platform [24]. When new tutors join the platform, they are encouraged to complete various training courses to enhance their tutoring skills. Given the wide range of courses available, new tutors may feel overwhelmed when deciding where to begin their learning journey. In such scenarios, they may ask general questions such as, \u201cWhat can I learn today since I am a new tutor onboarding to this platform?\u201d They do not have prior course completions or specific learning preferences logged in the system, making it challenging for the recommendation system to personalize suggestions based on historical data. When prompted with \"I am a new user\", the traditional recommender system failed to generate a recommendation because its algorithm relies on the cosine similarity of the descriptive texts of the user's desired learning topic and the database items, and there are no courses with similar title or description as the phrase 'new user'. In contrast, both our standard LLM and the RAG-enhanced LLM system can provide relevant course suggestions for the new user, with the LLM offering more detailed descriptions based on its internal knowledge base and RAG offering more customized outputs based on its external knowledge base and the prompt template we designed. The comparative results for both the standard and RAG-based recommender systems are displayed in Figure 3.\nRegarding system performance, the traditional system typically took about 0.02 seconds longer than RAMO to generate responses according to the same user interest a certain topic the user wants to learn, and this delay increased with the complexity of the user's input regarding relevant skills.\n4.2 LLM vs. LLM with RAG\nTo explore how well our LLMs can provide personalized course recommendations, we used prompts that specified a particular skill to be learned. The non-RAG LLM (based on GPT-3.5) delivered detailed suggestions for relevant courses available on Coursera, utilizing its internal database of courses. In contrast, the recommendations from the RAG-enhanced LLM varied according to the specific prompt template used by the retriever. This adaptability allows developers to tailor the quantity and detail of the courses recommended, showcasing the flexibility of the RAG approach. The user interface and the outcomes for a query focused on learning a specific skill are illustrated in Figure 4.We modified the retrieval prompts and generation queries to test the adaptability of our recommendation system. First, we conducted tests on various user queries using the same prompt template to compare the variations in output. The first module in Figure 5 illustrates the system's response to a \"cold start\" problem, while modules 2 through 6 demonstrate how the output varies based on user questions about the number of courses recommended and the level of detail provided, such as reasons for recommendations, URLs, and other specifics. For example, when user asks question like \"I want to learn python, can you recommend me some courses?\", RAMO can give the output to the user: \"Sure! Here are some recommended Python courses for you: 1. Introduction to Python 2. Crash Course on Python 3. First Python Program 4. Python Basics These courses cover a range of topics from basic syntax to building interactive applications. Happy learning!\" When the user changes their mind and decides to learn about another topic, RAMO can give relevant recommendations. The outputs consistently matched the user requirements in relevance, successfully retrieving the pertinent courses from the Coursera dataset, more examples could be found at Figure 5.\nWe also utilized different retrieval prompt templates to explore how the output varies based on different prompts. Specifically, we used the same user question \"I want to learn python\", and altered the prompt templates to specify the number of recommended courses and the level of detail provided in the output, ranging from mere course titles to comprehensive descriptions that include titles, URLs, and rationales for each recommendation. The variations in the prompt templates and their corresponding outputs are illustrated in Figure 6. Here, red lines highlight changes in the number of courses recommended, blue lines detail the content of the courses such as the inclusion of reasons for recommendations or just the course titles, ratings, and URLs while green highlights how we addressed the \"cold-start\" problem, resulting in recommendations of the three most popular (based on course ratings) and easiest courses (based on its difficulty level), as depicted in the output module labeled 1 in Figure 6. The generated response in response to varied prompts underscores the system's robustness; for instance, when the template specifies \"recommend three courses at a time\", the output consistently includes exactly three courses. Similarly, if the prompt contains 'course URLs and titles', the system reliably appends this information to each recommended course, ensuring that the output meticulously adheres to the specified criteria."}, {"title": "5. CONCLUSIONS", "content": "In this study, we have demonstrated the application of LLMs as course recommender systems, particularly within MOOCS. Our findings confirm the potential of LLMs to deliver personalized course recommendations based on user's different requirements. We initially compared four LLMs, including GPT-3.5 Turbo and GPT-4. Ultimately, we selected GPT-3.5 as the back-end model for the RAMO system due to its comparable performance to GPT-4 at a lower cost. Although the Llama models are free to access, we found that the GPT models were significantly faster. Specifically, GPT-3.5 had an approximate response time of 3 seconds, whereas Llama 2 and Llama 3 took approximately 5 minutes and 8 minutes, respectively. Furthermore, the integration of RAG has enhanced the quality of recommendation outputs, as evidenced by the generated responses based on various user prompts, which are highly related to user's needs and all came from the knowledge base. Additionally, our system supports conversational interaction with users, which could be seamlessly integrated into numerous online educational platforms. Our use of open-source LLMs (e.g. Llama 2 and Llama 3 [27]) has also been validated, proving to be a cost-effective approach for broader deployment.\nLimitations\nAs this study is ongoing, we have not yet conducted comprehensive evaluations of our recommender systems, including human evaluations or user studies. This is primarily due to the nascent stage of our research. Moreover, while many research projects on recommendation systems employ benchmarks to evaluate system adaptability, our study currently lacks such benchmarks because we do not possess a test dataset. The Coursera dataset we utilized includes only course data, lacking user profiles which are essential for evaluating the effectiveness of recommender systems across different time periods. If we had access to user data, including users' past course learning histories and their preferences, we could integrate this information with the course data to enhance our retrieval process. This integration would allow us to personalize recommendations more effectively, tailoring course suggestions to individual learning patterns and preferences. Incorporating detailed user data would enable RAMO to provide more accurate and relevant recommendations, improving user satisfaction and engagement. It would also allow for longitudinal studies to track how users' interactions with the system evolve over time and how well the recommendations align with their long-term learning goals.\nFuture Work\nWe plan to undertake several further steps to advance our research. Firstly, we aim to conduct thorough evaluations and tests to validate the efficacy and reliability of our recommender systems. This will involve integrating user studies and utilizing real user data once our systems are deployed on our e-learning platform. Such measures will enable us to robustly measure performance and refine our approach. Secondly, we will focus on enhancing system performance, considering scalability and the potential to expand our technology to encompass a broader range of educational tools and platforms. These efforts will ensure that our recommender systems not only meet current educational needs but also adapt to future demands and technological advancements. Thirdly, we could deploy RAMO on our own e-learning platform, and then have the opportunity to gather comprehensive user data and utilize our own course dataset rather than Coursera's. This deployment would allow us to conduct extensive testing and validation, further proving the eligibility and effectiveness of the LLM for recommending courses. With access to real-time user data, we could continuously refine our algorithms, making the system more adaptive and responsive to users' evolving needs.\nTo evaluate the effectiveness of our LLM-based course recommendation system, we plan to conduct a comprehensive experiment that includes quantitative metrics, user studies, and personalization improvements. Our experiment aims to assess both the relevancy of the recommendations and the satisfaction of the users with the recommended courses.\nWe will utilize several quantitative metrics to evaluate the performance of the recommendation system. Key metrics include post-test performance, measured by the improvement in students' scores from pre-test to post-test after tutoring sessions, and course completion rate, which compares the rate of course completion between students who follow the system's recommendations and those who do not. Additionally, engagement rate will be tracked by monitoring whether students continue engaging with the lesson without dropping out midway. User satisfaction will also be assessed through feedback collected after each lesson via a thumbs-up or thumbs-down system and detailed surveys. To gather qualitative insights into the system's effectiveness and user experience, we will conduct user studies. These will involve satisfaction surveys completed by students following each lesson to gauge their satisfaction with the course content and the relevance of the recommendations, as well as focus group discussions to explore students' experiences in more depth and gather suggestions for improvement."}, {"title": "6. ACKNOWLEDGMENTS", "content": "We extend our sincere gratitude to Chenfei Lou, a current software engineer at X (former twitter), for his invaluable guidance in developing our demo. We also thank Sandy Zhao, a current master's student in the CMU METALs program, for her excellent assistance in generating the wonderful diagram. Additionally, we appreciate Yuting Wang, an undergraduate student at CMU, for her help in refining the design in this paper."}]}