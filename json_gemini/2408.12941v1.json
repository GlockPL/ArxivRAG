{"title": "iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations", "authors": ["Anjana Wijekoona", "Nirmalie Wiratunga", "David Corsar", "Kyle Martin", "Ikechukwu Nkisi-Orji", "Chamath Palihawadana", "Marta Caro-Mart\u00ednez", "Belen D\u00edaz-Agudo", "Derek Bridge", "Anne Liretd"], "abstract": "Explainable AI (XAI) can greatly enhance user trust and satisfaction in AI-assisted decision-making processes. Recent findings suggest that a single explainer may not meet the diverse needs of multiple users in an AI system; indeed, even individual users may require multiple explanations. This highlights the necessity for a \"multi-shot\" approach, employing a combination of explainers to form what we introduce as an \"explanation strategy\". Tailored to a specific user or a user group, an \u201cexplanation experience\" describes interactions with personalised strategies designed to enhance their AI decision-making processes. The iSee platform is designed for the intelligent sharing and reuse of explanation experiences, using Case-based Reasoning to advance best practices in XAI. The platform provides tools that enable AI system designers, i.e. design users, to design and iteratively revise the most suitable explanation strategy for their AI system to satisfy end-user needs. All knowledge generated within the iSee platform is formalised by the iSee ontology for interoperability. We use a summative mixed methods study protocol to evaluate the usability and utility of the iSee platform with six design users across varying levels of AI and XAI expertise. Our findings confirm that the iSee platform effectively generalises across applications and its potential to promote the adoption of XAI best practices.", "sections": [{"title": "1 Introduction", "content": "XAI systems must be able to address a range of explanation needs (such as transparency, scrutability, and trust) and must do so in a manner that is relevant to a range of stakeholders. It is also now a regulatory requirement in many parts of the world such as the right to obtain an explanation in the EU [8, 13]. It is essential for an Al system looking to implement XAI to learn from successful past experiences of XAI adaptations that reveal best practices. Case-based Reasoning (CBR) caters to this need whereby it learns from past experiences [2, 14]. The iSee platform has proposed utilising the CBR paradigm to capture knowledge and experience from successful adaptations of explainability within AI systems [37].\nIt is increasingly recognised that a single explanation is often insufficient to satisfy all situations and/or all stakeholders [3, 25]. Multi-shot explanations, allowing users to digest explanations from multiple algorithms over the course of a single interaction, have been demonstrated to provide more satisfactory user experiences [24, 37]. However, bespoke development of domain-specific and personalised explanation strategies is prohibitively expensive and requires significant domain and XAI expertise. Explainability toolkits have emerged to facilitate the development of explainability for use cases, including IBM-XAI 360 [4], Alibi [19] and Captum [20]. Where existing toolkits fall short, iSee fills this gap by providing support to design users, regardless of their level of expertise in XAI, to develop explanation strategies based on the collective experiences of others.\nThe iSee Cockpit is designed to elicit stakeholder requirements for explainability from the design user, which drives the underpinning CBR paradigm of the platform. Accordingly, in this paper, we evaluate the iSee Cockpit tools for capturing these requirements from real-world design users towards the design of multi-shot explanation strategies. We make two key contributions: 1) A formalisation of the multi-shot explanation experience underpinned by the CBR paradigm; and 2) The design and findings of a user experience evaluation of the iSee cockpit with insights from six design users, high-"}, {"title": "3 iSee Platform", "content": "The goal of the iSee platform is to help design users create and refine explanation strategies for their XAI systems to ensure end-user satisfaction. Using the CBR 4R steps, iSee is organised to retrieve, reuse, revise, and retain explanation experiences as cases. Figure 1 illustrates how the iSee platform is underpinned by the CBR paradigm. Central to CBR's 4Rs are its knowledge containers: case base, case similarity and case adaptation. In iSee, these containers are formalised using the iSee ontology for interoperability."}, {"title": "3.1 iSee platform overview", "content": "The iSee Cockpit elicits explainability requirements from a design-user, who is an expert of the AI system's design and its stakeholder needs. These requirements form the query to our case base of past experiences, facilitating the retrieval of the most suitable explanation strategies. iSee provides tools to automatically adapt a recommended solution to further match design-user requirements by reusing multiple explanation strategies from nearest neighbours. The design user can then evaluate a recommended (and adapted) strategy solution with a representative sample of their stakeholders to get feedback that can then be used for collaborative revision of the case description and solution. Once the stakeholder explanation needs are met, the design user can finalise the validated explanation strategy for their AI system thus forming a new case. The quality and coverage of cases in the case base enhances case-based recommendations. Accordingly retaining a complete anonymised case with the design user's consent is an important last step in iSee's 4R CBR cycle.\nThe iSee platform was implemented using a micro-service based approach. Each module of the platform (i.e. user interface, case retrieval, failure-driven adaptation, etc) can be hosted and executed independently on a single or multiple servers. Modules are logically connected to each other through standardised API endpoints, allowing flexibility for allocation of computational resources required to execute them."}, {"title": "3.2 Explanation Experience Case Base", "content": "An explanation experience case is a multi-faceted entity that encapsulates several knowledge constructs: the attributes of the AI system; user groups and their explanation needs; the explanation strategy; and user explanation experience feedback (see Figure 2). More formally the iSee case base is a collection of past explanation experiences, each case c represented as a triplet.\n$C = {CD, CS, CO}$\nWhere case description (D) covers the constructs related to explanation requirements, a solution (S) representing the explanation strategy and an outcome O capturing user feedback. Here a query q is a case where the solution and the outcome are empty (S, O = 0). The majority of the cases are selected from literature following a critical review and we include several anonymised industry cases. iSee utilises these cases to recommend strategies to design users who are looking to build explainability in their AI systems.\nAn explanation strategy, i.e, the case solution is modelled using a Behaviour Tree (BT) [9]. The example explanation strategy BT in Figure 3 is executed as follows. If the user asks a \"why\" kind of question, answer them with a GradCAM explanation and if they need to verify with another type of explanation (variant) provide the nearest neighbours; if the user is still not satisfied and asks a further \"what\""}, {"title": "3.3 Case Retrieval using Similarity Knowledge", "content": "The retrieval task finds similar cases from the case base using similarity knowledge, which specifies local similarity metrics and aggregates them into a global similarity score.\n$local_sim=\\begin{cases}WP & \\text{if } j\\in \\text{[AI Task, AI Method]}\\\\QI & \\text{if } j\\in \\text{[Technical Facilities, User Questions]}\\\\EM & \\text{otherwise}\\end{cases}$\nwhere j refers to a feature in D. The similarity metrics are as follows.\nWu & Palmer (WP) is a taxonomy path-based similarity metric originally implemented for calculating word similarities. For Al Task and Al Method case attributes, given the taxonomic representation from iSee ontology, it computes node similarity between the query and the case nodes based on node depths and distances from the most specific common ancestor [26].\nQuery Intersection (QI) is applicable for attributes where the data type is a set of ontology individuals such as attributes User Questions and Technical Facilities. Given the feature j from query q and case i, it calculates the similarity as the intersection between two sets normalised by the query set size as ($|c_q \\cap c_i|$/$|c_q|$).\nExact Match (EM) similarity indicates a string match. This is applied both for case attributes that are ontology individuals, and is the most common method of comparison.\niSee implements retrieval using CloodCBR [26] in two phases: 1) filter case base to only include cases that exactly match the query DataSetType (dt) (Equation 1); and 2) calculate pair-wise similarity to each filtered case to select the top k most similar cases (Equation 2).\n$C' = \\{c \\in C | c_{dt} = Cat\\}$ (1)\n$Top-k = argmax_k \\sum_{j=1}^{|D|} sim(c^q, c^i)$ (2)\nThe similarity between the query case c\u00ba, and a case c\u00b2 from the case subset C' is calculated as the aggregation of local similarities as in Equation 2. The single top case, is the case with the highest global similarity score among the top k and the recommended explanation strategy for the query requirements."}, {"title": "3.4 Explanation Strategy Reuse", "content": "The CBR methodology recommends solution adaptation before reuse to: 1) address unmet requirements on the query description; and 2) personalise the solution utilising domain knowledge. iSee offers a failure-driven adaptation algorithm to address the former and envision the latter to be a manual process.\nAdaptation of solutions is driven by the failure to fulfil the query's User Questions. The mismatch between the recommended case and the query is calculated using the Query Intersection similarity and when similarity is \u2264 1 we apply a stable marriage algorithm on the top k case solutions (k > 1) to find user question-explanation strategy sub-trees that satisfy all (or as many) of the user questions that appear in the user's query. The adapted explanation strategy BT is formed of these sub-trees. Figure 4 presents an example where 2 of"}, {"title": "3.5 Explanation Strategy Revision", "content": "iSee provides an editor and the following supporting tools for design users to revise explanation strategies.\nExplainer Applicability warns the design user about the implementation mismatches between the explainers in the recommended strategy and their query case. These mismatches include 1) the implementation framework supported by the explainers and that of the query AI model; 2) model access requirements of the explainers and model access provided by the design user (model file or API access to the predict function); and 3) labelled data requirements of the explainers and data provided by the design user.\nExplainer Substitution provides the design user with substitution recommendations for a selected explainer. This follows a similar approach to retrieval on a library of explainers where each is characterised using a set of semantic features (see Figure 5). The similarity between explainers is calculated as $e\\_sim(e^q, e^i) = \\sum_{j=1}^{M} local\\_e\\_sim(e^q, e^i)$ using the following local similarities.\n$local\\_e\\_sim = \\begin{cases}WP & \\text{if } j\\in \\text{[Al Tasks, AI Methods, Presentation]}\\\\QI & \\text{if } j\\in \\text{[Implementation Frameworks, Explanation Technique, Explanation Type]}\\\\EM & \\text{otherwise}\\end{cases}$\nSub-tree Substitution provides applicable sub-tree substitutions for a selected sub-tree. Substitutions are selected from the solutions of similar cases based on edit-distance similarity. iSee transforms the query and case sub-trees into directed graphs and calculates edit distance using the node similarities defined below where type(n) returns the strategy node type.\n$node\\_sim = \\begin{cases}e\\_sim & \\text{if type(n$^q$, n$^i$)} = \\text{Explainer}\\\\sem\\_sim(.) & \\text{if type(n$^q$, n$^i$)} = \\text{User Question}\\\\1 & \\text{type(n$^q$)} = \\text{type(n$^i$)}\\\\0 & \\text{type(n$^q$)} \\neq \\text{type(n$^i$)}\\end{cases}$"}, {"title": "3.6 Case Retention", "content": "Once the design user has a recommended solution, adapted and/or revised, they evaluate it with their stakeholders. iSee provides a chatbot interface where stakeholders can execute the strategy to create explanation experiences and provide feedback [36, 38]. Design users can utilise this feedback to iteratively improve the case description and the solution, aiming for stakeholder satisfaction.\nWe form the case outcome from the stakeholder feedback obtained using the XAI Experience Quality (XEQ) Scale [39] tool which was developed as a psychometric scale for measuring the quality of explanation experiences across 4 dimensions: Learning, Utility, Fulfilment and Engagement. The case outcome records the mean score in each dimension. During case retention, iSee creates an anonymised copy of the complete case and retains it in the case base. A case maintenance policy [11] can then be used to periodically review the case base considering case coverage."}, {"title": "3.7 Example Use Case: Radiograph Classification", "content": "We describe a radiograph classification system provided by an industry partner. The AI system is implemented using ConvNet-based architecture for binary classification of fractures in radiographs. The stakeholder explanation needs of this use case stem from two factors: 1) to improve the quality of their product for end-users; and 2) to increase regulatory compliance with relevant governance bodies. The design user described two user groups: 1) clinicians who are using the AI system for decision support; and 2) managers who are looking to evaluate the compliance, risk and regulatory requirements.\nUsing callouts of iSee screenshots in Figure 6, we illustrate how a design user can interact with the iSee retrieve, reuse, and revise tools to create a complete Explanation Experience case, containing both the case description and solution parts, and retain it in the case base. Firstly, an AI model description and implementation of a ConvNet model for binary classification of black and white radiography images is entered into the iSee Cockpit. Further details on how to access the model can also be provided. User groups and intents part of the description include details of a clinician persona, alongside corresponding intents in transparency and performance, thus completing the query case description $Q_D$. The completed case description parts can be used to query the iSee case base and retrieve a set of candidate cases containing previous best practices of explanation strategies. In the example in Figure 6, of the retrieved three cases containing variations of strategies include a combination of feature attribution and nearest neighbour-based explainers (top of blue callout). The design user can decide to reuse the recommended solution arrived at after iSee performs a failure-driven transformational adaptation to obtain a personalised strategy. After that, they can decide to perform a manual revise step using a strategy editor (bottom of blue callout), which will provide a ranked list of substitute explainers for any selected explainer node that the designer user wishes to change (as demonstrated here by highlighting the Integrated Gradients explainer node for substitution). Once revisions are complete, the case contains the refined solution component. It can be evaluated with target stakeholders to identify the case outcome (which is measured against the dimensions of the XEQ Scale). This allows the formation of a complete case, which can subsequently be retained in the case base to inform future practice."}, {"title": "4 User Experience Evaluation with Design Users", "content": "A summative assessment, using a mixed methods study, was performed to evaluate the user experience of design users. We aim to evaluate the following two dimensions.\n\u2022 Utility: Do the design users perceive the tool as fit for purpose?"}, {"title": "4.1 Study Protocol", "content": "We planned a two-stage user-centred evaluation session with a design user lasting approximately 1 hour. A session is standardised using the following protocol:\n1. To open the session, the researcher provided a brief overview of the iSee project and the objectives of the session. A toy example of a loan approval XAI system was used to illustrate the specific information requested on the Cockpit.\n2. We then conducted a concurrent Think-Aloud Protocol (TAP) where the participant was given access to the iSee Cockpit and asked to create their use case as a design-user of the system. Throughout the session, participants were encouraged to vocalise their thoughts. The researcher intervened only when necessary, (i.e. when the user sought clarification or was unable to proceed).\n3. On completion of the TAP, the participant was asked to respond to the User Experience Questionnaire (UEQ) consisting of 26 questions on a 7-step Likert scale.\n4. To conclude the session, the researcher asked participants a series of open-ended questions. These questions aimed to establish a design user profile and capture any additional comments or insights"}, {"title": "4.2 Recruitment", "content": "This study involved six design users, two conducted in person and four online over Microsoft Teams. They were recruited through existing academic and industry connections with the leading institution. These design users were distinct from any design users who were in-"}, {"title": "4.3 Study Instruments", "content": "The UEQ questions assessed user perception of usability (UEQ dimensions efficiency, perspicuity and dependability) as well as user experience (UEQ dimensions novelty and stimulation). The overall impression of the product is measured by the attractiveness dimension. For our study, we were interested in 2 out of the 4 objectives of the original UEQ framework related to establishing sufficient user experience and identifying areas of improvement. For a more detailed discussion on the UEQ, please refer to [32].\nThe UEQ benchmark is based on an analysis of 163 products consisting of business applications, development tools, e-commerce websites, social networks and mobile applications. Each dimension measures the scores in 5 categories from excellent, good, above average, below average and bad, and a new product is expected to reach good category in all dimensions. We note that the benchmark has not considered technological or research-oriented products like the iSee platform. Also, due to the limited number of use cases, we are not able to establish the statistical significance of the results. The Concurrent TAP method was used to obtain qualitative insights into user experience by asking users to verbalise their thoughts as they use a system to perform a specific task [16, 34]. These sessions facilitate targeted usability evaluation of specialist tools, as they allow flexible task performance and researcher intervention when required."}, {"title": "4.4 Analysis", "content": "Using the above instruments, the study generated three artefacts: 1) transcriptions of the audio and screen recordings; 2) researcher notes documented during TAP sessions; and 3) UEQ responses. They are utilised in a two-part quantitative and qualitative analysis: 1) measure user experience against established UEQ benchmarks; and 2) combine UEQ responses, the researcher notes, and transcripts to perform a thematic analysis of TAP session outcomes.\nWe used the recording to analyse the time taken to use the cockpit to produce a functional explanation strategy. The starting point was when participants clicked the 'Create Use Case' button, and the endpoint was when participants saved the evaluation questionnaire (which is the final stage of use case creation). The mean time taken was 25 minutes and 51 seconds and a breakdown of time taken per component of the Cockpit is available in Figure 7.\nDespite the relative freedom of the TAP session, all users converged on a similar progression through the components of the Cockpit interface. The similarity of user pathways highlights the interface is structured in a logical manner. Examining individual sessions, all 3 industry design users spent the majority of their time (%) identifying persona intents while academic design users primarily focused on configuring AI model settings. This suggests a difference in prioritisation for explanation strategy design where industry users are focused on end-user needs, while academics are focused on model details. We highlight that differences in expertise level do not seem to reflect in the time taken to complete the exercise. This promising result highlights the platform is equally suitable for expert and non-expert design users."}, {"title": "5 Results and Discussion", "content": "5.1 UEQ Findings\nOverall, the Cockpit has been scored above above average, and achieves good category on user experience (i.e. Stimulation and Novelty). Figure 8 presents how the iSee Cockpit scored across the six dimensions measured by UEQ. The y-axis scale ranges from +3 to -3 with the mean response of 0 representing a neutral sentiment. For Attractiveness, Perspicuity and Efficiency, the cockpit scores above average, while scoring below average for Dependability. It is noteworthy that the benchmark has been established on products intended for the general public whereas the Cockpit caters to a specialised group of expert users. Despite this, we have achieved above average, which is very promising.\nFigure 9 presents a detailed view of individual responses. We highlight that responses are overall positive; 20/26 questions are majority positive, with the remaining 6 questions being majority neutral or equally split between positive and neutral. Importantly, there are no questions with majority-negative responses (with only negative or neutral responses). We explore the justification for these responses by examining the TAP session transcripts in the following section."}, {"title": "5.2 TAP Findings", "content": "Here we present the findings of a thematic analysis broken down across the UEQ themes and evidenced using TAP session transcripts.\nAttractiveness Users had an overall positive view of the iSee Cockpit, and quickly identified the value that stakeholder-driven explanation strategies would add to their practice:\nEach time [an Al] gives a prediction, I don't trust it at face\nvalue. So getting the the evidence behind the prediction is one\nof the high priorities for me. - D3\nSimilar comments were made by all users. We take this as evidence the iSee Cockpit contributes to satisfying business needs for low-code development of explanation strategies. There were comments regarding the presentation of the Cockpit interface. Most comments targeted improving guidance for navigation or reducing verbiage (see below discussion of Perspicuity for examples). Otherwise, users were satisfied with the presentation."}, {"title": "6 Conclusion", "content": "In this paper, we have described the implementation and evaluation of the iSee system. The iSee platform is based on CBR for the reuse of best practices in creating multi-shot explanation experiences. We presented the findings of a comprehensive user-centred evaluation including both industry and academic participants where we demonstrated the utility and usability of the system via the recognised UEQ benchmark, and analysis of think-aloud session outcomes. Our findings highlighted that both expert and non-expert design users found iSee comparably useful in assisting the implementation of multi-shot XAI in their AI systems. In future, the iSee platform will continue to grow by evaluating and improving the iSee Cockpit to enhance the design user experience; improving the coverage of the case base for improved recommendations and extending the availability of explanation methods for improved adaptation and revision."}, {"title": "Ethical Statement", "content": "The study protocol was reviewed and approved by the lead institution's ethics review committee. Informed consent was obtained from all participants."}]}