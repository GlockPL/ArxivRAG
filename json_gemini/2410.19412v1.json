{"title": "Robust Time Series Causal Discovery for Agent-Based Model Validation", "authors": ["Gene Yu", "Ce Guo", "Wayne Luk"], "abstract": "Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures.\nThe approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches.\nBy tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.", "sections": [{"title": "Introduction", "content": "In the modern era, Agent-Based Models (ABMs) fall under the class of modelling and simulation techniques which are increasingly being used in domains such as theoretical economics, finance, social sciences, and epidemiology. It involves a bottom-up approach by putting together individual agents and their interactions so that various phenomena may be synthesised and then examined. The detailed methodologies of such models allow them to understand complexity and draw futuristic conclusions.\nRecent literature stresses the significant advantages of agent-based models (ABMs) over traditional economic models. Fagiolo (2019) pinpoints the major strengths of ABMs: their capacity to provide comprehensive narratives of interactions among agents with network structures, incomplete information learning processes, and competition in imperfect markets\u2014and the flexibility they provide in validating both model inputs and outputs [1]. This characteristic has gained much attention and prompted much research activity recently."}, {"title": "The Importance of ABM Validation", "content": "To successfully implement ABMs in reality, the \u201cvalidation\u201d of this model could be the decisive factor for its ability to truly reflect real-world reality. The validation process consists of comparing the model output with the actual data obtained in the real world to make sure it is reliable and effective. This process is utilised to establish the credibility of the model and the reliability of the predictions made.\nValidation is particularly challenging in fields with complex interactions and non-linear dynamics, such as finance. As Windrum et al. (2007) pointed out, significant effort is still needed to realise consistent and satisfactory techniques of ABM method implementation to real-world financial data [2].\nA key component in ABM validation is to find the cause-and-effect mechanisms from data. Besides highlighting the importance of correlational testing, causal matching between the ABM outputs and real-world data has recently been emphasized in validation. These approaches aim to understand and explain the origins and propagation of observed phenomena in financial systems [3]. The details of such causal discovery methods and their application in ABM validation will be further discussed in the following chapters."}, {"title": "Challenges to Address", "content": "Indeed, in spite of continuous progress in the ABM validation techniques, there are still the most significant challenges in such complex systems applications:\n1. Insufficient Robustness of Time Series Causal Discovery Methods: The causal discovery approaches that are commonly used today, such as VAR-LINGAM and PCMCI, can be quite susceptible to noise and variations in the data. This could create inconsistencies when the method is applied to different subsets of the same dataset or datasets with slightly different characteristics. For ABM validation purposes, the robustness of these techniques should be enhanced. False causality in this regard could be as harmful as wrong conclusions drawn up about the ABM system's validity and, consequently, about the underlying system mechanisms.\n2. Lack of Comprehensive Understanding of Dataset Characteristics' Impact: While previous studies have examined specific dataset characteristics, there is a lack of comprehensive understanding of how various dataset properties collectively affect the performance of causal discovery methods in ABM validation. The absence of a systematic comparison across a wide range of dataset types (e.g., linear vs. non-linear, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary, sparse vs. dense causal structures) hinders our ability to select appropriate validation techniques and interpret results accurately. This deficiency stands as a huge constraint in the area of creating functional ABM validation processes as a solution to the wide spectrum of complicated problems.\n3. Limitations in Existing ABM Validation Frameworks: Even though much progress has been made in ABM Validation, for instance, the framework proposed by Guerini et al. (2017) [3], current approaches still face several key limitations:\n(a) Insufficient Dataset Property Analysis: Existing frameworks often lack comprehensive tests for some important dataset properties. For instance, some of these may overlook important characteristics such as linearity and stationarity, which are essential for understanding the nature of the data and selecting appropriate modelling techniques.\n(b) Limited Options for Causal Discovery Methods: Most current frameworks rely on a single or limited set of causal discovery methods. This limitation may prevent us from obtaining optimal performance when dealing with different dataset characteristics or different priorities (such as accuracy vs. efficiency). The lack of method diversity limits the framework's adaptability to various scenarios and data types.\n(c) Narrow Range of Performance Metrics: The existing validation framework typically focuses on basic similarity tests or a limited set of performance metrics. This may cause to failure to capture the full range of model performance, especially in complex financial systems where causal relationships can be intricate."}, {"title": "Novel Approaches and Contributions", "content": "This research addresses the challenges mentioned above by utilizing several new approaches and providing the following key contributions:\n1. Robust Cross-Validated (RCV) Causal Discovery Method (Chapter 3): We introduce a novel approach to enhance the robustness of existing causal discovery methods. By applying cross-validation techniques to causal discovery algorithms such as VAR-LINGAM and PCMCI, we aim to mitigate the sensitivity of these methods to noise and data variations to improve the consistency and reliability of causal structure identification in complex time series data.\n2. Comprehensive Experimental Evaluation and Analysis (Chapter 4): We present a thorough empirical analysis of our proposed methods and existing approaches. This evaluation covers a wide range of characteristics, including linear vs. non-linear relationships, Gaussian vs. non-Gaussian noise, stationary vs. non-stationary behaviour, and sparse vs. dense causal structures. We also examine the scalability of methods with varying numbers of variables and time series lengths. We ran experimental evaluations on both synthetic datasets with controlled properties and a complex simulated fMRI dataset, so as to provide insights into method performance under various conditions.\n3. Context-Aware ABM Validation Framework (Chapter 5): Extending the work done by Guerini, we develop an enhanced ABM validation framework that addresses the weaknesses of existing one. In this framework, the user could choose a suitable method of causal inference based on the property of data sets or other validation needs, such as efficiency or accuracy. Another improvement we feature is to include a more comprehensive set of performance metrics for assessing the causal relations to get a more precise evaluation of model performance. This framework is designed to pre-process datasets and ensure their uniformity, analyse dataset attributes, run user-dependent or driven causal structure detection, and enhance validation evaluations.\nThese contributions enhance ABM validation in complex systems by improving causal discovery methods and offering a flexible validation framework. This research aims to increase the accuracy and reliability of ABMs in capturing real-world dynamics across various domains."}, {"title": "Report Structure", "content": "The remainder of this report is structured as follows: Chapter 2 provides the foundational background and reviews related work, covering Agent-Based Models in finance, existing ABM validation techniques, causal discovery methods and the current limitations. Chapter 3 introduces our novel Robust Cross-Validated (RCV) Causal Discovery Approach, detailing its theoretical foundations and implementation. Chapter 4 presents our experimental evaluation and analysis, including synthetic dataset generation, comparative analysis of causal discovery methods, and an application to a complex simulated fMRI dataset that mimics real-world neuroimaging data. Chapter 5 describes our Context-Aware ABM Validation Framework, explaining how it integrates improved causal discovery methods and enhances overall validation reliability. Finally, Chapter 6 concludes the report, summarizing key findings, discussing implications for ABM validation in complex systems and suggesting future research directions."}, {"title": "Agent-Based Models in Finance", "content": "Agent-Based Models (ABMs) are increasingly being employed for simulating systems' complexity, specifically in finance and economics. LeBaron (2000) underscores the involvement of such models in thinking dynamics of economic models' interactions as he states it is: \u201cbeginning to show promise as a research methodology that will greatly impact how we think about interactions in economic models\" [4]. The basic principle of ABM is based on the behaviour that each agent adapts to the conditions that affect the market. They are especially conducive to examining complex phenomena, those emergent market behaviors, such as price movements, can be generated by simple rules of interaction at the agent level.\nThe ABMs turned out to be particularly helpful in explaining the economic crisis and thus making way for the implementation of good policy measures. They could illustrate how individual interactions can lead to emerging economic patterns [5] or be used to study wealth dynamics and economic inequality [6]. Raberto, et al. (2012) took a step further, exploring other arenas besides borrowing and the debt of borrowers to find out how they valued economic production. Their research suggests that such structural models can demonstrate the potential to depict complex economic relationships by keeping the complex relationship between the physical and financial variables of the economy. [7].\nRecently, interest in highly accurate market simulation as a supportive tool for AI research into financial applications has increased. ABIDES (Agent-Based Interactive Discrete Event Simulation) is a platform tailored as a research space for Al agents used in financial markets, as shown in Byrd (2020) [8]. These platforms can provide researchers with the toolbox for simulating intricate market dynamics and applying AI algorithms within the context of realistic financial cases."}, {"title": "ABM Validation Approaches", "content": "Validation of Agent-Based Models (ABMs) is critical to ensure they accurately represent real-world systems and provide reliable insights [9]. The validation process typically involves several key dimensions. Ziegler categorized validity into three different types [10]:\n\u2022 Replicative Validity: Ensuring that the model fits external data.\n\u2022 Predictive Validity: Ensuring that the model matches data that can be acquired from the modelled system.\n\u2022 Structural Validity: Ensuring that the model accurately reflects the processes it is designed to simulate.\nThese concepts match the definition of validation stated in the literature, which emphasises the importance of demonstrating that a model correctly reflects the real world for its intended use [11, 12].\nIn the context of financial ABMs, validation techniques have evolved to address the complex dynamics of financial markets. Traditional methods often focus on comparing model outputs with historical data, but recent progress has led to more advanced approaches.\nOne of the important developments in ABM validation is the causal matching approach proposed by Guerini and Moneta [3]. This approach consists of five steps as shown in Figure 2.1: ensuring dataset consistency, analyzing ABM characteristics, estimating Vector Autoregression (VAR) models, identifying Structural VAR (SVAR), and assessing the validation [3]. This method aims to reproduce the causal relations observed in real financial systems, providing a more rigorous validation framework that goes beyond simple correlational matching.\nOther validation techniques include:\n\u2022 Indirect Calibration: This method compares ABM simulation results with real-world data, focusing on reproducing key statistical properties or stylized facts about financial markets [13].\n\u2022 Method of Simulated Moments: This approach estimates model parameters by matching moments of simulated data to moments of observed data [14].\n\u2022 Bayesian Estimation: This technique uses Bayesian inference to estimate model parameters that can incorporate prior knowledge and uncertainty quantification [15].\n\u2022 Pattern-Oriented Modelling: This approach uses many patterns observed in real-world systems to guide the development and validation of models to capture various aspects of financial systems accurately [16].\nWhile these methods have shown promise, challenges remain in validating complex financial ABMs. These include handling the high dimensionality of financial data, capturing non-linear relationships, and accounting for the often non-stationary characteristics of financial time series.\nRecent work has highlighted the importance of integrating parameter calibration, estimation, and space exploration into the empirical validation of ABMs in economics and finance [1]. This integrated approach allows for a more thorough validation process that considers multiple aspects of model performance."}, {"title": "Causal Graphs in Time Series", "content": "Before examining specific causal discovery methods, it is important to understand how causal relationships in time series data are represented graphically. While full-time causal graphs provide a comprehensive view of causal structures over an entire time series, modern causal discovery methods typically focus on inferring two more practical representations: window causal graphs and summary causal graphs. Figure 2.2 illustrates these two graphical representations.\nWindow causal graphs focus on causal relations within a specific time window. They typically include a fixed number of lags and provide a local view of the causal structure.\nSummary causal graphs condense the causal relations into a single, time-independent representation. They provide an overview of the causal structure. They retain only the relevant causal relations between variables, without taking into account the time lags."}, {"title": "Causal Discovery Methods", "content": "Causal discovery methods are of great interest in the context of ABM validation, particularly because of their ability to infer causal relations from observational time series data. These methods aim to identify the underlying causal structure of a system, which is important for understanding the mechanisms behind observed phenomena in financial markets."}, {"title": "VAR-LINGAM (Linear Non-Gaussian Acyclic Model)", "content": "Vector Autoregressive (VAR) models serve as a foundation for many causal discovery methods in time series analysis. VAR models capture linear interdependencies among multiple time series, expressing each variable as a linear function of past values of itself and past values of other variables [18]. The general form of a VAR model of order p, denoted as VAR(p), is:\n$X_t = c + A_1X_{t-1} + A_2X_{t-2} + ... + A_pX_{t-p} + \\epsilon_t$ (2.1)\nwhere $X_t$ is a k \u00d7 1 vector of variables at time t, c is a k \u00d7 1 vector of constants, A are kxk coefficient matrices, and $ \\epsilon_t$ is a k \u00d7 1 vector of error terms.\nWhile VAR models do not determine causality, they provide a basis for more advanced causal discovery techniques. VAR-LINGAM incorporates the basic VAR model with the Linear Non-Gaussian Acyclic Model (LiNGAM) to identify causal structures in non-Gaussian time series data [19], as shown in Algorithm 1. This method is particularly useful in finance data where non-Gaussian distributions are common.\nThe key insight of VAR-LINGAM is that it can determine the instantaneous causal order of variables when the error terms are non-Gaussian, which is not possible with standard VAR models. The VAR-LINGAM model can be represented as:\n$X_t = B_0X_t + \\sum_{k=1}^{\\tau} B_kX_{t-k} + \\epsilon_t$ (2.2)\nwhere $B_0$ is a strictly lower triangular matrix representing contemporaneous causal effects, and $B_k$ (k > 0) are unrestricted matrices representing lagged effects.\nRecent developments have made VAR-LINGAM more accessible through open-source Python packages, facilitating its application in various settings including time series cases, multiple-group cases, and hidden common cause scenarios [20]."}, {"title": "PCMCI (Peter and Clark algorithm with Momentary Conditional Independence)", "content": "PCMCI is a hybrid method that combines the PC algorithm with the concept of momentary conditional independence [21], as detailed steps shown in Algorithm 2. This method is designed to estimate causal graphs from large-scale time series datasets and to address the limitations of traditional methods in handling high-dimensional and nonlinear relationships."}, {"title": "Other Notable Methods", "content": "While VAR-LINGAM and PCMCI are the focus of this study, other notable causal discovery methods include:\n\u2022 Granger Causality: Despite its simplicity compared to more recent methods, Granger causality remains a fundamental tool for initial causal analysis in financial time series [22]. It's based on the principle that if a variable X Granger-causes Y, then past values of X should contain information that helps predict Y beyond the information contained in past values of Y alone.\n\u2022 tsFCI (Time Series Fast Causal Inference): An extension of the FCI algorithm adapted for time series data, tsFCI can handle hidden confounders and non-stationary time series [23]. It's particularly useful in financial contexts where unobserved factors often influence observable variables.\n\u2022 DYNOTEARS (Dynamic NOTEARS): This method extends the NOTEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning) algorithm to time series data [24]. DYNOTEARS can discover both contemporaneous and lagged causal relationships, making it suitable for analyzing complex financial dynamics.\n\u2022 CCM (Convergent Cross Mapping): Although not a traditional method for causal discovery, CCM is used to detect causality in nonlinear dynamical systems, which can be particularly relevant for financial markets [25]. It's based on the idea that if X causes Y, then historical values of Y can be used to estimate the states of X.\n\u2022 Transfer Entropy: An information-theoretic measure introduced by Schreiber (2000) that quantifies the directed flow of information between two time series. Its ability to distinguish between actual information exchange and shared information due to common history makes it particularly useful for analyzing the interdependence of multivariate time series data [26]."}, {"title": "Current Limitations", "content": "While causal discovery methods have significantly advanced ABM validation techniques, several limitations and challenges persist, corresponding to the key challenges identified in Chapter 1:"}, {"title": "Limitations in Robustness of Causal Discovery Methods", "content": "\u2022 Sensitivity to Noise and Data Variations: Existing causal discovery methods can be sensitive to noise and variations in the data. This sensitivity can lead to inconsistent results if applied to different subsets of the same dataset or datasets with slightly different characteristics. This instability reduces the reliability of ABM validation and makes it difficult to determine whether the discovered causal relationships reflect underlying economic or financial mechanisms [21].\n\u2022 Handling Complex Data: Financial time series often exhibit non-linear relationships, non-Gaussian distributions, and non-stationary behaviour. Although some methods like VAR-LINGAM and PCMCI can handle some aspects of these complexities, fully capturing the complexity of financial data remains a challenge [19, 21]."}, {"title": "Gaps in Understanding Dataset Characteristics' Impact", "content": "\u2022 Impact of Dataset Characteristics: The performance of causal discovery methods can vary significantly depending on dataset characteristics such as linearity, Gaussianity, stationarity, and the density of causal structures. However, our understanding of how these characteristics influence the performance of causal discovery methods in the context of ABM validation is limited [17]."}, {"title": "Shortcomings of Existing ABM Validation Frameworks", "content": "Existing ABM validation frameworks face several challenges in properly assessing complex models:\n\u2022 Challenges in Capturing Complex Emergent Dynamics: Many current validation approaches have difficulty measuring the fit of simulated results to real-world data. This is particularly true for models with complex and emergent behaviours, such as those found in financial systems. This limitation can lead to incomplete or inaccurate assessment of model validity [27].\n\u2022 Limited Adaptability in Causal Discovery: While existing frameworks like that of Guerini and Moneta (2017) [3] have made significant progress in integrating causal discovery methods, much work remains to be done in terms of method diversity and practical implementation. The ability to flexibly apply and compare multiple causal discovery techniques within a single framework is still not fully realised in many existing approaches. This may limit the framework's effectiveness across the wide range of data characteristics and model structures found in complex financial systems."}, {"title": "Other Challenges", "content": "\u2022 Computational Demands: Many causal discovery methods require substantial computing power, particularly when dealing with complex, non-linear connections in large datasets. This can restrict their use in intricate ABMs or extensive financial data analysis [28]. The computational burden often increases with model complexity and data volume, this poses challenges for real-time or large-scale analyses [29, 30, 31, 32].\nWhile computational efficiency is not the primary focus of our research, we consider this aspect and discuss potential optimisations and trade-offs in Chapters 4 and 5."}, {"title": "Proposed Approach", "content": "In this chapter, we present the first major contribution of this study, the Robust Cross-Validated (RCV) Causal Discovery Method. We propose a novel approach that focuses on reinforcing the robustness of existing causal discovery methods, specifically VAR-LINGAM and PCMCI, by utilizing cross-validation to yield more reliable results. It is this approach that tries to make causal structure deducing more consistent and unbiased, addressing the challenge of sensitivity to noise and data variations in current causal discovery approaches."}, {"title": "Current Limitations", "content": "Modern cause-and-effect discovery techniques face several big problems when used in complicated systems, especially in finance, weather, and brain science. This section outlines the key limitations that our proposed Robust Cross-Validated (RCV) method aims to address, as well as other important challenges in the field."}, {"title": "Key Limitations Addressed by RCV", "content": "\u2022 Sensitivity to noise: Many causal discovery algorithms, quite often those employing conditional independence tests, are very sensitive to noise in the observational data. This sensitivity causes the causal relations to be volatile and incapable of yielding reliable results.\n\u2022 Difficulty in handling non-linear relationships: Different methods deal with non-linear interactions differently; still, they typically encounter difficulties with intricate and time-varying non-linearities existing in many real systems. Runge et al. (2019) argue that many real systems are characterized by complex, non-linear dynamics that are hard to capture through classical causal discovery procedures. It should be noted that time series data is subject to very high noise levels, and shifts in regime often occur, making it very difficult to find stable causal relations even among the most clever of models.\n\u2022 Computational scalability: As the dimensionality of the system increases, many causal discovery approaches are found to be challenged by a marked deterioration of efficiency. This limitation not only affects the computational tractability of these methods but also their ability to accurately identify causal relationships in high-dimensional data. The experiments, conducted in Chapter 4, confirm that the capability of causal discovery methods weakens as the dimensionality increases with the number of variables. This limitation makes it difficult to apply these methods to extensive time series analyses, especially in areas like finance and climate science with high-dimensional data. While our approach aims to enhance accuracy, we want to strike a balance between improved accuracy and reasonable computational demands.\n\u2022 Lack of robustness across different data subsets: Causal relationships inferred from one subset of data may not hold for another, indicating a lack of robustness [33]. This lack of consistency highlights a significant problem in accurately identifying causal relations. It's particularly troublesome in rapidly evolving environments, like those seen in financial markets."}, {"title": "Other Notable Limitations", "content": "While not the primary focus of our RCV method, the following limitations are also significant in the field of causal discovery:\n\u2022 Assumption of causal sufficiency: Many approaches assume the sufficiency of the causal relation. The assumption is usually untrue regarding complex systems, where hidden variables can play crucial roles [34, 35].\n\u2022 State-dependent coupling: Complex systems often exhibit state-dependent relationships between variables. The character of these connections can change over time. In some states, variables might show positive coupling. In others, they may appear unrelated or even exhibit negative coupling, depending on the system's overall condition [25]. This dynamic behaviour creates significant challenges for conventional causal discovery methods that assume stable relationships."}, {"title": "Implications and Way Forward", "content": "These limitations underscore the need for causal discovery techniques that are both robust and adaptable. Such methods must be capable of handling the complex dynamics found in systems like finance, climate science, and neuroscience. Developing these techniques is important for enhancing our understanding of these systems and improving the validity of complex system models.\nIn the following sections, we will present our Robust Cross-Validated (RCV) method. This approach directly tackles key challenges: sensitivity to noise, handling of non-linear relationships, robustness across data subsets, and computational scalability. The theoretical foundations of our approach will be discussed in Section 3.2, followed by a detailed description of the RCV method in Section 3.3."}, {"title": "Theoretical Foundations", "content": "The Robust Cross-Validated (RCV) causal discovery method is built upon several key theoretical concepts. These concepts directly shape its design and implementation. This section will outline these fundamental ideas, and show how each element helps tackle the challenges we've identified in causal discovery for complex systems."}, {"title": "Potential of Cross-Validation for Causal Discovery", "content": "The core of our RCV method is inspired by the principles of cross-validation. This technique is widely used in machine learning and statistical modelling to evaluate model performance and generalisability [36]. It involves splitting the data into subsets, using a subset for model training and the rest for validation. This approach offers several benefits:\n\u2022 It helps assess how well a model performs on new, unseen data, which reduces the risk of overfitting [37].\n\u2022 It provides a more reliable estimate of model performance compared to using just one train-test split.\n\u2022 It allows comparison between different models or model configurations [38].\nGiven these benefits, we believe cross-validation could greatly improve causal discovery methods, especially for time series data. By applying cross-validation to causal discovery:\n\u2022 We could potentially identify causal relationships that remain consistent across different data subsets.\n\u2022 It could help reduce the risk of inferring false causal relationships due to noise or peculiarities in a specific dataset.\n\u2022 It could provide a measure of confidence in the causal structures discovered.\nHowever, using cross-validation for causal discovery in time series data poses unique challenges. Unlike predictive modelling, where the goal is to minimize prediction errors, causal discovery aims to accurately identify the underlying causal structure. This requires careful thought about how to divide time series data and measure the consistency of causal structures across different subsets."}, {"title": "Robust Statistics in Causal Inference", "content": "Robust statistics offers a framework for creating methods that are more resilient to deviations from model assumptions and the presence of outliers or noise in data. This approach is especially relevant for causal discovery in complex systems such as financial markets, where data often doesn't conform to ideal conditions [39].\nKey concepts from robust statistics that can be applied to causal discovery include:\n\u2022 Resistance to outliers: Methods that are less influenced by extreme observations or anomalies in the data. (This concept is incorporated into the RCV method through the Validate_And Adjust process, as detailed in Section 3.3.3)\n\u2022 Robust estimators: Statistical estimators that perform well under a wide range of conditions, not just under ideal circumstances.\n\u2022 Distribution-free methods: Techniques that do not rely on strong assumptions about the underlying data distribution.\nIncorporating these robust statistical principles into causal discovery algorithms can lead to more reliable and stable causal inferences, especially in the context of financial ABMs where data can be noisy and non-stationary [34]."}, {"title": "Causal Consistency and Variability in Time Series", "content": "When analysing time series data from various fields, it's important to consider two key aspects: how consistent causal relations are over time, and how they vary across different data subsets. These concepts are central to our RCV method's approach to robust causal discovery.\nCausal consistency measures how stable causal relationships remain over time or across different data regimes. In many real-world time series, consistency challenges arise due to changing conditions, cyclical patterns, and evolving system dynamics. Our RCV method addresses this through the Consistency metric (Equation 3.1 in Section 3.3.2), which measures how often a causal effect's direction is preserved across different data subsets.\nComplementary to consistency, causal variability refers to how much inferred causal structures change when the data is slightly altered or when the discovery method is changed. Low variability, or high stability, is crucial for ensuring that discovered causal relations are meaningful and not just artefacts of a specific data sample. The RCV method captures this through the Variability metric (Equation 3.2 in Section 3.3.2), which measures the stability of the estimated effect.\nTo tackle these issues effectively, our method provides measures of uncertainty or confidence in the inferred causal relationships via the consistency and variability metrics. It also allows for the incorporation of domain knowledge about the stability of certain causal mechanisms through adjustable thresholds $t_c$ and $ \\tau_v$ (Algorithm 3, Section 3.3.3).\nBy focusing on these aspects, our RCV method aims to develop more reliable causal discovery techniques for complex time series data. This leads to improved model validation and a deeper understanding of dynamic systems across various fields. The next section outlines our proposed methodology, which addresses these challenges by incorporating cross-validation and robust statistical techniques into the causal discovery process."}, {"title": "Proposed Methodology Overview", "content": "We propose a robust cross-validated approach to causal discovery based on the theoretical foundations discussed in the previous sections. This method integrates the principles of cross-validation with robust statistical techniques to address the challenges of causal discovery in complex time series data."}, {"title": "The Robust Cross-Validated Method", "content": "The Robust Cross-Validated (RCV) causal discovery method is a novel approach designed to enhance the reliability and stability of causal inference in complex systems, particularly financial Agent-Based Models (ABMs). The RCV method integrates cross-validation techniques with existing causal discovery algorithms to address the challenges of noise sensitivity, overfitting, and lack of robustness often encountered in traditional methods.\nKey components of the RCV approach include:\n\u2022 Application of a base causal discovery method to the full dataset.\n\u2022 K-fold cross-validation to assess consistency and variability of causal relationships (refer to Section 3.2.1 for the cross-validation strategy).\n\u2022 A validation and adjustment process to refine the initial causal structure.\nThe RCV method aims to produce causal structures that are more robust to noise, more consistent across different data subsets, and thus more reliable for use in ABM validation and financial system analysis."}, {"title": "Mathematical Formulation", "content": "Let D be the original dataset, and $D_1, D_2, ..., D_k$ be k subsets of D used for cross-validation. For each potential causal relationship $r_{ij}$ between variables i and j, we define two key metrics:\n$Consistency(r_{ij}) = \\frac{1}{k} \\sum_{m=1}^k 1{\\{sign(r_{ij}) = sign(r_{ij}^m)\\}\\}$ (3.1)\n$Variability(r_{ij}) = \\frac{std({r_{ij}^m}_{m=1}^k)}{|r_{ij}| + \\epsilon}$ (3.2)\nwhere:\n\u2022 $r_{ij}$ is the initial estimate of the causal effect from the full dataset\n\u2022 $r_{ij}^m$ is the estimate from the m-th fold\n\u2022 1{ } is the indicator function\n\u2022 std(.) denotes the standard deviation\n\u2022 $ \\epsilon$ is a small constant to avoid division by zero\nThe Consistency metric measures how often the direction of the causal effect is preserved across different subsets, while the Variability metric quantifies the stability of the effect size estimate.\nFor example, consider a causal relationship $r_{ij}$ between variables i and j:\n\u2022 High consistency (close to 1): The causal direction from i to j is consistent across most or all data subsets, indicating a robust relationship.\n\u2022 Low consistency (close to 0): The causal direction varies significantly across subsets, suggesting an unstable or spurious relationship.\n\u2022 Low variability: The strength of the causal effect is similar across subsets, indicating a stable relationship.\n\u2022 High variability: The strength of the effect varies greatly, suggesting sensitivity to data perturbations.\nIn our RCV method, these values are obtained through the Consistency and Variability metrics (Equations 3.1 and 3.2). The Validate_And Adjust step in Algorithm 3 uses these metrics to filter out relationships with low consistency or high variability, ensuring that only robust and stable causal relationships are retained in the final model. More specifically, causal relations with consistency below threshold $t_c$ are considered unstable and are filtered out, and those with variability above threshold $ \\tau_v$ are considered too sensitive to data perturbations and are also filtered out.\nThis approach allows the RCV method to systematically identify and retain only those causal relationships that demonstrate both directional consistency and strength stability across different subsets of the data."}, {"title": "Algorithm Description", "content": "The RCV algorithm can be described as follows:\nThe 'Validate_And_Adjust' function is a critical component that applies the consistency and variability checks to each causal relationship, retaining only those that meet the specified thresholds. It may also adjust the strength of retained relationships based on the cross-validation results.\nThe benefits and effectiveness of the RCV method, including its robustness to noise, ability to handle non-linear relationships, and improved computational scalability, are experimentally realised and quantified in Chapter 4's comprehensive evaluations."}, {"title": "Implementation Details", "content": "The RCV method has been implemented with two prominent causal discovery algorithms: VAR-LINGAM and PCMCI. This dual implementation demonstrates the flexibility of the RCV approach and allows for a comparative analysis of its effectiveness across different base methods. A comprehensive evaluation of these implementations, including a detailed comparison with their original counterparts, is presented in Chapter 4."}, {"title": "Base Causal Discovery Methods", "content": "1. VAR-LINGAM: Vector Autoregressive Linear Non-Gaussian Acyclic Model (VAR-LINGAM) is chosen for its ability to handle both contemporaneous and time-lagged causal relationships in time series data, making it particularly suitable for financial ABMs [19].\n2. PCMCI: The Peter and Clark algorithm with Momentary Conditional Independence (PCMCI) is selected for its effectiveness in dealing with high-dimensional time series data and its ability to control for autocorrelation [21].\nThe application of RCV to both methods allows us to assess its performance across different underlying assumptions and methodologies."}, {"title": "Cross-Validation Strategy", "content": "For both VAR-LINGAM and PCMCI implementations, we employ a standard k-fold cross-validation approach. Adopting this approach allows us to assess the consistency and stability of causal relationships across different subsets of the data, with the overall structure of the time series still preserved.\nThe number of folds (k) in the cross-validation process is a critical parameter. The procedure of tuning on synthetic data empirically has shown that, for time series data of length 1000, k = 7 was able to balance out the optimal results. Nevertheless, this optimal k should also take into account the specific nature of the data and the causal discovery algorithm used. The choice of k involves a trade-off between computational cost and the stability of the cross-validation results."}, {"title": "Threshold Selection", "content": "The choice of consistency ($T_c$) and variability ($ \\tau_v$) thresholds is crucial and can be determined through hyperparameter tuning:\n\u2022 For VAR-LINGAM: We found optimal values of $T_c$ = 0.4 and $ \\tau_v$ 0.4\n\u2022 For PCMCI: We found optimal values of $t_c$ = 0.7 and $ \\tau_v$ 0.4\nThe lower consistency threshold for VAR-LINGAM compared to PCMCI aligns with observations in the causal discovery literature. As noted by Assaad et al. [17"}]}