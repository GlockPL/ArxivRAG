{"title": "Balance-aware Sequence Sampling Makes Multi-modal Learning Better", "authors": ["Zhi-Hao Guan"], "abstract": "To address the modality imbalance caused by data heterogeneity, existing multi-modal learning (MML) approaches primarily focus on balancing this difference from the perspective of optimization objectives. However, almost all existing methods ignore the impact of sample sequences, i.e., an inappropriate training order tends to trigger learning bias in the model, further exacerbating modality imbalance. In this paper, we propose Balance-aware Sequence Sampling (BSS) to enhance the robustness of MML. Specifically, we first define a multi-perspective measurer to evaluate the balance degree of each sample. Via the evaluation, we employ a heuristic scheduler based on curriculum learning (CL) that incrementally provides training subsets, progressing from balanced to imbalanced samples to rebalance MML. Moreover, considering that sample balance may evolve as the model capability increases, we propose a learning-based probabilistic sampling method to dynamically update the training sequences at the epoch level, further improving MML performance. Extensive experiments on widely used datasets demonstrate the superiority of our method compared with state-of-the-art (SOTA) MML approaches.", "sections": [{"title": "Introduction", "content": "Multi-modal learning is emerging as a popular research area in artificial intelligence across various scenarios [Baltrusaitis et al., 2019; Xu et al., 2023], including autonomous vehicles [Ma et al., 2024], sentiment recognition [Singh et al., 2022], and information retrieval [Wan et al., 2024]. They have become the main impetus force for improving performance in these tasks through jointly integrating information from diverse sensors.\nAlthough modalities depict the same concept from different perspectives, each has its unique form [Zhang et al., 2024a]. For instance, text data is typically represented as word embeddings, while image data consists of pixels. This inherent heterogeneity endows each modality with distinct properties, such as convergence speed [Peng et al., 2022]. As a result, the weak modality (i.e., the slower-converging modality) tends to be underfitted during joint training, leading to modality imbalance, and may even cause the multi-modal model to fail its best uni-modal counterpart [Wang et al., 2020].\nRecently, many impressive studies have been proposed to address the modality imbalance problem from various perspectives [Jiang et al., 2024; Wei et al., 2024a; Zhang et al., 2024b; Su et al., 2024]. Considering the inherent modal differences, a straightforward idea is to manually control the optimization process between strong and weak modalities to realize rebalancing, such as learning rate adjustment [Yao and Mihalcea, 2022] and gradient modulation [Fan et al., 2023; Peng et al., 2022]. Other approaches attempt to facilitate multi-modal learning from the perspective of neural architecture. For instance, UMT [Du et al., 2021] leverages pre-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Imbalanced Multi-modal Learning", "content": "Recent research [Peng et al., 2022; Huang et al., 2022] has observed that many multi-modal models fail to outperform the best uni-modal counterpart. This phenomenon is caused by modality imbalance [Fan et al., 2024; Wei et al., 2024b], which indicates that each modality cannot be fully learned since there exists inhibition between them. Considering the existence of strong modality and weak modality, some representative [Wang et al., 2020; Fan et al., 2023] methods focus on balancing the optimization of individual modalities. In particular, OGM [Peng et al., 2022] proposes on-the-fly gradient modulation technique, which adaptively adjusts the optimization process for each modality by monitoring the discrepancy in their contributions to the learning objective. PMR [Fan et al., 2023] utilizes the prototypes to control updating direction for better uni-modal performance. Other studies [Du et al., 2021; Wu et al., 2022] attempt to boost MML performance by introducing supplementary modules. For instance, UMT [Du et al., 2021] trains the multi-modal model with knowledge distillation [Gou et al., 2021] from well-learned teacher encoders to obtain richer uni-modal representations. However, these methods increase model complexity and training costs to some extent. In this paper, from the perspective of input sample sequences, we address modality imbalance by guiding the model to progressively learn training samples in a balanced-to-imbalanced manner without additional modules."}, {"title": "2.2 Curriculum Learning", "content": "Inspired by the organized learning order of knowledge in human cognitive processes, curriculum learning (CL) [Bengio et al., 2009; Soviany et al., 2022] is proposed as a training paradigm that trains machine learning models from easier samples to hard ones. Since CL guides the model toward a better parameter space, recent studies have harnessed its power in various fields, including large language models [Wang et al., 2024], action recognition [Tong et al., 2023], and reinforcement learning [Narvekar et al., 2020]. Typically, a curriculum system consists of two main components: a difficulty measurer that evaluates the learning difficulty of samples, while a scheduler determines when and how to assign harder training subsets. In this paper, we introduce the idea of CL, but with a very different intention. Via learning from balanced samples first, the multi-modal model provides robust representations for subsequent imbalanced samples, as well as avoiding early-stage optimization dilemmas."}, {"title": "3 Methodology", "content": "In this section, we present our proposed method in detail. The overall architecture is shown in Figure 2, which consists of two main components, i.e., multi-modal training framework and Balance-aware Sequence Sampling (BSS). The former learns multi-modal representations, while the latter rebalances MML using a multi-perspective measurer and two optional schedulers (one heuristic and the other learning-based)."}, {"title": "3.1 Preliminary", "content": "Without any loss of generality, we consider a multi-modal sample with $u$ and $v$ modalities. Formally, let $D = {X,Y}$ denote the dataset, where $X = {x^{(i)}_{u}, x^{(i)}_{v}}_{i=1}^{n}$ represent $n$ training samples and $Y = {y_i | Y_i \\in {0,1}^c}_{i=1}^{n}$ is the corresponding category labels with a total of $c$ categories. MML aims to train a model to predict the category label of a given multi-modal sample.\nFor the MML prediction task, we usually employ a deep neural network to learn the representation of input sample from the original space into feature space. We use $f^{(j)}(.)$ as the feature extractor for $j$-th modality, $j \\in {u, v}$. Given sample $x^{(i)}$, the feature extraction can be expressed as:\n$e^{(i)}_{j} = f^{(j)} (x^{(i)}; \\Theta^{(i)}),$ \nwhere $e^{(i)}_{j} \\in R^d$ denotes the $d$-dimension feature embedding of $x^{(i)}$, the $\\Theta^{(j)}$ denotes the learnable parameters of $j$-th encoder. After extracting feature embeddings for all modalities, we adopt a fusion function $g(\u00b7)$ to fuse them. Then, we utilize a classifier (i.e., a fully-connected layer) to map the feature embedding into $R^c$. This procedure can be formulated as:\n$y_i = g(e^{(i)}_{u}, e^{(i)}_{v}),$\n$= softmax(W^T e_i + b).$"}, {"title": "3.2 Multi-perspective Measurer", "content": "In the following, we first introduce how to measure the balance degree of a multi-modal sample from the perspectives of correlation and information criteria, and then define the balance score for each sample.\nCorrelation Criterion: Different modalities exhibit inherent cross-modal correlation, as they describe the same concept via diverse representations, capturing complementary information. Although cross-modal correlation can be measured by various aspects, we here only focus on feature similarity and prediction similarity. According to our previous findings in Figure 1 (c), the latter outperforms the former. The reason is that prediction similarity is more effective in measuring modality balance, while MML benefits from the supervision of uni-modal predictions [Zou et al., 2023]. Formally, given a multi-modal sample $x_i = {x^{(i)}_{u},x^{(i)}_{v}}$, the prediction similarity is defined as:\n$sim(x^{(i)}_{u}, x^{(i)}_{v}) = \\frac{y^{(i)}_{u} \u00b7 y^{(i)}_{v}}{||y^{(i)}_{u}||_2 ||y^{(i)}_{v}||_2}$ \nHere, $||.||_2$ denotes $L2$ norm of the uni-modal predictions.\nInformation Criterion: While prediction similarity reflects the balance between modalities, it does not verify whether the predictions of each modality are correct. In other words, high prediction similarity may still occur even when all modalities"}, {"title": "3.3 Training Scheduler", "content": "Merely evaluating the balance score for each sample is insufficient to establish an appropriate training period, as it is also essential to control the presentation of samples from balanced to imbalanced, i.e., the sample sequence for each training epoch. Similar to human education, if teachers impart knowledge from easy to hard within a short span of time, students may become overwhelmed and fail to learn effectively. On the other hand, if teachers present knowledge too slowly, students may lose motivation. In the following, we introduce both a heuristic and a learning-based scheduler to construct sample sequences.\nHeuristic Scheduler: Inspired by curriculum learning (CL), we rank the training samples from balanced to imbalanced according to the defined balance score, and then employ a pace function [Hacohen and Weinshall, 2019] to control the growth speed of samples. In practice, there exist various pacing functions like baby step [Bengio et al., 2009], liner function [Wang et al., 2022], and root function [Platanios et al., 2019]. However, the effect of existing pacing functions on modality imbalance is not the focus of our work, and comparisons can be found in the supplementary materials. Here, we adopt a widely used root function to achieve this:\n$\\Aroot(t) = min (1, \\frac{1-\\lambda_0}{T_{grow}} \u00b7 t + \\lambda_0)$\n$\\Aroot(t)$ aims to map training epoch number $t$ to a scalar $\\lambda \\in (0,1]$, which means $\\lambda$ proportion of the most balanced samples are available at the $t$-th epoch. This function starts at $\\Aroot (0) > 0$ and ends at $\\Aroot(T_{grow}) = 1$. $\\lambda_0 \\in (0,1]$ is the initial proportion of the training samples, and $T_{grow}$ represents the training epoch when this function reaches 1 for the first time."}, {"title": "3.4 Model Inference", "content": "After training the modality-specific encoders and individual classifiers, the learned model can be applied to make prediction during inference stage. Following various late fusion strategies [Zhang et al., 2024b; Fan et al., 2024], we employ a simple weighted combination of logits output from each modality and their fusion:\n$\\Ztotal = Zmulti + \\frac{\\Sigma_{j=1}^{m} Z_{uni}^{(j)}}{m}$\nBased on Equation 12, the predicted category $\u0177$ for a given unseen multi-modal sample can be represented as:\n$\\hat{y} = argmax_i \\frac{e^{z_{total}}}{\\Sigma_{i=1}^{c} e^{z_{total}}}$\nwhere $c$ represents the number of category labels."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets: We validate our proposed method on six widely used datasets, including CREMA-D [Cao et al., 2014], Kinetics-Sounds [Arandjelovic and Zisserman, 2017], VGGSound [Chen et al., 2020], Twitter2015 [Yu and Jiang, 2019], Sarcasm [Cai et al., 2019], and NVGesture [Molchanov et al., 2016]. Among them, CREMA-D, Kinetics-Sounds, and VGGSound contain both audio and video modalities. CREMA-D includes 7,442 video clips"}, {"title": "4.2 Comparsion with SOTA MML Baselines", "content": "We conduct comprehensive comparisons to assess the superiority of our proposed method in addressing the imbalanced multi-modal learning problem. The classification performance on all datasets is reported in Table 1 and Table 2, where \"BSS-H\u201d and \u201cBSS-L\u201d denote the proposed method with heuristic scheduler and learning-based scheduler, respectively. Please note that \u201c-\" in Table 1 denotes that the cor-\""}, {"title": "4.3 Ablation Study", "content": "We conduct more ablation studies to verify the effectiveness of using different criteria for sample evaluation, i.e., uni-modal prediction similarity (PreSim) and training loss (Loss). Table 3 records the results under the learning-based setting, which reveal that: (1). Vanilla joint training may exacerbate modality imbalance. For instance, when the video modality converges, the audio modality remains insufficiently trained, leading to a significant gap between the two modalities (4.66%/6.41% in ACC/MAP) compared with other variants; (2). Both uni-modal prediction similarity and training loss, when employed as balance scores for sequence sampling, can boost classification performance. This is predictable, as prioritizing balanced samples helps reduce the gap between modalities, facilitating both uni-modal and multi-modal learning processes; (3). By integrating both criteria, BSS-L exhibits the best performance, demonstrating its effectiveness in modality imbalance scenarios."}, {"title": "4.4 Further Analysis", "content": "Sensitivity to Hyper-Parameters: In calibrating our proposed method, we identify two hyper-parameters: $\\alpha$ in Equation 5 and $\\beta$ in Equation 9, determining the strength for balancing classification loss and regulating the balance score, respectively. Figure 3 (a) depicts the performance of different $\\alpha$. With the increase of $\\alpha$, the accuracy of our method first increases and then decreases. This shows that proper uni-modal learning has a promoting effect, but over-considering uni-modal optimization may hinder multi-modal interactions. From Figure 3 (b), we can find that the performance is marginally affected by $\\beta$, suggesting the insensitivity of our method to hyper-parameters. Despite some fluctuations, our method still demonstrates excellent effectiveness, i.e., being consistently better than vanilla multi-modal learning.\nRobustness of the Pre-trained Model: We further explore the robustness of the large multi-modal pre-trained model on text-image datasets. We replace each modality encoder with the corresponding encoder pre-trained by CLIP [Radford et al., 2021] and fine-tune the model. The results are shown in Figure 3 (c) and (d), where \u201cCLIP+MLA\u201d and \u201cCLIP+Ours\u201d present that we apply the MLA's and our approach, respectively. From Figure 3 (c) and (d), we can draw the following observations: (1). Both CLIP+MLA and CLIP+Ours can outperform CLIP in all cases; (2). Via sequence sampling, the performance of our method is better than that of MLA.\nCase Study: We examine whether our method can effectively distinguish the balanced and imbalanced samples from a randomly ordered sequence. From the representative samples in Figure 4, we observe that balanced samples exhibit strong semantic consistency between modalities, reflected by high balance scores, whereas imbalanced samples typically display weak semantic connections or irrelevant information."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel multi-modal learning method, called balance-aware sequence sampling (BSS). By defining a multi-perspective measurer, we evaluate the balance of each training sample. Via the evaluation, we design a heuristic and a learning-based scheduler to construct sample sequences for the model at different training stages. Thus, BSS alleviates modality imbalance through sequence sampling in a balanced-to-imbalanced learning strategy, further boosting MML performance. Moreover, BSS can be integrated as a model-independent plugin into most existing MML approaches. Extensive experiments on widely used datasets validate the superiority of BSS over SOTA baselines."}]}