{"title": "Human-centered explanation does not fit all: The Interplay of sociotechnical, cognitive, and individual factors in the effect of Al explanations in algorithmic decision-making", "authors": ["Yongsu Ahn", "Yu-Ru Lin", "Malihe Alikhani", "Eunjeong Cheon"], "abstract": "Recent XAI studies have investigated what constitutes a good explanation in Al-assisted decision-making. Despite the widely accepted human-friendly properties of explanations, such as contrastive and selective, existing studies have yielded inconsistent findings. To address these gaps, our study focuses on the cognitive dimensions of explanation evaluation, by evaluating six explanations with different contrastive strategies and information selectivity and scrutinizing factors behind their valuation process. Our analysis results find that contrastive explanations are not the most preferable or understandable in general; Rather, different contrastive and selective explanations were appreciated to a different extent based on who they are, when, how, and what to explain - with different level of cognitive load and engagement and sociotechnical contexts. Given these findings, we call for a nuanced view of explanation strategies, with implications for designing AI interfaces to accommodate individual and contextual differences in AI-assisted decision-making.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has increasingly become integrated into everyday life, influencing decisions from personalized purchase recommendations to media content suggestions. However, concerns about Al's potential to produce unfair or harmful outcomes have highlighted the need for greater transparency and explainability in AI-driven decision-making [41]. In response, the field of eXplainable AI (XAI) has developed various explanation strategies aimed at making Al systems more transparent; However, a critical question still remains: what constitutes a good explanation in the context of Al decision-making?\nDrawing from cognitive and social sciences, Miller [47] introduced human-centered explanation properties-such as selective, contrastive, and conversational explanations-that have since gained considerable attention. Of particular interest are contrastive explanations, which highlight the differences between a decision and its alternatives, and selective explanations, which focus on the most relevant information. These strategies have been widely studied, from algorithmic methods to user-facing empirical studies.\nHowever, despite their theoretical appeal, findings on these explanation types have been inconsistent. For instance, while contrastive explanations have been shown to improve decision quality in certain tasks like emotion recognition [81], they have also failed to align AI trust with human perception in other contexts [75]. Similarly, while detailed explanations can enhance users' understanding of Al processes [10, 36], they also risk fostering over-reliance on AI decisions [7] and reducing trust in AI systems [35]. These mixed findings raise the question: are contrastive and selective explanations universally beneficial, or do their advantages depend on specific contexts?\nWe argue that the inconsistencies in previous studies stem from insufficient attention to the cognitive and contextual factors that influence how users evaluate explanations. Research in cognitive"}, {"title": "2 Related Work", "content": "Evaluating post-hoc explanations in the form of text has gained much attention as it provides rationales on AI-based decisions and impacts users' perceived trust and understandability [14, 41, 73]. Such explanations incorporating a variety of strategies and logics vary by techniques [1, 75] including feature importance or nearest neighbors or logics/strategies such as contrastive [25, 47, 53], counterfactual [22, 55, 74, 78] explanations in social and cognitive science, and case-based explanations from expert system studies [12, 20, 50, 76].\nHowever, recent XAI research has produced contradictory findings regarding the effect of explanation strategies. For example, providing such explanations in AI-assisted decision were found to help justify the decision and increase users' trust when compared with no explanation provided in some contexts such as medical chatbot [69] or self-driving context [59]. On the other hand, they tend to be distracting or lead to either overreliance [16, 71] or cognitive overload [51, 57] for lay users without a careful design of how explanations are presented [80]. The impact of different explanation strategies in AI-assisted decision-making is also diverging. For example, complete explanations with a greater information complexity, for example, were proved to gain trust better in medical diagnosis [35, 36, 39] while they led to over-reliance on AI in medical diagnosis in [7]. Contrastive explanations known as intuitive and human-friendly were helpful in improving decision quality with semantic evidence in emotion recognition task [81], while it"}, {"title": "2.1 Valuation of Explanation Strategies in AI-assisted Decision-making", "content": "Evaluating post-hoc explanations in the form of text has gained much attention as it provides rationales on AI-based decisions and impacts users' perceived trust and understandability [14, 41, 73]. Such explanations incorporating a variety of strategies and logics vary by techniques [1, 75] including feature importance or nearest neighbors or logics/strategies such as contrastive [25, 47, 53], counterfactual [22, 55, 74, 78] explanations in social and cognitive science, and case-based explanations from expert system studies [12, 20, 50, 76].\nHowever, recent XAI research has produced contradictory findings regarding the effect of explanation strategies. For example, providing such explanations in AI-assisted decision were found to help justify the decision and increase users' trust when compared with no explanation provided in some contexts such as medical chatbot [69] or self-driving context [59]. On the other hand, they tend to be distracting or lead to either overreliance [16, 71] or cognitive overload [51, 57] for lay users without a careful design of how explanations are presented [80]. The impact of different explanation strategies in AI-assisted decision-making is also diverging. For example, complete explanations with a greater information complexity, for example, were proved to gain trust better in medical diagnosis [35, 36, 39] while they led to over-reliance on AI in medical diagnosis in [7]. Contrastive explanations known as intuitive and human-friendly were helpful in improving decision quality with semantic evidence in emotion recognition task [81], while it"}, {"title": "2.2 Individual and Cognitive Dimension of Explanatory Process", "content": "When making sense of a social situation such as interpersonal communication or decision-making processes, individuals go through certain cognitive processes to analyze, interpret, and remember information [54]. Previous studies found that these process are influenced by individuals' own cognitive tendency or given contexts. For example, people tend to have their own decision-making style [6, 21, 60] whether they rely on hunches or a thorough search for information. According to the Motivation-Opportunity-Ability (MOA) model [44], individuals are either empowered or hindered in exhibiting behavioral changes or attributing decisions when making sense of their success and failure in their careers or education [77]. These cognitive processes can further influence the degree to which individuals seek more information to reason about situations, manifest in either spontaneous or deliberate modes within the dual processing theory [46, 54, 56].\nA number of studies [16, 23, 52] have found that all these cognitive traits are highly dependent on their demographics. For example, older adults rely more on emotions and experience rather than being rational due to aging cognitive ability [42]. The level of education also influences the cognitive load, intelligence, thinking, and working memory [23]. Rational thinkers tend to actively seek explanations when finding the best recommendations than intuitive thinkers.\nDespite these findings, there is limited understanding of the complex cognitive dimensions that affect individuals' willingness to seek explanations and their valuation of specific explanation strategies in AI-assisted decision-making. Our study explores these connections, highlighting the need for careful designed explanations that consider these cognitive processes."}, {"title": "2.3 Evaluating Explanations in Various AI-assisted Decision Contexts", "content": "The effectiveness of various explanation types in enhancing trust and understanding has been studied across diverse AI-assisted decision contexts, including human-robot interaction [3, 24, 62, 64, 65], self-driving technologies [10, 34, 59], and medical diagnosis [31, 39, 61].\nFor instance, in medical context, providing explanations helped improve health awareness, facilitate learning, and aid decision-making by offering patients new information about their symptoms [69] or help laypeople understand complex medical concepts during cancer diagnosis [39]. In self-driving contexts, explanations provided in a timely manner during sequential driving scenes have been found to improve understanding [10, 34, 59].\nDespite all these studies highlighting the impact of various explanation styles within a certain context, it remains uncertain how different explanation types affect the levels of trust, understandability, and other aspects of user perceived values across multiple contexts. While previous studies [17, 37, 67] have conceptually examined that AI transparency is entangled with sociotechnical contexts, our research empirically demonstrates that the effectiveness of explanations varies based on the application context, highlighting the need for context-aware design in explainable Al systems."}, {"title": "3 Study Purpose and Overview", "content": "Our study aims to examine two key aspects of explanation strategies in Al-assisted decision-making: (1) how contrastive and selective explanations-commonly accepted as properties of good explanations-are perceived by laypeople, and (2) which factors influence the evaluation of these explanation strategies in AI-assisted decisions. We explore various AI-assisted decision-making scenarios, where human subjects receive decisions related to their personal circumstances in different sociotechnical contexts. These decisions are accompanied by textual explanations that provide the rationale behind the outcomes.\nTo ground our study in concrete examples, we developed a range of written scenarios across various decision-making contexts, such as medical diagnoses and loan approvals. These scenarios demonstrate how explanations are provided and how participants might evaluate them in different sociotechnical contexts. Below, we present one of the six selected scenarios.\nLoan approval case as a motivating scenario: On a Tuesday morning, Jane, a sixty-seven-year-old retiree, applied for a loan to renovate her home. A few hours later, she received a notification from the bank stating that her loan request had been denied. The AI-based system explained the decision was based on her credit score and financial history. This result surprised Jane, as she had always been responsible with her finances. She wanted to understand why the loan was denied-whether it was her age, retirement status, or something else in her profile.\nJane found an AI-based chatbot on the bank's website offering explanations for loan decisions. She asked the chatbot why her loan was denied, and it provided a detailed response outlining the factors behind the decision."}, {"title": "3.1 Evaluation of Contrastive and Selective Explanations", "content": "In such scenarios, Al systems must not only provide explanations to meet explainability requirements but also tailor them to users' varying characteristics, levels of understanding, and expectations within AI-assisted decision-making contexts.\nOur selection of explanation types, informed by a thorough literature review, enables us to examine different forms of contrastive"}, {"title": "3.2 Understanding of Human Valuation Process of Explanations", "content": "Given an array of contrastive and selective explanations, which explanation strategies will people find preferable and intuitive? While theoretical frameworks advocate for these types of explanations as generally intuitive and human-friendly, we posit that their value is highly contextual depending on factors such as when, how, and for whom the explanations are provided.\nPrior research in cognitive and social science has presented numerous findings on how individuals engage with explanatory processes or decision-making contexts differently based on their personal traits and situational contexts. Drawing from the literature review (see details in Section 3.4), we present the framework for conceptualizing human valuation of explanation. As presented in Fig. 3A, we synthesize them as an interplay of various factors, which can fall into one of the following dimensions: 1) individual-dependent factors: Individuals are known to have inherent demographic traits and decision-making style that shape their preferences and ability based on their inherent traits such as demographics and decision-making style. 2) context-dependent factors: When given a decision in a sociotechnical context (e.g., denied a loan), individuals may perceive the context (e.g., does the decision have a significant consequences?), in turn exhibiting a different level of cognitive engagement depending on whether they are motivated, perceive a certain level of opportunities or ability to seek explanations. When presented with an explanation from an Al system, individuals may experience different levels of cognitive load, impacting their interpretation of the provided explanation. Depending on these factors, users will evaluate explanations regarding different explanatory values.\nIn our study, we scrutinize this framework by investigating RQ3 and RQ4 as outlined in Section 3.3, aiming to identify key aspects of the human valuation process of explanations."}, {"title": "3.3 Research Questions", "content": "Based on two facets of our study purposes as described above, we distill them into four key research questions listed below:\nEvaluating laypeople's preferences for contrastive and selective explanations: The first part of our study focuses on evaluating preferences for contrastive and selective explanations across various Al-assisted decision-making scenarios. We use both quantitative and qualitative approaches: a survey experiment to measure preferences and an analysis of responses to open-ended questions to explore the reasons behind these preferences (i.e., what features of the explanations influenced their choices) (see detailed methods in Sections 4.1 and 4.4)). Through this, we aim to critically examine the commonly held assumption that contrastive and selective explanations are inherently valuable in all contexts.\n\u2022 RQ1. How are contrastive and selective explanations preferred in general and within specific sociotechnical contexts?\n\u2022 RQ2. Which aspects and properties of explanations are linked to the distinct valuation of those explanation strategies?\nExploring factors influencing preferences for contrastive and selective explanations: In addition to evaluating preferences, we aim to gain deeper insights into the factors that shape how people value different explanations. In our experiment, we translate these factors into specific survey items and analyze their interactions, examining both individual variables and their combined effects. This design seeks to explain the factors influencing preferences for contrastive and selective explanations.\n\u2022 RQ3. How do individual, contextual, and cognitive factors interact in the valuation process of explanations?\n\u2022 RQ4. How do these factors collectively influence the preference on valuation of contrastive and selective explanations?"}, {"title": "3.4 Theoretical Foundation and Rationale behind Variable Selection", "content": "To design the framework, we have reviewed literature throughout a wide range of disciplines. Studies in social and cognitive science have been to examine theories and findings about how people attribute decisions and process information based on a given context, leading to shaping individual, cognitive, and contextual factors. An extensive review from and outside of XAI literature have led us to examine explanatory values (i.e., criteria for evaluating explanations in Al contexts).\nFrom various disciplines outside of XAI literature. Literature from social and cognitive science have established theories and findings on individuals' cognitive capability and status in the decision-making and explanatory process. Our objective is to integrate these insights into AI decision contexts, to not only theoretically support our framework but also make connections between human and AI studies, from human-human and human-Al communication.\n\u2022 Decision-making styles: Research in Psychology and Business has studied how individuals have different levels of cognitive tendency based on inner traits such as gender, age, or education. Especially when processing decisions and relevant information, individuals tend to exhibit different decision-making styles (See detailed literature review in Section 2.2). We introduce five different styles in a scale called General Decision Making Style (GDMS) [63], including rational, avoidant, intuitive, dependent, and swift (details in Table 3).\n\u2022 Cognitive engagement: In the literature from social science such as Education and Consumer Behavior, scholars have attempted to establish theories, often referred to as social attribution, theorizing individuals' internal cognitive states that drive behavioral changes (See detailed literature review in Section 2.2) triggered by events such as a"}, {"title": "4 Method", "content": "We designed a survey experiment to investigate the process of evaluating explanation and answer the research questions. The survey was designed as a between-subject experiment, where participants were randomly assigned to one of the six decision scenarios.\nOverall, the survey was designed to emulate the process of humans' valuation of explanation strategies, consisting of multiple stages (a-f) as illustrated in Fig. 3B (see the details for the survey material in the Appendix (Section C)): a) Participants were asked questions about their individual characteristics including demographics and decision-making style; b) A randomly assigned scenario was presented to engage participants in the AI-assisted decision-making context. To devise each scenario, we consulted with domain experts to determine the details of the outcomes and features, and various user cases that serve as counterparts of the focal data subject regarding contrastive and analogous comparisons; c) Questions regarding perceived contextual properties (i.e., high-stakes, professional, timely) and cognitive engagement (i.e., motivation, opportunity, ability) were prompted to examine context-dependent factors; d) Six explanation strategies were presented; e,f) users were asked to rank explanations to rate their cognitive load as well as explanatory values. Finally, we collected participants' opinions and reasoning for the explanation variants in free-form text responses."}, {"title": "4.1 Study Design", "content": "We designed a survey experiment to investigate the process of evaluating explanation and answer the research questions. The survey was designed as a between-subject experiment, where participants were randomly assigned to one of the six decision scenarios.\nOverall, the survey was designed to emulate the process of humans' valuation of explanation strategies (Fig. 3A), consisting of multiple stages (a-f) as illustrated in Fig. 3B (see the details for the survey material in the Appendix (Section C)): a) Participants were asked questions about their individual characteristics including demographics and decision-making style; b) A randomly assigned scenario was presented to engage participants in the AI-assisted decision-making context. To devise each scenario, we consulted with domain experts to determine the details of the outcomes and features, and various user cases that serve as counterparts of the focal data subject regarding contrastive and analogous comparisons; c) Questions regarding perceived contextual properties (i.e., high-stakes, professional, timely) and cognitive engagement (i.e., motivation, opportunity, ability) were prompted to examine context-dependent factors; d) Six explanation strategies were presented; e,f) users were asked to rank explanations to rate their cognitive load as well as explanatory values. Finally, we collected participants' opinions and reasoning for the explanation variants in free-form text responses."}, {"title": "4.2 Study Implementation", "content": "Participants. We recruited participants from Prolific crowdsourcing platform. Workers living in the US (age 18+) are fluent in English were eligible to participate in this study to make sure their ability to reason about the given scenario in English. One of the three rounds aimed to recruit senior people (age 55 and older), as they are typically underrepresented on the crowdsourcing platform. A total of 839 participants took part in the study. We excluded respondents who left their responses as default or who did not respond to all survey questions. This yielded a final sample of 698 participants for our data analysis (390 females, age: 222 participants were between the ages of 18 and 24 and 62 were 65 or older; 1 participant preferred not to state age, and 3 participants preferred not to say their gender).\nSample size determination. We used the Mann-Whitney test to determine the sample size. Based on the power analysis with a significance level of 0.05 and a medium effect size, we determined that at least 106 samples per group were needed, resulting in a suggested minimum sample size of 636 for six survey variants, in order to ensure that the effect of the explanations could be tested.\nProcedure. The survey experiment was conducted on Prolific for three rounds in 2022, on January 29, March 29, and April 25. Participants receive a small amount of compensation with the base rate"}, {"title": "4.3 Statistical Analyses", "content": "We conducted multiple types of statistical analyses to examine the research questions listed in Section 3.3. As a pre-processing step, we converted all ranking responses to a relative rating variable, such that higher scores indicate positive cognitive loads and explanatory values.\nFor RQ3, we employed the Mann-Whitney U test to determine if significant differences existed between two distinct sets of scenarios (e.g., high-stakes vs. low-stakes contexts). The Wilcoxon signed-rank test was additionally utilized for identifying significant differences between pairs of styles, acknowledging that the relative ratings across these styles are interdependent. In our pairwise testing, we applied the Bonferroni correction to adjust for multiple comparisons. Given that our analysis involved both 6 scenarios and 6 styles, the significance level was accordingly adjusted to 0.05/6, which is approximately 0.0083.\nTo examine RQ4 in scrutinizing the interplay of individual-, cognitive-, and explanation-dependent factors, we used Structural Equation Modeling (SEM) to take into account all direct and indirect relationships between variables included in our study framework (Fig. 3A). For these models per explanation type, we converted participants' ratings of explanation styles into a binary variable indicating whether a particular style was absolutely favorable (i.e., ranked as top or second one) or not."}, {"title": "4.4 Analysis for Open-ended Questions", "content": "In the survey, we further collected open-ended responses from participants to ask for their detailed rationales for their overall preferences with two questions: (1) Detailed rationales on their valuation process of ranking the explanations with the question, \u201cplease briefly describe the rationale on ranking the explanations,\u201d and (2) general perceptions on values of explanations and Al systems, \u201cwhat is the most important aspect of explanations for you? Do you have any additional comments about the AI system?\u201d\nTo extract and summarize the characteristics of their evaluation process from texts, we took a two-step approach. First, we used qualitative coding to identify recurring themes in participants'"}, {"title": "5 Results", "content": "First, we examine how explanation strategies were perceived differently in terms of perceived values of explanations overall and within each decision context, which are summarized in Fig. 4 and Fig. 5 respectively.\nOur analysis found Complete explanations were evaluated as higher in all explanatory values than any other explanation types. Conversely, Case-based (hetero) was considered the least favorable in all valuation aspects. Counterfactual and Case-based (homo) were ranked second-to-worst in terms of sufficient and understandable, and trustworthiness and usefulness respectively."}, {"title": "5.1 How do participants' perceived value of explanations differ, overall and within sociotechnical contexts?", "content": "First, we examine how explanation strategies were perceived differently in terms of perceived values of explanations overall and within each decision context, which are summarized in Fig. 4 and Fig. 5 respectively.\nOur analysis found Complete explanations were evaluated as higher in all explanatory values than any other explanation types. Conversely, Case-based (hetero) was considered the least favorable in all valuation aspects. Counterfactual and Case-based (homo) were ranked second-to-worst in terms of sufficient and understandable, and trustworthiness and usefulness respectively."}, {"title": "5.2 Which aspects and properties of explanations are linked to the distinct valuation of those explanation strategies?", "content": "Based on the analysis of open-ended responses, we examine participants' detailed rationale on what aspects and properties of explanations made them prefer certain explanations over the others. We find evidence on all facets of explanation strategies listed in Table 2 including contrastive strategies (how and what/who to compare) and information selectivity (information complexity and alignment) as well as general expectations over Al systems' explainability as follows."}, {"title": "5.2.1 Participants appreciate different types of explanatory properties, mostly pertaining to information complexity.", "content": "First, most of the participants (524/698, 75.1%) tended to evaluate explanation strategies based on a variety of explanatory properties to further elaborate on their preferences in the quantitative results. The five most frequent properties across all explanation styles were easy (126 times), clear (66), relevant (38), specific (36), and detailed (34)."}, {"title": "5.2.2 Participants favor explanations they can readily accept based on their prior knowledge or beliefs.", "content": "In the 135 responses (19.3%), participants expressed preferences for explanation styles when the explanation mentioned one or more feature(s) they perceived as critical or unnecessary in the decision-making.\nThese feature-oriented valuation, spanning 34 types of features for 205 times, varied across decision contexts but appeared mainly in less knowledge-intensive decision-making contexts. For instance, in the loan approval context, participants often confidently referred to employment (Loan-: 17) or annual income (Loan-: 17) as critical factors of loan approval based on their beliefs or prior experiences as commented, \u201cincome matters.\u201d or \u201cnot being employed is the most critical problem. There is no other reason that matters.\u201d. In movie recommendations, the browsing history was often considered as a critical factor (Recom-: 30, Recom+: 23) because this feature \u201cmade the most sense\u201d and they feel \u201cthis will give me the best choice for movies.\u201d The movie genre (Recom-: 9, Recom+: 7) was also often perceived as an understandable and important feature to the given recommendations, \u201cthe relation to the genre and my previous recommendations make it the best and easiest to understand.\u201d. On the other hand, the least frequent contexts were medical diagnosis (Medi-: 10, Medi+: 12) with mentions of some features such as blood pressure (Medi-: 2, Medi+: 3) or cholesterol (Medi-: 2, Medi+: 1). This suggests that, while some participants prefer explanations that seem more plausible to them, possibly due to their prior beliefs and knowledge, this preference may vary depending on the level of professional knowledge required for the decision context.\nOn the other hand, some features, especially those related to demographics, also provoked negative preferences over explanations. A number of participants (Recom-: 15, Recom+: 12, Loan-: 5) considered all demographic-related features (such as gender, marriage, or age) as not useful, necessary, or relevant to the Al's decision. Some of them raised their concerns about privacy or profiling issues of collecting and processing personal data."}, {"title": "5.2.3 Rather than simply contrastive, human-friendly explanations depend on whether, what, and how to compare.", "content": "77 participants (11.0%) showed diverse user preferences over different types of comparisons, based on factors like what or whom is being compared against their own status. In Table 2, explanations are presented based on their average ranking over a group of participants and highlighting liked (red) or disliked (blue) strategies.\nWe found that some participants did not want any types of comparisons and preferred Complete over all others (+: Complete; -: all others). They preferred explanations to \u201cfocus on themselves\u201d rather than others because \u201ceveryone has different personal context\u201d but Al may not consider all contexts when making comparisons. Another group of participants did not like being compared to others but rather to their own previous or hypothetical state (+: Counterfactual, Contrastive (t); -: all others). On the other hand, others preferred certain comparative explanations. For example, a group of participants favored comparison with typical or similar others (+: Case-based (homo), Case-based (hetero); -: all others) or those with opposite outcome (+: Contrastive (o), Contrastive (t); -: all others)."}, {"title": "5.2.4 Participants have different expectations over Al explainability.", "content": "Participants' expectations about the role of explanations in AI systems varied significantly across the 25 responses (3.5%). First, some expressed their views on the objectives of XAI systems. Ten participants, for instance, anticipated that AI systems would provide professional information or be used for complex tasks. Notably, eight participants preferred explanations that included more statistics and numbers, citing that such information \u201chelps [in] visualizing the status.\u201d Two participants mentioned that Al systems should present professional information that could provide more learning opportunities, as opposed to merely presenting well-known terms and factors such as BMI or blood pressure.\nSecond, four participants argued that AI systems should do less, suggesting that the use of XAI should be limited or restricted to human experts. They voiced concerns such as, \u201cI [would] rather hear this from a human [than AI]; it is useful when used [alongside] doctors\u201d and \u201cAI is just for diagnosing; to tell me whether I should go to a doctor.\u201d"}, {"title": "5.3 How do individual, contextual, and cognitive factors interact in the valuation process of explanations?", "content": "In the second part of our study, we conduct an in-depth analysis of how the distinct values of various explanation strategies, as discussed above, are shaped by a range of factors and their complex interactions. We begin by examining the relationships between sociotechnical and cognitive factors and how they interact with the evaluation of explanations.\nSociotechnical contexts are perceived as having distinct contextual properties. First, we confirmed that the six scenarios in our study exhibited different characteristics along the three dimensions, including high-stakes, professional, and timely based on participants' ratings (Fig.9). For instance, Loan-, Medi-, and Driv- were rated as more high-stakes and timely than Recom+ and Recom- (Mann-Whitney U test with p < 0.001 on each pair of the former and latter groups). Similarly, Medi- and Medi+ were rated as more professional contexts than Recom+ and Recom- (Mann-Whitney U test with p < 0.001 on each pair of the former and latter groups).\nParticipants' cognitive engagement tend to differ across sociotechnical contexts. We also find that, depending on decision contexts with different contextual properties, participants tend to exhibit different level of having motivations, opportunities, and abilities. For instance, participants in more high-stakes scenarios, such as Loan- and Medi-, showed higher motivation to investigate the information due to perceived benefits or threats from the consequences of the decision, compared to those in less high-stakes scenarios, such as Recom+ and Recom- (Mann-Whitney U test with p < 0.001 on every pair of the former and latter groups). In the"}, {"title": "5.4 How do these factors collectively influence the preference on valuation of contrastive and selective explanations?", "content": "Considering all factors including individual-, context-, and explanation-dependent factors, we investigate the interplay of factors in affecting the valuation of explanations. As described in Section 4.3, structural equation models (SEM) are used to test the effects of various factors in explaining whether a participant prefers a particular explanation or not for five explanatory values. Fig. 7 provides a summary of the significant factors identified by the path analysis for each explanatory strategy. For predicting five explanatory values in each of the explanation styles (e.g., Case-based (hetero)), the estimated standardized effects of the significant factors are shown with a 95% confidence interval.\nOverview. Our path analysis reveals a combination of direct and indirect effects of individual-, context-, and explanation-dependent factors on the valuation process. Across the results of path analysis"}, {"title": "6 Discussion", "content": "Recent research in explainable AI (XAI) has focused on identifying the key features of user-friendly explanations and exploring cognitive frameworks that enhance users' understanding of AI-assisted"}, {"title": "6.1 Effectiveness of Contrastive and Selective explanations", "content": "As discussed in Section 1, the properties of human-friendly explanations-especially contrastive and selective explanations-have garnered increasing attention in recent XAI research and are often used as theoretical foundations [4, 38]. However, our study presents empirical evidence that challenges two key properties in AI-assisted decision-making contexts:\n(1) Contrastive explanations: In some contexts, contrastive explanations were not perceived as the most understandable or preferred. While earlier studies highlight their intuitive and human-friendly nature, our findings suggest their effectiveness is highly context-dependent (e.g., contrastive explanations were less effective in time-sensitive situations or when comparing outcomes like winning vs. losing). Participant preferences also varied based on the type of contrast (e.g., comparing with others' outcomes or with their own previous state). For example, when contrasting with another individual's outcome (Contrastive (o)), participants found explanations actionable, whereas comparing their current state to a previous one (Contrastive (t)) was perceived as more relevant to personal decision-making. However, as discussed in Section 5.2.3, some participants found Contrastive (o) explanations less relevant, as they often focused on their own circumstances.\n(2) Selective explanations: Our analysis of open-ended responses revealed that selective explanations should not be assumed universally effective. Participants preferred explanations that highlighted specific features (e.g., annual income, browsing history), especially when those features aligned with their expectations. However, this preference was more pronounced in simpler decision contexts, such as loan approvals or movie recommendations, where professional knowledge is not required. In more complex contexts, like medical decision-making, participants were less likely to express preferences related to selective information, as noted in Section 5.2.2.\nOverall, our findings challenge the assumption that contrastive and selective explanations are universally intuitive or advantageous. These strategies should be carefully tailored to individual needs and contextual factors when incorporated into Al systems."}, {"title": "6.2 Need to Balance Individual Engagement and Cognitive Load", "content": "Recent XAI literature has introduced cognitive frameworks, such as cognitive forcing [8] and evaluative AI [49], aimed at enhancing user engagement and mitigating the risk of over-reliance. These studies suggest using cognitive interfaces that encourage slow, deliberate thinking [33] or offer multiple explanations with contrasting outcomes, allowing users to generate their own hypotheses rather than relying solely on system-driven recommendations. While these approaches can improve engagement, they also risk increasing cognitive load, particularly for users inclined toward intuitive thinking. It is unrealistic to expect participants who typically overlook system-generated explanations to actively engage with more complex explainability tasks. Our findings suggest that these approaches should be tailored to individual levels of engagement rather than implemented as one-size-fits-all solutions."}, {"title": "6.3 The Role of Content and Tone in Shaping User Perceptions of AI Explanations", "content": "Our research revealed that the design of explanations significantly affects participants' perceptions. While most explanations were well received, 15 participants expressed dissatisfaction, particularly with the use of demographic data. One participant noted, \u201c[Anything] involving demographics or comparisons raises questions of profiling,\u201d while another expressed distrust in the system, stating, \u201c[The] data they collect on me makes me distrust the system.\u201d These responses indicate that incorporating personal or comparative data can cause discomfort and raise concerns about privacy and fairness in AI systems.\nAdditionally, the tone of explanations emerged as an important factor in shaping user perceptions. Six participants stressed that the explanations should not sound \u201ccold or sarcastic.\u201d They preferred Al systems that communicated in a polite, conversational manner, suggesting that the emotional tone is as crucial as the content itself. While our study primarily focused on clarity and utility, these findings highlight the importance of incorporating human-like warmth into AI explanations, particularly in customer support scenarios, as noted in prior research [32]."}, {"title": "6.4 Design implications for AI chatbot interfaces", "content": "With the growing adoption of chat-based Al services and large language models", "systems": "n\u2022 Personalized explanations: Explanations should be tailored to individual users based on their cognitive traits and contextual factors. This can be achieved by gathering information about the user's engagement", "knowledge": "To prevent users from favoring explanations that reinforce incorrect prior knowledge (Section 5.2.2)", "What does this feature mean?": "r \u201cWhy is this feature important?\u201d).\n\u2022 Interactive explanation customization: Systems should offer users options to customize the style of explanations they receive. Users"}]}