{"title": "Weak Scaling Capability in Token Space: An Observation from Large Vision Language Model", "authors": ["Tenghui Li", "Guoxu Zhou", "Xuyang Zhao", "Qibin Zhao"], "abstract": "The scaling capability has been widely validated with respect to the number of parameters and the size of training data. One important question that is unexplored is that does scaling capability also exists similarly with respect to the number of vision tokens? This study fills the gap by investigating the relationship between the number of vision tokens and the performance of vision-language models. Our theoretical analysis and empirical evaluations reveal that the model exhibits weak scaling capabilities on the length $N_\\text{V}$, with performance approximately $S(N_\\text{V}) \\approx (c/N_\\text{V})^\\alpha$, where $c, \\alpha$ are hyperparameters. Interestingly, this scaling behavior remains largely unaffected by the inclusion or exclusion of the user's question in the input. Furthermore, fusing the user's question with the vision token can enhance model performance when the question is relevant to the task. To address the computational challenges associated with large-scale vision tokens, we propose a novel architecture that efficiently reduces the token count while integrating user question tokens into the representation. Our findings may offer insights for developing more efficient and effective vision-language models under specific task constraints.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) based on transformer architectures have demonstrated remarkable proficiency across a broad spectrum of tasks, including text generation, translation, and question-answering (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b;"}, {"title": "2 Background and Related Works", "content": "The majority of vision-language models are primarily driven by the advancement of two fundamental components: the large language model and the vision encoder.\nBased on the training strategy, vision-language models can be broadly categorized into two paradigms: natively multimodal models and hybrid models combining independently pretrained components. Natively multimodal models, such as Fuyu-8B (Bavishi et al., 2023), are jointly pretrained on vision and language tasks, enabling end-to-end learning and achieving seamless cross-modal alignment. Nevertheless, training from scratch for the large model is an extremely computationally expensive process, which presents significant challenges for researchers and practitioners with limited resources. Hybrid models, in contrast, integrate independently pretrained vision and language components into a unified framework. After integration, the unified model is fine-tuned on joint vision-language data to achieve cross-modal alignment and optimize performance. This two-stage approach reduces the computational burden of joint pretraining while enabling the reuse of state-of-the-art"}, {"title": "2.1 Background", "content": "The majority of vision-language models are primarily driven by the advancement of two fundamental components: the large language model and the vision encoder.\nBased on the training strategy, vision-language models can be broadly categorized into two paradigms: natively multimodal models and hybrid models combining independently pretrained components. Natively multimodal models, such as Fuyu-8B (Bavishi et al., 2023), are jointly pretrained on vision and language tasks, enabling end-to-end learning and achieving seamless cross-modal alignment. Nevertheless, training from scratch for the large model is an extremely computationally expensive process, which presents significant challenges for researchers and practitioners with limited resources. Hybrid models, in contrast, integrate independently pretrained vision and language components into a unified framework. After integration, the unified model is fine-tuned on joint vision-language data to achieve cross-modal alignment and optimize performance. This two-stage approach reduces the computational burden of joint pretraining while enabling the reuse of state-of-the-art"}, {"title": "2.2 Related Works", "content": "Pretrained large language models and vision encoders offer a solid foundation for addressing multimodal tasks. There are several strategies for integrating vision information into language models, including modifying the language model structure with adapter modules or simply treating vision tokens as instructions and concatenating them with text tokens.\nInserting Adapter Modules: By incorporating additional adapter modules into the main architecture of the language model, visual features can be seamlessly integrated alongside textual features. These adapters act as intermediary layers, facilitating the interaction between the two modalities by learning specialized transformations, allowing the model to effectively process and align multimodal information. For instance, Flamingo(Alayrac et al., 2022) applies cross-attention mechanisms between vision and language tokens, enabling it to handle open-ended tasks such as visual question answering and captioning. Similarly, CogVLM(Wang et al., 2023) incorporates a vision expert module comprising the attention and the MLP in each layer of the language model. While inserting adapter modules can achieve deeper cross-modality alignment, it comes at the cost of increased computational complexity.\nConcatenating Vision Tokens: A straightforward yet efficient strategy for integrating vision and language information is to concatenate the vision tokens with the text tokens. This approach leverages the architecture of the language model to process the vision tokens in the same way as text tokens, allowing the model to bridge both modalities through its internal mechanisms without requiring additional architectural modifications.\nFor example, LLAVA (Liu et al., 2024a), and MiniGPT-4 (Zhu et al., 2023) integrate vision tokens into the input sequence of the language model by concatenating them with text"}, {"title": "2.3 Insight of Vision Token Reduction", "content": "It is crucial to investigate the feasibility of reducing the number of vision tokens while maintaining the quality of the vision-language model. Such approach could help to reduce"}, {"title": "3 Branch by Appending Tokens", "content": "In this section, we will introduce a branching mechanism generated by appending tokens and its theoretical discussion. Briefly speaking, the upper bound of the distance between two branches is $\\text{O} (\\sqrt{\\psi(N)N^2 + (1 - \\psi(N))N})$, where N is the sequence length and $\\psi(N)$ indicate the behavior of a specific sequence model."}, {"title": "3.1 Preliminaries", "content": "Vision-language models operate on two primary types of inputs: text tokens and vision tokens. Considering concatenation-based methods, the formatted input sequence of these two sets of tokens can be represented as:\n$[T_1,...,T_k, |M_{k+1}, M_{k+2},\\cdots, M_{k+n}],T_{k+n+1},T_{k+n+2},...]$,\nwhere $T_1, T_2,..., T_k$ are text tokens, $M_{k+1}, M_{k+2},..., M_{k+n}$ are vision tokens, and $T_{k+n+1},...$ indicate remaining text tokens.\nTo establish the conceptual foundation of the problem, we consider an idealized model of sequence generation. This model assumes the following settings:\n\u2022 Valid Tokens: Vision tokens are well-aligned with text tokens and can be treated as newly introduced tokens in the sequence.\n\u2022 Text Prefix: The initial portion of the sequence consists of a fixed prefix of text tokens.\n\u2022 Incremental Addition: New tokens from different sources are appended to the same prefixed text tokens incrementally, one token at a time."}, {"title": "3.2 Branches Under Mapping", "content": "To assess the threshold value for significant sequence divergence, it is first necessary to define a measure of distance between two branches. In this subsection, a mapping mechanism is introduced to help estimate the distance between two branches.\nConsider two sequences of tokens, $[a_1, a_2,...]$ and $[b_1, b_2,...]$. We assume the existence of a transformation mapping $g_i(\\cdot)$ on each token, such that $g_i(a_i) = a_i$ and $g_i(b_i) = b_i$. The transformation mapping $g_i(\\cdot)$ should reflect the behavior of the model. Specifically, we consider the mapping $g_i(\\cdot)$ as,\n$g_i(a_i) := \\text{SequenceModel}(\\hat{a}_1,\\ldots, \\hat{a}_i)$,\nwhere the sequence model takes all preceding tokens as input to generate the output token. For a more specific case, we consider the sequence model as a decoder-only transformer model. The mapping $g_i(\\cdot)$ could be considered as\n$g_i(a_i) = \\text{LLM}(\\hat{a}_1,\\ldots,\\hat{a}_i)[-1]$,\nwhere LLM represent the decoder-only transformer model without the final linear layer, and LLM(\u2026)[-1] denotes the last item of output sequence.\nThe distance between the two branches is then computed based on the mapped tokens, $\\hat{a}_i$ and $b_i$, providing a quantitative measure of how far apart the two sequences have diverged over time.\nTo further understand the divergence, consider the mapped tokens at each time step i as points in a high-dimensional space, where the sequence progression can be likened to a random walk. Suppose all branches begin at the same initial point, represented as the origin in this space. For instance, in the sequence $[a_1,\\ldots,a_N]$, the branch starts at the origin at time step 0. At time step 1, the branch moves from the origin to the point $a_1$. This process continues step by step, with the branch moving from its current position towards $a_i$ at each subsequent time step i. The movement of the branch through this high-dimensional space forms a relatively random path. The final destination of the branch after N steps being represented by the cumulative sum $\\sum_{i=1}^{N} a_i$.\nThe difference between the two branches after N steps can be expressed as,\n$(\\sum_{i=1}^{N} a_i) - (\\sum_{i=1}^{N} b_i) = \\sum_{i=1}^{N} (g_i(\\hat{a}_i) - g_i(b_i)) := \\sum_{i=1}^{N}\\Delta g_i,$\\nwhere $\\Delta g_i$ denotes the difference $g_i(\\hat{a}_i) - g_i(b_i)$ for each time step i. To gain a more nuanced understanding of the distance between the two branches, a weight vector w with $||w||_F = 1$ is introduced, where $||\\cdot ||_F$ denotes the Frobenius norm. The weighted distance between the branches can then be analyzed via the expectation,\n$E \\big[w^T\\big(\\sum_{i=1}^{N} (g_i(\\hat{a}_i) - g_i(b_i))\\big)\\big]$"}, {"title": "3.3 Properties of The Bound", "content": "The inequality (10) provides a general bound for the expectation of the distance between two sequences of tokens. This bound is influenced by two key factors: the length of the branch sequence N and the function $\\psi(N)$, which represents the expectation of the cosine of the angle between two tokens in the sequence. To quantify the impact of these two factors, we define the function $f(N) = \\sqrt{\\psi(N)N^2 + (1 - \\psi(N))N}$.\nLet us first assume that $\\psi(N) = \\psi$ is a constant for all N, $f(N) = \\sqrt{\\psi N^2 + (1 - \\psi)N}$.\nThe function f(N) exhibits two extreme cases depending on the value of N. When N is small, the term $(1 - \\psi)N$ becomes more significant compared to $\\psi N^2$, leading to an approximation of $f(N) \\approx O(\\sqrt{N})$. Conversely, when N is large, the term $\\psi N^2$ dominates over $(1 - \\psi)N$, resulting in an approximation of $f(N) \\approx O(N)$. A brief overview of the function f(N) under these conditions is illustrated in Figure 2.\nNext, we consider the case where $\\psi(N)$ is a function of N.\nSince $\\psi(N) = \\sup E(\\cos \\Theta_{ij})$, it follows that $-1 < \\psi(N) < 1$ for all N > 1. The maximum value of $\\psi(N)$ is attained when $\\cos \\Theta_{ij} = 1$ for all pairs, while the minimum value is reached under the condition that all $\\cos \\Theta_{ij} = -1$. In the case where $\\psi(N) = 0$, it indicates that the expected cosine similarity between different pairs is zero, suggesting no correlation between the pairs.\nFor N = 1, the function f(N) simplifies to f(1) = 1. This is because with a single token in the sequence, the distance between the two sequences is trivially zero, leading to a constant value of 1 in this special case.\nWhen N > 1, to ensure the square root is computable, requires that $\\psi(N)N^2 + (1 - \\psi(N))N \\geq 0$. This implies that $\\psi(N) \\geq -1/(N - 1)$. As N increases, $-1/(N - 1)$ will approach to zero. To ensure that the square root function is computable for any given value"}, {"title": "3.4 Threshold Derivation", "content": "Based on the characteristics of the bound introduced in subsection 3.3, it is possible to derive a threshold value for sequence divergence. This threshold helps in understanding at what point the divergence between sequences becomes significant. In the following, we propose two methods to derive this threshold.\nThe first method for deriving the threshold involves analyzing the ratio of the bound with respect to contributions of the terms in the bound. As N increases, the bound approaches O(N). A ratio can be calculated as,\n$\\frac{\\sqrt{\\psi(N)N^2 + (1 - \\psi(N))N}}{\\sqrt{\\psi(N)N^2}} \\propto \\frac{1 - \\psi(N)}{\\psi(N)N}$.\nThe ratio measures the relative contribution of the second term in the bound compared to the first term. To derive an appropriate threshold, set $\\rho$ to a predefined threshold value, we have $(1 - \\psi(N))/(\\psi(N)N) \\leq \\rho$. Solving for N gives,\n$N \\geq \\frac{1 - \\psi(N)}{\\psi(N)\\rho}$.\nThe second method for deriving the threshold involves a direct analysis of the function $\\psi(N)$. As discussed in subsection 3.3, $\\psi(N)$ is a decreasing function of N.\nInitially, when the branching sequence is short, $\\psi(N)$ is expected to be large, reflecting a high similarity between the sequences due to the significant overlap in their context and information. As the length of the branch sequence increases, the model has more room to diverge from the original sequence, leading to a gradual decrease in $\\psi(N)$. Eventually, the divergence between the sequences becomes significant, and $\\psi(N)$ stabilizes at a low value.\nIn order to identify a threshold, we observe the gradient $d\\psi(N)/dN$. Since N is discrete, the gradient computation can be approximated by the difference,\n$\\Delta \\psi(N) = \\psi(N + 1) - \\psi(N)$."}, {"title": "3.5 Empirical Estimation of The Upper Bound", "content": "In this subsection, the expectation $E \\big[\\big(\\sum_{i=1}^{N}\\Delta g_i\\big)^2_F\\big]$ and the function $\\psi(N)$ is estimated empirically on pretrained large language models and a dataset of paired sequences. Based on the empirical estimates on them, we derive a threshold value that indicates the sequence length at which the divergence between the original and branching sequences becomes significant.\nTo facilitate this estimation, a dataset of paired input sequences is required. Each pair should consist of an initial sequence and a branching sequence of varying lengths. Specifically, a sequence comprises three components: a system prompt, a query, and an answer. In each pair, the system prompt and query remain identical, while the answers differ, creating positive and negative sequences. The difference $\\Delta g_i$ is computed specifically based on the 'answer' component of the positive and negative sequences. This approach allows for a focused analysis of how divergence in the answers reflects changes in the sequence and helps in deriving meaningful empirical estimates for $\\psi(N)$ and the associated expectation.\nIn order to conduct the empirical analysis, a dataset that is well-suited to the task at hand is required. The dataset requirements are closely aligned with those used in Direct Preference Optimization (DPO) (Rafailov et al., 2024). Specifically, we utilize the dataset 'ocra-dpo-pairs' \u00b9, which contains the necessary paired sequences to conduct this estimation.\nThe detailed computation process is described as follows.\nFirstly, the positive and negative sequences are converted to the input tokens $a_i$ and $b_i$. Secondly, the input tokens are passed through the mapping $g_i(\\cdot)$. The mapping is taken as the last token on the output sequence of a large language model, excluding the last output embedding layer,\n$g_i(a_i) = \\text{LLM}(\\ldots,\\hat{a}_t,\\hat{a}_{t+1},\\ldots,\\hat{a}_{t+i})[-1];$\n$g_i(b_i) = \\text{LLM}(\\ldots, b_t, b_{t+1},\\ldots, b_{t+i})[-1].$\nThirdly, the difference $\\Delta g_i = g_i(a_i) - g_i(b_i)$ is computed only on the 'answer' component, and N equals to the length of the 'answer' component. The reasoning behind this is that the 'answer' component is the only part of the sequence that differs between the positive and negative sequences. If the answer of the positive and negative sequences are not the same, then the longer sequence is truncated. Lastly, the empirical estimation is taken based on various amount of collected data.\nRecalling Equation (2), we take the same weight of each element of the token, picking w with all entries equal to $1/||w||_F$, the expectation could be estimated as,\n$E\\big[\\big(\\sum_{i=1}^{N}\\Delta g_i\\big)^2_F\\big]$"}, {"title": "4 Proposed Model", "content": "In this section, we introduce a vision-language model that leverages a concatenation-based approach on pretrained foundation models. The inputs to the large language model are formed by concatenating text tokens with vision tokens, which is widely adopted in vision-language models for its straightforward yet effective integration of multimodal information.\nTo enhance the integration between vision and language modalities, a fusion mechanism is designed to optimize the vision tokens before they are concatenated with the text tokens. This mechanism is designed to control the number of vision tokens while preserving the most relevant information, thereby improving the quality of the final representation."}, {"title": "4.1 Preliminaries", "content": "As discussed in Section 3, the expectation of distance between two branches can be expressed as a function of N, the number of newly appended tokens. With N increase, the difference between the two branches stabilizes, suggesting a convergence in behavior. In light of the preceding discussion in Subsection 2.3, and the perspective of information theory, reducing"}, {"title": "4.2 Main Architecture", "content": "The main architecture of the proposed model is illustrated in Figure 5.\nThere are three primary components in the proposed model: a vision encoder, a fusion module, and a large language model backbone. Each module is designed to fulfill a specific function, ensuring the seamless integration of vision and language modalities.\nThe vision encoder processes input images and generates a set of vision tokens, encapsulating the visual information extracted from the input. Nevertheless, when employing CLIP ViT-H/14 (Radford et al., 2021) as the vision encoder, the resolution of input images is restricted to (336,336). While this resolution is adequate for general tasks, it becomes a limitation for applications requiring fine-grained details, such as recognizing small objects,"}, {"title": "4.3 Formats of Token Sequences", "content": "In this subsection, we introduce the formats of token sequences employed in the proposed architecture outlined in Subsection 4.2. Two distinct token sequence formats are introduced, each corresponding to a critical stage in the main architecture. The first format represents the input to the fusion module, while the second format pertains to the input to the large language model.\nInput format to the fusion module: The input to the fusion module is constructed by concatenating three distinct types of tokens: the vision tokens, which are generated by the vision encoder and encapsulate visual features extracted from the input image; the text tokens, derived from the question text and mapped through the embedding layer; and a set of learnable queries, which are sourced from a fixed codebook and designed to facilitate selective attention within the fusion module."}, {"title": "4.4 Loss Functions", "content": "Since the fusion module is newly introduced and initialized randomly, it requires careful training to effectively adapt to the task. To achieve this, we design a training strategy that incorporates complementary loss functions, each addressing specific aspects of the task. Two key loss functions are employed: the contrastive loss and the identity loss.\nThe training pipeline involves two principal stages. Initially, preliminary training is conducted on the vision encoder, the fused model with the contrastive loss, and subsequently, fine-tuning is performed with the large language model backbone on generation loss.\nContrastive loss: One key component is a contrastive loss, inspired by the CLIP framework, which ensures alignment between the fusion module's output and the text encoded by the CLIP text encoder. This strategy allows the fusion model to effectively learn how to align its fused vision tokens with the semantically rich text tokens generated by the lightweight CLIP text encoder.\nSimilar to the CLIP loss, the contrastive loss is applied to the \u3008|eos|\u3009 token, a special marker indicating the end of an answer. In our case, the fusion module is designed to handle multiple question-answer pairs simultaneously. For questions and learnable queries, a single input sample may contain multiple groups of questions, each paired with its corresponding learnable queries. Within each group, the final token is designated as \u3008eos|\u3009 to indicate"}, {"title": "5 Experiments", "content": "In this section, we present the experimental results. For a detailed description of the dataset employed, please refer to Appendix B. The training process begins with a base model"}, {"title": "5.1 Scaling Analysis of Model Performance", "content": "In the following, we investigate the scaling properties of model performance with respect to the number of vision tokens $N_\\text{V}$. To achieve this, we employ a scaling law analogous to the one proposed in (Kaplan et al., 2020). The relationship between the performance metric S($N_\\text{V}$) and the number of vision tokens $N_\\text{V}$ is expressed as follows:\n$S(N_\\text{V}) \\approx \\big(\\frac{c}{N_\\text{V}}\\big)^\\alpha$,\nwhere c and $\\alpha$ are task-specific parameters that are estimated for each benchmark. The parameter c reflects the baseline performance of the model, while $\\alpha$ determines the rate at which performance changes as a function of token count. Smaller $\\alpha$ values indicate slower degradation in performance with token reduction, while larger $\\alpha$ values signify a more pronounced impact. The insights derived from the fitted curves offer valuable guidance for optimizing vision-language models. Understanding the scaling dynamics could help to balance computational resources with desired performance levels effectively. For a detailed visualization of the related results, please refer to Appendix E.\nTo improve numerical stability, we apply the logarithm to both $N_\\text{V}$ and S($N_\\text{V}$), and fit the data via the following mean square optimization problem:\n$\\underset{\\alpha,c}{\\text{argmin}} \\frac{1}{|\\Omega_{N_\\text{V}}|} \\sum_{n \\in \\Omega_{N_\\text{V}}} (z - \\alpha \\log(n) - \\log(S(n)))^2$,\nwhere $|\\Omega_{N_\\text{V}}|$ indicate the number of sampled points. Upon fitting, the constant c is derived as $c = e^{z/\\alpha}$. Utilizing the default BFGS optimizer provided by SciPy, we fit the performance data to generate the curves illustrated in Figures 8 and 9, and the corresponding fitted parameters, c and $\\alpha$, are summarized in Table 1.\nThe fitted curves reveal a clear logarithmic relationship between the number of vision tokens $N_\\text{V}$ and performance across most benchmarks, regardless of whether the models incorporate the user's questions as input. This trend aligns well with the theoretical scaling behavior described in Equation 21. The parameter $\\alpha$ represents the slope of the logarithmic relationship, indicating the rate of performance improvement as the number of tokens"}, {"title": "5.2 Fusion With User's Questions on Performance", "content": "In the majority of cases, it can be observed that fusing the text questions token from user do help improving the performance of the model.\nIn the following, we will compare the performances of the models with and without the fusion of the text questions token from user."}, {"title": "5.3 Compare to The Baseline Models", "content": "The performance of our model is comparable to the baseline models on some benchmarks."}, {"title": "6 Conclusion and Future Work", "content": "In this study, we investigate the relationship between the number of inserted vision tokens and the performance of the related vision language models. Our empirical analysis demonstrates that the model exhibits weak scaling capabilities, with performance approximately S($N_\\text{V}$) \u2248 ($c/N_\\text{V}$)^\u03b1, where c and $\\alpha$ are hyperparameters. Notably, this scaling behavior remains largely unaffected by the inclusion or exclusion of the user's question in the input. Furthermore, we find that fusing the user's question into the input can enhance model performance when the question is contextually relevant to the task. Compared to baseline models, our proposed model achieves comparable performance on some benchmarks, highlighting its potential to enhance efficiency and effectiveness in vision-language tasks.\nDespite these findings, our study has several limitations. Firstly, the observed scaling behavior is restricted to a specific range of vision token counts, leaving open questions about the model's performance under extreme conditions or beyond this range. Secondly, the additional fine-tuning applied to the model with $N_\\text{V}$ = 256, N\u2083 = 8, using only 10% of the dataset after the training on the full dataset. This strategy may introduce bias and limit the generalizability of our results. Lastly, all training and fine-tuning operations were performed solely on the vision tokens, while the large language model was kept frozen. This approach may affect the generalizability of our findings in situations where the language model is fine-tuned.\nIn future work, we aim to address these limitations and try to extend our findings to a broader range of vision token counts and model configurations."}, {"title": "Appendix A. Illustration of The Theoretical Upper Bound", "content": "Complementing the theoretical analysis, we present visual illustrations of the theoretical upper bounds across various large language models."}, {"title": "Appendix B. Datasets", "content": "Our model training utilizes a combination of existing datasets and newly generated data to enhance performance across various tasks. The primary datasets employed include LLAVA V1.5 MIX665K (Liu et al., 2024a), BAAI-SVIT (Zhao et al., 2023), and mPLUG DocDownstream 1.0 (Ye et al., 2023).\nTo further augment the training data, we generated new question-and-answer pairs using detailed descriptions from BAAI-SVIT, with the Llama-3 8B model serving as the agent. The generated questions were of two types: yes/no questions and multiple-choice questions (e.g., selecting one option from A, B, C, or D). To maintain data quality, any generated text that did not conform to the correct format was excluded from the dataset.\nAnother source of augmented training data comes from the structure-aware parsing task in CCpdf, derived from mPLUG DocDownstream 1.0. To enhance data quality, we utilized the nougat-base model (Blecher et al., 2023) to convert a subset of images into better-formatted text. To ensure the validity of the outputs, we applied several filtering criteria: (1) generated texts that were too short were discarded, (2) cases where the length difference between the original and generated text was excessively large were excluded, (3) generated texts containing endless repeated patterns were removed, and (4) texts that lacked sufficient similarity to the original under the embedding space, as measured using all-mpnet-base-v2 (Reimers and Gurevych, 2019), were filtered out. These steps ensured that the final dataset maintained high quality and relevance for training."}, {"title": "Appendix C. Implementation Details", "content": "Our implementation is built upon two widely used open-source pretrained models: CLIP ViT-H/14 (Radford et al., 2021) and Llama-2 7B (Touvron et al., 2023b). The CLIP ViT-H/14 vision encoder processes image inputs with a resolution of (336,336), using a stride of 14, resulting in 24 \u00d7 24 = 576 tokens, each with a hidden size of 1024. The Llama-2 7B model, serving as the backbone language model, features a hidden size of 4096.\nThe experiments were conducted on high-performance hardware comprising 8 NVIDIA A100 GPUs, each with 40 GB of memory. For evaluation on more accessible hardware, we utilized NVIDIA RTX A6000 GPUs, each with 48 GB of memory.\nThe image preprocessing pipeline involves several steps to adapt raw pixel images for input to the CLIP vision encoder. First, as introduced in subsection 4.2, the images are processed into global and local views, with each view independently passed through the CLIP encoder to generate vision tokens.\nBased on the available hardware resources, we adopt an HD-9 cropping strategy, which generates nine local image views and one global image view as input to the CLIP vision encoder. Each view has a resolution of (336, 336), resulting in a maximum input image resolution of (1008 \u00d7 1008). This setup initially produces 5760 tokens, which is computationally expensive to process.\nTo address this, we apply a local merging operation that reduces the token count by a factor of four. Neighboring vision tokens are merged in a 4\u00d74\u21d21 manner using a 2D convolution with a kernel size of 3, padding of 1, and a stride of 2. This reduces the vision token count to 1440. After appending \u3008|new line|\u3009 tokens to mark the end of each view, the"}, {"title": "Appendix D. Detailed Results", "content": "For evaluation, we utilize the VLMEvalKit (Duan et al., 2024) to compute scores across various benchmarks. Different configurations of $N_\\text{V}$ and Ns are tested, including: 768(8), 512(8), 384(8), 256(8), 128(8), 64(8), 32(8), 16(8), 8(8), 1(1), where the first number represents $N_\\text{V}$ and the second number in parentheses denotes Ns. The losses and gradient norms of the further fine-tuned models are shown in Figure 15.\nThe evaluation spans a diverse set of benchmarks, including MME (Fu et al., 2023), HallusionBench (Guan et al., 2024), POPE (Li et al., 2023d), OCRBench (Liu et al., 2024c), COCO VAL (Lin et al., 2014), RealWorldQA2, MMStar (Chen et al., 2024a), SEEDBench IMG (Li et al., 2023b), SEEDBench2 (Li et al., 2023a), SEEDBench2 Plus (Li et al., 2024), ScienceQA TEST (Lu et al., 2022), AI2D TEST\u00b3, OCRVQA TESTCORE (Mishra et al., 2019), ChartQA TEST (Masry et al., 2022), and TextVQA VAL (Singh et al., 2019).\nThe baseline models are: 360VL-70B 4, InstructBLIP-7B (Dai et al., 2023), InternLM-XComposer2-4KHD (Dong et al., 2024), LLaVA-v1-7B, LLaVA-v1.5-13B, LLaVA-v1.5-7B (Liu et al., 2024a), MiniGPT-4-v1-7B, MiniGPT-4-v2 (Zhu et al., 2023), mPLUG-Owl2 (Ye et al., 2024), OpenFlamingo v2 (Alayrac et al., 2022), Phi-3-Vision (Abdin et al., 2024), Qwen-VL-Chat (Bai et al., 2023b).\nThe results are summarized in the following tables, with all benchmark data for the models obtained from VLMEvalKit (Duan et al., 2024). The blank cells indicate that the scores are not available from VLMEvalKit.\nIn the column titled \"Fused Model Inputs,\" the following terms are used to describe the input configurations of the model:"}, {"title": "Appendix E. Further Analysis on N\u03b9", "content": "The performance of the proposed model exhibits a relationship with the number of learnable queries ($N_\\text{V}$). In the following, we focus on the general trends and patterns observed in the results.\nTo better visualize the impact of $N_\\text{V}$ on the model's effectiveness, we normalize the results across benchmarks using min-max scaling, ensuring consistency in comparison regardless of differing scales. The normalization is x \u21d0 (x-min(x))/(max(x) \u2013 min(x)), which scales all results to the range [0, 1]. This preprocessing step makes it easier to compare performance trends across various benchmarks.\nThe normalized results reveal several distinct trends when plotted. To better understand these patterns, we apply K-means clustering, dividing the results into four clusters. These clusters provide insights into how $N_\\text{V}$ influences the model's performance, enabling a more structured analysis of its scaling capability and identifying optimal configurations for various scenarios.\nWe begin our analysis with the model that takes vision tokens, the user's question, and learnable queries as inputs. The results, illustrated in Figure 16, reveal distinct clusters that demonstrate the relationship between the number of learnable queries ($N_\\text{V}$) and the model's performance.\nA global observation from the results is that the model performs poorly across all benchmarks when $N_\\text{V}$ is small. As $N_\\text{V}$ increases, the performance improves significantly, eventually"}, {"title": "Appendix F. Example of Inference", "content": "The following examples illustrate two demonstrations of the generation results produced by the models."}]}