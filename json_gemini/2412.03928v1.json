{"title": "MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction", "authors": ["Mithun Parab", "Pranay Lendave", "Jiyoung Kim", "Thi Quynh Dan Nguyen", "Palash Ingle"], "abstract": "In image-assisted minimally invasive surgeries (MIS), under- standing surgical scenes is vital for real-time feedback to surgeons, skill evaluation, and improving outcomes through collaborative human-robot procedures. Within this context, the challenge lies in accurately detect- ing, segmenting, and estimating the depth of surgical scenes depicted in high-resolution images, while simultaneously reconstructing the scene in 3D and providing segmentation of surgical instruments along with detec- tion labels for each instrument. To address this challenge, a novel Multi- Task Learning (MTL) network is proposed for performing these tasks concurrently. A key aspect of this approach involves overcoming the op- timization hurdles associated with handling multiple tasks concurrently by integrating a Adversarial Weight Update into the MTL framework, the proposed MTL model achieves 3D reconstruction through the inte- gration of segmentation, depth estimation, and object detection, thereby enhancing the understanding of surgical scenes, which marks a signifi- cant advancement compared to existing studies that lack 3D capabilities. Comprehensive experiments on the EndoVis2018 benchmark dataset un- derscore the adeptness of the model in efficiently addressing all three tasks, demonstrating the efficacy of the proposed techniques.", "sections": [{"title": "1 Introduction", "content": "Efficient 3D reconstruction for surgical scene understanding of MIS environ- ments is crucial during robotic surgery since it enhances precision, reliability and reduces trauma and recovery time. However, surgeons can still face work- flow disruptions due to limited tactile feedback or system issues. Moreover, the complexity of surgical environments, characterized by factors such as smoke, bodily fluids, varying lighting conditions, and partial occlusions, poses signifi- cant challenges to image interpretation [16]. Hence, there arises a pressing need for 3D reconstruction of surgical scenes, it enhances surgical performance, of- fers real-time feedback, aids in skill assessment for novice surgeons, and enables"}, {"title": "2 Related Work", "content": "The use of object detection, segmentation, tracking, and depth estimation in medical imaging spans various surgical and diagnostic specialties. Diverse method- ologies leverage datasets like the EndoVis, with approaches including single-task learning (STL). Furthermore, 3D reconstruction [17] plays a crucial role in sur- gical understanding. Our focus lies on multitask learning for surgical instrument tasks, recognizing its benefits in generalization and computational efficiency."}, {"title": "2.1 Segmentation and object detection", "content": "CNN models are utilized for binary, semantic, and instance segmentation in sur- gical tool tracking. TernausNet [8], excels in binary segmentation, while ISINet [24], TraSeTR [24], and AP-MTL [9] focus on instance segmentation. Considering the computational demands, lighter techniques like binary segmentation can en- hance processing speed. Moreover, combining binary segmentation with object detection can yield comparable outcomes to instance and semantic segmenta- tion methods. Advancements in instrument detection include TernausNet-16 [8],"}, {"title": "2.2 Depth Estimation", "content": "Traditionally hampered by data scarcity, stereo-based 3D reconstruction relied on handcrafted methods [21], [25] validated on specialized datasets [5]. However, new advancements in monocular depth estimation now rival the capabilities of stereo [17] and LiDAR setups. This ensures cost-effectiveness and system flex- ibility, while harnessing the latest developments to provide precise visual guid- ance in robotic surgery. MTL leverages semantic segmentation for scene context in monocular depth estimation, addressing issues like blurry object boundaries and enhancing accuracy, as pioneered in [6]. MTL networks optimize efficiency by sharing a single feature extraction backbone, cutting down on inference time and memory usage, enabling models like SENSE [11] to predict multiple outputs concurrently."}, {"title": "2.3 Multi-Task Learning", "content": "In computer vision, various MTL models like MaskRCNN [7] have been devised for concurrent semantic segmentation and object detection tasks. Recently, ef- forts have been made by UberNet [12] and AP-MTL [9] to refine MTL models through multi-phase training approaches and fine-tuning. However, while MTL has seen significant progress in segmentation and detection tasks, it trails be- hind in depth estimation for comprehensive 3D reconstruction. MSDESIS [17] at- tempted multitask depth estimation and segmentation utilizing a stereo camera setup, which could present challenges in hardware and space-limited MIS proce- dures. Therefore, additional research is required to explore monocular multi-task learning approaches in robotic MIS environments."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 MTL Architecture", "content": "The proposed model architecture comprises an Encoder 3.2, Decoder 3.2, and task-specific heads 3.2, integrated into a unified Multi-Task Learning Trans- former (MT3DNet) 3.2 framework is depicted in Fig. 1."}, {"title": "3.2 Encoder.", "content": "The encoder architecture is designed to extract hierarchical features from in- put data by progressively transforming and downsampling it through multiple stages. Each stage in the encoder consists of a series of operations that refine the representation, leveraging both convolutional and attention-based mechanisms. The stages are stacked sequentially, each having a defined number of layers and different configurations for patch size, overlap, and dimensionality reduction. At the core of the encoder, each stage performs patch merging, where the spatial resolution of the input is reduced while preserving essential features. This is achieved using the concept of overlap patch merging, where overlapping patches are combined to enhance the information retention during downsampling. The input is then passed through a series of encoder blocks, each block consisting of self-attention and feed-forward layers. These blocks are meant to capture long- range dependencies and intricate patterns in the data.\nThe depth of the encoder determines the number of encoder blocks per stage, allowing the network to progressively refine its features at different levels of ab- straction. Each encoder block is then equipped with a regularization technique, controlled by a dynamic drop path probability, which aids the learning process by randomly dropping certain paths during the training process. This helps pre- vent overfitting and promotes better generalization. The output of each stage of the encoder is passed through a normalization layer to stabilize learning and ensure consistent feature scaling across the network. The multi-stage design al- lows the encoder to capture both local and global information with each stage progressively handling larger contexts and more abstract features.\nDecoder. The Decoder component of the architecture takes as input a set of features (F) with dimensions corresponding to the batch size, number of chan-"}, {"title": "Task Heads.", "content": "After passing through the Decoder, the processed features are fed into task-specific heads, which are responsible for reducing the feature dimen- sionality to match the requirements of individual tasks. These task heads are tailored according to the specific task at hand, such as segmentation, monocular depth estimation, or object detection. Each task head receives its input tensor from the Decoder and performs the final processing before producing the output for the respective task. This task head stage represents the final block of the Multi-Task Learning (MTL) architecture.\nMulti-Task Learning Transformer (MT3DNet). MT3DNet is a compre- hensive architecture that integrates Encoder, Decoder, and task heads into a unified framework for multi-task learning. Leveraging Transformer-based archi- tectures, MT3DNet efficiently processes input data and addresses multiple tasks simultaneously. The Encoder utilizes Transformer blocks to effectively encode input data, capturing intricate relationships. The subsequent Decoder refines encoded features for task-specific processing. Task heads provide modular and task-specific feature representations, optimizing performance for each task. This integration offers a versatile and efficient solution for multi-task learning across domains and applications."}, {"title": "4 Loss and Evaluation Metrics", "content": ""}, {"title": "4.1 Losses", "content": "In the Multi-Task Learning (MTL) paradigm, the total loss function (1) encom- passes individual losses from segmentation (2), depth estimation (6), and object detection (10) tasks.\nTotal Loss (\\mathcal{L}_{total}) = w_1 \\times \\mathcal{L}_{seg} + w_2 \\times \\mathcal{L}_{depth} + w_3 \\times \\mathcal{L}_{detection} (1)\nwhere, w1, w2 and w3 are weights for each task's loss.\nSegmentation Loss: The segmentation loss combines binary cross-entropy (BCE) or cross-entropy (CE) loss with the mean Intersection over Union (mIoU) loss to handle both binary and multi-class segmentation tasks. The general form is given as:\n\\mathcal{L}_{seg} = \\alpha \\cdot \\mathcal{L}_{CE/BCE} + \\beta \\cdot (1 - mIoU), (2)\nwhere:"}, {"title": "\u2022", "content": "\\mathcal{L}_{CE/BCE} represents the cross-entropy loss for multi-class or binary cross-entropy loss for binary segmentation.\n\u2022 mIoU is the mean Intersection over Union, defined as:\nmIoU = \\frac{\\sum_{c=1}^{C} Intersection_c}{\\sum_{c=1}^{C} Union_c} (3)\nwhere C is the number of classes, Intersection is the overlapping area between predicted and ground truth for class c, and Union is their union.\n\u2022 \u03b1 and \u03b2 are weighting factors for balancing the loss components.\nFor binary segmentation, the BCE loss is computed as:\n\\mathcal{L}_{BCE} = \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)], (4)\nwhere yi is the ground truth, pi is the predicted probability, and N is the number of pixels.\nFor multi-class segmentation, the CE loss is:\n\\mathcal{L}_{CE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log (p_{i,c}), (5)\nwhere yi,c and pic are the ground truth and predicted probability for class c at pixel i.\nThe combined loss ensures both accurate pixel-wise predictions and effective region-wise overlap.\nDepth Loss: The depth loss is defined as a weighted combination of three components: SSIM loss, edge loss, and mean absolute error (MAE). Let Dp and Dt represent the predicted depth and target depth, respectively. The total depth loss \\mathcal{L}_{depth} is given by:\n\\mathcal{L}_{depth} = w_1 \\mathcal{L}_{SSIM} + w_2 \\mathcal{L}_{edge} + w_3 \\mathcal{L}_{MAE}, (6)\nwhere w1, w2, and w3 are the weights assigned to each loss component. The components are defined as follows:\n\u2022 SSIM Loss:\n\\mathcal{L}_{SSIM} = 1 - SSIM(D_p, D_t), (7)\nwhere SSIM(\u00b7, \u00b7) is the structural similarity index function.\n\u2022 Edge Loss: The edge loss is computed using a Sobel filter S to extract gradients from the predicted and target depth maps:\n\\mathcal{L}_{edge} = \\frac{1}{N} \\sum_{i=1}^{N} |S(D_p)_i - S(D_t)_i|, (8)\nwhere N is the number of pixels in the depth map."}, {"title": "\u2022 Mean Absolute Error (MAE):", "content": "\\mathcal{L}_{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |D_p - D_t|. (9)\nObject Detection Loss: The object detection module integrates Smooth L1 Loss for bounding box localization and cross-entropy for classification.\nObject Detection Loss (\\mathcal{L}_{detection}) = Smooth L_1 Loss + CE (10)"}, {"title": "4.2 Adversarial Weight Updates for Multi-Task Learning", "content": "In multi-task learning, tasks often exhibit conflicts, making balanced optimiza- tion crucial. We propose an adversarial approach that dynamically adjusts task weights to achieve equilibrium, inspired by Multiplicative Weights Update (MWU). This ensures tasks contribute proportionally to their significance, minimizing conflicts while improving overall performance. Below, we present the core algo- rithm for this process.\nAlgorithm 1 Adversarial Weight Update for Multi-Task Learning\nRequire: Task losses {lt}t=1, initial weights {wt}t=1, learning rate \u03b7, small constant\nEnsure: Updated weights {wnew}\n1: Compute updated weights:\nnew \u2190 wt exp(\u2212\u03b7\u00b7lt) \u2200t \u2208 {1, . . . , T}\nnew\n\u2208 {1, . . . , T}\nnew + \u03f5\n2: Normalize weights:\nwt\nT\nnew = \u2200t\n\u2211k=1 wk\nThis adversarial adjustment is then integrated with gradient-based updates. Task gradients are collected and their interrelations are captured using a gradient Gram matrix. We solve for optimal weights by employing a regularized MWU- inspired approach:"}, {"title": "Algorithm 2 Optimal Weight Computation via Gradient Alignment", "content": "Require: Task gradients G \u2208 RT\u00d7P, regularization term \u03bb, unit vector 1 \u2208RT\nEnsure: Optimal weights w*\n1: Compute gradient Gram matrix:\nG GT G GT + \u03bbI\n2: Solve for optimal weights:\nw* arg min || Gw \u2013 1||2\nw\n3: Normalize weights:\nwt*\nT\nw \u2190 \u2211\nwk + \u03b5\n\u2200t \u2208 {1, . . . , T}\nThe use of these weight updates drives this multi-task learning framework to reliably align gradients and optimise the tasks without explicit optimisation dynamics, hence being robust and efficient in the overall learning process."}, {"title": "5 Experimental Results", "content": "In our experiment, we used the Adan optimizer [23] with an initial learning rate of 0.001 and a weighted decay of 0.02. We also incorporated a Reduce on Plateau scheduler and an exponential learning rate scheduler to enhance learning dynamics, ensuring efficient convergence and improved performance throughout training, contributing to robustness and effectiveness. Leveraging the publicly available EndoVis18 dataset, comprising 14 videos of porcine surgeries with The"}, {"title": "model with", "content": "and without the Adversarial Weight Update algorithm. As shown in Table 2, the model with the Adversarial Weight Update demonstrates significant improvements over the baseline. Specifically, the Dice coefficient for segmenta- tion has increased by 4.5%, and the mean Average Precision (mAP) for object detection has improved by 7.4%. These improvements highlight the effective- ness of the proposed approach in achieving better task-specific performance and overall robustness in multi-task learning.\nFuture research might be focused on the exploration of the generalizabil- ity of MT3DNet to other medical imaging modalities and surgical procedures. Further, exploring real-time implementation and integration into existing surgi- cal systems could open the doors for practical deployment in clinical settings. Another possible area of further research is exploring the potential integration of additional sensory data, such as tactile or force feedback, which would en- rich the understanding and guidance capabilities of the system during surgical procedures."}, {"title": "6 Conclusion", "content": "In this study, a novel multi-task learning framework called MT3DNet was pro- posed for segmenting, localizing, and estimating depth in image-assisted Mini- mally Invasive Surgery environments. With the use of Adversarial Weight Up- date, challenges of optimization inherent with the management of multiple tasks simultaneously were addressed to bring about optimality. Extensive experiments on the EndoVis2018 dataset demonstrated the model's ability to handle all three tasks, which demonstrated the effectiveness of our methodologies Importantly, the approach enables 3D reconstruction by integrating segmentation, depth esti- mation, and object detection, which improves surgical scene understanding sig- nificantly over prior studies that lack 3D capabilities. The novel approach with"}]}