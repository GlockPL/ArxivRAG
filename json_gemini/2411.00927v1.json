{"title": "ReSpAct: Harmonizing Reasoning, Speaking, and Acting\nTowards Building Large Language Model-Based Conversational AI Agents", "authors": ["Vardhan Dongre", "Xiaocheng Yang", "Emre Can Acikgoz", "Suvodip Dey", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "abstract": "Large language model (LLM)- based agents\nhave been increasingly used to interact with\nexternal environments (e.g., games, APIs etc.)\nand solve tasks. However, current frameworks\ndo not enable these agents to work with users\nand interact with them to align on the details\nof their tasks and reach user-defined goals, in-\nstead, in ambiguous situations these agents may\nmake decisions based on assumptions. This\nwork introduces ReSpAct (Reason, Speak, and\nAct), a novel framework that synergistically\ncombines the essential skills for building task-\noriented \"conversational\" agents. ReSpAct ad-\ndresses this need for agents, expanding on the\nReAct approach. ReSpAct framework enables\nagents to interpret user instructions, reason\nabout complex tasks, execute appropriate ac-\ntions, and engage in dynamic dialogue to seek\nguidance, clarify ambiguities, understand user\npreferences, resolve problems, and use the in-\ntermediate feedback and responses of users to\nupdate their plans. We evaluated ReSpAct with\nGPT-4 in environments supporting user inter-\naction, such as task-oriented dialogue (Multi-\nWOZ) and interactive decision-making (Alf-\nworld, WebShop), ReSpAct is flexible enough\nto incorporate dynamic user feedback and ad-\ndresses prevalent issues like error propagation\nand agents getting stuck in reasoning loops.\nThis results in more interpretable, human-like\ntask-solving trajectories than baselines relying\nsolely on reasoning traces. In two interac-\ntive decision-making benchmarks, AlfWorld\nand WebShop, ReSpAct outperforms strong\nreasoning-only method ReAct by an absolute\nsuccess rate of 6% and 4%, respectively. In the\ntask-oriented dialogue benchmark MultiWOZ,\nReSpAct improved Inform and Success scores\nby 5.5% and 3%, respectively.", "sections": [{"title": "1 Introduction", "content": "Instruction-following is a fundamental capability\nfor intelligent agents operating in real-world envi-\nronments. Recent works such as (Wei et al., 2022;\nintuitive for agents. For instance, if a user requests,\n\"Arrange a trip to Hawaii,\" they likely don't expect\nthe agent to book hotels, flights, and rental cars for\nrandom dates without confirming the details first.\nExisting reasoning and decision-making ap-\nproaches for language agents augment the agent's\naction space with a language model, allowing the\nagent to generate free-form thoughts in natural lan-\nguage that help contextualize and reason about the\ntask at hand. By alternating between task-solving\nactions and language thoughts, these agents can\nperform multi-step reasoning and compose useful\ninformation for solving complex tasks. However,\nsuch frameworks do not explicitly incorporate user\ninteraction and feedback into the agent's reasoning\nprocess. In real-world scenarios, engagement with\nusers can provide valuable information, clarifica-\ntion, and guidance that can significantly improve an\nagent's task-solving capabilities. In this paper, we\npropose ReSpAct, a framework for task-oriented\nconversational agents that allows the agent to ac-\ntively engage with users through dialogue actions.\nBy introducing a new action space for user inter-\naction, the agent can ask clarifying questions, re-\nquest feedback, and incorporate user responses into\nits evolving context. This human-in-the-loop ap-\nproach enables the agent to leverage user insights,\nadapt to user preferences, and refine its task-solving\nstrategy based on user input.\nThe ReSpAct framework, as shown in Fig. 1,\nis a critical step towards moving from agents to\n\"conversational\" agents, which can proactively so-\nlicit information from the user, give feedback or\ntake any follow-up directions.  ReSpAct\nframework aims to enable such an experience with\nLLMs, expanding on the ReAct framework. This\nwould result in a more controllable dialogue experi-\nence instead of letting the agent struggle to handle\nall cases by itself without any help. Recent work on\nthe developer side, such as LangGraph\u00b9 attempts\nto remedy this problem by building a manually\ncrafted dialogue flow. In contrast, the ReSpAct\napproach allows the model to decide on when to\n\"speak\" based on its reasoning.\nOur key contributions are as follows:\n\u2022 We introduce the ReSpAct framework enabling\ninteractive agents with LLMs, expanding on the\nReAct approach.\n\u2022 We perform experiments showing the value"}, {"title": "2 Related Work", "content": "2.1 Language Models and Reasoning\nLogical reasoning often involves breaking down\ncomplex inputs into a series of intermediary steps,\nguiding the model toward a final goal or answer\nstep by step, as demonstrated by (Wei et al., 2022)\nand its several different variants (Kojima et al.,\n2022; Madaan and Yazdanbakhsh, 2022; Wang\net al., 2022). However, these methods often suffer\nfrom error propagation due to compound errors as\nthe sequence length increases (Guo et al., 2018;\nChen et al., 2022), where mistakes made in earlier\nsteps accumulate, leading to greater inaccuracies\nin later stages. Approaches such as (Creswell et al.,\n2022), and (Creswell and Shanahan, 2022) pro-\npose sophisticated ways to break down reasoning\nprocess, (Madaan et al., 2024; Shinn et al., 2024)\nuse an iterative approach to refine the model's rea-\nsoning. However, these methods overlook the cru-\ncial role of human feedback. ReSpAct allows the\nagent to engage in dialogue with the user to seek\nassistance, feedback, and guidance, preventing a\ncascade of subsequent errors.\n2.2 Language Models and Decision-Making\nLLMs have been adapted for decision-making and\nacting tasks, serving as policy models in interactive\nenvironments. In robotics, LLMs function as high-\nlevel controllers for lower-level control policies\n(Ahn et al., 2022; Huang et al., 2022; Driess et al.,\n2023) and have proven useful in tasks like web\nnavigation (Deng et al., 2024a; Zheng et al., 2024).\nThey are particularly effective in text-based envi-\nronments (Shridhar et al., 2020b; Liu et al., 2023),\nwhere techniques like ReAct (Yao et al., 2022b)\nenable reasoning and action based on text. While\nsome work incorporates dialogue to aid decision-\nmaking (L\u00f9 et al., 2024; Deng et al., 2024b), these\napproaches do not fully integrate dialogue, reason-\ning, and decision-making. In contrast, ReSpAct\nallows agents to reason about both actions and dia-\nlogues, enabling fluid transitions between reason-\ning, acting, and user interaction for more coherent\nand effective decision-making in complex environ-\nments.", "latex": []}, {"title": "2.3 Leveraging Language Feedback", "content": "Previous works (Nguyen et al., 2022; Dai et al.,\n2020; Chai et al., 2014) show that equipping au-\ntonomous embodied agents with communication\nskills improves their reliability by enabling them\nto leverage human knowledge during collaborative\ntasks. For example, (Dai et al., 2024) demonstrates\nthe effectiveness of language instructions in navi-\ngating complex environments, and (Chi et al., 2020)\nshows how a confused agent can signal for help,\nreceiving guidance to complete tasks.\nRecent work in conversational systems has ex-\nplored using LLMs in task-oriented dialogues\n(TOD) through fine-tuning (Gupta et al., 2022;\nSu et al., 2022; Feng et al., 2023) and in-context\nlearning (Hu et al., 2022). (Hude\u010dek and Dusek,\n2023) examines instruction-finetuned LLMs in\nmulti-turn dialogues, while (Zhang et al., 2023;\nXu et al., 2024b) use prompting schemas to build\nautonomous agents. However, these approaches\nstruggle to interpret instructions, resolve ambigui-\nties, and act appropriately."}, {"title": "3 ReSpAct: Reason + Speak + Act in\nInteractive Settings", "content": "Consider a setup where an agent can interact with\nan environment to perform tasks and achieve spe-\ncific goals. When the agent operates in these en-\nvironments, at each time step t, it receives an ob-\nservation o\u209c from the environment, where o\u209c \u2208 O\nand O represents the observation space. Then it\nexecutes an action a\u209c based on its policy \u03c0, where\na\u209c \u2208 A and A represents the action space. The\npolicy \u03c0is a function that maps the agent's cur-\nrent context c\u209c to an action a\u209c. Formally, we can\ndefine this policy as \u03c0 : C \u2192 A where C repre-\nsents the context space. The context c\u209c encapsulates\nthe relevant information available to the agent at\ntime step t, including the current observation and\nthe history of previous observations and actions:\nc\u209c = (o\u2081, a\u2081,\u2026, o\u209c\u208b\u2081, a\u209c\u208b\u2081, o\u209c). As highlighted\nin (Yao et al., 2022b), learning the optimal pol-\nicy can be challenging, especially when the map-\nping from the context to the appropriate action is\nhighly implicit and requires extensive computation.\nIn complex real-world scenarios, the agent may\nneed to reason about the task, consider multiple\nfactors, and handle incomplete or ambiguous infor-\nmation. Therefore, augmenting the agent's action\nspace \u00c2 = A \u222a L allows it to think by taking\naction a\u209c \u2208 L from the language space. These ac-", "latex": ["o_t \\in O", "a_t \\in A", "\\pi : C \\rightarrow A", "c_t = (o_1, a_1,\\ldots, o_{t-1}, a_{t-1}, o_t)", "\\hat{A} = A \\cup L"]}, {"title": "3.1 Advancing Human-Agent Collaboration\nin Alfworld", "content": "Alfworld (Shridhar et al., 2020b) is a synthetic en-\nvirnoment built on the TextWorld framework (C\u00f4t\u00e9\net al., 2019), aligned with the embodied ALFRED\nbenchmark (Shridhar et al., 2020a). The environ-\nment includes six categories of tasks, such as find-\ning hidden objects (e.g., locating a key inside a\ncabinet), moving objects (e.g., placing a cup on a\ntable), manipulating objects with other objects (e.g.,\nheating potato in a microwave), and examining ob-\njects (e.g., inspecting a book under a desklamp).\nThe ReSpAct framework demonstrates significant\nadvantages when applied to the Alfworld environ-\nment by enabling dynamic, bidirectional commu-\nnication. As shown in Fig. 3, The agent can ask\ncontextually relevant questions, provide status up-\ndates, and seek clarification when uncertain (e.g.,\n\"Where should I look for the candles first?\"). This\napproach integrates reasoning, speaking, and acting\nseamlessly, allowing flexible and responsive inter-\nactions compared to ReAct, where users primarily\nedited thought traces post-generation.\nMoreover, ReSpAct's seamless integration of\nreasoning, speaking, and acting creates a more flex-"}, {"title": "3.2 Harmonizing Dialogue and Actions in\nTask-Oriented Dialogue Systems", "content": "MultiWOZ is an established standard dataset for\ntask-oriented Conversational AI (Budzianowski\net al., 2018). It consists multi-turn dialogues in sev-\neral domains like restaurant, hotel, train, attractions,\ntaxi, and train. Most dialogues are about complet-\ning a given multi-domain goal, such as booking a\ntable at a restaurant whose specifications are given,\nand potential follow-up tasks, such as arranging\na taxi to that restaurant, which make this dataset\nperfect for demonstrating the power of ReSpAct.\nThe established metrics are \"Inform\" and \"Suc-\ncess\" rates, which compute whether the given goal\nhas been achieved. More specifically, the Inform\nmetric checks that the system returned a valid en-\ntity satisfying all the goals or constraints. for ex-\nample, a valid hotel from area south. The success\nmetric measures whether the system returned the\nrequested attributes (for example, phone number\nand address) in the response.\nOur implementation follows the AutoTOD sys-\ntem (Xu et al., 2024a), employing the ReAct frame-\nwork for MultiWOZ. Clearly, the\nReSpAct dialogue is more informative and success-\nful for the user (and probably cheaper). As seen,\nReSpAct interaction differs from ReAct as follows:\n\u2022 The agent should hesitate to make assumptions,\nand instead ask the user about them. For example,\nin the first turn, instead of randomly choosing an\nattraction, the ReSpAct agent tries to get more\npreferences from the user.\n\u2022 If an assumption is made, it must be implicitly\nor explicitly confirmed by the user. For example,\nin the second turn, ReAct assumes the location"}, {"title": "3.3 Dialogue-Driven Collaboration for\nonline-shopping in WebShop", "content": "WebShop (Yao et al., 2022a) is a benchmark for\nevaluating AI agents in complex e-commerce sce-\nnarios, featuring 1.18M products and 12k human-\ngenerated instructions. Agents navigate using\nsearch and click commands, processing structured\nand unstructured texts, which increases task com-\nplexity. The goal is to purchase products that meet\nuser specifications, requiring advanced natural lan-\nguage understanding and decision-making.\nThe integration of user interaction, ReSpAct en-\nhances agent's decision-making in WebShop. User\nfeedback improves, performance, in areas such as\nsearch refinement, clarifying ambiguous instruc-\ntions, prioritizing requirements, suggesting alter-\nnatives, navigating, specifying implicit needs, han-\ndling invalid actions, and confirming purchases;.\n3.4 User Simulation\nTo evaluate our agent's performance in a controlled\nand scalable manner, a user simulator is integrated\ninto the agent's interaction loop for experimenting\nwith ReSpAct. When the agent performs a 'speak'\naction to interact with the user, instead of requir-\ning human input, the agent utterance is directed\nto the user simulator. The simulator then provides\na response based on the current state and Oracle\nknowledge. The main purpose of the user simula-\ntor is to provide contextually appropriate responses\nto the agent's queries, emulating a knowledgeable\nhuman user. It is designed to comprehend the task\nobjectives, monitor the agent's progress, and pro-\nvide a response only when requested by the agent.\nMore details can be found in Appendix A.1"}, {"title": "4 Experimental Setup", "content": "In our experiments, we evaluate ReSpAct across\nmultiple task-oriented decision-making environ-\nments, employing a human-in-the-loop approach\nto demonstrate its versatility. The agent is tested\non multi-step tasks in common household envi-\nronments using Alfworld (Shridhar et al., 2020b),\ntasked with making reservations in the MultiWoz\ndialogue setup (Budzianowski et al., 2018), and\ninstructed to purchase products in Webshop (Yao\net al., 2022a).\nWe use ReAct as a baseline for comparison, a\nreasoning-only approach. For these experiments,\nwe focus on frozen GPT models, particularly GPT-\n40 (Achiam et al., 2023), which is prompted with\nfew-shot exemplars. These exemplars guide the\nmodel in generating a mix of domain-specific ac-\ntions, free-form reasoning (\"thoughts\"), and dia-\nlogue actions interleaved throughout task execution.\nThe in-context examples provided (see Appendix\nD) contain dense sequences of actions interspersed\nwith sparse thoughts and dialogue actions at rele-\nvant points. To scale our experiments, we imple-\nment a user simulator in each environment, which\nplays a critical role in replicating user interactions.", "latex": []}, {"title": "4.1 Alfworld", "content": "To prompt ReSpAct, we adopt a similar prompt-\ning strategy as used in ReAct; we randomly an-\nnotate three trajectories from the training set for\neach task type, where each trajectory includes in-\nterleaved thoughts, speak actions, corresponding\nuser responses, and environment actions. We eval-\nuate our approach on 134 unseen evaluation games\nacross various task types, following the methodol-"}, {"title": "4.2 MultiWoz", "content": "In comparing the ReAct and ReSpAct for handling\nuser queries in MultiWOZ, the key differences re-\nvolve around how each model balances reasoning,\ninteraction with the user, and autonomy. While\nReAct relies heavily on reasoning based on assump-\ntions and API querying to guide decision-making,\nReSpAct not only reflects on its actions but also\nharnesses user feedback effectively.\nFor ReSpAct we have randomly chosen 100 di-\nalogues, similar to other tasks for evaluation, and\noptimized the additional prompts using the dev set.\nPlease check Appendix D.1 for the exact ReSpAct\nprompt for MultiWOZ. Basically we have added\nprompts, covering the cases of too many results,\nasking for required arguments of an action, like\nbooking, or clarification of type vs. name in an\nentity. GPT apparently is better than\nLlama model in following the ReSpAct instruc-\ntions, resulting in larger improvement over ReAct.", "latex": []}, {"title": "4.3 WebShop", "content": "For Webshop, we use the preconstructed action\nspace of search and click commands and browser\nfeedback. Performance is evaluated using two met-\nrics: (1) average score, defined as the percentage\nof desired attributes covered by the chosen product,\naveraged across all episodes, and (2) success rate,\ncalculated as the percentage of episodes where the\nchosen product satisfies all requirements.\nWe evaluated the agents using a set of 100 test in-\nstructions, comparing ReSpAct against ReAct. With the\nuser simulator, ReSpAct achieves a score of 32.7\nand a success rate of 12%, while with human user,\nit demonstrates significantly better performance,\nwith a score of 85.8 and a success rate of 50%."}, {"title": "5 Ablation Studies", "content": "5.1 User Simulator\nThis ablation study examines the impact of user\nassistance quality on AI agent performance in Alf-\nworld tasks. We simulated three user types: Helpful\nKnowledgeable, Helpful Perturbed, and Unhelpful.\nHowever,\nperformance degrades significantly with ambigu-\nous (52.9%) or misleading (32.09%) user assis-\ntance. See Appendix A.1 for more details.", "latex": []}, {"title": "5.2 ReSpAct-Inner Monologue", "content": "In Inner Monologue (IM), the agent's actions are\nmotivated by an \"inner monologue,\" introduced by\n(Huang et al., 2022), which serves as a form of\nself-communication to guide the agent's decision-\nmaking process. ReAct, on the other hand, intro-\nduced a more flexible and sparse form of reasoning\ntraces for decision-making. To understand the inter-\nplay of Reasoning, Dialog, and Action, we employ\nan IM-style variant of ReSpAct with a thought pat-\ntern composed of dense external feedback. Our\nablation corroborates the findings from (Yao et al.,\n2022b) where IM-style prompting struggles to com-\nplete tasks successfully. Comparing ReSpActwith\nits IM variant, ReSpAct significantly outperforms ReSpAct-IM\nacross all tasks. Although ReSpAct-\nIM allows for user guidance, it frequently becomes\noverly reliant on interaction, leading to unnecessary\ndialogue and inefficiencies in task completion (See.\nAppendix C.1). ReSpAct, by contrast, strikes a\nbetter balance between seeking feedback and main-"}, {"title": "5.3 Schema-Guided Conversational Agent", "content": "This ablation study investigates how guiding an\nagent's communication using a dialog act schema\nimpacts task efficiency and interaction quality in\nAlfworld. The dialog acts are derived from (Gella\net al., 2022), originally developed for human-robot\ndialog. The agent is guided to adhere to a pre-\ndefined set of dialog acts (e.g.,  See Appendix\n17 for the complete list). We observe that  dominates the dialog interac-\ntions, suggesting a focus on object location and\ndisambiguation tasks, while other acts are used\nless frequently. We also observe more variability\nin turn count and a marginal drop in performance.\nReSpAct is more efficient (SR \u2191, \u03bc\u2193) and consis-\ntent (\u03c3\u2193) by comparison. See Appendix B Table 8\nfor detailed analysis."}, {"title": "6 Conclusions", "content": "ReSpAct framework enables dynamic, context-\naware interactions that extend beyond basic\ncommand-response exchanges. By fostering mean-\ningful dialogue, this framework allows AI agents to\nnot only explain their decision-making processes\nbut also adapt their actions in response to user feed-\nback, transforming them into truly \"conversational\"\nagents. Such capabilities are crucial for creating\nmore intuitive, trustworthy, and effective AI assis-\ntants that can operate in complex, real-world sce-\nnarios. One can also incorporate stateful policies\nin ReSpAct for higher precision, such as asking\nto confirm all arguments of reservations before fi-\nnalizing them, or using a particular API for action\ndepending on the current state, similar to follow-\ning a dialogue flow. This is important for policy\nalignment of LLMs for task-completion."}, {"title": "7 Limitations", "content": "The ReSpAct method shows promise in integrating\nreasoning, speaking, and acting for task-oriented\nconversational agents, but it has limitations. The\nframework's effectiveness is validated on specific\nbenchmarks such as Alfworld, WebShop, and Mul-\ntiWOZ, which may not fully represent the variety\nof real-world tasks. The real world is a more com-\nplex, unstructured environment where user intent\nis more challenging to interpret. While our method\nhighlights how human feedback is critical for a rea-\nsoning agent's success in decision-making, over-\nreliance on user input can lead to inefficiencies\nthat potentially frustrate the user. Striking the right\nbalance between agent autonomy and user involve-\nment is still an open challenge and requires further\nresearch."}, {"title": "8 Impact Statement", "content": "The ReSpAct framework improves LLM-based\nagents by enabling interactive, policy-guided ac-\ntion determination while keeping humans in the\nloop. This approach enhances collaboration and\ntask success by ensuring agents seek clarification\nand guidance rather than acting on assumptions.\nHowever, increasing agent autonomy may intro-\nduce risks, such as over-reliance or security con-\ncerns in sensitive environments. ReSpAct mitigates\nthese risks by emphasizing human involvement and\ndynamic dialogue, promoting better alignment and\nsafety. Further research is needed to explore poten-\ntial challenges and ensure responsible AI use."}]}