{"title": "Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models", "authors": ["Alec Wright", "Alistair Carson", "Lauri Juvela"], "abstract": "This paper introduces Open-Amp, a synthetic data framework for generating large-scale and diverse audio effects data. Audio effects are relevant to many musical audio processing and Music Information Retrieval (MIR) tasks, such as modelling of analog audio effects, automatic mixing, tone matching and transcription. Existing audio effects datasets are limited in scope, usually including relatively few audio effects processors and a limited amount of input audio signals. Our proposed framework overcomes these issues, by crowdsourcing neural network emulations of guitar amplifiers and effects, created by users of open-source audio effects emulation software. This allows users of Open-Amp complete control over the input signals to be processed by the effects models, as well as providing high-quality emulations of hundreds of devices. Open-Amp can render audio online during training, allowing great flexibility in data augmentation. Our experiments show that using Open-Amp to train a guitar effects encoder achieves new state-of-the-art results on multiple guitar effects classification tasks. Furthermore, we train a one-to-many guitar effects model using Open-Amp, and use it to emulate unseen analog effects via manipulation of its learned latent space, indicating transferability to analog guitar effects data.", "sections": [{"title": "I. INTRODUCTION", "content": "Digital audio effects modelling is a field of research that aims to emulate analog audio effects processors digitally, allowing musicians to replace analog hardware with software. Popular targets for modelling include phasers [1], flangers [2], compressors [3], distortion pedals [4] and guitar amplifiers [5]. In recent years, machine learning techniques, specifically neural networks, have widely been applied to modelling of guitar amplifiers [6]\u2013[9], and other effects [10]\u2013[15].\nOne popular application of neural modelling techniques is guitar amplifier profiling or capture. This allows users to create an emulation of a guitar amplifier or pedal, by collecting training data from the target device. There are numerous commercial products available, both software and hardware based, that offer this as a feature, allowing non-expert users to create high-quality emulations for use in music performance and production contexts.\nOpen-source software for neural effects modelling is also available, allowing anyone with a personal computer, audio interface and internet connection to capture their own devices. Two popular examples are GuitarML [16] and Neural Amp Modeler [17]. Online communities have formed with users sharing their amp and effects pedal captures. We propose using these captures as part of a synthetic data framework, Open-Amp, augmenting audio that can subsequently be used for downstream tasks. This addresses a gap in the datasets and augmentation frameworks currently available, the lack of large-scale and diverse training data for audio effects.\nRecent work has highlighted the application of audio effects data for a range of tasks, for example in audio effect and mixing style transfer [18], [19], audio effect removal [20]\u2013[22] and zero-shot guitar amplifier modelling [23]. Additionally, it had been shown that for related tasks such as Automatic Music Transcription (AMT) for guitar, adding effects processing such as guitar amplifier emulations to training data can aid in performance and improve generalisation [24], [25].\nTypically audio effects data is either collected from physical devices, either manually [26], [27] or using mechanical actuators [28]. This however introduces issues related to scaling, as it requires access to and time to gather data from each physical device. More frequently, data is produced by applying digital audio effects as a kind of augmentation. Existing software, however, lacks the diverse range of effects processing devices used by musicians.\nThe contributions of this paper are as follows:\n\u2022\tRelease Open-Amp\u00b9 - a Python package which allows for online augmentation of any input audio with a diverse range of guitar amplifier and distortion effects models.\n\u2022\tUse Open-Amp to train a guitar effects encoder, and demonstrate transferability of the learned embeddings on downstream classification tasks\n\u2022\tCreate a one-to-many guitar effects model with Open- Amp, and use its learned latent space to enroll novel analog audio effects processing devices\nWith this, we show that Open-Amp is a flexible and efficient way of creating the large-scale and diverse audio effects data that is needed to create audio effect foundation models."}, {"title": "II. RELATED WORK", "content": "A. Data Augmentation\nThere are existing packages that apply a range of audio effects, for data augmentation and differentiable DSP purposes. Pedalboard [29] is an audio effects augmentation software package, that offers a range of basic audio effects, including distortion. The Differentiable Audio Signal Processors in Pytorch (DASP) [30] and Differentiable Digital Signal Processing (DDSP) [31] libraries offer a range of differentiable audio effects and common audio processors. TorchAudio [32] also offers commonly used audio transformations and effects.\nWhilst these packages have great utility in data augmentation pipelines, they all offer relatively simple implementations of audio effects. In practice, there are a vast range of products available for each class of audio effect that a musician might use. For example, searching any online music gear store for guitar amplifiers will reveal hundreds, if not thousands, of products on offer. These devices all have unique tonal characteristics. In contrast, the existing audio effects augmentation libraries offer relatively limited options for emulating guitar amplifiers, generally providing memoryless nonlinear functions and filters, which could be used to create block- based Wiener-Hammerstein models [33], for example.\nPedalboard can be used to load audio effects plugins, however this is not always practical for data augmentation purposes for two reasons, firstly, machine learning training is often carried out on Linux servers and the majority of plugins released do not include a Linux build. This does not prevent offline data generation on a different operating system, however it is often desirable to apply data augmentations on-the-fly during training. Secondly, whilst a diverse range of audio effects plugins are available, they are generally commercial products, so not readily available to researchers.\nB. Audio Effects Datasets\nThere are several existing datasets intended for use in modelling/classification of audio effects. The GUITAR-FX- DIST (GFX) dataset [34] consists of guitar processed with 13 different digital emulations of distortion effects. The EGFX dataset contains electric guitar processed by 12 different analog audio effects, either distortion, reverb, modulation or delay [35]. The EGDB dataset consists of electric guitar processed by various digital guitar amplifier models, and is intended for the task of automatic transcription [24]. The Amp-Space Dataset [36] also contains data collected from a diverse range of devices. However, there is a limited amount of diversity in the input signal processed by the devices.\nWhilst these datasets have been great contributions to the field of machine learning for audio effects, there are a number of challenges and limitations\nFirstly, existing datasets choose a collection of input guitar signals, such as single held notes (EGDB, GFX) or guitar playing (EGDB, amp-space), which are subsequently 'baked-in' to the dataset. This limits the domain that the dataset is applicable over. For example, it is possible that guitar effects models trained on single notes, perform poorly on polyphonic and other more complex guitar signals.\nSecondly, the diversity of devices is often quite limited, with relatively small collections of devices available, 13 for GFX, 12 for EGFX and 6 for EGDB, and 52 for amp-space. This, as mentioned earlier, is potentially insufficient when compared to the number and range of devices available.\nFinally, audio effects generally have multiple parameters. Guitar amplifiers typically have in excess of five parameters that control of timbre of the device. It is desirable to include a sufficient sampling of these parameters in any dataset produced [37]. This is challenging to achieve for analog audio effects, with existing research relying on mechanical automation [28], [36]. Other published datasets either do not vary the effect parameters (EGFX, EGDB), or are using digital emulations which allow for straightforward control over the effect parameters (GFX)."}, {"title": "III. EXPERIMENTS", "content": "Our proposed data synthesis framework primarily addresses the first two points raised in Sec. II-B. The input signal can be any audio signal chosen by the user, as the models themselves are available to render audio. The diversity of devices available is also very great, and likely to increase as more users create and upload models of their devices.\nFor the experiments presented in this paper, we used the \"Proteus Tone Packs\" collection from the Guitar ML tone library\u00b2. This collection of models consists of 59 guitar amplifier captures and 101 effects pedal captures. Sixty-five of the models include a single parameter, typically the gain, as a conditioning parameter. It should be noted, however, that additional models can be included using Open-Amp, for example using different architectures, and including even more devices. All models are single-layer LSTMs with hidden size 40 [7], [8].\nTo demonstrate the transferability of the Open-Amp framework, we use it to train neural network models to achieve two tasks, guitar distortion effects classification, and guitar effects emulation. For all experiments, the data was rendered online during training, utilising multi-process dataloading in PyTorch to ensure minimal batch loading time, with unprocessed electric guitar audio from the IDMT-Guitar transcription dataset [38] as input.\nA. Guitar Effects Classification\nWe trained a guitar effects encoder on data synthesised by Open-Amp, using a contrastive framework based on SimCLR [39]. The encoder architecture is similar to the 'Music Effects Encoder' used in [40]. It consists of one-dimensional convolutional blocks, with each block containing two convolutional layers with residual connection. Each convolutional layer is followed by a batch normalization layer and a ReLU activation. The output convolutional layer has 64 channels, and the encoding is averaged over time to produce the final embedding with 64 dimensions. The encoder has six blocks with kernel size of five and channels growing progressively from 16 to 64, giving a total of 112888 learnable parameters.\nTo generate each training batch, random 1-second clips are sampled from the input data. For each device, a positive pair consists of two different clips from the input dataset, processed by one of the effects models. This is repeated for a random selection of the available models. Negative examples then consists of all examples in the batch that were processed by different effects models. We used a batch size of 128 and trained for 200,000 iterations using the normalized temperature-scaled cross-entropy loss [39].\nWe evaluate our contrastively trained encoder on an existing classification task from the GFX dataset, using results from the original paper as a baseline [34]. This dataset consists of thirteen different distortion effects, applied to clean guitar. The effects are either applied to mono guitar (single string) or polyphonic guitar (two strings), and the effect parameter settings are either sampled from a discrete distribution of four possible values, or continuously, creating four subsets, Mono Continuous, Mono Discrete, Poly Continuous and Poly Discrete. In the original paper introducing this dataset, a convolutional neural network, FxNet, was trained to classify the effect class, with various combinations of training and test dataset presented to test generalisation.\nWe use our contrastively trained encoder to extract embeddings from the GFX dataset. We create two classifiers based on these embeddings, a k-nearest neighbours (KNN) classifier and a single layer MLP with hidden size 100 and Rectified Linear Unit (ReLU) non-linearity. We report the classifier accuracy in Table I, and compare it to the accuracy reported for the FxNet presented in [34]. Note that our contrastively trained encoder has not seen any data from the GFX dataset during training. For the KNN classifier, the performance is worse than the baseline, however typically by a small margin. The single-layer MLP however, is able to achieve better accuracy than the baseline FxNet in six out of eight cases. For comparison, the FxNet has a reported 760,000 learnable parameters, compared to our encoder which has just 112,888. This demonstrates the transferability of the encoder's learned embeddings, and thus of Open-Amp, to downstream tasks.\nAs a further ablation we carry out additional classification tasks, on four different datasets, the previously introduced EGFX, EGDB and GFX datasets (see Sec. II-B), as well as \"Open30\", a dataset consisting of guitar audio processed by 30 randomly selected devices from the Proteus Tone Pack, synthesised using Open-Amp. This dataset contains 1104 two-second clips from each target device, using data from the IDMT transcription dataset [38] as input. We then train three additional encoders using the contrastive framework described earlier on the previously introduced datasets, EGFX, EGDB and GFX. These encoders were trained for 100,000 iterations, with a batch size equal to the number of guitar effects contained in each dataset. These three encoders, in addition to a version of the Open-Amp encoder introduced earlier that was only trained for 100,000 iterations, were then used to extract encodings over the four datasets. These encodings were then used to fit a single-layer MLP classifier, with various permutations of training and test set. If the training and test sets are from the same dataset, a random class-balanced train-test split of 85-15 was used. The results are shown in Table II. The results show that the encoder trained using Open- Amp performs best on all datasets, further demonstrating transferability to unseen data.\nB. Guitar Effect Emulation\nHere we explore the application of Open-Amp to a one-to- many guitar effect emulation task. Recent work investigated this problem using a propriety dataset [23], but we demonstrate that a similar model can be trained using Open-Amp. This involved two stages: training a foundation model on Open- Amp synthetic devices then enrolling unseen analog devices into the model. Open-Amp effects models with a conditioning parameter were treated as five separate models, with the conditioning value linearly spaced from 0 to 1, resulting in a total of 394 synthetic devices.\n1) Foundation model: The foundation model architecture was a temporal convolutional network (TCN) conditioned with feature-wise linear modulation (FiLM) [23], [41], [42]. A single layer is shown in Fig. 1. This consisted of 2 TCN blocks each with $L = 8$ layers with $C = 16$ convolutional channels, a kernel size of $K = 3$ and dilation growth $D = 2$. The model was conditioned on a learnable look-up table with a unique embedding for each of the $M = 394$ (synthetic) devices seen during training. We trained three versions of the model with embedding dimensions of $E = \\{16,64, 256\\}$. The model was trained on an input signal of 35 minutes of (clean) audio data, giving a total of 230 hours of synthetically generated training data. The data was segmented into 2s clips, with batch size of $N = 16$, and the model trained for 8 epochs (approximately 60 hours on an NVIDIA GeForce GTX 1080). The loss function was the sum of the error-to-signal ratio (ESR) [6] and multi-resolution spectral loss (MRSL) [43].\nThe results on unseen test audio can be seen in Table III. Five devices from the training set were selected to show the spread of results: the best, 25th percentile, median, 75th percentile and worst performing devices from the Emb-64 model (in terms of combined ESR and MRSL). A baseline one-to-one TCN was trained for each device, with the results shown in the first column. For all devices the one-to-one model outperforms the one-to-many models, as was observed in [23]. In many cases however, the best performing one-to- many model (Emb-256) provides comparable results to the baseline and is at most 5dB (ESR) worse for the devices presented. Generally the results improve as the embedding dimension increases.\n2) Unseen device enrolment: To test the generality of the learned embedding space, we introduce unseen analog audio effects to the foundation models: namely three dis- tortion/overdrive pedals from the EGFX database [35]. The database contains 57 minutes of clean/processed audio for each device, from which we select a 90-5-5 train-validation- test split. For each device, we freeze all parameters in the foundation model except the embedding space, and fine-tune the model in a one-to-one fashion, therefore teaching the model to learn a new embedding for the unseen device. The initial embedding was set to the Open-Amp embedding which gave the lowest loss over the unseen data. As a baseline we train a fully-learnable one-to-one TCN for each device. We run the experiments on different subsets of the full training dataset, ranging from 3s (0.1%) to 53 minutes (100%).\nThe results in Fig. 2 show that the proposed embedding enrolment method can give comparable results to training a one-to-one model, especially for higher embedding dimensions and fewer training data. In many cases the discrepancies between the baseline and the embedding models are similar to those seen in Table III, indicating that the embedding space learned by the model from synthetic data can represent unseen analog audio effects to a similar degree of accuracy. As the duration of training data decreases, the performance of the proposed models converge with that of the baseline and in some case does slightly better. This suggests that this foundation-enrolment approach may be useful when training data scarcity is an issue."}, {"title": "IV. CONCLUSION", "content": "In this work, we presented Open-Amp, a large-scale synthetic audio effects augmentation framework. Utilising crowd- sourced models from open-source neural effect modelling soft- ware, Open-Amp allows for flexible online data augmentation with a diverse range of realistic distortion effect models. Our guitar effects encoder, trained with Open-Amp, outperforms classifiers trained on domain-specific audio effect datasets across various audio effects classification tasks, indicating that Open-Amp augmentation generalises well to unseen effects. Furthermore, we use Open-Amp to train a one-to-many guitar effects model, and use the learned latent space for enrolment of unseen devices. Our framework is suitable for online effects augmentation, enabling its integration into data augmentation pipelines and making it valuable for creating foundation mod- els in audio effects that can be used for a wide range of machine learning tasks. Future work will involve expanding Open-Amp to include more effects, adding variable sample rates to the models [44], as well as adding incorporating white-box models of common circuits, such as the tone-stack [45], [46], to further expand the range of augmentations available."}]}