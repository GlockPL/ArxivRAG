{"title": "Transcending Language Boundaries: Harnessing LLMs for\nLow-Resource Language Translation", "authors": ["Peng Shu", "Junhao Chen", "Zhengliang Liu", "Hui Wang", "Zihao Wu", "Tianyang Zhong", "Yiwei Li", "Huaqin Zhao", "Hanqi Jiang", "Yi Pan", "Yifan Zhou", "Constance Owl", "Xiaoming Zhai", "Ninghao Liu", "Claudio Saunt", "Tianming Liu"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a wide range\nof tasks and domains. However, their performance in low-resource language translation, partic-\nularly when translating into these languages, remains underexplored. This gap poses significant\nchallenges, as linguistic barriers hinder the cultural preservation and development of minority\ncommunities. To address this issue, this paper introduces a novel retrieval-based method that\nenhances translation quality for low-resource languages by focusing on key terms, which involves\ntranslating keywords and retrieving corresponding examples from existing data. To evaluate the\neffectiveness of this method, we conducted experiments translating from English into three low-\nresource languages: Cherokee, a critically endangered indigenous language of North America;\nTibetan, a historically and culturally significant language in Asia; and Manchu, a language with\nfew remaining speakers. Our comparison with the zero-shot performance of GPT-40 and LLAMA\n3.1 405B, highlights the significant challenges these models face when translating into low-resource\nlanguages. In contrast, our retrieval-based method shows promise in improving both word-level\naccuracy and overall semantic understanding by leveraging existing resources more effectively.", "sections": [{"title": "1 Introduction", "content": "Low-resource languages, often spoken by small and marginalized communities, face critical challenges\nin preservation, communication, and cultural transmission. Many of these languages lack extensive\nwritten documentation or digital resources, which hinders their use in critical domains like health-\ncare, education, and public services. The absence of adequate linguistic resources not only threatens\nthe survival of these languages but also creates barriers to services, often leaving native speakers dis-\nconnected from modern applications and opportunities. In healthcare, for example, the inability to\ncommunicate in one's native language can lead to misunderstandings between patients and medical\nprofessionals, potentially impacting the quality of care and patient outcomes. The same challenges\napply to legal, educational, and government services, where effective communication is essential. For\nthese communities, access to reliable and accurate translations in their native language is crucial.\nAddressing this problem with traditional methods of machine translation has proven challenging[43].\nHistorically, machine translation has been dominated by well-resourced languages like English, French,"}, {"title": "2 Related Work", "content": "LLMs have revolutionized NLP, with applications across diverse fields such as education, healthcare,\nrobotics, etc [30\u201332, 34, 40, 63, 73, 85]. The foundation of LLMs lies in the transformer architecture\nintroduced by Vaswani et al. [69]. With its self-attention mechanism, the transformer effectively\nhandles long-range dependencies, surpassing earlier models like Recurrent Neural Networks (RNNs) [6]\nand Long Short-Term Memory (LSTM) networks [18], which struggled with processing long sequences\nand parallelization. Transformers have achieved state-of-the-art (SOTA) performance on tasks such\nas machine translation. The BERT model [13], developed from Transformers, further enhanced the\nefficiency and scalability of NLP models through self-supervised pretraining.\nMore recently, generative language models, particularly the GPT series [4, 58], were the first to\ndemonstrate zero-shot and few-shot in-context learning abilities without being finetuned on specific\ndownstream tasks. These decoder-only models, pretrained on massive datasets from diverse sources,\nintegrate a wide range of world knowledge, enabling efficient autoregressive generation with emergent\nabilities and scaling to larger and more powerful models. These factors are key to today's LLMs.\nInstructGPT [54] refined the framework by incorporating reinforcement learning from human feedback\n(RLHF) [7], aligning model outputs more closely with human preferences.\nBuilding on these advancements, OpenAI released ChatGPT in 2022, marking a new milestone\nin LLM development. Its large-scale pretraining and RLHF mechanisms enabled it to excel in tasks\nsuch as question answering, summarization, and dialogue, making it widely popular for its human-like\nconversational abilities. Subsequent models like GPT-4 and GPT-4V further integrated multimodal\ncapabilities and enhanced reasoning abilities, enabling more complex tasks. Open-source LLMs such as\nLLAMA [67] and Mistral [26] have also demonstrated competitive performance, pushing LLMs toward\nmore open and widespread use in real-world applications. Indeed, LLMs and related vision-language\nmodels have demonstrated significant practical impact in diverse domains [11, 12, 23, 24, 36, 39, 42, 45\u2013\n50, 60, 66, 72, 76, 81, 83].\nWith these advancements, the integration of LLMs into various real-world applications has garnered\nincreasing attention. LLMs' human-like understanding and reasoning abilities pave the way toward\nArtificial General Intelligence (AGI) and can facilitate societal development across a wide range of\ndomains [32, 35, 44, 82\u201384]."}, {"title": "2.1 Large Language Models", "content": "Machine Translation for low-resource languages has been a long-standing challenge in the field of NLP.\nWhile machine translation for high-resource languages, such as English, Chinese, or Spanish, has seen\nconsiderable improvements, particularly with the advent of NMT techniques, low-resource languages\nhave lagged due to the scarcity of large parallel corpora and linguistic resources.\nEarly efforts in machine translation, particularly for low-resource languages, were based on rule-\nbased and SMT approaches. Rule-based systems relied on linguistic rules crafted by experts and\nextensive lexicons, but they were often labor-intensive and brittle when applied to complex languages.\nSMT, introduced in the 1990s, offered a data-driven approach, where models learned translation prob-\nabilities from bilingual text corpora. However, SMT required large volumes of parallel data, which\nwas a significant limitation for low-resource languages, and its reliance on phrase-based techniques\noften failed to capture complex linguistic phenomena like morphology and syntax in underrepresented\nlanguages.\nWith the advent of Neural Machine Translation (NMT), particularly sequence-to-sequence (Seq2Seq)\nmodels with attention mechanisms, the field of Machine Translation entered a new era. NMT demon-\nstrated superior performance over SMT in many language pairs by leveraging deep learning to create\nricher representations of source and target sentences. However, NMT's effectiveness heavily depends on"}, {"title": "2.2 Machine Translation on Low-Resource Language", "content": "the availability of large datasets, making it less applicable to low-resource settings. Researchers began\nexploring techniques to alleviate the data scarcity issue. Transfer learning became a popular method,\nwhere models pretrained on high-resource languages are fine-tuned on low-resource languages. This\ntechnique allowed low-resource languages to benefit from knowledge gained from related high-resource\nlanguages. Multilingual NMT further extended this idea, training models to translate between multiple\nlanguages simultaneously. This method improved performance for low-resource languages by allowing\nthem to share representations with high-resource languages, leveraging multilingual data in a shared\nmodel architecture.\nData augmentation techniques, such as back-translation, have become widely used for low-resource\nMachine Translation. In back-translation, a model is used to translate monolingual target language\ndata into the source language, thereby creating pseudo-parallel corpora. This approach significantly\nboosts training data for low-resource pairs and has been shown to improve translation quality in lan-\nguages with limited direct parallel data. Additionally, unsupervised machine translation emerged as a\npromising approach for low-resource languages by eliminating the need for parallel data entirely. Unsu-\npervised Machine Translation leverages monolingual corpora in both the source and target languages\nand learns to align and translate between them using iterative back-translation and shared latent\nrepresentations. However, while these methods show promise, they still struggle with low-resource\nlanguages that have extremely limited or non-existent monolingual resources.\nMore recently, LLMs such as GPT-3, GPT-4, and LLaMA have emerged as powerful tools for a\nrange of NLP tasks, including translation. These models, with billions of parameters and trained\non extensive multilingual data, have shown promise in addressing low-resource translation through\nprompt-based methods. By employing prompt engineering, LLMs can perform zero-shot or few-shot\ntranslation tasks by conditioning on examples or instructions provided as input prompts. However,\ntheir performance on low-resource languages, particularly those with complex linguistic features or\nlimited presence in training data (e.g., indigenous languages like Cherokee), remains an area of active\nresearch."}, {"title": "3 Low-Resource Language", "content": "In this paper, we focus on one Native American low-resource language: Cherokee. We also design a\nLLMs empowered Machine Translation task based on Cherokee. The Cherokee language, known as\nTsalagi gawonihisdi in its native form, belongs to the Iroquoian language family. It is primarily spoken\nby the Cherokee people, whose traditional homelands are in what is now the southeastern United\nStates. Today, as a result of the federal policy to deport Native peoples from their homes east of the\nMississippi River in the 1830s, there are two Cherokee nations in Oklahoma (the Cherokee Nation\nand the United Keetoowah Band of Cherokee Indians) and one in North Carolina (the Eastern Band\nof Cherokee Indians). Despite centuries of colonial pressures, the Cherokee language has persisted\nthrough various revitalization efforts. However, it still remains classified as a critically endangered\nlanguage according to UNESCO [77].\nThe historical trajectory of the Cherokee language is inseparable from the history of the Cherokee\npeople. Before European contact, Cherokee was an exclusively oral language [78]. The Cherokee\nsyllabary, created in the early 19th century by Sequoyah (also known as George Guess), revolutionized\nthe language by providing a standardized writing system. The syllabary consists of 85 symbols, each\nrepresenting a syllable, making it fundamentally different from an alphabetic system like English, where\nletters represent individual phonemes. Cherokee is a polysynthetic language, meaning that words often\nconsist of multiple morphemes that combine to convey complex meanings. In Cherokee, single words\ncan function as complete sentences in English. When discussing parts of speech, verbs are the most\nmorphologically complex part of the Cherokee language. A typical Cherokee verb consists of several\ncomponents:"}, {"title": "3.1 Cherokee", "content": "\u2022 Pronominal prefix, which indicates the subject or object\n\u2022 Prepronominal prefix, that provides additional information such as tense, aspect, or negation\n\u2022 Verb root, the core meaning of the verb\n\u2022 Aspect markers, indicating whether an action is completed or ongoing\n\u2022 Directional suffixes, specifying the direction of the action\nNouns in Cherokee however, are typically simpler than verbs. They may consist of a root and optional\nsuffixes. But they do not show the same level of inflection as verbs. Cherokee does not mark nouns for\ngender or case, although it does differentiate between animate and inanimate entities. Cherokee syntax\nfollows a verb-final word order (SOV). In other words, Cherokee sentences typically have the subject\nfirst, followed by the object, and then the verb. However, Cherokee exhibits considerable flexibility\nin its word order due to the rich morphological marking of verbs. It allows speakers to emphasize\ndifferent parts of a sentence by rearranging the word order without losing clarity.\nThe Cherokee language is crucial for preserving cultural identity and heritage, as it carries tra-\nditional knowledge, oral stories, and ceremonies that define the Cherokee people. It has historical\nsignificance due to the creation of the Cherokee syllabary, which empowered the Cherokee Nation in\nthe colonial era and remains a symbol of resilience. Cherokee contributes to linguistic diversity, offer-\ning insights into unique grammatical structures and language typology. It also hold valuable culture\nknowledge. Efforts to revitalize the language support indigenous sovereignty and ensure intergenera-\ntional communication, allowing future generations to retain their cultural roots. We hope by alleviating\nLLMs, linguists and computer science researchers can contribute to revitalize Cherokee. It is one of\nthe most vital goals for this paper."}, {"title": "3.2 Tibetan", "content": "Tibetan, a language with a rich historical legacy, is first attested from the mid-seventh century CE\nduring the Tibetan Empire, which adopted the Chinese style of documenting political events. The\nearliest form, known as Old Tibetan, is found in the empire's documents and inscriptions, while\nClassical Tibetan emerged after a significant language reform and standardization in the ninth century\n[65].\nMorphologically, Tibetan shows both simplicity and complexity. Many basic vocabulary items are\nmonosyllabic, reflecting its Sino-Tibetan roots, but the language also contains a significant number of\nbimorphemic and disyllabic nouns. Compound nouns and light verb constructions (Noun + Verb com-\nbinations) are prevalent, and while they are common in modern Tibetan, they do not form compounds\nin the same formal sense as compound nouns [3].\nTibetan utilizes tones to differentiate words that are otherwise phonetically identical, although the\npresence and nature of tones vary by dialect. Classical Tibetan, from which modern Tibetan dialects\nevolved, was not tonal, and the development of tones is a relatively recent phenomenon in the language's\nevolution. Tibetan is primarily a Subject-Object-Verb (SOV) language. It uses postpositions rather\nthan prepositions, and word order plays a critical role in sentence structure. The language has an\nergative-absolutive alignment, which is evident in its case marking and verb agreement patterns.\nTibetan verbs are often complex and consist of several components, particularly in classical and\nliterary forms. A typical Tibetan verb consists of the following components:"}, {"title": "3.3 Manchu", "content": "Manchu, part of the Tungusic language family, was historically the language of the ruling elite of\nthe Qing Dynasty, which governed China from 1644 to 1912. The Manchu language, spoken by the\nManchu people, was used extensively in the imperial court for religious ceremonies, diplomatic and\nideological discourse, and for communicating with bannermen, nobility, and military officials [9]. Today,\nhowever, Manchu is considered a highly endangered language, with fewer than 100 native speakers left,\npredominantly among older generations in the remote regions of northeast China [68]. In Heilongjiang,\nthe northernmost province of Manchuria, Manchu was still spoken by a considerable population at the\nstart of the twentieth century. By the century's end, however, only a few small groups of middle-aged\nand elderly speakers remained, and the outlook for the language's continued survival appears bleak [53].\nThe language faces a severe risk of extinction due to limited transmission across generations and the\nincreasing dominance of Mandarin Chinese in daily life and formal education.\nThe historical development of Manchu is closely tied to the Manchu people's rise to power and the\nsubsequent cultural assimilation policies of the Qing Dynasty. Originally a language with no written\nform, Manchu adopted a script based on Mongolian in the 17th century. This script is vertically\noriented and bears no resemblance to Chinese writing systems, making it unique among the major\nlanguages of the region. The script consists of syllables represented by various glyphs, creating a\nlogographic system distinct from alphabetic languages like English.\nLinguistically, Manchu is an agglutinative language with a Subject-Object-Verb (SOV) word order,\nmeaning that verbs typically come at the end of a sentence. It employs vowel harmony and features\ncomplex morphological rules, which allow for a large variety of meanings to be expressed through\nprefixes and suffixes attached to a verb root. Manchu verbs, much like those in polysynthetic languages\nsuch as Cherokee, carry substantial grammatical information, including tense, aspect, mood, and voice.\nA typical Manchu verb may include multiple morphemes that together convey complex meanings, a\nfeature common to languages in the Tungusic family.\nA typical Manchu verb consists of several components:"}, {"title": "\u2022 Pronominal prefix, which indicates the person (subject or object) involved in the action", "content": "\u2022 Verb root, which provides the main action or meaning of the verb\n\u2022 Aspect markers, indicating whether an action is completed, ongoing, or repetitive\n\u2022 Mood markers, which express the speaker's attitude toward the action (e.g., imperative, inter-\nrogative)\n\u2022 Negative particles, which are added to negate the action of the verb\n\u2022 Directional suffixes, specifying the direction or orientation of the action (e.g., toward or away\nfrom the speaker)\nManchu nouns, however, are simpler than verbs in terms of morphology. Unlike verbs, Manchu\nnouns do not have extensive inflection, and they are not marked for gender or number. However,\nManchu makes a distinction between animate and inanimate entities, which is reflected in both the\nsyntax and verb conjugation rules of the language. Additionally, Manchu nouns are often modified\nby possessive suffixes and directional markers, allowing for precise expression of spatial and relational\nconcepts.\nThe Manchu language is critical for understanding the culture and governance of the Qing Dynasty.\nThousands of historical documents, including military records, imperial edicts, and literary texts, were\nwritten in Manchu. Many of these documents remain untranslated, making proficiency in Manchu\nessential for historians studying Qing Dynasty China. Manchu also holds significant cultural value for\nthe Manchu people, preserving traditional knowledge, oral histories, and religious practices.\nThe challenges facing the revitalization of Manchu are similar to those encountered by other en-\ndangered languages, including limited resources, a dwindling speaker base, and the overwhelming\ndominance of global languages like Chinese and English. However, the continued interest from schol-\nars and the rise of community-driven language preservation efforts offer hope for the survival of this\nhistorically significant language."}, {"title": "3.4 Techniques Leveraging LLMs for Low-Resource Machine Translation", "content": "Fluent Cherokee speakers are few in number, and while there are multiple initiatives to teach the\nlanguage to Cherokee school children, it is a race against time to preserve the language and the\ncultural knowledge it contains before another generation passes away. Today, translation from Cherokee\nto English is a laborious process that involves transliterating the syllabary into the Latin Alphabet,\nproducing a rough English analog, and then working with Cherokee elders to convey the cultural\nsubtleties of the original text. With the recent, rapid technological advances, however, LLMs may\naid the process of translation in both directions, with the goal of producing passable texts that fluent\nspeakers can then polish."}, {"title": "3.4.1 Zero-shot and Few-shots Translation", "content": "Due to a lack of training data for low-resource languages, most LLMs cannot properly translate low-\nresource languages. Moreover, it is hard to finetune these LLM without sufficient language resources.\nThus, various research efforts focus on improving LLMs to perform translation tasks with minimal or\nno task-specific training, commonly referred to as zero-shot and few-shot learning.\nZero-shot translation refers to the capability of LLMs to translate between language pairs with-\nout having seen explicit examples of these translations during training. For instance, Zhang et al.\nproposed strategies such as random online back-translation and language-specific modeling, improv-\ning zero-shot performance in multilingual NMT by approximately 10 BLEU score [75]. Gao et al.\nintroduced Cross-lingual Consistency Regularization (CrossConST), which enhances zero-shot perfor-\nmance by bridging the representation gap across languages, proving effective in both high-resource\nand low-resource language settings [17]. Although zero-shot translation can provide good results for\ncertain languages, particularly when leveraging shared linguistic patterns from pretraining on multi-\nlingual corpora, it basically leverages well-resourced languages (often English) to facilitate translation\nbetween low-resource language pairs. Thus, the performance will degrade when applied to highly\nunderrepresented languages. This highlights the need for additional strategies such as fine-tuning or\nincorporating domain-specific data.\nCompared with no training examples in zero-shot translation, few-shot translation involves expos-\ning a model to a small number of translation examples before performing the task. In this scenario,\nLLMs can adapt quickly to specific language pairs or domains with minimal data, offering a highly\nflexible solution for low-resource languages. Zhang et al. explored few-shot learning in machine trans-\nlation using LLMs [80]. They compared prompting, few-shot learning, and fine-tuning approaches,\nfinding that few-shot learning often outperforms zero-shot prompting when the model is given even a\nfew translation examples. This research highlighted the flexibility and efficiency of LLMs, especially\nwhen fine-tuned using the QLORA method, which allows the models to adapt to machine translation\ntasks with minimal data and memory usage. The study by Cahyawijaya et al. extensively explores\nin-context learning (ICL) and cross-lingual in-context learning (X-ICL), focusing on their application\nto low-resource languages [5]. This work highlights the effectiveness of using in-context examples from\nhigh-resource languages to perform translation tasks in low-resource languages. By providing seman-\ntically aligned in-context examples, LLMs can bridge the language gap and enhance understanding\nof low-resource languages without fine-tuning. However, they also point out that label alignment can\nsometimes degrade performance in low-resource languages, and propose an alternative approach called\nquery alignment, which focuses on aligning the input distribution rather than the labels. Additionally,\nGuo et al. proposed the Talent method, which utilizes a textbook-based learning approach to improve\nLLM translation performance on low-resource languages [19]. By creating structured textbooks con-\ntaining vocabulary lists and syntax patterns, LLMs are guided to absorb this knowledge before the\ntranslation task, significantly improving performance in few-shot settings for low-resource languages"}, {"title": "3.4.2 Multilingual Pretrained Model", "content": "LLMs such as mBART [38], MarianMT [27], and T5 [59] are pretrained on multilingual corpora\nutilizing techniques such as masked language modeling (MLM) or denoising autoencoding. These\napproaches facilitate the learning of shared representations across diverse languages. By leveraging\nthis multilingual knowledge, these models demonstrate an ability to transfer learned representations\nto low-resource languages, thereby enabling effective cross-lingual generation. This transfer learning"}, {"title": "3.4.3 Fine-tuning LLMs", "content": "One effective approach to adapting a pretrained multilingual model while mitigating the risk of over-\nfitting is the use of adapters [21]. Adapters are compact, trainable neural network modules that are\ninserted between the layers of a pretrained model. These modules allow the model to be fine-tuned\nfor new tasks or languages without requiring the retraining of the entire network, thus preserving the\ngeneral knowledge captured during pretraining. Adapter fine-tuning is particularly advantageous for\nlow-resource machine translation tasks, where large datasets are not readily available. Adapters enable\nthe model to leverage the knowledge from high-resource languages while being fine-tuned on smaller,\ntask-specific datasets, effectively reducing the risk of overfitting. By adjusting only a small number of\nparameters, adapters maintain the computational efficiency of the base model, making them suitable\nfor scenarios with limited computational resources.\nAmong all types of fine-tuning methods, Low-Rank Adaptation (LoRA) fine-tunning [22] is one of\nthe most successful strategies. LoRA is dessigned to reduce the computational overhead and memory\nrequirements when fine-tuning large models. Instead of updating all the parameters of the LLM,\nLORA introduces low-rank matrices into the model's architecture, allowing adaptation of only a subset\nof parameters while keeping most of the model's original weights frozen. LoRA proposes an efficient\nmethod for fine-tuning large-scale pretrained models by leveraging matrix decomposition. Specifically,\nLORA decomposes the weight matrices of the model into lower-dimensional matrices, and during the"}, {"title": "3.4.4 Back-Translation", "content": "To overcome the scarce availability of large parallel corpora researchers have developed back-translation,\na widely adopted data augmentation technique that leverages monolingual data to improve transla-\ntion performance. Back-translation is a semi-supervised learning technique used in NMT to generate\nadditional synthetic parallel data. The process involves training a model to translate from the target\nlanguage to the source language and using this model to translate monolingual data in the target\nlanguage back to the source language. This synthetic source-target data pair is then used to augment\nthe original parallel dataset, thus improving the performance of the translation model in the source-\nto-target direction. LLMs can also leverage back-translation by fine-tuning a model to first generate\nhigh-resource language translations. Since evaluating the accuracy and quality of high-resource lan-\nguages is significantly more feasible compared to low-resource languages, this approach ensures the\ngeneration of abundant, high-quality augmented data for low-resource language tasks. Among Back-\nTranslation, several variations of back-translation have been developed to enhance its effectiveness.\nThe most common and straightforward form of back-translation involves using a single translation\nmodel to generate synthetic source sentences from monolingual target data. Previously this model\nis simple but suffers from overfitting to the synthetic data if not properly balanced with authentic\nparallel data. However, LLMs serve as a highly effective alternative due to their extensive and versatile\nknowledge base, which enables them to outperform previous simple model with high accuracy and\nadaptability.\nIterative back-translation is an extension of the basic technique where the forward and backward\nmodels are trained in tandem, improving each other's performance over successive iterations. It in-\nvolves several steps:(1) Train an initial forward translation model from source to target language using\nthe available parallel data. (2) Train the reverse model from target to source using the parallel data.\n(3) Generate synthetic data using the reverse model and retrain the forward model with the aug-\nmented dataset. (4) Generate synthetic target sentences using the updated forward model and retrain\nthe reverse model with this new data. This iterative process continues until the model performance\nconverges, leading to better generalization as both models reinforce each other through the generation\nof progressively higher-quality synthetic data. Fine-tuned LLMs are particularly well-suited for iter-\native back-translation due to their ability to capture complex linguistic patterns and context across\nlanguages. Their extensive pre-trained knowledge allows for nuanced understanding and generation\nof text, making them ideal for producing accurate and contextually appropriate translations, even\nin low-resource language settings. Additionally, their adaptability enables continuous improvement\nthrough fine-tuning, further enhancing their performance in iterative back-translation tasks."}, {"title": "3.4.5 Retrieval-Augmented Generation", "content": "Retrieval-Augmented Generation (RAG) typically involves three key stages: Indexing, Retrieval, and\nGeneration [33, 51]. Indexing aims to transform raw data into a vectorized database. This stage begins\nby converting external data\u2014ranging from structured to unstructured formats\u2014into a standardized\nformat. An embedding model then encodes the processed data into smaller chunks, which are stored\nin the vectorized database. Indexing establishes a robust foundation for efficient and precise retrieval.\nRetrieval involves applying RAG algorithms to search for and expand the prompt. In this stage,\nthe user's initial query is encoded into an input vector using the same embedding model utilized\nduring Indexing. The system then computes the similarity between the input vector and the stored\nchunks, selecting the top K most relevant chunks based on their proximity. Generation utilizes a"}, {"title": "3.5 LLMs Empowered Machine Translation Task", "content": "We implement a RAG LLM low resource language translator by combining retrieval-based techniques\nwith LLMs to ensure accurate and context-aware translations. The overall architecture is shown\nin Figure 1. The system utilizes dictionary entries, which are indexed through two complementary\napproaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings\nin systems refer to a process where keywords are linked directly to the documents or data entries\nthat contain or are relevant to those keywords. The keyword retriever will retrieve corresponding\ndocuments according to the key-to-document mapping, if the keyword is inside our storage. The\nvector embedding indexing process organizes raw linguistic data into retrievable units by associating\nwords with dictionary definitions and using text-embedding-ada-002 model to encode the text into\nhigh-dimensional vectors that capture semantic relationships beyond mere surface forms."}, {"title": "3.5.1 Methodology", "content": "synthetic prompt formed by combining the user's query with the retrieved documents. This enriched\nprompt provides the LLM with additional contextual information, which helps mitigate hallucinations\nor constrain the generated responses. Consequently, the Generation stage enhances the accuracy and\nreliability of the model's outputs.\nAdvanced RAG methods have been developed to address the limitations of naive RAG by optimizing\nvarious components, including query prompts [16, 51, 57], indexing structures, similarity calculations,\nand prompt integration. Additionally, some advanced approaches leverage retrieval data to enhance\ntraining datasets, particularly in low-resource domains.\nFor instance, [62] retrieves relevant instances and uses large language models (LLMs) to gener-\nate new samples that integrate both original and retrieved data, thereby addressing data scarcity in\nspecialized domains. [56] involves utilizing retriever models to identify relevant samples and expand\nthe set of positive examples within privacy policy question-answering datasets, enhancing the training\nprocess. However, these methods often rely on external data, which poses challenges when the data is\nsensitive and cannot be accessed due to privacy concerns, or when computational resources are limited."}, {"title": "3.5.2 Testing Data", "content": "Given the extreme scarcity of parallel translations between Cherokee and English literary works, this\nexperiment utilizes three Cherokee literary sources that have available translations: the New Testa-\nment, Peter Parley's Geography, and The Pilgrim's Progress. These texts provide a rare opportunity\nto assess translation performance, as they offer both Cherokee and English versions for comparative\nanalysis.\nCherokee New Testament is the largest single book written in Cherokee. This translation of the New\nTestament of the Bible into the Cherokee language, using the Cherokee syllabary, was completed in the\nearly 19th century. It contains the verse written in Cherokee using Cherokee syllabary and Cherokee\nLatin transliteration, which provides the pronunciation of the Cherokee syllabary text using Latin\nletters. The translation was spearheaded by Samuel Worcester, a missionary, and Elias Boudinot,\na prominent Cherokee, with significant involvement from Cherokee speakers. The Cherokee New\nTestament was printed and widely distributed by the Cherokee Nation and became a vital text in\npreserving the Cherokee language and literacy among Cherokee people. Given its importance in\nCherokee history and its length compared to other works in Cherokee, it is considered one of the most\nsubstantial works written in the language.\nPeter Parley's Geography, written by Samuel Griswold Goodrich under the pseudonym Peter Parley,\nwas a popular 19th-century educational book designed to teach geography to young readers. First\npublished in the early 1820s, the book introduces geographical concepts in an accessible and engaging\nmanner, using illustrations, maps, and simple language to explain the physical world, cultures, and\nplaces beyond the reader's immediate surroundings. Notably, Peter Parley's Geography was one of\nthe earliest geography textbooks aimed at children, emphasizing both geographical knowledge and\nmoral lessons. Its influence was significant during the 19th century, and its translation into Cherokee\nexemplifies the efforts to provide educational material to indigenous populations, reflecting a period\nof educational reform and expansion.\nThe Pilgrim's Progress is one of the most significant works in English literature and Christian\nallegory written by John Bunyan and first published in 1678. The book narrates the journey of its\nprotagonist, Christian, from his home in the \"City of Destruction\" to the \"Celestial City,\" symbolizing\nthe believer's path to salvation. Divided into two parts, the story combines spiritual lessons with vivid\ncharacters and settings, reflecting the struggles, temptations, and rewards of living a faithful Christian\nlife.\nWe conducted the translation for a transcript from the Harris-Trump presidential debate to assess\nthe performance of our model in the context of news and current events [20]. This evaluation aimed to\ndetermine how well the system could handle complex, real-world content and provide timely informa-\ntion to Cherokee speakers. The debate transcript was chosen for its rich linguistic features, including\npolitical terminology, colloquial expressions, and nuanced argumentation.\nWe also utilize the New Testament to evaluate our model on Tibetan, a language of significant\nhistorical and cultural importance in Asia, and Manchu, a critically endangered language with less\nthan 100 speakers [74]. Due to a lack of resources, the Bible is one of the only available materials\nacross Cherokee, Manchu, and Tibetan languages. We test Tibetan with This evaluation seeks to\ndetermine whether our model is capable of effectively handling other low-resource languages, further\nvalidating its generalizability and performance beyond Cherokee. By extending our analysis to these\nlanguages, we aim to explore the model's robustness in addressing the unique challenges posed by\ndifferent low-resource linguistic contexts."}, {"title": "3.6 Test Design and Evaluation", "content": "To evaluate the performance and capabilities of our model and original LLMs in low-resource ma-\nchine translation, we conducted an experiment comparing our model with GPT-40 and LLaMA 3.1\n405B models. The objective of the experiment was to assess the translation abilities of these mod-\nels in generating low-resource languages, focusing primarily on Cherokee translations from English\nsource sentences. Specifically, we randomly selected text from three test datasets for Cherokee and"}, {"title": "4 Experiment Results", "content": "We show our evaluation results in table 1. Figure 2 to 4 show three examples applying LLMs for each\nCherokee literary source. We then exhibit translation of 2024 president debate for both presidential\ncandidates in figure 5 and 6. Finally, figure 7 and 8 include one test example for Tibetan and Manchu\ntranslation. Following [61], we use the transliteration of Manchu to the Latin alphabet when generating\nthe Manchu translation. The complete evaluation examples and more details are shown in the appendix\nA.1."}, {"title": "The formula for the normalized human evaluation score is shown as following:", "content": "Human Evaluation = \n$\\frac{1}{n} \\sum_{i=1}^{n} \\frac{Fluency_i + Grammaticality_i + Faithfulness_i}{5 \\times 3}$\n(1)\nIn this formula, n is the number of sentences, and the sum of full grade of fluency, grammaticality and\nfaithfulness, 5 point each, is 15 in total. Fluencyi, Grammaticality, and Faithfulness; are the expert\nscores for sentence i in each respective dimension."}, {"title": ""}]}