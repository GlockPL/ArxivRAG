{"title": "GeoTransformer: Enhancing Urban Forecasting with Geospatial Attention Mechanisms", "authors": ["Yuhao Jia", "Zile Wu", "Shengao Yi", "Yifei Sun"], "abstract": "Recent advancements have focused on encoding urban spatial information into high-dimensional spaces, with notable efforts dedicated to integrating sociodemographic data and satellite imagery. These efforts have established foundational models in this field. However, the effective utilization of these spatial representations for urban forecasting applications remains under-explored. To address this gap, we introduce GeoTransformer, a novel structure that synergizes the Transformer architecture with geospatial statistics prior. GeoTransformer employs an innovative geospatial attention mechanism to incorporate extensive urban information and spatial dependencies into a unified predictive model. Specifically, we compute geospatial weighted attention scores between the target region and surrounding regions and leverage the integrated urban information for predictions. Extensive experiments on GDP and ride-share demand prediction tasks demonstrate that GeoTransformer significantly outperforms existing baseline models, showcasing its potential to enhance urban forecasting tasks.", "sections": [{"title": "Introduction", "content": "In urban forecasting tasks involving predicting economic indicators and human mobility, classical methods usually rely on low-dimensional numeric data, which includes Point of Interest (POI) data(Li et al. 2022), survey data(Roskam and Kunst 2008; DHS et al. 2013), GPS records (Wang et al. 2023a; Moreira-Matias et al. 2013; Pappalardo et al. 2013), demographic census, geospatial features and so forth.\nRecently, many researchers have explored using high-dimensional information to fully capture the complexities of urban dynamics. A main direction is utilizing urban imagery for feature extraction and prediction (Ayush et al. 2020; Li et al. 2022; Yeh et al. 2020; Abdelhalim and Zhao 2024). A more advanced direction is encoding the urban information to a high-dimensional space representation, either through spatial representation learning (Mai, Li, and Lao 2023; Wu et al. 2024) or fusing satellite images and sociodemographic data (Wang et al. 2024). While these studies have established frameworks for encoding urban information, there has been limited investigation into optimizing these high-dimensional data for urban forecasting.\nNumerous established methods in natural language processing (NLP) and computer vision have effectively harnessed high-dimensional data. Particularly, Transformer architecture and attention mechanisms have proven highly effective in decoding high-dimensional representations of natural language (Vaswani et al. 2017a; Devlin et al. 2018; Radford et al. 2018) and images (Dosovitskiy et al. 2020a; Rombach et al. 2022). Urban representations, characterized by high dimensionality and spatial correlations, are well-suited for applying similar decoding methods. Some studies have explored the use of attention mechanisms to capture spatial dependencies, particularly through Graph Attention Networks (GAT) (Feng and Tassiulas 2022; Zheng et al. 2020). However, these approaches primarily depend on graph structures and spatial embeddings. In fact, incorporating geospatial statistical priors might be more effective in encoding spatial dependencies, potentially reducing the need for complex data structuring and simplifying the system design.\nTo address this issue, we propose GeoTransformer, a deep architecture akin to Transformer (Vaswani et al. 2017b) that combines geospatial characteristics prior to the attention mechanism. The novel geospatial attention mechanism calculates attention between different urban regions weighted by distance, incorporating information from surrounding areas. This approach implicitly includes spatial dependencies in the attention mechanism. Extensive experiments demonstrate that our model significantly outperforms several baseline methods in predicting GDP and ride-share demand.\nOur main contributions can be summarized as follows:\n\u2022 We propose GeoTransformer, a transformer-akin deep architecture designed for decoding high-dimensional urban representations and enhancing prediction accuracy.\n\u2022 We introduce a geospatial attention mechanism that prioritizes attention between regions weighted by spatial proximity, assigning higher weights to closer regions.\n\u2022 Experiments demonstrate that our method significantly outperforms baseline models in tasks of GDP and ride-share demand prediction."}, {"title": "Related Work", "content": "High Dimensional Input for Urban Forecasting\nUrban forecasting leverages multiple data sources and advanced computational techniques to address challenges faced by cities. Traditional methods usually adopt low-dimensional numeric data for prediction. In recent years, with the need rising for complex prediction tasks and the development of deep learning techniques, researchers have explored various ways to leverage high-dimensional urban information. One direction is directly using various urban imagery for feature extraction and prediction. Albert, Kaur, and Gonzalez employ Convolutional Neural Networks (CNNs) to detect land-use patterns from satellite images. Ayush et al.; Li et al.; Yeh et al. utilize urban imagery to predict socioeconomic status. Abdelhalim and Zhao leverage roadside imagery to predict transit travel time.\nRecently, encoding urban information into a high-dimensional representation marks a new direction in enhancing flexibility and prediction accuracy for different urban forecasting tasks. Mai, Li, and Lao develop spatial representation learning (SRL), utilizing deep neural networks (DNNs) to encode and feature spatial data in the forms of points, polylines, polygons, graphs, etc. Wang et al. propose a theoretical framework deep hybrid model (DHM) fusing demographic numeric data and satellite images into urban latent representations. However, how to leverage these urban representations more effectively in prediction tasks remains unexplored.\nTransformer and attention mechanism(Vaswani 2017) has been widely adopted for its efficiency in capturing relationships within high-dimensional representations. They are first proposed to solve natural language understanding and then visual models like ViT(Dosovitskiy et al. 2020b) and Stable Diffusion (Rombach et al. 2022) also utilize attention mechanisms to comprehend images and the spatial relationships between image patches. Given its strength in handling high-dimensional data and capturing spatial relationships, the attention mechanism is well-suited for adaptation in modeling urban high-dimensional representations.\nApproaches to Modeling Spatial Dependency\nSpatial dependency has traditionally been addressed using two primary approaches. First, feature engineering methods like k-nearest neighbors (KNN) are employed to convert surrounding spatial information into features. Second, spatial statistical methods such as spatial error regression, spatial lag models(Anselin 2013), and geographically weighted regression (GWR) (Fotheringham, Charlton, and Brunsdon 1998) have been widely used to account for spatial autocorrelation.\nIn recent years, Graph Neural Networks (GNNs) have become a widely adopted method for capturing spatial dependencies, especially in spatiotemporal prediction(Wang et al. 2023b; Zhuang et al. 2023; Cai et al. 2020). Urban built environment data such as road networks, are inherently suitable for representation as graph structures due to their topological properties. However, grid-based high-dimensional urban representations often lack clear graph-like structures, making graph construction particularly challenging. Moreover, the intrinsic complexity of such high-dimensional data can further hinder the effective training of models.\nRecent advancements have seen the integration of graph structures with attention mechanisms (Zheng et al. 2020; Feng and Tassiulas 2022), particularly through Graph Attention Networks (GATs)(Zhang, Yu, and Liu 2019). While these models incorporate spatiotemporal attention mechanisms, their reliance on graph structures and spatial embeddings remains significant for capturing spatial dependencies. Integrating spatial statistical priors could simplify the model's reliance on complex data structures and streamline system design. Tobler's First Law of Geography (Tobler 1970), which posits that closer entities are more likely to be related, could provide a valuable framework for capturing spatial dependency. Despite its potential, the integration of spatial statistical priors with attention mechanisms remains unexplored.\nTherefore, in this study, we aim to introduce an attention mechanism guided by spatial statistical priors as a new paradigm for effectively leveraging high-dimensional urban representation data.\nEconomic and Mobility Indicators Prediction\nGross Domestic Product (GDP) is a critical indicator in urban studies, offering vital insights into the economic health, developmental patterns, and sustainability of cities(Ribeiro et al. 2021; Mahtta et al. 2022; Brilhante and Klaas 2018). Leveraging Nighttime Light (NTL) data to estimate GDP as an economic indicator has become a widely adopted approach (Liu et al. 2021; Sun et al. 2020). There is substantial research indicating a strong association between GDP and urban spatial characteristics (Zhang, He, and Yuan 2020; Zhaoa and Chenb 2023), with numerous studies employing machine learning methods to model and predict regional GDP using structured urban information or urban imagery data(Kopczewska 2022; Chen et al. 2022b). For example, Chen et al. utilize deep learning methods to predict county-level GDP grids. While substantial research has established the relationship between GDP and urban spatial characteristics, the application of high-dimensional urban representations for GDP prediction remains underexplored. Given the critical role of GDP in validating urban prediction models, we selected GDP estimates derived from Nighttime Light (NTL) data as a benchmark for evaluating our approach.\nIn addition to economic indicators, mobility indicators such as ride-share demand serve as a critical indicator for urban forecasting, and it has significant implications for transportation planning, economic activity, and population behavior (Komanduri et al. 2018; Ma and Hanrahan 2020; Zhang and Zhang 2018). For instance, Shaheen et al. emphasize that ride-share services can influence traffic patterns, reduce private vehicle ownership, and improve accessibility in urban environments. Researchers have developed utilizing deep learning methods to predict ride-share demand from urban spatial information(M\u00fchlematter et al. 2023; Zhao et al. 2022; Zhang and Zhao 2022). Zhao et al. couple graph neural networks (GNNs) with the spatial-temporal influence of the built environment. Zhang and Zhao propose a Clustering-aided Ensemble Model to improve ridesourcing demand forecasting. Similarly, few studies have explored using high-dimensional urban representations to predict ride-share demand. Given the significance of ride-share demand as a key indicator with strong spatial dependencies, it serves as an ideal benchmark for evaluating the efficacy of our proposed method."}, {"title": "Methodology", "content": "As illustrated in Figure 1, our framework adopts an encoder-decoder structure. The encoder leverages existing state-of-the-art methods to process numeric and imagery inputs, representing city regions in a latent space through a mixing operator. Our novel GeoTransformer decoder model then utilizes this latent representation to predict GDP and ride-share demand.\nEncode the city: Mixing Operator\nTo fuse satellite imagery and sociodemographic information into a high-dimensional latent representation, we adopt a mixing operator $\\mathcal{M}$, which can be represented as:\n$Z_n = \\mathcal{M}(x_n, I_n)$ (1)\nin which $x_n$ and $I_n$ represent the sociodemographics and satellite imagery inputs, and $z_n$ represents the latent representation of an urban area.\nFollowing the work Deep Hybrid Model (Wang et al. 2024), we adopt a supervised autoencoder (SAE). The SAE maps the imagery into latent variables through the encoder $E(I_n)$, and reconstructs images through the decoder $D(Z_n)$ and sociodemographic through a supervision network $F(Z_n)$ (Figure 2). It is trained by optimizing the sum of autoencoder loss $\\mathcal{L}_{AE}$ and sociodemographic supervision loss $\\mathcal{L}_{sup}$:\n$\\mathcal{L}_{SAE} = (1 - \\lambda) \\mathcal{L}_{AE} + \\lambda \\mathcal{L}_{sup}$ (2)\nwhere $\\mathcal{L}_{sup}$ denotes the absolute error of the sociodemographic variables $X$. $\\mathcal{L}_{AE}$ represents the sum of L1 image reconstruction loss $\\mathcal{L}_{rec}$, the KL divergence loss $\\mathcal{L}_{kl}$, and the learned perceptual image patch similarity loss $\\mathcal{L}_{lpips}$:\n$\\mathcal{L}_{sup} = \\alpha_0 |x - F(z)|$ (3)\n$\\mathcal{L}_{AE} = \\alpha_1 \\mathcal{L}_{rec} + \\alpha_2 \\mathcal{L}_{kl} + \\alpha_3 \\mathcal{L}_{lpips}$ (4)\nin which all the $\\alpha$ terms are the adaptive hyperparameters used for scaling each loss.\nFor each autoencoder loss, $\\mathcal{L}_{rec}$ measures the L1 distance between the input images $I_n$ and reconstructed images $I'_n$ in the pixel space, $\\mathcal{L}_{kl}$ measures the distance between the estimated latent distribution and the standard normal prior distribution, and $\\mathcal{L}_{lpips}$ is calculated by extracting feature stacks from L layers of a pre-trained neural network, unit-normalizing them in the channel dimension $(Y_{In,hw}, Y'_{In,hw} \\in \\mathbb{R}^{H \\times W_1 \\times C_1})$, and scaling by vector $w_l$ before computing the L2 distance.\nThe mixing operator allows for the fusion of sociodemographics and satellite imagery information in a high-dimensional latent space. Each region of a city is then encoded into a latent representation using the encoder of the trained SAE.\nGeoTransformer: Geospatial-weighted Attention Mechanisms\nWe propose a novel approach GeoTransformer to decode the urban latent representations for downstream tasks including GDP and ride-share demand prediction.\nAs shown in Figure 3, our method adopts a structure akin to the Transformer (Vaswani 2017) decoder. Since the downstream tasks do not involve sequence prediction, the self-attention module is optional in this study. We introduce a geospatial attention mechanism to capture the spatial dependencies and provide location information between city regions. The outputs of the decoder layers are then passed through a fully connected linear layer. The whole system $G$ can be represented as:\n$Y_n = G(Z_n, KNN(Z_n, Z))$ (5)\nin which the input includes the latent representation of a region $Z_n$ and its k-nearest neighbors' latent representations $KNN(z_n, Z)$ from all regions $Z$, and $y_n$ is the predicted output.\nThe geospatial attention mechanism is the core component of our method. It computes attention scores between city regions and applies geospatial weighting to these scores. The attention calculation is represented as:\n$GeoAtt(Q, K, V) = W \\cdot softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (6)\nwhere the query matrix $Q$ contains the latent representation of the query region $z_n$ for which the target variable is being predicted, while the value matrix $V$ consists of $z_n$ itself and the latent representations of its neighbors as value regions. $d_k$ denotes the dimension of the key vectors. Intuitively, as the similarity between two city regions is not directly related to target variables, we set the key matrix K as trainable weights rather than being identical to V.\nW is the spatial weighting factor, computed based on the spatial distance between query regions in Q and value regions in V. Our model provides linear and non-linear spatial weighting options based on the assumption of Tobler's First Law of Geography (Tobler 1970), suggesting that regions closer to the query region are assigned higher weights.\nLinear Normalization scales the distances within 0 to 1, inversely transforming shorter distances into higher weights:\n$W_j = 1 - \\frac{d_j}{max(d)}$ (7)\nwhere $d_j$ is the distance to the $j^{th}$ value region, and $max(d)$ are the maximum distances observed.\nInverse Distance Weighting (IDW) (Shepard 1968) calculates weights inversely proportional to the distance:\n$W_j = \\frac{1}{\\frac{d_j^p}{\\epsilon} + \\epsilon}$ (8)\nwhere $d_j$ is the distance to the $j^{th}$ value region, the power value $p$ is set to 2 in standard applications, and $\\epsilon$ is a small constant to prevent division by zero. The weights are then subsequently normalized to ensure they sum to unity.\nGaussian Weighting applies a Gaussian function to the distances, providing a gradual decrease in weight as distance increases:\n$W_j = e^{-(\\frac{d_j}{2 \\sigma^2})}$ (9)\nwhere $d_j$ is the distance to the $j^{th}$ value region, and $\\sigma$ determines the spread of the weights. The weights are then subsequently normalized to ensure they sum to unity.\nAdhering to the established multi-head attention mechanism (Vaswani 2017), our model employs multiple attention heads to capture various spatial relationships concurrently. The calculation can be represented as:\n$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$ (10)\nwhere $head_i = GeoAtt(QW_i^Q, KW_i^K, VW_i^V)$ (11)\nwhere $W_i^Q$, $W_i^K$, $W_i^V$ and $W^O$ are the projection matrices. The amount of heads and layers of the decoder module is also adaptive.\nThe GeoTransformer architecture is designed to extract geospatial information in high-dimensional latent space, and it is also adaptable to various geospatial prediction tasks due to the flexibility of the output layers. The design of geospatial attention allows the model to capture and integrate spatial dependencies, utilizing local context from both the target region and its surroundings for more accurate predictions. Leveraging high-dimensional urban information and deep architectures, our GeoTransformer represents a novel approach to geospatial analysis and urban planning applications.\nGDP and Ride-share Demand Modeling\nFor the GDP modeling, we use the Global 1km\u00d71km gridded revised data (Chen et al. 2022a) which is based on nighttime light data from DMSP/OLS and NPP/VIIRS sensors. The data is calibrated across different years and sensors to produce a consistent high-resolution dataset. A regression model is constructed by linking night-time light intensity, population density, and other auxiliary variables with actual GDP. the gridded real GDP can be calculated using:\n$RGY_{ij,t+1} = \\begin{cases} \\frac{RGY_{ij,t}}{1 + g_{y_{ij,t}}}, & \\text{if } DN_{ij,t} \\neq 0 \\\\ 0, & \\text{if } DN_{ij,t} = 0 \\end{cases}$ (12)\nwhere $RGY^t_{ij}$ denotes the $j^{th}$ grid in the $i^{th}$ country's real GDP in the period of $t$ based on the revised real growth rate. The calculations are based on the hypothesis that there is no GDP when the DN value is zero. The original GDP data, gridded at 1km\u00d71km resolution, is resampled using bilinear interpolation.\nFor the ride-share demand modeling, we utilize taxi trip data from Chicago that is constant with the time frame of the remote sensing imagery. Focusing on the pickup locations to represent ride-share demand, we aggregate the demand for each census tract by calculating the total number of pickups within each tract. Subsequently, we apply area-weighted averaging to estimate the ride-share demand corresponding to each imagery. The area-weighted average ride-share demand $D$ for each imagery is calculated as follows:\n$D = \\frac{\\sum_{i=1}^n D_i A_i}{\\sum_{i=1}^n A_i}$ (13)\nwhere $D_i$ is the ride-share demand (number of pickups) in the $i$-th census tract, $A_i$ is the area of the $i$-th census tract, and $n$ is the total number of census tracts. This approach allows us to align the spatial resolution of the taxi trip data with the remote sensing imagery, facilitating accurate and effective modeling of ride-share demand."}, {"title": "Experiment", "content": "In this section, we provide our method's results in GDP and ride-share demand prediction and compare them with baseline methods. Ablation experiments are conducted to demonstrate the effectiveness of the geospatial attention component and model architecture design.\nExperiment Setup\nData Preparation. For the satellite imagery data, we utilized the National Agriculture Imagery Program (NAIP) four-band remote sensing imagery for the Greater Chicago Area, acquired in September 2019. The imagery has a high spatial resolution of 0.6 meters. Using Google Earth Engine, we download the dataset and subsequently divide it into tiles of size 512x512 to facilitate our analysis.\nFor spatial correspondence between structured and unstructured information, we created a fishnet grid with a cell size of 500m\u00d7500m, aligned it with the same geospatial location as the maptile data, and resampled the 1km\u00d71km gridded GDP data to match the fishnet.\nFor the ride-share demand, we use the Taxi Trips dataset obtained from the City of Chicago. The dataset includes fundamental trip information such as unique ID, pickup and drop-off timestamps, latitude and longitude coordinates, trip miles, fare, and other relevant details. To align the spatial resolution of the taxi trip data with the satellite imagery tiles, we performed a spatial join. This process allows us to calculate the ride-share demand corresponding to each tile, ensuring accurate and effective integration of the two data sources.\nIn addition to the imagery and ride-share data, we incorporate sociodemographic data from the American Community Survey (ACS) 2015-2019. This dataset provides detailed information at the census tract level, including variables such as median rent, racial composition, median household income, the Gini index, and so on.\nMixing Operator Training. We utilize the VAE component from Stable Diffusion (Rombach et al. 2022) as the initial weights for the mixing operator. To accommodate a 4096-dimensional latent space, linear layers are introduced at both the encoder's output and the decoder's input. The decoder reconstructs images using mean absolute error (MAE) as the reconstruction loss. An annealing strategy is adopted for the KL divergence loss, gradually increasing its weight ratio. Additionally, the latent space restores sociodemographic information through a feed-forward module, employing mean squared error (MSE) as the loss metric. The training process spans 100 epochs in total. The latent representation for each region is inferred using the encoder part of the mixing operator.\nBaselines. We train various baseline models for comparison. As detailed in Table 1, models in Panel 1 utilize either direct numeric or imagery input. Specifically, Model 1 employs a linear regression configured with 12-dimensional sociodemographic data as input. Model 2 utilizes a pre-trained Visual Transformer (ViT) (Dosovitskiy et al. 2020a), which is subsequently fine-tuned on the prediction tasks with the last 4 encoder layers and the output layer unfrozen. Panel 2 presents models that utilize latent representations as inputs to perform predictions. Following DHM(Wang et al. 2024), model 3 employs a polynomial regression. Model 4 adopts a Graph Attention Network (GAT) (Zhang, Yu, and Liu 2019) model with each city region as a node connected to its neighbors, forming the graph structure. All hyperparameters for training the baseline models are optimized through a grid search to ensure peak performance on the test set.\nGeoTransformer Training. As shown in Table 1 panel 3, we train the GeoTransformer (model 5-7) using three spatial weighting methods. The number of decoder layers is set to 4, and the number of attention heads is set to 16. For each model variant and task, the L2 regularization decay weight was optimized, with a range of 0.0001 to 0.1, while the number of neighboring regions is set to 49, 81, or 169. All parameters are grid-searched to ensure the best performance. For both GDP and ride-share demand predictions, mean squared error (MSE) is employed as the loss function. All models are trained on an RTX 4090 GPU running Ubuntu 22.04.\nEvaluation Metrics. We employ commonly used statistical metrics that evaluate the accuracy of the predictions, including Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R2). MSE evaluates the average squared differences between predicted and actual values, MAE measures the average absolute differences, and (R2) assesses the proportion of variance explained by the model.\nEvaluation Results\nTable 2 summarizes the predictive performance evaluation of baselines and GeoTransformer models in both GDP and ride-share demand prediction tasks.\nGDP Prediction. As shown in Table 2, all three Geo-Transformer models surpass every baseline model. Compared to Random Forest and ViT in panel 1, our best model (linear weighting) outperforms by 27% and 18% respectively in R2. In Panel 2, the GAT model exhibits robust decoding capabilities for urban latent representations and delivers the best predictive accuracy among baseline models, while our GeoTransformer with linear weighting surpasses it by 14% in R\u00b2. This demonstrates that our geospatial attention mechanism effectively models spatial dependencies without the need for predefined graph structures. In panel 3, among the three GeoTransformer models operating under identical parameters and training epochs, the linear weighting approach exhibits the best predictive performance.\nRide-share Demand Prediction. The results in Panel 1 of Table 2 indicate that ride-share demand is more strongly influenced by sociodemographic factors. Both polynomial regression and GAT models struggle to effectively decode latent representations for accurate predictions. However, our GeoTransformer model with IDW weighting outperforms the best baseline by 18% in R2 on the test set. Additionally, among the GeoTransformer variants, it is observed that the IDW model not only achieves the highest performance but also exhibits the fastest convergence.\nQualitative Analysis. To investigate the functionality of the geospatial attention mechanism, we visualize several query regions' attention maps with their neighboring value regions. We present an example in Figure 4, which (a) includes satellite images of the northwest area of the Gresham community, with the query region marked by a red marker (Figure 4), and its attention map for GDP prediction. The query region focuses on its own community area, nearby amenities (the park in the lower left and the supermarket and church in the upper right), and critical road infrastructure (the freeway entrance in the upper left). These results validate the effectiveness of the geospatial attention mechanism in accurately capturing urban features relevant to GDP.\nAblation Study\nTo investigate the necessity and effectiveness of each design in GeoTransformer, we conduct ablation experiments for geospatial weighting, trainable key matrices, and the number of neighboring value regions.\nGeospatial Attention Module. To demonstrate the effectiveness of the geospatial attention module, we conducted a controlled experiment by training an alternative version of the model devoid of the weighting attention mechanism. This baseline model maintained the same architecture and training parameters as the GeoTransformer models, featuring 16 heads, 81 neighbors, 4 layers, and an L2 regularization decay weight of 0.0001, trained over 50 epochs. As shown in Figure 5, the comparative analyses illustrate that all three geospatial attention variants significantly enhanced the GDP prediction accuracy.\nTrainable Key Matrices. Intuitively, the correlation between different regions in urban forecasting is not directly determined by the similarity between these regions. Therefore, we set the key matrix in the attention mechanism as trainable parameters to flexibly adjust the correlation between the value regions and the query region. To validate the effectiveness of this design, we train a GeoTransformer model by setting key matrices K fixed and identical to value matrices V, while keeping all other settings constant. As shown in Table 3, linear weighting GeoTransformer with trainable K achieves higher prediction accuracy over 5% in R2. It is also observed that the fixed K GeoTransformer converges faster during training, which is reasonable given its smaller number of trainable parameters. However, the flexibility of a trainable K allows the model to ultimately achieve higher accuracy.\nk-Nearest Neighbors. The spatial extent of surrounding information required by the query region may vary for different urban forecasting tasks. Therefore, we search for the optimal number of neighboring regions k for GDP and ride-share demand predictions. By centering on the query region and expanding outward in a square, k is determined by the square of the side length. We linearly increase the side length to identify the optimal k that maximizes model performance. For GDP prediction, providing the model with a broader range of urban information initially leads to an improvement in accuracy. However, as k increases further, the model may become overwhelmed by excessive information, resulting in a decline in accuracy (Figure 6).\nA similar trend is observed in the ride-share demand prediction, where increasing k initially enhances model accuracy. The model performs best when k is set to 49 or 81. However, further increasing k by incorporating additional neighboring information leads to a decline in accuracy (Figure 6).\nConclusion and Discussion\nWe propose a GeoTransformer structure to extract information from high-dimensional urban latent representations. The novel geospatial attention mechanism allows GeoTransformer to capture extensive urban information and its spatial dependency. The experimental results demonstrate the capacity of our method in urban forecasting tasks. An ablation study is conducted to validate the effectiveness of the geospatial attention mechanism and the design of GeoTransformer.\nSelf-attention module in Transformer (Vaswani 2017) is primarily intended for sequence prediction tasks. In urban forecasting, spatiotemporal prediction tasks also resemble sequence prediction. Although this study does not activate the self-attention module, we retain it in the GeoTransformer as it holds the potential for adapting to spatiotemporal sequence prediction tasks. This merits further exploration in subsequent research."}]}