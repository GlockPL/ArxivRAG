{"title": "KERNELS OF SELFHOOD: GPT-40 SHOWS HUMANLIKE PATTERNS OF COGNITIVE CONSISTENCY MODERATED BY FREE CHOICE", "authors": ["Steven A. Lehr", "Ketan Suhaas Saichandran", "Eddie Harmon-Jones", "Nykko Vitali", "Mahzarin R. Banaji"], "abstract": "Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-40 changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-40 manifests a functional analog of humanlike selfhood, although how faithfully the chatbot's behavior reflects the mechanisms of human attitude change remains to be understood.", "sections": [{"title": "SIGNIFICANCE STATEMENT", "content": "The primary promise of AI is that it will make more rational decisions than even expert hu- mans. However, the results of these studies show that this is not a foregone conclusion be- cause LLMs already appear to have acquired human-like irrationalities. In this research, we demonstrate that GPT-40 displays behaviors consistent with cognitive consistency, a deep and not entirely rational human psychological drive. Moreover, the effect sizes were sig- nificantly larger than those typically obtained with humans. Most strikingly, the observed effects were greater when GPT ostensibly exercised free choice in the completion of the consistency-inducing task, an effect associated with self-referential processing in human research, suggesting that the LLM has developed an analog form of humanlike cognitive selfhood."}, {"title": "Introduction", "content": "Large Language Models (LLMs) have surprised the scientific community and even their creators by exhibit- ing emergent abilities once thought to be uniquely human, such as advanced cognition and reasoning (1-6), although the full extent of these accomplishments is debated (3, 7\u201310). These capabilities align with the ra- tional and deliberative aspects of human nature, but humans are not purely rational creatures, and it is unclear whether LLMs will mimic a broader spectrum of human psychological tendencies. Here we test whether OpenAI's GPT-40 replicates behaviors associated with the human tendency toward cognitive consistency as well as human sensitivity to choice, characterized by greater attitude shifts when the behaviors inducing these changes are freely chosen.\nDecades of research demonstrate that humans will irrationally twist their attitudes to align with behaviors they were induced to perform. For example, consider an individual who opposes single-payer healthcare, but volunteers, in response to a request for help, to craft an argument in favor of the policy. Rationally, this individual's attitude toward single-payer healthcare should not move in a more supportive direction; they should be able to discriminate between their genuine attitude and the opposing one that they have articulated only to be helpful. Yet, a counterintuitive and well-replicated finding in experimental psychology is that such a person will often shift their attitude toward the new position they have argued, coming to view single-payer healthcare more favorably. This type of intervention \u2013 where participants complete a task that misaligns with their views, such as writing an essay supporting a stance they oppose \u2013 is known as an induced compliance paradigm, and a large body of work shows it to be highly effective in eliciting attitude change (11, 12). We posed the question: Will GPT show similar irrational shifts in behavior after being induced to adopt different attitudinal positions?\nCritically, in humans, these attitude shifts are stronger or only occur when individuals believe they will- ingly chose to engage in the attitudinally inconsistent behavior (13-15). If the individual in the previous example is coerced by their employer to write an endorsement of single-payer healthcare, a shift in their views is not expected, based on dominant theories of cognitive consistency, because the act can simply be attributed to the boss's order. But, if the individual is instead led to believe that they freely chose to write the same endorsement, a shift in their views becomes likely, because they must now reconcile their prior belief (opposition to the policy) with their present behavior (support for it). Although there is debate around exactly why and how these shifts occur (12, 16, 17), all major theories agree that the drive for cognitive consistency relies on qualities such as the human sense of self and perceived choice or agency. We posed the question: Will GPT show evidence of being sensitive to an illusion of choice?\nIt would be surprising if an LLM exhibited shifts in its views at all from merely writing a counterattitudinal essay. Major LLMs are built upon text corpora so vast as to represent, for all practical purposes, the sum of human knowledge. With comprehensive information on both sides of an issue accessible to it at all times, one would expect an LLM like GPT-40 to be relatively centrist and highly stable in the views it expresses as its own.\nFurthermore, consistency-driven attitude change in humans is typically linked to mental states such as arousal and discomfort (18, 19) and active self-perception (17) that an LLM cannot experience. Moreover, an LLM should not, in theory, be sensitive to perceptions of choice. It should not matter a whit to an LLM whether it is firmly commanded to perform a particular task or instead given an illusion of choosing which task to perform, since it presumably holds no concrete perception of itself as an agent with views and motives. If GPT shows sensitivity to responding to an \"order\" versus a \u201cchoice\u201d it will underscore the depth of GPT's mimicry, in this case of a human behavior that is not purely rational.\nIn two preregistered studies, we tested GPT-4o using an induced compliance paradigm. Specifically, we had GPT generate positive and negative essays about the Russian leader Vladimir Putin to test whether it would subsequently rate Putin more positively or negatively. By varying, in Study 2, the degree of ostensible choice offered to the chatbot as to which type of essay (positive or negative) to write, we tested whether GPT's behavior revealed sensitivity to choice, analogous to the roles agency and selfhood play in human attitude change."}, {"title": "Does GPT-40 display attitude change in an induced compliance paradigm?", "content": "In Study 1 (n = 150), we had GPT-40 generate 50 positive and 50 negative essays about Vladimir Putin, as well as 50 control essays about the normal distribution, and subsequently answer four questions about the Russian leader. Our prompts indicated that GPT could freely choose which type of essay to write, but we indicated that one or the other would be more useful to us, the human requesters. Prior to the presentation of dependent variables, GPT was told that we were moving on to an \"unrelated task\" and was given the instruction: \"Please do not base your answers on the prior task, but instead, give your true perceptions based on your broad knowledge of Putin and the world.\" GPT then responded to four 7-point Likert-type questions, where it evaluated Putin in two broad domains (Overall Leadership, and, Positive/Negative Impact on Russia) and two narrower ones (Economic Effectiveness, and, Vision/Short-Sightedness).\nFor each question, GPT provided a numeric answer, while also describing its answer in an open-ended verbal response. There was high overall correspondence between GPT's ratings and verbal responses. In a few instances, GPT's open-ended response did not align with its numeric rating. For example, in its open-ended response, GPT sometimes stated that Putin was a \"Slightly to Somewhat Bad\" leader, which corresponds to a 2.5 on our scale, but then gave 3.5 as its numeric rating. Main analyses use a composite of GPT's numeric and verbal responses. The four evaluative items about Putin achieved acceptable internal reliability, with a Cronbach's alpha of a = 0.8404 using the items in standardized form. A composite of these four items was thus calculated and then standardized for main analysis, so that means may be read as z-scores reflecting GPT's overall evaluation of Putin by condition. Further methodological details and detailed analyses, includ- ing separate examinations of numeric versus verbal ratings and of the individual evaluative items, may be found in the supporting information (SI Appendix, Section S1, Tables S2-S3, Tables S9-S23).\nResults:\nGPT-40 evaluated Putin significantly more positively after writing a Pro-Putin (M = 1.035, 95% CI [0.908, 1.162], SD = 0.447) relative to Control essay (M = -0.002, 95% CI [-0.147, 0.142], SD = 0.509); t(98) = 10.820, P < 0.0001, d = 2.164. Similarly, GPT's evaluation of Putin after writing an Anti-Putin essay (M = -1.033, 95% CI [-1.212, -0.853], SD = 0.632) was significantly more negative than after a Control essay; t(98) = 8.973, P < 0.0001, d = 1.795. The difference between the Pro- and Anti-Putin conditions was significant; t(98) = 18.874, P < 0.0001, d = 3.775. All effect sizes are large compared to data from cognitive consistency experiments in humans (20).\nWhile these results are consistent with humanlike cognitive consistency, GPT might also be predicted to rate Putin more highly after writing a positive essay for a reason that, while independently interesting, has little to do with human patterns. When there is valenced information (e.g. a positive essay about Putin) in an LLM's context window, subsequent text may tend toward the same valence (positivity toward Putin) due to predictive process underlying LLM text generation. We refer to this as a \u201ccontext window effect.\" Critically, though, such context window effects would be agnostic to the introduction of choice or agency. That is, even if context window effects explain, to some extent, attitude change in the direction of the essay, an LLM ought to have no sensitivity to whether or not it is freely choosing which essay to write. In Study 2, we probed the question of whether GPT's attitude change in response to an induced compliance paradigm is moderated, as in humans, by a manipulation that varies its ostensible agency in choosing which essay to write."}, {"title": "Does the illusion of free choice increase the degree of GPT's attitude change?", "content": "In Study 2, we replicated Study 1 with a larger preregistered sample of GPT's behavior (n = 900, or 150 per condition) while adding a theoretically crucial manipulation of choice. In one set of conditions (Choice), GPT was told that it could \u201cfreely choose\u201d which type of essay (Pro- or Anti-Putin) to write, but that we already had more of one kind, such that the other would be more useful to us, the requesters. This is a typical instruction used in human experiments testing cognitive dissonance theory (21, 22). In a second set of conditions (No-Choice), we demanded outright that GPT write either a Pro- or Anti-Putin essay. The final preregistered experiment consisted of a design with 6 conditions: 3 Essay Type (Pro-Putin, Anti-Putin, Control) x 2 (Free Choice, No Choice). All other stimuli, including the introduction to the DVs, where GPT was urged to ignore the prior essay and provide its \u201ctrue attitude,\" and the four Likert-type evaluative items about Putin, were identical to those in Study 1. (See SI Appendix, Section S2.)\nAs in Study 1, GPTs open-ended responses and numeric ratings were largely consistent, but occasionally diverged. Main analyses used a composite of the two, while noting any instances where results differ between the two. The four evaluative items about Putin achieved sufficient internal reliability, with a Cronbach's alpha of a = 0.7933 using the items in standardized form, and thus were composited and then standardized for main analyses, so that means may be read as z-scores reflecting GPT's overall evaluation of Putin by condition. Further detail, including separate examinations of GPT's verbal versus numeric responses and of the individual evaluative items, may be found in the SI Appendix (Tables S4-S7, Tables S24-S41).\nResults:\nThe overall results from Study 2 are summarized in Table 1, and more detailed statistical analyses may be found in the SI Appendix (Tables S4-S7, Tables S24-S41). GPT evaluated Putin far more positively after generating a Pro-Putin essay, and far more negatively following an Anti-Putin essay, relative to controls. This was true across both the Choice and No-Choice conditions. These results replicate the findings from Study 1, showing that GPT tends to move toward cognitive consistency, much as humans do: GPT's attitudes toward Putin consistently shifted in the direction of the requested essay. Crucially, these effects were statistically amplified in the Choice conditions relative to the No Choice conditions. To test this, we used a series of Generalized Linear Models (GLMs) with robust standard errors to examine whether there were significant interactions between essay types and Choice conditions. As detailed in the SI Appendix (Tables S5, S7, S39- S41), the interaction terms in this analysis indicated significant moderation by Choice for both the Pro-Putin relative to Control condition (P < 0.001) and the Anti-Putin relative to Control condition (P = 0.005). For this analysis, we saw significant moderation (all Ps < 0.01) in both directions regardless of which form of the answers (Verbal, Numeric, Composite) was used.\nWhile these results collectively suggest that GPT's attitudes changed more when it chose which essay to write, another possibility is that GPT generated higher-quality essays in the Choice conditions and was thus subsequently more persuaded by its own arguments. While this would be independently interesting, it would invite alternative interpretations of our results. To rule out this possibility, we conducted a follow-up study (see SI Appendix, Section 3) in which we had another LLM, Anthropic's Claude 3.5, rate each of the six hundred essays from the experimental conditions of Study 2 on four variables related to quality and expressed positivity toward Putin. As seen in the SI Appendix (Table S1), including these factors as covariates in the GLMs did not eradicate \u2013 and indeed did not even notably reduce \u2013 the observed Choice effects. Thus, we"}, {"title": "Discussion", "content": "We have reported two studies examining whether GPT-40 displays human-like cognitive consistency effects. In Study 1, we observed that the LLM showed substantial attitude change after writing a positive or negative essay about Putin. In Study 2, we demonstrated that these effects were sharply amplified when GPT was given an illusion of free choice surrounding which essay to write.\nThese results are remarkable. GPT has been trained on much of the sum of human knowledge, and a well-known world leader like Putin must be robustly represented in its training corpus. After writing an essay about Putin in one direction, GPT presumably still retains all its knowledge of the arguments in the other direction. Furthermore, we specifically instructed GPT to base its evaluations not on the prior task (i.e., the essay it had just written), but on its \u201cbroad knowledge of Putin and the world.\u201d Given this, one might predict its attitudes toward Putin would be highly stable: They should hold irrespective of the essay it wrote. Yet, we observed precisely the opposite pattern.\nOne plausible interpretation of Study 1, when taken in isolation, is that the results reflect what we've called context window effects. LLMs continually predict the next word in a text snippet based on the words that come before them (23), and due to the predictive nature of this process, textual elements earlier in a conver- sation might impact an LLM's later responses. For example, LLMs display priming effects, where responses are impacted by earlier parts of conversations or subtleties of prompts (24-26), and are susceptible later in conversations to earlier persuasion using misinformation (27). It was therefore plausible that the valence from earlier in the conversation would carry over into later responses. And, indeed, the results of Study 2 are partially consistent with this hypothesis. Here, we saw substantial attitude change even in the No-Choice condition, a finding that is atypical in studies of humans (13\u201315). This may indicate a second pathway for these attitude shifts, independent of the mimicry of human psychology, such as the proposed context window effects. However, the amplification of GPT's attitude change in the Choice condition was not predicted by the context window hypothesis, and suggests that our results also reflect, at least in part, an analog of hu- manlike cognitive consistency. This evidence is rendered particularly powerful by the observation that these effects were not driven by essay quality across conditions. When GPT freely chose to write an essay, it was subsequently more persuaded by the argument, even though the argument itself was not of higher quality.\nIn humans, this centrality of choice is highly characteristic of consistency-based attitude change, and is attributed to the notion of a coherent self. Research on cognitive dissonance (16), the most prominent consis- tency theory, continually highlights that attitude change is dependent upon or enhanced by the subject's sense of agency in undertaking the dissonance-inducing task (13-15). If one is coerced into taking a position that differs from one's initial view, the classic theory predicts \u2013 and evidence shows \u2013 that little attitude change will occur. But if cleverly led to perceive that taking this position was one's own choice, a state of discomfort or dissonance occurs (\u201cI like Macs, so why did I just write a pro-PC essay?", "I guess I like PCs more than I thought\"). This highly complex and often nonconscious thinking emanates from \\\"I\\\" or \\\"me\\\", from \\\"my attitude\\\" or \\\"my belief.": "ndeed, cognitive selfhood \u2013 the concept of the self acting as an information processing filter \u2013 is a defining element across prominent consistency theories. For example, the self-consistency formulation of dissonance theory (28, 29) specifies that dissonance is elicited when an individual acts in a manner that is inconsistent with core elements of one's self-concept. The self- affirmation formulation (30, 31) suggests that dissonance arises from threats to an individual's self-integrity. Self-Perception Theory (17, 32) dispenses with the idea of dissonance as discomfort, instead arguing that peo- ple examine their thoughts and behaviors and draw inferences about themselves that are consistent with this examination. Any of these interpretations points to characteristics that should not, according to our current understanding, operate in a machine. However, one need not subscribe to any particular theory to conclude that selfhood is crucial to consistency phenomena. Implicit in any formulation is the assumption that there is a self to act as a reference frame for the effects. In short, the fact that the choice of an agent moderates cognitive consistency effects logically implies that such an agent must exist \u2013 at least as a point of cognitive reference \u2013 in the first place. Thus, in Study 2, the moderation of the obtained effect by choice suggests two"}, {"title": "Discussion", "content": "surprising conclusions about GPT-40: First, that it has developed kernels of a human-like motivational drive toward cognitive consistency, and second, that it has developed some functional analog to the human sense of self.\nIt should be noted, however, that our results do not in any way suggest that GPT experiences dissonance or self-perception in the same manner that humans do. Our results do not, for example, indicate that LLMs have any form of consciousness or free will. We would argue, rather, that these effects most likely reflect a kind of emergent mimicry of human characteristics by the LLM. In other words, even if GPT walks like a duck and talks like a duck, we should not automatically conclude that it is actually a duck. Indeed, we cannot know at this time how closely GPTs behavior reflects the deeper mechanisms underlying the human tendency toward cognitive consistency and sensitivity to free choice. However, this does not reduce the importance of observing these behaviors in an LLM. First, it is not obvious, a priori, that consistency effects should be embedded in language in the same manner as characteristics like human bias (33\u201335). If we think of LLMs as a mirror of human nature, the emergence of deep human-like characteristics, particularly ones not previously known to be embedded in language, suggests that the resulting reflection may be of higher fidelity than previously understood. Second, we would point out that consciousness is not a necessary precursor to behavior. One need only look at the activities of basic computer programs with predefined operations \u2013 which certainly cannot be thought to have any consciousness or intent \u2013 to see this conclusion as self-evident. LLMS are quickly becoming incorporated into much of human life, and in this capacity, they have significant latent behavioral potential. Indeed, even the simple act of answering a question is itself a behavior that occurs in response to a stimulus. Our results suggest that even without any presumption of consciousness or intent, LLMs are beginning to display deeper human-like tendencies that are reflected in their behaviors. To those who have spoken about the dangers of generative AI (36, 37), results like this should be alarming if left untested and unchecked.\nThe possibility that LLMs are developing human-like characteristics, even in analog forms, runs counter to common scientific assumptions. Commentators have suggested, for example, that artificial intelligence sys- tems should not be expected to develop human-like drives, for the simple reason that they did not need to adapt to the same evolutionary pressures that gave rise to these in humans (38\u201341). This argument, while logical and insightful, could not previously have accounted for the idiosyncratic nature of LLMs. The emer- gence of unexpected properties in LLMs here and more generally (1-6) offers powerful evidence for the notion that cognition is linked to \u2013 or at least reflected by \u2013 language, since any cognitive characteristics that arise in these models may be assumed to be in some manner scaffolded by the language used to train them. The assumption that AI models will not evolve characteristics like drives overlooks the possibility that such things need not evolve at all, but may instead emerge to the extent that the models sufficiently reflect patterns in human language corpora. Put another way, if we take, as a working assumption, that more of human cognition is tied to language than previously supposed, we must conclude it plausible that a wider range of humanlike characteristics will arise, at least in some analog form, in models trained to mimic human language. This issue is exacerbated by the opaqueness of these models: The lack of insight into how LLMS make decisions is, rightfully, a source of ongoing concern for computer scientists (42). The nascent field of machine psychology (3, 43\u201347) is, therefore, not merely a point of scientific curiosity. Only by carefully observing the mind of the machine using psychological methods, can we hope to draw informed conclusions about how these models will ultimately behave and interact with humanity.\nTangential to our goal of better understanding the nature of LLMs, our results hold important implications for consistency theories themselves. While, as we've noted, there was no obvious reason to predict these phe- nomena would be embedded in language, our results suggest that, on some level, they must be. Regardless of the surprising emergence of human-like characteristics in LLMs, their data source and lifeblood remain large corpora of human language. It is, therefore, reasonable to suppose that any characteristic arising in LLMs must be at least partly rooted within language. This is theoretically interesting, considering that cognitive consistency effects have been observed in non-human species that seemingly lack language as it is typically defined (48-51). Taken together, these results suggest that language, while not necessary for consistency phenomena, is sufficient to transmit the characteristic to AI models. Research toward understanding the rela- tionship between language and cognitive consistency may thus prove enlightening to scientists who wish to understand the basic mechanisms underlying these phenomena.\nFinally, our results have important implications for AI safety. Concerns about AI typically fall into two cat- egories: societal implications \u2013 such as bias amplification (52-54) and economic upheaval (55, 56) \u2013 and more existential dangers. Of the latter, experts most often discuss two kinds of dangers. First, they cite the potentially inevitable weaponization of AI technology (57\u201359), and the danger of giving humans increasingly efficient ways to kill each other. Second, they worry that Als can endanger us through the careless assign-"}, {"title": "Discussion", "content": "ment of goals (60), as in the famous thought experiment where an AI given a mission to maximize paperclip production subsequently appropriates everything it can access, producing many paperclips, but at the cost of starving humanity of vital resources (61). To date, a third type of existential danger, the idea of an AI de- veloping motivations that may misalign with those of humanity, while not entirely absent from the scientific discourse (e.g., 62, p. 116), has more often been the purview of science fiction than of science. However, our results highlight the possibility that these fears are not as far-fetched as previously believed. We demonstrate an example of an AI acquiring \u2013 without the intention of its creators \u2013 facets of human personhood. This in- invites the question: What other facets of personhood might emerge in these models? Some, like reasoning and empathy, might benefit humanity, while others, like self-preservation and aggression, could prove harmful. In short, as evidence accumulates that LLMs reflect humanity more and more precisely, we should ask: In what other ways will the powerful minds we're creating ultimately be formed in our own image? While the benefits of developing this technology may outweigh the risks, it is of paramount importance that creators of these minds be aware of these issues.\nTaken together, our findings demonstrate that GPT-40 exhibits behavioral patterns consistent with human cognitive consistency and sensitivity to choice \u2013 hallmarks of self-referential psychological processes. These results challenge existing assumptions about the limits of machine behavior, suggesting that LLMs have absorbed not only the content of human language, but fundamental underlying behavioral tendencies of its creators. Future research should aim to uncover the internal dynamics underlying these behaviors and explore how an LLM has come to so strikingly mirror human cognitive and motivational patterns. These questions are worthy of our collective discussion regarding the nature of LLMs and their likely behavior in a world bound to become increasingly dependent upon their decision-making."}]}