{"title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal\nPerformance Impact via Token Pooling", "authors": ["Benjamin Clavi\u00e9", "Antoine Chaffin", "Griffin Adams"], "abstract": "Over the last few years, multi-vector retrieval methods, spearheaded\nby ColBERT, have become an increasingly popular approach to Neu-\nral IR. By storing representations at the token level rather than at\nthe document level, these methods have demonstrated very strong\nretrieval performance, especially in out-of-domain settings. How-\never, the storage and memory requirements necessary to store the\nlarge number of associated vectors remain an important drawback,\nhindering practical adoption. In this paper, we introduce a simple\nclustering-based token pooling approach to aggressively reduce the\nnumber of vectors that need to be stored. This method can reduce\nthe space & memory footprint of ColBERT indexes by 50% with\nvirtually no retrieval performance degradation. This method also\nallows for further reductions, reducing the vector count by 66%-\nto-75%, with degradation remaining below 5% on a vast majority\nof datasets. Importantly, this approach requires no architectural\nchange nor query-time processing, and can be used as a simple\ndrop-in during indexation with any ColBERT-like model.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep-learning based approaches are becoming increasingly popu-\nlar in retrieval. Effectively, this means using a neural network to\nassess the relevance of documents for a given query. Many different\napproaches to neural IR exist, such as single-vector dense represen-\ntations [32], learned sparse representations [8], or late-interaction\nmethods, such as ColBERT [11], which employ multi-vector repre-\nsentations by storing a vector for each token in a document.\nUsing multiple vectors to represent the sequences allows for\na more fine-grained and expressive representation compared to\ncompressing all the semantics into a single vector. As a result, this\nlatter approach has frequently been shown to generalize better to\nout-of-domain settings than dense representations [16, 34]. These\nproperties have led to these methods experiencing a considerable\namount of interest in the recent months\u00b9.\nHowever, the enhanced expressiveness of multi-vector represen-\ntations comes at a hefty storage and memory cost. It is relatively\ntrivial to store millions of dense representations using common\nindexing methods such as HNSW [20]. On the other hand, ColBERT\nand similar approaches require an order of magnitude more vectors\nper document. Many techniques have been proposed to alleviate\nthis issue: dimension reduction methods through a learned projec-\ntion layer [11], aggressive quantization using Inverted File [36]-\nProduct Quantization [10] (IVF-PQ) indexing [27] and an optimised\nindexing and querying mechanism [26].\nThese methods, while bringing the storage requirement down\nto the same order of magnitude as single-vector methods, does\nnot fully close the gap with storage and memory requirements\nof these approaches. This is especially apparent as dense single-\nvector representations can also be aggressively quantized [33], with\nrelatively little performance loss when used in conjunction with\nquantization-aware training [24].\nMoreover, the indexing methods required to achieve this level\nof compression with ColBERT add complex processing layers, con-\nstraining the index to a specific format. This renders them con-\nsiderably less flexible, notably in terms of document addition and\ndeletion (CRUD) than commonly used dense indexing methods such\nas HNSW [20]. As a result, ColBERT can appear as a worse option\nfor evolving corpora, limiting its practical use.\nAnother line of work has focused on attempting to reduce the\ntotal number of vectors needed to be stored. However, the exist-\ning methods have not reached widespread adoption, either due\nto requiring a modified pipeline and specific training for limited\ngains [9]; or as a result of minimal storage reduction in comparison\nto the negative retrieval performance impact [14].\nRecently, a study has also confirmed the intuition that different\ntokens have vastly varying levels of importance for multi-vector\nretrieval performance [15]. This is in line with a series of studies in\nthe adjacent field of Computer Vision, which have introduced token"}, {"title": "2 TOKEN POOLING", "content": "To try and mitigate the issues associated with the high number\nof vectors to store when using multi-vector retrieval methods, we\nintroduce a simple Token Pooling approach. This is applied at\nindexing time to reduce the effective number of tokens representing\na document. It can be used with any pre-trained ColBERT model,\nwithout any further training or modifications.\nOur core approach is simple and works as a two-step system:\nFirst, we devise a way to group individual vectors together, using\none of the three clustering methods detailed below. We then apply\nmean pooling in order to obtain a single vector which contains an\naverage representation of the cluster. The resulting set of pooled\nvectors serves as our new multi-vector document representation,\nand the original vectors are discarded. The main intuition behind\nthis approach is the belief that there is considerable redundancy\ntoken vectors within as single a document, and that tokens therefore\nhave varying importance [15]. Within this assumption, pooling the\nmost similar ones is unlikely to considerably modify the overall\ndocument representation.\nTo control the level of compression, we introduce a new variable\ncalled the POOLING FACTOR. This factor is, effectively, a compression\nfactor. For example, a pooling factor of 2 reduces the total number\nof vectors stored by a factor of 2, i.e. a 50% reduction."}, {"title": "2.1 Pooling Methods", "content": "We explore three pooling methods:\nSequential Pooling. This method acts as our baseline does not\nrequire any clustering. Tokens are pooled together based on the\norder in which they appear in the document, from left to right. In\nthis setting, the pooling factor dictates the number of sequential\ntokens pooled together. We do not use a sliding window, which\nmeans that each token is only ever pooled once. This baseline is\ninspired by the common intuition that the individual meaning of\nwords is greatly influenced by its direct neighbours [5, 7].\nK-Means based pooling. This method uses k-means cluster-\ning [18] based on the cosine distance between vectors. The pooling\nfactor is used in this setting to define the total number of clusters,\nwhich is set at INITIAL TOKEN COUNT/POOLING FACTOR + 1.\nHierarchical clustering based pooling. This method uses hier-\narchical clustering [21], again based on the cosine distance between\nvectors. We use Ward's method [31] to produce our clusters, which\nintuitively would be well-suited for this task, as it would seek to\nminimize the distance between the original vector and the pooled\noutputs. Additionally, it has generally been observed to perform\nwell for text data [12, 25].\nUsing this method, we effectively iteratively merge the vectors\nthat minimize the ward distance. In this setting, the pooling fac-\ntor is used to define the maximum number of clusters that can\nbe formed, as a constraint on the clustering algorithm. This effec-\ntively means that this method will result in at most INITIAL TOKEN\nCOUNT/POOLING FACTOR + 1 clusters."}, {"title": "3 EXPERIMENTAL SETTING", "content": "3.1 Implementation\nModels We conduct the vast majority of our experiments with\nColBERTv2 [27], trained on English MS-Marco [22]. To assess that\nour method is not specific to English nor to ColBERTv2, we also\nconduct a smaller set of experiments on Japanese using a Japanese\nversion of ColBERT, JaColBERTv2 [6].\nClustering All clustering methods are implemented using exist-\ning libraries and widely used implementations. We use SciPy [30]\nfor hierarchical clustering and a simple, PyTorch-based [23] imple-\nmentation for k-means clustering. We evaluate clustering on a wide\nrange of pooling factors: 2, 3, 4, 5, 6 and 8.\nIndexing All experiments are conducted using the official Col-\nBERT implementation to encode both queries and documents. 2-bit\nquantization and PLAID [26] indexing are also performed with\nthe original codebase. Experiments with non-quantized vectors are\nconducted using a basic HNSW indexing implementation, via the\nVOYAGER library. We provide further details on indexing parameters\nin Appendix A."}, {"title": "3.2 Evaluation", "content": "Data We evaluate our method on a varied mix of datasets, in order\nto capture the impact of the approach in different domains. To do\nso, we select all small to mid-sized datasets from BEIR [29], the\nmost commonly used retrieval evaluation suite, with the exception\nof ArguAna2, and from LoTTe [27], another benchmark commonly\nused for multi-vector approaches. We define as \"small and mid-\nsized\" any dataset containing fewer than 500,000 documents. We\ndecide on this cutoff in order to appropriately explore a variety\nof data sizes while keeping down the cost of experimentation and\nanalysis. In total, this results in 6 datasets from BEIR and 3 from"}, {"title": "4 RESULTS", "content": "4.1 Unquantized Results\nTable 1 presents the detailed results for various pooling factors\nand pooling methods. In the interest of simplicity and space, we\nonly report the results of sequential pooling for factors 2 and 4,\nas the observed performance degradation varies wildly between\ndatasets and increases is too quickly for this approach to be viable\nin comparison to the other ones. Worth noting however that, de-\nspite its overall noticeably weaker performance, sequential pooling\nperforms remarkably strongly on the SCIDOCS dataset, reaching the\nstrongest performance at a pooling factor of 2, before degrading be-\nhind the other two methods at factor 4. An overview of the relative\nperformance degradation of the best-performing pooling method,\nhierarchical pooling, across pooling factors on all the small-sized\nevaluation datasets can be found in Figure 1.\nWe observe that token pooling performs remarkably well in this\nsetting, on all four datasets. In fact, a pooling factor of 2, resulting\nin a vector count reduction of 50%, actually slightly increases\nretrieval performance on average, doing so on 3 out of 4 datasets.\nA pooling factor of 3 achieves an average performance degradation\nof less than 1%, despite reducing storage requirements by over 66%.\nWe observe a starker degradation from pooling factors of 4 onwards,\nwith an average degradation of 3% and a degradation of around 5%\non two of the evaluated datasets."}, {"title": "4.2 Quantized Results", "content": "The next experiment focuses on evaluating whether this method\ncan be combined with standard ColBERTv2 quantization, enabling\nits practical use on larger datasets.\nAs above, we report the detailed results in Table 2, with the full\nsequential results truncated, and the overview of relative perfor-\nmance degradation with hierarchical pooling in Figure 2.\nImmediately, we notice one obvious outlier: Touch\u00e9 [4]. Not\nonly does performance never decrease with pooling, but it steadily\nincreases, up to a 42.16% increase in retrieval performance at a pool-\ning factor of 6. This dataset has frequently been noted as producing\nunusual results and a recent study has shown that due to its nature\nas an argument-mining dataset repurposed for retrieval, it is an\noutlier in many ways and contains considerable noise [28]. As a\nresult, we choose to leave further exploration of this behavior for\nfuture work.\nThere is a second, less-pronounced, outlier in place of fiqa. How-\never, unlike Touch\u00e9, it does follow a similar trend to other datasets,\nalthough the performance degrades noticeably faster as the pool\nfactor increases. While we leave further analysis to future work,\nfiqa is a highly specialized dataset within the financial domain, and\nits queries tend to focus on very fine-grained details [19], which\ncould partially explain this quicker degradation."}, {"title": "4.3 Vector Count & Storage Reduction", "content": "Table 3 provides a comparison in the number of vector stored as well\nas the disk footprint of storing the full index. We use TREC-Covid\nat a truncated length of 256 tokens per document (ColBERTv2's\ndefault document length [27]) for these calculations. We report the\nnumber of vectors and index size for 16-bit single-vector dense\nrepresentations in an HNSW index, as well as PLAID-indexed Col-\nBERT at various pooling factors. Note that the index size reduction\nis slightly lower than the vector count reduction, due to the indexing\noverhead introduced by PLAID [26].\nWe choose to report 16-bit for dense vectors and 2-bit for PLAID,\nas both methods experience virtually no performance degradation,\ntherefore comparing index without quantization-related perfor-\nmance compromises."}, {"title": "4.4 Japanese Results", "content": "Finally, Table 4 shows the results of token pooling with hierarchical\nclustering applied to Japanese corpora, using the JaColBERTv2 [6]\nmodel. This evaluation is performed only in the quantized setting,\nwith 2-bit compression on a PLAID [26] index."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced TOKEN POOLING, a simple approach\nleveraging existing clustering methods and requiring no training\nnor model modifications to very effectively reduce the numbers\nof tokens needed to store for multi-vector retrieval models such\nas ColBERT. Our results show that the number of vectors, can\nbe reduced by 50% with little-to-no performance degradation on\nthe majority of datasets evaluated, thus drastically reducing the\nsize of ColBERT indexes. Reducing the vector count by 66% still\nexhibits minimal degradation, while further compression results in\nincreasing performance degradation. Notably, this method can lead\nto ColBERT being applied to a broader range of uses, as it facilitates\nthe use of addition/deletion (CRUD)-friendly indexing methods such\nas HNSW. Our experiments also show that these results hold true\neven when combined with ColBERT's 2-bit quantization process,\nallowing for even greater compression than previously possible.\nWe also reproduce our results using a Japanese ColBERT model,\nshowing that this is not limited to a single model, nor to English\ndocuments. With this paper focusing on introducing this method\nand demonstrating its empirical performance, we hope that our\nfindings will help support future research in better understanding\nthe role of individual tokens in multi-vector retrieval and develop\neven stronger compression methods."}, {"title": "A RETRIEVAL SETTING", "content": "The entire retrieval pipeline uses the same approach as the standard\nColBERTv2 [27] + PLAID [26], including the maxSim scoring func-\ntion [11]. For querying the PLAID index, we use the best-performing\nquery hyperparameters reported in a recent reproduction study of\nPLAID [17]: nprobe=8, tcs=0.3 and ndocs=8192.\nFor the HNSW index, we construct it using generous construc-\ntion hyperparameters M = 12 and EFconstruction = 200, designed\nto optimise retrieval performance. At querying time, we use large k\nvalues at candidate generation time to ensure the performance is not\nimpacted by the use of approximate search. These hyperparameter\nchoices effectively mean our results are similar to non-approximate\nsearches.\nOtherwise, we use the default ColBERTv2 and JacolBERTv2 pa-\nrameters, with a respective document length of 256 and 300 tokens.\nWe do not perform any particular optimisation step to maximise\nabsolute retrieval performance, as we are focused on the relative\nperformance of different methods. The exact same settings are used\nfor both the baseline runs, and all token pooling methods compared\nto them."}]}