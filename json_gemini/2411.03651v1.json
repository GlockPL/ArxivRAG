{"title": "Policy Aggregation", "authors": ["Parand A. Alamdari", "Soroush Ebadian", "Ariel D. Procaccia"], "abstract": "We consider the challenge of AI value alignment with multiple individuals that have different reward functions and optimal policies in an underlying Markov decision process. We formalize this problem as one of policy aggregation, where the goal is to identify a desirable collective policy. We argue that an approach informed by social choice theory is especially suitable. Our key insight is that social choice methods can be reinterpreted by identifying ordinal preferences with volumes of subsets of the state-action occupancy polytope. Building on this insight, we demonstrate that a variety of methods - including approval voting, Borda count, the proportional veto core, and quantile fairness - can be practically applied to policy aggregation.", "sections": [{"title": "Introduction", "content": "Early discussion of AI value alignment had often focused on learning desirable behavior from an individual teacher, for example, through inverse reinforcement learning [27, 1]. But, in recent years, the conversation has shifted towards aligning AI models with large groups of people or even entire societies. This shift is exemplified at a policy level by OpenAI's \"democratic inputs to AI\" program [41] and Meta's citizens' assembly on AI governance [8], and at a technical level by the ubiquity of reinforcement learning from human feedback [30] as a method for fine-tuning large language models.\nWe formalize the challenge of value alignment with multiple individuals as a problem that we view as fundamental-policy aggregation. Our starting point is the common assumption that the environment can be represented as a Markov decision process (MDP). While the states, actions and transition functions are shared by all agents, their reward functions - which incorporate values, priorities or subjective beliefs - may be different. In particular, each agent has its own optimal policy in the underlying MDP. Our question is this: How should we aggregate the individual policies into a desirable collective policy?\nA na\u00efve answer is to define a new reward function that is the sum of the agents' reward functions (for each state-action pair separately) and compute an optimal policy for this aggregate reward function; such a policy would guarantee maximum utilitarian social welfare. This approach has a major shortcoming, however, in that it is sensitive to affine transformations of rewards, so, for example, if we doubled one of the reward functions, the aggregate optimal policy may change. This is an issue because each agent's individual optimal policy is invariant to (positive) affine transformations of rewards, so while it is possible to recover a reward function that induces an agent's optimal policy by"}, {"title": "Related Work", "content": "Noothigattu et al. [28] consider a setting related to ours, in that different agents have different reward functions and different policies that must be aggregated. However, they assume that the agents' reward functions are noisy perturbations of a ground-truth reward function, and the goal is to learn an optimal policy according to the ground-truth rewards. In social choice terms, our work is akin to the typical setting where subjective preferences must be aggregated, whereas the work of Noothigattu et al. [28] is conceptually similar to the setting where votes are seen as noisy estimates of a ground-truth ranking [39, 9, 6].\nChaudhury et al. [7] study a problem completely different from ours: fairness in federated learning. However, their technical approach served as an inspiration for ours. Specifically, they consider the proportional veto core and transfer it to the federated learning setting using volumetric arguments, by"}, {"title": "Preliminaries", "content": "For $t \\in \\mathbb{N}$, let $[t] = \\{1, 2, ..., t\\}$. For a closed set $S$, let $\\Delta(S)$ denote the probability simplex over the set $S$. We denote the dot product of two vectors as $\\langle x, y \\rangle = \\Sigma_{i=1}^{d} x_i \\cdot y_i$ for $x, y \\in \\mathbb{R}^d$. A halfspace in $\\mathbb{R}^d$ determined by $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ is the set of points satisfying $\\{x \\in \\mathbb{R}^d \\mid \\langle x, w \\rangle \\le b\\}$. A polytope $O \\subseteq \\mathbb{R}^d$ is the intersection of a finite number of halfspaces, i.e., a convex subset of the $d$-dimensional space $\\mathbb{R}^d$ determined by a set of linear constraints $\\{x \\mid Ax < b\\}$ where $A \\in \\mathbb{R}^{k \\cdot d}$ is a matrix of coefficients of $k$ linear inequalities and $b \\in \\mathbb{R}^k$.\n3.1 Multi-Objective Markov Decision Processes\nA multi-objective Markov decision process (MOMDP) is a tuple defined as $M=(S, A, P, R_1, ..., R_n)$ for the average-reward case and $(S, d_{init}, A, P, R_1, ..., R_n, \\gamma)$ for the discounted-reward case, where $S$ is a finite set of states, $A$ is a finite set of actions, and $P:(S \\times A) \\rightarrow \\Delta(S)$ is the transition probability distribution. $P(s_t, a_t, s_{t+1})$ is the probability of transitioning to state $s_{t+1}$ by taking action $a_t$ in $s_t$. For $i \\in [n], R_i:S \\times A \\rightarrow \\mathbb{R}$ is the reward function of the $i$th agent, the initial state is sampled from $d_{init} \\in \\Delta(S)$, and $\\gamma \\in (0, 1]$ is the discount factor.\nA (Markovian) policy $\\pi(a|s)$ is a probability distribution over the actions $a \\in A$ given the state $s \\in S$. A policy is deterministic if at each state $s$ one action is selected with probability of 1, and otherwise it is stochastic. The expected average return of agent $i$ for a policy $\\pi$ and the expected discounted return of agent $i$ for a policy $\\pi$ are defined over an infinite time horizon as\n$J_i^{avg}(\\pi) = \\lim_{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}_{\\pi, P} \\left[ \\Sigma_{t=1}^{T} R_i(s_t, a_t) \\right] , J_i^{\\gamma}(\\pi) = (1 - \\gamma) \\mathbb{E}_{\\pi, P} \\left[ \\Sigma_{t=1}^{\\infty} \\gamma^{t-1} R_i(s_t, a_t) \\mid s_1 \\sim d_{init} \\right]$\nwhere the expectation is over the state-action pairs at time $t$ based on both the policy $\\pi$ and the transition function $P$.\nDefinition 1 (state-action occupancy measure). Let $P_t$ be the probability measure over states at time $t$ under policy $\\pi$. The state-action occupancy measure for state $s$ and action $a$ is defined as\n$d_\\pi^{avg}(s, a) = \\lim_{T \\rightarrow \\infty} \\frac{1}{T} \\mathbb{E}_{\\pi, P} \\left[ \\Sigma_{t=1}^{T} \\mathbb{I}[s_t = s] \\pi(a|s_t) \\right] , d_\\pi^{\\gamma}(s, a) = (1 - \\gamma) \\mathbb{E}_{\\pi, P} \\left[ \\Sigma_{t=1}^{\\infty} \\gamma^{t-1} \\mathbb{I}[s_t = s] \\pi(a|s_t) \\mid s_1 \\sim d_{init} \\right]$\nFor both the average and discounted cases, we can rewrite the expected return as the dot product of the state-action occupancy measures and rewards, that is, $J_i(\\pi) = \\sum_{(s, a)} d_\\pi(s, a) \\cdot R_i(s, a) =$"}, {"title": "Occupancy Polytope as the Space of Alternatives", "content": "In a MOMDP $M$, each agent $i$ incorporates their values and preferences into their respective reward function $R_i$. Agent $i$ prefers $\\pi$ over $\\pi'$ if and only if $\\pi$ achieves higher expected return, $J_i(\\pi) > J_i(\\pi')$, and is indifferent between two policies $\\pi$ and $\\pi'$ if and only if $J_i(\\pi) = J_i(\\pi')$. As discussed before, given a state-action occupancy measure $d_\\pi$ in the state-action occupancy polytope $O$, we can recover the corresponding policy $\\pi$. Therefore, we can interpret $O$ as the domain of all possible alternatives over which the $n$ agents have heterogeneous weak preferences (with ties). Agent $i$ prefers $d_\\pi$ to $d_{\\pi'}$ in $O$ if and only if they prefer $\\pi$ to $\\pi'$. We study the policy aggregation problem through this lens; specifically, we design or adapt voting mechanisms where the (continuous) space of alternatives is $O$ and agents have weak preferences over them determined by their reward functions $R_i$.\nAffine transformation and reward normalization. A particular benefit of this interpretation, as mentioned before, is that all positive affine transformations of the reward functions, i.e., $aR_i + b$ for all $a \\in \\mathbb{R}_{\\ge 0}$ and $b \\in \\mathbb{R}$, yield the same weak ordering over the polytope. Hence, we can assume without loss of generality that $J_i(\\pi) \\in [0, 1]$. Further, we can ignore agents that are indifferent between all policies, i.e., $\\min_{\\pi} J_i(\\pi) = \\max_{\\pi} J_i(\\pi)$, and normalize reward functions $\\frac{R_i - \\min_\\pi J_i(\\pi)}{\\max_\\pi J_i(\\pi) - \\min_\\pi J_i(\\pi)}$ such that $\\min_{\\pi} J_i(\\pi) = 0$ and $\\max_{\\pi} J_i(\\pi) = 1$. The relative ordering of the policies does not change since for all points $d_\\pi \\in O$ we have $\\sum_{s, a} d_\\pi(s, a) = 1$.\nVolumetric definitions. A major difference between voting over a continuous space of alternatives and the classical voting setting is that the domain of alternatives is infinite and not all voting"}, {"title": "Fairness in Policy Aggregation", "content": "In this section, we utilize the volumetric interpretation of the state-action occupancy polytope to extend fairness notions from social choice to policy aggregation, and we develop algorithms to compute stochastic policies provably satisfying these notions.\n5.1 Proportional Veto Core\nThe proportional veto core was first proposed by Moulin [26] in the classical social choice setting with a finite set of alternatives where agents have full (strict) rankings over the alternatives. For simplicity, suppose the number of alternatives $m$ is a multiple of $n$. The idea of the proportional veto core is that $x\\%$ of the agents should be able to veto $x\\%$ of the alternatives. More precisely, for an alternative $c$ to be in the proportional veto core, there should not exist a coalition $S$ that can \u201cblock\u201d $c$ using their proportional veto power of $|S|/n$. $S$ blocks $c$ if they can unanimously suggest $m(1 - |S|/n)$ candidates that they prefer to $c$. For instance, if $c$ is in the proportional veto core, it cannot be the case that a coalition of $60\\%$ of the agents unanimously prefer $40\\%$ of the alternatives to $c$.\nChaudhury et al. [7] extended this notion to a continuous domain of alternatives in the federated learning setting. We show that such an extension also applies to policy aggregation.\nDefinition 5 (proportional veto core). Let $\\epsilon \\in (0, 1/n)$. For a coalition of agents $S \\subseteq [n]$, let $veto(S) = \\frac{|S|}{n}$ be their veto power. A point $d_\\pi \\in O$ is blocked by a coalition $S$ if there exists a subset $O' \\subseteq O$ of measure $\\frac{vol(O')}{vol(O)} \\ge 1 - veto(S) + \\epsilon$ such that all agents in $S$ prefer all points in $O'$ to $d_\\pi$, i.e., $d_{\\pi'} \\succ_i d_\\pi$ for all $d_{\\pi'} \\in O'$ and $i \\in S$. A point $d_\\pi$ is in the $\\epsilon$-proportional veto core if it is not blocked by any coalition.\nA candidate in the proportional veto core satisfies desirable properties that are extensively discussed in prior work [26, 7, 24, 22]. It is worth mentioning that any candidate in the $\\epsilon$-proportional veto core, besides the fairness aspect, is also economically efficient as it satisfies $\\epsilon$-Pareto optimality. This holds since the grand coalition $S = [n]$ can veto any $\\epsilon$-Pareto dominated alternative."}, {"title": "Policy Aggregation with Voting Rules", "content": "In this section, we adapt existing voting rules from the discrete setting to policy aggregation and discuss their computational complexity.\nPlurality. The plurality winner is the policy that achieves the maximum number of plurality votes or \"approvals,\u201d where agent $i$ approves a policy $\\pi$ if it achieves their maximum expected return $J_i(\\pi) = \\max_{\\pi'} J_i(\\pi') = 1$. Hence the plurality winner is a policy in $\\arg \\max_\\pi \\Sigma_{i \\in [n]} \\mathbb{I}[J_i(\\pi) = 1]$. This formulation does not require the volumetric interpretation. However, in contrast to the discrete setting where one can easily count the approvals for all candidates, we show that solving this problem in the context of policy aggregation is not only NP-hard, but hard to approximate up to factor of a $1/n^{1/2 - \\epsilon}$. We establish the hardness of approximation by a reduction from the maximum independent set problem [20]; we defer the proof to Appendix A.\nTheorem 4. For any fixed $\\epsilon \\in (0, 1)$, there is no polynomial-time $\\frac{1}{n^{1/2 - \\epsilon}}$-approximation algorithm for the maximum plurality score unless P = NP.\nNevertheless, we can compute plurality in practice, as we discuss below.\n$\\alpha$-approval. We extend the $k$-approval rule using the volumetric interpretation of the occupancy polytope, similarly to the $q$-quantile fairness definition. For some $\\alpha \\in [0, 1]$, agents approve a policy $\\pi$ if its return is among their top $\\alpha$ fraction of $O$, i.e., $F_i(J_i(\\pi)) \\ge \\alpha$. The $\\alpha$-approval winner is a policy that has the highest number of $\\alpha$-approvals, so it is in $\\arg \\max_\\pi \\Sigma_{i \\in [n]} \\mathbb{I}[F_i(J_i(\\pi)) \\ge \\alpha]$. Note that plurality is equivalent to 1-approval. It is worth mentioning that there can be infinitely many policies that have the maximum approval score and, to avoid a suboptimal decision, one can return a Pareto optimal solution among the set of $\\alpha$-approval winner policies.\nTheorem 2 shows that for $\\alpha \\le 1/e$, there always exists a policy that all agents approve, and by Proposition 3 such policies can be found in polynomial time, assuming access to an oracle for volumetric computations. Therefore, the problem of finding an $\\alpha$-approval winner is \"easy\" for $\\alpha \\in (0, 1/e)$. In sharp contrast, for $\\alpha = 1$\u2014namely, plurality - Theorem 4 gives a hardness of approximation. The next theorem shows the hardness of computing $\\alpha$-approval for $\\alpha \\in (7/8, 1]$ via a reduction from the MAX-2SAT problem. We defer the proof to Appendix A.\nTheorem 5. For $\\alpha \\in (7/8, 1]$, computing a policy with the highest $\\alpha$-approval score is NP-hard. This even holds for binary reward vectors and when every $F_i$ has a closed form.\nGiven the above hardness result, to compute the $\\alpha$-approval rule, we turn to mixed-integer linear programming (MILP). Algorithm 3 simply creates $n$ binary variables for each agent $i$ indicating whether $i$ $\\alpha$-approves the policy, i.e., $F_i(J_i(\\pi)) \\ge \\alpha$ which is equivalent to $J_i(\\pi) \\ge F_i^{-1}(\\alpha)$. To encode the expected return requirement for agent $i$ to approve a policy as a linear constraint, we precompute $F_i^{-1}(\\alpha)$. This can be done by a binary search similar to Algorithm 2. Importantly, Algorithm 3 has one binary variable per agent and only $n$ constraints which is key to its practicability.\nBorda count. The Borda count rule also has a natural definition in the continuous setting. In the discrete setting, the Borda score of agent $i$ for alternative $c$ is the number of alternatives $c'$ such that $c \\succ_i c'$. In the continuous setting, $F_i(J_i(\\pi))$ indicates the volume of the occupancy polytope to which agent $i$ prefers $\\pi$. The Borda count rule then selects a policy among $\\arg \\max_\\pi \\Sigma_{i \\in [n]} F_i(J_i(\\pi))$."}, {"title": "Discussion", "content": "We conclude by discussing some of the limitations of our approach. A first potential limitation is computation. When we started our investigation of the policy aggregation problem, we were skeptical that ordinal solutions from social choice could be practically applied. We believe that our results successfully lay this initial concern to rest. However, additional algorithmic advances are needed to scale our approach beyond thousands of agents, states, and actions. Additionally, an interesting future direction is to apply these rules within continuous state or action spaces, as well as in online reinforcement learning setting where the environment remains unknown.\nA second limitation is the possibility of strategic behavior. The Gibbard-Satterthwaite Theorem [16, 33] precludes the existence of \"reasonable\u201d voting rules that are strategyproof, in the sense that agents cannot gain from misreporting their ordinal preferences; we conjecture that a similar result holds for policy aggregation in our framework. However, if reward functions are obtained through inverse reinforcement learning, successful manipulation would be difficult: an agent would have to act in a way that the learned reward function induces ordinal (volumetric) preferences leading to a higher-return aggregate stochastic policy. This separation between the actions taken by an agent and the preferences they induce would likely alleviate the theoretical susceptibility of our methods to strategic behavior."}]}