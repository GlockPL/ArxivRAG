{"title": "DYNAMATH: A DYNAMIC VISUAL BENCHMARK FOR EVALUATING MATHEMATICAL REASONING ROBUSTNESS OF VISION LANGUAGE MODELS", "authors": ["Chengke Zou", "Xingang Guo", "Rui Yang", "Junyu Zhang", "Bin Hu", "Huan Zhang"], "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that state-of-the-art VLMs like GPT-40 can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DYNAMATH, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DYNAMATH includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DYNAMATH allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 state-of-the-art VLMs with 5,010 generated concrete questions (10 per seed question). Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. In addition, many models show high consistency in answering these questions \u2013 the incorrectness of a certain variant of a seed question is not due to inherent randomness. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DYNAMATH provides valuable insights to guide the development of more reliable models for mathematical reasoning.", "sections": [{"title": "INTRODUCTION", "content": "Leveraging pretraining on vast Internet-scale datasets, Large Language Models (LLMs)  and Multi-modal Large Language Models (MLLMs)  have achieved remarkable performance across a wide range of tasks. Among them, Vision-Language Models (VLMs) stand out, showing exceptional promise as versatile assistants capable of integrating vision and language for problem-solving.\nAmong their visual comprehension abilities across different domains, mathematical reasoning stands out as a crucial measure of human-like intelligence, requiring both math knowledge and logical thinking. Recent work has proposed many benchmarks for evaluating the mathematical reasoning ability of VLMs. MATHVISTA (Lu et al., 2023)"}, {"title": "RELATED WORK", "content": "Mathematical Reasoning Benchmarks. Reasoning ability is a key indicator of intelligence, prompting researchers to develop various benchmark datasets to assess the mathematical reasoning capabilities of LLMs and VLMs. Numerous benchmarks have been proposed for evaluating this ability in the text-only domain, including . Additionally, recent research has begun to shift its focus towards the evaluation of robustness and the"}, {"title": "BENCHMARK DESIGN", "content": "We present DYNAMATH, a curated evaluation dataset aimed at assessing the robustness of visual language models (VLMs) in multimodal mathematical reasoning across a wide variety of mathematical tasks with dynamic visual and textual contexts."}, {"title": "DATASET COLLECTION", "content": "Our benchmark collection comprises two phases: seed question collection and program-based question generation. In the initial phase, we selectively curate a set of high-quality mathematics problems that necessitate reasoning based on visual information. The subsequent phase involves transforming each seed question into code-based prototypes, allowing for the generation of diverse concrete questions under randomly sampled conditions.\nSeed question Collection. The seed questions are sourced from existing visual math datasets and publicly available online resources. We identify 107 questions from MathVista (Lu et al., 2023), covering fundamental concepts in analytic geometry, planar geometry, and statistics. Additionally, we source 27 questions from MATH-V (Wang et al., 2024a), which serve as prototypes for topics related to arithmetic, puzzle tests, and solid geometry. To augment the dataset's breadth and depth, we included 45 questions based on scientific figures and 48 undergraduate-level questions focused on graph theory, drawn from the MMMU dataset (Yue et al., 2024) and various accessible educational materials. Furthermore, we incorporated 236 questions requiring advanced reasoning on topics such as functions, geometry, and statistics, all gathered from publicly available resources on the Internet. To diversify the question types represented in our collection, we also developed 38 new problems by ourselves covering linear algebra, set theory, and algorithmic flow.\nFollowing the collection of seed questions, we conducted a comprehensive review to eliminate any questions that included excessively complex images, as these would pose challenges for programmatic generation. Ultimately, as shown in Figure 3(b), our benchmark consists of 501 seed questions, with 227 (45.3%) sourced from established visual math datasets, while 274 (54.7%) are newly collected or developed from public resources.\nNote that our goal is not to create the most challenging, competition-level benchmark as in (Wang et al., 2024a), but rather to provide relatively easy benchmarks with diverse variants to evaluate robustness. Nonetheless, we ensure that the difficulty of our questions is comparable to the levels of datasets such as MATHVERSE (Zhang et al., 2024e) and MATHVISTA (Lu et al., 2023)."}, {"title": "Program-based Question Generation.", "content": "After establishing our seed questions, we recruited a group of college STEM students to annotate each question with the common strategies they employed in solving them. These annotations served as prototypes for developing corresponding programs tailored to each question. As illustrated in Figure 2, each question is represented as a carefully crafted Python program, which encompasses a defined range of conditions for sampling and algorithmic calculations to derive the solution. Additionally, we implemented a drawing function in each program, utilizing libraries such as Matplotlib and Pyglet to generate corresponding images based on varying conditions. Specifically, 470 of the question programs incorporate a plotting function that leverages the randomly sampled conditions to create the visual context of the question, while the remaining 31 question programs utilize fixed images, randomizing only the textual elements. This programmatic approach allows the generation of a large number of concrete benchmark questions by executing the generation program multiple times, facilitating the efficient creation of new problems and enabling the evaluation of the reasoning robustness of VLMs.\nIn DYNAMATH, we integrate various types of variants to enrich the diversity of question generation:\n1. Numerical Value Variants: Modifying numerical quantities to evaluate the VLM's proficiency in handling different numerical values and performing arithmetic operations.\n2. Geometric Transformations: Altering shapes, angles, dimensions, and relative positions to examine the spatial and geometric understanding of VLMs.\n3. Function Type Variants: Varying different types of mathematical functions (e.g., linear, quadratic) to evaluate how well models generalize across functional representations.\n4. Color Variants: Changing object or curve colors randomly to test the model's recognition of visual patterns and its robustness to superficial alterations.\n5. Symbolic Substitutions: Modifying symbolic elements such as mathematical operations to determine the model's adaptability to various symbolic representations.\n6. Graph Structure Variants: Modifying graph layouts, networks, or other structural representations to assess the model's comprehension of relationships and topological features.\n7. Real-life Contexts Variants: Adjusting the contents of real-world scenarios (e.g., calendars, time-related problems, or poker-like questions) to test the model's contextual understanding and application to practical situations.\nEach variant category targets a specific facet of mathematical reasoning, making DYNAMATH a comprehensive benchmark for evaluating the flexibility, robustness, and accuracy of VLMs in solving mathematical problems. Detailed diagrams of each variation are provided in Appendix A."}, {"title": "DATASET STATISTICS", "content": "Detailed statistics on the data composition of DYNAMATH are presented in Table 1. DYNAMATH encompasses nine mathematical topics: Solid Geometry (SG, 3.0%), Puzzle Tests (PT, 3.4%), Arithmetic (AR, 5.2%), Scientific Figures (SF, 9.0%), Graph Theory (GT, 9.6%), Algebra (AL, 10.2%), Plane Geometry (PG, 15.4%), Analytic Geometry (AG, 19.4%), and Statistics (ST, 25.0%). Examples for each topic are provided in Appendix C. Each topic necessitates a nuanced understanding of image context, foundational mathematical knowledge, practical reasoning abilities, and logical deduction skills. Importantly, the dataset is designed to cater to varying levels of difficulty, ranging from elementary to undergraduate education, with a notable focus on high school (55.3%) and undergraduate (32.1%) levels. In terms of question types, the dataset consists of 35.5% multiple-choice questions and 64.7% free-form questions. While VLMs might occasionally answer multiple-choice questions correctly by chance, free-form questions provide a more precise evaluation of the model's capabilities. Consequently, our dataset emphasizes free-form questions, distinguishing it from previous visual math benchmarks such as MATHVISTA (Lu et al., 2023), MATHVERSE (Zhang et al., 2024e), and MATH-V (Wang et al., 2024a), which predominantly include more than 50% multiple-choice questions.\nIn Figure 3(a), we depict the distribution of variant numbers among the 501 seed questions. Notably, approximately 30.5% of the seed questions have a possible variant number ranging from 10 to $10^2$. Nearly 93% of the seed questions contain more than 10 variants, and 17.4% of seed questions have more than $10^6$ potential variants, demonstrating the diversity of variations in our dataset."}, {"title": "EVALUATION PROTOCOLS", "content": "Our evaluation process consists of two stages: answer extraction and score calculation. Following the methodology of prior work , we utilize prompt engineering and template matching to extract answers. Prompts guide the model to generate responses in both full and short-answer formats. After generation, the short answer is extracted for comparison with the ground truth. Detailed prompts used in our experiments can be found in Appendix B.\nOur dataset contains $N = 501$ seed questions in total. For each seed question in the dataset, we generate $M = 10$ variants, resulting in a total of 5,010 concrete questions. Inspired by prior research in LLMs , we evaluate two metrics: average-case accuracy ($A_{avg}$) and worst-case accuracy ($A_{wst}$) over these variants. The two metrics are different from prior benchmarks that evaluate only a single instance of a question. The metrics are defined as follows:\n$A_{avg} = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{M} \\sum_{j=1}^{M} \\mathbb{I}[Ans(i, j) = GT(i, j)],$\n$A_{wst} = \\frac{1}{N} \\sum_{i=1}^{N} min_{j \\in [1, M]} \\mathbb{I}[Ans(i, j) = GT(i, j)],$\nwhere $Ans(i, j)$ and $GT(i, j)$ represent the generated answer and the ground truth answer for variant $j$ of question $i$. We also define reasoning robustness (RR) as the ratio between the average-case performance and the worst-case performance:\n$RR = \\frac{A_{wst}}{A_{avg}}$\nThe model's response uncertainty reflects both the impact of input changes and inherent uncertainty, the latter of which can be represented by the concept of repetition consistency (RC), similar to self-consistency. We define repetition consistency as:\n$RC(i, j) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{I}[Ans_k(i, j) = Ans(i, j)],$\nwhere $K$ is number of repetitions and $Ans_k(i, j)$ is the k-th repetition for j-th variant of i-th seed question. The repetition consistency represents the model's confidence in the answer $Ans(i, j)$."}, {"title": "EXPERIMENT", "content": "In this section, we conduct thorough experiments to assess the performance and reasoning robustness of various closed-source and open-source models on the DYNAMATH dataset. Subsequently, we present detailed quantitative results and qualitative analyses in Sections 4.2 and 4.3, respectively."}, {"title": "EXPERIMENTAL SETUPS", "content": "We evaluate the performance of two sets of models on the DYNAMATH benchmark, which involves 10 variations for each seed question, resulting in a total of 5010 questions. The first group comprises SOTA closed-source VLMs, such as GPT-4o, Gemini Pro 1.5, and Claude-3.5 Sonnet, with zero-shot and 3-shots with Chain-of-Thought (CoT) configurations. The second group consists of SOTA open source VLMs, including Qwen2-VL (7B, 72B) , InternVL2 (8B, 26B, 40B, 76B) , Llava-v1.5 (7B) , Llava-v1.6 (13B, 34B) , Deepseek-VL (7B) , and Llama 3.2 (90B) . We specifically explored open-source models with varying parameter sizes to analyze the impact of model size on reasoning robustness. The deployment of open-source models is based on the lmdeploy package (Contributors, 2023). Details regarding the prompts and hyperparameters used in this experiment are outlined in Appendix B.\nTo assess human performance, we generated a new variant dataset consisting of 501 concrete questions (1 variant per seed question). These questions were divided into 10 questionnaires, each containing 50 to 51 questions. We then recruited 10 undergraduates or graduates from STEM to help establish the baseline for human performance based on their average scores.\nFor the few-shot setup, we follow the standard approach by including three demonstration examples, each accompanied by the associated visual elements. Given the diverse range of topics covered in DYNAMATH, we provide topic-specific demonstration examples to ensure its relevance for each problem in DYNAMATH. Specifically, we curated five demonstration examples from MathVista (Lu et al., 2023) and MathVision (Wang et al., 2024a) for each topic, and then randomly selected three examples when evaluating DYNAMATH problems within the corresponding topic. In addition, we incorporate detailed reasoning steps in the demonstration examples, following a typical Chain-of-Thought (CoT) setup (Wei et al., 2022). Detailed demonstrations and prompts are listed in Appendix B.4."}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we present a detailed comparison of the top-performing VLMs on DYNAMATH, as shown in Table 2 and Table 3."}, {"title": "Overall Results on Average Accuracy.", "content": "Table 2 illustrates the average-case performance of a variety of closed-source and open-source models. We respectively highlight the highest scores in each category for closed-source and open-source models. Within the closed-source models, GPT-4o, Claude-3.5, and Gemini Pro 1.5 exhibit average accuracies higher than 60%, with Claude-3.5 achieving the highest zero-shot average accuracy at 64.8%. However, there remains an 11.0% disparity when compared to human performance, which stands at 75.8% average accuracy. This highlights the need for further development in the reasoning ability of VLMs. Regarding the 3-shot CoT performance, it is intriguing to note that there is no consistent improvement across different closed-sourced models, confirming findings from previous research . For instance, while 3-shot CoT GPT-4o manages to enhance zero-shot performance from 63.7% to 64.9%, both 3-shot CoT Claude-3.5 and 3-shot CoT Gemini Pro 1.5 experience a decline in performance (64.8% \u2192 62.5% and 60.5% \u2192 58.7% respectively). Moving on to the open-sourced models, although they generally underperform when compared to closed-sourced models, the gap has been narrowed by recent models such as Qwen2 and InternVL2, which have more than 70B parameters. This noteworthy progress is evident when comparing them to previous benchmark results like MATHVISTA , MATHVERSE , and MATH-V . It highlights the promising potential of open-source models in the visual math reasoning domain. Moreover, there is a clear scaling trend observed in open-source models, indicating higher performance as model sizes increase. For example, Qwen2-VL boosts its score from 42.1% to 55.1% when scaling its parameter size from 7B to 72B, while InternVL2 sees an increase from 39.7% to 54.0%."}, {"title": "Overall Results on Worst-case Accuracy.", "content": "Table 3 presents the worst-case accuracy of different models across 10 problem variants, revealing a significant decline in scores for all models. Notably, the highest-performing model, Claude-3.5, achieves a zero-shot score of only 35.3%, indicating current VLMs are not sufficiently robust to handle variations in context and images. The situation is even more concerning for open-source models: the best-performing model, Qwen2-VL-72B, achieves a score of 28.3%, while smaller models like Llava-v1.6-vicuna-13B score only 2.8%. Our evaluation results highlight the limited reasoning robustness of both open-source and closed-source models, underscoring the necessity for the community to address these limitations in future research."}, {"title": "Fine-grained Results.", "content": "In Table 2 and Table 3, we present detailed results categorized by different question topics and difficulty levels. From a topical perspective, we observe that the Puzzle Test (PT) topic challenges both open-source and closed-source models. The top-performing closed-source model, GPT-4o, and the leading open-source model, Qwen2-VL-72B, achieve average-case accuracies of 51.8% and 28.2%, respectively, while humans score 76.5%. Notably, all open-source models demonstrate poor performance (0.0%) on the worst-case accuracy metric, except Llama-3.2-90B (5.9%). Despite this gap, Table 2 shows that closed-source models such as Claude-3.5 can surpass human scores on specific topics like Algebra (AL), Graph Theory (GT), and Statistics (ST), which is promising. When considering difficulty levels, all models demonstrate a trend of"}, {"title": "Reasoning Robustness.", "content": "We use the reasoning robustness (RR) metric, defined in Eq 2, to measure the robustness of VLMs by evaluating the relative performance robustness across question variants. Figure 4 (top) compares the RR of all VLMs in our experiments. Notably, GPT-4o and Claude-3.5 exhibit the highest robustness among all tested models. Additionally, aligning with previous findings, closed-source models typically demonstrate greater robustness than open-source ones. However, Qwen2-72B and InternVL2-76B outperform Gemini, highlighting the robustness limitations of even large models like Gemini. In Figure 4 (middle), we compare the reasoning robustness across different question topics for GPT-4o and Qwen2-VL-72B. The results show that the two VLMs are particularly robust in Arithmetic and Algebra question types, indicating their strong arithmetic calculation abilities, which are less affected by changes in visual conditions. However, GPT-40 still exhibits weaknesses in the Puzzle Test. Similarly, Qwen2-VL-72B shows shortcomings in both Puzzle Test and Analytic Geometry topics, achieving nearly 0% RR and 30% RR, respectively. In terms of different variant types in DYNAMATH, as shown in Figure 4 (bottom), we find that both GPT-40 and Qwen2-VL-72B are sensitive to variations in graph structure, geometric transformation, and function type. Additionally, Qwen2-VL-72B is vulnerable to symbolic substitution variants. These weaknesses suggest directions for future improvement of these models."}, {"title": "Repetition Consistency.", "content": "To ensure a robust analysis and account for the inherent randomness in model outputs, we calculate repetition consistency (RC) as defined in Eq 3. This metric evaluates the model's output confidence across multiple generations for the same question. Specifically, we produce five responses for 501 questions and then compute their consistency relative to the first response. The results, detailed in Table 4, reveal the consistent outputs of four closed-source and"}, {"title": "QUALITY STUDY", "content": "Consistent Failure Cases. An interesting phenomenon we observed is that some seed questions are solvable in certain variants but result in consistent failures in others (repetition consistency RC = 1 for 5 or 10 repetitions). The example in Figure 1 is a representative case: the question is easily solvable when the absolute value function is at the origin, but any shifts tend to lead to consistent failures on GPT-40. We extensively examined our dataset and counted the number of such instances. Specifically, GPT-40, Gemini Pro 1.5, Qwen2-VL-72B, and InternVL2-76B exhibited 21.8%, 18.4%, 29.9%, and 28.3% of these types of questions, respectively, out of our 501 seed questions. These examples highlight the unreliability of VLMs on mathematical reasoning tasks.\nQualitative Examples of GPT-40. In this section and Appendix F, we provide a few qualitative examples of leading VLMs' answers. Our analysis reveals that current VLMs can consistently produce incorrect responses to specific question variants while generating accurate answers to others. As illustrated in Figure 1, GPT-40 demonstrates the ability to provide correct responses in variant 7, showcasing accurate perception, question understanding, and reasoning ability. However, in variant 9, where the underlying required capabilities remain the same with only a slight shift in the image, GPT-40 fails to accurately interpret the function's position with a high degree of confidence and consistency. This discrepancy raises concerns about the reasoning robustness of current VLMs. For additional examples of GPT-4o and other models, please refer to the Appendix F.\nMemorization Phenomenon. In our experiments, we observe a phenomenon where current VLMs tend to provide the same answer regardless of changing conditions, indicating memorization rather than reasoning based on generalized underlying principles. When we test variant questions that have the same structure but different parameters and images, the model frequently offers the same answer with high probability, ignoring the specific variations we introduced. Among the 171 questions incorrectly answered by Claude 3.5 Sonnet, this issue accounts for 4.1% of instances. A representative case is illustrated in Figure 5, where altering the period of a sinusoidal function (e.g., from 2 to \u03c0or 4\u03c0) does not affect the model's response, which consistently remains 2\u03c0. The existence of this phenomenon highlights the models' lack of comprehensive problem analysis and their limited ability to generalize across different scenarios.\nError Analysis. We conducted an error analysis on Claude 3.5 Sonnet to identify potential failure modes on DYNAMATH. Specifically, we analyzed the 169 questions where Claude 3.5 Sonnet failed, examining the reasoning paths and final answers in detail. The statistical distribution of various error types is presented in Figure 6. We considered five types of errors: figure reading errors, reasoning errors, knowledge errors, calculation errors, and hallucination errors. Figure reading errors account for 32.2% of the total errors, despite Claude 3.5 Sonnet having specially reinforced perception capabilities. This indicates that there is still a considerable way to go for VLMs to accurately read and interpret data from images. Reasoning errors account"}, {"title": "CONCLUSION", "content": "In this work, we introduce DYNAMATH, a dynamic visual math benchmark designed to systematically analyze the robustness of mathematical reasoning capabilities in current leading vision-language models (VLMs). By employing program-based problem generation, we can create diverse variants by altering visual and textual conditions in the seed problems. Our evaluation reveals that leading closed-source and open-source VLMs are sensitive to condition changes in question variants, despite their required underlying capabilities remaining the same. This raises significant concerns within the VLM community on mathematical reasoning tasks. Our detailed results and analysis not only identify the weak points of current VLMs but also shed light on the causes of their errors, thereby facilitating the evaluation and development of more robust VLMs. Moving forward, an intriguing approach to enhance VLM robustness involves leveraging adversarial training on DYNAMATH, or utilizing reinforcement learning from human feedback with fine-grained process rewards or more robust rewards.\nLimitation. Although our benchmark matches the difficulty levels of MATHVERSE and MATH-VISTA, one limitation of our work is that the difficulty level is relatively limited compared to MATH-V , due to the dynamic nature of the questions. Adapting very challenging questions into our program structures requires substantial human effort, which currently prevents us from curating a large number of complex visual math reasoning questions. In the future, we hope to leverage strong foundational models to aid in designing an automatic pipeline for dynamic math question design and generation."}]}