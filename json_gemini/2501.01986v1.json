{"title": "FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models", "authors": ["Tianyu Fu", "Tengxuan Liu", "Qinghao Han", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Xuefei Ning", "Yu Wang"], "abstract": "The increasing demand to process long and high-resolution videos significantly burdens Large Vision-Language Models (LVLMs) due to the enormous number of visual tokens. Existing token reduction methods primarily focus on importance-based token pruning, which overlooks the redundancy caused by frame resemblance and repetitive visual elements. In this paper, we analyze the high vision token similarities in LVLMs. We reveal that token similarity distribution condenses as layers deepen while maintaining ranking consistency. Leveraging the unique properties of similarity over importance, we introduce FrameFusion, a novel approach that combines similarity-based merging with importance-based pruning for better token reduction in LVLMs. FrameFusion identifies and merges similar tokens before pruning, opening up a new perspective for token reduction. We evaluate FrameFusion on diverse LVLMS, including Llava-Video-{7B,32B,72B} and MiniCPM-V-8B, on video understanding, question-answering, and retrieval benchmarks. Experiments show that FrameFusion reduces vision tokens by 70%, achieving 3.4 \u2013 4.4\u00d7 LLM speedups and 1.6 \u2013 1.9\u00d7 end-to-end speedups, with an average performance impact of less than 3%. Our code is available at https://github.com/thu-nics/FrameFusion.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (LVLMs) have shown outstanding abilities across various video understanding scenarios, including temporal and spatial perception, recognition, and reasoning [4, 14, 29, 34]. Growing applications also demand LVLMs' ability to process longer and more complex videos [18, 21, 24]. However, video understanding incorporates substantial computing overhead for LVLMs. Common LVLMs sample frames from the video, split each into image patches, and then sequentially embed them as vision tokens with a visual encoder. Though effective, it requires the processing of a huge amount of vision tokens for LVLMs. Take Google Gemini as an example, with the standard sampling rate of 1 frame per second (fps), it requires an astonishing amount of one million tokens to understand an hour's video [21]. The importance-based token pruning has been extensively explored by previous works to solve the efficiency problem. They reduce vision tokens with various importance matrices, like accumulative attention score [7, 23, 35] and L2-norm of token feature [3]. However, there exist extensive important yet similar vision tokens in LVLMs due to frame resemblance and similar visual elements, while the effect of these tokens is less explored. In this work, we recover similarity as a perpendicular property to importance for vision tokens reduction. The main idea is that although some frames are important for visual understanding, their high similarity leads to redundant information. These similar tokens, like those alike image patches from adjacent frames, hardly provide additional information, thus can be merged. To fully harness the potential of similarity-based merging, we study the token similarity distribution of LVLMs by answering three questions:\n1. Where and when does similarity appear?\n2. What is the value distribution of token similarity?\n3. How consistent is the ranking of token similarity across layers?\nBased on the observations, we propose FrameFusion to combine both similarity and importance for efficient LVLM token reduction. FrameFusion first evaluates the token similarity with minimum overhead and progressively merges similar tokens. After the similarity-based merging, FrameFusion then utilizes importance-based pruning to reduce tokens to the given computing budget.\nOur contributions are three-folds:\n1. We analyze the characteristics of token similarities across input positions and layers of LVLM.\n2. We propose FrameFusion to hierarchically merge similar vision tokens and prune unimportant ones in a plug-and-play manner.\n3. We conduct extensive experiments to show the effectiveness of FrameFusion on various video tasks.\nExperiments show that FrameFusion pushes the Pareto front further for token compression. It successfully reduces the token budget and computing FLOPs to 30%, while maintaining comparable performance with the original dense model. The simple yet effective design helps FrameFusion to generalize across various model sizes, input lengths, and video tasks."}, {"title": "2. Related Work", "content": "2.1. Large Vision Language Model (LVLMS)\nThe LVLM architecture typically consists of a vision encoder and a Large Language Model (LLM) [4, 13, 14, 18, 24, 29, 34]. The vision encoder converts visual inputs into token sequences, which the LLM then processes alongside text sequences to generate responses. Specifically, for video input, frames are first sampled temporally and then specially divided into sequences of image patches before sending to the vision encoder [14, 29, 34], as shown in Figure 1. Due to the high temporal and spatial resolution demands of complex video understanding tasks, token lengths can reach up to one million for an hour-long video [21], imposing significant computational overhead on LVLMs.\n2.2. Token Compression\nMotivated by the heavy overhead of video processing, token compression becomes an essential method for LVLM efficiency. Existing methods compress tokens at three subsequent processes.\nThe first branch of work reduces the initial visual inputs before sending them to the vision encoder. They set rules to mix different temporal sampling frequencies [28, 34] and special resolutions [5, 29] when converting videos to input sequences, introducing trade-offs between visual detail and efficiency. Despite the simplicity, they inevitably incur direct detail losses and neglect the visual content guidance during compression.\nOther works reduce tokens inside the vision encoder. They selectively retrieve [11] or condense [18] vision tokens in the vision encoder based on the guidance of text instructions. Yet, they require re-encoding all vision tokens if the text instruction changes, which incurs significant overheads for common multi-round conversation scenarios. Besides, an additional model fine-tuning is often needed to align the new vision encoding space.\nAnother branch of work focuses on token reduction in the subsequent LLM. For text-only tasks, previous works design static [8, 10, 26] or dynamic [9, 12, 19, 35] pruning pattern based on the importance of token (or KV-Cache). Emerging concurrent works highlight the specific token importance distribution for vision-language tasks [2, 16, 22, 31, 33], which further increase the sparsity of importance-based token pruning. However, as shown in Figure 4, token importance is inconsistent across different layers. It incurs prediction loss by pruning an unimportant token at shallow layers, which becomes important but inaccessible at deeper layers. FrameFusion falls in this category, exploring a more consistent and perpendicular token reduction method: similarity-based token merging."}, {"title": "3. Token Similarity Analysis", "content": "While token importance properties of LVLMs have been extensively studied [2, 16, 22, 33], a thorough and comparative analysis of token similarity remains lacking. Thus, we conduct oracle experiments to answer three key questions regarding the characteristics of token similarity, comparing them with token importance.\n3.1. Experiment Setups and Notion Definitions\nIn this section, we present the oracle results with the Llava-Video-7B [34] model tested on 128 video samples (64 frames at 1 fps) from the comprehensive VideoMME [6] datasets. All metrics are averaged over all test samples unless otherwise noted. Results on different models are similar and provided in Appendix 10.2.\nWe define token importance and similarity using the notations $I^{(l)} \\in \\mathbb{R}^{N}$ and $S^{(l)} \\in \\mathbb{R}^{N}$, respectively, where $l$ indicate the layer index of the LLM. For clarity, we omit $l$ where it does not introduce ambiguity. We use the subscript $t$ to index the token along the input length dimension N. The token features at layer $l$ are represented as $X^{l} \\in \\mathbb{R}^{N \\times d}$.\nFollowing previous works [2, 35], the token importance $I_t$ is quantified using the cumulative attention score, calculated by summing the post-softmax attention scores vertically across the t-th column and averaging this sum across all attention heads at layer l.\nFor token similarity $S_t$, we specifically define it as the cosine similarity between a vision token and its corresponding token from the preceding frame. The rationale for this definition is detailed in Section 3.2. Formally, if each frame is encoded into P vision tokens, token similarity is calculated using the following equation:\n$S_{t} = \\frac{X_{t-P}X_{t}}{||X_{t-P}||_2||X_{t}||_2}$ (1)\n3.2. Where and When Does Similarity Appear?\nWe first investigate which tokens are most likely to demonstrate high similarity, as these are expected to have a greater potential for merging. To ensure figure clarity within the limited resolution, we limit the number of frames so that each input sequentially consists of 14 system prompt tokens, 840 vision tokens (representing 4 frames at 210 tokens per frame), and 20 user instruction tokens. Figure 2 displays the N\u00d7N cosine similarity matrix for all input tokens at the first LLM layer. A prominent 210th sub-diagonal appears, reflecting high similarities between tokens i and i + P, where P = 210 corresponds to the number of vision tokens per frame. This pattern underscores our initial observation:\nObservation 1. Spatially corresponding vision tokens between adjacent temporal frames exhibit higher cosine similarities than other token pairs.\n3.3. What Is the Distribution of Token Similarity?\nTo decide the appropriate way for token compression, we explore the distribution of token similarities across different layers. As illustrated in Figure 3, although the average token similarity across layers remains relatively stable, there is a significant shift in the distribution of these similarities:\nObservation 2. High token similarities exhibit an obvious reduction in deeper model layers.\nThis trend is further confirmed by the variance in token similarity, which decreases from 6.0\u00d710-2 at the first layer to 3.7\u00d710-2 at the last layer. It illustrates a tightening of the distribution, with more numerical details in Appendix 10.1. The causal attention mechanism in LLMs contributes to this shift, where later vision tokens can aggregate information from earlier tokens, but not vice versa. The attention range differences lead to a divergence in the similarity of tokens that were initially similar, such as corresponding tokens from adjacent frames. This divergence accumulates as layers deepen.\nBased on this observation, FrameFusion prioritizes token merging at shallower layers rather than deeper layers to fully utilize the polarized high similarities.\n3.4. How Consistent Is the Ranking of Token Similarity Across Layers?\nThe ranking consistency of token properties guides the compression strategy designs. For importance-based token pruning, the consistency across layers determines whether tokens deemed unimportant at shallow layers might become important at deeper layers. Early works [2, 23] adopt cascaded token pruning, where pruned tokens are no longer accessible in subsequent layers. These methods prioritize efficiency by eliminating token computations in both attention and Feed Forward Network (FFN) modules, and the importance is only computed once for pruned tokens. In contrast, more recent works [17, 20, 35] adopt non-cascaded pruning, which maintains the accessibility of all tokens across all layers by only pruning the KV-Cache at the attention layer. Yet, these methods do not reduce computation for the FFN and incur higher costs due to the need to compute importance for all tokens at every layer. Previous researches [8, 25] highlight the inconsistency of token importance across layers, which justifies re-evaluating pruning decisions at each layer. However, does a similar conclusion hold for token similarity?\nTo quantitatively assess the ranking consistency of token importance and token similarity, we calculate their respective Spearman Rank Correlations (SRC) [30] between adjacent layers. The SRC measures the degree to which the ranking of metrics in one layer correlates with the rankings in other layers. As shown in Figure 4, the SRCs for token similarity approach 1 across layers, indicating consistent ordering. In contrast, the SRCs for token importance are consistently lower, indicating weaker consistency across layers. We observe the following:\nObservation 3. Token similarity demonstrates high rank consistency across different model layers.\nCombining Observations 2 and 3, we conclude an interesting phenomenon: although the token similarity distribution condenses as layers deepen, tokens with the highest similarities remain the most similar. Motivated by this high consistency in similarity, FrameFusion employs similarity-based merging in a cascaded manner. Once highly-similar tokens are merged at shallow layers, they are not separated in subsequent computations."}, {"title": "4. FrameFusion Design", "content": "Building upon the observations from Section 3, we introduce FrameFusion, a novel token compression technique for video LVLMs, exploring a new perspective of token similarity. The design of FrameFusion is detailed in Section 4.1, followed by an explanation of the rationale behind each key design choice in Section 4.2.\n4.1. Two-Stage Token Compression\nThe core concept of FrameFusion is illustrated in Figure 1: unlike traditional methods that primarily employ importance-based token pruning, FrameFusion also emphasizes similarity-based token merging. It merges similar tokens and prunes unimportant ones, retaining only those that are both important and unique. The two-stage token reduction process of FrameFusion is shown in Figure 6.\nMerging stage. In the initial merging stage, FrameFusion utilizes token similarity to merge vision tokens. It computes the token similarity $S^{(l)}$ at the current layer using Equation 1, where only N cosine similarities are computed between the corresponding vision tokens of adjacent frames. Tokens that exceed the similarity threshold $S_{\\text{threshold}}$ are grouped with their analogous tokens from the previous frame. These merging groups are transitive, allowing concatenated groups to form a larger group containing more than two tokens as shown in Figure 6. Within each group, FrameFusion performs element-wise averaging of all tokens, assigning the result to the first token in the group. This forward merging strategy ensures that subsequent vision tokens can still aggregate information from all preceding tokens using causal attention.\nThe merging stage is applied across successive shallow LLM layers to progressively merge similar tokens, until the number of similar tokens falls below the threshold $N_{\\text{threshold}}$ at a specific layer. Merging occurs right before the FFN module; however, to reduce the number of tokens for the attention module at the first LLM layer, an additional merging step is performed before it. After the merging stage, the remaining unique tokens advance to the pruning stage.\nPruning stage. After the merging stage, FrameFusion further prunes unimportant tokens. As discussed in Section 3.1, FrameFusion uses cumulative attention scores to represent token importance. Based on a user-defined computational cost budget, FrameFusion calculates the maximum number of remaining tokens k that fits within the budget. It then applies top-k importance pruning to retain only the important tokens from the remaining unique ones.\nThrough the merging and pruning stages, only the unique and important vision tokens are retained for subsequent processing, significantly boosting LVLM efficiency.\n4.2. Design Choice Rationales\nIn this subsection, we explain the key design choices of FrameFusion with rationales from key observations in Section 3.\nUnlike token importance, which reuses the existing N \u00d7 N attention scores, token similarity introduces a new, orthogonal metric. To avoid additional N \u00d7 N similarity computations for all token pairs, we leverage Observation 1 to compute only empirically similar token pairs with an O(N) complexity:\nDesign Choice 1. FrameFusion computes token similarities only between corresponding vision tokens of adjacent frames.\nAnother key design choice is determining the layers at which to perform token reduction. For importance-based pruning, previous study [2] observes the reduction in vision token importance after the initial two layers for LVLMs. Thus, it advises applying importance-based pruning after the second layer. Other works also confirm the accuracy advantage of aggressive pruning only at deeper layers [1, 8]. In contrast, merging relies on high token similarities and therefore prefers shallow layers, according to Observation 2. Based on the contradictory preferences of similarity-based merging and importance-based pruning, FrameFusion is designed as follows:\nDesign Choice 2. FrameFusion applies token merging at the initial successive layers, then pruning at a later layer.\nThe final key design choice is whether merged tokens should remain reduced in subsequent layers, i.e., whether to use cascaded token merging. This choice depends on whether tokens merged in shallow layers are likely to be needed in deeper layers. Given the high rank consistency and retention rate of token similarities discussed in Observation 3, we adopt the following design to achieve a better efficiency trade-off:\nDesign Choice 3. FrameFusion merges tokens in a cascaded manner.\nThese rationales for the three key design choices in FrameFusion enhance the explainability of its better performance."}, {"title": "5. Experiment", "content": "5.1. Setups\nBaselines. We compare FrameFusion with state-of-the-art token pruning baselines, StreamingLLM [26] and FastV [2]. Hyperparameters follow the official implementations and are detailed in Appendix7."}]}