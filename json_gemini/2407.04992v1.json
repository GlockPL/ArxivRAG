{"title": "Scalable Variational Causal Discovery Unconstrained by Acyclicity", "authors": ["Nu Hoang", "Bao Duong", "Thin Nguyen"], "abstract": "Bayesian causal discovery offers the power to quantify epistemic uncertainties among a broad range of structurally diverse causal theories potentially explaining the data, represented in forms of directed acyclic graphs (DAGs). However, existing methods struggle with efficient DAG sampling due to the complex acyclicity constraint. In this study, we propose a scalable Bayesian approach to effectively learn the posterior distribution over causal graphs given observational data thanks to the ability to generate DAGs without explicitly enforcing acyclicity. Specifically, we introduce a novel differentiable DAG sampling method that can generate a valid acyclic causal graph by mapping an unconstrained distribution of implicit topological orders to a distribution over DAGs. Given this efficient DAG sampling scheme, we are able to model the posterior distribution over causal graphs using a simple variational distribution over a continuous domain, which can be learned via the variational inference framework. Extensive empirical experiments on both simulated and real datasets demonstrate the superior performance of the proposed model compared to several state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Causal inference [28] offers a powerful tool for tackling critical research questions in diverse fields, such as policy decision-making, experimental design, and enhancing AI trustworthiness. However, current causal inference algorithms typically require an input of a directed acyclic graph (DAG), encapsulating causal relationships among variables of interest. Unfortunately, identifying the true causal DAG often necessitates extensive experimentation, which can be time-consuming and ethically problematic in certain situations, hindering the application of causal inference to high-dimensional problems. Therefore, there is a pressing need to explore methods for discovering the causal DAG solely from observational data, which is typically more readily available [33, 16]. Nevertheless, a major hurdle in causal discovery using observational data is the non-identifiability issue of causal models when multiple DAGs may induce the same observed data mainly due to scarce data, model misspecification, and limited capability of optimizers. Bayesian inference is a promising approach to mitigate this problem by estimating the posterior distribution over causal DAGs, allowing for capturing epistemic uncertainties in causal structure learning. Moreover, this richer representation can then be leveraged for various tasks, including active causal discovery, where we strategically collect additional data to refine our understanding of causal relationships [1, 35]."}, {"title": "2 Related Work", "content": "Discrete Optimization. These methods encompass constraint-based methods, score-based methods, and hybrid methods, which typically search for the true causal graph within the original combinatorial space of DAGs. Constraint-based methods [33, 40, 8] depend on results from various conditional independence tests, while score-based methods [11, 12, 14] optimize a predefined score to identify the final DAG by adding, removing, or reversing edges. In the meantime, hybrid methods [36, 27] integrate both constraint-based and score-based techniques to trim the search space, thus accelerating the overall learning process.\nContinuous Optimization. Optimizing in the discrete space of DAGs is known to be challenging due to the super exponential increase in complexity with the number of variables. To address this issue, several studies map the discrete space to a continuous one, thereby unlocking the application of various continuous optimization techniques. A pioneering study is NO TEARS [42], which introduced a smooth function to evaluate the DAG-ness of a weighted adjacency matrix. The causal structure learning problem is then tackled using an augmented Lagrangian optimization method. Subsequent studies, inspired by NO TEARS, have enhanced its efficiency by introducing low-complexity DAG constraints [21, 41], or extending it for non-linear functional models [38, 43, 20, 26]. In contrast to NO TEARS, recent studies [39, 23] introduce various DAG mapping functions, which facilitate direct optimization within the DAG space. Therefore, these mapping functions provide more scalable and direct approaches without the need to evaluate the DAG constraint.\nBayesian Causal Structure Learning. The above studies usually output the Markov equivalence class (MEC) of the true DAG or a single DAG, which may not adequately represent the uncertainty in certain practical scenarios. To address this challenge, Bayesian causal discovery methods produce a posterior distribution over causal DAGs. Several studies demonstrate DAG sampling in the discrete space using either Markov Chain Monte Carlo (MCMC) [34, 37, 19] or GFlowNets [10, 3]. However, these approaches expose slow mixing and convergence. Recent advancements aim for more efficient inference through the development of gradient optimization methods for Bayesian structure learning. However, existing studies still struggle in representing the acyclicity constraint. For instance, DiBS [22] exploits NO TEARS as a DAG regularizer and utilizes Stein variational approach to learn the joint distribution over DAGs and causal model parameters. However, its scalability is limited for large graphs due to the computational complexity associated with NO TEARS. In contrast, BCDnets [9] and DDS [6] exploit the ordering-based DAG decomposition to parameterize the DAGs distribution through the multiplication of upper triangular and permutation matrices. For approximating a discrete distribution over the permutation matrix, they utilized Gumbel-Sinkhorn operator, which poses a high time complexity, i.e., cubic with respect to the number of node. To reduce the complexity of the Gumbel-Sinkhorn operator, BayesDAG [2] exploits No-Curl constraint [39] which can decrease the number of iterations required for the Gumbel-Sinkhorn operator, yet the scalability of this approach remains a challenge, when dealing with large graphs."}, {"title": "3 Preliminary", "content": "3.1 Problem formulation\nLet \\(X \\in \\mathbb{R}^{n \\times d}\\) be an observational dataset consisting of \\(n\\) i.i.d. samples of \\(d\\) random variables from a joint distribution \\(P(X)\\). The marginal joint distribution \\(P(X)\\) factorizes according to a DAG \\(G = (V, E)\\), where \\(V = \\{1, 2, ..., d\\}\\) is a set of nodes corresponding to \\(d\\) random variables and \\(E\\) is a set of edges representing the dependency between nodes. In other words, \\(P(X) = \\prod_{i=1}^{d} P(X_i \\mid X_{pa(i)})\\), where \\(X_{pa(i)}\\) denotes a set of parents of nodes \\(X_i\\). We can model the data of a node \\(X_i\\) with a structural equation model as follows:\n\\[X_i = g_i(f_i(X_{pa(i)}), \\epsilon_i),\\]\nwhere \\(g_i\\) and \\(f_i\\) are deterministic functions and \\(\\epsilon_i\\) is an arbitrary noise. In this work, we consider a Gaussian additive noise model (ANM) [32, 13] as follows:\n\\[X_i = f_i(X_{pa(i)}) + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\\]\nThe DAG \\(G\\) can be represented by a binary adjacency matrix \\(A \\in \\{0,1\\}^{d \\times d}\\), where \\(A_{ij} = 1\\) indicates an edge from \\(X_i\\) to \\(X_j\\). As a result, Eq. (1) can be rewritten as:\n\\[X_i \\approx f_i(A_i X) + \\epsilon_i,\\]"}, {"title": "4 Proposed method", "content": "The main goal of this study is to propose a scalable and fully differentiable framework to approximate the posterior distribution over DAGs given observational data. To this end, we first introduce a novel measure to parameterize the probabilistic model over DAGS by extending the No-Curl characterization. Leveraging this probabilistic model for DAG sampling and integrating it with the variational inference framework, we then introduce a new Bayesian causal discovery method, named VCUDA (Variational Causal Discovery Unconstrained by Acyclicity), offering precise capture and effective generation of samples from the complex posterior distribution of DAGs."}, {"title": "4.1 Differentiable DAG sampling", "content": "As discussed in the previous section, a weighted adjacency matrix to represent a DAG does not align with our purpose. Therefore, we extend the No-Curl characterization by substituting the ReLU(.) by a tempered sigmoid(.) function, allowing us to represent a DAG using a binary adjacency matrix.\nTheorem 1. Let \\(A \\in \\{0,1\\}^{d \\times d}\\) be an adjacency matrix of a graph of \\(d\\) nodes. Then, \\(A\\) is DAG if and only if there exists a vector of priority scores \\(p \\in \\mathbb{R}^d\\) and a corresponding binary matrix \\(W \\in \\{0,1\\}^{d \\times d}\\) such that:\n\\[A = \\nu(W, p) = W \\cdot \\lim_{t \\rightarrow 0} \\text{sigmoid} \\Big(\\frac{\\text{grad}(p)}{t}\\Big),\\]\nwhere \\(t > 0\\) is a strictly positive temperature and \\(p\\) contains no duplicate elements, i.e., \\(p_i \\neq p_j, \\forall i, j\\).\nClosely related to our method, COSMO [23] also introduces a smooth orientation matrix for unconstrained DAG learning. It is crucial to highlight that our study is motivated by distinct objectives, specifically, addressing challenges related to scalability and generalization in Bayesian causal discovery. This convergence in ideas reaffirms the significance of our approach in independently tackling common challenges, underscoring its broader applicability and relevance.\nBased on Theorem 1, we further introduce a new probabilistic model over DAGs space as follows:\n\\[P(A) = \\int_{W} \\int_{p} P(W, p) dW dp\\]\\[\\text{s.t.} A = W \\cdot \\lim_{t \\rightarrow 0} \\text{sigmoid} \\Big(\\frac{\\text{grad}(p)}{t}\\Big),\\]\nwhere \\(P(W)\\) and \\(P(p)\\) are distributions over edges and priority scores, respectively. As a result, Eq. (9) provides a fast and assured sampling approach of DAGs without evaluating any explicit acyclicity constraints. We follow [6], utilizing the Gumbel-Softmax [15] to model the discrete distribution over edges. Let \\(\\varphi \\in [0, 1]\\) be the probability of the existence of an edge from node \\(X_i\\) to \\(X_j\\). The Gumbel-Softmax, which is a continuous distribution, enables a differentiable approximation of samples from a discrete distribution, e.g., Bernoulli distribution: \\(W_{ij} \\in [0, 1] \\sim \\text{Gumbel-Softmax}(\\varphi_{ij})\\), where \\(\\tau > 0\\) is the temperature parameter controlling the smoothness of the categorical during sampling. For example, a low value of \\(\\tau\\) generates more likely one-hot encoding samples, making the Gumbel-Softmax distribution resemble the original categorical distribution. Consequently, we can directly generate a DAG by sampling an edge matrix from a Gumbel-Softmax distribution \\(W \\sim \\text{Gumbel-Softmax}(\\varphi)\\), and a priority score vector from a multivariate distribution \\(p \\sim P_{\\psi}(p)\\): \\(A \\sim P_{\\varphi,\\psi}(A)\\) where \\(\\varphi\\) and \\(\\psi\\) are parameters defined distributions of \\(W\\) and \\(p\\), respectively.\nComputational complexity: Our proposed probabilistic model significantly speeds up the DAG sampling time compared with related studies using the Gumbel-Sinkhorn approach, such as BCDnets [9]. To elaborate, our proposed approach requires \\(\\mathcal{O}(d^2)\\) for sampling the edge matrix \\(W\\) and \\(\\mathcal{O}(d)\\) for sampling the priority scores vector \\(p\\). This leads to an overall computational complexity of \\(\\mathcal{O}(d^2)\\). A closely related study to our approach is BayesDAG [2], which suggests replacing ReLU(.) with Step(.). However, their"}, {"title": "4.2 Variational Inference DAG Learning", "content": "As mentioned earlier, Bayesian causal structure learning aims to determine the posterior distribution over DAGs. However, directly computing the posterior becomes infeasible due to the intractability of the marginal data distribution. To address this challenge, we turn to variational inference. Here, we leverage the probabilistic DAG model \\(P_{\\varphi,\\psi}(A)\\) introduced in Section 4.1 to approximate the true posterior distribution \\(P(A \\mid X)\\). In essence, we aim to optimize the variational parameters to minimize the KL divergence between the approximate and true posterior distributions that are equivalent to maximizing the evidence lower bound (ELBO) objective as follows:\n\\[\\max_{\\theta, \\varphi, \\psi} \\mathcal{L} = \\mathbb{E}_{W, p \\sim P_{\\varphi,\\psi}(W, p)} [\\log P_{\\theta} (X \\mid W, p)] - D_{KL} (P_{\\varphi,\\psi}(W, p) || P_{\\text{prior}} (W, p)).\\]\nThe objective in Eq. (10) consists of two terms: i) the first is the log-likelihood of the data given the causal structure model and ii) the second is the KL divergence between the approximate posterior distribution and the prior distribution. With appropriate choices of variational families and prior models, the optimized parameters \\(\\theta, \\varphi, \\psi\\) from Eq. (10) minimize the divergence between the approximate distribution and the true distribution, i.e., \\(P_{\\varphi,\\psi}(A) \\approx P(A \\mid X)\\).\nTo compute (i), we begin by sampling a DAG adjacency matrix \\(A\\) from the approximate distribution in each iteration. For every node \\(X_i\\), we reconstruct its values by applying masking on the observed data \\(X\\) with the sampled \\(A\\). This is then followed by a transformation \\(f_{i, \\theta}\\), parameterized using neural networks:\n\\[\\hat{X_i} = f_{i, \\theta}(A_i X),\\]\nwhere \\(A_i\\) is the \\(i\\)-th column in the adjacency matrix \\(A\\). By assuming that the data has a Gaussian distribution with unit variance, we can approximate the first term by the least square loss, i.e., \\(||X - \\hat{X}||^2\\).\nTo compute (ii), we initially employ a mean-field factorization for the variational model, i.e., \\(P_{\\varphi,\\psi} (W, p) = P_{\\varphi} (W) P_{\\psi} (p)\\). This mean-field factorization provides us a convenient way to calculate the KL divergence, represented as:\n\\[D_{KL} (P_{\\varphi,\\psi} (W, p) || P_{\\text{prior}} (W, p)) = D_{KL} (P_{\\varphi} (W) P_{\\psi} (p) || P_{\\text{prior}} (W) P_{\\text{prior}} (p))\\]\\[= \\mathbb{E}_{p,W \\sim P_{\\varphi, \\psi} (p,W)} [\\log \\frac{P_{\\text{prior}} (W) P_{\\text{prior}} (p)}{P_{\\varphi} (W)} + \\log \\frac{P_{\\text{prior}} (p)}{P_{\\psi} (p)}]\\]\n\\[= \\mathbb{E}_{p,W \\sim P_{\\varphi, \\psi} (p,W)} [\\log \\frac{P_{\\text{prior}} (W)}{P_{\\varphi} (W)} + \\log \\frac{P_{\\text{prior}} (p)}{P_{\\psi} (p)}]\\]\n\\[= \\int \\int P_{\\varphi} (W) P_{\\psi} (p) [\\log \\frac{P_{\\text{prior}} (W)}{P_{\\varphi} (W)} + \\log \\frac{P_{\\text{prior}} (p)}{P_{\\psi} (p)}] dWdp\\]\n\\[= \\int \\int P_{\\varphi} (W) P_{\\psi} (p) \\log \\frac{P_{\\text{prior}} (W)}{P_{\\varphi} (W)} dWdp + \\int \\int P_{\\varphi} (W) P_{\\psi} (p) \\log \\frac{P_{\\text{prior}} (p)}{P_{\\psi} (p)} dWdp\\]\n\\[= \\int P_{\\psi} (p) [\\int P_{\\varphi} (W) \\log \\frac{P_{\\text{prior}} (W)}{P_{\\varphi} (W)} dW] dp + \\int P_{\\varphi} (W) [\\int P_{\\psi} (p) \\log \\frac{P_{\\text{prior}} (p)}{P_{\\psi} (p)} dp] dW\\]\n\\[= \\int P_{\\psi} (p) D_{KL} (P_{\\varphi} (W) || P_{\\text{prior}} (W)) dp + \\int P_{\\varphi} (W) D_{KL} (P_{\\psi} (p) || P_{\\text{prior}} (p)) dW\\]\n\\[= D_{KL} (P_{\\varphi} (W) || P_{\\text{prior}} (W)) + D_{KL} (P_{\\psi} (p) || P_{\\text{prior}} (p))\\]\nConsequently, we can compute the KL divergence between the approximate posterior distribution and the prior distribution over DAGS via the sum of the KL divergence between the variational model and the prior distribution over the edge matrix \\(W\\) and the priority scores vector \\(p\\).\nVariational Families: For the distribution over the priority scores vector \\(p\\), we opt for the isotropic Gaussian, i.e., \\(p \\sim \\mathcal{N}(\\mu, \\sigma^2 I)\\). As discussed earlier, we choose the Gumbel-Softmax distribution [15] for the variational model of the edge matrix \\(W\\). These choices enable us to utilize the pathwise gradient, offering a lower variance approach compared to the score-function method [25]. To compute the gradient, we leverage the straight-through estimator [4]: for \\(p\\), we use the rounded value of \\(\\text{sigmoid} (\\text{grad}(p))\\) in the forward pass, and its continuous value in the backward pass. For \\(W\\), we use the discrete value \\(W_{ij} = \\text{arg max} \\{ \\gamma_{ij}, 1 - \\gamma_{ij} \\}\\) in the forward pass, and the continuous approximation \\(\\gamma_{ij}\\) in the backward pass.\nPrior Distribution: A well-chosen prior encapsulates existing knowledge about the model parameters, thereby guiding the inference process. In line with the belief in the sparsity of causal DAGs, we set a small prior \\(P_{\\text{prior}} (W_{ij})\\) on the edge probability. For an effective gradient estimation, we define the prior distribution of the priority scores vector as a normal distribution with a mean of zero and a small variance.\nIncorporating all the above design choices, the final loss can be expressed as follows:\n\\[\\max_{\\theta, \\varphi, \\psi} \\mathcal{L} = - \\sum_{i} (X_{ij} - \\hat{X_{ij}})^2\\]\\[ - D_{KL} (P_{\\varphi} (W) || P_{\\text{prior}} (W))\\]\\[- D_{KL} (P_{\\psi} (p) || P_{\\text{prior}} (p)),\\]\nwhere \\(\\hat{X_i} = f_{i, \\theta}(A_i X)\\) is the reconstructed data from the sampled DAG. In the implementation, we divide the total loss by the number of nodes \\(d\\) for stable numerical optimization. The training process of the proposed approach are summarized in Algorithm 1."}, {"title": "5 Experiment", "content": "In this section, we demonstrate extensive experiments showing the empirical performance of our proposed method on DAG sampling and DAG structure learning tasks.\nBaselines. Regarding DAG sampling, we compare the proposed method with the Gumbel-Sinkhorn and Gumbel-Top-k approaches [6] in terms of sampling time for thousands of variables. Unlike our method, which samples a priority score vector, these approaches focus on sampling a permutation matrix. The Gumbel-Sinkhorn method [24] leverages the Sinkhorn operator to approximate the distribution over the permutation matrix. Meanwhile, Gumbel-Top-k combines the Gumbel-Top-k distribution [17] with the Soft-Sort operator [30] to achieve faster sampling compared to Gumbel-Sinkhorn. Regarding DAG structure learning, we focus our comparison on differentiable methods and hence select five state-of-the-art causal discovery methods that belong to both point-estimations and Bayesian based baselines:\nDatasets. We benchmark these methods on both synthetic and real datasets. For synthetic datasets, we closely follow [20, 26, 6]. For generating causal DAGs, we consider Erd\u0151s-R\u00e9nyi (ER) and scale-free (SF) network models with average degree equal to 1. We vary the graph size in terms of number of nodes \\(d = \\{10, 50, 100\\}\\) and consider both linear and nonlinear Gaussian SEMs. For linear model, we generate a weighted adjacency matrix \\(W \\in \\mathbb{R}^{d \\times d}\\) with edges' weights randomly sampled from \\(\\mathcal{U}([-2, -0.5] \\cup [0.5, 2])\\). We then generate the data \\(X \\in \\mathbb{R}^{n \\times d}\\) following the linear SEM: \\(X = W X + \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\). For nonlinear model, we generate the data following \\(X_i = f_i(X_{pa(i)}) + \\epsilon_i\\), where the functional model \\(f_i\\) is generated from Gaussian Process with RBF kernel of bandwidth one and \\(\\epsilon_i \\sim \\mathcal{N}(0, 1)\\). In our experiments, we sample 10 datasets per setting where each dataset includes a ground truth of the causal DAG's adjacency matrix, a training dataset of 1,000 samples and a held out testing dataset of 100 samples. For real datasets, we closely follow [39, 22, 26]. We use Sachs dataset [31] which measures the expression level of different proteins and phospholipids in human cells. The data contains 853 observational samples generated from a protein interaction network of 11 nodes and 17 edges.\nEvaluation metrics. We use the area under the curve of precision-recall (AUC-PR) and the area under the receiver operating characteristic curve (AUC-ROC) between the ground-truth binary adjacency matrix and the output scores matrix, denoted as \\(S\\), where \\(S_{ij}\\) represents the possibility of the presence of an edge from \\(X_i\\) to \\(X_j\\). For point estimation methods, we get the scores matrix from the output before thresholding. For Bayesian-based methods, we get the scores matrix by averaging 100 sampled binary adjacency matrix from the learned probabilistic DAG model. We also evaluate the learned functional model \\(f_{\\theta} (.)\\) by computing the mean squared error (MSE) between the ground-truth node value \\(X_i\\) and the estimated node values \\(\\hat{X_i} = f_{i, \\theta}(A_i X)\\) on a held-out dataset.\nHyperparameters. We use a neural network with one hidden layer and ReLU activation to parameterize the functional models \\(f_i\\) in the nonlinear setting and real-world dataset. We perform a random search over the learning rate \\(lr \\in [10^{-3}, 10^{-1}]\\). The prior edge probability and scale of the priority scores vector are set to 0.01 and 0.1, respectively. Based on our ablation study (refer to Section 5.2), a temperature of 0.3 is chosen for its benefits. To prevent overfiting, VCUDA is trained by the Adam optimizer with the \\(l_2\\)-regularization of \\(1e^{-4}\\). Furthermore, early stopping is employed with validation loss checks every 10 epochs. We find that the model is convergent within 500 epochs.\n5.1 DAG sampling\nWe compare our DAG sampling model to two well-known models, including Gumbel-Sinkhorn and Gumbel-Top-k [6] on large-scale DAGs sampling. The results are shown in Appendix B, indicating"}, {"title": "5.2 DAG structure learning", "content": "Synthetic datasets. We present the results of DAG structure learning on ER/SF graphs with \\(d = \\{10, 50\\}\\) for both linear and nonlinear models in Figure 2 and Figure 3, respectively. The results exhibit the superior performance of VCUDA across all settings, particularly in terms of AUC-ROC. Specifically, the AUC-ROC values of VCUDA remain consistently high, always surpassing 0.9 for linear models and 0.8 for nonlinear models. In contrast, the AUC-ROC values of the other baselines show volatility depending on the settings. Additionally, VCUDA achieves a low MSE measures, outperforming most baselines including GranDAG, MCSL, and DDS across all settings. In comparison to DiBS, the MSE measures of VCUDA are comparable for both linear and nonlinear settings. We observe a notably superior performance of Bayesian baselines compared to point-estimation baselines, especially in scenarios with higher dimensions (e.g., \\(d = 50\\)). This disparity in performance can be attributed to the Bayesian methods' capacity to capture the uncertainty effectively. Furthermore, we study the performance of VCUDA and baseline methods on the high dimensional causal graph with \\(d = 100\\) for both linear and nonlinear models. We find that DiBS is computationally excessive producing an error in the benchmarking device, while"}, {"title": "6 Conclusion", "content": "We introduce VCUDA, a scalable approach for Bayesian causal discovery from observational data. By eliminating explicit acyclicity constraints, we propose a differentiable approach for DAGs sampling, enabling fast generation of large DAGs within seconds. In addition, the efficient sampling model enhances Bayesian inference for causal structure models when integrated into the variational inference framework. Extensive experiments on synthetic and real datasets showcase VCUDA's superior performance, outpacing other baselines in terms of multiple metrics, all achieved with remarkable efficiency."}, {"title": "Appendix for \u201cScalable Variational Causal Discovery Unconstrained by Acyclicity\"", "content": "A Theorem 2\nTheorem 2. Let \\(A \\in \\{0,1\\}^{d\\times d}\\) be an adjacency matrix of a graph of \\(d\\) nodes. Then, \\(A\\) is DAG if and only if there exists corresponding a vector of priority scores \\(p \\in \\mathbb{R}^d\\) and a binary matrix \\(W \\in \\{0,1\\}^{d\\times d}\\) such that:\n\\[A = \\nu(W, p) = W \\cdot \\lim_{t \\rightarrow 0} \\text{sigmoid} \\Big(\\frac{\\text{grad}(p)}{t}\\Big)\\]\nwhere \\(t>0\\) is a strictly positive temperature and \\(p\\) contains no duplicate elements, i.e., \\(p_i\\neq p_j \\forall i,j\\).\nProof. We first show that for any DAG \\(A\\), there always exists a pair \\((W, p)\\) such that \\(\\nu(W, p)=A\\). By leveraging Theorem 3.7 in [39], we can see that \\(p\\) implicitly define the topological order over vertices of \\(A\\) such that:\n\\(\\text{grad}(p)_{ij}>0\\) when \\(A_{ij}=1\\)\nThen,\n\\(\\lim_{t \\rightarrow 0} \\text{sigmoid}(\\frac{\\text{grad}(p)}{t}) =1\\) when \\(A_{ij}=1\\)\nFurthermore, we can choose \\(W\\) in the following way:\n\\[W_{ij} = \\begin{cases}\n0 & \\text{if } A_{ij} = 0\\\\\n1 & \\text{if } A_{ij} = 1\n\\end{cases}\\]\nFor an arbitrary topological (partial) order of the variables \\(\\pi=(\\pi_1, \\pi_2, ..., \\pi_d)\\), it always defines a DAG where each edge \\((i,j)\\) corresponding to \\(i \\nrightarrow j\\). To prove that the mapping \\(\\nu(W, p)\\) always emits a DAG, let define a vector \\(p \\in \\mathbb{R}^d\\) such that \\(p[\\pi[i]]=i\\). We have:\n\\(i0\\)\nTherefore, \\(\\lim_{t \\rightarrow 0} \\text{sigmoid}(\\frac{\\text{grad}(p)}{t})\\) outputs an acyclic binary adjacency matrix. Then, taking the element-wise multiplication with any \\(W\\) gives us a sub-graph of a DAG, which is also a DAG."}]}