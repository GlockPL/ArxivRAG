{"title": "Scalable Variational Causal Discovery Unconstrained by Acyclicity", "authors": ["Nu Hoang", "Bao Duong", "Thin Nguyen"], "abstract": "Bayesian causal discovery offers the power to quantify epistemic uncertainties among a broad range of structurally diverse causal theories potentially explaining the data, represented in forms of directed acyclic graphs (DAGs). However, existing methods struggle with efficient DAG sampling due to the complex acyclicity constraint. In this study, we propose a scalable Bayesian approach to effectively learn the posterior distribution over causal graphs given observational data thanks to the ability to generate DAGs without explicitly enforcing acyclicity. Specifically, we introduce a novel differentiable DAG sampling method that can generate a valid acyclic causal graph by mapping an unconstrained distribution of implicit topological orders to a distribution over DAGs. Given this efficient DAG sampling scheme, we are able to model the posterior distribution over causal graphs using a simple variational distribution over a continuous domain, which can be learned via the variational inference framework. Extensive empirical experiments on both simulated and real datasets demonstrate the superior performance of the proposed model compared to several state-of-the-art baselines.", "sections": [{"title": "1 Introduction", "content": "Causal inference [28] offers a powerful tool for tackling critical research questions in diverse fields, such as policy decision-making, experimental design, and enhancing AI trustworthiness. However, current causal inference algorithms typically require an input of a directed acyclic graph (DAG), encapsulating causal relationships among variables of interest. Unfortunately, identifying the true causal DAG often necessitates extensive experimentation, which can be time-consuming and ethically problematic in certain situations, hindering the application of causal inference to high-dimensional problems. Therefore, there is a pressing need to explore methods for discovering the causal DAG solely from observational data, which is typically more readily available [33, 16]. Nevertheless, a major hurdle in causal discovery using observational data is the non-identifiability issue of causal models when multiple DAGs may induce the same observed data mainly due to scarce data, model mis-specification, and limited capability of optimizers. Bayesian inference is a promising approach to mitigate this problem by estimating the posterior distribution over causal DAGs, allowing for capturing epistemic uncertainties in causal structure learning. Moreover, this richer representation can then be leveraged for various tasks, including active causal discovery, where we strategically collect additional data to refine our understanding of causal relationships [1, 35]."}, {"title": "2 Related Work", "content": "Discrete Optimization. These methods encompass constraint-based methods, score-based methods, and hybrid methods, which typically search for the true causal graph within the original combinatorial space of DAGs. Constraint-based methods [33, 40, 8] depend on results from various conditional independence tests, while score-based methods [11, 12, 14] optimize a predefined score to identify the final DAG by adding, removing, or reversing edges. In the meantime, hybrid methods [36, 27] integrate both constraint-based and score-based techniques to trim the search space, thus accelerating the overall learning process.\nContinuous Optimization. Optimizing in the discrete space of DAGs is known to be challenging due to the super exponential increase in complexity with the number of variables. To address this issue, several studies map the discrete space to a continuous one, thereby unlocking the application of various continuous optimization techniques. A pioneering study is NOTEARS [42], which introduced a smooth function to evaluate the DAG-ness of a weighted adjacency matrix."}, {"title": "3 Preliminary", "content": "3.1 Problem formulation\nLet $X \\in \\mathbb{R}^{n \\times d}$ be an observational dataset consisting of $n$ i.i.d. samples of $d$ random variables from a joint distribution $P(X)$. The marginal joint distribution $P(X)$ factorizes according to a DAG $G = (V, E)$, where $V = \\{1, 2, ..., d\\}$ is a set of nodes corresponding to $d$ random variables and $E$ is a set of edges representing the dependency between nodes. In other words, $P(X) = \\prod_{i=1}^d P(X_i | X_{pa(i)})$, where $X_{pa(i)}$ denotes a set of parents of nodes $X_i$. We can model the data of a node $X_i$ with a structural equation model as follows:\n$X_i = g_i (f_i (X_{pa(i)}), e_i),$   (1)\nwhere $g_i$ and $f_i$ are deterministic functions and $e_i$ is an arbitrary noise. In this work, we consider a Gaussian additive noise model (ANM) [32, 13] as follows:\n$X_i = f_i (X_{pa(i)}) + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N} (0, \\sigma^2),$   (2)\nThe DAG G can be represented by a binary adjacency matrix $A \\in \\{0, 1\\}^{d \\times d}$, where $A_{ij} = 1$ indicates an edge from $X_i$ to $X_j$. As a result, Eq. (1) can be rewritten as:\n$X_i f_i (A_i X) + \\epsilon_i,$   (3)"}, {"title": "3.2 Bayesian causal structure learning", "content": "Solving Eq. (4) yields a single point DAG solution that comes with practical limitations, particularly in addressing the non-identifiability problem in DAG learning. This limitation stems from the fact that the true causal DAG is only identifiable under specific conditions. For instance, identifiability in the linear Gaussian SEM holds in the equal variance noise setting [29]. In real-world scenarios, the non-identifiability issue may surface due to the limited number of observations, leading the point estimation approach to converge toward an incorrect solution. Considering these challenges, it proves beneficial to model the uncertainty in DAG learning, where Bayesian learning emerges as a standard approach.\nThe ultimate goal of Bayesian causal structure learning methods is to approximate the posterior distribution over the causal graph given the observational data, denoted as $P(G | X)$. Using Bayes' rule, the posterior $P(G | X)$ can be expressed through the prior distribution $P(G)$ and the marginal likelihood $P(X | G)$ as follows:\n$P(G | X) = \\frac{P(X | G)P(G)}{\\sum_{G'} P(X | G')P(G')},$   (5)\nwhere the marginal likelihood $P(X | G)$ is defined as a marginalization of the likelihood function over all possible parameters for $G$:\n$P(X | G) = \\int P(X | G, \\theta_G)P(\\theta_G | G)d\\theta_G.$   (6)\nThe main challenge in Bayesian causal discovery is the intractability of the denominator in Eq. (5) due to the expansive space of DAGs."}, {"title": "3.3 DAG representation", "content": "To deal with the computational challenge in DAG learning, several studies [42, 21, 41] have shifted the combinatorial search to a continuous optimization problem, proving to be more scalable and adaptable to different SEMs. Specifically, researchers have introduced various smooth functions to evaluate whether a directed adjacency matrix represents a DAG, i.e., $A \\in D \\Leftrightarrow h(A) = 0$. Consequently, solving Eq. (4) can be accomplished through a constrained optimization method, such as the augmented Lagrangian method. Despite these advancements, existing methods still fall short in ensuring a valid acyclic output, possibly requiring additional post-processing steps [5]. Therefore, we aim to find a convenient representation for the DAG space that allows direct optimization within it. This approach guarantees the output of a DAG at any stage during the learning process.\nA common approach for representing a DAG involves utilizing a permutation matrix $\\Pi \\in \\{0, 1\\}^{d \\times d}$ and an upper triangular matrix $U \\in \\{0, 1\\}^{d \\times d}$, i.e., $A = \\Pi U \\Pi^T$. This formulation arises from the inherent property of a DAG, where there exists at least one valid permutation (or causal order) $\\pi \\in \\mathbb{R}^d$, implying that there is no direct edge from node $X_{\\pi(j)}$ and $X_{\\pi(i)}$ if $\\pi(i) < \\pi(j)$. To seamlessly integrate this formula into a continuous DAG learning framework, we can parameterize the permutation matrix and the upper triangular matrix using Gumbel-Sinkhorn [24] and Gumbel-Softmax [15] distributions, respectively. Yet, this approach introduces a high complexity (i.e., $O (d^3)$) due to the Hungarian algorithm [18] used in the forward pass of Gumbel-Sinkhorn.\nMotivated by the same objective, [39] introduce No-Curl, a simple DAG mapping function that facilitates a projection of any weighted adjacency matrix from an arbitrary directed graph to the DAG space through a straightforward process: applying element-wise multiplication with the gradient of a priority scores vector associated with the graph vertices. We will provide a detailed explanation of the No-Curl characterization, as it constitutes a crucial component of our proposed method.\nLet $p \\in \\mathbb{R}^d$ be a vector representing priority scores of $d$ nodes in a directed graph $G$. The gradient of $p$, denoted by $grad (p) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d \\times d}$, is defined as follows:\n$grad (p)_{ij} = p_j - p_i.$   (7)\nThen, No-Curl proposes mapping $p$ and a weighted directed adjacency matrix $W$, denoted by $\\gamma (W,p) : \\mathbb{R}^{d \\times d} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d \\times d}$, to the DAG space by:\n$\\gamma (W, p) = W \\cdot ReLU (grad (p)),$   (8)\nwhere $ReLU$ is the rectified linear unit activation function. For a detailed proof of No-Curl, we refer to [39]. Here, we provide an intuitive explanation of the mapping $\\gamma (W, p)$: the priority scores vector $p$ implicitly corresponds to a causal order of $d$ nodes when we sort $p$ in increasing order. To avoid cycles, we exclusively permit edges from nodes with lower scores to nodes with higher scores, i.e., $p_i < p_j \\rightarrow (i \\rightarrow j)$, which is equivalent to $grad (p)_{ij} > 0$. Hence, $ReLU (grad (p))$ eliminates cycle-inducing edges by zeroing out all $grad (p)_{ij} \\leq 0$. Finally, we remove spurious edges through a Hadamard product with a weighted adjacency matrix $W$.\nUnfortunately, the No-Curl characterization is specifically crafted for a weighted DAG representation. Indeed, we favor a binary representation, since it offers greater flexibility to model both linear and non-linear functional relationships. To address this preference, we propose a novel adaptation of the No-Curl approach for representing binary adjacency matrices of DAGs, as detailed in the next section."}, {"title": "4 Proposed method", "content": "The main goal of this study is to propose a scalable and fully differentiable framework to approximate the posterior distribution over DAGs given observational data. To this end, we first introduce a novel measure to parameterize the probabilistic model over DAGS by extending the No-Curl characterization. Leveraging this probabilistic model for DAG sampling and integrating it with the variational inference framework, we then introduce a new Bayesian causal discovery method, named VCUDA (Variational Causal Discovery Unconstrained by Acyclicity), offering precise capture and effective generation of samples from the complex posterior distribution of DAGs."}, {"title": "4.1 Differentiable DAG sampling", "content": "As discussed in the previous section, a weighted adjacency matrix to represent a DAG does not align with our purpose. Therefore, we extend the No-Curl characterization by substituting the $ReLU(.)$ by a tempered $sigmoid(.)$ function, allowing us to represent a DAG using a binary adjacency matrix.\nTheorem 1. Let $A \\in \\{0, 1\\}^{d \\times d}$ be an adjacency matrix of a graph of $d$ nodes. Then, A is DAG if and only if there exists a vector of priority scores $p \\in \\mathbb{R}^d$ and a corresponding binary matrix $W \\in \\{0, 1\\}^{d \\times d}$ such that:\n$A = \\nu (W, p) = W \\cdot \\lim_{t \\rightarrow 0} sigmoid (grad(p)),$   \nwhere $t > 0$ is a strictly positive temperature and $p$ contains no duplicate elements, i.e., $p_i \\neq p_j \\forall i, j$.\nBased on Theorem 1, we further introduce a new probabilistic model over DAGs space as follows:\n$P(A) = \\sum_{W} \\int P (W, p) dp$\ns.t. A = W \\cdot \\lim_{t \\rightarrow 0} sigmoid (grad(p)),   (9)\nwhere $P(W)$ and $P(p)$ are distributions over edges and priority scores, respectively. As a result, Eq. (9) provides a fast and assured sampling approach of DAGs without evaluating any explicit acyclicity constraints. We follow [6], utilizing the Gumbel-Softmax [15] to model the discrete distribution over edges. Let $\\phi \\in [0, 1]$ be the probability of the existence of an edge from node $X_i$ to $X_j$. The Gumbel-Softmax, which is a continuous distribution, enables a differentiable approximation of samples from a discrete distribution, e.g., Bernoulli distribution: $W_{ij} \\in [0, 1] \\sim Gumbel$-$Softmax$(\\phi_{ij})$, where $T > 0$ is the temperature parameter controlling the smoothness of the categorical during sampling. For example, a low value generates more likely one-hot encoding samples, making the Gumbel-Softmax distribution resemble the original categorical distribution. Consequently, we can directly generate a DAG by sampling an edge matrix from a Gumbel-Softmax distribution $W \\sim Gumbel$-$Softmax$(\\varphi)$, and a priority score vector from a multivariate distribution $p \\sim P_{\\psi}(p)$: $A \\sim P_{\\varphi,\\psi} (A)$ where $\\varphi$ and $\\psi$ are parameters defined distributions of $W$ and $p$, respectively."}, {"title": "4.2 Variational Inference DAG Learning", "content": "As mentioned earlier, Bayesian causal structure learning aims to determine the posterior distribution over DAGs. However, directly computing the posterior becomes infeasible due to the intractability of the marginal data distribution. To address this challenge, we turn to variational inference. Here, we leverage the probabilistic DAG model $P_{\\varphi,\\psi}(A)$ introduced in Section 4.1 to approximate the true posterior distribution $P (A | X)$. In essence, we aim to optimize the variational parameters to minimize the KL divergence between the approximate and true posterior distributions that are equivalent to maximizing the evidence lower bound (ELBO) objective as follows:\n$\\max_{\\theta,\\varphi,\\psi} L = \\mathbb{E}_{W,p\\sim P_{\\varphi,\\psi}(W,p)} [log P_{\\theta} (X | W, p)] - D_{KL} (P_{\\varphi,\\psi} (W, p) || P_{prior} (W, p)).$   (10)\nThe objective in Eq. (10) consists of two terms: i) the first is the log-likelihood of the data given the causal structure model and ii) the second is the KL divergence between the approximate posterior distribution and the prior distribution. With appropriate choices of variational families and prior models, the optimized parameters $\\theta,\\varphi,\\psi$ from Eq. (10) minimize the divergence between the approximate distribution and the true distribution, i.e., $P_{\\varphi,\\psi}(A) \\approx P(A | X)$.\nTo compute (i), we begin by sampling a DAG adjacency matrix $A$ from the approximate distribution in each iteration. For every node $X_i$, we reconstruct its values by applying masking on the observed data $X$ with the sampled $A$. This is then followed by a transformation $f_{i,\\theta}$, parameterized using neural networks:\n$\\hat{X}_i = f_{i,\\theta} (A_i\\hat X),$   (11)\nwhere $A_i$ is the ith column in the adjacency matrix $A$. By assuming that the data has a Gaussian distribution with unit variance, we can approximate the first term by the least square loss, i.e., $\\| X - \\hat X \\|^2$.\nTo compute (ii), we initially employ a mean-field factorization for the variational model, i.e., $P_{\\varphi,\\psi} (W, p) = P_{\\varphi} (W) P_{\\psi} (p)$. This mean-field factorization provides us a convenient way to calculate the KL divergence, represented as:\n$D_{KL} (P_{\\varphi,\\psi} (W, p) || P_{prior} (W, p))$\n$= \\mathbb{E}_{p,W\\sim P_{\\varphi,\\psi}(p,W)} [log \\frac{P_{prior} (W) P_{prior} (p)}{P_{\\varphi} (W)}\n+ log \\frac{P_{prior} (p)}{P_{\\psi} (p)}].$\n$= \\mathbb{E}_{p,W\\sim P_{\\varphi,\\psi}(P,W)}[log \\frac{P_{prior} (W)}{P_{\\varphi} (W)} + log \\frac{P_{prior} (p)}{P_{\\psi} (p)}].$\n$= \\int \\int P_{\\varphi} (W) P_{\\psi} (p) [log \\frac{P_{prior} (W)}{P_{\\varphi} (W)} + log \\frac{P_{prior} (p)}{P_{\\psi} (p)}] dWdp$\n$= \\int P_{\\psi} (p) [\\int P_{\\varphi} (W) log \\frac{P_{prior} (W)}{P_{\\varphi} (W)} dW ] dp$\n$+ \\int P_{\\varphi} (W) [\\int P_{\\psi} (p) log \\frac{P_{prior} (p)}{P_{\\psi} (p)} dp ] dW.$\n$= \\int P_{\\psi} (p) D_{KL} (P_{\\varphi} (W) || P_{prior} (W)) dp$\n$+ \\int P_{\\varphi} (W) D_{KL} (P_{\\psi} (p) || P_{prior} (p)) dW$\n$= D_{KL} (P_{\\varphi} (W) || P_{prior} (W)) + D_{KL} (P_{\\psi} (p) || P_{prior} (p))$   (20)\nConsequently, we can compute the KL divergence between the approximate posterior distribution and the prior distribution over DAGS via the sum of the KL divergence between the variational model and the prior distribution over the edge matrix $W$ and the priority scores vector $p$.\nVariational Families: For the distribution over the priority scores vector $p$, we opt for the isotropic Gaussian, i.e., $p \\sim N(\\mu, \\sigma^2I)$. As discussed earlier, we choose the Gumbel-Softmax distribution [15] for the variational model of the edge matrix $W$. These choices enable us to utilize the pathwise gradient, offering a lower variance approach compared to the score-function method [25]. To compute the gradient, we leverage the straight-through estimator [4]: for $p$, we use the rounded value of $sigmoid (grad(p))$ in the forward pass, and its continuous value in the backward pass. For $W$, we use the discrete value $W_{ij} = arg \\max \\{W_{ij}, 1 - W_{ij}\\}$ in the forward pass, and the continuous approximation $W_{ij}$ in the backward pass."}, {"title": "5 Experiment", "content": "In this section, we demonstrate extensive experiments showing the empirical performance of our proposed method on DAG sampling and DAG structure learning tasks.\nBaselines. Regarding DAG sampling, we compare the proposed method with the Gumbel-Sinkhorn and Gumbel-Top-k approaches [6] in terms of sampling time for thousands of variables. Unlike our method, which samples a priority score vector, these approaches focus on sampling a permutation matrix. The Gumbel-Sinkhorn method [24] leverages the Sinkhorn operator to approximate the distribution over the permutation matrix. Meanwhile, Gumbel-Top-k combines the Gumbel-Top-k distribution [17] with the Soft-Sort operator [30] to achieve faster sampling compared to Gumbel-Sinkhorn. Regarding DAG structure learning, we focus our comparison on differentiable methods and hence select five state-of-the-art causal discovery methods that belong to both point-estimations and Bayesian based baselines:\nDatasets. We benchmark these methods on both synthetic and real datasets. For synthetic datasets, we closely follow [20, 26, 6]. For generating causal DAGs, we consider Erd\u0151s-R\u00e9nyi (ER) and scale-free (SF) network models with average degree equal to 1. We vary the graph size in terms of number of nodes $d = \\{10, 50, 100\\}$ and consider both linear and nonlinear Gaussian SEMs. For linear model, we generate a weighted adjacency matrix $W \\in \\mathbb{R}^{d \\times d}$ with edges' weights randomly sampled from $U ([-2, -0.5] \\cup [0.5, 2])$. We then generate the data $X \\in \\mathbb{R}^{n \\times d}$ following the linear SEM: $X = W X + \\epsilon$, where $\\epsilon \\sim N (0, I)$. For nonlinear model, we generate the data following $X_i = f_i (X_{pa(i)}) + \\epsilon_i$, where the functional model $f_i$ is generated from Gaussian Process with RBF kernel of bandwidth one and $\\epsilon_i \\sim N (0, 1)$. In our experiments, we sample 10 datasets per setting where each dataset includes a ground truth of the causal DAG's adjacency matrix, a training dataset of 1,000 samples and a held out testing dataset of 100 samples. For real datasets, we closely follow [39, 22, 26]. We use Sachs dataset [31] which measures the expression level of different proteins and phospholipids in human cells. The data contains 853 observational samples generated from a protein interaction network of 11 nodes and 17 edges.\nEvaluation metrics. We use the area under the curve of precision-recall (AUC-PR) and the area under the receiver operating characteristic curve (AUC-ROC) between the ground-truth binary adjacency matrix and the output scores matrix, denoted as S, where $S_{ij}$ represents the possibility of the presence of an edge from $X_i$ to $X_j$. For point estimation methods, we get the scores matrix from the output before thresholding. For Bayesian-based methods, we get the scores matrix by averaging 100 sampled binary adjacency matrix from the learned probabilistic DAG model. We also evaluate the learned functional model $f_{\\theta} (.)$ by computing the mean squared error (MSE) between the ground-truth node value $X_i$ and the estimated node values $\\hat X_i = f_{i,\\theta}(A_i\\hat X)$ on a held-out dataset.\nHyperparameters. We use a neural network with one hidden layer and ReLU activation to parameterize the functional models $f_i$ in the nonlinear setting and real-world dataset. We perform a random search over the learning rate $lr \\in [10^{-3}, 10^{-1}]$. The prior edge probability and scale of the priority scores vector are set to 0.01 and 0.1, respectively. Based on our ablation study (refer to Section 5.2), a temperature of 0.3 is chosen for its benefits. To prevent overfiting, VCUDA is trained by the Adam optimizer with the $l_2$-regularization of 1e-4. Furthermore, early stopping is employed with validation loss checks every 10 epochs. We find that the model is convergent within 500 epochs."}, {"title": "5.1 DAG sampling", "content": "We compare our DAG sampling model to two well-known models, including Gumbel-Sinkhorn and Gumbel-Top-k [6] on large-scale DAGs sampling."}, {"title": "5.2 DAG structure learning", "content": "Synthetic datasets. We present the results of DAG structure learning on ER/SF graphs with $d = \\{10, 50\\}$ for both linear and nonlinear models. The results exhibit the superior performance of VCUDA across all settings, particularly in terms of AUC-ROC. Specifically, the AUC-ROC values of VCUDA remain consistently high, always surpassing 0.9 for linear models and 0.8 for nonlinear models. In contrast, the AUC-ROC values of the other baselines show volatility depending on the settings. Additionally, VCUDA achieves a low MSE measures, outperforming most baselines including GranDAG, MCSL, and DDS across all settings. In comparison to DiBS, the MSE measures of VCUDA are comparable for both linear and nonlinear settings. We observe a notably superior performance of Bayesian baselines compared to point-estimation baselines, especially in scenarios with higher dimensions (e.g., d = 50). This disparity in performance can be attributed to the Bayesian methods' capacity to capture the uncertainty effectively. Furthermore, we study the performance of VCUDA and baseline methods on the high dimensional causal graph with d = 100 for both linear and nonlinear models."}, {"title": "6 Conclusion", "content": "We introduce VCUDA, a scalable approach for Bayesian causal discovery from observational data. By eliminating explicit acyclicity constraints, we propose a differentiable approach for DAGs sampling, enabling fast generation of large DAGs within seconds. In addition, the efficient sampling model enhances Bayesian inference for causal structure models when integrated into the variational inference framework. Extensive experiments on synthetic and real datasets showcase VCUDA's superior performance, outpacing other baselines in terms of multiple metrics, all achieved with remarkable efficiency."}, {"title": "Appendix for \u201cScalable Variational Causal Discovery Unconstrained by Acyclicity\"", "content": "A Theorem 2\nTheorem 2. Let $A \\in \\{0,1\\}^{d\\times d}$ be an adjacency matrix of a graph of $d$ nodes. Then, A is DAG if and only if there exists corresponding a vector of priority scores $p \\in \\mathbb{R}^d$ and a binary matrix $W \\in \\{0,1\\}^{d\\times d}$ such that:\n$A = \\nu (W, p) = W\\cdot \\lim_{t \\rightarrow 0} sigmoid(\\frac{grad(p)}{t}),$\nwhere $t>0$ is a strictly positive temperature and $p$ contains no duplicate elements, i.e., $p_i \\neq p_j \\forall i, j$.\nProof. We first show that for any DAG A, there always exists a pair $(W, p)$ such that $\\nu (W, p) = A$. By leveraging Theorem 3.7 in [39], we can see that $p$ implicitly define the topological order over vertices of A such that:\n$\\grad (p)_{ij} > 0$ when $A_{ij}=1$\nThen,\n$\\lim_{t \\rightarrow 0} sigmoid(\\frac{\\grad (p)}{t}) = 1$ when $A_{ij}=1$\nFurthermore, we can choose $W$ in the following way:\n$W_{ij}=\\begin{cases} 0 & \\text{if } A_{ij}=0 \\\\ 1 & \\text{if } A_{ij}=1 \\end{cases}$\nFor an arbitrary topological (partial) order of the variables $\\pi = (\\pi_1, \\pi_2, ..., \\pi_d)$, it always defines a DAG where each edge $(i, j)$ corresponding to $i \\lneq j$. To prove that the mapping $\\nu (W, p)$ always emits a DAG, let define a vector $p \\in \\mathbb{R}^d$ such that $p [\\pi [i]] = i$. We have:\n$i \\lneq j \\Rightarrow \\pi_j > \\pi_i$\n$\\Rightarrow p_j > p_i$\n$\\Rightarrow \\grad_{ij} > 0$\nTherefore, $\\lim_{t \\rightarrow 0} sigmoid(\\frac{\\grad(p)}{t})$ outputs an acyclic binary adjacency matrix. Then, taking the element-wise multiplication with any W gives us a sub-graph of a DAG, which is also a DAG.\nB Additional results\nSampling time."}]}