{"title": "GRAPHMOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via Introducing Self-Rethinking Mechanism", "authors": ["Chen Tang", "Bo Lv", "Zifan Zheng", "Bohao Yang", "Kun Zhao", "Ning Liao", "Xiaoxing Wang", "Feiyu Xiong", "Zhiyu Li", "Nayu Liu", "Jingchi Jiang"], "abstract": "Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple smaller expert models as opposed to a single large network. However, these experts typically operate independently, leaving a question open about whether interconnecting these models could enhance the performance of MoE networks. In response, we introduce GRAPHMOE, a novel method aimed at augmenting the cognitive depth of language models via a self-rethinking mechanism constructed on Pseudo Graph MoE networks. GRAPHMOE employs a recurrent routing strategy to simulate iterative thinking steps, thereby facilitating the flow of information among expert nodes. We implement the GRAPHMOE architecture using Low-Rank Adaptation techniques (LoRA) and conduct extensive experiments on various benchmark datasets. The experimental results reveal that GRAPHMOE outperforms other LORA based models, achieving state-of-the-art (SOTA) performance. Additionally, this study explores a novel recurrent routing strategy that may inspire further advancements in enhancing the reasoning capabilities of language models.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), exemplified by the GPT series, have dramatically advanced the field of Natural Language Processing (NLP), demonstrating exceptional performance across a variety of tasks including commonsense reasoning(Yang et al., 2024), creative generation(Loakman et al., 2023), dialogue generation(Lv et al., 2024; Zhao et al., 2023, 2024b), and summarization(Goldsack et al., 2023; Zhao et al., 2024a). In recent years, there has been a growing interest in refining LLM architectures to deliver superior performance with reduced parameters and lower GPU memory consumption(Tang et al., 2023). A prominent strategy in this endeavor is the Mixture-of-Experts (MoE) architecture (Cai et al., 2024), which pre-trains multiple Feed-Forward Networks (FFNs) as specialized experts, activating a select few during inference. This strategy allows experts to operate optimally within their domains of expertise, providing enhanced performance compared to LLMs with a similar number of activated parameters.\nHowever, expert models are employed independently through a linear routing strategy. Given that these models are optimized for distinct input distributions, we hypothesize that fostering collaboration among expert models akin to connected nodes in a graph network may further exploit their problem-solving capabilities. Inspired by the knowledge aggregation on pseudo graph (Tang et al., 2023), we propose employing a recurrent routing strategy wherein expert models function as graph nodes. We posit that with increased graphical message transmission between expert nodes, these models can progressively narrow the semantic gap between the MoE model outputs and human references. Thus, we introduce GRAPHMOE(Graph Mixture of Experts), a pioneering approach aimed at deepening the cognitive capabilities of language models by integrating a self-rethinking mechanism into constructed pseudo-graph MoE networks. This methodology may bolster the reasoning capabilities of LLMs by enabling incremental problem-solving, akin to the Chain-of-Thoughts (CoT) strategy.\nGRAPHMOE emulates human-like iterative reasoning by allowing the model to continuously revisit and refine its comprehension of input data through multiple reasoning cycles. This is accomplished by implementing a recurrent routing strategy on a pseudo-graph formed by expert models. These models transmit their outputs as signals for selecting subsequent expert batches and as inputs encapsulating aggregated graph features from prior reasoning rounds a mechanism we term the \u201cself-rethinking mechanism\u201d. Given the challenges inherent in pre-training a MoE LLM from scratch, we chose to implement the GRAPHMOE architecture utilizing Low-Rank Adaptation (LoRA) techniques and enabling evaluation of GRAPHMOE via Supervised Fine-Tuning (SFT) across several benchmarks."}, {"title": "2 Related Work", "content": "2.1 Parameter-Efficient Fine-Tuning (PEFT)\nLLMs have demonstrated remarkable improvements in general natural language understanding (NLU) and natural language generation (NLG) tasks (Guo et al., 2023; Chang et al., 2024; Tang, 2024). However, their performance often suffers from limited generalization and effectiveness in domain-specific applications. To address this, numerous studies have proposed Parameter-Efficient Fine-Tuning (PEFT) methods, which optimize only a small subset of LLM parameters, significantly reducing computational overhead.\nOne of the most popular PEFT methods is Low-Rank Adaptation (LoRA) (Hu et al., 2021a), which introduces two low-rank matrices to each weight matrix W in LLMs. The product of these matrices represents the weight adjustment AW. Building upon LoRA, subsequent works have introduced more efficient variants, such as VeRa (Kopiczko et al., 2023), AdaLoRA (Zhang et al., 2023), DORA (Liu et al., 2024), and MoSLORA (Wu et al., 2024b). These\n2.2 Transformer & Recurrent Models\nThe self-attention mechanism, as a fundamental algorithm within Transformers (Vaswani, 2017), facilitates parallel sequence processing and enhances model capacity through increased width. However, it lacks the temporal reasoning capabilities inherent in recurrent architectures, which contribute to model depth, such as Long Short-Term Memory (LSTM) networks (Hochreiter, 1997) and Gated Recurrent Units (GRUS) (Chung et al., 2014). To address this limitation, recent research has investigated integrating Transformer architectures with recurrent structures (Peng et al., 2023;\nPramanik et al., 2023; Zhang et al., 2024; Tang, 2024). These efforts aim to blend recurrent neural network capabilities with the ability to capture long-distance dependencies. In this study, we likewise endeavor to incorporate a recurrent mechanism to model multiple expert systems. However, rather than focusing on capturing long-distance dependencies, our primary objective is to emulate the stepwise cognitive processes characteristic of human cognition. To achieve this, we employ GRUs to augment reasoning depth by aggregating hidden representations from attention features at each stage of the recurrent routing process. This integration is designed to enhance the model's proficiency in apprehending complex dependencies and thereby improve its overall reasoning capabilities.\n2.3 Mixture of Experts (MoE)\nThe concept of Mixture of Experts (MoE) was first introduced by Jacobs et al. (1991), who proposed training multiple networks (experts) on different subsets of data and aggregating their outputs. Recently, as LLMs have become a focal point of research, MoE layers have been integrated into Transformer-based architectures. Specifically, researchers have replaced standard Feed-Forward Networks (FFNs) with sparse MoE layers, employing novel routing strategies (Zuo et al., 2021; Zhong et al., 2024; Wu et al., 2024a;\nMuqeeth et al., 2023; Fu et al., 2024) and advanced expert segmentation techniques (Dai et al., 2024; He, 2024; Jiang et al., 2024; Xiao et al., 2024).\nNumerous studies have investigated the application of Parameter-Efficient Fine-Tuning (PEFT) methods to introduce additional trainable parameters for implementing pseudo MoE structures within LLMs (Dou et al., 2024; Luo et al., 2024; Gao et al., 2024; Li et al., 2024; Gou et al., 2023). These models expand the conventional single feed-forward network (FFN)"}, {"title": "3 Method", "content": "In this section, we present the construction of the pseudo-graph network of MoE model designed to interconnect expert models as graph nodes, facilitating information exchange among these models in a manner akin to the human brain. In this analogy, different neurons are responsible for specific abilities and collaborate through signal transmission across synapses. We apply our GRAPHMOE methodology to additional LoRA networks, and introduce our approach in three distinct subsections: \u00a73.1 MoE Transformation, \u00a73.2 Pseudo Reasoning Graph Construction, and \u00a73.3 GRAPHMOE Training.\n3.1 MoE Transformation\nIncorporating multiple expert models into LoRA layers, inspired by the work of Li et al. (2024) and Dou et al. (2024), the standard Feed-Forward Networks (FFNs) can be converted into MoE structures comprising n experts, denoted as {$E_i$}$_{i=1}^n$. As illustrated in the bottom-right corner of Figure 2, for the l-th layer of the LLM (1<l<L), we freeze the original FFN parameters and train three pairs of low-rank matrices for each expert: $A_{Down_i} \\in \\mathbb{R}^{r \\times d_1}$, $B_{Down_i} \\in \\mathbb{R}^{d_2 \\times r}$,\n$A_{Gate}$, $B_{Gate}$, $A_{Up}$ and $B_{Up}$. Here, i represents the i-th expert, and $d_1$, $d_2$ are the dimensions of the corresponding weight matrices $W_{Down_i} \\in \\mathbb{R}^{d_2 \\times d_1}$, with other dimensions defined similarly. Each expert $E_i(\\cdot)$ is constructed as a fusion of the original FFN and trainable low-rank matrices, as shown in Equations 1 and 2, where $\\sigma(\\cdot)$ denotes activation functions (e.g., SiLU, ReLU), and $\\odot$ denotes element-wise multiplication.\n$E_i(x) = W_{Down_i} (\\sigma (\\mathbb{(}W_{Gate_i}, x) \\odot (W_{Up_i}, x)))$   (1)\n$W_{Gate_i} = W_{Gate} + B_{Gate_i} A_{Gate_i}$\n$W_{Down_i} = W_{Down} + B_{Down_i} A_{Down_i}$   (2)\n$W_{Up} = W_{Up} + B_{Up_i} A_{Up_i}$\nAdditionally, we introduce a trainable linear layer\n$R \\in \\mathbb{R}^{n \\times d}$, referred to as the Router, where n denotes the total number of experts, and d represents the dimension of the hidden state. The Router determines the importance of each expert during forward propagation by computing a relevance score for every expert. Based on the Router's output, we select the top k experts and normalize their corresponding weights to ensure effective routing, as described in Equations 3 and 4.\n$\\hat{s} = Softmax(Rx)$   (3)\n$s = Top-k(\\hat{s})$   (4)\n$\\hat{s}[i], if \\hat{s}[i] is in the top k$,\n$[s']_i =\n$0, otherwise.$\nFinally, as shown in Equation 5, the output of the MoE module is computed as a weighted sum of the selected experts. This result replaces the original output of the vanilla FFN module.\n$y = MoE(x) = \\sum_{i=1}^n s[i] \\cdot E_i(x)$   (5)\n3.2 Pseudo Reasoning Graph Construction\nThe development of the pseudo reasoning graph rep-resents a pivotal step in implementing our proposed self-rethinking mechanism, which is designed to enhance the internal consistency of LLMs (Liang et al., 2024). The pseudocode outlining the inference\nprocess of GRAPHMOE is detailed in Algorithm 1. For clarity and conciseness, the terms Gate, Up, and Down in the subscripts are abbreviated as G, U, and D, respectively.\nInitially, the pseudo graph is constructed as illustrated in the top right corner of Figure 2. Within this framework, expert models are represented as graph nodes, each functioning according to the procedure described in \u00a73.1. A distinctive virtual node is simultaneously established alongside these expert model graph nodes. Unlike other nodes, the virtual node is characterized by a graph feature vector and a GRU module. This node serves as a recurrent router, acting as a conduit for expert node connectivity and a layer for forward feature aggregation.\nIn contrast to conventional knowledge graphs, there is no prior knowledge that defines the edges between nodes. Instead, the edges connecting graph nodes are dynamically generated at each reasoning round, denoted as t, with the virtual node serving as the intermediary, as indicated by the black dashed arrows in Figure 2. This process facilitates the dynamical activation of temporal edges between nodes, enabling feature transformation across nodes and the newly created edges. Consequently, expert nodes at each reasoning round can communicate and collaborate to address the given task effectively.\nTherefore, the core of this method lies in implementing the functionality of the virtual node. In this\nwork, we employ a Low-Rank Gated Recurrent Unit2 (GRU) to serve as the virtual node. Specifically, we reduce the GRU's hidden size to d' << d and incorporate a low-rank linear layer to compute its output gf. The virtual node integrates features from previous reasoning round and processes the expert results $y^l_i$, subsequently selecting k experts for the next round.\nThe reasoning (self-thinking) process involves a total of T rounds of MoE computations. The first round is processed as described in Equations 1-5. After obtaining the representation y from the MoE, it is fed into the Low-Rank GRU along with the hidden state $h_{t-1}$ from the previous timestep (here, \u201cround\u201d and \"timestep\" are equivalent since there is only one step per round). A residual connection is applied by adding the result to the input of the previous round, x, to produce the input for the next round, $x_{t+1}$. This iterative process extracts meta-information for subsequent reasoning rounds, as shown in Equations 6-7.\n$z_t = \\sigma (W_z [h_{t-1}; y])$ , $W_z \\in \\mathbb{R}^{\\bar{d} \\times (d + \\bar{d})}$ (6)\n$r_t = \\sigma (W_r [h_{t-1}; y])$ , $W_r \\in \\mathbb{R}^{\\bar{d} \\times (d + \\bar{d})}$\n$\\hat{h}_t = \\sigma (W_h [r_t h_{t-1}; y] + b_h)$\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\hat{h}_t$\ng_t = W_g h_t, $W_g \\in \\mathbb{R}^{d \\times \\bar{d}}$\n$x_{t+1} = x + g_t$  (7)\nIn a nutshell, from a model architecture standpoint, the GRU and the Router together form the aforementioned \"virtual node\". Specifically, the GRU functions to aggregate information from multiple experts $E_i$, whereas the Router is responsible for determining\n3.3 GRAPHMOE Training\nBuilding upon the architecture introduced in \u00a73.1 and \u00a73.2, this section provides a detailed explanation of the training methodology for GRAPHMOE.\nTrainable Components. As shown in Figure 2, the modules marked with a flame icon represent the trainable components in GRAPHMOE. Specifically, we need to train 3 \u00b7 Ln pairs of low-rank adapters applied to the MoE modules, as well as a single linear layer serving as the Router. Additionally, in each decoder layer, the Low-Rank GRU module requires training 4 linear layers: $W_z, W_r, W_h$, and $W_g$.\nInspired by Li et al. (2024), we also enhance the attention module by adding 4 pairs of trainable low-rank matrices to each decoder layer. These matrices are"}, {"title": "4 Experiment", "content": "4.1 Experimental Settings\nAs there are multiple methodologies for applying MoE to LoRA layers, the aforementioned GRAPHMOE architecture is developed on the foundation\nof existing LoRA combined with MoE base models, designated as GRAPHMOE(base model). In the GRAPHMOE(base model), the conventional MoE module is substituted with our proposed pseudo reasoning graph (PRG).\nTo evaluate GRAPHMOE, we select a diverse range of commonsense reasoning datasets that offer unique challenges across multiple task types. These include question-answer tasks such as ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), and SocialIQA (Sap et al., 2019), each designed to assess different aspects of commonsense and contextual reasoning. For classification, we use BoolQ (Clark et al., 2019), which tests the model's ability to handle yes/no questions. Hellaswag (Zellers et al., 2019) is employed for science completion tasks, requiring selection of the most plausible outcomes from rich contextual setups. Winogrande (Sakaguchi et al., 2021) addresses fill-in-the-blank tasks, facilitating evaluation of nuanced language understanding through large-scale ambiguities.\nThese datasets collectively provide a platform for assessing our model's accuracy and generalization across various reasoning aspects. Experiments are conducted using brain floating point 16-bit (BF16), as using full precise float 32 (FP32) results in experiments being over four times slower, which is unacceptable given our computing constraints.\n4.2 Baselines\nIn our experimental setup, all baselines are evaluated by fine-tuning LLMs on datasets. This is achieved by keeping the parameters of the base language model frozen and training only the additional adapters. We use the LLaMA-3-8B model as our base language model throughout the experiments. To benchmark our proposed GRAPHMOE model, we chose to compare it against the traditional Low-Rank Adaptation (LORA) (Hu et al., 2021b) approach, as well as three recently introduced SOTA LoRA+MoE methods:\nMOLA (Gao et al., 2024), LoRAMOE (Dou et al., 2024), and MixLoRA (Li et al., 2024). Considering Weight-Decomposed Low-Rank Adaptation (DoRA) (Liu et al., 2024) has been shown to effectively enhance the LoRA method, we also develop DoRA variants for each of these selected baseline models to benchmark against our method.\n4.3 Evaluation Metrics\nThe evaluation of all methods was conducted using the accuracy metric across all datasets. In order to accurately extract the answers provided by LLMs for\n4.4 Implementation Details\nOur hyperparameters strictly follow the configurations detailed in (Li et al., 2024) including the LoRA and DORA methods and their three MoE derivatives, e.g.\nMixLORA or MixDoRA. We summarize the settings for LORA/DORA and their derivatives of MoE-based methods, as shown in During evaluation, the batch size is set to 8."}, {"title": "5 Experimental Result", "content": "5.1 Evaluating the Performance of GRAPH\u039c\u039f\u0395\nAs illustrated the implementation of GRAPHMOE, a pseudo graph-based MoE network incorporating a recurrent routing strategy, has resulted in a significant performance enhancement across all SOTA LORA+MoE baselines. This underscores GRAPHMOE's ability to boost performance by fostering improved collaboration among expert models, achieved solely by replacing the MoE module within different methods. The observed improvement stems from an optimized mechanism utilizing the expert models to learn the representations with the feature transmission among themselves.\nThe degree of enhancements varies across models, influenced by their specific base architectures. This variability is attributed to the integration level, or the combination mode, between the MoE and LoRA components, which can impact the parameter space for incremental learning and thereby modify the marginal benefits of GRAPHMOE. Specifically, as highlighted by Li et al. (2024), the MoLA and LoRAMOE configurations incorporate LoRA modules as expert models by integrating them with outputs from either the attention layers or feedforward network (FFN) layers\u2014an approach termed the \u201cPlugged-in\u201d method. In contrast, MixLoRA employs a \u201cfused\u201d integration strategy, embedding the LoRA expert models directly within the original network layers. This design allows the LORA experts to exert a more immediate and substantial influence on the conventional LLM components, resulting in the most significant enhancement of the original LoRA+MoE architecture.\nAs observed GRAPHMOE consistently exerts positive effects\u2014or at least avoids significant negative impacts\u2014on model accuracy across diverse tasks. In some instances, only one out of eight tasks shows a slight accuracy decrease of 0.1, and these instances occur with different tasks when applying\n5.2 Main Results\nThe experimental results are detailed Due to limitations in computational resources, the experiments conducted in this study employ reduced precision BF16 for both training and inference phases. However, we include the full precision results of MixLORA as a reference for comparison. It is evident that all reproduced models experience a significant performance decline compared to their full precision counterparts. Nonetheless, our GRAPHMOE models with MixLoRA and MixDORA as base models still achieve SOTA performance even compared to the full precision results. This substantial improvements compared to other SOTA LORA+MoE models further demonstrate the effectiveness of the GRAPHMOE architecture.\nMoreover, MixLoRA does not represent the optimal base model with the LoRA+MoE framework, as MOLA and LoRAMOE have better accuracy scores. This indicates that the performance gains achieved by GRAPHMOE surpass those of existing methodologies applied within the LoRA+MoE framework. The introduction of a self-rethinking mechanism is crucial in enhancing the representation learning capabilities inherent to the MoE approach and potentially even\n5.3 Analysis of Workload Balance\nThe primary distinction between MixLoRA and GRAPHMOE(MixLoRA) lies in the self-rethinking mechanism facilitated by the recurrent router. We conducted a further analysis of the impact of expert model selection at each routing stage. The findings indicate that the GRAPH\u039c\u039f\u0395 routing strategy enables a more balanced selection of expert models, as demonstrated by the standard deviation metrics of 0.0313 for MixLORA and 0.0249 for GRAPHMOE(MixLoRA), representing a reduction of over 20% in standard deviations across 8 experts.\nThe aforementioned phenomenon suggests that multiple iterations of the reasoning process allow for a greater number of experts to be involved in addressing problems, thereby enhancing the effectiveness of MoE in representation learning. In each reasoning step, a distinct set of expert models is more likely to be activated to incrementally construct the representations within the MoE. This approach allows more expert\n5.4 Sensitivity Analysis and Overhead\nDue to computational constraints, our sensitivity analysis was conducted exclusively on the ARC-E and ARC-C tasks, as these have the smallest dataset sizes. This choice allowed for significantly faster model execution compared to other tasks. The results are depicted It is evident that the hyperparameters selected in previous experiments were not optimal, suggesting that GRAPHMOE could achieve greater improvements in base models with a better selection of hyperparameters. The primary objective of this sensitivity analysis is to provide insights into the efficacy of our self-rethinking mechanism.\nIn subfigure (a), as the Reasoning Round T increases, both tasks exhibit an increase in accuracy, indicating that the self-rethinking mechanism indeed enhances the effectiveness of MoE by deepening its cognitive processing. However, accuracy significantly declines once T reaches a certain threshold, specifically T=4 for ARC-E and T=3 for ARC-C. We postulate that this decline is due to overfitting, implying that the model risks \u201coverthinking\u201d the problem. This is further evidenced by the ARC-C task in where an increase in the GRU hidden size does not guarantee improved performance. This phenomenon underscores the importance of the self-rethinking mechanism itself over the newly introduced trainable parameters of the GRU. In subfigure (c) of Figure 5, it is observed that, compared to conventional LoRA+MoE (MixLoRA in this experiment) models, which employ a single reasoning round, the increase in inference time for GRAPHMOE is only\napproximately 0.16 times for each additional reasoning round. This increase is both manageable and acceptable. In summary, these findings demonstrate the efficacy of our proposed method and its promising impact on enhancing the cognitive depth of MoE."}, {"title": "6 Conclusion", "content": "In conclusion, this study introduces GRAP\u0397\u039c\u039f\u0395, a novel approach that enhances the cognitive depth of language models through the integration of a self-rethinking mechanism in pseudo-graph Mixture-of-Experts (MoE) networks. Unlike traditional MoE models that operate independently, GRAPH\u039c\u039f\u0395 facilitates communication among expert nodes, allowing for iterative refinement and enhanced reasoning capabilities. Implemented using Low-Rank Adaptation techniques (LoRA), GRAPHMOE demonstrates significant performance improvements across various benchmark datasets, outperforming existing LORA & LoRA+MoE baseline models and achieving state-of-the-art results. This work not only highlights the potential of employing graph-based recurrent routing strategies to implicitly increase the cognitive depth of LLM via \u201cself-rethinking\u201d, but also opens avenues for further exploration in leveraging interconnected expert networks for advanced problem-solving and reasoning tasks in natural language processing.\nLimitations\nDespite the notable performance improvements achieved by the GRAPHMOE framework, there are several limitations to be acknowledged.\n\u2022 While the GRAPHMOE architecture consistently leads to performance gains across diverse tasks and base models, the degree of improvement is variable and can depend on the interaction between the MoE and LORA components. This variability underscores the need for comprehensive exploration into the integration modalities and parameter space to ensure maximum efficiency across different LORA+MOE configurations.\n\u2022 The workload distribution analysis indicates a marked improvement in workload balance with GRAPHMOE. However, the potential impact of workload imbalance on model performance over a broader range of tasks remains underexplored. Further investigation into balancing expert model selection and activation across diverse scenarios could unveil additional pathways for performance optimization.\n\u2022 The sensitivity analysis indicated potential overfitting issues when increasing the reasoning rounds beyond a particular threshold, revealing the necessity for careful hyperparameter tuning to mitigate issues of over-complexity and model overthinking.\nIn summary, while the GRAPHMOE framework shows promise in enhancing cognitive depth and performance of MoE architectures, future research should address computational precision limits, explore broader integration strategies, and focus on"}]}