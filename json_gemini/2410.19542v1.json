{"title": "BRAIN-LIKE FUNCTIONAL ORGANIZATION WITHIN LARGE LANGUAGE MODELS", "authors": ["Haiyang Sun", "Lin Zhao", "Zihao Wu", "Xiaohui Gao", "Yutao Hu", "Mengfei Zuo", "Wei Zhang", "Junwei Han", "Tianming Liu", "Xintao Hu"], "abstract": "The human brain has long inspired the pursuit of artificial intelligence (AI). Recently, neuroimaging studies provide compelling evidence of alignment between the computational representation of artificial neural networks (ANNs) and the neural responses of the human brain to external stimuli, suggesting that ANNs may employ brain-like information processing strategies. While such alignment has been observed across sensory modalities-visual, auditory, and linguistic-much of the focus has been on the behaviors of artificial neurons (ANs) at the population level, leaving the functional organization of individual ANs that facilitates such brain-like processes largely unexplored. In this study, we bridge this gap by directly coupling sub-groups of artificial neurons with functional brain networks (FBNs), the foundational organizational structure of the human brain. Specifically, we extract representative patterns from temporal responses of ANs in large language models (LLMs), and use them as fixed regressors to construct voxel-wise encoding models to predict brain activity recorded by functional magnetic resonance imaging (fMRI). This framework effectively links the AN sub-groups to FBNs, enabling the delineation of brain-like functional organization within LLMs. Our findings reveal that LLMs (BERT and Llama 1\u20133) exhibit brain-like functional architecture, with sub-groups of artificial neurons mirroring the organizational patterns of well-established FBNs. Notably, the brain-like functional organization of LLMs evolves with the increased sophistication and capability, achieving an improved balance between the diversity of computational behaviors and the consistency of functional specializations. This research represents the first exploration of brain-like functional organization within LLMs, offering novel insights to inform the development of artificial general intelligence (AGI) with human brain principles.", "sections": [{"title": "INTRODUCTION", "content": "The human brain, with its unparalleled capacities in perception, cognition, reasoning, and creativity, stands as the pinnacle of biological intelligence and complexity (Sporns et al., 2000; Bassett &\nGazzaniga, 2011). Understanding the mechanisms behind these cognitive abilities has been one of\nthe most formidable challenges in neuroscience for decades (Brodmann, 1909; Hubel & Wiesel,\n1979; Belliveau et al., 1991; Bear et al., 2020). Despite significant advances, the intricate processes\nthrough which the brain organizes and interprets information\u2014transforming raw sensory inputs into"}, {"title": "RELATED WORK", "content": ""}, {"title": "NEURAL ENCODING OF COMPUTATIONAL LANGUAGE MODELS", "content": "Neural encoding studies have demonstrated that computational language models based on deep neu-ral networks exhibit considerable representational alignment to neural activity in the human brain\n(Abdou, 2022; Schrimpf et al., 2021; Oota et al., 2024; Antonello et al., 2024). Most prior research"}, {"title": "INTERPRETING BEHAVIORS OF INDIVIDUAL ANS", "content": "Researchers have developed various strategies to interpret the behaviors of individual ANs in com-putational language models (Zhao et al., 2024). These strategies include feature attribution, prob-ing, neuron activation analysis, attention visualization, adversarial example, and inverse recognition,\namong others (Wu et al., 2023; Zhang et al., 2022; Yeh et al., 2023; Wang et al., 2022). Recently,\nresearchers have employed more advanced models such as GPT-4 to automate the interpretation of\nlarge scale individual ANs in less capable models such as GPT-2 (Bills et al., 2023). Singh et al.\n(2023) similarly use LLMs to generate candidate explanations for text modules, such as a neuron in\nLLM, based on the n-grams that elicit the most activation from the neuron. Synthetic data is then\ngenerated based on these explanations, and the neuron's activation to the data is assessed to identify\nthe top candidate explanations. While these strategies provide valuable insights into our understand-ing of language models, the functional organization of individual ANs has rarely been explored.\nFurthermore, the behaviors of individual ANs have yet to be linked to neural response, leaving the\nquestion of whether the organization of ANs mirrors the functional structure and organization found\nin the brain inadequately addressed."}, {"title": "METHODS", "content": ""}, {"title": "OVERVIEW", "content": "The study overview is illustrated in Figure 1. We begin by defining artificial neurons (ANs) in LLMs\nand quantifying their temporal responses to external stimuli $X \\in R^{t\\times n}$ (Figure 1a). Subsequently,\nwe employ a sparse representation (Mairal et al., 2009) scheme to learn a set of representative\ntemporal response patterns, referred to as a dictionary $D_{AN} \\in R^{t\\times k}$ (Figure 1b). Afterwards, we use\nthe dictionary $D_{AN}$ as regressors to build voxel-wise encoding models to predict fMRI brain activity.\nThe encoding coefficients associated with each atom reveal how that atom couples with functional\nactivity of the entire brain (Figure 1c). By integrating this coupling relationship with the association\nbetween ANs and $D_{AN}$ established during learning of representative temporal responses, we infer\nbrain-like functional organization in LLMs."}, {"title": "ARTIFICIAL NEURONS IN LLMS AND THEIR TEMPORAL RESPONSES", "content": "In this study, we focus on four LLMs: the pre-trained BERT model (Devlin et al., 2018), which\nserves as a foundational transformer-based language model, and three progressively advanced mod-els from the evolutionary Llama family, Llama 1-3 (Touvron et al., 2023a;b; Dubey et al., 2024).\nBERT, with its bidirectional encoder, is a widely recognized baseline model to capture rich, con-textualized word representations. In contrast, the Llama models, employing a decoder-based archi-tecture, represent a more advanced, contemporary approach, exhibiting superior performance across\ndiverse tasks. Examining the evolution of these LLMs may offer insights into the development of\nfunctional organization within these models.\nBuilding on the established definitions of Artificial Neurons (ANs) in large language models (LLMs)\n(Bills et al., 2023; Samek et al., 2021), we define each neuron in the second fully connected layer of\nthe feed-forward network within each transformer block as an individual AN (Figure 1a). In BERT,\nthis applies to the encoder blocks, while in the Llama models, it applies to the decoder blocks. With\nthis definition, the number of ANs corresponds to the dimensionality of the output embedding in\neach transformer block. For instance, BERT consists of 12 layers, yielding 9,216 ANs (12 layers\n\u00d7 768 dimensions per layer), whereas the Llama models, with 32 layers, define 131,072 ANs (32\nlayers \u00d7 4096 dimensions per layer).\nGiven a text input, the temporal responses of each AN are formally defined as it activations in\nresponse to the sequence of input tokens. The temporal responses of all ANs at layer I can be\nreadily obtained through the layer's output $X_l \\in R^{t\\times n_l}$, where t is the the number of tokens in the\ninput sequence, and n is the number of ANs at layer l, corresponding to the dimensionality of the\noutput.\nAdditionally, it is critical to synchronize the temporal responses of artificial neurons (ANs) with the\nfMRI timeline. To achieve this, we align the text tokens with the corresponding fMRI volumes using\nthe time-stamped word-level transcripts from the Narratives fMRI dataset (Nastase et al., 2021).\nHowever an fMRI volume generally spans multiple text tokens, we follow common practice in brain\nencoding studies by averaging the ANs' responses over these tokens within each fMRI time interval.\nThis produces a temporal response curve that matches the length of the fMRI sequence. Finally, we\nconvolve the temporal response curve of each AN with a canonical hemodynamic response function\n(HRF) implemented in SPM\u00b9, to account for the hemodynamic delay inherent in fMRI recordings."}, {"title": "REPRESENTATIVE TEMPORAL RESPONSE PATTERNS OF ANS", "content": "Identifying representative temporal response patterns of ANs is crucial for simplifying the analysis\ngiven the vast number of ANs in models like BERT and Llama. With thousands of ANs in each\nmodel (e.g., 9,216 in BERT and 131,072 in Llama), analyzing individual responses is not only\nimpractical but also risks obscuring key trends due to factors such as noise and self-correlation\namong the ANs. In this study, we employ a sparse representation scheme (Mairal et al., 2009) to\nlearn a set of representative patterns from the temporal responses of the entire group of ANS.\nGiven the set of temporal responses $X \\in R^{t\\times n}$, where n is the total number of ANs and t is the\nlength of the temporal responses, the objective is to find a sparse representation $A_{AN} \\in R^{k\\times n}$ over\na dictionary $D_{AN} \\in R^{t\\times k}$, minimizing the reconstruction error while imposing a sparsity constraint\non $A_{AN}$ (Mairal et al., 2009):\n\nmin ||X - D_{AN}A_{AN}||_2 + \\lambda_{AN} ||A_{AN}||_1\nA_{AN}\n\n(1)\n\nwhere $\\lambda_{AN}$ is a regularization parameter that controls the trade-off between reconstruction accu-racy and sparsity of $A_{AN}$. In this context, $D_{AN}$ represents a set of basis vectors or atoms, which is\nthe representative temporal patterns that capture the essential dynamics of the ANs' temporal re-sponses. The sparsity constraint ensures that each temporal response is characterized by only a few\nkey patterns. By learning this dictionary, we can express the entire set of temporal responses X as a\ncombination of these representative patterns, weighted by the sparse coefficients in $A_{AN}$."}, {"title": "VOXEL-WISE ENCODING OF FMRI BRAIN ACTIVITY", "content": "We construct voxel-wise encoding models to establish the relationship between ANs and brain ac-tivities. This approach allows us to determine how the temporal responses of ANs can predict or\naccount for the neural signals captured by fMRI. The encoding models are based on a similar scheme\nto the one used for learning representative temporal response pattern. The key difference is that we\nfix the dictionary $D_{AN}$ to learn a sparse representation $A_f \\in R^{k\\times N}$ for reconstructing the fMRI brain\nactivity $S \\in R^{t\\times N}$, where N is the number of voxles:\n\nmin ||S - D_{AN}A_f||_2 + \\lambda_f || A_f||_1\nA_f\n\n(2)\n\nEach row in $A_f$ indicates the importance of the corresponding atom of $D_{AN}$ in reconstructing fMRI\nbrain activities at each voxel (Figure 1c). It is noted that the voxel-wise encoding models are con-structed for each subject independently. A one-sample t-test over the entire population of subjects is\nconducted for each voxel to examine whether the encoding coefficient of a given atom $D_{AN}$ is above\nchance level (FDR corrected). Rather than simply showing voxel-level activations, this statistical\nmap provides a spatial depiction of the brain regions linked to each representative pattern, offering\na more interpretable perspective. For simplification, we refer to this as a brain map."}, {"title": "RELATIONSHIP INFERENCE BETWEEN ANS AND BRAIN NETWORKS", "content": "The representative temporal response patterns $D_{AN}$ serve as a bridge between ANs and brain activity.\nSpecifically, the $i^{th}$ AN can be associated with the $j^{th}$ atom of $D_{AN}$ where $A_{AN}(\u00b7, i)$ is maximized.\nIn this way, each atom in $D_{AN}$ corresponds to a subset of ANs. Simultaneously, the voxel-wise\nencoding models link each atom in $D_{AN}$ to specific brain regions, forming a brain map that typically\nspans multiple brain networks, such as auditory, language and visual networks. We utilize a network\ncorrespondence tool (Kong et al., 2024) to automatically identify the brain networks involved in\nthese brain maps by referring to the 17 FBNs reported previously (Yeo et al., 2011). This approach\nallows us to infer the relationship between subsets of ANs and brain networks, revealing how these\nsubsets and corresponding temporal patterns align with the brain's functional architecture."}, {"title": "IMPLEMENTATION DETAILS", "content": "We use the \"Narratives\" fMRI dataset (Nastase et al., 2021) in this study. The fMRI data were\nacquired when human subjects listened to 27 spoken stories and released with various pre-processed\nversions. We use the AFNI-nosmooth version of one fMRI session, the \u201cShapes\", due to the high\nspatial resolution (2 \u00d7 2 \u00d7 2mm\u00b3), adequate number of subjects (59 subjects) and the integrity of\nthe narrative stimuli. The fMRI volumes before the onset and after the end of the story are dis-carded. The time courses of each voxel is normalized to have unit norm. For the LLMs, we use the\npre-trained BERT\u00b2 and Llama family\u00b3 (Llama1-7B, Llama2-7B and Llama3-8B). In the sparse rep-resentation of ANs' temporal responses, the dictionary size (k) is set as 64, and the sparsity constraint\nparameter $\\lambda_{AN}$ is set as 0.15 for all the LLMs. The $\\lambda_f$=0.08 is used in the sparse reconstruction of\nfMRI activity."}, {"title": "RESULTS", "content": ""}, {"title": "SPARSE REPRESENTATION OF ANS AND BRAIN ACTIVITY", "content": "The temporal responses of ANs can be effectively represented by the dictionary $D_{AN}$, as evidenced\nby the high R\u00b2 values shown in Figure 2(a). Among the Llama family, the R\u00b2s values are com-parable, with measurements of 0.5021\u00b10.1119, 0.5005\u00b10.1114 and 0.5032\u00b10.1139, respectively.\nThe BERT model demonstrates relatively higher R\u00b2 values (0.6005\u00b10.1127) compared to the Llama\nfamily. This discrepancy may be attributed to the significantly smaller number of ANs in BERT"}, {"title": "BRAIN MAP ANALYSIS", "content": "The brain maps reveal intricate functional interactions and competitions among well-established\nFBNs. Figure 3 shows two exemplar brain maps corresponding to atom 9 and atom 17 in Llama3\n(Figure 3a), along with the automatic brain network labelling of atom 9 which exhibits the concurrent\nactivation of the language, salienceA and salienceB networks, and the deactivation of the lateral\nvisual (LatVis) cortex (Figure 3b). A comprehensive visualization for all the 64 brain maps across\nthe four LLMs is provided in A.1.\nWe observed notable variability in the involvement of FBNs in brain maps across different FBNs\n(Figure 4a). A subset of FBNs, including the LatVis cortex, language network, default mode network\n(DMN), working memory (WM) network, primary auditory cortex, salience network, fronto-parietal\nnetwork (FPN) and dorsal attention network (DAN), are more frequently engaged in brain maps,\nwith both positive (activation) and negative (deactivation) involvement. On the contrary, the meidal\nvisual (MedVis) cortex, parietal memory (ParMemory) cortex, sensorymotor network (SMN), Lim-bicA and LimbicB are less frequently involved. Notably, the patterns of FBN engagement in brain\nmaps are consistent across the four models.\nIn line with previous findings on neural language processing, our results highlight the engagement of\nfunctionally specialized brain regions/networks including the primary auditory cortex, visual cortex,\nlanguage and FPN (Friederici, 2011; Caucheteux & King, 2022; Schrimpf et al., 2021). More im-portantly, our results further underscore the importance of domain-general brain regions/networks in\nthis process, particularly the DMN, WM network and DAN. These findings are consistent with pre-vious neuroimaging studies using dynamic naturalistic stimuli (e.g., auditory stories and movies),\nwhich suggest that the DMN plays a key role in integrating incoming extrinsic information, tem-porarily stored in the WM, with prior intrinsic information over relatively long timescales to form\ncontext dependent models (Yeshurun et al., 2021).\nThe brain maps also exhibit relatively complex functional interactions among FBNs. Specifically,\nmost brain maps involve the concurrent activation or deactivation of multiple FBNs, as illustrated\nby the distribution of the brain maps associated with different number of FNBs (Figure 4b). In this\ncontext, our experimental results highlight the cooperative interaction of FBNs in neural language\nprocessing (Horwitz & Braun, 2004; Schoffelen et al., 2017; Fedorenko et al., 2024)."}, {"title": "EVOLUTION OF BRAIN-LIKE FUNCTIONAL ORGANIZATION WITHIN LLMS", "content": "We present a detailed analysis of the FBN components involved in brain maps across the four models\nin Figure 5, with the aim of exploring the evolution of brain-like organization patterns within these\nmodels. The color-coding in Figure 5 represents the Dice coefficient obtained from FBN labelling\n(Kong et al., 2024), quantifying the spatial overlap between brain maps and FBNs. Positive and\nnegative values denote activation and deactivation of FBNs, respectively. The y-axis is the FBN\nindex, reordered in descending order according to the frequency of FBN involvement in brain maps\n(both activation and deactivation), with the actual reordered indices provided at the bottom of each\nsub-figure. The x-axis is the brain map index, organized according to the presence order of FBNs\n(activation first, followed by deactivation). One noteworthy observation is that a greater number of\nbrain maps with identical FBN labels appears in more advanced LLMs, as highlighted by the braces\nin Figure 5. In addition, the overall distribution of FBN involvement is noticeably sparser in Llama3\ncompared to other models. This observation suggests that more advanced LLMs may promote\nmore compact brain-like functional organizations. One possible explanation for this observation\nis that more advanced LLMs tend to learn more compact representational policies and integrate\nthese policies more efficiently to achieve improved performance on language tasks.\nIt is hypothesized that brain maps with identical FBN labels share similar functional interactions\namong the associated FBNs. To test this hypothesis, we focused on a subset of brain maps displaying\nfunctional interactions between the LatVis (activation) and the language network (deactivation), a\npattern consistently observed across the four LLMs (Figure 5, braces with stars). For each LLMs\n(Figure 6a-d), we show one exemplar brain map (Figure 6, first column) from this subset (subset size:\n5/4/6/3 for BERT/Llama1/Llama2/Llama3, respectively), and evaluate the temporal consistency of\nthe subset by calculating the inter-atom Pearson correlation coefficients (Figure 6, second column)\nof their temporal responses (columns in $D_{AN}$). We also illustrate the distribution of number of ANs\non LLM layers (Figure 6, third column).\nOur results show that the variability of temporal correlation coefficients decrease sequentially in\nBERT, Llamal, Llama2 and Llama3, as evidenced by the standard deviations (0.2141, 0.1765,\n0.1603 and 0.0233 for the four models, respectively). The high variability in BERT, Llamal and\nLlama2 indicates that the subset of atoms in these models exhibit distinct functional processing\npatterns, despite involving identical FBNs. Meanwhile, the subset of atoms in Llama3 shows the\nhighest temporal consistency (0.236, Figure 6e) compared to other models. Notably, the moder-ate value of temporal consistency in Llama3 implies a coexistence of both shared and distinctive\nfunctional processing patterns among those atoms. These findings provide novel evidence for the\nprinciple of functional organization in LLMs: the ANs in more advanced models are organized to\nachieve an enhanced balance between the diversity of computational behaviors and the consistency\nof functional specializations.\nTo further investigate the properties of those atoms within LLMs, we identified the ANs that anchor\nto a each specific atom and evaluated the consistency of their distribution pattern on LLM layers\nby calculating the average Pearson correlation coefficient over all possible atom pairs. Our results\n(Figure 6f) show that the AN distribution patterns are more consistent in Llama3 compared to BERT,\nLlamal and Llama2, suggesting a more hierarchical organization of ANs within Llama3. Intrigu-ingly, we observed a greater concentration of ANs in the deeper layers of Llama3. Given that this\nsubset of atoms reveals the activation of LatVis and deactivation of the language network, this find-ing resonates with neuroscience evidence suggesting that visual imagery is represented at a higher\nlevel of the language hierarchy (Speed et al., 2024; Zwaan, 2003; Bergen et al., 2007), highlighting\nthe potential for Llama3 to capture complex linguistic and cognitive processes."}, {"title": "CONCLUSION AND DISCUSSION", "content": "In this study, we explored the brain-like functional organization within LLMs. We built a neural\nencoding model that uses the representative patterns learned from the temporal responses of AN\npopulations defined in LLMs as fixed regressors to predict functional brain activity. These repre-sentative patterns serves as a bridge between AN sub-groups to functional brain networks, enabling\nus to disentangle how individual ANs within LLMs are functionally organized to support their un-precedented capabilities in language tasks. The proposed framework addresses a key limitations\nin previous research that examined the behaviors of artificial neurons at a population level, which\nhas hindered a clear understanding of the functional organization within LLMs. Our experimental\nresults demonstrate that the brain-like functional organization within LLMs evolves with their capa-bilities, where more advanced LLMs achieve an improved balance between diverse computational\nbehaviors and consistent functional specializations.\nThe present study acknowledges several limitations. First, we fixed the number of atoms (dictionary\nsize) in the dictionary which describes the representative patterns of temporal responses of ANs, de-spite the fact that the number of ANs varies across different LLMs. Identifying model-specific dic-tionary size may facilitate a more accurate depiction of the brain-like functional organization within\nLLMs. Second, our assessment of the coupling relationships between AN sub-groups and functional\nbrain networks was conducted for only a limited number of atoms. However, these coupling rela-tionships described by the remaining atoms could carry valuable clues to investigate neural language\nprocessing in the human brain. For example, Figure 7(a-b) illustrate two atoms in Llama3 exhibiting\nopposite brain activity patterns in the language network, FPN, and LatVis. In accordance, the cor-responding distributions of ANs on layers display inverse patterns. On the contrary, Figure 7(c-d)\nshow two atoms demonstrating opposite brain activity patterns in LatVis, MedVis and ParMemory,\nwhile the corresponding distributions of ANs on layers remain similar. Thus, future research could\naim to further elucidate the brain-like functional organization within LLMs and the neural mech-anism underlying language processing by linking brain activity patterns with ANs' computational\nbehaviors, specifically their selective responses to external stimuli. Third, our experiments were\nlimited to one fMRI session, validating and evaluating this framework on a larger scale fMRI cohort\nis essential for future studies. Finally, applying the proposed framework to the foundation models in\nother modalities could provide additional evidence regarding the brain-like functional organization\nin modern artificial general intelligence models."}, {"title": "APPENDIX", "content": ""}, {"title": "THE BRAIN MAPS AND DISTRIBUTION OF ANS ON LAYERS", "content": ""}]}