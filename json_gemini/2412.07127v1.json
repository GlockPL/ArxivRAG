{"title": "Deep Learning-Enhanced Preconditioning for Efficient Conjugate Gradient\nSolvers in Large-Scale PDE Systems", "authors": ["Rui Li", "Song Wang", "Chen Wang"], "abstract": "Preconditioning techniques are crucial for enhancing the ef-\nficiency of solving large-scale linear equation systems that\narise from partial differential equation (PDE) discretization.\nThese techniques, such as Incomplete Cholesky factorization\n(IC) and data-driven neural network methods, accelerate the\nconvergence of iterative solvers like Conjugate Gradient (CG)\nby approximating the original matrices. This paper introduces\na novel approach that integrates Graph Neural Network\n(GNN) with traditional IC, addressing the shortcomings of\ndirect generation methods based on GNN and achieving sig-\nnificant improvements in computational efficiency and scala-\nbility. Experimental results demonstrate an average reduction\nin iteration counts by 24.8% compared to IC and a two-order-\nof-magnitude increase in training scale compared to previous\nmethods. A three-dimensional static structural analysis utiliz-\ning finite element methods was validated on training sparse\nmatrices of up to 5 million dimensions and inference scales\nof up to 10 million. Furthermore, the approach demonstrates\nrobust generalization capabilities across scales, facilitating\nthe effective acceleration of CG solvers for large-scale linear\nequations using small-scale data on modest hardware. The\nmethod's robustness and scalability make it a practical solu-\ntion for computational science.", "sections": [{"title": "Introduction", "content": "The efficient solution of large-scale systems of linear equa-\ntions arising from the discretization of partial differential\nequations (PDE) remains a central challenge in computa-\ntional science (Johnson 2009; Thomas 2013). These prob-\nlems are typically expressed as $Ax=b$ where $A$ is a sparse\nmatrix ($A\\in R^{n \\times n}$) and $x$ and $b \\in R^n$ represent the unknown\nsolution vector and the right-hand side vector, respectively.\nDue to the sparse nature of $A$, iterative methods are preferred\nas they only need to handle non-zero elements, thereby sav-\ning computational resources and storage space by gradually\napproximating the true solution from an initial guess (Saad\n2003; Golub and Van Loan 2013). In contrast, direct meth-\nods process all elements, including zeros, leading to unnec-\nessary computational and storage overhead.\nIn many PDE problems, such as equilibrium equations of\nelastodynamics, Poisson equations, Laplace equations, and\ndiffusion equations, the resulting matrices are typically sym-"}, {"title": "Related Work", "content": "Preconditioning techniques are essential for accelerating it-\nerative solvers used for large linear systems. In numerical\ncomputing libraries supporting scientific and engineering\ntasks (Balay et al. 1997; Balay et al. 2019), various precon-\nditioning algorithms are tailored to the characteristics of lin-\near systems, such as size, sparsity, definiteness, and sym-\nmetry. Common methods include splitting-based methods\n(Jacobi, or Gauss-Seidel), factorization-based methods (In-\ncomplete LU, or IC), and Multigrid methods (Nocedal and\nWright 1999; Trottenberg et al. 2000; Khare and Rajaratnam\n2012).\nTransforming the original system into a preconditioned\none incurs both the time cost of generating the precondi-\ntioner and additional computational overhead. Thus, select-\ning a preconditioner requires balancing the improvement in\nconvergence of the original problem with the efficiency of\nthe solver. Jacobi and Gauss-Seidel preconditioners are\ncost-effective but offer limited performance improvement,\ntypically converging well only for matrices with a dominant\ndiagonal. For matrices with poor condition numbers, more\nexpensive methods such as incomplete factorizations and\nMultigrid methods are necessary. Incomplete factorizations\n(Khare and Rajaratnam 2012; Golub and Van Loan 2013),\nlike ILU for general sparse matrices and IC for symmetric\npositive definite matrices, improve the matrix condition\nnumber while maintaining a sparse structure. Multigrid\nmethods, being the most computationally intensive, offer\nthe best convergence improvement but have practical limi-\ntations (Trottenberg et al. 2000). Geometric Multigrid re-\nquires structured grid information and is effective mainly for\nelliptic PDEs like the Poisson equation. Algebraic Multigrid\n(AMG), dependent on matrix algebraic properties, may ex-\nhibit inconsistent performance and complex implementation.\nDeep learning methods for predicting and adjusting pre-\nconditioners are gaining attention for their adaptability com-\npared to traditional methods (Ackmann et al. 2020; Azulay\nand Treister 2022; Zou et al. 2023). These approaches use\ntraining data for design and optimization but are still in the\nearly stages and lack extensive datasets and benchmarks\nfound in fields like computer vision or natural language pro-\ncessing. Current research focuses on specific PDE or solver.\nFor sparse matrices, using CNN to generate preconditioners\nor parameters is promising, but handling very large matrices\nis challenging due to hardware limitations (G\u00f6tz and Anzt\n2018; Yamada et al. 2018; Sappl et al. 2019; Cali et al. 2023).\nGiven that over 99.99% of matrix elements are zero, graph\nrepresentations offer a viable solution. In this context, GNN,\nwhich maintain permutation invariance, are considered an\nideal method for processing sparse matrices. Tang et al.\n(2022) utilized Graph Convolutional Network and Graph\nPooling to extract embeddings and recommend precondi-\ntioners for iterative algorithms, enabling automatic selection\nbased on matrix characteristics and reducing expert involve-\nment. Additionally, some researchers used Message-Passing\nGNN updating its edges to approximate matrix, achieving\npreconditioners comparable to IC. The minimal model pa-\nrameters allow for rapid inference during the generation\nphase, positively impacting solver acceleration. And these\nstudies explored issues related to enhancing generalization\nthrough the inclusion of solution vector $x$ in training (Li et\nal. 2023) or improving training scale and efficiency through\nloss function optimization (H\u00e4usner et al. 2023). These find-\nings provide valuable insights for future work. Some reports\ninvestigated using GNN to predict the prolongation matrix"}, {"title": "Proposed Method", "content": "Learning Definition\nInspired by the IC approach, neural network methods pre-\ndict preconditioners by establishing a mapping between the\nsparse matrix $A$ and the approximate lower triangular matrix\n$L$. Both $L$ and the lower triangular portions of $A$ share the\nsame sparsity pattern. The method can be mathematically\nexpressed as follows:\n$L_{pred} = GNN(A)$ (1)\nExperimentally, it was observed that the model output $L$\nclosely approximates the result given by IC, even without\nsupervised IC information during training. This spontaneous\nimitation of IC by the model, a phenomenon previously un-\nexplored in the literature, will be analyzed in detail in sub-\nsequent sections. Based on this observation, we propose an\ninnovative approach that shifts the model's focus from di-\nrectly predicting $L$ to enhancing IC. This idea can be ex-\npressed as:\n$L_{pred} = L_{IC} +GNN(A)$ (2)\nwhere $L_{IC}$ is the result of IC, and the GNN output acts as a\ncorrection term. Previous studies have shown that GNN can-\nnot outperform IC when directly predicting $L$ (H\u00e4usner et al.\n2023; Li et al. 2023). However, our approach intuitively\ndemonstrates that neural networks hold significant potential\nfor enhancing IC, thereby overcoming the performance bot-\ntleneck.\nModel Architecture\nGraph structures aptly represent adjacency matrices, partic-\nularly suited for sparse matrices (Khare and Rajaratnam\n2012; Moore et al. 2023). We use GNN to predict the cor-\nrection values for IC, leveraging a message-passing archi-\ntecture to update graph feature representations by aggregat-\ning neighbor information (Battaglia et al. 2018).\nAs show in Figure 1, the inputs to our GNN model include\nthe node features $x$ and edge features $e$ of the sparse matrix.\nThe model updates $x$ and $e$ through several message-passing\nsteps, ultimately outputting the edge feature $e$. Since the ma-\ntrix $A$ is symmetric, the model's input indices only consider\nthe lower triangular region, reducing the number of input"}, {"title": "Loss Function", "content": "To approximate the original matrix $A$, we train the mapping\nmodel using the reduction of the Frobenius norm distance\nbetween matrices:\n$\\min_\\theta ||L_{pred}L_{pred}^T - A||_F$\n(3)\nTo improve training efficiency and optimize the matrix-\nto-matrix multiplication in the loss function, the objective\nfunction uses an approximation of the 2-norm distance, the\nHutchinson trace estimator (Avron and Toledo 2011):\n$||LL^T - A|| = Trace((LL^T \u2013 A)^T (LL^T \u2013 A))$\n$\u2248 E_z[z^T (LL^T \u2013 A)^T (LL^T \u2013 A)z_i]$\n$= E_z [||(LL^T - A)z||_2^2] = \\frac{1}{m} \\sum_{i=0}^{m} ||(LL^T - A)z_i||_2^2$ (4)\nwhere $z_i$ are i.i.d. Rademacher random variables.\nThis transforms the loss computation into matrix-vector\nmultiplication, significantly reducing training time. H\u00e4usner\net al. (2023) reported a reduction in computational overhead\nby approximately an order of magnitude. Actually, the GNN\nmodel operations involve no algebraic operations on the ma-\ntrix $A$ (Fey and Lenssen 2019). Instead, sparse matrices are\ninput as COO-represented indices and values, and the model\nweights operate directly on these indices and values to up-\ndate the nodes $x$ and non-zero entries $e$ of $A$. The model out-\nput, likewise, consists of the indices and values of the L-ma-\ntrix. Thus, in calculating the loss and implementing it's au-\ntomatic back-propagation, the output indices and values re-\nquire an additional conversion to perform the multiplication\nof sparse matrices and vectors using the sparse format (CSR)\nbuilt into the training computational library. However, this\nprocess of sparse matrix conversion and matrix computation\nintroduces additional time and space overheads.\nTo mitigate this overhead, we propose an improved\nHutchinson trace loss computation method that performs\nsparse matrix-vector multiplication using only simple oper-\nations on indices and values (and m is set to 1):\n$Loss = ||LL^T z - Az||_2^2 = ||L_{coo}L_{coo}^T z - A_{coo}z||_2^2$ (5)\nwhere the COO subscript denotes the sparse matrix in terms\nof indices and values.\nThe new computational procedure for a single matrix-\nvector multiplication, as well as two consecutive multiplica-\ntions, is as follows:"}, {"title": "Experiments", "content": "Datasets\nThe linear equation system examined in this paper pertains\nto the static structural analysis PDE. The sparse matrix gen-\neration process encompasses 3D structural modeling, load\nconfiguration, and finite element discretization (FE). De-\ntailed generation procedures and source code are available\nat https://github.com/zurutech/stand (Grementieri and\nFinelli 2022). Unlike previous studies where sparse matrices\nfrom Poisson, heat, and wave equations exhibit more uni-\nform numerical characteristics, structural analysis presents\ncomplexities. These arise from varied material properties,\ngeometries, boundary conditions, and loading modes, caus-\ning significant variations in the values of sparse matrices.\nThe numerical values in this study span a wide range (-\n1\u00d7109 ~ 1\u00d7107), complicating neural network learning. We\nselected discrete grid's degrees of freedom (DoFs) of 10k,\n100k, 1M, and 10M. The number of NNZ typically scales\nlinearly with the sparse matrix dimension: NNZ \u2248 k \u00d7 DoFs.\nThe 3D FE-method used, with a factor k\u2248 11.5, yields cor-\nresponding NNZ of 118k, 1.17M, 12M, and 121M, respec-\ntively.\nBaseline Methods\nWe compare several traditional and learning-based precon-\nditioner methods:\nJacobi: Uses the reciprocal of the diagonal elements as\nthe inverse approximation of the original matrix.\nIncomplete Cholesky Preconditioner: Employs no fill-ins\n(IC(0)), utilizing the highly efficient C++ implementation of\nILU++ with Python bindings (Mayer 2007).\nLearning-based Preconditioner: Similar to the NeuralIF\napproach (H\u00e4usner et al. 2023), this model infers the matrix"}, {"title": "Results", "content": "Training Scale and Time\nAs detailed in Algorithms 1 and 2, the estimation of\nHutchinson trace loss can be efficiently executed through\nseveral operations involving indices and values. These op-\nerations include index selections, element-wise multiplica-\ntions, and accumulation, which eliminate the need for matrix"}, {"title": "Generalization", "content": "Sparse matrix generated by the discretization of PDE tend\nto be highly structured and pattern repetitive (e.g., similar\nsparse patterns of submatrices, condition numbers, feature\ndistributions), often exhibiting self-similarity at different\nscales (Bertaccini and Durastante 2018). As a result, neural\nnetwork can capture these local features independently of\nmatrix size and can generalize to unseen larger scale data.\nTo further explore the cross-scale generalization of the\ndata-driven preconditioning, we selected the model with\nNNZ = 1.17 M for testing across all scales. From the results\n(shown in Figure 4), we observe that: 1. At scales smaller\nthan the training model, both methods perform as expected.\n2. At scales larger than the training model, both methods us-\ning small-sized data become progressively less effective as\nthe scale increases, compared to models trained at the same\nscale. But the difference is, even after increasing the scale\nby two orders of magnitude (up to NNZ = 121 M), our\nmethod still requires fewer iterations than IC, whereas NIC\nbecomes unsuitable at higher scales."}, {"title": "Discussion", "content": "Previous studies, along with the results presented in this pa-\nper, indicate that neural network directly predicting approx-\nimate preconditioners (e.g., NIC) do not consistently outper-\nform traditional IC methods in terms of performance\n(H\u00e4usner et al. 2023; Li et al. 2023). However, our proposed\nmethod demonstrates significant improvements over IC. To\ninvestigate the underlying reasons, we conducted a detailed\nanalysis by comparing the discrepancies between NIC, as\nwell as our method, and IC on the diagonal and off-diagonal\nelements of the matrices."}, {"title": "Conclusions", "content": "This paper presents an innovative approach for enhancing\npreconditioners using deep learning methods, particularly\nby focusing on improving the Incomplete Cholesky (IC)\npreconditioner. Our method introduces a Graph Neural Net-\nwork that acts as a correction term to the IC results, signifi-\ncantly reducing the number of iterations and overall compu-\ntational time required for solving large sparse linear systems.\nOur experimental results demonstrate that the proposed\nmethod consistently outperforms traditional and other learn-\ning-based preconditioners across various matrix sizes. This\nperformance improvement is attributed to its ability to better\ncapture and enhance the off-diagonal elements of the matrix,\nwhich are critical for maintaining the overall structure and\naccuracy of the preconditioner. Furthermore, our method\nshows superior generalization ability, maintaining effective-\nness even when applied to matrices significantly larger than\nthose used during training. This capability is particularly\nvaluable in practical applications, as it enables the effective\nacceleration of Conjugate Gradient solvers for large-scale\nlinear systems using low-cost hardware and small-scale data."}, {"title": "Supplementary Materials", "content": "The Supplementary section includes the following:\n\u2022\tPDE Discretization and Sparse Matrices\n\u2022\tPreconditioned Conjugate Gradient\n\u2022\tGraph Neural Network\n\u2022\tNode Features\n\u2022\tAdditional Results\n1. PDE Discretization and Sparse Matrices\nIn complex systems and large-scale simulations, PDE often\ncannot be solved analytically. Numerical methods, such as\nfinite difference or finite element, are employed for discreti-\nzation. This process divides the continuous spatial and tem-\nporal domains into discrete cells or grid points. Each grid\npoint's value is influenced by its neighbors, forming a large\nsystem of linear equations, typically expressed as $Ax=b$,\nwhere $A\u2208R^{n\u00d7n}$, with n representing the degrees of freedom\n(DoFs) post-discretization, and x and b \u2208 Rn being the so-\nlution vector and the known terms, respectively.\nDue to interactions limited to neighboring points, most el-\nements of matrix A are zero, making it sparse. The number\nof non-zero elements (NNZ) per row depends on the prob-\nlem\u2019s dimensionality and the discretization method. For in-\nstance, a two-dimensional five-point or a three-dimensional\nseven-point discretization has an NNZ per row of 5 or 7\nrespectively. In finite element methods, commonly using tri-\nangles or quadrilaterals in 2D and tetrahedra or hexahedra\nin 3D, NNZ per row is relatively fixed, typically ranging\nfrom a few to tens. Given k as the average NNZ per row, the\ntotal NNZ in A approximates k \u00d7 DoFs. For discrete prob-\nlems with hundreds of thousands of DoFs, the matrix\u2019s spar-\nsity exceeds 99.99%, with NNZ to total elements DoFs\u00b2 ra-\ntio being about 1-k/DoFs.\n2. Preconditioned Conjugate Gradient\nFor large-scale linear equations with millions of unknowns,\nthe Conjugate Gradient (CG) method efficiently handles\nsymmetric positive definite sparse matrices. Each iteration\nsearches the solution vector\u2019s update direction along the pre-\nvious step\u2019s residuals\u2019 conjugate direction until achieving\nthe desired accuracy. The method ensures convergence in no\nmore than n steps.\n$||x - x_i|| \\leq 2||x - x_0|| \\frac{\\sqrt{\\kappa(A)}-1}{\\sqrt{\\kappa(A)} +1}^i$ (6)\nConvergence performance hinges on the sparse matrix A\u2019s\nspectral properties. After i iterations, the upper bound error\nis determined by the condition number $\\sqrt{\\kappa(A)}$ (see Eq.1),\nthe ratio of the maximum to the minimum eigenvalue.\nLower condition numbers yield fewer required iterations.\nEffective preconditioning techniques enhance $\\sqrt{\\kappa(A)}$, ex-"}, {"title": "3. Graph Neural Network", "content": "Graph structures aptly represent adjacency matrices, partic-\nularly suited for sparse matrices. Established graph algo-\nrithms are integral to matrix analysis and linear algebra.\nGraph Neural Networks present substantial opportunities for\nsolving numerical problems with data-driven approaches.\nSpecifically, GNN models operate on non-zero elements,\nminimizing computational resources and memory needs,\nthus managing large-scale matrices effectively.\nA sparse matrix is representable as an undirected graph\nGraph = (V, E), where V denotes the discrete mesh nodes,\nand E \u2286 V \u00d7 V represents node connections as edges with\nnon-zero elements. Nodes v\u2208V have eigenvectors\nx, \u2208 R\u207f, and edges eij connecting nodes i and j possess ei-\ngenvectors zij \u2208 R\u1d50.\nMessage-passing, a core concept in GNN like Graph Con-\nvolutional Networks, involves nodes updating their repre-\nsentation by aggregating neighbor information. A GNN\nmodel usually stacks multiple message-passing layers, and\nthe process is iterative: the features of the l+1-th layer are\nupdated by the features of the l-th layer. A single message\npassing usually consists of two main steps:\nMessage Aggregation: each node collects and aggregates\nmessages from its neighbors, for example:\n$a_i^{(l+1)} = \\phi_a (x_i^{(l)}, m_i^{(l)})$ (7)\n$m_i^{(l+1)} = \\sum_{j\u2208N(i)} \\phi_m (x_j^{(l)}, z_{ji}^{(l)})$ (8)\nwhere \u03c6a is a parameterized function that is updated by back-\npropagation of the objective loss. Then, all the neighbors\nj\u2208 N(i) of node i complete the aggregation of the mes-\nsages by one or more permutation-invariant operations.\nCommonly used aggregation functions include sum, mean,\nmax and min.\nUpdate: Each node updates its own representation based\non the aggregated messages and possible previous represen-\ntations, for example:\n$x_i^{(l+1)} = \\phi_e (x_i^{(l)}, m_i^{(l+1)})$ (9)\nwhere \u03c6e is also a learnable parameterized function.\nLocalized processing enables message-passing-based\nGNN to scale across graph sizes. Despite large graph sizes,\nrecursive and iterative message passing gradually extends\nmessages to distant nodes through multiple layers, with each\nlayer processing one-hop neighbor messages, maintaining\ncomputational complexity manageable."}, {"title": "4. Node Features", "content": "The edge features of the graph are derived from the non-\nzero elements of the sparse matrix, e \u2208 R\u00b9. The node fea-\ntures encompass the local degree profile (5 dimensions),\nmatrix diagonal dominance (2 dimensions), and node posi-\ntions (2 dimensions), represented as x \u2208 R\u2077. For node posi-\ntions, we adopt the Transformer Position Embedding\nmethod, utilizing only a set of sine and cosine functions in\nthis study. Detailed descriptions of node features are list in\nTable 1."}, {"title": "5. Additional Results", "content": "Fill-in Dropout\nThe complexity of PCG algorithm is O(NNZ). A natural ap-\nproach to reducing the computation time associated with the\npreconditioning transformed residual is to decrease the\nnumber of non-zero elements in the preconditioner. One\ncommon technique for this is Fill-in Dropout, where certain\nnon-zero elements in the generated preconditioner are selec-\ntively set to zero according to a predefined criterion. In this\nstudy, we employed the method described in Equation (5) to\ndiscard non-zero elements in L whose absolute values are\nvery small, thereby reducing the number of elements in-\nvolved in the computation. This method has been positively\nvalidated in previous research (H\u00e4usner et al. 2023).\n$L_{pred} = GNN(A)+ L_{IC}$\n$\\tilde L_{pred}$ = { 0, if abs(Lpred) \\leq \u03b5   (10)\nLpred, if abs(Lpred) > \u03b5\nAs an example, for a sparse matrix with DoFs equal to 1\nmillion, we analyzed not only the computation time at each\nstage (P-time and CG-time) but also the time spent on trian-\ngular solves during each iteration, as shown in Table 2. The\nresults indicate that: 1) Eliminating some very small ele-\nments does not significantly deteriorate convergence. For\nexample, even with a 25.5% reduction in non-zero elements\ncompared to the original matrix, the number of iterations"}, {"title": "5. Additional Results", "content": "Fill-in Dropout\nThe complexity of PCG algorithm is O(NNZ). A natural approach to reducing the computation time associated with the preconditioning transformed residual is to decrease the number of non-zero elements in the preconditioner. One common technique for this is Fill-in Dropout, where certain non-zero elements in the generated preconditioner are selectively set to zero according to a predefined criterion. In this study, we employed the method described in Equation (5) to discard non-zero elements in L whose absolute values are very small, thereby reducing the number of elements involved in the computation. This method has been positively validated in previous research (H\u00e4usner et al. 2023).\n$L_{pred} = GNN(A)+ L_{IC}$\n$\\tilde L_{pred}$ = { 0, if abs(Lpred) \\leq \u03b5   (10)\nLpred, if abs(Lpred) > \u03b5\nAs an example, for a sparse matrix with DoFs equal to 1 million, we analyzed not only the computation time at each stage (P-time and CG-time) but also the time spent on triangular solves during each iteration, as shown in Table 2. The results indicate that: 1) Eliminating some very small elements does not significantly deteriorate convergence. For example, even with a 25.5% reduction in non-zero elements compared to the original matrix, the number of iterations only increased by 3.2%. 2) As the number of non-zero elements decreases, the overall computation time of the PCG gradually increases. This is primarily due to the increased time required for the triangular solves.\nTriangular solving for sparse matrices is inherently non-parallelizable, as each element of the vector x must be computed sequentially, row by row, as shown in Equation (6). During actual computation, the processor (such as CPU) frequently accesses data from the non-zero elements a and the vector x in a random manner. While the CRS (Compressed Row Storage) format used for storing the sparse matrix a ensures that elements are stored in row index order, providing good cache locality and high access efficiency, the large-scale vector x may have access locations that are far apart, leading to low cache hit rates and poor data access efficiency. The sparser the matrix, the more scattered the access locations for x, resulting in higher cache miss frequencies and increased time for the processor to wait for data to be loaded from main memory, thus reducing overall computational efficiency. Consequently, for large-scale sparse matrices, algorithms involving triangular solves, such as IC-PCG, must consider not only computational complexity but also the issue of access inefficiency.\n$x_i = \\frac{1}{a_{ii}}(b_i - \\sum_{j\\neq i} a_{ij} * x_j)$\n$x_i = \\frac{1}{a_{ii}}[b_i - (a_{i,}*x)], for i to N$ (11)\nThe additional computations introduced by the preconditioner, such as the triangular solves in IC-PCG, constitute a major portion of the time spent in each iteration. The experiments above demonstrate that allowing the preconditioner to approximate the original matrix with a higher sparsity can accelerate PCG convergence. Therefore, improving computational access efficiency and reducing the additional computation time introduced by the preconditioner will be a key focus of future research in this work. For instance, storing x in CRS format as well, although it incurs additional space overhead, or designing the preconditioner structure or post-processing to have a denser layout distribution near the diagonal could be potential approaches to address this issue."}]}