{"title": "Cross-Camera Distracted Driver Classification\nthrough Feature Disentanglement and Contrastive\nLearning", "authors": ["Simone Bianco", "Luigi Celona", "Paolo Napoletano"], "abstract": "The classification of distracted drivers is pivotal\nfor ensuring safe driving. Previous studies demonstrated the\neffectiveness of neural networks in automatically predicting\ndriver distraction, fatigue, and potential hazards. However, recent\nresearch has uncovered a significant loss of accuracy in these\nmodels when applied to samples acquired under conditions that\ndiffer from the training data. In this paper, we introduce a\nrobust model designed to withstand changes in camera position\nwithin the vehicle. Our Driver Behavior Monitoring Network\n(DBMNet) relies on a lightweight backbone and integrates a\ndisentanglement module to discard camera view information\nfrom features, coupled with contrastive learning to enhance the\nencoding of various driver actions. Experiments conducted on\nthe daytime and nighttime subsets of the 100-Driver dataset validate\nthe effectiveness of our approach with an increment on average of\n9% in Top-1 accuracy in comparison with the state of the art. In\naddition, cross-dataset and cross-camera experiments conducted\non three benchmark datasets, namely AUCDD-V1, EZZ2021 and\nSFD, demonstrate the superior generalization capability of the\nproposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "As of 2019, road traffic accidents are the leading killer\nof children and youth aged 5 to 29 years and are the 12th\nleading cause of death when all ages are considered [1].\nTraffic accidents pose a growing problem and are projected\nto rank as the seventh leading cause of global mortality by\n2030. According to statistics from the European Road Safety\nObservatory (ERSO), 5 to 25% of traffic accidents in Europe\nstem from driver distraction. The ERSO defines distracted\ndriving as \"the diversion of attention away from activities\ncritical for safe driving toward a competing activity, which\nmay result in insufficient or no attention to activities critical\nfor safe driving\". Commonly observed distractions include\nsmartphone usage, eating, or conversing with passengers, all of\nwhich heighten the risk of accidents and should be rigorously\navoided. Detecting these distracting behaviors through auto\nmatic in-vehicle computational systems enables early inter-\nvention to avoid distraction-related accidents. In this context,\nautomatic Driver Monitoring Systems (DMS) have emerged as\na novel Advanced Driver Assistance System (ADAS) technol-\nogy to mitigate car accidents [2]. DMSs exploit machine and\ndeep learning techniques and are trained on datasets acquired\nin both real and simulated scenarios, often with varying pro-\ntocols and experimental setups [3]\u2013[6]. Significant differences\namong these setups include the interior configuration of the\nvehicle, the position and posture of the driver and the camera\nplacement. Consequently, models trained on specific datasets\nexhibit poor generalization to different data [7]. To deploy\nthese models on non-production embedded systems in various\nvehicles while maintaining performance, the models must be\nrobust to prior variations. Furthermore, they must generalize to\nnew conditions without increasing computational complexity,\nconsidering the limited capacity of the target devices.\nThis paper addresses the challenge of cross-camera dis-\ntracted driver classification by introducing the Driver Behavior\nMonitoring Network (DBMNet). This model is based on the\nuse of a lightweight Convolutional Neural Network (CNN) for\nencoding RGB images to facilitate its deployment on commer-\ncial off-the-shelf devices. To make the model robust across\ndifferent camera views, we propose a feature disentanglement\nmodule. This module separates action-relevant features from\nview-relevant features, allowing the proposed model to learn\nhigh quality features that are invariant to camera and view\nvariations. This is achieved by a weighted combination of the\nview queries and the feature vector determined by the back-\nbone. The weights are calculated on the basis of the similarity\nbetween the test sample and the training views. Moreover, we\nemploy a supervised contrastive learning approach with triplet\nloss to enhance the learning of disentangled features. While\ncross-entropy loss theoretically allows the model to discern\nsimilarities within categories and differences between them, it\ndoes not explicitly guarantee this outcome. In contrast, triplet\nloss explicitly compels the model to identify similar features\nfor samples within the same category and distinguish features\nfor samples from different categories. In our model, one triplet\nloss brings representations of different actions captured by\nthe same camera closer together while separating them from\nidentical actions captured by the same camera. This improves\nthe model's ability to discriminate between different views\nand enhances the effectiveness of the disentanglement module.\nAnother triplet loss brings representations of identical actions"}, {"title": "II. RELATED WORKS", "content": "Large-scale image recognition networks have proven ef-\nfective in recognizing distracted driving. Abouelnaga et al.\n[10] employed a genetic-weighted ensemble of four CNNs,\nfocusing on facial and hand features. Baheti et al. [11]\noptimized VGG16, reducing parameters and introducing L2\nregularization. Koay et al. [12] favored CNNs over Vision\nTransformers and proposed the OWIPA approach with pose\nestimation. Shaout et al. [13] used SqueezeNet for real-time\ndistraction detection. However, in all these works cross-dataset\nperformance was overlooked.\nSpecially designed models like MobileVGG with 2.2M\nparameters [14] advanced real-time detection. Nguyen et al.\n[15] developed a lightweight CNN with adaptive feature map\nextraction. Li et al. [16] proposed OLCMNet, an accurate\nlightweight network with an Octave-Like Convolution Mixed\n(OLCM) block. Liu et al. [17] used knowledge distillation\nand NAS, achieving high accuracy with 0.42M parameters.\nMittal et al. [18] introduced CAT-CapsNet with impressive\naccuracies. Despite the reported improvements, fewer param-\neters may limit learning capabilities, especially in cross-dataset\nscenarios.\nTo mitigate the detrimental effects of background noise on\nrecognition accuracy, Leekha et al. [19] proposed the use of\nan iterative graph-based foreground segmentation algorithm\nto separate the driver from the background, while Yang et al.\n[20] employed Gaussian Mixture Models (GMM) for body\nextraction. Qin et al. [21] introduced D-HCNN with 0.76M\nparameters, focusing on HOG features. Dey et al. [22] used\nobject detection for multiple ROIs, enhancing generalization.\nBehera et al. [23] proposed a contextual modeling approach\nthat integrates posture and semantic context. They validated\ntheir approach using a self-annotated SFD test set and provided\nthe first cross-validation results for the SFD dataset. Wang et\nal. [24] applied Fast-RCNN for DOA extraction. Keypoint-\nbased approaches, like Li et al. [25] and Bera et al. [26],\nemphasized topological information. While effective, these\nmethods demand extensive pre-processing, impacting real-time\nperformance. Duan et al. [27] proposed S-Softmax classifier\nand a dynamic 2D supervision matrix for improving cross-\ndataset performance.\nResearchers simplified training strategies for enhanced gen-\neralization. Masood et al. [28] used pretrained-weight VGG\narchitectures for reduced training time. Li et al. [29] intro-\nduced Multi-Teacher Knowledge Distillation for lightweight\nmodels. Peng et al. [30] employed Few-Shot Learning for\nfeature calibration. Contrastive and unsupervised methods, like\nHu et al. [5] and Li et al. [31], focused on improved training\nstrategies.\nCompared to previous research, this paper tackles the issue\nof distracted drivers by enhancing model robustness without\nincreasing computational demands. Notably, this is the first\nwork to conduct comprehensive experiments using the leave-\none-camera-out strategy, accurately reflecting real-world sce-\nnarios where the recognition model is trained under certain\nconditions and tested under different ones. This rigorous setup\nensures our findings are both practical and applicable to real-\nworld conditions."}, {"title": "III. THE PROPOSED DRIVER BEHAVIOR MONITORING\nNETWORK", "content": "A. Motivation\nImage analysis-based DMSs encounter a significant chal-\nlenge due to the variability in vehicle interior configurations,\nresulting in disparate camera viewpoints [4], [30]. Conven-\ntional supervised deep learning methods, often employing\ncross-entropy loss, struggle to develop robust representations\ncapable of generalizing across different views. In these varied\nperspectives, where driver actions are observed, it becomes\ncrucial to design supervised techniques that mitigate the drop\nin effectiveness when handling data acquired under conditions\nthat differ from the training set.\nMoreover, deploying these DMSs on devices with limited\ncomputational capacity necessitates the use of efficient and\nlightweight models, ensuring real-time predictions. Hence,\nrobust solutions must be sought, incorporating deep learning\nmodels that respond to previous requirements.\nB. Overview\nGiven the factors mentioned above, we present the Driver\nBehavior Monitoring Network (DBMNet), illustrated in Figure\n1. This method encodes an RGB image by leveraging the\nlightweight CNN architecture detailed in [4] as its backbone.\nThe proposed feature disentanglement module aims to pri-\noritize action encoding over camera-view information. This\nis achieved by linearly combining view queries with features"}, {"title": "C. The proposed model", "content": "a) Backbone: Our model consists of a GhostNet-v1.0\n[32] as the backbone. Given an input image $X \\in R^{C\\times H\\times W}$,\nthe backbone $f_\\theta$ outputs a feature vector $f = f_\\theta(X) \\in R^{1\\times D}$\nas output, where D is the number of features.\nb) Feature disentanglement: The output of the backbone\nis passed to the feature disentanglement module. This module\nconsists of V learnable view queries $Q \\in R^{D\\times V}$, where\nV represents the number of views for the training samples.\nThrough our learning process, we determine, for each view,\nwhich features are most relevant for characterizing the driver\nactions. The disentangled feature vector is obtained as a\nweighted combination of the view queries with the feature\nvector f, determined by the backbone. The weighting, denoted\nas w, is computed based on which view or views the test\nsample most closely resembles. In practice, estimating the\ndisentangled features is carried out as follows:\n$f = w \\times f$,\nwith\n$w = p_v Q$,\nwhere $p_v$ is the probability distribution over V views that is\nestimated by the view classifier.\nc) Classifiers: The model consists of two classifiers,\nnamely the Action classifier for categorizing the action per-\nformed by the driver and the View classifier for the estimation\nof the camera viewpoint. The Action classifier takes as input\nthe disentangled feature vector $\\hat{f}$ and outputs the probability\ndistribution over the A action classes, $p_a \\in R^{1\\times A}$, as follows:\n$p_a = (W_a\\hat{f}) + b_a,$\nwhere $W_a \\in R^{D\\times A}$ and $b_a \\in R^{1\\times A}$ are the weights and bias\nof the fully-connected head for action classification.\nInstead, the View classifier processes the feature vector f\nproduced directly from the backbone to produce the probabil-\nity distribution, $p_v$, on the V views as follows:\n$p_v = (Wf) + b_v$.\nIn the previous equations, $W \\in R^{D\\times V}$ and $b \\in R^{1xV}$\nare the weights and bias of the fully-connected layer for view\nclassification."}, {"title": "D. Training procedure", "content": "a) Training data input: During training, our model pro-\ncesses batches of image triplets with an approach similar to\nthat of Siddiqui et al. [33]. Each triplet comprises an anchor\nimage, denoted as a, which portrays a driver's action from a\nspecific viewpoint. Another image in the triplet, referred to\nas $s_v$ (same view), displays a different action captured from\nthe identical viewpoint as a. Finally, we include an image of\nthe same action as a, but captured from a distinct viewpoint,\ndenoted as $s_a$ (same action). We construct these triplets to\nprovide both positive and negative samples for our supervised\ncontrastive learning, which will be presented in the following\nsection. By supplying both positive and negative samples,\nthe model can develop a comprehensive understanding of the\ndiscriminative characteristics within each action or viewpoint\ncategory through sample comparisons. This methodology di-\nverges from the learning-by-classification approach, wherein\nthe loss function solely penalizes classification errors without\nexplicitly promoting intraclass compactness and interclass\nseparation [34].\nb) Loss function: Our DBMNet is trained through a\ncomposite loss function that is defined as follows:\n$L = L_{ace} + L_{vce} + L_{ac} + L_{vc}.$\nIn the previous loss, there are two cross-entropy losses mainly\ndedicated to the optimization of linear layers for view ($L_{vce}$)\nand action ($L_{ace}$) categorization. Cross-entropy losses are only\napplied to the anchor image in this way:\n$L_{vce} = -\\sum_{m=1}^{V} y_v^{(m)}log(p_v^{(m)}),$\n$L_{ace} = -\\sum_{m=1}^{A} y_a^{(m)}log(p_a^{(m)}).$\nwith A and V being the number of actions and the number\nof views, respectively. $p_a^{(m)}$ is the probability of the m-th\ncategory with ground-truth $y_a^{(m)}$ for the action of the anchor\nsample, while $p_v^{(m)}$ is the probability of the m-th category\nwith ground-truth $y_v^{(m)}$ for the anchor sample viewpoint.\nThe previous two cross-entropy losses are complemented\nby two opposite triplet losses. Both losses use the same triplet\nconstruction previously defined. The difference lies in the role\nassumed by $s_v$ ed $s_a$ in each loss.\nIn the action triplet loss (Eq. 8) the objective is to learn\nfeatures for action encoding that are invariant regardless of the\nviewpoint. Hence, $s_a$ serves as the positive sample, whereas\n$s_v$ serves as the negative sample. This choice has the purpose\nof bringing closer the representations of $s_a$ and a which\nhave the same action. Conversely, the distance between the\nrepresentations of a and $s_v$ that depict a different action from\nthe same point of view is instead maximized:\n$L_{ac} = \\sum_{n=1}^{N} [\\delta + D(f_a, f_{sa}) - D(f_a, f_{sv})]_+$,\nwhere $f_a$, $f_{sa}$, and $f_v$ denote the disentangled feature outputs\nfrom the a, $s_a$, and $s_v$ images, respectively. D(...) is the\npairwise Euclidean distance function, and $\\delta$ is a margin\nparameter. It is expected that the learned disentangled features\nexhibit view-invariance, as they are encouraged to be as similar\nas possible when derived from two different images depicting\nthe same action regardless of the viewpoint from which they\nwere acquired."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present the datasets used in the experi-\nments, describe the experimental setup, and discuss the results\nof our extensive experimentation.\nA. Dataset\nWe conduct extensive experiments on 100-Driver [4]. The\n100-Driver dataset comprises 470,000 images illustrating 100\ndrivers engaged in 22 distinct actions during driving sessions.\nThese actions include 21 distracted behaviors and one instance\nof normal driving (see Table I for the list of all actions).\nThe driving sessions were recorded in both daytime and\nnighttime in five different vehicles, comprising two sedans,\ntwo SUVs, and one van. Within each vehicle, four cameras\nwere strategically placed on the left front, right front, and\nright side of the drivers. This arrangement served dual pur-\nposes. Firstly, having a dataset with multiple cameras aids in\ndeveloping more robust models that can account for variations\nin camera perspectives, thus evaluating the models' general-\nization across different cameras. Secondly, the multi-camera\ndataset offers an opportunity to enhance system performance\nby considering the content captured from multiple camera\nangles. Figure 2 reports samples from the four cameras in\nboth day and night times.\nWe also perform cross-dataset evaluation on the AUCDD-\nV1 [3], EZZ2021 [8], and SFD [9] datasets. Image for these\ndatasets are annotated based on 10 different driver actions:\nSafe/Normal driving, Texting (right), Talking with cellphone\n(right), Texting (left), Talking with cellphone (left), Adjusting\nradio, Drinking, Reaching behind, Hair and makeup, Talking\nto passenger. AUCDD-V1 consists of 12,978 training images\nand 4,331 test images. We evaluate model performance only\non the test images. In the EZZ2021 dataset, 9 drivers are\ndepicted performing the previous actions. Since a data split is\nnot provided, all images are used for testing. The SFD dataset,\nused in a Kaggle competition, comprises 22,424 training\nimages and 79,726 test images. The training images are labeled\nwith 10 class labels, while the test images are not labeled.\nTherefore, only the training set is used for evaluation."}, {"title": "B. Experimental Setup", "content": "1) Evaluation: We run experiments in a cross-camera setup\nwhich is implemented by applying the leave-one-camera-out\nprotocol on the 100-Driver dataset. Given that the cameras\nwere placed at 4 different points, for each round 3 cameras\nare used for training and the other one for evaluation. Our\ntrain-val-test splits are obtained by aggregating the splits\ndefined in [4] according to our protocol. We measure method\nperformance in terms of Top-1 and Top-5 accuracy.\n2) Backbones: We select three efficient backbone models\nto assess the impact of varying parameter counts on our\nmethod including GhostNet-v1.0 [32], MobileNetV3-S [35],\nand SqueezeNet-v1.1 [36].\n3) Implementation details: Our experiments are performed\non a computer featuring an Intel i7-8700 CPU @3.20GHz and\nan NVIDIA GeForce GTX 1070 Ti. The operating system is\nUbuntu 22.04, and the framework is PyTorch 2.0.1. We utilize\nthe SGD optimizer with a momentum of 0.9 and a weight\ndecay equal to 5 \u00d7 10-4. The batch size is set to 64. The\nGhostNet-v1.0 backbone is pretrained on ImageNet. The view\nqueries are randomly initialized by sampling from a normal\ndistribution with mean equal to 0 and variance 1.\nThe learning rate is initialized to 0.01 and reduced by a\nfactor of 10 at 10 and 30 epochs. The inputs are first resized\nto 224 x 224 pixels. We then use random rotation in the range\n[-30\u00b0, 30\u00b0], random perspective, random color transformations\nin terms of contrast, saturation and hue, Gaussian blur, and\nrandom erasing [37] for data augmentation. Training is per-\nformed for a total of 50 epochs. The best-performing epoch\non the validation set in terms of Top-1 accuracy is then chosen\nfor evaluation on the test data. The margin & of triplet losses is\nempirically set to 1. Performance metrics are Top-1 and Top-5\naccuracy."}, {"title": "C. Results", "content": "1) Ablation study: In this section we present the results\ndetailing the contribution of the key components within the\nproposed model. Specifically, we compare three different effi-\ncient backbones and we assess the contribution of contrastive\nlearning, the influence of the disentanglement module, and\nfinally, the effect of their combined integration.\na) Model backbone: Table II reports the comparison of\nthe three model architectures in terms of number of parameters\nand number of floating point operations per second (FLOPS).\nThe GhostNet-v1.0 model has 5.2 million parameters and 0.16\nGFlops, which indicates a moderate computational complexity\nwith a relatively higher number of parameters compared to the\nother models. On the other hand, the MobileNetV3-S model,\ndemonstrates a significantly lower number of parameters at\n2.5 million and the lowest computational complexity with\n0.06 GFlops. Finally, the SqueezeNet-v1.1 model features the\nsmallest number of parameters at 1.2 million but has the\nhighest computational complexity among the three, with 0.35\nGFlops.\nThe results reported in Table III reveal notable performance\ndifferences among the considered backbones in the leave-one-\ncamera-out experiments. GhostNet consistently demonstrates\nsuperior performance across most daytime and nighttime sce-\nnarios. For instance, it achieves the highest Top-1 accuracy of\n62.51% and Top-5 accuracy of 93.75% when evaluating D2.\nThis suggests that GhostNet is particularly effective in han-\ndling variations across different camera configurations. Mo-\nbileNetV3 also performs well, especially during nighttime ac-\nquisitions, where it shows competitive results in some scenar-\nios. For example, it achieves a Top-1 accuracy of 33.18% for\nN1 and 47.75% for N2, indicating its robustness in lower light\nconditions. However, its overall performance remains slightly\nlower than GhostNet's across most evaluations. SqueezeNet,\non the other hand, consistently underperforms compared to\nGhostNet and MobileNetV3. It struggles particularly in more\nchallenging scenarios such as D4 and N4, where it records a\nnotably low Top-1 accuracy of 7.91% and 4.22%, respectively.\nThis highlights potential limitations of SqueezeNet in handling\ndiverse and complex data conditions.\nb) Contrastive learning contribution: This version of\nDBMNet consists of the Backbone, and the two classifiers,\nnamely the View and Action classifiers. The model is opti-\nmized by combining cross-entropy losses and triplet losses\nfor each task. The results for this version are reported in the\nfirst row of Table IV. As it is possible to see, the lack of the\nfeature disentanglement module compared to the full solution\n(the results of which are shown in the last row of the table)\nnegatively impacts the results. On average, there is a loss of\n5.42% and 4.05% on Top-1 and Top-5 accuracy, respectively.\nThe highest gap corresponding to 13.54% of Top-1 accuracy\nis registered for Camera 3.\nc) Feature disentanglement module contribution: In this\nexperiment, the architecture of DBMNet remains unchanged.\nThe difference with the complete version lies solely in the\nchoice of loss function utilized for model optimization. Specif-\nically, we incorporate only the two cross-entropy losses. The\noutcomes for this variant are depicted in the second row of\nTable IV. We emphasize that also the performance of this\nsolution is inferior to that of the full version of DBMNet.\nNotably, there is a 6.94% drop in Top-1 and 8.79% drop in\nTop-5. More significantly, these results are even lower than\nthose achieved with Contrastive learning alone. Consequently,\nit is evident that while the Feature Disentanglement module\noffers benefits to DBMNet, the use of Contrastive learning\nyields even greater advantages.\nd) Full solution analysis: DBMNet capitalizes on the ad-\nvantages offered by both the Feature disentanglement module\nand Contrastive learning, as outlined in the previous sections.\nOur focus is on another important aspect of our method in this\nsection. Given that DBMNet performs feature disentanglement\nby highlighting the features for the driver's actions based on\nthe view that is most similar to the test view, it is imperative to\nensure the effectiveness of view classification. We register an\naverage performance for the four rounds of leave-one-camera-\nout equal to 98.54% on the validation set of Driver-100. This\nmeans that the view classification is accurate and should be\nable to correctly guide the choice of the view queries.\n2) Comparison with other methods: In this section, we\ncompare the results achieved by our DBMNet with those of\nfour recent methods, namely Duan et al. [27], GhostNet [4],\nHu et al. [5], and OLCMNet [16]. It is worth noting that Duan\net al. trained a GhostNet-v1.0 by exploiting their proposed S-\nSoftmax classifier for improving model robustness. GhostNet\nis the same architecture used in our method as the backbone,\nand unlike our method, it is trained exclusively through cross-\nentropy loss optimization for driver action classification. Hu\net al. [5] trained a ResNet-18 using a compound loss, which\nincludes the proposed Clustering Supervised Contrastive Loss\n(C-SCL) for improving action representations, along with the\ncross-entropy loss used for the classification head. For a fair\ncomparison with our DBMNet, we replaced the ResNet18\narchitecture with our GhostNet-v1.0. Finally, OLCMNet is a\nlightweight network with an OLCM block to improve model\neffectiveness.\nTable V reports the results for each round of our leave-one-\ncamera-out protocol. According to the results, several consid-\nerations can be made. First, the worst result obtained by our\nmodel is for the D4 camera, Top-1 accuracy 11.29%, compared\nwith the best result obtained for D2 of 62.51% Top-1 accuracy.\nThis result was expected since the framing of the driver is\nvery different from the other three cameras (see Figure 2\nfor a better understanding). Second, the cameras are ranked\nin descending order based on their Top-1 performance as\nfollows: D2, D3, D1, and D4. This ranking serves as a valuable\nindicator of the viewpoints offering superior driver visibility,\nthereby guiding the optimal placement of cameras. Third, our\nDBMNet demonstrates superior performance compared to the\nother two methods with an average improvement of 9% across\nall four cameras. In particular, the most significant disparity\nis observed against GhostNet, where DBMNet achieves a\nremarkable 17.20% increase in Top-1 accuracy and a 16.44%\nincrease in Top-5 accuracy on camera D3. Still, on camera\nD3, DBMNet exhibits the largest performance improvement\nover Hu et al., with an improvement of 7.32% and 5.87% in\nTop-1 and Top-5 accuracy, respectively.\nFigure 3 shows the confusion matrices for each cross-\ncamera configuration. For the daytime subset we can the\nfollowing observations. Confirming that the best performance\nis obtained for D2 and D3, there is a prominent diagonal in\nthe respective confusion matrices. The confusion matrix for D1\nshows a high bias in the class \"Hair / Makeup\" (#7). Many"}, {"title": "V. CONCLUSIONS", "content": "Driver Monitoring Systems (DMSs) need to be robust to\nchanges in vehicle configuration and camera positions. In this\npaper, we introduced DBMNet, a novel network designed\nfor distracted driver classification, ensuring consistent per-\nformance regardless of camera placement. Given that DMSs\noperate on embedded devices with constrained computational\nresources, our model exploits GhostNet, a lightweight CNN,\nas the backbone. We improved the invariance to the camera\nview of the representation using a feature disentanglement\nmodule. In addition, we employed a supervised contrastive\nlearning approach to increase consistency between intra-class\nrepresentations and differentiate inter-class representations.\nExperimental results on the 100-Driver dataset demonstrated\nthe effectiveness of the proposed model with an increment on\naverage of 9% in Top-1 accuracy with respect to state-of-the-\nart methods. Through the analysis of features both before and\nafter the feature disentanglement module, we determined that\nthe objective of discarding view information is accomplished.\nSome limitations are worth noting. Given the inherent\ncomplexity of the task, the accuracy results obtained are\nmodest, indicating a need for further refinement to enhance\nthe generalization capacity of our DBMNet. This requirement\nis particularly pronounced due to the low accuracy observed\nin the \"Normal driving\" class, which often gets confused with\nother distraction classes. Additionally, the inference time on\nour computational setup stands at 1.15 \u00d7 10\u20132 seconds, which\nremains relatively high for ensuring timely detection of driver\ndistraction, hindering the implementation of effective safety\nsystems."}]}