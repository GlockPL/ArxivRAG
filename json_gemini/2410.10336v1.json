{"title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning", "authors": ["Joshua Ong Jun Leang", "Aryo Pradipta Gema", "Shay B. Cohen"], "abstract": "Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (Co-MAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). COMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, COMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks\u00b9.", "sections": [{"title": "Introduction", "content": "Complex mathematical reasoning remains a significant challenge for large language models (LLMs; Luo et al., 2024; Li et al., 2023; Meadows and Freitas, 2023). Techniques like Chain-of-Thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022) have improved LLM performance by encouraging the generation of intermediate reasoning steps. However, CoT explanations are not always faithful to the actual reasoning process of the model (Bentham et al., 2024; Turpin et al., 2024; Yee et al., 2024); with final answers that may not logically follow from the reasoning chain, suggesting that LLMs can fabricate reasoning paths (Lyu et al., 2023).\nRecent efforts to improve reasoning faithfulness have relied on integrating external solvers (Lyu et al., 2023; He-Yueya et al., 2023; Jiang et al., 2024a). However, these approaches introduce new challenges: LLMs often generate erroneous or syntactically invalid code when translating natural language into formal statements, causing solvers to fail (Wen et al., 2024).\nTo address these issues, we propose Chain of Mathematically Annotated Thought (CoMAT), a novel approach that leverages symbolic reasoning entirely within LLMs. By eliminating external solvers, COMAT avoids issues related to code generation failures, offering a more robust solution for a broad range of mathematical tasks. Notably, COMAT performs well where there are difficulties with code-based methods, such as multilingual datasets and Olympiad-level problems. As shown in Appendix A, code-based methods often struggle with Olympiad-level mathematics, while CoMAT demonstrates greater adaptability. To our knowledge, COMAT is the first method to apply symbolic reasoning across diverse mathematical reasoning tasks, including multilingual datasets, Olympiad-level problems, and common benchmarks.\nEmpirically, COMAT consistently outperforms"}, {"title": "CoMAT: Chain of Mathematically Annotated Thought", "content": "Traditional CoT methods in mathematical reasoning rely heavily on natural language explanations, which can introduce ambiguity and inconsistencies (Lu et al., 2024). These methods may lead models to fabricate reasoning paths or misinterpret variable relationships, compromising accuracy in complex tasks. CoMAT addresses these limitations by adopting a structured, symbolic approach that enforces mathematical consistency and reduces ambiguity (Tam et al., 2024). By using well-defined symbolic logic, COMAT ensures that each reasoning step adheres to sound mathematical principles, enhancing the model's ability to solve problems accurately (Zhong et al., 2024).\nCOMAT extends traditional CoT by incorporating symbolic transformations as the core component of the reasoning process. While CoT typically follows a $(Q, R, A)$ structure where $Q$ is the question, $R$ is the reasoning, and $A$ is the final answer COMAT introduces a more structured process: $(Q, S, R, A)$, where $S = (S_1, S_2, S_3, S_4)$ represents the four steps in the symbolic reasoning pipeline, designed to break down complex problems into formal, interpretable sequences of logical operations. This structured decomposition enhances transparency and allows for systematic verification of each step. Examples of manual annotation for verifiability can be found in Appendix B."}, {"title": "CoMAT's Two-Stage Process", "content": "COMAT consists of two main stages: Symbolic Conversion and Reasoning Execution. Each stage is critical to ensuring the accuracy and faithfulness of the reasoning process.\nSymbolic Conversion. In this stage, the LLM transforms a natural language query $Q$ into a symbolic representation, $S = (S_1, S_2, S_3, S_4)$. This involves identification and definition, structural logic translation, explicit factual representation, and question formalisation, all carried out by the LLM. This stage acts as the foundation for accurate reasoning by converting ambiguous natural language into well-structured mathematical logic.\nReasoning Execution. Once the problem is translated into its symbolic form $(S)$, the model applies logical reasoning to derive the solution. The logical reasoning is followed by stepwise reasoning, similar to Kojima et al. (2022) \u2013i.e., prompting the model with the phrase \"Let's think step-by-step\". By grounding the reasoning in the symbolic structure, COMAT ensures that each step aligns with mathematical logic, reducing the risk of errors or illogical steps. The final answer, $A$, is then generated based on this rigorous reasoning process."}, {"title": "Case Study", "content": "To demonstrate how COMAT works in practice, consider the following math problem related to ticket sales, based on Figure 2:\nTaylor Swift is planning a concert tour. The venue can hold 50,000 fans. VIP tickets cost $250 each, and regular tickets cost $100 each. If the total revenue from ticket sales is $6,500,000 and all tickets are sold, how many VIP tickets were sold?\n1. Identification and Definition. CoMAT first identifies and defines the relevant variables and constants to ensure precision in the symbolic reasoning process. For instance:\n\u2022 Variables: $v$ (number of VIP tickets), $r$ (number of regular tickets)\n\u2022 Constants: $T$ (total capacity of 50,000), $P_v$ (price of VIP tickets, 250), $P_r$ (price of regular tickets, 100), $R$ (total revenue, 6,500,000)\n2. Structural Logic Translation: Next, COMAT extracts the key variables and translates the problem into formal rules that define their relationships, ensuring the reasoning process is grounded in well-defined constraints:\n\u2022 $v + r = T$\n\u2022 $P_v v + P_r r = R$\n\u2022 $v \\geq 0, r \\geq 0$ (non-negative constraints)\n3. Explicit Factual Representation: CoMAT then integrates all relevant facts into the logical structure to avoid omitting key information:\n\u2022 $T = 50,000$\n\u2022 $P_v = 250$\n\u2022 $P_r = 100$\n\u2022 $R = 6,500,000$\n4. Question Formalisation: CoMAT formalises the question into symbolic expression to ensure that the reasoning process remains objective and free of bias from answer options. However, the model may inherently choose to parse the question in natural language rather than a symbolic representation. In this case:\nIn our example, we are tasked with finding $v$, the number of VIP tickets:\nFind $v : (v + r = T) \\wedge (P_v \\cdot v + P_r \\cdot r = R)$\n5. Reasoning Execution: The problem is then solved step-by-step using the symbolic representation, as demonstrated:\nStep 1: Express $r$ in terms of $v$ using $v + r = T$:\n$r = T - v = 50,000 - v$\nStep 2: Substitute into the revenue equation $P_v v + P_r r = R$:\n$250v + 100(50,000 - v) = 6,500,000$\nStep 3: Simplify:\n$250v + 5,000,000 - 100v = 6,500,000$\nStep 4: Solve for $v$:\n$v = \\frac{1,500,000}{150} = 10,000$\n6. Derivation of Final Answer: The final answer is then derived solely based on the logical reasoning applied. In this case:\nThe number of VIP tickets sold is 10,000.\n7. (optional) Answer Matching: In multiple-choice QA tasks, CoMAT match the final answer to the most similar one from the provided options, without considering the order or labelling of the options. This final step ensures that the symbolic reasoning remains unbiased by the format of the answer choices.\nThis step-by-step symbolic reasoning ensures that each operation is transparent and verifiable, reducing the potential for errors often introduced in purely natural language reasoning. CoMAT's structured methodology facilitates easier error tracing, as shown in Figure 3. In cases where the final answer is incorrect, each step can be individually examined to identify mistakes an advantage over traditional CoT, where explanations may not align with the actual reasoning steps, making the reasoning process less precise or harder to verify (Turpin et al., 2024; Li et al., 2024)."}, {"title": "Experimental Setup", "content": "Datasets. We selected a total of seven datasets, including four in English: AQUA (Ling et al., 2017) and GSM8K (Cobbe et al., 2021), which were extracted from Math Word Problem (MWP) datasets,"}, {"title": "Results", "content": "Table 1 compares CoMAT, CoT (Kojima et al., 2022), Faithful CoT (Lyu et al., 2023) and baseline models across various benchmarks. COMAT consistently outperforms traditional CoT and Faithful CoT in most datasets, particularly on tasks requiring advanced mathematical reasoning.\nFor open-source models, Qwen2-7b shows significant improvements with CoMAT. Under CoT, it often struggle to provide reasoning on Olympiad-Bench dataset. With CoMAT, its performance increases on English OlympiadBench (5.19% \u2192 20.92%), and Chinese OlympiadBench (8.09% \u2192 13.24%). Qwen2-72b also benefits from CoMAT, especially on OlympiadBench and GaoKao, with English OlympiadBench accuracy increasing (27.74% \u2192 32.17%), highlighting CoMAT's ability to improve reasoning in models that initially underperform.\nAmong closed-source models, Gemini shows notable gains with CoMAT, with an 8.18% improvement on English OlympiadBench. Similarly, GPT-4o shows substantial gains on Mandarin datasets, with performance on GaoKao rising from 63.27% (CoT) to 71.43% (CoMAT).\nHowever, there are some cases where COMAT does not outperform CoT. For instance, Qwen2-72b and GPT-4o show declines on AQUA (79.13% \u2192 72.44% and 84.25% \u2192 83.46%, respectively). These decreases suggest that on simpler tasks where models already perform well, the added complexity of symbolic reasoning may not yield significant benefits.\nWhen comparing CoMAT with Faithful CoT on MWP datasets, Faithful CoT shows a minor gain on GSM8K (93.70% \u2192 95.0%). However, CoMAT demonstrates a significant 9.86% improvement on AQUA, highlighting its ability to outperform Faithful CoT without relying on external solvers. COMAT also surpasses Faithful CoT on MMLU-Redux. We excluded Faithful CoT from comparisons on GaoKao and OlympiadBench due to its inability to execute most questions using external solvers, as outlined in Appendix A. CoMAT effectively mitigates these limitations, proving its capability to handle complex tasks without reliance on external solvers.\nGenerally, we observe the most significant gains on the challenging datasets (GaoKao and OlympiadBench), which require advanced reasoning. For instance, CoMAT improves average performance on the English OlympiadBench (23.68% \u2192 30.74%), and GaoKao (56.13% \u2192 60.70%). This suggests that CoMAT is particularly effective at enhancing reasoning capabilities for complex tasks. On simpler datasets like AQUA, where the base models already perform well, the average gains are smaller but still present, such as a 4.48% increase at MMLU-Redux. The average results for each model are illustrated in Figure 4.\nOverall, integrating symbolic reasoning into the Chain-of-Thought process significantly enhances language models' ability to tackle complex mathematical reasoning. The improvements are more substantial on advanced tasks, aligning with CoMAT's design to handle intricate reasoning steps through structured symbolic representations."}, {"title": "Step Contribution Analysis", "content": "We conducted an ablation study to assess the impact of each step in the CoMAT prompt on model performance. We tested 16 variations by removing Steps 1, 2, 3, or 4 individually, as well as combinations (i.e., removing steps 1 and 2 or removing steps 3 and 4).\nFigure 5 shows the change in accuracy compared to the full CoMAT prompt. We found that each individual step plays a crucial role in maintaining the accuracy of CoMAT. Removing Step 1 leads to the most significant drop in accuracy (6.91%). This suggests that Step 1 is fundamental to CoMAT, potentially serving as the foundation upon which other steps build; its absence may disrupt critical initial processes or data preparation necessary for accurate performance. Omitting Step 2 or Step 3 results in smaller declines (1.38% and 2.63% respectively), indicating these steps are important but less critical than Step 1. Interestingly, removing both Steps 1 and 2 results in a smaller performance drop (2.43%) than removing Step 1 alone, suggesting overlapping functionalities where the model compensates using remaining steps. Overall, the full COMAT prompt achieves the highest average accuracy of 77.45%, reaffirming the importance of retaining all steps for optimal performance.\nWe further quantified each step's contribution using Shapley-value (Shapley, 1953) analysis (Figure 6). The analysis shows that all steps positively impact performance, with Steps 1 and 2 having the greatest influence, aligning with the expectation"}, {"title": "Multilingual Analysis", "content": "To assess the impact of low-resource languages on CoMAT's performance, we evaluate the models on MGSM dataset (Shi et al., 2022), focusing on Swahili (SW), Bengali (BN), Thai (TH), and Telugu (TE). While CoMAT has shown strong performance in high-resource languages like English and Mandarin, our analysis reveals mixed results in low-resource language settings relative to CoT. This suggests that while CoMAT is effective, further optimisation may be needed to fully adapt COMAT to low-resource languages.\nAs shown in Table 2, Gemini-1.5-Pro demonstrated notable improvements in low-resource contexts, with an average performance increase of 5.60% after applying CoMAT. Significant gains were observed in Bengali (BN) and Swahili (SW), where accuracy increased to 82.40% and 83.20%, respectively. These improvements indicate that COMAT can enhance reasoning in lower-resource settings when applied to certain models.\nHowever, the performance of GPT-4o model slightly declines, from 89.50% to 88.30%. This suggests that while CoMAT remains effective overall, certain models might struggle to generalise as effectively in low-resource contexts. In languages like Thai (TH) and Telugu (TE), the model's performance remained relatively stable but did not show the same level of improvement observed in higher-resource languages.\nThese mixed results indicate that while COMAT shows promise for improving reasoning in low-resource languages, particularly with models like Gemini, further optimization may be required to fully optimise its performance across models and language contexts with limited training data."}, {"title": "Answer Order Swapping", "content": "Building on the findings of Gupta et al. (2024), which demonstrated that altering answer choices in the MMLU dataset could affect model accuracy, we extended this investigation to both the MMLU-Redux and AQUA datasets using GPT-4o as our baseline. We have also introduced an additional challenge by including a random answer option, as detailed in Appendix D.2.\nFigure 7 illustrates the effect of this option swapping. On the AQUA dataset, both CoT and CoMAT models experienced accuracy drops, highlighting their sensitivity to answer structure changes. However, COMAT was more resilient, with only a 0.17% decrease and a low standard deviation of 0.91%. In contrast, GPT-4o with CoT saw a significant drop of 14.96% and a higher standard deviation of 3.50%, indicating greater inconsistency when options were shuffled. This highlights CoMAT's stability and robustness under altered conditions.\nOn the MMLU-Redux dataset, GPT-4o baseline showed a slight improvement, with a 1.34% standard deviation. CoT and CoMAT both had minor accuracy decreases, with CoMAT achieving 85.05% (standard deviation of 1.65%), slightly outperforming CoT at 84.43% (1.87% standard deviation). Detailed results can be referred to in Appendix D.1."}, {"title": "Related Work", "content": "Logical Reasoning. Logical reasoning tasks require models to handle complex logical structures (Cummins et al., 1991). Traditional methods include rule-based (Robinson, 1965) and neural-based approaches (Amayuelas et al., 2022; Gerasimova et al., 2023) for interpreting symbolic representations. Recent advancements integrate LLMs into symbolic reasoning, introducing frameworks like Logic-LM (Pan et al., 2023), SAT-LM (Ye et al., 2024) and LeanReasoner (Jiang et al., 2024a). These frameworks use LLMs to convert natural language into symbolic syntax processed by external reasoning tools, enhancing performance through self-consistency and non-linear reasoning (Wang et al., 2023; Zhang et al., 2022). However, they often rely on external tools, assuming LLMs cannot parse symbolic expressions as reliably as rule-based reasoners.\nSymbolic Chain-of-Thought Prompting. Symbolic CoT prompting (Lyu et al., 2023) combines natural language (NL) and symbolic language (SL) in the reasoning chain. NL decomposes complex queries into subproblems, and SL programs (e.g., Python, LEAN) handle each subproblem. A deterministic solver executes the SL to derive the final answer, ensuring faithfulness while NL aids interpretability. Recent efforts aim to reduce reliance on SL programs by leveraging LLMs (Xu et al., 2024), but these methods focus primarily on logical reasoning rather than complex mathematical tasks and still require verifiers to ensure accuracy.\nMathematical Reasoning. Mathematical reasoning with LLMs has been explored widely (Lewkowycz et al., 2022; Luo et al., 2024; Ahn et al., 2024; Imani et al., 2023; Chen et al., 2024; Meadows and Freitas, 2022; Mirzadeh et al., 2024), with CoT methods yielding significant performance gains (Jiang et al., 2024c; Chu et al., 2023; Ranaldi and Freitas, 2024). Deep problem understanding (Zhong et al., 2024), structured formats (Tam et al., 2024), and building supervision models for reasoning (Lightman et al., 2023; Jiang et al., 2024b) also enhance accuracy. Other studies focus on premise selection and symbolic frameworks for systematic evaluation (Meadows et al., 2023; Ferreira and Freitas, 2020)."}, {"title": "Conclusion", "content": "We propose CoMAT, a simple yet effective framework that decomposes complex mathematical reasoning into two stages Symbolic Conversion and Reasoning Execution. CoMAT operates entirely within LLMs, eliminating reliance on external solvers and ensuring a transparent and accurate reasoning process. By avoiding external solvers, COMAT mitigates issues related to code generation failures, providing a more robust solution for a broad range of mathematical tasks. Our analysis highlights four key steps in the CoMAT process and demonstrates its effectiveness across various datasets with different levels of complexity and linguistic diversity, including English, Mandarin, and low-resource languages. CoMAT consistently outperforms traditional CoT, achieving state-of-the-art performance and enhancing consistency when answer options are shuffled, demonstrating robustness and reliability. Despite its simplicity, CoMAT offers a scalable and effective solution for complex mathematical reasoning, providing greater faithfulness and verifiability across a wide range of tasks."}, {"title": "Limitations", "content": "While CoMAT demonstrates strong performance, there are several potential limitations. Firstly, our evaluation was limited to the current set of symbolic steps, where some steps have been previously used in other reasoning frameworks. For instance, a step similar to our step 1 is used by Lyu et al. (2023) and Xu et al. (2024). Although our approach proved effective, further research is needed to assess COMAT's performance using additional symbolic languages to ensure a more comprehensive evaluation. Secondly, while CoMAT enhances verifiability and faithfulness, it introduces a higher computational overhead compared to CoT due to the structured nature of its formalisations. This process involves generating additional symbolic representations, leading to a larger token count, which increases both computational costs and API usage. Consequently, scaling COMAT could require more substantial computational resources than traditional CoT methods. Lastly, CoMAT has been focused within the mathematical reasoning. Further research could explore extending symbolic reasoning to other domains, beyond mathematical reasoning, to evaluate its effectiveness in broader tasks."}, {"title": "Implementation Details", "content": "In all our experiments, we used OpenAI GPT-4o models (gpt-4o-2024-08-06 and gemini-1.5-pro-001) as well as Qwen2-7B and Qwen2-72B. For API details regarding GPT-4o, please refer to https://platform.openai.com/docs/models/gpt-4o, and gemini can be reffered at https://ai.google.dev/gemini-api/docs/models/gemini. For the open-source models, Qwen2-7B and Qwen2-72B (https://huggingface.co/docs/transformers/en/model_doc/qwen2) serve as our primary inference models, and we conducted all experiments using two 80-GB A100 GPUs per dataset for inference. The following hyperparameters were consistently applied across all experiments:\n\u2022 Temperature: 0.0 (for greedy decoding)\n\u2022 Max tokens: 3500\n\u2022 Experiment Setting: All experiments are conducted through a 1-shot setting, with the same example across all evaluations."}, {"title": "Evaluation Metrics", "content": "Majority of our datasets use exact match as our evaluation metrics. For the OlympiadBench datasets, where exact match is not a suitable evaluation metric, we used the GPT-4o-mini model as a benchmark to assess how closely the answers of the model aligned with the ground truth. To prevent the model from generating reasoning that could influence its decision, we input only the final three sentences, which typically contain both the answer and the correct solution. The task was to determine whether these two elements matched, without providing any intermediate reasoning."}, {"title": "Extended Results and Analysis", "content": "In this section, we will present more results and analysis that did not fit into the main text:\nWe examine the sensitivity of various prompts in CoMAT by experimenting with 16 different variants. In each variant, we omit individual steps or combinations of steps from the COMAT process to assess their impact on performance.\nAs shown in Table 3, while AQUA and GaoKao occasionally perform better when certain steps are omitted, the overall average performance consistently decreases compared to the original CoMAT prompt. This indicates that CoMAT performs most effectively when all steps are included. The results highlight the importance of each step in contributing to the overall performance, demonstrating that CoMAT's structure is essential for achieving optimal results."}, {"title": "Option Swapping", "content": "This section supports the ablation studies presented in Figure 7. We experimented by shuffling the options and adding an additional random option, creating 5 different variants for the choice sets. These variants are illustrated in Figure 8.\nFor each dataset, we conducted three evaluations per variant and calculated the average to ensure consistency. The results reveal a notable difference between CoT and CoMAT in terms of performance stability. CoMAT exhibits more consistent results following the option modifications, as evidenced by the standard deviation. For example, in the AQUA dataset, CoMAT demonstrated a low standard deviation of 0.91%, whereas CoT had a significantly higher standard deviation of 3.50%, underscoring the variability in CoT's performance. These findings further support the consistency of CoMAT's performance, as previously highlighted in Figure 7. Although both models showed a decline in performance after the option changes, CoMAT's stability is worth noting. The drop in performance across models suggests that further investigation into the impact of option variation may be valuable."}, {"title": "Shapley Value Analysis Experimental Detail", "content": "To further analyse the sensitivity of individual steps in the CoMAT pipeline, we follow a Shapley value analysis for the different steps we have. The goal of this analysis is to quantify the contribution of each reasoning step to the overall performance by calculating Shapley values, thereby assessing the performance impact of omitting specific steps.\nIn this section, we detail the methodology used to compute Shapley values based on step omissions and their corresponding performance outcomes. Our approach systematically evaluates the marginal contribution of each step by experimenting with various combinations of omitted steps and using performance metrics to estimate their impact on accuracy.\nEach test case configuration is generated by omitting certain steps from the CoMAT pipeline. For each configuration, the performance is evaluated using a binary correctness metric, is_correct, indicating whether the process was successful. The result for each configuration is stored in a data structure, where the omitted steps are represented as binary vectors.\nWe denote the performance of the system when a specific subset $S$ for steps is omitted by $v(S)$. For each subset of omitted steps, the mean performance (correctness) is computed from the available data. These performance values $v(S)$ are stored in a dictionary, which allows us to later compute marginal contributions for each step.\nFor each step i, we compute its marginal contribution by comparing the performance when the step is included in the missing set versus when it is excluded. For a given permutation $\\pi$, let $S_i$ be the set of steps preceding i in $\\pi$, and let $S_i \\cup \\{i\\}$ be the set when i is included. The marginal contribution $\\Delta_i(\\pi)$ for each step i is given by:\n$\\Delta_i(\\pi) = v(S_i \\cup \\{i\\}) \u2013 v(S_i),$"}, {"title": "Step Length Analysis", "content": "In this section, we present two analyses: one examining the output length at each reasoning step, and the other focused on the number of mathematical tokens used. We use three distinct datasets \u2013 AQUA, Gaokao, and OlympiadBench-EN\u2014to assess variations in difficulty and language contexts. Specifically, we compare the average response lengths across the reasoning steps of the AQUA dataset for four models: Gemini, Qwen-7b, GPT-4o, and Qwen-72b. These models were evaluated on three datasets, as shown in Figure 9.\nThe reasoning step 5 of the COMAT model consistently produces the longest responses across most datasets. This observation is expected, as step 5 involves reasoning execution, which naturally requires more extensive elaboration. On more complex datasets, such as OlympiadBench-EN, the number of mathematical tokens tends to increase significantly during symbolic conversion, reflecting the need for more intricate calculations. Interestingly, models evaluated on multilingual datasets demonstrate a tendency to use more mathematical tokens, likely due to the challenges posed by cross-linguistic transfer in mathematical reasoning.\nGPT-4o consistently generates longer responses that include more mathematical tokens when compared to the other models, indicating its more detailed approach to problem-solving. Conversely, while Qwen-7b generates longer outputs on commonly used datasets like AQUA, it tends to produce shorter responses on less frequently encountered datasets. This disparity suggests a potential trade-off between general dataset familiarity and the model's ability to handle more specialised or complex tasks.\nThis analysis highlights the nuances of how different models approach mathematical reasoning and response generation, revealing that model performance can vary significantly based on the dataset's complexity and the underlying linguistic challenges."}]}