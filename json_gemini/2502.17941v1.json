{"title": "OPTIMAL BRAIN APOPTOSIS", "authors": ["Mingyuan Sun", "Zheng Fang", "Jiaxu Wang", "Junjie Jiang", "Delei Kong", "Chenming Hu", "Yuetong Fang", "Renjing Xu"], "abstract": "The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of deep learning, neural networks have become deeply integrated into all sectors of our daily life. Convolutional Neural Networks (LeCun et al., 1998; Krizhevsky et al., 2012; He et al., 2016; Ma et al., 2023) and Transformers (Vaswani et al., 2017; Dosovitskiy et al., 2021) are two typical structures used most widely. As researchers continuously innovate, the performance of neural networks improves, but the number of parameters and the computational complexity also increase significantly. Therefore, how to efficiently reduce the parameter size and computational overhead of neural networks while maintaining their performance as much as possible has become a crucial problem.\nExtensive research (LeCun et al., 1989; Molchanov et al., 2016) demonstrates that pruning is a powerful tool for dealing with this issue. Generally speaking, pruning can be divided into two main streams: unstructured pruning and structured pruning. Unstructured pruning (Guo et al., 2016; Han et al., 2015; Dong et al., 2017) involves the removal of individual weights or neurons from a neural network. The key advantage of unstructured pruning lies in its flexibility and the fine-grained control it offers over the model's architecture. This method often requires specialized hardware or software to exploit the resultant sparsity for computational efficiency. Structured pruning (Anwar et al., 2017; Yeom et al., 2021) removes entire neurons, channels, or layers from a neural network, which is more frendly to software since parameters of neural networks are mainly structured data, such as tensor, matrix, and vector. Removing entire neurons directly corresponds to a slicing or selecting operation on the structured data, which is easy to implement and more compatible with standard hardware accelerators, such as GPUs and TPUs.\nHanson & Pratt (1988) is one of the earliest works to explore structured pruning. The underlying idea is that significant weights typically possess greater magnitudes, as they need to process and transmit"}, {"title": "2 RELATED WORK", "content": "Model Compression Model compression is an area that focuses on creating smaller, faster, and more efficient models suitable for deployment in environments with limited resources, like mobile devices or embedded systems. There are several typical fields within this area, including quantization (Courbariaux et al., 2015; Rastegari et al., 2016; Pouransari et al., 2020), knowledge distillation (Hinton et al., 2015; Chen et al., 2021; Zhou et al., 2021), neural architecture search (Liu et al., 2018; Zoph & Le, 2016; Pham et al., 2018), and network pruning (Molchanov et al., 2019; 2016). Quantization, outlined in works like Hubara et al. (2018) and Jacob et al. (2018), focuses on reducing parameter precision to accelerate inference and decrease model size, enabling deployment on devices with limited resources. Knowledge distillation, as introduced by Hinton et al. (2015); Romero et al. (2014), leverages a smaller \"student\" model to mimic a larger \"teacher\" model, effectively compressing the knowledge and achieving high performance with less computational demand. Neural Architecture Search (NAS), with seminal contributions from Zoph & Le (2016), automates the discovery of optimal architectures, often outperforming human-designed models in efficiency and accuracy. Pruning techniques, highlighted in work by Han et al. (2015), remove non-essential weights or neurons, significantly reducing model complexity and enhancing inference speed without major accuracy losses. Together, these techniques represent the forefront of model compression research, addressing the balance between performance and computational efficiency necessary for advanced AI applications.\nNetwork Pruning Network pruning, initially recognized as an importance estimation problem (Molchanov et al., 2019; Chauvin, 1988; Yu et al., 2018; He et al., 2020), has been prompting researchers to focus on finding accurate criteria that reveals the importance of parameters or neurons in neural networks. Molchanov et al. (2016) operated under the assumption that each layer in feed-forward networks held equal importance and introduced a heuristic for global scaling normalization. However, this approach did not prove effective in networks incorporating skip connections. Additionally, the method relies on using network activations to calculate its criterion, resulting in increased memory demands. In contrast, pruning methods that focus on batch normalization (Gordon et al., 2018; Huang & Wang, 2018; Liu et al., 2017; Ye et al., 2018) bypass the need for sensitivity analysis and are applicable on a global scale. In intricate network architectures (Liu et al., 2021; Luo & Wu, 2020; You et al., 2019; Zhang et al., 2021), parameter interdependencies often require their joint pruning. This collective pruning of interlinked parameters has been an area of focus in structured pruning research since its early stages. Fang et al. (2023) proposes to build a dependency"}, {"title": "3 PRELIMINARY", "content": "Consider a feed-forward neural network with parameters \u03b8 and L layers. Similar to OBD (LeCun et al., 1989), when we add small perturbation \u03b4\u03b8 on \u03b8, the second-order Taylor expansion of the perturbation on the objective function is given by\n$$\\delta L(\\theta) = \\frac{\\partial L}{\\partial \\theta} \\delta \\theta + \\frac{1}{2} \\delta \\theta^T H \\delta \\theta + o(||\\delta \\theta||^3),$$\nwhere H is the Hessian matrix that represents the second-order derivatives between all parameter pairs. It is usually infeasible to compute the Hessian matrix due to its complexity of \\(O(n^2)\\), where n is the number of parameters in the network (LeCun et al., 1989; Hassibi & Stork, 1992). By expanding the first and second term of eq. (3), we can define the perturbation of the loss caused by \u03b4\u03b8 as\n$$\\frac{\\partial L(\\theta)}{\\partial \\theta} \\delta \\theta = \\frac{\\partial L}{\\partial \\theta_i} \\delta \\theta_i + \\sum_j \\frac{\\partial^2 L}{\\partial \\theta_i \\partial \\theta_j} \\delta \\theta_i \\delta \\theta_j + o(||\\delta \\theta||^3).$$\nThe first term of eq. (4) is leveraged to estimate the improtance of neurons in Molchanov et al. (2016). Same to prior works, we ignore the higher order term \\(o(||\\delta \\theta||^3)\\). Current works (Hassibi & Stork, 1992; Yu et al., 2022; Benbaki et al., 2023) that leverage the second-order Taylor expansion term approximate the Hessian matrix with Fisher information matrix. This approximation, if applied to eq. (4), would change its second-order term to \\(\\sum_j \\frac{\\partial^2 L}{\\partial \\theta_i \\partial \\theta_j} \\delta \\theta_i \\delta \\theta_j\\approx \\frac{\\partial^2 L}{\\partial \\theta_i \\partial \\theta_j} \\delta \\theta_i \\delta \\theta_j\\). However, this approximation is not accurate enough to capture the second-order loss perturbation caused by \u03b4\u03b8 and \u03b4\u03b8\u0458. Therefore, we focus on a theoretical analysis on how to calculate the original second-order term \\(\\sum_j \\frac{\\partial^2 L}{\\partial \\theta_i \\partial \\theta_j} \\delta \\theta_i \\delta \\theta_j\\)."}, {"title": "4 METHOD", "content": "4.1 DEFINITION\nWe derive from a general form of linear layers in modern neural networks. For layer l \u2208 [1, L], we denote the weight parameter as W(1) \u2208 \\(R^{l_{out} \\times l_{in} \\times P_{weight}}\\) and bias parameter as b(1) \u2208 \\(R^{l_{out}}\\). We denote"}, {"title": "4.2 SERIES CONNECTIVITY", "content": "Definition 4.1 (Series Connectivity). In a neural network at layer l, if there exists a layer l' such that there is a differentiable function mapping the output of layer l' to the input of layer l, we say layer l' and l are in series connectivity. Specifically:\n\u2022 layer l' is in lower series connectivity to layer l.\n\u2022 layer l is in upper series connectivity to layer l'.\nIn fig. 1a, there are differentiable functions from Y(11) to X(12) and X(13), respecively, so layer l\u2081 is in series connectivity with both layer 12 and layer 13. Without loss of generality, we take layer 11 and layer 12 as an example. Then\n$$\\frac{\\partial L}{\\partial y^{(l_1)}} = \\frac{\\partial L}{\\partial y^{(l_2)}} \\frac{\\partial x^{(l_2)}}{\\partial y^{(l_1)}} \\frac{\\partial y^{(l_2)}}{\\partial x^{(l_2)}}$$\ndifferentiable to \\(\\theta^{(l_2)}\\)\nalso X(12) is differentiable to \\(\\theta^{(l_1)}\\). According to our analysis in the beginning of this section, a nonzero Hessian submatrix exists between layers 11 and 12.\nTheorem 4.2. For layer l in a neural network where layers lup \u2208 Lup and layers llow \u2208 Llow are in upper and lower series connectivity to layer l, respectively, then for weight parameter w(1) and bias parameter b(1) of layer l, we have\n$$\\sum_{l' \\in L_{up} \\cup L_{low}} \\sum_j \\frac{\\partial^2 L}{\\partial \\theta_j \\partial w^{(l)}} \\delta \\theta_j = \\sum_{l' \\in L_{up}} \\sum_j \\frac{\\partial \\frac{\\partial L}{\\partial x^{(l_{up})}}}{\\partial w^{(l)}} \\delta w^{(l)} + \\frac{\\partial L}{\\partial x^{(l_{up})}} X_{mn}^{(l)}$$\n$$\\frac{\\partial \\frac{\\partial L}{\\partial y^{(l)}}}{\\partial w^{(l)}} \\delta w^{(l)},$$\nin which \\(X_{mn}^{(l)}\\) is given by\n$$X_{mn}^{(l)} = \\sum_{l_{low} \\in L_{low}} \\sum_k \\frac{\\partial^2 \\frac{\\partial L}{\\partial y^{(l_{low})}}}{\\partial x^{(l_{low})}} \\delta \\theta_{k}^{(l_{low})},$$\n$$X_{mn}^{(l)} = \\sum_{l_{low} \\in L_{low}} \\sum_k \\frac{\\partial \\frac{\\partial L}{\\partial y^{(l_{low})}}}{\\partial x^{(l_{low})}} \\delta \\theta_{k}^{(l_{low})},$$\n$$X_{mn}^{(l)} = \\sum_{l_{up} \\in L_{up}} \\sum_j \\frac{\\partial \\frac{\\partial L}{\\partial y^{(l_{up})}}}{\\partial x^{(l_{up})}} \\delta \\theta_{j}^{(l_{up})},$$"}, {"title": "4.3 PARALLEL CONNECTIVITY", "content": "For recent novel neural network structures such as Transformer (Vaswani et al., 2017), matrix multiplication plays a crucial and effective role in achieving their impressive performance. It also introduces parallel connectivity to layers and lead to the nonzero Hessian matrices between these connected layers.\nDefinition 4.3. In a neural network, if there exist two layers l and l' such that there are differentiable functions respectively mapping the outputs of layer l and l' to the inputs X(left) and X(right) of a matrix multiplication operation Y(mul) = X(left)X(right), we say layer l and l' are in parallel connectivity. Denote the multiplication operation as layer lm, then\n\u2022 layer l is in left parallel connectivity to layer lm.\n\u2022 layer l' is in right parallel connectivity to layer lm.\nIn fig. 1a, layer 12 and layer 13 are in parallel connectivity. Similarly, the gradient of loss w.r.t. parameters of layer 12 is \\(\\frac{\\partial L}{\\partial \\theta^{(l_{2})}} = \\frac{\\partial L}{\\partial y^{(l_{4})}} \\frac{\\partial y^{(l_{4})}}{\\partial y^{(l_{2})}}\\), in which \\(\\frac{\\partial y^{(l_{4})}}{\\partial y^{(l_{2})}}\\) is differentiable to \\(\\theta^{(l_{3})}\\) and vice versa. Therefore, the nonzero Hessian submatrices resulting from parallel connectivity should also be considered.\nTheorem 4.4. For a multiplication operation Y(mul) = X(left)X(right) in a neural network, where X(left) \u2208 \\(R^{l_{row} \\times l_{hid}}\\) and X(right) \u2208 \\(R^{l_{hid} \\times l_{col}}\\), layers l\u0131 \u2208 Lleft and lr \u2208 Lright are in left and right parallel connectivity to this multiplication operation, respectively. Consider two surrogate weight"}, {"title": "4.4 JACOBIAN-VECTOR PRODUCT FORWARD PROPAGATION", "content": "From a computational overhead perspective, the main calculation part within theorem 4.2 and the-orem 4.4 focuses on \\(\\frac{\\partial \\hat{X}}{\\partial w}\\), the gradients of X(1) w.r.t. the parameters of all layers that are in lower series connectivity to \\(\\hat{Y}^{(l)}\\), as we need to separately back-propagate each entry of X(1), usually a large matrix in attention modules. To address this issue, we introduce Jacobian-Vector Product Forward Propagation (JVPF), a method capable of computing \\(\\frac{\\partial \\hat{X}}{\\partial w}\\) for all layers with an acceptable computational expense.\nLet's look at a layer in lower series connectivity to layer l and denote it as layer llow. The derivative of X(1) w.r.t. a single layer llow can be expressed as\n$$\\frac{\\partial \\hat{X}^{(l)}}{\\partial \\theta^{(l_{low})}} = (\\frac{\\partial \\hat{X}^{(l)}}{\\partial \\hat{y}^{(l_{low})}}) \\frac{\\partial \\hat{y}^{(l_{low})}}{\\partial \\theta^{(l_{low})}} = \\frac{\\partial \\hat{y}^{(l)}}{\\partial \\hat{y}^{(l_{low}+1)}} \\frac{\\partial \\hat{X}^{(l_{low}+1)}}{\\partial \\hat{y}^{(l_{low})}} \\frac{\\partial \\hat{y}^{(l_{low})}}{\\partial \\theta^{(l_{low})}},$$\nindicating that we can calculate it in a layer-by-layer manner. Further, let us group layers in lower series connectivity to layer l into several groups, each of which contains both parameter layers and nonparameter layers that are in series connectivity to each other. We denote the ith group as Gi = {li1, li2,\u00b7\u00b7\u00b7, liN; }, where N\u00bf is the number of layers in group G\u00bf and layer li(j+1) is subsequent to layer lij. Then we have\n$$\\frac{\\partial \\hat{X}^{(l)}}{\\partial \\theta^{(l_{low})}} = \\sum_{i} \\sum_{l_{i \\in G_i}} (\\frac{\\partial \\hat{y}^{(l)}}{\\partial \\hat{X}^{(l_{ij})}}) = \\sum_{i} \\sum_{l_{i \\in G_i}} (\\frac{\\partial \\hat{y}^{(l)}}{\\partial \\hat{y}^{(l_{i_N})}} ) \\cdots (f^{(l_{il})}) \\cdots (f^{(l_{i(N-1))}})),$$\nwhere\n$$f^{(l_{ij})} = \\left\\{\n\\begin{array}{ll}\n\\frac{\\partial \\hat{y}}{\\partial \\theta^{(l_{ij})}} + l_{ij} \\text{ has parameters, }\\\\\n(\\frac{\\partial \\hat{y}}{\\partial \\theta^{(l_{ij})}}) \\frac{\\partial \\hat{X}}{\\partial \\theta^{(l_{ij})}} \\text{ else. }\n\\end{array}\n\\right.$$\n\n\\(\\hat{X}^{(l_{ij})}\\), \\(\\hat{y}^{(l_{ij})}\\), and \\(\\frac{\\partial \\hat{y}}{\\partial \\theta^{(l_{ij})}}\\) are the input, output, and parameters of layer lij. Once we replace the forward function of each layer with eq. (14), we could calculate \\(\\frac{\\partial \\hat{X}}{\\partial w}\\) for each layer l through one forward propagation process, and all series connectivity groups can be calculated in parallel. An intuitive demonstration of JVPF is shown in fig. 1b."}, {"title": "4.5 PRUNING STRATEGY", "content": "Utilizing theorem 4.2 and theorem 4.4 along as our proposed JVPF, which offers an effi- cient choice to obtain the essential intermedi- ate values, we can calculate the Hessian-vector product element of each parameter as their im- portance scores with several batches of traning data. Please refer to appendix B for the de- tailed importance score acquisition. This im- portance score can be leveraged to conduct un- structured pruning for each parameter individu- ally, or conduct structured pruning for each pa- rameter group.\nStructured Pruning Following Fang et al. (2023), parameters from in-layer and inter-layer connections can be organized into several groups G. The importance scores belonging to each group is defined as IG = {IG)|gi \u2208 G}. For each group gi \u2208 G, importance scores are summed on every neuron over parameters of upper layers Lu C gi with length Nu and lower layers Li C gi with length N\u2081, as illustrated in in fig. 2.\nUnstructured Pruning The importance score in unstructured learning is more straightforward. By eliminating the process of gathering importance, considering each parameter layer as a group, and each parameter as a neuron, the definition of importance score becomes similar to what is described in structured pruning scenarios. Experimentally we found that the gradients of some parameters are zero due to gradient vanishing, making us hard to judge the importance of these parameters. This would have little influence on structured pruning since neurons' importance scores are gathered through multiple weights. However, in unstructured pruning, the importance score of each weight only depends on itself. By adding the magnitude of the corresponding weight in the importance score term, the problem is resolved.\nIn each pruning step, we calculate every importance score \\(\\frac{\\partial \\hat{I}}{\\partial w}\\) of the i,jth neuron in ith group over traning data of B batches, and normalize them within each group. Then we rank them from lowerst to highest. The lowest p percentage of parameters are pruned. We can gradually increase p to iteratively prune the model to a specific FLOPs or parameter percentage."}, {"title": "5 RESULTS", "content": "We empirically study the performance of OBA on CNNs where only series connectivity exists, and attention networks where both series connectivity and parallel connectivity are present. We focus on pruning towards as small FLOPs as possible to reduce the computation overhead of model inference. For structured pruning, we primarily compare our methods with those that leverage Hessian matrix information (Hassibi & Stork, 1992; Wang et al., 2019). In the context of unstructured pruning, we evaluate our approach against the state-of-the-art unstructured pruning method, CHITA (Ben-baki et al., 2023). Classical importance acquisition methods Weight (the magnitude of weights), OBD (LeCun et al., 1989) and Taylor (Molchanov et al., 2016) are also added into comparison for both structured and unstructured pruning tasks. In our experiments, we choose \u03b4\u03b8\u2081 = \u03b8i. The implementation details can be found at appendix A.\n5.1 STRUCTURED PRUNING\nCurrent structured pruning workflows can be roughly divided into one-shot pruning and iterative pruning. The former prunes the fine-tuned model towards a sparsified network and fine-tunes it after pruning, whereas the latter prunes the model iteratively and fine-tunes the model after each pruning step. One-shot pruning is more efficient than iterative pruning, but the latter is more effective. We evaluate our method on both of these two workflows.\n5.1.1 IMPORTANCE SCORE RANK CHARACTERIZATION\nWe first measure the ability of our method to characterize the importance of each neuron for structured pruning. Intuitively, the importance score of a neuron should be positively correlated to the"}, {"title": "5.2 UNSTRUCTURED PRUNING", "content": "We also evaluate the performance of OBA on unstructured pruning task. We follow a similar setting of the multi-stage pruning in CHITA (Benbaki et al., 2023) to have a fair comparison. Since Taylor (Molchanov et al., 2016), Weight, and OBD (LeCun et al., 1989) all obtain the importance scores from each parameter, we can ignore their importance gathering steps and add them into comparison. The results are shown in table 4 and table 5. Given the varying performance of the initial unpruned networks, we directly compare the accuracy ratio relative to the raw accuracy of all methods. The main version of CHITA, i.e. CHITA-CD, has very large computational time and memory cost on Resnet50, making itself infeasible for such huge networks. Thus we implemented the more effi-cient CHITA-BSO for comparison. It can be seen that Taylor and OBD fails on this task as their accuracies rapidly fall into 10% in the first pruning step. OBA's results on high sparsities surpass CHITA++ by a huge margin, proving itself to be effective in unstructured pruning task."}, {"title": "5.3 RUN TIME ANALYSIS", "content": "Here, we empirically study the time consumption of OBA. Table 6a shows the time consumption of each part of OBA, and the time costed by regular training. It can be seen that the time cost of computing parallel connectivity is the most time-consuming part of OBA, nearly same to the time of series connectivity. In network structures that do not contain multiplication operations, the time cost of OBA would be much lower. In table 6b, 200 batches of data with a batch size of 64 are"}, {"title": "6 CONCLUSION AND LIMITATION", "content": "In this paper, we propose Optimal Brain Apoptosis, a novel method for pruning neural networks. We first provide theoretical analysis on modern neural network structures to figure out nonzero Hessian submatrix conditions between layers. Then we propose an efficient approach that directly calculates the Hessian-vector product values for each parameter in the network, thereby calculating the second-order Taylor expansion for each parameter without any approximation. We empirically demonstrate the efficacy of our method on both structured pruning and unstructured pruning.\nLimitation OBA, in its current form, can be applied to network structures including MLPs, CNNs, and Transformers. For networks with more complex architectures, like RNNs and State Space Mod-els that handle time-series data, computing the Hessian matrix becomes more difficult and necessi-tates additional research. This is an interesting area that warrants further exploration in the future."}, {"title": "A IMPLEMENTATION DETAILS", "content": "In our structured pruning experiments, we align our settings to EigenDamage (Wang et al., 2019), which is a good Hessian based pruning method. For each pruning step, we obtain the importance score from 200 batches of data to calculate the importance score. We set the batch size to 64 and the learning rate to 0.001 for the fine-tuning process. We use the SGD optimizer with a momentum of 0.9 and a weight decay of 5 \u00d7 10-4. The learning rate is divided by 10 at the 80th and 120th epochs. We set the maximum epochs to 150 for CIFAR10 and CIFAR100 datasets. For ImageNet experiments, we use the same settings as Fang et al. (2023) of ResNet50 and ViT-B/16. We set the maximum epochs to 100 for ImageNet experiments."}, {"title": "BALGORITHMIC DETAILS", "content": "As can be seen in algorithm 1, We first conduct a forward propagation process on the network and record the output gradient of the loss w.r.t. the output of each layer (line 1-2). Then we back-propagate these gradients for each parameter layer with corresponding weight \\(\\delta \\theta\\) to obtain the term\n$$\\sum_{l' \\in L_{up}} \\sum_j \\frac{\\partial^2 L}{\\partial^2 y^{(l)}} \\delta \\theta_j^{(l)}  \\text{(line 3-5)}.$$\n1. Upper Series Connectivity: These terms are recorded in the gradient of the corresponding parameters, so that we can obtain \\(\\sum_{l' \\in L_{up}} \\sum_j \\frac{\\partial^2 L}{\\partial^2 y^{(l)}} \\delta \\theta^{(l)} - \\frac{\\partial^2 L}{\\partial^2 y^{(l)}} \\delta \\theta - \\delta \\theta^{(l)})\\) in Equation (6) by multiplying the parameters with their gradients (line 6-7).\n2. Next, We obtain \\(\\hat{X}_{mn} = \\sum_{l \\in L_{low}} \\sum_k \\frac{\\partial \\hat{X}}{\\partial \\theta_{k}} (\\frac{\\partial \\hat{y}}{\\partial \\theta^{(l_{low})}})\\) for all parameter layers through the JVPF, these values are useful for calculating Lower Series Connectivity cases and Parallel Connectivity cases(line 9).\n3. Lower Series Connectivity: We obtain Equation (7) and add them into the importance scores (line 12).\n4. Parallel Connectivity: In the meantime, for all attention layers that induce parallel connectivity, we back-propagate the gradient with the surrogate inputs \\(\\hat{X}_{left}\\) and \\(\\hat{X}_{right}\\) according to Equations (12) and (13) (line 13-17), and multiply the parameters with their gradients that are in Parallel Connectivity with the attention layer (line 20-21)."}, {"title": "C NORMALIZATION METHODS", "content": "In our implementation, We leverage these normalization methods on importance scores for each group, including:\nNo Normalization (None) When the normalizer is set to None, the original values of importance scores are returned without any modification. This means that the data is used as-is, with all its original properties (such as scale and distribution) intact:\n$$I_{normalized} = I_j.$$\nStandardization (or Min-Max Normalization) This method scales the data so that it fits within a specific range, typically 0 to 1. This is achieved by subtracting the minimum value of the data and then dividing by the range of the data:\n$$I_{normalized} = \\frac{I_j - min(I)}{max(I) - min(I)}.$$\nMax Normalization In this approach, every importance score is divided by the maximum importance score of corresponding group to ensure that all the normalized values fall between 0 and"}, {"title": "D PRUNED LAYERS VISUALIZATION", "content": "We visualized the layer-wise FLOPs of pruned models by OBA and other methods for ResNet32 and VGG19 on CIFAR100 with a target FLOPs of 6%. It can be seen that compared with other methods, the difference of OBA between FLOPs of different layers is smaller, resulting in a smoother model in terms of number of neurons across layers. This significantly helps to improve the model's performance across various datasets and provide useful guidance for researchers to design and prune neural networks."}, {"title": "E PROOFS", "content": "E.1 THEOREM 4.2\nFor any two layers llow and lup in series connectivity, where lup is upper than llow. Note that \\(\\frac{\\partial L}{\\partial w_{k}^{(l_{low})} \\partial b_{k}^{(l_{up})}}\\) is a zero matrix because parameters \\(w_{k}^{(l_{low})}\\) and \\(b_{k}^{(l_{up})}\\) are independent of each other. With this prior, \\(\\frac{\\partial L}{\\partial w_{k}^{(l_{low})} \\partial w_{k}^{(l_{up})}}\\) is actually \\(\\frac{\\partial L}{\\partial w_{k}^{(l_{low})} \\partial w_{k}^{(l_{up})}}\\). We first calculate term \\(\\sum_{l' \\in L_{up}} \\sum_j \\frac{\\partial \\frac{\\partial L}{\\partial y^{(l_{l})}}}{\\partial x^{(l_{up})}} \\delta w^{(l)} (up) \\delta w^{(l)} (up) \\) for the lower layer llow.\nLemma E.1.\nand\nwhere \\(\\frac{\\partial y^{(l_{up})}}{\\partial x^{(l_{up})}}\\) \\(\\in\\) \\(R^{(l_{out} - mout) \\times (l_{in} - m_{in})}\\) is the jacobian matrix of \\(y^{(l_{up})}\\) with respect to \\(x^{(l_{up})}\\) taking \\(delta w^{(l_{up})}\\) as the weights.\nProof. Let \\(J^{(l_{up})}\\) = \\(\\frac{\\partial y}{\\partial x^{(l_{up})}}\\). The element-wise derivative of loss w.r.t. \\(X^{(l_{up})}\\) is given by\nand the element-wise derivative of loss w.r.t. \\(W^{(l_{up})}\\) is given by\nApplying chain rule, we have\nand"}, {"title": "E.2 THEOREM 4.4", "content": "Here we follow the notations in definition 4.3 to denote each value of the two layers.\nLemma E.3. Let \\(\\hat{X}^{(l_{left})}\\) \\(\\in\\) \\(R^{l_{row} \\times l_{hid}}\\) and \\(\\hat{X}^{(l_{right})}\\) \\(\\in\\) \\(R^{l_{hid} \\times l_{col}}\\) be two surrogate weight matrices such that"}]}