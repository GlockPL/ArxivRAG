{"title": "Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's Mathematical Reasoning", "authors": ["Yanan Chen", "Ali Pesaranghader", "Tanmana Sadhu"], "abstract": "Recent advances in Large Language Models (LLMs) have raised interest in their formal reasoning capabilities, particularly in mathematics. While closed LLMs like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K (Cobbe et al., 2021a), it remains unclear whether small to medium-sized open LLMs can achieve similar performance, questioning their reliability. To close this gap, we propose a post-training approach leveraging a mixture of opinions (MoO) from weaker ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that, each post-training sample is augmented with Chain-of-Thought (CoT) reasoning steps and answers from ancillary LLMs, enabling the main LLM to learn from diverse perspectives. We compare MoO with standard supervised fine-tuning (SFT), few-shot prompting, and the Mixture of Agents (MoA) method (Wang et al., 2024a) on mathematical reasoning benchmarks. Our results show that incorporating weaker LLMs' opinions improves mathematical reasoning by an average of 5%, highlighting the value of diverse perspectives in reasoning tasks.", "sections": [{"title": "Introduction", "content": "While LLMs have demonstrated impressive performance across various tasks, their reliability in mathematical and logical reasoning remains uncertain. Some studies emphasize their capabilities, yet a closer examination reveals significant limitations (Chen et al., 2024; Xie et al., 2024a,b; Valmeekam et al., 2024; Mirzadeh et al., 2024; Wang et al., 2024b; Kambhampati, 2024). Minor variations in input prompts can drastically alter reasoning steps and predictions, exposing sensitivity and bias (Zhuo et al., 2024; Anagnostidis and Bulian, 2024). Additionally, as reasoning complexity increases, the probability of arriving at a correct answer decreases exponentially, highlighting LLMs' struggles with multi-step reasoning tasks (Valmeekam et al., 2024; Stechly et al., 2024).\nTo address these challenges, researchers have explored multi-agent collaboration, iterative reasoning, and debate-based frameworks (Wang et al., 2024a; Du et al., 2023; Guo et al., 2024; Wang et al., 2024c; Liu et al., 2024; Hong et al., 2023). One notable example, Mixture of Agents (MoA) (Wang et al., 2024a), leverages LLMs' collaborative tendencies, where diverse responses\u2014despite imperfections-collectively improve predictions. MoA has shown success in preference alignment tasks, such as AlpacaEval 2.0 (Dubois et al., 2024) and MT-Bench (Zheng et al., 2023), by aggregating outputs from multiple models. However, its applicability to mathematical reasoning remains unexplored, and our preliminary experiments indicate that MoA is ineffective in this domain, even with additional layers. Furthermore, MoA primarily relies on large, closed-source models (e.g., GPT-4), limiting accessibility. Our preliminary tests with smaller LLMs (3B-13B) showed no significant improvements in mathematical reasoning or preference alignment.\nTo overcome these, in Section 2, we propose Mixture-of-Opinions (MoO) as a fine-tuning approach that enhances a main LLM by integrating diverse reasoning paths from weaker ancillary LLMs. Each training sample is enriched with Chain-of-Thought (CoT) reasoning steps and their corresponding answers from ancillary models, allowing the main model to learn from varied perspectives. We validate MoO on three mathematical reasoning benchmarks requiring advanced cognitive skills, demonstrating that incorporating diverse opinions significantly enhances reasoning accuracy. This highlights the potential of leveraging multiple perspectives, even from weaker models, to improve reasoning performance. Finally, we believe this work benefits the community by promoting the use of diverse opinions during post-training to enhance complex reasoning tasks."}, {"title": "Methodology: Mixture of Opinions", "content": "We illustrate our MoO framework in Figure 1, presenting three phases: (1) the MoO dataset curation, (2) post-training with the MoO dataset, and (3) inference with the post-trained model. We describe our framework and explain each phase below.\nFramework. Given a pool of candidate LLMs, i.e., $L_1... L_{n+1}$, we assume their original performance for a specific task is ranked as $L_{n+1} > L_n > ... > L_i > .. > L_2 > L_1$, meaning each $L_{i+1}$ is stronger than $L_i$. In the MoO framework, $L_{n+1}$ is considered as the main LLM, while the remaining LLMs are referred to as ancillary LLMs whose roles are to provide auxiliary and diverse opinions to enhance the overall reasoning performance of the main LLM. Notably, while the ancillary LLMs are assumed to be weaker than the main LLM, they have comparable reasoning abilities and can consistently generate well-structured responses.\nPhase I. The MoO Dataset Curation \u2013 In this step, we collect opinions from the ancillary LLMs to curate the MoO dataset for fine-tuning. For a given question, together with few-shots from the training set, the ancillary LLMs are prompted to generate their responses (i.e., opinions) which include the CoT reasoning steps and final answers. As shown in Figure 1 and Toy Prompt A.6.4, we add the opinions (from the ancillary LLMs) between the question and ground-truth (CoT-formatted) answer. Eventually, our MoO curated dataset contains each training example augmented with opinions from ancillary LLMs. It is worth mentioning that the opinions from the ancillary LLMs are added by following a fixed order to guarantee consistency between the training and inference sets.\nPhase II. Post-Training: Fine-Tuning with the MoO Data \u2013 Once the post-training set is curated with a mixture of opinions, we use it to fine-tune the main LLM. This process is comparable to (traditional) ensemble learning, as augmented opinions from a committee of weaker models help to improve the overall performance. During the post-training phase, the model learns to analyze and synthesize insights and rationales from various external opinions, enhancing its reasoning ability to generate the best possible answer.\nPhase III. Inference \u2013 During inference, for a given question, we first prompt the ancillary LLMs with few-shot examples from the training set to generate the opinions in the same order as in the training stage. Then, the test question and the corresponding opinions together are fed to the fine-tuned (main) LLM for generating an answer."}, {"title": "Experiments", "content": "Settings.\nBenchmarks. We use three benchmarks, namely GSM8K (Cobbe et al., 2021a), AQuA-RAT (Ling et al., 2017), and MATH (Hendrycks et al., 2021), widely used for arithmetic and mathematical reasoning in the literature. These benchmarks provide Chain-of-Thought reasoning traces which are suitable for generating opinions by ancillary LLMs. Appendix A.2 provides more details.\nBaselines. We consider three baselines for our experiments: (1) In-Context Learning (ICL) with few-shot prompting, (2) Supervised Fine-tuning (SFT) with the original training set, and (3) Mixture of Agents (MoA) with few-shot prompting. More details are available in Appendix A.3.\nMain and Ancillary LLMs. As Table 1 presents, Phi-3-medium-4k (14B) and Gemma-2-9B show the best ICL performance, while the Llama models have a moderate performance in comparison. This superiority of the Phi and Gemma models could be due to their larger sizes. Hence, to ensure a fair comparison, we consider both a larger model and a moderate model as the main LLMs. That is, we select Phi-3-medium-4k and Llama-3.1-8B as the main models while the other models, which are weaker than or comparable to them, are employed as ancillaries. For example, when we consider Llama-3.1-8B as the main model, the other Llama and Mistral models are used as ancillaries."}, {"title": "Main Results", "content": "Table 1 shows that MoO (i.e., post-training with the mixture of opinions) improves the performance against the three benchmarks. For example, MoO with Llama-3.1-8B achieves an accuracy of 75.98%, moderately exceeding the 73.46% accuracy achieved by ICL with Llama-3.1-8B on the GSM8K benchmark. Furthermore, MoO with Llama-3.1-8B substantially outperforms its ICL and SFT baselines on both the AQUA-RAT and MATH benchmarks. We also observe similar trends for MoO with Phi-3-medium-4k. Contrastively, MoA underperforms Phi-3-medium-4k's and Llama-3.1-8B's ICL and SFT baselines. This demonstrates that MoA is not a reliable choice for mathematical reasoning tasks. We construe that, integrating (auxiliary) opinions during fine-tuning (during post-training) effectively improves the performance of LLMs on such tasks."}, {"title": "Ablation Studies", "content": "We conduct three ablation experiments to examine the impact of ancillary LLMs and CoTs within the MoO framework. For this, we use Llama-3.1-8B as the main LLM and report our results in Table 2.\nDo the Opinions from Ancillary LLMs Matter? (Row 1 in Table 2). As seen in the table, removing the opinions of ancillary LLMs causes a drop compared to the original MoO version (i.e., the Full Version). This indicates that ancillary LLMs play an important role in the MoO framework.\nDo the Weakest Ancillary LLMs Make Any Contribution? (Row 2 in Table 2). For this experiment, we ignore the three Mistral models and fine-tune the main LLM without their opinions. As shown in the table, we observe a 2% decrease against AQUA-RAT and a minor decrease against GSM8K and MATH. This shows that the weakest ancillary LLMs can also contribute to further improvement, though slightly."}, {"title": "Do CoT-Based Opinions Outperform Answer-Only-Based Opinions?", "content": "(Row 3 in Table 2). Lee et al. (2023) showed incorporating a step-by-step chain of thoughts in the context for supervised fine-tuning is beneficial. These logical progressions act as detailed scratchpads which have a significant impact on the model's ability to reason effectively. To confirm this, in this experiment, we ignore the CoT steps within each opinion and only keep the direct answer. The table shows a noticeable performance drop compared to MoO (Full Version) which indicates that the MoO framework highly relies on the CoT reasoning steps from the ancillary LLMs for boosting the performance of the main LLM."}, {"title": "Discussion", "content": "We further discuss different aspects of MoO below.\nAncillary LLMs' Size. In our preliminary experiments, using only the smallest LLMs as ancillaries yielded minimal improvement, highlighting the need to include medium-sized LLMs as well. The effectiveness of smaller LLMs was limited by their tendency to generate repetitive content.\nOpinions of Weaker LLM's Matter. We emphasize that, as shown in our ablation study, the opinions of ancillary LLMs, though weaker than the main LLM, expose it to diverse reasoning paths, including both correct and incorrect, ultimately improving its overall performance.\nMoO vs. Multi-Agent Methods. For this, we highlight the consensus within the community that, in multi-agent systems, each agent specializes in a specific sub-task, collaborating to achieve a larger goal. This contrasts with the main motivation behind our MoO framework. It is noteworthy that the closest work to ours is Mixture-of-Agents (MoA), which we used as a baseline for our experiments.\nMoO vs. Mixture-of-Experts (MoE). The main difference between MoO and MoE (Shazeer et al., 2017) lies in their motivations and architectures. In MoE, each expert is typically specialized in handling specific inputs or task components. In contrast, MoO focuses on improving performance during the post-training stage by curating a mixture of opinions from LLMs, all prompted with the same question.\nMoO vs. Distillation at Post-Training. In distillation techniques, a more capable teacher model trains a student model by transferring its knowledge. In contrast, our MoO framework enhances the reasoning ability of a main model (a more capable model) by post-training with a mixture of opinions (i.e., a fusion of different reasoning strategies) obtained from both ancillary models (weaker models) and the main model itself.\nMoO and Inference-Time Scaling. In Section 2, we presented MoO as a post-training approach to improve the reasoning of the main LLM. While the main LLM uses ancillary LLMs' opinions for predictions during inference, the primary performance improvement stems from post-training, especially when compared to ICL and MoA, as shown in Table 1. Importantly, we do not use majority voting or search techniques during inference. Instead, we construe that the main LLM learns to attend to correct reasoning strategies during post-training.\nFlexibility and Scalability. Although we focused on small to medium-sized LLMs, MoO is model-agnostic and works with models of any size, both open and closed, i.e., a larger LLM like GPT-4 can serve as the main model, while smaller LLMs such as GPT-3.5 act as ancillary models. This flexibility allows MoO to adapt to various scenarios, choosing LLMs based on task needs and available resources."}, {"title": "Conclusion and Future Work", "content": "We introduced the Mixture-of-Opinions (MoO) approach to enhance the performance of a stronger LLM (main LLM) by leveraging the reasoning capabilities of multiple weaker LLMs (ancillary LLMs). We evaluated MoO on mathematical reasoning benchmarks and showed that it outperforms various baselines.\nFor future work, we will consider rejection sampling to improve data curation. We also will compare MoO (equipped with medium to large-sized LLMs) against MoE models, e.g., Mixtral (Jiang et al., 2024), and post-training distillation works like Sky-T1 (Team, 2025). Additionally, we will enhance MoO with automated CoT generation methods, such as Auto-CoT (Zhang et al., 2022)."}, {"title": "Limitations", "content": "In this work, we applied our proposed method to models of size from 3B to 14B which are popular and feasible model sizes in the research community. We have not experimented with larger models, e.g., 70B-sized models. We primarily presented a feasible approach tested on representative LLMs such as Llama and Mistral; however, the conclusions drawn may not encompass all LLMs or use cases."}, {"title": "Ethics Statement", "content": "This work focused on fine-tuning small LLMs (referred to as the main model) with the fusion of opinions from weaker or equally matched LLMs (referred to as the ancillary models), to improve the main model's mathematical reasoning. It is important to note that while this study captures certain reasoning challenges, it may not reflect all real-world scenarios. Our findings underscore the ongoing complexity and difficulties of addressing smaller LLMs' reasoning abilities. We acknowledge that this study is not exhaustive, emphasizing the need for continued research to improve the reasoning capability of LLM-based methods."}, {"title": "Appendix", "content": "Related Works\nLLM Reasoning and Planning. Many efforts have been made to enhance LLMs' reasoning and planning capabilities, such as Chain of Thought (Wei et al., 2022; Kojima et al., 2022; Fu et al., 2022; Stechly et al., 2024), Tree-of-Thought (Yao et al., 2024), Graph-of-Thought (Yao et al., 2023), and Program-of-Thoughts (Chen et al., 2023). Some works propose verification steps to improve responses (Lightman et al., 2023; Cobbe et al., 2021b; Kumar et al., 2024), but empirical studies show that LLMs still struggle with complex reasoning and planning tasks (Chen et al., 2024; Xie et al., 2024a; Valmeekam et al., 2024; Mirzadeh et al., 2024; Wang et al., 2024b; Xie et al., 2024b).\nModel Ensemble. These methods aim to harness the strengths of multiple LLMs by combining their outputs, re-ranking results, or predicting the best-performing model for specific inputs. Techniques like GENFUSER (Jiang et al., 2023) and Mixture-of-Agents (MoA) (Wang et al., 2024a) focus on output fusion. Among them, MoA is the closest to our MoO, which uses an LLM-based aggregator and proposers in a forward layer-wise structure. However, MoA relies on very large LLMs with strong instruction-following capabilities, which are lacking in small to medium-sized LLMs.\nMulti-Agent Systems. Multi-agent collaboration methods use multiple task-specific LLMs (Guo et al., 2024; Wang et al., 2024c; Liu et al., 2024; Hong et al., 2023). These methods are computationally expensive and may not always outperform single-agent solutions with strong prompts.\nMixture-of-Experts (MoE). An MoE model, originally proposed by Shazeer et al. (2017), is a type of ensemble model where multiple \"experts\" are trained, and each expert specializes in different aspects of the input data. The key idea is that different experts handle different parts of the problem, and a \"gating\" network decides which expert (or combination of experts) to use for a given input. Our MoO paradigm is inspired by MoE, but it applies the concept at the data level.\nContext Enhancement for SFT. The works by Li et al. (2023) and Lee et al. (2023) incorporate Chain-of-Thought reasoning and detailed scratchpads into SFT for reasoning tasks. Our MoO approach falls under this category as it enhances"}, {"title": "Benchmarks: Details", "content": "We consider three widely used benchmarks for arithmetic and mathematical reasoning: GSM8K (Cobbe et al., 2021a), AQuA-RAT (Ling et al., 2017), and MATH (Hendrycks et al., 2021). These benchmarks provide Chain-of-Thought (CoT)-formatted answers, making them suitable for our study. These include mathematical problems ranging from elementary and high school to college levels, making them excellent testbeds for evaluating the effectiveness of our approach. The details of the benchmark are provided in Table 3."}, {"title": "Baselines: Details", "content": "Baseline 1. In-Context Learning (ICL) with Few-Shot Prompting the Off-The-Shelf LLMs. We first evaluate the performance of multiple representative open LLMs on the three benchmarks via in-context learning (ICL). We evaluate their performance against the test split with few-shots randomly drawn from the training set, where the number of shots varies in {1, 2, 4, 8, 16, 24, 32} as long as the model's context window permits. Similar to the previous studies, we witnessed that the performance initially increased with more shots but then stagnated to the point that adding more shots did not help. Here, we only report the best-performing ICL in the table (i.e., usually 8-shots is the best choice), leaving out the details for brevity.\nBaseline 2. Supervised Fine-tuning (SFT) the LLMs with the Original Training Set. We also include the SFT as a baseline. For each training sample, we concatenate the question and answer into a text and employ standard SFT\u00b9 . We use all training samples shown in Table 3."}, {"title": "Mixture of Agents (MoA) with Few-Shot Prompting", "content": "MoA (Wang et al., 2024a) leverages multiple LLMs to iteratively enhance the generation quality. Initially, LLMs in the first layer independently generate responses to a given prompt. These responses are then presented to the agents in the next layer (which may reuse a model from the first layer) for further refinement. This layer-wise refinement process continues for several cycles until obtaining a more accurate and comprehensive response. For fair experiments, instead of employing commercial LLMs such as GPT-4 Omni and large-size open LLMs such as Qwen1.5 110B Chat as in the MoA original work, here we employ open LLMs of relatively small size for MoA. That is, we set relatively weaker LLMs as proposers and stronger LLMs as the final aggregator, based on Table 1. We inject the same few-shots (as in Baseline 1) in the prompting of proposers and the final aggregator, instead of zero-shot in the original MoA work. We consider that because our earlier experiments showed that having few-shots was necessary for LLMs of sizes 3B to 14B as they are not good enough in following instructions and always need few-shots to generate outputs in the desired format. An example can be seen in Appendix A.6.3."}, {"title": "Fine-tuning Setup", "content": "It is worth mentioning that we used the instruct versions of LLMs with 4-bit quantization\u00b2, for less memory occupation and faster inference speed. We fine-tune the main LLMs such as Llama-3.1-8B (Instruct) for 5 epochs with a batch size of 1 and gradient accumulation steps of 4. We use a constant scheduler learning rate of $5 \u00d7 10^{-5}$ and no warm-up. We also disable packing among training samples to avoid cross-contamination. For a fair comparison, we train the model in 4-bit as we use 4-bit quantized models for ICL. The maximum sequence length is set to 4,096 as a balance between computation efficiency and context coverage length. For efficient fine-tuning, we also employ Low-Rank Adaptation with r = 16 and alpha = 16, instead of full-parameter training. Finally, all our experiments are conducted on NVIDIA A10 GPUs."}, {"title": "Inference or Response Generation Setup", "content": "For inference, we allow the models to generate until either their context size limit is reached, or one of the end-of-response tokens such as </s> and <|endoftext|>, or QUESTION: and SOLUTION:\u00b3 is generated. Most of the time, the responses (either for opinion or CoT answer) are shorter than 500 tokens and will not reach the context size limit. For all experiments, we use greedy decoding to generate responses. Our preliminary experiments showed that this is the most reliable setting compared to a setting with a temperature set to a value greater than zero. Technically, if sufficient hardware is available, the opinion generation stage could be done in parallel which in return saves time."}]}