{"title": "Inference Scaling FLaws: The Limits of LLM Resampling with Imperfect Verifiers", "authors": ["Benedikt Stroebl", "Sayash Kapoor", "Arvind Narayanan"], "abstract": "Recent research has generated hope that inference scaling could allow weaker language models to match or exceed the accuracy of stronger models, such as by repeatedly sampling solutions to a coding problem until it passes unit tests. The central thesis of this paper is that there is no free lunch for inference scaling: indefinite accuracy improvement through resampling can only be realized if the \"verifier\" (in this case, a set of unit tests) is perfect. When the verifier is imperfect, as it almost always is in domains such as reasoning or coding (for example, unit tests have imperfect coverage), there is a nonzero probability of false positives: incorrect solutions that pass the verifier. Resampling cannot decrease this probability, so it imposes an upper bound to the accuracy of resampling-based inference scaling even with an infinite compute budget. We find that there is a very strong correlation between the model's single-sample accuracy (i.e. accuracy without unit tests) and its false positive rate on coding benchmarks HumanEval and MBPP, whose unit tests have limited coverage. Therefore, no amount of inference scaling of weaker models can enable them to match the single-sample accuracy of a sufficiently strong model (Fig. 1a). When we consider that false positives have a negative utility compared to abstaining from producing a solution, it bends the inference scaling curve further downward. Empirically, we find that the optimal number of samples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we show that beyond accuracy, false positives may have other undesirable qualities, such as poor adherence to coding style conventions.", "sections": [{"title": "1 Introduction", "content": "Scaling the amount of compute used during inference is a promising way to improve LLM performance. Techniques include reasoning (69; 71; 53), reflecting on model outputs to revise candidate solutions (55; 80), and compositions of these and other atomic techniques (51; 70).\nInference scaling through resampling stands out for its simplicity and broad applicability. It works by generating many candidate outputs until one is satisfactory, based on feedback from a verifier (58; 49; 7; 22; 36). Unlike techniques such as majority voting where gains from inference scaling quickly plateau (Table 1), resampling has given rise to the hope of usefully scaling inference compute by many orders of magnitude.\nWe provide evidence that tempers this assumption. Our key concern is that the generalization gap \\u2014 where a model performs well on benchmarks but fails to generalize to the real-world is amplified when using repeated sampling to lift the performance of weaker models.\nSpecifically, we study the use of unit tests as verifiers for coding benchmarks, to see if inference scaling for less capable models allows us to match the accuracy of more capable models. We make the following contributions.\nReview of inference scaling techniques and their limitations (Section 2). We review papers on inference scaling, categorizing the primary techniques and listing their domain-specific applications and known limitations.\nDemonstration of generalization gap (Section 3). We provide empirical evidence on two benchmarks, HumanEval+ and MBPP+ (42), showing that the apparent gains from resampling with imperfect verifiers are unlikely to translate into real-world performance. Despite achieving comparable results to stronger models on standard unit tests, less capable models suffer from a larger generalization gap-producing incorrect solutions that fail the extended test suite (false positives) at higher rates than stronger models.\nIn particular, we observe that even if given an infinite inference budget, in many cases a weaker model cannot match the performance of a single invocation of a sufficiently strong model.\nEmpirical analysis to understand the limitations of inference scaling with imperfect verifiers (Section 4). We examine how introducing a cost (negative utility) for returning false positives impacts the optimal number of resampling attempts on HumanEval+. We find that even with an infinite inference budget, the optimal number of samples is often finite and very low (e.g., K\u2264 5 in Fig. 4). Hence, resampling quickly reaches a point of diminishing returns without bridging the performance gap for smaller models. If the cost of an incorrect solution is higher than the benefit of a correct solution, the optimal K can be zero the risk of a false positive for a weak model is high enough that it is effectively useless (Fig. 4). In Appendix A.3 we present a theoretical model that complements the findings in this section.\nEvidence that this affects code quality beyond correctness (Section 5). We show that the reliance on imperfect verifiers not only affects the functional correctness but also overall quality of the generated code. We evaluate candidate solutions on HumanEval+ based on various readability metrics such as adherence to naming conventions (that we specify in the prompt) like snake_case and camelCase, line-level commenting, and guidelines regarding the maximum line length and number of lines in function implementations. We find that false positive solutions are lower quality across all models and metrics when compared to true positive solutions. While other aspects of code quality such as simplicity and modularity are harder to test automatically, we speculate that the same pattern holds for those properties as well.\nWe also conduct a qualitative analysis to identify recurring error types causing a larger generalization gap for weaker models."}, {"title": "2 Scaling inference compute with verifiers", "content": "Table 1 provides an overview of the main techniques for scaling inference compute with LLMs. Some methods such as majority voting (68; 10) or resampling using verifiers (7; 72) generate many candidate solutions and then select one. Other methods such as reasoning (69) and critique (55; 47) refine a single solution. In practice, these methods can be combined in flexible ways and the distinction between them is not always clear (Appendix A.1.1). Note that our notion of inference scaling excludes methods such as those used to train OpenAI's ol series of models, since we are only looking at improvements during inference time to available language models, rather than training improvements.\nAll these methods except verifier-based resampling are known to have important limitations that cast doubt on how much scaling is truly possible, as summarized in Table 1. Resampling using verifiers has a different control flow than other methods, which gives it an intuitive appeal (Fig. 2): we can potentially regenerate solutions indefinitely until one is correct. This enthusiasm around resampling is partly driven by the empirically observed inference scaling laws, which suggest that the fraction of tasks for which we find at least one correct solution scales predictably with the number of samples over multiple orders of magnitude (7)."}, {"title": "3 Repeated sampling with weaker models leads to worse generalizability", "content": "In computer programming tasks, unit tests are commonly employed as verifiers to assess the correctness of candidate solutions generated by language models. While unit tests are practical and efficient, they often suffer from imperfect test coverage, leading to false positives where incorrect solutions pass the tests (20). This affects many benchmarks such as HumanEval (11), APPS (24), or MBPP (3). This imperfection raises the question: Do less capable models produce false positives implementations that pass the standard unit tests but fail the comprehensive ones at a higher rate than stronger models?\nExperimental setup. To investigate this, we conducted experiments on two widely used coding benchmarks: HumanEval+ and MBPP+. MBPP consists of simple programming tasks designed to evaluate the basic coding abilities of models (3). HumanEval+ and MBPP+ are extensions of the original HumanEval and MBPP benchmarks (42) and contain additional hidden test cases to assess correctness beyond the unit tests included in the original benchmarks.\nWe evaluated multiple models of varying capabilities, including weaker and stronger models, generating at least 50 samples for each model and benchmark task. We used the standard unit tests provided with the benchmarks as imperfect verifiers to filter the candidate solutions. To assess the generalization gap, we then evaluated solutions that passed the original benchmark test sets on the more comprehensive hidden test cases (see Appendix A.5 for details). These tests are extensive, and we assume that solutions that pass the full set of tests are correct. (If this assumption is not true, the generalization gaps that we reveal only grow bigger.)\nFindings. Weaker models exhibit a higher probability of producing false positives compared to stronger models (Fig. 3). The false positive probability scales inversely with the true capability. This linear relationship holds with remarkable consistency across models of various model families, including Cohere's Command models, GPT-40, and the Llama 3.1 family. This suggests that while weaker models appear to perform well on standard benchmarks through increased sampling, they fail to generalize effectively and, importantly, they generalize worse than more capable models. They tend to generate fragile solutions that exploit the limitations of the unit tests. We speculate that this is because weaker models' \"true understanding\" of the programming tasks is worse.\nThe empirical results reinforce a core insight. Suppose $P_{strong}(Correct) > P_{weak}(Correct | Pass Verifier)$. That is, the single-sample accuracy of a strong model ex-"}, {"title": "4 How many samples are optimal?", "content": "In the previous section we looked at the behavior of resampling in the limit as the number of samples grows large. Now we look at inference scaling curves, which allow us to study how accuracy varies as a function of the number of samples.\nWe add one important detail: we model the cost of false positives, such as code that passes unit tests but has subtle bugs. The cost of bugs (which might result in buggy software being deployed) is not easily comparable to the labor-saving benefit of correct solutions, and this cost-benefit ratio can vary greatly depending on the application. So we consider many possible values for the cost-benefit ratio, including zero, which is the setting considered in previous work on inference scaling. The ratio can potentially be much higher than 1 in some applications, such as security sensitive ones, since bugs might translate to exploitable software vulnerabilities.\nExperimental setup. For each model of interest, we generated 200 samples for each task in the HumanEval benchmark. For each K < 200, If a passing solution was found within K samples, we assigned rewards based on the outcome: a true positive yielded a benefit of 1, while a false positive incurred a cost, with values set according to different cost-benefit ratios: 0, 1, 2, 4, or 8 (Fig. 4). If"}, {"title": "5 False positive solutions are low-quality even beyond correctness", "content": "While correctness is a fundamental criterion for evaluating code generated by LLMs, it is not the only determinant of code quality. High-quality code possesses attributes beyond mere functionality, such as readability, maintainability, and efficiency. Readability simplifies error-checking and is considered one of the most useful properties of high-quality code (8). It can be measured using various metrics, including code length guidelines (e.g., PEP8), adherence to naming conventions like snake_case or camelCase, and consistent commenting (79). Intuitively, shorter code with clear variable names and informative comments is generally easier to read and maintain.\nTo understand the relationship between imperfect verifiers and code quality, we evaluated the readability of code generated by various models in our setup.\nExperimental setup. In our experiments on HumanEval, we evaluate the readability scores of candidate solutions that pass standard unit tests and the more comprehensive test suite. For each measure of code readability, we use a different prompt instructing the model to adhere to the desired guidelines (see Appendix A.5 for detailed prompts). We rely on the prompts and implementation from Zheng et al. (79).\nThe results show notable differences in code quality between false positives and robust implementations. False positives, passing only the standard but not the extended unit tests, tend to have worse code quality across all metrics (see Fig. 7). This trend is consistent across models of varying capabilities. This suggests that the limitations of imperfect verifiers for coding tasks extend beyond correctness issues but also affect other code characteristics important for software development. This affects weaker models more, given that they are more prone to generate false positives.\nAn open question arising from our findings is whether fine-tuning LLMs on code quality metrics could improve not only the quality of generated code but also robustness (29), potentially mitigating the prevalence of false positives."}, {"title": "6 Discussion", "content": "We study a setting where all generators are paired with the same verifier. The verifier has imperfect coverage, but no false negatives (i.e. it doesn't reject correct solutions). In real-world deployment settings, human-written unit tests are rarely available and we would need to rely on the use of automated test generation techniques. These approaches include symbolic execution (46), specialized transformers (62), and LLMs (12; 9; 56). Model-generated tests introduce new challenges including a disparity between verifiers and a risk of false negatives. This could widen the generalization gap. Recent work has sought to improve test quality by increasing coverage (52; 1), identifying edge cases (46), or reproducing user-specific bugs (31; 43). We leave an investigation of the impact of model-generated unit tests as a next step.\nOur findings weaken the support for our previous paper's claim that evaluations must be cost-controlled in order to be useful (32); here, even when disregarding computational cost, resampling with imperfect verifiers is inherently limited.\nLimitations. Our experiments focus solely on repeated sampling in the context of coding tasks. While this domain offers a clear example of the challenges posed by imperfect verifiers, other domains might exhibit different behavior. Future work could extend these findings to tasks such as reasoning (25), web agents (5; 23), or agent-user interaction (76). Another limitation is prompt sensitivity, which affects LLM evaluations (6; 38). While we followed the original authors' implementation provided with the HumanEval+ and MBPP+ benchmarks (42), prompt engineering could influence false positive generation. Additionally, we did not investigate how benchmark contamination contributes to our findings, as models could be overly optimized for passing the standard test cases. Finally, we did not explore mitigation strategies such as training models to improve robustness against verifier weaknesses or refining solutions after they passed the verifier (51). Similarly, we did not test alternative strategies to inference scaling that, e.g., induce more diversity during sampling, such as PlanSearch (65)."}, {"title": "A Appendix", "content": "A.1 Additional details on Section 2\nA.1.1 Edge cases in our generator-verifier setting\nThe setting described in Fig. 2 considers verifiers and generators as distinct components, where verifiers score and accept or reject individual samples from the generator's output to enable accuracy improvements through resampling.\nThis creates interesting edge cases with methods like Chain-of-Verification (CoVe) (17), Tree of Thoughts (ToT) (75), and Reward Models (RMs) (57) where verification and generation are more tightly coupled. While ToT fits within our framework by producing aggregate scores and a decision on whether the problem is solvable from a given state or not (i.e. potentially rejecting solutions), CoVe differs fundamentally in its verification approach. Instead of producing numeric scores and accepting or rejecting solution candidates, CoVe uses verification of intermediate facts used for answering a question to improve a single response through iterative refinement. This makes CoVe less suitable for inference scaling through resampling because there is no way to distinguish between the quality of multiple samples and using a verifier's verdict for resampling."}, {"title": "A.2 Additional details on Section 3", "content": "Sample Collection. To evaluate the generalization gap between weaker and stronger models, we collected multiple samples per model and task. For both benchmarks and each model, we used samples generated with a temperature setting of 0.8. For sample generation, we use the implementation provided by Liu et al. (42), and other than the temperature use their default settings. We collected a minimum of 50 samples for each model and task. For most models in our experiments Vicuna 7B, Mistral 7B, CodeT5p 16B, CodeGen, CodeGen2, Code Llama 7B, and Code Llama 13B, we used samples made available by Liu et al. (42). These were collected using the same temperature setting (i.e., 0.8). We had access to 200 samples per model and task for these models. Additionally, on HumanEval+, we collected 200 samples for Llama 3.1, Phi-3, GPT-40 and the Command family of models. For Command-Light, we even collected 1000 samples for each task to reduce the number of tasks without any solutions passing the HumanEval unit tests (Fig. 13). On MBPP+, we collected 50 samples for each model and task.\nA.2.2 Additional details on excluded tasks from MBPP+\nTasks excluded by original EvalPlus authors (21 tasks). These exclusions are based on an update to MBPP+, during which the authors removed several broken tasks, reducing the total to 378 tasks. These tasks were excluded because of issues with the oracle implementation leading to unreliable evaluations .\nIn addition to the task excluded by the MBPP+ creators, in our evaluations, we excluded a total of 57 tasks from the benchmark for two main reasons:"}, {"title": "A.3 Additional details on Section 4", "content": "In addition to the empirical analysis presented in Section 4, in this appendix, we provide a theoretical model that formalizes the limitations of inference scaling with imperfect verifiers and generalizes our findings to other benchmarks. We build on the verifier-based judge setup introduced by Davis et al. (16). We provide a Python notebook with the implementation of our model\nEven with zero computational cost, the optimal number of samples is finite and very low (K \u2264 3). For this plot, we set the parameters as empirically observed for Llama 3.1 8B on HumanEval (see Table 3 for the exact values). The left plot shows the expected value of generating additional candidate solutions as a function of the number of attempts K for various cost-benefit ratios. For all cost-benefit ratios, the expected value peaks at very low K, after which it begins to decline, indicating negative returns from additional sampling. The right depicts the probabilities of generating a correct solution vs. a false positive at each step K. As K increases, the likelihood of generating a correct solution decreases, while the probability of generating a false positive increases. There is a trade-off between continued sampling and increasing risk, emphasizing the limitations of scaling inference compute with imperfect verifiers. Note that when setting the cost of a false positive to be 10 times higher than the benefit of a true positive, the optimal number of samples becomes K = 0 (Fig. 15)."}, {"title": "Model setup", "content": "The underlying model consists of two components:\nGenerator: Produces candidate solutions to a task, with different success probabilities based on task difficulty.\nTasks are either easy ($T_1$) or hard ($T_2$), with prior probabilities $p_1$ and $p_2$ respectively.\nThe probabilities of generating a correct solution are $r_1$ for easy tasks and $r_2$ for hard tasks, so $r_1 > r_2$.\nVerifier: An imperfect verifier checks the correctness of generated solutions.\nCompleteness (c): Conditional probability of accepting a correct solution.\nSoundness (s): Conditional probability of rejecting an incorrect solution."}, {"title": "Probability of rejection", "content": "The probability that a sample is being rejected by the verifier, denoted $\u00df_i$, is given by:\n$\u00df_i = (1 - c)r_i + s(1 - r_i)$ (1)\nwhere i = 1 for easy tasks and i = 2 for hard tasks. These probabilities ($\u03b2_1$ and $\u03b2_2$) determine how likely a generated solution is to be rejected depending on the task type."}, {"title": "Belief updates", "content": "After each rejection, the belief that the task is of type T2 (hard) increases. The posterior probability that the task is of type $T_1$ or $T_2$ after k rejections is:\n$P_{T_2}^{(k)} = \\frac{\u00df^{k-1}p_1}{\u00df^{k-1}p_1+ \u00df^{k-1}p_2}$ (2)\nAs more rejections occur, it usually becomes more likely that the task is hard (T2). In Fig. 14, we see how the belief that the task is easy decreases, while the belief that the task is hard increases as the number of attempts K grows."}, {"title": "Probability of correct and false positive solutions", "content": "For the k-th attempt, the probability of generating a correct solution or a false positive depends on the task type. The overall probabilities are weighted by the posterior beliefs $P_{T_i}^{(k)}$.\nThe belief-weighted probability of returning a correct or false positive at attempt k, conditional on the k - 1 previous attempts being rejected are:\n$P_{TP}^{(k)} = P_{T_1}^{(k)} . P_{TP,T1} + P_{T_2}^{(k)} . P_{TP,T2}$ (3)\n$P_{FP}^{(k)} = P_{T_1}^{(k)} . P_{FP,T1} + P_{T_2}^{(k)} . P_{FP,T2}$ (4)\nwhere:\n$P_{TP,T_1} = c.r_1$, $P_{TP,T_2} = c.r_2$\n$P_{FP,T_1} = (1 - r_1) \u00b7 (1 \u2212 s)$, $P_{FP,T_2} = (1 - r_2) \u00b7 (1 \u2212 s)$\nIn Fig. 14, the right plot shows the evolution of $P_{TP}$ and $P_{FP}$ as the number of attempts K increases. Initially, the probability of generating a correct solution is higher, but for higher K, the probability of generating a false positive increases."}, {"title": "Expected value of generating additional solutions", "content": "The expected value of generating a solution at the k-th attempt is:\n$EV_k = [V_{TP} P_{TP}^{(k)} + V_{FP} P_{FP}^{(k)}] . \\beta_i^{k-1} . [p_{T_1}^{(k)} + p_{T_2}^{(k)}]$ (5)\nwhere:\nVTP is the benefit for a correct solution.\nVFP is the cost for a false positive being \"accepted\" as the solution."}, {"title": "Optimal number of attempts", "content": "The total expected value after K attempts is:\n$Reward = \\sum_{k=1}^{K} EV_k$ (6)\nThe optimal number of attempts, Kopt, is the value of K that maximizes the reward, which are shown across models and for various VFP/VTP-ratios in Fig. 15."}, {"title": "A.4 Inference scaling curve for GPT-40", "content": "Figure 16: Inference scaling curves in the presence of a cost for GPT-40. In addition to the models in Fig. 1, we provide the inference scaling curves for GPT-40 as the model with the highest single-sample accuracy on on HumanEval+ in our experiments. We find that the benefits of search are minimal (i.e. curves are flat) in line with what we expect from the empirical task difficulty distribution shown in Fig. 10."}, {"title": "A.5 Additional details on Section 5", "content": "A.5.1 Details on data and implementation\nWe used the implementation provided by Zheng et al. (79) to collect samples and evaluate the different code readability metrics. Each code quality metric had a separate prompt instructing the model to follow certain guidelines (Appendix A.5.2). For each model and code quality instruction, we generated 50 samples per task on HumanEval+. As for our main experiments, we set the temperature to 0.8. All other parameters were set to their default value as provided with the implementation."}, {"title": "A.5.2 Prompt examples for readability metrics", "content": "1) Naming conventions\nPlease generate the Python code to solve the following problem, and use camelCase for both function names and variable names.\\n\\nProblem:\\n\\n{problem}\nPlease generate the Python code to solve the following problem, and use snake_case for both function names and variable names.\\n\\nProblem:\\n\\n{problem}\n2) Code length\nPlease generate the Python code to solve the following problem, where each line is less than 70 characters long and each function is less than 30 lines long.\\n\\nProblem:\\n\\n{problem}\n3) Commenting guidelines\nPlease generate the Python code to solve the following problem, and add comments for each line in each function.\\n\\nProblem:\\n\\n{problem}"}, {"title": "A.5.3 Qualitative examples of false positives", "content": ""}, {"title": "B Reproducibility statement", "content": "We release code to reproduce all experimental results of this paper in a GitHub repository5. This repository also contains all code samples for all models used in our experiments. We also provide an implementation of the theoretical model in Appendix A.3 as a Python notebook6."}]}