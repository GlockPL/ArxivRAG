{"title": "AFFAKT: A Hierarchical Optimal Transport based Method for Affective Facial Knowledge Transfer in Video Deception Detection", "authors": ["Zihan Ji", "Xuetao Tian", "Ye Liu"], "abstract": "The scarcity of high-quality large-scale labeled datasets poses a huge challenge for employing deep learning models in video deception detection. To address this issue, inspired by the psychological theory on the relation between deception and expressions, we propose a novel method called AFFAKT in this paper, which enhances the classification performance by transferring useful and correlated knowledge from a large facial expression dataset. Two key challenges in knowledge transfer arise: 1) how much knowledge of facial expression data should be transferred and 2) how to effectively leverage transferred knowledge for the deception classification model during inference. Specifically, the optimal relation mapping between facial expression classes and deception samples is firstly quantified using proposed H-OTKT module and then transfers knowledge from the facial expression dataset to deception samples. Moreover, a correlation prototype within another proposed module SRKB is well designed to retain the invariant correlations between facial expression classes and deception classes through momentum updating. During inference, the transferred knowledge is fine-tuned with the correlation prototype using a sample-specific re-weighting strategy. Experimental results on two deception detection datasets demonstrate the superior performance of our proposed method. The interpretability study reveals high associations between deception and negative affections, which coincides with the theory in psychology.", "sections": [{"title": "Introduction", "content": "Video deception detection has attracted a huge interest in various fields including law enforcement, jurisprudence, national security, business and interviewing (Chebbi and Jebara 2023). In the earlier days, researchers proposed several statistic-based methods (Jaiswal, Tabibu, and Bajpai 2016; Rill-Garc\u00eda et al. 2019; Mathur and Matari\u0107 2020), which utilize facial features, such as OpenFace (Baltrusaitis et al. 2018), action units (Jaiswal, Tabibu, and Bajpai 2016) and face landmarks (Rill-Garc\u00eda et al. 2019), to achieve classification by applying machine learning methods on facial features with statistically significant differences. The performance of these methods heavily relies on the expert knowledge to construct and select valid feature sets. Hereafter, several deep learning based"}, {"title": "Related Work", "content": "Video Deception Detection. Video deception detection is one of the main tasks in affective computing and psychological researches (Krishnamurthy et al. 2018; P\u00e9rez-Rosas et al. 2015; Borza, Itu, and Danescu 2018). Earlier methods relied on manually selected statistical features including OpenFace (Jaiswal, Tabibu, and Bajpai 2016; Rill-Garc\u00eda et al. 2019), action units (Jaiswal, Tabibu, and Bajpai 2016; Avola et al. 2019), or gestures (\u015een et al. 2020) to achieve deception detection. Recently, the powerful representation capability of\nOptimal Transport. Optimal transport (OT) (Peyr\u00e9 and Cuturi 2019) is a mathematical framework, seeking the most efficient way of transporting one distribution of mass into another. Let $p = \\sum_{i=1}^{n} a_i dx_{A_i}$ and $q = \\sum_{j=1}^{m} b_j dx_{B_j}$; be n and m dimensional discrete probability distributions for two finite sets $X_A = \\{X_{A_i}\\}_{i=1}^n$, $X_B = \\{X_{B_j}\\}_{j=1}^m$ respectively, where $a \\in \\Delta_n$ and $b \\in \\Delta_m$, $\\Delta_n$ and $\\Delta_m$ are the probability simplex of $\\mathbb{R}^n$ and $\\mathbb{R}^m$, and $dx_{*}$ refers to a point mass located at coordinate $X_{*} \\in \\mathbb{R}^d$. Denoting $M \\in \\mathbb{R}^{n \\times m}$ as the cost matrix with $M_{i,j} = M(X_{A_i}, X_{B_j})$, which means the cost to transport one unit of mass between elements of the sets. Then, the transport plan matrix T is obtained by solving:\n$\\begin{equation} \\text{OT}(p, q) = \\min_{T\\in \\Pi(p,q)} <T, M >_F  \\tag{1} \\end{equation}$\nwhere $<, >_F$ is the Frobenius dot-product. The constrain $\\Pi(p,q) := \\{T\\in \\mathbb{R}_+^{n \\times m}| \\sum_{i=1}^n T_{i,j} = b_j, \\sum_{j=1}^m T_{i,j} = a_i\\}$ enforces T to have p, q as its marginals. It should be noted that T can be interpreted as the probabilistic correspondence between the elements of p and q. If the transport cost $M_{i,j}$ between $X_{A_i}$ and $X_{B_j}$ is high, then a low correlation $T_{i,j}$ should be obtained. Eq. (1) is a linear assignment problem, which is expensive to solve. Fortunately, a entropy regularized OT has been developed as follows:\n$\\begin{equation} \\text{OT}(p, q) = \\min_{T\\in \\Pi(p,q)} <T, M >_F -\\epsilon H(T)  \\tag{2} \\end{equation}$\nHere, $H(T) = -T\\log T$ is the entropic regularization. Eq. (2) can be solved by the Sinkhorn algorithm efficiently (Cuturi 2013).\nHierarchical Optimal Transport. Hierarchical optimal transport usually contains high-level and low-level OT, where high-level OT learn the optimal transport plan with a given cost matrix, and the given cost matrix depends on the solution of low-level OT. Recently, H-OT has been recently studied for various tasks including multimodal distribution alignment (Lee et al. 2019), few-shot learning (Guo et al. 2022). For example, in (Lee et al. 2019), H-OT was used to leverage cluster structure in data to improve alignment in noisy, ambiguous, or multimodal settings. (Guo et al. 2022) proposed a novel distribution calibration method for few-show learning, where an adaptive weight matrix representing the relations between the base classes and novel samples is computed by hierarchical optimal transport."}, {"title": "The Proposed Method", "content": "Our proposed method AFFAKT for video deception detection contains four modules shown in Fig. 2 (b): (1) Encoder layer, (2) Hierarchical Optimal Transport Knowledge Tranfer (H-OTKT) module, (3) Classification layer, and (4) Sample-specific Re-weighting Knowledge Bank (SRKB) module. Assuming a video deception detection dataset is denoted as $D = \\{(V_i,y_i)|V_i \\in \\mathbb{R}^{F \\times 3 \\times H \\times W},y_i \\in \\{1, ..., L_t\\}\\}_{i=1}^{N_t}$, where $(V_i, y_i)$ is the i-th video sample and its ground truth label, $N_t$ represents the number of samples in D, F is the number of video frames, H and W are the height and width of video frames, $L_t$ is the number of target categories (i.e., deceptive and truthful). Our idea is to improve the classification performance on $D^t$ by transferring useful and correlated knowledge from a large-scale VFER dataset $D^s = \\{(V_j, y_j)\\}_{j=1}^{N_s}$, where $y_j \\in \\{1, ..., L_s\\}$, $L_s$ is the number of categories in $D^s$, and $N_s$ is the number of samples in $D^s$. Each category stands for one expression. In order to utilize $D^s$ efficiently, a pre-trained encoder G is firstly employed to extract VFER feature representation $X^s \\in \\mathbb{R}^{N_s \\times d}$ in advance, i.e., $X^s = G(V^s)$, where d is the embedding dimension. By grouping $X^s$ with the ground truth labels, $X^s = \\{X^{s,k} \\in \\mathbb{R}^{J_k \\times d}\\}_{k=1}^{L_s}$ with $\\sum_{k=1}^{L_s} J_k = N^s$, where $X^{s,k}$ stands for feature embeddings of $J_k$ samples in the k-th class. This process is shown in Fig. 2 (a). Pseudo-code of AFFAKT, all the symbol notations and their descriptions used in this paper are summarized in appendix.\nEncoder Layer\nFor a video deception detection dataset, widely-used video and audio pre-trained models E, i.e., VideoMAE (Tong et al.\nHierarchical Optimal Transport Knowledge Transfer (H-OTKT)\nAs presented before, we aim to improve the classification performance by transferring VFER domain knowledge $X^s$ to target deception domain. Since the label space and the distribution of two domains in the feature space are different, one of the key questions is how much knowledge of facial expression data should be transferred. Based on hierarchical optimal transport, we propose H-OT Knowledge Transfer (H-OTKT) module with high-level and low-level OT illustrated in Fig. 3.\nIn particular, high-level OT learn the optimal correlation between classes of VFER dataset and samples of deception dataset with a given cost matrix, where the cost matrix depends on the total low-level OT distance between each target deception sample and all samples from each class of VFER dataset.\n$X_t$ is firstly mapped into $X'_t = F_1(X_t) \\in \\mathbb{R}^{n \\times d}$ by an MLP $F_1$, such that the feature spaces between source and target domain could be the same, where n is the batch size. Let $\\Omega = \\sum_{k=1}^{L_s} \\delta_{Q_k}$ as the discrete uniform distribution over $L_s$ classes of VFER dataset, $Q_k$ is the representation vector of k-th class. And $P = \\sum_{i=1}^{n} \\delta_{X_i}$ as the discrete uniform distribution over n target deception samples. Then, according to Eq. (2), the entropic regularized OT between P and Q is:\n$\\begin{equation} \\text{OT}_{high} (P, Q) = \\min_{T\\in \\Pi(P,\\Omega)} <T, M >_F -\\epsilon H(T)  \\tag{3} \\end{equation}$"}, {"title": "", "content": "where $T \\in \\mathbb{R}^{n \\times L_s}$ and $M \\in \\mathbb{R}^{n \\times L_s}$ are the transport plan and the cost matrix between facial expression classes and target deception samples. Each element $T_{i,k}$ indicates the importance of the k-th class in VFER dataset for the i-th sample in deception mini-batch, determining which class and how much of knowledge should be transferred. Besides, T should satisfy the following constraint:\n$\\begin{equation} \\Pi(P, Q) := \\{ \\sum_{i=1}^{n} T_{i,k} = \\frac{1}{L_s}, \\sum_{k=1}^{L_s} T_{i,k} = \\frac{1}{n}\\}.  \\tag{4} \\end{equation}$\nIt is apparently that the solution T relies on the cost matrix M, simply applying cosine similarity with the features of samples from deception mini-batch and the mean of features from each class of VFER dataset may lead to sub-optimal solution. Moreover, the contribution of different samples in each class may be various. So, we utilize another optimal transport formulation to obtain the optimal M. According to (Guo et al. 2022), the empirical distribution of the k-th class is expressed as $Q_k = \\sum_{j=1}^{J_k} \\delta_{X^{s,k}_j} \\rho^k_j$, where the importance $\\rho^k_j$ of the j-th sample in the k-th source class is obtained by logistic regression score. Therefore, a low-level entropic regularized OT is further defined as follows:\n$\\begin{equation} \\text{OT}_{low} (P, Q) = \\min_{T^{low,k} \\in \\Pi(P,Q_k)} < T^{low,k}, M^{low,k} >_F - \\epsilon H(T^{low,k})  \\tag{5} \\end{equation}$\n$\\Pi(P, Q_k) := \\{ \\sum_{i=1}^{J_k} T^{low,k}_{i,j} = \\frac{1}{J_k} , \\sum_{j=1}^{J_k} T^{low,k}_{i,j} = \\frac{1}{n} \\}$ is the constrain, and $T^{low,k}$ is the transport plan between each sample in mini-batch and samples in the k-th source domain class. $M^{low,k} \\in \\mathbb{R}^{n \\times J_k}$ is determined by cosine similarity, i.e., $M^{low,k}_{i,j} = 1 - \\text{cos}(X'_i, X^{s,k}_j)$. The cost matrix M in high-level OT of Eq. (3) will be replaced by the total OT distance between each target deception sample and all sample in each class of VFER dataset, i.e., $M_{:,k} =< T^{low,k}, M^{low,k} >_F$.\nFor the optimization, both Eq. (5) and Eq. (3) are solved by Sinkhorn algorithm (Cuturi 2013) hierarchically. Using the OT distance calculated from low-level OT as the cost M of high-level OT adaptively, H-OTKT is able to obtain the transport weight T between deception samples and facial expression classes, which is the potential correlation mapping of facial expression classes for target samples.\nOnce we obtained correlation mapping T by solving Eq. (3), knowledge transformation can be performed. For each sample in deception domain, more knowledge from highly associated classes should be transferred, while knowledge from uncorrelated classes should not be transferred. To realize it, the transferred knowledge $X^{trans} \\in \\mathbb{R}^{n \\times d}$ is represented as follows:\n$\\begin{equation} X^{trans} = F_2 ( [\\sum_{k=1}^{L_s} T_{i,k} (\\frac{1}{J_k} \\sum_{j=1}^{J_k} X^{s,k}_j)] ); i = 1,..., n  \\tag{6} \\end{equation}$\nwhere $\\frac{1}{J_k} \\sum_{j=1}^{J_k} X^{s,k}_j$ denotes the average feature of samples belonging to the k-th class in source domain; $T_{i,k}$ quantifies the correlation weight between the k-th source class and i-th deception sample; n. is used for scaling due to the constraint in high-level OT. And $F_2$ is an MLP. In order to integrate the"}, {"title": "", "content": "transferred knowledge $X^{trans}$ with features $X'_t$ extracted from target samples, the fused representation of deception detection samples are calculated as:\n$\\begin{equation} X^{fused} = \\xi' X^{trans} + (1 - \\xi') X'_t  \\tag{7} \\end{equation}$\nwhere $\\xi'$ is the weight of transferred feature $X^{trans}$. Since it's hard to learn excellent $X'_t$ at the beginning of the training phase, a curriculum learning strategy (Kumar, Packer, and Koller 2010; Wang, Chen, and Zhu 2021) is adopted as $\\xi' = \\frac{1}{2}(1-\\text{cos} (\\frac{\\pi e}{N_e}))$, where e is the current training epoch number and $N_e$ is the total training epoch number. As $\\xi'$ is gradually increased, a better $X'_t$ is gained for H-OTKT.\nClassification Layer\nThe final classification layer contains one MLP with softmax, which takes $X^{fused}$ as input and outputs the predicted label $\\hat{y} \\in \\mathbb{R}^{n \\times L_t}$:\n$\\begin{equation} \\hat{y} = F_3(X^{fused})  \\tag{8} \\end{equation}$\nHere, $F_3$ is the MLP classifier. With ground truth label $y_t = [y_1, ..., y_n]$, the classification loss function is formulated as:\n$\\begin{equation} L_{ce}(\\hat{y}, Y) = -E_{y_t} [\\text{log }\\hat{y}]  \\tag{9} \\end{equation}$\nwhere E is expectation. To reduce the difference between distribution spaces from source and target domain in H-OTKT, and further improve the final prediction, another loss function is defined based on the Sinkhorn divergence (Feydy et al. 2019) to obtain the space discrepancy between class average of $X^s$ and $X'_t$ (Nguyen and Luu 2022):\n$\\begin{equation} L_{ot}(X'_t, X^s) = d_{sor}(P, Q) - \\frac{1}{2} d_{sor}(P, P) - d_{sor}(Q, Q)  \\tag{10} \\end{equation}$\nwhere $d_{sor}(\\cdot, \\cdot)$ is the total OT cost between two distributions solved by the regular OT (Eq. (1)) with cosine similarity as cost function. Then the total loss function is formulated as:\n$\\begin{equation} L = L_{ce} + \\eta L_{ot}  \\tag{11} \\end{equation}$\nIn Eq. (11), the $L_{ce}$ term optimizes the whole network to improve the classification performance while the $L_{ot}$ term is used for reducing the discrepance between the source feature space and the target feature space.\nSample-specific Re-weighting Knowledge Bank (SRKB) Module\nOptimal knowledge from proper classes in facial expression dataset has been obtained by H-OTKT. Empirically, samples in deception dataset have varying semantics, thus the class relation would be various because of random data sampling (Wang et al. 2020). In order to more efficiently use learned knowledge from H-OTKT, and further improve the robustness during testing phase, a plug-in Sample-specific Re-weighting Knowledge Bank (SRKB) module with no additional trainable parameters"}, {"title": "", "content": "is constructed.\nAiming at obtaining more robust and general effective information from VFER dataset and eliminate the randomness caused by data sampling (Wang et al. 2020), a correlation prototype $B = [B_1^{[+]}] \u2026\u2026 [B_{L_t}^{[+]}]^T \\in \\mathbb{R}^{L_t \\times L_s}$ is constructed to store the robust category relation between two domains. B is initialized by $\\frac{1}{L_s}$ to ensure $\\sum_{k=1}^{L_s} B_{l,k} = 1, l = 1, ..., L_t$. The l-th row of B demonstrates the association weights between the l-th class of target domain and each class of source domain. During training phase, momentum updating (Laine and Aila 2016) is introduced to update the correlation prototype B by:\n$\\begin{equation} B_l = \\alpha B_l + (1 - \\alpha) \\frac{1}{n} [\\sum_{i=1}^{n} I_{y_t=l}] ( \\frac{1}{\\sum_i T_{i,l}} \\sum_i T_{i,l}); l = 1,..., L_t  \\tag{12} \\end{equation}$\nwhere $I_{y_t=l} = 1$ if and only if the label of i-th deception sample $y_i$ equals to l, otherwise $I_{y_t=l} = 0$. And $\\alpha$ is the momentum factor. In Eq. (12), the relation between facial expression classes and each target categories (i.e., truthful and deceptive) is accumulated into the correlation prototype B with the guidance of label $y_t$. The underlying invariant relation between all deceptive samples and the source domain expressions is preserved.\nAccording to the previous results of affective computing field (Rill-Garc\u00eda et al. 2019) and psychology (DePaulo et al. 2003), deception should have high correspondences with some specific facial expression, such as fear, and sad. Ideally, for the i-th target deception sample, $T_i$ should be sparse, where several elements are much higher than the others. This indicates the standard deviation of a sparse $T_i$ should be larger if $T_i$ is valid, otherwise it could be smaller. When an unsatisfactory transport plan $T_i$ with small standard deviation is obtained, it is more reliable to use the corresponding correlation prototype $B_{I_i}$ to quantify the correlation mapping instead, where $I_i$ is the category that $T_i$ may belong to. Otherwise, calculated $T_i$ could be improved by its corresponding correlation prototype $B_{I_i}$. In our experiment, $B_{I_i}$ has smallest distance with $T_i$ among $\\{B_1 ... B_{L_t}\\}$. Mathematically, it can be formulated as:\n$\\begin{equation} T_i = \\sigma_i T_i + (1 - \\sigma_i) B_{I_i}; s.t. \\sigma_i =  \\begin{cases}   0, \\text{ std}(T_i) < \\nu \\\\   \\frac{\\text{ std}(T_i) - \\nu}{1 - \\nu}, \\text{ otherwise }  \\end{cases}  \\tag{13} \\end{equation}$\nwhere std(\u00b7) is standard deviation function, and $\\nu$ is a threshold. When the standard deviation of $T_i$ is greater than $\\nu$, we believe H-OTKT module finds a valid transport plan T to measure the importance between categories in source domain and samples in target domain. Otherwise, H-OTKT fails to find a valid transport plan due to noise, and $T_i$ would not be used for transferring knowledge. During the testing phase, T in Eq. (6) will be replaced by the obtained T."}, {"title": "Experiments", "content": "We make comparisons with several deception detection methods on RTL (P\u00e9rez-Rosas et al. 2015) and DOLOS (Guo et al. 2023) in visual, audio and fused modalities to validate AFFAKT. Three in-the-wild VFER datasets are included, i.e., DFEW (Jiang et al. 2020), FERV39K (Wang et al. 2022), MAFW (Liu et al. 2022).\nOverall, our proposed method obtains the best performance across all evaluation metrics, which suggests that our proposed method is effective and advanced for deception detection. Besides, deep learning based models have achieved better results compared to machine learning based models. Such results indicate that deep learning model exhibits better feature extraction ability than traditional methods. Moreover,\nAblation Studies\nAblation studies are conducted to verify H-OTKT and SRKB modules proposed in this paper. As our method contains four Influence of H-OTKT module. To validate the effectiveness of our proposed H-OTKT module, we add the H-OTKT module based on the baseline method (Case B in Table 2). Case B contains encoder layer, H-OTKT module and classification layer, it is trained with total loss function Eq. (11). Recall that the H-OTKT captures the optimal relation mapping between VFER classes and deception samples, and performs knowledge transfer. Compared with the results of Case A, when DFEW is utilized as source dataset, the ACCs increase on almost all target datasets and modalities, which demonstrates that H-OTKT can facilitate deception detection accuracy by transferring knowledge from the source domain to the target deception domain. Moreover, when source dataset is FERV39K or MAFW, ACCs increase for RLT data with visual modality, while decrease a little for others, because of"}, {"title": "", "content": "influence of pre-trained encoder G. In order to evaluate the effect of pre-trained encoder G, we employ Former-DFER (Zhao and Liu 2021) as the pre-trained encoder G to extract the source features (the process in Fig. 2 (a)). Comparing the results of Table 3 with that of Table 5, ACCs on RTL and DOLOS datasets show better performance when the source features are extracted by MAE-DFER. This phenomenon demonstrates that better source feature space structure could facilitate knowledge transformation based on H-OT.\nInterpretability Studies\nIn order to analyze the learned correlation prototype B, we show the value of B in Table 6 when best accuracy is achieved on the two deception datasets by AFFAKT in visual modality.We refer readers to the appendix for the other modalities and their analysis. According to the results shown in Table 5, the best ACCs are achieved for RLT and DOLOS when the source domain is selected as DFEW in visual modality. Therefore, the corresponding results for both deception dataset in visual\nAs demonstrated by Table 6, sad is significant related to deceptive for both RLT and DOLOS, while happy is remarkably correlated to truthful. Such results are also supported by psychological theory that liars will be less positive and pleasant than truth tellers (DePaulo et al. 2003; Zloteanu 2020; Hauch et al. 2015). For neutral and fear, neutral has a higher similarity with truthful than deceptive on both datasets.\nTable 6 shows that AFFAKT can automatically establish proper relations between classes of facial expressions and deception"}, {"title": "Conclusions", "content": "This paper presents a novel video deception detection method AFFAKT, aiming at addressing the challenge of insufficient high-quality large-scale labeled training datasets. We first develop H-OTKT module to perform knowledge transformation from related facial expression classes to deception samples, which estimates different weights of facial expression to deception samples by H-OT. Moreover, we design a correlation prototype based module SRKB to retain the invariant information within deceptive and truthful samples during training, which maintains the robust relation information between source and target classes. During testing phrase, SRKB integrates the predicted transport plan and the learned correlation prototype using a sample-specific re-weighting technique to leverage transferred knowledge. Extensive ex-periments have been conducted, showing that our proposed method outperforms other detection methods."}, {"title": "Datasets and Experimental Settings", "content": "Video Deception Detection Datasets. We conduct the experiments on two most widely used datasets in video deception detection task, Real-Life Trial (RLT) dataset (P\u00e9rez-Rosas et al. 2015) and DOLOS dataset (Guo et al. 2023).\nExperimental Settings\nOur experiments are conducted with Ubuntu 20.04, Python"}, {"title": "Detailed Description about Comparison Methods", "content": "Traditional Machine Learning based Deception Detection Methods\nFirstly, we would like to introduce the statistical features that"}, {"title": "", "content": "Deep Learning based Deception Detection Methods\nIn this paper, we make comparisons with several deep learn-ing methods. These methods can be separated by their back-bone structure: Long Short-Term Memory (LSTM) (Graves\nTransfer Learning based Methods\nThere have been a small number of researches that tried totransfer knowledge from other related dataset to enhancethe detection performance with deep learning based meth-ods. Therefore, we adapt several common kinds of transferlearning strategy to the deception detection task.\nThe Standard Deviation Report Between Folds\nThe average value and the standard deviation between dif-ferent folds are shown in Table 5. Beside the analysis in the"}, {"title": "Sensitive Analyses", "content": "Fig. 5 shows the variation tendencies of AFFAKT's predic-tion with the changing of parameters, four parameters arethat can be achieved when a balance between facial expression\nInterpretability Studies\nIn order to further analyze the learned correlation prototype\ndatasets. According to the results shown in comparison"}, {"title": "Notations in Our Method", "content": "Table 7 lists all notations that appear in our method and theircorresponding descriptions."}]}