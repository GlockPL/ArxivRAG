{"title": "NeuralDEM \u2013 Real-time Simulation of Industrial Particulate Flows", "authors": ["Benedikt Alkin", "Tobias Kronlachner", "Samuele Papa", "Stefan Pirker", "Thomas Lichtenegger", "Johannes Brandstetter"], "abstract": "Advancements in computing power have made it possible to numerically simulate large-scale fluid-mechanical and/or particulate systems, many of which are integral to core industrial processes. Among the different numerical methods available, the discrete element method (DEM) provides one of the most accurate representations of a wide range of physical systems involving granular and discontinuous materials. Consequently, DEM has become a widely accepted approach for tackling engineering problems connected to granular flows and powder mechanics. Additionally, DEM can be integrated with grid-based computational fluid dynamics (CFD) methods, enabling the simulation of chemical processes taking place, e.g., in fluidized beds. However, DEM is computationally intensive because of the intrinsic multiscale nature of particulate systems, restricting either the duration of simulations or the number of particles that can be simulated. Moreover, the non-trivial relationship between microscopic DEM and macroscopic material parameters necessitates extensive calibration procedures. Towards this end, NeuralDEM presents a first end-to-end approach to replace slow and computationally demanding numerical DEM routines with fast, adaptable deep learning surrogates. NeuralDEM is capable of picturing long-term transport processes across different regimes using macroscopic observables without any reference to microscopic model parameters. First, NeuralDEM treats the Lagrangian discretization of DEM as an underlying continuous field, while simultaneously modeling macroscopic behavior directly as additional auxiliary fields. Second, NeuralDEM introduces multi-branch neural operators scalable to real-time modeling of industrially-sized scenarios - from slow and pseudo-steady to fast and transient. Such scenarios have previously posed insurmountable challenges for deep learning models. Notably, our largest NeuralDEM model is able to faithfully model coupled CFD-DEM fluidized bed reactors of 160k CFD cells and 500k DEM particles for trajectories of 28s which amounts to 2800 machine learning timesteps. NeuralDEM will open many new doors to advanced engineering and much faster process cycles.", "sections": [{"title": "1 Introduction", "content": "In recent years, real-time numerical simulations [2, 20, 34, 41, 100], have emerged as new modeling paradigm, enabling immediate analysis and decision-making based on live data and conditions. Unlike traditional simulations, which may take hours or even days to run, real-time simulations provide instantaneous feedback, allowing users to interact with and adjust parameters on the fly. Moreover, in engineering, fast simulations are driving the design of safer and more efficient structures and machines by accurately predicting their behavior under different conditions, thereby allowing extensive scans of vast parameter spaces, and eliminating the need for expensive physical prototypes."}, {"title": "2 Background", "content": "In this section, we introduce the relevant core phenomena of DEM. A more in-depth explanation can be found, e.g., in the review article form Blais et al. [14] or the textbook from Norouzi et al. [75]. Further, we introduce neural operators and discuss their applicability to model particulate systems."}, {"title": "2.1 Discrete element method", "content": "In a system of solid particles with masses $m_i$, radii $r_i$, positions $\\vec{r}_i$, and velocities $\\vec{v}_i$, each of them has to obey Newton's second law\n$$\n\\frac{d}{dt} m_i \\vec{v}_i = \\vec{F}^{(ext)} + \\vec{F}^{(pc)} + \\vec{F}^{(pf)}.\n$$\nParticle i experiences forces of external origin, most important gravity $\\vec{F}^{(ext)} \\approx m_i \\vec{g}$, contact forces with the nearby grains and walls $\\vec{F}^{(pc)} = \\sum_{j\\neq i} \\vec{F}_{i,j}$, and the influence of a surrounding fluid phase $\\vec{F}^{(pf)}$ if present and relevant.\nThe contact force between solid particles i and j is commonly approximated with spring-dashpot models for both the normal $\\vec{F}_{i,j}^{(n)}$ and tangential component $\\vec{F}_{i,j}^{(t)}$,\n$$\n\\vec{F}_{i,j}^{(n)} = - k^{(n)} \\delta^{(n)} \\vec{n}_{i,j} + \\zeta^{(n)} \\vec{v}_{i,j},\n$$\n$$\n\\vec{F}_{i,j}^{(t)} = min \\left[ - k^{(t)} \\vec{\\delta}_{i,j}^{(t)} + \\zeta^{(t)} \\vec{v}_{i,j}^{(t)}, \\mu \\vec{F}_{i,j}^{(n)} \\right],\n$$\nwhere the tangential force is limited by Coulomb's friction law. Material properties enter these expressions in terms of the spring stiffnesses $k^{(n,t)}$, damping coefficients $\\zeta^{(n,t)}$ and sliding friction $\\mu$. $k^{(n,t)}$ give rise to the reaction against normal and tangential overlap $\\delta^{(n,t)}$ between two grains in the respective directions $\\vec{n}_{i,j}$ and $\\vec{t}_{i,j}$, and $\\zeta^{(n,t)}$ account for viscous dissipation caused by the normal and tangential relative velocities $\\vec{v}_i - \\vec{v}_j$ during contact. While simple geometric shapes such as perfect"}, {"title": "2.1.1 Coupled particle-fluid simulations", "content": "Particles will also experience a force from a surrounding fluid phase. While it may be neglected if no significant relative velocities occur, it can be a crucial factor for particle dynamics otherwise. The dominant contributions are usually caused by gradients of the pressure and by the drag force, i.e., the resistance against relative velocity between fluid and grain, so that\n$$\n\\vec{F}^{(pf)} = - V_i \\nabla p + \\beta (\\vec{u}_f - \\vec{v}_i).\n$$\nThe drag coefficient $\\beta$ depends on the particle size and the local flow conditions. A multitude of empirical correlations can be found in the literature to take into account the impact of Reynolds number, particle volume fraction $a_p$, size distribution, etc. [44].\nThe fluid velocity itself is governed by the filtered Navier-Stokes equations [6]\n$$\n\\frac{\\partial}{\\partial t} a_f + \\nabla \\cdot a_f \\vec{u}_f = 0\n$$\n$$\n\\frac{\\partial}{\\partial t} a_f \\vec{u}_f + \\nabla \\cdot a_f \\vec{u}_f \\vec{u}_f = \\nabla \\cdot \\sigma_f - \\vec{f}^{(pf)}\n$$\nwhich differ from their single-phase counterpart in two regards. The presence of particles reduces the locally available volume to a fraction $a_f = 1 \u2013 a_p$, and the density of force Equation (4) exerted by the fluid on the particles is felt by the fluid in opposite direction because of Newton's third law. The coupled solution of the CFD Equations (5) and (6) and the DEM Equation (1) gives rise to CFD-DEM simulations.\nFor a proper definition of the field quantities $a_p$ and $\\vec{f}^{(pf)}$, Lagrangian particle information needs to be mapped onto Eulerian fields. To this end, a filter function $g_l(r)$, e.g., a Gaussian with width l, is employed in terms of\n$$\na_p(\\vec{r}) = \\sum_i g_l(|\\vec{r} - \\vec{r}_i|) V_i\n$$\n$$\n\\vec{f}^{(pf)} (\\vec{r}) = \\frac{\\sum_i g_l(\\vec{r} - \\vec{r}_i) V_i \\vec{F}_i^{(pf)}}{\\sum_i g_l(\\vec{r} - \\vec{r}_i) V_i}\n$$\nAn analogous definition as Equation (8) can be invoked to define the spatial field distribution of any particle property. As a matter of fact, the target quantities of most particle simulations are not necessarily connected to single-particle properties located exactly at the positions of each grain. Instead, one might be interested in the spatial distribution of, e.g., particle volume fraction, residence time or temperature. Two strategies are available to obtain these fields: (i) One carries out a DEM simulation and postprocesses particle data according to Equation (8). The trajectory and properties of each grain are only needed as an intermediate step for the DEM simulation. (ii) One can try to directly formulate particle EOMs in an Eulerian fashion by filtering the Lagrangian ones and solving them disregarding discrete properties. As demonstrated by the two-fluid model [30], this can significantly reduce computational costs but can come with a serious degree of uncertainty [18] because not all particle properties lend themselves to a straight-forward formulation in terms of fields.\nEven if the resulting inaccuracies are acceptable, such simulations are still cumbersome because of the restriction to small timesteps (which is also present in an Eulerian formulation) and the lack of a direct relationship between macroscopic behavior and microscopic parameters. An attractive solution to this predicament might be offered by neural operators that can be trained with detailed particle data to predict any underlying field quantities in a highly efficient way."}, {"title": "2.2 Neural operators learning for scientific and engineering applications", "content": "In recent years, deep learning tools have been extensively integrated into scientific modeling, and have resulted in breakthroughs in, e.g., protein folding [1, 43], material discovery [9, 10, 69, 102], or weather modeling [12, 15, 52, 74, 76]. Driven by applications in CFD [35, 36, 48, 56, 91], deep neural network based surrogates, most importantly neural operators [50, 56, 66], have emerged as a computationally efficient alternative [105]. In addition to computational efficiency, neural operators offer the potential to introduce generalization capabilities across phenomena, as well as generalization across characteristics such as boundary conditions or coefficients [39, 68].\nNeural operators [50, 54, 56, 66] are formulated with the aim of learning a mapping between function spaces, enabling outputs that remain consistent across varying input sampling resolutions. Following the framework of Kovachki et al. [50], we assume U, V to be Banach spaces of functions defined on compact domains $X \\subset \\mathbb{R}^{d_x}$ or $Y \\subset \\mathbb{R}^{d_y}$, respectively, which map into $\\mathbb{R}^{d_u}$ or $\\mathbb{R}^{d_v}$. A neural operator $\\mathcal{G}: \\mathcal{U} \\rightarrow \\mathcal{V}$ approximates the ground truth operator $G: \\mathcal{U} \\rightarrow \\mathcal{V}$.\nWhen training a neural operator $\\mathcal{G}$, a widely adopted approach is to construct a dataset of N discrete data pairs $(u_{i,j}, v_{i,j'}), i = 1, ..., N$, which correspond to $u_i$ and $v_i$ evaluated at spatial locations $j = 1,..., K$ and $j' = 1, ..., K'$, respectively. Note that K and K' can, but need not be equal, and can vary for different i, which we omit for notational simplicity. Figure 3 (first row) shows the operator learning problem, i.e., the mapping of an input function $u_i$ to an output function $v_i$ via an operator $\\mathcal{G}$. The functions are given via K and K' discretized input and output points, respectively.\nOn this dataset, $\\hat{\\mathcal{G}}$ is trained to map $u_{i,j}$ to $v_{i,j'}$ via supervised learning, as sketched in Figure 3 (bottom row), where $\\hat{\\mathcal{G}}$ is composed of three maps [3, 84]: $\\hat{\\mathcal{G}} := \\mathcal{D} \\circ \\mathcal{A} \\circ \\mathcal{E}$, comprising the encoder $\\mathcal{E}$, the approximator $\\mathcal{A}$, and the decoder $\\mathcal{D}$. First, the encoder $\\mathcal{E}$ transforms the discrete function samples $u_{i,j}$ to a latent representation of the input function. Then, the approximator $\\mathcal{A}$ maps the latent representation to a representation of the output function. Lastly, the decoder evaluates the output function at spatial locations j'. The neural network $\\hat{\\mathcal{G}}$ is then trained via gradient descent, using the gradient of, e.g., a mean squared error loss in the discretized space $\\mathcal{L}_i = \\sum_{j'} ||v_{i, j'} - \\hat{v}_{i,j'}||^2_2$, where $||\\cdot||_2$ is the Euclidean norm."}, {"title": "2.2.1 Discretization convergence", "content": "Neural operators are well suited to describe the evolution and interaction of physical quantities over space and time, i.e., continuously changing fields. Most notably, since the solutions are continuous fields, the mapping should neither depend on K, the number of input locations, nor on K', the number of decoded output locations. The property that neural network outputs remain consistent across different input sampling resolutions is referred to as discretization convergence [50, 56, 66]. Neural operators are proven to be discretization convergent in the limit of mesh refinement [50], meaning they converge to a continuum operator in the limit as the discretization is refined. In theory and if properly designed, neural operators can be evaluated at any data discretization. Nevertheless, there is a minimum threshold for the number of points needed for accurate representation. However, strong evidence indicates that neural networks can capture physical phenomena effectively without requiring the same level of fine-grained discretization as traditional numerical methods [48].\nNeural operator architectures need to ensure discretization convergence of their components. When encoding the discretized input function, popular choices to preserve discretization convergence are graph neural operators [55, 57], transformers [37, 90, 96] or combinations thereof [3]. For decoding, recent works [93] have shown that $\\mathcal{D}$ can be considered as neural field [70, 86, 97], which allows for point-wise evaluation at the output grid or output mesh [3, 47, 49, 93]."}, {"title": "2.2.2 Deep learning for particulate systems", "content": "While most state-of-the-art neural operator approaches are predominantly designed for geometrically simple domains with regular grids, neural operator formulations for particle- or mesh-based dynamics remain limited. In such cases, graph neural networks (GNNs) [16, 45, 83] with graph-based latent space representations are a prevalent approach to build neural surrogates. Often, predicted node accelerations are numerically integrated to simulate the time evolution of multi-particle systems [67, 79, 82, 88, 89]. For the modeling of granular dynamics, Li et al. [58] predict contact forces when inputting microstructures of grain packings. Similarly, Cheng & Wang [21] estimate contact forces in compressed granular assemblies. Mayr et al. [67] introduce Boundary-GNNs to model granular flows through hoppers, rotating drums, and mixers. All these models are limited by the number of particles, and operate on 10k \u2013 20k particles at most, although often much less.\nGNNs inherently possess a strong inductive bias for Lagrangian dynamics, which, however, presents a significant downside since the number of nodes, and thus the computational complexity grows with the number of Lagrangian particles. Thus, computational complexity gets quickly infeasible for an increasing number of particles [3, 72]. However, motivated by recent successes in latent space generative modeling [27, 80], latent space modeling has emerged as a new modeling paradigm in neural operator learning [3, 38, 94, 107]. In this work, we follow the argumentation of Alkin et al. [3], i.e., neural operators with large model complexity are powerful enough to capture inherent field characteristics when applied to Lagrangian or multiphysics simulations. For DEM simulations, we argue that those field characteristics need not be explicitly present in the training data, rather, they might emerge from the bulk behavior of the particulate systems."}, {"title": "3 NeuralDEM", "content": "NeuralDEM presents the first end-to-end solution for replacing computationally intensive numerical DEM routines and coupled CFD-DEM simulations with fast and flexible deep learning surrogates. NeuralDEM introduces two conceptually novel modeling paradigms:\n1.  Physics representation: We model the Lagrangian discretization of DEM as an underlying continuous field, while simultaneously modeling macroscopic behavior directly as additional auxiliary fields. NeuralDEM encodes different physics inputs which are representative for DEM dynamics and/or multi-physics scenarios. Examples are particle displacement, particle mixing, solid fraction, or particle transport.\n2.  Multi-branch neural operators: We introduce multi-branch neural operators scalable to real-time modeling of industrial-size scenarios. Multi-branch neural operators build on"}, {"title": "3.1 Physics representation", "content": "As common when training deep learning surrogates, we train on orders of magnitude coarser timescales than what a classical solver requires to be stable and accurate. For the numerical experiments in this paper, the timescale relation is at least $1000\\Delta t_{DEM} = \\Delta t_{ML}$. Additionally, for learning the dynamics of particle movement, we use the particle displacement, which is defined as the difference between the position $r_i \\in \\mathbb{R}^3$ of particle i at timestep $t_{ML}$ and the position of the same particle at timestep $t_{ML} + \\Delta t_{ML}$. Finally, we use the term transport to denote the particle movement integrated over multiple timesteps $\\Delta t_{ML}$.\nNeuralDEM models the Lagrangian discretization of DEM as a continuous field in a compressed latent space, leveraging the insight that the effective degrees of freedom of physical systems is often much smaller than its input dimensionality [59]. Therefore, we assume that there exists some underlying field that describes the particle displacements in a DEM simulation and learn this underlying field over the whole domain instead of a displacement per particle. However, particle displacements can fluctuate depending on their exact position within the bulk of the material. Such fine-grained details are lost when going to a field-based representation which smoothes out these variations. This makes field-based models unable to move particles accurately around in space, which would be required to get macroscopic insights into the simulation dynamics.\nTo circumvent this issue, we introduce additional auxiliary fields that model the macroscopic insights directly instead of calculating them in post-processing from the particle locations. For example, by modeling the accumulated particle movement over a long period of time via a \"transport\" field, we can learn macroscopic properties directly instead of integrating short-term movements which would require precise prediction of the fluctuations thereof. This is visualized in Figure 4.\nEven over such a large timestep, the evolution of a flow and its properties at each point is mainly determined by the field values in a nearby, bounded subdomain which grows with the the step size, and hardly influenced by very distant points [60]. This behavior can be resembled by the attention mechanism of transformer networks [90]."}, {"title": "3.2 Multi-branch neural operators", "content": "Emerging bulk behavior of classical solvers as motivation. Classical solvers can create full simulations via precisely updating microscopic properties such as the particle positions at extremely high time resolution, with optional coupling to, e.g., a fluid phase, which is updated with similar precision and timescale. Similarly, NeuralDEM aims to extract the physical dynamics and simulation state updates from the microscopic properties also used in classical solvers, which we call \u201cmain-phase(s)\" where each main-phase is processed by one main-branch transformer in our model. Using an example of a particle-fluid coupled simulation, one main-branch predicts particle displacements, while a second main-branch predicts fluid velocities and pressures. All main-branches are tightly coupled via frequent information exchange during the model forward pass.\nMicroscopic inaccuracies of neural operators. While the model is trained using microscopic properties, neural operators are not able to predict microscopic properties, such as the particle displacements, accurately enough because neural operators are not as precise as classical solvers and operate on much coarser time resolution. It is therefore not feasible to exclusively rely on an accurate prediction of these microscopic properties in the main-branches. Creating simulations by moving initial particle positions according to the predicted displacements would quickly result in unphysical states (e.g., overlapping particles) and becomes inaccurate.\nMacroscopic modeling via auxiliary fields. An important observation regarding the systems we model is that the insights that a classical solver can provide into the physical dynamics are rarely on a microscopic level and more often on a macroscopic level, where the macroscopic properties are extracted from the microscopic results of the classical solver. Motivated by this intuition, we introduce additional off-branches, which are trained to model macroscopic processes such as particle mixing or particle transport directly during training. Similar to classical solvers, where the macroscopic process does not influence the microscopic updates, off-branches do not influence any of the main-branches. Instead, each off-branch creates its predictions by repeatedly processing its own data, as well as retrieving information from the microscopic state of the main-branches (without influencing them).\nMulti-branch transformers. The central neural network component of NeuralDEM are multi-branch transformers. Multi-branch transformers, as the name suggests, consist of multiple branches: main-branch(es) and off-branch(es). Each branch is a stack of transformer [90] blocks where weights are not shared between branches. Each branch operates on a set of so-called tokens, which are obtained by embedding the input into a compressed latent representation. Main-branch(es) concatenate all tokens before each attention operation along the set dimension, allowing interactions between them, followed by splitting tokens again into the different branches, akin to multi-modal diffusion transformer (MMDiT) blocks [27]. Additionally, multi-branch transformers can include arbitrarily"}, {"title": "3.3 Scalar parameter conditioning", "content": "Physical simulations often require various scalar parameters such as material properties (e.g., friction or particle size) or geometry variations (e.g., slope angles or outlet width) to define the simulation properties. It is vital to provide these scalars also to the machine learning model to produce accurate results. A common way to do this is by feature modulation [78] which scales and shifts intermediate feature activations based on a vector representation of the scaler parameters. As NeuralDEM is a transformer architecture, we use DiT-style modulation [77] which scales, shifts and gates the activations of each attention and MLP block based on a learned vector representation of the scalar parameters. This form of conditioning allows NeuralDEM to generalize across geometries and across non-trivial particle-particle interactions by condition on respective variables.\nA particular intriguing property of this conditioning mechanism is that it allows us to condition on parameters that describe only the macroscopic material behavior. For example, our model can be conditioned on the measured parameters, like the internal friction angle or the flow function coefficient from a shear cell device, instead of requiring the microscopic friction parameters necessary"}, {"title": "3.4 Flexible model architecture for variable simulation use-cases.", "content": "As physical simulations exhibit a broad range of dynamics, and relevant macroscopic insights can vary drastically depending on the use-case, our multi-branch transformer architecture should be seen as a flexible framework that enables various use-cases instead of a \"set in stone\" architecture. Components can become redundant in certain settings, or special use-cases could require additional components. For example, in simulations with laminar or pseudo-steady dynamics, the whole simulation is fully specified by the initial state, making encoding subsequent states and interaction between branches redundant. However, in very unsteady systems all components of the multi-branch transformer architecture are very much necessary to produce accurate time evolution as slightly different initial states can lead to vastly different instantiations of dynamics, which requires a physically accurate state at each timestep and interactions between the states of different branches.\nAdditionally, the initial encoding of physics phases benefits from specialized designs, depending on the input data. For irregular grid data (e.g., particles), we use the supernode pooling from UPT [3] which aggregates information around particles via message passing to so-called supernodes, which are randomly selected particles. Fluid phases are typically represented via regular grid data, which is computationally more efficient and allows efficient coupling to, e.g., particle simulations. For regular grid data, we use the vision transformer patch embedding [26] which splits the input into non-overlapping patches and embeds them using a shared linear projection.\nFinally, decoding is performed using the same architecture for both particle and grid data, using a perceiver-based neural field decoder [40], which is queried at locations $\\vec{Y}_{i,j'=1,...,K'}$ in parallel. This type of decoding first embeds query locations to be used as queries for the perceiver cross-attention and uses the latent tokens as keys and values. This results in a point-wise evaluation of the latent space based on the query position, which is what enables effective parallelization.\nWe use a standard pre-norm vision transformers architecture [8, 26] where each branch of the multi-branch transformer corresponds to a single vision transformer. The total number of blocks is evenly distributed across encoder, approximator and decoder."}, {"title": "4 Numerical experiments", "content": "We test the NeuralDEM framework on two industrially relevant use cases: hoppers and fluidized bed reactors, both visualized in Figure 6 and described in Table 1. We evaluate NeuralDEM on different metrics: (i) Effectiveness of field-based modeling w.r.t. macroscopic quantities. We extract and compare emerging macroscopic physics phenomena. (ii) Scalability towards industry relevant simulation sizes. We train on simulations with up to half a million particles and anticipate good scaling behavior to much higher numbers. (iii) Physically accurate time extrapolation. We show that our models can faithfully model dynamics for long-time horizons \u2013 in a fraction of the time that a classical solver would take. (iv) Generalization to unseen regions in the design parameter space."}, {"title": "4.1 Simulation setup and problem scale", "content": "Hoppers are industrially used for short as well as long term storage of particulate material, showcasing slow and pseudo-steady macroscopic behavior. DEM is the preferred method since the air around the particles can usually be neglected due to the slow velocities in the system. In our experiments, the hopper geometry, as shown in Figure 6a, is filled with 250k particles, which gradually exit the domain over the simulation duration when the hopper empties. Timestepping of DEM solvers strongly depends on particle size as well as particle properties. A timestep of $\\Delta t_{DEM} = 10 \\mu s$ is required for the tested numerical experiments with LIGGGHTS [46].\nFluidized bed reactors are characterized by fast and transient phenomena and are widely used in industry for a variety of processes. Fluidized bed reactors showcase strong interactions of the particles with the surrounding fluid, necessitating an accurate modeling of particles, the gas phase, as well as particle-gas interactions. Thus, modeling approaches need to combine DEM parts with simulations of the surrounding fluid. For data generation, we use a coupled CFD-DEM approach [31] which is built upon LIGGGHTS [46] and OpenFOAM [95]. The geometry of the setup is sketched in Figure 6b and the dimensions for both cases can be found in Table 6c. The reactor is filled with 500k particles and the fluid, i.e., air, that is uniformly pushed into the reactor from the bottom is modeled on a grid of 160k hexahedral cells."}, {"title": "4.2 Hopper", "content": "We consider hopper simulations with the hopper geometry depicted in Figure 6a. With its outlet closed, the hopper is initially filled with particles, to roughly 250k grains on average (particle counts can vary based on the outlet slope $\u0430_{HO}$). Then the outlet at the bottom of the hopper is opened and grains start to flow out. By default, we do not refill any new particles into the hopper but consider an operation mode where the material is continuously refilled in Section 4.2.7. Different simulations in the dataset vary hopper geometry and particle friction as specified in Table 2. We create a dataset of 1000 simulations with a train/validation/test split of 800/100/100. This variability in simulation parameters results in different flow regimes (\u201cfunnel flow\u201d or \u201cmass flow\") where the dynamics are slow and pseudo-steady. In funnel flow, particles primarily move down a funnel above the outlet, whereas in mass flow, material moves uniformly down towards the outlet, see Figure 7. Each simulation is run to cover 40 physical seconds at most (the simulation is stopped if no particles remain in the hopper). Snapshots are stored in 0.1 s intervals (resulting in 400 ML timesteps $\\Delta t_{ML}$) which is the data that NeuralDEM models are trained on. The DEM solver requires 10k timesteps per 0.1 physical seconds and a single simulation takes roughly 3 hours on 16 CPUs. We additionally evaluate the parameters of each simulation in a shear cell to get its internal friction angle $\\theta$ and flow function coefficient $ffc$ to use it as conditioning instead of the microscopic friction parameters (see Section 4.2.6)."}, {"title": "4.2.1 Multi-branch neural operator architecture for hopper experiments", "content": "The considered hopper simulations exhibit slow and pseudo-steady dynamics, resulting in a state that is well-defined from the scalar input parameters (timestep, particle friction, hopper angle) alone. Therefore, it is neither necessary to encode the previous state nor to have interactions between branches, as the scalar input parameters already provide full information about the state to all branches. Therefore, we opt for a decoder-only architecture consisting of a single transformer block per branch that starts from 32 static learnable latent tokens and prepares them for decoding via a perceiver cross-attention block that takes positional embeddings as queries and uses the latent tokens as keys and values. Both the transformer and the cross-attention block use DiT [77] modulation to incorporate the scalar parameters (timestep, $\u0430_{HO}$, $\u00b5_s$ and $\u03bc_r$). The whole model consists of 50M parameters.\nWe train models in the hopper setting for 10k updates using a batchsize of 256, a peak learning rate of $10^{-4}$ which is warmed up for 1k updates followed by a decaying cosine schedule afterwards. The loss is summed for all branches and LION [19] is used as optimizer."}, {"title": "4.2.2 Flow regime and visualization via occupancy and transport field", "content": "To evaluate the macroscopic modeling of the flow regime inside the hopper, we use the transport field and evaluate it at the occupied positions as defined via the occupancy field. We bin the z coordinate of the transport prediction into 8 bins which results in \"stripes\" of particles at the initial timestep. These stripes then evolve in time as particles flow out. By evaluating the volume of each stripe at every timestep, we get detailed information on the different emerging flow regimes in the hopper.\nIn the mass flow regime, as shown in Figure 7a, material flows to the outlet quite uniformly, resulting in a steady flow and first in - first out operation. NeuralDEM can model this behavior accurately, as visualized in Figure 7c, where layer after layer leaves the hopper. Contrary, in the funnel flow regime, as shown in Figure 7b, the material primarily moves down a funnel above the outlet. This results in a layer inversion, as visualized in Figure 7d, meaning particles from higher layers overtake particles from the lower layers through the funnel and the layers higher up in the hopper will empty first. These emerging macroscopic phenomena are perfectly modeled by NeuralDEM as well.\nNotably, NeuralDEM exclusively models fields, which allows us to evaluate transport and occupancy at arbitrary positions with arbitrary resolution. For example, in the evaluations of this section, we use a tetrahedral grid with 80k cells. Our model can seamlessly make predictions thereof despite seeing only particle positions during training."}, {"title": "4.2.3 Outflow rate, drainage time, and residual material via occupancy field", "content": "The occupancy field defines the occupied volume of the remaining mass in the draining hopper. Given an initial packing, we can evaluate the occupancy field at the initial positions of each particle and track the number of occupied positions over the whole simulation to then create predictions of outflow rate, drainage time, and residual material.\nOccupancy field. To define whether or not a position is occupied, we introduce a hyperparameter that defines a radius around each particle position. All positions within this radius are considered occupied whereas positions that are not within the radius of any particle are considered unoccupied. We choose the radius to be larger than the particle radius to avoid classifying empty spaces between densely packed spherical particles as unoccupied.\nOutflow rate. The outflow rate can simply be calculated by subtracting the number of occupied positions at time t from that one timestep later at t + $\\Delta t_{ML}$. To avoid fluctuations due to the burn-in and ending phase of the simulation, we calculate the outflow rate as the average outflow starting from timestep 50 (5s) over a 100 timestep (10s) duration and normalize it by the initial particle count.\nDrainage time. We consider the hopper to be \"drained\" by specifying a threshold of particles that are located above the outlet (\"are falling down\") for both classical DEM and NeuralDEM generated"}, {"title": "4.2.4 Residence time", "content": "Another interesting quantity that emerges at the macroscopic level is residence time, i.e., how long it takes for each particle to exit the hopper. Therefore, we predict the number of timesteps that each particle resides within the hopper. Visualizing the residence time at the initial timestep can identify stale regions and can also be used to characterizes the flow regime. We show the initial frame of the residence time prediction for different flow regimes and stale regions in Figure 10."}, {"title": "4.2.5 Generalization capabilities", "content": "To investigate the generalization capabilities of NeuralDEM, we split the dataset by excluding a parameter range of 20 degrees from the training set for testing. In this setting, the NeuralDEM model is evaluated on parameter combinations that are far away from the ones seen during training. Nevertheless, the NeuralDEM model makes reasonable predictions as shown in Figure 11."}, {"title": "4.2.6 Macroscopic parameter conditioning", "content": "A common use case in the industry is that given some material, one wants to simulate the material in, e.g., a hopper to get insights into its behavior or flow dynamics. However, DEM solvers require a precise specification of the microscopic material parameters, which are often unknown. In order to run a DEM simulation of a material with unknown microscopic parameters, those parameters first have to be inferred via, e.g., a calibration procedure [22] before the DEM simulation of the material can be run. In the case of our hopper simulation, one would need to infer the values of particle sliding and rolling friction.\nInstead, the flexible conditioning methodology (as described in Section 3.3) allows NeuralDEM to condition on macroscopic parameters instead of microscopic ones. To this end, we evaluate the in-"}, {"title": "4.2.7 Refilling operation mode", "content": "For neural operator modeling, it is difficult to represent particles that do not exist at the initial timeframe. This is due to the typically stochastic nature of the refilling process, and one would need to accurately model this process during inference in order to avoid unrealistic states, e.g., overlapping particles. However, our field-based modeling paradigm allows NeuralDEM to model the macroscopic behavior of refilled particles without requiring their accurate positions.\nTo showcase this setting, we train a NeuralDEM model to predict the evolved transport and residence time when continuously refilling new particles into the hopper. As the refilling operation mode can go on indefinitely, the model needs some kind of reference frame to predict \"how did particles move w.r.t. the reference frame\" and \"how long particles have been in the hopper w.r.t. the reference frame\". To this end, we sample a random past timestep during training and predict transport and residence time w.r.t. this past timestep. Figure 13 shows the evolved transport over 50 ML timesteps (5s) in comparison to a ground truth DEM simulation."}, {}]}