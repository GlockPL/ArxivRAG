{"title": "VLMine: Long-Tail Data Mining with Vision Language Models", "authors": ["Mao Ye", "Gregory P. Meyer", "Zaiwei Zhang", "Dennis Park", "Siva Karthik Mustikovela", "Yuning Chai", "Eric M Wolff"], "abstract": "Ensuring robust performance on long-tail examples is an important problem for many real-world applications of machine learning, such as autonomous driving. This work focuses on the problem of identifying rare examples within a corpus of unlabeled data. We propose a simple and scalable data mining approach that leverages the knowledge contained within a large vision language model (VLM). Our approach utilizes a VLM to summarize the content of an image into a set of keywords, and we identify rare examples based on keyword frequency. We find that the VLM offers a distinct signal for identifying long-tail examples when compared to conventional methods based on model uncertainty. Therefore, we propose a simple and general approach for integrating signals from multiple mining algorithms. We evaluate the proposed method on two diverse tasks: 2D image classification, in which inter-class variation is the primary source of data diversity, and on 3D object detection, where intra-class variation is the main concern. Furthermore, through the detection task, we demonstrate that the knowledge extracted from 2D images is transferable to the 3D domain. Our experiments consistently show large improvements (between 10% and 50%) over the baseline techniques on several representative benchmarks: ImageNet-LT, Places-LT, and the Waymo Open Dataset.", "sections": [{"title": "1 Introduction", "content": "In many real-world robotic applications, such as autonomous driving, the model must be able to handle a wide variety of situations. Ensuring the model's performance in long-tail scenarios remains a challenging problem. Previous efforts on long-tail learning include improving model optimization (Cui et al. 2019a, 2021), model modification (Menon et al. 2020; Wang et al. 2020), and fine-tuning from pre-trained models (Long et al. 2022; Shi et al. 2023). While these works significantly improve the model's performance on long-tail data, they mainly focus on how to improve the model given a fixed dataset. Alternatively, this paper focuses on mining for long-tail examples that can be added a training set to improve model performance. In many real-world applications, there exists a relatively small amount of labeled data and a vast amount of unlabeled data. Take autonomous driving for example, the vehicles collect enormous quantities of raw sensor data while driving. Most of this data is uninteresting; thus, annotating it would be costly without providing much benefit. However, a tiny portion of the data consists of interest long-tail situations unavailable in our existing training set. As a result, automatic mining for this long-tail data is of the utmost importance.\nIdentifying long-tail examples from a large pool of data can be challenging. Existing approaches primarily utilize model uncertainty as a key signal (Gal, Islam, and Ghahramani 2017; Jiang et al. 2022; Choi et al. 2021). The underlying assumption is that the model tends to be less confident in its predictions for long-tail examples. However, an example that results in a high model uncertainty might not be a long-tail example but a hard example. For instance, in the study by (Jiang et al. 2022) on LiDAR-based outdoor 3D detection, hard examples generally consist of long-range or occluded vehicles. These hard examples are common, yet the model produces highly uncertain predictions for them, which significantly reduces the effectiveness of using uncertainty as a criterion for selecting long-tail examples.\nRecently, foundation models such as large language models (LLMs) (Chowdhery et al. 2023; Touvron et al. 2023; OpenAI 2023; Chiang et al. 2023) and large vision language models (VLMs) (Alayrac et al. 2022; Dai et al. 2023; Awadalla et al. 2023; Liu et al. 2024; Zhu et al. 2024) have emerged. These models are trained with internet-scale data and have demonstrated impressive few-shot and zero-shot performance on recognition tasks (Radford et al. 2021; Liu et al. 2024). Leveraging the knowledge of VLM also benefits other tasks such as image clustering (Kwon et al. 2024) and out-of-domain detection (Jiang et al. 2024).\nWe argue that VLMs offer a more comprehensive understanding of long-tail examples due to their rich semantic extraction capabilities. However, the challenge lies in leveraging VLMs to enhance the performance of task-specific models on these examples. Directly deploying VLMs is often impractical due to latency concerns and the complexity of adapting them to task with different modalities. This paper demonstrates that the knowledge embedded in VLMs can serve as an effective signal for mining long-tail examples, thereby improving the quality of training data and enhancing the performance of task-specific models. The intuition is straightforward: since VLMs have been exposed to a highly diverse set of examples, they are capable of providing a more comprehensive understanding of semantic information. By comparing the semantic information described by the VLM,"}, {"title": "2 Related Work", "content": "Long-tail Perception Most existing work on long-tail visual recognition considers objects with less frequent class labels as long-tail examples and develops approaches to improve prediction accuracy in such label-imbalance situations. Applications include 2D tasks such as image classification (Cui et al. 2019b; Kang et al. 2019; Liu et al. 2019), segmentation (Hsieh et al. 2021; Wang et al. 2021; Wu et al. 2020), and object detection (Li et al. 2020; Tan et al. 2020, 2021a; Hyun Cho and Kr\u00e4henb\u00fchl 2022). Typical approaches for long-tail perception include data reweighting and resampling (Cui et al. 2019a; Cao et al. 2019; Khan et al. 2019; Huang et al. 2019; Zhang et al. 2021; Hyun Cho and Kr\u00e4henb\u00fchl 2022; Chawla et al. 2002; Han, Wang, and Mao 2005), gradient balancing (Tang, Huang, and Zhang 2020; Tan et al. 2021b), model or logits modifications (Wang et al. 2020; Alshammari et al. 2022; Menon et al. 2020; Ren et al. 2020), knowledge transfer or distillation (Wang, Ramanan, and Hebert 2017; Chu et al. 2020; Kim, Jeong, and Shin 2020; Liu et al. 2021; Xiang, Ding, and Han 2020; Li, Wang, and Wu 2021), representation learning (Zhong et al. 2021; Cui et al. 2021; Du et al. 2024), and fine-tuning pre-trained models (Ma et al. 2021; Tian et al. 2022; Dong et al. 2022; Long et al. 2022; Shi et al. 2023).\nCompared with 2D vision, long-tail 3D object detection is less explored. (Peri et al. 2023) developed feature shar-\ning and camera-LiDAR fusion mechanisms to improve 3D detection on objects with infrequent class labels. (Ma et al. 2023) proposed a late-fusion approach of camera and LiDAR features for 3D detection on uncommon classes. Both (Peri et al. 2023; Ma et al. 2023) focus on inter-class long-tail examples. (Sick, Walter, and Abhau 2023) used shapeaware anchor distributions and heatmaps combined with camera-LiDAR fusion to improve 3D detection on objects with less frequent shapes. Those approaches aim at improving the detection on long-tail examples by modifying the model architecture rather than mining long-tail data. (Jiang et al. 2022) was the first to study intra-class long-tail 3D detection and proposed a density-based (using a flow model) and uncertainty-based approach for mining long-tail examples. Compared with (Jiang et al. 2022), we consider a new data mining approach by leveraging the knowledge from a pre-trained VLM.\nActive Learning This work is closely related to poolbased active learning, where the goal is to actively select samples from a large set of unlabeled data to be labeled. Current active learning approaches can be classified into two categories: uncertainty-based and density-based. Uncertainty-based approaches use a model's level of uncertainty as a signal to identify hard examples. The methods to measure uncertainty vary, with some representative approaches including ensemble variance (Beluch et al. 2018; Choi et al. 2021), entropy of the predicted distribution (Holub, Perona, and Burl 2008; Segal et al. 2022), and Bayesian modeling (Gal, Islam, and Ghahramani 2017; Harakeh, Smart, and Waslander 2020) of the logits. The other category selects examples such that the data distribution formed by those examples closely approximates the population data distribution (Sinha, Ebrahimi, and Darrell 2019; Sener and Savarese 2017; Gudovskiy et al. 2020). These approaches are not designed specifically for long-tail data mining. Despite some close connections between longtail data mining and active learning, the goals are different. Active learning aims to reduce labeling costs by properly selecting a smaller amount of data, while data mining aims to discover long-tail data. To the best of our knowledge, we are the first to utilize a VLM to select data for labeling."}, {"title": "3 Method", "content": "Our goal is to find long-tail examples from a large pool of unlabeled data. Depending on the problem and data, the long-tail examples can be either inter-class, i.e. less common class categories, or intra-class, i.e. less common variations within a class. The main idea of our approach is to leverage the world knowledge from a VLM and LLM to extract keywords that describe the examples, and the frequency of the keywords is used to select the data to be labeled. Furthermore, our proposed Pareto mining can be used to incorperate other long-tail signal. An overview of our proposed method is illustrated in Figure 1."}, {"title": "3.1 VLMine: VLM Knowledge for Long-tail Example Mining", "content": "VLMine is a straightforward mining procedure consisting of two steps: extraction of representative keywords and example selection via keyword frequency.\nExtraction of representative keywords. We summarize the examples with a set of keywords. Given an image, we first prompt the VLM to describe the image, and then we summarize the description into a set of keywords. This can be done using a rule-based approach or by querying a LLM. After extracting the keywords, we also apply some standard post-processing, such as lemmatization and stop word removal. Note that each example might have a different number of keywords.\nExample selection via keyword frequency. We compute the frequency of the keywords within all of the examples. The novelty score of each example is defined as a reverse monotonic transformation of the pooled keyword frequency. Depending on the data and the task, the pooling operator can be average pooling (i.e. averaging the frequency of all the keywords) or min-pooling (i.e. choosing the frequency of the least common keyword). Concretely, suppose an example has n keywords with frequency $k_1, k_2, ..., k_n$, depends on the choice of pooling operator, its novelty score s is defined as\n$s := \\phi(minki) ki) or s:= \\phi(\\Sigma iki/n), $ (1)\nwhere $\\phi$ is any reverse monotonic transformation, e.g. $\\phi(x) = -x$. Examples with higher novelty scores are assumed to be more rare. The choice of pooling function is task dependent. For example, in object detection, each example can contain multiple objects with most being commonly occurring objects. Thus, a majority of the keywords will be associated to common objects. In this case, the minpooling operator is a better choice. However, for image classification, we expect most of the keywords to be associated with the class and thus average pooling becomes a more reasonable choice.\nWe could directly ask the VLM to identify examples that are less common. However, such an approach selects examples based on the VLM's knowledge, which introduces bias. The examples that the VLM believes to be uncommon might not actually be rare in the task database."}, {"title": "3.2 Pareto Mining: Combining Multiple Data Mining Signals", "content": "The novelty score determined by VLMine is modelagnostic. It purely uses the knowledge from a VLM and possibly a LLM without utilizing any information from the task-specific model we want to improve, e.g. an image classifier or object detector. We find that the signal provided by VLMine is often complementary to the signals obtained from traditional model-based approaches such as uncertainty-based mining. On the other hand, uncertaintybased mining algorithms, which all rely on the task-model's knowledge, are often correlated with each other. Intuitively, if multiple signals are highly correlated, a long-tail example that is missed by one technique is likely to be missed by another. However, if the signals are complementary, even if an example is missed by one algorithm, we have a higher chance to detect it with another. Therefore, it is beneficial to develop a mining strategy that can integrated multiple signals.\nAn obvious approach is to combine the signals from multiple algorithms is to use a linear combinations of their scores. However, the final result will heavily rely on the weights of the linear combination, which may be hard to determine since the magnitude of each signal might be very different and there is not an explicit objective to optimize. Refer to Appendix A.3 for an analysis.\nWe propose a simple and general approach to combine the signals from multiple algorithms. The idea is motivated by the concept of a Pareto frontier in multi-objective opti-"}, {"title": "4 Experiments", "content": "Our experiments are organized as follows. In Section 4.1 and 4.2, we evaluate VLMine and Pareto mining using a poolbased active learning setup. Our goal is to demonstrate that the proposed method is effective in identifying both interand intra-class long-tail examples. Details on the prompt engineering and implementations are also discussed in these sections. Additional visualizations, studies, and remarks are presented in the appendix. In Appendix A.1, we provide visualizations to illustrate that VLMine successfully mines interesting long-tail examples. In Appendix A.2, we visualize examples identified by combining model-agnostic and model-based signals through Pareto mining. In Appendix A.3, we conduct an ablation study to understand how the design of the prompt influences the final performance, and to compare Pareto mining against other methods for combining novelty scores. In Appendix A.4 and A.5, we discuss limitations related to the VLM and their impact on VLMine."}, {"title": "4.1 Long-tail 2D Image Classification", "content": "Setup. We consider two long-tail datasets for the image classification task: ImageNet-LT and Places-LT. The datasets were created by sampling a subset of the original dataset following a Pareto distribution (Liu et al. 2019). ImageNet-LT is an object-centric dataset with a training split consists of 115.8K images from 1000 categories and cardinality ranging from 5 to 1280. Places-LT is a scene-centric dataset with a training set that consists of 5 to 4980 images for each of its 365 classes for a total of 62.5K images. For both datasets, the validation and test splits are balanced and contain 20 and 100 examples per class, respectively.\nWe conduct pool-based active learning experiments following (Settles 2009; Jiang et al. 2022). For each class, we sub-sample 20% of the training examples, keeping the distribution of classes the same. This subset forms the \"labeled\" pool, and the rest of the training set and the entire validation set are considered the \"unlabeled\" pool from which we will mine data. The test set remains unchanged and is used for evaluation.\nWe use predictive entropy (Shannon 1948) and variation ratios (Freeman 1965) as the baselines, which have been shown to be strong acquisition functions based on the analysis in (Gal, Islam, and Ghahramani 2017). Predictive entropy,\n$\\- \\sum_C p(y=c|x)log p(y=c|x), $ (2)\ncalculates the entropy of the predicted class distribution, and higher entropy values imply a higher novelty for the example. Likewise, the variation ratio,\n$1\\- max_C p(y=c/x), $ (3)\nmeasures the lack of confidence in a prediction, where a larger ratio is assumed to indicate a higher chance of an example being part of the long-tail.\nPrompt and Implementation Details. For these experiments, we use LLaVAv1.5-7B (Liu et al. 2024) as the VLM and a heuristic to generate keywords. For ImageNet-LT, we prompt the VLM with the following: What are the possible classes for this image? Give three possible answers. For Places-LT, the"}, {"title": "4.2 Long-tail 3D Object Detection", "content": "We evaluate our proposed method on the real-world task of 3D object detection for autonomous driving. For this challenging setting, VLMine needs to identify long-tail objects within LiDAR point clouds using the corresponding camera images.\nSetup. For our experiments, we use the large-scale Waymo Open Dataset (Sun et al. 2020). The dataset consists of 1150 sequences with each sequence containing 20 seconds of driving data captured at 10 Hz. Each frame consists of a LiDAR point cloud and 5 camera images. We follow the active learning setup from (Jiang et al. 2022), which achieves the state-of-the-art for this task. We randomly split the sequences within the training set into a 10% \"labeled\" pool and a 90% \"unlabeled\" pool. The large \"unlabeled\" pool can be viewed as the stream of data coming from the autonomous fleet, from which we want to mine long-tail examples. For these experiments, the goal is to identify a small portion of the \"unlabeled\" pool (only 3% following (Jiang et al. 2022)) that adds the most value when added to the training set.\nExisting self-driving benchmarks lack fine-grained interor intra-class labels. For that reason, (Jiang et al. 2022) uses the shape of an object as a proxy for intra-class long-tailness. Their assumption is that a vehicle with a larger size and a pedestrian with a smaller height are part of the long-tail. Hence, they evaluate the mining techniques by decomposing the Average Precision (AP) metrics for the vehicle and pedestrian classes. Specifically, they report detection performance on larger vehicles (longer than 8 and 10 meters) and smaller pedestrians (shorter than 1.4 and 1.2 meters). Although these length/height-based decompositions are far from ideal for evaluating long-tail performance, according to the analysis in (Jiang et al. 2022), these road users are uncommon in the Waymo Open Dataset. Our baseline for comparisons is D-REM (Jiang et al. 2022), which is an uncertainty-based approach using an ensemble of models to infer the novelty score.\nPrompt and Implementation Details. For these experiments, we again use LLaVAv1.5-7B (Liu et al. 2024) as the VLM, but generate keywords using a LLM, GPT3.5-turbo (OpenAI 2023). We prompt the VLM to describe the camera images with the following: Describe the uncommon or abnormal vehicles, pedestrians, and cyclists related to traffic in this image. Afterwards, we prompt the LLM to summarize the description into a set of keywords using: Please return keywords for each image description in this format: keyword1, keyword2, and"}, {"title": "5 Conclusion", "content": "This work proposes VLMine, a simple yet effective long-tail data mining algorithm that leverages the knowledge from a vision language model. VLMine provides a strong signal to detect long-tail examples which is complementary to the signals given by traditional uncertainty-based approaches. Furthermore, we show how our proposed Pareto mining can integrate multiple mining signals to further enhance performance."}, {"title": "A Appendix", "content": "A.1 Analysis of Mined Examples\nWe visualize the data mined by VLMine in Section 4.2 for the Waymo Open Dataset. Figure 8 shows the images and keywords for mined long-tail vehicles. Figure 9 depicts some typical failure modes and/or less accurate keywords from VLMine. In Figure 9a-d, since the objects are hard to describe, VLMine fails to give accurate keywords. However, such failures may not degrade the quality of the mined data. The content of the images is often part of the long-tail, and although the keywords are not accurate, their frequencies are lower resulting in a higher novelty score. Figure 9e illustrates a case where VLMine detects an improperly parked van but misses the fire truck. Figure 9f shows a case where VLMine wrongly recognizes a yellow wall as a yellow truck.\nA.2 Pareto Mining Visualization\nFigure 10 illustrates how the signal from VLMine differs from traditional uncertainty-based approaches like D-REM. We depict four types of examples corresponding to long-tail vehicles. Three are Pareto examples where at least one of the mining methods gives the example a high score. The other is a dominated example where both techniques give the example a low score. The examples that have a high novelty score for both D-REM and VLMine are indeed longtail, such as the trolley train and supply truck. D-REM will often score challenging yet common vehicles highly when VLMine gives them a small novelty score. We obverse that large vehicles tend to be given a higher uncertainty by DREM. As a result, typical examples with a low D-REM scores but a high VLMine scores are regular-sized but uncommon vehicles, such as a small-sized dump truck or a detached trailer. The examples where both scores are low are common low-complexity scenes with few objects.\nA.3 Ablation Studies\nPrompt Design We study how the design of the prompt influences the final results. For this study, we use ImageNetLT. For our experiments in Section 4.1, we prompt the VLM to provide a description of the image with What are the possible classes for this image? Give three possible answers. We find such a prompt, which we refer to as Prompt 1, will force the model to produce more informative descriptions of the images and thus benefits the data mining. To illustrate this, we conduct an experiment with the following prompt: Describe the object in the image, which we refer to as Prompt 2. Here are the outputs of the VLM when the two prompts are used for the image shown in Figure 7.\nPrompt 1: 1. Sports and Recreation: The image features a man holding a large fish, which could be a trophy catch from a fishing trip. This suggests that the man is likely an avid fisherman or enjoys spending time outdoors engaging in recreational activities. 2. Nature and Wildlife: The image showcases a man holding a fish, possibly a walleye, in a natural setting. This could be a representation of the beauty of nature and the importance of preserving and protecting aquatic ecosystems. 3. Fishing and Angling:\nPrompt 2: \"The object in the image is a large fish, specifically a muskie, which is being held by a man.\"\nIntegrating Novelty Scores The proposed Pareto mining provides a simple and principled mechanism to integrate the long-tail signals from multiple algorithms. A linear combination of the novelty scores is another way to leverage multiple signals. In this section, we compare Pareto mining with linear combination. Since the scores from different algorithms might have different statistics, we first normalized all the scores to have a mean of zero and a standard deviation of one. We combine all novelty scores for a particular example by averaging the normalized scores. Overall, we observe that Pareto mining consistently outperforms the linear combination.\nA.4 Remark on One-step Keywords Generation\nIt is noteworthy that we use a two-step approach to generate keywords for each example. We start by obtaining a description of the example by prompting the VLM, and then in the second step, we extract the keywords from the description using a rule-based approach or a LLM. Alternatively, we can directly ask the VLM to describe the example using keywords. However, we find that the VLM might output non-informative keywords, miss important objects, or generate inconsistent text. For example, using the following prompt: List the uncommon or abnormal vehicles, pedestrians and cyclists related to traffic in this image. Return only the keywords in the following format: keyword1, keyword2, etc. gives the subsequent results. For Figure 8a, the output is repetitive and inconsistent: \"Concrete truck, car, cement mixer, cement truck, white car, concrete truck, cement"}, {"title": "A.5 Remarks on Hallucination", "content": "The VLM may hallucinate on some input images, but we find that hallucinations often have repeating patterns. For example, a typical case is to hallucinate the presence of a dog when the image shows a busy city street. Below is an example of hallucination when Figure 13 is used as the input image: \"In the image, there are several cars and a bus, which are common vehicles on the road. However, there is also a dog crossing the street along with the group of pedestrians, which is an unusual sight. Additionally, there are multiple bicycles in the scene, including one with a person riding it, which is another uncommon mode of transportation in this particular scene. The presence of these unconventional vehicles and pedestrians creates a unique and lively atmosphere in the busy city street.\"\nSince such hallucinations share common patterns, they will appear a considerable amount of times when describing the images of a typical autonomous driving dataset. We argue that our approach is robust to these types of hallucinations for two reasons: if the hallucination is not related to the traffic object we are interested in, it will be filtered by the LLM; even if it is related to traffic objects, since it is repeated multiple times, the keywords that come from hallucination will have a relatively high frequency. Of course hallucination is not ideally, but we expect VLMine to improve as the underlining VLM improves."}]}