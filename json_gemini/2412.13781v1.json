{"title": "Meta-Reflection: A Feedback-Free Reflection Learning Framework", "authors": ["Yaoke Wang", "Yun Zhu", "Xintong Bao", "Wenqiao Zhang", "Suyang Dai", "Kehan Chen", "Wenqiang Li", "Gang Huang", "Siliang Tang", "Yueting Zhuang"], "abstract": "Despite the remarkable capabilities of large language models (LLMs) in natural language understanding and reasoning, they often display undesirable behaviors, such as generating hallucinations and unfaithful reasoning. A prevalent strategy to mitigate these issues is the use of reflection, which refines responses through an iterative process. However, while promising, reflection heavily relies on high-quality external feedback and requires iterative multi-agent inference processes, thus hindering its practical application. In this paper, we propose Meta-Reflection, a novel feedback-free reflection mechanism that necessitates only a single inference pass without external feedback. Motivated by the human ability to remember and retrieve reflections from past experiences when encountering similar problems, Meta-Reflection integrates reflective insights into a codebook, allowing the historical insights to be stored, retrieved, and used to guide LLMs in problem-solving. To thoroughly investigate and evaluate the practicality of Meta-Reflection in real-world scenarios, we introduce an industrial e-commerce benchmark named E-commerce Customer Intent Detection (ECID). Extensive experiments conducted on both public datasets and the ECID benchmark highlight the effectiveness and efficiency of our proposed approach.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Yang et al., 2024; Dubey et al., 2024) have demonstrated exceptional proficiency in diverse natural language processing tasks, e.g., general language understanding (Wei et al., 2022a), generation (Pu and Demberg, 2023), and reasoning (Wei et al., 2022b; Yao et al., 2024). However, recent quantitative analyses revealed that contemporary frontier LLMs frequently exhibit undesirable and inconsistent behaviors, including unfaithful reasoning (Turpin et al., 2024) and the production of seemingly plausible yet inaccurate hallucinations (Rawte et al., 2023), especially when applying for intricate tasks. Such flawed outputs significantly undermine trust in LLMs and pose substantial obstacles to their widespread adoption in real-world applications.\nThe undesirable phenomenon of LLMs is somewhat similar to human problem-solving, i.e., we humans do not always generate the best answer on our first try in complex real-life scenarios. While dealing with complex problems, individuals has the capacity to actively refine their answers through a cycle of trial, inspection and correction (Pan et al., 2023). This capacity called Reflection, enables us to perform better than machines in high-level reasoning and would be the most precious capacity for modern AI. To simulate this ability, LLMs\u2019 Reflection (Madaan et al., 2024; Shinn et al., 2023) is devised to mitigate the flawed outputs of LLMs, which utilizes feedback from external sources (e.g., the environment or other LLMs) to prompt the models to adapt their responses. This approach, as shown in Figure 1(a), enables the models to iteratively improve their performance by incorporating new information and adjusting their outputs based on external input, thereby enhancing their accuracy and reliability over time. Upon reflection, however, contemporary approaches heavily rely on high-quality external feedback or ground-truth golden labels (Huang et al., 2024; Dou et al., 2024), which are often unavailable during inference scenarios. Besides, reflection typically requires iterative multi-agent inference processes (Du et al., 2023), which are resource-intensive. These aforementioned issues significantly constrain the practical deployment of LLMs in real-world scenarios.\nIn this paper, we propose Meta-Reflection, a novel reflection mechanism that operates without external feedback and requires only a single inference pass. Drawing inspiration from human cognitive processes (Kolodner, 1992), where individuals leverage past experiences and reflections to address similar questions without additional trials, we introduce a learnable meta-reflection codebook to store and retrieve reflective insights, as shown in Figure 1(b). During optimization, reflections are constructed using the vanilla reflection mechanism and integrated into the meta-reflection codebook. At inference, question-specific insights are retrieved from the codebook to guide the LLM in solving problems. This method enables LLMs to produce high-quality responses in a single pass, effectively mimicking how humans utilize prior experiences in analogous situations. Extensive experiments are conducted with open-source LLMs on diverse benchmarks, including programming, mathematical reasoning, and customer intent detection in E-commerce Intelligent Customer Service (ICS) for industry-specific scenarios. To evaluate our method in the ICS domain, we introduce E-commerce Customer Intent Detection (ECID), a new Chinese dataset designed to identify users' core intents, critical for enhancing service quality. Results across domains validate the efficiency and effectiveness of our approach. Key contributions of this work include:\n\u2022 We propose Meta-Reflection, an innovative approach that achieves reflection in a single pass without iterative trials and feedback through well-designed codebook-based storage and retrieval mechanisms.\n\u2022 We present a new dataset for E-commerce Customer Intent Detection (ECID) in the intelligent customer service domain, comprising 1,170 cases from real-world application.\n\u2022 Extensive experiments across various domains"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Reflection for Large Language Models", "content": "Large language models (LLMs) (Achiam et al., 2023; Yang et al., 2024; Dubey et al., 2024), despite their exceptional performance, still exhibit undesired behaviors such as unfaithful reasoning (Turpin et al., 2024), hallucination (Rawte et al., 2023), and toxic generation (Zhang et al., 2024a). Reflection techniques (Pan et al., 2023; Shinn et al., 2023; Madaan et al., 2024) address these issues by utilizing feedback to guide LLMs in refining their outputs. For instance, Self-Refine (Madaan et al., 2024) uses a single LLM to generate, critique, and refine outputs, while Reflexion (Shinn et al., 2023) employs memory mechanisms and LLM agents to reflect on generations and feedback. Renze and Guven (2024) demonstrated the effectiveness of various reflection types across different domains. Nevertheless, reflection techniques often require high-quality external feedback or golden labels, typically unavailable during deployment (Huang et al., 2024; Dou et al., 2024), and frequently involve multi-agent inference processes, incurring significant computational costs. While Dou et al. (2024) incorporates reflective information through self-training, its implicit incorporation leads to suboptimal results. In this work, we propose Meta-Reflection, which incorporates reflective information into a learnable codebook, enhancing performance across various tasks."}, {"title": "2.2 Parameter-Efficient Fine-Tuning (PEFT)", "content": "Parameter-Efficient Fine-Tuning (PEFT) methods enable adaptation of large pretrained models to downstream applications while avoiding the computational costs of full parameter fine-tuning (Hu et al., 2023). These methods can be broadly categorized into two primary approaches: adapter-based and prompt-based methods. Adapter-based methods introduce additional trainable parameters to a frozen pretrained model, with notable implementations including LoRA (Hu et al., 2021) and Llama-Adapter (Zhang et al., 2023). Prompt-based methods transform the discrete optimization of identifying optimal hard prompts into a continuous optimization problem using soft prompts, exemplified by Prefix-Tuning (Li and Liang, 2021), Prompt-Tuning (Lester et al., 2021), and P-Tuning (Liu et al., 2022). In this work, we propose a lightweight learnable codebook module capable of storing and retrieving question-specific reflections, thereby enhancing LLM performance across diverse tasks."}, {"title": "3 Method", "content": "In this section, we first present the process of LLM-based reflection generation in Section 3.1. Next, we describe our proposed implicit feedback-free reflection approach in Section 3.2. Subsequently, we introduce the concept of adaptive meta-reflection alignment in Section 3.3. Finally, the overall optimization stage and inference stage are outlined in Section 3.4. The pipeline of Meta-Reflection is illustrated in Figure 2."}, {"title": "3.1 LLM-based Reflection Generation", "content": "Formally, consider a dataset $\\mathcal{U} = \\{(x,y)\\}_{i=1}^N$, where $x$ represents a question and $y$ represents its corresponding answer. An actor LLM agent $\\mathcal{M}$ is used to generate an initial output $\\hat{y}_{act} = \\mathcal{M}(x)$. However, this process may lead to unfaithful reasoning or hallucination (Pan et al., 2023). To address these issues, reflection methods (Shinn et al., 2023; Madaan et al., 2024) propose leveraging feedback from external environment or golden labels (Huang et al., 2024) to refine the initial output $\\hat{y}_{act}$. This feedback, denoted as $e = \\mathcal{E}(x, \\hat{y}_{act})$ where $\\mathcal{E}$ represents the environment, provides comprehensive assessment of the initial output. For instance, in programming tasks, feedback typically includes interpreter information or execution results, while for mathematical problems, it involves comparing outputs against correct answer $y$. Based on the feedback $e$, a reflector LLM agent $\\mathcal{R}$ generates reflections $r = \\mathcal{R}(x, e)$, which guide the actor model $\\mathcal{M}$ to produce refined responses $\\hat{y}_{ref} = \\mathcal{M}(x,r)$. As shown in Figure 2(a), this iterative process of generation, reflection, and refinement aims to enhance the quality and accuracy of the actor model $\\mathcal{M}$'s outputs, mitigating potential errors and improving overall performance (Pan et al., 2023). Throughout the reflection generation process, we systematically curate a new dataset $\\mathcal{D}_t = \\{(x, r, \\hat{y}_{ref})\\}_{i=1}^N$ containing reflection-question-answer triplets. Details and corresponding prompts are provided in the Appendix."}, {"title": "3.2 Implicit Feedback-free Reflection", "content": "As discussed in Section 1, reflection methodologies, while promising, are limited by their reliance on external feedback (Huang et al., 2024) and computationally intensive multi-agent inference processes, hindering practical deployment. Inspired by the adage \"One never falls into the same ditch twice,\" which suggests that people learn from past mistakes without repeated feedback, we propose implicit feedback-free reflection. As shown in Figure 2(b), this approach uses a learnable meta-reflection codebook to store and retrieve reflective insights, enabling efficient, feedback-free inference.\nMeta-Reflection Codebook. The meta-reflection codebook consists of implicit reflective units $P \\in \\mathbb{R}^{K\\times C}$, where $K$ and $C$ denote codebook length and feature dimension, respectively. The question $x$ serves as the query to retrieve the relevant reflective units from the codebook. Previous studies have demonstrated that intermediate layer features can provide sufficient preliminary understanding of input samples (Xin et al., 2020; Zhang et al., 2024b). Leveraging this insight, we utilize query representations from intermediate LLM layers, which contain rich semantic information for effective retrieval. Specifically, we position the meta-reflection codebook at the $L$-th layer ($0 < L < N$), where $N$ is the total layers of LLM, serving as a repository of reflective insights. To retrieve relevant reflective insights, the query is processed through the initial $L$ layers, transforming it into hidden states $H_{query}$. We subsequently employ mean pooling $P_{mean}$ to derive sentence-level representation as follows:\n$h = P_{mean}(H_{query}) \\in \\mathbb{R}^{1\\times C}$         (1)\nThe representation of the query is utilized to compute relevance score through:\n$S = \\sigma(\\frac{g(h)f(P^T)}{\\sqrt{K}}) \\in \\mathbb{R}^{1\\times K}$,                      (2)\nwhere $\\sigma$ denotes the softmax function, and $g(\\cdot)$ and $f(\\cdot)$ represent transformation functions implemented as two-layer MLPs, which serve to stabilize the training process (Liu et al., 2022). The resulting score $s$ quantifies the relevance between the question and reflective units from codebook, with higher scores indicating more applicable reflective units for the given query. Based on the score $s$, we select the top-k relevant reflection units from the codebook to form the sequence $P_{ref} \\in \\mathbb{R}^{k\\times C}$, maintaining their relative positions in the codebook. The concatenated sequence $\\{H_{query}; P_{ref}\\}$ is fed into the remaining ($N \u2013 L$) layers, incorporating question-specific reflective insights that guide the LLM's solution approach and enhance its performance. Notably, during the training phase, only the meta-reflection codebook is tunable while the backbone model remains frozen.\nSampling Strategy. To address the non-differentiable top-k function that impedes gradient back-propagation during training, and to enhance the sampling diversity, we employ Gumbel-Softmax technique (Jang et al., 2017) with additional tricks (Bengio et al., 2013) to derive the sampling process:\n$\\hat{s} = \\sigma(\\log(s) + \\epsilon_{gumbel}) \\in \\mathbb{R}^{1\\times K}$,\n$I = 1_{i \\in topk(s)} - sg[\\hat{s}] + \\hat{s} \\in \\mathbb{R}^{1\\times K}$,          (3)\nwhere $\\epsilon_{gumbel} \\in \\mathbb{R}^{1\\times K}$ represents the Gumbel noise, $sg[\\cdot]$ denotes the stop gradient operator and $1_{i \\in topk(s)}$ indicates whether an index belongs to the top-k indices. The resulting indicator vector $I$ identifies the selected reflective units. This strategy ensures both differentiability during training and diverse sampling of reflective units."}, {"title": "3.3 Adaptive Meta-Reflection Alignment", "content": "After acquiring the dataset $\\mathcal{D}_t$ as outlined in Section 3.1, our objective is to effectively leverage the information encapsulated within reflection $r$. As depicted in Figure 2(c), we employ a same frozen LLM but with different input as the teacher model $\\mathcal{M}_{ref}$, to process the input sequence $\\{x, r\\}$ and extract the hidden states for each layer, $\\{P_{que}, P_{ref}\\}_{i=1}^N$, where $P_{que}$ and $P_{ref}$ denote the hidden states of query and reflection sequences, respectively. The reflective units selected from the codebook are integrated into the final $N \u2013 L$ layers, yielding $\\{P_{ref}^l\\}$, with the purpose of aligning $\\{P_{ref}^l\\}$ and thereby embedding valuable information into the meta-reflection codebook. However, the dimensional variations and semantic misalignment between the ground-truth reflection $P_{ref}^l$ and the reflective units $P_{ref}^{l}$ pose challenges for precise alignment between these sequences. To overcome this, we employ the optimal transport (OT) algorithm (Rubner et al., 2000; Liu et al., 2020; Zhang et al., 2020), which applies the earth mover's distance (EMD) to gauge the semantic discrepancy between these two sequences.\nOT for Meta-Reflection Alignment. The EMD quantifies the distance between two discrete distributions as the minimum cost of transporting piles of dirt from \"suppliers\" to \"demanders\" (Zhu et al., 2022), framed as a linear optimization problem. Specifically, at the l-th ($L < l < N$) layer, we measure the distance required to transform $P_{ref}^{l'} \\in \\mathbb{R}^{k' \\times C}$ to $P_{ref}^l \\in \\mathbb{R}^{k \\times C}$. Let each unit $p_i \\in P_{ref}^{l'}$ possesses a total of $r_i$ quantities to transport, and each unit $p_j \\in P_{ref}^l$ requires $c_j$ quantities, forming the transport prototype:\n$\\Pi(r, c) = \\{\\Gamma \\in \\mathbb{R}^{k' \\times k} | \\Gamma 1_k = r, \\Gamma^T 1_{k'} = c\\}$,    (4)\nwhere $r \\in \\mathbb{R}^{k'}$ and $c \\in \\mathbb{R}^k$ are marginal weights for transportation matrix $\\Gamma$ respectively. $1$ is all-one vector with corresponding size, and $\\Pi(r, c)$ is the set of all possible distributions whose marginal weights are $r$ and $c$.\nWe define the cost per unit transported from supplier token $\\hat{i}$ to demander token $\\hat{j}$ as:\n$D_{ij} = \\frac{1}{||p_i|| ||p_j||}$          (5)\nwhere tokens with similar representations incur lower transport costs. Given this, we can define the linear optimization problem as follows:\n$\\mathcal{R}_{OT}(r, c) =  \\sum_{i}^{k'} \\sum_{j}^{k} D_{ij}\\Gamma_{ij}$               (6)\nHowever, The exact minimization over $\\Gamma$ is solved in polynomial time and can be computationally intractable (Arjovsky et al., 2017; Genevay et al., 2018). Therefore, to find the optimal $\\Gamma$, we utilize Sinkhorn Algorithm (Cuturi, 2013) as an efficient approximation method. The detailed algorithm and the optimization process are shown in Appendix A. With optimal transportation matrix $\\tilde{\\Gamma}$, the corresponding alignment loss for layer l is:\n$\\mathcal{L}_{OT}^l = (\\tilde{\\Gamma}, D)_F$,          (7)\nand the overall alignment loss is calculated as the mean across the last $N - L$ layers:\n$\\mathcal{L}_{OT} = \\frac{\\sum_{l=L}^{N} \\mathcal{L}_{OT}^l}{N - L}$          (8)\nThe alignment loss quantifies the semantic gap (Li et al., 2020) between the reflective units from the meta-reflection codebook and the ground-truth reflection. In our scenario, by minimizing the $\\mathcal{L}_{OT}$, the reflective insights from ground-truth reflection are incorporated into the codebook, enhancing the model $\\mathcal{M}$'s capacity to handle complex tasks and improve overall performance."}, {"title": "3.4 Optimization and Inference", "content": "We delineate the overall optimization and inference stages as follows:\nProgressive Optimization Stage. We employ a progressive optimization paradigm to enhance model performance. Initially, we utilize $\\mathcal{L}_{OT}$ to align the reflective units from codebook with ground truth reflections, infusing reflective information into the codebook of the model $\\mathcal{M}$. Subsequently, we leverage labels from dataset $\\mathcal{D}_t$ to fine-tune the codebook using the vanilla supervised learning loss $\\mathcal{L}_{SFT}$. This optimization paradigm ensures stable training progression and effective incorporation of reflective information, enhancing the model's ability to capture and utilize this knowledge while maintaining overall learning stability.\nInference Stage. During the inference stage, the input question $x$ serves as query to retrieve pertinent reflective units from the meta-reflection codebook, guiding the LLM in addressing complex tasks. The retrieval process, elucidated in Section 3.2, is executed only once at the generation of the initial token. Leveraging the characteristics of causal language models, this inference stage can also utilize KV caching (Pope et al., 2023) to mitigate computational overhead."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Datasets", "content": "We assess our method on diverse datasets across different domains: programming (i.e., MBPP, HumanEval), mathematical reasoning (i.e., GSM8K), and E-commerce customer intent detection (i.e., ECID). Details can be found in Appendix B and C.\nProgramming. We evaluate our approach on two Python code generation benchmarks (MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)), using Pass@k metric to measure the percentage of problems that successfully pass all unit tests within k attempts (Dou et al., 2024).\nMathematical Reasoning. For mathematical reasoning task, We employ the Grade School Math 8K (GSM8K) dataset (Cobbe et al., 2021) for evaluating Meta-Reflection. We utilize the Exact Match (EM) metric between the generated response and the correct answer (Madaan et al., 2024).\nE-commerce Customer Intent Detection (ECID). Intelligent Customer Service (ICS) in e-commerce"}, {"title": "4.2 Experimental Setup", "content": "Models. We evaluate Meta-Reflection across various open-source LLMs. For the actor models, we utilize Qwen-2-7B-Instruct (Yang et al., 2024), Llama-3.1-8B-Instruct (Dubey et al.,"}, {"title": "4.3 Main Results", "content": "Tables 1, 2, and 3 present the experimental results across three distinct domains: programming, mathematical reasoning, and e-commerce customer intent detection.\nOur empirical investigation reveals fundamental limitations in base LLMs' domain-specific capabilities, as demonstrated by CodeLlama's modest 40.4% performance on MBPP under the Pass@1 metric. This deficiency primarily stems from these models' insufficient domain knowledge and capabilities. While tuning with PEFT methods like LoRA demonstrate potential for improvement, the gains remain incremental-yielding mere 1.2% and 0.2% improvements in Zero-Shot and Few-Shot settings respectively. This suggests that current supervised learning paradigms, while domain knowledge internalization during finetuning, fail to address the critical need for guidance during inference.\nRecent advances in reflection-based methodologies, particularly Re-ReST, have shown promise by implicitly incorporating reflective guidance through refined self-training data, evidenced by LLaMA-3.1's 1.7% performance improvement over LoRA on GSM8K. However, these approaches still neglect the crucial aspect of explicit, granular guidance during the inference phase. Although leveraging RAG-retrieved reflections as explicit guidance appears promising, empirical results on benchmarks like GSM8K and ECID demonstrate suboptimal performance even compared to common reasoning approaches. This degradation occurs because retrieved reflections, though relevant to source problems, often lack precise applicability to similar cases and may introduce noise, particularly in mathematical tasks requiring fine-grained guidance. Comprehensive case studies supporting these findings are presented in Appendix G. Our proposed methodology addresses these limitations by providing explicit, fine-grained reflective guidance during inference, significantly outperforming existing approaches across all baseline metrics."}, {"title": "4.4 Inference Efficiency Analysis", "content": "We evaluate the inference efficiency of Meta-Reflection, with results presented in Table 4. Compared to existing reflection-based methods like Reflection-RAG that require separate encoders and knowledge base retrieval, our approach leverages LLM's intermediate layer representations for retrieval. Furthermore, RAG-based methods store knowledge in a discrete format, necessitating a large-scale knowledge base. In contrast, Meta-Reflection captures knowledge and reflective insights in a dense format, enabling the construction of a smaller, more compact knowledge base, thereby significantly reducing computational overhead. Notably, our method achieves comparable first-token latency to common reasoning approaches while maintaining the benefits of reflection-based reasoning, demonstrating its practicality for real-world applications."}, {"title": "4.5 Sensitive Analysis", "content": "We perform sensitivity analysis on three critical hyper-parameters of Meta-Reflection: inserted layers, codebook size, and number of reflective units. The Experimental results are presented in Figure 3."}, {"title": "4.6 Ablation Study", "content": "In this section, we conduct a comprehensive ablation study to evaluate the impact of various components in Meta-Reflection. We examine three key variants: 'w/o Codebook' (no meta-reflection codebook), 'w/o Sampling' (no sampling strategy defined in Equation 3), and 'w/o Alignment' (no alignment mechanism described in Equation 8). As illustrated in Figure 4, the meta-reflection codebook demonstrates significant effectiveness in storing and retrieving reflective units that guide LLMs through the problem-solving process. The ablation analysis further demonstrates that both the sampling strategy and meta-reflection alignment mechanism play crucial roles in maintaining solution diversity and incorporating reflective insights, respectively, thereby enhancing overall performance."}, {"title": "4.7 Visualization", "content": "We visualize the selection frequency distribution of reflective units within the meta-reflection codebook. As shown in Figure 5, the selection patterns of reflective units vary significantly. Notably, certain units exhibit higher selection frequencies, potentially reflecting commonly applicable insights, whereas others are selected less frequently, suggesting their specialized nature. Additional visualization results are provided in Appendix H."}, {"title": "5 Conclusion", "content": "In this paper, we introduce Meta-Reflection, a novel feedback-free reflection mechanism that operates with a single inference pass without requiring external feedback. Our approach incorporates reflective insights within a codebook structure, facilitating efficient storage, retrieval, and utilization of historical insights to guide LLMs in problem-solving tasks. To validate the practical applicability of our method, we propose a new industrial benchmark: E-commerce Customer Intent Detection (ECID). Comprehensive experiments conducted across diverse domains and the ECID benchmark demonstrate the effectiveness and efficiency of Meta-Reflection."}, {"title": "6 Limitations", "content": "This work introduces Meta-Reflection, a novel feedback-free reflection mechanism that operates with a single inference pass without requiring external feedback. However, Meta-Reflection is primarily applicable to parameter-accessible LLMs (e.g., Qwen and LLaMA) and cannot be extended to models where parameters are inaccessible through API-only interfaces (e.g., ChatGPT and Claude)."}, {"title": "A Sinkhorn Algorithm and Optimal Transport", "content": "The vanilla optimization problem of optimal transport, as formulated in Equation 6, aims to find the optimal transportation matrix $\\Gamma$. Nevertheless, the exact minimization over is generally computationally intractable (Arjovsky et al., 2017; Genevay et al., 2018; Li et al., 2020). To address this, the Sinkhorn Algorithm (Cuturi, 2013) is utilized to approximate $\\Gamma$. Specifically, the algorithm introduces a regularization term:\n$\\min_{\\Gamma \\in \\Pi(r,c)} (\\Gamma, D)_F + \\frac{1}{\\lambda} \\Gamma(\\log \\Gamma \u2013 1)$,          (9)\nwhere $(,)_F$ denotes Frobenius inner product, and $\\lambda$ is a hyper-parameter that controls the strength of regularization.\nWith this regularization term, the optimal $\\Gamma$ can be approximated as:\n$\\Gamma = diag(v)Qdiag(u)$,                   (10)\nwhere $Q = e^{-\\lambda D}$, and $v, u$ are two coefficient vectors whose values can be iteratively updated as:\n$v_i^{t+1} = \\frac{r_i}{\\sum_{j=1}^{k'} Q_{ij}u_j}$,\n$u_j^{t+1} = \\frac{c_j}{\\sum_{i=1}^{k} Q_{ij}v_i^{t+1}}$             (11)\nThe critical aspect then lies in determining the marginal weights r and c, which control the total supplying and demanding units, respectively. A larger weight indicates that the reflective unit exhibits semantic similarity to the ground truth reflection tokens. We define the weight as dot product between its embedding and the mean pooling embedding from the other set:\n$r_i = max\\{\\frac{p_i}{\\frac{k}{\\Sigma_{j=1} p_j}} - ,0\\}$,\n$c_j = max\\{\\frac{p_j'}{\\frac{k'}{\\Sigma_{i=1} p_i'}} -,0\\}$         (12)\nAfter obtaining the approximated optimal transportation matrix $\\tilde{\\Gamma}$, we can compute the loss as defined in Equation 7."}, {"title": "B E-commerce Customer Intent Detection (ECID) Benchmark", "content": "In the domain of Intelligent Customer Service (ICS) for e-commerce, effectively and efficiently discerning customers' core intentions when they contact ICS for assistance is critical to enhancing service quality (Cheng et al., 2024; Kolasani, 2023). In this work, we introduce an industrial benchmark, named E-commerce Customer Intent Detection (ECID) to evaluate our proposed method. This dataset is in Chinese, focusing on customer interactions within major Chinese e-commerce platforms. The following sections detail the construction of this dataset and elaborate on its specific tasks.\nTask. The primary objective of the ECID dataset is to infer the core intention of customers seeking ICS assistance, based on previous communication records between customers and customer service platforms, customer purchase histories, and order information. The core intention refers to the customer's current concern or the problem they wish to resolve. Specifically, each data point in the dataset comprises input information from five fields:\n\u2022 Customer Question. The specific issue or obstacle encountered by the customer.\n\u2022 Customer Request. Customer requirements, encompassing all objectives or desired outcomes expressed during interactions with the ICS, sellers, and platform customer service representatives, as well as any proactively initiated request.\n\u2022 Solution. Proposals offered by the platform or sellers to address the customer's issue.\n\u2022 Customer Attitude. The customer's attitudes towards the proposed solutions, as expressed during communication.\n\u2022 Processing status. PThe current state of the customer's submitted request.\nECID aims to match the aforementioned input information with the most appropriate intention from a predefined list. In real-world applications, we categorize intentions into 36 distinct types, each representing a specific issue customers seek to resolve. For the ECID dataset, a condensed list of six intentions is provided, from which the most relevant core intention must be selected. An illustrative example is presented in the accompanying Figure 6.\nData Processing. The ECID dataset is derived from customer service system records of the Taobao e-commerce platform, collected over a single day. From this collection, we randomly sampled 30,000 data points in an unbiased manner. Each data point comprises information from various sources, including customer-service representative chat logs, customer-seller communications, customer order details, and ongoing request processing records. We employed a fine-tuned LLM, specifically Qwen2-7B-Instruct, to extract the aforementioned five fields of information from the diverse sources.\nWe initially applied a rule-based method to eliminate incomplete or inconsistent data (such as newly registered users without any purchase history), resulting in approximately 4,000 refined data points. Subsequently, we utilized GPT-4-turbo-128k and Qwen2-72B-Instruct for data labeling. Using a voting system, we selected the most appropriate intention from a predefined list of 36 intentions, along with five secondary matching intentions, to create a set of candidate intentions and answers for each data point. To ensure high data quality, we discarded instances where the highest voting rate was below 80%. We also implemented human evaluation, randomly sampling and verifying the accuracy of answers. This rigorous process yielded 1,170 high-quality data points, each accompanied by a Chain-of-Thought (CoT) reasoning process. The dataset was partitioned into a 7:3 ratio for training and testing.\nWe conducted data anonymization to remove sensitive information from the dataset. Personal identifiable information, including customer names, addresses, and contact details, was redacted. Additionally, all monetary values within the dataset were masked using asterisks (*) to ensure confidentiality."}, {"title": "C Public Datasets", "content": "We evaluate our method across three public datasets spanning diverse domains: two programming benchmarks (MBPP and HumanEval) and one mathematical reasoning dataset (GSM8K).\nProgramming. For evaluating our method on programming tasks, we utilize two Python code programming benchmarks: MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021). The MBPP dataset consists of approximately 1,000 Python programming problems, while HumanEval encompasses 161 problems, each accompanied by comprehensive unit test cases. We adhere to the official train-test split for MBPP, employing its training set for model training. As HumanEval provides only a test set, we use it exclusively for evaluation purposes. Following Dou et al. (2024), we employ the Pass@k metric, which quantifies the percentage of problems where the model successfully passes all unit tests within k attempts. During the code generation process, in line with previous work by Roziere et al. (2023), the actor model is provided with the unit test cases.\nMathematical Reasoning. For mathematical reasoning evaluation, we employ the Grade School Math 8K (GSM8K) dataset (Cobbe et al., 2021), a comprehensive benchmark containing approximately 8,000 grade school mathematics word problems. This dataset is particularly valuable due to its linguistic diversity and high-quality annotations, featuring detailed human-curated solution trajectories and precise answers for each problem (Madaan et al., 2024). Following standard practices, we strictly adhere to the official train-test split (7,473 for training, 1,319 for testing) in our experimental setup. Performance is evaluated using the Exact Match (EM) metric, which assesses the precise correspondence between model-generated responses and ground-truth answers (Madaan et al., 2024), providing a rigorous measure of mathematical reasoning capabilities."}, {"title": "D Baselines", "content": "We evaluate our method against three categories of baselines: Common Reasoning", "below": "nCommon Reasoning Approaches. For common reasoning approaches", "tuning": "LoRA (Hu et al.", "search": "For LoRA", "baselines": "nRe-ReST (Dou et al.", "phases": "first refining the training dataset through reflective incorporation, then conducting model fine-tuning on the enhanced data. This approach enables implicit integration of reflective insights, allowing for improved performance during single-pass inference. We employ their official implementation*, adapting it to our experimental settings with corresponding datasets and base LLMs.\nReflection-RAG implements a Retrieval Augmented Generation (RAG) framework (Gao et al., 2023) for reflection"}]}