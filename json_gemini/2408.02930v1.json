{"title": "The Need for a Big World Simulator: A Scientific Challenge for Continual Learning", "authors": ["Saurabh Kumar", "Hong Jun Jeon", "Alex Lewandowski", "Benjamin Van Roy"], "abstract": "The \"small agent, big world\" frame offers a conceptual view that motivates the need for continual learning. The idea is that a small agent operating in a much bigger world cannot store all information that the world has to offer. To perform well, the agent must be carefully designed to ingest, retain, and eject the right information. To enable the development of performant continual learning agents, a number of synthetic environments have been proposed. However, these benchmarks suffer from limitations, including unnatural distribution shifts and a lack of fidelity to the \"small agent, big world\" framing. This paper aims to formalize two desiderata for the design of future simulated environments. These two criteria aim to reflect the objectives and complexity of continual learning in practical settings while enabling rapid prototyping of algorithms on a smaller scale.", "sections": [{"title": "Introduction", "content": "The real world is an unfathomably complex system, both to humanity and to the agents that it designs. The small agent, big world frame captures this perspective by positing that to an agent with bounded computational resources, or capacity, the world appears complex and non-stationary (Dong et al., 2022). As such, in order for said bounded capacity agent to continually fruitfully engage with the world, it must continuously ingest new knowledge while selectively retaining previously acquired knowledge. We refer to this process as continual learning.\nThus far, the study of continual learning has revolved around the discovery of phenomena which plague the naive application of algorithms designed in traditional machine learning settings. Such phenomena include the likes of \"catastrophic forgetting\" which describes an agent's tendency to forget information from the past (Kirkpatrick et al., 2017; Zenke et al., 2017), and \u201cplasticity loss\" which describes an agent's inability to absorb new information with increased time (Dohare et al., 2021; 2023; Kumar et al., 2023b; Lyle et al., 2023). These studies have brought about the design of synthetic environments which produce artificially generated and controllable data streams. These environments are designed to stress test an agent's resilience to these phenomena.\nHowever, if we frame the goal of continual learning to design agents which continue to engage fruitfully with the world, existing benchmarks do not capture the essence of the world. For instance, on current benchmarks, evaluation metrics for catastrophic forgetting measure an agent's ability to remember everything in the past, which is unlikely to be necessary to fulfill a definition of fruitful"}, {"title": "A Common Recipe for Synthetic Continual Learning Benchmarks", "content": "Synthetic benchmarks are useful for rapid prototyping, enabling a researcher to evaluate an agent's capabilities with respect to an array of identified challenges (Osband et al., 2020; 2022). Despite three decades of active research on continual learning (Ring, 1994; Thrun, 1998), there remains comparative difficulty in i) identifying the challenges unique to continual learning and (ii) measuring progress made on previously identified challenges. For example, recent work highlights that the challenge of catastrophic forgetting associated with existing continual learning benchmarks has been addressed by computationally inefficient solutions which are infeasible in most real-world scenarios (Prabhu et al., 2020). In this section, we briefly discuss a commonly used recipe for creating synthetic continual learning benchmarks and its fundamental limitations.\nMost synthetic continual learning benchmarks use the following recipe: take an existing non-continual dataset and create a continual learning problem by incorporating some form of non-stationarity. This data stream is often delineated into a sequence of tasks, where the non-stationary is applied at specific points in time to create a new task by transforming the observation distribution or the target function. Within each task, the environment faced by the agent is stationary. For example, in Permuted MNIST (Srivastava et al., 2013; Goodfellow et al., 2013; Kirkpatrick et al., 2017), each task involves a fixed, distinct permutation of the pixels of all images in the MNIST dataset, and the challenge is to classify the digit of each image with these new inputs (See Appendix A)."}, {"title": "Formalizing the Environment and Agent", "content": "Before outlining the criteria for a big world simulator, we first propose information-theoretic characterizations of an environment and an agent. These characterizations will help formalize the notion that the world appears complex and non-stationary to a bounded capacity agent, necessitating a trade-off between ingesting and ejecting information."}, {"title": "Notation", "content": "We begin with a review of notation and probability and information theory that we will use in this paper. We define random variables with respect to a common probability space (\u03a9, F,P). For a random variable $X : \\Omega \\rightarrow \\mathcal{X}$, we use $P(X \\in \\cdot)$ to denote the distribution of $X$. We use $H, I$ to denote (conditional) entropy and mutual information respectively. Concretely, for random variables $X, Y, Z$\n$H(X|Y) = \\mathbb{E}\\left[ \\ln \\frac{1}{P(X|Y)} \\right]$; $I(X; Y|Z) = H(X|Z) \u2013 H(X|Y, Z)$.\nWe use the notation $d_{KL}$ to denote the KL divergence, which maps two probability distributions to $\\mathbb{R_+}\\cup {+\\infty}$, where $\\mathbb{R_+}$ denotes the non-negative reals."}, {"title": "The Environment", "content": "Let $(\\mathcal{X}, \\mathcal{E}_X)$ be a measurable space which consists of the set of observations and a sigma-algebra on that set. Let $\\mathcal{H} = {\\mathcal{X}^n : n \\in \\mathbb{N}}$ denote the set of all histories of observations. An environment is identified by a function $f$ which maps $\\mathcal{H}$ to the set of probability measures $\\mathcal{E}_X \\rightarrow [0,1]$.\nWe assume that the environment generates a stream of observations which is characterized by a stochastic process $(X_0, X_1, X_2,...)$ defined with respect to a probability space $(\\Omega, \\mathcal{F}, P)$. The conditional distribution $P(X_{t+1} \\in \\cdot|H_t) = f(H_t)$, where for all $t \\in \\mathbb{Z_+}$, $H_t = (X_0, X_1, ..., X_t) \\in \\mathcal{X}^n$ denotes the history of observations up to time $t$."}, {"title": "The Agent", "content": "In this work, we consider an agent to be a map $\\pi: \\mathcal{U} \\times \\mathcal{X} \\rightarrow \\mathcal{U}$ where $\\mathcal{U}$ denotes a set of agent states. For all $t$, we let $U_t = \\pi(U_{t-1}, X_t)$ denote the agent state at time $t$, which is computed from the previous agent state $U_{t-1}$ and the latest observation $X_t$. We represent constraints placed on the agent via an informational constraint $I(H_t; U_t) < c$ for all $t$. We refer to this value $c$ as the capacity of the agent."}, {"title": "Predictions and Error", "content": "An agent is evaluated for its ability to accurately predict future observations. Concretely, for any agent $\\pi$, we define the error of $\\pi$ to be\n$\\mathcal{L} = \\limsup_{T \\rightarrow \\infty} \\mathbb{E}_{\\pi} \\left[ \\frac{1}{T} \\sum_{t=0}^{T-1} d_{KL}\\left(\\frac{P(X_{t+1} \\in \\cdot|H_t)}{\\text{environment probability}} || \\frac{P(X_{t+1} \\in \\cdot|U_t)}{\\text{agent prediction}} \\right)\\right]$\nWe characterize an agent's effectiveness via it's error. Since the function $f$ (environment probabili-ties) may depend arbitrarily on $H_t$ it is clear that an agent with limited capacity will incur nonzero error. For all $t$, $U_t$ will likely be missing information about $H_t$ which may be relevant for making pre-dictions about $X_{t+1}$. Therefore, an effective agent ought to selectively remember and forget aspects of $H_t$ which lead to the best predictions about the future, while obeying the capacity constraint."}, {"title": "Desiderata For A Big World Simulator", "content": "Equipped with formal characterizations of an environ-ment, an agent, and error incurred by an agent interacting with the environment, we now propose two criteria for a big world simulator."}, {"title": "There are no diminishing returns to increasing an agent's capacity.", "content": "In the small agent, big world setting, the world is enor-mously - perhaps even infinitely \u2013 complex relative to any finite-capacity agent. Since there is always more to learn about the world, a desired feature of continual learning agents is that endowing an agent with significantly in-creased capacity should invariably lead to substantial im-provement in its capabilities. We frame this feature as a property of a big world simulator.\nTo capture the notion that increasing capacity yields significant performance improvements, we establish a constraint on how the prediction error diminishes as the agent's capacity expands. First, let\n$L(c) = \\inf_{\\pi:I(H_t;U_t) < c} \\Sigma_{\\pi}.$\n$L(c)$ denotes the optimal error incurred by an agent with capacity $c$. We draw inspiration from the research in large language models which observes the desired phenomenon of continued reduction in out-of-sample error for every extra unit of available compute (Kaplan et al., 2020; Hoffmann et al., 2022). In these works, they establish a power law relationship between the error and capacity. This is in contrast to many classical statistical settings in which the reduction in out-of-sample error decays exponentially fast in $c$. We now present the notion of $k$-complexity:\nDefinition 4.1. For $k \\in \\mathbb{R_{++}}$, an environment is $k$-complex if\n$L(c) = O\\left(\\frac{1}{c^k}\\right)$.\nThis definition characterizes an environment's ability to consistently offer performance improve-ments in response to increases in agent capacity. An illustration of what the capacity vs error curve would look like for a 1-complex environment is provided in Figure 1."}, {"title": "An optimal finite capacity agent interacting with the environment will never stop learning.", "content": "Since our goal is to evaluate the continual learning capabilities of agents, the environment should be designed such that an effective capacity constrained agent must never stop learning. To formalize this concept, we begin by framing nonstationarity as a description of the agent's subjective experience.\nDefinition 4.2. An environment $f$ is non-stationary with respect to an agent if\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; X_{t+2: \\infty}|U_t) > 0$.\nThis states that an agent $\\pi$ will always continue to encounter new information in $X_{t+1}$ which will be relevant for making predictions about the future $X_{t+2: \\infty}$ beyond what is currently known in $U_t$. Therefore, in our environment design we hope to design $f$ such that it is non-stationary with respect to all finite-capacity agents $\\pi$.\nWe note that for most environments studied in the literature, $f$ is non-stationary with respect to an unbounded agent for which for all $t$, $U_t = H_t$. Concretely, in these environments,\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; X_{t+2: \\infty}|H_t) > 0$.\nFor instance, in an environment such as Permuted MNIST, there will continue to exist $t$ for which $X_{t+1}$ is data from a new permutation. As a result, this $X_{t+1}$ will contain information about the future sequence which is absent from $H_t$ and hence the environment is non-stationary even to an unbounded agent.\nHowever, we make the point that even if\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; X_{t+2: \\infty}|H_t) = 0,$\n$f$ may be non-stationary with respect to $\\pi$ due to capacity constraints. Since the agent is only able to retain finite bits of information about the past, there will always be more to learn about in the future. We argue that environments which satisfy Equation 1 along with Definition 4.2 better capture the essence of \"small agent, complex world.\"\nOur notion of non-stationarity relates to the work of Abel et al. (2023), which proposes a definition of continual reinforcement learning. That definition offers a formal expression of what it means for an agent to never stop learning. The characterization is subjective in that it defines non-convergence with respect to a basis, which is a fixed set of policies. Definition 4.2 is similarly subjective, defining non-stationarity with respect to a particular agent with a particular agent state.\nGiven our definition of non-stationarity, we return to the implications on the behavior of an optimal finite capacity agent. An agent with finite capacity may need to \"forget\" some information in its agent state in order to incorporate new information, a concept which we elaborate on in Section 5. There may exist environments which are non-stationary but still exhibit a convergent optimal policy. This could be the case for problem settings in which the entire history is equally informative about the future. In such settings, ingesting the new information in $X_{t+1}$ and forgetting old information could be equivalent to ignoring $X_{t+1}$ entirely given capacity constraints.\nHowever, such environments do not reflect reality nor do they align with our goals of developing effective continual learning algorithms. To ensure that the environment adequately evaluates contin-ual learning capabilities, it should be designed so that an optimal finite capacity agent never stops ingesting (and forgetting) information. Concretely, if we assume that the agent state $U_t$ is capacity constrained in that for all $t$, $I(U_t; H_t) < c$, then the environment should require an optimal capacity constrained agent to never stop learning i.e.,\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; U_{t+1}|U_t) > 0$.\nThere will always be new information that the agent will need to incorporate into its agent state by forgetting old information."}, {"title": "Forgetting and Implasticity", "content": "In this section, we draw the connection between our notion of error ($L$) and phenomena described in the literature (notably \"forgetting\" and \"plasticity loss\" or \"implasticity\"). We begin by pro-viding the following decomposition of error into forgetting and implasticity. This decomposition quantitatively improves upon the result of Kumar et al. (2023a) to provide definitions of forgetting and implasticity which better match our intuition. We defer the proof to Appendix B.\nTheorem 1. (forgetting and implasticity) For all agents $\\pi : \\mathcal{U} \\times \\mathcal{X} \\leftrightarrow \\mathcal{U}$, if for all $t$, $U_{t+1} = \\pi(U_t, X_t)$, then\n$\\mathcal{L} = \\limsup_{T \\rightarrow \\infty} \\frac{1}{T} \\sum_{t=0}^{T-2} I(X_{T:t+2}; U_t|U_{t+1}, X_{t+1}) + \\frac{1}{T} \\sum_{t=0}^{T-1} I(X_{T:t+1}; X_t|U_t)$.\nThe forgetting that an agent experiences at time $t + 1$ is the information about the future sequence $X_{T:t+2}$ which was contained in the previous agent state $U_t$, but not in the current agent state $U_{t+1}$ nor the most recent observation $X_{t+1}$. Since agent state $U_{t+1} = \\pi(U_t, X_{t+1})$, this mutual infor-mation exactly captures which relevant information was decanted from the agent state incorporate information from $X_{t+1}$. Note that forgetting is forward looking as it measures mutual information between the previous agent state and the future sequence as opposed to the past sequence as done in the literature. Forgetting about the past is not \"catastrophic\" insofar as what is forgotten is uninformative about the future.\nMeanwhile, the implasticity that an agent experiences at time $t$ is the information about the future sequence $X_{T:t+1}$ which was contained in $X_t$, which we failed to include in $U_t$. This directly reflects what is referred to in the literature as \"loss of plasticity\". An agent loses plasticity, or incurs loss from implasticity, if it fails to digest information about $X_t$ which is relevant for making predictions about the future $X_{T:t+1}$.\nTheorem 1 suggests that if our environment is k-complex, then an effective continual learning agent will effectively reduce $L$ by improving upon forgetting and/or implasticity with this additional compute. Perhaps with this additional capacity, the agent will be able to hold on to more relevant information throughout its experience and incur lower error due to not forgetting. Or perhaps, the agent will elect to extract relevant information out of the data at each time step and suffer less error from implasticity. However, it's evident that a capacity-constrained agent in a k-complex environment will inevitably experience error due to forgetting and implasticity.\nWe now draw the connection to the criterion that a finite capacity agent should never stop learning. Consider an agent $\\pi$ which stops learning i.e. for some $T$, $U_t = U_T$ for all $t > T$. If the $f$ is non-stationary w.r.t $\\pi$, then\n$(a)$\n$0 < I(X_{t+1}; X_{t+2: \\infty}|U_t)$\n$\\leq I(X_{t+1}; X_{t+2: \\infty}|U_{t+1})$\nimplasticity\nwhere $(a)$ follows from the definition of non-stationarity and $(b)$ follows from the fact that $\\pi$ stops learning. Therefore, if $f$ appears non-stationary to all agents $\\pi$, then an agent which stops learning will necessarily incur error due to implasticity. To mitigate this, an effective capacity-constrained agent ought to never stop learning."}, {"title": "An Illustrative Example: Turing-complete Prediction Environment", "content": "In this section, we propose a Turing-complete machine as an illustrative example of a big world simulator. A Turing-complete machine can be considered the biggest possible world because it is"}, {"title": "Conclusion", "content": "In this paper, we have highlighted the need for a big world simulator that accurately mirrors the challenges and complexities inherent in continual learning. We propose two properties which a big world simulator should exhibit: (1) there are no diminishing returns to increasing an agent's capacity, and (2) an optimal finite capacity agent should never stop learning. Furthermore, we reiterate the concrete objective of minimizing average error over an infinite horizon subject to a capacity constraint and provide a decomposition of this objective into two terms which closely"}, {"title": "Additional Details on Existing Continual Learning Benchmarks", "content": "In addition to Permuted MNIST, there are several other benchmarks that follow a similar recipe. The Split MNIST benchmark (Zenke et al., 2017; Nguyen et al., 2018) divides the MNIST dataset into tasks based on digit groupings, requiring the model to learn to distinguish between different sets of numbers sequentially. In the case of Incremental CIFAR-100 (Lopez-Paz & Ranzato, 2017), tasks are created by progressively introducing new classes, testing the model's ability to incorporate new knowledge without forgetting the previously acquired knowledge. This approach to creating a continual learning problem has also been applied to reinforcement learning, in which an agent plays a sequence of games in the Arcade Learning Environment (Abbas et al., 2023)."}, {"title": "Non-synthetic Continual Learning Benchmarks", "content": "There are also continual learning problems in which the dataset was curated specifically for the purposes of continual learning. In CoRE50, temporally correlated images for object recognition are introduced corresponding either to new classes or to new instances of an already encountered task (Lomonaco & Maltoni, 2017). Stream51 takes a similar approach, using temporally sequenced frames from natural videos (Roady et al., 2020). Lastly, there have also been efforts to curate datasets with natural non-stationarities that may better reflect the real-world challenges associated with continual learning, such as WILDS (Koh et al., 2021), Wild-time (Yao et al., 2022) and TemporalWiki (Jang et al., 2022)."}, {"title": "Proofs", "content": "We provide the proof of Theorem 1 below.\nTheorem 1. For all agents $\\pi : \\mathcal{U} \\times \\mathcal{X} \\rightarrow \\mathcal{U}$, if for all $t$, $U_{t+1} = \\pi(U_t, X_t)$, then\n$\\mathcal{L} = \\limsup_{T \\rightarrow \\infty} \\frac{1}{T} \\sum_{t=0}^{T-2} I(X_{T:t+2}; U_t|U_{t+1}, X_{t+1}) + \\frac{1}{T} \\sum_{t=0}^{T-1} I(X_{T:t+1}; X_t|U_t)$.\nProof."}, {"title": "Additional Experiments on Turing-compete Prediction Environment", "content": "The Turing-complete Prediction Environment is defined by the 3-tuple, {S, O, K}. The state-size, S, is the size of the entire world. While the Turing-completeness of Rule 110 depends on an infinite initial state, we simulate this with a periodic boundary condition. The dynamics of the local state transition are shown in Figure 4. The observation-size, O, controls the size of the observable world. The unobserved region of the state space is depicted by the shaded region in Figure 2. The prediction-horizon, K, controls the degree to which the non-stationarity induced by partial observability effects the prediction. This is because, for an elementary cellular automaton like Rule 110, each cell in the preceding state can only influence the cells to its left and right in the next state. Thus, the unobserved parts of the world can only influence the next state at the border of the observable region. The influence of the unobserved parts of the world on the prediction increases as the prediction-horizon increases. In our experiments, we set the state space to be 32 dimensional, the observation space to be the first 16 dimensions and prediction horizons K \u2208 {1,2,4,8,16}.\na small initial state and an agent with unbounded capacity could encode the dynamics of the full state by compress the entire history\nThe results are summarized in Figure 5. For each prediction horizon given in the legend, we trained a neural network of different widths and depths to predict the state at the given horizon. On the top figure, we see that longer prediction horizons lead to more non-stationarity. The optimal agent must never stop learning but regularization towards the initialization is needed to sustain plasticity at longer prediction horizons (Kumar et al., 2023b). On the bottom figure, we see that doubling the capacity leads to approximately half the error at larger prediction horizons suggesting that this indeed simulates the big world properties that we have outlined."}]}