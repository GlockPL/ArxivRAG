{"title": "The Need for a Big World Simulator: A Scientific Challenge for Continual Learning", "authors": ["Saurabh Kumar", "Hong Jun Jeon", "Alex Lewandowski", "Benjamin Van Roy"], "abstract": "The \"small agent, big world\" frame offers a conceptual view that motivates the need\nfor continual learning. The idea is that a small agent operating in a much bigger\nworld cannot store all information that the world has to offer. To perform well, the\nagent must be carefully designed to ingest, retain, and eject the right information.\nTo enable the development of performant continual learning agents, a number of\nsynthetic environments have been proposed. However, these benchmarks suffer\nfrom limitations, including unnatural distribution shifts and a lack of fidelity to the\n\"small agent, big world\" framing. This paper aims to formalize two desiderata for\nthe design of future simulated environments. These two criteria aim to reflect the\nobjectives and complexity of continual learning in practical settings while enabling\nrapid prototyping of algorithms on a smaller scale.", "sections": [{"title": "Introduction", "content": "The real world is an unfathomably complex system, both to humanity and to the agents that it\ndesignes. The small agent, big world frame captures this perspective by positing that to an agent with\nbounded computational resources, or capacity, the world appears complex and non-stationary (Dong\net al., 2022). As such, in order for said bounded capacity agent to continually fruitfully engage with\nthe world, it must continuously ingest new knowledge while selectively retaining previously acquired\nknowledge. We refer to this process as continual learning.\nThus far, the study of continual learning has revolved around the discovery of phenomena which\nplague the naive application of algorithms designed in traditional machine learning settings. Such\nphenomena include the likes of \"catastrophic forgetting\" which describes an agent's tendency to\nforget information from the past (Kirkpatrick et al., 2017; Zenke et al., 2017), and \u201cplasticity loss\"\nwhich describes an agent's inability to absorb new information with increased time (Dohare et al.,\n2021; 2023; Kumar et al., 2023b; Lyle et al., 2023). These studies have brought about the design of\nsynthetic environments which produce artificially generated and controllable data streams. These\nenvironments are designed to stress test an agent's resilience to these phenomena.\nHowever, if we frame the goal of continual learning to design agents which continue to engage\nfruitfully with the world, existing benchmarks do not capture the essence of the world. For instance,\non current benchmarks, evaluation metrics for catastrophic forgetting measure an agent's ability to\nremember everything in the past, which is unlikely to be necessary to fulfill a definition of fruitful"}, {"title": "A Common Recipe for Synthetic Continual Learning Benchmarks", "content": "Synthetic benchmarks are useful for rapid prototyping, enabling a researcher to evaluate an agent's\ncapabilities with respect to an array of identified challenges (Osband et al., 2020; 2022). Despite\nthree decades of active research on continual learning (Ring, 1994; Thrun, 1998), there remains\ncomparative difficulty in i) identifying the challenges unique to continual learning and (ii) measuring\nprogress made on previously identified challenges. For example, recent work highlights that the\nchallenge of catastrophic forgetting associated with existing continual learning benchmarks has been\naddressed by computationally inefficient solutions which are infeasible in most real-world scenarios\n(Prabhu et al., 2020). In this section, we briefly discuss a commonly used recipe for creating synthetic\ncontinual learning benchmarks and its fundamental limitations.\nMost synthetic continual learning benchmarks use the following recipe: take an existing non-\ncontinual dataset and create a continual learning problem by incorporating some form of non-\nstationarity. This data stream is often delineated into a sequence of tasks, where the non-stationary\nis applied at specific points in time to create a new task by transforming the observation distribution\nor the target function. Within each task, the environment faced by the agent is stationary. For exam-\nple, in Permuted MNIST (Srivastava et al., 2013; Goodfellow et al., 2013; Kirkpatrick et al., 2017),\neach task involves a fixed, distinct permutation of the pixels of all images in the MNIST dataset,\nand the challenge is to classify the digit of each image with these new inputs (See Appendix A)."}, {"title": "Formalizing the Environment and Agent", "content": "Before outlining the criteria for a big world simulator, we first propose information-theoretic charac-\nterizations of an environment and an agent. These characterizations will help formalize the notion\nthat the world appears complex and non-stationary to a bounded capacity agent, necessitating a\ntrade-off between ingesting and ejecting information."}, {"title": "Notation", "content": "We begin with a review of notation and probability and information theory that we will use in this\npaper. We define random variables with respect to a common probability space (\u03a9, F,P). For a\nrandom variable X : \u03a9 \u2192 X, we use P(X \u2208) to denote the distribution of X. We use H, I to denote\n(conditional) entropy and mutual information respectively. Concretely, for random variables X, Y, Z\n$H(X|Y) = E \\ln \\frac{1}{P(X|Y)}$; I(X; Y|Z) = H(X|Z) \u2013 H(X|Y, Z)$.\nWe use the notation dKL to denote the KL divergence, which maps two probability distributions to\nR+U {+\u221e}, where R+ denotes the non-negative reals."}, {"title": "The Environment", "content": "Let (X, Ex) be a measurable space which consists of the set of observations and a sigma-algebra on\nthat set. Let H = {X\" : n \u2208 N} denote the set of all histories of observations. An environment is\nidentified by a function f which maps H to the set of probability measures Ex \u2192 [0,1].\nWe assume that the environment generates a stream of observations which is characterized by\na stochastic process (X0, X1, X2,...) defined with respect to a probability space (\u03a9, F, P). The\nconditional distribution P(Xt+1 \u2208 \u2022|Ht) = f(Ht), where for all t \u2208 Z+, H+ = (X0, X1, ..., X\u2081) \u2208 Xn\ndenotes the history of observations up to time t."}, {"title": "The Agent", "content": "In this work, we consider an agent to be a map \u03c0: U \u00d7 X \u2192 U where U denotes a set of agent\nstates. For all t, we let Ut = \u03c0(Ut\u22121, Xt) denote the agent state at time t, which is computed from\nthe previous agent state Ut-1 and the latest observation Xt. We represent constraints placed on the\nagent via an informational constraint \u2161(Ht; Ut) < c for all t. We refer to this value c as the capacity\nof the agent."}, {"title": "Predictions and Error", "content": "An agent is evaluated for its ability to accurately predict future observations. Concretely, for any\nagent \u03c0, we define the error of to be\n$\\mathcal{L} = \\limsup_{T \\rightarrow \\infty} \\mathbb{E} \\pi [\\frac{1}{T} \\sum_{t=0}^{T-1} d_{K L} ( \\frac{P(X_{t+1} \\in \\cdot | H_t)}{\\text { environment probability }}|| \\frac{P(X_{t+1} \\in \\cdot | U_t)}{\\text { agent prediction }}) ]$.\nWe characterize an agent's effectiveness via it's error. Since the function f (environment probabili-\nties) may depend arbitrarily on Ht it is clear that an agent with limited capacity will incur nonzero\nerror. For all t, Ut will likely be missing information about Ht which may be relevant for making pre-\ndictions about Xt+1. Therefore, an effective agent ought to selectively remember and forget aspects\nof Ht which lead to the best predictions about the future, while obeying the capacity constraint."}, {"title": "Desiderata for a Big World Simulator", "content": "Equipped with formal characterizations of an environ-\nment, an agent, and error incurred by an agent interacting\nwith the environment, we now propose two criteria for a\nbig world simulator."}, {"title": "There are no diminishing returns to increasing an agent's capacity.", "content": "In the small agent, big world setting, the world is enor-\nmously - perhaps even infinitely \u2013 complex relative to any\nfinite-capacity agent. Since there is always more to learn\nabout the world, a desired feature of continual learning\nagents is that endowing an agent with significantly in-\ncreased capacity should invariably lead to substantial im-\nprovement in its capabilities. We frame this feature as a\nproperty of a big world simulator.\nTo capture the notion that increasing capacity yields\nsignificant performance improvements, we establish a\nconstraint on how the prediction error diminishes as the\nagent's capacity expands. First, let\n$L(c) = \\inf_{\\pi : \\tau, I(H_t;U_t)<c} \\Sigma_{\\pi}$.\nL(c) denotes the optimal error incurred by an agent with capacity c. We draw inspiration from the\nresearch in large language models which observes the desired phenomenon of continued reduction in\nout-of-sample error for every extra unit of available compute (Kaplan et al., 2020; Hoffmann et al.,\n2022). In these works, they establish a power law relationship between the error and capacity. This\nis in contrast to many classical statistical settings in which the reduction in out-of-sample error\ndecays exponentially fast in c. We now present the notion of k-complexity:\nDefinition 4.1. For k \u2208 R++, an environment is k-complex if\n$L(c) = O(\\frac{1}{c^k})$.\nThis definition characterizes an environment's ability to consistently offer performance improve-\nments in response to increases in agent capacity."}, {"title": "An optimal finite capacity agent interacting with the environment will never stop learning.", "content": "Since our goal is to evaluate the continual learning capabilities of agents, the environment should be\ndesigned such that an effective capacity constrained agent must never stop learning. To formalize this\nconcept, we begin by framing nonstationarity as a description of the agent's subjective experience.\nDefinition 4.2. An environment f is non-stationary with respect to an agent if\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; X_{t+2: \\infty} | U_t) > 0$.\nThis states that an agent \u03c0will always continue to encounter new information in Xt+1 which will\nbe relevant for making predictions about the future Xt+2:00 beyond what is currently known in Ut.\nTherefore, in our environment design we hope to design f such that it is non-stationary with respect\nto all finite-capacity agents \u03c0.\nWe note that for most environments studied in the literature, f is non-stationary with respect to an\nunbounded agent for which for all t, Ut = Ht. Concretely, in these environments,\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; X_{t+2: \\infty} | H_t) > 0$.\nFor instance, in an environment such as Permuted MNIST, there will continue to exist t for which\nXt+1 is data from a new permutation. As a result, this Xt+1 will contain information about the\nfuture sequence which is absent from Ht and hence the environment is non-stationary even to an\nunbounded agent.\nHowever, we make the point that even if\n$\\limsup_{t\\rightarrow \\infty} I(X_{t+1}; X_{t+2: \\infty}| H_t) = 0$,\nf may be non-stationary with respect to due to capacity constraints. Since the agent is only\nable to retain finite bits of information about the past, there will always be more to learn about in\nthe future. We argue that environments which satisfy Equation 1 along with Definition 4.2 better\ncapture the essence of \"small agent, complex world.\"\nOur notion of non-stationarity relates to the work of Abel et al. (2023), which proposes a definition\nof continual reinforcement learning. That definition offers a formal expression of what it means for\nan agent to never stop learning. The characterization is subjective in that it defines non-convergence\nwith respect to a basis, which is a fixed set of policies. Definition 4.2 is similarly subjective, defining\nnon-stationarity with respect to a particular agent with a particular agent state.\nGiven our definition of non-stationarity, we return to the implications on the behavior of an optimal\nfinite capacity agent. An agent with finite capacity may need to \"forget\" some information in its\nagent state in order to incorporate new information, a concept which we elaborate on in Section 5.\nThere may exist environments which are non-stationary but still exhibit a convergent optimal policy.\nThis could be the case for problem settings in which the entire history is equally informative about\nthe future. In such settings, ingesting the new information in Xt+1 and forgetting old information\ncould be equivalent to ignoring Xt+1 entirely given capacity constraints.\nHowever, such environments do not reflect reality nor do they align with our goals of developing\neffective continual learning algorithms. To ensure that the environment adequately evaluates contin-\nual learning capabilities, it should be designed so that an optimal finite capacity agent never stops\ningesting (and forgetting) information. Concretely, if we assume that the agent state Ut is capacity\nconstrained in that for all t, I(Ut; Ht) < c, then the environment should require an optimal capacity\nconstrained agent to never stop learning i.e.,\n$\\limsup_{t\\rightarrow \\infty} II(X_{t+1}; U_{t+1}|U_t) > 0$.\nThere will always be new information that the agent will need to incorporate into its agent state by\nforgetting old information."}, {"title": "Forgetting and Implasticity", "content": "In this section, we draw the connection between our notion of error (L) and phenomena described\nin the literature (notably \"forgetting\" and \"plasticity loss\" or \"implasticity\"). We begin by pro-\nviding the following decomposition of error into forgetting and implasticity. This decomposition\nquantitatively improves upon the result of Kumar et al. (2023a) to provide definitions of forgetting\nand implasticity which better match our intuition.\nTheorem 1. (forgetting and implasticity) For all agents \u3160 : U \u00d7 X \u2194 U, if for all t, Ut+1 =\n\u03c0(Ut, Xt), then\n$\\mathcal{L} =\n\\limsup_{T\\rightarrow \\infty}  \\frac{1}{T}  \\sum_{t=0}^{T-2}I(X_{T:t+2}; U_t|U_{t+1}, X_{t+1}) +  \\frac{1}{T}  \\sum_{t=0}^{T-1}I(X_{T:t+1}; X_t|U_t)$.\nThe forgetting that an agent experiences at time t + 1 is the information about the future sequence\nXT:t+2 which was contained in the previous agent state Ut, but not in the current agent state Ut+1\nnor the most recent observation Xt+1. Since agent state Ut+1 = \u03c0(Ut, Xt+1), this mutual infor-\nmation exactly captures which relevant information was decanted from the agent state incorporate\ninformation from Xt+1. Note that forgetting is forward looking as it measures mutual information\nbetween the previous agent state and the future sequence as opposed to the past sequence as done\nin the literature. Forgetting about the past is not \"catastrophic\" insofar as what is forgotten is\nuninformative about the future.\nMeanwhile, the implasticity that an agent experiences at time t is the information about the future\nsequence XT:t+1 which was contained in Xt, which we failed to include in Ut. This directly reflects\nwhat is referred to in the literature as \"loss of plasticity\". An agent loses plasticity, or incurs loss\nfrom implasticity, if it fails to digest information about Xt which is relevant for making predictions\nabout the future XT:t+1.\nTheorem 1 suggests that if our environment is k-complex, then an effective continual learning agent\nwill effectively reduce L by improving upon forgetting and/or implasticity with this additional\ncompute. Perhaps with this additional capacity, the agent will be able to hold on to more relevant\ninformation throughout its experience and incur lower error due to not forgetting. Or perhaps,\nthe agent will elect to extract relevant information out of the data at each time step and suffer\nless error from implasticity. However, it's evident that a capacity-constrained agent in a k-complex\nenvironment will inevitably experience error due to forgetting and implasticity.\nWe now draw the connection to the criterion that a finite capacity agent should never stop learning.\nConsider an agent which stops learning i.e. for some T, Ut = Ur for all t > T. If the f is\nnon-stationary w.r.t \u03c0, then\n(a)\n$0 <I(X_{t+1}; X_{t+2: \\infty} | U_t)$\n$\\@I(X_{t+1}; X_{t+2: \\infty}| U_{t+1})$\nwhere (a) follows from the definition of non-stationarity and (b) follows from the fact that stops\nlearning. Therefore, if f appears non-stationary to all agents \u03c0, then an agent which stops learning\nwill necessarily incur error due to implasticity. To mitigate this, an effective capacity-constrained\nagent ought to never stop learning."}, {"title": "An Illustrative Example: Turing-complete Prediction Environment", "content": "In this section, we propose a Turing-complete machine as an illustrative example of a big world\nsimulator. A Turing-complete machine can be considered the biggest possible world because it is"}]}