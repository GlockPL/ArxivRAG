{"title": "A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media", "authors": ["Dung Ha Nguyen", "Anh Thi Hoang Nguyen", "Kiet Van Nguyen"], "abstract": "This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing Pre-trained Language Models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1-3%.", "sections": [{"title": "1 Introduction", "content": "The explosive growth of social media has transformed how we communicate, generating an immense volume of informal, dynamic text. Platforms like Facebook, X, and Instagram contribute billions of posts daily, each encapsulating diverse linguistic, cultural, and personal expressions. According to LocaliQ\u00b9, every minute in 2024 sees about 510,000 posts, 4 million Facebook post likes, 360,000 tweets on X, 694,000 video reels shared on Instagram, 41.6 million WhatsApp messages, and over 6 million Google searches. This abundant yet chaotic data presents a significant challenge for Natural Language Processing (NLP), especially in low-resource languages such as Vietnamese.\nFor low-resource languages, the problem is compounded by the lack of large, well-annotated datasets essential for training robust NLP models. Vietnamese social media language, in particular, is marked by several complexities:\n1. Linguistic Diversity: Users from different regions bring varying vocabularies and grammatical structures, complicating the creation of standardized linguistic norms.\n2. Spelling Errors and Typos: Informal communication on social media often leads to a high frequency of spelling errors and typos, introducing additional noise for NLP models to handle.\n3. Emergence of Neologisms and Slang: Social media often features new terms, creative expressions, and slang, which are typically absent from traditional dictionaries and standardized grammar.\n4. Rapid Evolution: The language used on social media evolves quickly, with trends and new words appearing and disappearing rapidly, posing a challenge for consistent vocabulary tracking and normalization.\n5. Foreign Language Influence: Frequent incorporation of foreign terms, particularly English, further complicates efforts to standardize Vietnamese vocabulary.\nThis study focuses on addressing two main challenges: spelling errors and typos, and the emergence of neologisms. Spelling and typing inaccuracies arise from users' unfamiliarity with the language or lack of meticulousness, exacerbated by the omission of diacritics which leads to ambiguity and comprehension difficulties. Neologisms emerge as users prioritize efficiency in messaging, resorting to abbreviations, teencode, and slang, particularly among younger demographics.\nThese challenges underscore the critical need for lexical normalization, which is transforming non-standard words (NSWs) into their standard forms, in social media. While normalization may reduce some unique features of social media language [1], it has been shown to improve performance in numerous downstream NLP tasks, such as named entity recognition [2], POS tagging [3], dependency and constituency parsing [4], sentiment analysis [5], and machine translation [6]. Conventional NLP tools, primarily designed for standardized"}, {"title": "2 Fundations of Lexical Normalization and Data Labeling", "content": "2.1 Lexical Normalization\nThe job of lexical normalization, as defined by van der Goot et al. [8], is limited to social media data and can be represented as follows in this paper:\nDefinition 1 Lexical normalization is the task of transforming an utterance into its standard form, word by word, including both one-to-many (1-n) and many-to-one (n-1) replacements.\nIt should be noted that lexical normalization relies on a certain degree of correspondence between the words of target sentence and the terms of source sentence. Therefore, we did not add, remove, or rearrange any words in the sentence while it is being standardized.\nDue to the dynamic nature of social network language, characterized by rapid evolution and regional/demographic variation, establishing conventions for standard and non-standard forms is crucial. This study identifies and categorizes the following types of NSWs within our training set.\n* Abbreviations: Shortenings intentionally created by users to expedite communication on social networks. For example: \"mng\u201d\u2192\u201cm\u1ecdi ng\u01b0\u1eddi\u201d (everyone) and \u201cko\u201d\u2192\u201ckh\u00f4ng\u201d (no).\n* Spelling errors: Mistakes arising from user knowledge gaps or typos. For example: \u201cddungss\u201d\u2192\u201c\u0111\u00fang\u201d (right) and \"suy ngh\u1ec9\u201d \u2192 \u201csuy ngh\u0129\" (thought).\n* Teencode: A unique form of language, including special symbols and symbols to secretly convey messages, is the way the young generation \u201cencodes\u201d language in text communication. For example: \u201cch\u1ea7m zn\u201d\u2192\u201ctr\u1ea7m c\u1ea3m\u201d (depression), \u201ctrm\u00faa hm\u1ec1\u201d \u2192 \u201cch\u00faa h\u1ec1\u201d (clown).\n* Unmarks: Omissions of diacritics within sentences, potentially due to typos, time constraints, or mitigating the impact of sensitive words. For example: \"goi st5k da het thoi gian su dung\" \u2192 \"g\u00f3i st5k \u0111\u00e3 h\u1ebft th\u1eddi gian s\u1eed d\u1ee5ng\" (\"st5k package has expired\u201d), \u201cdu di me\u201d\u2192\u201c\u0111\u1ee5 \u0111\u0129 m\u1eb9\u201d (motherfucker)."}, {"title": "2.2 Existing Lexical Normalization Methods", "content": "While lexical normalization remains a nascent field of research, preliminary studies have already demonstrated its significant potential. The W-NUT workshop established a pioneering shared task on lexical normalization, focusing on User-Generated Content (UGC) data from English tweets [9]. Subsequent research explored various machine learning approaches for this task. For instance, one study employed a Random Forest Classifier to predict the appropriate normalized form of words, leveraging features like support, confidence, string similarity, and POS tags [10]. Other researchers investigated the application of Conditional Random Fields (CRF) models [11, 12] and recurrent neural networks (RNN) models [13, 14] to this problem.\nMoNoise [15] stands as a prominent technology in vocabulary normalization. This model, initially designed for English text, leverages spelling correction and word embeddings to generate candidate normalized forms, which are then ranked using a Feature-Based Random Forest Classifier. Later advancements extended the capabilities of MoNoise to standardize multilingual vocabularies, including Dutch, Spanish, Turkish, Slovenian, Croatian, and Serbian.\nPre-trained language models (PLMs) have emerged as an alternative approach for textual standardization. Research by [16] demonstrates that the BERT model [17] can effectively tackle this task by framing it as a token prediction problem. This approach proves particularly efficient when training data is limited. Furthermore, [18] explores the application of the pre-trained mBART model [19] as an encoder for automatic de-noising, essentially translating \"bad English\" sentences into grammatically correct ones. This method offers the advantage of considering entire sentences during normalization and can be readily extended to additional languages without significant computational overhead.\nCompared to well-resourced languages like English, research on textual standardization in Vietnamese is in its early stages. However, recent studies demonstrate promising progress. In 2016, a method for standardizing Vietnamese Twitter data employed the Dice coefficient or n-gram for spelling correction, followed by SVM classification with various feature types [20]. Subsequent research in 2019 focused on speech-to-text data standardization using a combined approach [21]. Random Forest initially classified NSWs, with subsequent steps tailored to the NSW type. Numerical text was expanded using defined rules, letter types were standardized with a deep learning sequence-to-sequence model, and pronunciation was derived for other NSWs. In 2022, another study addressed text standardization for Text-to-Speech (TTS) systems using news clippings [22]."}, {"title": "2.3 Data Labeling", "content": "This section provides a foundational overview of data labeling, a critical step in supervised machine learning tasks.\nData labeling is the cornerstone of supervised machine learning, where models learn to map inputs to desired outputs based on labeled examples. This process involves assigning labels or additional information to raw data (text, images, audio, video) to create training datasets.\nDefinition 2 For lexical normalization, data labeling is treated as a token-level sequence labeling problem, where an input sentence $x = x_1 = (x_1,x_2,\\cdots,x_n)$ consists of n tokens, and each token $x_i$ has a corresponding label $y_i$ in the gold label sequence $y = y^ = (y_1, y_2,\\cdots, y_n)$. The labeled dataset $D_\\text{L}$ is a set of pairs of sentences and their corresponding gold label sequences, represented as $D_\\text{L} = \\{(x, y)\\}$, where each pair $(x, y)$ consists of a sequence of tokens x and its corresponding sequence of labels y.\n* Manual Labeling: This method relies on human experts to meticulously assign labels to data points. It necessitates a deep understanding of the labeling context and goals, alongside precision and consistency. While manual labeling guarantees high-quality and accurate labels, which are crucial for robust machine learning models, it can be time-consuming and resource-intensive, especially for large datasets. Imagine image recognition tasks where specialists must manually label objects like cars, pedestrians, or traffic signs in thousands of images.\n* Automatic Labeling: This approach leverages algorithms or pre-trained machine learning models to automate the labeling process, significantly reducing time and effort, particularly with extensive datasets. Popular techniques include pre-trained models, clustering algorithms, and rule-based methods. While automatic labeling may not achieve the same accuracy as manual labeling, it serves as a valuable tool for creating preliminary datasets or streamlining the manual process by reducing the initial workload."}, {"title": "2.4 Existing Data Labeling Approaches", "content": "Deep learning has revolutionized various tasks and domains due to its exceptional performance. However, this success hinges on training complex architectures with labeled data. Manually collecting, managing, and labeling this data is a significant bottleneck, prompting renewed interest in classical approaches that address label scarcity.\nActive Learning (AL) [25] tackles this by selecting the most informative data points for labeling, optimizing label allocation. Semi-Supervised Learning (SSL) [26] leverages unlabeled data to improve model performance. Transfer Learning (TL) [27] pre-trains a model on a similar domain to enhance performance on a new one. While these techniques are valuable, they still require a foundation of labeled data, limiting their ability to fully address label scarcity.\nTo alleviate labeling burden, researchers have explored alternative, less expensive sources of labels. Distant supervision utilizes external knowledge sources to generate noisy labels [28]. Crowdsourcing [29], heuristic rules [30], and feature annotation [31] are other approaches. A key question lies in combining these methods effectively and in a principled manner.\nProgrammatic Weak Supervision (PWS) offers a compelling answer [32]. In PWS, users define weak supervision sources (e.g., heuristics, knowledge bases, pre-trained models) as Labeling Functions (LFs). These user-defined programs provide labels for specific data subsets, collectively creating a vast training set. However, LFs can introduce noise at varying rates and may generate conflicting labels for some data points. To address these issues, researchers developed label models that synthesize labels generated by LFs [33]. These labels are then used to train the final model. Recent work explores combining label models and the final model into joint models [34].\nPWS demonstrates flexibility by integrating with other machine learning methods. It can enhance AL by querying labels that best align with existing labels based on LFs [35]. Conversely, AL can assist PWS by directing expert"}, {"title": "2.5 A Weakly Supervised Approach to Data Labeling for Lexical Normalization in Vietnamese Social Media", "content": "While lexical normalization has received attention in other languages, research specifically focused on Vietnamese social media text is scarce. Existing studies have limited exploration of PLMs for this task and haven't extensively addressed data from contemporary online platforms like social networks. To address these gaps, we propose a direct text normalization approach, bypassing an error detection step, that leverages datasets extracted from popular Vietnamese social media platforms such as Facebook, TikTok, and YouTube. This data is rich in NSWs, and the proposed model aims to both generalize and map these NSWs to their standard forms.\nInspired by prior research [37-39], we propose applying and fine-tuning prominent Vietnamese PLMs like PhoBERT, BARTPho, and ViSoBERT for Vietnamese textual standardization. This approach capitalizes on the capabilities of modern language models, potentially improving the accuracy and efficiency of normalization specifically within the context of Vietnamese social media data.\nIn the lexical normalization task, raw data consists of sentences containing one or more NSWs. Each NSW requires labeling with its corresponding standard form. The objective of model is to transform sentences containing NSWs into sentences where these NSWs have been normalized. Existing textual standardization studies have relied on manually pre-labeled datasets, often limited in size (e.g., 2,000 sentences). Our work proposes a novel approach that combines Pseudo-Labeling with Self-Training (PWS-ST) within the SSL framework, inspired by [7]. This approach tackles data scarcity by automatically expanding the training set size. The self-training algorithm trains a prediction model on pre-labeled data to generate pseudo-labels for unlabeled data. PWS utilizes LFs to assign weak labels to unlabeled data. Furthermore, we incorporate a neural network to assess the quality of these weak labels, ensuring only high-confidence labels are added to the training data for model learning.\nUltimately, our work aims to develop an automatic labeling framework for text normalization, a crucial task in NLP, particularly when dealing with social media data characterized by frequent use of non-standard language. Unlike sequence classification tasks where the model predicts a single label for the entire input, text normalization requires sequence labeling, assigning a label (standard form) to each individual token within the input sentence. This"}, {"title": "3 Datasets and Data Pre-processing", "content": "3.1 Datasets\nViLexNorm [24], specifically designed for vocabulary standardization, is collected from two prominent Vietnamese social media platforms: Facebook and TikTok. It comprises 10,467 comment pairs, each containing an original sentence and its corresponding post-normalized version.\nThis research leverages the ViLexNorm dataset for two key purposes:\n(1) Fine-tuning the pre-trained language model: We employ the ViLexNorm dataset to fine-tune a pre-trained language model for the task of Vietnamese vocabulary standardization. This process enhances the model's ability to identify and normalize informal or dialectal variations commonly found in Vietnamese social media text.\n(2) Pre-labeled data for labeling framework construction: The ViLexNorm dataset serves as a pre-labeled dataset (denoted as $D_L$) within the framework we construct for model training. This dataset provides valuable seed labels to guide the learning process.\nTo comprehensively assess the generalizability of model across various Vietnamese social media text analysis tasks, we incorporate five publicly available datasets. A detailed description of each dataset is provided in Table 2.\n* ViHSD [40]: This dataset encompasses user comments on various topics including entertainment, celebrities, social and political issues. Extracted from Facebook and YouTube, ViHSD contains a total of 33,400 comments labeled into three categories: CLEAN, OFFENSIVE, and HATE. This dataset serves the task of hate speech detection.\n* UIT-VSMEC [41]: Designed for emotion recognition, the UIT-VSME\u0421 dataset includes comments extracted from public Facebook posts. It comprises 6,927 comments, each assigned one of six emotion labels: Enjoyment, Sadness, Anger, Fear, Disgust, Surprise, and Other.\n* ViHOS [42]: This dataset focuses on Vietnamese hate and offensive span detection. ViHOS includes 11,056 comments with labeled spans of offensive or hateful content.\n* ViSpamReviews [43]: Collected from leading Vietnamese online shopping platforms, ViSpamReviews serves the tasks of spam detection and classification. This dataset includes 19,870 rows containing customer reviews and product/service comments.\n* UIT-VISFD [44]: Announced for aspect-based sentiment analysis (ABSA) and sentiment classification, UIT-ViSFD consists of customer reviews on e-commerce platforms for 10 popular smartphone brands in Vietnam. The"}, {"title": "3.2 Data Preprocessing", "content": "This section details the data preparation process employed in our research. We leverage two primary data sources:\n* Lexical normalization datasets ($D_L$): This dataset serves as the foundation for generating labeled data ($D_L$). We utilize the normalized labels within ViLexNorm to guide the creation of $D_L$.\n* Social media text processing datasets ($D_U$): We exploit five publicly available datasets (ViHSD, UIT-VSMEC, VIHOS, ViSpamReviews, and UIT-ViSFD) to extract comments and sentences for the creation of unlabeled data ($D_U$). These datasets provide a rich source of real-world Vietnamese social media text.\nThe data preparation process incorporates several key steps as outlined in the provided module:\n3.2.1 Basic Preprocessing\nMost of the utilized datasets have undergone basic preprocessing procedures during their initial publication. Therefore, our team focused on addressing task-specific issues related to text normalization."}, {"title": "3.2.2 Named Entity Recognition (NER) Pipeline", "content": "Protecting user privacy is essential in social media data analysis. Social network texts can contain sensitive information like usernames and email addresses. \u03a4\u03bf ensure anonymity, we use Named Entity Recognition (NER) to identify and mask such details.\nOur method applies the NER Pipeline by NlpHUST2 to detect person names, replacing them with placeholders like \u201c@username\" This approach safeguards user identities while adhering to data protection regulations. For example, \u201cPh\u01b0\u01a1ng Nguy\u1ec5n \u00ea ik ch\u1ed7 n\u00e0y \u0111iiii\" is anonymized to \"@username \u00ea ik ch\u1ed7 n\u00e0y \u0111iiii\u201d (English: @username hey let's go to this place)."}, {"title": "3.2.3 Word Segmentation", "content": "Word segmentation, a crucial step in NLP, involves dividing a continuous character stream into individual words. While languages like English rely on spaces for word boundaries, languages such as Vietnamese, Chinese, and Japanese lack these clear delimiters, making segmentation more complex.\nThe performance of word segmentation significantly impacts downstream tasks. In this work, we leverage the VnCoreNLP\u00b3 tool, a popular NLP suite developed by the Institute of Information and Communications Technology (ICT) at Hanoi University of Science and Technology. VnCoreNLP utilizes a machine learning model trained on extensive Vietnamese corpora, enabling effective and accurate word segmentation. This tool facilitates efficient processing of large text volumes with high accuracy. For example: \u201cH\u1ea1nh Nguy\u1ec5n con m\u1eb9 n\u00f3 \u0111o\u1ea1n h\u00f4m qua t l\u00e0m r\u1ed3i nh\u01b0ng m\u00e0 nay l\u1ed7i\u201d \u2192 \u201c@username con_m\u1eb9 n\u00f3"}, {"title": "3.2.4 Tokenization", "content": "Tokenization, a fundamental step in NLP, follows word segmentation and involves splitting text into even smaller units called tokens. These tokens can encompass individual words, punctuation marks, or even special characters.\nAlthough spaces effectively separate words in English and other Latin languages, Vietnamese requires additional considerations after segmentation. In our approach, we leverage the results of the word segmentation step to create tokens by utilizing spaces as delimiters. This approach ensures accurate tokenization for Vietnamese text analysis. For example: \"Ph\u01b0\u01a1ng Ng m\u1ed1t c\u00f3 b\u1ed3 r\u1ed3i \u0111\u0103ng \u0111\u1ee1 si ngh\u0129 \u201d \u2192 [\u2018@username', 'm\u1ed1t', 'c\u00f3', 'b\u1ed3', 'r\u1ed3i', '\u0111\u0103ng', \u2018\u0111\u1ee1', \u2018si ngh\u0129']"}, {"title": "3.3 Dataset Description", "content": "After processing, the labeled dataset ($D_L$) consists of 10,463 entries, each with corresponding labels as exemplified in Table 3.8. The data is divided into three sets: training, development, and test, with a ratio of 8:1:1. The structure of the dataset is illustrated in Table 3."}, {"title": "4 Our Proposed Framework", "content": "4.1 Overall Framework Architecture\nThe proposed framework draws inspiration from the ASTRA labeling framework [7] and is designed to harness both labeled and unlabeled data for lexical normalization tasks. As shown in Figure 4, the architecture employs a weak supervision paradigm, utilizing rules as weak supervision sources to generate weak labels for unlabeled data. At its core, the framework integrates a Student"}, {"title": "4.2 Student Base Model", "content": "The Student model serves as the initial and crucial component of the proposed framework. Initially, the Student model is trained on a labeled dataset $D_L$. This trained model is subsequently employed to generate pseudo-labels for samples in the unlabeled dataset $D_U$. In traditional self-training methods, these pseudo-labeled samples are directly incorporated into the training data, and the model undergoes iterative training. However, the proposed framework enhances this self-training process by integrating weak labels from the Teacher model (detailed in Section 4.3).\nIn this study, the Student model utilized comprises pre-trained language models tailored for Vietnamese, addressing the specific challenge of lexical normalization. Specifically, we experiment with three models: ViSoBERT [37], PhoBERT [38], and BARTpho [39].\n1. ViSoBERT is a pre-trained language model tailored for Vietnamese social media text, trained on a 1GB dataset from platforms like Facebook, YouTube, and TikTok. It excels at interpreting NSWs such as emojis, abbreviations, and slang, using a transformer architecture and a specialized Sentence-Piece tokenizer, making it superior to traditional models in understanding Vietnamese online communication.\n2. PhoBERT, a pre-trained language model tailored for Vietnamese, is built on the ROBERTa architecture and leverages a substantial 20GB corpus from various sources including news websites, online forums, and social media to enhance NLP task performance. PhoBERT is trained using the Masked Language Modeling (MLM) method, where random tokens are masked"}, {"title": "4.2.1 Token-Level Alignment Tokenization", "content": "Language models must predict the standard word corresponding to each NSW in the input sentence. Therefore, the positions of NSWs in the input sentence and their corresponding labels in the output sentence need to be aligned.\nMuller et al. (2019) [16] proposed two tokenization algorithms for lexical normalization: Independent Alignment and Parallel Alignment. Both algorithms can tokenize sentence pairs and ensure alignment between the source and target sentences. However, these algorithms are only applicable when using a WordPiece tokenizer. This paper introduces a simpler tokenization algorithm that can be applied to any tokenizer. Specifically, the paper experiments with the PhoBERT tokenizer, which uses the Byte-Pair Encoding (BPE) method, as well as ViSoBERT and BARTpho, which utilize the SentencePiece tokenizer. Nonetheless, our tokenization algorithm imposes stricter requirements on input sentence pairs. Instead of processing input as text strings as in study [16], the source and target sentences must be provided as lists with equal lengths, where each element in the source list is aligned with the corresponding element in the target list. Before tokenization, we preprocess the input text to achieve the desired format.\nOur proposed tokenization algorithm is illustrated in Figure 5. We perform the following steps for token-level alignment tokenization:\n* Use a tokenizer to simultaneously tokenize both sentences."}, {"title": "4.2.2 Enhance Language Model Architecture for Lexical Normalization", "content": "In the previous section, we introduced the <space> token into output sentences to facilitate the identification of corresponding tokens in input sentences that require removal. Since <space> isn't initially included in the vocabularies of ViSoBERT, PhoBERT, and BARTpho models, we expanded each vocabulary size accordingly: ViSoBERT to 15,003, PhoBERT to 64,001, and BARTpho to 43,001. Additionally, we augmented the final softmax layer with a vector initialized based on the standard distribution of the pre-trained model's existing embeddings. This adjustment ensures that the embedding of <space> aligns with the model's learned embedding space, enhancing its ability to handle instances where this new token is used.\nMoreover, as discussed earlier, we introduced the <mask> token into input sentences to prompt the model to learn to fill in tokens and generate standardized vocabulary forms. However, incorporating <mask> introduces challenges during prediction since we don't know when to apply it. To address this, we integrated a classification module into the architecture of pre-trained model. This module uses the hidden state from the last layer of each token to predict the number of <mask> tokens needed following each token."}, {"title": "4.3 Teacher Model", "content": "4.3.1 Weak Rules\nDictionary\nLexical normalization using dictionaries is a widely adopted method for automating spelling error correction and standardizing text. This approach involves using dictionaries to identify NSWs and replace them with appropriate corrections."}, {"title": "4.3.2 Rule Attention Network (RAN)", "content": "The Rule Attention Network (RAN) enhances label assignment tasks by integrating multiple sources of weak supervision, such as regular expressions and dictionaries, each with adaptable reliability weights. This approach aims to refine the traditional methods of managing unlabeled data. Specifically, RAN aids in making more precise labeling decisions when only a limited number of weak rules are available.\nHeuristic rules, like regular expressions and dictionaries, are helpful for text standardization. However, these rules frequently fail to capture a wide range of cases, leaving a significant portion of data unlabeled in conventional weak supervision approaches. To mitigate this issue, pseudo-labels predicted by a base Student model are utilized. By applying the Student model to the unlabeled dataset $D_U$, predictions $p_\\theta(y|x)$ are generated, serving as an additional source of weak supervision. This strategy increases the coverage of the weak supervision sources, thereby enhancing the overall labeling process.\nFor each data sample $x_i$, a set of heuristic rules $R_i$ is applied. RAN aims to integrate the weak labels derived from these heuristic rules with pseudo-labels produced by the Student model to generate a soft label $q_i$ for each data sample. To achieve this, RAN employs trainable weights to estimate and adjust\n\\begin{equation}\nq_i = \\frac{1}{Z_i} ( \\sum_{j:r_i\\in R_i} \\alpha^r p(y|r_i) + \\alpha^p p_\\theta (y|x) + \\alpha^u u )\n\\end{equation}\nwhere $ \\alpha^r$ and $ \\alpha^p$ represent the confidence weights for the labels $q^r$ predicted by rule $r_i$ and the pseudo-label $p_\\theta (y|x)$ predicted by the Student model for data sample $x_i$. The uniform distribution u assigns equal probabilities to all K classes, represented as $u = [\\frac{1}{K}, ..., \\frac{1}{K}]$. The weight $\\alpha^u$ for the uniform distribution is calculated as $ \\alpha^u = |R_i + 1 - \\sum_{j:r_i\\in R_i} \\alpha^r - \\alpha^p|$. $Z_i$ is a normalization factor ensuring that $q_i$ is a valid probability distribution. The uniform distribution u acts as a smoothing factor to prevent overfitting, especially when only a few weak rules apply to a sample.\nIn Equation 1, a rule $r_i$ with a higher confidence weight $\\alpha^r$ will contribute more to $q_i$. If $ \\alpha = 1$ for all $r_i \\in R_i \\cup p_\\theta$, RAN simplifies to a majority voting scheme. Conversely, if $\\alpha^u = 0$ for all $r_i \\in R_i \\cup p_\\theta$, RAN predicts $q_i = u$.\nTypically, heuristic rules are predefined and often operate based on simple patterns. For example, a dictionary may dictate that the abbreviation\u2018ca\u2019 should be translated to the standard form 'c\u00f4ng an', but in some cases, 'ca'should remain unchanged. Thus, these rules often have limitations and cannot capture the complex nature of the data. To address this issue, RAN leverages embeddings of data samples. Embeddings are dense vector representations of input sentences, capable of capturing their semantics. These embeddings are usually generated by neural network models, in this case, the Student model.\nBy utilizing embeddings, RAN can understand the context in which rules should be applied to the input sentence. Moreover, rules are also represented by embeddings, allowing the model to learn semantic similarities between rules and specific sentences. RAN employs an attention mechanism to compute how much attention should be placed on each rule for each input sentence. Specifically, RAN utilizes the hidden state representation of a data sample $x_i$ as $h_i \\in R^{d_\\theta}$, then applies a multi-layer perceptron (MLP) network to map this embedding to the same space as rule embeddings $(e_j = g(r_i) \\in R^d)$. Subsequently, the attention weight $\\alpha^r_i$ is computed using a sigmoid activation function to determine the relevance of each rule to $x_i$, as formulated in Equation 2. This mechanism allows RAN to dynamically adjust the attention placed on each rule based on the context of the input sentence $x_i$, enhancing its ability to effectively utilize weak supervision sources for label assignment.\n\\begin{equation}\n\\alpha^r_i = \\sigma (f(h_i)^T e_j) \\in [0,1]\n\\end{equation}"}, {"title": "4.4 Framework Training Procedure", "content": "Before training the labeling framework, it is essential to prepare a dataset containing weak labels generated by regular expressions and dictionaries. These labels remain fixed throughout the training process, enabling the framework to learn and adjust the weight of each rule based on individual data samples. Specifically, two columns, \u2018regex_rule' and 'dict_rule', are added to the datasets $D_L$ and $D_U$, containing the predictions from regular expressions and dictionaries, respectively.\nThe training process for the complete weak supervision framework, depicted in Figure 7, involves five key steps: (1) training the Student model, (2) predicting pseudo-labels for $D_U$, (3) training the Rule Attention Network (RAN), (4) retraining the Student model with the $D_U$ dataset, and (5) fine-tuning the Student model with the labeled dataset $D_L$. Steps 2 through 5 are repeated iteratively for a predetermined number of iterations.\nStep 1: Traning Student model. The initial phase involves training the Student model on the labeled dataset $D_L$ using supervised learning techniques. Upon completion, the model weights are saved, and the model's performance is evaluated on the development and test sets derived from $D_L$.\nStep 2: Predicting pseudo-labels for $D_U$ dataset. In this step, pseudo-labels are predicted for the unlabeled dataset $D_U$. Given the large size of $D_U$ and limited training resources, a random subset $D_\\text{pseudo}$ is first sampled from $D_U$ for prediction. The Student model then generates pseudo-labels for $D_\\text{pseudo}$ while also extracting input data representations (embeddings), which will be used to train the RAN in the next step. The pseudo-labels predicted by regular expressions and the dictionary are directly sourced from the pre-constructed dataset.\nStep 3: RAN Training. The RAN is trained using a combination of unsupervised and supervised learning methods to fully exploit the datasets $D_L$ and $D_U$. During the unsupervised learning phase, the Minimum Entropy"}, {"title": "4.4", "content": "In the supervised learning phase, the model is fine-tuned on the labeled dataset $D_L$ using the Cross-Entropy loss function, as formulated in Equation 4. Here, qi represents the pseudo-label predicted by the Teacher model, and yi is the true label. Fine-tuning on the labeled dataset enhances the model's predictive accuracy. After training, the weights of RAN are saved, and its prediction results are evaluated on the development and test sets. The trained RAN is then used to predict soft labels for the $D_\\text{pseudo}$ dataset.\n$L_\\text{sup} = -\\sum_{(x_i,y_i) \\in D_L} y_i \\log q_i$\nStep 4: Re-trainning Student model with $D_\\text{pseudo}$ dataset. The Student model is retrained on the $D_\\text{pseudo}$ dataset using supervised learning, with the soft labels predicted by the Teacher model serving as the labels. Unlike hard labels, which indicate a specific class for a data sample, soft labels represent"}, {"title": "5 Experiments and Results", "content": "5.1 Experimental Settings\nThe experiments in this paper were conducted using Colab Pro with GPU L4. Specifically, experiments involving the PhoBERT and BARTpho models were executed using Colab Pro with GPU A100.\n5.1.1 Experiment 1: Evaluating the Labeling Capability of the Proposed Framework\nOur main experiment is designed to assess the labeling capability of the weakly supervised labeling framework. Labeling capability encompasses the ability to normalize words requiring normalization, maintain vocabulary that does not require normalization, and the accuracy of the entire sentence post-normalization. Specific metrics will be presented in Section 5.2.\nWe will sequentially run the framework with three different Student models: ViSoBERT, PhoBERT, and BARTpho. Concurrently, we will compare the performance of framework against two baselines:\n1. Using Only the Student Model: This involves training the Student model on the training set without any additional configurations.\n2. Traditional Self-training: This involves training the Student model using supervised learning on the training set, then using the trained model to"}, {"title": "5.1.2 Experiment 2: Investigating Diacritics Removal Ratios in Training Data to Enhance Sentence Normalization Capabilities", "content": "In addition to our primary experiment, we also augment the training dataset to improve the framework's ability to normalize sentences lacking diacritics (e.g., 'dang o tieu vuong quoc ma an noi kieu day ha' \u2192 '\u0111ang \u1edf ti\u1ec3u v\u01b0\u01a1ng qu\u1ed1c m\u00e0 \u0103n n\u00f3i ki\u1ec3u \u0111\u1ea5y h\u1ea3' - English: \u2018Even though you're in the emirate, you talk like that'). Specifically, we replicate the training and development sets and remove diacritics from a proportion p of the characters in each sentence within the newly created replicas. The experimental values for p are 1, 0.8, and 0.5. We will compare the framework's performance across these three p values to determine the most optimized ratio."}, {"title": "5.2 Evaluation Metrics", "content": "5.2.1 Metrics for Evaluating Vocabulary Normalization Accuracy\nLet #need_norm represent the number of words that require normalization, #pred_need_norm represent the number of words for which the model performs normalization (i.e., it provides a prediction different from the word in the input sentence), and TP need norm represent the number of #need_norm words that are correctly predicted. Recall, Precision, and F1-score metrics are defined as follows.\nrecall = $\\frac{TP_\\text{ need \\_norm}}{\\#need\\_norm}$\nprecision = $\\frac{TP_\\text{ need \\_norm}}{\\#pred\\_need\\_norm}$"}, {"title": "5.2.2 Metrics for Evaluating the Integrity of Words Not Requiring Normalization", "content": "During the prediction process, words that do not require normalization should remain unchanged. However, the model may inadvertently alter these words, contradicting the objective of the problem. Thus, evaluating the integrity of words that do not require normalization is also crucial.\nLet #need_no_norm denote the words that do not require normalization (and should remain unchanged during prediction), and TPneed_no_norm denote the number of #need_no_norm words correctly predicted as unchanged. The metric for evaluating the integrity of words that do not require normalization"}]}