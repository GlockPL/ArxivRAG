{"title": "Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained Foundation Models", "authors": ["Alain Andres", "Javier Del Ser"], "abstract": "Exploration remains a significant challenge in reinforcement learning, especially in environments where extrinsic rewards are sparse or non-existent. The recent rise of foundation models, such as CLIP, offers an opportunity to leverage pretrained, semantically rich embeddings that encapsulate broad and reusable knowledge. In this work we explore the potential of these foundation models not just to drive exploration, but also to analyze the critical role of the episodic novelty term in enhancing exploration effectiveness of the agent. We also investigate whether providing the intrinsic module with complete state information \u2013 rather than just partial observations \u2013 can improve exploration, despite the difficulties in handling small variations within large state spaces. Our experiments in the MiniGrid domain reveal that intrinsic modules can effectively utilize full state information, significantly increasing sample efficiency while learning an optimal policy. Moreover, we show that the embeddings provided by foundation models are sometimes even better than those constructed by the agent during training, further accelerating the learning process, especially when coupled with the episodic novelty term to enhance exploration.", "sections": [{"title": "Introduction", "content": "Exploration is a fundamental challenge in reinforcement learning (RL), particularly in environments where extrinsic rewards are sparse or absent. In such scenarios, agents often struggle to efficiently explore and learn, frequently failing to discover meaningful strategies that lead to successful task completion. To address these challenging RL tasks, various methods have been proposed, including approaches that maximize entropy Haarnoja et al. [2018], techniques that increase the uncertainty of the agent's parameters Fortunato et al. [2018], and imitation learning to mimic expert behaviors Zheng et al. [2022]. Albeit effective, these proposals have their own drawbacks, such as the need for careful configuration tuning, or the requirement of externally gathered data.\nInspired by human psychology, intrinsic motivation encourages agents to be curious (i.e., seek novel situations) in the environment, fostering a more robust and adaptive learning process. This typically involves generating an intrinsic reward $r$ that quantifies the novelty or surprise, which is then added to the actual extrinsic reward from the environment $r_e$. This addition leads to an updated reward signal $r_t = r_e + \\beta r_i$, wherein $\\beta \\in \\mathbb{R}^+$ balances between exploiting known information in the environment with fostering the agent's curiosity. However, the effectiveness of these methods often depends on the quality of the state representations used to compute the novelty bonus, which are commonly learned during the agent's training process."}, {"title": "Related Work", "content": "Intrinsic Motivation for enhanced exploration in RL. In response to the novelty that the agent experiences during its learning process, the reward signal can be augmented to foster explorative behaviors. This is usually tackled via count-based methods that encourage visits to less frequent states Bellemare et al. [2016], Ostrovski et al. [2017], or by reducing uncertainty/prediction error in the environment where the agent is deployed Pathak et al. [2017], Burda et al. [2018]. These lifelong rewards are significantly beneficial when being combined with episodic information Raileanu and Rockt\u00e4schel [2020], Flet-Berliac et al. [2021], Zhang et al. [2022], being sometimes even more interesting to use solely the latter episodic information Andres et al. [2022], Henaff et al. [2023a], Wang et al. [2023]. Thus, deriving methods that capture this episodic information has become of interest within the community working on sparse RL tasks Savinov et al. [2019], Jo et al. [2022], Henaff et al. [2023b].\nFoundational Models and RL. Recently, some approaches have relied on Vision Language Models (VLM) to generate auxiliary rewards by calculating the cosine similarity between the language goal and the input image Mahmoudieh et al. [2022]. However, their effectiveness seems to be related to the model size and the dataset used for learning them Rocamonde et al. [2024], Baumli et al. [2024]. While these rewards can be noisy, they have proven to be useful during pretraining Adeniji et al. [2023] and can be refined through contrastive reward alignment techniques Fu et al. [2024]. Similarly, VLM embeddings have been employed to generate intrinsic rewards, driving exploration more effectively in sparse environments Gupta et al. [2022]. Additionally, LLMs have been leveraged to guide the learning process, providing high-level instructions Du et al. [2023] and shaping the reward function Chu et al. [2023]."}, {"title": "Methodology", "content": "In this section we motivate the research questions stated in the introduction based on the definition of the rewards at the core of RIDE ad FoMORL.\nRIDE: Rewarding Impact-Driven Exploration. RIDE is one the state-of-the-art exploration strategies proposed for environments with sparse rewards. It computes intrinsic rewards based on the impact an agent's actions have on the environment. Specifically, it measures the difference between consecutive states in a learned state representation space, with larger differences indicating more significant exploration. Mathematically:\n$r^{RIDE}(s_t, s_{t+1},a_t) = \\frac{||\\phi(s_{t+1}) - \\phi(s_t)||_2}{\\sqrt{N_{ep}(s_{t+1})}}$\nwhere $\\phi(\\cdot)$ refers to the embedding network that is learned during training, $|\\cdot|_2$ stands for $L_2$-norm, and $N_{ep}(s_{t+1})$ is the number of times the next state $s_{t+1}$ has been visited.\nFoMoRL: Foundation Models for Semantic Embeddings. FoMoRL proposes a different ap- proach by using pretrained state representations from a foundation model, instead of relying on domain-specific state representations that need to be learned, thereby accelerating exploration. The intrinsic reward in FoMoRL is computed as:\n$r^{FoMoRL}(s_t, s_{t+1}, a_t) = \\frac{||clip(s_{t+1}) - clip(s_t)||_2}{\\sqrt{N_{ep}(s_{t+1})}}$\nwhere $\\phi(\\cdot)$ is replaced by the outputs of CLIP's visual embeddings.\nMotivation. Although Equations (1) and (2) were originally proposed considering the full informa- tion about the environment, in practice they have been applied using either the state $s_t$ or the partial observation $o_t$. Furthermore, depending on the environment, this information can be represented as an encoded/compact representation, $\\{s^{enc}, o^{enc}\\}$, or as an RGB visualization when images are used, $\\{s^{rgb}, o^{rgb}\\}$. Our work focuses on understanding the impact of these different representations on exploration. Specifically, we aim to analyze: (1) the effect of using either state or observation in the intrinsic curiosity module, (2) the differences between employing an encoded representation or an image, and (3) the importance of the episodic novelty term in these diverse setups."}, {"title": "Experimental Setup", "content": "MiniGrid. Experiments are conducted over the MiniGrid benchmark, a collection of procedurally generated grid-based environments that challenge RL agents to explore and navigate effectively in the presence of reward sparsity. In MiniGrid, the environment information can come in two different forms: a compact, encoded vector that summarizes key information about the grid ; and an RGB image that provides a detailed visual representation. Specifically, we evaluate performance across MultiRoom (MN7S4, MN7S8, MN12S10), KeyCorridor (KS3R3, KS4R3), and ObstructedMaze (O2D1h) environments.\nAlgorithms. For training an agent, we use Proximal Policy Optimization (PPO) Schulman et al. [2017] across all experiments, utilizing RIDE and FoMoRL to generate intrinsic rewards. In our setup, RIDE operates exclusively with encoded information, using either the encoded state $s^{enc}$ or observation $o^{enc}$ to compute the intrinsic rewards. Conversely FoMoRL, which is pretrained on image data, utilizes RGB representations $s^{rgb}$ or $o^{rgb}$ for the calculation of its reward. Regardless of the intrinsic motivation technique, when incorporating the episodic novelty term, we use the encoded observation $N_{ep}(o^{enc})$ to track state visitation counts. Similarly, the agent's policy is always fed with encoded observations $\\pi(\\cdot|o^{enc})$. More information about the selected model architectures and hyperparameters can be found in Appendix A."}, {"title": "Results", "content": "Impact of Partial vs Full Observability. Table 1 provides the number of steps required for RIDE and FoMoRL to converge to the optimal policy in different MiniGrid environments, considering both partial observations and full state information. For both approaches, the use of full state information significantly augments the sample-efficiency. In complex MultiRoom environments only FoMoRL with full state access manages to effectively train. However, in KeyCorridor and ObstructedMaze, RIDE performs slightly better than FoMoRL. We hypothesize that this is because these environments contain a higher diversity of objects, allowing RIDE to effectively distinguish between different states and learn a better representation. In contrast, FoMoRL relies on pretrained embeddings from CLIP, which remain frozen during training and may lack the adaptability required to fully capture the variations in these more diverse environments.\nRole of the Episodic Term. Table 2 highlights the critical importance of the episodic term in accel- erating the learning process. Without this term, the sample efficiency is significantly reduced. In fact, the absence of the episodic term has a particularly negative impact on RIDE's performance, as it is unable to learn in MultiRoom-N7-S8 and MultiRoom-N12-S10. FoMoRL, however, demonstrates greater resilience to the removal of the episodic term, likely due to its reliance on latent knowledge from pretrained embeddings. Nonetheless, akin to the results with the episodic term, FoMoRL still obtains slightly worse results in some environments."}, {"title": "Conclusions", "content": "We have investigated the use of foundation models and episodic novelty to enhance exploration in sparse RL environments. Our results evince that providing full state information to the intrinsic module significantly accelerates the agent's learning process, enabling faster convergence to learn optimal policies. The episodic novelty term is critical for effective exploration; without it, agents in some environments struggle to learn. However, when prelearned state embeddings are used (as in FoMoRL), access to full state information can reduce the dependence on the episodic term. This benefit is lessened if the embedding must be learned during training (as in RIDE), as the initial exploration can hinder the characterization of suitable state representations.\nIn the future we plan to extend our approach to more MiniGrid environments, other RL benchmarks with visual inputs, and explore alternatives such as alignment approaches that build on top of the embeddings provided by CLIP and other foundation models."}, {"title": "Model Architectures and Hyperparameters", "content": "Intrinsic motivation. We conducted a grid search over the range $\\beta \\in \\{0.1, 0.05, 0.01, 0.005, 0.001, 0.0005,0.0001\\}$ to determine the best scaling factor for $r_i$ in each environment. Table 3 summarizes the selected coefficients when considering both full and partial information with the inclusion of the episodic term. Table 4 provides the chosen values when the episodic component is not used.\nPPO. We used a discount factor $\\gamma = 0.99$, a clipping factor $\\epsilon = 0.2$, 4 epochs per training step, and $\\lambda = 0.95$ for GAE. We employed 16 parallel environments to gather rollouts of size 128, resulting in a total horizon of 2,048 steps between updates.\nNeural Network Architecture. We used an actor-critic architecture with shared weights between the actor and critic networks. The shared network consisted of three convolutional layers (each with 32 filters of size 3 \u00d7 3, a stride of 2, and padding of 1), followed by a fully connected layer with 256 units. This shared representation was then fed into two separate heads: one for the actor (policy) and another for the critic (value function)."}]}