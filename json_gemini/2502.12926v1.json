{"title": "Towards more Contextual Agents: An extractor-Generator Optimization Framework", "authors": ["Mourad Aouini", "Jinan Loubani"], "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable success in solving complex tasks across a wide range of general-purpose applications. However, their performance often degrades in context-specific scenarios, such as specialized industries or research domains, where the absence of domain-relevant knowledge leads to imprecise or suboptimal outcomes. To address this challenge, our work introduces a systematic approach to enhance the contextual adaptability of LLM-based agents by optimizing their underlying prompts critical components that govern agent behavior, roles, and interactions. Manually crafting optimized prompts for context-specific tasks is labor-intensive, error-prone, and lacks scalability. In this work, we introduce an Extractor-Generator framework designed to automate the optimization of contextual LLM-based agents. Our method operates through two key stages: (i) feature extraction from a dataset of gold-standard input-output examples, and (ii) prompt generation via a high-level optimization strategy that iteratively identifies underperforming cases and applies self-improvement techniques. This framework substantially improves prompt adaptability by enabling more precise generalization across diverse inputs, particularly in context-specific tasks where maintaining semantic consistency and minimizing error propagation are critical for reliable performance. Although developed with single-stage workflows in mind, the approach naturally extends to multi-stage workflows, offering broad applicability across various agent-based systems. Empirical evaluations demonstrate that our framework significantly enhances the performance of prompt-optimized agents, providing a structured and efficient approach to contextual LLM-based agents.", "sections": [{"title": "Introduction", "content": "Large Language Model (LLM)-based agents have achieved remarkable success in tackling complex tasks across diverse, general-purpose applications, demonstrating versatility in understanding, reasoning, and generating coherent responses across a wide array of domains [WMF+23][ZCJ+24][LOS+23]. However, when applied to context-specific scenarios, such as specialized industries or research domains, LLM performance often suffers from a lack of domain-specific knowledge, resulting in less accurate and less reliable outcomes for tasks requiring detailed and precise understanding. For instance, [L+23] found that smaller, specialized clinical models can substantially outperform larger LLMs in tasks requiring specific domain knowledge. To tackle this challenge, we present a systematic method for improving the contextual adaptability of LLM-based agents. Our approach focuses on optimizing the agents defined by prompts that control their behavior, roles, and interactions, ensuring more accurate performance in domain-specific tasks. Another key advantage of our proposed approach is its ability to mitigate the problem of prompt sensitivity highlighted by [VBK24], resulting in agents that maintain stable performance across varying prompts and task conditions.\nClassical prompting techniques, such as chain-of-thought prompting [WWS+22], program-of-thoughts prompting [CMWC22], and multistage pipeline architectures [WTC22] [KSL+22] [SSC+23], have significantly improved the reasoning capabilities of LLMs. However, these methods often remain insufficient for achieving optimal performance in LLM-based agents, particularly in complex, context-dependent tasks. Current approaches to constructing LLM-based agents predominantly rely on manual prompt engineering, involving predefined templates and human-crafted modifications\u2014a process that is both time-intensive and prone to suboptimal outcomes [AC24]. Recent advancements in declarative language model pipelines [KSM+24][OORP+24][WSNA24][WZS+25] aim to optimize critical prompt components, such as instructions and demonstrations, yet these approaches do not cover other critical components that define prompt-optimized agents behavior. This gap underscores the necessity of developing a systematic and automated framework for optimizing LLM-based agents, which is crucial to enhance their efficiency, adaptability, and reliability across diverse, context-specific applications.\nIn this article, we address the problem of optimizing LLM-based agents by formulating it as a structured learning task, and by decomposing the prompt-optimized agent into several components. We propose an Extractor-Generator framework that automates optimizing contextual LLM-based agents by systematically refining their performance. In the first stage, a feature extraction module analyzes a dataset of gold-standard input-output pairs to identify key contextual features. These extractions are performed in parallel on a small dataset. In the second stage, a prompt-generation mechanism employs an iterative optimization strategy, detecting underperforming cases and applying self-improvement techniques. Unlike prior research, which focuses on manually refining prompts or leveraging external knowledge sources, our framework automatically refines agents components based on iterative feedback. This two-step approach enhances the effectiveness of prompt-optimized agents, allowing the model to generalize better across diverse inputs while minimizing errors in context-specific applications. By iteratively refining the performance of feedback-based prompts, the framework reduces error propagation, thereby improving the reliability of LLM-based agents. While initially designed for single-stage workflows, its modular structure inherently supports extension to multistage processes, demonstrating potential applicability across a wide range of agent-based systems. Empirical results show that our framework substantially improves the performance of prompt-optimized agents, offering a systematic and efficient method to enhance the capabilities of contextual LLM-based agents."}, {"title": "Designing Extractor-Generator Framework", "content": "and enhance the performance of LLM-based agents through a systematic, data-driven process. Our approach is divided into two key stages:\n1. Features Extraction: This stage involves analyzing task-specific data to generate a matrix M of feature vectors. These features capture essential patterns and contextual signals that influence agent behavior and performance.\n2. Prompt-Components Generation: Using the extracted feature representations, this stage focuses on constructing and refining prompt components. The goal is to optimize the agent's behavior by generating contextually relevant instructions, demonstrations, and other prompt elements that guide the model's decision-making.\nFormally, given the matrix of feature vectors M and an LLM-based agent \u03a8, the optimization problem can be mathematically defined as follows:\n\u03a8* = arg max \u03a3 f(\u03a8(x), y)\nwhere f is an objective function and IO is the set of gold input-output pairs. Figure 1 shows the overall architecture of features extraction and prompt generation.\nFeatures Extraction\nGiven a small set of gold-standard input-output pairs IO, we extract the key contextual features using an Extractor-LLM. This extractor operates as a multi-agent system with a fully distributed topology, where each agent is specialized in capturing a distinct feature dimension. While all agents utilize the same underlying Extractor-LLM, multiple extractors can be employed to enhance robustness and coverage. For each extracted feature, a self-improvement mechanism is applied, as illustrated in Figure 2, to iteratively refine the feature representation. Consequently, each input-output pair is transformed into a feature vector that encodes task-relevant contextual information. The resulting contextual feature vectors serve as a foundation for generating the core prompt components, as described in the following section."}, {"title": "Components Generation", "content": "We consider the matrix MN,L whose N rows are defined by the vectors {Vx,y}(x,y)\u2208IO of extracted features, where N is the dimension of the data and L is the features dimension. A prompt Pt is then generated iteratively using sub-matrices MB,L, randomly sampled batches of rows from MN,L, as shown in Algorithm 1. Each prompt Pt is initialized and self-improved by a Generator LLM-based agent G, and its performance st is evaluated on the dataset. The best-performing prompt Pb, based on its evaluation st, is further optimized using an additional refinement step shown in Algorithm 2. At each iteration, it evaluates the prompt's effectiveness on individual input-output pairs (x, y) using an evaluation metric \u03bc. If u indicates underperformance, i.e. if \u03bc(P\u00bf, \u03a8(x), y) < \u03bb for some threshold \u5165, the prompt is updated and self-improved using G. This process repeats up to a few number of times (three times for example), or until the threshold is met, to ensure self-improvement. The overall performance si of the updated prompt is then calculated for the entire dataset. If the new performance si surpasses the current best score s, the prompt and score are updated accordingly. After a maximum number of iterations, the optimal prompt Popt with the highest performance is returned. Figure 3 shows the steps followed in every iteration."}, {"title": "Self-Improvement", "content": "We use a mechanism of self-improvement three times in our approach; features extraction in Algorithm 1, first prompt generation in Algorithm 1, and updating and optimizing prompt in Algorithm 2. Self-improvement in a workflow refers to the process where the system iteratively enhances its own performance by leveraging previous results as feedback. This involves feeding the program's past outputs back into itself, allowing it to analyze errors,"}, {"title": "Multistage Workflow", "content": "The generalization of the proposed Extractor-Generator Framework from single-stage to multi-stage workflows involves extending the optimization process from individual agents to the entire multi-agent system. Initially, each agent undergoes optimization using the same iterative feature extraction and prompt-generation method to enhance task-specific performance. Once agent-level optimization is complete, the framework applies the same optimization strategy at the system level, ensuring that inter-agent interactions and collaborative performance are jointly optimized. This system-level optimization is guided by a predefined workflow topology, which dictates the structural arrangement and operational orchestration of agents within the multi-stage pipeline. Workflow topologies can follow various configurations depending on task requirements. Effective connectivity between agents is essential to facilitate the seamless exchange of contextual information, ensuring consistency and coherence across stages. By integrating agent-level and system-level optimization within a structured yet adaptable workflow, the framework enhances the robustness and efficiency of LLM-based agents in complex, multi-stage applications."}, {"title": "Experiments", "content": "To rigorously evaluate the effectiveness of our optimization framework for LLM-based agents, we conduct a series of experiments across five distinct application domains: finance, healthcare, e-commerce and retail, law, and cybersecurity. Each domain-specific dataset comprises 150 examples for optimization and 300 examples for testing, presented in the form of Frequently Asked Questions (FAQs) to ensure a diverse and contextually rich evaluation set.\nWe evaluate and compare our contextual agent with multiple prompting pipelines, including chain-of-thought (CoT) prompting, sequential CoT and self-consistency CoT. We consider the optimal versions of these pipelines with only instructions optimized by our Extractor-Generator Framework. The evaluation relies on an answer relevancy metric, assessed by a dedicated LLM-based judge agent. This agent evaluates the generated responses based on correctness, coherence, and informativeness, providing a consistent and systematic framework for performance assessment."}, {"title": "Conclusion", "content": "In this work, we introduced the Extractor-Generator Framework, a systematic and scalable solution for optimizing LLM-based agents in context-specific tasks. By implementing a two-stage process\u2014feature extraction and prompt generation\u2014our approach overcomes the limitations of manual prompt engineering, which is often labor-intensive, error-prone, and suboptimal. The framework identifies task-relevant contextual features through a distributed multi-agent extraction process and applies an iterative self-improvement mechanism to generate optimized prompts.\nEmpirical evaluations demonstrate that the proposed method significantly enhances the performance of LLM-based agents by improving their ability to generalize across diverse contexts while maintaining semantic consistency. The framework's adaptability extends naturally from single-stage to multi-stage workflows, highlighting its potential for broader applications in complex, dynamic environments.\nThe results of this study underscore the critical role of automated, context-aware prompt optimization in enhancing the effectiveness and reliability of LLM-based agents. By shifting from static, handcrafted prompts to a dynamic, data-driven optimization process, the Extractor-Generator Framework provides a robust foundation for more efficient and contextually intelligent language model applications across a wide range of domains."}]}