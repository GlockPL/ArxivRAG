{"title": "Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue", "authors": ["Daniela B\u00f6hm", "Matthias Lanzinger", "Reinhard Pichler", "Alexander Selzer", "Georg Gottlob", "Davide Mario Longo", "Cem Okulmus"], "abstract": "Query optimization has played a central role in database research for decades. However, more often than not, the proposed optimization techniques lead to a performance improvement in some, but not in all, situations. Therefore, we urgently need a methodology for designing a decision procedure that decides for a given query whether the optimization technique should be applied or not.\nIn this work, we propose such a methodology with a focus on Yannakakis-style query evaluation as our optimization technique of interest. More specifically, we formulate this decision problem as an algorithm selection problem and we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that our approach indeed leads to a statistically significant performance improvement.", "sections": [{"title": "1 INTRODUCTION", "content": "Query optimization has played a central role in database research for decades. Some generally accepted techniques such as replacing Cartesian product plus selection by a join or projecting out attributes not needed further up in the query plan as soon as possible are guaranteed to (almost) always lead to a performance improvement. However, more often than not, optimization techniques proposed in the literature lead to a performance improvement in some, but not in all, situations. Moreover, it is usually a non-trivial task to delineate the situations where the application of a specific optimization is advantageous and where it is not. This also applies to optimization techniques which, in theory, should (almost) always outperform the conventional query evaluation methods.\nA prominent example of an optimization technique is Yannakakis' algorithm [76] in case the query is acyclic. Several applications and extensions of this algorithm in recent time (see, e.g., [9, 36-38, 42, 70, 71]) witness the renewed interest in this approach. The key idea of this algorithm is to first eliminate all dangling tuples (= tuples that will not contribute to the final result of the query) via semi-joins and then compute the join with the guarantee that all intermediate results thus obtained will be extended to tuples in the final result. Hence, in a sense, Yannakakis' algorithm solves the (in general NP-complete) join ordering problem that aims at avoiding the explosion of intermediate results. In theory, such a strategy of completely avoiding the generation of useless intermediate results should always be superior to conventional techniques that just try to compute the joins in an optimal order and thus aim at minimizing useless intermediate results. However, in practice, it turns out that Yannakakis' algorithm leads to a performance improvement in some cases but, by no means, in all cases.\nIn this work, we revisit a sub-class of acyclic queries called 0MA (= zero materialization aggregate) queries [25], where the theoreti- cal advantage of Yannankakis-style query evaluation is even more extreme: 0MA-queries are a restricted class of join queries with an aggregate on top and which can be evaluated by carrying out only semi-joins, i.e., completely avoiding the need for computing any joins. More precisely, after traversing the join tree T of such a query by semi-joins in bottom-up direction, the result can be computed from the relation resulting at the root node of T.\nIn theory, it is \"clear\" that such a join-less evaluation must always outperform conventional query evaluation techniques that first fully evaluate the underlying join query and only then apply the aggregate as a kind of post-processing. Alas, empirical evaluation on queries from several benchmarks shows that, in practice, this is not necessarily the case. Actually, for the query in Figure 1, Yannankakis-style evaluation was significantly faster (by a factor of"}, {"title": "2 PRELIMINARIES", "content": "Conjunctive Queries and beyond. The basic form of queries studied here are Conjunctive Queries (CQs), which correspond to select- project-join queries in the Relational Algebra. It is convenient to consider CQs as Relational Algebra expressions of the form $Q = \\pi_U (R_1 \\bowtie ... \\bowtie R_n)$. Here we assume w.l.o.g., that equi-joins have been replaced by natural joins via appropriate renaming of attributes. Moreover, we assume that selections applying to a single relation have been pushed immediately in front of this relation and the $R_i$'s are the result of these selections. The projection list $U$ consists of attributes occurring in the $R_i$'s.\nTo go beyond CQs, we will also consider the extension of Relational Algebra by the grouping operator $\u03b3$ and aggregate expressions. In other words, we are interested in queries of the form\n$Q = \\gamma_{g_1,...,g_e, A_1(a_1),...,A_m(a_m)} (R_1 \\bowtie ... \\bowtie R_n)$    (1)\nwhere $\u03b3_{g_1,...,g_e, A_1(a_1),...,A_m(a_m)}$ denotes the grouping operation for attributes $g_1,..., g_e$ and aggregate expressions $A_1(a_1),..., A_m(a_m)$. The grouping attributes $g_1,..., g_e$ are attributes occurring in the relations $R_1,..., R_n$, the functions $A_1, ..., A_m$ are (standard SQL) aggregate functions such as MIN, MAX, COUNT, SUM, AVG, MEDIAN, etc., and $a_1, ..., a_m$ are expressions formed over the attributes from $R_1,..., R_n$. Note that we have omitted the projection $\u03c0_U$ in Equation (1), since it can be taken care of by the grouping. A simple query of the form shown in Equation (1) is given in SQL-syntax in Figure 1.\nAcyclicity. Several forms of acyclicity have been studied in the literature [11, 22]. Our notion of acyclicity is the so-called \u03b1-acyclicity. An acyclic conjunctive query (an ACQ, for short) is a CQ $Q = \u03c0_U (R_1 \\bowtie ... \\bowtie R_n)$ that has a join tree, i.e., a rooted, labeled tree $(T, r, \u03bb)$ with root $r$, such that (1) $\u03bb$ is a bijection that assigns to"}, {"title": "3 RELATED WORK", "content": "Acyclic queries and Yannakakis-style query evaluation. Yannakakis' algorithm [76] has recently received renewed attention for the opti- mization of hard join queries. Recent work has focused on bringing its advantages into DBMSs from the outside via SQL query rewriting [24, 25, 41], and similar methods such as generating Scala code expressing Yannakakis' algorithm as Spark RDD-operations [16]. Even more recently, Yannakakis-like approaches have been pro- posed, which aim to reduce the overhead of the full Yannakakis' algorithm for enumeration, with its 3 traversals, by instead propa- gating up additional data and computing the whole query in only one traversal. Such approaches have been integrated into Spark SQL [42], Umbra [9], DuckDB [29], and Apache DataFusion [7]. Further research extends Yannakakis' algorithm to non-equi-join queries, such as differences of CQs [36], acyclic queries with com- parisons [71], and queries with theta-joins [38].\nDecompositions. In order to go beyond acyclic queries, a major area of research seeks to extend Yannakakis-style query answer- ing to \"almost-acyclic\" queries via various notions of decomposi- tions and their associated width measures, such as hypertree-width, soft hypertree-width, generalized hypertree-width, and fractional hypertree-width [3, 26, 28, 41]. Several implementations [1, 16, 57, 68] combine Yannakakis-style query execution with worst-case optimal joins [53]. To address the problem of minimal-width de- compositions not necessarily being cost-optimal, approaches of"}, {"title": "4 FORMULATING THE ALGORITHM SELECTION PROBLEM", "content": "Whenever a new, optimized method for query evaluation is pre- sented, we need a decision program that decides whether the new method should be applied or not. We are thus faced with an algo- rithm selection problem, where we have to decide, for every database instance and query, which query evaluation method should be applied. In this section, we describe the steps needed to actu- ally formulate the precise algorithm selection problem at hand. An overview of this workflow is given in Figure 3."}, {"title": "4.1 Benchmark Data", "content": "We aimed to gather a diverse set of data and queries from different domains, designed for different purposes. Thus, we have chosen sev- eral widely used benchmarks, which represent a variety of queries and datasets: (1) The JOB (Join Order Benchmark) [43], which was introduced to study the join ordering problem, is based on the real- world IMDB dataset and contains realistic join-aggregation queries with many joins and various filter conditions, (2) STATS/STATS- CEB [31] is based on the Stackexchange-dataset, and contains many join queries not following FK/PK-relationships, (3) Four different datasets (namely cit-Patents, wiki-topcats, web-Google and com- DBLP) from SNAP (Stanford Network Analysis Project) [44], a collec- tion of graphs, which we combine with synthetic queries introduced in [42], (4) LSQB (Large-Scale Subgraph Query Benchmark) [51], which was designed to test graph databases as well as relational databases, consists of synthetic data and hard queries based on a social network scenario, and (5) HETIONET [35]. The latter is less known in the database world. It contains real-world queries on real-world data from a heterogeneous information network of biochemical data, and is part of the new CE benchmark[14], which has, for instance, been recently used in [9] and [7].\nWe are only interested in ACQs (possibly with group-by and aggregates). Most of the queries in the chosen benchmarks are CQs with additional filter conditions applied to single tables. These filter conditions can be taken care of by the preparatory step; so they pose no problem. However, not all of the CQs are acyclic; so we have to eliminate the cyclic ones from further consideration. The number of (acyclic) CQs of each dataset is given in Table 1.\nNote that some of the queries in the benchmarks are enumer- ation queries and some already contain some aggregate (in par- ticular, MIN) and satisfy the 0MA conditions. Of course, also from the enumeration queries, we can derive 0MA queries by putting an appropriate aggregate expression (again, in particular, with the MIN aggregate function) into the select-clause of the query. We do this by randomly choosing a table occurring in the query and one column of this table (mostly the first column in the table). We will see in Section 4.2. that it makes no significant difference which table and attribute we choose for turning a query into 0MA form, as we will vary the table and attribute anyway."}, {"title": "4.2 Data Augmentation", "content": "As can be seen in Table 1, we get 219 acyclic queries from the chosen benchmarks. Since we want to use these queries to train"}, {"title": "4.3 Selection of DBMSS", "content": "We want to check the effectivity of the optimized evaluation follow- ing Yannankakis' algorithm on a wide range of database technolo- gies. We have therefore chosen three significantly different DBMSs, namely (1) PostgreSQL 13.4 [64] as a \"classical\" row-oriented rela- tional DBMS, (2) DuckDB 0.4 [58] as a column-oriented, embedded database, and (3) Spark SQL 3.3 [78] as a database engine specifi- cally designed for distributed data processing in a cluster. These DMBSs represent a broad spectrum of architectures and charac- teristics and they, therefore, give a good overview of the range of existing DBMSs."}, {"title": "4.4 Query Rewriting", "content": "To recall our motivation, we aim to find methods that determine the effectiveness of various optimization methods - with a focus on Yannakakis' algorithm for this work - on various DBMSs before potentially having to commit to the significant effort of modifying existing query engines. Hence, we make use of ideas from recent works [16, 24, 25, 41] that present query rewritings, where a single SQL query is rewritten into an equivalent series of queries, with the aim to guide DBMSs to utilize a given optimization method. In Appendix B, we illustrate the query rewriting approach that we use in this paper, using an example query from our benchmark."}, {"title": "4.5 Feature Selection", "content": "We choose different kinds of features that we derive from the struc- ture of the query itself, from the join tree constructed in the process"}, {"title": "4.6 Running the Queries", "content": "The whole evaluation is performed on a server with 128GB RAM, a 16-core AMD EPYC-Milan CPU, and a 100GB SSD disk, running Ubuntu 22.04.2 LTS. After a warm-up run, the original query, as well as the rewritten version, is evaluated five times, and then we take the mean of those five runtimes. In total we get 6 data points for each of the 4677 queries: each query is run against 3 DBMSs, where the query evaluation happens once with and once without optimization. Aggregated information on these runtimes is provided in Section 6.\nIn addition to the six runtimes, we also add a \"feature vector\", consisting of the features described in Section 4.5. These provide the input to the training of ML models to be described next."}, {"title": "5 SOLVING THE ALGORITHM SELECTION PROBLEM", "content": "Several decisions have to be made to solve the algorithm selection problem that results from the steps described in the previous section. In particular, we have to (1) formulate the concrete ML task and then (2) select ML model types together with hyperparameters appropriate to our context. Before we can start training and testing the models, we have to (3) split the data (in our case the SQL queries) into training/validation/test data. Finally, we have to (4) define selection criteria for determining the \"best\" model, which will then be used as basis of our decision program between the original query evaluation method of each system and a Yannakakis-style evaluation."}, {"title": "5.1 Formulating the ML Task", "content": "Our ultimate goal is the development of a decision program be- tween two evaluation methods for acyclic queries. Hence, we are clearly dealing with a classification problem with 2 possible outcomes. In the sequel, we will refer to these two possible outcomes as 0 vs. 1 to denote the original evaluation method of the DBMS vs. a Yannakakis-style evaluation (enforced by our query rewriting).\nOn the other hand, it also makes sense to consider a regression problem first and, depending on the predicted value, classify a query as 0 (if the predicted value suggests faster evaluation by the original method of the DBMS within a certain threshold) or 1 (otherwise).\nAs target of the regression problem we would like to choose the difference $t_{rewritten} - t_{original}$, where we write $t_{rewritten}$ and $t_{original}$ to denote the time needed by Yannakakis-style evaluation and by the original evaluation method of the DBMS, respectively. However, as will become clear in our presentation of the experimental results in Section 6, the actual runtime values are very skewed, in the sense that their distribution shows high variance. Hence, the difference we focus on is also highly skewed. To get more reliable results, we therefore perform a log-transformation, which we provide below. Since we have negative values, however, this cannot be applied directly. Instead, we multiply the log of the absolute values with"}, {"title": "5.2 Selecting the ML Model Types", "content": "We have chosen 7 Machine Learning model types for our algorithm selection problem, namely k-nearest neighbors (k-NN), decision tree, random forest, support vector machine (SVM), and 3 forms of neural networks (NNs): multi-layer perceptron (MLP), hypergraph neural network (HGNN) and a combination of the two. MLP is the \"classical\" deep neural network type. Hypergraph neural networks, introduced in [23], are less known. With their idea of representing the hypergraph structure in a vector space, it seems well suited to capture structural aspects of conjunctive queries. Just like MLPs, also the HGNNs produce an output vector. In our combination of the two model types, we provide yet another neural network, which takes as input the two output vectors produced by the MLP and the HGNN and combines them to a joint result using addtional layers.\nA major task after choosing these ML model types is to fix the hyperparameters. For instance, in case of k-nearest neighbors, we settled for k = 5 after some preliminary experiments. For random forest, we fixed the number of combined decision trees as 100. For the support vector machine, we chose 3 different kernels, namely linear, poly, and rbf.\nFor the 3 types of neural networks, several additional hyperpa- rameters had to be chosen. As loss function, we chose the cross entropy for classification and the mean squared error (MSE) for regression. For the training of the NNs, we set the number of epochs (i.e., how often the dataset is used in one training step) and also the batch size (i.e., after how many runs we update the weights) to 100, with the learning rate set to 0.001. The number of hidden layers for the MLP is either 1 or 2, and we experiment with various numbers of nodes. The HGNN has two or three convolutional layers with a kernel size of 3x3 and then one max-pooling layer. For the combina- tion of the MLP and the HGNN, we use the best performing model of the MLPs and the best performing model of the HGNNs and combine the outputs by applying one, two or three linear layers. Finally, the number of features is reduced. More precisely, several features that occur multiple times as min, max, mean, median 25%- and 75% quartiles are reduced to a single node via a custom NN. A detailed list of all hyperparameter values, in particular for the 3 types of neural networks used here, is provided in Appendix D."}, {"title": "5.3 Labeling and Splitting the Data", "content": "After running the 4677 queries mentioned in Section 4.2 on the 3 selected DBMSs according to Section 4.3, we have to prepare the input data for training the ML models of the 7 types mentioned in Section 5.2. Recall from Section 4.5 that each query is character- ized by a feature vector specific to each of the 3 DBMSs. For our supervised learning tasks (classification and regression), we have to label each feature vector for each of the 3 DBMSs. As explained in Section 5.2, we want to train our models both, for classification and"}, {"title": "5.4 Selection Criteria for the \"Best\" Model", "content": "In order to ultimately choose the \"best\" model for our decision program between the original evaluation method of each DBMS and the Yannakakis-style evaluation, we compare, for every feature vector, the predicted classification with the actual labeling. We refer to classification 1 as \u201cpositive\u201d and classification 0 as \"negative\u201d. This leads to 4 possible outcomes of the comparison between pre- dicted and actual value, namely TP (true positive) and TN (true negative) for correct classification and FP (false positive) and FN (false negative) for misclassification. They give rise to the 3 most common metrics: accuracy (shortened to \"Acc\"), which is the pro- portion of correct classifications, precision (shortened to \"Prec\u201d), which is defined by TP / (TP + FP), and recall (shortened to \"Rec\u201d), defined as TP / (TP + FN).\nOf course, the natural goal when selecting a particular model is to maximize the accuracy. However, in our context, we consider the precision equally important. That is, we find it particularly important to minimize false positives. In other words, in case of doubt, it is better to stick to the original evaluation method of the DBMS rather than wrongly choosing an alternative method.\nFor regression, we aim at minimizing the mean squared error (MSE). But ultimately, we also map the (predicted and actual) differ- ence between the runtime of the original minus Yannakakis-style evaluation to a 0 or 1 classification. Hence, we can again measure the quality of a model in terms of accuracy, precision, and recall.\nApart from the purely quantitative assessment of a model in terms of accuracy, precision, and recall, we also carry out a qual- itative analysis. That is, for each of the misclassified cases, we want to investigate by how much the chosen evaluation method is slower than the optimal method. And here, we are again partic- ularly interested in the false positive cases. Apart from aiming at high accuracy and precision, we also want to make sure for the false positive classifications, that the difference in the runtimes between the 2 evaluation methods is rather small."}, {"title": "6 EXPERIMENTAL RESULTS", "content": "In this section, we present experimental results obtained by putting the algorithm selection method described in Sections 4 and 5 to work. We thus first evaluate in Section 6.1 the performance of vari- ous machine learning models on the raw dataset of query runtimes obtained for the selected and augmented benchmarks on the chosen DBMSs. Afterwards, in Section 6.2, we evaluate the performance gains by combining the best algorithm selection model with our rewriting method for evaluating acyclic queries. In particular, we perform experiments to answer the following key questions:\nQ1 How well can machine learning methods predict whether Yannakakis-style query evaluation is preferable over stan- dard query execution?\nQ2 Can we use these machine learning models to gain insights about the circumstances in which Yannakakis-style query evaluation is preferable over standard query execution?\nQ3 How well does good algorithm selection performance translate to query evaluation times on different DBMSs?\nQ4 To what extent can we optimize for precision while main- taining end-to-end runtime and accuracy?\nIn the remainder of this section, we present our experimental results and a discussion as to how they answer our key question. Further details are provided in Appendix E."}, {"title": "6.1 Model Training", "content": "In a first step, we will compare the performance of various learned models in terms of accuracy, precision, and recall. Table 2 com- pares the performance of the best classification models (with the hyperparameters given in Table 3) on the 0MA queries only, as well"}, {"title": "7 CONCLUSION", "content": "Summary of results and some lessons learned. In this work, we have addressed the problem that optimization methods in query eval- uation usually lead to a performance improvement in some, but not in all, situations. The optimization method at the center of our investigation was Yannakakis-style query evaluation. We have for- mulated the question when to apply the optimization method and when to stick to the original evaluation method of the DBMS as an algorithm selection problem. By making use of ML-techniques, we have presented a methodology for solving this problem. Our empir- ical evaluation has shown that decision trees (as a simple and easy to interpret) ML model type are very well suited for this task, as is witnessed by very high accuracy and precision values. The analysis of the decision trees obtained for the different DBMSs (and, in par- ticular, of the Gini coefficients of the selected features) has revealed interesting differences among the DBMSs. For instance, while es- timates on the result size are most important for PostgreSQL, the decision in case of DuckDB is most strongly influenced by prop- erties of the query such as query type (enumeration vs. simple aggregate query) and properties of the chosen join tree. Statistical analysis of the runtimes obtained with the resulting decision proce- dure has verified that the selective application of the optimization technique (with decisions based on the trained ML model) indeed may yield a significant performance gain.\nNext steps. Our work does not stop here. Given our aim of testing optimization methods, we believe our positive results here justify the complex task of integrating some of these methods, in particu- lar with a focus on Yannakakis' algorithm, into real-word DBMSs. These optimization methods should then be used in combination with algorithm selection mechanisms, in the vein of the one pre- sented here. As another research question, we also aim to consider a wider range of optimization techniques: since the structure of the underlying join tree clearly influences Yannakakis's algorithm when used for query evaluation, one may also investigate the use of machine learning methods to quickly determine optimal join trees, and as a next step even use them to heuristically search for optimal hypergraph decompositions, which would then allow one to use such optimization methods also on cyclic queries."}, {"title": "A FURTHER INFORMATION FOR SECTION 4.2", "content": "In this section we aim to give further details into the data augmenta- tion. In Example 4.1, we have illustrated the filter augmentation. We continue here by adding another example to illustrate enumeration augmentation.\nExample A.1. Consider the query qaug1, given in Example 4.1. We present three different enumeration augmentations, whereby the query output is transformed from producing a single output to enumerating a number of rows. One option to \"enum-augment\" qaug1 is to swap the aggregation function MIN(v.Id), with a projec- tion on the Id column of the users table, and the UserID column of the votes table producing the query $q^{aug1}_{e1}$. Other options, shown below, lead to the queries $q^{aug1}_{e2}$ and $q^{aug1}_{e3}$.\nSELECT MIN(u.Id)\nFROM votes as v, badges as b, users as u\nWHERE u.Id = v. UserId AND v. UserId = b.UserId\nAND v. BountyAmount>=40 AND v. BountyAmount<=50\nAND u. DownVotes=0\nSELECT u.Id, v.UserId\nFROM votes AS v, badges AS b, users AS u\nWHERE u.Id = v. UserId AND v. UserId = b.UserId\nAND v. BountyAmount>=40 AND v. BountyAmount<=50\nAND u. DownVotes=0\nSELECT u.Id, b.UserId\nFROM votes AS v, badges AS b, users AS u\nWHERE u.Id = v. UserId AND v. UserId = b.UserId\nAND v. BountyAmount>=40 AND v. BountyAmount<=50\nAND u. DownVotes=0\nSELECT v.UserId, b.UserId\nFROM votes AS v, badges AS b, users AS u\nWHERE u.Id = v. UserId AND v. UserId = b.UserId\nAND v. BountyAmount>=40 AND v. BountyAmount<=50\nAND u. DownVotes=0"}, {"title": "B DETAILS ON THE QUERY REWRITING", "content": "In this section we aim to give the reader technical insights into the query rewriting process, where we follow the approach of [25]. This approach involves rewriting each query into an equivalent sequence of SQL queries (i.e. one that gives the same result) but which \"forces\u201d (or, at least, guides) the DBMSs to apply a Yannakakis-style evaluation strategy. The rewrite process is visualized in Figure 8.\nWe thus take a SQL query as input and first transform it into a hypergraph. The approach by [25] begins by applying the GYO- reduction [77]. One thus verifies that the CQ is acyclic and, if so, constructs a join tree. This approach does not support cyclic"}, {"title": "C FURTHER INFORMATION FOR SECTION 4.5", "content": "In this section, we aim to illustrate the complete feature vector for two example queries from our data set, namely \"HETIO_2-01-CbGaD\", termed $q_1$ and \"HETIO_3-06-CdGuCtD\" termed $q_2$, shown below.\nSELECT MIN(c.nid)\nFROM compound c, binds b, gene g, associates a, disease d\nWHERE c.nid = b.sid AND b.tid = g.nid AND g.nid = a.tid AND a.sid = d.nid\nSELECT MIN(c1.nid)\nFROM compound c1, downregulates d1, gene g,\nupregulates u2, compound c2, treats t, disease d\nWHERE c1.nid=d1.sid AND d1.tid = g.nid AND\ng.nid = u2.tid AND u2.sid = c2.nid AND\nc2.nid = t.sid AND t.tid = d.nid\nTheir calculated join trees are given in Figures 10 and 11, re- spectively. Recall from Section 4.5 that some of the features of the queries are determined by the join tree. In Table 7, the values of all features of these two queries are given. For the set-based fea- tures, we also explicitly show the six statistical data points that are extracted from the set to produce the feature vector used for the models to be trained."}, {"title": "D FURTHER INFORMATION FOR SECTION 5", "content": "We provide here a list of all hyperparameter values for the various model types considered here, i.e., three types of neural network models, k-NN, random forests, and SVM, and decision trees. The details are given in Table 8. The experiments presented here are a continuation of the work in [10], to which we can refer for even more detailed explanations of the models and hyperparameters, as well as further literature on the machine learning concepts. In"}, {"title": "E FURTHER DETAILS ON THE EXPERIMENTAL EVALUATION", "content": "We present here further information on the experiments conducted in Section 6.\nFull details on Gini coefficients. Recall that in Section 6 we men- tion the role of Gini coefficients and list only the five features with the highest coefficients for PostgreSQL and DuckDB. As was men- tioned, the Gini coefficient, as used in this paper, measures the contribution of the feature to the outputs of the model. In Table 9 we then give a complete overview of the 8 highest Gini coefficients among the features of the trained decision tree models for all three DBMSs, including SparkSQL.\nFurther Details on the Significance Tests. We briefly commented in Section 6 on the significance tests we performed. Here we give a more detailed report, including explanations on how these tests are defined.\nFor our significance tests, we compare the mean or median of the runtimes achieved when applying our decision procedure versus always using the original evaluation method of the DBMSs and always using Yannakakis-style evaluation, respectively.\nSince we compare runtimes obtained with the same test set, we need to use statistical tests, which take these dependencies into consideration. For the median we take the Wilcoxon sign-rank test [73] and for the mean we use a paired sample t-test [59].\nWilcoxon sign-rank test. The null hypothesis of this test for two (dependent) groups A and B is that the medians are equal: $H_0$: $median(A) = median(B)$. To get the test statistic the differences between all pairs of group A and B are calculated and ranked. Additionally, the sign of the difference is used, so that all ranks of the positive differences are summed and the same for the negative ones. The minimum of these two is the test statistic, which then can be compared to the Wilcoxon signed rank table to get the p-value.\n$t = \\frac{\\overline{d} \\sqrt{n}}{s}$ with $\\overline{d} = \\frac{1}{n} \\sum_{i=1}^{n} d_i$, $s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (d_i - \\overline{d})^2}$. Here the t-test tables can be used to get the p-value and again if it is smaller than alpha, the null can be rejected and we can conclude that the means are significantly different.\nFor both tests, we choose 0.1 as alpha value as a common choice. In Table 10, we have the p-values for the Wilcoxon sign-rank test (Wilcoxon s.-r.) in the left group of 2 columns with numbers and for the paired sample t-test (p.s. t-test) in the right-most group of 2 columns. Inside each group, the first column compares the median resp. mean of the runtimes obtained by the original evaluation method (Orig) of each DBMS with the evaluation method chosen by the decision procedure (Dec). The second column of each group compares the median resp. mean of the runtimes obtained by the rewritten queries (Rewr, i.e., by Yannakakis-style query evaluation) with the evaluation method chosen by the decision procedure (Dec). We can see that all p-values are extremely small. For any reasonable choice of alpha value (note that alpha values such as 0.1 or 0.05 are common choices; the p-values in the table are much smaller) we can reject the null hypothesis in all cases. This means that the decision procedure indeed yields a significant performance improvement (in terms of means and median of runtimes)."}]}