{"title": "Large Language Models for Anomaly Detection in Computational Workflows: from Supervised Fine-Tuning to In-Context Learning", "authors": ["Hongwei Jin", "George Papadimitriou", "Krishnan Raghavan", "Pawel Zuk", "Prasanna Balaprakash", "Cong Wang", "Anirban Mandal", "Ewa Deelman"], "abstract": "Anomaly detection in computational workflows is critical for ensuring system reliability and security. However, traditional rule-based methods struggle to detect novel anomalies. This paper leverages large language models (LLMs) for workflow anomaly detection by exploiting their ability to learn complex data patterns. Two approaches are investigated: 1) supervised fine-tuning (SFT), where pre-trained LLMs are fine-tuned on labeled data for sentence classification to identify anomalies, and 2) in-context learning (ICL) where prompts containing task descriptions and examples guide LLMs in few-shot anomaly detection without fine-tuning. The paper evaluates the performance, efficiency, generalization of SFT models, and explores zero-shot and few-shot ICL prompts and interpretability enhancement via chain-of-thought prompting. Experiments across multiple workflow datasets demonstrate the promising potential of LLMs for effective anomaly detection in complex executions.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing complexity and scale of modern systems, computational workflows are growing in complexity while their reliability, security, and performance are becoming rather important. A critical factor in ensuring workflow execution reliability is the ability to detect anomalies. These anomalies can be indicators of various system issues, and they are manifested by unexpected behavior in hardware, such as high usage of computing resources, memory consumption, and I/O operations. To address the problem of anomaly detection in modern systems, methods that rely on rule-based systems, statistical analysis, and machine learning techniques [1]\u2013[4] have become quite popular in recent years.\nDespite their effectiveness, a considerable amount of data preprocessing must be done to perform this detection because typical methods are limited to analyzing images or numerical values. Furthermore, to facilitate this data preprocessing, a lot of expert knowledge is needed to be put into carefully collecting and correlating low-level system statistics with workflow execution metadata that can be used to convert the raw logs into other formats. Adding to the complexity is the need for substantial ML expertise to navigate the wide array of available anomaly detection methodologies effectively. The field of ML presents a vast spectrum of models and techniques, each with its customization and application nuances. This diversity, while beneficial, also imposes a steep learning curve and necessitates a deep understanding of ML principles to tailor these models to specific anomaly detection tasks. Furthermore, the process of setting up and training these models\u2014integrating them into a system's workflow\u2014poses an additional challenge. This aspect of ML model deployment and maintenance may not align well with the skill set of system administrators, who are typically more versed in direct system maintenance rather than in the nuances of ML model training and tuning.\nLarge Language Models (LLMs) and their wide-spread democratization efforts have the potential to significantly transform anomaly detection in HPC systems by streamlining data preprocessing, enhancing pattern recognition, simplifying the deployment of machine learning models, enabling real-time monitoring, and fostering a supportive community ecosystem. By automating complex data processing tasks and offering advanced analytical capabilities, LLMs reduce the need for extensive expert knowledge, making sophisticated anomaly detection accessible to system administrators without deep technical backgrounds. Furthermore, their ability to process and analyze streaming data in real-time can ensure prompt detection and mitigation of potential system issues.\nA primary critique of LLMs concerns their energy/power consumption and model size, which are seen as barriers to their practical application in HPC data analysis. However, this perspective overlooks the significant advances in energy-efficient technologies and the optimization of LLMs for operation on a wide range of devices, from high-end servers to compact, low-power devices such as smartphones. These emerging technologies not only mitigate the energy and resource demands of running sophisticated LLMs but also expand their accessibility and usability across various platforms. Consequently, as these energy-efficient techniques continue to evolve and LLMs become increasingly optimized for smaller devices, the practicality of deploying LLMs for anomaly detection in HPC systems and beyond-becomes ever more feasible. This trajectory underscores the viability of LLMs as a transformative tool in anomaly detection, promising significant advancements in HPC system management and maintenance.\nWe develop an approach that leverages pre-trained Large Language Models (LLMs) to directly detect anomalies from log files generated during the execution of computational workflows. Specifically, we adapt these pre-trained models through Supervised Fine-Tuning (SFT) and Prompt Engineering via In-Context Learning (ICL). SFT employs a pre-trained model and trains on a smaller dataset of labeled examples for a specific task [5]. Unlike the training of LLMs in an unsupervised way, the SFT often consists of an input and a desired output. By updating the parameters of LLMs again through SFT, the model improves the performance for a downstream task. However, one common issue with LLMs is that they can perpetuate biases present in the data used to train them [6], especially when for the binary classification problem. Another common issue is catastrophic forgetting (CF) [7], which occurs in machine learning when a model forgets previously learned information as it learns new information. This is a common problem in supervised fine-tuned models, where the model is trained on a new task after it has already been trained on one or more previous tasks.\nIn-context learning (ICL), on the other hand, is an emerging paradigm where LLMs perform tasks by leveraging a few examples provided within the context of query [8] rather than relying on supervised fine-tuning with labeled data. ICL heavily relies on prompt engineering, providing examples and contextual cues that guide the LLMs in efficiently understanding and executing the desired task. A well-engineered prompt not only presents the LLM with relevant information but subtly instructs it on generating the appropriate output. It involves structuring the examples in a way that highlights patterns or relationships, using natural language templates that align with the task's goals, or including explicit instructions that direct the model's attention to critical aspects of the problem. This alignment can be performed specifically for the anomaly detection problem where the prompts contain information about the job features and brief statistics about the job execution facilitating anomaly detection in the workflow. Furthermore, prompts can also include the instruction for reasoning steps through Chain-of-Thought (CoT [9]), providing explainable output from LLMs.\nTo this end, we make the following contributions to the scope of this paper:\n1) Investigate the efficacy of LLMs for anomaly detection and evaluate the performance of supervised fine-tuning models and in-context learning in detecting anomalies in computational workflows.\n2) Address the biases, overcome the catastrophic forgetting, and explore the generalization through transfer learning and online detection.\n3) Explore the ability of ICL with zero-shot, few-shot learning, and study the interpretable output from ICL through Chain-of-Thought (CoT).\nWith the use of LLMs for anomaly detection in computational workflows, we seek to contribute to the development of effective and efficient methods for detecting anomalies in computational workflows."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Several approaches have been proposed in the literature for anomaly detection in computational workflows. These approaches can be broadly classified into rule-based systems, statistical analysis, and machine-learning techniques. Rule-based systems rely on predefined thresholds and patterns to detect anomalies. For example, [10] proposed a rule-based system that uses a set of heuristics to identify anomalies in Linux computational workflows.\nWhile rule-based systems are simple to implement, they are limited by their inability to adapt to changing patterns in behavior and are often brittle in the face of new anomalies. Statistical analysis techniques, that use statistical information such as mean, median, and standard deviation, have been used to detect anomalies in computational workflows as well. For example, [11] proposed an approach that uses statistical methods to identify anomalies in network traffic logs. However, statistical analysis techniques are sensitive to outliers and may not be effective in detecting anomalies that do not deviate significantly from the mean.\nMachine learning techniques, such as decision trees, random forests, and clustering, have also been applied to anomaly detection in computational workflows. For example, [12] presented a simple and effective algorithm for spectral clustering, a method for grouping data points based on their pairwise similarities. The algorithm utilizes the eigenvectors of the graph Laplacian to represent similarities between the data points. The authors also provide a theoretical analysis of the algorithm and show that it can be used to cluster data in a variety of settings.\nMore recently, deep learning techniques, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), have been applied to anomaly detection in computational workflows. For example, [13] proposed a general methodology for mining console logs to detect large-scale system problems. The authors first parse the logs by combining source code analysis with information retrieval to create composite features. They then analyze these features using machine learning to detect operational problems. The authors evaluate their methodology on a dataset of console logs from a large-scale production system and show that it can be used to effectively detect a variety of system problems, including performance problems, software bugs, and malicious activity.\nSeveral works also take advantage of LLMs for anomaly detection in system logs. LogBERT [14] is a self-supervised anomaly detection framework based on BERT that learns the patterns of normal log sequences by two novel self-supervised training tasks and is able to detect anomalies where the underlying patterns deviate from normal log sequences. Similarly, BERT-log [15] also trains a BERT model but with labeled data to detect anomalies in logs. UniLog [16] and LTanomaly [17] are both Transformer-based anomaly detection methods for system logs, but UniLog is a pre-trained model, while LTanomaly is a Transformer variant that is specifically designed for syslog anomaly detection. While impressive results have been demonstrated, these approaches are not easily extensible and applicable to other workflows beyond the ones used in these papers. This is because these papers introduce their own unique tokenization which does not generalize to different logging systems with different vocabularies. This limits the usage of these approaches once a new logging system is deployed. In contrast, our approach leverages pre-trained models, and therefore, is easy to generalize to different kinds of logs and different workflows, which is demonstrated in our results."}, {"title": "III. LLMS FOR ANOMALY DETECTION", "content": "In this section, we will describe the supervised fine-tuning and in-context learning in details, and their advantages in anomaly detection tasks. An overview of our approach is provided in Figure 1."}, {"title": "A. Supervised Fine-Tuning", "content": "Supervised fine-tuning (SFT) is used to adapt pre-trained language models to new tasks or domains. The process involves feeding a labeled dataset of the target task or domain to the pre-trained model and adjusting the model's parameters while minimizing the loss on the new task. By using labeled data from the target task, the model can learn to recognize patterns and features that are specific to the new task, while still leveraging the knowledge it has gained from the large amounts of data it was pre-trained on.\nFollowing this, we detect the anomalies in computational workflows by fine-tuning the pre-trained models on the labeled dataset of the target task, i.e., sentence classification. Our approach involves treating the logs generated by the computational workflows as a sequence of sentences and applying the fine-tuned model to classify each sentence as normal or anomalous. Toward this end, we use a combination of pre-trained models and evaluate their performance on Flow-Bench dataset. A template that parses a system log entry into a sentence with labels is provided in Figure 2.\nInstead of training LLMs from scratch as done in [15], [18], there are several advantages of using the SFT approach:\n\u2022 Reduced training time and resources: SFT allows us to leverage the knowledge gained by the pre-trained model, reducing the amount of training time and resources required to achieve good performance on the target task. This can save a significant amount of time and computational resources.\n\u2022 Improved performance: SFT has been shown to improve the performance of pre-trained models on a wide range of NLP tasks, including text classification, sentiment analysis, and question answering. By adapting pre-trained models to the target task, we can achieve better performance than training a model from scratch.\n\u2022 Easy domain adaptation: SFT allows us to adapt pre-trained models to new domains, enabling them to learn domain-specific features and patterns. This can be useful for tasks like anomaly detection, where the target domain may be different from the domain the model was pre-trained on.\n\u2022 Better Generalization: SFT can lead to better generalization to unseen data compared to training a model from scratch since the pre-trained model has already learned to recognize many features that are useful for the target task.\n\u2022 Smaller dataset requirements: SFT can be more effective with smaller datasets than training a model from scratch since the pre-trained model has already learned to recognize many features that are useful for the target task. This can be particularly useful for tasks where labeled data is scarce or difficult to obtain, e.g., anomalies in computational workflows."}, {"title": "B. In-Context Learning", "content": "In-context learning (ICL) explores the LLMs' ability to enable few-shot learning and improve the generalization capabilities of the model. In contrast to the SFT, ICL does not train the model explicitly, instead, it applies prompts (input context) to guide the LLMs applying on downstream tasks. To highlight the ICL approach, we highlight several advantages of using ICL as follows:\n\u2022 Improved generalization: ICL enables models to learn from the context provided in the input data, which can improve their generalization capabilities. This can be especially useful for anomaly detection in system logs, where the data can be highly variable and complex.\n\u2022 Reduced need for labeled data: ICL can enable models to learn from unlabeled data, reducing the need for expensive and time-consuming labeling efforts. This can be particularly beneficial in the context of anomaly detection, where labeling data can be difficult and resource-intensive.\n\u2022 Improved interpretability: ICL also provides insights into the features and patterns that are important for detecting anomalies, making it easier to interpret the model's predictions and identify false positives or negatives. Especially, Chain-of-Thought (CoT) [9] is a method that can be used to generate prompts that guide the model to generate the desired output by providing explainable results.\nUnder the ICL paradigms, there are different types of prompts that can be used to guide the LLMs' learning, including zero-shot prompts, one-shot prompts, and few-shot prompts. Zero-shot prompts provide the model with a natural language description of the task, without any examples. In this case, the model must rely solely on its prior knowledge and the context provided to explore the ability of LLMs. One-shot prompts and few-shot prompts provide the model with either a single example or a few examples of the task, respectively. Generally, the examples provided involve the label of cases, particularly in the anomaly detection task, the example could be either the normal, anomalous or even mixed examples together. This is useful for tasks where labeled data is scarce or difficult to obtain, as it allows the model to learn from a small amount of data. Figure 3 provides the template of the prompt for ICL. It contains two parts in general, the task description, which guides the LLMs to understand the task, and the examples, which provide the context for the task. In our case, we explicitly ask the model to output the category of job described, without any reasoning or explanation. The contextual example, in this case, is the sentences that describe the job with its features extracted from the raw log file, and explicitly note the label of the job.\nBesides, another key advantage of ICL is that it can be fine-tuned based on domain-specific datasets as well, enabling it to adapt to new domains and tasks. Similar to SFT, fine-tuning on ICL also applies the labeled data from the target domain, capturing the specific features and patterns that are relevant to the task."}, {"title": "C. Pre-trained Models", "content": "Pre-trained models, such as BERT [19], GPT [20], and ALBERT [21] leverage the Transformer architecture [22] to ascertain statistical patterns and linguistic structures in the data. These models, trained on the large corpus of freely available text data have become the backbone of many state-of-the-art NLP systems, empowering researchers and practitioners to achieve remarkable performance with reduced training time and resources. These models have accelerated progress in NLP and continue to drive advancements in various language understanding and generation tasks.\nFor text classification tasks, where the goal is to assign a category or label to a given text input, encoder-only models are commonly employed. These models, such as BERT [19] (Bidirectional Encoder Representations from Transformers) and ROBERTa [23] (Robustly Optimized BERT Approach), process the input text in its entirety and generate contextualized representations, which can then be used for classification. Typically, the SFT for classification tasks involves adding a classification head on top of the pre-trained model and fine-tuning the model on a labeled dataset.\nOn the other hand, for causal language modeling tasks, where the objective is to predict the next token in a sequence given the preceding context, decoder-only models are well-suited. These models, such as GPT [20] (Generative Pre-trained Transformer) and its variants, generate text in an autoregressive manner, making them suitable for tasks like text generation, machine translation, and summarization. Unlike the SFT, which predicts the label of the given sentences, ICL outputs more context-aware results, which involve the generation of words and sentences based on the context provided.\nTo the scope of our anomaly detection task, we will select a set of encoder-only models for SFT tasks, and decoder-only models for ICL tasks."}, {"title": "IV. EXPERIMENTS", "content": "Our experiments are conducted on a single NVIDIA A100 GPU with 40GB memory. We implemented in PyTorch [24] and Huggingface's Transformers library [25] for our experiments. The detailed configurations of each individual model and optimizer are presented in the Artifact Appendix."}, {"title": "A. Dataset and Data Processing", "content": "To conduct our experiments we adopted the workflow data from Flow-Bench [26], a collection of three computational workflows for anomaly detection. The contributors of this dataset manually created a set of anomaly templates that represent different types of anomalies that could occur in the workflow data, such as missing data, incorrect data, or unexpected patterns (e.g., performance degradation). They then adopted these templates to create instances of anomalies in the data by injecting them into real workflow executions, at various points. The benchmark design also took steps to ensure that the anomalies were realistic and representative of real-world scenarios. For example, they ensured that the anomalies were not too frequent or too rare and that they were distributed across the data in a way that was consistent with real-world patterns. Flow-Bench contains 1211 execution traces of three computational workflows, that we briefly describe here.\n\u2022 The 1000 Genome Workflow identifies mutational overlaps using data from the 1000 Genomes Project [27] in order to provide a null distribution for rigorous statistical evaluation of potential disease-related mutations across populations. The instance of the workflow DAG available in the dataset, has a total of 137 jobs nodes and 289 edges.\n\u2022 The Montage Workflow uses the Montage astronomical image toolkit [28] to transform astronomical images, captured by the Digitized Sky Survey (DSS) [29], into custom mosaics for further analysis of the deep sky. The instance of the workflow DAG available in the dataset, has a total of 539 nodes and 2838 edges.\n\u2022 The Predict Future Sales Workflow uses real historical sales data in order to train machine learning models that accurately predict the sales of the following month. The instance of the workflow DAG available in the dataset, has a total of 165 nodes and 581 edges.\nAdditionally, in Flow-Bench, alongside the normal - baseline data, the authors have included two main anomaly classes that model performance degradation, CPU and HDD, with multiple subclasses based on the magnitude of the slow downs. In the CPU case, the authors instructed their workers to advertise a fixed number of cores, but then use affinity and cgroups to limit the actual cores that could do processing. In the case of HDD, they limited the average read and write speed of their workers. For a more detailed description of the workflows and the data available in Flow-Bench we would like to redirect the reader to [26].\nTo process the logs, we first convert them into tabular format, where each row represents a log entry and each column represents a field in the log, including timestamps, job status, time duration, I/O operations, etc. In order to omit the variance of timestamps, we select time durations of states of a job, along with I/O and CPU operations as the features for anomaly detection. Figure 2 provides a template of the parsed log into sentences.\nHaving generated the parsed sentences for each job, we split the entire dataset into train, validation, and test sets with the ratio of 8:1:1. Table I shows the statistics of the dataset, including number of normal and anomalous nodes (jobs), and the percentage of anomalies in each split."}, {"title": "B. SFT Models", "content": "We begin our results by validating how much we can gain from supervised fine-tuning by comparing the performance of pre-trained models and SFT models on the test set. Figure 4 shows the accuracy of pre-trained models and SFT models on the test set of 1000 Genome dataset. Notably, SFT models outperform pre-trained models in general, with a significant margin of improvement in several models. In addition to the LLMs, we include conventional machine learning models, MLP and GNN, as baselines for comparison. Following the setup of models in the work [30], our results demonstrate that the SFT models achieve comparable performance to these classical machine learning models. However, the SFT models offer the advantage of being more versatile and accessible for the anomaly detection task, even without requiring extensive machine learning expertise.\nWe also try to identify the relationship between model size and its performance. Figure 5 shows the training time of 1000 Genome dataset and the number of parameters for the SFT models. The training time increases with the number of parameters, which is expected. However, the performance of SFT models does not necessarily increase with the number of parameters. For example, two distilbert models have a good performance in accuracy, while a larger model, xlnet-base-cased takes a longer time to train with a larger amount of parameters, resulting in worse performance when compared with distilbert. It is not necessary to conclude that a larger model is not helpful; the performance highly depends on the model being fine-tuned and the size of data used for training. Insufficient training data may result in underfitting in the xlnet model due to a limited ability to learn better embeddings for the task. Lastly, we also evaluate the performance of SFT models concerning the training epochs. As training LLMs requires a significant investment of time and resources, we aim to determine the potential benefits of increasing the number of epochs. Figure 6 shows that accuracy, F1, precision, and recall scores on the validation set improve with just a few epochs of training. However, additional epochs leads to overfitting, resulting in worse performance. It is worth pointing out that the training time per epoch on 1000 Genome data is about 260 seconds on average. Therefore, in practice, a few epochs of supervised fine-tuning are sufficient to transfer the model to the target task."}, {"title": "C. Online Detection", "content": "Online detection of anomalies in computational workflows is a critical task that can help identify potential security threats or system failures in real time. SFT models have emerged as a powerful tool for this task, leveraging the knowledge gained from large amounts of labeled data to adapt to new tasks and domains. With the automatically parsed text sentence, we can apply the SFT models to predict the label of the system logs in real time.\nFigure 7 depicts an illustration of real-time anomaly detection. The figure shows the timestamp at T1 the computational workflows indicate that the wms_delay is 6.0, which the SFT model identifies as a normal occurrence. However, as new log data becomes available, e.g., at T4, the SFT model is able to detect anomalies in real time, allowing for the prompt identification and mitigation of potential issues.\nMeanwhile, we also evaluate the early detection of SFT models by checking the first time the model predicts a correct label of the job. Figure 8 shows the statistics of early detection in the test set of 1000 Genome dataset, where the x-axis is the feature processed from the log in sequential order and the y-axis is the number of samples that are first identified successfully. Recall the feature of the job, involving the timestamp of each stage, we can identify the stage of the job when the anomaly is detected. For example, the anomaly is detected at the first stage of the job, which is the wms_delay stage. The figure shows that the SFT models can detect anomalies at the early stage of the job, which could significantly help mitigate the potential issues of the job."}, {"title": "D. Debiasing LLMs", "content": "In the context of anomaly detection, the LLMs may output biased labels for normal and anomalous inputs, which can lead to incorrect or unfair results. One source of bias in SFT is the pre-trained LLM itself. LLMs are trained on massive datasets of text and code, which may contain biases that are reflected in the model's output. For example, an LLM trained on a dataset of news articles may be biased towards certain political viewpoints.\nAnother source of bias in SFT is the dataset utilized for fine-tuning. If the dataset is not representative of the population that the LLM will be used on, this can lead to biases in the model's output. For example, if the dataset for a text classification task only contains examples written by white males, the model may be biased against other groups of the population. Ideally, given the empty sentence, which means without any pre-knowledge of the job, the model should predict normal and abnormal jobs with almost equal probability.\nIn Figure 9(a), we present the prediction of an empty string [\"\"] from the pre-trained models with 10 independent runs. The figure shows that for a couple of models, the prediction is biased either towards normal or anomalous. To address this, we artificially increase the size of training data by inserting both labels into the empty input sentence, preserving its prediction to be fair without any pre-knowledge of the job. The model is forced to learn more robust features and reduce its reliance on any single feature or pattern extracted from the job, thus helping to mitigate the impact of biases in the data and improve the model's performance on unseen data. Figure 9(b) shows the prediction of the same empty string from the debiased models by augmented training data. Clearly, the gap between normal and anomalous prediction is reduced, which indicates the model is less biased towards normal or anomalous."}, {"title": "E. Transfer Learning", "content": "Furthermore, SFT has been increasingly applied in the context of transfer learning, which is a technique that allows models to leverage knowledge learned from one task to improve performance on another related task. In transfer learning, a pre-trained model is fine-tuned on a new dataset, and SFT is used to adapt the model to the new task's specific characteristics. By using SFT, the model can learn to recognize new features and patterns that are relevant to the new task while still leveraging the knowledge learned from the pre-training task. This approach has been shown to be effective in various NLP tasks such as language translation, question answering, and text classification. For instance, a pre-trained language model can be fine-tuned on a new language pair using SFT to improve its translation accuracy. SFT has also been applied in computer vision tasks such as image classification, object detection, and segmentation, where a pre-trained model is fine-tuned on a new dataset to improve its performance on the new task.\nTo demonstrate the effectiveness of SFT in transfer learning, we first present the performance of the transferred model without fine-tuning the new dataset. Figure 10 shows the accuracy scores of models that were trained on one dataset and evaluated on another dataset. The y-axis of the graph shows the dataset that the model was trained on, and the x-axis shows the accuracy score on the dataset that the model was evaluated on. For example, the model trained on the sales prediction dataset but evaluated on the 1000 Genome dataset still achieves an accuracy of 0.7523, meaning that the underlying hidden features learned from the Sales Prediction dataset can be generalized to the 1000 Genome dataset. However, it is not always the case in the opposite direction. A model learned from 1000 Genome does not perform well on Montage and Sales Prediction workflows. Therefore, a fine-tuning step is required to adapt the model to the new task, meaning that given a small set of labeled data from the target task, we can fine-tune the model to improve its performance on the target task. Figure 11 shows the accuracy scores of an SFT model trained on the 1000 Genome dataset and with accumulated training data from Montage, the evaluated accuracy on Montage workflow is improved from below 0.7 to above 0.8. Notably, having more available data in the target domain may not always lead to better performance, as the model may overfit the target domain and lose its ability to generalize to other domains.\nThe transfer learning enables the SFT to be more effective and fine-tuned on new datasets, which can iteratively increase the generalization of updated parameters for LLMs and improve the performance of anomaly detection."}, {"title": "F. Overcoming Catastrophic Forgetting", "content": "CF is a problem because it can lead to the model performing poorly on the previous tasks. To overcome this issue, we can freeze the parameters of the model that were learned during pre-training, and only update the parameters that are specific to the new task. It prevents the model from making significant changes to its parameters. When the model is trained on a new task, it needs to make changes to its parameters in order to learn the new task. However, if the model is making too many changes to its parameters, then it may forget the previous tasks. By freezing a large portion of the model parameters, we are preventing the model from making significant changes to its parameters. This helps to reduce the risk of the model forgetting the previous tasks. This also allows the model to retain the knowledge it has gained from pre-training while still adapting to the new task and has shown to be effective in the domain of computer vision [31].\nTo show the effectiveness of freezing the parameters, we compare the performance of SFT models with and without freezing the parameters. Table II shows the performance of SFT models on 1000 Genome dataset (denoted as D1 in the table) with different training strategies. Columns of SFT (D1) (All) and SFT (D1 + D2) (All) indicate the supervised fine-tuning of the entire model based on 1000 Genome dataset and fine-tuned transfer learning based on the Montage dataset (denoted as D2) again, respectively. The last column SFT (D1 + D2) (Linear) indicates the training by freezing the pre-trained parameters and only updating the last linear layer for prediction on both 1000 Genome and Montage datasets. The results show that without freezing the parameters, the model gets worse once it is fine-tuned on a new dataset. However, by freezing the parameters, the model can retain the knowledge it has gained from what learned on D1 while still adapting to the D2. Meanwhile, the precision score is even higher than the model purely trained on D1. This is mainly due to the new dataset D2 that has a different distribution of normal and anomalous jobs, which can help the model to learn more robust features and reduce its reliance on any single feature or pattern extracted from the jobs. Another advantage of freezing the parameters is that it can significantly reduce the training time, as the model only needs to update a small portion of its parameters, which is indicated in the last row of the table."}, {"title": "G. ICL results", "content": "In this section, we present the results of the in-context learning (ICL) approach. As ICL is essentially a text generation task, we explore the performance on a set of different decoder-only models, including GPT2 [32], Mistral-7B-v0.1 [33], and LLama2-7B [34].\nFor the model training, as LLMs are large and computationally expensive to train, we use the pre-trained models and apply the quantization and LoRA techniques to reduce the memory footprint and improve the inference speed.\na) Quantization.: To save memory and reduce the inference time, we apply the quantization technique [35] to the fine-tuned model. Quantization is a model compression technique that reduces the memory footprint and improves the inference speed of deep learning models by converting the model's weights from floating-point to fixed-point numbers. More specifically, we apply the BitAndBytes [36] with enabled 4bit quantization to replace the linear layers and enabled float16 computational type for the tensors which might be different than the input time.\nb) LoRA.: To further reduce the memory footprint and improve the inference speed, we apply the Low-Rank Adaptation (LoRA) [37] technique to the fine-tuned model. LoRA is a parameter-efficient fine-tuning (PEFT) technique that has gained significant importance in the field of LLMs. Models like Mistral-7b and LLama-7b typically have billions of parameters, making them computationally expensive to fine-tune and deploy in resource-constrained environments. LoRA addresses this challenge by introducing a small number of task-specific rank decompositions to the model's weight matrices. Instead of updating all the parameters during fine-tuning, LORA only modifies a small subset of parameters, reducing the memory footprint and computational requirements. This approach allows for efficient adaptation of LLMs to specific tasks or domains while maintaining the model's general language understanding capabilities. Without further clarification, we set the rank of the LoRA to 64, the scaling factor to 128, and the LORA layer dropout to 0.05 for all models. Table III shows the accuracy of the ICL models on the 1000 Genome dataset. We compare the accuracy scores of pre-trained models with and without fine-tuning in few-shot settings. To demonstrate the efficiency of quantization and LoRA, we also provide the number of trainable parameters and their percentage of the total parameters in the model as well. First, LoRA significantly reduces the number of parameters in training, getting less than 2% of its total parameters. Second, we also present few-shot with different types of examples for ICL. This is crucial for the anomaly detection problem because getting the ground truth label in the real world is expensive and time-consuming. We present three different settings, with negative-only samples (normal jobs), positive-only samples (anomalous jobs), and mixed samples (both normal and anomalous jobs). We set the number of examples to be 5 for each setting, and the results show that given a mix of both positive and negative samples, the LLMs can achieve better accuracy. Moreover, comparing the positive-only and negative-only cases, the examples of positive samples contribute more to the model's prediction, meaning that the model leads to more observable anomalies.\nFurthermore, we also provide the number of examples in the prompts for ICL with the pre-trained model in Figure 12, where we differentiate models by line styles and few-shot learning by colors. Note that when the number of examples equals 0, it's zero-shot learning in general without accessing the contextual information. The figure shows that the number of examples in the prompts is increasing with the model size, which is expected. Furthermore, considering the efficiency of LLMs, smaller models like GPT2 are more applicable under ICL as they only require a few examples to achieve a similar performance compared with large models like Mistral-7B and LLama2-7B."}, {"title": "H. Interpretability by CoT", "content": "Instead of providing the simple category of a job from ICL", "step-by-step\". It clearly prompts the model's output to be more explainable and interpretable, which can be used to validate the model's decision-making process. In this case, the model```json\n{\n  ": "itle", "Large Language Models for Anomaly Detection in Computational Workflows: from Supervised Fine-Tuning to In-Context Learning": "authors\": [\n    \"Hongwei Jin", "George Papadimitriou": "Krishnan Raghavan", "Pawel Zuk": "Prasanna Balaprakash", "Cong Wang": "Anirban Mandal", "Ewa Deelman": "abstract\": \"Anomaly detection in computational workflows is critical for ensuring system reliability and security. However, traditional rule-based methods struggle to detect novel anomalies. This paper leverages large language models (LLMs) for workflow anomaly detection by exploiting their ability to learn complex data patterns. Two approaches are investigated: 1) supervised fine-tuning (SFT), where pre-trained LLMs are fine-tuned on labeled data for sentence classification to identify anomalies, and 2) in-context learning (ICL) where prompts containing task descriptions and examples guide LLMs in few-shot anomaly detection without fine-tuning. The paper evaluates the performance, efficiency, generalization of SFT models, and explores zero-shot and few-shot ICL prompts and interpretability enhancement via chain-of-thought prompting. Experiments across multiple workflow datasets demonstrate the promising potential of LLMs for effective anomaly detection in complex executions.", "sections": [{"title": "I. INTRODUCTION", "content": "With the increasing complexity and scale of modern systems, computational workflows are growing in complexity while their reliability, security, and performance are becoming rather important. A critical factor in ensuring workflow execution reliability is the ability to detect anomalies. These anomalies can be indicators of various system issues, and they are manifested by unexpected behavior in hardware, such as high usage of computing resources, memory consumption, and I/O operations. To address the problem of anomaly detection in modern systems, methods that rely on rule-based systems, statistical analysis, and machine learning techniques [1]\u2013[4] have become quite popular in recent years.\nDespite their effectiveness, a considerable amount of data preprocessing must be done to perform this detection because typical methods are limited to analyzing images or numerical values. Furthermore, to facilitate this data preprocessing, a lot of expert knowledge is needed to be put into carefully collecting and correlating low-level system statistics with workflow execution metadata that can be used to convert the raw logs into other formats. Adding to the complexity is the need for substantial ML expertise to navigate the wide array of available anomaly detection methodologies effectively. The field of ML presents a vast spectrum of models and techniques, each with its customization and application nuances. This diversity, while beneficial, also imposes a steep learning curve and necessitates a deep understanding of ML principles to tailor these models to specific anomaly detection tasks. Furthermore, the process of setting up and training these models\u2014integrating them into a system's workflow\u2014poses an additional challenge. This aspect of ML model deployment and maintenance may not align well with the skill set of system administrators, who are typically more versed in direct system maintenance rather than in the nuances of ML model training and tuning.\nLarge Language Models (LLMs) and their wide-spread democratization efforts have the potential to significantly transform anomaly detection in HPC systems by streamlining data preprocessing, enhancing pattern recognition, simplifying the deployment of machine learning models, enabling real-time monitoring, and fostering a supportive community ecosystem. By automating complex data processing tasks and offering advanced analytical capabilities, LLMs reduce the need for extensive expert knowledge, making sophisticated anomaly detection accessible to system administrators without deep technical backgrounds. Furthermore, their ability to process and analyze streaming data in real-time can ensure prompt detection and mitigation of potential system issues.\nA primary critique of LLMs concerns their energy/power consumption and model size, which are seen as barriers to their practical application in HPC data analysis. However, this perspective overlooks the significant advances in energy-efficient technologies and the optimization of LLMs for operation on a wide range of devices, from high-end servers to compact, low-power devices such as smartphones. These emerging technologies not only mitigate the energy and resource demands of running sophisticated LLMs but also expand their accessibility and usability across various platforms. Consequently, as these energy-efficient techniques continue to evolve and LLMs become increasingly optimized for smaller devices, the practicality of deploying LLMs for anomaly detection in HPC systems and beyond-becomes ever more feasible. This trajectory underscores the viability of LLMs as a transformative tool in anomaly detection, promising significant advancements in HPC system management and maintenance.\nWe develop an approach that leverages pre-trained Large Language Models (LLMs) to directly detect anomalies from log files generated during the execution of computational workflows. Specifically, we adapt these pre-trained models through Supervised Fine-Tuning (SFT) and Prompt Engineering via In-Context Learning (ICL). SFT employs a pre-trained model and trains on a smaller dataset of labeled examples for a specific task [5]. Unlike the training of LLMs in an unsupervised way, the SFT often consists of an input and a desired output. By updating the parameters of LLMs again through SFT, the model improves the performance for a downstream task. However, one common issue with LLMs is that they can perpetuate biases present in the data used to train them [6], especially when for the binary classification problem. Another common issue is catastrophic forgetting (CF) [7], which occurs in machine learning when a model forgets previously learned information as it learns new information. This is a common problem in supervised fine-tuned models, where the model is trained on a new task after it has already been trained on one or more previous tasks.\nIn-context learning (ICL), on the other hand, is an emerging paradigm where LLMs perform tasks by leveraging a few examples provided within the context of query [8] rather than relying on supervised fine-tuning with labeled data. ICL heavily relies on prompt engineering, providing examples and contextual cues that guide the LLMs in efficiently understanding and executing the desired task. A well-engineered prompt not only presents the LLM with relevant information but subtly instructs it on generating the appropriate output. It involves structuring the examples in a way that highlights patterns or relationships, using natural language templates that align with the task's goals, or including explicit instructions that direct the model's attention to critical aspects of the problem. This alignment can be performed specifically for the anomaly detection problem where the prompts contain information about the job features and brief statistics about the job execution facilitating anomaly detection in the workflow. Furthermore, prompts can also include the instruction for reasoning steps through Chain-of-Thought (CoT [9]), providing explainable output from LLMs.\nTo this end, we make the following contributions to the scope of this paper:\n1) Investigate the efficacy of LLMs for anomaly detection and evaluate the performance of supervised fine-tuning models and in-context learning in detecting anomalies in computational workflows.\n2) Address the biases, overcome the catastrophic forgetting, and explore the generalization through transfer learning and online detection.\n3) Explore the ability of ICL with zero-shot, few-shot learning, and study the interpretable output from ICL through Chain-of-Thought (CoT).\nWith the use of LLMs for anomaly detection in computational workflows, we seek to contribute to the development of effective and efficient methods for detecting anomalies in computational workflows."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Several approaches have been proposed in the literature for anomaly detection in computational workflows. These approaches can be broadly classified into rule-based systems, statistical analysis, and machine-learning techniques. Rule-based systems rely on predefined thresholds and patterns to detect anomalies. For example, [10] proposed a rule-based system that uses a set of heuristics to identify anomalies in Linux computational workflows.\nWhile rule-based systems are simple to implement, they are limited by their inability to adapt to changing patterns in behavior and are often brittle in the face of new anomalies. Statistical analysis techniques, that use statistical information such as mean, median, and standard deviation, have been used to detect anomalies in computational workflows as well. For example, [11] proposed an approach that uses statistical methods to identify anomalies in network traffic logs. However, statistical analysis techniques are sensitive to outliers and may not be effective in detecting anomalies that do not deviate significantly from the mean.\nMachine learning techniques, such as decision trees, random forests, and clustering, have also been applied to anomaly detection in computational workflows. For example, [12] presented a simple and effective algorithm for spectral clustering, a method for grouping data points based on their pairwise similarities. The algorithm utilizes the eigenvectors of the graph Laplacian to represent similarities between the data points. The authors also provide a theoretical analysis of the algorithm and show that it can be used to cluster data in a variety of settings.\nMore recently, deep learning techniques, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), have been applied to anomaly detection in computational workflows. For example, [13] proposed a general methodology for mining console logs to detect large-scale system problems. The authors first parse the logs by combining source code analysis with information retrieval to create composite features. They then analyze these features using machine learning to detect operational problems. The authors evaluate their methodology on a dataset of console logs from a large-scale production system and show that it can be used to effectively detect a variety of system problems, including performance problems, software bugs, and malicious activity.\nSeveral works also take advantage of LLMs for anomaly detection in system logs. LogBERT [14] is a self-supervised anomaly detection framework based on BERT that learns the patterns of normal log sequences by two novel self-supervised training tasks and is able to detect anomalies where the underlying patterns deviate from normal log sequences. Similarly, BERT-log [15] also trains a BERT model but with labeled data to detect anomalies in logs. UniLog [16] and LTanomaly [17] are both Transformer-based anomaly detection methods for system logs, but UniLog is a pre-trained model, while LTanomaly is a Transformer variant that is specifically designed for syslog anomaly detection. While impressive results have been demonstrated, these approaches are not easily extensible and applicable to other workflows beyond the ones used in these papers. This is because these papers introduce their own unique tokenization which does not generalize to different logging systems with different vocabularies. This limits the usage of these approaches once a new logging system is deployed. In contrast, our approach leverages pre-trained models, and therefore, is easy to generalize to different kinds of logs and different workflows, which is demonstrated in our results."}, {"title": "III. LLMS FOR ANOMALY DETECTION", "content": "In this section, we will describe the supervised fine-tuning and in-context learning in details, and their advantages in anomaly detection tasks. An overview of our approach is provided in Figure 1."}, {"title": "A. Supervised Fine-Tuning", "content": "Supervised fine-tuning (SFT) is used to adapt pre-trained language models to new tasks or domains. The process involves feeding a labeled dataset of the target task or domain to the pre-trained model and adjusting the model's parameters while minimizing the loss on the new task. By using labeled data from the target task, the model can learn to recognize patterns and features that are specific to the new task, while still leveraging the knowledge it has gained from the large amounts of data it was pre-trained on.\nFollowing this, we detect the anomalies in computational workflows by fine-tuning the pre-trained models on the labeled dataset of the target task, i.e., sentence classification. Our approach involves treating the logs generated by the computational workflows as a sequence of sentences and applying the fine-tuned model to classify each sentence as normal or anomalous. Toward this end, we use a combination of pre-trained models and evaluate their performance on Flow-Bench dataset. A template that parses a system log entry into a sentence with labels is provided in Figure 2.\nInstead of training LLMs from scratch as done in [15], [18], there are several advantages of using the SFT approach:\n\u2022 Reduced training time and resources: SFT allows us to leverage the knowledge gained by the pre-trained model, reducing the amount of training time and resources required to achieve good performance on the target task. This can save a significant amount of time and computational resources.\n\u2022 Improved performance: SFT has been shown to improve the performance of pre-trained models on a wide range of NLP tasks, including text classification, sentiment analysis, and question answering. By adapting pre-trained models to the target task, we can achieve better performance than training a model from scratch.\n\u2022 Easy domain adaptation: SFT allows us to adapt pre-trained models to new domains, enabling them to learn domain-specific features and patterns. This can be useful for tasks like anomaly detection, where the target domain may be different from the domain the model was pre-trained on.\n\u2022 Better Generalization: SFT can lead to better generalization to unseen data compared to training a model from scratch since the pre-trained model has already learned to recognize many features that are useful for the target task.\n\u2022 Smaller dataset requirements: SFT can be more effective with smaller datasets than training a model from scratch since the pre-trained model has already learned to recognize many features that are useful for the target task. This can be particularly useful for tasks where labeled data is scarce or difficult to obtain, e.g., anomalies in computational workflows."}, {"title": "B. In-Context Learning", "content": "In-context learning (ICL) explores the LLMs' ability to enable few-shot learning and improve the generalization capabilities of the model. In contrast to the SFT, ICL does not train the model explicitly, instead, it applies prompts (input context) to guide the LLMs applying on downstream tasks. To highlight the ICL approach, we highlight several advantages of using ICL as follows:\n\u2022 Improved generalization: ICL enables models to learn from the context provided in the input data, which can improve their generalization capabilities. This can be especially useful for anomaly detection in system logs, where the data can be highly variable and complex.\n\u2022 Reduced need for labeled data: ICL can enable models to learn from unlabeled data, reducing the need for expensive and time-consuming labeling efforts. This can be particularly beneficial in the context of anomaly detection, where labeling data can be difficult and resource-intensive.\n\u2022 Improved interpretability: ICL also provides insights into the features and patterns that are important for detecting anomalies, making it easier to interpret the model's predictions and identify false positives or negatives. Especially, Chain-of-Thought (CoT) [9] is a method that can be used to generate prompts that guide the model to generate the desired output by providing explainable results.\nUnder the ICL paradigms, there are different types of prompts that can be used to guide the LLMs' learning, including zero-shot prompts, one-shot prompts, and few-shot prompts. Zero-shot prompts provide the model with a natural language description of the task, without any examples. In this case, the model must rely solely on its prior knowledge and the context provided to explore the ability of LLMs. One-shot prompts and few-shot prompts provide the model with either a single example or a few examples of the task, respectively. Generally, the examples provided involve the label of cases, particularly in the anomaly detection task, the example could be either the normal, anomalous or even mixed examples together. This is useful for tasks where labeled data is scarce or difficult to obtain, as it allows the model to learn from a small amount of data. Figure 3 provides the template of the prompt for ICL. It contains two parts in general, the task description, which guides the LLMs to understand the task, and the examples, which provide the context for the task. In our case, we explicitly ask the model to output the category of job described, without any reasoning or explanation. The contextual example, in this case, is the sentences that describe the job with its features extracted from the raw log file, and explicitly note the label of the job.\nBesides, another key advantage of ICL is that it can be fine-tuned based on domain-specific datasets as well, enabling it to adapt to new domains and tasks. Similar to SFT, fine-tuning on ICL also applies the labeled data from the target domain, capturing the specific features and patterns that are relevant to the task."}, {"title": "C. Pre-trained Models", "content": "Pre-trained models, such as BERT [19], GPT [20], and ALBERT [21] leverage the Transformer architecture [22] to ascertain statistical patterns and linguistic structures in the data. These models, trained on the large corpus of freely available text data have become the backbone of many state-of-the-art NLP systems, empowering researchers and practitioners to achieve remarkable performance with reduced training time and resources. These models have accelerated progress in NLP and continue to drive advancements in various language understanding and generation tasks.\nFor text classification tasks, where the goal is to assign a category or label to a given text input, encoder-only models are commonly employed. These models, such as BERT [19] (Bidirectional Encoder Representations from Transformers) and ROBERTa [23] (Robustly Optimized BERT Approach), process the input text in its entirety and generate contextualized representations, which can then be used for classification. Typically, the SFT for classification tasks involves adding a classification head on top of the pre-trained model and fine-tuning the model on a labeled dataset.\nOn the other hand, for causal language modeling tasks, where the objective is to predict the next token in a sequence given the preceding context, decoder-only models are well-suited. These models, such as GPT [20] (Generative Pre-trained Transformer) and its variants, generate text in an autoregressive manner, making them suitable for tasks like text generation, machine translation, and summarization. Unlike the SFT, which predicts the label of the given sentences, ICL outputs more context-aware results, which involve the generation of words and sentences based on the context provided.\nTo the scope of our anomaly detection task, we will select a set of encoder-only models for SFT tasks, and decoder-only models for ICL tasks."}, {"title": "IV. EXPERIMENTS", "content": "Our experiments are conducted on a single NVIDIA A100 GPU with 40GB memory. We implemented in PyTorch [24] and Huggingface's Transformers library [25] for our experiments. The detailed configurations of each individual model and optimizer are presented in the Artifact Appendix."}, {"title": "A. Dataset and Data Processing", "content": "To conduct our experiments we adopted the workflow data from Flow-Bench [26], a collection of three computational workflows for anomaly detection. The contributors of this dataset manually created a set of anomaly templates that represent different types of anomalies that could occur in the workflow data, such as missing data, incorrect data, or unexpected patterns (e.g., performance degradation). They then adopted these templates to create instances of anomalies in the data by injecting them into real workflow executions, at various points. The benchmark design also took steps to ensure that the anomalies were realistic and representative of real-world scenarios. For example, they ensured that the anomalies were not too frequent or too rare and that they were distributed across the data in a way that was consistent with real-world patterns. Flow-Bench contains 1211 execution traces of three computational workflows, that we briefly describe here.\n\u2022 The 1000 Genome Workflow identifies mutational overlaps using data from the 1000 Genomes Project [27] in order to provide a null distribution for rigorous statistical evaluation of potential disease-related mutations across populations. The instance of the workflow DAG available in the dataset, has a total of 137 jobs nodes and 289 edges.\n\u2022 The Montage Workflow uses the Montage astronomical image toolkit [28] to transform astronomical images, captured by the Digitized Sky Survey (DSS) [29], into custom mosaics for further analysis of the deep sky. The instance of the workflow DAG available in the dataset, has a total of 539 nodes and 2838 edges.\n\u2022 The Predict Future Sales Workflow uses real historical sales data in order to train machine learning models that accurately predict the sales of the following month. The instance of the workflow DAG available in the dataset, has a total of 165 nodes and 581 edges.\nAdditionally, in Flow-Bench, alongside the normal - baseline data, the authors have included two main anomaly classes that model performance degradation, CPU and HDD, with multiple subclasses based on the magnitude of the slow downs. In the CPU case, the authors instructed their workers to advertise a fixed number of cores, but then use affinity and cgroups to limit the actual cores that could do processing. In the case of HDD, they limited the average read and write speed of their workers. For a more detailed description of the workflows and the data available in Flow-Bench we would like to redirect the reader to [26].\nTo process the logs, we first convert them into tabular format, where each row represents a log entry and each column represents a field in the log, including timestamps, job status, time duration, I/O operations, etc. In order to omit the variance of timestamps, we select time durations of states of a job, along with I/O and CPU operations as the features for anomaly detection. Figure 2 provides a template of the parsed log into sentences.\nHaving generated the parsed sentences for each job, we split the entire dataset into train, validation, and test sets with the ratio of 8:1:1. Table I shows the statistics of the dataset, including number of normal and anomalous nodes (jobs), and the percentage of anomalies in each split."}, {"title": "B. SFT Models", "content": "We begin our results by validating how much we can gain from supervised fine-tuning by comparing the performance of pre-trained models and SFT models on the test set. Figure 4 shows the accuracy of pre-trained models and SFT models on the test set of 1000 Genome dataset. Notably, SFT models outperform pre-trained models in general, with a significant margin of improvement in several models. In addition to the LLMs, we include conventional machine learning models, MLP and GNN, as baselines for comparison. Following the setup of models in the work [30], our results demonstrate that the SFT models achieve comparable performance to these classical machine learning models. However, the SFT models offer the advantage of being more versatile and accessible for the anomaly detection task, even without requiring extensive machine learning expertise.\nWe also try to identify the relationship between model size and its performance. Figure 5 shows the training time of 1000 Genome dataset and the number of parameters for the SFT models. The training time increases with the number of parameters, which is expected. However, the performance of SFT models does not necessarily increase with the number of parameters. For example, two distilbert models have a good performance in accuracy, while a larger model, xlnet-base-cased takes a longer time to train with a larger amount of parameters, resulting in worse performance when compared with distilbert. It is not necessary to conclude that a larger model is not helpful; the performance highly depends on the model being fine-tuned and the size of data used for training. Insufficient training data may result in underfitting in the xlnet model due to a limited ability to learn better embeddings for the task. Lastly, we also evaluate the performance of SFT models concerning the training epochs. As training LLMs requires a significant investment of time and resources, we aim to determine the potential benefits of increasing the number of epochs. Figure 6 shows that accuracy, F1, precision, and recall scores on the validation set improve with just a few epochs of training. However, additional epochs leads to overfitting, resulting in worse performance. It is worth pointing out that the training time per epoch on 1000 Genome data is about 260 seconds on average. Therefore, in practice, a few epochs of supervised fine-tuning are sufficient to transfer the model to the target task."}, {"title": "C. Online Detection", "content": "Online detection of anomalies in computational workflows is a critical task that can help identify potential security threats or system failures in real time. SFT models have emerged as a powerful tool for this task, leveraging the knowledge gained from large amounts of labeled data to adapt to new tasks and domains. With the automatically parsed text sentence, we can apply the SFT models to predict the label of the system logs in real time.\nFigure 7 depicts an illustration of real-time anomaly detection. The figure shows the timestamp at T1 the computational workflows indicate that the wms_delay is 6.0, which the SFT model identifies as a normal occurrence. However, as new log data becomes available, e.g., at T4, the SFT model is able to detect anomalies in real time, allowing for the prompt identification and mitigation of potential issues.\nMeanwhile, we also evaluate the early detection of SFT models by checking the first time the model predicts a correct label of the job. Figure 8 shows the statistics of early detection in the test set of 1000 Genome dataset, where the x-axis is the feature processed from the log in sequential order and the y-axis is the number of samples that are first identified successfully. Recall the feature of the job, involving the timestamp of each stage, we can identify the stage of the job when the anomaly is detected. For example, the anomaly is detected at the first stage of the job, which is the wms_delay stage. The figure shows that the SFT models can detect anomalies at the early stage of the job, which could significantly help mitigate the potential issues of the job."}, {"title": "D. Debiasing LLMs", "content": "In the context of anomaly detection, the LLMs may output biased labels for normal and anomalous inputs, which can lead to incorrect or unfair results. One source of bias in SFT is the pre-trained LLM itself. LLMs are trained on massive datasets of text and code, which may contain biases that are reflected in the model's output. For example, an LLM trained on a dataset of news articles may be biased towards certain political viewpoints.\nAnother source of bias in SFT is the dataset utilized for fine-tuning. If the dataset is not representative of the population that the LLM will be used on, this can lead to biases in the model's output. For example, if the dataset for a text classification task only contains examples written by white males, the model may be biased against other groups of the population. Ideally, given the empty sentence, which means without any pre-knowledge of the job, the model should predict normal and abnormal jobs with almost equal probability.\nIn Figure 9(a), we present the prediction of an empty string [\"\"] from the pre-trained models with 10 independent runs. The figure shows that for a couple of models, the prediction is biased either towards normal or anomalous. To address this, we artificially increase the size of training data by inserting both labels into the empty input sentence, preserving its prediction to be fair without any pre-knowledge of the job. The model is forced to learn more robust features and reduce its reliance on any single feature or pattern extracted from the job, thus helping to mitigate the impact of biases in the data and improve the model's performance on unseen data. Figure 9(b) shows the prediction of the same empty string from the debiased models by augmented training data. Clearly, the gap between normal and anomalous prediction is reduced, which indicates the model is less biased towards normal or anomalous."}, {"title": "E. Transfer Learning", "content": "Furthermore, SFT has been increasingly applied in the context of transfer learning, which is a technique that allows models to leverage knowledge learned from one task to improve performance on another related task. In transfer learning, a pre-trained model is fine-tuned on a new dataset, and SFT is used to adapt the model to the new task's specific characteristics. By using SFT, the model can learn to recognize new features and patterns that are relevant to the new task while still leveraging the knowledge learned from the pre-training task. This approach has been shown to be effective in various NLP tasks such as language translation, question answering, and text classification. For instance, a pre-trained language model can be fine-tuned on a new language pair using SFT to improve its translation accuracy. SFT has also been applied in computer vision tasks such as image classification, object detection, and segmentation, where a pre-trained model is fine-tuned on a new dataset to improve its performance on the new task.\nTo demonstrate the effectiveness of SFT in transfer learning, we first present the performance of the transferred model without fine-tuning the new dataset. Figure 10 shows the accuracy scores of models that were trained on one dataset and evaluated on another dataset. The y-axis of the graph shows the dataset that the model was trained on, and the x-axis shows the accuracy score on the dataset that the model was evaluated on. For example, the model trained on the sales prediction dataset but evaluated on the 1000 Genome dataset still achieves an accuracy of 0.7523, meaning that the underlying hidden features learned from the Sales Prediction dataset can be generalized to the 1000 Genome dataset. However, it is not always the case in the opposite direction. A model learned from 1000 Genome does not perform well on Montage and Sales Prediction workflows. Therefore, a fine-tuning step is required to adapt the model to the new task, meaning that given a small set of labeled data from the target task, we can fine-tune the model to improve its performance on the target task. Figure 11 shows the accuracy scores of an SFT model trained on the 1000 Genome dataset and with accumulated training data from Montage, the evaluated accuracy on Montage workflow is improved from below 0.7 to above 0.8. Notably, having more available data in the target domain may not always lead to better performance, as the model may overfit the target domain and lose its ability to generalize to other domains.\nThe transfer learning enables the SFT to be more effective and fine-tuned on new datasets, which can iteratively increase the generalization of updated parameters for LLMs and improve the performance of anomaly detection."}, {"title": "F. Overcoming Catastrophic Forgetting", "content": "CF is a problem because it can lead to the model performing poorly on the previous tasks. To overcome this issue, we can freeze the parameters of the model that were learned during pre-training, and only update the parameters that are specific to the new task. It prevents the model from making significant changes to its parameters. When the model is trained on a new task, it needs to make changes to its parameters in order to learn the new task. However, if the model is making too many changes to its parameters, then it may forget the previous tasks. By freezing a large portion of the model parameters, we are preventing the model from making significant changes to its parameters. This helps to reduce the risk of the model forgetting the previous tasks. This also allows the model to retain the knowledge it has gained from pre-training while still adapting to the new task and has shown to be effective in the domain of computer vision [31].\nTo show the effectiveness of freezing the parameters, we compare the performance of SFT models with and without freezing the parameters. Table II shows the performance of SFT models on 1000 Genome dataset (denoted as D1 in the table) with different training strategies. Columns of SFT (D1) (All) and SFT (D1 + D2) (All) indicate the supervised fine-tuning of the entire model based on 1000 Genome dataset and fine-tuned transfer learning based on the Montage dataset (denoted as D2) again, respectively. The last column SFT (D1 + D2) (Linear) indicates the training by freezing the pre-trained parameters and only updating the last linear layer for prediction on both 1000 Genome and Montage datasets. The results show that without freezing the parameters, the model gets worse once it is fine-tuned on a new dataset. However, by freezing the parameters, the model can retain the knowledge it has gained from what learned on D1 while still adapting to the D2. Meanwhile, the precision score is even higher than the model purely trained on D1. This is mainly due to the new dataset D2 that has a different distribution of normal and anomalous jobs, which can help the model to learn more robust features and reduce its reliance on any single feature or pattern extracted from the jobs. Another advantage of freezing the parameters is that it can significantly reduce the training time, as the model only needs to update a small portion of its parameters, which is indicated in the last row of the table."}, {"title": "G. ICL results", "content": "In this section, we present the results of the in-context learning (ICL) approach. As ICL is essentially a text generation task, we explore the performance on a set of different decoder-only models, including GPT2 [32], Mistral-7B-v0.1 [33], and LLama2-7B [34].\nFor the model training, as LLMs are large and computationally expensive to train, we use the pre-trained models and apply the quantization and LoRA techniques to reduce the memory footprint and improve the inference speed.\na) Quantization.: To save memory and reduce the inference time, we apply the quantization technique [35] to the fine-tuned model. Quantization is a model compression technique that reduces the memory footprint and improves the inference speed of deep learning models by converting the model's weights from floating-point to fixed-point numbers. More specifically, we apply the BitAndBytes [36] with enabled 4bit quantization to replace the linear layers and enabled float16 computational type for the tensors which might be different than the input time.\nb) LoRA.: To further reduce the memory footprint and improve the inference speed, we apply the Low-Rank Adaptation (LoRA) [37] technique to the fine-tuned model. LoRA is a parameter-efficient fine-tuning (PEFT) technique that has gained significant importance in the field of LLMs. Models like Mistral-7b and LLama-7b typically have billions of parameters, making them computationally expensive to fine-tune and deploy in resource-constrained environments. LoRA addresses this challenge by introducing a small number of task-specific rank decompositions to the model's weight matrices. Instead of updating all the parameters during fine-tuning, LORA only modifies a small subset of parameters, reducing the memory footprint and computational requirements. This approach allows for efficient adaptation of LLMs to specific tasks or domains while maintaining the model's general language understanding capabilities. Without further clarification, we set the rank of the LoRA to 64, the scaling factor to 128, and the LORA layer dropout to 0.05 for all models. Table III shows the accuracy of the ICL models on the 1000 Genome dataset. We compare the accuracy scores of pre-trained models with and without fine-tuning in few-shot settings. To demonstrate the efficiency of quantization and LoRA, we also provide the number of trainable parameters and their percentage of the total parameters in the model as well. First, LoRA significantly reduces the number of parameters in training, getting less than 2% of its total parameters. Second, we also present few-shot with different types of examples for ICL. This is crucial for the anomaly detection problem because getting the ground truth label in the real world is expensive and time-consuming. We present three different settings, with negative-only samples (normal jobs), positive-only samples (anomalous jobs), and mixed samples (both normal and anomalous jobs). We set the number of examples to be 5 for each setting, and the results show that given a mix of both positive and negative samples, the LLMs can achieve better accuracy. Moreover, comparing the positive-only and negative-only cases, the examples of positive samples contribute more to the model's prediction, meaning that the model leads to more observable anomalies.\nFurthermore, we also provide the number of examples in the prompts for ICL with the pre-trained model in Figure 12, where we differentiate models by line styles and few-shot learning by colors. Note that when the number of examples equals 0, it's zero-shot learning in general without accessing the contextual information. The figure shows that the number of examples in the prompts is increasing with the model size, which is expected. Furthermore, considering the efficiency of LLMs, smaller models like GPT2 are more applicable under ICL as they only require a few examples to achieve a similar performance compared with large models like Mistral-7B and LLama2-7B."}, {"title": "H. Interpretability by CoT", "content": "Instead of providing the simple category of a job from ICL, we also explore the interpretability of the model by generating the chain-of-thought (CoT) [9"}]}]}