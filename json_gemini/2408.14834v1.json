{"title": "Strategic Optimization and Challenges of Large Language Models in Object-Oriented Programming", "authors": ["Zinan Wang"], "abstract": "In the area of code generation research[21], the emphasis has transitioned from crafting individual functions to developing class-level method code that integrates contextual information. This shift has brought several benchmarks such as ClassEval [5] and CoderEval [28], which consider class-level contexts. Nevertheless, the influence of specific contextual factors at the method level remains less explored. These factors include the method's own description, its interactions with other modules, and the broader project context. The varied impacts of these factors on code generation outcomes-such as pass rates, error type distributions, and developer support-could also have economic implications, notably in terms of the token consumption required for API calls. Furthermore, the extent of a method's external module interactions, or \"coupling\", significantly influences code generation, yet quantitative analyses of this impact are sparse.\nOur research focused on method-level code generation within the Object-Oriented Programming (OOP) framework. We devised experiments that varied the extent of contextual information in the prompts, ranging from method-specific to project-wide details. We introduced the innovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic viability of incorporating additional contextual layers. Our findings indicate that prompts enriched with method invocation details yield the highest cost-effectiveness. Additionally, our study revealed disparities among Large Language Models (LLMs) regarding error type distributions and the level of assistance they provide to developers. Notably, larger LLMs do not invariably perform better. We also observed that tasks with higher degrees of coupling present more substantial challenges, suggesting that the choice of LLM should be tailored to the task's coupling degree. For example, GPT-4 exhibited improved performance in low-coupling scenarios, whereas GPT-3.5 seemed better suited for tasks with high coupling. By meticulously curating prompt content and selecting the appropriate LLM, developers can optimize code quality while maximizing cost-efficiency during the development process.", "sections": [{"title": "INTRODUCTION", "content": "In the past, automated code generation primarily relied on simple templates or rule engines[29]. While effective in certain scenarios, these methods often lacked flexibility and the ability to adapt to complex requirements[3]. With the rapid advancement of Natural Language Processing(NLP)[23], LLMs, particularly OpenAI's GPT series, have made significant strides in the field of intelligent code generation [17]. These models not only comprehend and generate human language but have also extended their capabilities to understand complex programming languages and autonomously generate code. From automatically completing code snippets to generating entire functional code structures, the application of LLMs is driving advancements in software development.\nAlthough LLMs have made progress in code generation, existing research has primarily focused on simple isolated code tasks, such as generating solutions for algorithmic problems, with little consideration for tasks with complex contextual information in practical scenarios. These studies and benchmarks, such as HumanEval, often concentrate on evaluating the model's ability to generate single functions or small code blocks. There is a lack of consideration for scenarios involving complex contexts in actual development environments. Regarding existing OOP code generation test frameworks, like ClassEval, while they consider class-level context such as basic class structures and frameworks, they do not deeply analyze the impact of varying degrees of contextual information on the generated results. These contextual details can range from specific descriptions of method functionalities to encompassing other modules invoked by them, to broader project integration information descriptions.\nBesides, considering that tasks with higher coupling are more complex, this increases the difficulty of generation. Additionally, the degree of coupling itself is a characteristic of the target method. To determine which LLM is more suitable for specific tasks, it is essential to study the degree of coupling. Frameworks like CoderEval have begun to focus on code independence (i.e., different levels of independence ranging from \"completely standalone\" to \"requiring other modules in the project to run\"). However, there is still a lack of systematic quantitative analysis on the coupling degree of the target code. This involves factors such as the number of APIs, variables, and classes called by the target method, and how this coupling affects the quality of the generated code.\nTo address these research gaps, this experiment was designed to control the prompt richness using prompt levels (i.e., from Base-Level containing only the name and signature of the target method to Project-Level containing information related to the entire project)."}, {"title": "BACKGROUND", "content": "Current research exhibits a notable deficiency in attention and investigation into method-level code generation for OOP. In this paper, \"method-level code generation\" is defined as the process of generating code for a specific method of a given object within an OOP project, utilizing Large Language Models (LLMs) based on contextual information and precise method descriptions. This study selects the CoderEval [28] dataset as the cornerstone of its experiments, aggregating 460 self-contained executable code generation tasks from genuine open-source projects, encompassing both Python and Java languages. The rationale behind choosing this dataset is to offer an evaluation platform that closely resembles a real development environment, aiming to more accurately capture and exhibit the performance and challenges of automated code generation tools in practical application scenarios.\nSpecifically, we focus on the Java subset of the CoderEval dataset, comprising 230 tasks, which we have significantly refined and applied to ensure that our experimental design and data usage reflect the real-world demands of Java development as comprehensively as possible, particularly in terms of code quality assessment and the practical value of generation tasks. To augment the practicality and efficiency of the experiments, we have meticulously dissected and reassembled the CoderEval dataset. We have segmented and adjusted the parts of the code description, achieving a precise gradation of code hints. This step has established a solid foundation for subsequent data processing and analysis.\nAs depicted in the Figure 1, we present a code generation task at the highest level of hint precision. The definitions of precision levels and hint content will be elaborated on in the following sections. Generally, \"input\" signifies the information provided, which is transformed into hints for the generation task; \"code\" denotes the target code to be generated; \"output\" illustrates an example of the generated code. For each task, the experiment conducts ten iterations of code generation to ensure the stability and reliability of the outcomes.\nThe term \"input\" encompasses exhaustive information from the fundamental definition of methods to the comprehensive structure of the entire project. It includes the method's signature and name, which provide the model with basic invocation and identification mechanisms. A succinct description (human_label) and an elaborate docstring offer detailed explanations that aid the model in understanding the method's purpose and operational specifics. The oracle_context supplies additional contextual information, such as utilized APIs and relevant variables, enhancing the model's adaptability to the code environment. The class_level and repo_level broaden the scope to detailed structures at the class and project levels, showcasing the internal makeup of classes and the extensive logical relationships within the project. Lastly, the level section indicates the code snippet's self-containment, denoting its independence within the overall project. This amalgamation of information ensures the generated code's precision and its high coordination with the project's overarching architecture and functional requirements."}, {"title": "RELATED WORKS", "content": "With the emergence of LLMs, significant achievements have been made in the field of NLP [14, 15, 19, 20, 25], and they are gradually being applied to various aspects of software engineering, especially code generation. These models, through pre-training and fine-tuning, can learn and generate logically coherent code snippets. For instance, Baptiste Rozi\u00e8re et al. propose Code Llama [18], and Mark Chen et al. propose CodeX [1], which have achieved certain results by fine-tuning for code generation tasks. They demonstrate advantages in tasks such as automated code completion, bug fixing, and direct code generation from descriptions.\nBuilding upon these, researchers are exploring various ways to further leverage the potential of the LLMs themselves [9, 11, 13, 22, 26, 27, 30]. This includes proposing more advanced prompting engineering methods, such as Chain-of-Thought [26], as proposed by Jason Wei et al., which significantly enhance the accuracy and"}, {"title": "STUDY DESIGN", "content": "The experiments were conducted based on the data provided by CoderEval, which is a benchmark for OOP code generation, containing 460 code generation tasks. We selected 230 Java generation tasks from the set for testing. The prompt data used for generation in the experiments was integrated from the information provided by CoderEval. It consolidated all the descriptive information about the target methods available in this project. This facilitated detailed descriptions of the methods and enabled the composition of prompt content for subsequent experimental design."}, {"title": "Research FrameWork", "content": "Figure 2 illustrates the complete process of this experiment, ranging from data preprocessing to code generation and subsequent evaluation. We can use each prompt level for each target method as an example to explain the overall code generation process: First, we extract the corresponding Prompt Level from the Dataset, forming the Prompt Data. Utilizing this data, we invoke the OpenAI API for code generation, obtaining the Generated Codes. Subsequently, all Generated codes undergo unit testing within the corresponding projects in the CoderEval Docker. Finally, we obtain the Result Data, which indicates whether each generated code snippet passed the predefined test cases."}, {"title": "Experiment Settings", "content": "We have selected GPT-3.5 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) as the basic models for our experiments due to their widespread application and superior performance among all LLMs. These models offer exceptional language understanding and generation capabilities, making them highly suitable for complex code generation tasks. Additionally, their extensive use and robust community support ensure the reliability and reproducibility of our experimental results.\nTo ensure the comparability of experiments and the consistency of results, all models were configured with uniform parameters. We set the temperature to 0, while keeping the remaining variables at their default configurations. This approach ensures both the equality of experimental conditions and facilitates the subsequent evaluation of each model's baseline performance without specific optimizations."}, {"title": "Evaluation Metrics", "content": "The Pass@K metric is a crucial indicator for assessing the quality of code generation. It evaluates whether the code generated across multiple attempts passes all test cases at least once. This metric not only prioritizes the correctness of the code but also focuses on the consistency and reliability of the generation system. Moreover, it assesses the capability of code generation tools to successfully generate correct code over multiple attempts. The mathematical principle behind this metric is as follows:\n$pass@k := Eproblems 1 (\\substack{n-c\\\\ nk})$\nWhere:\n\u2022 n is the total number of samples generated per task.\n\u2022 c is the count of correct samples that pass the unit tests.\n\u2022 k is the parameter of interest in pass@k, representing the number of top attempts considered.\nFrom an economic perspective, this study innovatively introduces the concept of \"Prompt-Token Cost-Effectiveness\" (PTCE) to analyze the cost-effectiveness of prompt token information. We first calculated the average token consumption for each prompt level. Then, based on the specific Pass@K and target K value for each prompt level, we could calculate PTCE(K). It is defined as:\n$PTCE(K) = \\frac{APass@K}{AToken Cost}$\nWhere:\n$APass@K = Pass@K_{curr} - Pass@K_{prev}$\n$AToken Cost = Token Cost_{curr} - Token Cost_{prev}$\nIn software development practice, many metrics do not have an absolute correct solution, particularly those involving subjective evaluations, such as the degree of help to developers. Faced with this complexity, we can draw on the judgments of experienced software development practitioners and experts. Their professional knowledge and practical experience provide us with valuable insights, aiding in more precise classification and assessment of different situations.\nIn this experiment, to ensure the accuracy of manual evaluations, we hired 10 senior software engineers to participate in the review process. It is important to note that these engineers are not co-authors of this paper. Each engineer has at least six years of experience in object-oriented development, and their professional backgrounds and extensive knowledge provided solid professional support for this study. These practitioners have a wide range of practical experience in the field of software development, ensuring the practicality and reliability of our research results.\nTo ensure the comprehensiveness and fairness of the evaluations, we employed a cross-review method. Specifically, each generated result was assessed by three independent experts. By comparing the evaluations from all experts, we ensured the consistency of the review outcomes and properly addressed any disagreements that arose. Particularly, for code that received entirely different assessments from three experts, it was reviewed by another three experts in a second round of reviews. If the results of the second review still showed inconsistencies, the most experienced expert served as a mediator, making the final decision based on the existing scores and feedback. This process ensured that our evaluation results were well-substantiated and persuasive."}, {"title": "Research Questions", "content": "We have devised a series of experiments to investigate the performance of Large Language Models (LLMs) in various programming tasks, considering multiple key factors: the precision of prompts and its impact on code generation quality, the effect of code task coupling on LLM performance, and the potential of LLMs to alleviate workload in Object-Oriented Programming (OOP) development. Specifically, our research questions are as follows:\n(1) RQ1: How does prompt information impact code generation performance?\nThrough evaluating the generated code's Pass@K, error type distributions, and cost-effectiveness, we aim to optimize prompt strategies to enhance the practicality of generated code and uncover performance differences between models in the code generation process. This endeavor seeks to deepen our understanding of how prompt richness impacts the introduction of programming errors.\n(2) RQ2: How do prompts assist developers?\nWe defined different levels of helpfulness value and employed human expert voting to evaluate and analyze the distributions.\n(3) RQ3: How does coupling impact generated code?\nUsing the concept of Fan-Out, we quantified coupling and further defined three coupling levels. We analyzed the Pass@K for the generated code at each level, thereby specifically examining the influence of the target code's coupling degree on the generation results."}, {"title": "STUDY RESULT", "content": "In this section, we will introduce the main contents of the experiment and discuss the conclusions drawn from these contents. Our study focuses on three research questions (RQs). These RQs include the impact of prompt information richness on code generation, which includes the influence on code generation's Pass@K and the distribution of error types. Additionally, we explore the practical value of code generated by different LLMs and prompt levels in real-world development scenarios. Finally, we discuss the impact of the inherent coupling (Fan-Out) of the target methods on the generated code. These research questions will help us gain a deeper understanding of the potential and challenges of LLMs in code generation."}, {"title": "RQ1: How does Prompt Richness Impact Generated Code?", "content": "This study focuses on the impact of prompt information richness on code generation, including Pass@K for generated code and the types of errors encountered. The experiments cover multiple levels of prompt richness, aiming to comprehensively capture how the richness of prompt affects both the pass rate and the occurrence of errors in the generated code."}, {"title": "RQ1.1: How does Prompt Richness Impact Pass@K?", "content": "In order to facilitate the measurement of prompt richness, we established a prompt information level table. This encompassed contextual information ranging from the target method to the project level. We divided this information into seven levels, with each level's name, content, and the definition of the changes relative to the previous level, as illustrated in Figure 1.\nFigures 3 and 4 illustrate the changes in code generation Pass@K for GPT-3.5 and GPT-4 across different prompt levels. From the data, it can be clearly observed that as the prompt information increases, the overall Pass@K for generated code exhibits an upward trend. Taking GPT-3.5's Pass@5 as an example, it is 42.4% at L1, increasing to 55.92% at L4, and reaching 59.19% at L6. This demonstrates a significant positive correlation between the richness of prompts and the performance of generated code.\nFurther analysis of the figures reveals that the L4 prompt level brought about a significant performance boost. Taking Pass@10 as an example, relative to L3, GPT-3.5 and GPT-4 achieved improvements of 10% and 8.7% respectively at L4. Additionally, by comparing the L1 and L2-1 curves across the two models, we found that merely increasing the human-provided humanlabel did not"}, {"title": "RQ1.2: Cost-Effectiveness Analysis of Prompt Richness.", "content": "In practical production scenarios, we also need to consider the cost of utilizing LLMs. This cost is often evaluated based on the number of tokens consumed. In the code generation domain, token consumption largely depends on the richness of the input prompt information. Therefore, this RQ focuses on analyzing the cost-effectiveness of the additional information at each prompt level, termed PTCE.\nTo establish a consistent baseline for calculation, we employ the tiktoken tool provided by OpenAI\u00b9 and utilize the cl100k_base model to compute the token consumption of GPT-3.5 and GPT-4.\nTable 2 presents the token consumption, increment relative to the previous level, and the prompt-token cost-effectiveness at each level of prompt information for K=3, 5, and 10. The data indicates"}, {"title": "RQ1.3: How does Prompt Richness Affect the Distribution of Error Types?", "content": "In this RQ, we analyzed the error types in the generated code from the perspective of different prompt richness. Specifically, the error types involved in the experiments and their definitions are as follows [12]:\n(1) Syntax Error: Refers to errors in the generated code that do not comply with the rules of the programming language, making it unable to be compiled or interpreted. These errors often involve basic coding format issues, such as mismatched parentheses or missing semicolons.\n(2) Factual Error: Refers to errors in the generated code based on incorrect logical assumptions or the use of incorrect programming syntax and features, resulting in discrepancies between the logic or functionality implemented and the expected outcome. For example, using non-existent functions or libraries.\n(3) Faithfulness Error: Refers to errors in the generated code that, while syntactically correct and potentially executable, do not meet the user's original requirements or intentions for problem-solving.\nBesides, Faithfulness Error is further categorized as:\n(1) Inadequate Understanding: The generated code fails to fully implement all expected functionalities, lacking crucial features or functionalities.\n(2) Misinterpretation: The generated code does not faithfully reflect the original requirements or intentions, altering the expected behavior.\n(3) Overdesign: The generated code introduces unnecessary complexity in attempting to meet requirements, exceeding the original scope of requirements.\nTo begin with, we use Tree-Sitter, a tool for detecting syntax questions, to annotate Syntax Error within the erroneous code. Subsequently, for the remaining code data, manual annotation is performed to differentiate between Factual Error and faithfulness errors. For the code annotated as having Faithfulness Error, further annotation is conducted to categorize them into Inadequate Understanding, Misinterpretation, and Overdesign. The specific distribution of error types is presented in Figure 5.\nThrough the analysis of Figure 5, we can clearly observe that as the amount of prompt information gradually increases, the total number of errors in both models shows a decreasing trend, with Misinterpretation errors exhibiting the largest reduction across all error categories. Correspondingly, Syntax Errors and Overdesign issues are on the rise. This indicates that when improving prompt precision, the probability of different error types also changes. Therefore, when utilizing LLMs for code generation, developers need to strike a balance between the quantity and quality of information."}, {"title": "RQ2: How do Prompts Help Developers?", "content": "In the process of applying LLMs for code generation in practical development scenarios, encountering erroneous code generation is a common occurrence. Considering that not all erroneous code is entirely valueless, each generated code segment has the potential to assist developers from the perspectives of structure, logic, and other aspects. This experiment aims to evaluate the helpfulness value of the generated code under different levels of prompt information through human evaluation.\nIn the experiment, the helpfulness-value of the generated code in assisting developers in writing the target code will be divided into five levels, ranging from 0 to 4, for human evaluation. The definitions of each level are as follows:\n(1) 0: The code segment is completely valueless and irrelevant to the problem.\n(2) 1: The code segment has some value, containing information relevant to the problem, but it is easier to write the solution from scratch.\n(3) 2: The code segment is somewhat helpful, requiring major changes, but still useful.\n(4) 3: The code segment is helpful but has some issues and requires minor modifications to solve the problem.\n(5) 4: The code segment is very helpful, highly relevant and informative, and can be directly used to solve the problem.\nThe annotation results are shown in Figure 6. Through observation, we can discern a positive correlation between code helpfulness value and prompt level. As the richness of prompt information increases, the quantity of code with higher helpfulness values gradually rises. Notably, the L4 level exhibits outstanding performance in code generation assistance, with the highest number of codes having a helpfulness value of 3 or above in the distributions of both models. Furthermore, although the overall pass rate of validation is below 50%, more than half of the codes have a helpfulness value of 2 or higher, indicating that even codes that fail testing possess a certain degree of reference value.\nFrom the perspective of model selection, GPT-4 demonstrates superior performance in generating codes with the highest helpfulness value. In contrast, GPT-3.5 is more prone to producing codes with a helpfulness value of 3 or above across all prompt information levels, suggesting that even at lower prompt levels (such as L1), GPT-3.5 can provide relatively high assistance in practical applications. These observations offer crucial references for developers when choosing models, assisting them in selecting the most suitable model configuration based on specific application requirements to achieve optimal code generation results."}, {"title": "RQ3: How does Coupling Impact Generated Code?", "content": "In software engineering, coupling is a metric that measures the degree of interdependence between code modules. To a certain extent, it reflects the complexity and difficulty of writing code. This study aims to investigate the impact of target code coupling on the quality of generated code by LLMs. The experiment is grounded in the concept of fan-out, wherein static code analysis of the target method is performed to extract the sum of the number of APIs, classes, and variables called by each method, serving as a quantitative indicator of coupling.\nThe distribution of the quantified measure of coupling is depicted in Figure 7. To enhance the controllability of the analysis, this study categorizes the coupling measure into three classes based on its distribution characteristics: low coupling, medium coupling, and high coupling:\n(1) Low Coupling: Fan-Out <= 3 (covering from the minimum value to near the 25th percentile)"}, {"title": "Medium Coupling", "content": "3 < Fan-Out <= 6 (covering a significant portion from the 25th to the 75th percentile)"}, {"title": "High Coupling", "content": "Fan-Out > 6 (exceeding from the 75th percentile to the maximum value)\nBased on this classification, the study will investigate the relationship between coupling and Pass@K.\nAs is shown in the Figure 8, it is evident that a higher coupling degree of the generation task does not necessarily correspond to a lower Pass@K. This necessitates a comprehensive analysis based on the specific LLM. Different LLMs exhibit varying suitability for different coupling degrees. For instance, GPT-4 performs better on low-coupling tasks but underperforms on high-coupling tasks. Conversely, GPT-3.5 demonstrates superior performance on tasks with medium coupling degrees. Moreover, we note that different models are impacted by coupling degrees to varying extents. Compared to GPT-4, GPT-3.5 is less affected by task coupling degrees, exhibiting more stable Pass@K in code generation. Consequently, developers should consider the actual task circumstances when selecting an LLM. If the task possesses a high coupling degree and requires high stability, GPT-3.5 may be a more suitable choice. However, if the tasks are primarily low-coupling, GPT-4.0 could be a more ideal option."}, {"title": "DISCUSSION", "content": "In this study, we systematically explore the efficiency and quality of method-level code generation in OOP using large language models such as GPT-3.5 and GPT-4. We discuss the internal and external threats revealed by the experimental content and results, and propose future research directions to address these limitations."}, {"title": "Threats", "content": "Internal Threats. First, the dataset used in the experiments has limitations in terms of scale and diversity. The experiments employed a relatively small dataset from the CoderEval benchmark, comprising only Java tasks, which may limit the generalizability of the results. However, the tasks were carefully chosen from high-quality projects, ensuring the reliability of the findings. Furthermore, the language utilized in the tests is representative of mainstream OOP practices. In future research, larger and more diverse datasets will be incorporated, spanning multiple OOP languages, thereby enhancing the universal applicability of the research outcomes.\nAdditionally, the design suffers from inadequacies in the quantification method for coupling degree. The assessment of coupling relied primarily on intuitive quantitative accumulation, a simple approach, yet sufficient for providing useful preliminary insights. Future plans include employing static code analysis tools for a more detailed quantitative analysis of coupling.\nExternal Threats. In the present experiments, the amount of LLMs tested is limited. The research was primarily confined to GPT-3.5 and GPT-4, limiting the diversity of models, but these have been widely validated as efficient and reliable. Future work will introduce additional models such as Llama3, Codex, PolyCoder, etc., to enhance the comprehensive assessment and comparison of different models' performances.\nFurthermore, there is a possibility of data leakage in the experiments. The code data source from CoderEval originates from public GitHub repositories, which may have already been exposed to OpenAI, posing an unavoidable risk. However, the experiment has minimized this impact as much as possible. Future research will employ stricter data validation measures to ensure the accuracy of data research. These insights not only strengthen the transparency and depth of the research but also provide clear directions for improvement in future studies, aiming to further advance automated programming technologies."}, {"title": "Suggestion", "content": "Based on the experimental results, we can offer some practical suggestions to help OOP developers improve efficiency in their actual workflow.\nWhen selecting prompt content, it is advisable to include information about other modules invoked by the target method. This not only helps enhance the quality of code generation but also achieves better cost-effectiveness.\nExcessive information may lead to more over-design errors. Developers should not simply assume that more prompt content is better. Instead, the prompt content should be designed within an appropriate scope. This not only avoids excessive token consumption but also prevents wasting developers' time.\nWhen choosing the LLM to use, the coupling degree of the target method needs to be considered. One cannot simply assume that"}, {"title": "CONCLUSION", "content": "This study investigates the influences of prompt richness on the quality of method-level code generation in OOP. Through extensive experiments, we determined that the richness of prompts significantly impacts code quality, aiding developers by maximizing code generation quality without substantial increases in token consumption. Specifically, an optimal level of prompt content, such as L4, offers the best cost-effectiveness. However, our findings also indicate that increasing prompt richness does not uniformly reduce errors; in fact, overly detailed prompts can lead to over-design issues. Furthermore, the usefulness of generated code, as evaluated by experts, suggests that not all incorrect codes are unhelpful. Additionally, LLMs do not always correspond to higher usefulness-value.\nOur research also highlights the importance of method coupling in code generation: GPT-4 excels in tasks with low coupling, while GPT-3.5 may be better suited for high-coupling scenarios. These insights enable developers to devise more effective prompt strategies and leverage even uncompilable code during development.\nIn future, this research will expand to involve more diverse LLMs, a broader range of programming languages, and an increased number of testing tasks to enhance the generalizability of our findings. The future work aims to refine automated code generation in OOP by investigating the effects of richer prompt information, potentially using techniques such as vector databases to extend model context. This approach seeks to fine-tune the automated OOP code generation process, thereby improving development efficiency and making significant strides toward more intelligent programming environments."}]}