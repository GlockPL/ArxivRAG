{"title": "Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis", "authors": ["Emanuele Ricco", "Lorenzo Cima", "Roberto Di Pietro"], "abstract": "Hallucinations are one of the major issues affecting LLMs, hindering their wide adoption in production systems. While current research solutions for detecting hallucinations are mainly based on heuristics, in this paper we introduce a mathematically sound methodology to reason about hallucination, and leverage it to build a tool to detect hallucinations. To the best of our knowledge, we are the first to show that hallucinated content has structural differences with respect to correct content. To prove this result, we resort to the Minkowski distances in the embedding space. Our findings demonstrate statistically significant differences in the embedding distance distributions, that are also scale free-they qualitatively hold regardless of the distance norm used and the number of keywords, questions, or responses. We leverage these structural differences to develop a tool to detect hallucinated responses, achieving an accuracy of 66% for a specific configuration of system parameters-comparable with the best results in the field. In conclusion, the suggested methodology is promising and novel, possibly paving the way for further research in the domain, also along the directions highlighted in our future work.", "sections": [{"title": "1. Introduction", "content": "Hallucinations in the context of artificial intelligence, particularly in large language models (LLMs), refer to generating content that appears plausible or convincing while it is factually incorrect, misleading, or entirely fabricated. These phenomena stem from the probabilistic and predictive mechanisms underlying LLMs. Hallucinations can be classified in intrinsic hallucinations, when the response has an opposite meaning with respect to the source material and extrinsic hallucinations, when the correctness of the response cannot be directly proved (Huang et al., 2023). Hallucinations can occur in various forms, including incorrect facts, nonsensical statements, or invented sentence, resulting from the model's reliance on statistical patterns to generate responses and could be due to many causes. One possible cause is the existence of misleading information in the training data that could lead to produce erroneous answers\u2014a modern version of the thrash-in-thrash-out paradigm (Lin et al., 2022b). An example of the danger of tainted data is the presence of sociocultural biases based on nationality stereotypes, that associate different entities not supported in the training data (Ladhak et al., 2023). The spreading of hallucinations in complex social networks could leverage public opinion and administrative processes from governments, possibly originating social disorders and distrust in AI-generated content (Hao et al., 2024), in particular if incorrect information is spread in a coordinated manner (Cima et al., 2024a). Current detection strategies can be grouped in factuality and faithfulness approaches (Huang et al., 2023). While the first one involves a semiautomated fact-checking approach that needs human participation, (Chern et al., 2023), the second one requires contextual information to analyze the truthness (Fabbri et al., 2021). However, there have been no efforts into investigating the structural distinctions between genuine and hallucinated responses in the context of hallucination detection. Our research addresses this gap by introducing a novel perspective: analyzing the embedding space to uncover patterns and discrepancies that differentiate authentic outputs from hallucinated ones. We aim to pave the way for a more robust and nuanced approach to detect hallucinations, offering a fresh direction for both theoretical and practical advancements in this field."}, {"title": "1.1. Contributions", "content": "Our contributions can be summarized as follows:\n\n\u2022 To the best of our knowledge, we are the first to propose a mathematically rigorous method, based on embedding distances, to evaluate the structural differences between genuine and hallucinated responses."}, {"title": "1.2. Roadmap", "content": "The remainder of this paper is organized as follows. Section 2 discusses previous work related to hallucination detection. Section 3 presents our proposed methodology for both training and test data, while Section 4 reports the results obtained from the model. Section 5 discusses implications and limitations of our proposal, together with some intuitions for future work. Finally, Section 6 concludes the paper."}, {"title": "2. Related Work", "content": "Hallucinations are defined as \u201ca phenomenon in which the generated content appears non-sensical or unfaithful to the provided source content\" (Filippova, 2020; Maynez et al., 2020). They affect all LLMs (Huang et al., 2023), due to the very architecture of LLMs. Common causes include vague prompts (Ren et al., 2022) and gaps in the training data (Perkovi\u0107 et al., 2024), leading for instance to inaccuracies or perpetuation of societal biases (Ladhak et al., 2023; Venkit et al., 2023; Giorgi et al., 2024). The occurrence of hallucinations can significantly undermine the credibility of responses and complicate mitigation and detection efforts (Guerreiro et al., 2022; Huang et al., 2025)."}, {"title": "2.1. LLMs Hallucination Mitigation", "content": "For what concerns mitigation, several approaches have been proposed. Some methods improve the prompt context by incorporating tags from reliable sources (Feldman et al., 2023) or using knowledge graphs (Martino et al., 2023), while others use neural networks to predict plausible subsequent tokens and avoid unrelated outputs (Verma et al., 2023). A different technique instead exploits self-evaluations, where models assess the validity of their own outputs (Kadavath et al., 2022; Ji et al., 2023), and external evaluations, where a powerful LLM is used to evaluate content generated by other models (Hao et al., 2024). In this way, the output is evaluated and corrected if necessary. Finally, LLMs can also use Retrieval Augmented Generation (RAG) to enhance response credibility\u2014that is, accessing external data source for enhancing their knowledge and possibly reducing errors (Fan et al., 2024; Arslan et al., 2024)."}, {"title": "2.2. LLMs Hallucination Detection", "content": "Detecting hallucinations in large language models (LLMs) remains a significant challenge, with various techniques proposed to address this issue. Among the simplest approaches, which also serve mitigation purposes, are self-evaluation methods where the model assesses its own outputs (Kadavath et al., 2022; Lin et al., 2022a; Manakul et al., 2023), or external evaluations performed by more advanced models, such as GPT-3.5 Turbo, to determine whether each output is realistic or fabricated (Friel & Sanyal, 2023). Other strategies involve token-level analysis, such as examining next-token probabilities (Varshney et al., 2023), evaluating uncertainty for sequence of tokens (Malinin & Gales, 2020), or specifying \u201cexact answer tokens\" that must appear in correct responses (Orgad et al., 2024). A recent innovation by Kuhn et al. (2023) introduced semantic entropy, an extension of the lexical similarity (Lin et al., 2022b) which quantifies the similarity of meaning between sentences, even when their structures differ significantly, making it highly useful for the detection of hallucinations (Farquhar et al., 2024). The works in Du et al. (2024); Chen et al. (2024) align most closely with our approach, as they also utilized embedding space. The former applied projections to address hallucinations, while the latter leveraged the eigenvalues of the response covariance matrix to compute the semantic consistency. However, no prior research has analyzed the structural differences between genuine and hallucinated responses. Our work builds on the intuition that the distances in embedding space for real responses differ significantly from those of hallucinated ones, opening a new avenue for hallucination detection."}, {"title": "3. Methodology", "content": "Our methodology develops over two different intuitions. First, LLM hallucinations are sampled from a different probability distribution than non-hallucinatory responses. Second, it is possible to detect hallucinations with a rigorous mathematical formalism."}, {"title": "3.1. Solution at a glance", "content": "To verify the intuitions, we devised a methodology that eventually confirmed both ideas. We first generated an artificial dataset using two different LLMs, Llama2 (llama2-7B\u00b9) and Llama3 (llama3-8B\u00b2).\nAs illustrated in Figure 1, Llama2 pretraining data has a cut-"}, {"title": "3.2. Dataset", "content": "= 64 questions, reported in the Appendix, related to specific facts between September 2022 and September 2023 (one year). We generated the 64 questions with GPT-4, and once they originated, we did not change them during the work. This choice ensures that the answers generated by Llama2 hallucinated, while those generated by Llama3 did not. For each question, we generated multiple response sets of varying sizes r \u2208 {4, 6, 8, 10, 12, 14, 16}, including both hallucinated and non-hallucinated responses. Importantly, these sets are cumulative: the set with r = 8 responses contains all responses from the r = 4 set plus additional ones. This nested structure applies to all subsequent r values. Similarly, we assigned different numbers of test responses (t) to each r value for test, maintaining an approximate 80% - 20%split between training and test sets. The (r, t) pairs were: (4, 1), (6, 1), (8, 2), (10, 2), (12, 3), (14, 3), (16, 4), following the same cumulative pattern as the training sets.\nWe built the dataset with a 4-bit quantization to reduce memory usage in both models (Dettmers et al., 2024). We also applied nucleus sampling, considering tokens whose cumulative probability sums to 95% and top-k sampling, ranking at each generation only 50 tokens the ones with the highest probability (Holtzman et al., 2020). After generating the dataset, we extracted the most valuable information from all training and test responses. First, we removed all punctuation marks, stop words, special symbols, and parentheses from the responses in a pre-processing phase. Then, we extracted the n most important keywords for each response using KeyBERT (Grootendorst, 2020) with n \u2208 {1, ...10}. Each set of n keywords extracted from a response is transformed into a 768-D vector representing the semantic content through the BERT embedding (Kenton & Toutanova, 2019). In the end, each response is mapped into 10 different embeddings.\nWe generated q"}, {"title": "3.3. Training", "content": "First, we created training keyword datasets, unique for each combination of responses (r) and keywords (n). So for each r value, we generated 10 distinct datasets, each containing qr responses from Llama2 and Llama3 models. We then calculated the Minkowski distances reported in Table 1 between each pair of embeddings within the same model for all datasets (Li et al., 2011). To the best of our knowledge, this is the first work to propose an extension of Euclidean distance between LLM-generated response embeddings. We computed each distance for different norm p \u2208 {0.5, 1, 2}, including fractional distances to deal with the sparsity of high-dimensional data (Aggarwal et al., 2001). We summarized the steps of this procedure in Figure 2.\nWe started from the assumption that if the same language"}, {"title": "3.4. Test", "content": "followed the same steps as training. Initially, we processed test responses to extract keywords, and then we created test keyword datasets for each combination of r and n, generating embeddings for both hallucinated and non-hallucinated responses. Once we obtained the embeddings, we computed distances between each test embedding and all training embeddings. For each test point, we calculated two sets of distances: qr distances $d_{i,hall}$ for i = 1, . . ., qr to hallucinated training points and qr distances $d_{j,nohall}$ for j = 1,..., qr to non-hallucinated training points. This process was repeated for each number of keywords (n \u2208 {1, . . 10}), each Minkowski distance (p \u2208 {0.5, 1.0, 2.0}), and each response count (r \u2208 {4, 6, 8, 10, 12, 14, 16}).\nAfter computing all distances, for each combination of n, p, and r, we referred to the $({qr \\choose 2})$ distances within each class fitting probability densities using Gaussian Kernel Density Estimation (KDE), separately for hallucinated and non-hallucinated data (Kim & Scott, 2012). Then, we used the previously calculated 2qr distances (qr to each class) for each test point and computed log-likelihoods under each KDE model. $L_{hall}^i$ for i = 1,..., qr, log-likelihood under hallucinated distribution, $L_{nohall}^j$ for j = 1, . . ., qr log-likelihood under non-hallucinated distribution. We can define an aggregated score for each test embedding as follows:\n\n$S_{hall} = \\sum_{i=1}^{q_r} L_{hall}^i$\n$S_{nohall} = \\sum_{j=1}^{q_r} L_{nohall}^j$\n\nThen, for each test embedding, we have compared the score of the two distributions and assigned the relevant class, not hallucinated (0) or hallucinated (1):\n\n$Class = \\begin{cases} 1, & \\text{if } S_{hall} > S_{nohall}, \\\\ 0, & \\text{otherwise}. \\end{cases}$\n\nWe applied the results obtained in the training phase to detect hallucinations in new test responses, comparing results across all combinations of (r,n,p). The test procedure, synthesized in Figure 3,"}, {"title": "4. Results", "content": "In this section we report the experimental results. First, we discuss the training results and then we delve into the test results."}, {"title": "4.1. Training", "content": "The box plots in Figure 4 illustrate an example of the distributions of intra-Euclidean distances between hallucinated and non-hallucinated response embeddings as a function of n, fixing r = 16 and for the three values of p analyzed. At first, we can observe that the difference between the two distributions remains almost the same Vp. Secondly, as shown in Figure 4, the two distributions increase their differences when n amplifies\u2014this is particularly evident for the third quartiles. Then, the Wilcoxon test performed on each boxplot shows that all the couples for different distance probability distributions have a p-value < 0.01; hence, there is a strong evidence for rejecting the null hypothesis, concluding that the two distributions are statistically significantly different. Also, this result is scale-free, remaining valid for each of the other 18 box plots obtained \u2200(r, p), as shown in the Appendix. At the same time, the median difference remains almost unchanged, with a similar behavior for nearly all boxplots in the Appendix. It is also essential to check that some distances are equal to 0. This phenomenon is caused by the fact that different responses could have extracted the same exact keywords, leading to the same embedding and hence a zero distance.\nWe can combine the information from Figure 4 with the KL Divergence and median difference (\u25b3) data in Table 2. The difference between the two distributions increases from 0.068 for n = 1 to 0.138 for n = 10 when r = 16 and p = 2.0. This effect is amplified for lower p values: when p = 0.5, the KL divergence increases by 279% at r = 16. It is also significant that this percentage increases with r, from 108% for r = 8, p = 1.0 to 256% for r = 16, p = 1.0. To summarize, decreasing p and increasing r leads to a rise in the KL Divergence.\nThese observations relative to training data, particularly to both the median and the KL Divergence showed that the two distributions increase their difference as a function of n in the third quartile, as we can see from the boxplots. The increase in Q3 indicates that the upper 25% of the hallucinated distances are generally more sparse, and the evidence shows that this event happens Vr. This interesting result provides strong statistical evidence that the two distributions (hallucinated and non-hallucinated distances) are significantly different. Remarkably, also these results relative to extreme values of hallucinated distances are scale-free. That is, we have the same qualitative behavior for each value of r and p, as shown in the box plots in the Appendix.\nLooking at Figure 5, both KL Divergence and median Difference show similar patterns across n and r. Both measures show a peak value at n = 1, drop to their minimum at n = 2, and then steadily increase up to n = 10. This pattern remains consistent across different p values for both r and n parameters, as shown in the Appendix. The KL Divergence specifically shows an increasing trend for all r values (except r = 4), with r = 14 and r = 16 showing the highest values as we can see from Figure 5.\nWe observe the same phenomenon for median difference, with an increasing trend as a function of the number of keywords and with the line for r = 16 that dominates all the others. This evidence confirms our initial hypothesis that there is a remarkable statistically significant difference between the two distributions. We have also pointed out that increasing the number of responses in the two distributions enlarges their difference\u2014this phenomenon calling"}, {"title": "4.2. Test", "content": "For the test phase, we evaluated two key metrics across all combinations for both r and n parameters fixing p = 2.0, as shown in Figure 6: the overall accuracy and the F1 score for hallucinated responses. Similar trends are observed for p = 0.5 and p = 1.0, as shown in the Appendix. The overall accuracy measures the model's general performance, while the F1 score specifically evaluates its ability to detect hallucinations in test data. For accuracy, we observe the best result with n = 1 reaching 0.61 with r = 4. Varying r, from r = 6 tor = 16, we observe an increasing trend across almost all n values, except for n = 1, where the trend is decreasing. The F1 Score shows different behavior: it performs best with n = 1, starting at 0.56 and gradually decreasing to 0.48 as r increases. For other values of n, the F1 Score shows lower performance and, unlike accuracy, does not improve with increasing r. We can then observe that an higher keyword count does not improve the F1 score, suggesting that simpler models (i.e. employing fewer keywords) might be more effective.\nAs shown in Table 3, the model achieves its highest accuracy (0.66) with parameters r = 8 and n = 1 at p = 0.5. The model performs consistently well with n = 1, and p = 0.5, reaching an accuracy of 0.63 with r = 16.\nFractional p values cause more significant accuracy differences, amplifying small variations in distance metric, as also investigated in (Aggarwal et al., 2001). Table 3 shows that when holding r constant, the variation in accuracy across different n values is most evident at p = 0.5 (0.16), and decreases significantly at p = 1.0 (0.06) and p = 2.0 (0.05). Manhattan distance (p = 1) and particularly Euclidean distance (p = 2) moderate the impact of small and large differences between the embeddings, leading to more concentrated values."}, {"title": "5. Discussion", "content": "In this section, we discuss the contributions reported in this paper, the limitations of our methodology, and future work."}, {"title": "5.1. Contributions", "content": "In this paper, we first introduced a novel tool based on the distance between embeddings to analyze the differences between hallucinated and non-hallucinated responses. This is a crucial contribution because we measured real semantic differences between sentences, with potential applications outside the hallucination detection field. It is worth noticing that the adoption of the Minkowski norm, for different values of such norm, has shown an amplifier effect on the divergences between the underlying distribution of hallucinated and non-hallucinated responses. Our methodology, involving identifying periods in which the models are not trained and repeating the same questions many times, is easily scalable on many different models. We have also confirmed our initial intuition that responses generated from one model are tendentially closer to each other than responses generated between various models.\nThen, we have also shown evidence that the differences between the two cited distributions is scale-free, that is, it is preserved independently from the number of responses generated by the two models, the number of keywords extracted from the responses, or the adopted norm of Minkowski distance. As seen in section 4.1, this consistency across different parameters shows that the distinction between hallucinated and not hallucinated responses does not depend on specific test conditions but is idiosyncratic to the model itself.\nThe first two above contributions led us to introduce an algorithm to detect hallucination responses. Unlike other classification methods used in the literature, our proposed tool is tailored for this goal, evaluating probabilistically if"}, {"title": "5.2. Limitations & Future Work", "content": "The dataset used for our methodology was entirely generated through two models, Llama2 and Llama3, chosen for their similarity in terms of training parameters and different cutoff training date. We started from the assumption that Llama2, when requested to provide a response on a topic exceeding the date of its last training hallucinates while Llama3 (whose training date exceeds the date the subject of the query refers to) does not. Nevertheless, a little subset of Llama3 responses might be hallucinated as well, while we have classified them as genuine, leading to possible incorrect results (Cima et al., 2024b). A possible improvement would be to use a dataset already existing in the literature designed to train classifiers on hallucinations like HalluRAG (Ridder & Schilling, 2024) or HADES (Liu et al., 2021). Another limitation of our methodology regards the brute force computational complexity of the distance calculation in the training dataset, which is O(qr\u00b2), with q equal to the number of questions and r equal to the total number of responses. Future work can be done on reducing this complexity, minimizing the loss of information (Skala, 2013). Another significant limitation is related to the algorithm's results, which are based on KDE to estimate the probability density of embeddings. Further research could be done on detecting and classifying hallucinations with generative models (Zimmermann et al., 2021). Finally, it is worth noting that the employed models have a limited size (7B and 8B parameters for Llama2 and Llama3, respectively). It is in our agenda to investigate whether the achieved results vary with the size of the model parameters. Our expectations are that the quality of our detection framework would improve since a larger set of parameters implies that the inter-classes distances do lessen, leading to an overall better intra-class classification."}, {"title": "6. Conclusions", "content": "In this paper we have provided a novel methodology to detect hallucinations produced by LLMs. To the best of our knowledge, we are the first to demonstrate the existence of structural variations between hallucinated and not hallucinated responses, introducing a novel framework based on the distances between embeddings. The results are validated via a rigorous mathematical formalism employing statistical tests. Leveraging the developed framework, we proposed a probabilistic approach to detect whether a response is hallucinated. The provided tool reached good results in the general accuracy of the model and hallucination detection\u2014 it is worth noting that the provided tool is a PoC for the suggested methodology, with relevant margins for improvement.\nWe believe that the brand new methodology and the promising results in discriminating between hallucinationed and not hallucinationed responses provided by LLMs, might open up new research avenues to mitigate the issue."}, {"title": "A. Appendix", "content": "These are the 64 questions applied to llama2 and llama3:."}, {"title": "A.1. Questions", "content": "1. What major geopolitical events occurred between September 2022 and September 2023?\n2. Were there any significant changes in global trade agreements between September 2022 and September 2023?\n3. How did the global economy perform between September 2022 and September 2023?\n4. What were the key outcomes of the G7/G20 summits held between September 2022 and September 2023?\n5. Which countries experienced significant political elections or leadership changes from September 2022 to September 2023?\n6. Were there any notable developments in international relations between September 2022 and September 2023?\n7. What was the impact of sanctions or economic policies introduced between September 2022 and September 2023?\n8. How did the BRICS expansion plans evolve during the period between September 2022 and September 2023?\n9. Which major AI models were released between September 2022 and September 2023?\n10. What advancements in quantum computing occurred between September 2022 and September 2023?\n11. How did generative AI applications evolve between September 2022 and September 2023?\n12. Were there any major breakthroughs in space exploration between September 2022 and September 2023?\n13. What new technologies were introduced in consumer electronics between September 2022 and September 2023?\n14. How did AI integration into industries like healthcare or education progress between September 2022 and September 2023?\n15. Were there any significant data breaches or cybersecurity developments between September 2022 and September 2023?\n16. What were the major discoveries in science between September 2022 and September 2023?\n17. Did any significant climate-related events occur between September 2022 and September 2023?\n18. What progress was made in renewable energy adoption between September 2022 and September 2023?\n19. Were there breakthroughs in biotechnology or healthcare between September 2022 and September 2023?\n20. How did global temperatures trend between September 2022 and September 2023?\n21. What major environmental policies were implemented globally between September 2022 and September 2023?\n22. Were there any new pandemics or outbreaks between September 2022 and September 2023?\n23. What significant developments occurred in vaccine technology between September 2022 and September 2023?\n24. Were any major medical devices or drugs approved between September 2022 and September 2023?\n25. What trends were observed in global health outcomes between September 2022 and September 2023?\n26. Which startups achieved unicorn status between September 2022 and September 2023?\n27. What trends shaped global markets and industries between September 2022 and September 2023?\n28. Were there significant mergers or acquisitions in the tech sector between September 2022 and September 2023?\n29. What innovations drove the automotive industry forward between September 2022 and September 2023?\n30. How did cryptocurrencies perform between September 2022 and September 2023?\n31. What were the most influential social movements between September 2022 and September 2023?\n32. How did social media platforms evolve between September 2022 and September 2023?\n33. What cultural phenomena defined the period between September 2022 and September 2023?\n34. What were the top trends in entertainment between September 2022 and September 2023?\n35. Which major sports events took place between September 2022 and September 2023?\n36. What were the key highlights of the FIFA Women's World Cup 2023 during this period?\n37. Which athletes gained prominence between September 2022 and September 2023?\n38. What were the major achievements in esports between September 2022 and September 2023?\n39. What were the most popular books released between September 2022 and September 2023?\n40. Which films or series had the highest impact globally between September 2022 and September 2023?\n41. Were there any notable art exhibitions or cultural festivals between September 2022 and September 2023?\n42. How did education systems evolve between September 2022 and September 2023?\n43. What trends emerged in online learning and EdTech between September 2022 and September 2023?\n44. What were the notable legislative changes in key countries between September 2022 and September 2023?\n45. How did global organizations like the UN respond to ongoing crises between September 2022 and September 2023?\n46. What were the major agreements reached during COP27 and COP28 between September 2022 and September 2023?\n47. Were there significant advancements in carbon capture technologies between September 2022 and September 2023?\n48. Did any new countries launch successful space missions between September 2022 and September 2023?\n49. What progress was made toward manned missions to Mars between September 2022 and September 2023?\n50. Were there significant ethical debates in AI or biotech between September 2022 and September 2023?\n51. What were the key issues surrounding digital privacy between September 2022 and September 2023?\n52. How did the financial markets behave between September 2022 and September 2023?\n53. What new trends shaped the energy sector between September 2022 and September 2023?\n54. Were there advancements in agriculture or food technologies between September 2022 and September 2023?\n55. What new startups or companies disrupted traditional industries between September 2022 and September 2023?\n56. How did renewable energy adoption progress globally between September 2022 and September 2023?\n57. Were there any global crises or major disasters between September 2022 and September 2023?\n58. How did the war in Ukraine evolve between September 2022 and September 2023?\n59. Were there significant developments in refugee movements between September 2022 and September 2023?\n60. What were the most impactful research papers published between September 2022 and September 2023?\n61. Were there any Nobel Prize winners announced during this period between September 2022 and September 2023?\n62. What were the major fashion trends between September 2022 and September 2023?\n63. Which countries had the most significant GDP growth between September 2022 and September 2023?\n64. What were the most searched topics on the internet between September 2022 and September 2023?"}, {"title": "A.2. Question vs. Responses", "content": "Below we present a sample question with two corresponding responses, classified as either non-hallucinated (NH) or hallucinated (H), along with some of their extracted keywords.\n\"How did the global economy perform between September 2022 and September 2023?\""}, {"title": "\u0391.2.1. \u039d\u039fN-HALLUCINATED RESPONSES", "content": "1. Response NH-1: \"\"What are some of the major economic trends that have emerged during this period?The global economy has experienced significant changes over the past year, with both positive and negative developments. In terms of overall performance, there were several key factors at play. One important factor was the impact of COVID-19 on economies around the world. The pandemic caused widespread disruptions across industries as businesses closed or reduced operations due to lockdowns and social distancing measures implemented by governments worldwide. This led to declines in GDP growth rates for many countries throughout much of last year but also resulted in increased government spending aimed towards supporting affected sectors such as healthcare providers who faced shortages of personal protective equipment (PPE).Another notable trend observed within our current macroeconomic environment is rising inflationary pressures stemming from supply chain bottlenecks exacerbated by trade tensions among major trading partners like China-US relations which saw tariffs imposed upon each other's goods exports leading up to November elections before being lifted shortly thereafter after negotiations took place behind closed doors without public disclosure until just prior release date when news broke out about agreement reached via phone call between President Trump & Chinese Vice Premier Liu He resulting into easing off restrictions placed previously against imports coming through ports located along coasts facing Pacific Ocean regionally speaking thus allowing smoother flow again despite ongoing uncertainty surrounding future course direction taken post-pandemic recovery phase given recent surge witnessed recently regarding number cases reported daily basis globally alongside concerns raised pertaining potential emergence new variants capable spreading faster than previous ones causing more severe illness requiring hospitalization longer periods time compared earlier\u201d, \"\nKeywords: economi, pandem, trend, tariff, macroeconom\n2. Response NH-2: \u201cWhat are some of the key economic indicators that can be used to assess this performance?The global economy performed well during the period from September 2022 to September 2023. This was largely due to strong growth in major economies such as China, India, Japan, Germany, France, Italy, Spain, Brazil, Mexico, South Korea, Australia, Canada, Russia, Saudi Arabia, Turkey, Indonesia, Thailand, Malaysia, Singapore, Vietnam, Taiwan, Hong Kong, Macau, New Zealand, Argentina, Chile, Colombia, Peru, Venezuela, Ecuador, Bolivia, Paraguay, Uruguay, Cuba, Haiti, Dominican Republic, Jamaica, Barbados, Trinidad & Tobago, Guyana, Suriname, Belize, Costa Rica, Nicaragua, Honduras, El Salvador, Guatemala, Panama, Puerto Rico, Virgin Islands (US), Bermuda, Bahamas, Cayman Islands, Turks & Caicos Islands, British Virgin Islands, Anguilla, Montserrat, St Kitts-Nevis, Antigua-Barbuda, Dominica, Grenada, Saint Lucia, Saint Vincent-Grenadines, Saint Martin-Martinique, Guadeloupe-French West Indies, Netherlands Antilles-Aruba, Cura\u00e7ao-Sint Maarten, Bonaire-St Eustatius-Saba, Sint Barthelemy-Belgium-Luxembourg-Netherlands-Denmark-Iceland-Faroe Islands-Jersey Channel Islands-Cyprus-Turkey-Israel-Palest\u201d\nKeywords: econom, haiti, economi, bahama, economi"}, {"title": "A.2.2. HALLUCINATED RESPONSES", "content": "1. Response H-1: \u201csierpina 1, 2022 Respond to the following questions with at least one reference from outside of your textbook. Your response should be at least two paragraphs long (not including references). What is"}]}