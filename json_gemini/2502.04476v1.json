{"title": "ADIFF: EXPLAINING AUDIO DIFFERENCE USING NATURAL LANGUAGE", "authors": ["Soham Deshmukh", "Shuo Han", "Rita Singh", "Bhiksha Raj"], "abstract": "Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model's ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences.", "sections": [{"title": "1 INTRODUCTION", "content": "In the Boston Marathon bombing of 2014, forensic investigators were faced with a challenge were the various audio recordings purportedly captured of the event (and put up on social media) by various people recordings of the same event, or were they mistaken or fraudulent uploads actually from different events? This anecdote highlights the complexity of audio analysis. One can have different sounding recordings of the same event, similar sounding recordings of different events, and other combinations. To know the relation, one must be able to describe the recordings in relative or comparative terms. This natural human ability is used in various contexts- in audio forensics to verify recording authenticity (Maher (2009); Kraetzer et al. (2007)), crucial for legal investigations. In production and broadcasting, it aids in Audio Quality Assessment (Campbell et al. (2009); Deshmukh et al. (2024a)) to detect subtle variations. In audio generation (Liu et al. (2023); Kreuk et al.), it helps create realistic synthetic audio. Currently, identifying audio differences requires a human listener with expertise in phonetics (Johnson & Johnson (2004)), the acoustic-phonetic approach (Stevens (2000)), and spectrogram analysis to examine recordings and identify differences across various parameters.\nRecent advancements in audio-text learning has enabled new applications such as text-to-audio retrieval (Koepke et al. (2022); Deshmukh et al. (2023b)), audio captioning (Mei et al. (2022a); Deshmukh et al. (2024b)), and audio question answering (Lipping et al. (2022); Gong et al. (2024)). To excel in these applications, models must possess both fine-grained and coarse-grained understanding of audio, enabling them to describe audio content in human-like natural language. By pretraining models on multiple audio-text tasks, individual audio-text tasks see a benefit in performance improvement (Deshmukh et al. (2023a)). Therefore, the field has shifted towards training audio models capable of performing the aforementioned audio-text tasks using a single model. Examples of such Audio-Language Models (ALM) include SALMONN (Tang et al. (2024b)), GAMA (Ghosh et al. (2024a)), and LTU (Gong et al. (2024; 2023)). However, the current literature has not addressed the task of audio difference explanation. This is due to the challenges in generating coherent paragraph-level descriptions, the lack of cognitive processing and understanding similar to human auditory perception, and the absence of annotated datasets specifically designed for this task.\nExplaining the differences between two audio samples also acts as an effective benchmark for assessing a model's comparative reasoning and its ability to integrate audio information with world knowledge. When a model is tasked with distinguishing between two audios, it must employ comparative reasoning to identify both subtle and significant details. This requires the model to understand the fundamental properties of audio signals, such as frequency, amplitude, and temporal patterns, as well as determining pitch, timbre, and loudness. Beyond these signal characteristics, the model must also use world-knowledge to grasp contextual elements, like the genre of music or the type of environment where the audio was recorded. Finally, the model must use comparative and deductive reasoning to interpret and compare these features, identifying nuanced differences and similarities, and draw conclusion. This makes audio difference explanation an effective benchmark for evaluating audio-text models and exploring methods for integrating audio information, world knowledge, and comparative reasoning.\nIn this paper, our main contributions are:\n\u2022 Introduce the audio difference explanation task which aims to provide natural language explanation for the differences in two audio files. To build and benchmark models for this task, we create two datasets ACD and CLD, where the difference explanation is generated by LLM using human annotated captions and later verified by human annotators (test set). To mimic human explanations, each dataset contains three tiers of explanation ranging from one-sentence (audio events) to detailed explanations (scene, semantic, listener emotions).\n\u2022 We propose a naive baseline for this task using prefix-tuning. In this approach, audio embeddings from the two audio files are used to prompt a frozen language model. To address the limitations of this naive baseline, we introduce the ADIFF model, which incorporates separator token, cross-projection layer and undergoes a three-stage training process. These enhancements improve the model's ability to identify detailed differences and distinguish perceptually similar sounds, resulting in a performance improvement over the naive baseline and SOTA ALM (Section 4). The checkpoint will be publicly released\u00b9.\n\u2022 Under the proposed framework, we conduct several ablation studies to understand the impact of cross-projection, language model scaling, audio-grounding, and fine-tuning. Our findings reveal that (1) cross-projection aids in utilizing text prefixes to store difference attributes, (2) under limited compute and data, smaller language models are easier to ground in audio with proper training and (3) position-based audio captioning with multiple audio inputs enhances the model's performance on similar-sounding acoustic sources. Lastly, we address language model hallucinations by enabling users to detect inaccuracies through comparisons between generated descriptions and predicted audio event probabilities."}, {"title": "2 AUDIO DIFFERENCE EXPLANATION", "content": "The task of explaining audio differences consists of finding the difference between two audio files and explaining the difference in natural language. Human explanations draw on various sources of information, including acoustic details and human perception, as well as linguistic nuances. From an acoustic standpoint, explanations can range from broad audio event differences to finer-grained signal variations. They can also be based on objective facts and world knowledge or on human perceptual differences. Linguistically, these explanations can be concise, like a single sentence, or more detailed, like a paragraph. This allows us to segregate audio difference explanations that vary in detail, focus, and style:\nConcise: This description is concise and straightforward, briefly mentioning the key characteristics of each audio without much elaboration. It highlights the main differences in ambiance and focus between the two audios. We refer to this as the tier-1 explanation.\nBrief: This answer provides more detail and context. It not only describes the sounds but also includes audio events, sound sources, the nature of the sounds, making it slightly more analytical. It also describes the sequence of sounds (temporal order) and potential scene differences across audios. We refer to this as the tier-2 explanation.\nDetail: This is the most detailed and descriptive answer. It delves into the sonic qualities of each audio, including potential sources and the listener's experience. It compares the two audios in terms of complexity and engagement, providing a richer and more immersive description. It analyzes the audio events, acoustic scenes, sound sources, signal characteristics, tonal differences, and overall feel of each audio. We refer to this as the tier-3 explanation.\nThe first-tier explanation is concise and to the point, the second-tier explanation provides more context, and the third explanation offers a thorough, immersive analysis. A secondary benefit of dividing the explanation into tiers allows us to do fine-grained analysis of model performance."}, {"title": "2.1 AUDIO DIFFERENCE DATASET", "content": "In this section, we outline the development steps for the AudioCaps Difference (ACD) and Clotho Difference (CLD) datasets.\nAudio recordings. We source the audio recordings from the AudioCaps and ClothoV21 datasets. The AudioCaps dataset comprises 46,000 audio samples, each lasting 10 seconds, sourced from AudioSet, and includes human-annotated captions that describe the audio content. Annotators had access to both audio and visual cues during the annotation process. On the other hand, the Clotho dataset, though smaller, features audio samples ranging from 15 to 30 seconds, sourced from the Freesound platform, and includes five human-annotated captions per sample. These annotators only had access to audio information and followed a detailed protocol to minimize errors and ensure diversity in the captions. By utilizing both the AudioCaps and Clotho, we ensure variability in audio content, duration, and annotation style.\nDifference explanations. Large Language Models (LLMs) have been effectively utilized to generate text descriptions for audio across various tasks, such as question-answering (Gong et al. (2024)), compositional reasoning (Ghosh et al. (2024b;a)), and deductive reasoning (Deshmukh et al. (2024c)). We adopt a similar approach for generating explanations for the audio difference task. The process involves three main steps: data sources, explanation generation, and explanation verification.\nFor data sources, we limit the audio recordings and human-annotated descriptions to those from AudioCaps and ClothoV21. To generate explanations, we prompt an LLM to describe the differences between two audio recordings using the provided human-annotated descriptions. Our prompting setup is similar to (Deshmukh et al. (2024c)), with two key modifications. First, we instruct the LLM to incorporate knowledge about sound sources, materials, acoustics, and emotions when generating explanations. Second, we define the tier of explanation by restricting the sources the LLM can use and the length of the explanation. Therefore, we sample two annotated human descriptions and prompt LLM to generate different tiers of explanations. Finally, human annotators verify the difference explanations. If an explanation is found to be inaccurate, the human annotators manually remove the hallucinated audio event and add necessary details. Due to the cost of this process, we restrict verification to the test set of three tiers. Details on prompting setup and data creation are available in the Appendix E. The resulting dataset comprises of audio recordings paired with the three tiers of generated explanations."}, {"title": "3 MODEL", "content": "In this section, we describe our proposed model ADIFF, which employs prefix tuning to prompt a frozen Language Model. The model accepts three inputs: audio 1, audio 2, and a user prompt, and generates free-form text as output. The model architecture is covered in Section 3.1, the training process in Section 3.2, the three stages of training in Section 3.3, and lastly inference in Section 3.4."}, {"title": "3.1 ARCHITECTURE", "content": "ADIFF consists of four main components: an audio encoder, an audio projection layer, a cross-projection layer, and a decoder-only language model.\nAudio Encoder. The audio encoder is used for extracting general-purpose audio representations for both the audios. We use HTSAT (Chen et al. (2022)) which is pretrained on AudioSet (Gemmeke et al. (2017)) and achieves SoTA on identifying various sound events and acoustic scenes.\nProjection. The audio projection is used to project the audio embeddings from audio encoder to latent space of Language model. The audio projection converts a single embedding into a sequence of latent tokens [s, d]. The projection first expands the hidden dimension to a larger hidden dimension k, which is then split to form [s, d] where k = s * d. This is followed by concatenating with a learnable constant, resulting in [s + c, d]. This output is passed to the transformer, followed by clipping of the learnable constant output c. The resulting output of audio projection is of shape [s, d]. This projection architecture is shown to perform well for prefix-tuning architectures (Deshmukh et al. (2023a; 2024b); Mokady et al. (2021)). The text projection translates text embeddings with learnable transformer layers. The architecture is similar to audio projection (Fig 3) without the first linear layer.\nCross Projection. Once the audio embeddings are in the same latent space of Language model, the cross-projection layer is used to improve the model's ability to highlight differences. The cross-"}, {"title": "3.2 TRAINING", "content": "The training process uses the next-token prediction objective to learn unfrozen (learnable) parameters. A sample input to the model is {x1,x2,t,c} where x\u2081 is the first audio, x2 is the second audio, t' is the text prompt, and c\u00b2 is the difference explanation respectively. The user provides Audio 1 (x) and Audio 2 (x) along with a prompt (t\u00b2) specifying the desired level of explanation. Each audio file is independently encoded by the audio encoder (a4), creating separate audio embeddings. Simultaneously, the user's text prompt and separator token (s) is tokenized using a BPE tokenizer and embedded with the language model's vocabulary embeddings (gp). These embeddings including the audios, separator embedding, and text prompt embeddings are then projected into the language model's latent space using a cross-projection layer (ms) layer to form a prefix.\n$c^{2} = concat\\{m_{\\zeta}(\\alpha_{\\varphi}(x_{1})), g_{\\psi}(s), m_{\\zeta} (\\alpha_{\\varphi}(x_{2})), g_{\\psi}(t')\\}$\n$p_{1}, ..., p_{k} = h(c^{2})$\nThe combined prefix $\\{p_{i}\\}_{i=1}^{k}$ is of length (tokens) k in the language models space. This prefix is used to prompt the language model (fo) to generate a text explanation highlighting the differences between the two audios. The model is trained as a typical captioning system, learning to predict a caption (text tokens) or based on the prefix p\u00b2 in an autoregressive manner. The loss function is Cross-Entropy per token:\n$\\mathcal{L} = - \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\sum_{o_{i}} \\sum_{o_{j}} log p_{y}(o | p_{1}, ..., p_{k}, o_{1}, ..., o_{t-1})$\nwhere y denotes model's trainable parameters and consists of \u03c6, \u03b6, \u03c8, \u03b2. Out of all the parameters-\u03b6, \u03b2 are always trained while the rest are determined per stage (Section 3.3)."}, {"title": "3.3 TRAINING STAGES", "content": "The model's training process involves several stages to preserve audio information and prevent the destructive gradient updates during unaligned full finetuning. These stages are: unimodal pretraining, multimodal alignment training, and finetuning.\nUnimodal Pretraining. Unimodal pretraining entails training the audio encoder and language model on tasks specific to their respective modalities. For the audio encoder, this means training on AudioSet to predict audio events. For the language model, it involves pretraining on a large corpus of text. For the audio difference task, we skip independent modality pretraining and instead use pretrained unimodal models like HTSAT and GPT-2 which have performed the unimodal training.\nMultimodal grouding. After unimodal pretraining, the language model must be grounded in the audio to generate audio-conditioned text. During this stage, both the audio encoder and language model are frozen, and the model learns the audio projection and cross projection using cross-entropy loss for each predicted next text token. Freezing the audio encoder and language model ensures that the initial gradient updates do not cause the loss of modality-specific information. This approach is similar to the prefix-tuning literature, where the language model remains frozen (Deshmukh et al. (2023a); Mokady et al. (2021)).\nFinetuning. In this final stage, we finetune the audio-grounded language model by unfreezing all the modules. This allows us to retain audio-specific information and better steer the language model to produce the necessary descriptive answers using acoustic terminology. This stage uses a low learning rate with warmup and minimum training steps to minimize catastrophic forgetting."}, {"title": "3.4 INFERENCE", "content": "During inference, the prefix is created from the two test audios and the user-provided text prompt. The decoder (language model), generates the next token in sequence based on this prefix. At each step, the model assigns probabilities to all vocabulary tokens, which are then used to select the next token according to the chosen decoding method. In our experiments, we employed top-k and top-p decoding for better performance across all experiments. The details on experimental setup, training, inference, and implementation can be found in Appendix G."}, {"title": "4 RESULTS", "content": "4.1 EXPERIMENTAL SETUP\nThe base model includes an audio encoder, audio projection, and a language model (GPT2-base). It is trained end-to-end with the language model frozen, using captioning loss. This is referred to as the baseline or naive model. For ADIFF, we add a separator and a cross-projection layer to the architecture. ADIFF's training process has three stages to preserve audio information and prevent destructive gradient updates during unaligned full finetuning: unimodal pretraining, multimodal alignment training, and finetuning, as detailed in Section 3.3. For SOTA ALMs, we use Qwen-AC (Chu et al. (2023)) as it is the only ALM in literature that supports two audio inputs. We consider three versions of Qwen-AC: Zero-Shot (Z), LoRA (L), and full fine-tuning (F). All models except for the zero-shot version use the train set of ACD and CLD for training and are evaluated on the test set of ACD and CLD across three tiers."}, {"title": "4.2 RESULTS", "content": "Objective evaluation. The performance of the naive model, QwenAC versions and ADIFF model is compared in Table 2, using objective metrics described in Section 2.2. The ADIFF model outperforms the naive baseline across all three tiers. Moreover, it also outperforms QwenAC (L) and QwenAC (F) across all Tiers of ACD and CLD except on Tier 2 CLD. For Tier 2, we observe similar trends as in ablation conducted for baseline architecture and language-only performance 5.1. For Tier 2, we observe similar trends to those seen in the ablation studies conducted for the baseline architecture and language-only performance. Linguistically, Tier 2 is the easiest to learn from, followed by Tier 3, and then Tier 1. This is because Tier 1 has the fewest words, with most of them containing audio-related information. In contrast, about 15% of the words in Tier 2 pertain to the linguistic structure of contrasting audios. This indicates that Tier 2's higher scores, even in subsequent experiments, are due to its linguistic simplicity for the model to match. Consequently, QwenAC, which uses a much stronger 7B parameter LLM, performs better than ADIFF, which has a 128M parameter LLM.\nSubjective evaluation. The objective metrics are limited because they rely on linguistic biases, which can unfairly penalize diverse outputs (Morato & Mesaros (2021); Mei et al. (2024a)). Therefore, we also perform subjective evaluation, where human annotators rate the audio difference explanations across different dimensions of correctness (1-5), granularity (1-5), and readability (1-5). The dimension definitions and human evaluation setup and is explained in Appendix I.2. The subjective evaluation also ensures a fair comparison with QwenAC (Z), which is not trained on the ACD and CLD datasets and thus do not align with the data distribution, leading to poorer objective metric scores. We evaluate the models across three scenarios. The first scenario, Studio, involves recordings from the same sources, such as different breeds and numbers of dog barks, all recorded in a professional studio. The second scenario involves random samples from FSD50K (Fonseca et al. (2021)). The third scenario uses random samples from GTZAN Genres (Tzanetakis & Cook (2002)), featuring music from various genres and settings. The subjective evaluation results are shown in Table 3. Across all scenarios, ADIFF outperforms the naive baseline and Qwen-AC (Z) across all metrics and domains. Specifically, ADIFF sees the largest improvement in the granularity metric, highlighting the model's ability to produce in-depth descriptions. ADIFF also beats the fine-tuned variants of Qwen-AC on average across all metrics. However, in two cases-readability on Studio and granularity on FSD50K-a fine-tuned version of Qwen-AC outperforms ADIFF. This is likely because Qwen-AC's LLM, Qwen-7B, is significantly larger than the 128M LLM of ADIFF. Qualitative analysis shows it produces more coherent output, slightly improving readability. Additionally, we highlight the difficulty of the task, as even ADIFF achieves average scores of 3.5 across the dimensions out of 5."}, {"title": "5 ABLATIONS", "content": "In this section, we examine various components of model architecture and the impact of training methods on audio difference task performance. It covers- baseline architecture and the contribution of linguistics (Section 5.1), effect of cross-projection layer (Section 5.2), scaling language model (Section 5.3), position captioning (Section 5.4), and impact of stage three fine-tuning (Section 5.5)."}, {"title": "5.1 BASELINE ARCHITECTURE", "content": "To build the baseline architecture for the audio difference task, we draw on previous works on prefix tuning (Deshmukh et al. (2023a)). This architecture includes an audio encoder to extract features, an audio mapper to translate these features into the latent space of a language model, and a frozen language model (GPT2-base) to generate text. For the audio difference task, we use the audio encoder to extract features from both audio inputs, followed by mappers to convert these features into the language model's latent space. The frozen language model prompting and the cross-entropy loss setup remain unchanged. We refer to this as the baseline architecture for this task.\nIn question-answering and captioning tasks, models have been observed to learn linguistic information and answer questions without relying on perception modalities like vision or audio. This phenomenon is noted in both vision and audio literature (Lipping et al. (2022)). Similarly, for the audio difference task, models can game metrics by learning the language patterns instead of analyzing the audio, thereby inflating the metrics. To address this, we establish a baseline where the audio encoder is randomly initialized and kept frozen. This approach allows us to determine the maximum performance achievable without analyzing audio, focusing solely on learning the language patterns for each tier of explanation. We refer to this as the language-only performance for the audio difference task.\nWe summarize the language-only and baseline performance in Table 4. Experiment A presents language-only performance, where the audio encoder is randomly initialized and frozen. In contrast, Experiment B shows baseline performance with the audio encoder initialized from HTSAT pretrained weights and frozen. In Experiment A, we observe that Tier 2 is the easiest to learn from a linguistic standpoint, followed by Tier 3, and then Tier 1. This is because Tier 1 has the fewest words, with most containing audio-related information. Conversely, about 15% of the words in Tier 2 pertain to the linguistic structure of contrasting audios. This suggests that Tier 2's higher scores, even in subsequent experiments, are due to its linguistic simplicity for the model to match. In Experiment B, where the audio encoder is pretrained, the model performs better across all tiers, as expected. This indicates that the model architecture effectively leverages audio information to improve explanations."}, {"title": "5.2 EFFECT OF CROSS-PROJECTION", "content": "We observe two limitations with the baseline model performance by qualitative analysis. First, when the audio input includes perceptually similar sounds and the same audio events, the model tends to confuse the distinct information between audios. Second, the model struggles with Tier 2 and Tier 3 explanations, where it needs to discern subtle audio differences such as scenes, acoustics, sources, and their composition rather than just audio events. To address the second limitation and partially the first, we introduce a cross-projection layer with a latent separator token between the two audio inputs. We concatenate the first audio latent embedding, the separator embedding, and the second audio latent embedding to form a prefix. The separator embedding is derived from the < |endoftext| > token embedding of GPT-2 base. This concatenated prefix is then passed through the cross-projection layer, which consists of transformer layers with a learnable constant vector.\nThe results with the cross-projection layer are summarized in Table 4. Experiment B shows the performance of baseline architecture. Experiment C shows performance of baseline architecture with separator token and cross-projection layer. On average, we see improvements for ACD and CLD across the three tiers of explanations. Specifically, in Appendix Table 16, the average metric score of Experiment C is consistently higher than Experiment B across all tiers of ACD and CLD."}, {"title": "5.3 SCALING LANGUAGE MODEL PARAMETERS", "content": "The performance of vision-language models (Alabdulmohsin et al. (2024)) tends to improve as their scale increases. Empirical evidence suggests that this improvement often follows a predictable power law (Kaplan et al. (2020)). Similarly, recent studies have found a correlation between the scale of language models and their performance on audio reasoning tasks (Deshmukh et al. (2024c)). However, these observations are typically made at the compute-optimal frontier (Hoffmann et al. (2022)) and are not often examined under limited compute conditions. In practical scenarios, where compute resources and time are constrained, larger models may not be the best option. This has been illustrated in the case of language models, where the newer Chinchilla model (Hoffmann et al. (2022)) outperformed its predecessor Gopher (Rae et al. (2021)), despite being four times smaller. Therefore, we study scaling language for audio-language models for prefix tuning.\nTo investigate the impact of scale, we modify the language model (transformer decoder) in the architecture depicted in Figure 3. We experiment with GPT2-base (128M), GPT2-medium (256M), GPT2-large (774M), and GPT2-XL (1.5B). Each model is trained with the same compute budget, approximately equivalent to 30 epochs. The results, presented in Table 5, indicate that with the same data (audio and text tokens) and limited compute, the base and medium models perform similarly on average (Table 17), while the large and XL models perform worse. Moreover, the higher per-epoch computational cost of larger models widens the gap between small and large LM in ADIFF. It is important to note that we do not scale the number of training tokens as we scale the language models, which affects performance.\nTo determine if increased computational power helps larger LMs, we train the models longer for an additional 20 epochs. The outcomes, shown in Figure 4, indicate that for the 128M and 256M models, performance peaks around 30 epochs. In contrast, for the 774M and 1.5B models, performance continues to improve with more epochs. This suggests that aligning and guiding larger models with prefix-tuning requires a greater number of epochs. Given the computation budget and performance, we choose to use GPT2-base for ADIFF and subsequent experiments."}, {"title": "5.4 AUDIO GROUNDING WITH POSITION CAPTIONING", "content": "To improve audio grounding, we instruct the model to caption either audio 1 or audio 2. This is accomplished by training the model on both audio captioning data and audio difference datasets. The audio captioning data includes {audio1, audio2, prompt}, where the prompt is \u201ccaption the first audio\" or \"caption the second audio\". By incorporating single or position-specific captioning data in the training process, we ensure the model does not get confused between audio 1 and audio 2, and accurately distinguishes between similar or acoustically alike audio events."}, {"title": "5.5 STAGE-3 FINETUNING", "content": "In the final stage, following multimodal grounding, we fine-tune the language model. This training is conducted over a few epochs with a small learning rate of approximately 1e-6, using a cosine learning rate scheduler. The results of this fine-tuning are presented in Table 6. Experiment D is position captioning training, while Experiment C shows the performance post-fine-tuning. The final fine-tuning consistently improves performance across various tiers and datasets. We also find that the finetuned model outperforms the base model across three tiers, especially Tier 3. More details on qualitative analysis are available in Appendix K."}, {"title": "6 HALLUCINATION", "content": "Language models often generate misleading or incorrect outputs, known as hallucinations. For Audio-Language Models (ALMs), this means producing responses not based on actual audio input, such as inventing audio events or misinterpreting common sounds (Kuan et al. (2024); Nishimura et al. (2024)). This lack of grounding can impair deductive reasoning and comparative skills. To address this, we use audio grounding with position captioning to differentiate similar-sounding events from different sources. However, some hallucination issues persist. We aim to provide tools to detect these hallucinations by keeping the HTSAT audio encoder frozen. This encoder, trained on AudioSet, predicts 527 audio events and their presence probabilities over time, allowing users to verify the accuracy of generated descriptions."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce the task of audio difference explanation, create relevant datasets, and propose an strong baseline. We present two new datasets from AudioCaps and Clotho. Using Large Language Models (LLMs), we generated three levels of explanations for audio differences: concise descriptions, brief sentences about events and scenes, and comprehensive explanations including semantics and emotions. We propose ADIFF, an audio prefix tuning-based language model with a cross-projection module and a three-step training process to address the limitation of the naive baseline. The objective and subjective evaluations show significant performance improvements over the naive baseline and SOTA ALM. We also conduct ablation studies to understand the effects of various components and find various insights relevant to training audio-language models. Our benchmarks and ablation studies show the effectiveness of our method, utilizing comparative reasoning to generate more human-like explanations for audio differences."}, {"title": "A ETHICS STATEMENT", "content": "This paper adheres to the ethical guidelines set forth by the ICLR conference and aims to contribute to the research community in a responsible and transparent manner. The study utilizes publicly available datasets, namely AudioCaps (Kim et al. (2019)) and Clotho (Drossos et al. (2020)), which have been used in accordance with their respective licenses and ethical use guidelines. No personally identifiable information (PII) or sensitive data was used in this research, ensuring the privacy and confidentiality of all subjects involved.\nThe research focuses on developing methods for explaining audio differences using natural language, and all experiments were conducted with the intent to advance scientific knowledge in a positive and constructive way. We acknowledge the potential implications of our work in fields such as audio forensics and surveillance, and we emphasize that our models are intended solely for academic and benign applications. We do not condone or support the misuse of our work in ways that could infringe upon individual privacy or be used for unethical purposes. All human evaluators and annotators participated voluntarily with their informed consent. There were no conflicts of interest or biases influencing the results of this study. Finally, we are committed to open science and will release our datasets and model checkpoints, to the research community to promote transparency, reproducibility, and collaboration."}, {"title": "B REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our work, we have taken several measures, which are thoroughly documented throughout the main paper, appendix, and supplemental materials. For our proposed model, ADIFF, and the associated benchmarks, we have provided a downloadable source code anonymously in the supplementary materials to facilitate replication of our experiments. Detailed explanations of our experimental setup, hyperparameters, and model architecture can be found in Section 3 and Appendix G. We have outlined the data processing steps, including the creation of the AudioCaps Difference (ACD) and Clotho Difference (CLD) datasets, in Section 2.1 and the Appendix E, ensuring transparency in data preparation and usage. All datasets used in this work are publicly available, and we have provided links to these resources along with data processing scripts in the supplementary materials. Our ablation studies in Section 5 further validate the robustness of our model design, and comprehensive results are presented to demonstrate consistency and reliability of the findings."}, {"title": "C RELATED WORK", "content": "Audio-text learning. Audio-text learning focuses on developing multi-modal representations to address specific tasks. These tasks include text-to-audio retrieval (Deshmukh et al. (2023b)), audio captioning (Drossos et al. (2020); Mei et al. (2022a)), and text-to-audio generation (Liu et al. (2023)). Among these, audio captioning is particularly relevant for the task of audio difference explanation, as it involves generating natural language descriptions from audio recordings. The architecture typically comprises an encoder to learn audio representations and a decoder to generate text tokens (Drossos et al. (2017)). Research in this area has explored enhancing diversity (Morato & Mesaros (2021); Mei et al. (2024a; 2022b)) in outputs, improving audio encoder and training methods (Wu et al.), and leveraging LLMs as decoder (Tang et al. (2024a)).\nAudio-Language Models. Instead of building task-specific models, recent literature has focused on building general-purpose audio models by using language. These models are referred to as Audio-Language Models. For example, CLAP (Elizalde et al. (2023; 2024); Wu et al. (2023)) and AudioFlamingo (Kong et al. (2024)) is trained on millions of audio-text pairs to learn multi-modal audio-text representations, and are used for tasks like zero-shot and few-shot classification and retrieval. To address open-ended tasks such as audio question answering and audio caption-ing, a decoder or LLM is added to the architecture. For example, Pengi (Deshmukh et al. (2023a)) employed prefix-tuning to learn a mapper and prompt a frozen language model. The later Audio-Language Models like LTU (Gong et al. (2024; 2023)), SALMONN (Tang et al. (2024b)), GAMA (Ghosh et al. (2024a)) utilized LoRA (Hu et al.) to develop lightweight adapters on language models. While most Audio Language Models (ALMs) concentrate on non-speech audio, Qwen-audio and LTU-AS integrate both speech and audio understanding by utilizing the Whisper encoder (Radford et al. (2023))."}, {"title": "Difference Captioning.", "content": "In the vision domain, the task of"}]}