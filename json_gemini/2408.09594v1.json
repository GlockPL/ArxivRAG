{"title": "Moonshine: Distilling Game Content Generators into Steerable Generative Models", "authors": ["Yuhe Nie", "Michael Middleton", "Tim Merino", "Nidhushan Kanagaraja", "Ashutosh Kumar", "Zhan Zhuang", "Julian Togelius"], "abstract": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game content creation, yet challenges in controllability and limited training data persist. This study addresses these issues by distilling a constructive PCG algorithm into a controllable PCGML model. We first generate a large amount of content with a constructive algorithm and label it using a Large Language Model (LLM). We use these synthetic labels to condition two PCGML models for content-specific generation, a diffusion model and the five-dollar model. This neural network distillation process ensures that the generation aligns with the original algorithm while introducing controllability through plain text. We define this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering an alternative to prevalent text-to-image multi-modal tasks. We compare our distilled models with the baseline constructive algorithm. Our analysis of the variety, accuracy, and quality of our generation demonstrates the efficacy of distilling constructive methods into controllable text-conditioned PCGML models.", "sections": [{"title": "Introduction", "content": "Procedural Content Generation (PCG) is crucial to many types of video games, including roguelikes and other games that are designed around infinite content variety. PCG is also important for empowering users and designers to create various types of game content. Many games feature generators for content such as quests, maps, levels, and items. These generators are built on different algorithms and underlying principles, including search, constraint satisfaction, machine learning, and hand-crafted rules.\nAn important concern in content generation is controllability. Many content generators feature this to some degree; in its simplest form, controllability can be selecting a map size or number of enemies. While this basic level of control may be suitable for some applications, it can be limiting in others. On the opposite end of the spectrum is text-conditioned content generation. In this form, any idea that can be expressed through natural language can serve as guidance for a generator. The rise of Text-to-Image (T2I) models has demonstrated the value of such flexible means of control over content generation. While Text-to-Image capabilities have improved massively in recent years, these models are not well suited for many game-content generation tasks, where each game has a unique set of constraints\nProcedural Content Generation via Machine Learning (PCGML) (Summerville et al. 2018) uses machine learning models to learn from existing game content and generate diverse artifacts. A common challenge for many PCGML methods is a lack of quality data to learn from. Training controllable generators places further requirements on the training set, needing diverse labels for training data. To achieve text-conditioned PCGML, we require accurate, descriptive labels for huge amounts of game data. Data labeling has long been an expensive undertaking-requiring massive amounts of human labor\u2014 making it infeasible for most game developers. Instead, we propose leveraging the impressive capabilities of Large Language Models (LLMs) to eliminate this hurdle.\nWe propose a novel method for generating infinite synthetic datasets for text-conditioned PCGML by integrating traditional PCG algorithms with a Large Language Model. We use the PCG algorithm from the dungeon roguelike Brogue (Walker 2019) to create an extensive collection of game maps, then automatically generate descriptive labels for each. We test this synthetic dataset by training two Text-to-Map (T2M) generative models and evaluating their output via human evaluation and CLIP score. In short, we train generative models to mimic a black-box PCG algorithm using only its generated artifacts, while introducing natural language text conditioning.\nThis process can be seen as analogous to knowledge distillation typically used to describe transferring the capabilities of one neural network into another. However, in this case, we are distilling an arbitrary algorithmic process into a neural network. This is a somewhat unprincipled type of distillation, but nevertheless yields impressive results; we therefore name the method Moonshine.\nIn summary, our contributions are:\n1. We devise a strategy for creating extensive synthetic datasets for text-conditioned PCGML using traditional PCG methods combined with LLMs."}, {"title": "Related Work", "content": "Text-to-game-Map (T2M) generation is a form of procedural dungeon generation used for generating content inside a dungeon crawler-type video game (Viana and Dos Santos 2019). Togelius et al. (2011) identify two primary approaches in the current landscape of dungeon generation: constructive algorithms and search-based algorithms. Constructive algorithms directly generate dungeon levels through a variety of methods, such as Cellular Automata or Generative Grammars. Search-based algorithms utilize meta heuristics (such as evolutionary algorithms) to optimize dungeon content against a set of criteria, using cycles of generation, evaluation, and selection to iteratively refine solutions. Human controllable content generation is important for designers to input their own preferences into generation. However, Viana and Dos Santos (2019) highlights the scarce reliance on mixed-initiative approaches within dungeon generation, where human design interacts with computer-generated content.\nIn the context of Procedural Content Generation (PCG), van der Linden, Lopes, and Bidarra (2014) proposed a methodology for generating dungeons in role-playing games using a combination of graph theory and geometric algorithms. Their approach focuses on creating diverse and engaging gameplay experiences through the procedural generation of dungeon layouts, room connections, and game objects. The authors demonstrated the effectiveness of their approach through an evaluation of the generated dungeons, showcasing the potential of procedural content generation in enhancing gameplay variety and player engagement. They identified controllability as the key challenge - ensuring procedurally generated dungeons adhere to the desired criteria for gameplay progression, difficulty curves, and consistent theming."}, {"title": "Synthetic Data Labeling", "content": "Most synthetic data labeling approaches come from the domain of text-to-image (T2I) generative models and captioning. He et al. (2024) introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts. This is particularly noteworthy as prompt engineering has been recognized as an effective method for controlling the output of T2I generative models, but it is also laborious due to the need for manually crafted prompts.\nBuilding on the theme of automation in the realm of AI, another noteworthy work by Yang et al. (2023) proposes Adaptive Language-Image Pre-training (ALIP). ALIP is a bi-path model that integrates supervision from both raw text and synthetic captions. This work addresses the issue of intrinsic noise and unmatched image-text pairs in web data, which can potentially affect the performance of representation learning. The authors validate ALIP with experiments on different scales of models and pre-training datasets, showing that ALIP achieves state-of-the-art performance on multiple downstream tasks."}, {"title": "Text-to-Image Generation", "content": "To facilitate this improved interpretation ability we look to the field of text-to-image generation. The Contrastive Language-Image Pre-training (CLIP) architecture, which maps images and text into a shared multimodal embedding space, (Radford et al. 2021) consists of an image encoder for visual inputs and a text encoder for textual descriptions. These encoders are trained jointly using a contrastive loss function. This enables CLIP to learn a broad range of visual concepts directly from natural language descriptions and perform various vision tasks in a zero-shot manner by comparing the embeddings of text prompts with the embeddings of images to find the best match. By adapting the image being encoded and decoded to a vectorized game level, we can apply the same techniques to procedural dungeon generation in hopes of fusing textual prompts and game levels.\nThe CLIP-GEN architecture (Wang et al. 2022) is a self-supervised framework to train a text-to-image generator without relying on expensive paired text-image datasets. CLIP-GEN differs from conventional supervised approaches by leveraging a pre-trained CLIP model. Unlike existing methods that require explicit text descriptions as input, CLIP-GEN enables language-free training by conditioning the image generation process on the CLIP image embeddings extracted from unlabeled images. The authors demonstrate CLIP-GEN's ability to synthesize high-fidelity images corresponding to a diverse set of text prompts, achieving comparable or superior performance to fully-supervised state-of-the-art methods. CLIP-GEN's self-supervised nature and effective distillation of CLIP's multimodal knowledge pave the way for more accessible and scalable text-to-image generation techniques."}, {"title": "Synthetic Dataset Generation", "content": "Our pipeline for synthetic map-description generation follows three main steps:\n1. Extract a map data point from a game using a traditional PCG algorithm.\n2. Compute metadata and heuristics for the map.\n3. Pass metadata and heuristics into an LLM prompt to generate the map description."}, {"title": "Map Extraction", "content": "We extract the map data from the open-source rogue-like dungeon game Brogue (Walker 2019). This game uses a traditional constructive PCG algorithm to generate varied and complex dungeon maps. Unlike Binary Space Partitioning or Cellular Automata (Shaker, Togelius, and Nelson 2016), Brogue's algorithm ensures playability, taking the terrain layout into consideration when generating various difficulties for players.\nThe original Brogue game contains diverse tilesets with terrain, objects, monsters, items, and more. This research focuses on the terrain tiles to simplify the original game content. We scale the maps to a consistent size 32 \u00d7 32 with a tileset of 14 terrain tiles, as detailed in Table 1."}, {"title": "Map Metadata Analysis", "content": "We perform multiple heuristic calculations to extract metadata for each map. This metadata is provided in the LLM's prompt when generating a label for the map. A visual example of metadata is shown in the Fig. 1.\n1. We place a binary mask over each independent room and connecting path.\n2. We divide the map into a grid of cardinal and inter-cardinal directions (N, S, E, W, NE, NW, SE, SW), and assign each room a direction using its midpoint.\n3. We extract the tile counts for each room, order them by quantity and add them to the room's description.\n4. We record the connected room pairs and the path to connect the pair."}, {"title": "Synthetic Description Generation", "content": "We design our pre-generation prompt into 4 sections: Setting, Response format, Examples, and Rules. We introduce the task and the goal for the LLM; specify the exact response format; provide the LLM with few-shot examples of human-authored map descriptions; and a list of rules that it should obey when generating.\nWe then provide the LLM with an integer tile grid representation of the map, a dictionary of integers to tile names, and the meta-data information. We task the LLM to generate 10 text descriptions for each map. This process is repeated for each unique map."}, {"title": "Text2Map: Generation Strategy", "content": ""}, {"title": "Task Definition: Text-to-game-Map Generation", "content": "Our approach to Text-to-game-Map (T2M) generation is similar to Text-to-Image generation, but differs due to the discrete grid nature of the map data. Rather than generating a grid of continuous pixel values, our model generates a grid of probability vectors, then selects the most probable tile at each cell."}, {"title": "Map Representation", "content": "Each map m in our dataset is represented as a three-dimensional matrix of dimensions H \u00d7 W \u00d7 C where H and W denote the map's height and width, respectively, and C represents the tileset size. Each cell $m_{i,j}$ within the matrix is a vector of length C and follows a classification distribution, ensuring that the sum of probabilities across all tiles in a cell equals one.\n$\\forall i \\in [0, H]; j \\in [0, W]; \\sum_{k=1}^{C} m_{i,j,k} = 1$\nIn the training dataset, each map is represented in a one-hot encoded format, where each cell is set to indicate a specific tile choice $m_{i,j,k*} = 1$, and all other tiles are set to zero $m_{i,j,l} = 0$ for l \u2260 k*."}, {"title": "Text Embedding Model", "content": "We use a pre-trained text embedding model gte-large-en-v1.5 (Li et al. 2023) to extract the text embedding vectors. This model is one of the state-of-the-art small models in the Massive Text Embedding Benchmark (MTEB). gte-large-en-v1.5 has a maximum input size of 8192 tokens and outputs a 1024-dimensional text embedding vector."}, {"title": "Generative Model", "content": "We define a comprehensive generative model G, which produces maps M conditioned on text embedding t. This model, parameterized by weights \u03b8, may optionally incorporate input z from a noise distribution to enhance generation diversity. By integrating textual context into the map generation process, the model produces content that is both contextually relevant and varied, based on the input text.\nWe evaluate two multi-modal strategies to investigate the effects of distilling the constructive PCG algorithm into controllable T2M models."}, {"title": "Five-Dollar-Model", "content": "Five-Dollar-Model (proposed by Merino et al. (2023)) is a streamlined model for Text-to-game-Map generation. It is a feed-forward neural network, which maps a text embedding vector directly to a map output. The transformation from text to map can be represented as:\n$M' = G_\\theta(T)$\nThe model is only designed to learn the reconstruction error via self-supervised learning.\n$L = ||M \u2013 M'||_2^2$\nIn this strategy, training can be conceptualized as a classification task. The text embedding vector serves as a discriminative feature that the model uses to \"classify\" into the correct map representation. This perspective aligns the generative task with classification paradigms, focusing on feature utilization and output accuracy."}, {"title": "Moonshine Diffusion Model", "content": "Our Moonshine diffusion model starts with a H \u00d7 C array of random normal noise and gradually denoises it across multiple timesteps, each time moving closer to the map distribution. This iterative refinement uses a model that learns and predicts the noise to be removed at each step (Rombach et al. 2022). Conditioning on the text embedding vector ensures that the denoising trajectory is aligned with the semantics of the input text, allowing the generation of maps that possess visual appeal and are contextually relevant to the input text.\nAs illustrated in Fig. 3, our diffusion model $G_\\theta$ is based on a conditional UNet comprising Resnet and cross-attention blocks that effectively learn the constrained content passed from the text embeddings. At each time step t, the model takes a noisy map $m_t$ and the text embedding vector t as input, and outputs the predicted noise $ \\hat{\\epsilon}$ of the map. We adopt the same setting as the Denoising Diffusion Probabilistic Model (DDPM) (Ho, Jain, and Abbeel 2020) to add the Gaussian noise to the map at each timestep. The denoising process can be mathematically formulated as follows:\n$m_t = DDPM(\\epsilon, m_0, t) = \\sqrt{\\bar{\\alpha}_t} m_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$\n$\\epsilon \\sim N(0,1)$\n$\\hat{\\epsilon} = G_\\theta(m_t, t, t)$\nIn the DDPM, $\\bar{\\alpha}_t$ denotes a decreasing scheduler. The training objective involves minimizing a loss function which focuses on the accurate prediction of noise, $\\epsilon$. The loss function, L, is defined as follows:\n$L = E_{m_0, t, \\epsilon \\sim N(0,1), t \\sim U(1,T)} (||\\epsilon \u2013 G_\\theta(m_t, t, t)||^2)$\nThis iterative denoising process continues until t = 0, resulting in a map $m_0$ that resembles the original training data while incorporating the semantics of input text t."}, {"title": "Experiment and Result", "content": ""}, {"title": "Evaluation Methods", "content": "We used OpenAI's GPT4-Turbo (gpt-4-turbo-2024-04-09) to generate map descriptions. Due to price constraints, we only generate descriptions for 3,000 maps (theoretically, however, they can be generated indefinitely). Each map is given 10 LLM-generated descriptions, which are further divided into 5 long and 5 short descriptions. Long and short descriptions vary in detail and breadth. For example, the long and short descriptions for one map are:\n\u2022 Long Description \"A diverse terrain with four main areas, each featuring a combination of fungus and ground. The north west region is dotted with stone and ashes amidst more ground and fungus.\u201d\n\u2022 Short Description \"Four area division: ground, fungus, scarce stones and ash fragments\u201d\nWe evaluate these descriptions using the following five metrics: BLEU (Papineni et al. 2002), ROUGE-L (Lin 2004), METEOR (Lavie and Agarwal 2007), SPICE (Anderson et al. 2016), and CLIP Score (Hessel et al. 2022). These metrics provide us with information pertaining to whether our descriptions are diverse, human-like, and their semantic relations."}, {"title": "Evaluation of Generated Descriptions", "content": "We separately evaluate the effects of the generated long and short descriptions, including comparisons within each type and against human references. Additional comparisons can be found in the appendix.\nComparison within Generated Descriptions For each type of description, we selected the first description as a reference, calculated the scores against the other four descriptions, and averaged the results across all 3,000 maps (Table 2). Long descriptions outperformed short descriptions in almost all metrics, suggesting that long descriptions better align with human-like quality and capture more detailed and relevant information."}, {"title": "Evaluation of Generated Maps", "content": "Tables 4 and 5 show generated maps by the Moonshine distillation. Table 4 compares source maps from Brogue with generated maps, using LLM-generated descriptions. Table 5 compares source and generated maps using human-authored input. Specific tile types, locations, and ranges in prompts, are highlighted in red, blue, and orange, respectively.\nHuman Evaluation Models can be relatively correct in reflecting the specific types of tiles included in the description. Both models are better at capturing the description of the main area. When describing ... central area... or a vast area of..., models are capable of generating a wide area and containing the tiles specified. However, sometimes models are not able to respond to content in detail, such as a dot of..., scattered around, fews.... This may be because a proportion of synthetic descriptions describe only the main area and neglect to explain the details.\nAs a result, the model ignores these tiny details when it is trained. Although not all of the generated regions are connected, we can also observe that the model learns the hidden features of connection beyond the descriptions. The Five-Dollar Model is weak at generating diverse results compared to the Diffusion Model. Based on the fixed prompt, the result of the Five-Dollar Model will not change. Because this End-to-End model mainly learns mapping relationships, it does not consider the diversity of generations. The diffusion model, however, surpasses the Five-Dollar Model by generating various results that reflect the text condition and generate additional details.\nCLIP Score Evaluation We calculate the CLIP Score between the ground truth maps Brogue generated and the maps generated by the Five-Doller Model or Diffusion Model, respectively. The scatter plots are shown in Fig. 4 which exhibits a concentrated distribution of data points, showing a strong correlation of the similarity between the Brogue ground truth and the two generative models' predictions. The diffusion model demonstrates a high degree of clustering around the diagonal, indicating good predictive accuracy."}, {"title": "Conclusion", "content": "We introduce Moonshine, a method for distilling traditional PCGL algorithms into text-conditioned PGCML models using synthetic data generation. Using an automated map-labeling approach via Large Language Models, we minimize the need for human annotation a major obstacle for Text-to-game-Map models. Additionally, we provide our open-source dataset of dungeon maps with associated descriptions for use in further T2M research.\nBy combining traditional constructive PCG algorithms with the semantic understanding of large language models (LLMs), we enable the creation of diverse, human-like synthetic descriptions for existing PCG map data. We found that longer descriptions generated by LLMs capture more semantic information and align more closely with human references, whereas shorter descriptions correspond more with map image information in pretrained CLIP models. However, our fine-tuned CLIP model shows that longer descriptions integrate better with maps, suggesting that while a frozen model may perform better with short descriptions, longer descriptions are preferable for fine-tuning multimodal-aligned models.\nWe explored two T2M models -the Five-Dollar Model and the Diffusion Model- and found that while both generate maps correlated with their respective text descriptions, longer descriptions offer more accurate semantic reflections. The Five-Dollar Model, which utilizes a feedforward network to directly map text embeddings onto a map, achieved slightly higher overall scores compared to the Diffusion Model. However, it lacked diversity, producing relatively uniform results. In contrast, the Diffusion Model, which incrementally restores a noise map to a reconstructed map, shows greater potential for diversity.\nHowever, the limitations in the dataset size for training the Diffusion Model suggest that extending training over more epochs with a larger synthetic dataset could improve its performance. A larger dataset of human written descriptions would provide valuable information regarding the human likedness of the generated descriptions. Additionally, a study should be conducted to assess the use of the distilled Moonshine models with a designer trying to construct specific map outputs. We look only at a subset of procedural dungeon generation in terrain: items, enemies, and even stories all contribute to better model distillation.\nIn summary, Moonshine's automated distillation generation process can be an effective method for creating controllable content generators. For optimal results in description generation and model selection, we recommend prioritizing longer descriptions and considering the Diffusion Model for tasks requiring diverse outputs. Future research should explore more human written comparisons in both training data and validation. In addition, future research can explore ways to encode additional metadata into the map descriptions, such as items, enemies, and stories."}, {"title": "APPENDIX", "content": ""}, {"title": "Training Server Specifications", "content": "The server used for running analysis and training all models was a Linux server (Ubuntu 04.04) with a Intel Core i7-6850K 6-Core / 12 thread 3.60 GHz CPU, RTX 24GB VRAM GPU, and 32 GB of DDR5 ram. Training was done using CUDA 12.2."}, {"title": "Model Hyperparameters", "content": "Table 6 shows the hyperparameters for the diffusion model as well as the values tested."}, {"title": "Prompt Construction", "content": "We design our prompt into 4 sections: Setting, response format, examples, and rules. We use a markdown format with highlighted keywords as described in (He et al. 2024)\nSetting We describe to the LLM that it is a helpful data annotator and free to generate sentences outside of English grammar. Then we describe the problem statement, that it is a 2d text-to-map generative model and that it is given a map and meta-data information as input. We further describe how integers corresponds to tiles, and the format of metadata.\nResponse Format We ask the LLM to generate TEN good and precise descriptions of the map. Each description should be diverse, human-like, and creative and has no repeating descriptions.\nExamples We provide LLM few-shot examples of human-authored map descriptions. The LLM receives the meta-data for the map and then a series of 10 map descriptions in the described format. These human descriptions were gathered from a preliminary survey of human responses from our map images.\nRules We detail the rules of the LLMs generation. We give it some possible ways it can describe the map summarize the map's main content in a high-level description, use fluffy wording, and Be very blunt with no yapping. We then give it a series of hard rules designated by you WILL or you WILL NOT. We give it two rules to alter the size of the map descriptions. Half of the descriptions should be 1 to 3 sentences describing the map and the other half 5 to 15 words highly summarizing the map. We tell the LLM to describe all major areas of the map, keep your generation diverse, and not to use any specific pronoun in the meta-data. We then give it a list of words it is not allowed to use that we noticed it liked to repeatedly use in its descriptions: \"serene\", \"tranquil\", \"sprawling\", \"navigating\", \"labyrinth\", \"venture\", \"expanse\", \"subterranean\". Lastly, we again tell the model that it will not repeat any of your descriptions\"."}, {"title": "Finetuned CLIP Model for Desciption Length", "content": "In the histogram for the finetuned CLIP model shown in Fig. 5, the distribution reveals a notable improvement in the performance of long descriptions (blue) compared to short ones (orange). This enhancement indicates that finetuning has effectively adjusted the model's sensitivity to detail, allowing longer descriptions to often outperform or equal the scores of shorter descriptions.\n\u2022 Score Distribution: In the finetuned model, the distribution shows that long descriptions (blue) not only have a higher peak but also demonstrate an improved range compared to the pre-trained model. This indicates that after finetuning, long descriptions often score as well as or better than short descriptions.\n\u2022 Peak Comparison: The peak for long descriptions is higher and more pronounced than for short descriptions (orange). This shift suggests that the finetuning process has notably enhanced the model's ability to appreciate the detail and complexity in longer descriptions, aligning them more closely with the image content."}]}