{"title": "Explaining Spectrograms in Machine Learning: A Study on Neural Networks for Speech Classification", "authors": ["Jesin James", "Balamurali B. T.", "Binu Abeysinghe", "Junchen Liu"], "abstract": "This study investigates discriminative patterns learned by neural networks for accurate speech classification, with a specific focus on vowel classification tasks. By examining the activations and features of neural networks for vowel classification, we gain insights into what the networks \"see\" in spectrograms. Through the use of class activation mapping, we identify the frequencies that contribute to vowel classification and compare these findings with linguistic knowledge. Experiments on a American English dataset of vowels showcases the explainability of neural networks and provides valuable insights into the causes of misclassifications and their characteristics when differentiating them from unvoiced speech. This study not only enhances our understanding of the underlying acoustic cues in vowel classification but also offers opportunities for improving speech recognition by bridging the gap between abstract representations in neural networks and established linguistic knowledge.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of speech recognition has experienced remarkable progress, primarily driven by the widespread adoption of deep neural networks (DNNs) to train speech recognition models. The successful application of DNNs in speech recognition has led to significant advancements in various domains such as automatic speech recognition, voice assistants, and language understanding. Spectrograms, which provide a visual representation of the frequency content of a speech signal as it evolves over time, offer a promising alternative to conventional speech representations in the context of DNN-based speech recognition. Convolutional neural network (CNN) is a type of DNNs originally designed for image processing. However, CNNs have been successfully adapted to process spectrograms, capturing temporal dependencies and extracting meaningful features. Also, the field of computer vision using neural networks has progressed extensively with large networks such as ResNet [7], VGGNet [22], DenseNet"}, {"title": "2 Background and Related Work", "content": "Interpretability and Performance Improvements: Despite the impressive results achieved by these models, they are often regarded as black boxes, which limits researchers and practitioners from gaining a deep understanding of the specific features and patterns that contribute to their decision-making process. This lack of transparency and interpretability hinders further improvements in model performance.\nUnoptimised Model Training: The process of human annotation of spectrograms relies heavily on linguistic insights, allowing annotators to focus on specific aspects relevant to speech analysis. However, many DNNs used in speech recognition are not optimized using linguistics knowledge, leading to unoptimised model training. These neural networks often treat spectrograms as mere 'images', lacking a comprehensive understanding of the frequency axis and its significance in speech analysis. As a result, the model's ability to refine and optimize speech recognition is limited.\nThis study addresses the above knowledge gaps by investigating what neural networks learn from spectrograms. Focusing on two specific problems, namely vowel classification and voiced-unvoiced classification, we aim to unravel the black box nature of neural networks trained on spectrograms. The main contributions of this paper are:\n1. Designing experiments to explore the relationship between neural networks' learning from spectrograms and human interpretation of spectrograms.\n2. Employing visualising techniques to identify the regions of a spectrogram that are considered 'important' by DNNs in specific speech recognition tasks.\n3. Explaining the results to uncover insights into DNN's understanding of spectrograms in relation to human interpretation of the same.\nIn this study, interpretation, refers mapping an abstract concept, like a predicted class, into a human-understandable form such as images or texts. An explanation consists of interpretable features that contributed to a classification"}, {"title": "2.1 Spectrograms and their Significance in Linguistics", "content": "Spectrograms provide visual representations of speech signal frequencies over time. They are crucial in linguistics for analyzing acoustic properties of speech and offer detailed insights into temporal and spectral characteristics, allowing researchers to study articulatory gestures and acoustic cues.\nIn phonetics, spectrograms are instrumental in studying speech and its production [4]. They provide a tool to analyze the fine-grained details of articulatory movements, such as formant patterns and consonant releases [28]. Spectrograms allow linguists to investigate phonetic features like voicing, place and manner of articulation, and vowel quality, helping in the characterization and classification of speech sounds across languages [6,29]. There are even spectrogram reading competitions in conferences such as International Phonetics Association Conference and Australasian Speech Science and Technology Association Conference.\nIn phonology, spectrograms aid in understanding phonological processes and patterns [13]. They facilitate the identification of phonemic contrasts, allophonic variations, and sociolinguistic phenomena such as regional accents and dialectal variations [8]. Spectrograms also aid in the study of language variation, speech disorders, language acquisition, and cross-linguistic differences in phonetic patterns. While spectrograms have limitations in capturing prosody, intonation, and"}, {"title": "Voiced and Unvoiced Speech", "content": "Voiced speech is produced when the vocal folds vibrate, resulting in a periodic airflow. They include sounds such as vowels and voiced consonants. In spectrograms, voiced sounds exhibit a characteristic pattern as seen in Fig. 1(a) to (e). They display regular bands of energy, known as formants, which represent the resonant frequencies of the vocal tract. Voiced speech has a harmonic structure and exhibit sustained energy throughout their duration.\nUnvoiced sounds are produced without vocal fold vibration. They include voiceless consonants. The spectrograms of unvoiced sounds lack a clear harmonic structure and exhibit a more random and dispersed distribution of energy across a wide range of frequencies, as seen in Fig. 1(f). Unvoiced sounds are characterized by transient bursts of energy concentrated around the onset and release of the sound [20]."}, {"title": "Vowel Sounds", "content": "The combination of formant information and supplementary spectral features in spectrograms enables linguists to distinguish and analyze vowel sounds in their linguistic investigations. Formants correspond to the resonant frequencies of the vocal tract during vowel production. By observing the positioning, spacing, and relative intensity of these formants in the spectrogram, linguists can identify and categorize different vowel sounds. For example, in Fig. 1 (a) to (e), the vowels have distinct formant patterns. The first formant of /i/ is 385 Hz, /u/ is 400 Hz, /\u00e6/ is 800 Hz, /3/ in 590 Hz and /a/ is 710 Hz (formant estimation was done by observing the spectrograms and verified using Praat [2]). These values are within the frequency range expected for American English [17]. Differences exist for the second, third and fourth formants too, as seen by the formant bands at different frequencies in the Fig..\nSpectrograms offer insights into acoustic cues related to vowel articulation, including vowel duration and spectral shape. The shape of the spectral pattern in a vowel is influenced by the tongue position and the openness of the oral cavity, which determine the shape and configuration of the vocal tract. High vowels like /i/ and /u/ typically exhibit a more concentrated spectral shape with higher energy in the higher frequency range (See 1 (a) and (e)). This results from the tongue being positioned closer to the roof of the mouth, giving rise to a narrower constriction in the vocal tract and emphasizing higher-frequency resonances. The distinction in vowel duration can be observed using the spectrogram's horizontal axis. E.g., comparing the horizontal axis of Fig. 1 (a) and (c), we can observe /i/ is a longer vowel than /3/."}, {"title": "2.2 Explaining what Deep Neural Networks Learn", "content": "Researchers have employed various methods to gain insights into the learning process of deep neural networks [16]. Visualization techniques, such as highlighting the important areas in an image for a specific prediction, are commonly used [21,15]. Other approaches include sensitivity analysis, Taylor decomposition, and backward propagation techniques [16]."}, {"title": "3 Methodology", "content": "Class activation maps (CAMs) are visualization techniques to explain the decision-making process of deep neural networks, specifically in CNNs for image classification tasks [26,21,15]. CAMs provide valuable insights into the influential regions within an image that contribute to predicting a particular class label. By leveraging the gradients during the backward propagation process, CAMS capture the importance of spatial locations, highlighting the regions that significantly influence the final classification decision. These maps have proven to be effective in explaining the reasoning behind deep learning models, allowing researchers and practitioners to comprehend which areas of an image play a crucial role in making accurate predictions. CAMs offer a visual explanation by generating heatmaps that emphasize the relevant regions responsible for the classification decision.\nExplanations of deep learning models have been widely explored in various domains, including image classification, pattern recognition [1], and medical applications [9,19]. While activation mapping has been extensively applied in visual recognition tasks, its direct application to speech recognition is less common. One example is the use of CAM to explain the results of detecting oral cancer speech using a ResNet-based classifier with spectrograms as input [5].\nIn this study, we propose adapting the concept of CAMs to spectrograms with the aim of identifying the specific frequency regions that provide the most informative cues for speech classification tasks."}, {"title": "3.1 Database", "content": "The LJSpeech corpus ([11]) was selected as the database for training and evaluating the classification models in this study. This corpus consists of American English recordings by a speaker who identifies as female, along with text transcriptions, all sampled at 22,050 Hz. To align the recordings with their respective transcriptions WebMAUS [12] was utilized with American English option.\nWe limit the scope of the study to five vowels chosen to span over the American English vowel space [14]: /i/ (high, front), /\u00e6/ (low, front), /3/ (mid), /a/ (low, back) and /u/ (high, back). For each vowel, start and end times were identified, and the appropriate segments were extracted. The resulting dataset comprised a total of 79,269 single-vowel recordings.\nUnvoiced consonants were also extracted from LJSpeech corpus. The selected consonants were /p/, /t/, /k/, /f/, /s/, /t\u222b/, /S/, /\u03b8/. Only 79,269 instances of these consonants were included to match the number of vowels."}, {"title": "3.2 Experiment Design", "content": "The methodology encompasses three experiments:\n1. Vowel classification using all frequency components present in the speech signal\n2. Vowel classification focusing on the region containing formants, i.e. 4000 Hz\n3. Voiced vs unvoiced classification focusing on the region containing formants, i.e. 4000 Hz\nThe first experiment assesses the classification model's ability to identify relevant patterns in the spectrogram using speech in LJSpeech corpus considering all frequency components present, i.e. upto sampling frequency/2 = 11,025 Hz. Due to linguistics knowledge that the first four formants of the selected vowels have frequency less than 4000 Hz, the second experiment restricts the maximum frequency to 4000 Hz. This limitation narrows down the scope of visual representation provided to the network, allowing for a more focused analysis of vowel characteristics already used by linguists. The final experiment investigates the network's capability to accurately distinguish between voiced and unvoiced sounds. From linguistic knowledge, we know that the existence of fundamental frequency is a distinguishing feature between these two categories [18]. This experiment aims to examine whether the model would accurately identify fundamental frequency along with other relevant frequency of importance in differentiating between voiced and unvoiced sounds."}, {"title": "3.3 Classification Model Training", "content": "For this study, the ResNet-101 model was used. ResNet-101 is an enhanced version of the original ResNet [7] with 101-layers, addressing issues related to network depth and degradation by employing residual learning frameworks. ResNet-101 already pretrained on the ImageNet dataset [3] was subsequently fine-tuned using the vowel and consonant datasets mentioned earlier. Three instances of ResNet-101 were fine-tuned for this purpose, one for each experiment employing a 70/30 train-test split, each using softmax activation function to make the final decision. The hyperparameters remained consistent with the base ResNet-101 model, with a batch size of 32, learning rate of 0.0001, and Adam as the optimizer. The training process was conducted on a local machine equipped with two Nvidia RTX 3090 GPUs, each possessing 24GB of VRAM. Training time amounted to approximately 3.5 hours each for Experiment 1, 2 and 6 hours for Experiment 3."}, {"title": "3.4 Class Activation mapping (CAMs)", "content": "The three models were compared using CAMs, using the approach reported in [26]. CAMs provide insight into how the model's focus shifts when presented with different inputs while keeping the model architecture consistent. To generate the CAMs, first the spectrograms for each class were resized to 224 \u00d7 224. Then, the"}, {"title": "4 Results and Discussion", "content": "dot product between the final convolutional layer's feature for a class and the softmax weights of the output layer was calculated. The result was normalized and scaled to the same resolution. Finally, the resulting array was transformed into a heatmap. The heatmap was then superimposed on the spectrogram of a speech signal to obtain the CAMs in Figures 2, 3, 4. The code for training ResNet-101 and generating CAMs is made available\u00b3."}, {"title": "4.1 Vowel classification using all Frequency Components", "content": "Class Activation Map Analysis The CAM analysis provides insights into the discriminative properties of the considered vowels as seen in Fig. 2). Darker red regions in the CAM, indicating higher importance, were predominantly observed in the high-frequency region for three vowels: frequency > 5500 Hz for /i and /u/, but > 3500 Hz for /\u00e6/ (Fig. 2 (a), (b), (e)). Among these, the vowel /i/ is observed to be the darkest in the high-frequency region, followed by /\u00e6/and /u/. These observations suggest that high-frequency components play a crucial role in distinguishing these vowels. The presence of high energy in the high-frequency region of their spectrograms in Fig. 1 (a), (e) likely contributes to their distinctiveness in the CAMs. This is also expected as both /i/ and /u/ are high vowels having high energy in high frequency range.\nInterestingly, for the /\u00e6/ vowel in Fig. 2 (b), an additional region of importance was identified in the low-frequency range. Specifically, a strong band of frequencies between 500 and 1000 Hz exhibited a darker region in the CAM. This finding indicates the potential role of this frequency band in predicting the presence of the /\u00e6/ vowel and this region corresponds to the first formant of /\u00e6/, as seen in Fig. 1 (b) .\nIn contrast, the /3/and /a/ displayed similar characteristics in the high-frequency region (frequency > 5000 Hz) as in Fig. 2 (c) and (d), with no prominent dark red heatmap. However, in the low-frequency range of 500 to 1000 Hz, as seen in Fig. 2 (c) the vowel /3/ demonstrated a darker region that appeared in the latter half of the spectrogram. On the other hand, the vowel /a/ exhibited a similar trend, albeit with the frequency of interest slightly higher, ranging from 1000 to 3500 Hz.\nConfusion Matrix Analysis Analysis of the confusion matrix revealed a high overall accuracy > 96% as seen in the resulting confusion matrix in Fig. 2 (f). However, some minor misclassifications were observed. The vowel /a/ misclassified as /i/ was found to be the lowest, which can be attributed to the disjoint nature of their CAMs. The distinct patterns in the CAMs for /a/ and /i/ contribute to their accurate discrimination seen in Fig. 2 (a) and (d).\nMisclassifications were observed among the vowels/3/, /\u03b1/, and /u/. Some of /3/ were misclassified as /a/ and /u/, and similar misclassification were observed in /a/ v.s. /3/ and /u/ v.s. /3/. This similarity in misclassifications"}, {"title": "4.2 Vowel Classification focusing on Region of Formants", "content": "As seen in Fig. 3, the comparison between CAMs obtained by focusing on the region upto 4000 Hz and ones with all frequencies revealed some differences in the importance regions.\nFor /\u00e6/, the CAM remained similar between the two.\nFor vowel /3/, a similarity was observed in the CAM's shape. But the importance seen in the low frequency region for when considering all frequencies shifted upward. The red region shifted from 500 Hz - 1000 Hz in Fig. 2 (c) to 1500 Hz - 2500 Hz in Fig. 3 (c). On the other hand, for the vowel /u/ and /i/, a new region of importance emerged in the low frequency region around 500 to 1000 Hz.\nThe CAM for the vowel /a/ exhibited significant differences between the two cases. The region of importance appeared completely swapped between the original sampling frequency case and the frequency limited cases.\nDespite the subtle differences observed in the CAM between the original and frequency limiting cases, the resulting confusion matrices showed remarkable similarity. Misclassifications of /a/ as /i/ still resulted in the lowest number, possibly due to the presence of a region of high importance in /a/ as a strong band observed around frequencies < 1000 Hz in /a/ but not in /i/). However, similarities were observed in the high-frequency region of their CAMs.\nThe vowel /\u00e6/ and /a/ showed similar CAMs, and it was not surprising to find a slightly higher misclassification rate between /\u00e6/ and /a/ and vice versa in this investigation. Interestingly, /3/ and /u/ exhibited disjoint CAM,"}, {"title": "4.3 Voiced vs Unvoiced Classification", "content": "CAMs were utilized to investigate the region of importance in the spectrograms of voiced speech in distinguishing them from unvoiced speech. The results are shown in Fig. 4.\nThe CAMs revealed that the region of importance for unvoiced speech predominantly lies below 700 Hz. Notably, this frequency range corresponds to the general location of fundamental frequency, that are typically absent in non-voiced speech. Though this location is contrary to expectation as discussed in Section 3, the model's choice to focus on this disjoint spectral region indicates its discriminative power in accurately distinguishing between voiced and non-voiced speech. This observation is further supported by the excellent performance reflected in the confusion matrix as seen Fig. 3 (f), where the model achieved an accuracy exceeding 98%. Although there were some misclassifications observed between voiced and non-voiced speech samples, they were minimal."}, {"title": "Discussion", "content": "Observing all the CAMs, when all frequencies are available to the network to make decisions, the high vowels /i/ and /u/ used both the formant region (< 4000 Hz) and spectral shape characteristics to make decisions. The low-back /a/ and mid vowel /3/ use only the formant region, while /\u00e6/ used both the formant region (< 4000 Hz) and spectral shape characteristics to make decisions. When the frequency range was limited to 4000 Hz, majority of the vowels focused on the first and second formant frequency regions, which is less than 1500 Hz for /i/, /a/ and /\u00e6/. However, /u/ focused on a narrower low frequency < 1000 Hz, where this vowel's first and second formants fall. /3/ assigns high importance to a region around 2000 Hz, which corresponds to its third formant. Observing the CAMs in the voiced vs unvoiced detection, it is clear that the region of importance corresponds to where the first four formants of voiced speech would lie in.\nOverall, this analysis revealed that the neural network which was not pre-trained on spectrograms, but only fine-tuned on them are focusing on formants in most cases to make their decisions. This is similar to what linguists would do. However, high frequencies are also considered in the decision making in some cases, which may not be needed depending on the task at hand. These high frequencies maybe a result of noise in the database. Based on this observation, there is scope to inform deep learning models on the region of interest to consider based on linguistics knowledge."}, {"title": "5 Conclusion", "content": "In conclusion, this study explored the interpretation of spectrograms in machine learning for speech classification. The prediction results demonstrated higher accuracy in vowel classification models trained using ResNet-101. Insights were"}]}