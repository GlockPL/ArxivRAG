{"title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec", "authors": ["Uzma Mushtaque"], "abstract": "Transformer based models are increasingly being used in various domains including recommender systems (RS). Pretrained transformer models such as BERT have shown good performance at language modelling. With the greater ability to model sequential tasks, variants of Encoder-only models (like BERT4Rec, SASRec etc.) have found success in sequential RS problems. Computing dot-product attention in traditional transformer models has quadratic complexity in sequence length. This is a bigger problem with RS because unlike language models, new items are added to the catalogue every day. User buying history is a dynamic sequence which depends on multiple factors. Recently, various linear attention models have tried to solve this problem by making the model linear in sequence length (token dimensions). Hydra attention is one such linear complexity model proposed for vision transformers which reduces the complexity of attention for both the number of tokens as well as model embedding dimensions. Building on the idea of Hydra attention, we introduce an efficient Transformer based Sequential RS (HydraRec) which significantly improves theoretical complexity of computing attention for longer sequences and bigger datasets while preserving the temporal context. Extensive experiments are conducted to evaluate other linear transformer-based RS models and compared with HydraRec across various evaluation metrics. HydraRec outperforms other linear attention-based models as well as dot-product based attention models when used with causal masking for sequential recommendation next item prediction tasks. For bi-directional models its performance is comparable to the BERT4Rec model with an improvement in running time.", "sections": [{"title": "Introduction", "content": "For most online services, recommendations allow an effective strategy for exposing users to relevant items. In most scenarios this relevance is computed using user's past interactions with various items. User preferences evolve over time. The sequence in which items are bought reveals a buying pattern for every user. For example, a user who bought a computer might need to buy accessories for the computer. Identifying temporal semantics of user interactions in a sequence and using that to make future predictions is termed as long-term sequential recommendation (LSR). Various models have been used to capture sequential dynamics of user behavior [1], [2]. Most of these models predict the next item given a sequence of historical user interactions. Recurrent neural networks (RNNs) and their variants have been widely used for sequential recommendations. These models encode user-item interactions in a unidirectional manner. This encoding/hidden representation is then used to predict the next item or items [3].\nAttention [4] based approaches using bi-directional networks like BERT [5] exhibit superior performance by learning encodings bi-directionally. Originally these models were introduced for natural language processing (NLP) related tasks and time series problems. However, given the similarity of LSR with NLP problems, variants of NLP models are used in recommender system applications [2]. These models are modified to specifically solve recommender system problems of next item prediction and next basket prediction. Despite the similarity between NLP tasks and LSR, there are some fundamental differences. The general idea of these models is to consider items as words in a sequence like NLP models and represent each item as a token. The basic building block of these models is the dot-product attention mechanism that calculates the attention matrix for different items based on their relevance in the sequence. One limitation of these models is the complexity of the dot product operation for sequential recommendation tasks which is quadratic in the number of tokens(items). This becomes an issue when the sequence length (N) is much greater than the item embedding size (d). Several approaches have been proposed to deal with this issue such as Fixed Patterns (FP), Combination of Patterns (CP), Learnable Patterns (LP), Low-Rank Methods, Kernels etc. [6]. Various transformers with linear complexity have also been introduced [7][8]. When dealing with LSRs these methods exhibit low accuracy [9]. The LinRec model [10] addresses these shortcomings by introducing L2-normalized linear attention for long-term sequential recommendations. This model changes the order of operations to calculate attention. Flash attention model demonstrated superiority over these models even using a few thousand tokens and achieving reasonable performance [12].\nThere is a trade-off between efficiency and accuracy which is not specifically studied in the RS domain. Unlike NLP, LSR poses an additional modelling challenge because unlike language models, new items (movies, products etc.) are introduced almost every day which leads to additional tokens (item representations) being added to the model. There is a need for more efficient and accurate models that are linear in tokens as well as embedding (model) dimensions. Most of the 'linear' attention models trade computation across tokens for computation across embedding dimensions[11]. To the best of our knowledge, most of the linear models focus on model complexity with respect to either sequence length(N) or model dimensions(d). To address these research gaps, we introduce a novel transformer-based model called HydraRec that is linear in both sequence length and model dimensions. HydraRec significantly"}, {"title": "Background", "content": "In this section we discuss the dot-product attention mechanism widely used in Transformer based models [4]. Next, we describe the sequential recommendation problem and conduct a literature review of existing work."}, {"title": "Dot Product Attention", "content": "The most significant part of a Transformer model is the scaled dot-product attention [4], which captures the context of a given token w.r.t other tokens in the sequence. If the length of a sequence is denoted by N and d is the dimensionality of each token. The scaled dot-product attention (A) is given by:\n$A(Q, K, V) = softmax\\frac{Q K^T}{\\sqrt{d}}V$  (1)\nHere $X \\in \\mathbb{R}^{N \\times d}$ is the input matrix. Weight matrices $W_Q, W_K, W_V$ are learnt from the training process and $Q, K, V \\in \\mathbb{R}^{N \\times d}$ are query, key and value matrices. The $softmax$ function is applied row-wise to the fraction A is referred to as the attention matrix. The dimensionality of A is fixed and the attention information can be transferred to the sequence easily. The disadvantage of the $Q K^T$ calculation is the complexity associated with it which is quadratic in sequence length (number of tokens). This is an issue for problems involving LSR given the memory and time complexity associated."}, {"title": "Sequential Recommendations", "content": "Sequential Recommendation Systems (SRS) are the sequence of interactions a user makes in a chronological order over a specified period. SRS can be short or long. In general, if the item embedding is denoted by d and the number of historical interactions is N, then the ratio N/d greater than 1.5 is considered a long sequence [10]. In this study we focus on LSR, and the computational cost associated with the models that consider SRS. We do not restrict our experimentation to the ratio restriction of 1.5 because the goal of this study is to establish the effectiveness of HydraRec for all types of SRS tasks."}, {"title": "Related Work", "content": "SRS have been modelled by capturing user historical interactions via Markov chains (MCs) [16]. Later many hybrid approaches involving MCs were also introduced to solve the SRS problem [17] [18]. A more recent trend is to use sequential deep learning models like Recurrent Neural Networks (RNNs) and their variants for SRS. User interaction sequences can be used to model user behavior by using models like Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) [19]. The general idea of these models is to efficiently combine past observations and create vector representations. Some of the recent works in this category are [20], [21], [22], [23]. In addition to RNNs, there are other deep learning models like Convolution Neural Networks (CNNs) and MLP networks that have been used to solve the problem of SRS [24], [25].\nTransformer based models that use attention mechanism are found to be extremely effective in modelling sequential data [4][26]. Some methods have used attention with other models to model session-based recommendations [1]. Deviating from the complete transformer architecture are encoder [5] only and decoder only models [27].These models have been used for modeling SRS effectively. Most transformer models in NLP applications calculate dot product self-attention via causal masking. Many such models have found application in recommender system literature [2] [28]. Bi-directional frameworks like BERT[5] calculate the dot product bi-directionally taking context from both ends into account. One BERT based model used for SRS is BERT4Rec [13] which is an encoder only model and uses a multi-head self-attention mechanism. A significant limitation of these models when applied to long SRS is the computational and memory complexity. The dot product operation is quadratic in the number of tokens (vocabulary size in NLP models and number of items in RS models).\nThis is a bigger problem for RS due to the ever-growing catalogue size. Linear attention models have tried to solve this problem via a decomposable kernel [7]. LinRec [10] applies linear attention to SRS problem and achieves a complexity that is linear in the number of tokens but quadratic in embedding dimensions.\nMost retail and/or subscription platforms have items added to their catalogue every day and therefore user interaction sequences grow over time. For extremely large sequences it is required that the models are extremely efficient without losing semantics behind sequential interactions. Temporal dependencies are also modeled by vision transformers [29] and the goal is to maintain efficiency. One such vision transformer model is Hydra Attention model [11] that takes the linear attention calculation a step further by maximizing the number of attention heads resulting a model that is linear in token and embedding dimensions. The paper also introduces various kernels and discusses the effectiveness of the choice of various kernels in calculating linear attention."}, {"title": "Methodology", "content": "In this work, we build both unidirectional and a bi-directional encoder-based model that works on the principle of Hydra attention called HydraRec. Hydra attention is based on the idea of increasing the number of heads in multi-headed attention and its impact on the overall performance of the model. We build HydraRec on linear attention (like LinRec) and experiment with other kernels as well as number of heads. The original paper suggests replacing self-attention with different strategies. HydraRec only replaces SoftMax self-attention with Hydra attention. Additionally, our goal is to compare the performance of linear mechanisms when used for the SRS problem, therefore we keep the base model as BERT4Rec and experiment with various linear attention mechanisms."}, {"title": "Methodology", "content": "In this section we state the LSR problem mathematically, describe the HydraRec model, its architecture and discuss the training process."}, {"title": "Problem Statement", "content": "For any recommendation problem, the goal is to match a given set of users $U = \\{U_1, U_2 ... U_u\\}$ to a given set of items $V = \\{V_1, V_2 ... V_{|v|}\\}$. The SRS problem also involves user interaction history in chronological order. Let the interaction sequence for any user $u \\in U$ given by $Seq_u = [v_1, ..., v_i, ..., v_{n_u}]$. Here $n_u$ is the interaction sequence length. More formally, the SRS problem is to predict the item that the user will interact with at time step $n_u + 1$. To solve this problem, we model the following probability for all possible items $v \\in V$: $p(v_{n_u+1} = v|Seq_u)$."}, {"title": "Model Architecture", "content": "In the current work, we build the HydraRec model using Bert4Rec architecture only. The attention computation can be extended to any other transformer-based model. HydraRec model comprises of L bi-directional transformer layers capable of sharing information across all positions (for item sequences) from the previous layer along with the transformer layer. We experiment with unidirectional model too by modifying the attention masking strategy by effectively creating a causal (left to right) attention mask."}, {"title": "Transformer Layer", "content": "Like many other transformer-based models, HydraRec computes the hidden representation $h_i$ for each token i given sequence length N. Next, these hidden representations are stacked together into a matrix $H^l \\in \\mathbb{R}^{N \\times d}$. The attention function from equation (1) is computed on all positions simultaneously. These hidden representations are an input to the transformer layer which consists of two sub-layers: Multi-Head Self-Attention (MHSA) layer and Position-Wise Feed Forward (PFF) Network Layer. MHSA is an integral part of transformer architecture where each head (h) creates its own attention matrix. This can be represented as:\n$MH(H^l) = [h_1; h_2; ... ; h_{head}]W^O$\nwhere $h_i = Attention(H^lW^Q_i, H^lW^K_i, H^lW^V_i)$"}, {"title": "The HydraRec Model", "content": "Vision transformers have used different number of heads to better scale image related problems [11]. In the original attention model [4], each head H has its own subset of features d/H, therefore equation (1) can be re-written as:\n$A(Q_h, K_h, V_h) = softmax(\\frac{Q_h K_h^T}{\\sqrt{d}})V_h \\forall V \\in \\{1, ..., H\\}$  (4)\nHowever, this does not impact the overall complexity of the model with dot-product attention. On the other hand, with Linear attention adding the number of heads decreases the number of operations because the complexity becomes $O(\\frac{N d^2}{H})$. Using O(Nd\u00b2) and equation (3) becomes:\n$A(Q_h, K_h, V_h, \\phi) = \\phi(Q_h)(\\phi(K_h)^TV_h) \\forall h \\in \\{1, ..., H\\}$   (5)\nIn (5), $Q_h, K_h, V_h$ are column vectors with dimensionality $\\mathbb{R}^{N \\times 1}$. Vectorizing the operation across heads gives hydra attention:\n$Hydra(Q, K, V, \\phi) = \\phi(Q) \\sum_{i=0}^{N} \\phi(K)^T V^T$  (6)\nThe multiplication $\\phi(K)^T V^T$ is elementwise multiplication. The final summation multiplication with $\\phi(Q)$ is also elementwise. Hydra attention is different from the original scaled dot-product attention because it creates a more generic feature vector for the entire input sequence with the $\\sum_{i=0}^{N} \\phi(K)^T V^T$ operation. Multiplication with $\\phi(Q)$ is equivalent to filtering the relevance of each token (item) in the sequence. This is a more generic approach towards modeling contextual information within a sequence.\nThe complexity of this model is $O(\\frac{N d^2}{H})$. Here the idea is that increasing the number of heads decreases the number of operations. If H = d, then the model complexity becomes O(Nd). This is true for both time and space complexity. We incorporate this attention mechanism in one of the well-known encoder based architectures specifically used for the LSR problem \u2013 BERT4Rec [13] and call this"}, {"title": "Model Training", "content": "The overall model architecture uses PFF which is a concatenation of Feed forward Network (FFN) applied to the output of the attention layer described above. The FFN layer comprises of two affine transformations with a GELU (Gaussian linear unit) activation in between. Transformer layers are stacked with residual connection around every two sub-layers. Like BERT4Rec, the output layer L in HydraRec receives the final output $H^l$ for all items of the input sequence. We train the model on a cloze task [13] i.e. in each sequence a randomly masked item must be predicted. To produce a distribution over the target items a 2-layer FFN with GELU activation is used again.\nWe focus our analysis on encoder-based models, specifically BERT4Rec architecture. The original BERT [5] model was trained for two tasks (next item/token prediction and next sentence prediction), however we narrow our training to the task of next item prediction only similar to [13]. Training bi-directional models for RS tasks can lead to the prediction task becoming trivial as the model will not learn anything useful, therefore we adopt the strategy outlined by [13] where some percentage of the sequence items are masked (just like the masked language model in BERT [5]) and used as labels for the learning task. Therefore, the loss for each masked sequence is the negative log-likelihood of the masked targets. Additionally, we consider two"}, {"title": "Experiments", "content": "In this section we describe the experiments conducted with real-world datasets to establish the overall performance of HydraRec. In summary we answer the following research questions:\nRQ1: How is the overall accuracy of HydraRecUni and HydraRecBi in comparison to scaled dot-product attention in a transformer architecture e.g. BERT4Rec?\nRQ2: How do other linear attention models compare with HydraRec in accuracy?\nRQ3: How does the runtime of HydraRec compare to other models?\nRQ4: How does changing the number of heads affect the accuracy of HydraRec?\nHere we define accuracy as the value of the two-evaluation metrics for the validation set. Given that this model builds on a variant of Linear attention, our focus is to compare its effectiveness for the LSR task against prominent linear attention-based models. BERT4Rec (with scaled dot-product attention) is the baseline model. Other models are built on the same architecture with the only update in attention mechanism calculations. We test both variants of HydraRec for accuracy. Theoretical complexity of HydraRec can be linear in embedding and sequence dimensions under certain cases. To analyze this experimentally, we capture the system runtime for training of each of these models."}, {"title": "Datasets", "content": "We evaluate the proposed set of models on three real-world representative datasets:\n1) ML-1m: MovieLens (1 million) - This dataset has 1 million movie ratings and is a popular choice for recommender system problems.\n2) ML-20m: MovieLens (20 million) - This is the 20 million ratings version of the MovieLens data.\n3) Beauty: Amazon Beauty Rating - This is a ratings dataset for beauty related products sold on the Amazon Website.\nMovie Lens datasets have longer sequence length for each user. Beauty dataset has shorter sequences in general. A similar pre-processing strategy is used for all three datasets. For each user we create a sequence of items based on the timestamp of the rating (chronological order). Because some of these sequences can be extremely large, we put an upper limit to the sequence length as a hyperparameter. We experimented with different sequence lengths. Padding with zeros was used for sequences shorter than the maximum length."}, {"title": "Task and Evaluation Metrics", "content": "The task at hand is leave-one-out evaluation (next item prediction) like [13][24]. For every user the last item is used for testing. As described in section 3.3 the training is done for a cloze task with some items randomly masked (with an input masking probability of 10 percent for all experiments). Because an LSR task involves the next item prediction, a special token for 'mask' is attached at the end of each sequence that the model predicts. The two variants of HydraRec use two different strategies for masking as described above. We use two well-known evaluation metrics that are used in RS research [30] : (1) Normalized Discounted Cumulative Gain (NDCG) for top k (10), 2) Hit Ratio (HR) for top k (10) recommendations. A higher value indicates a better performance for both these evaluation metrics."}, {"title": "Implementation Details", "content": "All parameters are initialized using Gaussian distribution. For HydraRec, we set the hyper-parameters as suggested by [31], including Transformer Layer as 2, attention head equal to 8 and inner FFN layer as 256. We experimented with embedding dimension of 16, 32, 64 and 128 and sequence lengths of 10, 20, 30, 50, 80 and 100. For HydraRec we also experimented with the number of heads equal to the number of embedding dimensions ranging from 16,32,64, 256. The maximum number of training epochs was 200. We report the best results on a validation set (90-10 split) captured for each configuration of the hyperparameters. All experiments are conducted on L4 GPU."}, {"title": "Performance Comparison", "content": "In this section we present the results of our experiments. Each experiment was conducted with a batch size varying from 16 to 256, maximum sequence length from 16 to 100 and number of training epochs ranging from 32 to 200. Here the maximum sequence length is a representation of user buying/watching sequence. This has greater relevance for the next item prediction tasks because user buying history impacts what the customer selects next. We are experimenting with cloze tasks. The results for each model under all hyperparameters were recorded and the best values are presented. Additionally, we are calling all models using the decomposition technique (equation 5) as linear models (LM) followed by a number to maintain order. HydraRec (equation 6) is a model that is linear in token dimensions and also achieves linearity for some special cases (number of heads becomes equal to embedding dimensions). We experimented with the original Hydra attention model"}, {"title": "Discussion", "content": "HydraRec is a transformer-based attention model that is linear in sequence and model complexity. This model is used for the next item prediction task of the LSR problem. When comparing the model with existing linear models along accuracy, HydraRecUni outperforms every model. The HydraRecBi variant gives comparable performance to other linear models and sometimes a better performance than dot-product attention. The key advantage of HydraRec lies in complexity improvement and hence a significant reduction in runtime for problems that require longer training times. This is particularly beneficial for problems involving longer sequences, for example when a lot of customer data on an online shopping website (like Amazon) is used to recommend items for next purchase. Additionally, data sparsity is a known issue for recommender system problems leading to lower accuracy and increased runtime. HydraRecUni is a unidirectional model with linear attention. For problems that require the temporal context of a customer buying pattern but can compromise on some contextual information by only taking unidirectional sequence into consideration, this model can be useful. However, for cases where efficiency matters more than accuracy and also for models involving sparse datasets that need to be"}, {"title": "Conclusion and Future Work", "content": "In this study we developed a novel sequential recommendation model called HydraRec, to be used for the LSR problem. The model has computational complexity which is linear in sequence length and for certain cases it can be linear in embedding dimensions. Linearity is achieved via a decomposition technique of the attention model which leads to a model linear in sequence length and further by increasing the number of heads linearity in embedding dimensions can be achieved. We tested the performance of this model by incorporating it in BERT4Rec architecture. Two variants of HydraRec capture two modeling scenarios of the LSR problem. HydraRecUni utilizes the sequence of item selected by a user to predict the next item by using causal masking for future items. HydraRecBi, on the other hand, uses bi-directional context calculations similar to the BERT4Rec model.\nHydraRec is theoretically and experimentally more efficient, as is evident from the actual runtime recorded. The accuracy of Hydra attention as compared with other Linear attention mechanisms is better when used for the LSR task. If the task involves causal masking, then HydraRec outperforms dot-product based attention model as well. For a bi-directional model its accuracy is comparable and sometimes better than BERT4Rec. A significant achievement of this model is the saving in runtime which becomes prominent as the number of epochs increases. This points to the cases when model accuracy is important hence training time is more. Under such scenarios HydraRec can be more useful than other linear models. One important constraint of all the models used in this study is that they are used under an encoder transformer architecture. An important addition to this work could be to experiment with decoder architectures as well to better understand the overall effectiveness of Hydra attention in recommender-systems. We experimented with sequence length and recorded results that were the best across all evaluation metrics. A dedicated ablation study for hyperparameters like the impact of sequence length using all datasets from different"}]}