{"title": "Improving Trip Mode Choice Modeling Using Ensemble Synthesizer (ENSY)", "authors": ["Amirhossein Parsia", "Melina Jafari", "Sina Sabzekar", "Zahra Amini"], "abstract": "Accurate classification of mode choice datasets is crucial for transportation planning and\ndecision-making processes. However, conventional classification models often struggle to ad-\nequately capture the nuanced patterns of minority classes within these datasets, leading to sub-\noptimal accuracy. In response to this challenge, we present Ensemble Synthesizer (ENSY) which\nleverages probability distribution for data augmentation, a novel data model tailored specifically\nfor enhancing classification accuracy in mode choice datasets. In our study, ENSY demonstrates\nremarkable efficacy by nearly quadrupling the F1 score of minority classes and improving overall\nclassification accuracy by nearly 3%. To assess its performance comprehensively, we compare\nENSY against various augmentation techniques including Random Oversampling, SMOTE-NC,\nand CTGAN. Through experimentation, ENSY consistently outperforms these methods across\nvarious scenarios, underscoring its robustness and effectiveness.", "sections": [{"title": "1. Introduction", "content": "In travel behavior research, understanding, and predicting travel mode choices have a pivotal role in the travel\ndemand forecasting process. Factors influencing travelers' mode choices range from tangible elements like distance\nand travel time to intangibles like safety, reliability, and aesthetics (Birr, 2018). As advancements in technology usher in\nnew modes of transportation, such as ride-sourcing, autonomous vehicles, and electric scooters, and with the increasing\navailability of diverse travel options, data and information sources will be altered. This calls for an evolution in the\nfield of mode choice prediction modeling.\nTraditionally, mode choice prediction has been dominated by discrete choice models (DCMs) rooted in the\nprinciples of random utility maximization. While these models provide interpretability, they necessitate extensive\nefforts in specification and estimation, often requiring segmentation of travel markets based on various explanatory\nvariables (Hensher, Rose and Greene, 2015), (De Ortuzar and Willumsen, 2011), (Richards and Zill, 2019), (Hillel,\nBierlaire, Elshafie and Jin, 2021). Enter machine learning (ML) algorithms, which present a paradigm shift by avoiding\nrigid assumptions about data structures. ML techniques have entered the realm of mode choice modeling, offering a\nmore flexible and efficient approach to understanding the nuances of travel behavior (Hillel et al., 2021), (Wang, Mo,\nHess and Zhao, 2021), (Wang, Mo and Zhao, 2020), (Pulugurta, Arun and Errampalli, 2013).\nIn transportation research, datasets often exhibit a skewed distribution of classes, with some modes being\nsignificantly more prevalent than others. Thus, the challenges of imbalanced datasets in the context of mode choice\nprediction present a critical obstacle for accurate model performance. As ML applications become more prevalent,\nthe issue of class imbalance becomes more pronounced, with traditional methods leaning towards the majority class\nand neglecting minority classes (Mohammed, Rawashdeh and Abdullah, 2020). To address this, several methods have\nemerged, including data augmentation techniques such as Synthetic Minority Over-sampling Technique (SMOTE)\n(Chawla, Bowyer, Hall and Kegelmeyer, 2002) and Generative Adversarial Networks (GANs) (Aziira, Setiawan\nand Soesanti, 2020). Moreover, due to the nature of mode choice problem datasets, handling categorical features is\ninevitable; which is more challenging than merely numerical datasets.\nEach of the data augmentation methods aims to balance the class distribution, enhancing the robustness and\naccuracy of machine learning models in predicting travel behavior and contributing to a more comprehensive\nunderstanding of the diverse aspects of mode choice in modern transportation contexts (Rezaei, Khojandi, Haque,\nBrakewood, Jin and Cherry, 2022). However, there is no one-size-fits-all solution to overcome class imbalance\nobstacles. Researchers have put extensive efforts into finding a solution, with most efforts proving fruitless (Rezaei\net al., 2022), (Diallo, Lozenguez, Doniec and Mandiau, 2022), (Li, Wang, Wu, Chen and Zhou, 2021), (Chen and\nCheng, 2023). Therefore, a new method is proposed in this paper to overcome the failure in improving the accuracy of\nminor classes in mode choice prediction. The method called ENSY, utilizes the probability distribution of the existing\ndataset to generate synthetic data points. Subsequently, a classifier assesses the generated data points for quality, and\nfinally, the original dataset is augmented by incorporating valid synthetic data points.\nIn the course of this paper, the London Passenger Mode Choice dataset (Hillel, Elshafie and Jin, 2018) and the Korea\nTransport Database (KTDB, 2016) have undergone examination, and the aforementioned strategy is employed. The\nresults from ENSY are compared to the results from previous data augmentation methods. Extreme Gradient Boosting,\nRandom Forest, and Neural Networks are employed and the performance of models on both raw and augmented datasets\nis systematically compared."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Research on Mode Choice Classification", "content": "Travel mode choice modeling is one of the most studied areas in travel behavior research and it is a crucial step\nin the travel demand forecasting process. The process of travel demand forecasting consists of the four-step model,\nincluding trip generation, trip distribution, modal split, and trip assignment (Kadiyali, 2013).\nAlthough several methods have been utilized for mode choice modeling, the field has been dominated for many\nyears by the application of DCMs, including the Multinomial Logit (MNL), Nested Logit (NL), and Mixed Logit\n(MXL) models (Zhao, Yan, Yu and Van Hentenryck, 2020). (McFadden, 1974) pioneered the use of the MNL model\nin travel behavior modeling. In recent years, ML methods have gained traction in mode choice modeling. Notable ML\ntechniques applied include Decision Trees, Neural Networks, and Support Vector Machines, and by combining some\nML concepts such as ensemble methods, the prediction power has been enhanced (Hillel et al., 2021), (Wang et al.,\n2021), (Wang 2019), (Pulugurta et al., 2013), (Zhang, Ji, Wang and Yang, 2020), (Rasouli and Timmermans, 2014).\n(Wang and Ross, 2018) compared the performance of Extreme Gradient Boosting (XGB) with traditional MNL\nmodels. Interestingly, while both XGB and MNL models demonstrated high prediction accuracy for travel mode\nchoices, challenges arise when predicting choices involving cycling, which constitutes a small share of the dataset.\n(Sekhar, Minal and Madhu, 2016) explored the efficacy of a Random Forest (RF) Decision Tree mode choice model,\nshowcasing its superiority over MNL models. This RF model not only achieved higher prediction accuracy but also\ndemonstrated efficiency on large databases, emphasizing benefits such as accurate classification, scalability to large\ndatasets, and internal error estimation.\nIn a study by Richards and Zill (Richards and Zill, 2019), the effectiveness of machine learning techniques in\naddressing the mode choice problem was assessed, with Gradient Boosting emerging as the top performer among the\nmodels considered. However, across this research and others, the issue of minority classes posed challenges due to\nsevere class imbalance.\nConsequently, XGB appears to outperform numerous ML classifiers, with the impact of class imbalance evident\nin predicting smaller shares, presenting a challenge that has seen limited efforts for resolution."}, {"title": "2.2. Approaches to Handle Class Imbalance in Mode Choice Prediction", "content": "As mentioned before, the class imbalance problem in ML occurs when skewed class distributions hinder classifiers\nfrom effectively learning information in minority classes, resulting in suboptimal performance. Four mainstream\nmethods are explored to balance class distribution (Majeed and Hwang, 2023)."}, {"title": "2.2.1. Data-Level Methods", "content": "These methods involve undersampling the majority classes or oversampling the minority classes, with one approach\nbeing a combination of both to enhance learning and generalization (Chaipanha and Kaewwichian, 2022), (Menardi\nand Torelli, 2014). (Chen and Cheng, 2023) systematically investigated the fusion of statistical and ML methods, with\nsix Over/Under-Sampling (OUS) techniques. The examination revealed that while prediction models using the original\ndataset demonstrated superior aggregate prediction performance, the majority of OUS techniques proved advantageous\nin enhancing the disaggregate prediction performance of machine learning models. Notably, Random Under-Sampling\n(RUS) and oversampling techniques exhibited significant improvements in predicting minority modes while preserving\noverall prediction performance and model interpretability."}, {"title": "2.2.2. Algorithm-Level Methods", "content": "These methods modify ML model workflows, employing techniques like guiding Support Vector Machine\nhyperplanes and designing objective functions (Batuwita and Palade, 2010), (Edward, 2003). (Qian, Aghaabbasi, Ali,\nAlqurashi, Salah, Zainol, Moeinaddini and Hussein, 2021) presented a novel approach to address imbalanced mode\nchoice data using an adjustable kernel-based Support Vector Machine (SVM) classification model. The proposed\nmethod employs the likelihood-ratio chi-square test and weighting measures for optimal kernel function selection,\nincorporating a transformation function to expand class limits and rectify irregular boundaries. Notably, the SVM with\nAdjustable Kernel model significantly enhances prediction accuracy, improving from 82.33% to an impressive 99.81%\nfor the largest sample size. However, for the smallest category (the motorcycle/moped), the developed models failed\nto improve the accuracy."}, {"title": "2.2.3. Cost-Sensitive Methods", "content": "These methods minimize misclassification costs, assigning higher costs to the minority classes (Wang and\nJapkowicz, 2010). (Kim, 2021) assessed that RF and XGB exhibit superior performance compared to Artificial Neural\nNetworks. Despite efforts to address challenges related to imbalanced datasets by applying class-specific weights during\nML model training, all models displayed suboptimal performance in predicting the minority class, specifically the\nchoice of cycling."}, {"title": "2.2.4. Ensemble Methods", "content": "These methods involve training multiple classifiers to enhance accuracy in imbalanced scenarios (Song, Wang,\nYe, Zaretzki and Liu, 2023). (Wang 2019) established an empirical benchmark for predicting travel mode choice\nby employing 86 machine learning classifiers across 14 model families. The analysis reveals that ensemble models,\nspecifically Boosting, Bagging, and RF, outperform other classifiers. Notably, Bagging attains the highest prediction\naccuracy among the 14 model families. Although, using ensemble learning led to higher accuracy in both major and\nminor classes rather than non-ensemble classifiers, the class imbalance problem remained unsolved.\nTo sum up, a vast amount of effort has been put into the area of handling imbalanced mode choice datasets. However,\nnone have reached a comprehensive and solid solution to the problem at hand, which calls for robust and innovative\napproaches."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Ensemble Synthesizer (ENSY)", "content": "In this section, we describe the proposed data augmentation approach designed to address the class imbalance in\nthe dataset. The methodology combines two stages which are the generator and the validator. The generator produces\nsynthetic instances for each class using a probabilistic approach and a classification-based validator decides whether\nthe generated sample should be discarded or used for augmentation. The objective is to generate high-quality synthetic\nsamples for the classes while ensuring that only instances recognized as belonging to said classes are added to the\ntraining data."}, {"title": "3.1.1. Generator", "content": "Fig. 1 shows the flowchart of the ENSY method. As mentioned, the generator leverages probability distributions\nto create synthetic instances. Synthetic sample generation occurs independently for each class, and the probabilities\nare derived from the distribution of the remaining classes, i.e. to generate a synthetic sample for class i in D, instances\nbelonging to class i are excluded from D and the resulting dataset D', is then passed through the generator. Furthermore,\neach feature is also synthesized separately nd then the generated value for each feature is concatenated to create one\nrow of synthetic instances.\nNumerical Features To discern the underlying patterns of each numerical feature efficiently, a Gaussian Mixture\nModel (GMM) is fitted to the values of the feature in dataset D'. A GMM is a probabilistic model that represents a\nmixture of several Gaussian (normal) distributions. Mathematically, a GMM is defined as follows:\nFor our one-dimensional case, the probability density function (PDF) of a GMM with K components is given by:\n$p(x) = \\sum_{i=1}^{K} \\pi_{i} \\cdot N(x/\\mu_{i}, \\sigma_{i}^{2})$\nWhere:\n\u2022 x is the variable (feature) being modeled.\n\u2022 $\\pi_{i}$ is the weight of the i-th component, representing the probability of choosing the i-th Gaussian distribution.\nThe weights $\\pi_{i}$ are non-negative and sum to 1. They represent the contribution of each Gaussian component to\nthe overall distribution. Larger weights mean that the corresponding Gaussian component has a more significant\ninfluence.\n\u2022 $N(x/\\mu_{i}, \\sigma_{i}^{2})$ is the i-th Gaussian distribution with mean $\\mu_{i}$ and variance $\\sigma_{i}^{2}$, which is given by:\n$N(x|\\mu_{i}, \\sigma_{i}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{i}^{2}}} exp(-\\frac{(x - \\mu_{i})^{2}}{2\\sigma_{i}^{2}})$\nThe process of fitting a GMM involves estimating the parameters $\\pi_{i}$, $\\mu_{i}$, and $\\sigma_{i}^{2}$ from the data. This is often done\nthrough the Expectation-Maximization (EM) algorithm (Sugiyama, 2016), where the algorithm iteratively refines its\nestimates. Fig. 2 depicts an example of a fitted GMM on a numerical feature.\nIt is important to note that while GMM efficiently captures the underlying patterns of one-dimensional numerical\nfeatures, the choice of the number of components must be carefully considered to accurately represent the actual data.\nAdditionally, during sampling using GMM, some generated values may exceed the minimum and maximum values of\nthe actual data. This can be particularly problematic for features that logically cannot exceed a certain value (e.g., a cost\nfeature that cannot be negative). In such cases, generated values exceeding a predefined limit can either be discarded\nor converted to the limit value.\nCategorical Features For each categorical feature C in the dataset D', synthetic values are generated based on\nrandom numbers between 0 and 1, mapped according to the observed frequencies of different categories. Let C have\nK unique categories. Calculate the cumulative distribution function, CDF(c;) for each category based on the observed\nfrequencies in D':\n$CDF(c_{i}) = \\sum_{j=1}^{i} P(c_{i}), 1 \\leq i \\leq K$\nWhere P(c;) is the observed probability of category c\u00a1 in D'. Next, generate a random number R between 0 and 1,\nthen map it to a category c; based on the CDF values:\n$R_{C} = min\\{c_{i} : R \\leq CDF(c_{i})\\}$\nWhere Re is the synthetic categorical value generated for categorical feature C. This approach ensures that\nsynthetic values for the categorical feature are generated randomly based on the observed frequencies, maintaining\nthe distribution of the original dataset.\nIn essence, the generator plays a crucial role in generating synthetic instances, ensuring that the generated values\nfor each feature mirror the distribution seen in the modified dataset, D'. Operating with a straightforward yet effective\napproach, it iteratively processes each feature, whether categorical or numerical, to generate values aligned with the\nunderlying patterns of D'. These feature-specific values are then concatenated, resulting in a comprehensive synthetic\ninstance."}, {"title": "3.1.2. Validator", "content": "The validator is pivotal in ensuring the quality of generated instances, ultimately enhancing classification accuracy.\nTo achieve this, we begin by training a classifier on the original dataset. This classifier, meticulously tuned for accurate\nclassification and adept at learning feature boundaries specific to each class, serves as a benchmark for evaluating the\ngenerated instances. Our empirical evidence supports the use of SVM or XGB, yielding superior results in this context.\nSubsequently, the generated instances from the generator undergo scrutiny by the trained classifier. If an instance\nis identified as not belonging to class i, it is promptly discarded. Conversely, instances correctly classified are stored\nfor subsequent augmentation. This iterative process of generation and validation continues until the desired number of\nhigh-quality instances for class i is achieved. By employing this meticulous validation step, we ensure that the generated\ninstances align closely with the characteristics of class i, contributing to a more refined and accurate augmentation\nprocess."}, {"title": "3.2. Baseline Methods", "content": "In this section, we introduce baseline data augmentation techniques chosen for their contributions to the field. These\nmethods act as benchmarks for evaluating our model, ENSY. We explore the strengths and limitations of each baseline\ntechnique, offering reasons for the development of ENSY. Our goal is to show that ENSY effectively tackles class\nimbalance and improves synthetic data generation, as demonstrated through a thorough evaluation of performance\nmetrics and model architecture."}, {"title": "3.2.1. Random Oversampling", "content": "Random Oversampling (ROS) is a commonly used method to tackle imbalances in ML datasets. It involves\nrandomly duplicating instances from the minority class, helping balance class distributions. The simplicity of ROS\nis a strong point, providing an easy yet effective way to expose the classifier to more examples from the minority class.\nHowever, ROS has its limitations. It tends to duplicate existing patterns in minority classes without adding new\ninformation. In contrast, our method, ENSY, takes a different approach. As shown in Fig. 3d, ENSY not only explores\npatterns but actively enhances the classification process by creating diverse synthetic instances. This sets ENSY apart,\nnot just by balancing class distributions but also by introducing valuable variations. This could potentially improve the\nclassifier's ability to recognize detailed patterns within the minority class."}, {"title": "3.2.2. Synthetic Minority Over-sampling Technique for Nominal and Continuous Features (SMOTE-NC)", "content": "This method is an extension of the original SMOTE algorithm, specifically designed for datasets featuring both\nnominal and continuous features. Aimed at addressing the class imbalance, SMOTE-NC combines oversampling and\nfeature space interpolation to generate synthetic instances for the minority class (Chawla et al., 2002). The key steps\nof the SMOTE-NC algorithm are as follows:\nNominal Feature Handling For nominal features, SMOTE-NC employs a modified approach to generate synthetic\ninstances. For each minority class instance, the algorithm identifies its k nearest neighbors within the same class. A\nsynthetic instance is then created by randomly selecting neighbors and replacing nominal features with the mode of\nthe selected neighbors (Chawla et al., 2002).\nContinuous Feature Handling For continuous features, SMOTE-NC follows the traditional SMOTE procedure. It\nselects a minority class instance and its k nearest neighbors. A synthetic instance is generated by interpolating the\ncontinuous feature values of the selected neighbors (Chawla et al., 2002).\nCombined Synthesis Synthetic instances for nominal and continuous features are integrated to form the final\naugmented dataset (Chawla et al., 2002). While SMOTE-NC confines the generated data within the initial dataset's\nbounds through interpolation (Fig. 3b), ENSY takes a different path by exploring the hyperplane. A distinguishing\naspect of ENSY's methodology is grounded in the understanding that validated data, drawn from the distribution of\nthe rest of the classes, naturally exhibits a skewness towards the borderlines of that class's data points, as opposed\nto its center. This skewness towards the borderlines aids the classifier in learning these crucial distinctions better,\ncontributing to improved differentiation among the classes."}, {"title": "3.2.3. Conditional Tabular Generative Adversarial Network (CTGAN)", "content": "Generative Adversarial Networks (GANs) form a class of machine learning models where a generator and\ndiscriminator are pitted against each other in a training process. The generator aims to create synthetic data that\nis indistinguishable from real data, while the discriminator's role is to differentiate between genuine and synthetic\nsamples. This adversarial setup assists in the improvement of both the generator's ability to create realistic data and\nthe discriminator's sharpness in distinguishing between real and generated instances.\nIn our exploration of data augmentation techniques, we delve into Conditional Tabular GAN (CTGAN) (Xu,\nSkoularidou, Cuesta-Infante and Veeramachaneni, 2019). This specialized GAN variant is designed to synthesize\ntabular data, particularly effective for datasets with a mix of categorical and numerical features. It leverages conditional\ngenerative modeling, allowing us to control the characteristics of the generated data, making it well-suited for\napplications where preserving specific attributes is crucial. Furthermore, CTGAN can be conditioned on the minority\nclass label, enabling the targeted generation of synthetic instances for the underrepresented class.\nCTGAN consists of a generator network and a discriminator (or critic) network. The generator network is trained\nto minimize generator loss, encouraging the generation of synthetic samples that are difficult for the discriminator\nto distinguish from real instances. Conversely, the discriminator aims to minimize discriminator loss, distinguishing\nbetween real and synthetic samples accurately.\nDespite CTGAN's efficacy in generating synthetic values, achieving optimal training requires careful tuning for a\nsmooth and reliable process. While Python packages such as SDV and YData provide extensive modification options\nfor CTGAN implementation, our approach employs an open-source version for more nuanced monitoring, enabling\nprecise adjustments at every stage of the model. Fig. 4 illustrates the training progress of our implementation.\nThe objective during training is for the discriminator loss to oscillate around 0, indicating its inability to discern\nbetween real and fake samples, while the generator loss should stabilize at a negative value, signifying the realism of\nthe generated data successfully fooling the discriminator.\nDespite its significant capabilities, CTGAN is not without its shortcomings. When the model is trained on the\nentire dataset, it tends to overfit the majority classes, neglecting the intricate patterns within the minority classes.\nConversely, fitting the model on each class independently poses challenges as it tends to generate noise due to its limited\nunderstanding of the patterns present in the rest of the classes (Fig. 3c). Furthermore, as the dataset size increases,\nthe computational demands become more time-consuming, and fine-tuning becomes a challenging task that demands\nextensive knowledge and trial-and-error exploration."}, {"title": "3.3. Metrics", "content": "In this study, we employ several key metrics to evaluate the performance of data augmentation techniques. These\nmetrics provide insights into different aspects of classification accuracy."}, {"title": "3.3.1. Overall Accuracy", "content": "Overall accuracy is a commonly used metric to assess the general correctness of a classification model (Vujovic,\n2021), (Hossin and Sulaiman, 2015). It is calculated as the ratio of correctly predicted instances to the total number of\ninstances in the dataset.\n$Overall Accuracy = \\frac{Number of Correct Predictions}{Total Number of Instances}$"}, {"title": "3.3.2. Precision", "content": "Precision measures the accuracy of positive predictions made by the model. It is calculated as the ratio of correctly\npredicted positive instances to the total number of positive predictions (true positives and false positives).\n$Precision = \\frac{True Positives}{True Positives + False Positives}$"}, {"title": "3.3.3. Recall (Sensitivity)", "content": "Recall, also known as sensitivity or true positive rate, evaluates the model's ability to capture all positive instances.\nIt is calculated as the ratio of correctly predicted positive instances to the total number of actual positive instances (true\npositives and false negatives).\n$Recall = \\frac{True Positives}{True Positives + False Negatives}$"}, {"title": "3.3.4. F1-score", "content": "The F1-score provides a balance between precision and recall. In other words, it corresponds to the harmonic\naverage of the precision and the recall. This metric is especially useful in imbalanced datasets.\n$F1 = \\frac{2 \\times (Precision \\times Recall)}{Precision + Recall}$\nThese metrics collectively provide a comprehensive assessment of the classification performance, considering\naspects such as overall correctness, precision in positive predictions, and the ability to identify all positive instances."}, {"title": "4. Dataset Description", "content": "The London Passenger Mode Choice (LPMC) (Hillel et al., 2018) dataset, derived from the London Travel Demand\nSurvey (LTDS), comprises 81,086 trips made by 31,954 individuals across 17,616 households over three years (April\n2012 to March 2015). On the other hand, the Korea Transport Database (KTDB, 2016) is based on the national\nhousehold travel survey data collected in South Korea in 2016. The survey was conducted in 202,316 households.\nThese two datasets serve as valuable resources for understanding urban multi-modal transport network dynamics and\npredicting mode choice (Table 1, 2).\nThe raw datasets need to undergo various preprocessing steps before they can be analyzed further. Firstly, columns\nthat do not add valuable information, such as unique identifiers, are eliminated from the dataset. Next, features with\nstrong linear correlations to other features are excluded to guarantee that the remaining features possess valuable\nand unique information. The last step involves generating new features derived from existing columns to eliminate\nredundancy and simplify the dataset. As these datasets do not contain any duplicate rows or missing values, the\nmodified versions do not require any additional modifications. In this paper, the modified LPMC dataset comprises 17\nfeatures, including 8 categorical (Trip purpose, Car ownership, ...) and 9 numerical (Distance, Age, Time, ...) features.\nConversely, the modified KTDB contains 17 features, with 12 being categorical (Age Range, Trip purpose, Income\nRange, ...) and 5 being numerical (Time, Distance, Cost, ...)."}, {"title": "5. Results and Discussion", "content": "In this section, we present the outcomes of our experiments, comparing the classification performance of baseline\nmethods (ROS, SMOTE-NC, CTGAN) with our proposed method, ENSY. The experiments were conducted on\nmodified versions of the LPMC and KTDB datasets, randomly split into training and test datasets, with 15% allocated\nto the test set.\nAs evidenced by the results presented in Table 3 and Table 4, ENSY consistently showcased notable improvements\nin the F1-score for minority classes across diverse scenarios. Given that the F1-score strikes a balance between precision\nand recall, these findings suggest that ENSY effectively addresses class imbalance.\nAnalyzing the performance of different classification models, we observed that XGB consistently achieved the best\nresults, followed by Random Forest, while Neural Networks (NN) yielded comparatively lower performance.\nSurprisingly, data augmentation, including ENSY, did not significantly enhance classification performance when\nRandom Forest was employed. In these scenarios, the raw dataset outperformed the augmented counterparts, aligning\nwith findings from (Chen and Cheng, 2023).\nAn interesting observation emerged when the entire dataset was augmented using CTGAN before splitting. In this\ncase, RF, XGB, and NN consistently achieved overall accuracies exceeding 99%, aligning with (Majeed and Hwang,\n2023). However, this trend did not hold when only the training dataset underwent augmentation.\nTable 5 and Table 6 illustrate that RF achieves the highest overall accuracy in raw data. However, ESNY\ndemonstrates significant potential for enhancing overall accuracy in both XGB and NN. Once more, the tables highlight\nthat XGB outperforms RF and NN in terms of performance.\nThese findings underscore the nuanced impact of data augmentation techniques on different classification models.\nWhile ENSY showcased consistent improvements for minority classes, the choice of classifier and the nature of\naugmentation can significantly influence overall performance."}, {"title": "6. Conclusion", "content": "In conclusion, our study underscores the success of ENSY in mitigating class imbalance and enhancing overall\naccuracy, as evidenced by a notable 3% improvement in overall accuracy for the LPMC dataset. Furthermore, the\nalmost quadrupled F-1 score for the minority class (Cycling) reflects a significant advancement in addressing the\nchallenges posed by imbalanced datasets.\nSimilarly, for the KTDB dataset, ENSY demonstrated its effectiveness by contributing to an improvement of almost\n1.5% in overall accuracy, coupled with an impressive doubling of the F-1 score for the minority class (Taxi). These\nresults affirm the potential of ENSY as a valuable tool in mode choice prediction models, showcasing its ability to offer\nsubstantial performance gains in different contexts.\nNotably, the improvements were particularly pronounced when employing XGB, which consistently outperformed\nNeural Networks (NN) and Random Forest (RF), even in scenarios without augmentation. This highlights the\nrobustness and efficacy of ENSY, especially in conjunction with XGB. The effects of different models used as validators\nfor ENSY, such as NN, warrant further exploration and study to understand their nuances and potential impact on model\nperformance.\nWhile the improved accuracy and F-1 scores are promising, they prompt a necessary reflection on whether the\nefforts invested in developing and implementing ENSY are justified. Our study suggests that, despite the advancements,\nthere exist trade-offs that necessitate ongoing research and refinement. Exploring avenues for further improvement\nremains an essential aspect of future work, with potential areas of exploration including parameter tuning, ensemble\nmethods, and additional feature engineering.\nIn summary, our research positions ENSY as a promising solution to the challenges of class imbalance in mode\nchoice prediction models. However, ongoing efforts are crucial to optimize its performance, understand its limitations,\nand justify its application in practical transportation scenarios."}, {"title": "Statements and Declarations", "content": "\u2022 Funding Sources\nThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit\nsectors.\n\u2022 Competing Interests\nThe authors have no relevant financial or non-financial interests to disclose.\n\u2022 Author Contribution\nConceptualization: Amirhossein Parsi, Melina Jafari, Sina Sabzekar, Zahra Amini; Methodology: Amirhossein\nParsi, Melina Jafari, Sina Sabzekar, Zahra Amini; Data data collection and analysis: Amirhossein Parsi,\nMelina Jafari; Writing-original draft preparation: Melina Jafari, Amirhossein Parsi; Writing-review and editing:\nAmirhossein Parsi, Zahra Amini; Supervision: Zahra Amini"}]}