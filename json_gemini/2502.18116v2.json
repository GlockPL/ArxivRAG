{"title": "Bayesian Optimization for Controlled Image Editing via LLMs", "authors": ["Chengkun Cai", "Haoliang Liu", "Xu Zhao", "Zhongyu Jiang", "Tianfang Zhang", "Zongkai Wu", "Jenq-Neng Hwang", "Serge Belongie", "Lei Li"], "abstract": "In the rapidly evolving field of image generation, achieving precise control over generated content and maintaining semantic consistency remain significant limitations, particularly concerning grounding techniques and the necessity for model fine-tuning. To address these challenges, we propose BayesGenie, an off-the-shelf approach that integrates Large Language Models (LLMs) with Bayesian Optimization to facilitate precise and user-friendly image editing. Our method enables users to modify images through natural language descriptions without manual area marking, while preserving the original image's semantic integrity. Unlike existing techniques that require extensive pre-training or fine-tuning, our approach demonstrates remarkable adaptability across various LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian optimization strategy to automatically refine the inference process parameters, achieving high-precision image editing with minimal user intervention. Through extensive experiments across diverse scenarios, we demonstrate that our framework significantly outperforms existing methods in both editing accuracy and semantic preservation, as validated using different LLMs including Claude3 and GPT-4.", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of visual content manipulation, image editing has gained significant attention due to its practical applications across various domains. Unlike traditional image generation models such as Stable Diffusion (Rombach et al., 2022) and DALL-E 3 (Ramesh et al., 2022), our work is specifically focused on improving control in the image editing process. Recent advancements in controllable synthesis, such as those by Hertz et al. (Hertz et al., 2022) and Brooks et al. (Brooks et al., 2023), have introduced methods to fine-tune and guide transformations in existing images rather than generating new ones from scratch.\nControllable synthesis in generation technology (Guan et al., 2025; Yao et al., 2024; Hertz et al., 2022; Brooks et al., 2023; Patashnik et al., 2021; Jiang et al., 2024) has recently attracted significant attention due to its expanded range of applications. Models such as Pix2Pix and Cycle-GAN have demonstrated the ability to transform images from one domain to another, effectively applying controllable synthesis to tasks like style transfer and image enhancement (Isola et al., 2017; Zhu et al., 2017). Recent advances like ZONE have enabled instruction-driven modifications without pre-defined training samples (Li et al., 2024b), while new frameworks have emerged for intuitive and localized image editing by manipulating internal attention mechanisms (Brooks et al., 2023). This represents a shift towards more granular control over AI-generated content, enabling precise, region-specific alterations without additional input masks.\nHowever, despite these advancements, existing methods face several critical challenges. First, most state-of-the-art local editing methods heavily rely on mask priors to constrain the editing regions\u2014either through manual input or derived from attention map analysis and semantic segmentation\u2014making them less accessible for non-expert users. Additionally, methods like cross-attention control and diffusion models often encounter challenges in fine-tuning model parameters to align with user requirements, resulting in a disparity between desired and actual outputs. These issues are particularly pronounced in applications that demand detailed modifications based on user instructions. With the LLMs-Driven adaptation, recent various applications (Cai et al., 2024b,a; Li et al., 2024a; Shi et al., 2025; Liu et al., 2024; Shi et al., 2024) have advanced developed.\nTo address these challenges, we propose BayesGenie: a novel framework that achieves precise localized editing without any form of mask guidance. Our approach uniquely combines the semantic understanding capabilities of LLMs with the parameter optimization power of Bayesian methods (OpenAI, 2023; Snoek et al., 2012). BayesGenie leverages LLMs to generate detailed prompts from user requirements, which then guide a Stable Diffusion model to modify images accurately. The framework employs Bayesian Optimization to systematically explore the parameter space, particularly the image and text Classifier Free Guidance (CFG) weights, to maximize output quality.\nOur method provides an end-to-end solution, where users are only required to provide a textual description, eliminating the need for manual selection or marking of specific image regions. This approach streamlines user interaction, enhancing intuitiveness and accessibility. Moreover, our method operates without pre-training or fine-tuning on specific datasets, instead leveraging the capabilities of multiple multimodal LLMs to produce high-quality outcomes.\nOur experimental results demonstrate that the integration of LLMs and Bayesian Optimization enables more intuitive and accurate image editing. As shown in Figure 1, our method can effectively implement specific modifications while maintaining the scene's overall coherence and aesthetic integrity.\nIn summary, our contributions are:\n\u2022 We propose BayesGenie, a novel image editing framework that enables precise localized editing without manual region annotations or mask creation. Our model-agnostic approach leverages LLMs for region understanding and semantic interpretation, making it possible to perform accurate local edits based purely on natural language descriptions, while ensuring high-precision image editing.\n\u2022 We introduce an automated parameter optimization system based on Bayesian Optimization that eliminates the need for manual parameter tuning or pre-training. This system automatically discovers optimal editing parameters through iterative refinement, independent of the model training, making our framework immediately deployable across different scenarios without requiring specialized adjustments.\n\u2022 Through extensive experiments, we demonstrate that our off-the-shelf framework achieves superior performance and broad adaptability across various editing scenarios. The framework's effectiveness has been validated with different multimodal LLMs, showcasing its versatility and robustness while maintaining both local precision and global consistency."}, {"title": "1.1 Related Work", "content": "Image-to-Image Generation Models Image-to-image translation models have become increasingly significant in the field of computer vision. Generative Adversarial Networks (GANs) and auto-regressive models have been pivotal, with notable architectures like Instruct Pix2Pix, CycleGAN, and PixelCNN demonstrating impressive results (Isola et al., 2017; Zhu et al., 2017; Van Den Oord et al., 2016). Diffusion models, such as SR3 and ADM, have emerged as powerful alternatives, offering superior quality and diversity in image generation tasks by progressively refining noisy images to high-quality outputs (Saharia et al., 2022; Dhariwal and Nichol, 2021).\nThe Instruct Pix2Pix framework represents a significant advancement in the field of image editing (Brooks et al., 2023). This model has been widely used in various image-to-image generation tasks, such as converting hand-drawn sketches into photographs (M Shetty K Raghavendra, 2022), transforming abstract maps into realistic map images (Li et al., 2024c) and de-noising images taken in harsh environments for crowd counting (Khan et al., 2023). Instruct Pix2Pix employs Classifier-Free Guidance (CFG) for both image and text conditions, adjusting the weights of these inputs to control the generated output. It enables users to"}, {"title": "Methodology", "content": "Our system architecture integrates LLMs and Bayesian optimization for image editing (Figure 2). An LLM processes the original image and modification requirements to generate a textual prompt capturing the desired changes. This prompt and the original image are then fed into a diffusion model to generate a modified image. BayesGenie enhances this process through dynamic prompt refinement and parameter optimization. The LLM evaluates each generated image, scoring it based on requirement satisfaction and providing feedback for improvements. Bayesian optimization then iteratively adjusts key parameters, specifically 'text_cfg_scale' and 'image_cfg_scale', which balance text and image components in the diffusion model. This optimization minimizes the negative LLM score, maximizing alignment between generated images and desired outcomes."}, {"title": "2.1 Dynamic Prompt Optimization with LLMS", "content": "In our approach, the prompt is dynamically optimized through an iterative optimization process. Initially, the LLM generates a prompt based on the user's modification requirements, which guides the diffusion model to generate a preliminary image. Once the image is generated, the LLM evaluates it by assigning a score based on how well it aligns with the user's specifications and provides feedback on areas needing improvement, such as adding more details or adjusting object positioning. This feedback is then used to refine the prompt for the next iteration. The process repeats, with the refined prompt guiding the generation of a new image, until the desired result is achieved or the maximum number of iterations is reached."}, {"title": "2.2 Preliminaries", "content": "Assumption 1. Consider the existence of a set of optimized guidance scales, denoted as $s_1$ and $s_T$"}, {"title": "2.3 Bayesian Optimization", "content": "The optimization process involves the repeated evaluation of the objective function using Bayesian optimization, where each evaluation includes generating a modified image with the current CFG parameters and scoring it through the LLM. The system iteratively samples the parameter space and updates its model of the objective function landscape based on the results of previous evaluations, aiming to find the optimal guidance scales that yield the highest quality edited images.\nSpecifically, the objective function of Bayesian optimization is\n$f(s) = f([s_1, s_T])$, (3)\nwhere $s = [s_1, s_T]$ represents our two guidance parameters (ImageCFG and TextCFG), and $f(s)$ represents the quality score evaluated by the LLM based on both semantic alignment and image quality.\nBayesian optimization uses a Gaussian Process (GP) to approximate the objective function $f(s)$. The Gaussian Process assumes that all possible function values have a joint Gaussian distribution:\n$f(s) \\sim GP(\\mu(s), k(s, s'))$ (4)\nwhere $\\mu(s)$ is the mean function and $k(s, s')$ is the kernel function that defines the similarity between parameter settings. We employ the Mat\u00e9rn kernel function for its robustness in optimization tasks."}, {"title": "2.4 Scoring Evaluation", "content": "In the context of our optimization task, accurate evaluation of the generated images is crucial. While traditional metrics like CLIP scores are commonly used, they fall short in evaluating fine-grained image modifications. To address this limitation, we utilize LLMs for a more nuanced evaluation, leveraging their robust multimodal understanding capabilities.\nOur evaluation is guided by a predefined 0-shot prompt designed to ensure consistency and objectivity (see Supplement for the full prompt). This prompt directs the LLM to integrate three distinct"}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Evaluation Protocol", "content": "In evaluating our approach, we carefully considered various metrics commonly used in image processing tasks. While traditional metrics like SSIM and PSNR are widely used, they present significant limitations for instruction-guided image editing evaluation:\n\u2022 Pixel-level Comparison: SSIM and PSNR operate on pixel-level comparisons, which would unfairly penalize intentional edits even when they successfully follow the instructions.\n\u2022 Semantic Understanding: These metrics cannot evaluate whether edits align with semantic instructions or distinguish between"}, {"title": "3.2 Experimental Setup", "content": "Baselines Selection Most state-of-the-art local editing methods, including ZONE (Li et al., 2024b), heavily rely on mask priors to constrain the editing regions-either through manual input or derived from operations such as attention map analysis and semantic segmentation. In contrast, our method, to the best of our knowledge, is the first to aim for localized editing without relying on any form of mask guidance. As such, comparing our approach with mask-guided methods would neither be fair nor meaningful.\nInstead, we focus our evaluations on methods that, like ours, do not utilize masks or region segmentation in their pipeline:\n\u2022 InstructPix2Pix (Brooks et al., 2023), which performs editing purely based on text instructions and ranks second to ZONE in their reported experiments.\n\u2022 DALLE-3, which is widely recognized as the current strongest image generation model due to its massive parameter scale.\nDataset and Tasks We constructed a balanced evaluation dataset comprising over 500 images, with the following editing operations:\n\u2022 Adding objects to images\n\u2022 Removing objects from images\n\u2022 Modifying existing objects\nThis diverse set of tasks was selected to comprehensively evaluate our method's versatility and effectiveness across different editing scenarios."}, {"title": "3.3 Results", "content": "In our experiments, BayesGenie effectively handled the three key image editing tasks, demonstrating robust performance across all scenarios. The results indicate that BayesGenie produces visually consistent images that align closely with user specifications, thanks to the LLM's multimodal understanding capabilities.\nQualitative Analysis Figure 4 shows how Bayesian optimization improves accuracy over iterations, initially with incorrect placement and mismatched background features, but gradually aligning the replacement to match the original image. Figure 5 illustrates our method's ability to add, remove, and modify elements in a scene while preserving the core features, demonstrating successful addition, removal, and substitution tasks in different visual contexts, showcasing our model's performance in fine-grained image editing."}, {"title": "4 Discussions", "content": "Cost and efficiency The algorithm requires relatively low computational resources and costs. The costs for GPT-40 are shown in the table below, with the main expense being the Prompt tokens for Bayesian loop evaluation. The total cost for running the algorithm once to generate a 512x512 image is 0.176 dollar. Additionally, this experiment was conducted on a machine with a single RTX 4080, and each algorithm run takes approximately 2.5 minutes.\nIn terms of iterations, more iterations represent a finer exploration of the solution space, allowing the algorithm to more precisely converge on an optimal result. Our Bayesian optimization strikes a balance between accuracy and cost by using 20 iterations, as increasing beyond this number has shown diminishing returns in terms of accuracy improvements, while still incurring higher computational costs. This choice ensures that the algorithm remains both computationally efficient and capable of generating high-quality outputs without unnecessary resource consumption."}, {"title": "5 Conclusion", "content": "Our work introduces BayesGenie, a novel and model-agnostic framework that combines Bayesian"}, {"title": "A Examples of Prompt Generation", "content": "In this section, we present examples of input images alongside the corresponding requirements and GPT-4 generated prompts. These examples illustrate how the Large Language Model (LLM) interprets user instructions and generates detailed prompts that guide the image modification process."}, {"title": "B Scoring Evaluation Prompt", "content": "This appendix provides the specific prompt used for evaluating the modified images generated by the model. The prompt is designed to ensure that the generated images adhere to the specified requirements, while also maintaining the overall integrity and coherence of the original image. The evaluation prompt outlines the criteria for scoring the generated image, which includes assessing the degree of alteration and its alignment with the original content.\nThe following is a requirement for modifying an image:\nBelow are two images: the original image and the generated image after modification.\nThe first one is the original Image.\nThe second one is the generated Image.\nPlease evaluate whether the generated image meets the requirement. Provide a score from 0 to 100 based on the following criteria:\n1. If the generated image is altered too much compared to the original image, give a low score.\n2. If the generated image is altered too little, give a low score.\n3. If the generated image meets the requirement well, give a high score.\nThe return should begin with: The score is:\nEnsure the scores follow a normal distribution, with the majority of scores being around the middle range, and only exceptional cases scoring very low or very high. Also, provide a brief explanation for the score."}, {"title": "C Ethics and Participant Consent", "content": "This study was conducted following the ethical guidelines established by our institution's Research Ethics Committee, all participants in the study provided informed consent before participating. Participants were fully informed about the purpose of the study, which aimed to evaluate AI-generated image modifications. Participation was voluntary, with the right to withdraw at any point before submitting the survey responses. No personally identifiable information was collected, ensuring complete anonymity. Anonymised data was used for academic publications and presentations, with participants given the option to consent to future use in ethically approved research. Participants filled out the survey via social media platforms and were offered a chance to win a \u00a310 Amazon gift card as an incentive for their participation."}]}