{"title": "Improve Machine Learning carbon footprint using Parquet\ndataset format and Mixed Precision training for regression\nalgorithms", "authors": ["Andrew Antonopoulos"], "abstract": "This study was the 2nd part of my dissertation for my master's degree and compared\nthe power consumption using the Comma-Separated-Values (CSV) dataset format\nand Parquet dataset format with the default floating point (32-bit) and Nvidia's mixed\nprecision (16-bit and 32-bit) while training a regression ML model. The same custom\nPC as per the 1st part [1] was built to perform the experiments, and different ML hyper-\nparameters, such as batch size, neurons, and epochs, were chosen to build Deep\nNeural Networks (DNN). A benchmarking test with default hyper-parameter values for\nthe DNN was used as a reference, while the experiments used a combination of\ndifferent settings. The results were recorded in Excel, and descriptive statistics were\nchosen to calculate the mean between the groups and compare them using graphs\nand tables. The outcome was positive when using mixed precision combined with\nspecific hyper-parameters. Compared to the benchmarking, the optimisation for the\nregression models reduced the power consumption between 7 and 11 Watts. The\nregression results show that while mixed precision can help improve power\nconsumption, we must carefully consider the hyper-parameters. A high number of\nbatch sizes and neurons will negatively affect power consumption. However, this\nresearch required inferential statistics, specifically ANOVA and T-test, to compare the\nrelationship between the means. The results reported no statistical significance\nbetween the means in the regression tests and accepted Ho. Therefore, choosing\ndifferent ML techniques and the Parquet dataset format will not improve the\ncomputational power consumption and the overall ML carbon footprint. However, a\nmore extensive implementation with a cluster of GPUs can increase the sample size\nsignificantly, as it is an essential factor and can change the outcome of the statistical\nalysis.", "sections": [{"title": "Introduction", "content": "The greenhouse effect is a natural phenomenon related to the sun's radiation, which\ntravels towards the Earth [2]. The radiation reaches the earth and is absorbed by the\nland and ocean, and some are released toward space [2]. Most of it is captured and\nretained by greenhouse gases, a combination of chemical compounds that help keep\nEarth at a suitable temperature for all living beings [3]. Gases like carbon dioxide are\nproduced naturally or by human activities, and by increasing it will also increase the\nEarth's temperature, affecting everyone's life [3]. The carbon footprint is the total\namount of carbon dioxide emitted by human actions and is measured in grams of\nCO2 (Carbon dioxide) equivalent per kilowatt hour (gCO2e/kWh) [4]. The higher the\ncarbon footprint, the more impact it will have on the environment.\nMachine Learning (ML) has become very popular in many industries, and various\nservices, such as cybersecurity, healthcare, and finance, have adopted it [5]. Millions\nof people use ML services hosted in the Cloud and specifically in big data centres\n[6]. This forces service providers to build big data centres to store the hardware and\nsupport growth. The data centres require cooling systems and power generators to\nmaintain thousands of servers, consuming substantial power sources such as water\nand electricity [6]. Therefore, ML services are increasing and overloading many data\ncentres worldwide, which can affect their sustainability, eventually increasing the\ncarbon footprint and affecting the environment.\nData centres are using energy from non-fossil-fuelled technologies (solar, wind,\nhydro) instead of fossil-fuelled technologies (coal, oil, gas) [4]. However, there are no\ncarbon-free forms of generating energy [4], and optimising ML services is a potential\ncandidate to help reduce the carbon footprint."}, {"title": "Background", "content": ""}, {"title": "Methodology", "content": "Large datasets with a minimum of 1 GB of data have been used to assess power\nconsumption over a period of time. This is essential because the longer the model\ntraining takes, the more power consumption data will be generated. Therefore, the\nregression CSV data were pre-processed before being used for the model training,\nensuring the accuracy of the data.\nThe regression dataset contains information on used car sales, such as models,\nprices, and production years, and is in CSV format. The owner published the data on\nthe Kaggle platform [16], which was scraped using web crawlers; it contains most of\nCraigslist's relevant information on car sales, including columns like price, condition,\nmanufacturer, latitude/longitude, and 18 other categories.\nHowever, CSV and Parquet were used to collect data for analysis and comparison\nduring the experiments.\nVarious experiments\nwere created by utilising different ML optimisation techniques and hyperparameters.\nThe data were collected into an Excel file and used for analysis during the\nexperiments. This procedure was repeated until it satisfied all the experiment use\ncases."}, {"title": "3.1 Collecting computation power consumption data", "content": "Identifying the hardware and software to collect power consumption data is a crucial\nstep. The Graphics Processing Unit (GPU) accounts for around 70% of power\nconsumption. In comparison, the Central Processing Unit (CPU) is responsible for\n15%, Random Access Memory (RAM) for 10%, and the remaining 5% from other PC\ncomponents [20]. Therefore, the GPU, CPU and RAM are critical components\nbecause they directly impact the ML lifecycle. SSD or HDD are also crucial but are"}, {"title": "3.2 ML optimisation techniques", "content": "Optimisation is crucial when creating a more efficient DNN because it has a certain\nlevel of complexity. Hyper-parameter optimisation techniques, such as the number of\nhidden layers, batch size, neurons, and epochs, cannot be modified individually and\nmanually because they require a lot of time and experience [22]. If a non-optimal\nhyper-parameter is chosen for a particular reason, the DNN will consume more\nprocessing power [23]. The hyper-parameter will require fine-tuning to achieve the\nideal results, but DNNs may fail to train or receive inefficient results because of the\nnon-optimal values [24].\nAs per the classification test in 1st part [1], the same hyper-parameters and mixed\nprecision were used for the benchmarking and experiments, as shown below:"}, {"title": "3.3 Power Consumption Data", "content": "shows the architecture and how data were collected. Multiple third-party\nsoftware extracted the RAM, CPU, and GPU utilisation and power consumption data\nin Watts. The data were collected in an Excel file for comparison and generating the\naverage value. The PSU was connected directly to the wattmeter, but reading the\nvalues manually was required because the software was unavailable.\nCode Carbon, a Python library, was integrated into the Python code, and data was\nseamlessly collected while the code was running. However, Code Carbon cannot\nstore historical data, and Comet has been used to retrieve the average value over a\nperiod of time. Comet is a web service that pulls data from Code Carbon via an API\nto monitor GPU and CPU power consumption and utilisation. The collected data from\nall the software and the wattmeter was imported into Excel for further analysis.\nWatts have been chosen because they measure the power consumed by a device.\nThe higher the wattage, the more significant the amount of electrical power the PC\nuses over a period of time."}, {"title": "3.4 Data Analysis Technique", "content": "A similar approach as per the 1st part [1] was used, and descriptive statistics were\nadapted to assess the central tendency of the power consumption values. The author\nused a component bar chart to illustrate the comparison between the average of each\npiece of hardware [27]. However, further analysis of the findings using inferential\nstatistics was required because the differences between the average values were too\nclose. To achieve this task, ANOVA was used to evaluate the relationship between\nthe tests and multiple T-tests were used to check whether the difference between\nexperiments was statistically significant [28]. summarises the steps that\nfollowed during the analysis."}, {"title": "Testing and Results", "content": ""}, {"title": "4.1 Introduction", "content": "The GPU has played a vital role in ML and model training because it is powered by\nTensor Cores, which are specialised cores that enable mixed precision and can\naccelerate training and learning performance [29]. Using a GPU that supports Tensor\nCores, we can utilise the mixed precision functionality, accelerating the throughput\nand reducing Al training times [29]."}, {"title": "4.2 Regression", "content": "Similar to the classification test [1], the initial step was to load the dataset, and the\nmean type from the descriptive statistics was used to calculate the average.\nThe original dataset has 426,881 rows and was required to execute pre-tests to\ndetermine if the PC's RAM can handle the dataset size during pre-processing. The\nauthor tried with different datasets, reducing the rows by 50,000 in each test. The\nconclusion was that with a dataset of 150,000 rows, the PC's RAM could process the\ndataset and had enough memory for other processes related to the operating system."}, {"title": "4.3 Benchmarking", "content": "Two benchmarking tests have been completed, one for the CSV and another for the\nParquet dataset format. shows the configuration for the DNNs.\nThe same methodology has been followed as the classification benchmarking [1].\nThe floating points were 32 bits, the default value, and neurons were 1024. The\nregression dataset has numerical values, making it easier for the GPU to process the\ndata. Therefore, the batch size and epochs were adjusted, which is responsible for\nthe duration it takes to train the model. By increasing the epochs, the model training\ntook longer, allowing more accurate measurements to be collected.\nshows the power consumption during the CSV benchmarking testing. Before\nthe model training, the power consumption was within normal PC operational values,\nand the RAM was 12 Watts because of the fixed value from Code Carbon. During\nmodel training, the GPU increased to 44 Watts, the CPU to 27 Watts and the overall\npower consumption to 126 Watts. confirms that GPUs were used during the\nmodel training with utilisation at 40%."}, {"title": "4.4 Experiments", "content": "For each regression experiment, a procedure similar to the classification [1] was\nfollowed. Different batches and neurons were chosen to produce a variety of results\nthat could be compared with the benchmarking data. The common factor is the mixed\nprecision and the epochs, which keep the same model training duration between\nexperiments.\nshows the DNN network configuration for each CSV and Parquet experiment.\nThe CSV experiments used the same configuration as the associated Parquet\nexperiment to provide a fair comparison."}, {"title": "Analysis and Evaluation", "content": ""}, {"title": "5.1 Introduction", "content": "During the analysis, the same four groups as per the classification [1] were used to\nidentify a potential statistical significance based on their means using the ANOVA.\nEach group has four values: GPU, CPU, RAM and total\npower consumption, which were taken from the Wattmeter.\nANOVA can be used when we have more than two groups, but if there is a significant\ndifference, it does not illustrate where the significance lies [27]. Therefore, multiple T-"}, {"title": "5.2 Regression Analysis", "content": "Both regression tests, for the CSV and Parquet, have been conducted using the same\nprinciples as the classification analysis [1].\nThe assumptions were the following [31]:\n1. The data in each group are normally distributed\n2. The data in each group have the same variance\n3. The data are independent\nsummarise the data collected during each regression test for\nthe CSV and Parquet."}, {"title": "5.3 Limitations of the Analysis", "content": "The propositions might not be valid because the analysis has limitations. To begin\nwith, the RAM power consumption is based on a fixed value provided by the software.\nThe CPU and GPU measurements varied between the software and required using\nthe average value. However, by monitoring the utilisation, the author could verify the\nGPU usage across all the tests. Additionally, manually calculating the average for the\nWattmeter values was required.\nA significant limitation is the sample size, which included a single GPU, CPU, RAM\nand Wattmeter. Therefore, if there is a slight difference in the relationship between\nvariables or groups, as in this research, a large sample size will help obtain a more\naccurate statistic test [34]."}, {"title": "6 Conclusion and Future Work", "content": "In this research, the author discussed the potential improvement of the ML carbon\nfootprint by investigating different ML optimisation techniques. Current literature"}]}