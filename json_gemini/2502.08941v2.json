{"title": "Analysis of Off-Policy n-Step TD-Learning with Linear Function Approximation", "authors": ["Han-Dong Lim", "Donghwan Lee"], "abstract": "This paper analyzes multi-step temporal difference (TD)-learning algorithms within the \"deadly triad\" scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, in the second part, two n-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) [31] seeks to find an optimal sequence of decisions in unknown systems through experiences. Recent breakthroughs showcase RL algorithms surpassing human performance in various challenging tasks [1, 13, 19, 25, 27, 29, 36]. This success has ignited a surge of interest in RL, both theoretically and experimentally.\nAmong various algorithms, temporal-difference (TD) learning [30] stands as a cornerstone of RL, specifically for policy evaluation. Its convergence has been extensively studied over decades [34]. However, a critical challenge emerges within the \u201cdeadly triad\u201d scenario, characterized by linear function approximation, off-policy learning, and bootstrapping [7, 31, 35]. In such scenarios, TD-learning can diverge, leading to unreliable value estimates.\nRecently, gradient temporal-difference learning (GTD) has been developed and investigated in various studies [11, 18, 20, 32, 33]. This method addresses the deadly triad issue by employing gradient-based schemes. However, the GTD family of algorithms requires somewhat restrictive assumptions about the underlying environment, which constitutes a limitation of the method. Another approach, such as emphatic method [12] or adding a regularization term [3], fixes the deadly triad issue but converges to a biased solution. [39] also used a regularization, which results in a biased solution. Furthermore, a target network update and a projection step are required. A comprehensive overview of off policy TD-learning algorithms can be found in [9].\nOn the other hand, TD-learning is usually implemented within the context of single-step bootstrapping based on a single transition, which is known as single-step TD-learning. These methods can be extended to include multiple time steps, a class of algorithms known as multi-step TD learning, to enhance performance. Recently, multi-step approaches [5, 8, 10, 22\u201324, 26, 28, 31, 34, 37], including n-step TD-learning and TD(\u03bb), have become integral to the success of modern deep RL agents, significantly improving performance [14, 15, 28,38] in various scenarios. Despite these empirical successes and the growing body of analysis on multi-step RL, to the best of the author's knowledge, the effects and theoretical underpinnings of n-step TD-learning have yet to be fully explored.\nMotivated by the aforementioned discussions, this paper conducts an in-depth examination of the theoretical foundations necessary to understand the core principles of n-step TD-learning methods and their model-based counterparts, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free RL counterparts. First, we investigate the convergence conditions for n-step projected value iteration and present an algorithm for solving the projected n-step Bellman equation. We show that the projected Bellman operator becomes a contraction mapping for sufficiently large n, ensuring the convergence of the corresponding algorithms. We also establish a relationship between this convergence and the singularity of the matrix governing the n-step TD method. Next, we demonstrate that n-step TD methods effectively mitigate the challenges of the deadly triad when the sampling horizon, n, is sufficiently large. Our thorough analysis of the conditions on n offering valuable insights, and we provide an interesting example why sharpening the bound might be difficult. Overall, we present the relationship between the choice of n for the convergence of n-step projected value iteration, the singularity of the matrix, and the stability of the n-step TD method.\nLastly, following the spirit of [4], we prove the asymptotic convergence of n-step TD-learning method under both the i.i.d. and Markov observation models. The asymptotic convergence relies on the theoretical properties derived based on its model-based counterparts. We investigate the ODE counterpart of the n-step TD-learning, which inherits the properties of the model-based deterministic counterparts."}, {"title": "2 Preliminaries", "content": "2.1 Notation\nThe adopted notation is as follows: R: set of real numbers; \u211d+: set of positive real numbers; \u211d\u207f: n-dimensional Euclidean space; \u211d\u207f\u02e3\u1d50: set of all n \u00d7 m real matrices; A\u1d40: transpose of matrix A; A > 0 (A \u226f 0, A \u2265 0, and A \u2264 0, respectively): symmetric positive definite (negative definite, positive semi-definite, and negative semi-definite, respectively) matrix A; I: identity matrix with appropriate dimensions; \u03bb\u2098\u1d62\u2099(A) and \u03bb\u2098\u2090\u2093(A) for any matrix A: the minimum and maximum eigenvalues of A; |S|: cardinality of a finite set S; ||\u22c5||\u221e : infinity norm of a matrix or vector;\n2.2 Markov decision process\nA Markov decision process (MDP) is characterized by a quintuple \ud835\udcdc := (\ud835\udce2, \ud835\udcd0, P, r, \u03b3), where \ud835\udce2 is a finite state-space, \ud835\udcd0 is a finite action space, P(s'|s, a) represents the (unknown) state transition probability from state s to s' given action a, r : \ud835\udce2 \u00d7 \ud835\udcd0 \u00d7 \ud835\udce2 \u2192 \u211d is the reward function, and \u03b3 \u2208 (0, 1) is the discount factor. In particular, if action a is selected with the current state s, then the state transits to s' with probability P(s'|s, a) and incurs a reward r(s, a, s'). For convenience, we consider a deterministic reward function and simply write r(s\u2096, a\u2096, s\u2096\u208a\u2081) =: r\u2096\u208a\u2081,\u2096 \u2208 {0, 1, ...}. As long as the reward function is bounded, we can assume that the reward function follows a probability distribution depending on (s, a, s').\nThe stochastic policy represents a probability distribution over the action space. Consider a policy \u03c0 : \ud835\udce2 \u00d7 \ud835\udcd0 \u2192 [0, 1] representing the probability, \u03c0(a|s), of selecting action a at the current state s, P^\u03c0 denotes the state transition probability matrix under policy \u03c0, and d^\u03c0 : \ud835\udce2 \u2192 \u211d denotes the stationary probability distribution of the states s \u2208 \ud835\udce2 under \u03c0. We also define R^\u03c0(s) as the expected reward given the policy \u03c0 and the current state s. The infinite-horizon discounted value function with policy \u03c0 is \u03c5^\u03c0(s) := E [\u03a3\u2096\u208c\u2080^\u221e \u03b3\u1d4fr(s\u2096, a\u2096, s\u2096\u208a\u2081) | s\u2080 = s], where E stands for the expectation taken with respect to the state-action trajectories under \u03c0. Given pre-selected basis (or feature) functions \u03d5\u2081, ..., \u03d5\u2098 : \ud835\udce2 \u2192 \u211d, the matrix, \u03a6 \u2208 \u211d^(|\ud835\udce2|\u00d7m), called the feature matrix, is defined as a matrix whose s-th row vector is \u03d5(s) := [\u03d5\u2081(s) ... \u03d5\u2098(s)]. Throughout the paper, we assume that \u03a6 \u2208 \u211d^(|\ud835\udce2|\u00d7m) is a full column rank matrix, which can be guaranteed by using Gaussian basis or Fourier feature functions. The policy evaluation problem is the problem of estimating v^\u03c0 given a policy \u03c0. In this paper, we will denote V^\u03c0 \u2208 \u211d^(|\ud835\udce2|) to be a vector representation of the value function, i.e., the s-th element of V^\u03c0 corresponds to v^\u03c0(s)."}, {"title": "Definition 1 (Policy evaluation problem)", "content": "In this paper, the policy evaluation problem is defined as finding the least-square solution\n$\\theta_* = \\arg \\min_{\\theta \\in \\mathbb{R}^m} f(\\theta), \\quad f(\\theta) = \\frac{1}{2}|| \\Pi(\\mathbb{R}^\\pi + \\gamma P^\\pi \\Phi \\theta) - \\Phi \\theta ||_{D_\\beta}^2$,\nwhere V^\u03c0 := \u03a3\u2096\u208c\u2080^\u221e (\u03b3\u1d4f P^\u03c0\u1d40)\u1d4f R^\u03c0 \u2208 \u211d^(|\ud835\udce2|) is the true value function, R^\u03c0 \u2208 \u211d^(|\ud835\udce2|) is a vector enumerating all R^\u03c0(s), s \u2208 \ud835\udce2, D^\u03b2 is a diagonal matrix with positive diagonal elements d^\u03b2(s), s \u2208 \ud835\udce2, and ||x||_D := \u221a(x\u1d40Dx) for any positive-definite D. Here, d^\u03b2 can be any state visit distribution under the behavior policy \u03b2 such that d^\u03b2(s) > 0, \u2200s \u2208 \ud835\udce2. The solution can be written as\n$\\Phi \\Theta_* = \\Pi V^\\pi.$\n(1)\n\u03a0 is the projection onto the range space of \u03a6, denoted by R(\u03a6): \u03a0(x) := arg min_(x'\u2208R(\u03a6)) ||x \u2212 x'||_(D^\u03b2). The projection can be performed by the matrix multiplication: we"}, {"title": "2.3 Review of GTD algorithm", "content": "In this section, we briefly review the gradient temporal difference (GTD) learning developed in [32], which tries to solve the policy evaluation problem. Roughly speaking, the goal of the policy evaluation is to find the weight vector \u03b8 such that \u03a6\u03b8 approximates the true value function V^\u03c0. This is typically done by minimizing the so-called mean-square projected Bellman error loss function [32,33]\n$\\min_{\\Theta \\in \\mathbb{R}^m} \\operatorname{MSPBE}(\\theta):= \\frac{1}{2}||\\Pi(\\mathbb{R}^\\pi + \\gamma P^\\pi \\Phi \\theta) - \\Phi \\theta||^2_{D_\\beta}.$\n(2)\nNote that minimizing the objective means minimizing the error of the projected Bellman equation (PBE) \u03a6\u03b8 = \u03a0(R^\u03c0 + \u03b3P^\u03c0 \u03a6\u03b8) with respect to ||\u22c5||_(D^\u03b2). Moreover, note that in the objective of (2), d^\u03b2 depends on the behavior policy, \u03b2, while P^\u03c0 and R^\u03c0 depend on the target policy, \u03c0, that we want to evaluate. This structure allows us to obtain an off-policy learning algorithm through the importance sampling [26] or sub-sampling techniques [32].\nA common assumption in proving the convergence of GTD [11,18,32,33] is the following assumption:\nAssumption 1 \u03a6\u1d40D^(\u03b2)(\u03b3P^\u03c0 \u2212 I)\u03a6 is nonsingular.\nPlease note that Assumption 1 always holds when \u03b2 = \u03c0, while it is in general not true. It will be clear in further section how this assumption can be relaxed using n-step methods. A sufficiently large choice of n can relax this assumption. Moreover, the value of n is chosen to be finite, which clearly differs with the Monte-Carlo setting where n \u2192 \u221e.\nSome properties related to (2) are summarized below for convenience and completeness.\nLemma 1 (Lemma 3 in [18]) Under Assumption 1, the following statements hold true:\n(1) A solution of (2) exists, and is unique.\n(2) The solution of (2) is given by\n$\\theta_* := -(\\Phi^T D^\\beta (\\gamma P^\\pi - I)\\Phi)^{-1} \\Phi^T D^\\beta \\mathbb{R}^\\pi.$\n(3)"}, {"title": "3 Multi-step projected Bellman operator", "content": "Let us consider the n-step Bellman operator [30]\n$\\mathcal{T}_n(x) := (I + \\gamma P^\\pi + \\cdots + \\gamma^{n-1}(P^\\pi)^{n-1})\\mathbb{R}^\\pi + \\gamma^{n}(P^\\pi)^{n}x.$\nThen, the corresponding projected n-step Bellman operator (n-PBO) is given by \u03a0\ud835\udcaf\u2099. Based on this, the corresponding n-step projected value iteration (n-PVI) is given by\n$\\Phi \\theta_{k+1} = \\Pi \\mathcal{T}_n (\\Phi \\theta_k), \\quad k \\in \\{0, 1, ...\\}, \\theta_0 \\in \\mathbb{R}^m.$\n(4)\nNote that at each iteration k, \u03b8\u2096\u208a\u2081 can be uniquely determined given \u03b8\u2096 because \u03a0\ud835\udcaf\u2099(\u03a6\u03b8\u2096) belongs to the image of \u03a6, and the unique solution solves \u03a6\u03b8 = \u03a0\ud835\udcaf\u2099(\u03a6\u03b8\u2096), and is given by\n$\\theta_{k+1} = (\\Phi^T D^\\beta \\Phi)^{-1} \\Phi^T D^\\beta \\mathcal{T}_n(\\Phi \\theta_k).$\n(5)\nMoreover, it is important to note that \u03a0 \u2208 \u211d^(|\ud835\udce2|\u00d7|\ud835\udce2|) is a projection onto the column space of the feature matrix \u03a6 with respect to the weighted norm ||\u22c5||_(D^\u03b2), and satisfies the nonexpansive mapping property ||\u03a0x \u2212 \u03a0y||_(D^\u03b2) \u2264 ||x \u2212 y||_(D^\u03b2) with respect to ||\u22c5||_(D^\u03b2). On the other hand, for the Bellman operator \ud835\udcaf\u2099, we can consider the two cases depending on the behavior policy and target policy:\n(1) on-policy case: the behavior policy and target policy are identical, i.e., \u03b2 = \u03c0,\n(2) off-policy case: the behavior policy and target policy are different, i.e., \u03b2 \u2260 \u03c0.\nIn the on-policy case \u03b2 = \u03c0, it can be easily proved that \ud835\udcaf\u2099 is a contraction mapping with respect to the norm ||\u22c5||_(D^\u03b2) with the contraction factor \u03b3\u207f.\nLemma 2 If \u03b2 = \u03c0, the mapping \ud835\udcaf\u2099 satisfies\n$||\\mathcal{T}_n(x) - \\mathcal{T}_n(y)||_{D_\\pi} \\leq \\gamma^n ||x - y||_{D_\\pi}, \\quad \\forall x, y \\in \\mathbb{R}^{|\\mathcal{S}|}.$\nProof. The proof can be easily done by following the main ideas of [34, Lemma 4], and omitted here for brevity.\nTherefore, n-PBO, \u03a0\ud835\udcaf\u2099, is also a contraction with the factor \u03b3\u207f.\nLemma 3 If \u03b2 = \u03c0, the mapping \u03a0\ud835\udcaf\u2099 satisfies\n$||\\Pi\\mathcal{T}_n (x) - \\Pi\\mathcal{T}_n(y)||_{D_\\pi} < \\gamma^n ||x - y||_{D_\\pi}, \\quad \\forall x, y \\in \\mathbb{R}^{|\\mathcal{S}|}.$\nThe above result implies that \u03a0\ud835\udcaf\u2099 is a contraction. In conclusion, by Banach fixed point theorem, n-PVI in (4) converges to its unique fixed point because n-PBO \u03a0\ud835\udcaf\u2099 is a contraction with respect to ||\u22c5||_(D^\u03b2).\nOn the other hand, in the off-policy case \u03b2 \u2260 \u03c0, \ud835\udcaf\u2099 is no more a contraction mapping with respect to ||\u22c5||_(D^\u03b2), and so is \u03a0\ud835\udcaf\u2099. Therefore, n-PVI in (4) may not converge in"}, {"title": "Definition 2", "content": "A solution of the n-PBE, \u03b8\u2099\u2217, if exists, is defined as a vector satisfying\n$\\Phi \\theta_n^* - \\Pi \\mathcal{T}_n(\\Phi \\theta_n^*).$\n(6)"}, {"title": "Theorem 1", "content": "The matrix A defined in (8) is Schur if and only if \u03a0\ud835\udcaf\u2099 is a contraction.\nProof. Noting that n-PVI is equivalently written by (7), one can easily prove that the convergence of n-PVI is equivalent to that of the linear system in (7). Moreover, from the standard linear system theory, (7) converges to a unique point if and only if A is Schur. Then, since \u03a0\ud835\udcaf\u2099 is an affine mapping, one arrives at the desired conclusion using Lemma 8.\nRemark 1 Theorem 1 implies the equivalence between the matrix A being Schur and \u03a0\ud835\udcaf\u2099 being a contraction. Lemma 8 ensures the equivalence of \u03a0\ud835\udcaf\u2099 being a contraction and convergence of n-PVI. Therefore, we can conclude that A is Schur if and only if n-PVI converges.\nIn the next theorem, we establish a connection between the contraction property of \u03a0\ud835\udcaf\u2099 and the nonsingularity of \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6, which plays an important role throughout the paper."}, {"title": "Lemma 4", "content": "(Corollary 5.6.16 in [16]) If M \u2208 \u211d^(n\u00d7n) satisfies ||M|| < 1 for some matrix norm ||\u22c5||, then I \u2212 M is nonsingular, and\n$||(I - M)^{-1}|| \\leq \\frac{1}{1 - ||M||}$"}, {"title": "Theorem 2", "content": "If A is Schur, then \u03a6\u1d40D^(\u03b2)(I \u2212 A) = \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6 is nonsingular.\nProof. If A is Schur, then \u03c1(A) < 1, where \u03c1 is the spectral radius. Then, following similar arguments as in the proof of Lemma 7, one can prove that there exists a matrix norm ||\u22c5|| such that ||A|| < 1. Then, by Lemma 4, we have that I - A = I \u2212 (\u03a6\u1d40 D^\u03b2\u03a6)\u207b\u00b9\u03a6\u1d40D^(\u03b2)\u03b3\u207f(P^\u03c0)\u207f is nonsingular. Equivalently, \u03a6\u1d40D^(\u03b2)(I \u2212 A) = \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6 is nonsingular.\nRemark 2 With sufficiently large n, we can now relax the Assumption 1. However, the nonsingularity of \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6 does not imply that n-PBO, \u03a0\ud835\udcaf\u2099, is a contraction mapping with respect to some ||\u22c5||. To support this argument, we provide a counter example below."}, {"title": "Example 1", "content": "Let us consider a MDP with two states and a single action:\n$\\Phi := \\begin{bmatrix} 1 & 0.5 \\\\ 1 & -0.5 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \\quad P = \\begin{bmatrix} 0 & 0.1 \\\\ 0 & 1 \\end{bmatrix}$,\nand n = 1 and \u03b3 = 0.99. Then, \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6 is non-singular but \u03a0\ud835\udcaf\u2099 is not a contraction mapping, i.e., the spectral radius of \u03b3\u03a0(P^\u03c0)\u207f is bigger than one.\nIn summary, we have proved until now that\n(\u03a0\ud835\udcaf\u2099 is contraction \u27fa A is Schur)\n\u21d2 \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6 is non-singular,\nwhile the converse does not necessarily holds.\nNext, we establish and summarize several results such as a sufficient condition on n such that A becomes Schur."}, {"title": "Lemma 5", "content": "The following statements hold true:\n(1) There exists a positive integer n\u2081 such that A is Schur:\n$n_1 \\leq [\\frac{\\ln(\\frac{1}{||(\\Phi^T D^\\beta \\Phi)^{-1} \\Phi^T D^\\beta||_\\infty ||P||_\\infty})}{\\ln(\\gamma)}].$\n(2) Suppose that n \u2265 n\u2081 so that A is Schur. Then, \u03a6\u1d40D^(\u03b2)(I \u2212 \u03b3\u207f(P^\u03c0)\u207f)\u03a6 is nonsingular. Moreover, n-PVI in (4) converges to the unique fixed point \u03b8\u2217 and satisfies\n$||\\theta_k - \\theta_* ||_\\infty \\leq ||A||_\\infty^k ||\\theta_0 - \\theta_*||_\\infty.$"}, {"title": "Theorem 3", "content": "For all n \u2265 n\u2081, \u03a0\ud835\udcaf\u2099 is a contraction with respect to ||\u22c5||\u221e. Then, we have\n$||\\Phi \\theta_n^* - V^\\pi ||_\\infty \\leq \\frac{1}{1 - \\gamma^n ||\\Pi||_\\infty} ||\\Pi V^\\pi - V^\\pi||_\\infty$\n(10)\nand\n$||\\Phi \\theta_* - \\Phi \\theta_n^*||_\\infty \\leq \\frac{\\gamma^n ||\\Pi||_\\infty}{1 - \\gamma^n ||\\Pi||_\\infty} ||\\Pi V^\\pi - V^\\pi||_\\infty$\n(11)\nProof. By hypothesis, \u03a0\ud835\udcaf\u2099 is a contraction, which means that there exists a unique solution \u03b8\u2099 satisfying n-PBE, which can be rewritten by\n$\\Pi \\mathcal{T}_n(\\Phi \\theta_n^*) - V^\\pi = \\Phi \\theta_n^* - V^\\pi.$\nThe left-hand side can be written as\n$\\Phi \\theta_n^* - V^\\pi = \\Pi \\mathcal{T}_n(\\Phi \\theta_n^*) - V^\\pi = \\Pi(\\mathcal{T}_n(\\Phi \\theta_n^*) - V^\\pi) + \\Pi V^\\pi - V^\\pi = \\Pi(\\sum_{k=0}^{n-1} \\gamma^k(P^\\pi)^k \\mathbb{R}^\\pi + \\gamma^n(P^\\pi)^n \\Phi \\theta_n^* - V^\\pi) + \\Pi V^\\pi - V^\\pi = \\Pi (\\gamma^n (P^\\pi)^n \\Phi \\theta_n^* - \\gamma^n (P^\\pi)^n V^\\pi) + \\Pi V^\\pi - V^\\pi.$\nNext, taking the norm ||\u22c5||\u221e on both sides of the above inequality leads to\n$||\\Phi \\theta_n^* - V^\\pi||_\\infty \\leq ||\\gamma \\Pi(P^\\pi)^n (\\Phi \\theta_n^* - V^\\pi)||_\\infty + ||\\Pi V^\\pi - V^\\pi||_\\infty \\leq \\gamma^n ||\\Pi||_\\infty ||\\Phi \\theta_n^* - V^\\pi||_\\infty + ||\\Pi V^\\pi - V^\\pi||_\\infty,$\nwhich yields\n$(1 - \\gamma^n ||\\Pi||_\\infty) ||\\Phi \\theta_n^* - V^\\pi||_\\infty \\leq ||\\Pi V^\\pi - V^\\pi||_\\infty.$\nBy hypothesis, n \u2265 n\u2080 implies that 1 \u2212 \u03b3\u207f||\u03a0||\u221e > 0 holds. Therefore, the last inequality leads to (10).\nSimilarly, combining (6) and (1) yields\n$\\Phi (\\theta_* - \\theta_n^*) = \\Pi (\\mathbb{V}^* - \\sum_{k=0}^{n-1} \\gamma^k (P^\\pi)^k \\mathbb{R}^\\pi + \\gamma^n (P^\\pi)^n \\Phi \\theta_n^* - \\mathbb{V}^\\pi) = \\gamma^n \\Pi((P^\\pi)^n \\Phi \\theta_* - (P^\\pi)^n V^\\pi).$"}, {"title": "4 Deterministic algorithm", "content": "In this section, we will consider another class of model-based iterative algorithms motivated by the methods for solving general linear equations [17]. In particular, let us first consider the n-PBE in (6) again\n$\\Phi^T D^\\beta \\mathcal{T}_n (\\Phi \\theta) = \\Phi^T D^\\beta \\Phi \\theta,$\nwhich can be written as the following linear equation form:\n$\\Phi^T D^\\beta (\\mathbb{R}^\\pi + \\gamma P^\\pi \\mathbb{R}^\\pi + \\cdots + \\gamma^{n-1}(P^\\pi)^{n-1}\\mathbb{R}^\\pi) =: b \\\\ \\Phi^T D^\\beta (I - \\gamma^n (P^\\pi)^n) \\Phi \\theta =: N.$\nWe consider a Richardson type iteration [17] of the form\n$\\theta_{k+1} = \\theta_k + \\alpha \\Phi^T D^\\beta (\\mathcal{T}_n(\\Phi \\theta_k) - \\Phi \\theta_k),$\n(12)\nwhere \u03b1 > 0 is the step-size. Combining (12) and the fixed point equation in (6), it follows that\n$\\theta_{k+1} - \\theta = (I - \\alpha N)\\theta,$\n(13)"}, {"title": "Theorem 4", "content": "There exists a positive integer n* < \u221e such that \u03a6\u1d40D^(\u03b2)(\u03b3\u207f(P^\u03c0)\u207f \u2212 I)\u03a6 becomes negative definite and Hurwitz. Moreover, n \u2264 n^(th) where\n$n_{th} = \\frac{\\ln(\\max \\{\\frac{d_{min} \\lambda_{min}(\\Phi^T \\Phi)}{d_{max} \\max_{s \\in \\mathcal{S}} ||\\phi(s)||^2}, \\frac{d_{min} \\lambda_{min}(\\Phi^T \\Phi)}{d_{max} \\lambda_{max}(\\Phi^T \\Phi) \\sqrt{|\\mathcal{S}|}} \\})}{\\ln(\\gamma)},$\nwhere d^(min) = min_(s\u2208S) d^(\u03b2)(s), d^(max) = max_(s\u2208S) d^(\u03b2)(s), and \u03d5^(max) = max_(s\u2208S) ||\u03d5(s)||^2. Furthermore, it becomes negative definite for all n \u2265 n^(th).\nProof. Since\n$\\Phi^T D^\\beta (\\gamma^n (P^\\pi)^n - I)\\Phi = \\Phi^T D^\\beta (\\gamma^n (P^\\pi)^n)\\Phi - \\Phi^T D^\\beta \\Phi \\leq \\Phi^T D^\\beta (\\gamma^n (P^\\pi)^n)\\Phi - d_{min} \\lambda_{min}(\\Phi^T \\Phi)I,$\nit is enough to show that\n$x^T (\\Phi^T D^\\beta (\\gamma^n (P^\\pi)^n)\\Phi) x \\leq d_{min} \\lambda_{min}(\\Phi^T \\Phi) ||x||^2.$\n(14)\nfor x \u2208 \u211d\u1d50 except at the origin. There are two approaches in bounding x\u1d40(\u03a6\u1d40D^(\u03b2) (\u03b3\u207f(P^\u03c0)\u207f)\u03a6) x. The first is\n$x^T (\\Phi^T D^\\beta (\\gamma^n (P^\\pi)^n)\\Phi) x = \\gamma^n \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{s' \\in \\mathcal{S}} [(P^\\pi)^n]_{ss'} x^T \\phi(s) \\phi(s')^T x \\leq \\gamma^n \\phi_{max} ||x||^2.$\nTherefore, we require\n$n \\geq \\frac{\\ln (\\frac{d_{min} \\lambda_{min}(\\Phi^T \\Phi)}{\\phi_{max}})}{\\ln(\\gamma)}.$\n(15)\nMeanwhile, another approach to satisfy (14) is\n$\\lambda_{max} (\\Phi^T D^\\beta (P^\\pi)^n \\Phi + \\Phi^T ((P^\\pi)^T)^n D^\\beta \\Phi) \\leq 2 d_{min} \\lambda_{min}(\\Phi^T \\Phi).$\n(16)"}, {"title": "Lemma 6", "content": "Suppose that a matrix B is Hurwitz stable. Then, there exists a sufficiently small \u03b1' > 0 such that I + \u03b1B is Schur stable for all \u03b1 < \u03b1'. If we define the positive real number \u03b1\u2217 as the supremum of \u03b1\u2217 such that I + \u03b1B is Schur for all \u03b1 < \u03b1\u2217, then\n$\\alpha^* > \\frac{1}{\\lambda_{max}(P) \\lambda_{max}(B^T B)},$\nwhere P \u227b 0 satisfies B\u1d40P + PB = \u2212I [6].\nProof. If B is Hurwitz stable, then by the Lyapunov argument, there exists a Lyapunov matrix P \u227b 0 such that B\u1d40P + PB = \u2212I [6]. Next, we have\n$(I + \\alpha B) P (I + \\alpha B) = P - \\alpha I + \\alpha^2 B^T P B.$"}, {"title": "Theorem 5", "content": "(Convergence) Suppose that n > n\u2217 so that \u03a6\u1d40D^(\u03b2)(\u03b3\u207f(P^\u03c0)\u207f \u2212 I)\u03a6 is Hurwitz. Then, there exists a positive real number \u03b1\u2217 such that for any \u03b1 \u2266 \u03b1\u2217, the iterate in (12) converges to \u03b8\u2217.\nProof. By Lemma 6, there exists a sufficiently small \u03b1\u2217 > 0 such that I \u2212 \u03b1N is Schur stable.\nIn this section, we have proposed a different algorithm in (12) from the classical dynamic programming in Section 3 and the gradient descent methods in Section 2.3, and analyzed its convergence based on the control system perspectives [6]. All the iterative algorithms studied until now assume that the model is already known. In the next section, we will study model-free reinforcement learning algorithms based on these algorithms."}, {"title": "5 Off-policy n-step TD-learning", "content": "For convenience", "25": "which stores the samples collected following the behavior policy. From this experience replay buffer", "manner": "E_(\u03b2", "X(U)": "\u03a3_(a\u2208A) \u03c0(a|s)/\u03b2(a|s) * \u03b2(a|s)X(a) = E_(\u03c0"}, {"X(U)": "for X: A \u2192 R.\nAlgorithm 1 n-step off-policy TD-learning\n1: Initialize \u03b8\u2080 \u2208 \u211d\u1d50.\n2: for iteration step i \u2208 {0", "do\n3": "Sample s\u2080 \u223c d^\u03b2", "oracle.\n4": "Update parameters according to\n$\\theta_{i+1"}], "\u03c1_(n-1)": "\u03a0_(k=0)^(n-1) (\u03c0(a_k|s_k))/(\u03b2(a_k|s_k)) is the importance sampling ratio", "\u03a6\u03b8_i\n5": "end for\nAlgorithm 2 n-step off-policy TD-learning: Markovian observation model\n1: Initialize \u03b8\u2080 \u2208 \u211d\u1d50.\n2: Sample (s_0", "\u03b2.\n3": "for iteration step i \u2208 {0", "do\n4": "Sample a_(n-1+i) ~ \u03b2(\u22c5 | s_(n-1+i)) and s_(n+i) ~ P(\u22c5 | s_(n-1+i)", "a_(n-1+i)).\n5": "Update parameters according to\n$\\theta_{i+1} = \\theta_i + \\alpha_i \\rho_i (G_i - V_{\\theta_i} (s_i)) \\varphi(s_i),$\n(18)\nwhere \u03c1_i = \u03a0_(k=0)^(n-1) (\u03c0(a_(k+i)|s_(k+i)))/(\u03b2(a_(k+i)|s_(k+i))) is the importance sampling ratio, \u03c6(s) = \u03a6^(Tes) is the s-th row vector of \u03a6, G_i = \u03a3_(k=0)^(n-1) \u03b3^k r_(k+i) + \u03b3^n V_(\u03b8_i) (s_n), and V_(\u03b8_i) (s) = \u03c6^T \u03a6\u03b8_i"}