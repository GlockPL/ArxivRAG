{"title": "Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Hao Liu"], "abstract": "Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose REPST, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of REPST is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.", "sections": [{"title": "1 Introduction", "content": "Spatio-temporal time series forecasting aims to predict future states of real-world complex systems by simultaneously learning spatial and temporal dependencies of historical observations, which plays a pivotal role in diverse real-world applications, such as traffic management [21, 37], environmental monitoring [12], and resource optimization [6]. In recent years, deep learning has demonstrated great predictive power and led to a surge in deep spatio-temporal forecasting models [38, 16]. For example, Recurrent Neural Networks (RNNs) and Graph Neural Networks (GNNs) are widely leveraged to capture complex spatio-temporal patterns [16]. Despite fruitful progress, existing approaches are typically limited to the one-task-one-model setting, which lacks general-purpose utility and inevitably falls short in handling practical data-scarce scenarios, e.g., newly deployed monitoring services."}, {"title": "2 Preliminaries", "content": "In this section, we introduce some basic notations and formally define the spatio-temporal forecasting problem we aim to address."}, {"title": "3 Methodology", "content": "As illustrated in Figure 1, REPST consists of three components: a spatio-temporal reprogramming block, a frozen Pre-trained Language Model (PLM), and a learnable output mapping layer. To be specific, in spatio-temporal reprogramming block, we first decompose the input signals in Fourier space and diffuse the high-frequency signals based on a pre-defined spatial topology structure. Then we utilize a differentiable discrete sampling strategy to build the relationship between the decoupled signals and text modality. After that, we employ a frozen PLM to obtain spatio-temporal representations for each node. Finally, a learnable mapping function generates future predictions based on the representations of PLM. We will introduce each component of REPST in detail below."}, {"title": "3.1 Spatio-Temporal Reprogramming", "content": "Recent studies have revealed that PLMs possess rich spatio-temporal knowledge and reasoning capabilities [10, 29, 18]. Regardless, PLMs showcase proficient language understanding and processing skills, but inherently lack an in-depth comprehension of intricate spatio-temporal patterns hidden in numerical data as they are not included in the pre-training dataset. In this section, we address the divergence between these two modalities through a carefully designed reprogramming module, making PLMs more easily to perform spatio-temporal forecasting tasks.\nSpatio-Temporal Decoupling. As aforementioned, language models face difficulties in comprehending the hybrid dynamics of spatio-temopral signals, which encompasses evolving patterns over both space and time. To fully unlock the spatio-temporal knowledge, we propose to decouple original inputs $X \\in \\mathbb{R}^{N\\times T\\times C}$ into temporal intrinsic signals $X_{int} \\in \\mathbb{R}^{N\\times T\\times}$ and spatial diffusive signals $X_{dif} \\in \\mathbb{R}^{N\\times T\\times C}$, denoted as $X = X_{int} + X_{dif}$. Concretely, our spatio-temporal decoupling block consists of two key components: Fourier analysis and structural diffusion operator, which are introduced as follows.\nInspired by previous works [27, 42] that model the time series in Fourier space, we first decompose the input spatio-temporal data in the frequency space using Fourier analysis. By doing so, we disentangle temporal intrinsic signals and spatial diffusion signals from intricate spatio-temporal time series by Fourier analysis and structural diffusion operator. Specifically, we calculate the frequency representations $\\mathcal{X} = \\text{Fourier}(X)$ for all nodes in the look-back window of $T$ steps and sort them based on the amplitude, where Fourier($\\cdot$) denotes the Fast Fourier Transform (FFT) operator. We take the top percentage of $\\epsilon$ as the high-frequency signals $\\mathcal{X}_{high}$, which represent the local fluctuations that can be diffused to spatial neighbors. The remaining low-frequency signals $\\mathcal{X}_{low}$ are the intrinsic signals along time dimension. The whole process can be formally defined as:\n$\\mathcal{X}_{high} = f_h(\\epsilon, \\mathcal{X}), \\mathcal{X}_{low} = f_l(\\epsilon, \\mathcal{X}),$ (2)\nwhere $f_h(\\cdot)$ and $f_l(\\cdot)$ are the filters that only pass the specific spectrums above and below $\\epsilon$, respectively. Intuitively, the low-frequency signals preserve the intrinsic characteristics of each node, whereas the high-frequency signals implicitly reflect the information of other nodes obtained through spatial interactions, such as traffic congestion and air pollution diffusion. We extensively conduct experiments to verify the effectiveness of this decoupling method. To enrich the diffusion information, we explicitly propagate the high-frequency signals over space via a structural diffusion operator. Motivated by spectral graph convolution [47], we leverage the eigenvectors of the Laplacian matrix of the pre-defined spatial graph $G$, defined as\n$X_{dif} = \\text{Fourier}^{-1}(U^T \\mathcal{X}_{high}),$ (3)\n$X_{int} = \\text{Fourier}^{-1}(\\mathcal{X}_{low}),$ (4)\nwhere $U$ represents the matrix of eigenvectors of the graph Laplacian matrix $M_G$ performed as $M_G = U\\Lambda U^T$, where $\\Lambda$ is the diagonal matrix of eigenvalues. The eigenvector matrix $U$ is used to diffuse the high-frequency signals $\\mathcal{X}_{high}$ in the Fourier domain, resulting in a frequency domain representation that incorporates the graph structural information. Finally, we perform an inverse FFT $\\text{Fourier}^{-1}(\\cdot)$ on the low- and high-frequency components to convert them back to the time domain and replace the original $X$ by concatenating $X_{int}$ and $X_{dif}$ as $X_{dec} = X_{int}||X_{dif} \\in \\mathbb{R}^{N\\times T\\times 2C}$. After Fourier analysis, we are able to effectively capture the underlying dynamics of the spatio-temporal data, which facilitates more precise modality alignment in semantic space.\nAdditionally, to enhance the information density of decoupled signals, we employ patching strategy [30, 44] to construct patches as the input tokens for PLMs. Given the decoupled signals $X_{dec}$, we divide the time series of each node as a series of non-overlapped patches $X_{dec}^P \\in \\mathbb{R}^{N \\times P \\times L_p \\times 2C}$, where $P = [T/T_p] + 1$ represents the number of the resulting patches, and $T_p$ denotes the patch length. Next, we encode the patched signals as patched embeddings $X_{enc} \\in \\mathbb{R}^{N\\times P \\times D}$:\n$X_{enc} = \\text{Conv}(X_{dec}^P, \\theta_p),$ (5)\nwhere $N$ stands for the number of nodes, and $D$ is the embedding dimension. Conv($\\cdot$) denotes the patch-wise convolution operator using filters with $1 \\times 1$ kernal size and $\\theta_p$ represents the learnable parameters of the patch-wise convolution. Unlike previous works[23, 26] that simply regard each node as a token, our model treats each patch as one token, allowing to construct relationships among"}, {"title": "Differentiable Discrete Reprogramming", "content": "patches. By doing so, our model can preserve fine-grained local semantic information, allowing a more comprehensive understanding of spatio-temporal data.\nExisting approach [17] aligns time series data by using a compressed vocabulary of the whole vocabulary $E \\in \\mathbb{R}^{V\\times d}$ of the PLMs, where $V$ is the vocabulary size and $d$ is the dimension of the embedding. However, due to the complex dynamic spatio-temporal dependencies, the semantics of spatio-temporal data are usually richer than that of ordinary time series. Therefore, such a method may encounter information bottleneck. Besides, they adopt linear mapping to condense the vocabulary, which mixes and distorts the original word meanings, leading to imprecise reprogramming. We resolve these issues by sampling original word embeddings from the whole vocabulary $E$ discretely and adaptively, which retains unmixed semantic information and makes full use of the expressive vocabulary at the same time.\nGiven the whole vocabulary $E$, we introduce a word mask vector $m \\in \\mathbb{R}^{V\\times 1}$ to select the most relevant words, where m[i] $\\in$ {0, 1}. In specific, we first obtain m through a linear layer followed by a Softmax activation, denoted as $m = \\text{Softmax}(EW)$, where $W$ is a learnable matrix. Afterward, we sample Top-K word embeddings from $E$ based on probability m[i] associated with word i for reprogramming. Since the sampling process is non-differentiable, we employ Gumbel-Softmax trick [14] to enable gradient calculation with back-propagation, defined as\n$m'[i] = \\frac{\\text{exp}((\\text{log } m[i] + g_i)/\\tau)}{\\Sigma_{j=1}^{V} \\text{exp}((\\text{log } m[i] + g_j)/\\tau)},$ (6)\nwhere $m'$ is a continuous relaxation of binary mask vector $m$ for word selection, $\\tau$ is temperature coefficient, $g_i$ and $g_j$ are i.i.d random variables sampled from distribution Gumbel(0, 1). Concretely, the Gumbel distribution can be derived by first sampling $u \\sim \\text{Uniform}(0, 1)$ and then computing $g_i = -\\text{log}(-\\text{log}(u))$. By doing so, we can expand vocabulary space while preserving the semantic meaning of each word.\nAfter obtaining the sampled word embeddings $E' \\in \\mathbb{R}^{K\\times d}$, we perform modality alignment by using cross-attention. In particular, we define the query matrix $X_q = X_{enc}W_q$, key matrix $X_k = E'W_k$ and value matrix $X_v = E'W_v$, where $W_q$, $W_k$, and $W_v$. After that, we calculate the reprogrammed patch embedding as follows\n$Z = \\text{Attn}(X_q, X_k, X_v) = \\text{Softmax}(\\frac{X_q X_k^T}{\\sqrt{d}})X_v,$ (7)\nwhere $Z\\in \\mathbb{R}^{N\\times P \\times d}$ denotes the aligned textual representations for the input spatio-temporal data."}, {"title": "3.2 Spatio-Temporal Predictor", "content": "Based on the aligned representation, we utilize the frozen PLMs as the backbone for further processing. Roughly, PLMs consist of three components: self-attention, Feedforward Neural Networks, and layer normalization layer, which contain most of the learned semantic knowledge from pre-training. The reprogrammed patch embedding $Z$ is encoded by this frozen language model to further process the semantic information and generates hidden textual representations $Z_{text}$. A learnable mapping function is then used to generate the desired target outputs, which map the textual representations into feature prediction.\nOverall, in our REPST, the process of predicting the future states $Y$ based on the history observation $X$ can be simply formulated as:\n$Z = \\text{De-Reprogramming}(X, E),$ (8)\n$Z_{text} = \\text{PLM}(Z),$ (9)\n$\\hat{Y} = \\text{Projection}(Z_{text}),$ (10)\nwhere De-Reprogramming($\\cdot, \\cdot$) represents the decoupling-based spatio-temporal reprogramming block and PLM($\\cdot$), Projection($\\cdot$) is the prozen PLM and learnable mapping function.\nModel Optimization. Following the previous GNN-based works [37, 32], our REPST aims to minimize the mean absolute error (MAE) between the predicted future states $\\hat{Y}$ and ground truth $Y."}, {"title": "Scalability", "content": "This provides us effective capability to generate predictions among various spatio-temporal scenarios.\n$\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y_i}|,$ (11)\nHere, $\\hat{y}, y$ represents a sample from $\\hat{Y}$ and $Y$, and $N$ represent the total number of samples.\nTechnically, the proposed REPST can be viewed as utilizing GPT-2 after performing cross-attention over $N \\times P$ patches and $V'$ word embeddings, which has the time and memory complexities that scale with $O(N \\cdot P \\cdot V' + (N \\cdot P)^2)$. Notably, the frozen GPT-2 blocks account for $O((N \\cdot P)^2)$, which do not participate in back propagation. To reduce such computational burden that undermines the application of the proposed method to large $N$, we train the model by partitioning the pre-defined spatial graph into multiple sub-graphs. In practice, we train REPST by sampling a sub-graph each time. By doing so, we can effectively reduce the computational costs and enable the model to scale to large $N$."}, {"title": "4 Experiments", "content": "We thoroughly validate the effectiveness of REPST on various real-world datasets, including the overall forecasting performance, the few-shot generability, the ablation study, and the case study. We first introduce the experimental settings, including datasets and baselines. Then we conduct experiments to compare the overall performance and few-shot of our REPST with other previous works. Furthermore, we design comprehensive ablation studies to evaluate the impact of the essential architectures and components. Finally, we conduct case studies to analyze the performance qualitatively."}, {"title": "4.1 Experimental Settings", "content": "We conducted experiments on four commonly used real-world datasets [34, 19], each varying in the fields of traffic, solar energy, and air quality. The traffic datasets, Beijing Taxi and NYC Bike, are collected every 30 minutes from hundreds of individual detectors spanning the freeway systems across all major metropolitan areas of Beijing and NYC. These datasets are extensively used in spatio-temporal forecasting studies due to their comprehensive coverage and detailed temporal resolution. The Air Quality dataset includes six indicators (PM2.5, PM10, NO2, CO, O3, SO2) to measure air quality, collected hourly from 35 stations across Beijing, providing a detailed temporal and spatial view of pollution levels. Lastly, the Solar Energy dataset records variations every 10 minutes from 137 photovoltaic (PV) plants across Alabama, capturing the dynamic changes in solar energy production across different locations and times. Each dataset comprises tens of thousands of time steps and hundreds of nodes, offering a robust foundation for evaluating spatio-temporal forecasting models."}, {"title": "4.1.2 Baselines", "content": "We extensively compare our proposed REPST with the state-of-the-art forecasting approaches, including (1) the GNN-based methods: Graph Wavenet [37], D2STGNN [33] (2) non-GNN-based state-of-the-art models: STID [32] and STNorm [4] which emphasizes the integration of spatial and temporal identities; (3) PLM-based time series forecasting models: FPT [46]. We reproduce all of the baselines based on the original paper or official code."}, {"title": "4.2 Overall Performance of REPST", "content": "Table 1 reports the overall performance of our proposed REPST as well as baselines in 4 real-world datasets with the best in bold and the second underlined. As can be seen, REPST outperforms all baselines in all spatio-temporal forecasting settings.\nNotably, REPST surpasses the state-of-the-art PLM-based time series forecaster FPT [46] by a large margin in spatio-temporal forecasting tasks, which can demonstrate that simply leveraging the PLMs cannot handle problems with complex spatial dependencies. The alignment of spatio-temporal data and textual representations plays an essential role in the excellent performance of our proposed model. Additionally, the performance of the impressive PLM-based time series forecaster FPT [46] is still ineffective in Solar Energy, Air Quality and traffic scenarios, indicating that PLM methods for time-series are inapplicable for spatio-temporal scenarios. Our spatio-temporal reprogramming block leverages a wide range of vocabulary and sample words that can adequately capture the spatio-temporal patterns, which do make an impact on unlocking the capabilities of PLMs to capture fine-grained spatio-temporal dynamics."}, {"title": "4.3 Few-shot Performance", "content": "PLMs are trained using large amounts of data that cover various fields, equipping them with cross-domain knowledge. Therefore, PLMs can utilize specific spatio-temporal related textual representations to unlock their capabilities for spatio-temporal reasoning, which can handle the difficulties caused by data sparsity. To verify this, we further conduct experiments on each field to evaluate the predictive performance of our proposed REPST in data-sparse scenarios. Our evaluation results are listed in Table 2. Concretely, all models are trained on 1-day data from the train datasets and tested on the whole test dataset. REPST consistently outperforms other deep models and PLM-based time series forecasters in various datasets. This illustrates that our REPST can perform well on a new downstream dataset and is suitable for spatio-temporal forecasting tasks with the problem of data sparsity.\nSpecifically, PLM-based forecaster FPT and our REPST show competitive performance over other deep model baselines in few-shot experiments. It demonstrates that PLMs contain a wealth of time series and spatio-temporal related knowledge from pre-training. Moreover, the capabilities of spatio-temporal reasoning can be enhanced by limited data. This shows a reliable performance when transferred to data-sparse scenarios."}, {"title": "4.4 Ablation Study", "content": "To figure out the effectiveness of each component in our REPST, we further conduct detailed ablation studies on Solar Energy and Beijing Air Quality with three variants as follows:\n\u2022 w/o Pre-trained Language Model backbone (PLM): it removes the pre-trained language model backbone.\n\u2022 w/o Spatio-temporal Decoupling Block (ST-Decoupling): it removes the Spatio-temporal decoupling block.\n\u2022 w/o Differentiable Discrete Reprogramming (DDR): it removes the discrete mapping vocabulary and utilizes the dense mapping function to decrease the vocabulary size.\nFigure 4 shows the comparative performance of the variants above on Solar Energy and Air Quality. Based on the results, we can make the conclusions as follows: (1) The PLM benefits modeling relationships among patches, effectively constructing spatial dependencies by leveraging multi-head attention. (2) The spatio-temporal decoupling block which decomposes input spatio-temporal data into intrinsic signals and diffusive signals can actually enable PLMs to better understand different data compositions. This results in relieving the burden of modality alignment as well. (3) The impressive performance in w/o Differentiable Discrete Vocabulary demonstrates that leveraging the whole vocabulary and selecting relevant discrete textual information achieves accurate reprogramming which enables PLM to better understand the underlying spatio-temporal patterns."}, {"title": "5 Related Works", "content": "Recently, Pretrained Language Models(PLMs) have shown outstanding performance on time series analysis tasks, including forecasting [46, 8, 43], classification[35], Anomaly Detection[46]. We have witnessed a large amount of effort in handling time series analysis tasks leveraging the power of PLMs [3, 46, 7, 20]. However, they fail to address the problem of the modality gap between time series and natural language. To solve this problem, Time-LLM[17] first utilizes reprogramming, which is an effective approach to handle the problem of modality gap. Its goal is to identify a trainable input data transformation function H that is applicable across all target input data, enabling the reprogramming of input data into the source data space. Examples include reprogramming an acoustic model to handle time series data [41].\nHowever, Time-LLM [17] only maintains a small compressed vocabulary, which might not be expressive enough for encoding complex spatio-temporal patterns, which is essential for spatio-temporal forecasting. Additionally, in order to handle the problem of over-fitting, Time-LLM obtains a small vocabulary by mapping the whole vocabulary of PLMs densely. Unlike previous methods, we design a framework based on a spatio-temporal reprogramming block to cater to complicated challenges in spatio-temporal scenarios, which can make use of the whole vocabulary of PLMs while relieving the risk of over-fitting."}, {"title": "5.2 Spatio-Temporal Forecasting", "content": "Deep learning based spatio-temporal forecasting models have gained great focus these years due to their impressive performance in multiple fields, such as traffic flow [24, 32, 25], air quality [12, 11], electricity[6], etc [28, 39, 13]. Early deep learning approaches often employed GNN to process spatial information and attracted great attention. Spatio-temporal Neural Networks (STGNNs) utilize Graph Convolution Networks (GCNs) to model spatial dependencies by processing predefined graph structures and sequential models for temporal combination. For example, DCRNN [21] combines GCN with RNN and its variants. The static graph failed to reflect the complete spatial dependencies accurately, however, which may change with time. An increasing trend is to utilize adaptive spatio-temporal graph neural networks in order to capture the dynamic spatial dependencies. Graph WaveNet [37] proposes to learn a normalized adaptive adjacency matrix without a pre-defined graph. AGCRN [1] designs a Node Adaptive Parameter Learning enhanced layer to learn node-specific patterns. Additional, attention mechanisms are widely used in STGNNs, such as GMAN [45], STAEFormer [24] and ASTGNN [9]. Moreover, MLP-based STID [32] reaches the state-of-the-art performance on most of the spatio-temporal datasets by utilizing multiple embeddings.\nDue to the exceptional generation capabilities of large language models, researchers are turning their attention to LLMs and considering the possibility of applying large language models to spatio-temporal data forecasting tasks. Several works [23, 26, 40, 15] explore the solution to leverage the Pre-trained language models to handle spatio-temporal tasks, among which, UrbanGPT [22] provides an elegant and performance-guaranteed end-to-end solution for spatio-temporal forecasting. It integrates spatio-temporal data with traditional textual information, enabling it to understand and predict urban dynamics. However, UrbanGPT does not explicitly align spatio-temporal information and natural language. This paper proposes a specially designed reprogramming block for spatio-temporal information and textual representations that can specifically align these two modalities."}, {"title": "6 Conclusion", "content": "In this paper, we explored the novel application of adopting Pre-trained Language Models (PLMs) for spatio-temporal forecasting by reprogramming spatio-temporal inputs into textual representations. To address the challenges in spatio-temporal modeling, we developed a spatio-temporal reprogramming block that decomposes input data into two types of spatio-temporal signals and transforms them into patches. These patches are then aligned textually using a differentiable discrete reprogramming strategy. The forecasting is made by a frozen GPT-2 backbone based on the reprogramed patches. Extensive experiments demonstrate that our proposed framework, REPST, achieves state-of-the-art performance on real-world datasets and exhibits exceptional capabilities in few-shot scenarios."}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Baseline Models", "content": "\u2022 D2STGNN: D2STGNN [33] is an advanced model designed to improve the accuracy and efficiency of traffic prediction by addressing the complexities inherent in spatial-temporal data. By decoupling spatial and temporal components, the model reduces complexity, making it more computationally efficient without sacrificing accuracy.\n\u2022 STAEFormer: STAEFormer is a cutting-edge model that elevates the standard Transformer architecture for traffic forecasting by incorporating Spatio-Temporal Adaptive Embeddings. These embeddings dynamically encode both spatial and temporal dependencies, allowing the model to capture the complex, evolving patterns typical in traffic data. The spatial embeddings represent geographical relationships between traffic nodes, while the temporal embeddings account for time-related patterns like rush hours or seasonal variations. Unlike traditional static embeddings, it adaptively adjusts to the changing traffic conditions, enhancing the model's ability to predict future traffic flows with greater accuracy.\n\u2022 STNorm: STNorm [4] normalizes data to better capture underlying patterns in both spatial and temporal dimensions. By addressing the variability in data across different time steps and locations, STNorm improves the accuracy of predictions, offering a robust approach to handling complex, dynamic datasets.\n\u2022 STID: STID [32] emphasizes the integration of spatial and temporal identities to enhance predictive performance. It employs unique identifiers for spatial and temporal components to effectively capture and utilize the inherent structure and patterns in the data.\n\u2022 FPT: FPT [46] demonstrate that partly frozen pre-trained models on natural language or images can handle all main time series analysis tasks."}, {"title": "A.2 Dataset Descriptions", "content": "We follow the same data processing and train-validation-test set split protocol used in the baseline models, where the train, validation, and test datasets are strictly divided according to chronological order to make sure there are no data leakage issues. As for the forecasting settings, we fix the length of the lookback series as 24, and the prediction length is 24. Four commonly used real-world datasets vary in fields of traffic (Beijing Taxi, NYC Bike), solar-energy and air quality (Air Quality), each of which holds tens of thousands of time steps and hundreds of nodes. Beijing Taxi and NYC Bike datasets are collected in every 30 minutes from nearly 40,000 individual detectors spanning the freeway system across all major metropolitan areas of NYC and Beijing, which are widely used in previous spatio-temporal forecasting studies. Air Quality dataset holds 6 indicators (PM2.5, PM10, NO2, CO, O3, SO2) to measure air quality. They are collected from 35 stations in every 1 hour. And solar-energy dataset collect the every 10 minutes variations of 137 PV plants across Alabama. Notably, we construct the graph for 35 stations by leveraging the time series similarity between nodes. The details of datasets are provided in Table 3"}, {"title": "A.3 Evaluation Metrics", "content": "Three metrics are used for evaluating the models: mean absolute error (MAE), mean absolute percentage error (MAPE) and root mean squared error (RMSE). Lower values of metrics stand for"}, {"title": "Implementation Details", "content": "better performance. RMSE and MAE measure absolute errors, while MAPE measures relative errors.\n$\\text{MAE} = \\frac{1}{N}\\sum_{i=1}^N |y_i - \\hat{y_i}|,$\n$\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y_i})^2},$\n$\\text{MAPE} = \\frac{100\\%}{N} \\sum_{i=1}^N |\\frac{y_i - \\hat{y_i}}{y_i}|,$\nwhere $y_i, \\hat{y_i}$ represents a sample from $\\hat{Y}$ and $Y$, and $N$ represent the total number of samples."}, {"title": "A.4 Implementation Details", "content": "Algorithm 1: REPST -Overall Architecture\nInput: history observation $X$, pre-defined spatial graph $G$, Fourier boundary $\\epsilon$, patch length $L_p$.\nResult: predictive future states $\\hat{Y}$.\n/* Step 1: Spatio-Temporal Decoupling */\n1 Decouple the spatio-temporal input into high-frequency signals and low-frequency signals by Equations: $X_{high} = f_h(\\epsilon, \\mathcal{X}), X_{low} = f_l(\\epsilon, \\mathcal{X});$\n2 Obtain spatial diffusion signals by diffusing high-frequency signals in spatial space by Equation:\n$X_{dif} = Fourier^{-1}(U^T \\mathcal{X}_{high});$\n3 Obtain temporal intrinsic signals by: $X_{int} = Fourier^{-1}(\\mathcal{X}_{low});$\n4 Divide series in each node into patches;\n5 Encode the patches as $X_{enc}$ by convolution operator: $X_{enc} = Conv(X_{dec}, \\theta_p);$\n/* Step 2: Differentiable Discrete Reprogramming */\n6 Initialize learnable vocabulary mask $m = Gumbel\\_Softmax(EW);$\n7 Obtain the sampled word embedding $E' = mask(m, E);$\n8 Reprogramming the spatio-temporal embedding into textual representations $Z$ by Equation 7;\n9 Encode by PLMS: $Z_{text} = PLM(Z)$;\n10 Generate final prediction by linear mapping function: $\\hat{Y} = Projection(Z_{text});$\n11 Return predictive future states $\\hat{Y}$."}, {"title": "B Show Cases", "content": "To provide the visualization of the prediction effect, we list the prediction showcases of certain nodes contained in dataset Solar Energy. Concretely, we visualize the prediction and ground in 326 time steps of four nodes from the node set."}, {"title": "C Experimental Details", "content": ""}, {"title": "C.1 Hyper Parameter Settings", "content": "For our prediction tasks, we aim to predict the next 12 steps of data based on the previous 12 steps. Both the historical length (T) and prediction length ($\\tau$) are set to 12. Moreover, the parameters for the convolution kernel in patch embedding layers are set to 3 and the number of the multi-head attention larers of reprogramming layer is set to 1. Additionally, we obtain the embedding of patches with the dimension of 64."}, {"title": "C.2 Further Experimental Setup Descriptions", "content": "During the reprogramming phrase, we sample 1000 most relevant words to capture the complex dynamic spatio-temporal dependencies. It is important to note that the missing data of the training"}, {"title": "D Broader Impact", "content": ""}, {"title": "D.1 Impact on Real-world Applications", "content": "Our work copes with real-world spatio-temporal forecasting, which is faced with problems of data sparsity and intrinsic non-stationarity that poses challenges for deep models to train a domain foundation model. Since previous works thoroughly explore the solutions to deal with various spatio-temporal dependencies, we propose a novel approach which leverage the power of pre-trained language models to handle spatio-temporal forecasting tasks, which fundamentally considers the natural connection between spatio-temporal information and natural language and achieves modality alignment by leveraging reprogramming. Without additional effort on prompts engineering [22, 40] which is a time-cost but essential part in enhancing the capabilities of pre-trained language models, our REPST automatically learns the spatio-temporal related vocabulary which can unlock the domain knowledge of pre-trained language models to do spatio-temporal reasoning and predictive generation."}, {"title": "D.2 Impact on Future Research", "content": "In this paper, we find that models trained on natural languages can handle spatio-temporal forecasting tasks, which is totally a different data modality from natural language. This demonstrates that aligning different data modality properly can unlock the domain knowledge obtained by the pre-trained model during the training process. Therefore, there is a possibility that models that pre-trained on data from various domains hold the capability to handle problems in different fields even if in different modalities. The underlying reasons why pre-trained models can handle cross-modality tasks still remains to be explain."}, {"title": "E Limitation", "content": "Our proposed model does not respectively consider the optimization of computational complexity. Due to the large amount of parameters obtained by pre-trained language models, it costs more time to train the model compared to other deep learning based models. Although training models by partitioning the per-defined spatial graph and keeping PLMs frozen can relieve, the problem of scalability stills plagues in our work. Therefore, methods for modality alignment between spatio-temporal information and textual representations which aim at computational efficiency can be promising for spatio-temporal forecasting with pre-trained language models, which leaves our future work."}]}