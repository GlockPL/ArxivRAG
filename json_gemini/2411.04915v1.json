{"title": "Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping", "authors": ["Bavo Lesy", "Ali Anwar", "Siegfried Mercelis"], "abstract": "Recently, there has been growing interest in autonomous shipping due to its potential to improve maritime efficiency and safety. The use of advanced technologies, such as artificial intelligence, can address the current navigational and operational challenges in autonomous shipping. In particular, inland waterway transport (IWT) presents a unique set of challenges, such as crowded waterways and variable environmental conditions. In such dynamic settings, the reliability and robustness of autonomous shipping solutions are critical factors for ensuring safe operations. This paper examines the robustness of benchmark deep reinforcement learning (RL) algorithms, implemented for IWT within an autonomous shipping simulator, and their ability to generate effective motion planning policies. We demonstrate that a model-free approach can achieve an adequate policy in the simulator, successfully navigating port environments never encountered during training. We focus particularly on Soft-Actor Critic (SAC), which we show to be inherently more robust to environmental disturbances compared to MuZero, a state-of-the-art model-based RL algorithm. In this paper, we take a significant step towards developing robust, applied RL frameworks that can be generalized to various vessel types and navigate complex port- and inland environments and scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the field of autonomous navigation and its logistic applicability have garnered significant attention, with promising results in industrial settings such as warehousing [1] and airborne deliveries [2]. However, an equally promising, yet less practiced area is autonomous shipping, with potential especially in autonomous inland waterway transport (IWT). IWT is one of the most CO2-efficient transport modes per ton of goods carried [3], making it a suitable candidate to reduce emissions. However, IWT has been declining for several years in countries such as The Netherlands and Belgium due to a shortage of skippers and the harsh competition of road transport. Autonomous shipping can greatly benefit IWT by reducing the number of skippers required, consequently reducing the operating costs and improving the competitiveness of IWT. The Central Commission for the Navigation of the Rhine (CCNR) has defined six levels of autonomy for autonomous vessels (AVs)\u00b9. Currently, most autonomous vessels reach level 2 autonomy, still requiring a skipper or human operator to be involved at all times to take control when necessary. In order to reduce the number of skippers, at least level 3 autonomy should be achieved so that a single human operator can supervise multiple vessels at the same time.\nMuch research aims at improving vessel autonomy, with major advancements in motion planning methods that utilize supervised deep learning techniques [4]. Motion planning deals with finding an optimal and feasible route for a (semi-)autonomous vehicle to reach its goal, whilst avoiding collisions and adhering to various constraints. Within the field of motion planning, deep reinforcement learning (RL) has shown promising results, outperforming traditional control schemes [5] [6]. Traditional control methods for motion planning and collision avoidance, such as the dynamic window approach (DWA) [7] and model predictive control (MPC) [8] require extensive parameter tuning to achieve optimal performance. Such methods are often tailored to specific scenarios, relying on predefined models of the vessel and its environment. Changes in the operational context, such as engine wear and tear, changes in climate conditions, or increased traffic in the area, may require the control system to be re-calibrated or re-designed.\nRL, on the other hand, excels in such environments because of its data-driven nature. RL does not require predefined models or extensive manual tuning. In RL, an agent (or controller) aims to solve a sequential decision-making problem by learning through trial and error. The agent learns a policy \u3160 that determines which actions to take in which state. Unlike supervised learning techniques, where explicit data labeling is required, RL techniques do not need a human-labeled dataset, instead learning from direct interaction with the environment, allowing it to adapt to a wide variety of conditions.The agent needs to have significant interaction with the environment before learning an adequate policy. Because of this, the environment in which the agent learns is typically a simulator in which the agent can explore freely, without the risks, time, or costs associated with performing a wrong"}, {"title": "II. RELATED WORKS", "content": "Since the literature in the field of autonomous shipping is vast, we focus specifically on motion planning and collision avoidance. For a more extensive view of deep learning methods for autonomous vessels, the reader is referred to [11] and [12]. Zhang et al. [13] employ a hybrid motion planning approach, autonomously generating both global and local paths, based on the artificial potential field (APF) and velocity obstacle (VO) algorithms, respectively. Tang et al. [14] utilize an LSTM model to predict the movement of AVS and plan a trajectory with MPC. Furthermore, Chun et al. [15] use RL to avoid collisions while complying with regulations. However, the assumption is made that all vessels share their position and velocity. In IWT, this is problematic since there are many static obstacles and smaller vessels that do not share this information. This is why in our work we rely on the observations of a ranging sensor to detect static and dynamic obstacles.\nVanneste et al. [16] focus on safe collision avoidance with static and dynamic obstacles, while proposing a model-predictive RL method for motion planning in a novel au-tonomous shipping simulator within the MOOS-IVP framework [17]. Recent work by Herremans et al. [6] extends on this, using a model-based reinforcement learning (MBRL) approach for path planning and collision avoidance in the same simulator. Waltz et al. [18] propose 2-level RL, focusing on high-level path planning and low-level path following. However, these works focus solely on performance in one specific environment, not investigating the robustness of their solutions under varying environmental conditions."}, {"title": "A. Autonomous Shipping", "content": "Much research aims to improve the robustness of RL algo-rithms, with methods such as domain randomization [19] and dynamics randomization [10] showing promising results and successfully transferring policies to the real world. Zhang et al. [20] enhance robustness by taking into account disturbances in context transitions, while Zhang et al. [21] investigate robustness in a multi-agent setting.\nPinto et al. [22] introduce an adversarial learning method to increase policy robustness. A second agent is introduced, which tries to degrade the performance of the primary agent by introducing disturbances into the simulator, turning the problem into a two-player zero-sum game. This adversarial learning approach has been extended by Zhai et al. [23], who utilize concepts of $H\u221e$-control to improve learning stability and robustness. Recent work by Sung et al. [24] also addresses robustness from a control-theoretic perspective by incorporating $L\u2081$ Adaptive Control into MBRL. Herremans et al. [25] adapt MBRL to account for model uncertainties, significantly improving robustness. Rigter et al. [26] introduce adversarial learning in the context of offline RL, increasing robustness in areas not covered by the initial dataset. Most of the existing literature focuses on introducing novel robustness methods whilst limiting the application to relatively simple environments. In a more practical context, Wang et al. [27] propose an adversarial RL approach to increase the robustness of AVs, focusing on the lower-level depth control of an autonomous underwater vehicle. Our work focuses on eval-uating the robustness of RL methods for higher-level motion planning and collision avoidance on inland waterways within the MOOS-IVP autonomous shipping simulator [16]."}, {"title": "B. Robust RL", "content": "Since we focus on higher-level motion planning and col-lision avoidance, and not lower-level control, we employ a three-degree-of-freedom (3-DOF) kinematic model [6] to simulate vessel movement. Using this model, the motion of a vessel at timestep t can be represented by a pose vector $p_t$ and a velocity vector $v_t$, where: $p_t = [x_t, y_t, \u03b8_t]$, with $(x_t, y_t)$ representing the position of the vessel at time t and $\u03b8_t$ the orientation of the vessel. $v_t = [s_t, w_t]^T$ is used to describe the rate at which the position and orientation of the vessel change. $s_t$ denotes the linear velocity and $w_t$ the angular velocity of the vessel. The rudder (yaw) and thrust (surge) of the vessel can be controlled with input $u_t = [u_{thrust}, u_{rudder}]^T$. During simulation, $v_t$ and $p_t$ are updated as follows:\n$v_{t+1} = v_t + \\begin{bmatrix} \\frac{u_{thrust}}{m} \\\\ \\frac{u_{rudder}}{R} \\end{bmatrix} dt$,\n(1)\n$p_{t+1} = p_t + \\begin{bmatrix} cos(\u03b8t) \\\\ sin(\u03b8t) \\\\ w_t \\end{bmatrix} v_t dt$,\n(2)\nwhere m is the mass of the vessel and R is the turn rate of the vessel."}, {"title": "III. METHODOLOGY", "content": "In RL, an agent learns to solve a sequential decision-making problem through continuous interaction with an environment. In this work, we solve the problem of motion planning in the MOOS-IVP simulator. This problem can be mathe-matically formulated as a Markov Decision Process (MDP). In this framework, the problem can be defined by a tuple (S, A, T, R, \u03b3), where S is the set of all possible states, A is the set of all possible actions, T represents the transition model and R the reward model, which defines the return after taking an action in a given state. Lastly, \u03b3 is the discount factor, which adjusts the weight of immediate and future return. The agent determines which action a to be executed at each timestep t by employing a policy function \u03c0. The objective (Eq. 4) in RL is finding an optimal policy \u03c0* which maximizes the expected accumulated return across an episode:\n$J(\u03c0) = \u0395_{\\pi} [\\sum_{t=0}^{H} \u03b3^t R_t]$,\n(4)\nwhere H denotes the horizon or length of the episode. The optimal policy can then be defined as follows:\n$\u03c0^*(s) = arg max_{\\pi} J(\u03c0)$\n(5)\nMore formally, the problem of motion planning can be defined as follows: Given a starting location and a goal, deter-mine the optimal path to the goal while avoiding obstacles. In each episode, the vessel starts at a random location within a randomized port environment, which includes static and dynamic obstacles, as well as a randomly placed goal. The optimal policy \u03c0* (Eq. 5) identifies the optimal path to the goal while avoiding obstacles by controlling the thrust and rudder of the vessel. However, the vessel does not know the location of obstacles or the port walls. Instead, it relies on a ranging sensor to observe its surroundings and detect obstacles. An episode ends when the goal is reached (positive return) or if a collision occurs (negative return)."}, {"title": "A. Simulation", "content": "The goal of this work is to examine the robustness of RL algorithms. The algorithms used are the model-free Soft-Actor Critic (SAC) algorithm [28] and the model-based MuZero algorithm [29]. MBRL combines traditional RL with planning by learning an internal transition model which approximates the dynamics of the actual environment. This model allows for future reasoning and planning on which actions to take. This has shown to reduce the number of environment interactions needed to achieve performance comparable to that of model-free RL [30]. We choose to examine the robustness of SAC, as Eysenbach et al. [31] theoretically prove that the SAC algorithm is inherently robust to context disturbances. SAC is compared to MuZero, an algorithm with no inherent robust properties, as we reproduce the experiment by Herremans et al. [6]. As seen in Eq. 1 and Eq. 2, the mass and turn rate of the vessel affect the kinematics of the simulation. To examine the robustness of the algorithms, we can vary these parameters to emulate vessels different from the training vessel and see how they perform. We adapted the simulator to allow for changes to these parameters during runtime."}, {"title": "B. Reinforcement Learning", "content": "Fig. 3 shows the mean return during the training process. A higher return indicates longer and more successful episodes with fewer collisions. After training, both SAC and MuZero achieve similar performance. However, SAC achieves this performance after significantly fewer training timesteps. This could be because the model-free approach does not need to learn a transition model of the complex environment. In contrast to Herremans et al. [6], we show that a model-free approach can also achieve an adequate policy for motion planning in the MOOS-IVP simulator. To assess robustness, we modified the vessel mass m and turn rate R after training and analyzed the return compared to the standard values for both SAC and MuZero. On Fig. 2, we notice that SAC is more robust than MuZero. As the mass of the vessel increases, MuZero's performance rapidly deteriorates, whereas SAC maintains similar performance for larger mass values and degrades later. This can be attributed to the fact that SAC is based on maximum entropy RL [28], where an agent aims to maximize the expected return whilst acting as randomly as possible. Random actions can be beneficial in the presence of environmental disturbances, since previously learned actions may no longer be effective. At first glance, we would expect that SAC also performs better for the lower mass values. However, lowering the mass makes it easier to control the vessel, as the influence of the control action increases (Eq. 1). This could be why MuZero performs better in this case.\nWe see similar results for the turn rate, with SAC generally performing substantially better for both higher and lower turn rates. Performance drops off only at relatively high turn rates, with decent return even at ten times the nominal turn rate. This could be because the vessel usually makes small turns, not significantly affected by the turn rate (Eq. 2). However, lower turn rates degrade the performance faster, due to limiting the vessel's ability to make rapid turns when necessary."}, {"title": "C. Experimental Setup", "content": "This paper examines the robustness of RL algorithms in a practical and complex environment. We extend previous work [6] and show that a model-free approach can also successfully navigate various port and inland environments in less training time compared to a model-based approach. We show that SAC is indeed inherently more robust to environmental disturbances than MuZero. However, we only examined the mass and turn rate separately. Future work will include the incorporation of a more sophisticated kinematics model, which includes drag. Adapting the drag coefficient and drag area, along with the mass and turn rate, can emulate vessels with completely different shapes and sizes. Furthermore, we will evaluate existing robust RL methods [25] [26] in a more practical envi-ronment. Since we now have the ability to adapt the simulation parameters at runtime, we can implement adversarial learning methods, as those discussed in [22] and [23].\nLastly, because robust RL aims to close the sim2real gap, we would like to transfer these approaches to a real-world unmanned surface vehicle (USV) and examine their effec-tiveness. For this, a number of implementation challenges"}, {"title": "IV. RESULTS", "content": "Since we now have the ability to adapt the simulation parameters at runtime, we can implement adversarial learning methods, as those discussed in [22] and [23].\nLastly, because robust RL aims to close the sim2real gap, we would like to transfer these approaches to a real-world unmanned surface vehicle (USV) and examine their effec-tiveness. For this, a number of implementation challenges"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In conclusion, we found that maximum entropy RL solves some robust RL problems. We also want to acknowledge some of the limitations of our study."}]}