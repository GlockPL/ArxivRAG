{"title": "CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models", "authors": ["Rikhiya Ghosh", "OlaDimeji Farri", "Hans-Martin von Stockhausen", "George Marica Vasile", "Martin Schmitt"], "abstract": "The healthcare industry is currently experiencing an unprecedented wave of cybersecurity attacks, impacting millions of individuals [5]. With the discovery of thousands of vulnerabilities each month, there is a pressing need to drive the automation of vulnerability assessment processes for medical devices, facilitating rapid mitigation efforts. Generative AI systems have revolutionized various industries, offering unparalleled opportunities for automation and increased efficiency[37]. This paper presents a solution leveraging Large Language Models (LLMs) to learn from historical evaluations of vulnerabilities for the automatic assessment of vulnerabilities in the medical devices industry. This approach is applied within the portfolio of a single manufacturer, taking into account device characteristics, including existing security posture and controls. The primary contributions of this paper are threefold. Firstly, it provides a detailed examination of the best practices for training a vulnerability Language Model (LM) in an industrial context. Secondly, it presents a comprehensive comparison and insightful analysis of the effectiveness of Language Models in vulnerability assessment. Finally, it proposes a new human-in-the-loop framework to expedite vulnerability evaluation processes.", "sections": [{"title": "1 INTRODUCTION", "content": "A software vulnerability is a weakness or flaw in a software system that can be exploited by attackers to compromise the integrity, availability, or confidentiality of the system or the data it processes. These vulnerabilities can exist at various levels of a software system code, design, configuration, or operation of the software. Examples of software vulnerabilities include buffer overflows, SQL injection, cross-site scripting (XSS), and insecure deserialization. There are several regulatory authorities, non-profits and companies that regularly update the growing list of software vulnerabilities. The most prominent of these organizations include MITRE corporation 1 which, in conjunction with National Institute of Standards and Technology (NIST) and National Vulnerability Database (NVD) 2, maintain the Common Vulnerabilities and Exposures (CVE)[33] database. The CVE database is a system for uniquely identifying and cataloging known vulnerabilities in software and hardware. Each CVE entry includes a standardized identifier (e.g., CVE-2022-12345), a brief description of the vulnerability, and any relevant references or links to additional information. The goal of CVE is to provide a common language for discussing vulnerabilities and to facilitate the sharing of information about them among security professionals, vendors, and the general public. There are around 250K vulnerabilities at present, and it grows by thousands every month.\nMedical devices are prone to various security threats that can compromise patient safety, privacy, the integrity of medical data, and the availability of diagnostic or treatment devices. Some common vulnerability threats include (a) software vulnerabilities, like buffer overflow, injection attacks, authentication bypass etc, (b) issues with proper encryption, (c) weak authentication measures, (d) third party component-related vulnerabilities, (e) use of unsupported legacy systems that are vulnerable but unpatchable, (f) insecurely implemented/operated wireless connectivity such as missing authentication and weak encryption, and (g) lack of adequate security controls. Recent studies[5] show that healthcare industry is the most vulnerable sector with 809 data compromises and 56 million"}, {"title": "2 PROBLEM STATEMENT", "content": "Starting with the US Food and Drug Adminstration's (FDA) Guidance on Postmarket management of Cybersecurity in medical devices [38]. MDMs are required to monitor all 3rd party software components used in medical devices for emerging vulnerabilities and evaluate them towards whether they have a negative impact"}, {"title": "3 RELEVANT WORK", "content": "Language models (LMs) have been a pivotal area of research in natural language processing (NLP), driving significant advancements in various NLP tasks. The large data revolution in NLP was introduced by Transformer architecture [11] by enabling parallel computation and efficient modeling of long-range dependencies in text. The architecture is constituted by an encoder-decoder design, featuring multiple layers of both self-attention and feedforward neural networks. In recent years, advancements in high-performance computing, scalability, architecture, and data availability have facilitated the evolution from models like BERT [11] and GPT [44] to more sophisticated iterations such as GPT-4 [2], Llama3 [55], Mixtral [20], and other powerful Large Language Models (LLMs). These models exhibit remarkable performance not only in text-based systems but also in multimodal domains. Language Models have been used extensively in vulnerability management (a) to determine CVSS metrics from CVE description, (b) to establish a mapping between"}, {"title": "4 METHODOLOGY", "content": "We define the vulnerability assessment problem as follows: given an asset A and notification N, an evaluation is generated.\n(A, N) \u2192 evaluation\nAn evaluation can be a VEXCATEGORY, CVSS environmental metrics, Internal comment, or Customer Comment. VEXCATEGORY is generated along with VEXJUSTIFICATION, which further elucidates"}, {"title": "4.2 Training of DAPT model", "content": "DAPT model is trained by continuously pretraining the LLM using next token generation. Since the quality of data is of utmost importance, we proceed with careful preprocessing of DAPT dataset, and then pretraining the LLM with preprocessed data.\n4.2.1 Preprocessing. Preprocessing of the public dataset part of DAPT dataset consists of the following steps:\n\u2022 Identification of attributes of the dataset that are used to form a data point. A data point in the dataset refers to details of a CVE. The identified attributes represent title and descriptions of the CVEs, base and temporal vectors, affected product and software version, unaffected software version (if any), and mitigation (if any).\n\u2022 Cleaning descriptions. The descriptions are preprocessed for removing URLs, non-UTF8 characters, removing formatting if any. In case there are more than one description in the CVE, and there is not more than 70% overlap between the descriptions, they are merged into one longer description.\n\u2022 Template-based formation of vector description. The vectors are expanded to form template-based description of the vectors. For a vector component that is of the format  the expanded vector description is \" is \".\n\u2022 Template-based combination of the data point attributes. The template for the CVE detail is: \"CVE description: . Affected product:  less than . Unaffected version:  and higher. Vector: \".\nPreprocessing of the organization dataset consists of the following procedures:\n\u2022 Identification of notifications in the Notifications dataset that consist of descriptions with text other than the descriptions of the constituent CVEs. Most notifications contain CVE descriptions from the public dataset followed by expert comments on the notification. In addition, the nonstandard notifications are also included in this dataset.\n\u2022 Cleaning description. This follows the same procedure of cleaning descriptions as the public dataset preprocessing. In addition, the CVE ids are cleaned from the description.\n\u2022 Template-based formation of vector description. Vector description is formed in the similar fashion as the public dataset.\n\u2022 Template-based combination of notification attributes. The template for notification detail is: \"Notification description: . Vector: \""}, {"title": "4.2.2 Model Training.", "content": "The DAPT dataset is randomly split into 90:10 split with 288K training data points and 32K validation data points. We have continuously pretrained MPT-7B [54] base model autoregressively for next token prediction with an objective function of cross entropy with the DAPT dataset. We have expanded the vocabulary of the MPT-7B base model to include names of the components and the organization software. We have used only the first names of the components and software and we had 539 new tokens to add to the 50K vocabulary of MPT. The model is trained using DeepSpeed [45] zero-3 optimization with Lion optimizer [6]"}, {"title": "4.3 Training of Instruction-tuned model", "content": "Instruction tuning requires formation of an instruction tuning dataset in a pre-defined format. We have used the instruction tuning dataset format of the Alpaca dataset [53] to form our dataset. Then we have finetuned the DAPT model for instruction tuning with the new dataset.\n4.3.1 Formation of Instruction-tuning dataset. Formation of instruction-tuning dataset follows the following procedure:\n\u2022 Merging of Evaluations dataset with Assets and Notifications dataset based on the Asset ID and Notification ID.\n\u2022 Cleaning of notification description. This follows similar procedure as preprocessing of organization dataset.\n\u2022 Identifying components common to both Asset and Notification and forming a list of descriptions of these common components.\n\u2022 Formation of vector descriptions for both notification vector and evaluation vector. This follows the same vector text description format as described in DAPT dataset formation section. For evaluation vector description formation, we subtract the components already present in the notification vector from evaluation vector components and form the evaluation vector description using these new vector components only. In addition, the CVSS format version is also extracted. In case CVSS format version is not specified, the vector components are used to match to the corresponding vector components for CVSS versions 2, 3.0 and 3.1, and corresponding the highest version with all the component matches is considered as the CVSS version. In case such version is not found, we have used the help of security experts to correct the vector as well as get the correct version number.\nWe obtain all the expected base, temporal and environmental vector components using the CVSS version. In case of notification vector text, we find all missing base and temporal vector components in the text and add them to notification vector description in the format \" is XXXX\". For evaluation vector, we find all missing environmental vector components in the evaluation vector and add them to the evaluation vector text in the same format as missing notification vector description. To be noted is that the order of vectors in the description is fixed.\n\u2022 Formation of vector justification. We break up the vector justification category into constituent words and use that as an explanation of VEXCATEGORY.\n\u2022 Formation of instruction data. The instruction format uses the following format:\nThe instructions and responses for the different types of evaluations are shown in 1.\n\u2022 Removal of incomplete evaluation data, data under investigation, long text data. In practice, VEXCATEGORY also has two more classes: UnderInvestigation and EndofLife. UnderInvestigation category is assigned to the evaluations for which monitoring and/or mitigation measures are underway."}, {"title": "4.4 Inference", "content": "The inference framework includes generation of evaluations using the trained LLM, using pre-defined rules to avoid easily identifiable erroneous generations, and using faster inference frameworks to improve model throughput. We have implemented a human-in-the-loop system where the AI-generated vulnerability evaluations are validated by a product cybersecurity expert with higher priority to the evaluations with Affected VEXCATEGORY.\n4.4.1 Generation of evaluations. Evaluation generation follows a zero shot model inference where the model is provided a prompt in the pre-defined instruction format, containing information about sub-organization name, software name and version, asset name and version, notification description, list of component descriptions for components present in both asset and notification, base and temporal versions and CVSS version. The maximum generated tokens for VEXCATEGORY/VEXJUSTIFICATION generation is set to 25, Internal Comments is 125, Customer comments is 100, and Vectors is 100. The inference/test dataset is divided into two sub-datasets based on the size of tokens in the inference instruction. For instructions longer than 920 tokens, the sequence length of the trained model is increased to 150 + maximum token length of the longest instruction. If the size of the instruction is less than 920 tokens, the trained model without any changes is used to inference the dataset. Since MPT-7B is trained with ALIBI [43] encodings, the maximum sequence length during inference can be set upto 84K tokens, which has not been observed in production yet.\n4.4.2 Rule-based correction. Post-processing of model outputs involves using system knowledge in the form of pre-defined rules to correct mistakes in model generation. The rules are as follows:"}, {"title": "4.4.3 Model serving.", "content": "Model serving is performed using vLLM architecture [21] on one Tesla A100 SXM4 GPU. Model throughput during production is improved using rule based system where generation of Vector is skipped when VEXCATEGORY is Not Affected, and sequence length-based batching is performed."}, {"title": "5 RESULTS", "content": "Evaluation of our methodology follows a two-step process which evaluate the model-generated results as well as the end-to-end results from the inference pipeline. In addition to the test set which was set apart during training, we have also conducted an evaluation of the model outputs in a real-time deployment scenario over a period of two months for new assets and vulnerabilities."}, {"title": "5.1 Evaluation Metrics", "content": "We use ROUGE-L and micro-F1 for evaluating the model responses: ROUGE-L[27] is used for evaluating the responses model generated for Internal Comment and Customer Comment, whereas micro-F1 [41] is used for VEXCATEGORY, VEXJUSTIFICATION and Vector. ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence) is a metric used to evaluate the quality of summaries produced by automatic summarization systems. ROUGE-L measures the longest common subsequence (LCS) between the model-generated summary and the reference (human-generated) summary. Micro F1, or micro-averaged F1 score, is calculated by taking the harmonic mean of precision and recall, with equal weighting given to both metrics."}, {"title": "5.2 Benchmarking CVE-LLM against other open source LLMS", "content": "We have used the open source models that are of similar size as CVE-LLM, and are the highest performing models currently on LLM Leaderboards [17]. We trained them using the DAPT and SFT methods described in Methodology section. In addition, we have also performed RAG over our test dataset with Llama3-70B."}, {"title": "5.3 Ablation Studies with the model", "content": "We conducted ablation studies with respect to different components of model training and inference:\n(a) Changes in dataset, (b) Dataset size, (c) DAPT-SFT vs SFT-only system, and (d) Impact of beam size, temperature, and nucleus sampling during model inference, and (e) Sequence length during training, and For each of these experiments, the test dataset consists of the same data points from the Evaluations dataset. The training data points have varied across the datasets, but we have made sure that all the possible assets are represented in the training datasets."}, {"title": "5.3.1 Changes in dataset.", "content": "For each of these experiments, we trained CVE-LLM-base model on the reconstructed dataset. We have tried the following changes in dataset:\n\u2022 Combining all the evaluations into one instruction (CI)\n\u2022 Combining customer comments, VEXCATEGORY and VEXJus-TIFICATION into one type of instruction (CV)\n\u2022 Separate generation of VEXJUSTIFICATION and VEXCATEGORY (CWJ)\n\u2022 No delimiter at the end of training text (WD), and\n\u2022 No Placeholder for unused vector categories (NP)\nCI, CV and CWJ are concerned with experiments to see whether we can combine some of these instructions to achieve good results. WD refers to the experiment where we see how the model performs when we do not put a dedicated delimiter. NP is concerned with how the template for a Vector output should look like. NP refers to the case where the response contains only the vectors that are mentioned in the standard CVSS vector format. The training details of this ablation study is shown in Table 5 and the test results are shown in Table 4. These datasets vary in their sizes of their training, validation and test datasets. For example, CI has only 88K instructions to train with due to combining instructions, which CWJ has 440K instructions because all the instructions are separated.\nWe observe that the variation of Rouge-L score for Internal Comment is the lowest, while the highest variation is seen for the classification-based generations of VEXCATEGORY and Vector. The model performs better with diversity in instructions, and higher number of tokens in training. Size of the dataset does not impact performance much by itself, but combined with other factors may degrade performance. Spurious text generation is a problem with LLM finetuning, and putting a delimiter enhances performance"}, {"title": "5.3.2 Effect of Dataset size.", "content": "We have experimented with changing size of the instruction tuning training dataset from 150K instructions to 440K instructions. The datasets are formed by making sure that all the types of assets and all the types of notifications are always represented in the dataset. The minimal dataset that affords this invariability is the 150K training dataset, and for the rest of the datasets we have randomly selected from the remaining data points.\nWe have noticed that Customer Comment shows the least variation in Rouge-L score, while VEXCATEGORY shows the highest micro-F1 variance. In addition, the performance improvement stagnates for most instructions as we reach the 440K instruction dataset size mark."}, {"title": "5.3.3 Effect of Domain Adaptation.", "content": "The results after Supervised Finetuning of Domain-adapted CVE-LLM-Base vs Supervised Fine-tuning of MPT-7B is shown in Table 6. Domain adaptation and vocabulary expansion leads to a much better performance for all the instruction types, as shown in Table 6"}, {"title": "5.3.4 Effect of Sequence Length in Training.", "content": "In this experiment, we changed the sequence length in configuration of the model to 512, 750 and 1024, and reduced the training dataset to accommodate only the instructions that had less than the sequence length. shows the effect of training sequence length on the model outcome for all the instruction types. We find out that though the dataset size does not change much, there is more variation in performance. As training sequence length increases, the performance on datasets with longer instructions also improves."}, {"title": "5.3.5 Effect of inference parameters.", "content": "In generative model inference, parameters such as temperature, beam size, and nucleus sampling significantly influence the quality and diversity of the generated outputs. Temperature controls the randomness of the output, with higher values leading to more diverse results. Nucleus Sampling (top-p sampling) dynamically adjusts the candidate set of tokens to balance between diversity and coherence based on the cumulative probability threshold 'p'. Beam Size in beam search influences the depth of search for probable sequences, balancing between computational cost and output quality. Generally these parameters are adjusted to optimize the trade-off between diversity and coherence of the generated text. We performed inference on test dataset (N=44K) using CVE-LLM-Eval model to conduct the experiment on the effect of inference parameters.\nTemperature and nucleus sampling. In order to test the effect of temperature, we maintained the generation parameters at their default settings and conducted tests by varying the temperature values from 0 to 1 in increments of 0.1 to evaluate the outcomes. Similarly, for top-p sampling too, we changed the parameter 'top-p' from 0 to 1 in increments of 0.1, keeping all other generation parameters in default settings. For both of the experiments, no changes were observed."}, {"title": "5.4 Inference time", "content": "We have used different techniques to optimize for the time required in inference. The first experiment used popular model serving algorithm vLLM adapted for MPT-7B to find required inference time for one Tesla A100 SXM4 GPU for each of the instruction types and found speedup of ten times using both compared to no model serving. The experiment results are shown in Table 7\nIn addition, the adaptive generation by dividing the inference batch into batches of small and large token lengths also improves the average performance over the test dataset to 4.5 second per"}, {"title": "6 DISCUSSION", "content": "In this section, we will elucidate our observations related to training and performance of LLMs for vulnerability management. We have learned valuable lessons in understanding the benefits and pitfalls of using LLMs, and have identified areas of improvement for the system."}, {"title": "6.1 Observations on Model Training", "content": "In our experiments we tested several hypothesis about diversity of instructions and dataset size. Firstly, we observe that the size of the dataset does not independently impact performance; rather, the diversity of instructions significantly influences the model's efficacy. Higher diversity leads to better performance. When all instructions were consolidated into a single instruction type, there was a marked decline in performance. This decrease was primarily due to the model generating extraneous text, either supplementing or replacing the expected response. Secondly, the responses corresponding to instruction types that appeared later in the consolidated instruction model had higher degradation of performance. This is corroborated by training with combination of customer comments and category as well. Thirdly, representation of related assets and notifications are important, since the model performance dips when tested on completely different types of assets and notifications.\nLLMs have reportedly underperformed in text classification tasks; however, their performance has improved when reasoning elements are incorporated into the text [50]. Our experiments yielded similar observations. We experimented on using reasoning for VEXCATEGORY and found that this leads to higher F1 than without using reasoning. In our study, we used VEXJUSTIFICATION as a reasoning for VEXCATEGORY, as well as examined whether adding Customer Comment as an explanation would improve the system. There are no major improvements with adding Customer Comments as reasoning and also leads it to lower performance for Customer Comments. Chain-of-Thought prompts which have been shown to enhance model reasoning power would probably be useful to enhance our model in the future. Additionally, we have observed that the total number of classes affect the performance of the system"}, {"title": "6.2 Observations on Inference", "content": "We have noticed model output variations with beam size, with decline in performance with higher beam sizes. Increase in beam search size increases the diversity of text, and that leads to less contextually relevant outputs, as seen in our experiments. Low beam size is sufficient for high quality generation because of the specialized nature of the dataset. The difference between probabilities of less probable outcomes and highly probable outcomes, given the context, is low. This leads to no temperature or nucleus sampling-based variations in model-generated outputs."}, {"title": "6.3 Error analysis", "content": "We have used MPT-7B to train CVE-LLM for two reasons: (1) Expansion capability to at least 20K tokens without significant loss of performance, and (2) It performs better than other open source models we tested with our dataset However, among all the LLMs we have trained, majority of the issues we have encountered are remarkably similar, differing primarily in their severity. The generated text for both Internal Comment and Customer Comment has been observed to have omitted or falsely included several critical details, particularly those concerning the affected software versions, the recommended update versions, the names of the software or components, and other similarly named entities that constitute essential information. The other type of error commonly seen for the LLMs is spurious text generation, which is something we observed quite frequently for trained Llama2 models. Introduction of stop tokens mitigated the problem to a great extent for MPT-7B, but not for the other models. In spite of having the ability to extend the sequence length of the trained model beyond the training sequence length, the performance on long sequences still falls behind. Long instructions comprises of multiple vulnerabilities, and constitute only 10% of our test dataset. In the future, we plan to segment it into multiple vulnerabilities and use more robust semantic understanding of each vulnerability to perform vulnerability chaining. We also tested the system only with zero shot instructions at the inference, and we surmise that we can utilize language understanding capabilities better by using few shot methods to assess vulnerabilities for assets.\nThis study illustrates the capacity of large language models (LLMs) to learn from expert-curated historical data, thereby enabling the automation of vulnerability management for medical"}, {"title": "7 DISCLAIMER", "content": "The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed."}, {"title": "A ENVIRONMENTAL VECTOR TEST DATASET RESULTS", "content": "The environmental metrics found in the test dataset and their corresponding micro-F1 are shown in 9. Figure 5 shows the confusion matrix for these environmental metrics. Due to differences in CVSS versions, there are some environmental metrics that are not applicable to certain evaluations, and their values are represented as 'N/A'. The value 'XXXX' for environmental metrics refers to the cases where the expert did not use any value for that environmental metrics."}, {"title": "B COMMON ERRORS IN GENERATION WITH CVE-LLM", "content": "The common errors found in model outputs are mostly related to (1) Wrong generation, (2) Omission of key information, (3) Wrong key information, and (4) Addition of spurious details Examples of these errors are shown in Table 8 and 10.\nWe noticed that most of the errors in generation are due to the probabilistic regurgitation of training text, and there is less semantic understanding of the paradigm. We surmise that this can be corrected post-hoc with verification of the entities involved in the generation, chain of thought prompts and reasoning modules."}]}