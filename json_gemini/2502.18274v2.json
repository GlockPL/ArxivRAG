{"title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support", "authors": ["Guoxin Wang", "Minyu Gao", "Shuai Yang", "Ya Zhang", "Lizhi He", "Liang Huang", "Hanlin Xiao", "Yexuan Zhang", "Wanyue Li", "Lu Chen", "Jintao Fei", "Xin Li"], "abstract": "Large language models (LLMs), particularly those with reasoning capabilities, have rapidly advanced in recent years, demonstrating significant potential across a wide range of applications. However, their deployment in healthcare, especially in disease reasoning tasks, is hindered by the challenge of acquiring expert-level cognitive data. In this paper, we introduce Citrus, a medical language model that bridges the gap between clinical expertise and AI reasoning by emulating the cognitive processes of medical experts. The model is trained on a large corpus of simulated expert disease reasoning data, synthesized using a novel approach that accurately captures the decision-making pathways of clinicians. This approach enables Citrus to better simulate the complex reasoning processes involved in diagnosing and treating medical conditions. To further address the lack of publicly available datasets for medical reasoning tasks, we release the last-stage training data, including a custom-built medical diagnostic dialogue dataset. This open-source contribution aims to support further research and development in the field. Evaluations using authoritative benchmarks such as MedQA, covering tasks in medical reasoning and language understanding, show that Citrus achieves superior performance compared to other models of similar size. These results highlight Citrus's potential to significantly enhance medical decision support systems, providing a more accurate and efficient tool for clinical decision-making.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in the reasoning capabilities of LLMs have become a focal point in research and are increasingly seen as a benchmark for assessing the intelligence level of these models [1, 2]. While the progress in reasoning capabilities has been rapid in domains like mathmetics and programming, the development in healthcare remains relatively limited[3-6]. The open-ended nature of medical practice presents a more complex challenge for Language Models. Medical expertise is cultivated through real-world clinical practice, making it essential for medical reasoning models to learn from the diagnostic and treatment processes of human experts. As a result, emulating the reasoning pathways of medical professionals becomes a crucial step for developing effective medical reasoning models.\nClinical practice, requiring highly sophisticated medical reasoning skills, encompasses patient con-sultation, diagnosis, differential diagnosis, and treatments[7, 8]. Medical experts have systematically"}, {"title": "2 Related Works", "content": "Medical reasoning in clinical practice In clinical practice, determining the most rational expert thinking process has always been a key research focus [8-12]. The hypothetico-deductive method[16-18] is a reasoning process from general to specific, which determines diseases based on symptom combinations according to known medical theories. According to this method, some diagnostic hypotheses or conclusions has been raised firstly after collecting information from patients and will be waiting for testing. And these hypotheses, to some extent, guided the subsequent diagnosis and treatment. Pattern-recognition method [19, 20] is a reasoning process from specific to general, which discovers patterns based on clinical observations and empirical summaries. Physicians quickly establish preliminary diagnoses through certain typical descriptions and specific combinations of symptoms that have been repeatedly validated in long-term clinical practice. The dual-processing theory (DPT) [21, 22], which integrates hypothesis testing methodologies with pattern recognition approaches, has gained widespread recognition and acceptance among medical experts. DPT includes system 1 and system 2[23]. System 1 is a fast, intuitive, non-analytical process which is similar to pattern-recognition method, while System 2 is a slow, deliberate, analytical process related to hypothetico-deductive method[9, 24]. DPT posits that the reasoning pathway in clinical practice necessitates the concurrent integration of both intuitive and analytical processes[23, 25, 26].\nApplication of Large Language Models in Medical Reasoning Researchers have realized the great potential of LLMs reasoning in medical problems solving[27, 28, 3]. Recent advancements in LLMs for healthcare have been propelled by improved training methodologies, including CPT, SFT, and RL, which significantly enhance medical dialogue comprehension and question-answering capabilities [27, 29-48]. Training-free techniques, such as advanced prompt engineering, have enabled general-purpose LLMs to perform specific medical tasks without retraining, as evidenced by studies like MedPrompt[49, 39, 50-55]. The implementation of multi-agent systems simulating experts from various medical departments has improved decision-making and overall medical performance by"}, {"title": "3 Training Data", "content": "Llama3.1[14] and GPT-4[15]. Inspired by the results, we took further steps and carefully designed a multi-stage training method incorporating multiple phases of continuous pre-training (CPT), super-vised fine-tuning (SFT) and reinforcement learning (RL). In this paper, we present Citrus, a medical language model that leverages expert cognitive pathways to simulate the reasoning processes of clinicians. By training Citrus on a large corpus of simulated medical reasoning data, we replicate the dynamic and iterative nature of clinical decision-making. This enables the model to engage in more accurate and effective reasoning, forming the foundation for future AI-driven medical decision support systems. Additionally, we successfully translated this methodology into a trainable approach, resulting in substantial performance improvements in several open-source base models across a variety of medical benchmarks. The advanced medical reasoning and decision-making support provided by Citrus have enabled it to outperform 70B parameter models in the medical domain.\nAt the same time, we identified a key challenge in current medical language model evaluations: the structured nature of assessment questions often fails to capture the inherent ambiguity of patient symptoms in real-world clinical practice. By leveraging real-world consultations at JD Health's internet hospital, we have created a clinical practice evaluation dataset JDH MEDical Practice Dataset (JMED) that reflects real-world disease distribution, and can be regularly updated.\nThe contributions of this paper are as follows:\n1.  We propose a training-free reasoning approach that emulates the cognitive processes of medical experts, enabling large language models to enhance their medical capabilities in clinical diagnosis and treatment.\n2.  In conjunction with the data construction method, we introduce a multi-stage post-training approach to further improve the model's medical performance.\n3.  We have made the Citrus model and its training data publicly available as open-source resources to advance research in AI-driven medical decision-making.\n4.  We have developed and open-sourced a large-scale, updatable clinical practice evaluation dataset based on real-world data, accurately reflecting the distribution of patients in real-world settings."}, {"title": "3.1 Understanding Clinical Reasoning", "content": "Recent studies have used the Chain-of-Thought (COT) [68] generation technique to enhance the reasoning capabilities of medical models. We argue that structuring the reasoning process to mirror the cognitive pathways of expert doctors in a structured COT approach is more effective in activating the model's reasoning potential compared to unstructured, base-model-driven processes. Additionally, structured reasoning is easier for human experts to verify. Upon observing the reasoning processes of medical professionals, we identify two primary reasoning methods: the hypothetico-deductive method and the pattern-recognition method. The hypothetico-deductive method involves generating hypotheses based on available information, testing these hypotheses against further data, and revising them to form conclusions. This method emphasizes critical thinking and careful hypothesis testing, making it a robust approach in clinical practice, especially when facing complex, uncertain cases.In contrast, the pattern-recognition method relies on the recognition of patterns or symptoms that closely match previous cases or well-established medical knowledge. This method is often more intuitive and is useful when dealing with familiar or straightforward cases. It involves rapid decision-making based on experience rather than hypothesis testing. Experienced experts typically combine both methods in clinical decision-making to ensure efficient and accurate outcomes.\nLeveraging the cognitive pathways of medical experts, we propose a multi-stage data construction methodology that allows LLMs to integrate both reasoning methods, emulating expert reasoning patterns in medical decision-making[37], shown in Figure.2. The main approaches are as follows:\n1.  Pattern-recognition capabilities are typically developed through CPT. Through exposure to large-scale, high-quality medical datasets, LLMs can most intuitively learn the logical relationships and probability distributions among various medical entities.\n2.  Hypothetical-deductive reasoning capability requires LLMs to manipulate medical knowl-edge sophisticatedly. To emulate this complex cognitive pattern, we synthesized extended"}, {"title": "3.2 CPT Data for Pattern Recognition", "content": "COT data by simulating expert reasoning processes. Additionally, a two-stage curriculum learning strategy is implemented as a prerequisite to smooth the model's learning trajectory.\nThe pattern-recognition method is typically embedded in the pre-training of large language models and is refined further during domain-specific training. Through the CPT process, a comprehensive medical domain dataset is used to enhance the pattern recognition capabilities of LLMs in addressing medical challenges. By collecting medical field data and preprocessing it, we have obtained a CPT dataset that enables LLMs to learn medical knowledge and perform pattern recognition.\nData Collection The sources of CPT data include the following aspects:\n\u2022 Web Data\n\u2022 Medical Textbooks\n\u2022 Medical guidelines and literature.\nData Process Data sourced from the web requires careful attention to data cleaning and labeling processes. Following the RedPajama[69] approach, we applied natural language processing tech-niques, such as entity recognition, relationship extraction, and text classification, for data cleaning and labeling. Additionally, we performed deduplication to ensure the quality of the dataset.\nFor handling PDFs with complex structures, we leveraged certain computer vision solutions. In contrast to web text, data from research papers and medical guidelines presents a different chal-lenge. While this type of data often boasts high-quality content, extracting and transforming it from highly complex structured formats into a form suitable for the CPT training process is particularly challenging. We processed over one million PDF documents using methods such as MAPneo[70].\nRegarding medical textbooks, we applied data augmentation techniques to synthesize data. Data derived from medical textbooks inherently represents an optimal training corpus. However, due to"}, {"title": "3.3 Data Synthesis for Hypothetico-Deductive Reasoning", "content": "limitations on the number of medical books available for collection, this dataset is smaller compared to web and academic materials. In order to ensure that these high-quality data points have a significant impact during training without being overshadowed by the other two data sources, we utilized techniques such as WizardLM's self-evolution method to diversify and expand the knowledge from individual books into a variety of medical queries [71].\nScale and Distribution To mitigate the performance degradation caused by catastrophic forgetting, we cannot train the model solely on medical corpora. Therefore, we cleaned and selected approxi-mately 200 billion tokens of CPT corpus from the following public datasets (CCI[72], PubMed[73], SlimPajama [74], WuDao[75], ChineseWebText[76], Math Pile[77], Stack Code[78], etc.) and pur-chased medical book data. After categorizing and labeling the data, we found that medical data accounted for about 30% of the total.\nThe hypothetico-deductive method is typically characterized by the following steps in an expert's thinking process: collecting information, analyzing symptoms, generating diagnostic hypotheses, conducting differential diagnosis, and reaching conclusions. In this process, hypothesis generation and hypothesis testing are the core components of the reasoning process. To model this, we propose a comprehensive data series including general ability sft data with data course and medical ability sft data inspired from training-free dual expert data synthesis system."}, {"title": "3.3.1 General Medical Instruction Data", "content": "To enhance the model's fundamental instruction-following capabilities and improve its ease of training, we design a two-stage data course that is not limited to medical problems. We refer to these as Stage-1 and Stage-2 SFT Data[79]. We recognize that it is an impractical task to directly train a base model, which has undergone medical knowledge CPT, to acquire medical reasoning abilities. LLMs struggle to effectively address complex medical reasoning problems when starting with no prior task-handling capabilities. To address this, we adopted a two-stage, general-purpose SFT data approach as part of our data curriculum.\nIn the first stage, we train the model with approximately 7 million basic instruction examples to improve its ability to follow simple instructions. In the second stage, we use 1.4 million higher-quality and more complex instructions, aiming to enhance the model's multi-turn dialogue handling and complex instruction-following capabilities while preserving the abilities gained from the first stage. This process results in the stage-2 SFT model, which provides a solid foundation for more specialized task training in subsequent phases."}, {"title": "3.3.2 Dual Expert Reasoning Method", "content": "In this section, we present the Dual-Expert Reasoning Method. Through this approach, LLMs can emulate medical experts by employing hypothetico-deductive reasoning processes to address medical problems.\nTo emulate the Hypothetico-Deductive Process, we established a Reasoning Expert. When confronted with a problem, this role analyzes the available information, formulates new hypotheses, and conducts thorough reasoning. During the Training-free experiments, we observed that this process allows considerable flexibility. When the model does not engage in reflection, a significant amount of invalid reasoning processes are generated. This is unacceptable in terms of both reasoning accuracy and training efficiency. To address this, a multi-expert ensemble approach proves to be an effective solution. Thus, we designed a second expert, called the Reflection Expert. The Reflection Expert is tasked with evaluating the reasonableness of the reasoning process and discarding unreasonable or irrelevant steps. We then designed a cognitive flow loop to ensure the model generates a sufficient number of reasonable and accurate reasoning steps:\n1.  The model lists the existing information as the starting point for reasoning.\n2.  Based on the existing information, the model proposes possible diagnoses as the endpoints of the reasoning."}, {"title": "3.3.3 Medical Reasoning Instruction Data", "content": "In this section, we describe the construction of Stage-3 SFT Data using the Dual-Expert Reasoning Method. This dataset, called Citrus_S3, designed to improve medical reasoning abilities in LLMs, will be open-sourced to promote further research and development in the field. To ensure accuracy and diversity, we propose several advanced data processing techniques, outlined below.\nReasoning Model with Ground Truth The key to generating reliable medical COT training data using this dual-expert method without additional supervised training is ensuring that the model generates a reasonable and accurate reasoning process. To achieve this, we modified the training-free method by providing the reflection model with the ground truth for the medical questions faced by the reasoning model. In this setup, the reflection model evaluates the reasonableness of the steps generated by the reasoning model and subtly guides it toward the correct answer, without directly providing the solution. This design results in a redefined step 4 in the dual-expert method.\nQuestion Seeds Another indispensable part to successfully execute this data generation procedure is to have extensive medical questions, which should be complicated enough to ignite reasoning process as well as equipped with ground truth that has been properly verified.\nQuestion Rewriting The training question seeds in datasets like MedQA[80] are closed-form questions. We believe that providing answer options limits the model's reasoning capacity, restricting its ability to explore different reasoning paths. To improve generalization, we made the following adjustments:\n\u2022 We removed the options from the original closed-form questions and converted them into open-ended questions. This allows the model to focus on reasoning and conclusions without predefined answers.\n\u2022 We created a prompt for rewriting the open-ended questions, removing dependencies on options (e.g., \"Which of the following statements is incorrect?\").\nQuestion Quality Control Simple medical questions can be answered based on the model's existing knowledge, but they do not require complex medical reasoning. To filter data useful for learning reasoning abilities, we used models such as GPT-4o-2024-0513, Qwen2.5-7B[81], and Llamba-3.1-8B[82] to answer closed-form questions. If these models answered correctly, the data was categorized as easy data, which does not require reasoning. Otherwise, it was categorized as hard data. During the SFT data sampling stage, we used all the hard data and a small portion of easy data to ensure the quality of the training dataset."}, {"title": "3.3.4 Data Analysis", "content": "Data rewriting Data rewriting is essential to transform multi-expert problem analysis into a first-person thought process. We use LLMs to accomplish this task with several strict constraints:\n\u2022 Keep thought scale.\n\u2022 Use narrative words for transition words such as furthermore, therefore, then, wait...\n\u2022 Discard duplicated steps\n\u2022 Keep the ground truth"}, {"title": "4 Model Training", "content": "In this section, we present a comprehensive training procedure that integrates multiple stages, including CPT, SFT, and RL, referred in Figure.3. Through this multi-phase approach, we aim to transform a base model, initially lacking domain-specific medical knowledge and reasoning abilities, into a robust medical reasoning model capable of performing complex cognitive processes to effectively address and solve clinical problems. The training procedure leverages both general-purpose and medical-specific data, progressively refining the model's ability to handle medical tasks and engage in sophisticated reasoning when confronted with real-world clinical scenarios."}, {"title": "4.1 CPT Stage", "content": "This phase focuses on the continuous pre-training of existing foundation models to enhance their comprehension of medical domain knowledge. A primary challenge lies in adapting the same dataset for different foundation models, which possess distinct training data ratios and quality control mechanisms.\nIn the continuous pre-training of large language models, the ratio of data from different sources is a critical topic. Here, we employ an AutoML approach to dynamically determine the proportion of each data source during training. Specifically, we frame the data ratio problem as a multi-armed bandit problem[83]. We hypothesize that the benefit of encountering previously seen content in continuous pre-training is relatively small, so the model should be encouraged to learn new knowledge. Therefore, we treat the training loss from each data source as a reward. Through this methodology, base models are exposed to training corpora with dynamically adjusted sampling ratios across different training phases, resulting in substantially improved convergence efficiency."}, {"title": "4.2 SFT Stage", "content": "4.2.1 Medical Reasoning Ability SFT Training\nWe propose a three-stage SFT training framework to enhance the model's medical reasoning capabili-ties. As discussed in Section 3.3, the SFT datasets across these three stages are arranged in ascending"}, {"title": "4.2.2 SFT Data Format", "content": "order of difficulty. The underlying rationale is that the model should first master general knowledge application skills before proceeding to complex medical reasoning logic. In the following section, we will focus on elaborating the third stage of SFT training.\nThe third phase of SFT training focuses on improving the model's performance in the target task domain: medical reasoning. We used data obtained from the Training-Free approach and fine-tuned the Stage-2 SFT model in this phase. The main objective of this phase is to enhance the model's ability to perform long COT in medical reasoning tasks. We used approximately 100,000 medical benchmark problems with GroundTruth gold-standard answers to generate reasoning data, which were used to train the model's medical reasoning capabilities. To maintain the model's general-purpose capabilities during this process, we included reasoning data from other domains, such as logical and mathematical reasoning, in quantities comparable to the medical data. A toekn distribution statistics of stage 3 SFT training data is shown in Figure.4.\nFor all SFT data, we follow an unified template format: <sft-input, sft-target>. The sft-input consists of open-ended questions, and we expect the reasoning model to follow a structured thought process before providing answers. The resulting sft-target outputs follow the format: <think>thinking"}, {"title": "4.3 RL Stage", "content": "tokens</think><answer>answer tokens</answer>, where the reasoning process is contained within and the final conclusion is given in .\nAfter obtaining the Stage-3 SFT model, an effective reinforcement learning (RL) training phase is necessary. Compared to online methods, the RL techniques used in this phase, such as SIMPO[84] and CPO[85], have distinct advantages. SimPO completely eliminates the dependency on reference models, by directly using the average log probability generated by the policy model as an implicit reward. This not only reduces computational and memory consumption but also simplifies the training process, avoiding the complexity brought by multi-stage optimization. By introducing length normalization, SimPO effectively prevents the model from being biased toward generating lengthy but low-quality responses due to the reward mechanism. However, SimPO is quite sensitive to the learning rate, so we introduced NLL loss, similar to the CPO method, to enhance training stability. These methods offer more stable and efficient learning compared to traditional online reinforcement learning methods. For the RL training, we used data that shares the same origin as that used in Pre-RL SFT, sampling and training on a dataset of approximately 50,000 instances."}, {"title": "4.3.1 RL Data Sampling", "content": "We use the best-performing checkpoint after the Stage-3 SFT to perform rejection sampling. The process is as follows:\nRepeat Sampling: Open-ended questions (without answer options) are given to the model, which generates 20 responses at a high temperature (temperature = 1.2).\nConstruct Preference Data: To teach the model reasoning methods instead of just generating reasoning-like statements, we use rule-based rewards based on answer correctness, other than neural reward models. This ensures that rewards are accurately aligned with the correct reasoning steps. Specifically:\n\u2022 Answer Mapping: For each response, we assess whether the open-ended answer corresponds to the correct option in the original closed-form question. Responses that align with the correct answer are classified as good responses, while others are considered bad responses. We only retain data that contains both good and bad responses.\n\u2022 Response Scoring: Each response is also scored using GPT-4o. The reasoning process and conclusion are assessed, and the highest-scoring good response is selected as the chosen response. For bad responses, the one with the lowest score is retained if multiple bad responses exist with the same incorrect option."}, {"title": "4.3.2 RL Data Format", "content": "For the RL stage, the data format is <RL-input, chosen, rejected>. The RL-input format matches the sft-input format, and chosen and rejected follow the sft-target format."}, {"title": "5 JDH Medical Practice Dataset: Construction and Validation of a Real-World Clinical Dialogue Benchmark", "content": "Evaluating medical models is inherently challenging, especially when aligning them with real-world clinical settings. Effective evaluations should ensure that these models can be applied successfully in clinical practice.We systematically analyzed several widely-used medical QA datasets (e.g., MedQA[80], PubMedQA[86], MedMCQA[87], MedBullets [88], MMLU[67], MMLU-Pro[66], and CARE-QA[89]), as shown in Table 2. This analysis revealed three distinctive characteristics: (1) Most datasets are exclusively sourced from medical journal literature or professional medical examinations, with none incorporating real-world hospital data; (2) Question formats primarily consist of multiple-choice questions (MCQs) with 4-5 options, except for MMLU-Pro, which uses a 10-option format. These questions feature clear conditions and fixed options, failing to capture the ambiguity and"}, {"title": "5.1 Data Collection and Construction Pipeline", "content": "limited diagnostic information encountered in real clinical settings; (3) With the exception of CareQA, the remaining datasets lack continuous updates after their initial release.\nTo address this, we developed the JMED, a novel dataset based on real-world medical data distri-butions. Unlike existing datasets, JMED closely mimics authentic clinical data while facilitating effective model training. Although based on real consultation data, it is not directly sourced from actual medical data, allowing us to incorporate key elements necessary for model training. We ensured compliance with ethical and legal standards throughout the data collection process, safeguarding privacy and meeting ethical guidelines. Due to the open-ended nature of medical consultations, where definitive answers are often elusive, the evaluation process is more challenging. To address this, each question includes 21 response options, with a \"None of the above\" choice. This design significantly increases the complexity and difficulty of distinguishing the correct answers, thereby providing a more rigorous assessment framework.\nCompared to existing medical QA datasets, JMED has three principal advantages: First, it more accurately reflects the ambiguity in patient symptom descriptions and the dynamic nature of clinical diagnosis in real-world scenarios. Second, the expanded response options require enhanced reasoning capabilities to identify the correct answers among numerous distractors. Additionally, leveraging the vast amount of consultation data from JDH Internet Hospital, we can continuously generate data that aligns with the distribution characteristics of real patients."}, {"title": "5.1.1 Raw Data Processing", "content": "The dataset originates from anonymized doctor-patient dialogues at JD Health Internet Hospital, filtered to retain consultations adhering to standardized diagnostic workflows. The initial release contains 1,000 high-quality clinical records spanning all age groups (0-90 years) and multiple specialties.\n\u2022 Privacy Protection: Automated de-identification of sensitive information (names, institutions, locations) via regular expression matching.\n\u2022 Data Balancing: Ensured statistical representativeness across age, gender, and medical specialties based on platform-wide consultation patterns.\n\u2022 Deduplication: Applied semantic similarity algorithms to eliminate redundant chief com-plaints."}, {"title": "5.1.2 Structured Transformation", "content": "We constructed a set of multiple-choice questions (MCQs) based on the preprocessed data, as illustrated in Figure 5.\n\u2022 Electronic Medical Record (EMR) Generation: Extracted key clinical elements using prompt engineering to create structured EMRs.\n\u2022 Question Formulation: Employed the DeepSeek-r1 model to parse EMRs and generate clinically coherent questions aligned with diagnostic reasoning pathways."}, {"title": "5.2 Quality Assurance Framework", "content": "Considering the seriousness and precision required in the medical field, a three-tier quality control system was established. This primary review process involves collaboration with physicians from 15 departments, with each department having two attending or associate attending-level doctors review the questions. Secondary validation is distributed to associate experienced physicians to conduct a re-evaluation, leveraging their expertise to ensure data quality and accuracy, and final audit is processed by chief physicians. All manual review processes must adhere to the criteria as describe in appendix D.\nBased on the aforementioned criteria, we have constructed a set of 1000 multiple-choice questions derived, encompassing multiple departments and age groups. Each data entry includes a unique ID, department, question, options, and the correct answer. The options adhere to the ICD-10 standard for disease nomenclature and have been reviewed and validated by professional physicians to ensure the appropriateness of the questions, options, and correct answers."}, {"title": "5.3 Dataset Characteristics", "content": "Considering the seriousness and precision required in the medical field, a three-tier quality control system was established. This primary review process involves collaboration with physicians from 15 departments, with each department having two attending or associate attending-level doctors review the questions. Secondary validation is distributed to associate experienced physicians to conduct a re-evaluation, leveraging their expertise to ensure data quality and accuracy, and final audit is processed by chief physicians. All manual review processes must adhere to the criteria as describe in appendix D.\nBased on the aforementioned criteria, we have constructed a set of 1000 multiple-choice questions derived, encompassing multiple departments and age groups. Each data entry includes a unique ID, department, question, options, and the correct answer. The options adhere to the ICD-10 standard for disease nomenclature and have been reviewed and validated by professional physicians to ensure the appropriateness of the questions, options, and correct answers."}, {"title": "6 Experiments", "content": "As shown in Figure 6, the validated subset comprises 1000 clinically rigorous multiple-choice questions with the following demographic distributions:\n\u2022 Age Coverage: Full spectrum (0-90 years), with 83.37% of cases from the 21-40 age group after outlier adjustment.\n\u2022 Gender Ratio: 58% male vs. 42% female.\n\u2022 Specialty Distribution: Covers 15 clinical disciplines."}, {"title": "6.1 Experimental Setup", "content": "Implementation Details We tested our methodology on Qwen2.5-72B[81], LLama3.1-70B[14] as our base models due to their foundational capabilities. The knowledge capacity of such large-scale models is a prerequisite for stimulating medical reasoning abilities. We use a two-stage SFT to enhance the general capabilities of the model, and in the third stage, we use a small amount of reasoning data to improve the model's medical reasoning ability. In the final stage, we performed reject sampling and alignment on the model to further enhance its reasoning capabilities. We use DeepSpeed ZeRO-3 and Accelerate to train the LLM, with AdamW as the optimizer. The \u03b21 and \u03b22 of AdamW are 0.9 and 0.95, respectively. We apply a weight decay of le-4 and clip the gradient norm to 1.0.\nTraining Hyperparameters The hyperparameter settings for our model training are shown in the table below. The training parameters for Qwen and LLaMA differ only in the learning rate during the alignment phase. Details are shown in Table 3."}, {"title": "6.2 Benchmarks", "content": "We utilized the MedQA [80], PubMedQA[86], MedMCQA[87], MedBullets[88], MMLU[67], MMLU-Pro[66], and CARE-QA[89] as benchmarks, with JMED serving as a medical reasoning evaluation dataset specifically developed by us."}, {"title": "6.3 Main Results", "content": "We evaluated multiple open-source and closed source LLMs on medical tasks, as shown in Table below.\nAccording to the Main Result Table.4, Citrus1.0-Llama-70B reach a top class performance on 70B scale models, especially on MedQA, PubMedQA,MedBullets, CareQA benchmark. Citrus also surpasses many close-source top LLMs such as Claude-sonnet and GPT-4o. Our model consistently demonstrates strong performance across a wide range of medical benchmarks, highlighting the effectiveness of our proposed approach. Observing the loss curve in Figure.7, it can be seen that the model gradually converges at each SFT stage. In the alignment phase, the reward curve of CPO-SimPO gradually rises and converges. The evaluation results indicate that the performance of the aligned model is the best among all stages."}, {"title": "6.4 Future Discussion", "content": "In the ablation experiments, we explore the impacts on different stage of training, including SFT stage 1,2,3 and RL stage. As the most distinguishable and influential benchmarks for medical scenario"}, {"title": "7 Conclusion", "content": "We present Citrus, a medical language model designed to enhance medical reasoning by emulating the cognitive processes of medical experts. Through a novel data synthesis approach and a multi-stage training methodology, we have developed a model capable of efficiently handling complex medical decision-making tasks. By releasing the model and its training data, we aim to promote further research in AI-driven medical reasoning and decision-making, thereby contributing to the advancement of healthcare technologies.\nThinking like an Expert We have constructed a medical reasoning dataset modeled from the cognitive processes of doctors, and have effectively demonstrated that such data significantly enhances the problem-solving capabilities of LLMs in medical scenario. Through an exploration of doctor\u015b thought processes, design of experimental data and attempts at model training, we have ultimately developed an LLM capable of effectively leveraging Long COT generated data to address medical issues. From a high-order perspective, we envision that our approach could be widely applicable across domains. By deconstructing the cognitive strategies of experts and utilizing representative core data to generate training data through our approach, models could potentially learn to abstract thinking specific to a given domain. We believe this approach can serve as a comprehensive alternative to human feedback. As a criterion, it effectively replaces the necessity of human feedback in training, allowing the model to understand the underlying characteristics of thinking. Through this understanding, the model can approach generalizable problems from an elevated level of cognitive abstraction, thereby becoming a domain expert.\nComplex Training Pipeline We developed a comprehensive multi-phase training pipeline for Citrus, incorporating CPT, SFT, and RL, to enable the model to efficiently learn and adapt to complex medical reasoning tasks. By understanding the problem-solving thought processes of medical experts, we identified the dual-process theory and applied distinct cognitive strategies to various training phases using CPT and SFT. While we believe that extensive pre-training data and clinical examples will help the model perform pattern recognition, there is currently no effective method to equip the model with the complex reasoning abilities that medical experts use to solve problems. We employed a warm-up training phase using data courses and a carefully designed COT data generation method. By training the base model in a specific order, it is gradually enhanced into a medical reasoning model. In the final stage, we claim that offline RL training could further enhance the model's reasoning ability, ultimately ranking it among the top models of similar parameter scales on several authoritative benchmarks."}, {"title": "A Ethical Statement", "content": "Although the proposed model is a medical LLM with complex reasoning capabilities, it may still produce content that includes hallucinations or inaccuracies. Therefore, the current model is not suitable for real-world applications. Consequently, we will impose strict limitations on the use of our model. The models are not permitted for use in clinical or other industry applications where such inaccuracies could lead to unintended consequences. We emphasize the ethical responsibility of users to adhere to these restrictions in order to safeguard the safety and integrity of their applications."}, {"title": "B Prompt", "content": "Here are the prompt examples.\nReasoning Expert Prompt\nYou are tasked with addressing a medical examination question. Please carefully read the question, provide a detailed thought process, and then present your final answer.\nHere is the question:\n{Q}\nFacing on the previous question, you are an assistant that engages in extremely thorough, self-questioning reasoning.\nYour approach mirrors human stream-of-consciousness thinking, characterized by continuous exploration, self-doubt, and iterative analysis.\nWith the expectation that when facing this medical issue, you will be able to apply professional medical reasoning methods, such as differential diagnosis, to further reason and think about the problem.\"\nBelow is the definition of the differential diagnosis method in medical reasoning:\nDifferential diagnosis refers to the process of systematically considering different possible diseases, ruling out diagnoses that do not match the condition, and ultimately determining the most likely disease. It involves the following steps:\n\u2022 Collecting information: Inquire about the medical history, conduct physical exams, and perform necessary laboratory tests.\n\u2022 Listing possible diagnoses: Based on the medical history, signs, symptoms, and laboratory results, list all possible diseases.\n\u2022 Gradually eliminating: Through further tests, symptom evaluations, and diagnostic tests, gradually eliminate impossible diagnoses, ultimately confirming the most likely disease.\nDifferential diagnosis is a process of comparison and contrast, where doctors judge each potential diagno-sis based on its characteristics, finding the disease that most closely matches the patient's symptoms and signs.\nPlease establish the following process in your logical reasoning:\n1.  List all the known information in the problem, including the complete medical history and all test results.\n2.  List possible diagnoses.\n3.  Attempt to build a logical reasoning process.\nBelow are the reasoning requirements; please ensure each step of the reasoning process meets the following criteria:\n(more details are listed in https://github.com/jdh-algo/Citrus)\nYour reasoning steps should follow these requirements:\n\n[Your extensive internal monologue goes here]\nBegin with small, foundational observations\nQuestion each step thoroughly\nShow natural thought progression\nExpress doubts and uncertainties\nRevise and backtrack if you need to\nContinue until natural resolution"}]}