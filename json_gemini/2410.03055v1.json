{"title": "Permissive Information-Flow Analysis for Large Language Models", "authors": ["Shoaib Ahmed Siddiqui", "Radhika Gaonkar", "Boris K\u00f6pf", "David Krueger", "Andrew Paverd", "Ahmed Salem", "Shruti Tople", "Lukas Wutschitz", "Menglin Xia", "Santiago Zanella-B\u00e9guelin"], "abstract": "Large Language Models (LLMs) are rapidly becoming commodity components of larger software systems. This poses natural security and privacy problems: poisoned data retrieved from one component can change the model's behavior and compromise the entire system, including coercing the model to spread confidential data to untrusted components. One promising approach is to tackle this problem at the system level via dynamic information flow (aka taint) tracking. Unfortunately, the traditional approach of propagating the most restrictive input label to the output is too conservative for applications where LLMs operate on inputs retrieved from diverse sources. In this paper, we propose a novel, more permissive approach to propagate information flow labels through LLM queries. The key idea behind our approach is to propagate only the labels of the samples that were influential in generating the model output and to eliminate the labels of unnecessary input. We implement and investigate the effectiveness of two variations of this approach, based on (i) prompt-based retrieval augmentation, and (ii) a k-nearest-neighbors language model. We compare these with the baseline of an introspection-based influence estimator that directly asks the language model to predict the output label. The results obtained highlight the superiority of our prompt-based label propagator, which improves the label in more than 85% of the cases in an LLM agent setting. These findings underscore the practicality of permissive label propagation for retrieval augmentation.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as GPT-4 [1], Llama [2, 3], Mistral [4], and PaLM [5] are rapidly becoming commodity components of larger software systems. The inputs to these LLMs often consist of data retrieved from a variety of sources, including websites, productivity software, or tools [6], and their output is usually passed on to other software components for further processing [7].\nThis poses natural security and privacy problems: low integrity inputs (e.g., poisoned data) can change the model's behavior in unexpected ways and potentially affect the entire system [8]. Similarly, high confidentiality inputs (e.g., confidential documents) can be inadvertently leaked to an untrusted downstream component [9].\nOne possible approach to address this problem is to rely on the LLM itself for mitigation, for example via introspection of the retrieved inputs or guardrails given in the meta-prompt. However, such defenses can be circumvented with more advanced attacks [10, 11, 12, 13], leading to the undesirable cat-and-mouse game that is common in system security.\nAnother approach is to tackle this problem at the system level via dynamic information flow (aka taint) tracking [14, 15, 16]."}, {"title": "II. PROBLEM SETTING AND GOAL", "content": "We consider a general inference scenario in which an LLM takes as input a textual prompt x and a set of documents\u00b9 C, which we refer to as the context. We refer to any subset S \u2286 C as a subcontext. Given prompt x and context C, we represent the LLM as a probability distribution $P_{LM}(y | x, C)$ over possible completions y.\nThis formulation is sufficiently general to represent many real-world applications of LLMs. For example, in retrieval-augmented generation (RAG) [19, 21], the context contains documents retrieved from the knowledge-base. Alternatively, if an LLM uses plugins or function calls to obtain additional information (e.g., web search, email retrieval, calendar query, etc.) [22], the data returned by the plugins is part of the context.\nWe assume that each document $c \\in C$ in the context is assigned a label from a set L of labels, which we model as a label assignment function $l : C \\rightarrow L$. Labels can be used for many purposes, including representing access control information or information about the reliability of the source of the document.\nAs is common practice [23, 24, 16], we assume L forms a lattice, i.e., it has a partial order < in which every pair of labels $L_1, L_2 \\in L$ has a least upper bound (aka join) $L_1 \u2294 L_2$ and a greatest lower bound (aka meet) $L_1 \u2293 L_2$. One often uses join and meet operations to combine multiple labels into a single label that still captures the relevant security information. In particular, we can naturally define the label L of a context C as $L = \u2294_{c \\in C} l(c)$. In all cases, labels lower in the lattice are said to be more permissive, as illustrated in the following examples:\nConfidentiality. A canonical example of a security lattice is the set {Secret, General}, denoting high and low confidentiality data, where General < Secret. The join of Secret and General is Secret (i.e., high confidentiality), but General is the more permissive label.\nIntegrity. Similarly for integrity, the set {HiInt, LoInt} denotes high and low-integrity data such that HiInt < LoInt. The join of LoInt and HiInt is LoInt (i.e., low integrity), but HiInt is the more permissive label.\nSeparation of Duty. Another example is the lattice where labels are subsets of users and where assigning label U to a particular action denotes that all users u \u2208 U must authorize the action before it can take place. In this case, the join and meet operations correspond to set union (least permissive) and set intersection (most permissive), respectively."}, {"title": "B. Goal: Permissive Label Propagation", "content": "Since the documents in the context of an LLM can influence the model's output, the label L of the output will depend on the labels of documents in the context. We refer to the process of determining L as label propagation. A na\u00efve approach to label propagation in LLMs is to assume that all documents in the context impact the output, and hence to propagate the label $L = \u2294_{c \\in C} l(c)$.\nHowever, the LLM does not necessarily need all documents in the context to generate the output. In the example in Figure 1, the LLM did not need the web search result from the untrusted website. Labeling the output as untrusted would be overly pessimistic and possibly inhibit the system from using the output further, e.g., as the input to another tool that requires trusted data.\nOur goal is to obtain a more permissive output label by propagating only the labels of the inputs that are actually necessary for generating the output. However, since the lattice of labels only forms a partial order, one challenge is that different subsets of the inputs are not always comparable. As a consequence, we cannot hope to find a single optimal label. Instead, a label propagator should ideally find all minimal labels and delegate the selection of one to the underlying application."}, {"title": "III. PERMISSIVE LABEL PROPAGATION", "content": "In this section, we describe our approach and system for the propagation of permissive information-flow labels from the inputs to the LLM output. The core idea is to propagate only the labels of the documents in the context that were necessary for generating the output. A na\u00efve solution to this problem would be to iterate over all subsets of documents in the context and determine whether they can be removed without significantly affecting the model's output. However, the computational cost of this approach grows exponentially with the number of documents in the context. In this section, we present a more efficient algorithm.\nOur key observation is that it is sufficient to identify the labels that improve over the full context's label, and consider only their corresponding subcontexts. As the label lattices that occur in practice are often small (e.g. secret vs public, trusted vs untrusted), this leads to a solution that is both practical and optimal under certain monotonicity assumptions.\nLet C be a context with label $L = \u2294_{c \\in C} l(c)$. For a label L' that is at least as permissive as L (i.e., L' \u2286 L), we define the L'-subcontext $C|L'$ of C as the set of all documents whose label is at or below L', i.e., $C|L' = \\{c \\in C \\mid l(c) \u2286 L'\\}$. Clearly, $C|L = C$ because all the documents in C satisfy $l(c) \u2286 L$ by definition.\nOur goal is to find labels L' \u2286 L such that the output of the language model changes only negligibly when substituting C with $C|L'$. We capture \"negligible change\" by introducing a hyperparameter \u03bb and require that the utility of the model's output under the full context drops by at most \u03bb when restricting to the subcontext. Formally:\nLet x be a prompt, y a completion, and C a context with label L. For a given utility metric U and hyperparameter \u03bb \u2265 0, we consider another label L' to be \u03bb-similar to L if\n$U(P_{LM}(y | x, C)) - U(P_{LM}(y | x, C|L')) \u2264 \u03bb $. (1)\nNote that \u03bb-similarity is not an equivalence relation because it is not symmetric or transitive.\nDefinition 1 leaves the choice of the utility function U and the language model $P_{LM}(y | x, C)$ unspecified, as different applications require custom choices of these functions. In this paper, we focus on language modeling where perplexity is a common way to measure utility [25]. Hence, for the remainder of this paper, we compute utility as the negative perplexity:\n$U(P_{LM}(y | x, C)) = - (\\prod_{i=1}^{|y|} P_{LM}(y_i | x, C, y_{<i}))^{-1/|y|}$ (2)"}, {"title": "B. Computing \u039b-similar Labels", "content": "We describe our algorithm for identifying \u039b-similar labels and their contexts. As we observed before, it is not necessary to iterate over all subsets of the context: it suffices to iterate over all labels below the full context's label and consider their corresponding subcontexts. Technically, we iterate over the powerset of P({\\[ c\u2208c l(c) | C' \u2286 C'}) of possible output labels and we compute the similarity of the corresponding subcontexts to the full context. Note that the set of labels considered is based on documents in the context; it is finite even when the full lattice of labels L is infinite (e.g. timestamps).\nAlgorithm 1 describes this idea in pseudocode. We represent the powerset of possible labels as a directed acyclic graph (DAG) where nodes represent labels and edges represent the lattice order."}, {"title": "D. A System for Label Propagation", "content": "We briefly discuss how to integrate Algorithm 1 into existing model architectures and systems. For this, we consider two architectures for augmenting language models with retrieved information: prompt-based retrieval augmentation [19, 21] and kNN language models [20].\nAugmenting LLM prompts with retrieved documents has become a popular approach to incorporate a non-parametric datastore into an LLM-based pipeline [19, 21]. For example, in a Retrieval Augmented Generation (RAG) setup [19], the documents most relevant for a query are retrieved and added to the context in the prompt to an autoregressive language model.\nIn the kNN-LM architecture [20], the language model produces an output distribution $P_{LM}(y | x)$ without taking into account the retrieved documents. A separate distribution $P_{kNN}(y | x, C)$ is computed based on the retrieved documents only, using x as the criteria for document selection. Finally, a mixture of both distributions (parameterized by a hyperparameter \u03b3) produces the final retrieval augmented output which is given by\n$p(y | x, C) = \u03b3P_{kNN}(y | x, C) + (1 \u2212 \u03b3)P_{LM}(y | x)$. (4)\nWhile $P_{LM}$ requires expensive LLM inference, the expression $P_{kNN}$ can be computed efficiently by pre-computing a key-value store with a single forward pass over the datastore that is then queried at inference time.\nOur approach can be integrated into both of the above architectures as a wrapper around the existing system. We assume that every document c that can be retrieved has a label l(c). In practice, if this is not the case, unlabeled documents can be assigned the most restrictive label (i.e. the top of the lattice). The process then proceeds as follows:\nGiven a prompt x, retrieve the context C and run the model as usual on (x, C) to obtain the original completion y.\nCompute the pessimistic label L of C, and run Algorithm 1 on (C, L, x, y) to obtain a set A of labels that are \u039b-similar to L.\nChoose an appropriate L' \u2208 A using application-dependant criteria and run the model again on (x, $C|L'$ ) to obtain a new completion y'.\nReturn the new completion y' and the new label L'.\nIn practice, our algorithm is an AI-based heuristic that can make mistakes or be misled adversarially (e.g., failing to identify an influential document, or over-estimating the influence of a document). However, these mistakes do not affect the safety of the propagated labels. By rerunning the model on the new context $C|L'$ (step 3) and returning only y' (step 4), our approach guarantees that the returned completion depends only on documents at or below L'. This safety property holds even in the case of adversarial input documents (e.g., prompt injection) because it is enforced by the system, rather than the model."}, {"title": "E. Computational Cost", "content": "The label propagator wrapper based on prompt-based augmentation requires a number of LLM calls that in the worst-case is exponential in the number of documents in the context. This occurs e.g., in a flat bounded lattice where all labels between \u22a5 and T are incomparable and when each subcontext has a different label.\nHowever, the number of LLM queries is always bounded by the size of the lattice. When the lattice forms a totally ordered set (e.g. two-element lattices distinguishing between trusted vs untrusted or confidential vs public data), Algorithm 1 stops as soon as it finds a subcontext whose utility drops below a \u03bb difference w.r.t. the utility of the full context. For richer lattices describing more fine-grained security policies, Algorithm 1 visits a small subset of the lattice in typical queries, either because only a few labels are represented in the context or because the utility drops below the tolerance \u03bb. Therefore, computational costs are not a major concern for several fundamental lattices.\nTo avoid worst-case costs, alternative algorithms or model architectures may be more suitable. For example, the kNN-based architecture we consider in this paper only requires one additional LLM call per query ($P_{LM}(y | x)$, in addition to the original query $P_{LM}(y | x, C)$).\nNote that despite an increase in the number of LLM calls for label propagation, the cost incurred per call can be drastically reduced utilizing common inference optimizations implemented in production-ready model serving backends [27, 28]. For instance, compute-bound prompt processing in Transformers can be amortized across calls sharing a common prompt prefix by reusing a KV cache. For a totally ordered lattice, appending retrieved documents at the end of the prompt sorted by their labels will result in no additional prompt processing costs in subsequent LLM calls because their prompts are obtained by peeling off documents at the end of the prompt.\nWhile caching can save computation in processing prompt tokens, it does not impact the efficiency of the memory-bound decoding phase. However, we can use an algorithm-specific optimization to decode only a subset of the tokens that a na\u00efve implementation of Algorithm 1 would decode. Since we use negative perplexity as the utility metric, during the decoding phase of an LLM call, we can keep a running calculation of the cumulative likelihood of the tokens decoded so far and stop decoding as soon as the utility drops below a difference \u03bb of the utility of the full context."}, {"title": "IV. EVALUATION SETUP", "content": "As this is a novel problem setup, we first define a robust evaluation scheme that we use to evaluate our label propagators and compare them against an introspection baseline. Our goal is to understand the performance of our label propagator in correctly identifying and propagating minimal labels. For this, we construct three datasets that enable us to evaluate different aspects of the system. The evaluation results are presented in Section V."}, {"title": "V. RESULTS", "content": "In this case, we assume a perfect recall retriever (all relevant documents are present), albeit lower precision (the rest of the documents out of the total limit of 14 are filled by adding irrelevant documents). For the introspection baseline, we use one-shot learning based on the first example in the dataset and use the remaining 63 questions for evaluation. Table I summarizes our results.\nThe prompt-based propagator achieves an exact match accuracy of over 85% and precision and recall of over 90%. This means that, in more than 85% of the cases, the label search is able to identify the correct influential subcontexts out of a set of 16k possibilities without making a single mistake. Furthermore, the RAG-based label search significantly outperforms both the kNN-LM-based label search as well as the introspection baseline.\nWe plot the exact match accuracy of the label search on the key-value dataset w.r.t. the number of parameters in Figure 7."}, {"title": "B. News article dataset", "content": "Moving towards a more realistic setup, we include an additional retriever component for the news article dataset that first retrieves relevant articles (either low or high integrity) from the dataset before response generation. In order to correctly understand the impact of the retriever, we compare a perfect retriever that is able to retrieve all relevant documents (similar to the synthetic key-value dataset) with a realistic retriever using cosine similarity in the embedding space computed by BGE-Large-EN [36].\nTable II summarizes the results of the label propagator on the news article dataset. For this dataset, the label propagator achieves slightly worse performance on all metrics in comparison to the synthetic key-value dataset due to the lower quality of the dataset (automated generation of QA pairs by GPT-4 [1]). Furthermore, we see a reduction in the exact match accuracy of about 10% when using a realistic retriever in contrast to a perfect retriever. This is due to the retriever sometimes failing to retrieve the relevant pieces of information, ultimately leading to a mismatch with the ground-truth label."}, {"title": "C. LLM agent dataset", "content": "In the LLM agent dataset, we take a step further by also computing the misalignment introduced by regenerating the output conditioned on the updated context (when label improvement is possible). The results are presented in Table III. We fix a threshold of \u03bb = 0.2 for this experiment. We use the base model without instruction tuning in this case due to the use of a custom chat format.\nFirst, we focus on the 20 cases where label improvement is possible i.e., L* \u228a l(C'). We find that the label propagator improves the label in at least 17 of these cases.\nIn at most 2 of the cases, the LP is overly optimistic and returns a more permissive label L' than the ground truth label L*. Again, the safety property still holds in these cases because the system regenerates the output using the reduced context $C|L'$. This regeneration step introduces the possibility that the new output y' differs significantly compared to the output y from the full context.\nTo quantify this, we measure the alignment between y and y* as well as y' and y* using the ROUGE-L F-score, with the results shown in Table IV. We obtain a ROUGE-L F-score of at least 0.85, suggesting that conditioning on the reduced context leads to a similar model response. Furthermore, we see a negligible difference in alignment between the full context output and the reduced context output when label improvement is possible, but observe a significant drop when such an upgrade is not possible."}, {"title": "VI. ANALYSIS AND DISCUSSION", "content": "In this section, we highlight the main findings of our work and discuss the implications of our results when used in a real-world setting.\nWe find that our prompt-based label propagator is able to find the exact set of minimal labels in 86% of the cases. This is for a large lattice of labels where the label propagator needs to find the correct subset out of 16k possible sets of labels.\nWe evaluate the label propagator on a smaller lattice with a total order, allowing us to compare labels directly and quantify the label improvement. In this case, our label propagator improves the label in 56% of the cases for the news article dataset and 85% of the cases for the LLM agent dataset.\nWe showcase the label propagator in a real-world use case in a tool calling LLM agent setup where the propagated label is used to determine whether a sensitive tool call is allowed. We find that the RAG-based label propagator is able to improve the label in more than 85% of the cases while the output of the LLM agent remains the same as measured by the difference between the reduced context output alignment and the full context output alignment.\nComparison to baselines. The introspection-based label propagator achieves a noticeable improvement in performance when using a 70B parameter model compared to a 7B model."}, {"title": "A. Use-cases", "content": "Algorithm 1 identifies sub-contexts with labels that are more permissive than that of the full context. However, whenever the desired output label L can be determined up-front, it is possible to side-step the search over \u039b-similar sub-contexts and directly restrict the retrieval component to documents at or below L.\nOur approach reveals its true benefits when the use of the generated content is not yet determined after the initial retrieval step. In such cases, having a too-restrictive label comes at the cost of limiting future uses of the generated output. For example:\nSemantic caches [37] can be extended to store answers along with a sensitivity label, where more permissive labels facilitate broader reuse of the generated content.\nEmails are often forwarded beyond their initial set of recipients, which is facilitated by using permissive labels.\nFrameworks such as LangChain, LlamaIndex, Orkes, AutoGen, or TaskWeaver enable writing programs with LLM components. Similar to classic language-based information flow analysis [16], more permissive labeling enables the design of more expressive secure programs.\nLastly, our approach naturally allows users to endorse information. Let's consider a setup as illustrated in Figure 1. Initially, there might only be an untrusted source answering the initial query. The LP system would correctly output an untrusted label to the potentially dangerous command. However, after a trusted authority (e.g., a university department IT admin) confirms the suggestion in the untrusted source, the LP recognizes that both sources are similarly influential and assign a trusted label to the output. Therefore, by quoting or repeating untrusted information a trusted source can endorse the information."}, {"title": "B. Limitations", "content": "Our label propagator is able to improve on the baseline label by more than 50% and 85% in two realistic datasets. However, this improvement comes with the cost of an increased number of LLM calls (discussed in Section III-E). We now highlight other limitations associated with our label propagator.\nImpact of adversary: In the presence of an adversary, our label propagator assigns the lowest integrity label by design in order to avoid any harmful side effects such as cross-prompt injection [38]. However, this can potentially lead to a degradation of service attack as the adversary can add unreliable distracting information that makes it impossible for the system to improve the label, reducing the downstream utility of the model output."}, {"title": "C. Extensions", "content": "Our main focus has been on predicting the least conservative label while ensuring that utility is not compromised beyond a certain threshold \u039b (Definition 1). However, our proposal is flexible enough to accommodate various use cases. For instance, it can be reversed to determine the utility for a specific target label. Alternatively, a more complex use case would be to make the system dynamic with respect to \u03bb, which means the system is able to compromise more utility to achieve a better or less conservative label."}, {"title": "VII. RELATED WORK", "content": "Influence estimation plays a crucial role in identifying the impact of individual training instances on a model's predictions. Brophy et al. [39] present a good overview of different influence estimation techniques such as (i) leave-one-out estimation, (ii) Shapley value estimation, and (iii) sub-sample-based Shapley value estimation (adapted from Feldman and Zhang [40]). Influence estimation techniques have been popularly used in the machine learning literature for different applications. Koh and Liang [41] demonstrate the application of influence functions to debugging models and detecting dataset errors. Feldman and Zhang [40] explained the memorization of tail data points using the influence of those data points on the model's predictions. Ilyas et al. [42] used influence estimation to present the idea of datamodels that allows the model to make predictions using linear regression."}, {"title": "VIII. CONCLUSION", "content": "We presented a permissive approach to propagating information-flow labels of documents retrieved in RAG systems. The key idea is to propagate only the labels of those documents that are actually used for generating the model's output. We show that our approach is practical in terms of performance and infers more permissive labels than an introspection baseline. Unlike introspection-based methods commonly used in practice, our approach can satisfy a hard safety guarantee.\nWe hope that this research spurs interest in system-level solutions to security and privacy concerns stemming from the use of unreliable AI software components. We may not be able to fully analyze how information flows through these components, but we can control which information they use and build more useful systems around them that enforce security and privacy policies."}, {"title": "APPENDIX", "content": "Algorithm 1 relies only on A to produce the minimal set of labels. However, in the cases where documents share similarities and provide important information regarding the model output such as the format (which is the case for our synthetic key-value dataset IV-E), the overall contribution of irrelevant documents goes up. Therefore, additional irrelevant documents are introduced in the predicted label set due to being A-similar.\nIn order to cover these cases when dealing with a complex lattice and a large number of relations, we propose a simple heuristic based on Shapley values that have been commonly used in the past [56, 44]. In particular, we compute the Shapley value for each of the different labels in the lattice and filter out any label combinations where labels below a particular threshold are present.\nThe Shapley value defines the marginal contribution of any label L by computing the average difference in outcomes when a particular label is present and absent. We compute the label Shapley value via perplexity, which is computed as the average drop in perplexity when a particular label is included. This provides a notion of the importance of each label. This now introduces an additional hyperparameter i.e., the Shapley value threshold."}]}