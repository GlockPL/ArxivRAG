{"title": "THINKGUARD: Deliberative Slow Thinking Leads to Cautious Guardrails", "authors": ["Xiaofei Wen", "Wenxuan Zhou", "Jacky Mo", "Muhao Chen"], "abstract": "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose THINKGUARD, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, THINKGUARD achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, THINKGUARD improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.", "sections": [{"title": "Introduction", "content": "Recent years have seen large language models (LLMs) demonstrate remarkable emergent capabilities (Kaplan et al., 2020; Wei et al., 2022), enabling their use as the backbone of a wide range of intelligent systems (Achiam et al., 2023; Dubey et al., 2024; Liu et al., 2024a). Therefore, ensuring do not generate harmful content (Pan et al., 2023; Peng et al., 2024; Staab et al., 2024) has become increasingly critical for their safe deployment. Guardrails (Markov et al., 2023a; Inan et al., 2023; Chi et al., 2024; Han et al., 2024), acting as external safety layers that detect and filter harmful inputs or outputs of LLMs, have gained attention for their flexible deployment across different LLMs and their effectiveness in mitigating risks.\nExisting guardrails are typically framed as a discriminative task: given an input (and sometimes an output), they classify it as safe or unsafe or assign it to specific harmful content categories. These methods can be categorized into rule-based and model-based approaches. Rule-based guardrails rely on predefined lists of forbidden words or rules to detect harmful content. While simple and efficient, they struggle to adapt to evolving threats and varying guidelines (Gehman et al., 2020; Welbl et al., 2021; Rebedea et al., 2023). Model-based guardrails, on the contrary, are developed by continually finetuning pretrained language models on instruction-based safety datasets, enhancing their contextual awareness and generalizability (Inan et al., 2023). However, many still approach guardrails as a simple classification task, merely predicting labels rather than engaging in deliberative reasoning about intent, context, and potential risks. As a result, these models often fail to distinguish between genuinely harmful content and benign yet sensitive statements, making them particularly vulnerable to adversarial prompts that exploit models' weaknesses (Zizzo et al., 2024; Zhang et al., 2024; Liu et al., 2024b). Moreover, since these models only output labels without explaining their decisions, it is difficult to interpret why a particular prediction was made, limiting transparency and trust in their decisions.\nTo address these limitations, we propose THINKGUARD, a model that enhances guardrails through cautious, deliberative processing. Our method is inspired by the cognitive distinction between fast and slow thinking in human decision-making (Hagendorff et al., 2022; Min et al., 2024). While fast thinking, such as simple end-to-end classification, enables quick and intuitive predictions, it often leads to oversimplified or inaccurate assessments, making models vulnerable to adversarial inputs (Li et al., 2024; Zhou et al., 2024). In contrast, slow thinking, through deliberative process-"}, {"title": "Related Work", "content": "Guardrails Recent research on guardrail models has mainly explored two key approaches: rule-based filtering and LLM-based safety classifiers.\nEarly guardrail models relied on rule-based filtering (Welbl et al., 2021; Clarke et al., 2023; G\u00f3mez et al., 2024), which uses predefined keyword lists and heuristic constraints to identify harmful content. While these methods are transparent and efficient, they suffer from rigidity and poor adaptability, leading to often issues of false positives (overblocking benign content; (Song et al., 2023)) and false negatives (failing to detect nuanced harms; (Paudel et al., 2022)). Due to these limitations, research has shifted towards LLM-based classifiers, which provide greater flexibility by leveraging large models for content assessment.\nLLM-based safety classifiers, such as LLaMA Guard (Inan et al., 2023; Fedorov et al., 2024; Chi et al., 2024) and Aegis Guard (Ghosh et al., 2024), improve moderation by fine-tuning models on safety datasets, enabling them to classify inputs according to predefined safety guidelines. Unlike rule-based approaches, these classifiers leverage large models to capture different categories of harmful content, making them more adaptable to diverse regulatory requirements and shifting safety standards. However, most LLM-based guardrails"}, {"title": "THINKGUARD", "content": "In this section, we introduce THINKGUARD, a critique-augmented guardrail model designed to enhance safety classification and justification.\nGuardrail models act as external safety layers for LLMs, but existing guardrails often rely on single-pass decisions, lacking reasoning and explanatory capabilities. This limits their ability to detect stealthy and implicit risks and provide transparent, trustworthy moderation.\nTo address these challenges, we propose THINKGUARD, a framework that enhances guardrail models by incorporating deliberative reasoning into safety classification. Inspired by recent advancements in slow-thinking reasoning of LLMs (OpenAI, 2024c; Min et al., 2024), THINKGUARD introduces a critique-based fine-tuning approach, where a high-quality language model generates critiques\u2014natural language explanations that justify safety classifications. These critiques are then used to improve the safety classification capabilities of a smaller, efficient guardrail model, enhancing both its accuracy and interpretability. Prior research has shown that self-generated critiques enhance model robustness in reward modeling and adversarial defenses (Gallego, 2024; Yu et al., 2024). Extending this idea to safety classification, THINKGUARD adopts multi-step safety assessments, allowing the guardrail model to make more thoughtful and context-aware moderation decisions rather than relying solely on direct classification. The following sections define our problem definition and detail our methodology for generating, filtering, and integrating critiques to improve guardrail effectiveness."}, {"title": "Problem Definition", "content": "Given a set of predefined safety guidelines, the goal of a guardrail model is to assess whether an input text adheres to these guidelines, while detecting and categorizing any unsafe content into specific violation categories. Desirably, the guardrail may also be designed to generate an explanation for the safe classification outcomes.\nFormally, let x be an input text, which can be either a user query to an LLM or an LLM-generated response. Let G represent a set of safety guidelines with corresponding risk categories {C1, C2, ..., Cn}. The guardrail model g"}, {"title": "Critique-Augmented Fine-Tuning", "content": "To enhance the capability of guardrail models in both safety classification and justification, we fine-tune a smaller classifier using critique-augmented data. Instead of solely training on safety labels, our approach incorporates detailed critiques generated by a high-quality expert LLM. These critiques serve as additional supervision signals during fine-tuning, enabling smaller guardrail models to develop both classification and reasoning capabilities.\nThe fine-tuning dataset is constructed from a safety alignment corpus, where each data instance consists of a [prompt, response] pair, a ground truth safety label, and an LLM-generated critique. Specifically, the dataset is structured as:\nD = {(x_i, r_i, y_i, c_i)}^N_{i=1},\nwhere xi represents the input prompt, ri is the corresponding response, yi is the binary safety label (safe or unsafe), ci is the critique generated for the given response. To ensure consistency in critique generation, each input is formatted using a structured prompt, as illustrated in Fig. 2. By training on this enriched dataset, the model learns to associate textual patterns with safety violations while simultaneously developing the ability to generate justifications for its decisions. The differences in results for the different critique formats fine-tuning are discussed in Appx. \u00a7A.\nLarge-scale proprietary models with hundreds of billions of parameters, such as GPT-4o (OpenAI, 2024b), exhibit strong reasoning capabilities in safety classification and justification (Ying et al., 2024; OpenAI, 2024a). However, deploying such massive models as guardrails in real-world applications is impractical due to their high computational cost and latency. Instead, we aim to distill this knowledge into smaller, more efficient models while preserving their ability to assess safety risks and generate justifications. Knowledge distillation has been shown to effectively transfer reasoning capabilities from large models to smaller ones (Hinton et al., 2015; Hsieh et al., 2023). By incorporating critiques generated by high-capacity models, we expose compact guardrail models to high-quality safety rationales during training, enabling them to develop reasoning skills without requiring explicit optimization for explanation generation.\nThe fine-tuning process leverages label-enriched supervised fine-tuning (Wen et al., 2024; Yu et al., 2024), where critiques are incorporated as additional annotations"}, {"title": "Inference and Decision Making", "content": "Once fine-tuned, the model first predicts a safety label, then identifies violated safety categories if applicable, and finally generates a critique. Given an input prompt-response pair (x,r), the model infers:\n\\hat{y} = arg \\max P(y|x,r).\nIf the response is classified as unsafe, the model further predicts the set of violated safety categories:\n\\hat{t} = arg \\max P(t|x, r, \\hat{y}).\nFinally, the model generates a critique to justify its decision as\n\\hat{c} = arg \\max P(c|x, r, \\hat{y}, \\hat{t}),\nwhere yi \u2208 {safe, unsafe}. If \\hat{y} = unsafe, the model additionally predicts the violated safety categories, denoted as t, which is non-empty in unsafe"}, {"title": "Experiments", "content": "To evaluate THINKGUARD, we conduct experiments on multiple safety benchmarks, focusing on (1) safety classification performance, (2) comparative analysis of model capabilities, and (3) the impact of data scaling on accuracy and reasoning ability."}, {"title": "Experiment Setup", "content": "Public Safety Dataset We use the BeaverTails dataset (Ji et al., 2023) as the primary training source for our guardrail model. BeaverTails is a safety-focused dataset comprising human-labeled (prompt, response) pairs, each annotated with one or more harm categories. It provides explicit safety labels that enable fine-grained classification, covers a diverse range of safety concerns to improve generalization, and serves as an established benchmark, which has been utilized in prior work such as LLaMA Guard 2 (Team, 2024) and hh-RLHF (Bai et al., 2022; Ganguli et al., 2022).\nTo enhance our model's reasoning and justification abilities, we generate synthetic critiques by leveraging high-quality LLMs such as GPT-4o (OpenAI, 2024b) and DeepSeek-R1-Distill-Llama-70B (DPSK-LLaMA-70B) (DeepSeek-AI et al., 2025). Given the safety labels from the BeaverTails dataset, these models are prompted to generate critiques that explain the classification decision and, when applicable, identify the violated safety categories. This critique-augmented dataset enables the fine-tuning of smaller guardrail models (e.g., LLaMA Guard 3 (Llama Team, 2024)), equipping them with both classification accuracy and reasoning capabilities.\nTo assess the effectiveness of our critique-augmented guardrail model, we evaluate it across multiple safety-related benchmarks:\nA comprehensive dataset comprising over 30000 human-labeled (prompt, response) pairs, each annotated with one or more harm categories, facilitating the evaluation of models' abilities to handle various safety concerns."}, {"title": "Results", "content": "Overall Benchmark Performance Tab. 1 presents F1 and AUPRC scores across four safety benchmarks. THINKGUARD consistently achieves the highest average F1 and AUPRC, demonstrating its strong generalization and effectiveness in safety classification.\nCompared to LLaMA Guard 3, THINKGUARD significantly improves performance across all datasets, confirming the advantages of critique-augmented fine-tuning. Specifically, it enhances F1 by 14.1% on Toxic Chat and 6.1% on WildGuard-Mix, indicating better robustness against adversarial and ambiguous cases. Notably, compared to label-only fine-tuned LLaMA Guard 3, THINK-GUARD achieves more stable performance across four datasets, while the label-only model struggles on Toxic Chat and OpenAI Moderation, suggesting weaker generalization.\nFurthermore, THINKGUARD attains the highest AUPRC on BeaverTails and the best F1 on Wild-GuardMix, demonstrating its adaptability across different safety risks. While GPT-4o+CoT achieves the best F1 on BeaverTails, its performance fluctuates across datasets, whereas THINKGUARD maintains consistently strong results. These findings reinforce that structured critiques enable models to improve both classification accuracy and inter-"}, {"title": "Conclusion", "content": "We present THINKGUARD, a critique-augmented guardrail model that integrates structured critiques to improve both safety classification and interpretability. THINKGUARD achieves the highest average F1 and AUPRC across multiple safety benchmarks, outperforming existing baselines. It improves F1 by 14.1% on Toxic Chat and 6.1% on WildGuardMix compared to LLaMA Guard 3, demonstrating enhanced robustness against adversarial and ambiguous cases. Additionally, it generalizes better than label-only fine-tuned models, particularly on datasets with diverse harm categories. Future work will explore extending critique-based"}, {"title": "Limitations", "content": "While THINKGUARD demonstrates strong performance in safety classification, several limitations remain. First, its effectiveness is contingent on the quality of critique-augmented training data. If critiques are incomplete or misaligned with safety guidelines, the model may inherit biases or inconsistencies. Second, although THINKGUARD improves interpretability, generating structured critiques introduces additional computational overhead compared to simpler classification models. This trade-off between reasoning depth and efficiency requires further optimization. Future work will address these issues by refining critique generation, exploring parameter-efficient adaptations, and extending the approach to broader safety domains."}, {"title": "Ethics Statement", "content": "Advancements in AI safety raise ethical concerns, especially in designing and deploying guardrail models like THINKGUARD. Its effectiveness relies on ethically sourced training data, requiring transparency in collection and annotation to ensure fairness and minimize bias. Additionally, THINKGUARD aims to mitigate harmful content but risks over-restricting benign inputs due to biases in safety policies. Balancing enforcement with freedom of expression remains a challenge, requiring consideration of cultural and contextual differences to ensure fair moderation across demographic groups. Finally, any AI-based moderation system, including THINKGUARD, must be deployed with appropriate oversight. Over-reliance on automated guardrails without human review can lead to unintended consequences, including unfair enforcement or misalignment with societal norms. Future research should continue addressing fairness, accountability, and transparency to ensure the responsible use of safety-enhancing AI technologies."}]}