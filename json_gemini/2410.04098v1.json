{"title": "The OCON model: an old but green solution for distributable supervised classification for acoustic monitoring in smart cities", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi"], "abstract": "This paper explores a structured application of the One-Class approach and the One-Class-One-Network model for supervised classification tasks, focusing on vowel phonemes classification and speakers recognition for the Automatic Speech Recognition (ASR) domain. For our case-study, the ASR model runs on a proprietary sensing and lightning system, exploited to monitor acoustic and air pollution on urban streets. We formalize combinations of pseudo-Neural Architecture Search and Hyper-Parameters Tuning experiments, using an informed grid-search methodology, to achieve classification accuracy comparable to nowadays most complex architectures, delving into the speaker recognition and energy efficiency aspects. Despite its simplicity, our model proposal has a very good chance to generalize the language and speaker genders context for widespread applica-bility in computational constrained contexts, proved by relevant statistical and performance metrics. Our experiments code is openly accessible on our GitHub.", "sections": [{"title": "I. INTRODUCTION", "content": "Acoustic sensing for the safety, security, and monitoring of urban and non-urban environments is becoming increasingly important. This trend is driven by the widespread adoption of the smart cities vision by Western municipalities [1], [2] and the need to protect ecosystems in wild areas [3]\u2013[5]. Solutions proposed in the literature over the years addressed two critical topics: communication networks and Machine Learning (ML) [6], [7]. Collected data need to be transmitted and processed, with the sequence of these operations depending on many factors (not addressed in this work). Recent technological advancements in both fields are worth exploring and new communication networks like 5G and 6G enable previously impossible applications by introducing new concepts such as network slicing, network functions virtualization, orchestration, multi-access edge computing, Open Radio Access Networks (O-RAN), and software-defined networking [8], [9]. Additionally, ML and Neural Networks (NNs) are extensively spreading across various fields, being highly desirable for acoustic monitoring due to their proved accuracy levels [10]\u2013[13].\nIn a smart city scenario (Fig.1), key areas of interest are Sound Event Detection (SED) and Audio Tagging (AT), par-ticularly for identifying the cause of exceeding a safe acoustic threshold. This operation can be performed either at the sensor location or remotely, especially when detailed information about the type of sound needs to be transmitted. This scenario fits perfectly within the field of the Internet of Sounds (IoS) [14], which involves the interconnected network of devices capable of capturing, processing, and transmitting audio data with constrained computational resources [15]. When adopting ML solutions for this purpose, care must be taken due to the limited computational capabilities of local sensing hardwares [16], [17]. One possible solution is to send all data to a remote cloud, another is to reduce the computational complexity of the NN to be used locally and improve its features abstraction capability, thus avoiding privacy issues too.\nThis paper evaluates a streamlined combination of Neu-ral Architecture Search (NAS) and Hyper-Parameters Tuning (HPs-T) for designing abstraction/classification NNs models. We propose a modular \u201cOne-Class One-Network\u201d (OCON)"}, {"title": "II. METHODOLOGIES", "content": "We started by collecting a reliable audio dataset, including multiple phonetic hues and gender diversity among speakers. We limited our linguistic research to the General American En-glish case-study, as defined by the International Phonetic As-sociation (IPA). We decided to focus only on well-established pre-processed datasets (Sec. II-A), designed by means of pre-arranged phraseological segments or specific words, like the /hVd/ containers (where vowels are placed between an \u201ch\u201d and a \"d\"). These segments were recorded, analyzed (formant analysis), and pre-processed to extract meaningful features (formant frequencies) suitable for our NNs model, obtained following this steps:\n(1) segment speech signals into semantic frames, either manually or automatically, following a pre-defined semantic grid (words/phonemes, and silences);\n(2) use Linear Predictive Coding/Analysis (LPC/LPA) to an-alyze isolated segments, obtaining a smoothed time-frequency aggregated spectral estimate (per audio frame);\n(3) extract the top N spectral peaks using any peak estima-tion algorithm, ensuring to track frame-by-frame continuities (contouring).\nAdditional post-processing is added to refine retrieved for-mant frequency tracks and create a suitable (features) vector for the input layer of our NNs model. These pre-processing proves to be crucial in allowing the networks to learn related abstract representations effectively, thereby optimizing recog-nition accuracy."}, {"title": "A. Datasets Review", "content": "As already discussed in [19], the choice of the dataset has been done basing on various reasoning such as phonetic complexity and gender balance."}, {"title": "B. Features Pre-Processing & Classification", "content": "The filename structure of the HGCW dataset encodes essential phonetic and speaker features, which are crucial for a preliminary statistical analysis.\nPre-processing solutions applied have already been dis-cussed in [19]. We remark that the presence of null features (in some samples) caused by authors algorithm failures, re-quired further samples filtering, leading to additional under-representation of certain phoneme and speaker classes, to maintain balance and thus learning consistency. Fundamental frequency tracks (F0) were retrieved by means of a 2-way auto-correlation/zero-crossing pitch tracker, followed by a halving/doubling result evaluation sub-routine [29], while for-mant frequencies were estimated using LPA and peak retrieval with parabolic interpolation [30]. The resulting frequency trajectories were additionally refined with an interactive audio spectral editor, which was used for manual examination and interpolation of discontinuities."}, {"title": "III. PRACTICAL IMPLEMENTATION", "content": "In order to achieve both phonetic and speakers gender classification, we propose the exploitation of a specific OCON proposal, which models multi-output classification tasks us-ing multiple independent exact-copies of the same optimized Multi-Layer Perceptron (MLP) reference architecture.\nThese configurations are derived through simplified and informed NAS experiments (pseudo-NAS) combined with HPs-T: in DL research, HPs-tuning involves optimizing ar-chitectural and learning parameters (such as layers, nodes, backpropagation optimizers, learning rate etc.) to minimize the network cost function, between the predicted result (class) and the provided ground-truth (label), in supervised learning contexts."}, {"title": "A. Architecture & Model", "content": "MLPs, also referred to as Feed-forward NNs or fully connected (FC) layers, are essentially stacks of Perceptrons (neurons) arranged in vertical layers (shallow NNs), whose function is:\n$Y_n = \\varphi(x, w_k) = \\varphi(x^T w_k) = \\varphi(\\sum_{k=0}^{K} x_n w_k)$\nwhere xn are the input features, $w_k$ a set of scaling coefficients (weights) and $\\varphi(.)$ a non-linear ReLU function (activation) [41]:\n$\\varphi(x) = max(0, x) = \\begin{cases} x \\text{ if } x > 0, \\\\ 0 \\text{ otherwise} \\end{cases}$\nIntroduced in the '90s, the One-Class-One-Network (OCON) model [42] served as a solution for NNs paral-lel distributed processing, aiming to overcome limitations of architectures that required full re-training when altering their dataset classes. Today the OCON resembles a simplified form of architecture ensembling 4, where multiple complex networks are combined through other blocks or algorithms, to enhance the overall model accuracy. In the Anomaly Detection and Computer Vision fields [43]\u2013[45], the One-Class approach consists of distributing a multi-output classification across a bank of independent sub-networks, each functioning as a context-specific binary classifier. In our study, we divided a 12-phoneme and 3-genders classification tasks respectively into a bank of 12 and 3 independent and distributable classifiers, with identical architectural topology, aiming for an optimal average architecture estimation.\nIf a discrete output label is needed, a context-specific output algorithm must be devised. While no literature references were found regarding OCON-specific output algorithms, figures in [42] suggest the involvement of a MaxNet sub-network [46]."}, {"title": "B. Pseudo-NAS & HPS-T search", "content": "The term pseudo-NAS, as discussed in Sec.III-A and in [19], refers to the a priori constraint applied to the architecture topology (MLP). Our model evaluation will determine the optimal number of layers, and nodes per layer required to effectively address both phoneme and speakers gender recog-nition.\nConversely, grid-based HPs search is a statistical method where all possible combinations of NNs HPs are independently sampled and evaluated through straightforward learning cy-cles. While theoretically effective, it can be a time-consuming solution due to the exponential increase in computational requirements (for narrowing resolutions): typically, all possible combinations must be tested before selecting the optimal one.\nWe achieved a good trade-off by establishing independent resolutions for each HP beforehand, employing an informed iterative approximation, summarized as follows:\n(1) define a specific subset of HPs (not necessarily all at once, potentially fixing others);\n(2) sample each HP with an arbitrary resolution;\n(3) test each combination of HPs and evaluate resulting temporary best estimates. These can either serve as inheritable optimal estimates for subsequent heuristic stages or guide parameter resolution sampling towards local good estimates, in search of better sets;\n(4) repeat steps (2) to (3) as much as needed, to refine and improve the model configuration.\nAcknowledging that this simplified approach roughly ap-proximates theoretical grid-search, leading to potential mis-leading local minima in model costs, our goal remains to identify an average One-class topology in a computationally feasible manner."}, {"title": "IV. MODEL TRAINING & RESULTS DISCUSSION", "content": "A parallelized set of independently trainable One-Class ar-chitectures was implemented and trained using CPU runtimes (on Google Colab) to efficiently measure isolated training cycle performances and resource consumptions. We stress that the OCON architecture learning relies on the backpropagation loop of each MLP. During inference, it involves extracting sample features, computing 12 parallel one-hot encodings, and performing an ArgMax search to determine the maxi-mum value (predicted label) within the 12-logit probabilities vector. Following this, we conducted phoneme recognition experiments to evaluate the efficiency of each dataset sub-structure (Sec.II-B). An Early-Stopping training strategy [52] was adopted, incorporating a two-variable escape condition: a minimum loss threshold (averaging among the last 50 training samples' loss) and a minimum test accuracy threshold based on the last batch results. These variables were further em-pirically assessed to ensure practical convergence of training cycles, with each cycle not exceeding a maximum amount of 25-30 minutes. While the learning phases may not be fully optimized, they were deemed satisfactory for the purposes of our study."}, {"title": "A. Phonemes recognition", "content": "In [19] we evaluated the OCON model using the steady-state (SS) dataset variant (Table VII): training revealed that several loss functions and training accuracy curves visibly plateaued, punctuated by periodic spikes indicating instances of consistent-learning batch re-shuffling (Fig.10). Interestingly, the er and iy phoneme classes exhibited substantial repre-sentation, showing almost no changes (in curve trends) post-encoding or re-shuffling (Table VII).\nOverall accuracies were computed using a binary threshold of 0.5 across the entire dataset classes. While certain MLPS effectively segregated probabilities, notable errors persisted between phonemically (aural) similar classes, such as ae and eh, and er and ei.\nHidden dataset biases, such as similarities in formantic disposition between children and women utterances, were re-examined and filtering out these biases led to slight im-provements in class boundaries separation, despite increasing training loops duration. Attempts to enhance speakers gender boundaries by re-introducing F0s data, proved to be unsuc-cessful (AVG acc.: 88.80%, OCON acc.: 74%).\nThe most effective feature set consisted of 4 temporal tracks of 3 formant ratios (Table VIII), which significantly boosted accuracy, reduced training times, and mitigated side effects of Early-Stopping, approaching the accuracy goal referenced in [53] of 90% (Table XI)."}, {"title": "B. Speaker recognition", "content": "We aim to determine the minimum amount of formant features required for identifying speakers' gender with our best model. The overall architecture was simplified to 3x One-Classes (Table IX): men, women and children, with normalized FOs reintroduced in the input set, to improve classes separabil-ity. Class-dependent Early-stopping criteria were defined due to the significant amount of adjustments required for proper training convergence: 0.36, 0.08, 0.45 loss thresholds, 80%, 97%, 80% accuracy thresholds (respectively for children, male and women).\nWomen and children classes faced challenges in loss mini-mization while the men MLP converged rapidly to low error rates (almost 100% of accuracy). These results suggest better class representation for men and confirmed known difficulties in aural partitioning between children and certain adult female voices (aural similarities). Evaluation conducted upon the entire dataset's inference revealed lower False Positives (FPs) for the male class and higher FP rates for children and women inferences.\nThe OCON model achieved a speaker genders recognition accuracy between 80% to 85%, suggesting potential increasing reliability according to higher time-tracks number (more than 3x formant ratios, per speaker). To finalize the statistical overview [54], [55] we provide related confusion matrices (Tables X), Receiver Operating Characteristics (ROC) curves analysis, Area-Under-the-Curve (AUC) computations and De-tection Error Tradeoffs (DET) evaluations [55]\u2013[57]: see re-lated notebooks in our GitHub repository."}, {"title": "C. Energy efficiency", "content": "We focused on experimental sustainability grabbing insights from the Green-AI field [58], [59]. Using CodeCarbon, a custom Python-API for Intel-RAPL and Nvidia-smi libraries, we tracked CPU, disk, and RAM usage during model training. Energy metrics and estimated CO2 emissions were recorded in a .csv file and analyzed through a web-based applet developed by the GESSI research group (Universitat Polit\u00e8cnica de Catalunya), as part of the GAISSA research project. This analysis provided efficiency and accuracy labels (Fig.11) via HuggingFace database comparison, indicating strong sustainability of our pseudo-NAS approach, without compromising resulting accuracies.\nThe entire speakers task training cycle required approxi-mately 36 minutes, with an average consumption of 42.5W for CPU and 4.27W for RAM. Carbon dioxide emissions (CO2eq) were estimated as the product between grams of CO2 emitted per KW-hour of electricity (0.025KW/h for CPU, 0.003KW/h for RAM) and the energy consumed by the computational infrastructure: resulting in 0.008Kg, with an emission rate of 3.75 \u00d7 10-6Kg/s."}, {"title": "V. CONCLUSIONS AND FUTURE WORKS", "content": "We are aware that a single Perceptron can easily predict speech signal samples approximating LPA results [60]. Our model proposal can therefore be seen as an ad-hoc integration head for a complex Perceptron-based formant neural frame-work. Despite the active research on formant estimation lever-aging convolutional and recurrent layers (backbone stages) [61], [62], we believe that our approach, employing pseudo-NAS/HP-T techniques entirely scripted and executed on Colab free-tier notebooks, can be broadly re-applied to effectively evaluate the efficiency of newer SoA CNNs building blocks (lihtweight-CNNs) in terms of parameters reduction and com-putational complexity.\nOur foundational research model demonstrates high dis-tributability, with each classifier independently re-trainable and sufficiently lightweight for constrained computational contexts (or hardwares), making it suitable for integration into pre-existent complex architectures and on-board sensory con-strained hardware. Optimization techniques such as parameters pruning and quantization could further improve its mem-ory consumption at inference time. Additionally, its modular structure allows for easy adaptation to different language and speakers grouping contexts (being more LGBTQIA+ friendly, despite the use of simple binary-labeled dataset).\nA pre-compiled TorchScript version of our classification stages, successfully run on testbed E2E sensing and processing devices (kindly provided by the developer company).\nWe try to challenge the notion that larger (and hetero-geneous) datasets or complex (Transformer-based) models inherently yield better accuracies: asserting instead that our approach offers good generalizability and adaptability, despite known limitations in training sample size. While we encoun-tered difficulty in finding extensive pre-processed datasets, we will re-validate our findings by expanding our dataset sources, potentially validating TI-MIT, UCLAPhoneticsSet and AudioSet.\nOur proposal for linear features processing confirms that altering speech signal spectra in non-linear auditory-based ways it's not always optimal for descriptive speech modeling. However, we intend to reconsider solutions proposed in the existing literature.\nRegarding sustainability, we're pleased to find that the CO2e emissions for fully retraining our model are just over half the emissions from the entire lifecycle of a single cigarette.\nFuture research could explore enhancing label (class) se-lection by applying training assurance scaling coefficients to output One-Class probabilities, aiming to increase model reliability. This approach involves analyzing epochs during which the classifier maintains the loss below specified Early-stopping thresholds. Further refinement of output probabilities could utilize derivatives of the loss curve, particularly useful in cases of too rapid training error minimization."}]}