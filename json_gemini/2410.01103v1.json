{"title": "APPROXIMATELY ALIGNED DECODING", "authors": ["Daniel Melcer", "Sujan Gonugondla", "Pramuditha Perera", "Haifeng Qian", "Wen-Hao Chiang", "Yanjun Wang", "Nihal Jain", "Pranav Garg", "Xiaofei Ma", "Anoop Deoras"], "abstract": "It is common to reject undesired outputs of Large Language Models (LLMs);\nhowever, current methods to do so require an excessive amount of computation,\nor severely distort the distribution of outputs. We present a method to balance the\ndistortion of the output distribution with computational efficiency, allowing for the\ngeneration of long sequences of text with difficult-to-satisfy constraints, with less\namplification of low probability outputs compared to existing methods. We show\nthrough a series of experiments that the task-specific performance of our method\nis comparable to methods that do not distort the output distribution, while being\nmuch more computationally efficient.", "sections": [{"title": "INTRODUCTION", "content": "Language models sometimes generate undesirable outputs, such as syntactically-incorrect code, hal-\nlucinated PII, or profanity. These conditions, which we collectively refer to as errors for the remain-\nder of the paper, can be detected with incremental parsers, regular expression matching, or even\nsimple substring searches. However, once detection occurs, there are several competing methods for\nmitigating errors in the output.\nOne set of methods, constrained generation (Beurer-Kellner et al., 2024; Geng et al., 2024; Melcer\net al., 2024), avoids errors by disabling the generation of any token that immediately leads to such\nan error. While this method is effective, it can lead to the amplification of low-probability outputs.\nAnother class of methods avoids errors without any amplification of low-probability outputs, at the\ncost of additional computation. Rejection sampling is the simplest such method; i.e. if the output\ncontains an error, simply generate another sample until the output is acceptable. Adaptive Sam-\npling with Approximate Expected Futures (ASAP) (Park et al., 2024) provides a performance im-\nprovement over rejection sampling while maintaining the output distribution by effectively sampling\nwithout replacement, but there are still many situations in which it may converge too slowly."}, {"title": "PRELIMINARIES", "content": "We present a method that allows for a useful midpoint in the tradeoff between computational effi-\nciency and maintenance of the output distribution. In our experiments, we show that our method\nobtains task-specific performance on par with ASAp, while converging significantly faster when the\nconstraints are difficult to satisfy.\nWe first describe autoregressive language models and their properties. We then discuss speculative\ndecoding, a method closely related to the algorithm that we will introduce."}, {"title": "AUTOREGRESSIVE LANGUAGE MODELS", "content": "Algorithm 1 describes repeated sampling from a language model. This process results in an implicit\nprobability distribution over V*: $P(x_{1...n}) = \\prod_{i \\in [1...n]} P(x_i|x_{1...i-1})$.\nNote that there are several other methods for token selection; i.e. greedy selection, beam search, etc.\nWhile we focus on sampling, the techniques we present may also be applicable to other methods."}, {"title": "SPECULATIVE DECODING", "content": "Language models with many parameters (\u201cLarge Language Models\", or LLMs) are useful, but can\nrequire considerable computational resources to evaluate. Moreover, the autoregressive sampling\nprocess is inherently sequential, meaning that additional parallel computation resources cannot be\nfully utilized to decrease generation latency, especially for longer sequences.\nSpeculative decoding (Leviathan et al., 2023; Miao et al., 2024; Cai et al., 2024; Li et al., 2024) is\none popular approach to decrease latency. This method assumes the existence of a small speculative\nmodel (SSM) S that approximates the output of the LLM, using fewer computational resources.\nGiven input tokens $x_{1...n}$, the SSM is autoregressively sampled for m tokens, resulting in tokens\n$x_{n+1...m}$. Then, the LLM P is used to compute $P(x_{i+1}|x_{1...i})$ for $i \\in [n ... m]$; this computation is\nparallelizable. Finally, Algorithm 2 is used to select a prefix $x_{1...k}$ for $k \\in [n, m]$ of tokens to accept;\nall later tokens are discarded. Additionally, because the probabilities $P(\\cdot|x_{1...k})$ have already been\ncomputed, Algorithm 2 samples a new token $x_{k+1}$. This process maintains the property that the\ndistribution of sequences produced by this process matches the sequence distribution of P."}, {"title": "PROBLEM STATEMENT AND EXISTING APPROACHES", "content": "Error Set B C V* is the set of strings containing errors.\nWe make the mild assumption that if string $x_{1...n} \\in B$, then all strings with $x_{1...n}$ as a prefix are\nalso members of B; i.e. adding additional text does not negate an error. Note that this assumption\nrequires careful design of the error set; for example, when profane words are substrings of benign\nwords (Francis, 2020), or un-parseable code can be made valid by adding additional text. B will\noften be infinite size; therefore, most sampling methods treat it as a black-box indicator function.\nWe define the probability distribution obtained by sampling P, except for any elements of B:\n$P_B(w) =\\begin{cases}0 & w \\in B \\\\ \\frac{P(w)}{\\sum_{w \\notin B} P(w)} & w \\notin B \\end{cases}$\nProblem 1. Given an autoregressive language model P over alphabet V, and error set B C V*,\nprovide a method to sample from $P_B$.\nRejection sampling is the most straightforward method for sampling from $P_B$; however, it may\nrequire a large number of evaluations as $\\sum_{w\\in B} P(w)$ approaches 1. For example, consider a domain\nwhere each token has, approximately, some non-zero probability p of being an error-we assume\nthat the language model has a somewhat consistent error rate per token. If d tokens are generated,\nan output has approximately a $(1 \u2013 p)^d$ probability of being error-free; thus requiring on average\n$\\frac{1}{(1-p)^d}$ generations. We consider such domains-domains where the probability of generating an\nerror approaches 1 for longer generations-to have dense error sets."}, {"title": "EXISTING APPROACH: CONSTRAINED GENERATION", "content": "Constrained generation attempts to solve the error-free generation problem by using a greedy algo-\nrithm: during token selection, the algorithm always avoids selecting any tokens that immediately\nlead to an error. Note that this algorithm assumes that if string $x_{1...n} \\notin B$, then there exists at least\none available token $x_{n+1} \\in V$ such that $x_{1...n+1} \\notin B$; however, this assumption may be weakened\nif backtracking is allowed, in cases where every token leads to an immediate error.\nThe constrained generation algorithm has the effect of sampling from the following probability\ndistribution for each token:\n$C_B(x_i|x_{1...i-1}) = NORMALIZE \\begin{cases}0 & x_{1...i} \\in B \\\\ P(x_i|x_{1...i-1}) & x_{1...i} \\notin B \\end{cases}$\nRepeated sampling of this distribution leads to some troubling properties."}, {"title": "EXISTING APPROACH: SAMPLING WITHOUT REPLACEMENT", "content": "As in Section 2.1, a repeated sampling process results in a derived distribution $C_B(x_{1...n}) =\\prod_{i\\in [1...n]} C_B(x_i|x_{1...i-1})$. It is often the case that for sequence $x_{1...n}$, $C_B(x_{1...n}) > P_B(x_{1...n})$;\ni.e. low-probability samples are amplified by the constrained generation process.\nThe fundamental issue is that the constrained generation algorithm commits to a given prefix, even\nif the most probable sequences beginning with that prefix are errors. Note that this distortion is even worse in low-entropy scenarios; if\n$P(B|x_{1...n}, A)$ were lowered to 0.0001, it would still be the case that $C_B(AB|x_{1...n}) = 0.5$. This\namplification effect compounds exponentially for longer sequences.\nAdaptive Sampling with Approximate Expected Futures (ASAp) (Park et al., 2024) is a technique to\nsample exactly from the distribution of $P_B$. ASAp begins similarly to rejection sampling, but it iter-\natively builds set $B \\subset B$ containing all encountered samples that have been rejected so far. Because\nB is finite, the conditional probabilities $P_B(x_i|x_{1...i-1})$ can be efficiently calculated, allowing for\nthe algorithm to sample from $P_B$ exactly. If the sampled sequence is a member of B, it is added to\nB, and the sampling process repeats.\nIn the limit of repeated samples, B will approach B, and therefore, $P_B$ will approach $P_B$. Impor-\ntantly, if $x \\sim P_B$ is sampled such that $x \\notin B$, this sample may be accepted, even though $B \\ne B$.\nThis procedure is equivalent to sampling without replacement, adapted to autoregressive genera-\ntion. While ASAp succeeds in cases where there are only a small number of errors that comprise\nthe majority of the probability mass, its generation speed suffers when there are a large number of\nerrors each error must be discovered before it is added to B. In dense probability sets, its perfor-\nmance characteristics are similar to rejection sampling, as there are an exponential number of error\nsequences that must be discovered as generation length increases."}, {"title": "EXISTING APPROACHES: POSTERIOR ESTIMATION", "content": "We note two additional methods that, although they use very different formalizations and implemen-\ntations from each other, rely on a similar core idea to approximately sample from $P_B$. In both cases,\nfor any given prefix $x_{1...n}$, these methods create an estimator of $\\mathbb{E}_{x_{n+1...m}\\in\\Sigma^*} P(x_{n+1...m}|x_{1...n}) \\times\n1_{x_{1...m}\\in B}$; i.e. the likelihood of an error in all sequences that begin a specific prefix, weighted by\nthe probability of generating each sequence. This is used to estimate $P_B$."}, {"title": "METHOD", "content": "As our experiments show, Approximately Aligned Decoding is an effective method to generate\nsequences under dense language model constraints.\nOne observation we made while implementing our experiments is that constrained generation,\nASAp, and AprAD are specializations of the same general algorithm, differing only in their back-\ntracking behavior upon the discovery of an error: ASAp always backtracks to the beginning of\nthe sequence, constrained generation only backtracks to immediately before the error, and AprAD\nprobabilistically backtracks, with the most likely targets being recent high-entropy tokens. Section\nC (Appendix) provides additional details on this generalization.\nThis hints that there may additional points in the tradeoff between generation speed and perfect\nreplication of the ideal output distribution. For example, it may be possible to introduce an parameter\nto the speculative sampling subroutine that adjusts how likely the algorithm is to accept a given\ntoken. As this probability is pushed towards zero, fewer tokens are accepted, and the resulting\nalgorithm approaches the behavior of ASAp. In contrast, if the probability is pushed towards one,\nthe algorithm doesn't backtrack as far, and the resulting behavior approaches constrained decoding.\nThe optimal point in this tradeoff will depend on the specific task and error set, and is an area\nfor future research. As defined, however, AprAD occupies a useful position on this spectrum, is\nstraightforward to implement, and exhibits positive results on a variety of tasks."}, {"title": "PREVIOUS ITERATIONS OF ASAP ARE (ALMOST) SMALL SPECULATIVE MODELS", "content": "For some iteration of ASAp, with B as the set of observed errors so far, let $x = (x_1,...,x_n)$ be\na trace drawn from $P_B$, where it is discovered that $x \\in B$. We observe that $P_B$ and $P_{B\\cup{x}}$ are\nalmost always near-identical probability distributions, with $P_{B\\cup{x}}$ generally as a \u201cmore accurate\u201d\ndistribution because it incorporates an additional error sample.\nOur method reduces computation by using the sample $x \\sim P_B$ to approximate a sample $x' \\sim\n P_{B\\cup{x}}$, in a similar manner to how speculative decoding uses a sample from a SSM to approximate\na sample from a LLM. By evaluating SPECSAMPLE($x$, $P_B$, $P_{B\\cup{x}}$), our method obtains a prefix\nof $x$ that can be used as a starting point for sampling again. Because the distributions of $P_B$ and\n$P_{B\\cup{x}}$ are so close to each other, this prefix is usually most of the length of $x$. In contrast, ASAp\nwould involve backtracking to the beginning of the generation. This process is given as Algorithm\n4; we refer to it as Approximately Aligned Decoding, or AprAD.\nHowever, AprAD does not perfectly maintain the output distribution: Algorithm 4 amplifies some\nsequence probabilities because it only invokes SPECSAMPLE after discovering an error. To main-\ntain the output distribution, SPECSAMPLE should always be invoked for strings $x_{1...n}$ where\n$\\exists i \\in [1...n], P_B(x_i|x_{1...i-1}) < P(x_i|x_{1...i-1})$-but the algorithm has no way of checking if\nthis condition holds without iterating through every suffix, negating any performance benefit.\nEven though the AprAD does not perfectly maintain the output distribution, we show in the fol-\nlowing sections that it provides a very useful midpoint in the tradeoff of computational complexity\nversus task-specific performance and accuracy."}, {"title": "ANALYSIS", "content": "Let $A_B(x_{1...n})$ represent the probability of the AprAD method producing sequence $x_{1...n}$.\nFor $x_{1...n} \\in B, A_B(x_{1...n}) = 0$. For all other sequences, we provide evidence that AprAD more\nclosely follows the ideal distribution, compared to constrained generation. While the nature of the\niterative process makes it difficult to write a closed form description of the probability amplification\n$\\frac{A_B(x_{1...n})}{P_B(x_{1...n})}$, less probability amplification occurs with AprAD than with constrained generation when\nan error is detected, as an error's probability mass is \u201cdistributed\u201d over many sequences due to the"}, {"title": "EVALUATION", "content": "While Section 4.2 shows that our method performs well in a simulated domain, the following exper-\niments test the sampling methods on a series of more difficult, real-world tasks."}, {"title": "LIPOGRAMS (TEXT GENERATION WITH LETTER EXCLUSIONS)", "content": "It is common in poetry or creative writing exercises to write text without using a specific letter; a\nproduct of this exercise is called a lipogram. Lipograms where the excluded letter is a vowel tend to\nbe more difficult to create than with other letters. Large language models often fail at this task, and\nmore generally, most tasks dependent on individual letters rather than entire tokens.\nWe use Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) to generate lipograms with vowels as the ex-\ncluded letter. We prompt the LLM to perform one of five simple tasks (detailed in Appendix A).\nEach task is appended to instructions to avoid using one of the five vowels, resulting in 25 prompts.\nFor each prompt, we generate a completion with four sampling methods: unconstrained generation,\nconstrained generation, ASAp, and AprAD, for up to 200 tokens. If the process reaches 2000 model\ninvocations, generation is interrupted, and the last sequence before an error was detected is returned.\nWe then ask human raters to score each completion on quality, regardless of if the constraint was\nfollowed, on a scale of 1-5. If the forbidden letter is detected in the output, we then mark the\nconstraint as violated. Otherwise, we also ask the human raters to decide if the output violates\nthe intent of the constraint; i.e. by answering in a foreign language, adding unnecessary accents,\nswapping for Cyrillic lookalike characters, etc. Additional information about the rating process is\nprovided in Appendix A."}, {"title": "BIGCODEBENCH HALLUCINATION AVOIDANCE", "content": "We created a program to incrementally detect hallucinated API calls in partial Python programs,\nbased on the Pyright language server (Microsoft, 2019). Note that in order to satisfy the assump-\ntions in Section 3-that any string with an error as a prefix is itself an error-we constructed the\ndetector to act conservatively. False negatives are strongly preferred over false positives. For ex-\nample, even if name foo is never defined, the detector does not consider the incomplete program"}, {"title": "DISCUSSION, FUTURE WORK, & CONCLUSION", "content": "As our experiments show, Approximately Aligned Decoding is an effective method to generate\nsequences under dense language model constraints.\nOne observation we made while implementing our experiments is that constrained generation,\nASAp, and AprAD are specializations of the same general algorithm, differing only in their back-\ntracking behavior upon the discovery of an error: ASAp always backtracks to the beginning of\nthe sequence, constrained generation only backtracks to immediately before the error, and AprAD\nprobabilistically backtracks, with the most likely targets being recent high-entropy tokens. Section\nC (Appendix) provides additional details on this generalization.\nThis hints that there may additional points in the tradeoff between generation speed and perfect\nreplication of the ideal output distribution. For example, it may be possible to introduce an parameter\nto the speculative sampling subroutine that adjusts how likely the algorithm is to accept a given\ntoken. As this probability is pushed towards zero, fewer tokens are accepted, and the resulting\nalgorithm approaches the behavior of ASAp. In contrast, if the probability is pushed towards one,\nThis is an undercount of the number of hallucinated names: many outputs include hallucinations, but fail\nbefore reaching the hallucinated variable or method name, resulting in some other error. Some hallucinated\nmethod names lead to an AttributeError being raised. However, AttributeError is also raised for improper use\nof None, and similar issues that are not a result of hallucination, so we do not count it as a NameError."}, {"title": "GENERALIZATION OF ERROR-FREE DECODING", "content": "Constrained generation, ASAp, and AprAD may all be generalized by their backtracking behavior\nafter an error is discovered. Algorithm 5 shows this generalization."}]}