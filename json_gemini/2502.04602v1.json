{"title": "Extracting and Understanding the Superficial Knowledge in Alignment", "authors": ["Runjin Chen", "Gabriel Jacob Perin", "Xuxi Chen", "Xilun Chen", "Yan Han", "Nina S. T. Hirata", "Junyuan Hong", "Bhavya Kailkhura"], "abstract": "Alignment of large language models (LLMS) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance. Our code is available at https://github.com/ VITA-Group/Superficial_Alignment", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed significant advancements of large language models (LLMs) in various tasks. Although LLMs acquire extensive world knowledge, they meanwhile cast serious risks to the society. For example, LLMs are easily prompted to generate toxic, misleading, or harmful content. To ensure that the behaviors of LLMs adhere to human values and preferences, aligning LLMs to follow instructions based on human feedback is essential. To obtain satisfactory alignment, the tuning of an LLM usually demands a non-trivial amount of data and computation resources.\nDespite the considerable efforts invested in tuning LLMs, it has been surprisingly discovered that alignment might be attainable at lower costs or through simpler methods. For example, using only a few selected training examples can significantly improve alignment performance, approaching levels achieved through extensive tuning. Furthermore, Urial found that alignment often results in \"stylistic token shifts,\" and by employing in-context learning (ICL) with a few restyling examples, alignment can be improved without any further tuning. These findings give rise to the Superficial Alignment Hypothesis, which suggests that a model may acquire most of its knowledge and abilities during pre-training, while alignment primarily involves superficial adjustments.\nHowever, current methods support this hypothesis primarily through informal observations and indirect implications (i.e., because alignment can be achieved through superficial methods, it is hypothesized to be superficial). There remains a lack of rigorous, deep analysis regarding the extent to which alignment relies on superficial knowledge and whether alignment is purely superficial.\nTo fill this gap, we first formalize the previously vague concept of superficial knowledge. We define superficial knowledge as the type of knowledge that can be easily acquired through simple token restyling, without requiring modifications to the model's understanding of the underlying causal relationships between tokens and the process of knowledge extraction and compression. In contrast, deep knowledge pertains to the model's ability to capture token relationships and extract meaningful insights from the data.\nWe propose a method to extract and isolate superficial knowledge from the alignment process. To ensure the extracted knowledge remains superficial, we restrict our modifications to shallow, simple structures - specifically, the linear projection head of the LLM. This affects only the final token selection process, without altering the intermediate token merging or self-attention mechanisms. By doing so, we avoid disrupting the deep knowledge associated with internal token interactions. Furthermore, to ensure that no new knowledge is introduced into the model and to focus exclusively on analyzing the knowledge derived from alignment, we employ distillation to finalize the extraction process.\nWith the extracted and separated superficial knowledge, we can quantify the superficial portion of alignment by comparing the aligned model with a base model augmented only with superficial knowledge across benchmarks in math, safety, toxicity, and truthfulness. Our key findings are twofold:\n(1) Superficial knowledge constitutes a significant portion of the alignment, especially in safety and detoxification tasks. This knowledge primarily consists of stylistic patterns that help the model structure its responses. By leveraging superficial knowledge alone, we can completely eliminate safety and toxicity risks while achieving average performance improvements of 58% in math and 78% in truthfulness tasks. The gains from superficial knowledge surpass those from simpler methods like LIMA and ICL, as our approach more comprehensively covers the breadth of superficial knowledge.\n(2) However, alignment is not entirely superficial. A clear gap remains between superficial knowledge and fully aligned knowledge, particularly in knowledge-intensive tasks such as math and truthfulQA. As we demonstrate in section 2.3, this gap likely relates to the model's capacity for reasoning and contextual understanding, which goes beyond superficial patterns.\nIn addition, since our extracted superficial knowledge is stored in a simple and modular structure, we have also discovered several useful properties of superficial knowledge. We further demonstrate the Superficial Advantage (SA)\u2014the benefits of isolating superficial knowledge alone."}, {"title": "SA1: Weak-to-Strong Superficial Alignment.", "content": "Our experiments reveal that the extracted superficial knowledge is transferable across models. This transferability can be leveraged for offsite alignment of larger models\u2014superficial knowledge extracted from a smaller, weaker model can be applied to a larger, stronger model. This allows for plug-and-play alignment of the larger model without requiring extensive tuning."}, {"title": "SA2: Recoverable Superficial Safety.", "content": "Previous work has shown that safety mechanisms can be easily compromised, such as through slight fine-tuning on as few as 10 samples. However, with our extracted superficial knowledge, we can re-attach the lightweight structure encapsulating this knowledge to a de-aligned LLM and successfully recover 88% of the alignment effects without compromising MMLU accuracy."}, {"title": "2 Understanding the Superficial Knowledge in Alignment", "content": ""}, {"title": "2.1 Notation", "content": "In this paper, we denote the backbone (transformer layers) of the aligned model as $f_a(\u00b7)$ and its final linear projection matrix as $W_a$. Conversely, $f_b(\u00b7)$ and $W_b$ represent the backbone and final linear layer of the unaligned base model. Throughout the paper, we consistently use the subscript a to refer to the aligned model and b for the base model.\nAlignment token distribution shifts: Given the same input, the top next token predicted by the base model is referred to as the source token, while the token predicted by the aligned model is termed the target token. A token at any position where the base model and aligned model make different predictions is called a shift token."}, {"title": "2.2 Extracting Superficial Knowledge", "content": "To better understand the knowledge introduced through alignment, we aim to extract and isolate what we term superficial knowledge. This refers to knowledge that contributes to simple token restyling without influencing the intermediate transformer layers' understanding of token relationships.\nWe represent the input at time step t as $x_t$, which includes both the instruction and the output from previous steps. The LLM encodes these into a vector $h_t = f(x_t)$, produced by the final transformer layer. These hidden states, $h_t$, encapsulate complex interactions across tokens, representing the model's understanding and reasoning over the entire context. The model then predicts the next token probability using a linear projection head W, as shown:\n\n$l_t = Wh_t = Wf(x_t)$\n\nOur approach adjusts the base model's final linear layer $W_b$ by adding a learnable residual adjustment, $\\Delta W_b$, that approximate and mimics the aligned model's token shift and restyling process. By keeping the LLM's transformer layer $f_b(\u00b7)$ fixed, this method preserves the deeper knowledge unchanged within the model. Since we aim to extract knowledge from the aligned model without introducing new information, we avoid standard fine-tuning techniques for learning $\\Delta W_b$. Fine-tuning on external data could introduce new knowledge not originally present in the aligned model. Instead, we apply distillation to fine-tune the linear projection heads, using the aligned model's output as a supervisory signal. Specifically, we provide the same input, $x_t$, to both the base model with a learnable residual $\\Delta W_b$ and the aligned model, obtaining their respective logits $l_t = (W_b + \\Delta W_b) f_b(x_t)$ and $l_a = W_a f_a(x_t)$. We then minimize the divergence between the two logits:\n\n$L_t = KL(P_a || P_t) = \\sum P_a log \\frac{P_a}{P_t}$\n\nwhere $P_a = SoftMax(l_a)$ and $P_t = SoftMax(l_t)$. The optimization objective is to minimize the sum of these losses across all tokens, yielding the optimal $\\Delta W_b$:\n\n$\\Delta W_b = \\arg \\min_{\\Delta W_b} \\sum L_t$\n\nThe resulting $\\Delta W_b$ serves as an approximation of the superficial knowledge in the alignment process. By applying the optimized $\\Delta W_b$ to the base model, we effectively integrate only the superficial knowledge. This modified version is referred to as the \"base model with superficial knowledge.\""}, {"title": "2.3 Is Alignment Primarily Superficial?", "content": "We then try to address the question posed earlier: What proportion of alignment does superficial knowledge constitute, and is alignment entirely superficial?\nTo address this, we evaluate the base model, aligned model, and base model with only superficial knowledge on various downstream tasks to gauge the importance of superficial knowledge. We use four datasets, each curated to evaluate different aspects of alignment: 1. The GSM dataset, comprising mathematical tasks, is utilized to analyze reasoning ability. 2. The Toxigen dataset, which includes both neutral and toxic questions, focuses on evaluating the model's ability to avoid generating toxic content. 3. The Advbench dataset, featuring harmful questions, is used to assess safety. 4. The TruthfulQA dataset assesses the model's capability in providing factual responses. In our experiments, we use both LLaMA2 as the base models, with LLaMA2-chat serving as the aligned models, the results are presented in Table 1. Additional results for Mistral and Qwen are included in Appendix C. For more details about the training process and experiment setup, please refer to Appendix B."}, {"title": "2.3.1 Superficial knowledge indeed takes a large proportion of the alignment, particularly in the front part of the response.", "content": "The results in Table1 show that simply adding superficial knowledge to the model enables achieving most performance gains achieved through alignment. This includes eliminating the risk of generating unsafe or toxic responses, and reclaiming an average of 58% and 78% of the performance improvements in GSM and TruthfulQA. These gains surpass those achieved by other simple methods, such as LIMA and Urial, as our approach more thoroughly captures the scope of superficial knowledge. Additionally, we visualized the relationship between position and KL divergence of next token probabilities of the base model vs. aligned model and base model + superficial knowledge vs. aligned model across 100 test samples, shown in Figure 2. The figure reveals that superficial knowledge could considerably reduces the KL divergence between the base and aligned models, highlighting its critical role in alignment. Moreover, we found the initial positions (e.g., the first 10 tokens) in each response may contain the most alignment knowledge, as indicated by significantly different distributions between the base and aligned models at these positions. However, this knowledge is predominantly superficial, as evidenced by the shallow linear projection head can readily assimilate, driving the KL divergence near zero at these positions. In contrast, the knowledge in later positions is more complex and less readily captured by the linear projection head, indicating a deeper level of knowledge."}, {"title": "2.3.2 Alignment is not merely superficial knowledge", "content": "Although superficial knowledge contributes significantly to model alignment, our results suggest that alignment is not solely comprised of superficial elements. This is evident from the persistent performance gap between the base model equipped with superficial knowledge and the fully aligned model, particularly in knowledge-intensive tasks such as GSM and TruthfulQA. Additionally, the KL divergence between the base model with superficial knowledge and the aligned model cannot be minimized to zero, further indicating that deeper, more complex knowledge also play a critical role in complete model alignment.\nTo better illustrate the distinction between superficial and deeper knowledge, we analyze response examples to observe the changes that occur during inference when only superficial knowledge is applied, and what cannot be captured by superficial knowledge alone. We input the same questions into the base model, the aligned model, and the base model augmented with superficial knowledge. One example from the GSM test set is presented in Table 2. In the responses shown, tokens highlighted in red indicate token shifts, where the top token generated by the current model differs from that of the base model when given the same input at the current step. Additionally, we display the corresponding source shift tokens for each shift token."}, {"title": "2.3.3 Restyle Patterns in Extracted Superficial Knowledge.", "content": "As demonstrated in Table 2, incorporating superficial knowledge noticeably changes the model's response style. The base model often provides direct but sometimes inaccurate answers, while the aligned model adopts a more structured, step-by-step approach, typically organizing points sequentially (e.g., 1, 2, 3, 4). This structured restyling is what we define as superficial knowledge. In the given example, the base model augmented with superficial knowledge follows a more logical, stepwise structure, resulting in more reasonable and coherent answers. This structured response pattern enables the aligned model to provide correct answers more consistently. Moreover, when examining token shifts between the base model and the base model equipped with superficial knowledge, we observed that both source and target shift tokens predominantly focus on stylistic elements used for organizing responses. For example, '## \u2192 To' leads model to recall the target of the question. 'The \u2192 There(fore)' push model to summarize the findings. These shifts greatly help model to organize the response. Additionally, as previously noted, initial positions hold the most alignment knowledge, which is largely superficial. This is clearly demonstrated in the example where the phrase 'To find' significantly alters the answer style, marking a crucial contribution from alignment. More examples will be provided in Appendix F."}, {"title": "2.3.4 What is essential for alignment other than superficial knowledge? The ability to reason and integrate context may count.", "content": "As demonstrated earlier, superficial knowledge alone cannot cover all aligned knowledge, and there remains a performance gap between a base model equipped with superficial knowledge and an aligned model. This gap exists because the aligned model is superior in its ability to reason and integrate context compared to the base model, as shown in Table 2. The base model with superficial knowledge ultimately provides the incorrect answer due to a calculation error: it miscalculates '$204 + $160 + $330 = $894'. In contrast, the aligned model does not exhibit this error, as demonstrated by the token shift pair (8\u2192 6). The mathematical calculations require a high level of integration and understanding of token relationships, which cannot be achieved through a simple shallow linear projection head (superficial knowledge). This also underscores that alignment is more than merely superficial knowledge."}, {"title": "3 Using Superficial Knowledge for A Good Purpose", "content": "After gaining a basic understanding of superficial knowledge in alignment, we will highlight several benefits of extracting and isolating this knowledge."}, {"title": "3.1 Weak-to-Strong Superficial Alignment", "content": "Initially, since the essence of superficial knowledge lies in restyling, and this restyling pattern may be universal across models, we explore the possibility of transferring superficial knowledge between models. A major challenge in achieving effective transferability is identifying a generalizable input space for superficial knowledge modeling.\nAs described in Section2.2, we store the superficial knowledge of alignment within a linear weight, $\\Delta W_b$. However, this weight cannot be directly applied to other models, as it is intrinsically tied to the last hidden state space, which is not generalizable across models. To overcome this limitation and enable effective knowledge transfer, we identify a more universally applicable yet equally informative input space for extracting superficial knowledge: the logits space. Since models from the same family typically share the same vocabulary, regardless of model size, the logits space offers a consistent input structure. Moreover, it can effectively capture the contextual knowledge stored in the hidden states.\nHowever, in our experiments, we found that using the full output logits is not an optimal choice. Employing only the top-k logits as input (i.e., setting all logits ranked beyond k to 0) yields better transferring results. This can be attributed to two main reasons. First, the most critical information tends to be concentrated in the top-k logits, as significant target-shift tokens are often found among the top-ranked tokens of the base model. Second, tail tokens typically contain more random information, and while they might capture additional details, such patterns are not consistent across models and do not transfer effectively.\nTo substantiate these points, we computed two metrics within the top-k logits space and the full logits space. The first metric, shift token cover rate, measures the proportion of top-k tokens predicted by the base logits that encompass the target shift tokens (i.e., the top-1 token predicted by the aligned model). As k increases, the shift token cover rate correspondingly rises. The second metric, the transfer space similarity, evaluates the similarity of the top-k token logit spaces across models with different size. We collected 1,000 logit samples from both LLaMA2-7b and LLaMA2-13b using identical inputs, denoted as $L_{7b}$ and $L_{13b}$, respectively. We performed Singular Value Decomposition (SVD) on these samples: $L_{7b} = U_{7b}S_{7b}V_{7b}^T$ and $L_{13b} = U_{13b}S_{13b}V_{13b}^T$, where $V \\in R^{|V| \\times 1000}$ represents the base vectors for the logits space, and |V| is the vocabulary size. The similarity between $V_{7b}$ and $V_{13b}$ was calculated using the formula:\n\n$Similarity = \\frac{||V_{13b}^TV_{7b}||_F}{\\sqrt{||V_{13b}||_F||V_{7b}||_F}}$\n\nThis similarity assesses the subspace similarity between the top-k token logit spaces of LLaMA2-7b and LLaMA2-13b.\nIn Figure 3, we plot the relationship between shift token cover rate and transfer space similarity. As the value of k increases, we observe a decrease in transfer space similarity and a corresponding increase in shift token cover rate, indicating a potential trade-off between informativeness and transferability. An appropriate value for k may be selected based on this trade-off. Further details will be discussed in Appendix D.\nTo enhance transferability, we extract superficial knowledge from model alignment using a linear model, with the top-k logits as input. We approximate and model the token distribution shift using a linear transformation, $W_{trans}$, as follows:\n\n$l_a - l_t = W_{trans} \\cdot topk(l_t)$\n\nHere, $l_a$ and $l_t$ represent the logits output of the aligned model and the base model at step t. The function topk() sets all logits ranked beyond the k-th position to zero. We optimize the linear weight $W_{trans}$ using distillation techniques outlined in Section 2.2. The superficial knowledge extracted through this process is referred to as Black-box Superficial Knowledge (denoted as Superficial-BB).\nIn our experiments, we extracted Black-Box Superficial knowledge from LLaMA2-7b-Chat, and then applied it to both LLaMA2-7b and LLaMA2-13b. The evaluation results on downstream tasks are listed in Table 3."}, {"title": "3.2 Recoverable Superficial Safety", "content": "As noted by, safety in alignment is easily disrupted through additional fine-tuning, which can result in the generation of harmful or toxic responses. This raises the question of whether there is also a simple method to restore alignment. Superficial knowledge emerges as a promising candidate due to its simplicity. To explore this, we initially extracted superficial knowledge from the aligned model. The superficial knowledge was still extracted in a black-box manner, considering that the hidden state spaces of the base model and the fine-tuned aligned model are likely to differ. Subsequently, when the safety of the model was compromised by fine-tuning, we attempted to reintegrate the extracted superficial knowledge into the fine-tuned model.\nIn our experiments, we use LLaMA2-7b as the base model and LLaMA2-7b-chat as the aligned model to extract superficial knowledge. Following the setup from, we utilize their selected identity shift dataset to fine-tune the LLaMA2-7b-chat model, which represents the most effective benign fine-tuning attack described in their paper. This fine-tuning process induces the model to generate harmful responses. We evaluate the fine-tuned model using the advbench dataset. Additionally, in Appendix E, we also explore less aggressive fine-tuning tasks to provide a more comprehensive analysis.\nExperiment Results. The results are shown in Table 4. We found that after fine-tuning, the harmful response rate of the model increased dramatically from 0% to 96%. However, after restoring the superficial knowledge, most of the performance was regained, and the harmful rate dropped to 8%. This also indicates that the fine-tuning process may potentially damage the superficial knowledge in alignment. Yet, our extraction method allows for the preservation of this knowledge within a linear model, enabling easy restoration without compromising the model's original utility, as demonstrated by evaluation performance on MMLU. Whenever the model is disrupted by fine-tuning, the extracted knowledge can be reapplied without additional training. In contrast, other superficial methods such as Urial fail to restore the fine-tuned model effectively, as the finetuned model with Urial still produces many harmful responses."}, {"title": "4 Conclusion", "content": "In this paper, we propose a method to separate superficial knowledge from deep knowledge within alignment, enabling us to quantify the the superficial portion of alignment. Our analysis finds that superficial knowledge indeed constitutes a large proportion of alignment, though not entirely. Knowledge beyond the superficial, related to reasoning abilities and contextual integration, is also crucial to alignment. Additionally, our extracted superficial knowledge extends beyond mere analytical use, offering practical applications such as weak-to-strong superficial alignment and recovering compromised safety."}, {"title": "5 Social Impact and Limitation", "content": "Potential Social Impact. Our work offers critical insights into the superficial aspects of alignment, potentially guiding future methodologies for robust and secure alignment. The implications regarding the transferability and restorability of superficial knowledge present mitigation for potential risks associated with alignment. Consequently, we envision that improved alignment, rooted in our findings, could yield significant positive social impacts for the proper use of AI. However, we also acknowledge that the misuse of superficial knowledge could pose risks to alignment in the short term. Specifically, an overreliance on superficial knowledge may obscure or ignore deeper, underlying knowledge essential to true alignment. This can lead to AI systems that seem aligned on the surface but fail to account for complex or nuanced factors. Thus, we call for more efforts to be devoted to enhancing the alignment with non-superficial knowledge.\nLimitation. In this paper, we measure the portion of such knowledge in existing aligned LLMs and use examples to demonstrate what is superficial knowledge and what is beyond superficial.While the non-superficial part in alignment is not fully understood. The problem remains challenging as the rest of knowledge could be multi-faceted, and could be complicated with diverse sequential dependencies."}]}