{"title": "Captions Speak Louder than Images (CASLIE): Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data", "authors": ["Xinyi Ling", "Bo Peng", "Hanwen Du", "Zhihui Zhu", "Xia Ning"], "abstract": "Leveraging multimodal data to drive break-throughs in e-commerce applications through Multimodal Foundation Models (MFMs) is gaining increasing attention from the research community. However, there are significant challenges that hinder the optimal use of multimodal e-commerce data by foundation models: (1) the scarcity of large-scale, high-quality multimodal benchmark datasets; and (2) the lack of effective multimodal information integration methods. To address these challenges, in this paper, we introduce MMECInstruct, the first-ever, large-scale, and high-quality multimodal instruction dataset for e-commerce. We also develop CASLIE, a simple, lightweight, yet effective framework for integrating multimodal information for e-commerce. Leveraging MMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted as CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models substantially outperform 5 categories of advanced baseline models in the in-domain evaluation. Moreover, CASLIE models show strong generalizability to out-of-domain settings. MMECInstruct and CASLIE models are publicly accessible through https://ninglab.github.io/CASLIE/.", "sections": [{"title": "1 Introduction", "content": "Multimodal data, encompassing diverse modes and types of information such as text and images, is ubiquitous and essential for many real-world applications (Antol et al., 2015; Wang et al., 2023; Mu et al., 2024; Chen et al., 2021). This holds true for e-commerce, where the product and user information is inherently multimodal (e.g., products have images and textual descriptions). Effectively harnessing multimodal data for e-commerce exhibits strong promise to allow for a more comprehensive depiction of product attributes and uncover deeper insights into customer preferences, which single-modal data alone may not enable (Wang et al., 2023; Peng et al., 2023). Particularly given the recent surge of many Large-Language Models (LLMs) on e-commerce tasks and their remarkable performance (Peng et al., 2024; Li et al., 2024b; Shi et al., 2023), it is reasonable to expect that multimodal data can drive new breakthroughs in e-commerce applications through the use of LLMs (i.e., unimodal foundation models) or Multimodal Foundation Models (MFMs).\nHowever, despite the richness of multimodal e-commerce data, there are significant challenges that hinder its optimal use by foundation models (Wang et al., 2023; Liu et al., 2023b): (1) Scarcity of large-scale, high-quality multimodal benchmark datasets for a large variety of e-commerce tasks. It is highly nontrivial to collect and curate such datasets due to the significant complexity of the data processing involved (e.g., selecting products that possess rich, high-quality data across all modalities). While initiatives for unimodal e-commerce benchmark datasets for LLMs have been undertaken (Peng et al., 2024; Li et al., 2024b; Shi et al., 2023), unfortunately, to the best of our knowledge, no such multimodal counterparts exist. (2) Lack of effective multimodal information integration methods for e-commerce tasks. Current LLM-based e-commerce models (Peng et al., 2024; Li et al., 2024b) often focus predominantly on one modality, typically text. The limited existing works on multimodal-ities (Chia et al., 2022; Yu et al., 2022) attempt to map different modalities into a shared latent space, inspired by the CLIP-based models (Radford et al., 2021) developed from the computer vision domain. This strategy ignores the uniqueness of e-commerce data, for example, an image of a big bottle of shampoo does not contain information on its scent, while user reviews praise the scent but complain about the bottle size \u2013 information alignment does not always occur.\nIn this paper, we aim to address these challenges and develop foundation models for e-commerce tasks, leveraging multimodal e-commerce data. We first introduce MMECInstruct, the first-ever, large-scale, and high-quality multimodal instruction dataset for developing and evaluating foundation models for e-commerce. MMECInstruct consists of 75,000 samples from 7 widely-performed and real-world e-commerce tasks. Each data sample includes an instruction, an image, a textual input, and an output. MMECInstruct is carefully curated to support a broad range of experimental settings, including in-domain (IND) evaluation for all 7 tasks, out-of-domain (OOD) evaluation (i.e., evaluation tasks on new products not included in the training set) for 5 tasks, and task-specific studies. We perform rigorous processing to ensure the high quality of the MMECInstruct.\nWe also develop CASLIE (CAptions Speak Louder than ImagEs), a simple, lightweight, yet effective framework for integrating multimodal information \u2013 images and text, for e-commerce tasks. CASLIE comprises 3 modules a context-conditioned caption generation module, a caption quality evaluation module, and a modality information fusion module. CASLIE enjoys the following innovations. (1) CASLIE produces context-conditioned (i.e., based on product titles, user reviews, etc.) textual representations (i.e., captions) of images, adaptively highlighting image details with respect to the given context. (2) CASLIE generates textual captions of images by integrating the extensive world knowledge encoded in its MFM-based captioner (Dubey et al., 2024). This design enriches the textual representations of images with additional information that may not be presented in images but is related to image details and beneficial to the target task. (3) By context-conditioned captioning, CASLIE explicitly translates visual content (e.g., images) into textual representations (e.g., captions). These textual representations can be seamlessly integrated with other textual data in the context (e.g., product title) enabling a unified view of multimodal data that can be easily used by any LLM-based e-commerce methods. (4) CASLIE deliberately excludes captions that do not provide information beneficial to the target task, ensuring a strategic and robust image information fusion.\nLeveraging MMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted as CASLIE-L, CASLIE-M, and CASLIE-S, on top of three powerful, general-purpose LLMs, such as Llama (Touvron et al., 2023; Dubey et al., 2024) and Mistral (Jiang et al., 2023) by instruction tuning. The CASLIE models are extensively evaluated across 5 categories of advanced baseline methods on both IND and OOD data. Our experiments show superior performance of CASLIE over baselines in IND evaluation, with a substantial improvement of 6.5% over the best baseline across the 7 tasks. In addition, CASLIE demonstrates strong general-izability to OOD settings, establishing a notable improvement of 3.3% over the best baseline."}, {"title": "2 Related Work", "content": "2.1 Large Language Models for e-Commerce\nGiven that LLMs have abundant world knowledge (Luo et al., 2023) and strong generalizability (Wei et al., 2022; Chung et al., 2024), recent studies introduce fine-tuned LLMs for generalist modeling in e-commerce. For example, P5 (Geng et al., 2022) is fine-tuned on T5 (Raffel et al., 2020) with a unified paradigm to perform multiple e-commerce tasks simultaneously. LLaMa-E (Shi et al., 2023) is fine-tuned on LLaMa (Touvron et al., 2023) to enable transfer learning among different e-commerce tasks. EcomGPT (Li et al., 2024b) fine-tunes LLMs with a chain of tasks for e-commerce. eCeLLM (Peng et al., 2024) introduces a large-scale, high-quality instruction dataset and LLMs fine-tuned from the dataset. However, all these studies are limited to only text data, unable to process multimodal data (e.g., texts and images) ubiquitous in e-commerce. In this paper, we develop CASLIE, a multimodal foundation model for e-commerce.\n2.2 Multimodal Learning for e-Commerce\nIn recent years, remarkable advancements in multimodal learning (Radford et al., 2021; Li et al., 2021; Alayrac et al., 2022; Stevens et al., 2024) have enabled significant process in integrating vision and language into e-commerce models. For example, CommerceMM (Yu et al., 2022) learns multimodal representations for various e-commerce tasks by aligning paired data from different modalities via contrastive learning. ECLIP (Jin et al., 2023) and FashionCLIP (Chia et al., 2022) introduce CLIP (Radford et al., 2021)-based contrastive pre-training frameworks to learn multimodal e-commerce data representations transferable to downstream tasks. However, CLIP-based models generate image representations from the"}, {"title": "3 MMECInstruct Dataset", "content": "We introduce MMECInstruct, a multimodal instruction dataset designed to adapt general-purpose MFMs for e-commerce. MMECInstruct is constructed under three principles: (1) Multimodal data: MMECInstruct contains both visual and textual content for each item in various e-commerce tasks, allowing foundation models to incorporate such multimodal data and enhance e-commerce performance. (2) Broad coverage: MMECInstruct comprises seven diverse and realistic tasks to enable versatile e-commerce modeling and benchmarking (Peng et al., 2024). (3) High quality: The dataset undergoes rigorous scrutiny to ensure accuracy and high quality. As demonstrated in the literature (Hoffmann et al., 2022; Gadre et al., 2024), high-quality instruction-tuning data plays a pivotal role in building powerful foundation models. Figure 1 presents the overview of MMECInstruct. To the best of our knowledge, MMECInstruct is the first of its kind.\n3.1 E-commerce Tasks\nMMECInstruct comprises seven widely-performed real-world e-commerce tasks with real-world data extracted from real e-commerce platforms, following the existing literature (Peng et al., 2024): (1) answerability prediction (AP) (Gupta et al., 2019), (2) category classification (CC) (Yang et al., 2022; Chen et al., 2021), (3) product relation prediction (PRP) (Ni et al., 2019; Xu et al., 2020), (4) product substitute identification (PSI) (Reddy et al., 2022), (5) multi-class product classification (MPC) (Reddy et al., 2022), (6) sentiment analysis (SA) (Wankhade et al., 2022; Daza et al., 2024), and (7) sequential recommendation (SR) (Li et al., 2023a; Hou et al., 2024; Petrov and Macdonald, 2023). Detailed description for these tasks are available in Appendix B.\n3.2 Vision-language Data\nMMECInstruct includes both visual and textual content for each item, fundamentally different from the text-only instruction data such as EcomInstruct (Li et al., 2024b) and ECInstruct (Peng et al., 2024). Particularly, each item may contain (1) product images and user review images as visual information, and (2) product titles, product categories, product brands, user queries, user reviews, and user questions as textual content. Specific samples in each task are described in Appendix B. The multimodal e-commerce data is enriched with synergistic information from multiple data modalities involved in e-commerce applications, enabling the development and benchmarking of different models for multimodal e-commerce tasks.\n3.3 High-quality Instructions\nHigh-quality instructions have been demonstrated to be critical in effectively adapting general-purpose LLMs to e-commerce (Peng et al., 2024). In MMECInstruct, to ensure its high quality,"}, {"title": "4 CASLIE: Multimodal Foundation Model for e-Commerce", "content": "CASLIE includes an enriched context-conditioned captioning module that generates context-conditioned captions from images (Section 4.1), a caption quality evaluation module that verifies caption qualities (Section 4.2), and a lightweighted multimodal information fusion module that integrates high-quality captions with item context information (Section 4.3) for performing e-commerce tasks. Figure 2 presents the overview of CASLIE. Compared to existing MFMs that generally fuse visual and textual information by embedding each modality, optimizing their alignment, and training customized fusion models, CASLIE offers a simple, light-weight, (pre-)training-free yet effective fusion framework, enabling a unified view of multimodal data for e-commerce tasks. Another advantage of CASLIE is its plug-and-play design: all its modules can be easily reimplemented when newer and more advanced models become available, allowing for seamless integration of the most suitable options.\n4.1 Enriched Context-conditioned Captioning\nCASLIE first employs a novel enriched context-conditioned captioning module, denoted as EC\u00b3. EC\u00b3 generates a textual caption for each given image, conditioned on the corresponding context, including the item that the image presents, textual descriptions and the user reviews of the item, the e-commerce task involving the item (and other related items), etc. EC\u00b3 is fundamentally different from the most popular CLIP-style methods in how they use images. CLIP-style methods, such as FashionCLIP (Chia et al., 2022) and BioCLIP (Stevens et al., 2024), generate image embeddings from the entire images, with a general assumption that each image as a whole embodies the \"aligned\" information with its textual descriptions. This may not be particularly true in many e-commerce applications, for example, when users' sentiments are only due to a very specific detail presented in the item image (e.g., a specific ingredient in a shampoo). By doing context-conditioned captioning, EC\u00b3 could underscore different image details conditioned on various contexts, eliminating irrelevant or noisy information from the images.\nEC\u00b3 leverages a SoTA, powerful pre-trained MFM for the caption generation through zero-shot prompting. It incorporates the context information and well-elaborated instructions to form a prompt. The detailed instruction templates are listed in Appendix C. A unique advantage of using pre-trained MFMs is that the MFMs carry extensive world knowledge. Therefore, the generated captions can be enriched with such knowledge that may not be presented explicitly in the images of interest but is relevant to the image details and beneficial to the target task. We use Llama-3.2-Vision-Instruct (Dubey et al., 2024) as the EC\u00b3 model.\n4.2 Caption Quality Evaluation\nExisting multimodal e-commerce methods use the available images for each item (Chia et al., 2022;"}, {"title": "5 Experimental Setup", "content": "They are instruction-tuned with LoRA (Hu et al., 2022) and Huggingface transformers library (Wolf, 2019) over the training and validation data of MMECInstruct. With a slight abuse of notations, we will refer to uniM\u00b3-L, uniM3-M, and uniM3-S also as CASLIE-L, CASLIE-M, and CASLIE-S when no ambiguity arises (i.e., after EC\u00b3 and CQE are applied in CASLIE).\n5 Experimental Setup\nWe compare CASLIE against 5 categories of baseline methods: (1) fine-tuned CLIP-based models, (2) fine-tuned LLMs, (3) e-commerce LLMs, (4) fine-tuned MFMs, and (5) SoTA task-specific models. We conduct IND and OOD evaluation on respective test sets (Section 3) for all the models.\nFine-tuned CLIP-based Models Fashion-CLIP (Chia et al., 2022) is a SoTA CLIP-based (Radford et al., 2021) model adapted to the e-commerce fashion domain and is skilled at various multimodal tasks. We fine-tune the Huggingface checkpoint of FashionCLIP on each task using the MMECInstruct training set and denoted the fine-tuned model as ft-FashionCLIP.\nFine-tuned LLMs We use 3 LLMs as the baselines. For Llama-2-13b-chat (Touvron et al., 2023), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Llama-3.2-3B-Instruct (Dubey et al., 2024), we fine-tune their checkpoints released in Huggingface on MMECInstruct training data using all tasks and only text as input. The fine-tuned models are denoted as ft-Llama-2-13B, ft-mistral-7B-v0.3, and ft-Llama-3.2-3B. We perform the zero-shot evaluation on the fine-tuned models since these models have already gained e-commerce knowledge.\nE-commerce LLMs We utilize eCeLLM-L and eCeLLM-M (Peng et al., 2024), a series of SOTA e-commerce LLMs, fine-tuned on various e-commerce tasks, as a baseline. For eCeLLM-L and eCeLLM-M, we perform a zero-shot evaluation using the checkpoints available on Huggingface since they already encompass a broad understanding of e-commerce concepts.\nFine-tuned MFMs We use fine-tuned LLaVA-NExT-interleave-qwen-7b (Li et al., 2024a) as the baseline, LLaVA-NExT-interleave-qwen-7b is a SOTA multi-image MFM able to process input textual and image information of one or multiple instances,"}, {"title": "6 Experimental Results", "content": "making it a suitable baseline for e-commerce tasks, particularly those evaluating multiple products simultaneously (e.g., PRP). We fine-tune the checkpoint of LLaVA-NExT-interleave-qwen-7b released in Huggingface on the training data of MMECInstruct. The fine-tuned model is denoted as ft-LLaVA-NExT-interleave. We also conduct the zero-shot evaluation for this baseline.\nSOTA Task Specific Models To evaluate the SR and CC tasks, we fine-tune RECFORMER (Li et al., 2023a), a popular language-based recommendation model, and Sentence-BERT (Reimers and Gurevych, 2019), which is adept at semantic similarity search tasks like retrieval, respectively. All other tasks are evaluated on the fine-tuned DeBERTa (He et al., 2021), which is a widely used BERT-based model known for its strong performance in various language understanding tasks.\nCQE Models In CQE, we use five models as the binary classifiers for MV: (1) Llama-3.2-3B-Instruct (Dubey et al., 2024), (2) Llama-3.1-8B-Instruct (Dubey et al., 2024), (3) Llama-3.2-Vision-Instruct (Dubey et al., 2024), (4) Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and (5) Phi-3.5-mini-Instruct (Abdin et al., 2024).\n6 Experimental Results\nWe conduct a systematic evaluation of CASLIE against all the baselines using the test set of each individual task in MMECInstruct. For a comprehensive evaluation, we utilize multiple metrics on each task. To enable a succinct presentation, for each task, we present only the performance at the primary metric, defined as follows: for AP and PSI, we use the F1 score; for CC and SR, we evaluate results primarily using Recall@1 (R@1); for MPC, we use accuracy (acc); and for PRP and SA, we employ the macro F1 score (M-F1) as the primary metric. Complete results for each task are reported in Appendix D. When comparing CASLIE with baselines, we report the mean of CASLIE's improvement over baselines per task as its overall improvement. Additional results are in Appendix D, E, and F.\n6.1 In-domain Evaluation\nThe left part of Table 2 shows the overall performance in IND evaluation.\n(1) CASLIE-M substantially outperforms 8 baselines at 27.8% on average across 7 tasks as shown in Table 2. These results demonstrate the remarkable effectiveness of CASLIE compared with the fine-tuned CLIP-based model, fine-tuned LLMs, e-commerce LLMs, fine-tuned MFMs, and the SOTA task-specific models across the widely-performed e-commerce tasks.\n(2) CASLIE-M achieves a significant 45.8% improvement over the ft-FashionCLIP fine-tuned on the training data of MMECInstruct. A key difference between CASLIE and FashionCLIP is that CASLIE uses the textual representation of images generated via context-conditioned captioning (EC\u00b3), adjusting the focus on image details with respect to the specific context. In contrast, FashionCLIP generates image representations without considering the specific context. Additionally, CASLIE could leverage the extensive world knowledge of LLMs to enrich the captions, while FashionCLIP considers the images solely using the vision encoder.\n(3) CASLIE exhibits superior performance over fine-tuned LLMs and e-commerce LLMs, as shown in Table 2. Specifically, CASLIE-M outperforms ft-Llama-2-13B by 17.8%, ft-Mistral-7B-v0.3 by 6.5%, ft-Llama-3.2-3B by 15.1%, eCeLLM-L by 25.2%, and eCeLLM-M by 37.1%. The results highlight the benefit of incorporating contextually relevant, textually represented image information into CASLIE. By integrating visual information with powerful LLMs, CASLIE enhances its ability to jointly learn e-commerce tasks from a multimodal perspective, enabling performance that text-only information cannot achieve.\n(4) CASLIE-M achieves a considerable 52.9% improvement over the fine-tuned MFM ft-LLaVA-NExT-Interleave, as demonstrated in Table 2. Notably, ft-LLaVA-NExT-Interleave struggles significantly with the task SR that requires processing multiple images, while CASLIE achieves state-of-the-art performance (0.053 in R@1 vs CASLIE-M's 0.223). The result substantiates the flexibility of CASLIE to effectively process multiple images and utilize rich visual information, hence improving performance on e-commerce tasks. Unlike fine-tuned MFMs, CASLIE leverage context-conditioned captions as the vision representation, emphasizing task-related information from images. CASLIE also helps avoid potential misalignment issues in MFMs, when images do not convey information concordant with texts. Additionally, CASLIE enriches the textual representation of images by incorporating world knowledge, further enhancing its performance compared to MFMs.\n(5) CASLIE-M outperforms SoTA task-specific models with a significant 22.1% improvement across all 7 tasks. Compared with SoTA task-specific models, which solely rely on textual information from each individual task, CASLIE could leverage both vision and language information of each task, and the information shared across diverse e-commerce tasks, as well as LLM's inherent knowledge and learning power, to significantly boost performance on each individual task.\n(6) Mid-size CASLIE-M performs best among CASLIE model sizes. Benefitting from the large-scale instruction-tuning dataset and powerful base model (Mistral-7B-Instruct-v0.3) mid-size fine-tuned models achieve most, balancing learning from instruction tuning while retaining knowledge from base models.\n6.2 Out-of-domain Evaluation\nThe right part of Table 2 presents the performance of CASLIE and baselines in OOD evaluation. Overall, CASLIE demonstrates strong generalizability to deal with new users and new products.\n(1) CASLIE-M surpasses the fine-tuned MFM ft-LLaVA-NExT-Interleave by a substantial 624.6% improvement across 4 tasks except for SR in the OOD setting, underscoring the strong generalizability of CASLIE. Fine-tuned MFMs appear struggling to transfer knowledge effectively in OOD scenarios, possibly due to that new products may have very different images or similar images but very different textual information. CASLIE translates images to context-conditioned textual representations, not only highlighting image information most pertinent to specific tasks, but also taking advantage of the well-known generalizability of LLMs (Touvron et al., 2023; Jiang et al., 2023; Dubey et al., 2024), and thus well generalizing to OOD scenarios.\n(2) Similarly, CASLIE-M demonstrates significant advantages over ft-FashionCLIP and eCeLLM-L in the OOD evaluation, with average improvements of 85.1% and 6.4% respectively. CASLIE could easily leverage LLMs' generalizability and world knowledge that ft-FashionCLIP doesn't enjoy. Meanwhile, the ability to integrate multimodal information via context-conditioned captions allows CASLIE to better capture nuanced product details, enabling it to generalize to new products more effectively than eCeLLM-M, which focuses primarily on text-based information.\n6.3 Task-specific and Generalist CASLIE\nWhen comparing the task-specific CASLIE, which is fine-tuned for each individual task, with the generalist CASLIE, which is fine-tuned across all the tasks together, we observe a trend consistent with that in prior research (Peng et al., 2024): the generalist CASLIE outperforms task-specific CASLIE on each individual task. As shown in Table 3, generalist CASLIE-L, CASLIE-M, and CASLIE-S exhibit significant improvements of 44.8%, 7.3%, and 15.4% over their respective task-specific CASLIE across all tasks except for PSI. These results highlight that training on all tasks together, CASLIE enjoys strong versatility and learns transferable knowledge across tasks to boost the performance on individual tasks. It is noteworthy that on PSI, all task-specific CASLIE models fail due to highly unbalanced labels (74% negatives), whereas generalist CASLIE models still achieve considerable performance. This demonstrates that certain e-commerce tasks (e.g., PSI) could substantially benefit from knowledge transfer through generalist modeling, underscoring its importance.\n6.4 Analysis on Captioning Models\nIn this section, we explore the impact of captioning models EC\u00b3 and caption quality evaluation models CQE on the performance of CASLIE, exemplified by CASLIE-M. We include BLIP2-OPT-2.7B (Li et al., 2023b) as a context-free captioning model and evaluate it as a baseline. Table 4 also compares the CASLIE-M using various individual captioning models, including LLaVA-1.5-7B (Liu et al., 2023a, 2024a), LLaVA-NEXT-mistral-7B (Liu et al., 2024b), and Llama-3.2-Vision-Instruct (Dubey et al., 2024). Table 4 presents the results.\n(1) Overall, using visual information through captioning is almost always better than not using visual information. Specifically, using BLIP2-OPT-2.7B to generate context-free captions from images brings a 1.8% average improvement compared with ft-Mistral-7B-v0.3, which does not use visual information at all; using LLaVA-NExT-mistral-7B in CASLIE for context-conditioned captioning results in 8.6% improvement over ft-Mistral-7B-v0.3. This shows the utility of visual information in e-commerce tasks and demonstrates that captioning is an effective way of utilizing images in e-commerce models.\n(2) Context-condition captioning beats context-free captioning for e-commerce. CASLIE-M, which employs Llama-3.2-Vision-Instruct as the captioning model by default, outperforms that using the context-free captioning model (BLIP2-OPT-2.7B) by 4.5%. This further highlights the advantage of using context conditioned captioning to enhance task performance compared to more generic, context-free approaches. Comparing all context-conditioned captioning models, we observe comparable results, but Llama-3.2-Vision-Instruct as the captioning model is slightly and consistently better overall.\nBesides captioning models, we also conduct an abolition study on using various evaluation strategies in CQE, detailed in Appendix E."}, {"title": "7 Conclusion", "content": "This paper open-sources a high-quality, multimodal instruction dataset MMECInstruct for e-commerce. To our knowledge, MMECInstruct is the first of its kind. We also develop CASLIE, a simple, yet effective framework integrating multimodal information for e-commerce. Leveraging MMECInstruct, we fine-tune the state-of-the-art MFMS (CASLIE series) within CASLIE for e-commerce. Our extensive evaluation of CASLIE models against the most advanced baseline models demonstrate that CASLIE models substantially outperform the best baseline model ft-Mistral-7B-v0.3 in both IND and OOD evaluations with improvements of 6.5%, and 3.3%, respectively."}, {"title": "8 Limitations", "content": "First, while our dataset MMECInstruct undergoes rigorous quality control, there remains a possibility that some samples may still contain noisy or inaccurate information (e.g., mismatch between text and image). This might hinder the performance of the CASLIE that is fine-tuned on this dataset. Second, the LLM-based captioning module EC\u00b3 might generate inaccurate or even hallucinated captions in rare occasions, where the captions do not truthfully represent actual objects in the images. This issue might be partially addressed via preference alignment and optimization (Gunjal et al., 2024) to reduce hallucination. Third, CQE can only decide whether or not the captions provide beneficial information within the given context but lacks interpretability to explicitly pinpoint the particular regions/details of the images that are beneficial to the tasks. For future work, we plan to lever-"}, {"title": "A More Information about Datasets", "content": "To pursue adherence to data usage requirements, we check the licenses of MMECInstruct data sources, ensuring their permission to publish. Table A1 presents the licenses of our curated dataset sources.\nA.1 Data Split\nRaw datasets of category classification (CC, discussed in Appendix B.2) product relation prediction (PRP, discussed in Appendix B.3) and sentiment analysis (SA, discussed in Appendix B.6) are first split into training, validation, and test data at 8:1:1 ratio. For answerability prediction (AP, discussed in Appendix B.1) product substitute identification (PSI, discussed in Appendix B.4), and query-product relevance classification (MPC, discussed in Appendix B.5), the raw datasets are already split. For the sequential recommendation (SR, discussed in Appendix B.7), we follow the convention (Hou et al., 2022), leaving the last products in sequence interactions as the test data and the second last products as validation data.\nIn general, (1) MMECInstruct contains 8K samples for each individual task. These are combined into a single set of 56,000 samples, forming the complete training set for MMECInstruct. (2) MMECInstruct includes a validation set of 1K samples for each individual task. These validation sets are combined into a single set of 7,000 samples, forming the complete validation set for MMECInstruct. (3) For each of the 7 tasks, MMECInstruct also includes an in-domain test set consisting of 1K samples. IND is defined in terms of products that belong to the same set of categories as those used in the training set. (4) To assess the generalizability of models to unseen samples and address the cold-start issue in e-commerce, we create out-of-domain test sets in MMECInstruct. OOD is defined as new products that are not seen during training, identified by their category information. Five tasks (AP, CC, PRP, SA, and SR) have products from different categories and are used with certain categories held out as OOD sets, as summarized in Table 1.\nFollowing prior research (Wei et al., 2022) and taking into account the high computational demands, we uniformly downsample the training sets for each individual task to 8K samples, the validation sets to 1K, and the test sets to 1K. This ensures an optimal balance between data volume and efficient processing for affordable LLM evaluation.\nA.2 Tasks Definitions\nFollowing ECInstruct (Peng et al., 2024), MMECInstruct comprises 7 widely-performed real-world tasks constructed from real-world data, which are ubiquitous and essential in the e-commerce domain. To be specific, here are the 7 tasks. Not all ECInstruct tasks are involved since some data sources lack vision information.\nAnswerability Prediction (AP) (Gupta et al., 2019): Predict if the product-related question is answerable based on the product information.\nCategory Classification (CC) (Yang et al., 2022): Retrieve the category of the product based on the product information.\nProduct Relation Prediction (PRP) (Ni et al., 2019; Xu et al., 2020): Identify the relationship between two product from \"also buy\", \"also view\", and \"similar\".\nProduct Substitute Identification (PSI) (Reddy et al., 2022): Predict if the product can serve as a functional substitute for the user's query.\nMulti-class Product Classification (MPC) (Reddy et al., 2022): Given a query and a product title, predict the relevance between the query and the product.\nSentiment Analysis (SA) (Hou et al., 2024; Wankhade et al., 2022): Identify the sentiment that the user expressed based on the product review text and review image.\nSequential Recommendation (SR) (Hou et al., 2024; Li et al., 2023a): Predict the next product that the user would be interested in based on the user's purchase history.\nA.3 Data Selection\nIn the AP, PRP, SA, and SR tasks, Tools category data from Amazon datasets (Gupta et al., 2019; Hou et al., 2024; Ni et al., 2019) serve as in-domain (IND) data sources, and Sports category data serves as out-of-domain (OOD) data.\nFor the MPC and PSI tasks, we directly process the row datasets (Reddy et al., 2022) from their original splits.\nFor the CC tasks, we select the 100 most frequent fine-grained categories as in-domain (IND) data, while categories ranked between 100 and 200 in frequency are used as out-of-domain (OOD) data."}, {"title": "B Data Processing", "content": "B Data Processing\nWe conduct the data processing following ECInstruct (Peng et al.", "3": 5.0}]}