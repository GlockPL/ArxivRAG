{"title": "ON TRAINING OF KOLMOGOROV-ARNOLD NETWORKS", "authors": ["Shairoz Sohail"], "abstract": "Kolmogorov-Arnold Networks have recently been introduced as a flexible alternative to multi- layer Perceptron architectures. In this paper, we examine the training dynamics of different KAN architectures and compare them with corresponding MLP formulations. We train with a variety of different initialization schemes, optimizers, and learning rates, as well as utilize back propagation free approaches like the HSIC Bottleneck. We find that (when judged by test accuracy) KANS are an effective alternative to MLP architectures on high-dimensional datasets and have somewhat better parameter efficiency, but suffer from more unstable training dynamics. Finally, we provide recommendations for improving training stability of larger KAN models.", "sections": [{"title": "1 Introduction", "content": "For the last decade, the Perceptron [1] has served as the defacto building block of deep neural networks that utilize a fully connected layer in their architecture. Recent breakthroughs in large language models are due partly to large stacks of Transformer [2] units, which also utilize Perceptrons as part of their internal machinery. The popularity of Perceptron based architectures can be attributed mainly to two things: flexibility for learning non-linear functions [3], and an inherent ability to parallelize on modern GPU architecture [4]. While Perceptron-free architectures have been shown to display impressive performance [5], the fully-connected Perceptron layer remains a staple in high performing models.\nRecently, a viable alternative to the Perceptron unit for fully connected layers has been proposed - the Kolmogorov- Arnold unit [6]. This unit relies on the Kolmogorov-Arnold representation theorem [7] to fit a B-spline [8] to input data, with the claim that this achieves performance on par or above that of the Perceptron when used in fully connected layers [6]. However, all recent work on KANs has relied on training schemes optimized over years of training Perceptron based networks, which may be sub-optimal to yield the best performance out of KAN architectures.\nOur contributions are as follows:\n\u2022 We test several combinations of initialization, optimizer, and learning rate on KANs of different sizes using a variety of real world datasets, then compare with MLPs (fixing the number of layers or the number of parameters).\n\u2022 We attempt to train KANs utilizing the HSIC Bottleneck [9] (an alternative to back propagation).\n\u2022 We explore KAN scalability with regards to intrinsic dimension, width, and depth.\n\u2022 We explore additional metrics of KANs compared to MLP architectures including a novel, dataset-agnostic efficiency metric.\nOur work can be seen as an extension of [10], where experiments are conducted utilizing much larger models (10m+ parameters), additional datasets, and more training schemes. We restrict our attention to fully connected architectures for ease of comparison. The focus of this paper is on discovering optimal training schemes for fully connected KANS while comparing with the performance of equivalently trained MLPs."}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 B-splines", "content": "A spline [11] of degree k is a function defined piecewise by polynomials of degree \u2264 k. Spline functions are usually designed to smoothly interpolate complicated continuous functions while avoiding some of the issues (such as Runge's phenomona [12]) posed by fitting a single, high dimensional polynomial. In fact, it can be shown that any continuous function defined completely over a real-valued range [a, b] admits such a decomposition into degree n \u2265 1 polynomial functions, with uniform convergence as n goes to infinity [7].\nFor any specific spline of degree k, it can be represented as a linear combination of B-splines (\"basis splines\") of degree k. This means that for an unknown function f(x), a degree k spline S(x) and sequence of degree k B-splines {B\u2081(x)...Bn(x)}, we have:\n\n$f(x) = S(x) = \\sum_{i=1}^{n}ciBi(x)$\n\n(1)\n\nfor x \u2208 [a, b]. Thus, B-splines are a powerful starting point when one wishes to fit a complex and partially observed function."}, {"title": "2.2 The Kolmogorov-Arnold unit", "content": "A single Kolmogorov-Arnold unit is defined using the following function:\n\n$\\varphi(x) = w_b\\alpha(x) + w_s \\sum_{i}C_i B_i(x)$\n\n(2)\n\nwhere wb, ws, and ci are learnable parameters, and \u03b1(x) is a non-linear activation function (such as silu(x) = $x\\over{1+e^{-x}}$).\nUsing the notation from (1), we have:\n\n$\\varphi(x) = w\\alpha(x) + w_sS(x)$\n\n(3)\n\nSimilar to MLPs, Kolmogorov-Arnold units can be placed in \"layers\" and composed. If we let \u03c6 represent the ith KA unit in layer l, then:\n\n$\\Phi^l = \\sum_{i=1}^{n^{l-1}}\\phi^l_i (x_i^{l-1})$\n\n(4)\n\nwhere ni-1 is the number of KA units in the previous layer, and $x_i^{l-1}$ is the activation from the ith unit of the previous layer (or the ith input feature, when l = 1). Thus, if we let \u03a6l denote a column vector < \u03c6... \u03c6 > of units in layer l, we can write a Kolmogorov-Arnold Network with K layers as:\n\n$KAN(x) = (\\Phi_K \\circ \\Phi_{K-1}... \\circ \\Phi_1)(x)$\n\n(5)"}, {"title": "3 Method", "content": ""}, {"title": "3.0.1 Architecture Choices", "content": "We test a shallow, medium, and deep architecture for each task (sizes are task dependent, see appendix 1.1 for details on width and depth). For each network, we keep the width and depth constant, but models can have more or less parameters depending on the parameterization of their units (a single KA-3 unit has more parameters than a single Perceptron unit). Note that the chosen widths are based on previous work with MLPs [13], and exploring large width KANs remains an open area of research. To help make MLP architectures more comparable to their KAN counterparts, a series of wide MLP architectures is also tested, where the width of each layer from the corresponding (small, medium,large) architecture is doubled but depth is kept the same. All networks utilize ReLU activation functions and a dropout probability of 0.2. For each network and architecture we vary the datasets and training schemes, as described below."}, {"title": "3.0.2 Datasets", "content": "We perform experiments using a variety of datasets spanning image classification, sentiment prediction, and scientific (tabular) data. We perform no image augmentation [14] or mixup [15]. Below is a breakdown of the datasets:"}, {"title": "3.1 Training Schemes", "content": "We define a training scheme for a particular model and dataset as the set of {initialization, optimizer, initial learning rate, batch size, stopping criteria} with the belief that this defines the training dynamics for a given model architecture and dataset. There is evidence to support this [21][22], as well as significant research performed on the effect of each of these individual factors on overall training success for MLPs [23]. We do not modify default learning rate schedules (which have shown to also have an effect on performance [24]).\nWe vary training schemes by testing three initialization schemes, three optimizers, and three learning rates, while keeping the batch size and stopping criteria fixed. This leads to 27 runs of each model on each dataset. We then perform experiments with the HSIC Bottleneck [9], a backpropagation-free algorithm. Finally, we take the best training schemes from these runs and perform additional experiments, varying the intrinsic dimension and activation function of the KANs to attempt further improvement.\nModels: & {MLP, KAN, MLP-Wide}\nInitialization: & {Kaiming-Normal, Kaiming-Uniform, Orthogonal}\nOptimizers: & {SGD, SGD-M, AdAM}\nActivtions: & {ReLU, Cosine}\nLearning Rates: & {0.05, 0.005, 0.0005}"}, {"title": "4 Results", "content": ""}, {"title": "4.0.1 Training with Backpropagation", "content": "We observe that both MLPs and KANs achieve comparable accuracies across a variety of datasets when compared using fixed network architectures. However, KANs contain more parameters than MLPs even when network width and depth are fixed (because of the additional intrinsic parameters of KA-units). When MLPs are scaled up (by increasing layer width) to match the number of parameters of a KAN of the same depth, performance of the wide MLPs is comparable (HIGGS) or better (MNIST, Fashion MNIST, CIFAR10, IMDB) than KANs. Generally, MLP architectures outperform KAN architectures of the same parameter count as measured by accuracy and training time when back propagation is used (see barplots for \"kan 2layers\" vs. \"mlpWide 2layers\").\nAdditionally, KANs seem to have a higher tendency to overfit (as measured by the difference between training and test accuracy) than MLPs for all training schemes. This necessitates additional caution in training large KANs to prevent overfitting early in the training process. In general, large KANs seem to perform best when utilizing a Kaiming Normal [25] weight initialization, an adaptive optimizer (such as AdAM), and low initial learning rate (<= 5e-4). Looking across datasets, another trend seems to emerge: final performance of KANs is more sensitive to choice of initialization, optimizer, and learning rate. For example, below is a boxplot of MNIST and Fashion MNIST performance under all backprop training schemes (varying initialization, optimzier, and learning rate):\nThis indicates that it may be prudent to evaluate additional regularization and training protocols, including back- propagation free approaches. This may help in fully utilizing the expressive power of KAN layers."}, {"title": "4.0.2 Training with the HSIC Bottleneck", "content": "One way to sidestep some of the challenges of training KANs with backpropagation is to use a backpropagation-free training algorithm. The HSIC Bottleneck [9] utilizes a layer-by-layer joint information objective, and has been shown to successfully train networks with large parameter counts while avoiding some of the issues with back-propagation such as vanishing gradients, and leading to more stable and rapid convergence. We hypothesize that given their tendency to overfit and sensitive dependence on optimizer, the HSIC Bottleneck may provide more stable and consistent training of KANs. We repeat the experiments above, except utilizing the HSIC objective instead of standard backpropagation."}, {"title": "5 Additional Experiments", "content": ""}, {"title": "5.1 The Role of Activation Functions", "content": "All initial training runs were performed with GELU activation functions, as in the original KAN architecture [6]. For this section we extract the best training scheme for each dataset's KAN and vary the activation function. Besides the GELU, we also evaluate the ELU and SILU activation functions, which (similar to GELU) attempt to mitigate the \"dead neuron\" [26] effect seen in ReLU activated networks, where certain neurons become stuck at producing zero valued outputs. While dropout and batch normalization can help with this [27], varying activation functions usually provides a synergistic effect.\nResults indicate that KANs almost universally perform best when using the GeLU activation as compared to the SiLU or ELU."}, {"title": "5.2 KANs - Scaling with Degree", "content": "For a fixed architecture, we can also opt to add more interpolation points for each B-spline and increase the degree. As each spline fit becomes more exact, we expect test error to lower to a point and then increase as over-fitting starts to occur. It is tempting to view increasing the degree of KA units in a layer as analogous to increasing the width of the layer, however this hypothesis needs to be tested. We test a KAN with a single hidden layer and alternatively increase the hidden layer's width or the degree of the KA units. We fix the training scheme to the best one found in the above evaluations (Kaiming Normal initialization, AdAM optimizer, 1e-4 learning rate)\nThe resulting test accuracies indicate that you cannot simply trade width for degree in KANs, and (on a per-parameter basis) increasing the degree in a layer seems to provide less performance utility than adding another layer of equal degree. This may not hold for deeper networks."}, {"title": "6 Performance Observations", "content": ""}, {"title": "6.1 Efficiency", "content": "We seek to compare model efficiency in a way that is hardware agnostic and dataset agnostic. It should capture intrinsic tradeoffs in model design and training schemes. To do this, we develop the following measure:\n\n$EF(M,T, D) = {1 \\over {EA^{*}(M,T,D)+1}} * {log(ID(D)-P(M))+1 \\over A^{*}(M,T,D)}$\n\nWhere ID(D) is the intrinsic dimension [28] of the test set D, $A^{*}(M,T, D)$ is the best test accuracy achieved by a model M on test set D under training scheme T, $EA^{*}(M,T, D)$ is the epochs taken to reach the best test accuracy, and P is the number of parameters in the model. Note that this function is only defined when P(M) > ID, and is maximized when a model with ID parameters reaches 100% accuracy without training. This concept is beneficial in model selection for practical problems, for example while the best model for MNIST reaches 99% test accuracy, it may be more interesting to study the model with the highest efficiency (i.e the 25k parameter, single hidden layer MLP that reaches 95%+ accuracy with a single epoch of training). Note that this measure depends on the dataset only through the difference of the ID term and the model size, capturing the fact that more complex datasets require more complex models to reach the same efficiency.\nWe see that when comparing architectures with an equal number of parameters (mlpWide 2layers and kan 2layers), the efficiency of KANs is higher. However, there is also higher average generalization error (for the KAN) with an equivelent number of parameters. Inspection of individual training runs shows this to be due largely to KANS reaching their highest accuracies early in the training process, then overfitting to training data. We see that as we add layers, efficiency of KAN architectures seems to diminish and generalization error stays the same or increases, further reinforcing the notion that vanilla KANs may not scale as well to deep architectures compared to MLPs.\nIn the graph below, model size is plotted against efficiency for all datasets jointly. ID estimates for CIFAR and MNIST are as calculated in [28]."}, {"title": "7 Conclusion and Future Work", "content": "Our study demonstrates that KANs provide competitive accuracies when compared to MLP architectures, but are more sensitive to choice of training scheme. For optimal performance KAN architectures must be trained using adaptive optimizers and low learning rates. Without such training schemes, KANs tend to overfit and exhibit high generalization error (compared to MLPs trained with the same scheme). Furthermore, KANs appear to be marginally more parameter efficient than MLPs, suggesting the possibility of smaller models to attain competitive accuracies. Because of a tendency to overfit much earlier during the training process than MLP architectures, caution must be used when scaling to very deep architectures using traditional MLP training schemes (future work evaluating residual connections in KAN architectures may help to alleviate some of these issues). These conclusions seem to hold regardless of training with backpropagation or the HSIC Bottleneck algorithm, and even when the number of parameters and number of layers is held constant. This suggests that while final accuracies remain competitive, KANs merit additional study on optimal training schemes to yield their full potential."}]}