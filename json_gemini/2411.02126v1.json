{"title": "Unsupervised detection of semantic correlations in big data", "authors": ["Santiago Acevedo", "Alex Rodriguez", "Alessandro Laio"], "abstract": "In real-world data, information is stored in extremely large feature vectors. These variables are typically correlated due to complex interactions involving many features simultaneously. Such correlations qualitatively correspond to semantic roles and are naturally recognized by both the human brain and artificial neural networks. This recognition enables, for instance, the prediction of missing parts of an image or text based on their context. We present a method to detect these correlations in high-dimensional data represented as binary numbers. We estimate the binary in-trinsic dimension of a dataset, which quantifies the minimum number of independent coordinates needed to describe the data, and is therefore a proxy of semantic complexity. The proposed algorithm is largely insensitive to the so-called curse of dimensionality, and can therefore be used in big data analysis. We test this approach identifying phase transitions in model magnetic systems and we then apply it to the detection of semantic correlations of images and text inside deep neural networks.", "sections": [{"title": "Introduction", "content": "Correlations among the features defining a data point force a data set to live in a manifold M whose geometry encodes the correlation structure of data. The most fundamental property of M is its intrinsic dimension[1] which is qualitatively defined by the number of independent variables needed to parameterize it without significant information loss. If one was able to estimate the intrinsic dimension in a generic big data scenario, one would be able to quantify the strength of correlations in real-world datasets, like a corpus of sentences or a large set of measurements from a camera. However, real-world data are represented in feature spaces that are outrageously large, and most of the available methods meet serious problems if the intrinsic dimension of the manifold is of the order of hundreds or more[2]. We developed an intrinsic dimension estimator that generalizes the concept of the intrin-sic dimension to the case of binary variables. We show that this allows circumventing the curse of dimensionality, and estimating reliably the intrinsic dimension even when it is of order 100,000. This result paves the way to a plethora of applications, allowing to address questions which have thus far been, to the best of our knowledge, intractable. For example, we can measure the range of semantic correlations in a corpus of sentences with hundreds of words, and we demonstrate that these correlations are destroyed when shorter, unrelated sentences are joined together. We show that our approach is effective in a variety of other fields and topics, from the study of phase transitions and of frustration-induced disorder in statistical physics, to image classification.\nDetecting and quantifying correlations in generic real-world data is a complex task since features live in high dimensional spaces and have interactions of arbitrary range and nature. For example, in an image, the meaning of a scene can emerge from the simultaneous pres-ence of two objects that are physically far away. Moreover, semantic correlations are often associated with collective effects, such as the simultaneous presence of three or more specific objects. A prominent example where non-trivial correlations emerge is the feature space of deep neural networks. Those spaces typically lack a well-defined order, and neurons that are close in the architecture are not necessarily coding related information. This hinders the direct interpretability of those feature spaces, leading many scientists, even today, to consider neural networks as \"black boxes\".\nThe main idea of this work is to use the intrinsic dimension (ID) of a dataset to detect and quantify correlations. If, for example, in a long text correlations between the first and the"}, {"title": "Estimating the Binary Intrinsic Dimension", "content": "second part of the text vanish, the ID of the whole text should be approximately equal to the sum of the ID of the two parts. More precisely, in data points including features describing independent information, the ID should scale linearly with the number of features N, namely\n$ID = \\gamma N$\nfor some positive constant y. If one would be able to estimate the ID as a function of text length (or as a function of the patch dimension, in the case of images) one would be able to detect the range of the correlation. Moreover, the difference between the sum of the IDs of two representations and the ID of a dataset composed by the concatenation of these representations provides a quantitative measure of the strength of the correlations[3]: for perfectly correlated representations the ID of the concatenation will be equal to the ID of the single representations.\nHowever, most of the available algorithms to perform ID estimation meet serious problems if the intrinsic dimension of the manifold is of the order of hundreds or more[2]. In particular, some of them fail to provide an estimate of the ID which scales linearly with the number of features even when those are generated independently. Indeed, methods relying on the statistics of local neighborhoods systematically underestimate the ID because the number of samples required to sample a dataset grows exponentially in the ID[4]. This problem is a manifestation of the so-called curse of dimensionality[5] and affects to different extent all the ID estimators we are aware of, even if recently some progress has been made[6].\nIn order to estimate the ID in very large feature spaces, we here renounce to develop a general-purpose ID estimator, and we focus on features that are binary or can be approx-imately considered as binary. Concretely, we developed an ID estimator for binary data, called BID (from Binary Intrinsic Dimension) which, as we show below, can cope numer-ically with extremely large dimensionalities. Binary variables are the basis of all digital computation but are also of fundamental importance in both theoretical and applied sci-ence. The so-called spin systems, in which a spin is a binary degree of freedom, provide standard models to understand phases of matter, such as spin glasses[7], and spin liquids[8], and the corresponding phase transitions, like spontaneous ordering[9]. Binary variables are also used to describe biological neurons[10, 11] or social dynamics[12]. In artificial neural networks, the activations are typically real numbers, but many efforts focus on finding more efficient representations: the so-called quantization methods[13, 14] aim to represent acti-vations or weights using small integer numbers or binary variables, in order to drastically save memory, energy, and computing time, while preserving performance. Activations and"}, {"title": "Estimating the Binary Intrinsic Dimension", "content": "weights can be transformed in binary variables by the sign function [15, 16]. Binarization has been shown to preserve the scalar product between the activations and weights[17]. In Supp. Inf. we show that binarization also approximately preserves the local neighbourhoods computed using the full-precision activations inside a Large Language Model (LLM). This suggests that binarizing in high dimensions is an operation that retains relevant information about the data[18].\nIn the following, we introduce an algorithm that can be used to estimate the binary intrinsic dimension (BID) of very large datasets of binary variables. We first show that it provides estimates that scale approximately linearly with system size when the system can be split into subsystems that are statistically independent, and reliably estimates the intrinsic dimension for bit streams of length up to L ~ O(10\u00ba), even using only O(1000) samples. We benchmark the algorithm on model systems in which the binary variables interact with an explicit energy function, which allows modulating their correlations. We show that the dependence of the BID on system size allows for characterizing the nature of correlations between the variables and spotting the transitions between ordered and disordered phases. We finally apply this approach to the study of the internal representations of images and text in large neural networks.\nOur algorithm is based on a generalization of exact results for distance distributions between statistically independent binary variables. Let $\\sigma\\in C = \\{0,1\\}^N$ be a string of N independent bits, and let each bit be uniformly distributed in {0,1}. Then, the probability of observing a Hamming distance r between two configurations \u03c3 and \u03c3' is\n$P_0(r) = \\frac{1}{2^N} \\binom{N}{r}$\nGeometrically, the coordinates of uncorrelated random bits are points uniformly distributed in the N-dimensional configuration space C, and the intrinsic dimension d coincides with the number of variables, N. If one would measure the empirical distribution of the hamming distance, by following a procedure akin to the one used in ref.[19], one could infer the value of N by the shape of such a distribution. However, such a procedure would fail if the bits are correlated since the empirical distribution would not match the theoretical distribution in eq. 1 for any value of N. In the case of correlated spins, we assume that the dimension"}, {"title": "Results", "content": "is a smooth function of the distance r, and therefore make the following ansatz:\n$P(r) = C \\frac{1}{2^{d(r)}} \\binom{d(r)}{r}$\nwhere C is the normalization constant. Eq. (2) reduces to (1) for d(r) = N and C = 1. We empirically found that model (2) fits accurately the observed distributions, at least locally, if one retains the first two terms of the Taylor expansion:\n$d(r) = d_0 + d_1 r$\nwhere do and d\u2081 are variational parameters. In Supp. Inf. we show a Maximum Likelihood Estimation approach to infer a scale-dependent BID that supports our assumption of linear dependence (3).\nIn order to infer do and d\u2081 we minimize the Kullbac-Leibler divergence between the empirical probability of Hamming distances Pemp(r) and the model P(r) given by Eqs. (2) and (3),\n$D_{KL}(P_{emp}||P) = \\sum_{r<r^*} P_{emp}(r) \\log{\\frac{P_{emp}(r)}{P(r)}}$\nwhere r* is a meta-parameter that allows constraining the fit to small distances (see Supp. Inf.). As a representative measure of the BID of a system, we consider the limit for small distances of d(r), which qualitatively corresponds to the local number of degrees of freedom. According to Eq. (3), we simply have BID = do.\nTo benchmark our approach we use spin systems from statistical physics (summarized in Fig. 1). In those systems, the variables are spins which can take two values (up or down) and which, therefore, are binary variables. By choosing an interaction law between the spins, one can generate sets of binary variables correlated in highly non-trivial manners. The spins (or bits) are generated by sampling through standard Monte Carlo techniques[20] the Boltzmann distribution $P(\\sigma) \\propto \\exp(-\\beta E(\\sigma))$, being E the energy function encoding the interactions between spins and $\\beta = 1/T$ the inverse temperature.\nWe first show that our algorithm is able to compute intrinsic dimensions that scale linearly with size when the input data consists of statistically independent subsystems. We generated 2500 bit streams by concatenating B blocks of N = 104 bits, where each block is generated by independently harvesting spins from a Boltzmann distribution at T = 2, with an energy"}, {"title": "Results", "content": "N=1\nfunction $E(\\sigma) = \\sum_{i=1}^N \\sigma_i \\sigma_{i+1}$ with periodic boundary conditions (\u03c3\u03bd+1 = \u03c3\u2081). Table I shows that the BID per bit is approximately constant from 104 bits to 106 bits, consistent with the system being constructed by concatenating independent blocks. Remarkably, the approach allows estimating intrinsic dimensions of order 105 using only 2500 data points. This is empirical evidence that the estimator proposed in this work is not affected by the curse of dimensionality, and can be used in extremely high dimensional spaces.\nNext, we show that the BID is capable of capturing correlations between bits even when those span very large distances. We consider spins harvested from the Boltzmann distri-bution of the two-dimensional ferromagnetic Ising model on the square lattice (see Supp. Inf. for details). At high temperatures, spin-spin correlations are short-range, namely, they decay exponentially fast with the lattice distance. The characteristic scale of these correla-tions, the correlation length (T), is much smaller than the system size L = \u221a\u00d1, implying that the BID must be asymptotically proportional to the number of bits. This is observed in the curve corresponding to T = 4.0 in Fig. 1A. The value of the BID per bit is close to 1, reflecting the presence of small correlations. The same behavior is observed at T = 3, with a relative BID that is smaller, consistently with the fact that correlations become more important. At low temperatures, the system is globally ordered (namely the majority of bits are either 0 or 1) but the fluctuations are also short-range correlated[21]. Thus the BID is again proportional to the number of bits, but its value is much lower, reflecting the presence of a dominant strong correlation between the variables \"forcing\" most of the bits to align. Near the critical temperature (T = 2.3) the relative BID does not reach a plateau, indicating that the correlation length is comparable with the size of the system. This is a well-known phenomenon observed in spin systems at criticality[21], which is captured by the BID through the sub-linear scaling with the number of bits. Fig. 1D shows the BID"}, {"title": "Results", "content": "details). In order to calculate its BID we construct a binary system by writing the state (or color) index q in a binary representation. The correlation length in this model has a maximum at the transition temperature but remains finite even in the thermodynamic limit[23]. This behavior is correctly captured by the BID scaling being asymptotically linear for every temperature (Fig. 1B), even close to the transition temperature T*. The BID shows an abrupt change around T*, consistent with the underlying first-order thermal phase transition.\nFinally, we considered the Ising model on the triangular lattice with antiferromagnetic first-neighbor interactions. Due to geometrical frustration, this model does not have a thermal phase transition but presents a smooth crossover towards a ground state that has an entropy proportional to the number of spins in the system[24] and moreover presents a power-law decay of correlations[25]. Fig. 1C captures the long-range correlations of the ground-state through the sublinear scaling of the BID per bit, only present at small temperatures. Decreasing the temperature, the BID per spin tends smoothly to a non-zero value for all system sizes studied (Fig. 1F). The bottom panels in Fig. 1 illustrate the viability of the model defined in eq. Eqs 2 and 3 to describe the observed histograms of the distances between bit streams in all the conditions considered in the tests: the black lines, corresponding to the model of Eq. 2, fit pretty accurately the observations in all the conditions, even if at different temperatures the observations are concentrated at very different distances.\nThese benchmark examples show that BID is able to characterize the collective behavior of a large number of bits. Analogously, in computer vision, the collective behavior of a large number of pixels gives rise to patterns, which encode the meaning of an image. We computed the BID of internal representations of a convolutional image classifier, constructing binary data representations simply using the sign of the real-valued activations (see Supp. Inf. for details). The BID behaves as a proxy of semantic content, as we show through the following size-scaling experiment. The first row of Fig. 2 shows an example of different crops for one element of ImageNet, where each crop is resized to the fixed full-image resolution. Fig. 2 shows how the BID of the representations inside Resnet18 depends on the initial crop size, for different layers. For small crop sizes the BID increases monotonically, since on average the semantic content related to the classification task is being added. For large enough crop sizes, the BID saturates, indicating that all the relevant content of the image was"}, {"title": "Results", "content": "already present in previous sizes. Later layers saturate earlier since they carry information that is only relevant for the classification task and on average adding more context does not make significant changes. Indeed, if we consider a sufficiently small crop of a generic image, it is not possible to assign a label to it, since it does not carry enough information to make a discrimination. Instead, semantic information emerges for big enough crop sizes but saturates when the crop is so large that the extra pixels do not bring new information. In Fig. 2 we also show the same analysis performed on images constructed by an ensemble of random patches of the same data set. In this case, the semantic content is lost, even if the local structure in the images is preserved. Consequently, the BID does not plateau, since large images correspond to a collection of independent crops, each bringing the addition of independent information.\nAs a last example, we show that the BID can be used to detect correlations in the feature maps learned by large transformer-based architectures trained on a natural language pro-cessing (NLP) task. The ID has been used to analyze the properties of deep autoregressive transformers for protein sequence analysis and image analysis[26] and more recently for lan-guage [27, 28]. However, to make the ID estimation possible in Ref. [26] the representations were averaged in the sequence dimension, mixing information associated with the different tokens. In Refs. [27, 28], the analyses are performed only on the representation of the last token. We here use our approach to estimate the binary intrinsic dimension of the represen-tation of whole sentences, probing in this manner the collective properties of all the neurons of a layer. We consider paragraphs from Wikitext consisting of more than 300 tokens, and we study how the BID scales with text length, binarizing activations through the application of the sign function. The initial representation (layer 0, Fig. 3, left panel) has short-range correlations, signaled by the BID per bit slowly drifting up as a function of the number of tokens (Fig. 3). Token embeddings are correlated because they must be compliant with the grammar structure of natural language, in which words are often used in groups. For long sentences, the BID scales approximately linearly with the sentence length: grammar-related correlations become less relevant in parts of a sentence separated by hundreds of words. Instead, the BID per bit of the last-layer representation learned by the model displays a clear power-law decay as a function of the number of tokens, similar to the one observed in scale-free critical spin systems (see Fig. 1A). Since the semantic content in text is a collective property of the whole sentence, the network to capture this property develops a long-range"}, {"title": "Discussion", "content": "chosen sentences from the same corpus. We observe that the power law decrease stops precisely at the boundary between phrases, and it is followed by an approximate plateau, consistent with the two halves of the paragraph containing independent information. In the last last part of the plot, the BID per bit starts decreasing again, as a result of the semantic correlations of the second half of the sentence. In Supp. Inf. we show that these results hold also for changing both corpus and network.\nThese findings are aligned with pioneering studies that have suggested power-law correla-tions between characters in human text but were numerically limited due to the difficulty of computing mutual information in high dimensions[29, 30]. Moreover, the critical behavior of activations complies with the more recent observation of neural scaling laws[31] where the loss function of autoregressive transformers decreases as a power-law of the number of parameters. Also consistent with our results, a recent work observed a critical phase transi-tion in GPT-2, with the temperature parameter appears in the autoregressive sampling[32]. To make computations feasible, the authors restricted the representation space to that of Part-Of-Speech (POS) tags only.\nIn conclusion, we introduced an intrinsic dimension estimator specifically designed for binary data. To the best of our knowledge, the only other estimator for such variables was recently introduced in the context of formal concept analysis [33]. However, it has only been benchmarked on simple binary data tables, whereas our approach is tailored for large bit streams. We tested examples with up to 106 bits without observing systematic errors in the scaling of the estimation with system size.\nThe spin system benchmarks demonstrate that our estimator can characterize the global correlation structure of different phases of matter and identify the corresponding phase transitions. When applied to data representations from deep neural networks, the scaling of BID allows us to infer correlations for both image recognition and language modeling tasks. In the case of image classification, the BID behaves as a proxy of semantic content, since it initially increases with image-crop size after which it plateaus, indicating no additional information is gained from extra pixels. Instead, for images constructed out of independent random patches this plateau does not take place. For sentences, the correlations extend throughout the text and the BID of the last transformer layer follows a power law when computed using representations from a real corpus. In contrast, in an artificial corpus generated by concatenating real sentences, these correlations break, showing that the power"}, {"title": "Conclusion", "content": "law is inherent to the data and not an artifact of our estimator.\nIt is noteworthy that the last two datasets consist of real-valued features, while the BID estimator is designed for binary data. Each feature was converted into a binary variable by taking its sign. In Supp. Inf. we show that the trends observed in Fig. 3 hold when using two bits per feature instead of one. Specifically, the relative BID estimated for this higher-precision representation scales with the same power law. This suggests that when the intrinsic dimension becomes extremely large (around 1000 or more), its trends are insensitive to the precision used to represent individual features, and binning retains the essential information of the representation, as suggested in Refs. [13, 15].\nThe BID of data representations is clearly not restricted to either images or text, which are taken here as use cases where semantic correlations are easily interpretable. We expect our algorithm to be useful in the analysis of generic high dimensional spaces where global correlations are unknown, for example data coming from stock markets or brain activity. In perspective it would be interesting to explore its relationship with the Shannon entropy estimated by compression algorithms[34]."}, {"title": "Supplementary Information", "content": "The meta-parameter r* is computed as the quantile of order a* of the empirical probability\nof distances Pemp(r), i.e., \u03b1* = r=0 Pemp(r), and we typically select the value of a* that\ngives the lowest KL divergence (4). The minimization of (4) is performed stochastically,\nmeaning we propose small random moves from (do, d\u2081) to (do + do, d\u2081 + \u03b4\u2081) for suitable \u03b4\u03bf\nand d\u2081 and we take them if they decrease the loss function, or reject them otherwise. When\ncomparing BID's of a system computed with different model parameters (e.g., temperature,\nor size) we always do it at a shared value of a*. Occasionally the smallest sampled distances\nare noisy and poorly sampled, which may induce overflow in the numerical minimization\nof (4), especially for very large bit streams. In this cases we introduce a regularizer rmin\ndefined as the quantile of order amin of Pemp to remove distances with r < rmin. Table II\nsummarizes the values of these meta-parameters used in this work.\nAs an example we show additional optimization results for the 2D Ising model on the\nsquare lattice (Fig 1A,D,G from main text). Fig. 4 shows the thermal dependence of d\u2081,\nlog (KL) and r* for different system sizes L. Notably d\u2081 has a maximum around T. Even\nif the error is maximum at Te, the maximum of log (KL) is close to 10-7, implying that the\nmodel fits accurately the data, as shown in Fig. 1G. r* instead develops an inflexion point\naround Te."}, {"title": "Spin systems", "content": "For every two dimensional spin model we sampled 5000 samples at each temperature with\nsingly spin flip dynamics. We simulated each of the 5000 Markov chains independently in a\ndifferent core with a different seed, so they are by construction independent from each other.\nFor Ising models the algorithm used is Metropolis-Hastings, whereas for the Potts model we\nused the Heatbath algorithm[20]. For Table I we used 2500 samples."}, {"title": "Ising models", "content": "For all the Ising models simulated in this work, the energy of a given configuration of N\nspins \u03c3 = (\u03c31, ..., \u03c3\u03b7) \u2208 {\u22121,1} is\n$E(\\sigma) = -J \\sum_{(i,j)} \\sigma_i \\sigma_j$\nwith periodic boundary conditions, where (i, j) stands for first-neighbor interactions between\nspins i and jin the corresponding lattice. J Is the coupling constant between spins, fixed\nto +1 in the ferromagnetic models and -1 in the antiferromagnetic case."}, {"title": "Potts model", "content": "The energy of a given configuration \u03c3 = (\u03c31, ..., \u03c3\u03bd) \u2208 {0,1,..., q \u2013 1} is\n$E(\\sigma) =  \\sum_{(i,j)} \\delta_{\\sigma_i \\sigma_j}$"}, {"title": "The BID of image representations", "content": "We followed the experimental setup of [36] by computing separately the BID of Resnet18\nrepresentations from 8 highly populated classes of ImageNet ('vizsla', 'koala', 'Shih-Tzu',\n'Rhodesian_ridgeback', 'English_setter', 'cabbage_butterfly', 'Yorkshire_terrier'), where\nfor each class we have roughly 1300 images. We measure the BID after each Resnet block,\ni.e., after each skip connection. Resnets have ReLU activation functions, so only two possible\noutputs of each unit are possible: zero or greater than zero. Taking the sign of such an\noutput automatically constructs the binary system under the convention sign(0) = 0. The\npreprocessing of images was done following PyTorch official documentation[37]. The layer\nindexation was taken following the list of \"graph_node_names\", from the official PyTorch\ndocumentation[38]."}, {"title": "The BID of text representations", "content": "Pretrained LLMs OPT350m and Pythia450m and associated tokenizers where down-\nloaded from Hugginface[39], together with Wikitext and OpenWebtext corpora. Model ar-\nchitectures are available online. The activation function in the last fully connected layer of\nthe transformer block in Pythia is GeLU, whereas in OPT the corresponding layer is linear\n(i.e., no activation function). Pythia450m has layer_norm before and after the self attention\nlayer, whereas OPT350m has it between the attention block and the fully connected layers.\nRegardless of these details the binarization corresponds to take the sign of the activations\n(which in every layer have mean zero), and we see no qualitative difference when comparing\nresults from Pythia on Wikitext to OPT on OpenWebtext (Fig. 5). Discarding sentences\nfrom Wikitext with less than 400 Tokens we kept around 7500 samples."}, {"title": "Additional formulas", "content": "Ns\nL(niki, Pi) = II Binomial(ni|ki, Pi),\ni=1\nwhere Ns is the number of samples in the dataset. This equation is maximized to obtain\nthe Maximum Likelihood Estimator d. Doing this, in the limit of large number of samples,\nthe equation that defines d takes the form\n$\\frac{VA(d)}{VB(d)} =  \\frac{\\langle na\\rangle}{\\langle nb\\rangle},$\nwhere $\\langle na \\rangle =  \\frac{1}{N_s} \\sum_{i=1}^{Ns} n_{a,i}$, and the same for $\\langle nb \\rangle$. To work specifically with Ising spin\nconfigurations \u03c3\u2208 {\u22121,1}~ we depart from [19] using Hamming distances, which can be\ndefined as\n$D_H(\\sigma, \\sigma') = \\frac{N - \\sigma \\cdot \\sigma'}{2}$\nThe number of points at Hamming distance r from any Ising spin configuration in d\ndimensions is\n$V_H(r, d) =  \\sum_{r'=0}^{r} \\binom{d}{r'}$\nwhere r \u2208 [0,d]. At this point we have to choose specific sets A and B to estimate d. To\nfavor the constant density p and constant dimension d on both sets, one can fix A to be the\nset of points at exactly distance ra from the data point i, and B the set of points at distance\nra or rb = ra + 1. We denote this two sets as \"thin shells\". Given this choice, (11) has the\nclose form solution\n$\\frac{VA}{VB} =  \\frac{r_b}{d+1},$\nfor which the MLE for the ID is\n$d(r_A) =  \\binom{\\langle n_b \\rangle}{\\langle n_A \\rangle} - 1.$\nIn order to interpret better the last equation, we observe that $\\langle na \\rangle \\propto P(r = ra)$, where\nP(r) is the probability of observing hamming distance r between any two configurations in"}, {"title": "Additional formulas", "content": "$d(ra) \\approx (ra + 1) \\frac{P(r = ra) + P(r = ra + 1)}{P(r = ra)} - 1 = $\n$\\approx (ra + 1) (2+d, log (P(r)) |_{r=rA} - 1.$"}]}