{"title": "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "authors": ["Jerry Huang", "Prasanna Parthasarathi", "Mehdi Rezagholizadeh", "Sarath Chandar"], "abstract": "Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One noted issue is the high latency associated with auto-regressive generation, rendering large LLMs use dependent on advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger target model's generation, has helped alleviate this, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains provided the candidates are effective. Further results show this to hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.", "sections": [{"title": "Introduction", "content": "With the introduction of the Transformer (Vaswani et al., 2017) has emerged the era of large language models (Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2024) and the development of LLMs capable of reasoning and acting in astonishingly human-like manner (Kaplan et al., 2020; Wei et al., 2023; Ouyang et al., 2022). However, the use of resource intensive models and techniques remains a pre-requisite and accordingly, methods have been developed and applied to alleviate concerns relating to the practical usability of these models (Dettmers et al., 2022; Dao, 2024). One major area that has observed consistent improvement over time is the auto-regressive decoding aspect of text generation, where each generation of a new token requires a complete inference pass through the model, which under-utilizes the property of attention and the ability of modern accelerators (e.g. GPUs, TPUs) to parallelize computations (de Jong et al., 2022; Kim et al., 2023a).\nA growing approach towards addressing this is speculative decoding (Xia et al., 2023; Leviathan et al., 2023). In speculative decoding, latency is reduced by minimizing the amount of high-latency sequential computations and replacing them with cheaper ones. Rather than sampling directly from the larger model, the sampling is approximated with samples from a smaller and cheaper model through accept-reject sampling. Specifically, a small draft model auto-regressively generates text which is then verified by a larger target model in parallel (Stern et al., 2018; Sun et al., 2021). Thus the large model does not need to generate text repeatedly but rather guides the small model by correcting outputs when it is truly incapable. This can reduce the number of calls to the large LLM, saving both time and memory. However two models are required, along with some similarity in their generative abilities in order for this method to see signficant speedups. While approaches exist to circumvent some of these needs (Yang et al., 2023; Zhang et al., 2023; Li et al., 2024; Cai et al., 2024; Hooper et al., 2024), these are often limited by the need for additional tuning (Liu et al., 2024b; Cai et al., 2024; Li et al., 2024), which is difficult in resource constrained settings, or quality degradation in generations (Kim et al., 2023b). Because of the evident size-cost tradeoff, this is very efficient if the draft model is well aligned to the target."}, {"title": "Methodology", "content": "However, while one can ensure that the final output follows the target distribution (Chen et al., 2023), selecting an inadequate draft model can lead to a lack of acceleration due to the signficant number of rejections that will occur. While other methods allow for changes in the output distribution shift (Kim et al., 2023a; Zhou et al., 2024; Fu et al., 2024) to further speed-up inference, such types of shifts can be problematic in many high-risk scenarios. From this perspective, the presence of multiple draft models, each suited for different settings, can be helpful for inference acceleration without degredations in generation quality. By dynamically choosing a draft model, speedups can be achieved on multiple domains with marginal additional costs. However this requires learning how to choose the best draft option given a context, introducing a decision making problem which needs to be solved.\nSo how can this decision making process be learned? We start by observing this as a contextual bandits (Woodroofe, 1979; Auer, 2003), where the goal is to have a policy select a draft model based on a given query. This also requires rewards from which the policy can learn to estimate and compare the ideality of different actions that can be taken. To this end, we use an offline process to estimate the contextual alignment between different draft models and the target model on a set of training examples (Figure 1), enabling the construction of a dataset that defines the preference a target can have towards specific draft candidates. This enables us to train a policy that can take into account such preferences without knowing further details about the draft models. By deloying this policy at inference time it becomes possible to weigh these preferences, leading to speedups in generation with the target. We further show that this policy is useful when using self-speculative decoding, whereby the draft model is a subset of the target model parameters. To summarize our contributions:\n\u2022 We frame a speculative decoding scenario as a contextual bandits problem, where multiple draft models serve as arms that each produce a reward, an abstraction of the inference speed-up relative to using the target model on its own which is not known a priori.\n\u2022 We demonstrate that offline training of a decision making agent through only the similarity between the draft and target model generations, the agent can correctly select which draft model to use for a given input query.\n\u2022 We show that the policy can balance tradeoffs in draft model alignment and generation speed by incorporating explicit information about the model within the reward."}, {"title": "Methodology", "content": ""}, {"title": "Motivation", "content": "Assume a large target model, Me, incurs large end-to-end latencies that one wants to avoid. Speculative decoding aims to solve the latency issue by using a draft model to approximate the target model. However, as previously discussed, the draft model must be similar to the target model otherwise the sampling distribution is too different and produce no speedups. Therefore, while draft models can"}, {"title": "Problem Formulation", "content": "When presented with a query q, selecting a draft model among multiple unique candidates can lead to varying performance based on the chosen option. From a contextual bandits lens, q is a context for which there are k arms that each returns an independent reward r. Each of arm corresponds to a different drafter whose reward is the time it takes to generate the output sequence through speculative decoding. Accordingly, each arm can produce a different reward for each q. The objective then consists of learning a policy \u03c0(\u00b7|q) which, for any given context q, can select among the arm which can produce the greatest reward. From a speculative decoding scenario, the goal is to select the draft model whose abilities best align with the target for that given query, as this will minimize the number of times the target model must be invoked.\nRandomly choosing a draft model risks signficant increases in latency, therefore learning to make the correct decision in a sample efficient manner is important. While the ideal reward is the real/observed speed-up, this can be expensive if the aligment with draft models is unknown. As such, a cheaper proxy may be necessary. However, two factors have a direct effect on the true reward: 1) the alignment between target and drafter and 2) the size of the drafter. This provides an alternative way to collect policy training data: use the draft models auto-regressively and compute alignment scores with the target outputs, then adjust these based on the size of the drafter. Next, we describe how we collect our data to train a policy offline."}, {"title": "Offline Data Collection", "content": "Given a set of queries Q = {qi}_{i=1}^{k}, we produce outputs based on each qi for the target model, , as well as each of the candidate drafters, {0}_{j=1}^{k}. We then use a similarity metric to compute scores for each candidate output\n$$s_{j}^{i} = f(o_i^j, o_i)$$"}, {"title": "Decision Making", "content": "With the offline dataset, it becomes possible to train a policy \u03c0 which can independently act on a context by choosing a drafter to use with the target. We consider each (qi, j, s) as state-action-reward tuples used to train \u03c0.\nWithin the contextual bandits reformulation, each query-action pair (qt, at) \u2208 Q \u00d7 A(qt) is the drafter which produced an observed reward r(qt, at). Here, we use the score  directly as the reward, as it acts as an estimate for the effectiveness of drafter j on the context. The policy is represented by a mapping \u03c0\u03b8(\u03b1|q) from Q \u00d7 A to R and we want to find policy parameters \u03b8* that maximize\n$$J = E_{q~P_q(\\cdot), a~\\pi_{\\theta}(\\cdot | q)}[r(q, a)]$$\nwhere Pq is the sampling distribution of the context. As the action space is discrete, integrating over the action space is equivalent to a summation and therefore\n$$\\int_{a \\in A(q)} \\pi_{\\theta}(a | q) da = \\sum_{a\\in A(q)} \\pi_{\\theta}(a | q) = 1$$\nand the gradient with respect to the policy is\n$$\\nabla_{\\theta} J_{\\theta} = E_{q~P_q(\\cdot), a~\\pi_{\\theta}(\\cdot | q)}[\\nabla log \\pi_{\\theta}(a | q) r(q, a)]$$\nwhich is equivalent to the REINFORCE (Williams, 2004) policy gradients method and we therefore use it to train our policy."}, {"title": "Experimental Results", "content": ""}, {"title": "Experimental Setup", "content": "Models and Tasks. We select publicly available LLMs to use for our experiments. We conduct a number of experiments, which we motivate by varying the draft options along different axes such as alignment with the target model, sizes of the draft models, architecture of the drafter/target and the level of independence between the draft and target models. Each of these forms a dedicated experiment detailed in the sections that follow.\nData Collection. For each experiment, we collect offline data using task-specific training dataset splits. Each model is used to generate a greedy decoded sample from each, which is used to construct a reward dataset. To score samples against the target model output, we use the ROUGE-L score.\nPolicy Training. To train our policy, the input is a sentence embedding of the query from the target model and the output is a distribution over the drafting candidates. We train on the offline dataset for 3 epochs using a fixed batch size of 64 and AdamW (Loshchilov and Hutter, 2019) with a learning rate of le-3 and weight decay le-2. All other hyperparameters are set to their default values in PyTorch. In all experiments, our policy consists of a 3 layer multi-layer perceptron. Hidden layers have a fixed dimensions of 512 with a tanh activation function. The input dimension is the hidden dimension size of the target model and the output size is the number of drafting options.\nInference. We sample using a temperature T of 1 and draft tokens y set at 7. We use both a policy that takes the greedy action and another that samples from the output distribution. The policy takes in a sentence embedding of the query and returns a distribution, from which a drafter is sampled and used to assist decoding for the specific query."}, {"title": "Results", "content": ""}, {"title": "Learning to choose the draft model", "content": "For our first experiment, we use a T5 (Raffel et al., 2020) encoder-decoder models. As the target, we use an instruction-finetuned (Wei et al., 2022) Flan-T5-XXL (Chung et al., 2022) while our draft candidates are publicly available T5-Small models, one the base version and another fine-tuned on text summarization. We evaluate on translation (IWSLT2017 EN-DE (Cettolo et al., 2017)) and text summarization (XSUM (Narayan et al., 2018)).\nThis highlights some immediate benefits of policy use, namely that it can identify the correct draft model for a context without any explicit information regarding the draft candidates themselves. Rather, generating sampling outputs from each draft model and the target individually is sufficient to develop a general ability to differentiate between domains through the use of the computed rewards."}, {"title": "Balancing quality and speed", "content": "It is also important that the draft model is sufficiently inexpensive to use relative to the target model. This motivates our second experiment, which is evaluated only on XSUM, but compares draft candidates that vary in terms of size and target model alignment. Multiple draft models are compared: a Flan-T5-Small (80M parameters), the same T5-Small (60M) models mentioned above, and Flan-T5-Base (220M).\nBalancing alignment and efficiency is therefore an issue to consider when deciding between candidates. This demonstrates the general flexibility that can come with using such a weighting scheme of different rewards, while demonstrating that even simpler proxies for the inference penalty are sufficient to properly balance the two."}, {"title": "How many examples need to be learned to differentiate?", "content": "It is further necessary to consider the number of examples that are needed for the decision maker to properly learn to differentiate between different examples. To this end, we investigate how quickly the policy can learn to use the annotated scores within the offline dataset to demonstrate a visible speed-up improvement. We re-use our models from the first experiment, but keep track of the decoding speed as the number of examples used to train our policy \u03c0\u03b8 increases.\nAs we can observe in Figure 3, learning to select the correct model occurs rather quickly, as training for fewer than a total of 10000 examples is sufficient to attain a level of performance that is equivalent to training on the entire offline dataset, which consists of nearly 400 thousand examples. This result demonstrates the general efficiency of this method, as collecting and training the policy on outputs from a minimal amount of examples shows the ability to generalize quite strongly."}, {"title": "Auto-regressive generation as an option", "content": "Scenarios exist where the draft models will not be useful, in which case using the target auto-regressively remains the most reasonable option.\nTo this end, we attempt to observe how providing this option to the decision maker can affect our previous experiments. We repeat the same experiment from Table 1 but allow our policy to learn to choose to generate auto-regressively. To avoid trivially perfect matching of outputs, we sample outputs from the target model and score against the greedy output. Due to the large size of the target compared to the drafters, we use a 0.5 to balance the size and quality scores."}, {"title": "Generalization to Multi-Task Drafters", "content": "To demonstrate the applicability of this method to more general settings, in particular cases where the draft models may be competent at multiple tasks, we further apply our policy-based selection method to SpecBench (Xia et al., 2024) using a Vicuna-33B (Chiang et al., 2023) target with smaller draft models. Given the size of SpecBench (480 examples, divided equally into 6 tasks), we use this exclusively as a test-set. To train our policy, we use the original task datasets from which SpecBench examples were extracted and sample even amounts of examples from each (2000). For MT-BENCH, there are only 80 total examples which are all included in the test set but which we sample with replacement to use for a training set. Accordingly, results on this task may be over-confident. Because Vicuna models are decoder-only Transformers, we adjust the sentence representation to be the final hidden representation of the input sequence. Our results show that our initial findings from a T5 architecture hold, suggesting that such a policy-based training method is both robust and generalizable to different settings."}, {"title": "Ablation with self-drafting", "content": "Despite the benefits of assisted decoding, drafting relies on the availability of small draft models that\n(1) Share a vocabulary with the target.\n(2) Align with the target on the tasks of interest.\nSuch models can be difficult to obtain, leading to self-drafting (Yang et al., 2023; Li et al., 2024; Hooper et al., 2024), where the draft model exists within the target. To explore the differences with this setting, we conduct an additional ablation.\nWe use a LLAMA-2-13B-Chat model (Touvron et al., 2023) using early exits, following (Kavehzadeh et al., 2024) with the use of a single language modeling head for all exits. While other methods exist, these can possess a combinatorial number of potential draft options and necessitate pre-determined path flows during inference (Zhang et al., 2023). Meanwhile, methods that use additional language modeling heads for parallel decoding require additional parameters which can both become irreconcilable with resource constraints or degrade generation quality (Cai et al., 2024).\nResults on ALPACA and TRIVIAQA, conducted on each dataset independently, under this setup (Table 6) show that although intermediate layer drafting results in a decrease in decoding speed, using a policy can minimize performance loss in particular with the presence of an auto-regressive option, highlighting that the proposed offline policy learning approach has potential for self-drafting as well. This demonstrates the use of a policy remains a useful manner to fall back to the most effective decoding options. Furthermore, when considering the case where the auto-regressive option is not available, we note that the policy methods are capable of recovering to a performance similar to the best case intermediate drafter on ALPACA. While this is not the case with TRIVIAQA, this is perhaps attributed to the short answers within this dataset."}, {"title": "Discussion", "content": "LLM Routing. LLMs have demonstrated remarkable capabilities across a range of tasks, but there exists wide variation in their costs and capabilities. Very broadly, more capable models tend to be more expensive than less capable models. This leads to a dilemma when deploying LLMs in the real-world - routing all queries to the largest, most capable model leads to the highest-quality responses but can be expensive, while routing queries to smaller models can save costs but may result in lower-quality responses. Similarly, not all models may be well suited for the same set of tasks, meaning that routing to the most suitable model can be of great importance as well.\nOur work shares a great deal of similarity with this notion of model routing, or selecting the best model based on the query. In particular, the set of draft models can be considered to be a group of sub-networks, similar to a Mixture-of-Experts (MoE) (Shazeer et al., 2017) style paradigm. The policy meanwhile acts as a router to the correct sub-network. More advanced routing techniques (Fedus et al., 2021; Ong et al., 2024) have been explored as a way to leverage the multitude of LLMs that exist in the wild, but have yet to be widely used within downstream settings such as speculative decoding."}, {"title": "Adaptive Speculative Decoding", "content": "Speculative decoding methods require the use of many pre-defined hyper-parameters which can signficantly influence acceleration, with even minor changes having noticable effects. Recent work has begun to explore how to decouple this process, such as by dynamically selecting the number of drafting tokens to generate at each decoding step (Wang et al., 2024; Liu et al., 2024a). Kavehzadeh et al. (2024) further discussed dynamically selecting a model per instance, however their method is limited to their specific setup due to needing to compute confidence scores after generation at early exits.\nWhile we do not introduce a new decoding algorithm, we make a first attempt to make the speculative decoding adaptive through the ability to switch between multiple draft models based on the input. However, more complex levels of adaptivity may be necessary as each decoding step may not be the same, necessitating perhaps a need to carefully adjust different hyperparameters through the process in order to maximize acceleration."}, {"title": "Decision Making for Assisted Decoding", "content": "Assisted decoding can require making multiple decisions. One of these is determining an ideal number of draft tokens to decode at each step. Another relates to how to reject tokens, which commonly uses either greedy (Xia et al., 2023) or sampling-based token-matching heuristics (Leviathan et al., 2023). However, there are trade-offs when enforcing specific choices, which requires further investigation to better understand how to tune such techniques.\nThis work proposes adding an additional decision at the beginning of the decoding process, namely at the beginning of the process under the assumption that multiple drafting options exist. While we limit ourselves to make a more complete analysis within a more self-contained setting, various ways to have these methods co-exist within one larger pipeline are possible. However such work is left for future exploration due to the non-trivial nature of understanding how different choices and effect overall reported results in conjunction."}, {"title": "Measuring Alignment Between Outputs", "content": "We observe that token-level similarity scores are effective for training the decision maker, which can be attributed to the fact that assisted decoding itself relies on matching the token-level distribution of outputs. As such, if the greedy-decoded output from a draft model highly resembles the target output, it follows that this will be represented by a higher degree of similarity between the probabilty distributions in the logit space, which can then lead to fewer rejections when sampling.\nHowever, such metrics have limitations (Deutsch et al., 2022) due to capturing primarily superficial elements of text, where marginal differences in distribution have large effects on the output text. Furthermore, different metrics may overfit specific tasks, necessitating the need for better measures of draft/target alignment, which can hopefully lead to better estimation of rewards for training improved policies, either by desigining better metrics themselves or by learning to compare features at different levels of granularity (ex. target and draft logits against text outputs). Additionally, semantic meaning also can play an important role, as outputs with signficant structure may still possess the same meaning, something that token-level similarity metrics will not adequately capture."}, {"title": "Speculative Decoding as Approximate Inference", "content": "Speculative decoding can be analogized as a form of approximate inference where due to the intractability of performing inference with a model of interest, approximation methods are used to learn an estimate of the model. While training the draft model is equivalent to performing variational inference (i.e. approximating an intractable distribution with a surrogate), this can be expensive. Accordingly, training only a policy can be seen as weighing a set of fixed distributions to act as a better surrogate for the target model.\nSome works have further attempted to study speculative decoding from this angle. In particular, Zhou et al. (2024) explore such a process by building a draft model through the use of KL-divergence losses, effectively building a posterior distribution of the target model based on likelihood information from the draft output. Liu et al. (2024b) meanwhile explore the same technique as the distribution of examples changes, building a draft model that can adapt to changing user inputs. Such settings also could perhaps benefit from multiple draft models, where conditioning on the query can enable more effective adaptation of draft models to better generalize to unseen settings."}, {"title": "Conclusion", "content": "This work presents the first work at attempting to integrate assisted generation within a setting where multiple black-box draft candidates exist. When no a-priori knowledge of which draft candidate is best suited for assisting the decoding of a given example, the problem can be modeled as a contextual bandits problem, where the goal is to estimate the unknown reward from each drafting option. Our work demonstrates that offline RL presents an efficient method for learning to distinguish the available options and provide accelerated decoding across various examples within this setting, with a logical way to collect offline data from models for learning. Our results and ablations show that learning a policy with this approach can adapt to general preferences while accounting for more complex aspects of the decision making, highlighting its robustness. Furthermore, such a method is scalable and robust to the introduction of more draft models or the removal of draft models, presenting a viable alternative to settings where a uniquely superior draft model may be unavailable.\nNevertheless, areas of further development exist. For example, learning in an online fashion may render this method more broadly applicable. Alternatively, exploring how to dynamically choose drafters at every decoding step rather than per example, as well as combining this direction of work with that which attempts to adaptively choose the speculation length at every step, are feasible ways of combining our findings with concurrent work in the hopes of reaping the benefits of all methods."}, {"title": "Limitations", "content": "This work has a few limitations which define the scope of future work."}, {"title": "Choice of draft models and data domains", "content": "Results may stem from the distinct boundaries that exist between domains/tasks. In settings where such boundaries are not well defined, outcomes may differ. However technical limitations and the absence of sufficent pre-trained models for comparision makes this difficult to explore immediately."}, {"title": "Additional storage and memory", "content": "The usage of multiple models that draft independently requires additional memory, which can be be more difficult to manage when there are explicit constraints on this front (self-drafting avoids this due to the use of a single model). Furthermore, collecting an offline dataset can be difficult in some specific scenarios where inference is burdensome, for example when input/output sequences are very long, or when many offline examples are required."}, {"title": "Self-Drafting", "content": "We work on a setting where we do not conduct any additional training of parameters that are explicitly linked to the language model itself, whether they are existing parameters or new paramters added as a result of the method. While there are ways in which our explored method can be applied to these as well, computational limitations make it difficult to rigorously conduct such studies at the moment and we leave it to future work for this reason."}, {"title": "Ethics Statement", "content": "This paper discusses the concept of dynamically choosing between of multiple black-box draft models for speculative decoding, proposing an offline reinforcement learning approach for adaptively selecting a good draft model for assistance. Our results are relate to the decoding speed of models, which is unlikely to lead to ethical concerns or problematic interpretations of such results."}, {"title": "Accept Rate Computation", "content": "To compute the accept rate of tokens, we define the number of generated tokens in a given draft as the total number of tokens generated by the draft model (this is equivalent to \u03b3). The number of accepted tokens in a given draft is the number of generated tokens that are validated as correct by the target model. When a token is rejected within a draft, all subsequent tokens are considered rejected as well. The accept rate is then the quotient of the total number of accepted tokens divided by the total number of generated tokens."}, {"title": "Computing Wall-Clock Performance", "content": "To compute the wall-clock time when using a policy, we include the amount of time used to infer on the policy. However, we do not include the time needed to generate the sentence representation.\nThis is because upon generating the original sentence representation, the large model's KV cache can be updated to store these for the future verification passes, meaning that they do not need to be recomputed again in the future. As such, we treat this initial pass through as being part of the first verification pass.\nAdditionally, one could theoretically save on the policy inference by performing batched inference on many examples at once. However, this is not particularly applicable in practice, where different inputs arrive at different times. As such, we treat each example individually and include these times within the per-example speeds."}, {"title": "Cost Function", "content": "When considering multiple draft candidates, we use the following simple function for generating fixed costs for the different models. Suppose the candidates have P = {p1, p2, ...,pk} parameters. Then the cost for the models are\n$$C_i = 1 - \\frac{e^{p_i}}{\\sum_j e^{p_j}}$$\nwhere j = argmax pi.\ni"}]}