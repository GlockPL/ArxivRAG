{"title": "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "authors": ["Jerry Huang", "Prasanna Parthasarathi", "Mehdi Rezagholizadeh", "Sarath Chandar"], "abstract": "Despite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever growing sizes only increasing the barrier for use. One noted issue is the high latency associated with auto-regressive generation, rendering large LLMs use dependent on advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger target model's generation, has helped alleviate this, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains provided the candidates are effective. Further results show this to hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.", "sections": [{"title": "1 Introduction", "content": "With the introduction of the Transformer (Vaswani et al., 2017) has emerged the era of large language models (Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2024) and the development of LLMs capable of reasoning and acting in astonishingly human-like manner (Kaplan et al., 2020; Wei et al., 2023; Ouyang et al., 2022). However, the use of resource intensive models and techniques remains a pre-requisite and accordingly, methods have been developed and applied to alleviate concerns relating to the practical usability of these models (Dettmers et al., 2022; Dao, 2024). One major area that has observed consistent improvement over time is the auto-regressive decoding aspect of text generation, where each generation of a new token requires a complete inference pass through the model, which under-utilizes the property of attention and the ability of modern accelerators (e.g. GPUs, TPUs) to parallelize computations (de Jong et al., 2022; Kim et al., 2023a).\nA growing approach towards addressing this is speculative decoding (Xia et al., 2023; Leviathan et al., 2023). In speculative decoding, latency is reduced by minimizing the amount of high-latency sequential computations and replacing them with cheaper ones. Rather than sampling directly from the larger model, the sampling is approximated with samples from a smaller and cheaper model through accept-reject sampling. Specifically, a small draft model auto-regressively generates text which is then verified by a larger target model in parallel (Stern et al., 2018; Sun et al., 2021). Thus the large model does not need to generate text repeatedly but rather guides the small model by correcting outputs when it is truly incapable. This can reduce the number of calls to the large LLM, saving both time and memory. However two models are required, along with some similarity in their generative abilities in order for this method to see signficant speedups. While approaches exist to circumvent some of these needs (Yang et al., 2023; Zhang et al., 2023; Li et al., 2024; Cai et al., 2024; Hooper et al., 2024), these are often limited by the need for additional tuning (Liu et al., 2024b; Cai et al., 2024; Li et al., 2024), which is difficult in resource constrained settings, or quality degrada- tion in generations (Kim et al., 2023b). Because of the evident size-cost tradeoff, this is very efficient if the draft model is well aligned to the target."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Motivation", "content": "Assume a large target model, $M_e$, incurs large end-to-end latencies that one wants to avoid. Speculative decoding aims to solve the latency issue by using a draft model to approximate the target model. However, as previously discussed, the draft model must be similar to the target model otherwise the sampling distribution is too different and produce no speedups. Therefore, while draft models can help, they are only reliable when their knowledge distribution resembles that of the target. Accordingly, using only one draft model may not serve well in general if the target has multiple expertises. But by dynamically choosing between different draft models in any given scenario, then benefits from each draft model can be observed as long as the decision maker is competent and efficient."}, {"title": "2.2 Problem Formulation", "content": "When presented with a query q, selecting a draft model among multiple unique candidates can lead to varying performance based on the chosen option.\nFrom a contextual bandits lens, q is a context for which there are k arms that each returns an independent reward r. Each of arm corresponds to a different drafter whose reward is the time it takes to generate the output sequence through speculative decoding. Accordingly, each arm can produce a different reward for each q. The objective then consists of learning a policy $\\pi(\\cdot|q)$ which, for any given context q, can select among the arm which can produce the greatest reward. From a speculative decoding scenario, the goal is to select the draft model whose abilities best align with the target for that given query, as this will minimize the number of times the target model must be invoked.\nRandomly choosing a draft model risks signficant increases in latency, therefore learning to make the correct decision in a sample efficient manner is important. While the ideal reward is the real/observed speed-up, this can be expensive if the aligment with draft models is unknown. As such, a cheaper proxy may be necessary. However, two factors have a direct effect on the true reward: 1) the alignment between target and drafter and 2) the size of the drafter. This provides an alternative way to collect policy training data: use the draft models auto-regressively and compute alignment scores with the target outputs, then adjust these based on the size of the drafter. Next, we describe how we collect our data to train a policy offline."}, {"title": "2.3 Offline Data Collection", "content": "Given a set of queries $Q = \\{q_i\\}_{i=1}^k$, we produce outputs based on each $q_i$ for the target model, $o_i$, as well as each of the candidate drafters, $\\{o_i^j\\}_{j=1}^k$. We then use a similarity metric to compute scores for each candidate output\n$s_i^j = f(o_i^j, o_i)$"}, {"title": "2.4 Decision Making", "content": "With the offline dataset, it becomes possible to train a policy $\\pi$ which can independently act on a context by choosing a drafter to use with the target. We consider each $(q_i, j, s_i^j)$ as state-action-reward tuples used to train $\\pi$.\nWithin the contextual bandits reformulation, each query-action pair $(q_t, a_t) \\in Q \\times A(q_t)$ is the drafter which produced an observed reward $r(q_t, a_t)$. Here, we use the score $s_i^j$ directly as the reward, as it acts as an estimate for the effectiveness of drafter j on the context. The policy is represented by a mapping $\\pi_\\theta(a|q)$ from Q \u00d7 A to R and we want to find policy parameters $\\theta^*$ that maximize\n$J = E_{q\\sim P_q(\\cdot), a \\sim \\pi_\\theta(\\cdot|q)}[r(q, a)]$\nwhere $P_q$ is the sampling distribution of the context. As the action space is discrete, integrating over the action space is equivalent to a summation and therefore\n$\\int_\\tau \\pi_\\theta(a|q)da = \\sum_{a\\in A(q)} \\pi_\\theta(a|q) = 1$\nand the gradient with respect to the policy is\n$\\nabla_\\theta J_\\theta = E_{q\\sim P_q(\\cdot), a \\sim \\pi_\\theta(\\cdot|q)}[\\nabla log \\pi_\\theta(a|q)r(q, a)]$\nwhich is equivalent to the REINFORCE (Williams, 2004) policy gradients method and we therefore use it to train our policy."}, {"title": "3 Experimental Results", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Models and Tasks. We select publicly available LLMs to use for our experiments. We conduct a number of experiments, which we motivate by varying the draft options along different axes such as alignment with the target model, sizes of the draft models, architecture of the drafter/target and the level of independence between the draft and target models. Each of these forms a dedicated experiment detailed in the sections that follow.\nData Collection. For each experiment, we collect offline data using task-specific training dataset splits. Each model is used to generate a greedy decoded sample from each, which is used to construct a reward dataset. To score samples against the target model output, we use the ROUGE-L score.\nPolicy Training. To train our policy, the input is a sentence embedding of the query from the target model and the output is a distribution over the drafting candidates. We train on the offline dataset for 3 epochs using a fixed batch size of 64 and AdamW (Loshchilov and Hutter, 2019) with a learning rate of le-3 and weight decay le-2. All other hyperparameters are set to their default values in PyTorch. In all experiments, our policy consists of a 3 layer multi-layer perceptron. Hidden layers have a fixed dimensions of 512 with a tanh activation function. The input dimension is the hidden dimension size of the target model and the output size is the number of drafting options.\nInference. We sample using a temperature T of 1 and draft tokens $\\gamma$ set at 7. We use both a policy that takes the greedy action and another that samples from the output distribution."}, {"title": "3.2 Results", "content": "Learning to choose the draft model. For our first experiment, we use a T5 (Raffel et al., 2020) encoder-decoder models. As the target, we use an instruction-finetuned (Wei et al., 2022) Flan-T5-XXL (Chung et al., 2022) while our draft candidates are publicly available T5-Small models, one the base version and another fine-tuned on text summarization. We evaluate on translation (IWSLT2017 EN-DE (Cettolo et al., 2017)) and text summarization (XSUM (Narayan et al., 2018)).\nBalancing quality and speed. It is also important that the draft model is sufficiently inexpensive to use relative to the target model. This motivates our second experiment, which is evaluated only on XSUM, but compares draft candidates that vary in terms of size and target model alignment. Multiple draft models are compared: a Flan-T5-Small (80M parameters), the same T5-Small (60M) models mentioned above, and Flan-T5-Base (220M).\nHow many examples need to be learned to differentiate? It is further necessary to consider the number of examples that are needed for the decision maker to properly learn to differentiate between different examples. To this end, we investigate how quickly the policy can learn to use the annotated scores within the offline dataset to demonstrate a visible speed-up improvement. We re-use our models from the first experiment, but keep track of the decoding speed as the number of examples used to train our policy $\\pi_\\theta$ increases.\nAuto-regressive generation as an option. Scenarios exist where the draft models will not be useful, in which case using the target auto-regressively remains the most reasonable option.\nTo this end, we attempt to observe how providing this option to the decision maker can affect our previous experiments. We repeat the same experiment from Table 1 but allow our policy to learn to choose to generate auto-regressively. To avoid trivially perfect matching of outputs, we sample outputs from the target model and score against the greedy output. Due to the large size of the target compared to the drafters, we use a 0.5 to balance the size and quality scores.\nGeneralization to Multi-Task Drafters. To demonstrate the applicability of this method to more general settings, in particular cases where the draft models may be competent at multiple tasks, we further apply our policy-based selection method to SpecBench (Xia et al., 2024) using a Vicuna-33B (Chiang et al., 2023) target with smaller draft models.\nAblation with self-drafting. Despite the benefits of assisted decoding, drafting relies on the availability of small draft models that"}, {"title": "4 Discussion", "content": "LLM Routing. LLMs have demonstrated remarkable capabilities across a range of tasks, but there exists wide variation in their costs and capabilities. Very broadly, more capable models tend to be more expensive than less capable models. This leads to a dilemma when deploying LLMs in the real-world - routing all queries to the largest, most capable model leads to the highest-quality responses but can be expensive, while routing queries to smaller models can save costs but may result in lower-quality responses. Similarly, not all models may be well suited for the same set of tasks, meaning that routing to the most suitable model can be of great importance as well.\nOur work shares a great deal of similarity with this notion of model routing, or selecting the best model based on the query. In particular, the set of draft models can be considered to be a group of sub-networks, similar to a Mixture-of-Experts (MoE) (Shazeer et al., 2017) style paradigm. The policy meanwhile acts as a router to the correct sub-network. More advanced routing techniques (Fedus et al., 2021; Ong et al., 2024) have been explored as a way to leverage the multitude of LLMs that exist in the wild, but have yet to be widely used within downstream settings such as speculative decoding.\nAdaptive Speculative Decoding. Speculative decoding methods require the use of many pre-defined hyper-parameters which can signficantly influence acceleration, with even minor changes having noticable effects. Recent work has begun to explore how to decouple this process, such as by dynamically selecting the number of drafting tokens to generate at each decoding step (Wang et al., 2024; Liu et al., 2024a). Kavehzadeh et al. (2024) further discussed dynamically selecting a model per instance, however their method is limited to their specific setup due to needing to compute confidence scores after generation at early exits.\nWhile we do not introduce a new decoding algorithm, we make a first attempt to make the speculative decoding adaptive through the ability to switch between multiple draft models based on the input. However, more complex levels of adaptivity may be necessary as each decoding step may not be the same, necessitating perhaps a need to carefully adjust different hyperparameters through the process in order to maximize acceleration.\nDecision Making for Assisted Decoding. Assisted decoding can require making multiple decisions. One of these is determining an ideal number of draft tokens to decode at each step. Another relates to how to reject tokens, which commonly uses either greedy (Xia et al., 2023) or sampling-based token-matching heuristics (Leviathan et al., 2023). However, there are trade-offs when enforcing specific choices, which requires further investigation to better understand how to tune such techniques.\nThis work proposes adding an additional decision at the beginning of the decoding process, namely at the beginning of the process under the assumption that multiple drafting options exist. While we limit ourselves to make a more complete analysis within a more self-contained setting, various ways to have these methods co-exist within one larger pipeline are possible. However such work is left for future exploration due to the non-trivial nature of understanding how different choices and effect overall reported results in conjunction.\nMeasuring Alignment Between Outputs. We observe that token-level similarity scores are effective for training the decision maker, which can be attributed to the fact that assisted decoding itself relies on matching the token-level distribution of outputs. As such, if the greedy-decoded output from a draft model highly resembles the target output, it follows that this will be represented by a higher degree of similarity between the probabilty distributions in the logit space, which can then lead to fewer rejections when sampling.\nHowever, such metrics have limitations (Deutsch et al., 2022) due to capturing primarily superficial elements of text, where marginal differences in distribution have large effects on the output text. Furthermore, different metrics may overfit specific tasks, necessitating the need for better measures of draft/target alignment, which can hopefully lead to better estimation of rewards for training improved policies, either by desigining better metrics themselves or by learning to compare features at different levels of granularity (ex. target and draft logits against text outputs). Additionally, semantic meaning also can play an important role, as outputs with signficant structure may still possess the same meaning, something that token-level similarity metrics will not adequately capture.\nSpeculative Decoding as Approximate Inference. Speculative decoding can be analogized as a form of approximate inference where due to the intractability of performing inference with a model of interest, approximation methods are used to learn an estimate of the model. While training the draft model is equivalent to performing variational inference (i.e. approximating an intractable distribution with a surrogate), this can be expensive. Accordingly, training only a policy can be seen as weighing a set of fixed distributions to act as a better surrogate for the target model.\nSome works have further attempted to study speculative decoding from this angle. In particular, Zhou et al. (2024) explore such a process by building a draft model through the use of KL-divergence losses, effectively building a posterior distribution of the target model based on likelihood information from the draft output. Liu et al. (2024b) meanwhile explore the same technique as the distribution of examples changes, building a draft model that can adapt to changing user inputs. Such settings also could perhaps benefit from multiple draft models, where conditioning on the query can enable more effective adaptation of draft models to better generalize to unseen settings."}, {"title": "5 Conclusion", "content": "This work presents the first work at attempting to integrate assisted generation within a setting where multiple black-box draft candidates exist. When no a-priori knowledge of which draft candidate is best suited for assisting the decoding of a given example, the problem can be modeled as a contextual bandits problem, where the goal is to estimate the unknown reward from each drafting option. Our work demonstrates that offline RL presents an efficient method for learning to distinguish the available options and provide accelerated decoding across various examples within this setting, with a logical way to collect offline data from models for learning. Our results and ablations show that learning a policy with this approach can adapt to general preferences while accounting for more complex aspects of the decision making, highlighting its robustness. Furthermore, such a method is scalable and robust to the introduction of more draft models or the removal of draft models, presenting a viable alternative to settings where a uniquely superior draft model may be unavailable.\nNevertheless, areas of further development exist. For example, learning in an online fashion may render this method more broadly applicable. Alternatively, exploring how to dynamically choose drafters at every decoding step rather than per example, as well as combining this direction of work with that which attempts to adaptively choose the speculation length at every step, are feasible ways of combining our findings with concurrent work in the hopes of reaping the benefits of all methods."}, {"title": "6 Limitations", "content": "This work has a few limitations which define the scope of future work.\nChoice of draft models and data domains\nResults may stem from the distinct boundaries that exist between domains/tasks. In settings where such boundaries are not well defined, outcomes may differ. However technical limitations and the absence of sufficent pre-trained models for comparision makes this difficult to explore immediately.\nAdditional storage and memory\nThe usage of multiple models that draft independently requires additional memory, which can be be more difficult to manage when there are explicit constraints on this front (self-drafting avoids this due to the use of a single model). Furthermore, collecting an offline dataset can be difficult in some specific scenarios where inference is burdensome, for example when input/output sequences are very long, or when many offline examples are required.\nSelf-Drafting\nWe work on a setting where we do not conduct any additional training of parameters that are explicitly linked to the language model itself, whether they are existing parameters or new paramters added as a result of the method. While there are ways in which our explored method can be applied to these as well, computational limitations make it difficult to rigorously conduct such studies at the moment and we leave it to future work for this reason."}, {"title": "7 Ethics Statement", "content": "This paper discusses the concept of dynamically choosing between of multiple black-box draft models for speculative decoding, proposing an offline reinforcement learning approach for adaptively selecting a good draft model for assistance. Our results are relate to the decoding speed of models, which is unlikely to lead to ethical concerns or problematic interpretations of such results."}]}