{"title": "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence", "authors": ["Mengyao Lyu", "Tianxiang Hao", "Xinhao Xu", "Hui Chen", "Zijia Lin", "Jungong Han", "Guiguang Ding"], "abstract": "Domain Adaptation (DA) facilitates knowledge transfer from a source domain to a related target domain. This paper investigates a practical DA paradigm, namely Source data-Free Active Domain Adaptation (SFADA), where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available in the target domain. Without referencing the source data, new challenges emerge in identifying the most informative target samples for labeling, establishing cross-domain alignment during adaptation, and ensuring continuous performance improvements through the iterative query-and-adaptation process. In response, we present learn from the learnt (LFTL), a novel paradigm for SFADA to leverage the learnt knowledge from the source pretrained model and actively iterated models without extra overhead. We propose Contrastive Active Sampling to learn from the hypotheses of the preceding model, thereby querying target samples that are both informative to the current model and persistently challenging throughout active learning. During adaptation, we learn from features of actively selected anchors obtained from previous intermediate models, so that the Visual Persistence-guided Adaptation can facilitate feature distribution alignment and active sample exploitation. Extensive experiments on three widely-used benchmarks show that our LFTL achieves state-of-the-art performance, superior computational efficiency and continuous improvements as the annotation budget increases. Our code is available at https://github.com/lyumengyao/lftl.", "sections": [{"title": "Introduction", "content": "Deep neural networks have thrived in computer vision but struggle when real- world data deviates from the ideal independent and identical distribution of training data, causing performance drops in well-trained models. This is where domain adaptation (DA) comes into play, which enables knowledge transfer from a source domain to a target domain of different data distribution. Most DA studies assume concurrent access to data from both domains to leverage the inter-domain relationship. Nonetheless, the reliance on labeled source data during adaptation could impede its widespread usage in real-world scenarios, considering stringent data protection regulations and resource deficiency in storage and computation. Therefore, source-free unsupervised DA (SFUDA) [16, 17, 25, 28, 40, 59,65,68] advocates for encapsulating knowledge within a source-trained model. They typically employ parameter sharing and hypothesis-driven learning for model adaptation. Yet, the absence of direct supervision from any domain can intensify the ill-posed nature of unsupervised DA [50], leading to the performance bottlenecks despite considerable efforts made.\nIn fact, the presence of the target sample pool during adaptation implies that minimal annotation effort is possible, yielding substantial performance gains. Hence we study a more practical DA setting, dubbed source-free active DA (SFADA). As shown in Fig 1 (L), the adaptation process is freed from source data, and meantime a minimum amount of annotation budget is available for iteratively querying labels in the target domain. Limited yet definitive supervisory signals intrinsically alleviate the ill-posedness phenomenon, and additionally, SFADA offers an appealing trade-off between labeling costs and adaptation performance regarding accuracy and efficiency.\nTaking both target annotation availability and source data inaccessibility into consideration, SFADA presents new challenges. During query selection, general active learning criteria [3,9,19, 31, 47, 48, 64] tend to fail under the domain shift phenomenon as observed in both previous Active DA (ADA) studies [8,39,41, 52, 53,60] and our experiments 4.2. On top of that, ADA typically employs source data as a reference to identify distinctive target samples, which violates our source-free setting and thus is not compatible here either. Therefore, SFADA necessitates a new active learning strategy to pinpoint the most innovative and informative target samples utilizing solely a source-trained model. Another challenge comes at the transfer stage. Without the source data, manifold alignment, typically employed via minimizing distribution divergence to boost performance, is impeded. Yet the"}, {"title": "Learn from the Learnt", "content": "In this section, we present LFTL for SFADA. As illustrated in Fig. 2, it alternates between Contrastive Active Sampling (CAS) and Visual Persistence-guided Adaptation (VPA)."}, {"title": "Overview", "content": "Given a labeled source domain S with restricted access and an unlabeled target domain T, our goal is to derive a model that can minimize the risk on the target domain through knowledge transfer with a small annotation budget. At"}, {"title": "Contrastive Active Sampling", "content": "In each active learning round, the aim is to find the most informative instances from the target domain, especially those diverging from the source, to benefit the adaptation of the model. Without directly referencing the source data, previous source-free approaches [7, 25, 28, 59] often rely on the hypothesis transfer to distinguish between source-like and source-dissimilar samples. However, when applied to the SFADA scenario, such solution [57] is constrained to a single round of active sampling and model updating, which restricts the potential applications of this setting.\nConsidering the iterative nature of the task context, we tackle this chal- lenge from the active perspective. Just as the source knowledge is encapsulated within the initial model Ms, the target knowledge learned in previous rounds is manifested in the hypotheses of the preceding model $M^{(r-1)}$. Compared to the hypotheses of the preceding model, samples that obtain higher prediction confidence from the newly updated model reflect the fresh insight just acquired. The larger the gap in predictive confidence, the better the current model has grasped the sample, and thus the less informative for the subsequent phase of sample selection. As illustrated in Fig 2, we first contrastively highlight the less informative samples as follows:\n$p^{(r)}(.x_i^u) = \\begin{cases} -log \\ p^{(r)} & \\text{if r = 0}, \\\\ -log \\ p^{(r)} + \\alpha (log \\ p^{(r)} - log \\ p^{(r-1)}) & \\text{if r > 0}, \\end{cases}$     (1)\nweighted by $\\alpha$, where p is derived from the softmax of the model logits d(g(f($x_i^u$))). We omit the superscript of round (r) for simplicity in the following text.\nBased on p, various active learning criteria can be employed. Without loss of generality, here we focus on the most confused classes. Specifically, we take the difference between the best-versus-second best (BvSB) guesses as the uncertainty indicator, defined as $ucm(x_i^u) = p(y_*|x_i^u) - p(y'_j|x_i^u)$, where $y_* = arg \\ max_{y \\in Y} p(y|x_i^u)$, and $y'_j = arg \\ max_{y \\in Y \\setminus {y_*}} p(y|x_i^u)$. Then the unlabeled target sample pool is ranked by:\nr($x_i^u$) = $\\sum_{k=1}^{N_{tu}} \\mathbb{I}(ucm(x_k^u) < ucm(x_i^u))$,                                              (2)\nwhere $\\mathbb{I}$ is the indicator function. A smaller ucm score indicates that the assessed target sample $x_i^u$ furnishes novel insights that diverge from the source domain and, in contrast to prior iterations, has not been grasped. Conversely, when the current stronger model, aided by a marginal increment of active labels, assigns more probability preference to one class for an unlabeled sample, CAS emphasizes it via a larger ucm score, lowering the priority of those familiar candidates. As the query-and-adaptation alternation proceeds, CAS continuously harvests fresh information for transfer by leveraging readily available intermediate results, without additional extra computational burden or memory usage.\nThe contrastively decoded margin evaluates each sample on an individual basis. However, the batch-mode active learning could be susceptible to outliers, since"}, {"title": "Visual Persistence-guided Adaptation", "content": "During each iterative round, the top-b most informative samples are queried for labels towards the adaption to the target domain. They offer reliable signals but also pose a challenge of making the best use of them. We first warm up the adaptation process via empirical risk minimization on the small amount of target instances with active labels:\n$L_{ce} = -E_{x_{t_1} \\sim T_1} q(x_{t_1})^Tlog[p(\\cdot|x_{t_1})]$,    (5)\nwhere q($x_{t_1}$) is the ground truth 1-hot vector for the labeled instance $x_{t_1}$.\nTo further minimize the risk on the target domain, we are motivated to bridge the distance between actively queried data and unlabeled data. Instead of seeking cluster centroids in the target pool, we directly take active samples as representative anchors, and encourage unlabeled data to form concentrated clusters around the nearest anchor on the visual embedding space via soft similarity minimization:\n$L_{ac} = -E_{x_i^u \\sim T_u} d_{t_u} log(d_{t_u}), \\ \\text{where} \\ d_{t_u} = d[D(f_t(x_i^u), f(T_i))]$.   (6)\nIn Eq. 6, the feature extractor of the adapted model outputs the visual repre- sentation of $x_i^u$ as $f_t(x_i^u)$. $f(T_i)$ is a $n_{t_1} \\times d$ matrix where each row represents a d-dimensional feature of a labeled anchor i. D denotes a distance metric. For simplicity and without loss of generality, we adopt cosine similarity to measure"}, {"title": "Experiments", "content": "We conduct experiments on three widely-used DA benchmarks: a) VisDA-C [38] aims to solve the simulation-to-reality shift, in which the"}, {"title": "Comparison with SOTAS", "content": "We consider the following competitors: SFUDA methods including SFDA [20], A\u00b2Net [59], SHOT [28], SHOT++ [28], CPGA [40], DaC [68] and SF(DA)\u00b2 [17], ADA methods including AADA [52], TQS [8], CLUE [39], SDM-AG [60] and LADA [53], and SFADA prior work MHPL [57] For fair inter- and intra-setting"}, {"title": "Efficiency Analysis", "content": "We compare with open-source SFUDA and ADA methods in terms of actual time consumption, including model training, active sampling and human annotation,"}, {"title": "Promises of Continual Performance Growing", "content": "In addition to the experiments presented in main results under varying bud- get constraints, in this section, we explore the trade-off between the cost and adaptation performance with increased budget and larger strides. The budget is increased to 2%, 5%, 10% and 20% successively, with the corresponding results summarized in Figure 4. We observe that the proposed LFTL framework per- forms notably better than competitors in terms of both accuracy and robustness. The performance superiority is consistent across different budget scenarios, from tight to generous, indicating its effectiveness in leveraging data resources. On the contrary, other methods either gradually deviate from the source domain throughout the process, failing to acquire domain-variant novel knowledge, or they solely support one-round active sampling as in MHPL. We emphasize that the promise of continual performance growth holds practical value in real-world applications, as it facilitates the effective feedback loop between data reflow and model iteration."}, {"title": "Discussion of Source Availability", "content": "Considering the performance margin of SFADA over ADA, we take LADA, the ADA SOTA, as an example for comparison to explore the role of source data in adaptation. We first ablate the source domain loss from LADA, denoted as SF- LADA to compare with LFTL. Given that the source data becomes unavailable, each epoch now iterates over the labeled target dataset instead of the labeled source dataset, the same as our LFTL. The averaged results in Tab. 5 show that, without specifically designed hypothesis transfer strategy, removing the source data from ADA methods will cause task models to forget the previously learnt knowledge in the source domain during adaptation. And the newly acquired information from the target domain is too limited to counteract it, resulting in noticeable accuracy gaps. This further validates the necessity of learning from the learnt under the source-free condition.\nThen we add a simple cross-entropy loss for the source data to LFTL, namely S-LFTL, while keeping the number of iterations unchanged. As shown in Tab. 5, S-LFTL improves over LFTL, demonstrating that direct source supervision could still provide more useful information than hypothesis transfer and memory recollection. The source data proves especially effective for smaller Office datasets, where the improvements are noticeable while the additional amount of additional time is affordable. For the large dataset, although the performance of S-LFTL does not match that of LADA within just 6% of its adaptation time, extending the training iterations to access all available data would guarantee further performance improvements."}, {"title": "Comparison with Active Learning Baselines", "content": "To validate the proposed active query strategy, we compare LFTL with active learning baselines. We choose Random, uncertainty-based EntMax [48], Least Confidence [24], BvSB [24], Bayesian [9], distribution-based K-means [64] and K-center greedy [47]. Experiments are performed on the large-scale VisDA-C dataset with 1% annotation budget in total for 10 rounds. The results for each round are listed in Tab. 6. It manifests that the proposed sampling strategy consistently outperforms its substitutes. As the iteration proceeds, information queried by other active learning methods become redundant, failing to fulfill the requirements of the DA task. In comparison, the CAS and class-level transfer- ability estimation we propose efficiently prioritize samples with local uncertainty, global intransferability and temporal stagnancy along the sample-and-adaptation progress, bringing continuous growth to the DA task."}, {"title": "Conclusion", "content": "In this paper, we investigate the challenging source-free active domain adaptation (SFADA) setting, where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available for the target domain. We present a shared framework for both active sampling and domain adaptation to learn from the hypotheses and feature representations of the learnt source model and actively iterated models. The proposed CAS criterion effectively prioritizes samples that are both informative to the current model and persistently challenging throughout the iterative process, and their visual understandings of actively queried samples are temporally preserved and exploited via the VPA strategy during adaptation.\nExtensive experiments on three DA benchmarks have shown that, in com- parison with ADA and SFUDA, SFADA provides a worthy trade-off between annotation costs and model performance in both accuracy and efficiency. In com- parison with previous SFADA competitors, our LFTL framework also presents superior performance and exhibits more flexibility than the one-round method."}]}