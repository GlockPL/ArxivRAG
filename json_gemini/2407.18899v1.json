{"title": "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence", "authors": ["Mengyao Lyu", "Tianxiang Hao", "Xinhao Xu", "Hui Chen", "Zijia Lin", "Jungong Han", "Guiguang Ding"], "abstract": "Domain Adaptation (DA) facilitates knowledge transfer from a source domain to a related target domain. This paper investigates a practical DA paradigm, namely Source data-Free Active Domain Adaptation (SFADA), where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available in the target domain. Without referencing the source data, new challenges emerge in identifying the most informative target samples for labeling, establishing cross-domain alignment during adaptation, and ensuring continuous performance improvements through the iterative query-and-adaptation process. In response, we present learn from the learnt (LFTL), a novel paradigm for SFADA to leverage the learnt knowledge from the source pretrained model and actively iterated models without extra overhead. We propose Contrastive Active Sampling to learn from the hypotheses of the preceding model, thereby querying target samples that are both informative to the current model and persistently challenging throughout active learning. During adaptation, we learn from features of actively selected anchors obtained from previous intermediate models, so that the Visual Persistence-guided Adaptation can facilitate feature distribution alignment and active sample exploitation. Extensive experiments on three widely-used benchmarks show that our LFTL achieves state-of-the-art performance, superior computational efficiency and continuous improvements as the annotation budget increases. Our code is available at https://github.com/lyumengyao/lftl.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have thrived in computer vision but struggle when real-world data deviates from the ideal independent and identical distribution of training data, causing performance drops in well-trained models. This is where domain adaptation (DA) comes into play, which enables knowledge transfer from a source domain to a target domain of different data distribution. Most DA studies assume concurrent access to data from both domains to leverage the inter-domain relationship. Nonetheless, the reliance on labeled source data during adaptation could impede its widespread usage in real-world scenarios, considering stringent data protection regulations and resource deficiency in storage and computation. Therefore, source-free unsupervised DA (SFUDA) [16, 17, 25, 28, 40, 59,65,68] advocates for encapsulating knowledge within a source-trained model. They typically employ parameter sharing and hypothesis-driven learning for model adaptation. Yet, the absence of direct supervision from any domain can intensify the ill-posed nature of unsupervised DA [50], leading to the performance bottlenecks despite considerable efforts made.\nIn fact, the presence of the target sample pool during adaptation implies that minimal annotation effort is possible, yielding substantial performance gains. Hence we study a more practical DA setting, dubbed source-free active DA (SFADA). As shown in Fig 1 (L), the adaptation process is freed from source data, and meantime a minimum amount of annotation budget is available for iteratively querying labels in the target domain. Limited yet definitive supervisory signals intrinsically alleviate the ill-posedness phenomenon, and additionally, SFADA offers an appealing trade-off between labeling costs and adaptation performance regarding accuracy and efficiency.\nTaking both target annotation availability and source data inaccessibility into consideration, SFADA presents new challenges. During query selection, general active learning criteria [3,9,19, 31, 47, 48, 64] tend to fail under the domain shift phenomenon as observed in both previous Active DA (ADA) studies [8,39,41, 52, 53,60] and our experiments 4.2. On top of that, ADA typically employs source data as a reference to identify distinctive target samples, which violates our source-free setting and thus is not compatible here either. Therefore, SFADA necessitates a new active learning strategy to pinpoint the most innovative and informative target samples utilizing solely a source-trained model. Another challenge comes at the transfer stage. Without the source data, manifold alignment, typically employed via minimizing distribution divergence to boost performance, is impeded. Yet the"}, {"title": "Title Suppressed Due to Excessive Length", "content": "labeling costs have increased the expectations for performance enhancement. This prompts us to ask how to exploit the newly acquired knowledge while consolidating the learnt domain-invariant information during adaptation? And following these, how to guarantee continuous performance improvement during the iterative query-and-adaptation process?\nTo fill in this gap, this paper formulates a novel SFADA paradigm to learn from the learnt (LFTL), as illustrated in Fig. 2. We employ the hypothesis and visual representations of target samples obtained from the learnt source model and actively iterated models to ascertain what to supplement and where to adapt in the target domain. We first conduct active learning from the learnt model. Although it is impossible to directly identify target samples the most divergent from the source, the hypothesis of learnt model can be utilized as an indication of how well the target domain knowledge is understood. Furthermore, as the model progressively adapts towards the target domain, the hypothesis of the current model becomes increasingly important to query newer labels. This has not been addressed by the one-round query of MHPL [57]. Inspired from contrastive decoding [6, 11,23,27,63], we propose Contrastive Active Sampling (CAS) to leverage the hypothesis of the model from the previous active round. It emphasizes samples that are both informative to the current model and persistently challenging throughout the iterative process, while deprioritizing samples that are redundant or deliver knowledge already acquired during active selection. Upon the contrastively decoded hypothesis, we take the difference between the best-versus-second best (BvSB) guesses [19], integrate a class-balancing factor, so that temporally-stagnant, sample-uncertain and class-minor targets can be effectively queried. Secondly, we perform transfer learning from the learnt visual representations. Since the alignment of cross-domain distributions cannot be directly achieved, we establish Visual Persistence-guided Adaptation (VPA) to maintain feature representations of active target samples throughout the whole process, where the understanding initially derived from the source domain and subsequently obtained from previous active rounds are effectively conserved via momentum updates. Then our learning objective encourages unlabeled samples to be centralized around these memory-preserving representatives, approximating a source-similar distribution while exploiting target-specific knowledge obtained from queried labels.\nGaining insights from the intermediate results already computed during iteration makes our method simple in architecture, effective in active learning and knowledge transfer, superior in terms of both accuracy and efficiency, and flexible in the trade-off between annotation plus time budget and adaptation accuracy. Take the performance on the VisDA-C [38] benchmark for example, the entire query-and-adaptation process obtains 87.4% with merely 1% labeling budget and 780 training iterations (0.3h). In contrast, SHOT++ operating under the SFUDA setting requires 21.6K iterations (5.8h) to achieve 87.3%. Even when factoring in the estimated annotation time (1.83h), our method is significantly more time-saving. Compared with LADA [53], the ADA state-of-the-art (SOTA), we achieve comparable performance without accessing the source data, while"}, {"title": "Title Suppressed Due to Excessive Length", "content": "benefiting from a 25% deduction in active sampling time and an approximately 17-fold increase in adaptation speed. Furthermore, it exceeds prior SFADA work MHPL [57] by a clear margin of 1.5%. Our main contributions are:\n*   a novel SFADA paradigm to learn from the learnt source model and actively iterated models, which frees it from specialized architecture and sophisticated learning schemes with superior accuracy and efficiency;\n*   a CAS strategy to learn from previous model hypothesis, thereby prioritizing targets that are confusing for the current model, less transferable in class membership and consistently challenging in previous active rounds;\n*   a VPA design to learn from previous feature representations, so that intrinsic distribution alignment and active sample exploitation can be achieved;\n*   SOTA adaptation accuracy, superior computational efficiency, and continual improvements extensively validated on varied-scaled benchmarks."}, {"title": "2 Related Work", "content": "Domain Adaptation (DA) is a specialized case of transfer learning that enables knowledge generalization from a source domain to a related target domain. Varying in the assumption of the label-set relationship between source and target domains, namely semantic shift, DA settings can be mainly divided into close-set DA, open-set DA [35,46], partial DA [4,26] and universal DA [45,66]. According to the assumption of target data accessibility, DA settings fall into unsupervised DA (UDA) [10,61], semi-supervised DA (SSDA) [13,44], weakly-supervised DA (WSDA) [54], zero-shot DA [37,58], one-shot DA [30], few-shot DA [12,34,62] as well as active DA (ADA) [8,39, 41, 52, 53, 60].\nAmong the subfields, the one most related to our setting is ADA, where the goal is to query the most informative target samples for annotation to best benefit classification on the target domain. Saha et al. [43] either infer pseudo labels on target samples or query them for oracle annotation, which is based on inter-domain similarity between source and target domains. Su et al. [52] train a domain discriminative model for domain alignment and target uncertainty estimation. While most of them utilize or are orthogonal to off-the-shelf active learning mechanisms, Fu et al. [8] argue that constructing a committee [33], i.e., ensemble to measure target sample uncertainty based on consensus between multiple predictions is more robust under domain shift. CLUE [39] is a k-means clustering-weighted uncertainty-based active learning strategy, and it optimizes with cross-domain minmax entropy [44]. Xie et al. [60] apply the margin loss to the source domain training to exploit hard source samples and a less domain-biased decision boundary, and they further supplement margin sampling with expected error reduction consistent with the training objective. LADA [53] bases its sampling criterion on the predictive purity of a local structure, and exploits both data sources during adaptation. Despite previous efforts, ADA methods typically necessitate the simultaneous presence of data from both the source and target domains to exploit the relationship between domains. However, when this"}, {"title": "Title Suppressed Due to Excessive Length", "content": "assumption fails in practical scenarios, we must tackle the challenges associated with the unavailability of source data.\nOn the other hand, SFUDA posits a transfer setting where neither the source data nor the target annotations are available. Kundu et al. [22] adopt a two-stage paradigm for universal DA, where the procurement stage trains a source model in consideration of future semantic shifts and domain gaps, so that the deployment stage is capable of operating without source data. Liang et al. [28] harness information maximization [15,50] to align different domains, and utilize self-supervision and curriculum learning techniques via pseudo label clustering. Xia et al. [59] introduce adversarial training to distinguish between source-similar and -dissimilar target samples for cross-domain alignment, facilitated with self-supervision and clustering to improve performance. Based on prediction confidence, DaC [68] also divides the target samples based on source similarity, employing a global structure for source-like targets while implementing a local structure for the distinctive samples. Without definitive supervision signal from either domain, SFUDA makes the alignment more challenging and exacerbates the ill-posedness of UDA. Consequently, it frequently demands intricate design and increased computations to attain desirable outcomes.\nGiven the accessibility of target samples, this paper focuses on the more practical SFADA paradigm, leveraging minimal resources for better performance. Prior work MHPL [57] integrates three active learning strategies to sample targets that are uncertain, diverse and different from the source domain, and then utilizes a neighbor focal loss to emphasize them in adaptation. However, the dependency on the source dissimilarity metric confines it to a one-round sampling method, failing to guarantee sustained performance improvement in real-world applications. SALAD [21] samples target with a binary weighted function between an entropy score and the expected model change. However the adaptation procedure incorporates an additional Guided Attention Transfer Network, leading to undesirable computational overhead and, as evidenced by the results, inefficient annotation utilization. In contrast, our LFTL offers a straightforward, broadly applicable, and growth-oriented solution that can make the best from previous knowledge and a limited budget to guarantee accuracy, efficiency, and a flexible trade-off between annotation resources and performance."}, {"title": "3 Learn from the Learnt", "content": "In this section, we present LFTL for SFADA. As illustrated in Fig. 2, it alternates between Contrastive Active Sampling (CAS) and Visual Persistence-guided Adaptation (VPA)."}, {"title": "3.1 Overview", "content": "Given a labeled source domain S with restricted access and an unlabeled target domain T, our goal is to derive a model that can minimize the risk on the target domain through knowledge transfer with a small annotation budget. At"}, {"title": "3.2 Contrastive Active Sampling", "content": "In each active learning round, the aim is to find the most informative instances from the target domain, especially those diverging from the source, to benefit the adaptation of the model. Without directly referencing the source data, previous source-free approaches [7, 25, 28, 59] often rely on the hypothesis transfer to distinguish between source-like and source-dissimilar samples. However, when applied to the SFADA scenario, such solution [57] is constrained to a single round of active sampling and model updating, which restricts the potential applications of this setting.\nConsidering the iterative nature of the task context, we tackle this challenge from the active perspective. Just as the source knowledge is encapsulated within the initial model Ms, the target knowledge learned in previous rounds is manifested in the hypotheses of the preceding model $M^{(r-1)}$. Compared to the hypotheses of the preceding model, samples that obtain higher prediction confidence from the newly updated model reflect the fresh insight just acquired. The larger the gap in predictive confidence, the better the current model has grasped the sample, and thus the less informative for the subsequent phase of sample selection. As illustrated in Fig 2, we first contrastively highlight the less informative samples as follows:\n$p^{(r)}(x_i^u) = \\begin{cases}\nlog p^{(r)}\\text{ if }r=0, \\\\\nlog p^{(r)} + \\alpha (log p^{(r)} - log p^{(r-1)}) \\text{ if } r > 0, \\end{cases}$ (1)\nweighted by $\\alpha$, where p is derived from the softmax of the model logits $\\delta(g(f(x_i^u)))$. We omit the superscript of round (r) for simplicity in the following text.\nBased on p, various active learning criteria can be employed. Without loss of generality, here we focus on the most confused classes. Specifically, we take the difference between the best-versus-second best (BvSB) guesses as the uncertainty indicator, defined as $ucm(x_i^u) = p(y_a|x_i^u) - p(y_j|x_u)$, where $y_a = arg \\max_{y \\in Y} p(y|x_i^u)$, and $y_j = arg \\max_{y \\in Y\\setminus y} p(y|x_i^u)$. Then the unlabeled target sample pool is ranked by:\n$\\tau(x_{iu}) = \\sum_{k=1}^{N_{tu}} \\Pi(ucm(x_{tu}) < ucm(x_{tu})),$ (2)\nwhere $\\Pi$ is the indicator function. A smaller ucm score indicates that the assessed target sample $x_i^u$ furnishes novel insights that diverge from the source domain and, in contrast to prior iterations, has not been grasped. Conversely, when the current stronger model, aided by a marginal increment of active labels, assigns more probability preference to one class for an unlabeled sample, CAS emphasizes it via a larger ucm score, lowering the priority of those familiar candidates. As the query-and-adaptation alternation proceeds, CAS continuously harvests fresh information for transfer by leveraging readily available intermediate results, without additional extra computational burden or memory usage.\nThe contrastively decoded margin evaluates each sample on an individual basis. However, the batch-mode active learning could be susceptible to outliers, since"}, {"title": "Title Suppressed Due to Excessive Length", "content": "the queried samples are not selected based on the underlying natural density distribution [1, 49]. Particularly in DA tasks, different classes show different sensitivity to the domain shift. Some of them are more robust to certain shifts, i.e., their learnt features present more domain-invariant patterns, whereas others are likely to suffer a steep performance drop [18]. These motivate us to incorporate a broader, semantic view of the target domain into our active criterion, where classes with higher transferability is estimated via the class memberships of reliable model hypotheses:\n$u_{ct}(c/T_u) = \\frac{\\sum_{\\tau} \\Pi (y^*=c) \\cdot I(\\tau (x_i^u)>(\\eta_{tu} - k))}{\\max_{\\tau = y^*} \\sum \\Pi(\\tau(x_i^u) > (\\eta_{tu} - k))},$ (3)\nEq. 3 measures the frequency of unlabeled samples being inferred as class c among a highest contrastive margins, wherein $\\kappa$ controls the range of statistics. Thus the classes with higher $u_{ct}$ scores can be downweighted in favor of less transferable classes.\nIn this way, we drive a hybrid active sampling criterion that favors target samples with local uncertainty, global intransferability and temporal stagnancy:\n$u^2 = u_{cm} + \\lambda u_{ct}(y^*), $ (4)\nwhere $\\lambda$ controls the importance of class-level cross-domain transferability."}, {"title": "3.3 Visual Persistence-guided Adaptation", "content": "During each iterative round, the top-b most informative samples are queried for labels towards the adaption to the target domain. They offer reliable signals but also pose a challenge of making the best use of them. We first warm up the adaptation process via empirical risk minimization on the small amount of target instances with active labels:\n$L_{ce} = -E_{x_{tl}\\sim T_l} q(x_{tl})^T log[p(\\cdot|x_{tl})],$ (5)\nwhere $q(x_{tl})$ is the ground truth 1-hot vector for the labeled instance $x_{tl}$.\nTo further minimize the risk on the target domain, we are motivated to bridge the distance between actively queried data and unlabeled data. Instead of seeking cluster centroids in the target pool, we directly take active samples as representative anchors, and encourage unlabeled data to form concentrated clusters around the nearest anchor on the visual embedding space via soft similarity minimization:\n$L_{ac} = - E_{x_{tu} T_u} d_{tu} log(d_{tu}), where d_{tu} = d[D(f_t(x_{tu}), f(T_l))].$ (6)\nIn Eq. 6, the feature extractor of the adapted model outputs the visual representation of $x_{tu}$ as $f_t(x_{tu})$. $f(T_l)$ is a $n_{tl} \\times d$ matrix where each row represents a d-dimensional feature of a labeled anchor $T_l$. D denotes a distance metric. For simplicity and without loss of generality, we adopt cosine similarity to measure"}, {"title": "Title Suppressed Due to Excessive Length", "content": "the relationship between different instances, and a normalization function is applied to transform the distances to [0, 1]. In decreasing the entropy of distances, each unlabeled sample is encouraged to get close to its nearest labeled anchors and meantime keep other clusters at a considerable distance. Consequently, as shown in Fig. 2, features are aggregated in a class-wise manner, which makes it easier for classifier decision boundaries to be established.\nHowever, in the context of the SFADA framework, as the labeled target subset $T_l$ and the model $M_t$ iterate for R rounds, the incremental domain-specific information obtained from the actively queried samples may intensify the cross-domain incompatibility [5, 51, 55]. Furthermore, there is a potential risk of domain-invariant knowledge being forgotten during the process. Without accessing the source domain data, previous source-free methods achieve domain alignment via freezed hypotheses [25, 28,59] or buffered embeddings [7] from the source classifier. In our multi-round iterative process, we introduce a visual persistence vault to guide the adaptation. It preserves judgements on the active anchors, which have been derived from source and intermediate models, via exponential moving average:\n$\\bar f(x_{tl}) \\gets \\gamma f(x_{tl}) + (1 - \\gamma) f(x_{tl}), $ (7)\nwhere $f(x_{tl})$ is derived from the adapted feature extractor $f_t$, and $\\gamma$ is the momentum parameter controlling the weighting decrease of previous observations, which is empirically fixed as 0.9. In replacing the most recent feature representation of active anchors $f(T_l)$ in Eq. 6 with the temporally ensembled $\\bar f(T_l)$, we obtain the visual persistence-guided adaption loss $L_{vpa}$. The learnt domain-invariant knowledge maintained by the persistence vault effectively supports the alignment in the target domain, and meantime the target-specific information is well exploited.\nAdditionally, the entropy minimization loss [16,28,57] is introduced to approach the ideal adaptation performance and foster discriminative features in the learnt manifold:\n$L_{ent} = -E_{x_{tu} T_u} P(\\cdot|x_{tu}) log[p(\\cdot|x_{tu})].$ (8)\nOverall, the combination of supervised cross-entropy, visual persistence guidance and entropy minimization losses yields:\n$L = L_{ce} + \\beta_1 L_{vpa} + \\beta_2 L_{ent}.$ (9)\nWe emphasize that no explicit supervision (e.g., pseudo labels) is enforced for unlabeled samples during the optimization, so that the error accumulation problem is greatly alleviated in the source data-free and target label-scarce setting."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Setup", "content": "Datasets. We conduct experiments on three widely-used DA benchmarks:\na) VisDA-C [38] aims to solve the simulation-to-reality shift, in which the"}, {"title": "4.2 Comparison with SOTAS", "content": "We consider the following competitors: SFUDA methods including SFDA [20], A\u00b2Net [59], SHOT [28], SHOT++ [28], CPGA [40], DaC [68] and SF(DA)\u00b2 [17], ADA methods including AADA [52], TQS [8], CLUE [39], SDM-AG [60] and LADA [53], and SFADA prior work MHPL [57] For fair inter- and intra-setting"}, {"title": "Title Suppressed Due to Excessive Length", "content": "comparison, consistent experimental conditions provided by SHOT are utilized for ADA methods based on the official codes of TQS [8], CLUE [39] and SDM-AG [60], CLUE [53] as well as the re-implementation of AADA [52] provided by CLUE [39]. Results not reported by MHPL and not reproducible due to unavailability of source code are indicated by a dash.\nWe summarize the adaption accuracies on VisDA-C, Office-Home and Office-31 in Tab. 1, 2 and 3, respectively. On the large-scale dataset VisDA-C, given merely 1% label annotations, LFTL is already capable of outperforming most of ADA methods. When the budget increased to 5%, it exceeded the SFADA SOTA by 1.5% and also outdid the source-accessible ADA SOTA under the same conditions by 0.7%. On the medium-sized Office-Home dataset, LFTL elevates the adaptation performance to a level of 78.8% with 5% annotation costs, which is comparable with SFUDA methods. We note a continuous enhancement in performance as the budget is raised to 10%, culminating in the highest 85.1% accuracy. Similar trend can also be observed in the small-sized Office-31 dataset. When 5% target samples are labeled, our results surpass MHPL in average performance and in four out of five subtasks, while being on par with the source-accessible LADA.\nThe results show that, given a minimum amount of annotations, noticeable improvements can be achieved over SFUDA with increased flexibility. Meantime, despite the inavailability of the source domain, learning from what the model already learnt can offset the absence of the source data via our proposed CAS strategy and VPA approach."}, {"title": "4.3 Efficiency Analysis", "content": "We compare with open-source SFUDA and ADA methods in terms of actual time consumption, including model training, active sampling and human annotation,"}, {"title": "Title Suppressed Due to Excessive Length", "content": "as presented in Fig. 3. We take the adaptation on the VisDA-C dataset for example, and provide 1% budget, i.e., 550 images of the target domain for active learning-based methods. Note that here we report the consumption of the whole sample-and-adaptation process, instead of the per-round time. All experiments were conducted on 1 NVIDIA GeForce RTX 3090 GPU, with 80 CPU cores and 256 GB memory.\nConsidering the training time for adaptation, SFUDA SOTAs often require sophisticated schemes (clustering, two-stage training, etc.) to adapt to a completely unlabeled domain, which necessitate more computational resource. For example, SHOT and SHOT++ takes 3.7h and 5.8h to converge respectively, and CPGA uses more than five days to attain the reported performance, which we omit in Fig. 3 to prevent it from dominating the figure. On the other hand, when equipped with both source and target data, ADA methods are expected to directly align different domains, as well as identifying novel samples, which adds to the training burden. For example, AADA, CLUE and TQS consume more than 6h mainly because of the adversarial training. TQS additionally trains five classifiers to construct a voting committee for active learning, culminating in an overall duration of 8.5h. LADA employs local neighbor search for more reliable training guidance, which takes 5.9h. SDM-AG improves on the optimization complexity yet still requires 5.1h. In contrast, drawing upon queried informative data, our LFTL leverages the iterative nature of active learning instead of additional computations. The proposed VPA retrospects on the knowledge gained from the source and intermediate models of previous rounds, which is temporarily ensembled in the VP vault for readily use. As the comparison shows, it significantly outperforms competitors with only 0.3h for adaptation while yieding superior accuracy.\nAs for the active sampling efficiency, we summarize the theoretical complexity and actual training duration for active methods in Tab. 4, where C and N to denote the number of classes and unlabeled instances respectively. AADA and our LFTL operate with O(NC+ N log N) complexity, which are the most efficient approaches. The clustering-based CLUE works at O(tNBD), in which D denotes the embedding dimension (512), B is the budget, and t is the number of clustering iterations (300). The ranking-based TQS and SDM-AG uses O(NCM + N log N) and O(NCD + N log N) respectively, in which M represents the number of members in the classifier committee (5 is adopted). The local neighbor search in LADA leads to a complexity of O(ND + N log N). The feature dimension D of SDM-AG and LADA is 256. Contrary to previous active criteria designed for sampling in the target domain, CAS contrasts the hypotheses generated by models from successive rounds to identify target samples that deviate from the source domain and persistently challenging for the recognition task. The comparison shows that the proposed CAS is effective, scalable and efficient.\nRegarding the human annotation time, given that image-level labels take 1 second per class [2,36], 1% target domain subset of VisDA-C would cost 550 \u00d7 12 \u2248 1.83h. As Fig. 3 shows, compared to non-active methods, although factoring in the annotation time introduces additional burden for ADA methods, we still retain the top place."}, {"title": "4.4 Promises of Continual Performance Growing", "content": "In addition to the experiments presented in main results under varying budget constraints, in this section, we explore the trade-off between the cost and adaptation performance with increased budget and larger strides. The budget is increased to 2%, 5%, 10% and 20% successively, with the corresponding results summarized in Figure 4. We observe that the proposed LFTL framework performs notably better than competitors in terms of both accuracy and robustness. The performance superiority is consistent across different budget scenarios, from tight to generous, indicating its effectiveness in leveraging data resources. On the contrary, other methods either gradually deviate from the source domain throughout the process, failing to acquire domain-variant novel knowledge, or they solely support one-round active sampling as in MHPL. We emphasize that the promise of continual performance growth holds practical value in real-world applications, as it facilitates the effective feedback loop between data reflow and model iteration."}, {"title": "4.5 Discussion of Source Availability", "content": "Considering the performance margin of SFADA over ADA, we take LADA, the ADA SOTA, as an example for comparison to explore the role of source data in adaptation. We first ablate the source domain loss from LADA, denoted as SF-LADA to compare with LFTL. Given that the source data becomes unavailable, each epoch now iterates over the labeled target dataset instead of the labeled source dataset, the same as our LFTL. The averaged results in Tab. 5 show that, without specifically designed hypothesis transfer strategy, removing the source data from ADA methods will cause task models to forget the previously learnt knowledge in the source domain during adaptation. And the newly acquired information from the target domain is too limited to counteract it, resulting in noticeable accuracy gaps. This further validates the necessity of learning from the learnt under the source-free condition.\nThen we add a simple cross-entropy loss for the source data to LFTL, namely S-LFTL, while keeping the number of iterations unchanged. As shown in Tab. 5, S-LFTL improves over LFTL, demonstrating that direct source supervision could still provide more useful information than hypothesis transfer and memory recollection. The source data proves especially effective for smaller Office datasets, where the improvements are noticeable while the additional amount of additional time is affordable. For the large dataset, although the performance of S-LFTL does not match that of LADA within just 6% of its adaptation time, extending the training iterations to access all available data would guarantee further performance improvements."}, {"title": "4.6 Comparison with Active Learning Baselines", "content": "To validate the proposed active query strategy, we compare LFTL with active learning baselines. We choose Random, uncertainty-based EntMax [48], Least Confidence [24], BvSB [24], Bayesian [9], distribution-based K-means [64] and K-center greedy [47]. Experiments are performed on the large-scale VisDA-C dataset with 1% annotation budget in total for 10 rounds. The results for each round are listed in Tab. 6. It manifests that the proposed sampling strategy consistently outperforms its substitutes. As the iteration proceeds, information queried by other active learning methods become redundant, failing to fulfill the requirements of the DA task. In comparison, the CAS and class-level transfer-ability estimation we propose efficiently prioritize samples with local uncertainty, global intransferability and temporal stagnancy along the sample-and-adaptation progress, bringing continuous growth to the DA task."}, {"title": "5 Conclusion", "content": "In this paper, we investigate the challenging source-free active domain adaptation (SFADA) setting, where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available for the target domain. We present a shared framework for both active sampling and domain adaptation to learn from the hypotheses and feature representations of the learnt source model and actively iterated models. The proposed CAS criterion effectively prioritizes samples that are both informative to the current model and persistently challenging throughout the iterative process, and their visual understandings of actively queried samples are temporally preserved and exploited via the VPA strategy during adaptation.\nExtensive experiments on three DA benchmarks have shown that, in comparison with ADA and SFUDA, SFADA provides a worthy trade-off between annotation costs and model performance in both accuracy and efficiency. In comparison with previous SFADA competitors, our LFTL framework also presents superior performance and exhibits more flexibility than the one-round method."}]}