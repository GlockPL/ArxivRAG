{"title": "DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach", "authors": ["Qian Chen", "Ling Chen"], "abstract": "Temporal Knowledge Graph (TKG) representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. However, existing approaches cannot capture the temporal evolution of high-order correlations in TKGs. To this end, we propose a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL). Specifically, a deep evolutionary clustering module is proposed to capture the temporal evolution of high-order correlations among entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters. In addition, an implicit correlation encoder is introduced to capture latent correlations between any pair of clusters under the guidance of a global graph. Extensive experiments on seven real-world datasets demonstrate that DECRL achieves the state-of-the-art performances, outperforming the best baseline by an average of 9.53%, 12.98%, 10.42%, and 14.68% in MRR, Hits@1, Hits@3, and Hits@10, respectively.", "sections": [{"title": "Introduction", "content": "Temporal Knowledge Graphs (TKGs) are collections of human temporal evolving knowledge [35], which are widely utilized in various fields, e.g., information retrieval [19], natural language under- standing [3], and recommendation systems [32]. A TKG represents events in the form of quadruples (s, r, o, t), where s and o denote the subject and object entities, respectively, r denotes the relation between s and o, and t represents the timestamp [35]. Event prediction in TKGs is an important task that predicts future events according to historical events [17]. TKG representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. Due to the complex temporal dynamics and multi-relations within TKGS, TKG representation learning poses great challenges to the research community.\nIn recent years, many TKG representation learning approaches have used graph neural networks [33] (GNNs) to model pairwise entity correlations [14, 1]. Furthermore, some researchers have leveraged derived structures, e.g., communities [39], entity groups [28], and hypergraphs [26, 29], to model high-order correlations among entities, i.e., the simultaneous correlations among three or more entities.\nThese approaches, however, lack the capability to capture the temporal evolution of high-order correlations in TKGs. As a kind of dynamic graph, TKGs inherently feature the temporal evolution of high-order correlations. For example, within TKGs focused solely on countries, a country entity may be affiliated with different international political organizations at different timestamps, with these organizations experiencing membership adjustment over time. In addition, the membership adjustment in international political organizations does not shift suddenly. Typically, numerous events occur before the formal adjustment of their membership, gradually pushing memberships away from or drawing them closer to international political organizations. For instance, the UK's official departure from the European Union (EU) was preceded by numerous events that incrementally estranged it from the EU.\nTo address the aforementioned deficiencies, a Deep Evolutionary Clustering jointed temporal knowl- edge graph Representation Learning approach (DECRL) is proposed for event prediction in TKGs. To the best of our knowledge, DECRL is the first work that integrates deep evolutionary clustering approaches into TKGs, which jointly optimizes TKG representation learning with evolutionary clustering to capture the temporal evolution of high-order correlations. Our main contributions are outlined as follows:\n\u2022 We propose a deep evolutionary clustering module to capture the temporal evolution of high-order correlations among entities, where clusters represent the high-order correlations between multiple entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure precise one-to-one alignment of soft overlapping clusters across timestamps, maintaining the temporal smoothness of clusters over successive timestamps.\n\u2022 We propose an implicit correlation encoder to capture latent correlations between any pair of clusters, which defines the interaction intensities between clusters to form a cluster graph. In addition, a global graph, constructed from all events of training set, is introduced to guide the assignment of different interaction intensities to different cluster pairs.\n\u2022 We evaluate DECRL on seven real-world datasets. The experimental results demonstrate that DECRL achieves the state-of-the-art (SOTA) performance. It outperforms the best baseline by an average of 9.53%, 12.98%, 10.42%, and 14.68% in MRR, Hits@1, Hits@3, and Hits @ 10, respectively."}, {"title": "Related work", "content": ""}, {"title": "TKG representation learning approach", "content": "GNNs and variants of recurrent neural networks (RNNs) are commonly integrated to model graph structural information and temporal dependency within TKGs, respectively [1, 34, 16, 14]. For example, TeMP [34] and RE-GCN [16] both utilize relation-aware GCN [24] to model the influence of neighbor entities and apply GRUs to model the temporal dependency. TiRGN [14] employs multi- relational GNNs to model graph structural information, and utilizes GRUs to capture the temporal dependency across sequential timestamps. However, these approaches often resort to stacking multiple layers to model the influence of distant neighbors, which may lead to the over-smoothing problem.\nRecent research advancements have introduced paths [15, 8, 18] to enhance the modeling capability of the latent pairwise correlations between entities. For example, xERTE [8] utilizes a time-aware graph attention mechanism to obtain and exploit local subgraphs. TLogic [18] employs a time- based random walk strategy to acquire logical paths. Furthermore, some researchers have leveraged derived structures, e.g., communities [39], entity groups [28], and hypergraphs [29], to model high- order correlations among entities. For example, EvoExplore [39] utilizes a temporal point process enhanced by a hierarchical attention mechanism to establish dynamic communities for modeling latent correlations among entities. DHyper [29] utilizes hypergraph neural networks to model high-order correlations among entities and among relations.\nWhile the promising results of introducing derived structures, existing approaches lack the capability to capture the temporal evolution of high-order correlations in TKGs. To address this gap, we propose DECRL, which is the first work to integrate deep evolutionary clustering approaches into TKGs. DECRL jointly optimizes TKG representation learning with evolutionary clustering to capture the temporal evolution of high-order correlations."}, {"title": "Deep evolutionary clustering approach", "content": "Existing deep evolutionary clustering approaches employ different strategies for discovering stable clusters [7, 21, 36, 20, 38, 37]. For example, DYNMOGA [7] employs a deep evolutionary clustering approach designed to identify evolving communities within dynamic networks, which treats the process as a multi-objective optimization problem aimed at cluster stability. sE-NMF [21] implements linear fusion of adjacency matrices across timestamps to enhance the temporal stability of clusters. Both DNETC [36] and CDNE [20] use constructed temporal smoothness loss functions to ensure a consistent and stable evolution of clustering results through successive timestamps. TRNNGCN [37] introduces a decay matrix to assess the influence of historical clustering results, thereby stabilizing the current clustering configuration.\nThese approaches straightforwardly incorporate prior clustering results with current timestamp results over time under the presumption that clustering results are relatively stable across different timestamps. However, the nuances between similar clusters are difficult to recognize in this way. In TKG representation learning, it is necessary to discern the subtle differences in high-order correlations. To bridge this gap, in our work, a cluster-aware unsupervised alignment mechanism is introduced, which ensures the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters over successive timestamps."}, {"title": "Preliminaries", "content": "Definition 1 (TKG). A TKG is a set of events formalized as G = {(s,r,o,t) | s, o \u2208 E, r \u2208 R, t \u2208 T}, where E, R, and T denote the set of entities, relations, and timestamps, respectively. Gt denotes the set of events at timestamp t.\nDefinition 2 (Entity Graph). The entity graph is constructed based on Gt, which is a multi-relational graph, and can be denoted as Gt = (Vt, Et), where Vt and Et denote the set of nodes and edges of the entity graph at timestamp t, respectively. The nodes in Vet represent entities, while the edges in Et represent relations between entities at timestamp t.\nDefinition 3 (Cluster Graph). The cluster graph is constructed based on Go, which is a fully connected graph, and can be denoted as Gt = (V, Et), where Vt and Et denote the set of nodes and edges of the cluster graph at timestamp t, respectively. The nodes and edges of cluster graphs represent clusters and the latent correlations between them, respectively, where each cluster represents the high-order correlations among entities.\nTask (Event Prediction). Given a query (s, ?, o, t), the event prediction task in TKGs aims to predict the conditional probability of all relations with the subject entity s, the object entity o, and the historical event sets G1:T-1, which can be denoted as p(r|s, o, G1:T-1), where T denotes the number of historical timestamps."}, {"title": "Methodology", "content": ""}, {"title": "Overview", "content": "The proposed DECRL approach is illustrated in Figure 1. At each timestamp, entity and relation representations updated by the cluster graph message passing module are merged with input represen- tations from the previous timestamp using a time residual gate, serving as the input for the current timestamp. The multi-relational interactions among entities are modeled by the relation-aware GCN. The deep evolutionary clustering module captures the temporal evolution of high-order correlations among entities, maintaining the temporal smoothness of clusters over successive timestamps through an unsupervised alignment mechanism. The implicit correlation encoder captures the latent correla- tions between any pair of clusters, enabling message passing within the cluster graph to update entity representations. Finally, the attentive temporal encoder captures temporal dependencies among entity and relation representations across timestamps, integrating them with temporal information for future event prediction. The computational complexity of DECRL can be found in Appendix A.1."}, {"title": "Evolutionary clustering module", "content": "The entity graph G = (Vt, Et) is constructed based on the historical events at timestamp t, which can be used to capture the structural dependency among concurrent events. Since the entity graph"}, {"title": "Cluster graph message passing module", "content": "Due to the intricate relationships and alliances formed among international political organizations, it is crucial to capture the latent correlations between clusters for event prediction. Herein, an implicit correlation encoder is proposed to capture the latent correlations between any pair of clusters. The cluster graph is designed to be a fully connected graph to avoid missing any relevant information between clusters. The representations of the latent correlations are formulated as:\ns = ReLU ((c,c))\ni,j\nwhere c and c are representations of the cluster i and j at timestamp t, respectively. is the transformation function, which is implemented by the multi-layer perceptron.\nIt is worth noting that clusters exhibit varying degrees of interaction, characterized by different intensities of latent correlations. Thus, we quantify the intensity of latent correlations between clusters, which is formulated as:\nq = \u03c3 (Conv(s))\ni,j\nwhere o is the sigmoid function, which ensures the resulting correlation intensity bounded between 0 and 1. Conv() represents the convolution operation.\nThe intensity of the latent correlations between clusters varies in the short term and intensifies over time; a higher frequency of interactions indicates a stronger latent correlation. To this end, we"}, {"title": "Time residual gate", "content": "The time residual gate is introduced to combine updated entity and relation representations with input representations of the current timestamp through a weighted mechanism. This approach preserves inherent characteristics while capturing the temporal evolution of entities and relations. For simplicity, the subscripts i and j of variables are omitted in the following sections without causing ambiguity. The final updated representations at timestamp t are formulated as:\nH\u207a = X\u207a \u2297 H + (1 \u2212 Xt) \u2297 Ht\u22121, Xt = \u03c3(W3Ht\u22121 + b)\nwhere H denotes the entity or relation representations updated by the cluster graph message passing module at timestamp t. Ht\u22121 denotes the output of entity or relation representations at timestamp t - 1, and also is the input of timestamp t. The time residual gate, i.e., X\u00b9, determines the inherent characteristics to be preserved, where o is the sigmoid function. W3 and b are learnable parameters."}, {"title": "Attentive temporal encoder", "content": "In the context of sequence modeling, the attentive temporal encoder is introduced to capture the temporal dependency among the final updated representations across timestamps. The position- enhanced representations of entity and relation are formulated as:\nz = [H\u00b2; \u0424(t)], \u0424(t) \u2713[cos(w1T'), cos (w2),..., cos(waT')]\nwhere Ht is the final updated representation of entity or relation at timestamp t. [;] denotes the concatenation operation. \u03a6(t) is the time position encoder. d denotes the value of the representation dimension. w\u2081 to wa are learnable parameters. Tt is the timestamp. Then, the temporal dependency is captured by a position-enhanced self-attention mechanism based on representation sequence z1:T-1 = {z1, z2, ..., zT\u22121}. For simplicity, the subscript t of variables is omitted in the following sections. The integrated entity or relation representation is formulated as:\nBm,n = (Wqzm, WkZn)\n\u221aa\n, am,n =\nexp(\u1e9em,n)\n\u2211 exp(\u03b2m,i)\ni=1\n, hm =\nT-1\n\u2211am,nzm,n\nn=1\nwhere Wq and Wk are learnable parameters. am,n is the attention weight. hm is the integrated representation of entity or relation, which integrates the temporal information."}, {"title": "Event prediction", "content": "ConvTransE [25] is employed as the decoder of DECRL, which predicts the probability of each relation between an entity pair, which is formulated as:\np(r|s, o, G1:T-1) = \u03c3(H\u2081ConvTransE(\u0113s, \u0113o))\nwhere is the probability vector of relations. o is the sigmoid function. Hr is the relation representa- tion matrix, each row of which corresponds to an integrated relation representation . ConvTransE(\u00b7) contains a one-dimensional convolution layer and a fully connected layer.\nThe event prediction training objective is minimizing the cross-entropy loss, which is formulated as:\nLTKG =\nS Nr\n1\n\u03a3\u03a3{Yij log Pij + (1 \u2212 Yi,j) log(1 \u2013 Pi,j)}\ni=1 j=1\nwhere S and N\u2081 denote the numbers of samples in the training set and relations, respectively. Yi,j denotes the label of relation j for sample i, of which the element is 1 if the event occurs, otherwise 0. Pij is the predicted probability of relation j for sample i, calculated by Equation 15.\nFinally, the total loss for DECRL is formulated as:\nL = (1 \u2212 1)LTKG + ALtemporal\nwhere X is a hyper-parameter that controls the trade-off between event prediction and temporal smoothness, which is bounded between 0 and 1."}, {"title": "Experiments", "content": "In this section, we evaluate the performance of DECRL through comprehensive experiments. We first describe the datasets and experimental settings, followed by a comparison with 12 SOTA TKG representation learning approaches. Next, we present an ablation study and hyper-parameter sensitivity analysis (see Appendix A.5). Finally, we offer case studies showcasing the effectiveness of DECRL."}, {"title": "Datasets and experimental settings", "content": "We evaluate DECRL on seven real-world datasets: ICEWS14 [30], ICEWS18 [9], ICEWS14C [29], ICEWS18C [29], GDELT [13], WIKI [12], and YAGO [9]. The ICEWS14 and ICEWS18 datasets span January 1, 2014, to December 31, 2014, and January 1, 2018, to October 31, 2018, respectively. We derive ICEWS14C and ICEWS18C datasets by filtering the original datasets to focus exclusively on events involving countries. The GDELT dataset spans January 1, 2018, to January 31, 2018. The WIKI and YAGO datasets are subsets of the Wikipedia history and YAGO3 [22], respectively. Dataset statistics and experimental settings are detailed in Appendices A.2 and A.3."}, {"title": "Comparison with baselines", "content": "DECRL is compared against 12 SOTA TKG representation learning approaches, categorized into shallow encoder-based approaches, i.e., TTransE [12] and HyTE [5], DNN-based approaches, i.e., RE-NET [9], Glean [6], TeMP [34], RE-GCN [16], DACHA [4], and TiRGN [14], and derived structure-based approaches, i.e., TITer [27], EvoExplore [39], GTRL [28], and DHyper [29]. All baselines are evaluated following consistent experimental settings with well-tuned hyper-parameters. The detailed descriptions of compared baselines can be found in Appendix A.4."}, {"title": "Ablation study", "content": "To dissect the contributions of each module within DECRL, we conduct the ablation study on ICEWS14. The detailed descriptions of variants are as follows:\n\u2022 DECRL-w/o-alignment: Removing unsupervised alignment mechanism.\n\u2022 DECRL-w/o-fusion: Removing fusion operation between clusters across timestamps.\n\u2022 DECRL-w/o-ICE: Removing implicit correlation encoder, resulting in a fully connected graph where all edges are assigned uniform weights.\n\u2022 DECRL-w/o-global-graph: Removing the guidance of the global graph on the assignment of different interaction intensities to different cluster pairs.\n\u2022 DECRL-w/o-Ltemporal: Removing the temporal smoothness loss term.\nTable 5 displays the MRR and Hits @ 1/3/10 results of DECRL and the variants on ICEWS14. From the ablation study, several conclusions can be drawn: Firstly, the most substantial performance degradation is observed in the DECRL-w/o-fusion variant, which indicates the effectiveness of capturing the temporal evolution of high-order correlations among entities. Moreover, the perfor- mance degradation of the DECRL-w/o-global-graph variant justifies the effectiveness of leveraging structural information from the global graph for guiding the assignment of different interaction intensities to different cluster pairs. Lastly, the performance degradation of DECRL-w/o-alignment and DECRL-w/o-Ltemporal variants shows the importance of preserving the temporal smoothness of clusters over successive timestamps."}, {"title": "Case study", "content": "To demonstrate the efficacy of DECRL, we conducted a case study with two variants: DECRL-w/o- alignment and DECRL-w/o-fusion, both of which are elaborated in Sub-section 5.3. In Figure 2, we utilize t-SNE [31] for the visualization of entity representations on ICEWS14C, where red dots represent individual entities in the TKGs, with their groupings indicating entity clusters. Notably, the entities marked in Figure 2, i.e., China, Thailand, Vietnam, Laos, Malaysia, Philippines, and Cambodia, are countries in East Asia participating in the Belt and Road Initiative. \u201cMiddle\" and \"Final\" denote the entity representations obtained after training at the penultimate epoch and the final epoch, respectively, for different variants.\nBy comparing the visualizations in the first row of sub-figures in Figure 2, we observe that DECRL exhibits the best entity clustering phenomena during the mid-training phase. By comparing the visualizations in the second row of sub-figures in Figure 2, Figure 2d (Final DECRL) showcases pronounced clustering phenomena, and inter-cluster distances indicate significant differences in clusters, i.e., international political organizations. Figure 2e (Final DECRL-w/o-alignment) demon- strates moderate clustering phenomena among countries, but inter-cluster distances are not significant enough. This observation implies that the lack of precise one-to-one alignment may lead to proximity between different clusters. Figure 2f (Final DECRL-w/o-fusion, which only models high-order correlations without temporal evolution) exhibits increased inter-cluster distances, with less distinct clustering phenomena. The comparison between Figure 2d (Final DECRL) and Figure 2f (Final DECRL-w/o-fusion) shows that capturing temporal evolution leads to better entity representations, as indicated by larger inter-cluster distances and tighter intra-cluster groupings. Comparing the first and third rows of Figure 2 further highlights the training progression, where modeling temporal evolution gradually enhances cluster separation and tightens the grouping of entities within clusters. This demonstrates that the capability of DECRL to model the temporal evolution of high-order correlations significantly enhances its ability to capture more nuanced cluster representations. Additional case study details are provided in Appendix A.7."}, {"title": "Conclusions and limitations", "content": "In this paper, a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL) is proposed for event prediction in TKGs, which jointly optimizes TKG representation learning with evolutionary clustering to capture the temporal evolution of high-order correlations. Comprehensive experiments are conducted on seven real-world datasets, including the comparison with baselines, ablation study, hyper-parameter sensitivity analysis, and case studies, which demonstrate the superior performance of DECRL. However, this study overlooks the continuous temporal evolution of diverse high-order correlations. Currently, the implicit correlation encoder assumes uniform correlations across all clusters, which may not reflect the complexity of real-world interactions between political organizations. In future work, we will address this issue by developing a multi relation-aware inter-cluster correlation encoder. Furthermore, DECRL currently models the temporal evolution of high-order correlations by considering only the previous and current timestamps. Future work will focus on explicitly modeling the continuous influence of high-order correlations from different past timestamps on the current timestamp."}, {"title": "Appendix", "content": ""}, {"title": "Complexity analysis", "content": "The time complexity of the relation-aware GCN is O(NN\u2081D\u00b2), where Ne and N\u2081 are the numbers of entities and relations, respectively. D is the dimension of representations. The time complexity of the evolutionary clustering module is O(N\u00a2N\u00a2D + N\u00b3 + ND), where N is the number of clusters. The time complexity of the cluster graph message passing module is O(N2D2). The time complexity of the time residual gate is O(D2). For the attentive temporal encoder, the time complexity is O(T2D), where T is the length of the history. For the event prediction module, the time complexity is O(D). The total complexity of DECRL is O(N\u00a2N\u2081D2 + N\u00b3 + N2D\u00b2 + T2D)."}, {"title": "Statistics of datasets", "content": "All datasets are split into training (80%), validation (10%), and test (10%) sets following [16]. The statistics of these datasets are summarized in Table 6."}, {"title": "Experimental settings", "content": "DECRL is implemented in Python using PyTorch and trained on one NVIDIA RTX 3080 GPU with 10GB memory. The source code is available on GitHub\u00b2. We leverage the Neural Network Intelligence (NNI) toolkit\u00b3 to automatically identify the optimal hyper-parameter values. The search space for the number of clusters N, the number of DECRL layers NDECRL layer, the length of historical windows Nhistorical window, and the value of A range from 1 to 20 with the step of 2, 1 to 5 with the step of 1, 1 to 14 with the step of 1, and 0.1 to 0.5 with the step of 0.1, respectively. The final hyper-parameter values are presented in Table 7. For NNI configurations, the maximum number of trials is set to 30, and the optimization algorithm used is the Tree-structured Parzen Estimator [2].\nWe utilize the Adam [10] optimizer with an initial learning rate of 0.01. The batch size is set to 16. The representation dimension is set to 200. The hidden sizes for the time residual gate and the attentive temporal encoder are both set to 200. The results reported are the averages across five independent runs. The evaluation metrics used in this paper include Mean Reciprocal Rank (MRR) and Hits @ 1/3/10, which represent the proportion of correct predictions ranked within the top 1, 3, and 10 positions, respectively, all expressed as percentages. Higher Hits@k and MRR scores indicate better performance."}, {"title": "Description of baselines", "content": ""}, {"title": "Shallow encoder-based approaches:", "content": "\u2022 TTransE [12] extends the TransE by incorporating timestamps as corresponding representa- tions.\n\u2022 HyTE [5] models timestamps as corresponding hyperplanes."}, {"title": "DNN-based approaches:", "content": "\u2022 RE-NET [9] leverages GCNs to model the influence of neighbor entities and uses RNNs to capture temporal dependencies among events.\n\u2022 Glean [6] employs CompGCN to model the influence of neighbor entities and utilizes GRUs to capture temporal dependencies among representations.\n\u2022 TeMP [34] utilizes relation-aware GCN to model the influence of neighbor entities and employs a frequency-based gating GRU to capture temporal dependencies among inactive events.\n\u2022 RE-GCN [16] uses relation-aware GCN to model the influence of neighbor entities and employs an autoregressive GRU to capture temporal dependencies among events.\n\u2022 DACHA [4] introduces a dual graph convolution network to obtain entity representations and uses a self-attentive encoder to model temporal dependencies among relations.\n\u2022 TiRGN [14] is the SOTA approach, using a multi-relational GCN to capture graph structure information and a double recurrent mechanism to model temporal dependencies."}, {"title": "Derived structure-based approaches:", "content": "\u2022 TITer [27] incorporates temporal agent-based reinforcement learning to search paths and obtain entity representations via the inductive mean.\n\u2022 EvoExplore [39] establishes dynamic communities to model the influence of neighbor entities.\n\u2022 GTRL [28] uses entity group modeling to capture the influence of distant and unreachable entities and employs GRUs to model temporal dependencies among representations.\n\u2022 DHyper [29] is the SOTA approach, which utilizes hypergraph neural networks to model high-order correlations among entities and among relations."}, {"title": "Hyper-parameter sensitivity analysis", "content": "In this sub-section, we study the hyper-parameter sensitivity of DECRL on ICEWS18C, including the length of historical windows Nhistorical window, the number of clusters Nc, the number of DECRL layers NDECRL layer, and the value of A. We show the performance changes by varying the hyper-parameter values in Figure 3.\nWe present the impact of Nhistorical window in Figure 3a. The results indicate that as Nhistorical window increases, the performance of DECRL gradually improves, peaking at the window length of 10. Beyond this point, performance declines rapidly, likely due to the inclusion of excessive irrelevant information in longer historical windows, which negatively impacts the performance of DECRL.\nIn Figure 3b, we present the effect of N.. The performance of DECRL remains relatively stable across most metrics as Ne increases. Hits@1 shows a slight initial increase until N = 8 and then stabilizes. MRR, Hits@3, and Hits@10 remain consistent, indicating that the performance of DECRL is not substantially affected by variations of Nc.\nWe show the impact of NDECRL layer and A in Figures 3c and 3d, respectively. In Figure 3c, performance metrics reach their peak when NDECRL layer = 2. Beyond this point, the performance of DECRL declines, likely due to increased model complexity, which raises the risk of over-fitting. Similarly, in Figure 3d, the performance metrics peak at X = 0.2, which suggests the need to set the appropriate trade-off between event prediction loss and temporal smoothness loss."}, {"title": "Entity prediction performance", "content": "We have conducted additional experiments to evaluate the performance of DECRL on the future entity prediction task, as shown in Table 8. Although DECRL does not achieve the SOTA performance in terms of MRR and Hits@1 metrics, it achieves the best results in Hits@3 and Hits@10 metrics. It is important to note that DECRL is not specifically designed for entity prediction tasks. Nevertheless, these results demonstrate the effectiveness and robustness of DECRL, particularly in capturing a broader range of relevant entities."}, {"title": "Case study", "content": "Table 9 presents the top 5 relations predicted by the SOTA DNN-based approach TiRGN, the SOTA derived structure-based approach DHyper, and our proposed approach DECRL for two test samples on ICEWS14C. The test samples pertain to the conflict between Russia and Ukraine that began in February 2014. During this period, Russia deployed military forces to the Crimean region of Ukraine, which resulted in widespread condemnations and sanctions from several countries, including the United States and various European nations. Correct predictions are underlined in Table 9. Compared to TiRGN and DHyper, DECRL predicts more correct relations and ranks the correct predictions higher. The results indicate that by modeling the temporal evolution of the high-order correlations among entities, DECRL can achieve more accurate prediction results."}, {"title": "Societal impacts", "content": "In this paper, a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL) is proposed for event prediction in temporal knowledge graphs, which offers more accurate and credible results. We summarize the positive and possible negative societal impacts as follows:\nPositive societal impacts:\n\u2022 Optimizing social governance: Temporal knowledge graph event prediction can help governments and relevant institutions foresee potential future events, enabling them to take preemptive measures and optimize social governance.\n\u2022 Supporting business decisions: Companies can use the prediction results for market analysis and business decisions, enhancing their competitiveness.\n\u2022 Enhancing public safety: By predicting potential threats and dangerous events, relevant departments can allocate resources in advance to ensure public safety.\n\u2022 Advancing academic research: This research can promote progress in the fields of temporal knowledge graphs and event prediction, providing new directions and methods for academic studies.\nNegative societal impacts:\n\u2022 Misuse risks: Accurate predictive technology might be exploited by malicious actors to forecast and manipulate events."}]}