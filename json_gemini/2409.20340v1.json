{"title": "Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization", "authors": ["Osama Mustafa"], "abstract": "The application of deep learning in cancer research, particularly in early diagnosis, case understanding, and treatment strategy design, emphasizes the need for high-quality data. Generative AI, especially Generative Adversarial Networks (GANs), has emerged as a leading solution to challenges like class imbalance, robust learning, and model training, while addressing issues stemming from patient privacy and the scarcity of real data. Despite their promise, GANs face several challenges, both inherent and specific to histopathology data. Inherent issues include training imbalance, mode collapse, linear learning from insufficient discriminator feedback, and hard boundary convergence due to stringent feedback. Histopathology data presents a unique challenge with its complex representation, high spatial resolution, and multiscale features. To address these challenges, we propose a framework consisting of two components. First, we introduce a contrastive learning-based Multistage Progressive Finetuning Siamese Neural Network (MFT-SNN) for assessing the similarity between histopathology patches. Second, we implement a Reinforcement Learning-based External Optimizer (RL-EO) within the GAN training loop, serving as a reward signal generator. The modified discriminator loss function incorporates a weighted reward, guiding the GAN to maximize this reward while minimizing loss. This approach offers an external optimization guide to the discriminator, preventing generator overfitting and ensuring smooth convergence. Our proposed solution has been benchmarked against state-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model, outperforming previous SOTA across various metrics, including FID score, KID score, Perceptual Path Length, and downstream classification tasks", "sections": [{"title": "II. INTRODUCTION", "content": "Cancer especially breast cancer is a very common disease, in the UK and all around the world. Every year there are a large number of cases reported with the death rate eventually high. Cancer diagnosis and treatment is a complex and time-consuming process due to the multi-dimensional and non-static nature of cancer. Deep Learning has been utilized for tasks such as disease classification, tumor segmentation, and designing treatment strategies. This has helped the clinicians in early diagnosis and treatment of cancer. As in other domains, the speed of advancement towards Artificial General Intelligence (AGI) enabled with Multi-Modal Intelligence has been exponential, the speed has been observed to be comparatively slow in medical domain. Designing deep learning systems that are robust and transparent, it requires a large amount of data to train these systems. Data is a major concern in the medical domain due to patient privacy. Thus scarcity of data slows down the overall speed of Deep Learning advancement in this domain.\nGenerative Adversarial Networks (GANs) have been widely adopted for tasks such as synthetic data generation and style transfer [7]. Considering its success in other domains it has been extensively employed in the histopathology domain for tasks such as synthetic data generation and stain normalization. It has most widely been used for stain normalization and similar applications. An extensive literature review suggests, that researchers have done some remarkable work in the past in the field of generative histopathology. This includes the work of Runz et al [25] where they utilized cycle consistent generative adversarial networks (CycleGANs) to perform normalization of HE-stained histological images. In this work, Pegah Salehi et al. [26] employed a Pix2Pix network for stain-to-stain normalization in histopathology images. In another work, In this work [29], the authors have utilized a Generative Adversarial Network for stain normalization in federated learning based systems. GANs have been utilized for other auxiliary tasks such as for fibrosis detection and quantification in [22]. There are various other applications [3], [5], [6], [15], [16], [18], [19], [23], [28], [30], [32]. This is another interesting work where the authors Tibor Sloboda et al. [30] have performed editable stain transformation of histological images using unpaired GANs Most of the work is focused on style transfer and dealing with style and color in histopathology image data. Although GANs have been utilized extensively in this domain, there has been comparatively less work reported on synthetic image generation from noise. GANs have been previously used for tasks where the feature representation has been on a higher level but in histopathology images, due to high resolution and complex feature representation, it is comparatively difficult to train a model to generate histopathology images due to stability issues in GANs. A GAN learns to generate a synthetic image starting from random noise. There has been some work reported on GANs for image generation such as in this work [15] the authors Li et al. propose a multi-scale conditional GAN for image generation to be used in data augmentation. In another work on image generation [31], the authors propose HistoGAN for selective synthetic augmentation to improve downstream accuracy of classification.\nHowever, GANs suffer from various issues although they have been the mainstream tool for data generation. Some of these issues are inherent to GANs which have been identified by the inventors themselves, while the other class of issues are those that have relevance to histopathology data such as complex feature representation. One of the major issues inherent to GAN that has been reported widely in the literature is the training imbalance and mode collapse. As in a GAN, the generator and discriminator train simultaneously in adversary to each other, they have their respective objective functions, the discriminator acts as a critic. As the generator generates a synthetic image out of random noise, the discriminator acts as a binary classifier and predicts whether the generated image is real or fake, and as the training progresses the generator tries to fool the discriminator. As the accuracy of the discriminator reaches at its highest, it does not let the discriminator learn further and it falls into local minima. This is the point where the generator starts to learn and generate noise. This is known as mode collapse. This has been referred to as \"The Helvetica Scenario\" in the original GAN work by Goodfellow et el. [8]. In the same work the authors have also identified that the generator and discriminator should be well synchronized during training to avoid mode collapse. While it seems to be a simple solution but it is an active area of research as attaining this balance needs to be done by adding an auxiliary term which has also been identified as a future research direction in this work by the inventor Goodfellow et al. [8]. Therefore in our work, we propose a Reinforcement Learning powered External Optimizer (RL-EO) as an auxiliary term that guides the generator-discriminator adversary towards smooth convergence. Another issue that has been reported is that during training the discriminator is critic for the generator, based on feedback from the discriminator the gradients are calculated and a weight update is performed for the generator. But the discriminator in a standard GAN is a binary classifiers that does not provide sufficient information for proper convergence, as it just provides a binary feedback that whether the generated image is real or fake. This refers to if the discriminator does not learns well and starts to overfit the training data, it does not helps GAN in generating quality data which also leads to low generation quality. As been identified in the work by Goodfellow et al. [9], the model families we use are intrinsically flawed. Ease of optimization has come at the cost of models that are easily misled and many of the times the model is learning linear distribution of the input data and convex optimization-based training leads to easy convergence but linear nature. This issue relates to the inherent issue, that histopathology images are difficult to classify due to complex feature representation. Thus to solve this issue we introduce a representation learning-based Siamese Neural Network trained via contrastive loss to provide additional support to the discriminator. We modify the loss function of the discriminator to incorporate this. Some of the previous works have also experimented with integrating FID or Wasserstein Distance as a loss function in discriminator, and have identified that using these metrics actively in the training such as in loss function leads to the model learning linear distribution of the input data, thus overfitting to the input data.\nThus in our work, we propose solutions for major challenges reported in GANs. The major contributions are:\n\u2022 Identification of the core reasons that lead to major issues in GANs such as training imbalance, mode collapse, and hard convergence.\n\u2022 A robust technique to train a Siamese Neural Network through multistage progressive fine-tuning, used as a similarity score generator for real and generated histopathology images.\n\u2022 A novel Reinforcement Learning-powered External Optimizer (RL-EO) that generates a reward signal for the discriminator and guides the discriminator to ensure balanced training and smooth convergence."}, {"title": "III. METHODOLOGY", "content": "This section details the methodology of the proposed framework. Our proposed solution strengthens the critic ability of the discriminator in the adversarial training by combining weakly supervised contrastive loss based SNN with GAN. As illustrated in Figure ??, we propose a novel framework in which we incorporate an external optimizer in the adversarial loop between the Generator and Discriminator in a standard GAN. The noise and training images are input to the pipeline, while the output from external optimizer (RL-EO) is passed as a weak reinforcement learning based reward signal to the discriminator. In the following sections, more details are provided explaining our proposed framework.\nIn order to provide external guidance to the discriminator to enable it to be more robust in acting as an adversary to the generator and to balance the generator discriminator training, we propose Mulltistage Finetuned Siamese Neural Network (MFT-SNN) as an external guide for convergence, acting as a reinforcement learning reward signaller for the discriminator. The proposed solution comprises of two parts where details for each part will be provided separately in the following sections.\n\u2022 Part-I: MFT-SNN: Multistage Progressive Finetuning based Siamese Neural Network.\n\u2022 Part-II: MFT-SNN as an RL-EO (Reinforcement Learning based External Optimizer) for the Generator-Discriminator training loop."}, {"title": "A. MFT-SNN: Multistage Finetuning based Siamese Neural Network", "content": "In this section, we provide further details on MFT-SNN's training and configuration. Siamese Neural Networks [14] are a class of neural networks that serve the purpose of similarity measurement between a pair of input data. SNNs compute similarity by employing two identical weight-sharing networks. SNNs have been applied in tasks like signature forgery and document fraud detection, but limited studies report their use for similarity measurement in histopathology image data. The reason is that Histopathology images are very high resolution and multi-scale, thus they are first divided into patches. Thus the division of WSI into small patches increases the problems as the feature representation of a complete WSI is divided into patches. A challenge with histopathology data is that SNNs typically use convolutional networks to learn combined high- and low-level features, but in this case, the relevant information lies mostly in the low-level features. So\n1) Training Objective: SNN is provided pairs of images with labels 0 and 1 during training, indicating similarity and dissimilarity. Based on contrastive learning, the network is trained to minimize the distance between similar pairs and maximize the distance between dissimilar pairs.\n2) Feature Extractor: A Siamese Neural Network's primary component is a feature extractor that is trained to extract feature embeddings from the pair of input images. For feature extraction, there are a range of available options from the class of convolutional architectures such as ResNet[18,34,40,101,152], InceptionNet, ResNeXt, and further variants. It is an option whether a pre-trained model is utilized or training is performed from scratch. Thus in our work, we employ a VGG-16 Autoencoder-based pre-trained architecture trained on ImageNet. Experimentation has been performed by employing various architectures and with/without pretraining. Experiments have also been performed comparing the performance of simple encoder-based architectures and encoder-decoder-based architectures. Results show that for this specific application, a pre-trained VGG-16 Autoencoder-based Siamese Neural outperforms its competitors based on performance. During the selection of feature extractor for SNN it is critical to consider the inference time complexity as later it gets integrated into the live training loop of GANs where two neural networks are being trained in adversary, the delay due to computational overhead affects the complete training process and creates a bottleneck."}, {"title": "3) Proposed Training Strategy", "content": "In this section, details are provided regarding the proposed training strategy i.e multi-stage progressive fine-tuning based training of Siamese Neural Networks. The proposed strategy has two important components: Multistage and Progressive.\n\u2022 Multistage Training: This refers to the training process of the Siamese Neural Network (SNN) in two distinct stages. In the first stage, the SNN is trained on complete whole slide images, which are resized for computational efficiency without being divided into patches. After saving the weights from this stage, the same model is loaded to train on whole slide patches. In the second stage, the whole slide image is divided into patches, and the patch-level data is used to train the SNN.\n\u2022 Progressive Training: This refers to a two-stage training process in which the last layers of the pre-trained feature extractor are fine-tuned progressively. In the first stage, the last eight layers of the pre-trained feature extractor are fine-tuned using full-scale whole slide images. In the second stage, five of these eight layers are frozen, and the last three layers are fine-tuned again using patch-level data. Thus, the first stage involves fine-tuning all eight layers, while the second stage progressively fine-tunes the last three layers after freezing the first five.\nThis proposed strategy outperforms other strategies such as single-stage training, transfer learning, or single stage fine-tuning. By training on full whole slide images in the first stage, the SNN learns global context and spatial relationships present in the entire slide. This stage helps in capturing large-scale patterns and structures that are critical for understanding the context of the entire image. Refinement with patch-level details allows the network to focus on smaller, more localized features and details within the slides. Freezing and then gradually unfreezing layers progressively helps to mitigate the risk of catastrophic forgetting. The MFT-SNN is trained with a contrastive loss. The fact that it generalizes to different levels of dissimilarity in testing demonstrates that the embedding space is well-formed, capturing the nuanced relationships between the images beyond the binary labels provided during training.\n4) Contrastive Learning: The Siamese Neural Network during training receives binary labels 0 and 1 for dissimilar and similar pairs of images. After training, when tested on test data with different levels of dissimilarity the model outputs a decreased similarity score with increased levels of dissimilarity. The model learned to quantify the similarity between images and shows a gradient of similarity scores that decrease with increasing dissimilarity, even though it was trained on binary labels. This indicates that the embedding space created by the Siamese network effectively represents the similarity continuum, which is a desirable outcome in contrastive learning."}, {"title": "B. Mathematical Formulation for MFT-SNN Training", "content": "Let:\n\u2022 $L\u2081$ be the loss function used in Stage 1."}, {"title": "Stage 2: Training on Patch-Level Data", "content": "Let:\n\u2022 $L_2$ be the loss function used in Stage 2.\n\u2022 $O_{ft2}$ be the parameters fine-tuned in Stage 2 (last 3 layers).\n\u2022 $O_{frozen}$ be the parameters frozen from the first stage.\n\u2022 $D_{patch}$ be the patch-level dataset.\nSince the model from Stage 1 is used, we have:\n$O_{frozen} = O_{fixed} \\cup O_{ft1}$"}, {"title": "Combined Training Process", "content": "The overall training process is represented by the sequential optimization problems:\n1. Stage 1 Optimization:\n$O_{ft1}^* = \\underset{O_{ft1}}{argmin} \\mathcal{L}_1(D_{wsi}, O_{fixed}, O_{ft1})$\n2. Stage 2 Optimization:\n$O_{ft2}^* = \\underset{O_{ft2}}{argmin} \\mathcal{L}_2(D_{patch}, O_{fixed} \\cup O_{ft1}^*, O_{ft2})$"}, {"title": "C. Reinforcement Learning-Based External Optimizer (RL-\u0395\u039f)", "content": "This section details how the proposed MFT-SNN is integrated into GAN's generator-discriminator training loop. It is a critical decision. In the proposed architecture, the Multistage Progressive finetuning-based Siamese Neural Network (MFT-SNN) is integrated as a Reinforcement Learning-Based External Optimizer (RL-EO) in the standard Generator-Discriminator training loop. In this work, the base GAN we selected is a standard GAN in its pure form, to ensure the transparency of performance increase due to the proposed modifications. To solve the variety of issues presented above leading to mode collapse and hard convergence, we integrate MFT-SNN as a valuable external guide for optimization. During training, the real and generated image is passed as an input pair to the MFT-SNN which computes the cosine similarity based on their extracted feature representations, the similarity score is a real-valued term between 0 and 1. In this work, we explored an ideal way to integrate the score into GAN. There are several ways it could be done such as passing it to the Generator module and passing it to the discriminator module. The similarity score is passed to the Discriminator. The following subsection presents more details."}, {"title": "1) RL-EO Output as Reward Signal to Discriminator", "content": "The output similarity score is calculated based on the generated and the real image, and is passed on to Discriminator as a RL reward signal. It is not passed directly to the generator because doing so would disrupt the adversarial relationship central to GANs. If this principle is violated, the generator starts producing images that closely resemble the real input image, leading it to learn the linear distribution of the input data and resulting in overfitting. This, in turn, diminishes the diversity of synthetic data generated. The role of a Discriminator is to teach and push the generator to learn to generate synthetic data by learning a latent space based on input data distribution. Thus we pass the reward signal to the Discriminator. The signal is passed on as a weak RL signal by assigning it a weight. This is done to stop RL-EO from overimpacting the generator in the adversary."}, {"title": "2) Modified Discriminator Loss Function", "content": "Discriminator Loss\nThe original discriminator loss is calculated as the sum of the loss for real images and the loss for fake images:\n$\\mathcal{L}_{D} = \\mathcal{L}_{D}^{real} + \\mathcal{L}_{D}^{fake}$"}, {"title": "Reward Calculation", "content": "The reward is based on the average similarity score between real and fake images:\n$reward = 0.3 \\times mean(similarity\\_scores)$"}, {"title": "Modified Loss Function", "content": "The modified loss function for the discriminator, incorporating the reward, is given by:\n$\\mathcal{L}_{D}^{modified} = \\mathcal{L}_{D} - reward$"}, {"title": "Gradient Descent Update", "content": "The gradients for the discriminator are updated as follows:\n$\\nabla_{O_D} \\mathcal{L}_{D}^{modified}$"}, {"title": "3) Reward Interpretation", "content": "As the reward is subtracted from the discriminator's loss, a larger reward leads to a greater reduction in overall loss, resulting in loss minimization. Therefore, in line with Reinforcement Learning principles, the GAN acts as an agent that aims to maximize the reward, which effectively minimizes the loss. Thus we integrate Siamese in a way that it not directly over-impacts the training, but rather the discriminator based on the reward signal pushes the generator which is an adversary to generate images as close to real distribution as possible. The closer the images the higher the similarity score which is the objective of GAN as an agent following the RL principle."}, {"title": "D. Base GAN", "content": "This section details the base GAN utilized in this work. However, we aimed to select a pure GAN architecture as the base to clearly demonstrate the performance boost from our proposed modifications. The base GAN we utilized is a basic GAN with slight modifications proposed in this work by Goodfellow et al. [27]. DCGANs extend GANs using convolutional and convolutional-transpose layers in the discriminator and generator, respectively [24]. All model weights are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.02. In addition to label smoothing, separate mini-batches are created for real and fake images. The generator's objective is to maximize log(D(G(Z))) instead of minimizing log(1 - D(G(Z))). The training process begins by training the discriminator with all real images, calculating gradients in a backward pass. Next, it trains with all fake images, accumulating gradients. After combining real and fake losses, an optimizer step is performed. For the generator, a fake batch is passed, and loss is computed with the discriminator using real labels as ground truth, followed by an optimizer step based on the computed gradients. The generator loss is:\n$\\mathcal{L}_G = \\frac{1}{N} \\sum_{i=1}^N log D(G(z_i))$"}, {"title": "IV. EXPERIMENTAL PROTOCOL", "content": "This section details the experimental protocol followed throughout the experimentation for consistency and reproducibility. This includes the details regarding the dataset selected, train configuration, and any additional settings."}, {"title": "A. Dataset", "content": "In this work, we work with a public dataset named BACH [1]. It is from the ICIAR 2018 grand challenge on breast cancer histology images. The dataset provides high-resolution whole slide images acquired using Leica SCN400 acquisition system, along with corresponding annotation files. Each WSI represents a complete tissue, and each WSI can have multiple class regions."}, {"title": "B. Data Preparation", "content": "Whole slide images have high spatial resolution, making it computationally impractical to process them in full. Therefore, they are typically divided into patches, and we used patches of 64x64x3 resolution. For experimentation, we retained 100,000 patches to ensure a normal distribution. Additionally, it's important to manage the significant white space in the background of whole slide images, as it does not provide meaningful information. Thus we applied an automated technique that first segments the tissue component in the whole slide image and then divides that specific region into patches [17]."}, {"title": "1) Data for MFT-SNN", "content": "This part of the proposed solution is a Siamese Neural Network trained in two stages. For the first stage of training, the spatial resolution of WSIs is lowered down to 224x224x3 without dividing into patches. For the second stage of training, the WSIs are divided into patches. A total of 60,000 patches were selected for training in the second stage. The MFT-SNN requires images to be in pair format with binary labels assigned 0 or 1 corresponding to similar or dissimilar respectively. For training in the second stage, we prepared one similarity level and two dissimilarity levels as illustrated in Image 9. For each patch, we applied data augmentation including brightness, contrast, and noise. Different patches of same WSI and from different WSIs form two levels of dissimilarity."}, {"title": "2) Data for RL-EO Integrated GAN", "content": "This is the part of the proposed solution where the GAN is trained after the integration of Reinforcement Learning based External Optimizer (RL-EO). A total of 100,000 patches we selected for training the GAN."}, {"title": "C. Model Training", "content": "This section presents details regarding model training for the proposed solution including any architectures trained for benchmarking. This includes the train configuration set for training the models. All models are trained on a NVIDIA Tesla P100 16GB GPU hardware."}, {"title": "1) Training MFT-SNN", "content": "For MFT-SNN, the training is done in two stages as the proposed technique is a multistage progressive fine-tuning based SNN. For stage 1 of MFT-SNN training. a pretrained VGG-16 based Auto-encoder is fine-tuned on our dataset for 10 epochs. While the initial layers are freezed, only last 12 layers are allowed to be trainable. Further parameters are Batch Size: 32, Learning Rate for Reconstruction (Adagrad optimizer): 0.001, Learning Rate for Similarity (Adam optimizer): 0.0005. Two loss functions have been employed in this training: Contrastive loss for Siamese training and Mean Square Error as a reconstruction loss for the Autoencoder part.\nThe contrastive loss function is defined as:\n$\\mathcal{L}_{contrastive} = \\sum_{i=1}^N [(1 - Y_i) \\cdot d_i^2 + Y_i \\cdot (max(0, m - d_i))^2]$"}, {"title": "V. EVALUATION", "content": "Quantitative and qualitative evaluations have been performed to evaluate the performance of the proposed framework. In this work, we also performed evaluation on a downstream classification task to assess the quality of generated synthetic data."}, {"title": "A. Evaluation Metrics", "content": "This section details the evaluation metrics utilized in this work. In order to evaluate the quality of generated data in reference to the real training data, we utilized Fr\u00e9chet Inception Distance (FID) which was introduced by Heusel et al. in this work [12]. Before this work, many other metrics were commonly used such as Inception Score (IS), Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and visual turning test. After arrival of FID, there has been a general consensus on employing FID for evaluating generative models such as GANs and Diffusion Models. We also report the performance based on following metrics: Kernel Inspection Distance (KID), Precision, Recall, and F1-score."}, {"title": "FID Definition", "content": "The FID between the distributions of real images $p_r$ and generated images $p_g$ is defined as:\n$FID(p_r, p_g) = ||\\mu_r - \\mu_g ||^2 + Tr (\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{\\frac{1}{2}})$\nWhere $\u03bc_r$ and $\u03a3_r$ are the mean and covariance of the feature vectors for the real images. $\u03bc_g$ and $\u03a3_g$ are the mean and covariance of the feature vectors for the generated images. $||\u03bc_r \u2013 \u03bc_g ||^2$ is the squared Euclidean distance between the means. Tr denotes the trace of a matrix."}, {"title": "Calculation", "content": "To calculate FID in this work, we follow these steps:\n1) For computational feasibility and due to continuous reevaluation during experimentation, we set the number of samples in both real and generated data distributions to calculate FID.\n2) We select 5000 random images from real data and we generate 5000 images using trained Generator from the GAN.\n3) Then we calculate FID using a single FID [11] repository for all of the experiments reported in this work."}, {"title": "Perceptual Path Length", "content": "We also selected a non-conventional metric to evaluate the proposed framework. Perceptual Path Length (PPL) is deployed to evaluate the smoothness and continuity of the latent space in Generative Adversarial Networks (GANs). Two points Z1 and Z2 are randomly sampled from the latent space. A linear interpolation is performed between these two points to get intermediate latent vectors. For each interpolated latent vector, the GAN generates an image. The perceptual distance between consecutive images is measured. The path length is the sum of these perceptual distances along the interpolation path."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "This section presents the results and corresponding discussion. In this section, we benchmark the proposed solution against the SOTA. To provide the readers with a clear picture of the performance improvement because of our proposed solution, the comparison has been done at various levels including comparison with a standard GAN, improved standard GAN, GAN variants, and denoising diffusion probabilistic model."}, {"title": "Ensuring Consistency in Evaluation Metric", "content": "In the previous work, researchers have done some remarkable work and reported their results via FID score as a metric. However, during this study, we identified that there is no consistency found in the FID implementation being utilized to report the work. However, it is important to note that FID is calculated based on feature representations extracted using a feature extractor, and a number of different implementations online use a number of different feature extractors trained on different datasets. While some of the works do identify in their work that they are utilizing a custom feature extractor or any modification to report FID scores. To address this issue and maintain consistency in our reported results, we utilize a single FID implementation, and train all the other architectures that we make comparisons with on a single dataset, and report the FID scores for transparency."}, {"title": "Results", "content": "We have trained the following architectures on our dataset to benchmark the performance of our proposed solution. The dataset is an open-source challenge dataset, the FID repository is also open-source for reproducibility.\nDifferent variants of GANs exist in the literature. We trained Deep Convolutional Generative Adversarial Network DCGAN for 200 epochs and same configuration as of our proposed model's training. In addition to a standard GAN, it incorporates convolutional and convolutional-transpose layers to improve the stability and quality. There is an interesting extension of DCGAN, the improved DCGAN [27] in which the authors propose several modifications such as separate real and mini batches, gradient clipping and more, we trained it for same number of epochs and train configuration respectively. It is the same architecture on top of which we propose our solution. Another important variant of GAN is Spectral Normalization GAN (SN-GAN) [21], where authors propose an approach to achieve training stability in a GAN by applying spectral normalization to the weights. This was also trained with the same train configuration. Two important variants that we trained on the selected dataset are Wasserstein GAN with Gradient Penalty (WGAN-GP) [10] and Least Squares GAN (LSGAN) [20]. In wasserstein GAN, the authors propose an approach in which the discriminator as a critic outputs real-valued scores using Wasserstein distance as a distance measure between real and generated images. They also replace weight clipping with gradient penalty. LSGAN utilizes least square loss. In addition to popular GAN variants, in this study we also train a Denoising Diffusion Probabilistic Model (DDPM) [13] on the selected dataset."}, {"title": "Evaluation on a Downstream Classification Task", "content": "We trained a classification model on synthetic data and tested it on real data, and compared the performance against a model trained on real data and tested on real data. This is a fundamental approach to test the quality of generated data."}, {"title": "1) Data Preparation", "content": "We selected the BACH dataset for this task. We selected two classes to proceed further with i.e. Benign and Invasive. The patch size we locked is 64x64. For the real test set, we selected a well-balanced distribution of 2000 Benign images and 7000 Invasive images. For model training on real data, we took a well-balanced distribution of 13000 Benign images and 23000 Invasive images. For model training on synthetic (generated) data, we took a well-balanced distribution of 10000 Benign images and 20000 Invasive images. For training, we selected a 70-30 train-valid split."}, {"title": "2) Model Training", "content": "We fine-tuned a pre-trained ResNet-152 by freezing all layers except the last 16 layers. We then concatenated a fully connected layer with 1024 units. Final layer units match the number of classes with softmax activation. The train configuration is as follows: epochs: 10, batch size: 32, Optimizer: Adam with lr=1e-5, LR Scheduler with a step size of 7, and gamma=0.1."}, {"title": "A. Results", "content": "For model training on synthetic data, the best validation accuracy is 93.27%. The test set is the same having the distribution mentioned above, and is completely unseen to the model. The test results are:\n1) The model trained on synthetic data, and tested on data gives a test accuracy of 77.41%. Class-wise accuracies are Benign: 75.0% and Invasive: 99.21%.\n2) The model trained on real data and tested on the same real data gives a test accuracy of 73.24%. Class-wise accuracies are Benign: 48.95% and Invasive: 80.19%.\nAs the classifier when trained on synthetic data performs much better than when trained on real data, this validates that the proposed GAN framework is able to generalize well and capture significant patterns in the data. It suggests that the synthetic data produced by the proposed GAN is not only representative of the real data but also provides a better diversity and balance of features compared to the real data, especially when the class-wise accuracy of the synthetic trained classifier on the Invasive class is 99.21%. As in this paper, we are proposing this work, in our future work we will test the scale of generalization on very large datasets and deeper networks."}, {"title": "VII. CONCLUSION", "content": "This study represents a significant contribution to the field of Generative AI, particularly in the context of GANs applied to histopathology image generation. We introduce a novel framework that incorporates Contrastive Learning-based Multistage Progressive Finetuning SNN and RL-based External Optimization. Our findings demonstrate that integrating an external guide during adversarial training enhances the discriminator's role as a critic, which strengthens the overall adversarial process. This improvement enables the generator to produce higher-quality images while maintaining balanced training to mitigate mode collapse in GANs.\nBy addressing some inherent issues within GANs, we lay the groundwork for future research. Our goal is to scale up this model using datasets from diverse domains and varying sizes. We encourage the research community to build upon this work and explore further advancements in this direction."}]}