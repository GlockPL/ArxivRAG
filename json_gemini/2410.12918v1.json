{"title": "Boosting Asynchronous Decentralized Learning with Model Fragmentation", "authors": ["Sayan Biswas", "Anne-Marie Kermarrec", "Alexis Marouani", "Rafael Pires", "Rishi Sharma", "Martijn de Vos"], "abstract": "Decentralized learning (DL) is an emerging technique that allows nodes on the web to collaboratively train machine learning models without sharing raw data. Dealing with stragglers, i.e., nodes with slower compute or communication than others, is a key challenge in DL. We present DIVSHARE, a novel asynchronous DL algorithm that achieves fast model convergence in the presence of communication stragglers. DIVSHARE achieves this by having nodes fragment their models into parameter subsets and send, in parallel to computation, each subset to a random sample of other nodes instead of sequentially exchanging full models. The transfer of smaller fragments allows more efficient usage of the collective bandwidth and enables nodes with slow network links to quickly contribute with at least some of their model parameters. By theoretically proving the convergence of DIVSHARE, we provide, to the best of our knowledge, the first formal proof of convergence for a DL algorithm that accounts for the effects of asynchronous communication with delays. We experimentally evaluate DIVSHARE against two state-of-the-art DL baselines, AD-PSGD and SWIFT, and with two standard datasets, CIFAR-10 and MovieLens. We find that DIVSHARE with communication stragglers lowers time-to-accuracy by up to 3.9\u00d7 compared to AD-PSGD on the CIFAR-10 dataset. Compared to baselines, DIVSHARE also achieves up to 19.4% better accuracy and 9.5% lower test loss on the CIFAR-10 and MovieLens datasets, respectively.", "sections": [{"title": "1 Introduction", "content": "Decentralized learning (DL) is a collaborative learning framework that allows nodes on the web to train a machine learning (ML) model without sharing their private datasets with others and without the involvement of a centralized coordinating entity (e.g., a server) [38]. During each round of DL, nodes independently train their models using their private dataset. Based on a specified communication topology, the updated local models are then exchanged with neighbors over the Internet and aggregated at each recipient node. The aggregated model serves as the starting point for the next round, and this process continues until convergence. This approach enables web-based applications, such as recommender systems [6, 13, 40] or social media [10, 27], to collaboratively leverage the capabilities of ML models in a privacy-preserving and scalable manner. Notable DL algorithms include Asynchronous decentralized parallel stochastic gradient descent (AD-PSGD) [39], Gossip learning (GL) [47], and Epidemic learning (EL) [12].\nIt is natural for nodes in any real-world network to have different computation and communication speeds. While the focus on DL has been surging due to its wide range of applicability [7], most existing works in DL consider a synchronous system without the presence of stragglers, i.e., nodes with slower compute or communication speeds than others [9, 12, 18, 39, 43, 54]. In synchronous DL approaches such stragglers can significantly prolong the time required for model convergence as the duration of a single round is typically determined by the slowest node [38]. Ensuring quick model convergence in the presence of stragglers and reducing their impact is crucial to improving the practicality of DL systems.\nThis work deals with communication stragglers in DL. This form of system heterogeneity is particularly present in web-based systems where nodes are geographically distributed and inherently have variable network speeds, resulting in delayed communications [20]. For instance, the network speeds of web-connected mobile devices can differ by up to two orders of magnitude [35]. Even when nodes are deployed over cross-region AWS instances, their network bandwidth can vary by up to 20x [20].\nIn the context of DL, this variability in communication speed results in slower model convergence. In most DL algorithms, a node sends its full model to a few other nodes (Fig. 2, left). Nodes with slow network links require more time to transfer larger models, which can lead to two negative outcomes for their parameter updates: (i) they are received later and become stale by the time they are merged, or (ii) they are ignored entirely as the recipient proceeds with aggregation without waiting for the contributions of slow nodes. Additionally, from the perspective of a sending node with a fast network link, a slow recipient can block valuable bandwidth. In many DL algorithms, a recipient only proceeds with aggregation after receiving the full model. Rather than communicating with faster nodes, the sender must wait, further exacerbating the straggling effect and hindering system performance.\nWe highlight the impact of communication stragglers in DL with an experiment where we measure the model convergence of AD-PSGD [39] and SWIFT [9], two state-of-the-art asynchronous DL algorithms, in a 60-node network. We consider an image classification task with the CIFAR-10 dataset [34] and use a non independent and identically distributed (non-IID) data partitioning [14, 42]. Fig. 1 shows the test accuracy of AD-PSGD and SWIFT over time, without communication stragglers (green curves) and with communication stragglers (red curves) where half of the nodes have 5x slower network speeds than others. For both AD-PSGD and SWIFT, we observe a significant reduction in model convergence speeds. To reach 56% test accuracy, AD-PSGD and SWIFT require 1.9\u00d7 and 2.2\u00d7 more time, respectively, in the presence of communication stragglers compared to when they are absent.\nTo address this issue, we introduce DIVSHARE: a novel asynchronous DL algorithm. DIVSHARE converges faster and achieves better test accuracy compared to state-of-the-art baselines in the presence of communication stragglers. Specifically, after finishing their local training (computation), nodes in DIVSHARE fragment their models into small pieces and share each fragment independently with a random set of other nodes (Fig. 2, right). Transferring smaller fragments allows nodes with slow network links to quickly contribute at least with some of their model parameters. Furthermore, the independent dissemination of fragments to random sets of nodes enables more efficient use of the collective bandwidth as model parameters reach a bigger stretch of the network. As a result, DIVSHARE exhibits stronger straggler resilience and attains higher model accuracy compared to other asynchronous DL schemes.\nContributions. Our work makes the following contributions:\n(1) We introduce DIVSHARE, a novel asynchronous DL approach that enhances robustness against communication stragglers by leveraging model fragmentation (Sec. 3).\n(2) We provide a theoretical proof of the convergence guarantee for DIVSHARE (Sec. 4). To the best of our knowledge, we are the first to present a formal convergence analysis in DL that captures the effect of asynchronous communication with delays. In particular, the convergence rate is influenced by the number of participating nodes, the properties of the local objective functions and initialization conditions, the parameter-wise communication rates, and the communication delays in the network.\n(3) We implement and evaluate DIVSHARE on two standard learning tasks (image classification with CIFAR-10 [34] and recommendation with MovieLens [21]) and against two state-of-the-art asynchronous DL baselines: AD-PSGD and SWIFT (Sec. 5). We demonstrate that DIVSHARE is much more resilient to the presence of communication stragglers compared to the competitors and show that DIVSHARE, with communication stragglers, speeds up the time to reach a target accuracy by up to 3.9\u00d7 compared to AD-PSGD on the CIFAR-10 dataset. Compared to both baselines, DIVSHARE also achieves up to 19.4% better accuracy and 9.5% lower test loss on the CIFAR-10 and MovieLens datasets, respectively."}, {"title": "2 Background and preliminaries", "content": "This work focuses on a scenario where multiple nodes collaboratively train ML models. This approach is often referred to as collaborative machine learning (CML) [48, 52]. In CML algorithms, each node maintains a local model and a private dataset. The private dataset is used to compute model updates and remains on the node's device throughout the entire training process."}, {"title": "2.1 Synchronous and asynchronous DL", "content": "Decentralized learning (DL) [4, 38, 44] is a type of CML algorithm in which nodes exchange model updates directly with other nodes. The majority of DL algorithms are synchronous, meaning they rely on global rounds where nodes perform computations in parallel, followed by communication with neighbors. In each round, nodes generally train their model using local data, exchange them with neighbors, and aggregate them before starting the next round. This synchronized process ensures consistency and predictable model convergence since the system progresses in synchronized rounds. However, this process also introduces inefficiencies in the presence of slower nodes (stragglers) as the system needs to wait for the slowest node to complete its computation and communication before progressing to the next round [17]. Thus, synchronous DL approaches can suffer from significant delays, particularly in settings with high variability in computing or communication speeds.\nIn contrast, asynchronous DL algorithms forego the notion of synchronized rounds allowing nodes to make progress independently [3]. Thus, faster nodes can continue updating and sending their models independently, which helps to mitigate the delays caused by stragglers. Designing asynchronous DL algorithms is a recent and emerging area of research [5, 9, 17, 41]. However, this asynchrony introduces new challenges. For instance, slower nodes spread possibly outdated model updates and slow down convergence for the entire network. The performance of local models can also get biased towards the faster nodes as their parameters are shared and mixed faster than those of the slower nodes."}, {"title": "2.2 System model", "content": "Our study specifically tackles the issue of communication delays in DL. We assume a setting where geo-distributed nodes communicate their locally trained models at their own pace independent of each other. We design DIVSHARE to be deployed within a permissioned network setting where node participation is static. This assumption is consistent with the observation that DL is commonly used in enterprise settings, where participation is often controlled [7, 8, 12]. Nodes also remain online throughout the training process. Furthermore, we assume that nodes in DIVSHARE faithfully execute the algorithm and consider threats such as privacy or poisoning attacks beyond scope. For clarity and presentation, we assume the nodes have comparable computation infrastructure that allows them to compute (e.g., perform their local training and model aggregation) at the same speed. We further discuss this aspect in Appendix E."}, {"title": "3 Design of DIVSHARE", "content": "We first describe the high-level operations of DIVSHARE in Sec. 3.1 and then provide a detailed algorithm description in the remaining subsections. A summary of notations is provided in Appendix A."}, {"title": "3.1 DIVSHARE in a nutshell", "content": "The main idea of DIVSHARE is that nodes fragment their model and send these fragments to a diverse, random set of other nodes. Sharing models at a finer granularity allows communication stragglers to contribute at least some of the model parameters quickly while still allowing nodes with fast communication to disseminate all their model fragments. Fragmentation is also motivated by our observations that, for an equal amount of communication, sending smaller model parts to more nodes results in quicker convergence than sending full models to a few nodes (see Sec. 5.3). We illustrate the workflow of DIVSHARE in Fig. 3 where we show a timeline of operations for two nodes: $N_i$ with fast and $N_j$ with slower communication speeds. The computation and communication operations are shown in the top and bottom rows for each node, respectively, where the $k$th local round of node $N_i$ is denoted by $t_i^k$.\nDuring a local round, each node maintains a receive buffer for received model parameters and a send queue for model fragments paired with destination identifiers, awaiting transmission. In each round, independently of others, a node $N_i$: (i) aggregates the model fragments received in local round $t-1$ (purple in Fig. 3); (ii) carries out SGD steps for local training (green); (iii) fragments the updated model to share in the next round (yellow); and (iv) clears the send queue and adds new pairs of model fragments and receiver identifiers. During steps (ii) and (iii), $N_i$ continues communicating fragments of its model from local round $t$ at its own pace.\nEach blue block in Fig. 3 indicates the transfer of a fragment to another node. We note that node $N_j$ with slow communication speeds may only manage to send a few fragments before it computes freshly updated model fragments, which then causes a flush of the send queue. This scenario is illustrated in Fig. 3, where unsent fragments are shown in red."}, {"title": "3.2 Problem formulation", "content": "Decentralized learning. We consider a set of $n \\geq 2$ nodes $N = \\{N_1,..., N_n\\}$, where $N_i$ denotes the $i$th node in the network for every $i \\in [n]$, who participate in this collaborative framework to train their models. $Z$ denoting the space of all data points, for each $i \\in [n]$, let $Z_i \\subset Z$, with $|Z_i| < \\infty$, be the local dataset of $N_i$. Let $N_i$'s data be sampled from a distribution $D^{(i)}$ over $Z$ and this may differ from the data distributions of other nodes (i.e., for each $z \\in Z_i, z \\sim D^{(i)}$). Staying consistent with the standard DL algorithms, we allow each node to have their local loss functions that they wish to optimize with their personal data. In particular, setting $d \\in \\mathbb{N}$ as the size of the parameter space of the models, for every $i \\in [n]$, let $f^{(i)} : \\mathbb{R}^d \\times Z \\leftrightarrow \\mathbb{R}_{\\geq 0}$ be the loss function of node $N_i$ and it aims to train a model $x$ that minimizes $f^{(i)}(x) = \\mathbb{E}_{z\\sim D^{(i)}}[f^{(i)}(x,z)]$. In practice, in every local round $k$, for its local training, each node independently samples a subset, referred to as a mini-batch, of points from their personal dataset and seeks to minimize the average loss for that mini-batch by doing SGD steps with a learning rate $\\eta > 0$. Hence, for any $i \\in [n]$, letting $\\xi^{(k)}$ to be the mini-batch sampled by $N_i$ in its local round $t_i^k$, we abuse the notation of $N_i$'s local loss for a single data point to denote the average loss for the entire mini-batch $\\xi^{(k)}$ computed for any model $x \\in \\mathbb{R}^d$ given by $f^{(i)}(x, \\xi^{(k)}) = \\mathbb{E}_{z \\in \\xi^{(k)}} f^{(i)}(x, z)$. The training objective of DIVSHARE is to collaboratively train the optimal model $x^* \\in \\mathbb{R}^d$ that minimizes the global average loss:\n$x^* = \\arg \\min_{x \\in \\mathbb{R}^d} F(x)$, where $F(x) = \\sum_{i \\in [n]} f^{(i)}(x)$.\nAsynchronous framework. For any given $i \\in [n]$ and $k = 1, 2, . . .$, while local rounds $t_i^k$, . . . capture each node's progress in its compute operations and communication (as described in Sec. 3.1), we introduce the notion of global rounds to track the network's overall training progress. Similar to the formalization made by Even et al. [17], we define global rounds $T_0 < T_1 . . .$, where at each $T_k$, a non-empty subset of nodes update their models (i.e., aggregate received models, perform local SGD steps, and fragment the updated model). Between two global rounds, $T_k$ and $T_{k+1}$, none to plenty of communication may asynchronously occur between"}, {"title": "3.3 The DIvSHARE algorithm", "content": "We now formally describe the DIVSHARE algorithm from the perspective of node $N_i$. A node in DIVSHARE executes three processes in parallel and independently of other nodes: (i) model aggregation, training and fragmentation (computation tasks, see Alg. 1 and Alg. 2), (ii) model receiving (Alg. 3), and (iii) model sending (Alg. 3). As mentioned before, each node keeps track of a receive buffer with incoming model fragments it has received during a local round, referred to as InQueue, and a send queue with model fragment and node destination pairs, referred to as OutQueue.\nComputation tasks. We formalize the computation tasks that a node $N_i \\in N$ conducts in Alg. 1. $N_i$ first initializes model $x^{(i,0)}$, and all nodes independently do this model initialization. In each of the $k = 1,..., T$ local rounds, where $T$ is a system parameter, $N_i$ first performs parameter-wise aggregation of all parameters in $x^{(i,k-1)}$ and all received fragments present in InQueue that were received in the previous round with uniform weights (Line 3). We note that the count of each received parameter by $N_i$ may differ. $N_i$ then resets InQueue (Line 4) and starts updating its model by performing H local SGD steps using a mini-batch sampled from its local dataset. The resulting model $x^{(i,k)}$ is then fragmented (Line 9), and the fragments are shared with other nodes in parallel to the computation during the next local round (as shown in Fig. 3).\nAlg. 2 outlines how nodes in DIVSHARE fragment their models and fill the send queue. Model fragmentation is dictated by the fragmentation fraction $\\Omega$, which specifies the granularity at which a model is fragmented. Specifically, a model is fragmented in $\\left[\\frac{1}{\\Omega}\\right]$ fragments. This resembles random sparsification, a technique used in CML to reduce communication costs [8, 23, 31, 53]. For each fragment $f$, we uniformly randomly sample J other nodes to send this fragment to (Line 5), and add each recipient node $N_j$ and the corresponding fragment as tuple $(j, f)$ to sending queue. Finally, we shuffle the order of (destination, fragment) pairs in the sending queue. This shuffling is important to ensure that slow nodes send diverse sets of model parameters within a single, local round. While we uniformly random shuffle the sending queue in our experiments, we acknowledge that different shuffling strategies can be used, e.g., we could prioritize the sending of more important parameters as done in sparsification [2, 15, 30].\nCommunication tasks. For every $i, j \\in [n]$ with $i \\neq j$, when receiving a model fragment $f$ from $N_j$, $N_i$ adds the parameters in $f$ to the receive buffer InQueue, associated with node $N_j$. If a parameter $i$ is received twice from $N_j$ during a particular local round, the older parameter InQueue[j][i] will be replaced with the latest one. In addition, $N_i$ runs a sending loop where it continuously sends information in OutQueue. Specifically, $N_i$ pops the tuple $(j, f)$ from OutQueue and sends fragment $f$ to node $N_j$. Due to space constraints, we provide the associated logic in Alg. 3 in Appendix D."}, {"title": "4 Convergence analysis", "content": "In this section, we theoretically analyze the convergence guarantees of DIVSHARE from the perspective of the global rounds. As discussed in previous works on asynchronous decentralized optimization [17, 32], in order to allow the nodes to hold heterogeneous data and their individual local objective functions, we need to make assumptions on the computation sampling. In particular, recalling that this work focuses on communication straggling in the network, we assume computation homogeneity i.e., every node computes in each round.\nNotations. $||.||$ denotes the Euclidean norm for a vector and the spectral norm for a matrix. A function $f$ is convex if for each $x, y$ and subgradient $g \\in df(x), f(y) \\geq f(x) + <g, y-x>$. When $f$ and $f^{(i)}$ are convex, we do not necessarily assume they are differentiable, but we abuse notation and use $\\nabla f(x, \\xi)$ and $\\nabla f^{(i)} (x)$ to denote an arbitrary subgradient at $x$. $f$ is $B$-Lipschitz-continuous if for any $x, y \\in \\mathbb{R}^d$ and $z \\in Z, |f(x, z) - f(y, z)|\\leq B||x - y||$. $f$ is $L$-smooth if it is differentiable and its gradient is $L$-Lipschitz-continuous.\nWe assume that in each round every node independently connects with other nodes to share each of its model fragments. In particular, for any $i, j \\in [n]$, let the probability that $N_j$ shares each of its model fragments with $N_i$ be $\\alpha_n$, making J the expected number of nodes that receive each of $N_j$'s model parameters.\nTo capture the effect of communication stragglers, let $k_{ji}$ denote the number of global rounds it takes for node $N_j$ to send one of its model fragments to node $N_i$. Consequently, define $K_j = \\max_{1<i<n} k_{ji}$ as the maximum delay for node $N_j$ to communicate with its neighbors, $K = \\max_{1<j<n} K_j$ as the global maximum communication delay, and $T = \\sum_{1<j<n} K_j$ as the total communication delay. We assume $K$ (and, therefore, $T$) to be finite. For any $i \\in [d]$, the model update $x^{(i,k)}$ is aggregated as:\n$x^{(i,k)} = \\frac{1}{1 + R_{i,k}} (x^{(i,k)} + \\sum_{1 \\leq j \\leq n} x_j^{(i,k - k_{ji})} 1(A_{jk-k_{ji},i}))$\\\nwhere $R_{i,k} = \\sum_{1 \\leq j \\leq n, j \\neq i} 1 (A_{jk-k_{ji},i})$ with $A_{jk-k_{ji},i}$ being the event where $N_j$ shared its model parameter $i$ in a fragment with $N_i$ in round $k - k_{ji}$. Note that $1 + R_{i,k}$ is the normalization factor and is always greater than 1 as the buffer always contains the $N_i$'s own model.\nWe refer to $X^i = (x_j^{(i,k - k_i)})_{1 \\leq i \\leq n, \\\\ 1 \\leq k_i \\leq K_i}$ as the sliding window of the network-wide $i$th model parameter containing all the information needed to generate a new global step in the algorithm, we enable ourselves to write losses on the sliding window as the vector of the losses. This communication step can be encoded by a random matrix $W_k$ representing the shift of the sliding window from $(x_j^{(i,k - k_i)})_{1 \\leq i \\leq n, \\\\ 1 \\leq k_i \\leq K_i}$ to $(x_j^{(i,k+1 - k_i)})_{1 \\leq i \\leq n, \\\\ 1 \\leq k_i \\leq K_i}$ by generating a new step using Eq. (1). Setting $\\alpha_{j,k - k_{ji},i} = \\frac{1(A_{jk-k_{ji},i})}{1 + R_{i,k}}$ as the initial weight, for every $i, j \\in [n]$ and $1 \\leq k_j \\leq K_j$, formally $W^k$ can be expressed as:\n$(w^k)_{ (i,k_i),(j,k_j)} =  \\begin{cases}  \\alpha_{j,k - k_{ji},i} \\text{ if } 2 \\leq k_i \\leq K_i \\\\  \\delta_{i,j} \\delta_{k_i - 1,k_j} + \\alpha_{j,k - k_{ji},i} \\text{ when } k_i = 1  \\\\  0 \\text{ otherwise } \\end{cases}$\nwhere, for any $\\alpha, \\beta \\in \\mathbb{R}, \\delta_{\\alpha,\\beta}$ is equal to 1 when $\\alpha = \\beta$, or 0 otherwise. For any $k > K$ and $k_e \\geq 1$, let $W^{(k:k+k_e-1)} = (W_{k+k_e-1}...W_k)$.\nTo develop the theoretical study of convergence of DIVSHARE, in addition to assuming the existence of the minimum of the loss function F, we make the standard set of assumptions (Assumptions 1 to 3) that are widespread in related works [8, 16, 17] and introduce Assumption 4 that is specific to the environment with asynchronous and delayed communication that we are considering.\nAssumption 1 (Condition on the objective). Denoting the minimum loss over the entire model space as $F^* = \\min_{x \\in \\mathbb{R}^d} F(x)$, let $A$ be an upper bound on the initial suboptimality, i.e., $A \\geq ||F(X)) - F^*||$, and let $D$ be an upper bound on the initial distance to the minimizer, i.e., $D \\geq \\min \\min ||X^{(K)} - (x^*)^i||_1$, where $x^* = \\arg \\min_{x \\in \\mathbb{R}^d} F(x)$ is the model that minimizes the loss and is assumed to exist.\nAssumption 2 (Condition on gradients). There exists $\\sigma^2 > 0$ such that for all $x \\in \\mathbb{R}^d, i \\in [n]$, we have $\\mathbb{E}_{x \\sim D^{(i)}}[\\nabla f^{(i)} (x, \\xi)] = \\nabla f^{(i)} (x)$ and $\\mathbb{E}_{x \\sim D^{(i)}}||\\nabla f^{(i)} (x) ||^2 < \\sigma^2$.\nAssumption 3 (Heterogeneous setting). There exists $\\lambda^2 > 0$ such that the population variance is bounded above by $\\lambda^2$, i.e., for all $x \\in \\mathbb{R}^d$, we have $\\sum_{i \\in [n]} ||\\nabla f^{(i)}(x) - \\nabla F(x)||^2 \\leq \\lambda^2$.\nAssumption 4 (Straggling-communication balance). For any $i \\in [n]$ and $k \\in \\mathbb{Z}_{\\geq 0}$, let $\\alpha^{(1)} = (1 - (1 - \\frac{J}{n-1})^n)$ and $\\alpha = (1 - (1 - \\frac{J}{n-1}))$. Then we assume that $(T - n) (\\frac{(\\alpha_n)^2}{T} + \\frac{\\alpha^2}{(\\alpha_n)^2 + \\alpha^2}) < 1$.\nRemark 1. Observing that T n equals 0 when the system is synchronous and increases as the sum of the delays grows, it effectively parameterizes the total amount of straggling in the system. On the other hand, the term $\\frac{1}{(\\alpha_n)^2 + \\alpha^2}$ can be interpreted as the communication rate - it decreases as J, the expected number of neighbors to which a node sends each of its model parameters, increases. This implies that communication speeds up as nodes engage in more frequent interactions within the network. Assumption 4 essentially strikes a balance by bounding the combined effects of straggling and the communication rate in the network. Appendix G discusses the asymptotic properties of Assumption 4 and provides analytical insights into the relationship between the average communication delays and the number of nodes in the network. This, in turn, demonstrates the practicality of adopting Assumption 4 in real-world settings and highlights the robustness of DIVSHARE in handling communication stragglers.\nTheorem 1 (Convergence of DIVSHARE). Under Assumptions 1 to 4 and if $f^{(i)}$ is $L$-smooth for all $i \\in [n]$, then $\\mathbb{E}_k ||\\nabla F(X_k)||^2 = 0$\n$\\left( \\frac{1}{k} \\sqrt{ \\frac{\\left(2 + \\lambda^2\\right) \\left(\\sigma^2 + \\lambda^2\\right)}{n}} + \\frac{\\lambda}{k} + \\frac{\\sqrt{\\sigma^2 + \\lambda^2} \\sqrt{\\ln n}}{n \\sqrt{k}} + \\frac{\\Lambda}{\\sqrt{k}} \\right)^2$\nwhere $\\Lambda^2 = ||\\mathbb{E} \\left[W\\right] \\Pi_F||$ with $\\Pi_F$ being the canonical projector on $F = \\{\\Pi_j: \\|\\Pi_j|| = 1\\}$, $\\Lambda = (\\alpha \\|\\log(\\delta_2)\\| + (1 - \\alpha) \\log(T)) \\alpha^{-1} \\|\\log(\\delta_2)\\|^{-2}$, and $\\hat{I} = LA$. In the interest of space, the proof is postponed to Appendix F.\nRemark 2. Th. 1 essentially shows how fast the average model of all the nodes parameter-wise converges. First, the slowest term, number-of-steps wise, does not depend on $\\Lambda$ thus on the delays, achieving the sub-linear rate of O $\\left(\\frac{1}{\\sqrt{k}}\\right)$ rounds, optimal for SGD, we achieve this by considering the convergence over a time sliding-window. For the numerators of the second and third terms, the rate is encoded in Assumption 4 with the coefficient $\\Lambda$ going to zero as $\\frac{1}{(\\frac{\\Lambda}{\\sqrt{k}})} (T-n) (\\frac{(\\alpha_n)^2}{T} + \\frac{\\alpha^2}{(\\alpha_n)^2 + \\alpha^2})$ goes to 0 (see Eq. (4) in Appendix F)."}, {"title": "5 Evaluation", "content": "We now describe our experimental setup (Sec. 5.1), compare DIVSHARE to baselines (Sec. 5.2), evaluate the sensitivity of DIVSHARE to its parameters (Sec. 5.3), and quantify the performance of DIVSHARE and baselines in real-world network conditions (Sec. 5.4)."}, {"title": "5.1 Experimental setup", "content": "Implementation. We implement DIVSHARE in Python 3.8 over DecentralizePy [14] and PyTorch 2.1.1 [49] to emulate DL nodes. For emulating communication stragglers, we used the KOLLAPS [19]"}, {"title": "5.2 Convergence of DIVSHARE against baselines", "content": "We first evaluate the convergence of DIVSHARE against the baselines. Fig. 4 shows the evolution of model utility over time, for both CIFAR-10 and MovieLens, in a setting without stragglers (with $f_s = 1$) and where half of the nodes are stragglers (with $f_s = 5$). Both baselines show comparable performance in all settings. We also observe that communication stragglers significantly slow the convergence of both AD-PSGD and SWIFT. Specifically, in the presence of communication stragglers, the baselines reach 12.5% and 10.2% worse model utilities in CIFAR-10 and MovieLens, respectively, compared to the scenario without stragglers. DIVSHARE outperforms both baselines in terms of the speed of convergence and model test utility across both datasets. The superior performance of DIVSHARE is especially evident in the presence of stragglers, achieving up to 19.4% relatively better accuracy and 9.5% lower test loss for the CIFAR-10 and MovieLens datasets, respectively. We attribute this to the ability of DIVSHARE to effectively aggregate models in small fragments and use the available bandwidth effectively."}, {"title": "5.3 Sensitivity analysis", "content": "In the following, we analyze the effect of the straggling factor $f_s$, varying levels of non-IIDness, and the fragmentation fraction $\\Omega$ on the performance of DIVSHARE and baselines.\nVarying the degree of communication straggling. We next explore the effect of the straggling factor $f_s$ and varying number of stragglers on the performance of AD-PSGD and DIVSHARE. Fig. 5 shows the heatmaps for the (a) final test accuracy after 15 min and (b) wall-clock time to achieve 60% test accuracy on CIFAR-10 for a varying number of stragglers and increasing $f_s$. Similar to Fig. 4, we observe that communication stragglers hinder the convergence of AD-PSGD. With only $n/8$ (7 of the 60) nodes being communication stragglers, increasing $f_s$ from 1 to 5 increases the time to reach the target accuracy by 43.4%. AD-PSGD is unable to attain the target accuracy of 60% with $n/2$ (30 out of 60) communication stragglers. In contrast, DIVSHARE displays minimal deviation from an ideal setting without stragglers as the number of stragglers and $f_s$ increases. As shown in Fig. 5(a), DIVSHARE consistently achieves better test accuracy compared to AD-PSGD and shows a speedup of at least 2.2\u00d7 over AD-PSGD. In the case of 15 stragglers ($n/4$) and $f_s = 5$, DIVSHARE achieves a speedup of 3.9\u00d7 over AD-PSGD to reach the same accuracy. Experiments with the MovieLens dataset shows similar trends and are provided in Appendix C. To conclude, DIVSHARE retains its strong performance even when half of the nodes are up to 5x slower in communication than the others.\nEffect of data heterogeneity. We analyze the effect of varying levels of data heterogeneity and $f_s$ on the time-to-accuracy speedup in DIVSHARE. Fig. 6(a) shows the heatmap of the speedup due to fragmentation, i.e., the percentage improvement in the time to reach 60% test accuracy for $f_s = 0.1$ vs. $f_s = 1$ with 30 stragglers. Each row represents decreasing data heterogeneity, with 10 being almost IID. Fig. 6(a) reveals that fragmentation in DIVSHARE is advantageous at all data heterogeneity levels and straggling factors. However, the speedup owing to fragmentation is amplified at high heterogeneity levels and high levels of straggling, achieving a speedup of up to 84%. In other words, as the learning task gets more difficult, DIVSHARE shows more efficiency and resilience to stragglers.\nLimits of fragmentation. The granularity of fragmentation, controlled by $\\Omega$, is an important parameter in DIVSHARE. To understand the limits of fragmentation in DIVSHARE, we evaluate DIVSHARE with different values of $\\Omega$, ranging from 0.01 to 1, 30 stragglers, and $f_s = 5$. Fig. 6(b and c) shows the time required to reach a target accuracy of 60% on CIFAR-10 without and with stragglers. In both cases, as we decrease the $\\Omega$ from 1, i.e"}]}