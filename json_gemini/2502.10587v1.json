{"title": "TOWARDS SELF-SUPERVISED COVARIANCE ESTIMATION IN DEEP HETEROSCEDASTIC REGRESSION", "authors": ["Megh Shukla", "Aziz Shameem", "Mathieu Salzmann", "Alexandre Alahi"], "abstract": "Deep heteroscedastic regression models the mean and covariance of the target distribution through neural networks. The challenge arises from heteroscedasticity, which implies that the covariance is sample dependent and is often unknown. Consequently, recent methods learn the covariance through unsupervised frameworks, which unfortunately yield a trade-off between computational complexity and accuracy. While this trade-off could be alleviated through supervision, obtaining labels for the covariance is non-trivial. Here, we study self-supervised covariance estimation in deep heteroscedastic regression. We address two questions: (1) How should we supervise the covariance assuming ground truth is available? (2) How can we obtain pseudo-labels in the absence of the ground-truth? We address (1) by analysing two popular measures: the KL Divergence and the 2-Wasserstein distance. Subsequently, we derive an upper bound on the 2-Wasserstein distance between normal distributions with non-commutative covariances that is stable to optimize. We address (2) through a simple neighborhood based heuristic algorithm which results in surprisingly effective pseudo-labels for the covariance. Our experiments over a wide range of synthetic and real datasets demonstrate that the proposed 2-Wasserstein bound coupled with pseudo-label annotations results in a computationally cheaper yet accurate deep heteroscedastic regression.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep heteroscedastic regression leverages neural networks as powerful feature extractors to regress\nthe mean and covariance of the target distribution. The target distribution is typically used for\ndownstream tasks such as uncertainty estimation, correlation analysis, sampling, and in bayesian\nframeworks. The key challenge in deep heteroscedastic regression lies in estimating heteroscedasticity,\nwhich implies that the variance of the target is input dependent and variable. Moreover, unlike the\nmean, the covariance lacks direct supervision and needs to be inferred.\nThe standard approach without the ground-truth covariance relies on optimizing the negative log-\nlikelihood to jointly learn the mean and covariance (Dorta et al., 2018). However, Skafte et al. (2019);\nSeitzer et al. (2022) show that in the absence of supervision, incorrect variance predictions lead to\nsub-optimal convergence. Subsequently, a flurry of recent works addresses this by either modifying\nthe negative log-likelihood (Skafte et al., 2019; Seitzer et al., 2022; Stirn et al., 2023; Immer et al.,\n2023) or improving the covariance through better parameterization (Shukla et al., 2024). However,\nthe methods introduce a trade-off between computational complexity and accuracy. Moreover,\nthe thematic message underlying these works is that estimating heteroscedasticity is challenging\nwhen annotations for the covariance are not available. Therefore, would having annotations for\nthe covariance alleviate this trade-off? To answer this, we explore the use of self-supervision for\ncovariance estimation in deep heteroscedastic regression. We study two questions:\n(Q1) How can we supervise the learning of the covariance assuming annotations are available?\nSince the negative log-likelihood is not formulated for supervising the covariance, we analyse the\nKL Divergence and the 2-Wasserstein distance to supervise the learning of the mean and covariance."}, {"title": "2 DEEP HETEROSCEDASTIC REGRESSION", "content": "Heteroscedastic regression is the task of modeling the mean and the variance of the target distribution.\nIn contrast to homoscedasticity, heteroscedastic models allow the variance to vary as a function of\nthe input. Deep heteroscedastic regression provides an advantage over non-parametric methods like\nthe Gaussian Processes (Le et al., 2005) because it can model complex features from inputs such as\nimages. This attribute has made it widely applicable in fields like active learning (Houlsby et al.,\n2011; Gal et al., 2017), uncertainty estimation (Gal & Ghahramani, 2016; Kendall & Gal, 2017;\nLakshminarayanan et al., 2017; Russell & Reale, 2021), image reconstruction (Dorta et al., 2018),\nhuman pose (Gundavarapu et al., 2019; Nakka & Salzmann, 2023; Tekin et al., 2017), and other\nvision-based tasks (Lu & Koniusz, 2022; Simpson et al., 2022; Liu et al., 2018; Bertoni et al., 2019).\nPreliminaries. Our goal is to learn the target distribution P(Y|X) for different X, where X \u2208 Rm\nis the input and Y \u2208 Rn is the target variable. While P(Y|X) is unknown, it is assumed to be\nnormally distributed: P(Y|X) = N(\u03bcY(X), \u03a3Y(X)). Our estimate of the target is P(Y|X) =\nN(\u03bcY (X), \u03a3Y(X)), where the mean \u03bc\u0302Y(X) = f\u03b8(X) and covariance \u03a3\u0302Y(X) = g\u03b8(X) are\nparameterized by neural networks. The standard approach in the literature to learn the target\ndistribution is through minimizing the negative log-likelihood, -E(X,Y)log P(Y|X). Specifically, the\nmean and covariance networks are trained to minimize (Nix & Weigend, 1994)\n\\begin{equation}\nL_{NLL}(\\theta, \\Theta) := E_{P(X,Y)} \\left[ \\frac{k}{2} \\log |\\Sigma_{\\Theta}(X)| + \\frac{1}{2} (Y - \\mu_{\\theta}(X))^T \\Sigma_{\\Theta}^{-1}(X) (Y - \\mu_{\\theta}(X)) \\right].\n\\end{equation}\nChallenges. The lack of supervision for the covariance results in an optimization challenge which is\nformalized in Skafte et al. (2019); Seitzer et al. (2022). The works observed that an underestimated\nvariance can increase the effective learning rate and disrupt optimization (Skafte et al., 2019), whereas\nan overestimated variance can decrease the effective learning rate and stop optimization (Seitzer et al.,\n2022). A number of recent approaches modify the negative log-likelihood to reduce the effect of\nthe predicted covariance during optimization. \u03b2-NLL (Seitzer et al., 2022) scales the negative log-\nlikelihood (Eq. 1) by the predicted variance resulting in the objective: LB-NLL = [\u00f4(\u0177)23] \u00d7 LNLL.\nHowever, since \u03b2-NLL does not originate from a valid distribution, the optimized values do not\nestimate the true variance. Stirn et al. (2023) decouples the estimation of the mean and variance\nby scaling the gradient of the mean estimator with the covariance, thereby eliminating the effect of\nthe predicted covariance on the mean. This leads to conflicting assumptions: the mean estimator\nassumes that the multivariate residual is uncorrelated while the covariance estimator is expected\nto identify correlations. Immer et al. (2023) proposed the use of natural parameterization of the\nunivariate normal distribution: n\u2081 = \u03bc/\u03c32 and n2 = \u22121/2\u03c32 for regression. While principled, the method"}, {"title": "3 ANALYSIS", "content": "We analyze the KL Divergence and the 2-Wasserstein distance, two widely used metrics for comparing\nand optimizing distributions. Our analysis focuses on multivariate normal distributions, which follows\na common assumption in machine learning that the residuals are normally distributed. We support our\nanalysis by studying Problem 1, which lets us visualize the convergence process of various methods\nusing bivariate normal distributions. We then seek to answer which metric is better suited for deep\nheteroscedastic regression.\nFormulation. A logical approach would be to replace each label with a distribution. Specifically,\nfor a sample xi, yi from the dataset, the pseudo target distribution can be set to N (yi, \u03a3(X) prior (X)).\nHowever, this approach requires calibrating the KL Divergence since the optimal value for the\ncovariance is not the same as the prior. We demonstrate this through a simple setting in Lemma 1."}, {"title": "3.1 KL DIVERGENCE", "content": "The KL Divergence between two continuous distributions p, q is DKL(p||q) = Ep log[p(x)/q(x)].\nFor two multivariate normal distributions (Zhang et al., 2024; Soch, 2020) we have\n\\begin{equation}\nDKL (P \\|\\| q) = \\frac{1}{2} \\left[ Tr(\\Sigma_q^{-1} \\Sigma_p) + (\\mu_q - \\mu_p)^T \\Sigma_q^{-1} (\\mu_q - \\mu_p) - k + ln \\left( \\frac{det \\Sigma_q}{det \\Sigma_p} \\right) \\right] ,  \n\\end{equation}\nSince the KL Divergence is asymmetric, we often let p and q be the target and predicted distributions\nrespectively. This definition also gives rise to popular alternatives such as the negative log-likelihood.\nWhile the KL Divergence has been well explored (Goodfellow et al., 2016; Arjovsky et al., 2017) from\na statistical viewpoint, we ask: how can the KL Divergence be formulated for deep heteroscedastic\nregression? Answering this is necessary since the KL Divergence is defined in terms of the means\nand covariances, which are unknown for the target distribution."}, {"title": "3.2 2-WASSERSTEIN DISTANCE", "content": "The Wasserstein distance is a metric for quantifying the distance between two probability distributions.\nIt defines the minimum \u201ccost\" required to morph one distribution into another. The 2-Wasserstein\ndistance measures the cost in proportion to the squared Euclidean distance. It is widely used in\noptimal transport theory and generative modeling (Arjovsky et al., 2017; Li et al., 2024), as it captures\nboth the shape and spread of distributions while penalizing long-distance transport more heavily.\nLet N\u2081(\u03bc\u2081, \u03a31), N2(\u03bc2, \u03a32) be two multivariate normal distributions. The 2-Wasserstein distance\nbetween them is given by\n\\begin{equation}\n||\\mu_1 - \\mu_2||^2 + Tr[\\Sigma_1 + \\Sigma_2 - 2(\\Sigma_2^{1/2} \\Sigma_1 \\Sigma_2^{1/2})^{1/2}] .\n\\end{equation}\nThis formulation, however, requires computing the root of a matrix, which typically involves eigen-\ndecomposition. Unfortunately, the eigendecomposition in popular deep learning frameworks can\npotentially lead to unstable gradients (PyTorch, 2024). If \u03a31 and \u03a32 are commutative (implying \u03a31\u03a32\n= \u03a32\u03a31), then the 2-Wasserstein distance is reduced to W2(N1, N2) = ||\u00b51-\u00b52||\u00b2+\\||\u03a31/2-\u03a32/2||.\nThis formulation allows us to directly predict the square roots, thereby avoiding the eigendecom-\nposition. However, for two covariance matrices to be commutative, they need to share the same\neigenbasis, implying that the matrices differ only in the variance of the individual random variables.\nFortunately, Theorem 1 allows us to expand this formulation to non-commutative covariance matrices\nby linking it to an upper bound on the true 2-Wasserstein distance."}, {"title": "3.3 GENERATING PSEUDO-LABELS FOR THE COVARIANCE", "content": "In the absence of labels for the covariance, existing approaches rely on the residual of the mean\nestimator as a signal to optimize the covariance. However, optimizing in this manner trades-off\naccuracy with computational complexity. While having labels would allow us to directly optimize\nthe covariance estimator, obtaining annotations for the covariance is non-trivial. Therefore, we take a\nstep in this direction and explore the possibility of self-supervision for the covariance. To this end,\nwe propose a simple heuristic, which when combined with the 2-Wasserstein distance, is surprisingly\neffective in supervising the covariance.\nIntuition. The neighborhood of a sample has been widely used in uncertainty quantification\n(Van Amersfoort et al., 2020; Skafte et al., 2019) and kernel methods (Hofmann et al., 2008).\nThe key idea is to infer properties of a sample based on its neighborhood. TIC (Shukla et al., 2024)\nlearns the covariance through a learnt e-neighborhood of the input, Cov(\u0176|X + \u20ac). We extend upon\nthis idea to obtain pseudo-labels for the covariance. Specifically, we use two concepts:\n1.  The target y has a high (co-)variance if it exhibits large variations in a small vicinity of x.\n2.  The closer xj is to xi, the likelier it is that yj is a potential label for xi.\nWe quantify these concepts through the use of (a) the Mahalanobis distance, to measure the degree of\ncloseness between samples xi and xj; and (b) a probabilistic interpretation of this distance to weight\ndifferent targets yj as being a potential label for xi.\nThe Mahalanobis distance between two points u, v with respect to a covariance matrix \u2211 is\n\\begin{equation}\nd_M(u, v; \\Sigma) := \\sqrt{(u - v)^T \\Sigma^{-1} (u - v)} .\n\\end{equation}"}, {"title": "4 EXPERIMENTS", "content": "How effective is self-supervision in deep heteroscedastic regression? We study this question through\na series of synthetic and real world datasets for regression. We use the same setup as Shukla\net al. (2024) which experiments on univariate sinusoidals, synthetic multivariate data, UCI Machine\nLearning repository (Markelle Kelly) and 2D human pose estimation (Andriluka et al., 2014; Johnson\n& Everingham, 2010; 2011). We provide a detailed description of the experimental setup and\nimplementation details in the appendix (B). For all our experiments, we set the nearest neighbors\nhyperparameter in the pseudo-label algorithm to ten times the dimensionality of the target. We\ncompare our approach with popular and state-of-the-art methods such as the vanilla negative log-\nlikelihood, \u03b2-NLL (Seitzer et al., 2022), Faithful heteroscedastic regression (Stirn et al., 2023),\nEmpirical-Bayes (Immer et al., 2023) and the Taylor Induced Covariance parameterization (NLL:TIC)\n(Shukla et al., 2024). In addition to the mean square error and the negative log-likelihood, we use\nthe KL-Divergence and the 2-Wasserstein distance as metrics when the ground truth covariance\nis known. We also use the Task Agnostic Correlations (TAC) metric introduced in (Shukla et al.,\n2024) to evaluate the covariance through its learnt correlations. Finally, we also report the additional\nmemory consumed and the time required for each method to run for different experiments."}, {"title": "4.1 SYNTHETIC DATA", "content": "Univariate. We use samples from different varying amplitude sinusoidals to compare the methods.\nIn Fig. 4, we observe that faithful heteroscedastic regression overestimates the covariance because of\nthe lack of synergy between the mean and variance estimator. While the mean estimator assumes\nhomoscedastic unit variance, the variance estimator models heteroscedasticity. Although the TIC\nformulation stabilizes convergence, unfortunately, convergence itself is slow. The KL Divergence"}, {"title": "4.2 REAL DATASETS", "content": "UCI Regression. We evaluate mean and covariance predictions by performing the same study\nas Shukla et al. (2024) on regression datasets from the UCI Machine Learning repository. We\nstandardize each dataset to have zero mean and unit variance. We randomly choose 25% of the\nfeatures as observations and the remaining 75% as the targets, adding considerable heteroscedasticity\nin the data. We conduct five trials and report the mean, highlighting top-performing methods which are\nstatistically indistinguishable. We evaluate different methods not only using performance on various\nmetrics but also through computational costs. While TIC outperforms\nother likelihood based baselines significantly in our TAC and NLL evaluation, this comes at the cost\nof significantly higher computational requirements. Although compute efficient methods such as\nFaithful leverage the mean squared error to accurately converge to the mean, it does not accurately\nconverge on the optimal covariance. In contrast, the 2-Wasserstein bound accurately converges in\nboth, the mean and covariance without additional computation overhead. Moreover, the vanilla\n2-Wasserstein formulation exhibited significant training instabilities on certain datasets such as\nsuperconductivity, motivating the use of the proposed bound which is stable to train."}, {"title": "5 CONCLUSION", "content": "We study deep heteroscedastic regression, noting the optimization challenges present due to the\nlack of annotations for the covariance. Therefore, we study methods for self-supervision which\nrequires us to define (1) a framework for supervision, and (2) a method to obtain pseudo-labels for\nthe covariance. We critically study the KL-Divergence, highlighting the need for calibration and\nnoting its susceptibility to residuals. Next, we study the 2-Wasserstein distance, proposing a bound\non the latter that is stable to optimize. Finally, we propose a simple neighborhood based heuristic\nwhich is effective in providing pseudo-labels for the covariance. Our experiments show that, unlike\nexisting approaches, the use of the 2-Wasserstein bound and pseudo-labels yields accurate mean and\ncovariance estimation while remaining computationally inexpensive. Our experiments on human pose\nshow the potential for a hybrid approach, where combining the 2-Wasserstein and NLL frameworks\nenables superior performance compared to using either method alone."}]}