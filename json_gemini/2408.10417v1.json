{"title": "Development of an AI Anti-Bullying System Using Large Language Model Key Topic Detection", "authors": ["Matthew Tassava", "Cameron Kolodjski", "Jordan Milbrath", "Adorah Bishop", "Nathan Flanders", "Robbie Fetsch", "Danielle Hanson", "Jeremy Straub"], "abstract": "This paper presents and evaluates work on the development of an artificial intelligence (AI) anti-bullying system. The system is designed to identify coordinated bullying attacks via social media and other mechanisms, characterize them and propose remediation and response activities to them. In particular, a large language model (LLM) is used to populate an enhanced expert system-based network model of a bullying attack. This facilitates analysis and remediation activity \u2013 such as generating report messages to social media companies \u2013 determination. The system is described and the efficacy of the LLM for populating the model is analyzed herein.", "sections": [{"title": "1. Introduction", "content": "Cyberbullying involves using technology, such as social media, to mistreat and harass others. It has become a pronounced problem due to the increasing ubiquity of online platforms that provide a means to conduct it. A significant amount of this cyberbullying is conducted by and targets teenagers.\nIt is difficult for teenage students to shut themselves off from the digital world in which the cyberbullying is taking place. Given how entrenched the use of digital apps is by today's youth, and the pronounced consequences of it \u2013 including victim self-harm, in some cases \u2013 cyberbullying is at least as much of a threat as physical bullying. Additionally, because of the obfuscation caused by the online environment, authorities (such as parents, teachers and law enforcement) may have difficulty determining what has occurred and who the actors participating are. Determining what can be done to remediate the situation is even more difficult.\nCyberbullying using AI can be particularly pronounced, as AI use provides a wealth of knowledge, incredible speed and other capabilities that can intensify a bullying campaign. This provides significant capabilities to a bully particularly in regards to youth victims even when the bully is another youth. For example, the speed of AI bullying could result in the victim feeling overwhelmed and unable to respond fast enough. The knowledge base of a large language model (LLM) creates an asymmetry, where an AI/LLM user may be generating bullying content that neither they \u2013 nor the victim \u2013 fully understand. This content may be particularly vile and effective. LLMs ability to write content with different tones, from different perspectives and even in different languages can create the impression that numerous people are posting similar content, making it more likely to be believed and intensifying the victim's feeling of being ganged up on.\nA partial solution to this challenge is to give responders \u2013 parents, teachers, school administrators, counselors, law enforcement and others a capability that is just as effective as that which a basic LLM provides to a would-be bully. This paper, thus, proposes an AI anti-bullying system (AABS). The system utilizes generative artificial intelligence (GAI) to analyze cyberbullying related messages circulating online and builds a model that clearly represents the transpired events. This model can then be used to generate reports and suggest remediation strategies to necessary authorities, such as teachers, parents, and school administrators. The proposed system is described and evaluated herein."}, {"title": "2. Background", "content": "Al has become the \u201clatest weapon in the arsenal of school bullies\" [1]. It is used in cyberbullying to create \u201cdeepfakes and other deceitful content\" [2], which are highly offensive, to target victims. It is being used \u201cproduce realistic, degrading and offensive content of children\u201d and number of families reporting cyberbullying is growing [3], globally. Teens are primary targets: the Australian eSafety Commissioner, for example, found that two-thirds of reports of bullying had a victim between 12 and 15 and often continued \u201cbullying occurring on school grounds\" [3]. In the United States, 59% of teens reported being bullied online, in one study [4].\nSchools and law enforcement are unprepared and, in addition to the direct harm it has caused, AI-based bullying has led to officials taking actions against victims, based on deceptive AI-generated content [5]. The legal system is also struggling to catch up, as there are only a limited number of laws directly covering these activities [5], though they may be proscribed by more general laws. While new laws are being introduced [6], a recent survey found that over 60% of school district technology leaders are \u201cvery\u201d or \u201cextremely' concerned\u201d about AI, including its potential use in cyberbullying [7].\nThis section discusses bullying response and mitigation (in Section 2.1) and technologies developed for bullying detection (in Section 2.2) and response (in Section 2.3). Finally, it concludes with a discussion regarding the use of Al in cyberbullying prevention in Section 2.4."}, {"title": "2.1. Bullying Response and Mitigation", "content": "A variety of research has been performed regarding response to bullying and cyberbullying.\nHolt, et al. [8] investigated the development of programs and program components to reduce bullying by changing the way teachers and community members respond to bullying. It was found that schools have opportunities to stop bullying, but do not use them. The community has a significant impact on a victim's ability to get help but needs a way to enforce its anti-bullying policies. Other sources of support for victims include parents, friends, and teachers. It was found that integrating daily measures that promote an anti-bullying environment helps to reduce the number of students that get bullied.\nMilosevic, et al. [9] performed qualitative research with pre-teen and teenage children. Their study indicated that AI would help prevent cyber bullying, and some respondents indicated that they would support using AI to prevent cyberbullying. However, most respondents indicated that they would not use AI to prevent cyberbullying because the AI would have to read the messages. Opponents to AI use raised privacy concerns and didn't want face backlash from peers for supporting AI use in cyberbullying prevention.\nDuring the study [9], respondents were presented with several scenarios for AI use. Most supported Al's use to identify and hide bullying, while only some supported hiding content that had the intention of excluding others. Respondents also supported providing functionality to report bullying content to a school administrator; however, many did not want to report bullying because of peer pressure. Finally, respondents were less supportive of a tool to allow victims to create an anti-bullying video to send it to the bully, as it could simply be ignored or might serve as an indication that the bullying is affecting its victims.\nAlthough bullying rates vary geographically, Saget, Yarmina and Planas [10] found that victims aged 12 to 18 showed a direct correlation with social maladjustments. To combat this, they proposed robotherapy. They found that the majority of international and national anti-bullying programs (which occurred 20 to 25 years ago) had success in reducing bullying and victimization by improving emotional competency, increasing peer acceptance, involving others, and preventing violence. Based on their analysis, they proposed a two-component plan that included teenager-robot interaction and psychologist-teenager interaction. The psychologist taught strategies, such as taking deep breaths, relaxing muscles, maintaining eye contact, and speaking with a calm voice and the robot"}, {"title": "2.2. Detection Technology", "content": "A variety of technical approaches can be used to detect cyberbullying. Orru, et al. [18], for example, discussed using multiple forms of analysis \u2013 including visual analysis, deepfake detection, keystroke dynamics, verbal analysis, and stress detection \u2013 to evaluate the risk of various forms of bullying within classrooms. Similarly, Litty, Jahin and Jesan [19] discussed existing methods for detecting cyberbullying on social media platforms. They discussed the advantages, challenges, and types of deep learning methods used and analyzed case studies of using deep learning for cyberbullying detection on social media platforms.\nOne key problem with creating models of cyberbullying is the ambiguity associated with cyberbullying scenarios. Training using datasets with ambiguous content can impair AI systems' functionality. Thus, identifying and removing ambiguous comments from a cyberbullying dataset allows AI models to be trained to more accurately identify cyberbullying. Gomez, Sztainberg and Trana [20] investigated two machine learning approaches to separate ambiguous and unambiguous statements and tested them on 19,000 YouTube comments. First, they used single-algorithm consensus filtering, which uses a support vector machine learning algorithm for data filtering. Second, they used multi-algorithm consensus filtering. This uses several naive Bayes, support vector and convolutional neural networks. Amazon's Mechanical Turk was used to supply human workers to determine if a comment was truly ambiguous. It was found that the two proposed classifiers provided a notable improvement in performance over the original datasets\nRamezanian and Niemi [21] attempted to mitigate privacy concerns with Al online message screening for cyberbullying content by designing a methodology where messages are screened in phases. Messages only receive the next phase of scrutiny if flagged in the current phase. This method also classifies users into normal, bully, and victim groups over time. The normal group only gets a small portion of their messages checked.\nDhiman, et al. [22] created a model which can be used to detect fake news that was trained on data from Indian news and fake news sources. Ide and Adewale [23] developed a system, designed to work with popular social media website, that filters online bullying messages at the time when they are sent-preventing the intended recipient from ever seeing them."}, {"title": "2.3. Response Technology", "content": "Children spend a notable amount of time socializing on mobile applications and, as such, cyberbullying among children takes place on these apps. In response, anti-bullying apps have been developed; however, typically, those in the target demographic of these apps are not involved in their design or development process. Hartikainen, et al. [24] explored involving children aged 7 to 15 in the design and prototyping of anti-bullying apps. Each small group brainstormed ideas and developed a prototype design. Each app had different features including facilitating adult support, peer support, self-help, providing entertainment, and tracking bullying incidents. Students were surveyed at the beginning and throughout the project. Additionally, the views of both students and teachers were analyzed at the project's end."}, {"title": "2.4. AI Technology Use", "content": "Jain [31] notes the significant potential for AI to serve as a \u201chero\u201d and \u201csilent guardian\u201d against bullying. This can take multiple forms. A phone app called ReThink, for example, uses AI to identify offensive messages and discourage them from being sent [32]. IBM's Watson AI has been used to monitor social media for bullying and warning of potential self-harm [33]. Aston University's Bullstop [34] software integrates with Twitter and prevents \"bullies and trolls\" from directly messaging potential victims using the software.\nVanderbilt University Medical Center and Florida State University developed a technology that could accurately identify 92% of those about to commit self-harm [4]. Instagram seeks to \u201cproactively detect bullying\" in photos, captions and other text and videos [35], while Facebook has deployed a self-harm identification technology [4]. Teenager Gitanjali Rao's Kindly software identifies bullying language in posts, drawing recognition from UNICEF [36].\""}, {"title": "3. Technology", "content": "This section discusses the proposed technology. Section 3.1 discusses the technical challenges that this technology is responsive to. Section 3.2 discusses the technologies that the proposed technology builds upon. Then, in Section 3.3, the technology itself is presented. Finally, Section 3.4 discusses the specific system operations that were conducted for the purposes of collecting the data that is analyzed herein."}, {"title": "3.1. Technical Challenges", "content": "Al provides a wealth of knowledge, incredible speed and other capabilities that could be useful to a bully. This work focuses on preventing bullying at the middle and high school levels; however, this could be applicable to younger students, college students and the general public.\nSpeed \u2013 AI can generate content at a much faster rate than a human. This is also a much faster rate than humans can process and respond to content. Thus, an individual being bullied by an AI may feel overwhelmed. They may also be unable to respond to the bullying activities, request post removal and take other needed actions at the speed of the AI.\nKnowledge - Large language models (LLMs) are trained with a wealth of human knowledge. This far exceeds any one individual's knowledge. LLMs have knowledge that may go well beyond the experience level of middle and high school students. This creates an asymmetry, where the AI/LLM user may be generating bullying content that neither they \u2013 nor the victim \u2013 fully understand. This content may be particularly vile and effective.\nOther Capabilities \u2013 A bully could prospectively leverage numerous other LLM capabilities, such as its writing capabilities. LLMs have the ability to write in different styles \u2013 and even in different languages \u2013 and the ability to write at different reading/age levels and with different levels of professionalism. This could be used to create numerous forms of content that prospectively appear to come from different locations."}, {"title": "3.2. Foundational Technologies", "content": "This technology builds on the Blackboard Architecture, which was introduced by Hayes-Roth in the 1980s [37], based on the Hearsay II system [38] which was developed in the late 1970s. The Blackboard Architecture has found numerous uses including tutoring [39], vehicle control [40] and protein modeling [41]. It is part of a sub- field of machine learning called eXplainable Artificial Intelligence (XAI) which is responsive to the limitations of Al systems' ability to \"explain their autonomous decisions to human users\" and their lack of understandability by humans [42].\nA modern Blackboard Architecture implementation [43] serves as the foundation for this work. It has been extended through the use of previously developed links [44], containers [44], common properties [45] and generic rules [45], which allow bullying networks to be more effectively modeled in this work. The Meta Llama 3.1 Instruct LLM [46] is used extensively, in this system, for natural language processing."}, {"title": "3.3. Proposed Technical Solution", "content": "A technology has been developed to identify a cyberbullying campaign to facilitate connecting the dots for bullying's first responders, including teachers, school administrators and \u2013 in pronounced cases \u2013 law enforcement. This is done by leveraging AI technologies.\nThe AABS system has several key components. It includes a search mechanism, which identifies content for processing, a model development component and a response and remediation component. A high-level depiction of the system is presented in Figure 1, which shows the linear flow of information from the search component to the model development component and, from there, to the response and remediation component. A more detailed workflow for AABS is presented in Figure 2."}, {"title": "Search Component", "content": "The search component identifies content for processing by the system. It can make use of interfaces provided by social media providers, search engines and others to identify prospective content that may be indicative of bullying. The filter at this point is very limited. The search component ensures that the content is related to the individual who is prospectively being bullied and within the timeframe that is being studied. If the individual has a common name or other factors result in lots of extraneous content being identified, additional criteria (such as other words to include or words to exclude messages based on) can be supplied. However, the goal is not to try to determine what content may be bullying at this point \u2013 it is to cast a proverbial \u2018wide net' to ensure that all potentially relevant content is considered.\nNotably, feedback from the model development component may identify other terms to include or exclude. This can be used to enhance the search component, after initial information is provided by it to the model development component."}, {"title": "Model Development Component", "content": "The model development component leverages the use of an LLM (specifically, initially using the Llama 3.1 Instruct LLM) to determine whether content is bullying-relevant. If it is, the content is analyzed and added to the bullying model."}, {"title": "Response and Remediation Component", "content": "The response and remediation component begins with a process of identifying bullying activities which are protected speech. For the purposes of this assessment, at the moment, speech which correctly identifies its speaker or is statedly anonymous and which is not threatening or defamatory is treated as protected speech. These elements are identified in the model, but not removed.\nProtected speech is not actionable, in many instances (particularly by public entity actors); however, it can demonstrate motive and intent and provide context for unprotected speech and conduct. Notably, if the entire model is comprised of protected speech, this process will end with a simple report stating this. Expansion of this system to facilitate expanded definitions of protected speech, to perform additional processing for non-public entities and to allow a target to respond to protected speech are topics for potential future work.\nOnce the model has been annotated with what is protected speech, it is output in a standardized prose form to be provided to a LLM to summarize and expand upon. This produces a report that can help teachers, administrators and law enforcement (if required) to understand the bullying attack, including its scope and implications.\nThe STBAM is provided to the remediation component to identify actions that can be taken to respond to and remediate the impact of the bullying attack. Examples of remediations are posting corrective posts, reporting posts and requesting post removal (depending on the policies of the social media sites they are posted on). The remediation component generates requests that can be sent to relevant parties to request actions. The attack report is also provided to the remediation component, so that it can be incorporated into requests, as relevant. Additionally, this content is incorporated in the response / remediation activities report which summarizes the proposed automated and manual response and remediation activities to take."}, {"title": "3.4. System Operations for Testing Content Acquisition", "content": "Content acquisition is the primary focus of the analysis presented herein. The AABS uses the Meta Llama 3.1 Instruct LLM with customized system prompts. The processing of a given set of input starts with sending it to the LLM with the system prompt shown in Listing 3. An overview of the process is presented in Figure 3."}, {"title": "4. Experimentation and Results", "content": "This section presents the experimentation using and results from testing the proposed technology. Section 4.1 discusses the testing process. Section 4.2 presents the results and discusses their impact."}, {"title": "4.1. Testing Process", "content": "Due to Llama 3.1's potential for content filtering, testing was not performed on prompts relating to bullying. Data was instead collected on prompts falling into two categories: commerce and travel. The keyword used for first category was \"purchase\u201d, as other keywords such as \u201ccommerce\" and \"buying or selling\" resulted in more erroneous data. The keyword used for the second category was \u201ctravel experience\", based on the generation prompt used for data creation (which was also performed using the Llama 3.1 Instruct LLM) using the prompts specified in Listings 11 and 12.\""}, {"title": "4.2. Results and Impact", "content": "The results of the network creation activities are presented in Table 1. In this table \"expected results\" means that the system has correctly modeled at least one example of behavior that is directly associated with the key topic as a link object. For example, if the key topic is \"purchase,\" then actions pertaining to buying and selling would be directly associated, while actions such as a sales associate approaching the buyer would not be directly associated. Note that the data for Test 1 was manually created as an example while the data for all other tests was generated using the previously described technique.\nThe errors thrown column indicates whether an error, due to the presence of the term \"the entire set\" as an object, occurred. The LLM expected an enumerable array of items.\nThe model creation technology described herein requires additional development to be ready for use outside of the research environment. It was fully successful only 30% of the time. It successfully identified at least part of the subjects, objects and/or actions at least 70% of the time.\nWith additional enhancement, it could be useful in a variety of fields. First, it could be used to assist in identifying harmful content. It could also find other uses related to cyberbullying. For example, using the proposed technology to analyze victim reports could help to reduce cyberbullying, as it could identify text that could be considered offensive. Then, before any harmful message is sent to another person, certain words could be removed. Alternately, the entire message could be prevented from sending or referred for additional review. This could benefit society by preventing negative messages, improving relationships, helping people communicate effectively, and promoting emotional growth.\nCommon trends between bullying messages could be discovered, as well. Since the system analyzes text and adds it to a bullying model, a single container that has many links connected to it could serve as an indicator of the relevance of the concept represented by that container. In addition to this, if two containers are highly connected by multiple links, this could imply that the two concepts are closely related. In the context of identifying harmful content, it can be said that two concepts are often used together in the analyzed bullying incidents.\nIn addition to identifying trends in harmful content, the system has the potential to enhance law enforcement investigations. For instance, in the most extreme cases of school bullying, the process of identification could be accelerated. Typically, school administration is notified by the victim or bystanders, police get called in (if warranted), and the bully faces punishment. Using the proposed technology could greatly speed up this process, since police would be able to gather information more quickly. Reports that contain common information could result in a model that has many links pointing to the container or containers with that information. In reports made from the model, it can be easily determined that many incidents relate to this piece of information, indicating its potential importance in the investigation.\nThe technology may also have broader security uses. It may be useful to identify data breaches, threats, and malware. Automation, using the technology, may increase efficiency. It may also have application to ensuring peaceful relations with other countries and preventing the spread of terrorist ideologies. It could also be used to detect false information and identify its origins. Being able to relate various reported incidents and stories allows models to be created that represent broad and potentially vague scenarios. The data in the resultant model may provide context and associations between different concepts that would have otherwise gone unnoticed."}, {"title": "6. Conclusions and Future Work", "content": "The proposed system is not without its flaws. Key topic detection was shown to, currently, be unreliable in a myriad of ways. In some cases, pronouns were not replaced with the proper nouns that were introduced earlier in the user prompt. Some names were be expanded into inferred constituent parts (e.g., \u201cGreek Islands\u201d into Mykonos, Santorini, and Crete). Actions often did not make sense, such as \"Petra rising cliffs.\" These issues can lead to significant cases of unreliability that could negatively impact the system's ability to be useful in certain scenarios and overall trust in the system. However, while it currently may not operate as intended, in some situations, it still has the ability to provide useful associations between concepts that may otherwise go unnoticed. Future development may further enhance the system's efficacy, as well.\nThere are a number of ways that the system can be improved. The system currently uses Llama 3.1 to process the data. While this model sometimes provides the required functionality for the given test cases, it is possible that an LLM trained specifically to identify key topic-related messages would perform better. This would be particularly important, if the topic might often involve content that would be filtered by the LLM, such as bullying content. In addition to this, editing the system and user prompts that are provided to the LLM could result in more accurate and consistent results. This would require additional testing and could cause the generated models to be more accurate and provide more insightful data."}]}