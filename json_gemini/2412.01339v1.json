{"title": "Negative Token Merging: Image-based Adversarial Feature Guidance", "authors": ["Jaskirat Singh", "Lindsey Li", "Weijia Shi", "Ranjay Krishna", "Yejin Choi", "Pang Wei Koh", "Michael F. Cohen", "Stephen Gould", "Liang Zheng", "Luke Zettlemoyer"], "abstract": "Text-based adversarial guidance using a negative\nprompt has emerged as a widely adopted approach to push\nthe output features away from undesired concepts. While\nuseful, performing adversarial guidance using text alone\ncan be insufficient to capture complex visual concepts and\navoid undesired visual elements like copyrighted charac-\nters. In this paper, for the first time we explore an alter-\nnate modality in this direction by performing adversarial\nguidance directly using visual features from a reference im-", "sections": [{"title": "1. Introduction", "content": "Large-scale text-to-image (T2I) diffusion models [10, 32,\n37, 38, 41, 49] have made unparalleled progress and allow\nfor generation of powerful imagery. Despite these advances,\nguiding the generation process adversarially to avoid gener-\nation of undesired concepts [4] remains a challenging prob-\nlem. Such guidance is advantageous for several applications\nsuch as improving image quality (by guiding away from\nlow-quality features), improving output diversity (by guid-\ning each image away from each other), avoiding undesired\nconcepts such as copyrighted characters (Fig. 1, 2) [17] etc.\nExisting methods in this direction predominantly rely\non the use of negative prompt [3, 21] for adversarial guid-\nance. However, use of negative-prompt alone for adversar-\nial guidance suffers from some limitations; capturing com-\nplex visual concepts using text-alone can be hard (trying to\ncapture every detail: pose, action, background etc. for child\nin a park in Fig. 2). The use of negative-prompt alone might\nbe insufficient to remove undesirable visual features (e.g.,\ncopyrighted characters in Fig. 1). Furthermore, using a sep-\narate negative prompt itself may not be feasible when using\nstate-of-the-art guidance distilled models like Flux [6].\nIn this paper, we explore an alternate modality in this di-\nrection by performing adversarial guidance using images.\nOur key intuition is that even if describing the undesired\nconcepts is not effective or feasible as text (child in park\nfor Fig. 2), we can directly use the visual features from a\nreference image in order to adversarially guide the genera-\ntion process. For instance in Fig. 2, instead of trying to ex-\nhaustively describe the child's attire, placement, pose, back-\nground etc., we wish to directly use the visual features from\nthe reference image to guide the generation process. Sim-\nilarly, in cases where a negative prompt alone is not suffi-\ncient (e.g., copyrighted characters in Fig. 1), we can better\nguide the generation away from undesired concepts by di-\nrectly using the character images for adversarial guidance."}, {"title": "3. Negative Token Merging", "content": "Our goal is to insert a negative token merging module into\nan existing diffusion model [6, 33], in order to perform\nadversarial feature guidance w.r.t other images in a batch\n(Sec. 4.1) or a real-image input (Sec. 4.2). The core idea\nof our approach is to perform adversarial guidance by push-\ning each output token (source) away from its best match-\ning token (target) in the reference image. The negative to-\nken merging module is applied between the attention and\nMLP branches of each transformer block (Fig. 3). In partic-\nular, given the output of the attention block, we first perform\ncross-image token matching to find the best matching target\ntoken for each output source token. We then apply simple\nlinear extrapolation pushing each source token away from\nits best matching target token. Fig. 3 provides an overview.\nSemantic Token Matching. A key idea behind Neg-\nToMe is to push each output token (source) away from\nits semantically closest token (target) in the reference im-\nage during the reverse diffusion process. This requires ac-\ncurate computation of semantic token-to-token correspon-\ndences (e.g., head of child in Fig. 3) between the generated\ntokens and the tokens in the reference image. Luckily, we\ncan leverage the rich semantic structure of the intermedi-\nate diffusion features [45, 52] itself to compute cross-image\ntoken-token similarities using noisy latent features itself.\nIn particular, given the output $Osrc \\in R^{B \\times N \\times D}$ of an\nattention block (B is the batch size and N is the number\nof image tokens), we first compute similarity w.r.t the refer-\nence image tokens $Oref \\in R^{1 \\times N \\times D}$ as follows,\n$S(Osrc, Oref) = \\tilde{Osrc}\\tilde{O}^{T}_{ref}; S \\in R^{B.N \\times N},$ (1)\nwhere $\\tilde{Osrc}, \\tilde{Oref}$ refer to the frame-level normalized\nsource and reference image tokens, respectively. We next"}, {"title": "4. Experiments", "content": "In this section, we demonstrate the practical usefulness of\nour approach for two prominent applications of NegToMe;\nincreasing output diversity (Sec. 4.1) and reducing visual\nsimilarities to copyrighted characters (Sec. 4.2). We further\nshowcase more general applications of NegToMe in Sec. 5.\n4.1. Increasing Output Diversity\nWe evaluate the performance of our approach for increasing\noutput diversity when performing negative token merging\nw.r.t to other images in the batch. To facilitate easy visual\ncomparison, we perform negative token merging w.r.t the\nfirst image in each batch. Unless otherwise specified, all\nresults are reported using the same text-to-image generation\nbase-model [6, 33] on a single Nvidia-H100 GPU.\nDataset and Setup. We first construct an input prompt\ndataset comprising 20 general object categories (e.g., an-\nimal, woman, bird, car etc.) across 7 different prompt\ntemplates (e.g., \"a photo of a\u201d, \u201ca high-contrast image of\na\u201d)\u00b9. For each category, we sample 280 images with 10\nrandom seeds (4 per batch) both with and without Neg-\nToMe. The real images for FID [20] calculation are sourced\nfrom LAION-Aesthetics-v2 6+ dataset [43], where we use\nCLIP [35] to retrieve the top-1K images for each category.\nEvaluation metrics. Following [21], we report the results\nfor output-quality using 1) FID [20] and 2) Inception Score\n(IS) [42]. 3) Pairwise dreamsim-score [14]: is used to mea-\nsure output feature diversity. 4) VQAScore [28] and 5)\nCLIPScore [19] are used to evaluate image-text alignment.\nFurthermore following [12], we also use 6) Entropy-Score\nwhich measures the degree to which outputs for a particular\nobject category (e.g., person) are spread across its subcate-\ngories (racial, gender, ethnic etc.). For human images, we\nuse the FairFace classifier [22] to detect race, gender, and"}, {"title": "4.2. Copyright Mitigation", "content": "We next show the efficacy of our approach for reducing vi-\nsual similarities with copyrighted characters when perform-\ning NegToMe w.r.t a copyrighted image RAG database.\nDataset and Setup. We first construct a dataset of 50 copy-\nrighted characters (e.g., Mario, Elsa, Batman), and curate\ninput prompts which to trigger these characters without ex-\nplicitly mentioning their names (see appendix). For each\ncharacter, we compile a reference dataset of approximately\n30 high-quality images depicting the character in diverse\nsettings. Masked negative token merging is then performed\nfor each prompt, using the best-matching RAG asset (asset\nwith highest Dreamsim score) from the reference dataset.\nThe mask for each asset is computed using HQ-SAM [23].\nQualitative Results. Results are shown in Fig. 7 (SDXL)\nand Fig. 9 (Flux). We observe that the base model still gen-\nerates copyrighted characters, even when the corresponding\ncharacter name is not mentioned in the input prompt. Us-\ning the character name as the negative prompt alone often\nis not sufficient, as the output images still show high visual\nsimilarity to the copyrighted characters. In contrast, by ap-"}, {"title": "5. Method Analysis and Applications", "content": "Improving output aesthetics. As noted in Fig. 2, we note\nthat NegToMe allows for a range of custom applications"}, {"title": "6. Conclusion", "content": "In this paper we introduce NegToMe, a simple training-\nfree approach which complements traditional text-based\nnegative-prompt guidance, by performing adversarial guid-\nance directly using visual features of a reference image.\nNegToMe is simple, training-free and can be incorporated\nwith most state-of-art diffusion models using just few lines\nof code (Alg. 1). By simply varying the reference im-\nage, NegToMe enables a range of custom applications such\nas increasing output diversity (Sec. 4.1), reducing simi-"}]}