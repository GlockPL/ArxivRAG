{"title": "From Human Hands to Robotic Limbs: A Study in Motor Skill Embodiment for Telemanipulation", "authors": ["Haoyi Shi", "Mingxi Su", "Ted Morris", "Vassilios Morellas", "Nikolaos Papanikolopoulos"], "abstract": "This paper presents a teleoperation system for controlling a redundant degree-of-freedom (DOF) robot manipulator using human arm gestures. We propose a GRU-based Variational Autoencoder (VAE) to learn a latent representation of the manipulator's configuration space, capturing its complex joint kinematics. A fully-connected neural network maps human arm configurations into this latent space, allowing the system to mimic and generate corresponding manipulator trajectories in real-time through the VAE decoder. The proposed method shows promising results in teleoperating the manipulator, enabling the generation of novel manipulator configurations from human gestures that were not present during training. Experimental evaluation highlights the effectiveness of this approach, although limitations remain concerning training data diversity.", "sections": [{"title": "I. INTRODUCTION", "content": "The convergence of Artificial Intelligence(AI) and robotic systems, has revolutionized a wide range of domains such as agriculture, healthcare medicine, warehousing, and manufacturing. In recent years, generative AI models for image and language generation (Large Language Models, LLMs) have proliferated, such as for image generation and natural language generation, for example ChatGPT, Midjourney, and Dall-E, to name a few. The combination of generative models and robots has also started to receive considerable attention. One aspect that has impeded its progress is that generative models are data-driven approaches. At the onset of LLMs, and image generation, copious available images and text on the Internet were mined for building massive training datasets. Training generative models for robotic applications requires a substantial amount of domain- and task-specific data, which, on the contrary, does not exist [1]. There is no protocol or standard for a taxonomy of robotic trajectory and task data for generating robotic actions for new operative scenarios. This is because the application scenarios are constrained. In theory, teleoperation technology can address these limitations by allowing the human operator to simultaneously supply new training exemplars to teach the robot to perform unforeseen tasks and to augment training sets for new task scenarios.\nManipulator teleoperation systems enable remote interaction with environments and can scale human motion to achieve larger or smaller action capabilities. These systems aim to accurately translate human decision-making and actions while ensuring the robust operation of the teleoperation system [2]. A common solution is for the operator to control the real-time end-effector's position and orientation with external devices with a 6 degrees of freedom (DOF) robot with the joint trajectory space calculated through standard inverse kinematics (IK). For example, an operator can use a haptic device to control the end-effector 6-DOF representation to achieve high-frequency teleoperation for industrial manipulators in workplace and factory settings [3]. Another approach instead uses an external RGB and RGBD (depth) camera to estimate the operator's 6-DOF hand pose [4], [5] for teleoperating the robot end-effector [6], [7]. Furthermore, as the virtual reality (VR) hardware market expands, inertial measurement unit (IMU) sensors and VR controllers are becoming a commonly used approach to estimate the operator's 6-DOF hand pose for integrating 6-DOF robot teleoperation and control [8], [9]. These approaches are less effective when applied to redundant DOF manipulators with more than six degrees of freedom. Redundant DOF manipulators afford multiple solutions in joint configuration space to achieve the same end-effector pose. Although this kinematic redundancy provides greater flexibility within complex and dynamic environments, it also increases the complexity of teleoperation. To address this problem and provide an intuitive human-robot interface, we propose a novel, computationally efficient, machine learning-based ap-proach to determine plausible robot joint and manipulator trajectories that mimic human kinematic motion behaviors, making it an ideal interface for co-robotics task scenarios. The approach embodies a training-model learning framework that can reduce or eliminate the typically highly technical and labor intensive requirement to re-program such robots to execute new tasks and operate in new environments.\nSpecifically, we present a Gate Recurrent Unit based Vari-ational Autoencoder (GRU-based VAE) architecture finding the latent representation of the redundant DOF robot manipulator configuration space. A feed-forward neural network converts human arm gestures into the latent distribution space"}, {"title": "II. RELATED WORK", "content": "One approach to achieve high-DoF manipulator teleoperation is Master-Slave or Twin-Master, where the operator manually controls another manipulator with identical kinematics to the target manipulator [10], [11]. However, this method requires an additional manipulator with the same kinematic structure as the remotely controlled device. This requires either two identical manipulators or building a custom one. Both options are costly and limit the manipulator system's adaptability to other types of manipulators.\nA Human arm is defined as a 7-DOF kinematic structure [12]. Hence, teleoperating a high-DOF manipulator by mapping the human arm joints to the target manipulator is another approach. Previous work [13], achieved humanoid robot (7-DOF arm) teleoperation by attaching IMU sensors to the human operator and mapping whole-body joints with dynamic filters between the operator and robot. However, a limitation of this method is that the required robot kinematic structure must be similar to the human arm kinematic structure. Other strategies have been explored to simplify the kinematic representation of the human arm for teleoperation [14]\u2013[16]. For example, [15] introduced elbow elevation angle as a constraint for a human arm mapped on the robot as a swivel motion. [16] proposes a method that separates a redundant 7-DOF manipulator as a 3-DOF manipulator attached to a 4-DOF end-effector. Using IMU sensors to couple the operator's hand with 3-DOF end-effector and elbow with 4-DOF end-effector position changes, with IK calculation for both sub-manipulators' joints in real-time to achieve teleoperation.\nOur work draws inspiration from the concept of modifying kinematic representations. While most manipulators use a combination of single DOF revolute or prismatic joints, human arm articulation kinematics are represented by a combination of ball-and-socket and condyloid joints to achieve complex joint movements. Finding robot combinatorial joint kinematics that can mimic complex human articulated arm motion is an essential step in our method. Specifically, and instead of imposing explicit constraints to redundant DOF robots, we propose a generative deep learning (DL) framework to find a low-dimensional robust implicit, kinematic representation that describes complex redundant DOF robotic motion. Our framework devises a GRU-based variational autoencoder deep neural network (DNN) that generates robot trajectories which faithfully mimic human gestural intent for completing tasks.\nA Variational Autoencoder (VAE) is a neural network architecture with an encoder, a latent space, and decoder module [17]. It was first introduced as an image-generative model that efficiently uses a neural network to approximate the likelihood function for the latent space distribution, which is modeled as a mixture of multiple Gaussian distributions derived from the training dataset. By sampling latent space features from Gaussian distributions, we can generate a new, unforeseen image by the decoder, which endows the characteristic features of an actual image. Such an architecture can also be used to prescribe 3D physical motions. [1] introduced a method that integrates two Variational Autoencoder (VAE) modules: one for processing visual sensor data and the other for the manipulator's trajectory. By utilizing the latent representation from the vision module, the decoder of trajectory-VAE can generate movement paths for object-reaching tasks in the 2D Cartesian plane.\nOn the other hand, the Recurrent Neural Networks (RNNs) are the most commonly used methods for sequential data prediction and feature extraction. For example, [9] utilizes Long Short-Term Memory (LSTM) as a neural network module to predict the human elbows and wrists position in order to implement an intuitive control for manipulator based on the time series data from an IMU sensor in a smartwatch. VAEs and RNNs can be integrated together for inference tasks. For example [18], proposed an LSTM-based VAE for missing word-imputing tasks. The LSTM allows the VAE model to consider global concepts of a sentence and generate more diverse and well-formed sentences compared with the standard RNN language model. Furthermore, in [19], they also present an LSTM-based VAE framework for robot-embodied Language Learning. This allows the system to generate the correct action description by executing the action."}, {"title": "III. METHODOLOGY", "content": "In this work, we propose a novel method that utilizes a VAE neural network architecture to learn a latent distribution space that can represent complex joint movements for a 7-DOF Kinova manipulator. By leveraging this probabilistic distribution, we can approximate the Kinova configuration space with limited training data. We also train a feed-forward neural network to map human arm kinematics to the learned latent space and use the VAE decoder to generate corresponding manipulator joint configurations. This approach involves creating datasets for both Kinova trajectories and human arm configurations. The following section details the data collection, model architecture, and overall workflow.\n1) KINOVA TRAJECTORY DATASET COLLECTION: We randomly select 500 start and end x, y, z end-effector positions in Cartesian space, and assign two corresponding random sets of valid Roll($\\phi$), Pitch($\\theta$), Yaw($\\psi$) orientation for each start and end position, i.e., p = [x, y, z, $\\phi$, $\\theta$, $\\psi$], p\u2208 R6. Inspired by [1], the MoveIt software is used to generate corresponding trajectories, including joint angles under varied time steps for every combination of initial start and end poses. Note that a jump from -180 degrees to 179 creates a discontinuity singularity of the collected joint angular data that can confound training the model [9]. To mitigate such singularities, each manipulator's angle d is converted to projected unit values (cos(d), sin(d)), which therefore results in 14 values to represent the 7-DOF manipulator. We use the Cubic Spline interpolation algorithm to appoint each trajectory time step into 0.1s (10Hz) intervals. Finally, we decompose each trajectory as a sequence of 2 time-step segments that represent the current and the next time-step trajectory features.\n2) KINEMATIC MAPPING: To achieve intuitive and efficient manipulator teleoperation with the human arm, a kinematic correlation between the manipulator and the human arm was first defined before the model training and data collection. We collect the operator's right-arm-related data for training. As shown in Fig. 1, J1,2 are mapped to the shoulder joint q1,2, J3 is mapped to the upper arm joint q3, J4 is mapped to the elbow q4, and J5 is mapped to the joint rotation of the forearm arm. However, J5,6,7 is also considered a universal joint representing the human wrist's spherical joint. Therefore, Joint J5 overlaps between the representation of forearm arm and wrist rotation. When J5,7 represents a flexion or extension action of the wrist, J5 should rotate conditionally to allow the direction of the rotation axis of J6 to match the rotation axis (q7) of the wrist's flexion or extension action, and J7 will rotate oppositely to counteract the unwanted rotation from J5 to keep the facing direction for the back of human hand and the end-effector consistent. On the other hand, if J5,7 represents either an ulnar or radial deviation (q6), J5 will only repose to the forearm arm rotation (q5), J7 should not rotate. In the end, J7 is not mapped to any human joint but is only used to counteract the rotation from J5.\n3) HUMAN ARM JOINTS CONFIGURATION DATASET COLLECTION: Once the VAE model is settled, we can construct the human arm joint configuration dataset using the XSens Awinda human skeletal motion tracking system. Wireless 6-DOF IMU sensors were attached to 11 upper human body segments (sternum, pelvis, head, L/R hands, L/R forearms, L/R upper arms, and L/R scapular skeletons joints) to approximate complete upper body skeletal motion."}, {"title": "B. Generative Models", "content": "1) GRU-BASED VARIATIONAL AUTOENCODER: We utilize a latent variable model, specifically a Variational Autoencoder (VAE), to approximate and predict the configuration space of the manipulator within the continuous probability latent space. We use 2 time-steps of joint position trajectory as the input to the VAE. The first time-step represents the manipulator's current joint positions, while the second time-step represents the next joint positions, 100 milliseconds later. As discussed in the related work, the encoder and decoder are crucial for approximating a latent distribution Q(z | x) and likelihood distribution P(x | z). To effectively capture the time-sequence information and enhance prediction accuracy, we utilized the GRU recurrent-based neural network architecture, for both the encoder and decoder in the VAE, as shown in Fig. 2. The encoder takes the Kinova robot trajectory as input feed into the GRU layer. Then, the selected features are passed to two separate single-layer neural networks that generate the mean $\\mu$ and log-variance $\\sigma$ to represent a finite number z of Gaussian distributions to approximate the true latent space distribution, where the size of z is a hyper-parameter. With the reparameterization trick equation 1 where noise $\\epsilon \\sim N(0, I)$, we can sample the latent feature Z for data reconstruction with the decoder and calculating a gradient and optimize the distribution when doing backpropagation during training.\n$Z = \\mu + \\sigma \\cdot \\epsilon$\nTo reconstruct the trajectory using the latent feature Z with the decoder, Z must be repeated to match the length of the target trajectory. Specifically, the input to the decoder is a matrix where Z is repeated twice.\nThe loss function for the VAE model has two components: (1) the trajectory reconstruction loss and (2) the KL-divergence, which are represented by 2.\n$LVAE = MAE(Input, Reconstr) + \\beta\\cdot KL (q(z|x) || p(z))$\n$KLD = -0.5 \\times \\sum(1 + \\sigma \u2013 \\mu2 \u2013 exp(\\sigma))$"}, {"title": "2) FEED-FORWARD NEURAL NETWORK", "content": "To transfer the human arm joint configuration data to the executable configuration of the manipulator, we integrated a fully-connected neural network module shown in the top part of Fig. 3. The fully connected module learns a data transformation from the human operator arm joints angle space to VAE latent feature space. We choose the Mean Absolute Error (MAE) as the loss function to penalize the error between the latent feature transformed from operator joints angle and target latent features, shown in 4.\n$Lmlp = MAE(y, \\hat{y})$\nIn addition, we select the Scaled Exponential Linear Units (SELU) as the activation function for our fully-connected module. SELU's self-normalizing properties can help lower the difficulty of the learning process since each target latent feature follows a Gaussian distribution. The activation function is shown in equation 5 where $\\lambda \\approx 1.0507$ and $\\alpha \\approx 1.67326$.\n$SELU(x) = \\lambda \\{\n\\, x \\qquad if x > 0 \\newline\n\\, \\alpha (e^x \u2013 \\alpha ) \\qquad if x < 0 $\n}"}, {"title": "IV. EXPERIMENT AND RESULTS", "content": "The experimental setup is shown in Fig 6. It shows three predefined target poses and the corresponding target area, which is the gray bounding box, and the gray dot shows the center of the target area, providing visual information for participants. Each participant is required to teleoperate the redundant robot manipulator for the target pose-reaching task, which uses the end-effector's tip to reach the gray dot for each target area while making the orientation of the end-effector as perpendicular as possible to the target area surface. The manipulator begins with the fixed starting configuration. This configuration aligns with the operator's arm being extended straight forward. The operator is required to sequentially reach each target location in numerical order, following the specified task requirements. The objective of the experiment is to test the accuracy of teleoperation and the generality of the proposed system under different operator anthropometric upper body measurements. Four participants engage in this experiment and have a mean height of 169\u00b11.24 cm and a mean arm span of 173.9\u00b13.0 cm. Note that participant 1 engages in training data collection but has not been practiced for the experiment. We hold a preparation phase for each participant, including setting up the Xsens Awinda body tracking system described in III-A.3 and measuring each participant's arm and span and full-body height for fine-tuning the body representation profile in the Xsens Awinda software. Then, we provide each participant ten minutes to move their arm to become familiar with the relation between their arm joint angles and the Kinova joint angles. In the real test procedure, each participant is required to finish the task five times. Tasks finishing time, end-effector pose, and right arm joints angle trajectory are recorded for each participant. Table II presents a summary of the results. The Euclidean distance quantifies the average distance between the end-effector's tip and the target center position across trials in Cartesian space when it touches the target surface. A smaller Euclidean distance indicates higher positional accuracy. Furthermore, the orientation difference between the target pose and the instant pose when the end-effector's tip touches the target surface is represented using the cosine similarity. A value closer to 1 indicates that the two orientations are aligned in the same direction. In all trials, participants successfully teleoperated the manipulator to reach the desired target area, with a mean absolute error 2.51\u00b10.75 cm, and the mean cosine similarity for all target poses is 0.97\u00b10.01. This supports the generality and accuracy of the proposed system."}, {"title": "V. LIMITATIONS", "content": "The initial training for mapping human arm gestures to the manipulator's latent distribution space showed promising performance for redundant robot manipulator teleoperation. However, the initial training data is collected from only one individual. This introduces potential biases, in other words, different individuals can have distinct conceptions of a robot manipulator joint configuration corresponding to their arm configuration. For example, in the experimental results, participant 3 took significantly longer to teleoperate the robot manipulator to reach the target pose. We observe that this participant attempted to use a lower back bending motion to control the Joint 0 of the manipulator, which should ideally correlate with the shoulder joint's abduc-tion/adduction. This variance highlights the need for more diverse data collection across different individuals to capture a broader range of natural human motions to correspond with the robot manipulator joint configuration. A comparable example is the MNIST dataset, where different individuals can interpret handwritten numbers differently, leading to variations in classification outcomes. Incorporating articulated thoracic and pelvic motion as additional features in human gesture mapping to the manipulator's latent distribution space can be a promising opportunity for future work. The proposed GRU-based VAE model focuses on approximating the robot's configuration space and generating joint angle trajectories. However, it currently lacks information about joint velocities, which could be crucial for improving the smoothness and responsiveness of the teleoperation. Future work could address this by extending the data collection pipeline to include joint velocity trajectories, as discussed in Section III-A.1. By incorporating velocity information into the latent distribution space, the model could generate smoother and more precise robot manipulator movements, enhancing teleoperation performance."}, {"title": "VI. CONCLUSION", "content": "This paper presents a teleoperation solution for a redundant 7-DOF robot manipulator. We developed and implemented a GRU-based VAE architecture finding a latent representation of the manipulator configuration space. The learned latent distribution space can utilize robot combinatorial joint kinematics that can mimic complex human articulated arm motion. In addition, a fully-connected neural network module mapped human arm gestures into latent distribution space representations, thus mimicking and generating corresponding robot manipulator configuration trajectories via the VAE decoder in real-time. The initial training for mapping human arm gestures to the manipulator's latent distribution space showed promising performance for redundant robot manipulator teleoperation and the decoder was able to generate new robot manipulator configurations based on human operator right arm gestures that were never encountered in the training process."}]}