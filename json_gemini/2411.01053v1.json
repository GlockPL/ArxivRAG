{"title": "Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities", "authors": ["Adriel Saporta", "Aahlad Puli", "Mark Goldstein", "Rajesh Ranganath"], "abstract": "Contrastive learning methods, such as CLIP, leverage naturally paired data\u2014for example, images and their corresponding text captions\u2014to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile's objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at https://github.com/rajesh-lab/symile.", "sections": [{"title": "1 Introduction", "content": "Contrastive learning leverages naturally paired data to learn general representations that transfer efficiently to downstream tasks [3, 35, 53]. A common contrastive approach is to maximize the mutual information between the paired modalities, ensuring that the learned representations retain sensitivity to all correlations between them. While SimCLR [12] popularized the use of the mutual information estimator InfoNCE [38] for data augmentations, CLIP [40] applied the approach to distinct modalities\u2014for example, images and their corresponding text captions\u2014where representations are learned using any encoder for each modality.\nWhile contrastive approaches are generally applied to two modalities, there is a rapidly expanding range of domains that require the integration of many types of data at once. For example, in robotics, agents combine information from visual, proprioceptive, and tactile sensors [18, 28]; healthcare providers analyze various types of patient data including imaging, biosignals, and genomics [10, 29]; and video encompasses RGB frames, audio waveforms, and text transcripts [55]. One strategy for handling multimodal data has been to design specialized architectures capable of processing all data types at once, which limits their general applicability and increases operational complexity [2, 47]. Another common approach is to apply two-modality contrastive objectives, such as CLIP, to pairs of available modalities [15, 44].\nIn this paper, we show that, despite its popularity, the pairwise application of CLIP fails to capture higher-order conditional information between modalities, thereby limiting the quality of the"}, {"title": "2 Background and motivation", "content": "In this section, we first provide background on the original CLIP objective for two modalities, and describe how it has been extended to additional modalities. We then present a simple problem set up for three modalities that illustrates where pairwise contrastive objectives fall short."}, {"title": "2.1 Pairwise contrastive learning", "content": "Given a batch of (x, y) pairs, separately encoded by $f_x$ and $f_y$, respectively, contrastive objectives such as CLIP maximize the similarity between representations of correctly paired (positive) samples and minimize the similarity between representations of incorrectly paired (negative) samples.\nAs is now standard in contrastive learning, in order to construct a batch of data, each modality is treated as the anchor in turn and used to construct a set of positive and negative samples. Letting $\\tau \\in \\mathbb{R}^+$ be a temperature parameter, the CLIP objective when x is the anchor modality is the categorical cross-entropy of correctly classifying the positive pair out of N possible pairs:\n$\\mathcal{L}_{x \\rightarrow y}^{(x+y)}(\\theta, \\tau) = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp \\left[(f_x(x) \\cdot f_y(y_i)) / \\tau\\right]}{\\sum_{j=1}^N \\exp \\left[(f_x(x) \\cdot f_y(y_j)) / \\tau\\right]}.$    (1)\nThe final CLIP objective is an average of the losses in each direction: $\\mathcal{L}_{CLIP}^{(\\theta,\\tau)} = [\\mathcal{L}_{x \\rightarrow y}^{(x+y)}(\\theta, \\tau) + \\mathcal{L}_{y \\rightarrow x}^{(y+x)}(\\theta, \\tau)]$. The dot product in Equation (1) serves as a scoring function that is trained to assign high values to positive pairs, which are sampled from the joint distribution $p_{x,y}$, and low values to negative pairs, which are sampled from the product of marginals $p_x p_y$.\nContrastive methods are typically designed to maximize the mutual information between x and y, which is defined as the Kullback-Leibler divergence from the joint distribution to the product of the marginal distributions: $I(x; y) = D_{KL}(p(x, y) || p(x)p(y))$. It has been shown that Equation (1) maximizes a lower bound on the mutual information between x and y [38, 39]. This information maximization ensures that the learned representations preserve all correlations between the modalities, which is essential for downstream tasks.\nIncorporating additional modalities. In order to learn a joint embedding space for more than two modalities, existing work has applied the CLIP objective in a pairwise fashion [1, 2, 9, 11, 14, 21,"}, {"title": "2.2 A simple one-dimensional problem for three binary modalities", "content": "While contrastive objectives were originally designed for two modalities, the naive pairwise extension of CLIP to additional modalities warrants a deeper analysis. To explore this further, we propose a simple problem setup for the following data generating process:\n$a, b \\sim \\text{Bernoulli}(0.5), \\qquad c = a \\text{ XOR } b$.\nUsing the pairwise CLIP objective, we fit three affine linear models to perform the zero-shot classification task of predicting whether b is 0 or 1 given a, c. See Appendix I for additional details.\nEven in this simple one-dimensional controlled setting where the target b is perfectly predictable from a and c, CLIP performs no better than random chance, with an accuracy of 0.5.\nCLIP failure analysis. It can be shown that even though the variables a, b, c are jointly dependent\u2014since c is a deterministic function of a and b\u2014they are pairwise independent (Appendix A):\n$I(a; b) = I(b; c) = I(a; c) = 0, I(a; b | c) > 0$.\nThis explains CLIP's poor performance for the above XOR experiment: the objective maximizes a lower bound on the mutual information between pairwise terms, and therefore was not designed to capture higher-order dependencies such as the dependence between a and b given c. Capturing conditional dependencies like this will require the formulation of a new contrastive learning objective."}, {"title": "3 Learning Symile representations", "content": "Instead of targeting the mutual information between pairs of modalities, we target the total correlation between any number of modalities, learning what we call Symile representations.\nTotal correlation [50]\u2014the higher-order generalization of mutual information\u2014is defined as the Kullback-Leibler divergence from the joint distribution to the product of the marginal distributions:\n$TC(x_1,\\ldots,x_M) = D_{KL}(p(x_1,\\ldots, x_M) || p(x_1) \\cdots p(x_M))$.\nIn words, total correlation is a symmetric statistical measure that captures the amount of information shared in a set of random variables. A higher total correlation implies more dependency among the variables, and a total correlation of zero indicates that the variables are independent.\nTotal correlation can be decomposed into a summation of mutual information terms. For example, in the case of three random variables,\n$TC(x, y, z) = [I(x; y) + I(z; x, y)] + [I(y; z) + I(x; y, z)] + [I(x; z) + I(y; x, z)]$\\newline$\\text{Symile target}$\n$= 2 \\cdot [I(x; y) + I(y; z) + I(x; z)] + I(x; y | z) + I(y; z | x) + I(x;z|y).    (2)$\\newline$\\text{pairwise information}\\newline(\\text{CLIP target})\\qquad\\text{higher-order information}$\nWhile, as discussed, contrastive learning was designed to capture the shared information between modalities, Equation (2) indicates that when there are more than two modalities, the scope of what to capture should extend beyond pairwise information to include conditional interactions (Figure 1)."}, {"title": "3.1 Deriving a multi-sample lower bound on total correlation", "content": "In order to eventually derive a contrastive objective by maximizing total correlation, we first establish a multi-sample lower bound on total correlation. This lower bound and, in the next section, the Symile objective are illustrated using three modalities for simplicity, but both can be extended to an arbitrary number of modalities, as shown in Appendix B.\nGiven a batch of N (x, y, z) triples, let\n$i \\sim \\text{Uniform}(\\{1,\\ldots,N\\})$    (3)\ndenote the index of the positive triple in the batch. Our goal is to estimate TC(x, y, z) given one positive triple sampled from the joint distribution, and N \u2212 1 negative triples sampled from the product of marginals:\n$x_i, y_i, z_i \\sim p_{x,y,z}(x_i, y_i, z_i), \\qquad x, y_{j\\neq i}, z_{j\\neq i} \\sim p(x)p_y(y_{j\\neq i})p_z(z_{j\\neq i})$.\nLetting $Y_N = \\{y_n\\}_{n=1}^N$ and $Z_N = \\{z_n\\}_{n=1}^N$ be the sets of all samples of y and z, respectively, this sampling procedure describes the following distribution:\n$\\underset{\\text{positive sample}}{y, z \\text{ from} \\qquad\\text{y,z from}\\qquad\\text{negative samples}}{p(x, Y_N, Z_n | i = i) = p(x) p_{y,z|x}(y_i, z_i | x) \\prod_{j\\neq i} p_y(y_j) \\prod_{j\\neq i} p_z(z_j)}$.    (4)\nWe derive the following lower bound in Appendix B:\nTheorem 3.1 (Total Correlation Lower Bound). Given the distributions in Equations (3) and (4), for any value i of i and any scoring function g, a multi-sample contrastive lower bound on total correlation is\n$TC(x, y, z) \\geq \\log N + \\mathbb{E}_{p(x, Y_N,Z_N | i=i)} \\log \\frac{\\exp g(x, y_i, z_i)}{\\sum_{j=1}^N \\exp g(x, y_j, z_j)}.$    (5)\nAs described in Section 2.1, in contrastive learning each modality is sequentially treated as the anchor, with a batch of corresponding positive and negative samples generated for each. Theorem 3.1 treats x as the anchor modality, but by symmetry holds when y or z acts as the anchor modality."}, {"title": "3.2 The Symile objective", "content": "We now derive the Symile loss by maximizing the total correlation lower bound in Theorem 3.1.\nInstead of using the dot product as a scoring function, as CLIP does, Symile uses its generalized form: the coordinate-wise sum of the element-wise product of a set of vectors. We call this the multilinear inner product (MIP): $\\oplus(\\lbrace x_i \\rbrace_{i=1}^M) = \\sum_d \\prod_{i=1}^M x_{i,d}$. As a scoring function, the MIP strikes a balance between computational simplicity and expressive power: it represents one of the simplest possible generalizations of the dot product to more than two modalities, and the vector multiplication ensures it is expressive enough to model any joint statistic.\nGiven a batch of $N'$ positive triples $(x_i, y_i, z_i)$, each with N \u2212 1 corresponding negative triples $(x, y', z')$, and letting $\\tau \\in \\mathbb{R}^+$ be a temperature parameter, the Symile loss is the negative of an empirical estimate of the expected log likelihood in Equation (5):\n$\\mathcal{L}_{Symile}^{(x+y,z)}(\\theta, \\tau) = - \\frac{1}{N'} \\sum_{i=1}^{N'} \\log \\frac{\\exp \\langle (f_x^{\\theta}(x_i), f_y^{\\theta}(y_i), f_z^{\\theta}(z_i)) / \\tau\\rangle}{\\sum_{j=1}^{N'-1} \\exp \\langle (f_x^{\\theta}(x_i), f_y^{\\theta}(y_i), f_z^{\\theta}(z_i)) / \\tau\\rangle + \\sum_{j=1}^{N-1} \\exp \\langle (f_x^{\\theta}(x_i), f_y^{\\theta}(y'_j), f_z^{\\theta}(z'_j)) / \\tau\\rangle}.$   (6)"}, {"title": "3.3 Learning sufficient statistics with Symile", "content": "An important property of Symile is that it learns sufficient statistics, which is central to the representations' effectiveness for downstream tasks.\nTheorem 3.3 (Symile Sufficient Statistics). Let x, y, z be three random variables whose optimal representations when trained using Symile are $f_x^*(x), f_y^*(y), f_z^*(z)$, respectively. The element-wise product of any subset of the representations is a sufficient statistic for predicting the remaining random variables.\nFor example, $f_x^*(x) \\odot f_z^*(z)$ is a sufficient statistic for predicting y, which can be expressed using the following conditional independence statement:\n$y \\perp x, z | f_x^*(x) \\odot f_z^*(z)$.\nThe proof can be found in Appendix G. The independence statement in Theorem 3.3 tells us that the element-wise product of the Symile representations of any subset of modalities contains all the information required to predict the remaining modalities. In other words, once Symile representations have been computed, access to the full data is no longer needed. Theorem 3.3 confirms Symile's ability to learn efficient modality-specific representations for downstream tasks."}, {"title": "3.4 Zero-shot prediction using the scoring function", "content": "Just as with CLIP, the optimal scoring function $g^*$ (Lemma 3.2) can be used to predict one of the modalities $y \\in \\mathcal{Y}$ using instances of the other modalities x, z. If p(y) is uniformly distributed, then the scoring function can be used to rank the candidates for y: $\\text{arg max}_{y\\in \\mathcal{Y}} p(y = y |x, z) = \\text{arg max}_{y\\in \\mathcal{Y}} g^*(x, y, z)$.\nHowever, this zero-shot approach, whether applied to Symile or to CLIP, does not lead to the Bayes optimal prediction and, consequently, does not always yield reliable results when p(y) is not uniformly distributed (see Appendix H for a detailed discussion). To address this issue, we can instead compute the desired conditional probability directly using the scoring function:\nTheorem 3.4 (Conditional Distribution using the Scoring Function). Let x, y, z be three random variables whose optimal representations when trained using Symile are $f_x^*(x), f_y^*(y), f_z^*(z)$, respectively. Let the MIP $\\langle f_x^*(x), f_y^*(y), f_z^*(z) \\rangle$ be the scoring function. Then,\n$p(y | x, z) = \\frac{\\exp [\\langle f_x^*(x), f_y^*(y), f_z^*(z) \\rangle] p(y)}{\\int_{\\mathcal{Y}} \\exp [\\langle f_x^*(x), f_y^*(y), f_z^*(z) \\rangle] p(y) dy}.$  (7)\nIf the marginal distribution of y is known, we could then perform zero-shot classification in one of two ways. When the distribution p(y | x, z) itself is of interest, as is often the case in healthcare [10], we could compute p(y | x, z) directly, following Equation (7). Alternatively, if only predictions are needed, we could use\n$\\langle f_x^*(x), f_y^*(y), f_z^*(z) \\rangle + \\log p(y)$\nto rank the possible values for y. If the marginal distribution of y is not known, then because $f_x^*(x) \\odot f_z^*(z)$ is a sufficient statistic for predicting y (Theorem 3.3), we could instead use $f_x^*(x) \\odot f_z^*(z)$ to train a simple model to predict any property of y, s(y): $p(s(y) | f_x^*(x) \\odot f_z^*(z))$.\nNote that although the above discussion centers on Symile, it applies equally to CLIP and its own scoring function, the dot product."}, {"title": "4 Related work", "content": "Contrastive learning beyond two modalities. As discussed, previous work has extended contrastive learning to multiple modalities by applying CLIP to pairs of available modalities. Tian et al. [49] distinguish between two such pairwise approaches: core view and full graph. The core view strategy fixes one modality and then averages the loss terms between that primary modality and each of the other modalities [1, 11, 44]. ImageBind [15] exemplifies this approach, using CLIP to align image embeddings with embeddings from five other modalities: text, audio, depth, thermal, and motion sensor data. One advantage of this strategy is that it avoids the need for datasets with all modalities (though each dataset must still align with a primary modality). As discussed in Sections 3.2 and 5.2, Symile representations can be learned even with modalities missing in the data.\nThe full graph strategy which we have referred to as pairwise CLIP in this paper is to consider all $\\binom{N}{2}$ contrastive losses [9, 14, 33, 34, 43]. For example, Guzhov et al. [19] extend CLIP to include audio with text-to-image, text-to-audio, and image-to-audio losses. While this pairwise strategy captures strictly more information than the one used by ImageBind, neither pairwise approach is able to capture the higher-order information that Symile does.\nPairwise CLIP has also been applied to architecture-specific fusion models that simultaneously process modalities to capture cross-modal interactions [2, 21, 52]. For example, Shvetsova et al. [47] train a Transformer to accept any number of modalities, using a weighted sum of contrastive losses across all input combinations. Such fusion approaches face a combinatorial explosion not only in the number of weighting coefficients to tune, but also in the number of forward passes required per batch. In contrast, Symile is architecture-agnostic and can learn modality-specific representations.\nTargeting higher-order information with contrastive learning. The use of contrastive methods to target higher-order information has been explored primarily within the context of multiple augmentations of the same data. For instance, Bai et al. [5] derive a total correlation estimator by"}, {"title": "5 Experiments", "content": "In this section, we empirically evaluate Symile on cross-modal retrieval tasks in three settings: a synthetic dataset, a multilingual dataset encompassing text, images, and audio, and a clinical dataset with chest X-rays, electrocardiograms, and blood labs. Throughout our experiments, we use pairwise CLIP as a baseline comparison since, as outlined in Section 4, it represents the only architecture-agnostic approach that applies contrastive objectives to more than two modalities. We release all datasets and code used in these experiments at https://github.com/rajesh-lab/symile."}, {"title": "5.1 Synthetic data", "content": "Building on the illustrative XOR experiment from Section 2, we first test Symile on a synthetic dataset drawn according to the following sampling procedure:\n$a_j, b_j \\sim \\text{Bernoulli}(0.5), \\qquad i \\sim \\text{Bernoulli}(p), \\qquad c_j = (a_j \\text{ XOR } b_j) \\cdot i + (1-i)$.\nWe fit three affine linear functions that map $a, b, c \\in \\mathbb{R}^5$ to representations $r_a, r_b, r_c \\in \\mathbb{R}^{16}$, respectively, and evaluate the model's ability to correctly predict $r_b$ given the pair $(r_a, r_c)$.\nResults. Figure 3 (left) compares Symile and CLIP across varying values of p. Both models start with a mean accuracy of 0.032\u00b10.001 (SE) at p = 0. As \u00ee\u00ee increases, Symile's accuracy progressively climbs, reaching perfect accuracy at \u00ee\u00ee = 1 \u00b1 0.0 (SE). In contrast, CLIP's accuracy remains nearly constant, barely surpassing the baseline random guessing rate of 0.031 (1/32).\nThis performance gap is a consequence of the changing information dynamics between the variables as \u00ee\u00ee moves from 0 to 1, as shown in Figure 3 (right). When p = 0, b shares no information with a and c\u2014either pairwise or conditionally\u2014rendering both models incapable of predicting $r_b$ from $(r_a, r_c)$. As p increases, the higher-order $I(a; b | c)$ and $I(c; b | a)$ rise, driving a corresponding improvement in Symile's performance. However, because the pairwise $I(a; b)$ and $I(b; c)$ are always zero, there is no value of p\u00f4 at which CLIP is able to predict $r_b$ from $(r_a, r_c)$."}, {"title": "5.2 Symile-M3: a multilingual dataset", "content": "We now evaluate Symile on a new multilingual dataset comprising 33 million (audio, image, text) samples. The dataset, Symile-M3, is specifically designed to test a model's ability to capture higher-order information between three distinct high-dimensional data types: by incorporating multiple languages, we construct a task where text and audio are both needed to predict the image, and where, importantly, neither text nor audio alone would suffice.\nDataset design and model setup. Let w represent the number of languages in the dataset. An (audio, image, text) sample is generated by first drawing a short one-sentence audio clip from Common Voice [4] spoken in one of w languages with equal probability. An image is drawn from ImageNet [45] that corresponds to one of 1,000 classes with equal probability. Finally, text containing exactly w words is generated based on the drawn audio and image: one of the w words in the text is the drawn image class name in the drawn audio language. The remaining w \u2013 1 words are randomly chosen from the ImageNet class names and written in one of the w languages such that there is no overlap in language or class name across the w words in the text. The words are separated by underscores, and their order is randomized. We release three versions of the dataset: Symile-M3-2, Symile-M3-5, and Symile-M3-10, corresponding to 2, 5, and 10 languages (w). Figure 4a shows an example of the data-generating process for Symile-M3-5. For each of the three datasets, 10M training, 500K validation, and 500K test samples were generated.\nWe use pre-trained encoders, freezing all parameters except for those in the text encoder's embedding layer and first encoder layer, which are fine-tuned. We train three linear projections to map each encoder's representation to the same 8192-dimensional space. The Symile loss is trained with O(N) negative sampling. See Appendix I for details.\nEvaluation and results. We evaluate the learned representations on the zero-shot retrieval task of finding an image of the appropriate class given the audio and text. The most probable image for a given query audio and text pair, selected from all possible candidate images in the test set, is that with the highest similarity score (Figure 2b). Symile-M3 was designed to ensure that neither text nor audio alone would suffice to predict the image. Therefore, success on this zero-shot retrieval task hinges on a model's ability to capture joint information between the three modalities.\nAs shown in Figure 4b, Symile successfully leverages this joint information, with mean accuracies of 0.939, 0.919, and 0.882 on Symile-M3-2, Symile-M3-5, and Symile-M3-10, respectively, calculated across 10 bootstrap samples of the test set, all with standard error less than 4.0 \u00d7 10\u20134. In contrast, CLIP, which captures pairwise information between image and text, can only predict an image randomly from among the w class labels present in the text, resulting in mean accuracies of 0.473, 0.187, and 0.094 on Symile-M3-2, Symile-M3-5, and Symile-M3-10, respectively, all with standard error < 3.01 \u00d7 10\u20134. Because CLIP cannot distinguish between the class labels in the text using the audio language, it can only pick a class label at random, bounding its accuracy by 1/w.\nMissing data. We also train Symile on a variant of Symile-M3-2 where each modality is independently missing with probability 0.5 or 0.65, corresponding, respectively, to probabilities 0.125 and 0.043 of a complete data sample in the training set (see Appendix I for details). As before, the test set consists of complete triples. As shown in Figure 4c, even when only 12.5% of the training data is"}, {"title": "5.3 Chest X-ray prediction using electrocardiograms and laboratory measurements", "content": "Zero-shot retrieval is widely used in the evaluation of representation learning for healthcare [6, 22, 29, 51, 56]. In this section, we evaluate the Symile objective on Symile-MIMIC, a clinical dataset comprised of chest X-rays, electrocardiograms, and blood labs from MIMIC-IV [17, 24, 27] and MIMIC-CXR [25, 26]. Since ECGs and labs are both safer than CXRs, this experiment explores whether an ECG and labs collected at admission are predictive of a CXR taken shortly thereafter.\nDataset design and model setup. Each data sample includes an ECG reading and blood labs taken within 24 hours of the patient's admission to the hospital, and a CXR taken in the 24- to 72-hour period post-admission (Figure 5a). Our analysis focuses on the 50 most common blood labs, with each sample containing at least one.\nWe split our dataset (11,622 admissions) into a train/validation development set (95% of patients) and a test set (5% of patients), ensuring there is no patient overlap across the splits. Following previous work, we use the ResNet-50 and ResNet-18 architectures [20] for the CXR and ECG encoders, respectively, and a three-layer neural network to encode the blood labs. All encoders are trained from scratch, and three linear projections map each encoder's representation to the same 8192-dimensional space. Given the limited size of the dataset, the Symile loss is trained with O(N2) negative sampling to mitigate overfitting. See Appendix I for details.\nEvaluation and results. We evaluate the learned representations on the zero-shot retrieval task of finding the most probable candidate CXR for a given query ECG and labs pair according to the similarity score. For each query ECG and labs pair in the test set, we sample nine negative CXR candidates from the remaining test samples, so that that each query has a total of 10 candidates: one positive (the true corresponding CXR) and nine negative.\nIn Figure 5b, we report mean accuracy for Symile and CLIP over 10 bootstrap samples of the test set. While both models surpass random chance (0.1), Symile achieves an average accuracy of 0.435 \u00b1 0.007 (SE), outperforming CLIP's 0.387 \u00b1 0.003 (SE). These results correspond to a 12.5% increase in accuracy for Symile over CLIP."}, {"title": "6 Conclusion", "content": "This work presents Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations, maintaining the simplicity of CLIP while delivering superior performance, even in cases of missing modalities. Because it targets total correlation, Symile captures strictly more information than CLIP, guaranteeing performance that matches or surpasses CLIP, except in cases where it known that only pairwise statistics are relevant. Given that such prior knowledge is rarely available, Symile should be favored over CLIP.\nFuture work. (1) The sigmoid-based loss function SigLIP [54] was recently introduced as a memory-efficient alternative to traditional softmax-based contrastive objectives. A potential avenue for future work would be to adapt Symile, and its use of the multilinear inner product, to this sigmoid loss. (2) The proposed implementation of Symile relies on an approximation for negative sampling, and future work could examine how this approximation scales when applied to settings with more than three modalities. (3) Future work could integrate pre-trained Symile representations into multimodal large language models, enabling them to capture higher-order information between modalities."}, {"title": "A Pairwise independence in binary XOR experiment", "content": "In this section, we show that the three variables in the XOR experiment in Section 2.2 are pairwise independent.\nLet\n$a, b \\sim \\text{Bernoulli}(0.5)$\n$C = a \\text{ XOR } b$.\nFirst, we will show that c \u223c Bernoulli(0.5):\n$P(c = 1) = \\sum_{a,b} P(c = 1 | a = a, b = b)P(a = a)P(b = b)$\n$= 0.25 \\sum_{a,b} P(c = 1 | a = a, b = b)$\n$= 0.25 \\cdot [P(c = 1 | a = 0, b = 0) + P(c = 1 | a = 0, b = 1)$\n$+ P(c = 1 | a = 1, b = 0) + P(c = 1 | a = 1, b = 1)]$\n$= 0.25 \\cdot [0 + 1 + 1 + 0]$\n$= 0.5$.\nNext, we will show that c | a \u223c Bernoulli(0.5):\n$P(c = 1 | a) = \\frac{P(a| c = 1)P(c = 1)}{P(a)} = 0.5$.\nBy symmetry, since c | a \u223c Bernoulli(0.5), then c | b \u223c Bernoulli(0.5)."}, {"title": "B Total correlation lower bound", "content": "Our goal in this section is to derive a lower bound on TC(m1, ..., m\u043c).\nWe start by describing in Appendix B.1 the sampling procedure for a batch of (m\u2081,..., mM) tuples. In Appendix B.2, we derive the desired lower bound in Theorem 3.1 (our proof was inspired by Poole et al. [39]'s derivation of the InfoNCE lower bound, which does not rely on an approximation used by Oord et al. [38]). In Appendix B.3, we show that the bound is closed at optimality. Finally, we use the lower bound to define the Symile objective in Appendix B.4."}, {"title": "B.1 Sampling procedure", "content": "We start by describing the sampling procedure for the batch of N M-tuples. In contrastive learning, the objective is to differentiate between positive and negative samples constructed from a given batch of matched data. In order to construct these samples, each modality is treated as the anchor in turn, and then for each anchor modality a corresponding set of positive and negative samples is generated.\nLet y be arbitrary in {1, . . ., M}, let my denote the anchor modality, and let m\u2212y denote the M \u2013 1 non-anchor modalities. Let\n$i \\sim \\text{Uniform}(\\{1, ..., N\\})$    (8)\ndenote the index of the positive M-tuple in the batch.\nWe draw $m_y$ from $p(m_y)$ and $m_{-y,i}$ from $p_{m_{-y}|m_y}(m_{-y,i} | m_y)$. We call $(m_y, m_{-y,i})$ our positive tuple.\nFor each non-anchor modality $m_{\\ell \\neq y}$, we draw N \u2013 1 samples of $m_{\\ell,j}$ from $p_{m_{\\ell}}(m_{\\ell,j})$, so that there are N \u2013 1 total negative tuples $(m_y, m_{-y,j})$.\nLet $\\mathbb{M}_{-y} = \\{m_{-y,n}\\}_{n=1}^N$ be the set of all samples of non-anchor modalities $m_{-y}$ in the batch.\nThis sampling procedure describes the following distribution:\n$\\underset{\\text{positive sample}}{m_{-y} \\text{ from }} \\underset{\\text{negative samples}}{p(m_y, \\mathbb{M}_y | i = i) = p(m_y) p_{\\mathbb{M}_{-y} | m_y} (m_{-y,i} | m_y) \\prod_{\\ell\\neq v} \\prod_{j\\neq i} p_{m_{\\ell}}(m_{\\ell,j})}$.    (9)\nLetting $M_{\\ell \\neq y} = \\{m_{\\ell,n}\\}_{n=1}^N$ be the set of all samples of modality $m_{\\ell}$ in the batch, the following properties hold by Lemma C.1:\n$p(m | i = i) = p(m)$\n$p(M_{\\ell \\neq y} | i = i) = \\prod_{j=1}^N p_{m_{\\ell}} (m_{\\ell,j}) = p(M_{\\ell})$."}, {"title": "B.2 Lower bound on total correlation", "content": "We now derive a lower bound on TC"}]}