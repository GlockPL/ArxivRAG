{"title": "Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization", "authors": ["Soham Sane", "Collins Aerospace"], "abstract": "Hybrid Group Relative Policy Optimization (Hybrid GRPO) is a reinforcement learning framework that extends Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) by incorporating empirical multi-sample action evaluation while preserving the stability of value function-based learning. Unlike DeepSeek's GRPO, which eliminates the value function in favor of purely empirical reward estimation, Hybrid GRPO introduces a structured advantage computation method that balances empirical action sampling with bootstrapped value estimation. This approach enhances sample efficiency, improves learning stability, and mitigates variance amplification observed in purely empirical methods. A detailed mathematical comparison between PPO, DeepSeek GRPO, and Hybrid GRPO is presented, highlighting key differences in advantage estimation and policy updates. Experimental validation in a controlled reinforcement learning environment demonstrates that Hybrid GRPO achieves superior convergence speed, more stable policy updates, and improved sample efficiency compared to existing methods. Several extensions to Hybrid GRPO are explored, including entropy-regularized sampling, hierarchical multi-step sub-sampling, adaptive reward normalization, and value-based action selection. Beyond reinforcement learning in simulated environments, Hybrid GRPO provides a scalable framework for bridging the gap between large language models (LLMs) and real-world agent-based decision-making. By integrating structured empirical sampling with reinforcement learning stability mechanisms, Hybrid GRPO has potential applications in autonomous robotics, financial modeling, and AI-driven control systems, such as Tesla's Full Self-Driving (FSD) and autonomous drone navigation. These findings suggest that Hybrid GRPO serves as a robust and adaptable reinforcement learning methodology, paving the way for further advancements in policy optimization.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has achieved significant success across various domains, from robotic control to game playing. Among policy optimization methods, Proximal Policy Optimization (PPO) Schulman et al. (2017) remains one of the most widely adopted techniques due to its balance between sample efficiency and stability. PPO leverages a value function V(s) to estimate expected returns, allowing for variance reduction in policy gradient updates. However, the reliance on bootstrapped value function approximation introduces bias, which can limit learning performance in certain settings.\nGroup Relative Policy Optimization (GRPO) was recently introduced by DeepSeek DeepSeek (2025) as an alternative to value function-based approaches. GRPO eliminates V(s) entirely and instead computes empirical returns by sampling multiple actions per state. While this approach removes function approximation bias, it significantly increases sample complexity and introduces high reward variance, which may impede convergence in practical applications.\nThis work presents Hybrid Group Relative Policy Optimization (Hybrid GRPO), a reinforcement learning framework that combines the strengths of both PPO and GRPO. Hybrid GRPO retains the value function V(s) while integrating multiple action samples per macro-step, extracting additional training data without sacrificing the stability provided by value function-based bootstrapping. By incorporating systematic empirical sampling and adaptive reward transformations, Hybrid GRPO increases data efficiency while reducing the variance typically associated with purely empirical return-based methods."}, {"title": "1.1 Contributions of This Work", "content": "This paper introduces a new reinforcement learning paradigm and provides both theoretical and empirical validation of Hybrid GRPO. The key contributions of this work are as follows:\n\u2022 A novel mathematical formulation for advantage estimation that integrates empirical return sampling while preserving V(s), bridging the gap between PPO and DeepSeek GRPO.\n\u2022 A multi-sample estimation strategy that improves data density by extracting multiple training samples per macro-step.\n\u2022 A comprehensive mathematical comparison of PPO, DeepSeek GRPO, and Hybrid GRPO, detailing the differences in their advantage estimation methods, policy updates, and sample efficiency.\n\u2022 Empirical validation through a custom synthetic simulation, demonstrating improved sample efficiency, faster convergence, and more stable policy updates compared to PPO and DeepSeek GRPO.\n\u2022 A discussion on potential future enhancements, including entropy-enhanced sampling, multi-step sub-sampling, adaptive reward scaling, and learned value-based action selection to further refine the Hybrid GRPO methodology.\nHybrid GRPO represents a new direction in reinforcement learning by combining empirical action sampling with structured value function estimation, ensuring a balance between sample efficiency, training stability, and variance reduction. The findings of this work suggest that multi-sample advantage estimation provides a more robust learning signal compared to both PPO and DeepSeek GRPO, paving the way for improved reinforcement learning architectures."}, {"title": "2 Mathematical Comparison: PPO vs. GRPO vs. Hybrid GRPO", "content": "This section presents a detailed mathematical comparison of three reinforcement learning approaches.\n1. Proximal Policy Optimization (PPO), which relies on a value function V(s) to estimate expected returns.\n2. DeepSeek's Group Relative Policy Optimization (GRPO), which completely eliminates V(s) and replaces it with empirical return estimates from multiple action samples.\n3. The proposed Hybrid GRPO, which retains V(s) while incorporating multiple sampled actions per macro-step to enhance the policy update.\nEach approach is analyzed in terms of its formulation of the advantage function, action sampling strategy, and reward estimation methodology."}, {"title": "2.1 Proximal Policy Optimization (PPO)", "content": "PPO relies on a learned value function V(s) as a baseline for estimating the advantages. The advantage function is defined as:\n$A_T = Q(s_T, a_T) \u2013 V(s_T)$ (1)\nExpanding Q(s_T, a_T) using the Bellman equation (Sutton and Barto, 2018):\n$A_T = [R_T + \\gamma V(s_{T+1})] \u2013 V(s_T)$ (2)\nwhere:\n\u2022 $R_T = r(s_T, a_T)$ is the reward obtained from executing action $a_T$ in state $s_T$.\n\u2022 V(s_T) is the estimated value function, computed using a critic network.\n\u2022 V(s_{T+1}) is the predicted value of the next state.\n\u2022 $\\gamma$ is the discount factor.\nThe PPO loss function optimizes a clipped surrogate objective:\n$L_{PPO} = E [min (\\rho_T A_T, clip(\\rho_T, 1 \u2013 \\epsilon, 1 + \\epsilon)A_T)]$ (3)\nwhere $\\rho_T$ is the probability ratio:\n$\\rho_T = \\frac{\\pi_\\theta(a_T|s_T)}{\\pi_{\\theta_{old}}(a_T|s_T)}$ (4)"}, {"title": "2.2 DeepSeek GRPO (Empirical Return-Based)", "content": "DeepSeek GRPO removes the value function entirely and instead estimates advantages using empirical action sampling (DeepSeek, 2025). At each macro-step T, multiple actions are sampled:\n$a_t \\sim \\pi_\\theta(a|s_t), t = 1, ..., N$ (5)\nFor each sampled action, an empirical reward is computed (Raw reward is normalized via a tanh() activation function):"}, {"title": "2.3 Hybrid GRPO: Multi-Sample Reward Estimation with V(s)", "content": "Hybrid GRPO extends the PPO framework by preserving the value function while incorporating multi-sampling at each macro-step. The primary objective is to extract more informative data points per state transition while maintaining the stability benefits of bootstrapped value function estimation. The key difference with Hybrid GRPO is that raw rewards are compared locally at each macro time step, T, and therefore pass through an activation function tanh() prior to being tabulated as extra data points.\nStep 1: Multi-Sampling Actions\nInstead of selecting a single action $a_T$, Hybrid GRPO samples multiple actions per macro-step:\n$a_t^{(t)} \\sim \\pi_\\theta(a|s_t), t = 1, ..., N$ (10)\nStep 2: Computing Empirical Rewards\nFor each sampled action $a_T^{(t)}$, the environment's reward function is evaluated:\n$R_t^{(t)} = r(s_t, a_t^{(t)})$ (11)\nStep 3: Applying a Reward Transformation Function\nA function f(R) is introduced to modify sampled rewards, improving stability:\n$R_t^{(t)} = f(R_t^{(t)})$ (12)\nwhere f(R) is an adaptive transformation function that can normalize, clip, or scale rewards. The default normalization function is tanh().\nStep 4: Computing Multi-Sample Advantage\nThe advantage function incorporates multiple empirical samples while still leveraging V(s):\n$A_T = \\frac{1}{N} \\sum_{t=1}^N [R_t^{(t)} + \\gamma V(s_{T+1}^{(t)}) - V(s_T)]$ (13)\nThis formulation introduces several improvements over PPO and DeepSeek GRPO:"}, {"title": "Step 5: Optimizing the Policy", "content": "The Hybrid GRPO policy loss remains similar to PPO:\n$L_{Hybrid-GRPO} = E [min (\\rho_T A_T, clip(\\rho_T, 1 \u2013 \\epsilon, 1 + \\epsilon)A_T)]$ (14)"}, {"title": "2.4 Summary of Differences", "content": "The key distinctions between PPO, DeepSeek GRPO, and Hybrid GRPO are summarized in Table 1."}, {"title": "3 Experimental Findings and Future Improvements", "content": "The core mathematical refinement introduced in Hybrid GRPO involves modifying the advantage function to incorporate multiple sampled actions per macro-step, thereby extracting richer training information from a single state transition while preserving the value function advantages. The proposed framework mitigates the variance amplification observed in empirical return-based approaches while improving sample efficiency compared to standard PPO. Our experimental validation, conducted in a custom synthetic simulation, demonstrates that Hybrid GRPO achieves faster convergence and improved policy stability."}, {"title": "3.1 Experimental Findings", "content": "The empirical evaluation of Hybrid GRPO was conducted using a controlled reinforcement learning environment with structured synthetic data. Details on this controlled experiment and synthetic simulation can be found at (Hybrid GRPO GitHub, 2025). The results, via careful monitoring of various training scenarios, indicated that:\n\u2022 Hybrid GRPO exhibits a lower variance in policy gradient updates than DeepSeek GRPO due to the retention of V(s) as a baseline estimator.\n\u2022 The increased data density per macro-step enables Hybrid GRPO to learn more efficiently compared to PPO, particularly in environments with sparse rewards.\n\u2022 Hybrid GRPO demonstrates superior convergence properties, reaching optimal policy performance with fewer training iterations compared to both PPO and DeepSeek GRPO.\n\u2022 The advantage estimation using multi-sample empirical rewards enhances learning stability while preserving the bootstrapped value function's role in reducing policy gradient variance.\nMore details and experimentation across various scenarios are necessary to validate these solutions, however, as this is the first implementation of this algorithm, this sandbox simulation was created to prove the basic validity of the alpha version of Hybrid GRPO. Future research aimed at validating these findings across various scenarios should consider this paper as the foundational theoretical framework for the algorithm."}, {"title": "3.2 Theoretical Implications", "content": "The Hybrid GRPO framework underscores the trade-offs between bootstrapped value function estimation and empirical sampling in reinforcement learning. The key theoretical insights derived from this work include:\n\u2022 Balancing Sample Complexity and Bias: PPO relies on a learned value function, reducing variance at the cost of introducing function approximation bias. DeepSeek GRPO, on the other hand, eliminates bias but at the expense of increased sample complexity. Hybrid GRPO preserves the stability of PPO while increasing training density through multi-action sampling.\n\u2022 The Role of Multi-Sample Estimation: By evaluating multiple action-value pairs per macro-step, Hybrid GRPO effectively extends the policy gradient's information capacity, leading to more precise updates in environments where reward structures are complex or sparse."}, {"title": "3.3 Future Research Directions", "content": "The development of Hybrid GRPO opens several avenues for further exploration and enhancement:"}, {"title": "3.3.1 Entropy-Regularized Sampling Strategies", "content": "To further improve exploration-exploitation trade-offs, Hybrid GRPO can be extended with entropy-regularized sampling mechanisms (Ziebart, 2008; Haarnoja et al., 2018). Drawing inspiration from Maximum Entropy RL (Ziebart, 2008) and Soft Actor-Critic (SAC) (Haarnoja et al., 2018), the policy can be modified to maximize entropy while optimizing reward. The entropy-regularized loss function can be formulated as:\n$L_{Hybrid-GRPO} = E [min (\\rho_T A_T, clip(\\rho_T, 1 \u2013 \\epsilon, 1 + \\epsilon)A_T) + \\alpha H(\\pi(\\cdot|s_T))]$ (15)\nwhere $H(\\pi)$ represents the entropy of the policy and \u03b1 is an adaptive temperature parameter. This modification would ensure greater policy robustness in dynamic environments."}, {"title": "3.3.2 Hierarchical Multi-Step Sub-Sampling", "content": "While Hybrid GRPO samples multiple actions per macro-step, the framework does not yet incorporate multi-step sub-sampling. Introducing a hierarchical sampling strategy, where sub-sampled transitions are used to refine bootstrapped value estimates, could further improve policy learning (Sutton and Barto, 2018; Mnih et al., 2016). A generalized n-step return formulation for Hybrid GRPO would be:\n$A_T = \\frac{1}{N} \\sum_{t=1}^N [\\sum_{k=0}^{n-1} \\gamma^k R_{t+k}^{(t)} + \\gamma^n V(s_{t+n}^{(t)})] - V(s_T)$ (16)\nwhere n represents the number of sub-sampled steps. This approach would allow Hybrid GRPO to better capture long-term dependencies in sequential decision-making tasks."}, {"title": "3.3.3 Adaptive Reward Normalization", "content": "Reinforcement learning systems often encounter environments with highly dynamic reward magnitudes. By integrating adaptive reward normalization, Hybrid GRPO can stabilize policy learning in scenarios with highly volatile reward landscapes (Popov et al., 2017; Horgan et al., 2018). A potential modification to the transformed reward function would involve batch-wise normalization:\n$R_t = \\frac{R_t - \\mu_R}{\\sigma_R + \\epsilon}$ (17)\nwhere \u00b5R and \u03c3R are computed over a rolling window of rewards. This adaptation would ensure consistent gradient updates across diverse reward distributions. It is important to note that the current release of (GRPO GitHub, 2025) Hybrid GRPO accepts a custom reward activation function."}, {"title": "3.3.4 Incorporating a Learned Value Model for Sampling Guidance", "content": "One of the most promising extensions of Hybrid GRPO involves incorporating a learned value model to guide action sampling. Instead of selecting actions purely based on policy probabilities, an additional state-action evaluation model can be trained to prioritize sampling high-value actions (Silver et al., 2016; Schaul et al., 2015). The modified sampling process would involve:\n$a_t^{(t)} \\sim arg max_a [Q_\\phi(s_T, a) + \\beta log \\pi_\\theta(a|s_T)]$ (18)\nwhere $Q_\\phi(s_T, a)$ is a learned value function guiding sampling, and \u03b2 is a weighting coefficient balancing the influence of the learned value function with the policy probability distribution."}, {"title": "4 Final Remarks", "content": "Hybrid GRPO represents a novel enhancement to policy optimization methods in reinforcement learning by integrating empirical sampling with bootstrapped value estimation. The experimental findings demonstrate its efficacy in improving sample efficiency, policy stability, and convergence speed over both PPO and DeepSeek GRPO. Originally, Group Relative Policy Optimization (GRPO) was introduced as a framework to improve large language models (LLMs) by optimizing token prediction efficiency while removing the reliance on a value function (DeepSeek, 2025). This design was particularly effective for autoregressive transformers, where the primary goal is to refine token-level probability distributions without introducing bootstrapping bias. However, the structure of Hybrid GRPO presents an opportunity to bridge the gap between LLMs and agent-based systems that operate in dynamic, real-world environments.\nWhile traditional GRPO has been focused on maximizing linguistic coherence and token-level performance, Hybrid GRPO extends these principles to systems that require continuous decision-making in response to external stimuli. In particular, Hybrid GRPO could be used to optimize real-time systems that depend on sequential decision-making beyond text generation, such as autonomous driving systems like Tesla's Full Self-Driving (FSD) (Tesla, 2023) and real-world robotic applications including autonomous drones developed by companies such as Skydio and DJI (Skydio, 2020). These systems require reinforcement learning methods that efficiently process sensory data, react to environmental uncertainty, and continuously improve control policies based on real-world interactions.\nOne of the fundamental challenges in applying GRPO to real-world agent-based tasks is the inherent difference in how value is estimated. LLMs benefit from token-based auto-regressive modeling, where future rewards can be inferred directly from linguistic structures and contextual embeddings. In contrast, autonomous systems operate in continuous state-action spaces where long-term dependencies, safety constraints, and physical feasibility must be considered. Hybrid GRPO addresses this challenge by retaining a value function to guide decision-making while incorporating empirical sampling to refine advantage estimates, allowing it to adaptively learn optimal policies in complex environments.\nAdditionally, Hybrid GRPO could provide a more scalable learning framework for training AI models across multiple domains, enabling seamless integration of reinforcement learning into both structured (LLMs) and unstructured (physical environments) settings. This shift has implications for real-time systems, such as industrial robotics, financial trading models, and AI-driven healthcare diagnostics, where learning efficiency and stability are critical. By introducing structured reward transformations, adaptive reward scaling, and multi-step sub-sampling, Hybrid GRPO enhances policy learning in a way that is not limited to the discrete nature of token-based models but can also be applied to continuous control problems in real-time environments.\nAs reinforcement learning continues to expand beyond traditional applications, Hybrid GRPO stands as a compelling framework for unifying policy learning across diverse AI systems. By integrating value-based optimization with empirical sampling, Hybrid GRPO may serve as a foundation for developing more robust reinforcement learning models that seamlessly transition between language-based AI and real-world autonomous agents."}]}