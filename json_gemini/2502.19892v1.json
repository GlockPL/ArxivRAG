{"title": "ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments", "authors": ["Jinghao Xin", "Zhichao Liang", "Zihuan Zhang", "Peng Wang", "Ning Li"], "abstract": "Deep Reinforcement Learning (DRL) has demonstrated potential in addressing robotic local planning problems, yet its efficacy remains constrained in highly unstructured and dynamic environments. To address these challenges, this study proposes the ColorDynamic framework. First, an end-to-end DRL formulation is established, which maps raw sensor data directly to control commands, thereby ensuring compatibility with unstructured environments. Under this formulation, a novel network, Transqer, is introduced. The Transqer enables online DRL learning from temporal transitions, substantially enhancing decision-making in dynamic scenarios. To facilitate scalable training of Transqer with diverse data, an efficient simulation platform E-Sparrow, along with a data augmentation technique leveraging symmetric invariance, are developed. Comparative evaluations against state-of-the-art methods, alongside assessments of generalizability, scalability, and real-time performance, were conducted to validate the effectiveness of ColorDynamic. Results indicate that our approach achieves a success rate exceeding 90% while exhibiting real-time capacity (1.2-1.3 ms per planning). Additionally, ablation studies were performed to corroborate the contributions of individual components. Building on this, the OkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and real-world experiments demonstrating its superiority and applicability in complex scenarios. The codebase and experimental demonstrations have been open-sourced on our website\u00b9 to facilitate reproducibility and further research.", "sections": [{"title": "I. INTRODUCTION", "content": "LOCAL planner portrays an instrumental role in robotic navigation systems. Functioning within a hierarchical framework, the local planner controls the robot to follow the path generated by a high-level global planner while dynamically bypassing newly encountered obstacles, enabling robots to execute unmanned tasks such as autonomous driving, environmental exploration, and rescue missions autonomously. Classical local planning algorithms, exemplified by the Artificial Potential Field (APF) [1] and the Dynamic Window Approach (DWA) [2], predominantly rely on manually designed rules or parameters, which requires expertise or metic-"}, {"title": "II. RELATED WORKS", "content": "This section provides a comprehensive review of pertinent studies on the critical components of DRL-based local planners, encompassing temporal perception models, architectures, and simulation platforms. Each subsection concludes with an analysis of their limitations."}, {"title": "A. Temporal Perception in DRL", "content": "Temporal perception is critical for DRL agents, enabling the comprehension, prediction, and decision-making required to operate in dynamic environments. Existing methodologies for enhancing temporal perception in DRL can be categorized into three groups: (1) Recurrent Neural Network (RNN)-based, (2) CNN-based, and (3) Transformer-based. RNN-based methods (e.g., LSTM [17]\u2013[20] and GRU [21]\u2013[23]) integrate recurrent architectures into policy or value networks to retain historical information. However, RNNs' dependence on initial hidden states restricts their applicability within on-policy DRL algorithms (e.g., A3C [24], PPO [25]), which inherently suffer from low sample efficiency. CNN-based approaches [6], [10]\u2013[12] address this limitation by reformatting raw sensor data into 2D images, stacking temporal sequences along the depth channel to enable temporal perception. This strategy facilitates the adoption of experience replay [3], ensuring the compatibility with off-policy DRL algorithms thus improving sample efficiency. However, CNNs' efficacy in capturing long-term temporal correlations remains debatable as it is originally designed for spatial feature extraction. To address this issue, Transformer-based methods, such as DTQN [26], conceptualize sequential timesteps as input tokens, leveraging self-attention mechanisms for long-term temporal modeling. However, Transformer-based models demand substantial training data [27]\u2013[29], which can be further exacerbated in online DRL settings, where agents must interact extensively with environments to learn useful skills. This constraint has forced researchers to resort to offline DRL (e.g., Decision Transformer [30], Trajectory Transformer [31], and Q-Transformer [32]) as a compromised alternative, which trains agents on pre-collected datasets. Nevertheless, offline methods face challenges in optimality and generalization due to the absence of online exploration. Meanwhile, the preparation of an offline dataset can be labor-intensive. In summary, RNN-based methods confront sample efficiency limitations, CNN-based approaches struggle with long-term temporal modeling, and Transformer-based architectures face challenges in effective learning. Further research is necessary to develop temporally robust DRL frameworks that balance sample efficiency, modeling capacity, and training feasibility."}, {"title": "B. Architectures of DRL-based local planner", "content": "DRL-based local planners are broadly classified into (1) pipeline-based and (2) end-to-end architectures. Pipeline-based approaches [6], [8], [9] decompose planning into modular components (e.g., object detection, localization, prediction, fusion, planning, and control), offering interpretability and scenario-specific customization. However, the sequential design compromises robustness and real-time performance, as discussed in Section I. In contrast, end-to-end methods [5], [10], [11], [33] map the raw sensor data directly to low-level control commands, making them more integrated, thus overcoming the flaws of pipeline-based approaches. Whereas, the design and training of end-to-end architectures are non-trivial, as extracting high-level features directly from noisy sensor data is challenging. Consequently, existing approaches typically rely on easier-to-train models (e.g., RNN, CNN), leaving a research gap between end-to-end architectures and advanced temporal models (e.g., Transformer). Hence, further works are necessitated to explore their effective integration and form a more powerful architecture for DRL-based local planners."}, {"title": "C. Simulation Platforms for DRL", "content": "Simulation platforms are indispensable for DRL, as they not only circumvent the prohibitive costs of real-world training but also accelerate the training process. Widely adopted platforms include Robotics Gym [34], PyBullet [35], Webots [36], Gazebo [37], and Sparrow [5]. Developed by OpenAI, Robotics Gym is a simulation platform based on the MuJoCo physics engine, providing a suite of continuous control tasks and serving as critical tools for robotic control research. PyBullet, a high-fidelity physics simulation library based on the Bullet physics engine, is commonly employed in DRL scenarios requiring precise physical interactions, such as robotic manipulation, locomotion, and collision detection. However, despite their advanced capabilities, these platforms are primarily designed for articulated robots and thus cannot adequately support the training of local planners for wheeled robots. Webots and Gazebo, developed by Cyberbotics and Open Robotics respectively, are two simulation platforms well-suited for wheeled robots. Both support diverse robotic platforms and sensors while providing extensive tools for modeling, simulating, and analyzing robotic behaviors across various environments. This enables researchers to explore different configurations and scenarios in simulation. Nevertheless, due to their high computational demands and lack of parallel simulation capabilities, these platforms exhibit limitations in data throughput and diversity. To address these challenges, [5] proposed the Sparrow simulator. Featuring vectorized diversity, Sparrow enables simultaneous simulations of diversified setups (e.g., environmental layouts, sensor noise, and robot kinematics) across multiple vectorized environments. This functionality enhances both data diversity and throughput, thereby ameliorating the DRL training significantly. However, Sparrow currently supports only static environment simulations, restricting its applicability in more complex and realistic scenarios involving dynamic obstacles. Furthermore, its diverse training maps require manual preparation, substantially increasing labor costs and diminishing the automaticity of DRL training. Therefore, further research is necessary to develop a more comprehensive and advanced platform apposite for DRL-based local planning."}, {"title": "III. METHODOLOGY", "content": "This section introduces the ColorDynamic. We commence with the end-to-end problem formulation, followed by the E-Sparrow simulator, the Transqer networks, and the SI data augmentation technique."}, {"title": "A. End-to-end Formulation for LPP", "content": "In this section, we introduce our end-to-end formulation for the LPP, which maps raw temporal sensor data directly to the low-level control command within the DRL diagram.\n1) Problem Formulation: Due to perceptual limitations (e.g., constrained lidar scanning range), the agent cannot fully observe the environment. Consequently, the LPP is formulated as a Partially Observable Markov Decision Process (POMDP) to facilitate the deployment of DRL. Let $s_t$ denote the true state of the environment at timestep $t$. The agent then receives a partial observation $o_t$ of $s_t$ through its sensors and takes an action $a_t$ based on its policy $\\pi(o_t; \\theta)$, where $\\theta$ represents the policy's parameters. Subsequently, the environment provides a reward $r_t$ and transitions to the next state $s_{t+1}$. The objective of the agent is to maximize the expected sum of discounted rewards $\\mathbb{E} [\\sum_{t=0}^{+\\infty} \\gamma^t r_t]$, where $\\gamma \\in [0,1]$ is the discount factor."}, {"title": "2) Observation", "content": "The primary objective of the agent within the LPP is to control the robot to reach its target point as quickly as possible while avoiding collisions with obstacles along the route, necessitating effective perception of the surrounding environment. Although sensors such as RGB and depth cameras provide abundant feature-rich data, their employment in the LPP, however, encounters substantial challenges: (i) Limited generalization capability: feature-rich sensors are prone to overfitting to environmental specifics [38] (e.g., colors, textures, and shapes), leading to degraded performance in unseen or dynamic environments. (ii) Low computational efficiency: the use of feature-rich sensors requires intricate preprocessing pipelines [6], which not only demand additional human effort but also impose significant computational costs, posing a critical challenge for real-time decision-making.\nOur prior research [5] demonstrated that single-stream lidar is highly effective for the LPP of mobile robots, as it provides essential collision-avoidance information, such as obstacle distances, while excluding extraneous data that may lead to overfitting. Additionally, the inherent simplicity of single-stream lidar data facilitates direct integration into neural networks, enabling end-to-end feature extraction and decision-making. This eliminates the need for hand-crafted preprocessing, consequently enhancing real-time performance. Moreover, this choice reduces redundant sensors, contributing to the simplicity, affordability, and energy efficiency of the robot system. Consequently, this study employs single-stream lidar as the perception sensor, with the agent's observation defined accordingly:\n$o_t = [K(t), L(t), L(t - 1), ..., L(t - T + 1)]$\nwhere $T$ is the temporal perception length, $L(t)$ is the lidar scanning result at timestep $t$:\n$L(t) = [ld_1, ld_2, ld_3,\u2026\u2026,ld_n]$\nwhere $n$ is the number of lidar beams. $K(t)$ in Eq. (1) denotes the kinematic information of the mobile robot at timestep $t$:\n$K(t) = [V^{ct}_l, V^{ct}_a,V^{rt}_l, V^{rt}_a, D^{2T}, \\alpha, V_l, V_a]$\nwhere $V^{ct}_l$ and $V^{ct}_a$ are the current target linear and angular velocity generated by the agent, respectively; $V^{rt}_l$ and $V^{rt}_a$ are the real target linear and angular velocity received by the robot, respectively; $D^{2T}$ is the distance from the robot to the target; $\\alpha$ is the robot's relative orientation to the target; $V_l$ and $V_a$ are the real linear and angular velocity of the robot, respectively.\nWe distinguish between $[V^{ct}_l, V^{ct}_a]$ and $[V^{rt}_l, V^{rt}_a]$ because, in robotic systems, the target velocity generated by the agent and the target velocity received by the robot at a specific timestep are often disparate due to transmission delays. Similarly, we differentiate between $[V^{rt}_l, V^{rt}_a]$ and $[V_l, V_a]$ as the robot's actual operational velocity cannot change abruptly due to physical constraints. Consequently, the target velocity received by the robot and its actual operational velocity at a given timestep are also distinct. We contend that such config-"}, {"title": "3) Action", "content": "The agent utilizes seven discrete actions to generate the target velocity $[V^{ct}_l, V^{ct}_a]$ for the robot:\nTurn Left: [0.1 m/s, 2 rad/s]\nTurn Left and Move Forward: [0.5 m/s, 2 rad/s]\nMove Forward: [0.5 m/s, 0 rad/s]\nTurn Right and Move Forward: [0.5 m/s, -2 rad/s]\nTurn Right: [0.1 m/s, -2 rad/s]\nMove Backward: [-0.5 m/s, 0 rad/s]\nSlow Down: [0.05 m/s, 0 rad/s]\nWe choose the discrete action space as we find it shows better stability and convergence properties in the LPP than the continuous action space. It is crucial to emphasize that the discrete action setup does not compromise the robot's movement continuity, as it only controls its target velocity. The actual control signal for the robot's actuator is generated by a low-level velocity tracker, namely the Proportional-Integral-Derivative [39], [40] controller, in a continuous manner."}, {"title": "4) Reward", "content": "The reward function for the LPP is given by:\nr = 200, if the robot reaches the target point.\nr = -200, if the robot collides with obstacles or exceeds the maximal local planning distance $d$.\nr=r_o+r_a + 0.5r_p + 0.5r_d \u2013 0.5, otherwise.\nThe episode will be terminated when reward cases and \u2461 occur. Here, $r_o$ and $r_a$ are orientation-based and action-based rewards, respectively. These two reward terms encourage the robot to move rapidly toward the target point. An illustration of $r_o$ is provided in Fig. 1, where\n$r_o = 1 - \\frac{min(0.25, |\\alpha|)}{0.25}$ (4)\n$r_a$ = 1 if the action generated by the agent is Move Forward, otherwise $r_a = 0$. $r_p$ is a penalty-based reward that discourages backward movement of the robot. $r_p = -1$ if the action is Move Backward, otherwise $r_p = 0$. $r_d$ denotes the distance-based reward, designed to incentivize robot locomotion towards the target point, and is defined as:\n$r_d = \\frac{D^{2T}(t - 1) \u2013 D^{2T}(t)}{V_{max}}$ (5)\nwhere $V_{max}$ is the maximal linear velocity of the robot. Note that the reward function is manually engineered, and its coefficients have been tuned in our pre-experiment."}, {"title": "B. Simulation Platform", "content": "Simulation environments have been instrumental in accelerating DRL research for robotics, allowing for safe and efficient training of complex behaviors without the need for costly and time-consuming real-world experiments. Gazebo [37], the official simulation platform of the Robot Operating System (ROS) [41], has been widely adopted for training DRL-based mobile robot systems [6], [42]\u2013[45] due to its high-fidelity simulation of the physical environment. However, the computational demands and the lack of parallel simulation support result in Gazebo's inefficiencies in both computational resource utilization and training time. For instance, [6] utilizes eight Tesla V100 GPUs to train a DRL-based local planner using Gazebo, while [45] reports 36 hours of Gazebo simulation to train a DRL-based navigator. This inefficiency substantially impedes the development of DRL-based mobile robot systems.\nTo address this challenge, our previous work [5] introduced a lightweight and efficient simulator, Sparrow\u00b2, specifically designed for mobile robot systems. Sparrow is characterized by its vectorized diversity, enabling simultaneous simulations of diversified setups (e.g., environmental layouts, sensor noise, and robot kinematics) across multiple vectorized environments. This capability enhances both the data diversity and throughput. With a desktop-class computer, Sparrow can yield a local planner within one hour of simulation training [5]. Furthermore, the simulation-trained local planner can be applied directly to real-world scenarios without further adaptation, owing to the strong generalization capabilities facilitated by vectorized diversity.\nDespite these advancements, the Sparrow simulator presented three notable limitations. First, it supported only static obstacle simulations. Second, it was restricted to single-agent simulation. Third, the creation of diverse training maps required manual effort, resulting in substantial human intervention. To address these limitations, this research introduces an enhanced version of the Sparrow simulator, termed E-Sparrow. As illustrated in Fig. 2, E-Sparrow now incorporates dynamic obstacle and multi-agent simulation capabilities. More importantly, it features Procedurally Generated Vectorized Diversity (PGVD) for training maps, automating the creation of diverse training environments. These enhancements provide E-Sparrow with a more comprehensive and powerful simulation ability, enabling the autonomous generation of unlimited diverse training samples with high data throughput. This will significantly contribute to the generalizability and scalability of DRL-based mobile robot systems. As these contributions are primarily development and implementation-focused, interested readers are encouraged to consult our open-sourced codebase\u00b9 for further details."}, {"title": "C. Transqer", "content": "This section details our proposed model, which facilitates end-to-end mapping from raw temporal perception to low-level actions. The model architecture, illustrated in Fig. 3, comprises three primary components: a Temporal Window Queue (TWQ), a Transformer encoder [16], and a Multi-Layer Perceptron (MLP) network. The TWQ, a first-in-first-out queue of length $T$, stores the temporal lidar data. The Transformer encoder, configured with $N_E$ layers, encodes this temporal information. Subsequently, the MLP network, possessing $N_p$ layers, predicts the Q-values, thereby enabling the derivation of corresponding actions. To ensure temporal consistency, the TWQ is padded with zeros following each environmental termination.\nThe proposed model is named as Transqer, signifying its function of transforming temporal data into Q-values utilizing a Transformer encoder (Trans-Q-er). During each environmental interaction, the Transqer operates as follows:\nAcquire the most recent lidar data $L(t)$ from E-Sparrow and append it to the TWQ.\nIntegrate temporal order into the TWQ using positional encoding [16].\nEncode the temporally-encoded lidar data using the Transformer encoder.\nAggregate the encoder output via average pooling across the temporal channel, resulting in the aggregated lidar data, $E(t)$.\nConcatenate $E(t)$ with the current lidar data $L(t)$ and robot kinematic information $K(t)$.\nMap the concatenated vector to Q-values using the MLP network.\nSelect actions corresponding to the maximum Q-values and execute them within the vectorized E-Sparrow environment.\nLeveraging the vectorized simulation capabilities of E-Sparrow and the parallel processing capacity of neural net-"}, {"title": "D. Symmetric Invariance", "content": "Prior research [27]\u2013[29] has indicated that Transformer models are demanding regarding training samples, both in terms of data volume and diversity. This demand can be further exacerbated within the context of online DRL due to the non-stationary nature of the training process, wherein the target Q-values associated with a given observation vary during training [3]. To accommodate this issue, we introduce a data expansion technique predicated on Symmetric Invariance (SI), as illustrated in Fig. 4. Given a raw transition $(o_t, a_t, r_t, o_{t+1})$ generated by the simulation environments, the SI-expanded transitions is given by $(o^{SI}_t, a^{SI}_t, r_t, done, o^{SI}_{t+1})$, where\n$o^{SI}_t = [K^{SI}(t), L^{SI}(t), L^{SI}(t - 1), ..., L^{SI}(t - T + 1)]$ (6)\n$K^{SI}(t) = [V^{ct}_l, -V^{ct}_l, V^{rt}_l, \u2013V^{rt}_l, D^{2T}, -\\alpha, V_l, -V_a]$ (7)\n$L^{SI}(t) = [ld_n,\u2026\u2026,ld_3, ld_2, ld_1]$ (8)\n$a^{SI}_t = [V^{ct}_l, -V^{ct}_l]$ (9)\nIn contrast to existing symmetry-based data expansion techniques [46]\u2013[48] that primarily focus on image-type perception data for supervised learning, our work extends this concept to DRL-based robotic systems and factor not only perception data but also the kinematic state of the robot."}, {"title": "E. ColorDynamic", "content": "Integrating the end-to-end problem formulation, the E-Sparrow simulator, the Transqer network, and the SI data expansion technique, we formally present our DRL solution to the LPP, as delineated in Algorithm 1. We name our approach as ColorDynamic, where \u201cColor\u201d originates from our previous work [5] and \u201cDynamic\u201d highlights the extension to dynamic environments.\nColorDynamic comprises two distinct phases: interaction and training. During the interaction phase, the Transqer interacts with the E-Sparrow simulation platform, aided by the TWQ, and the resultant data is stored in a replay buffer for subsequent training. In the training phase, a batch of data is sampled from the replay buffer and subsequently extended via SI according to Eq. (6)~(9). Following this, the raw and extended data are concatenated for DRL training. For the training algorithm, we employ DQN [3], an off-policy DRL algorithm that has demonstrated human-level decision-making capabilities in discrete action spaces. Additionally, the Double Q-learning [49] technique is incorporated to mitigate the notorious overestimation issue inherent in DQN. Moreover, the Actor-Sharer-Learner training framework [5], featuring vectorized interaction and decoupled training, is harnessed to improve training efficiency.\nColorDynamic exhibits several advantageous features: (a) Generalizability: The diversity provided by the E-Sparrow simulator, encompassing varied environmental lay-"}, {"title": "F. OPCD Navigation System", "content": "This section introduces the OkayPlan-ColorDynamic (OPCD) navigation system, based on the proposed ColorDynamic and our previous work, OkayPlan [50]. In this system, OkayPlan functions as a global planner, while ColorDynamic serves as a local planner.\n1) A Review of OkayPlan: The diagram of the OkayPlan algorithm is illustrated in Fig. 5, wherein the global path planning problem is formulated as an optimization problem, as follows:\n$F(Z) = G(Z) + \\delta \\cdot Q(Z)^\\sigma + \\mu \\cdot P(Z)^\\nu$ (10)\nHere, $F(Z)$ represents the fitness function, with $Z$ being the decision variables representing waypoint coordinates. $G(Z)$ represents the length of path, $Q(Z)$ represents the number of intersections with obstacles, $P(Z)$ represents the number of intersections with obstacle kinematics (velocity vectors), and $\\delta, \\sigma,\\mu,\\nu$ are four coefficients. The optimization problem, denoted by Eq. (10), is resolved by an efficient heuristic optimizer, SEPSO [51]. As exemplified by Fig. 5, this framework enables OkayPlan to identify a safer path (Path 3) in dynamic environments, as opposed to the shortest but risky path (Path 1). Furthermore, the calculations of $G(Z)$, $Q(Z)$, and $P(Z)$ can be efficiently performed through matrix operations involving the waypoints, obstacle vertices, and kinematic segment endpoints, which facilitates deployment on GPUs to achieve real-time performance. It has been reported that OkayPlan is capable of generating safe yet short paths in dynamic environments at a real-time execution speed of 125 Hz.\n2) Obstacle Definition: Based on the characteristics of obstacles, they are categorized into four Obstacle Layers (OL), as illustrated in Fig. 6. The distinction between \"known\" and \"unknown\" properties is determined by the extent to which the navigation system has prior knowledge of the obstacles. For instance, static buildings (OL1) can be represented on a"}, {"title": "IV. LOCAL PLANNING WITH COLORDYNAMIC", "content": "This section assesses the proposed ColorDynamic, focusing on the determination of hyperparameters, the evaluation of the trained local planner, the comparison with existing approaches, and the ablation of the proposed techniques. The software and hardware configurations supporting these experiments are summarized in Table V in the Appendix."}, {"title": "A. Experimental Setup", "content": "The proposed E-Sparrow simulator is employed to train and evaluate ColorDynamic in the context of local planning, with the experimental setups detailed as follows.\n1) Environment: The training map is configured with a size of 8 m, and the maximum local planning distance $d$ is set to 4 m. The number of static obstacles (OL3) is randomly sampled between 0 and 36. The number of dynamic obstacles (OL4) is set to 15, with a maximum velocity of 0.5 m/s, moving randomly within the map. Notably, the shape and location of obstacles are procedurally generated at the initialization of the environment. Episodes are truncated if the interaction steps exceed 500. The target is deemed reached when $D^{2T} < 0.3$ m.\n2) Robot: The robot has a diameter of 0.2 m, with maximum linear and angular velocities constrained to 0.5 m/s and 2 rad/s, respectively. Such configuration is derived from our real-world robot, as shown in Fig. 14(a). The control frequency is set to 10 Hz. The lidar scanning range covers 360\u00b0 \u00d7 3m, and consists of 24 lidar beams in total. The robot's initial pose is randomly generated at the commencement of each interaction episode. The robot is trained to repeatedly reach the target point. When reached, the target point will be randomly generated within $d$ around the robot.\n3) Hyperparameters: The maximum training steps are set to 3M, with a learning rate of 0.0001 and a batch size of 256. During training, the environment is reset every 32K steps to regenerate obstacles randomly, and the model is evaluated"}, {"title": "B. Model Complexity", "content": "Determining an appropriate model complexity is crucial for the efficacy of ColorDynamic. The model incorporates six critical hyperparameters to define its complexity:\nT: temporal perception length.\nW: width of linear layers for both the Transformer encoder and MLP.\nNE: number of Transformer encoder layers.\nNp: number of MLP layers.\nH: number of heads in the multi-head attention mechanism [16] within the Transformer encoder.\nD: feature dimension of the Transformer encoder.\nA comprehensive grid search across these hyperparameters is computationally prohibitive. Therefore, certain values are adopted empirically from existing studies, while the remaining are determined through a coarse search. Specifically, Np is set to 3, referencing Color [5]; H is set to 8, referencing Transformer [16]; and D is simplified to match the number of lidar beams, which is 24 in this study. Regarding the W and NE, a grid search was conducted over the parameter space [64, 128, 256, 512] \u00d7 [1, 2, 3], as illustrated in Fig. 8. Note that these curves are smoothed with an exponential factor of 0.95 for better readability. Meanwhile, these experiments were conducted across three random seeds, with the solid curves representing the mean values and the translucent shaded areas denoting the standard deviation. Same data processing procedures were applied to Fig. 9 and Fig. 11 as well. Subsequently, the temporal window length T was determined through a search over [5, 10, 20], which corresponds to temporal perception durations of [0.5, 1, 2]s, considering the control frequency is 10 Hz. The results of this search are shown in Fig. 8(c). The results reveal that the configuration [W = 64, NE = 3,T = 10] delivers the best overall performance in terms of both final reward and training stability. This configuration is therefore adopted as the default for ColorDynamic and for subsequent experiments."}, {"title": "C. Transqer v.s. DTQN", "content": "ColorDynamic harnesses Transqer as its core model. The work most similar to Transqer is DTQN [26]. Therefore, this section presents a comparison of Transqer and DTQN within the ColorDynamic framework in the context of LPP. The experiment adheres to the configurations outlined in Sections IV-A and IV-B, with results shown in Fig. 9.\nThe results indicate a considerable enhancement in Tran-sqer's performance over DTQN, both in terms of final performance and training stability. We contend such improvement is attributed to Transqer's segmentation of long temporal consequence into independent temporal windows, thereby mitigating the inefficiencies associated with direct learning from consecutive data, as discussed in Section III-C1."}, {"title": "D. Evaluation of ColorDynamic", "content": "This section assesses the local planner trained by ColorDynamic. To this end, we introduce three evaluation metrics:\nAverage speed: the mean velocity of the controlled robot.\nAverage planning time: the mean planning time required by the planner, specifically the duration of $a_t = \\pi(o_t;\\theta)$.\nSuccess rate: the success rate of the local planner in controlling the robot from a random start point to a random target point at a distance of 2 m. This distance is adopted from DRL-VO [6] for a fair comparison.\nThe selection of the optimal model trained by ColorDynamic is performed by evaluating the model every 6.4K environmental interactions during training. Subsequently, the best model is evaluated and compared under different configurations. To enhance reliability, the selection, evaluation, and comparison are performed over 100 random maps.\n1) Case Study: In this section, we present three case studies to demonstrate the high-level skills learned by ColorDynamic. As illustrated in Fig. 10, the model effectively overtakes slowly moving dynamic obstacles. In scenarios where a dynamic obstacle abruptly changes direction, creating a potential collision risk, the model adapts by altering its heading to bypass the obstacle. Additionally, when an obstacle moves directly toward the agent, the model strategically retreats to avoid being attacked. These advanced capabilities are attributed to the model's comprehensive understanding of environmental dynamics, which substantiates the effectiveness of the temporal perception capabilities of the Transqer networks.\n2) Comparison Study: To demonstrate the superiority of ColorDynamic, we compare its performance with two classical local planners, APF [1] and DWA [2], as well as two state-of-the-art learning-based methods, Color [5] and DRL-VO [6]. Note that the open-source models of Color\u00b3 and DRL-VO4 are employed for comparison without further modifications. The results, summarized in Table I, indicate that our model outperforms all other approaches in both average speed and success rate, highlighting its efficiency and safety. The superiorities stem from the E-Sparrow simulator and the Transqer networks. The PGVD feature of E-Sparrow exposes the agent to an unlimited variety of cases, while the Transqer"}, {"title": "E. Ablation Study", "content": "This section aims to examine the extent to which the proposed techniques contribute to ColorDynamic. To this end, we formulated the following ablation cases:\nNo Dynamic Obstacles: The velocities of the dynamic obstacles are set to zero throughout training.\nNo PGVD: The diversification (reset) frequency of obstacles is reduced from 32K steps to 320K steps. In this case, we maintain a low level of diversity rather than completely eliminating it to ensure effective training.\nNo SI: The SI data expansion technique is excluded from the training process.\nNo Transqer: The Transqer is substituted with a simple MLP network characterized by Ne layers and W width.\nNote that the evaluation setups of these ablation cases are identical to those introduced in Section IV-A. The ablation results illustrated in Fig. 11 demonstrate that data quality has the most substantial impact on performance. Specifically, the removal of dynamic obstacles leads to suboptimal policies. We have observed that these policies tend to exhibit risky behaviors, such as approaching dynamic obstacles too closely. On the other hand, the absence of PGVD results in significant fluctuations, primarily because the trained model overfits to the training environments and fails to generalize effectively to new scenarios when obstacles are regenerated. Fig. 11 also indicates that the Transqer network is the third most influential component in the performance of ColorDynamic. While a simple MLP network can handle dynamic environments effectively, given its high-speed replanning characteristic, it lacks temporal perception capabilities. This deficiency impairs its ability to learn high-level skills, as depicted in Fig. 10, ultimately degrading its performance. Lastly, the SI technique also demonstrates efficacy, yielding consistent improvement in the later training stage."}, {"title": "V. NAVIGATING WITH OPCD", "content": "This section aims to demonstrate the superiority and applicability of the proposed OPCD navigation system. To achieve"}, {"title": "A. Simulation Comparison of OPCD", "content": "1) Simulation Environment: We utilize the Gazebo simulator to conduct a comparative analysis of OPCD and existing approaches. As the official simulation platform for ROS, Gazebo is renowned for its high-fidelity simulations and has been extensively integrated with various navigation algorithms, thereby facilitating benchmark evaluations. As illustrated in Fig. 12, a complex 3D environment of 20m \u00d7 20m has been constructed, comprising static buildings (OL1, in purple), operational robots (OL2, in blue), temporarily placed obstacles (OL3, in orange), and pedestrians (OL4, in green). The robot under control, depicted in Fig. 12(b), is a two-wheeled differential drive robot, whose specifications are consistent with the robot used in the E-Sparrow, as outlined in Section IV-A2. Note that OL2 and OL4 are randomly wandering in the environment and will not actively avoid the agent, thus presenting additional challenges for the navigation systems under evaluation and necessitating enhanced safety measures. Furthermore, Rviz\u0151 is employed to visualize the information accessible to the agent, as shown in Fig. 12(d).\n2) Evaluation Metrics: As depicted in Fig. 12(d), the objective is to navigate the robot from the lower-right corner to the upper-left corner safely and efficiently. To measure the efficiency of the navigation system, two additional metrics were introduced:\nAverage distance: The mean travel distance from the starting point to the endpoint.\nAverage time: The mean travel time from the starting point to the endpoint.\nIt is important to note that these two metrics are only applied to successful cases, as failed cases, which may involve shorter travel distance and time, could distort the overall assessment of navigation efficiency.\n3) Experiment Results: The OPCD navigation system is compared with six benchmark navigation systems, as summarized in Table IV. The implementation of these benchmark systems is based on a widely recognized GitHub repository7. To ensure the reliability of the results, each system was evaluated 100 times, and the average results are presented in Table IV. Here, the Average planning time metric introduced in Section IV-D is excluded from the comparison, as the algorithms are implemented in different programming languages, rendering the comparison of computational time less meaningful.\nThe numerical results demonstrate that the proposed OPCD system exhibits superior safety in complex scenarios, achieving a 0.96 success rate. This favorable outcome can be attributed to three key features of the OPCD system. First,"}, {}]}