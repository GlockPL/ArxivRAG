{"title": "ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments", "authors": ["Jinghao Xin", "Zhichao Liang", "Zihuan Zhang", "Peng Wang", "Ning Li"], "abstract": "Deep Reinforcement Learning (DRL) has demonstrated potential in addressing robotic local planning problems, yet its efficacy remains constrained in highly unstructured and dynamic environments. To address these challenges, this study proposes the ColorDynamic framework. First, an end-to-end DRL formulation is established, which maps raw sensor data directly to control commands, thereby ensuring compatibility with unstructured environments. Under this formulation, a novel network, Transqer, is introduced. The Transqer enables online DRL learning from temporal transitions, substantially enhancing decision-making in dynamic scenarios. To facilitate scalable training of Transqer with diverse data, an efficient simulation platform E-Sparrow, along with a data augmentation technique leveraging symmetric invariance, are developed. Comparative evaluations against state-of-the-art methods, alongside assessments of generalizability, scalability, and real-time performance, were conducted to validate the effectiveness of ColorDynamic. Results indicate that our approach achieves a success rate exceeding 90% while exhibiting real-time capacity (1.2-1.3 ms per planning). Additionally, ablation studies were performed to corroborate the contributions of individual components. Building on this, the OkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and real-world experiments demonstrating its superiority and applicability in complex scenarios. The codebase and experimental demonstrations have been open-sourced on our website\u00b9 to facilitate reproducibility and further research.", "sections": [{"title": "I. INTRODUCTION", "content": "LOCAL planner portrays an instrumental role in robotic navigation systems. Functioning within a hierarchical framework, the local planner controls the robot to follow the path generated by a high-level global planner while dynamically bypassing newly encountered obstacles, enabling robots to execute unmanned tasks such as autonomous driving, environmental exploration, and rescue missions autonomously. Classical local planning algorithms, exemplified by the Artificial Potential Field (APF) [1] and the Dynamic Window Approach (DWA) [2], predominantly rely on manually designed rules or parameters, which requires expertise or meticulously engineering, thereby forming an impediment to their widespread applications.\nDeep Reinforcement Learning (DRL) has emerged as a promising solution to address these limitations. By enabling robots to proactively learn complex behaviors through interactions with environments, DRL has demonstrated remarkable efficacy in domains such as games [3], [4], robotics [5], [6], and large language models [7]. Despite these advancements, DRL-based local planners exhibit substantial limitations in highly unstructured and dynamic environments. This constraint arises from three interrelated shortcomings of current researches: (i) Pipeline-based Preprocessing. Contemporary DRL-based planners [6], [8], [9] generally rely on multi-stage preprocessing pipelines (e.g., object detection, tracking, and feature fusion) to distill useful environmental features. However, feature extraction from raw sensor data under unstructured environments can be non-trivial. Minor inaccuracies in early pipeline stages propagate cumulatively, often culminating in catastrophic model failures that compromise system robustness. Furthermore, the computational overhead of sequential preprocessing modules impedes real-time performance, rendering such architectures less practical for time-sensitive applications. (ii) Temporal-Spatial Modeling Dilemma. Existing works [6], [10]\u2013[12] typically employ Convolutional Neural Networks (CNNs) to process temporally stacked streaming sensor data. While CNNs demonstrate proficiency in spatial feature extraction, their capacity to model temporal dependencies, particularly long-range temporal correlations, remains questionable. Additionally, the localized receptive fields of convolutional operations constrain the model's ability to integrate global contextual information across spatial and temporal dimensions, leading to CNNs' suboptimal performance under dynamic environments. (iii) Inefficient Data Accessibility. DRL's trial-and-error learning paradigm necessitates extensive interaction with environments, posing prohibitive safety and economic risks for real-world training. A prevalent practice within the current research community involves training the planner in Gazebo [10]\u2013[15], a well-established platform for mobile robots simulation. While doable, the effectiveness of Gazebo for DRL training is considerably limited by three key factors: low data throughput, limited data diversity, and an absence of parallelized simulation capabilities. These limitations impede the acquisition of sufficiently diverse training data, thereby restricting the planner's ability to adapt to real-world conditions and diminishing its generalizability to previously unseen scenarios.\nTo address these challenges, a novel framework for DRL-based local planning systems is proposed, with the key contributions summarized as follows:"}, {"title": "II. RELATED WORKS", "content": "This section provides a comprehensive review of pertinent studies on the critical components of DRL-based local planners, encompassing temporal perception models, architectures, and simulation platforms. Each subsection concludes with an analysis of their limitations."}, {"title": "A. Temporal Perception in DRL", "content": "Temporal perception is critical for DRL agents, enabling the comprehension, prediction, and decision-making required to operate in dynamic environments. Existing methodologies for enhancing temporal perception in DRL can be categorized into three groups: (1) Recurrent Neural Network (RNN)-based, (2) CNN-based, and (3) Transformer-based. RNN-based methods (e.g., LSTM [17]\u2013[20] and GRU [21]\u2013[23]) integrate recurrent architectures into policy or value networks to retain historical information. However, RNNs' dependence on initial hidden states restricts their applicability within on-policy DRL algorithms (e.g., A3C [24], PPO [25]), which inherently suffer from low sample efficiency. CNN-based approaches [6], [10]\u2013[12] address this limitation by reformatting raw sensor data into 2D images, stacking temporal sequences along the depth channel to enable temporal perception. This strategy facilitates the adoption of experience replay [3], ensuring the compatibility with off-policy DRL algorithms thus improving sample efficiency. However, CNNs' efficacy in capturing long-term temporal correlations remains debatable as it is originally designed for spatial feature extraction. To address this issue, Transformer-based methods, such as DTQN [26], conceptualize sequential timesteps as input tokens, leveraging self-attention mechanisms for long-term temporal modeling. However, Transformer-based models demand substantial training data [27]\u2013[29], which can be further exacerbated in online DRL settings, where agents must interact extensively with environments to learn useful skills. This constraint has forced researchers to resort to offline DRL (e.g., Decision Transformer [30], Trajectory Transformer [31], and Q-Transformer [32]) as a compromised alternative, which trains agents on pre-collected datasets. Nevertheless, offline methods face challenges in optimality and generalization due to the absence of online exploration. Meanwhile, the preparation of an offline dataset can be labor-intensive.\nIn summary, RNN-based methods confront sample efficiency limitations, CNN-based approaches struggle with long-term temporal modeling, and Transformer-based architectures face challenges in effective learning. Further research is necessary to develop temporally robust DRL frameworks that balance sample efficiency, modeling capacity, and training feasibility."}, {"title": "B. Architectures of DRL-based local planner", "content": "DRL-based local planners are broadly classified into (1) pipeline-based and (2) end-to-end architectures. Pipeline-based approaches [6], [8], [9] decompose planning into modular components (e.g., object detection, localization, prediction, fusion, planning, and control), offering interpretability and scenario-specific customization. However, the sequential design compromises robustness and real-time performance, as discussed in Section I. In contrast, end-to-end methods [5], [10], [11], [33] map the raw sensor data directly to low-level control commands, making them more integrated, thus overcoming the flaws of pipeline-based approaches. Whereas, the design and training of end-to-end architectures are non-trivial, as extracting high-level features directly from noisy sensor data is challenging. Consequently, existing approaches typically rely on easier-to-train models (e.g., RNN, CNN), leaving a research gap between end-to-end architectures and advanced temporal models (e.g., Transformer). Hence, further works are necessitated to explore their effective integration and form a more powerful architecture for DRL-based local planners."}, {"title": "C. Simulation Platforms for DRL", "content": "Simulation platforms are indispensable for DRL, as they not only circumvent the prohibitive costs of real-world training but also accelerate the training process. Widely adopted platforms include Robotics Gym [34], PyBullet [35], Webots [36], Gazebo [37], and Sparrow [5]. Developed by OpenAI, Robotics Gym is a simulation platform based on the MuJoCo physics engine, providing a suite of continuous control tasks and serving as critical tools for robotic control research. PyBullet, a high-fidelity physics simulation library based on the Bullet physics engine, is commonly employed in DRL scenarios requiring precise physical interactions, such as robotic manipulation, locomotion, and collision detection. However, despite their advanced capabilities, these platforms are primarily designed for articulated robots and thus cannot adequately support the training of local planners for wheeled robots. Webots and Gazebo, developed by Cyberbotics and Open Robotics respectively, are two simulation platforms well-suited for wheeled robots. Both support diverse robotic platforms and sensors while providing extensive tools for modeling, simulating, and analyzing robotic behaviors across various environments. This enables researchers to explore different configurations and scenarios in simulation. Nevertheless, due to their high computational demands and lack of parallel simulation capabilities, these platforms exhibit limitations in data throughput and diversity. To address these challenges, [5] proposed the Sparrow simulator. Featuring vectorized diversity, Sparrow enables simultaneous simulations of diversified setups (e.g., environmental layouts, sensor noise, and robot kinematics) across multiple vectorized environments. This functionality enhances both data diversity and throughput, thereby ameliorating the DRL training significantly. However, Sparrow currently supports only static environment simulations, restricting its applicability in more complex and realistic scenarios involving dynamic obstacles. Furthermore, its diverse training maps require manual preparation, substantially increasing labor costs and diminishing the automaticity of DRL training. Therefore, further research is necessary to develop a more comprehensive and advanced platform apposite for DRL-based local planning."}, {"title": "III. METHODOLOGY", "content": "This section introduces the ColorDynamic. We commence with the end-to-end problem formulation, followed by the E-Sparrow simulator, the Transqer networks, and the SI data augmentation technique."}, {"title": "A. End-to-end Formulation for LPP", "content": "In this section, we introduce our end-to-end formulation for the LPP, which maps raw temporal sensor data directly to the low-level control command within the DRL diagram.\n1) Problem Formulation: Due to perceptual limitations (e.g., constrained lidar scanning range), the agent cannot fully observe the environment. Consequently, the LPP is formulated as a Partially Observable Markov Decision Process (POMDP) to facilitate the deployment of DRL. Let \\( s_t \\) denote the true state of the environment at timestep \\( t \\). The agent then receives a partial observation \\( o_t \\) of \\( s_t \\) through its sensors and takes an action \\( a_t \\) based on its policy \\( \\pi(o_t; \\theta) \\), where \\( \\theta \\) represents the policy's parameters. Subsequently, the environment provides a reward \\( r_t \\) and transitions to the next state \\( s_{t+1} \\). The objective of the agent is to maximize the expected sum of discounted rewards \\( E[\\sum_{t=0}^{\\infty} \\gamma^t r_t] \\), where \\( \\gamma \\in [0, 1] \\) is the discount factor."}, {"title": "B. Simulation Platform", "content": "Simulation environments have been instrumental in accelerating DRL research for robotics, allowing for safe and efficient training of complex behaviors without the need for costly and time-consuming real-world experiments. Gazebo [37], the official simulation platform of the Robot Operating System (ROS) [41], has been widely adopted for training DRL-based mobile robot systems [6], [42]\u2013[45] due to its high-fidelity simulation of the physical environment. However, the computational demands and the lack of parallel simulation support result in Gazebo's inefficiencies in both computational resource utilization and training time. For instance, [6] utilizes eight Tesla V100 GPUs to train a DRL-based local planner using Gazebo, while [45] reports 36 hours of Gazebo simulation to train a DRL-based navigator. This inefficiency substantially impedes the development of DRL-based mobile robot systems.\nTo address this challenge, our previous work [5] introduced a lightweight and efficient simulator, Sparrow, specifically designed for mobile robot systems. Sparrow is characterized by its vectorized diversity, enabling simultaneous simulations of diversified setups (e.g., environmental layouts, sensor noise, and robot kinematics) across multiple vectorized environments. This capability enhances both the data diversity and throughput. With a desktop-class computer, Sparrow can yield a local planner within one hour of simulation training [5]. Furthermore, the simulation-trained local planner can be applied directly to real-world scenarios without further adaptation, owing to the strong generalization capabilities facilitated by vectorized diversity.\nDespite these advancements, the Sparrow simulator presented three notable limitations. First, it supported only static obstacle simulations. Second, it was restricted to single-agent simulation. Third, the creation of diverse training maps required manual effort, resulting in substantial human intervention. To address these limitations, this research introduces an enhanced version of the Sparrow simulator, termed E-Sparrow. As illustrated in Fig. 2, E-Sparrow now incorporates dynamic obstacle and multi-agent simulation capabilities. More importantly, it features Procedurally Generated Vectorized Diversity (PGVD) for training maps, automating the creation of diverse training environments. These enhancements provide E-Sparrow with a more comprehensive and powerful simulation ability, enabling the autonomous generation of unlimited diverse training samples with high data throughput. This will significantly contribute to the generalizability and scalability of DRL-based mobile robot systems. As these contributions are primarily development and implementation-focused, interested readers are encouraged to consult our open-sourced codebase\u00b9 for further details."}, {"title": "C. Transqer", "content": "This section details our proposed model, which facilitates end-to-end mapping from raw temporal perception to low-level actions. The model architecture, illustrated in Fig. 3, comprises three primary components: a Temporal Window Queue (TWQ), a Transformer encoder [16], and a Multi-Layer Perceptron (MLP) network. The TWQ, a first-in-first-out queue of length \\( T \\), stores the temporal lidar data. The Transformer encoder, configured with \\( N_E \\) layers, encodes this temporal information. Subsequently, the MLP network, possessing \\( N_p \\) layers, predicts the Q-values, thereby enabling the derivation of corresponding actions. To ensure temporal consistency, the TWQ is padded with zeros following each environmental termination.\nThe proposed model is named as Transqer, signifying its function of transforming temporal data into Q-values utilizing a Transformer encoder (Trans-Q-er). During each environmental interaction, the Transqer operates as follows:\n1) Acquire the most recent lidar data \\( L(t) \\) from E-Sparrow and append it to the TWQ.\n2) Integrate temporal order into the TWQ using positional encoding [16].\n3) Encode the temporally-encoded lidar data using the Transformer encoder.\n4) Aggregate the encoder output via average pooling across the temporal channel, resulting in the aggregated lidar data, \\( E(t) \\).\n5) Concatenate \\( E(t) \\) with the current lidar data \\( L(t) \\) and robot kinematic information \\( K(t) \\).\n6) Map the concatenated vector to Q-values using the MLP network.\n7) Select actions corresponding to the maximum Q-values and execute them within the vectorized E-Sparrow environment.\nLeveraging the vectorized simulation capabilities of E-Sparrow and the parallel processing capacity of neural networks, the above procedures can be executed concurrently across multiple instances during simulation training, significantly enhancing the efficiency of the learning system. Prior to advancing, three specifics warrant further discussion.\n1) Why TWQ: It is widely recognized that off-policy DRL algorithms demonstrate superior sample efficiency compared to on-policy methods, a feature that is essential for the practical deployment of DRL in real-world scenarios. As outlined in Section II-A, while RNN-based methods are fundamentally incompatible with off-policy algorithms and CNN-based approaches exhibit limitations in temporal modeling, this study adopts the framework of Transformer-based architectures combined with off-policy DRL. Although prior research, such as DTQN [26], has demonstrated the feasibility of learning directly from consecutive samples (a continuous trajectory) with Transformer in an off-policy manner, it is inefficient due to the strong correlations inherent in consecutive samples. Specifically, even minor modifications to the Q-value can lead to substantial shifts in the policy, thereby altering the data distribution and disrupting the relationship between the Q-value and its target [3]. This phenomenon can lead to suboptimal policies or even unstable learning performance, which will be empirically demonstrated in our experimental section (see Fig. 9). To mitigate this challenge, we introduce a TWQ to break the continuous trajectory into independent temporal windows for each timestep \\( t \\), as denoted by \\( [L(t), L(t-1), ..., L(t - T + 1)] \\). It is important to note that this entire temporal window is used exclusively for Q-value prediction at timestep \\( t \\), and not for predictions from timestep \\( t \\) to \\( t - T + 1 \\). Concurrently, the first-in-first-out characteristic of the TWQ guarantees the allocation of a corresponding temporal window to each timestep, thereby ensuring the training for all timestep. These independent temporal windows are subsequently stored in a replay buffer and randomly sampled during training. This strategy circumvents the inefficiencies of learning consecutively from long sequences and simultaneously enhances the data utilization rate, ultimately bolstering performance.\n2) Why Average Pooling: The core function of the temporal perception module (the TWQ and the Transformer encoder) is to comprehend and forecast the environmental dynamics, the result of which is a tensor of shape \\( (N, T, D) \\), where \\( N \\) represents the number of vectorized environments, \\( T \\) denotes the temporal window length, and \\( D \\) signifies the feature dimension (corresponding to the number of lidar beams in this study). Prior to Q-value prediction, this tensor must be aggregated across the temporal channel, resulting in an output tensor of shape \\( (N, D) \\) for the MLP's inference. Three primary aggregation methods were considered: (i) average pooling, (ii) max pooling, and (iii) positional pooling (only using the most recent temporal position). Empirical results indicate that average pooling surpasses the other two methods in performance. We posit that this superiority stems from average pooling's ability to consolidate maximum information across the temporal channel. This consolidation is crucial for temporal understanding, whereas the other approaches predominantly focus on a singular, critical time point.\n3) Why concatenating: A potential question arises regarding the necessity of concatenating \\( L(t) \\) prior to Q-value prediction, given that \\( E(t) \\) already encapsulates lidar information. Our rationale is that \\( E(t) \\) primarily serves to facilitate comprehension and prediction of environmental dynamics, thereby guiding the agent's subsequent high-level decisions, such as bypassing, overtaking, or retreating. However, it may overlook crucial information for collision avoidance, specifically the instantaneous distance to obstacles. Consequently, \\( L(t) \\) is concatenated to emphasize this information. An ablation study was conducted in the pre-experiment to validate the hypothesis, revealing that the omission of \\( L(t) \\) leads to a degradation in performance. Furthermore, the robot's kinematic information, \\( K(t) \\), is also concatenated to furnish vital information for robot control and target reaching."}, {"title": "D. Symmetric Invariance", "content": "Prior research [27]\u2013[29] has indicated that Transformer models are demanding regarding training samples, both in terms of data volume and diversity. This demand can be further exacerbated within the context of online DRL due to the non-stationary nature of the training process, wherein the target Q-values associated with a given observation vary during training [3]. To accommodate this issue, we introduce a data expansion technique predicated on Symmetric Invariance (SI), as illustrated in Fig. 4. Given a raw transition \\( (o_t, a_t, r_t, o_{t+1}) \\) generated by the simulation environments, the SI-expanded transitions is given by \\( (o^{SI}_t, a^{SI}_t, r_t, o^{SI}_{t+1}) \\), where\n\\[ o^{SI}_t = [K^{SI}(t), L^{SI}(t), L^{SI}(t-1), ..., L^{SI}(t - T + 1)] \\]\n\\[ K^{SI}(t) = [V^{ct}_t, -V^{ct}_t, V^{rt}_t, -V^{rt}_t, D2T_t, -\\alpha_t, V^{i}_t, -V^{a}_t] \\]\n\\[ L^{SI}(t) = [ld_n, ..., ld_3, ld_2, ld_1] \\]\n\\[ a^{SI}_t = [V^{ct}_t, -V^{ct}_t] \\]\nIn contrast to existing symmetry-based data expansion techniques [46]\u2013[48] that primarily focus on image-type perception data for supervised learning, our work extends this concept to DRL-based robotic systems and factor not only perception data but also the kinematic state of the robot."}, {"title": "E. ColorDynamic", "content": "Integrating the end-to-end problem formulation, the E-Sparrow simulator, the Transqer network, and the SI data expansion technique, we formally present our DRL solution to the LPP, as delineated in Algorithm 1. We name our approach as ColorDynamic, where \u201cColor\u201d originates from our previous work [5] and \u201cDynamic\u201d highlights the extension to dynamic environments.\nColorDynamic comprises two distinct phases: interaction and training. During the interaction phase, the Transqer interacts with the E-Sparrow simulation platform, aided by the TWQ, and the resultant data is stored in a replay buffer for subsequent training. In the training phase, a batch of data is sampled from the replay buffer and subsequently extended via SI according to Eq. (6)~(9). Following this, the raw and extended data are concatenated for DRL training. For the training algorithm, we employ DQN [3], an off-policy DRL algorithm that has demonstrated human-level decision-making capabilities in discrete action spaces. Additionally, the Double Q-learning [49] technique is incorporated to mitigate the notorious overestimation issue inherent in DQN. Moreover, the Actor-Sharer-Learner training framework [5], featuring vectorized interaction and decoupled training, is harnessed to improve training efficiency.\nColorDynamic exhibits several advantageous features: (a) Generalizability: The diversity provided by the E-Sparrow simulator, encompassing varied environmental layouts, sensor noise, and robot kinematics, significantly contributes to the generalizability of the trained local planner to unseen environments and to unfamiliar robot kinematics. Furthermore, this diversity can be augmented through the SI technique, thereby further improving the model's generalizability. (b) Scalability: The inherent parallel execution capability of Transqer (or more broadly, neural networks) facilitates seamless extension from single-robot to multi-robot planning. (c) Real-time Performance: The end-to-end design eliminates pipeline preprocessing of raw sensor data, thereby tremendously improving real-time decision-making performance. (d) Dynamic Environments Adaptability: The temporal perception module of Transqer, comprising the TWQ and Transformer encoder, equips the trained planner with a robust capacity to comprehend, predict, and make decisions within dynamic environments. (e) Unstructured Environments Applicability: The planner's perception of its surroundings is accomplished through its onboard lidar instantaneously, negating the requirement for pre-constructed environmental maps or a priori knowledge of obstacles, thus enabling its applicability to unstructured environments.\nThe experimental section will validate above desirable features. Prior to this, a comparative analysis with existing relevant works, such as Color [5], DTQN [26], and DRL-VO [6], is presented here to elucidate the distinctions of our work. Specifically, ColorDynamic is an extension of our previous work, Color. While Color addressed the LPP in static environments, ColorDynamic extends this to dynamic environments. The extension is achieved through the incorporation of a new end-to-end formulation for LPP, an enhanced simulator E-Sparrow, a novel temporal model Transqer, and an effective data augmentation technique SI. In contrast to DTQN, the primary difference lies in the temporal perception module. ColorDynamic employs a TWQ to segment consecutive temporal data into independent temporal windows, thereby avoiding ineffective learning directly from sequential data within the context of off-policy DRL. In addition, while DTQN is presented as a DRL algorithm, it lacks a detailed discussion of"}, {"title": "F. OPCD Navigation System", "content": "This section introduces the OkayPlan-ColorDynamic (OPCD) navigation system, based on the proposed ColorDynamic and our previous work, OkayPlan [50]. In this system, OkayPlan functions as a global planner, while ColorDynamic serves as a local planner.\n1) A Review of OkayPlan: The diagram of the OkayPlan algorithm is illustrated in Fig. 5, wherein the global path planning problem is formulated as an optimization problem, as follows:\n\\[ F(Z) = G(Z) + \\delta \\cdot Q(Z)^{\\sigma} + \\mu \\cdot P(Z)^{\\nu} \\]\nHere, \\( F(Z) \\) represents the fitness function, with \\( Z \\) being the decision variables representing waypoint coordinates. \\( G(Z) \\) represents the length of path, \\( Q(Z) \\) represents the number of intersections with obstacles, \\( P(Z) \\) represents the number of intersections with obstacle kinematics (velocity vectors), and \\( \\delta, \\sigma, \\mu, \\nu \\) are four coefficients. The optimization problem, denoted by Eq. (10), is resolved by an efficient heuristic optimizer, SEPSO [51]. As exemplified by Fig. 5, this framework enables OkayPlan to identify a safer path (Path 3) in dynamic environments, as opposed to the shortest but risky path (Path 1). Furthermore, the calculations of \\( G(Z) \\), \\( Q(Z) \\), and \\( P(Z) \\) can be efficiently performed through matrix operations involving the waypoints, obstacle vertices, and kinematic segment endpoints, which facilitates deployment on GPUs to achieve real-time performance. It has been reported that OkayPlan is capable of generating safe yet short paths in dynamic environments at a real-time execution speed of 125 Hz.\n2) Obstacle Definition: Based on the characteristics of obstacles, they are categorized into four Obstacle Layers (OL), as illustrated in Fig. 6. The distinction between \"known\" and \"unknown\" properties is determined by the extent to which the navigation system has prior knowledge of the obstacles. For instance, static buildings (OL1) can be represented on a map, whereas other robotic obstacles (OL2) can communicate directly with the controlled robot. As a result, planning in known (structured) environments can be managed by the global planner. However, the global planner is only capable of generating a global path without providing control commands. Furthermore, the global path does not account for unknown obstacles. Therefore, a local planner is required for path following and collision avoidance with newly encountered unknown obstacles under unstructured environments.\n3) Integration of OkayPlan and ColorDynamic: Based on the aforementioned principles, the navigation system is designed and depicted in Fig. 7. This configuration necessitates a global planner capable of generating a path based on known information and dynamically recalculating shorter and safer paths as the known environmental information changes, as shown in Fig. 7 (a) and (b). Simultaneously, it requires a local planner with real-time temporal perception capabilities to comprehend, predict, and make decisions when encountering newly emerged unknown obstacles, thereby avoiding potentially unsafe local paths, as highlighted in Fig. 7 (c). The dynamic regeneration ability is guaranteed by OkayPlan, while the real-time temporal perception capability is provided by ColorDynamic. Their integration is achieved by selecting a sub-goal along the global path at a distance of 0.6d from the robot, which serves as the target point for ColorDynamic. Here, d is the maximum local planning distance. Recall that in the reward function, exceeding d is penalized and regarded as termination. Consequently, for scenarios where \\( D2T > d \\), the agent lacks familiarity. To account for this, a discount factor of 0.6 is applied when navigating, providing a buffer distance to accommodate potential retreats.\nOne might argue that, since ColorDynamic demonstrates proficiency in managing unknown obstacles, it should logically be able to handle known obstacles as well. If this assumption holds, then what is the necessity of OkayPlan? Being a local planner, in order to achieve real-time performance in unstructured environments, ColorDynamic is not integrated with a global map. Consequently, it is limited by a maximum local planning distance d, which can pose challenges for long-distance planning within OL1. Note that a greedy increase in d during training can lead to a degradation in performance. Concerning OL2, ColorDynamic is capable of addressing them independently. However, the OkayPlan from a higher and more global level can assist the local planner in avoiding high-risk collision areas proactively in advance, as will be validated in the experiments (see Fig. 13)."}, {"title": "IV. LOCAL PLANNING WITH COLORDYNAMIC", "content": "This section assesses the proposed ColorDynamic, focusing on the determination of hyperparameters, the evaluation of the trained local planner, the comparison with existing approaches, and the ablation of the proposed techniques. The software and hardware configurations supporting these experiments are summarized in Table V in the Appendix."}, {"title": "A. Experimental Setup", "content": "The proposed E-Sparrow simulator is employed to train and evaluate ColorDynamic in the context of local planning, with the experimental setups detailed as follows.\n1) Environment: The training map is configured with a size of 8 m, and the maximum local planning distance d is set to 4 m. The number of static obstacles (OL3) is randomly sampled between 0 and 36. The number of dynamic obstacles (OL4) is set to 15, with a maximum velocity of 0.5 m/s, moving randomly within the map. Notably, the shape and location of obstacles are procedurally generated at the initialization of the environment. Episodes are truncated if the interaction steps exceed 500. The target is deemed reached when \\( D2T < 0.3 \\) m.\n2) Robot: The robot has a diameter of 0.2 m, with maximum linear and angular velocities constrained to 0.5 m/s and 2 rad/s, respectively. Such configuration is derived from our real-world robot, as shown in Fig. 14(a). The control frequency is set to 10 Hz. The lidar scanning range covers 360\u00b0 \u00d7 3m and consists of 24 lidar beams in total. The robot's initial pose is randomly generated at the commencement of each interaction episode. The robot is trained to repeatedly reach the target point. When reached, the target point will be randomly generated within d around the robot.\n3) Hyperparameters: The maximum training steps are set to 3M, with a learning rate of 0.0001 and a batch size of 256. During training, the environment is reset every 32K steps to regenerate obstacles randomly, and the model is evaluated"}, {"title": "B. Model Complexity", "content": "Determining an appropriate model complexity is crucial for the efficacy of ColorDynamic. The model incorporates six critical hyperparameters to define its complexity:\n*   \\( T \\): temporal perception length.\n*   \\( W \\): width of linear layers for both the Transformer encoder and MLP.\n*   \\( N_E \\): number of Transformer encoder layers.\n*   \\( N_p \\): number of MLP layers.\n*   \\( H \\): number of heads in the multi-head attention mechanism [16] within the Transformer encoder.\n*   \\( D \\): feature dimension of the Transformer encoder.\nA comprehensive grid search across these hyperparameters is computationally prohibitive. Therefore, certain values are adopted empirically from existing studies, while the remaining are determined through a coarse search. Specifically, \\( N_p \\) is set to 3, referencing Color [5]; \\( H \\) is set to 8, referencing Transformer [16]; and \\( D \\) is simplified to match the number of lidar beams, which is 24 in this study. Regarding the \\( W \\) and \\( N_E \\), a grid search was conducted over the parameter space \\( [64, 128, 256, 512] \\) \\( \\times \\) \\( [1, 2, 3] \\), as illustrated in Fig. 8. Note that these curves are smoothed with an exponential factor of 0.95 for better readability. Meanwhile, these experiments were conducted across three random seeds, with the solid curves representing the mean values and the translucent shaded areas denoting the standard deviation. Same data processing procedures were applied to Fig. 9 and Fig. 11 as well. Subsequently, the temporal window length T was determined through a search over [5, 10, 20], which corresponds to temporal perception durations of [0.5, 1, 2]s, considering the control frequency is 10 Hz. The results of this search are shown in Fig. 8(c). The results reveal that the configuration \\( [W = 64, N_E = 3, T = 10] \\) delivers the best overall performance in terms of both final reward and training stability. This configuration is therefore adopted as the default for ColorDynamic and for subsequent experiments."}, {"title": "C. Transqer v.s. DTQN", "content": "ColorDynamic harnesses Transqer as its core model. The work most similar to Transqer is DTQN [26]. Therefore, this section presents a comparison of Transqer and DTQN within the ColorDynamic framework in the context of LPP. The experiment adheres to the configurations outlined in Sections IV-A and IV-B, with results shown in Fig. 9.\nThe results indicate a considerable enhancement in Transqer's performance over DTQN, both in terms of final performance and training stability. We contend such improvement is attributed to Transqer's segmentation of long temporal consequence into independent temporal windows, thereby mitigating the inefficiencies associated with direct learning from consecutive data, as discussed in Section III-C1."}, {"title": "D. Evaluation of ColorDynamic", "content": "This section assesses the local planner trained by ColorDynamic. To this end", "metrics": "n*   Average speed: the mean velocity of the controlled robot.\n*   Average planning time: the mean planning time required by the planner", "rate": "the success rate of the local planner in controlling the robot from a random start point to a random target point at a distance of 2 m. This distance is adopted from DRL-VO [6", "Study": "In this section"}, {"Study": "To demonstrate the superiority of ColorDynamic, we compare its performance with two classical local planners, APF [1", "2": "as well as two state-of-the-art learning-based methods, Color [5", "6": "."}]}