{"title": "Forking Paths in Neural Text Generation", "authors": ["Eric Bigelow", "Ari Holtzman", "Hidenori Tanaka", "Tomer Ullman"], "abstract": "Estimating uncertainty in Large Language Models (LLMs) is important for properly\nevaluating LLMs, and ensuring safety for users. However, prior approaches to\nuncertainty estimation focus on the final answer in generated text, ignoring inter-\nmediate steps that might dramatically impact the outcome. We hypothesize that\nthere exist key forking tokens, such that re-sampling the system at those specific\ntokens, but not others, leads to very different outcomes. To test this empirically, we\ndevelop a novel approach to representing uncertainty dynamics across individual\ntokens of text generation, and applying statistical models to test our hypothesis.\nOur approach is highly flexible: it can be applied to any dataset and any LLM,\nwithout fine tuning or accessing model weights. We use our method to analyze\nLLM responses on 7 different tasks across 4 domains, spanning a wide range of\ntypical use cases. We find many examples of forking tokens, including surprising\nones such as punctuation marks, suggesting that LLMs are often just a single token\naway from saying something very different.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) demonstrate impressive yet opaque capabilities that emerge during\nnext-word prediction (Brown et al., 2020; Kaplan et al., 2020; Bubeck et al., 2023), and a good deal\nof current research is devoted to understanding and interpreting LLM behavior (Chang et al., 2024;\nAnwar et al., 2024; Bricken et al., 2023; Holtzman et al., 2023; Aky\u00fcrek et al., 2022). LLMs are\noften treated as black boxes due to the sheer complexity of their internal workings, and because many\nstate-of-the-art models are only accessible at the level of inputs and outputs. One way to assess any\ndynamic system is to consider what possible things it could have done, but didn't. In text generation,\nwe can liken a text sequence to a path the system took through the semantic space of all possible paths,\nand ask: what other paths could the system have taken? Are there key points where re-sampling the\nsystem at that specific point, but not others, would lead to very different paths?\nWork on uncertainty estimation in LLMs tackles the related problem of assessing how likely an LLM\nis to respond with different final answers, e.g. the probability of responding \u201cA\u201d or \u201cB\u201d to a multiple\nchoice question (Kadavath et al., 2022; Tian et al., 2023; Guo et al., 2017; Ye et al., 2024). Previous\napproaches to black-box uncertainty estimation have yielded important insights by analyzing data\nsuch as the logit probabilities of the final tokens in an LLM's output, or the fraction of text responses\nthat end in the correct answer (Geng et al., 2024; Xiong et al., 2024). However, in our analogy, these\napproaches consider only the final destination, and not the paths leading to them.\nA major limitation of prior work on uncertainty estimation is that the last few tokens of an LLM's\noutput are largely determined by previous tokens. For example, a single wrong step when solving a\nmulti-step reasoning problem (e.g. \u201cThe current year is 2021 ... \") can cascade into a wrong final\nanswer (e.g. \"... The current British head of state is Queen Elizabeth.\u201d), or other undesired responses\n(Zhang et al., 2023). Uncertainty over intermediate tokens or reasoning steps will not be reflected\nin the final tokens of the LLM's response, since these tokens will be nearly deterministic (100%\nconfidence) given the rest of the text preceding them. A similar assumption is made in process-level\nsupervision (Lightman et al., 2023), which gives an LLM feedback for the correctness of each step of\nits solutions, in addition to its final answer (i.e. outcome-level supervision). Perhaps, then, we might\ngleam valuable insights by analyzing uncertainty in paths and not just outcomes.\nOur approach is to study uncertainty dynamics, or how an LLM's likelihood of producing different\nresponses changes as each new token is generated (Fig. 1). Specifically, we propose the Forking\nTokens Hypothesis: that in LLM text generation, there will be individual forking tokens which, if\ngenerated, lead to dramatic differences in subsequent text (Fig. 1). Uncertainty dynamics and forking\ntokens are unseen by prior approaches to 'static' uncertainty estimation such as taking the logits of\nthe final answer token, or re-sampling many full responses. This inspires a new 'dynamical' way of\nthinking about uncertainty in text generation, where we study the influence that individual tokens\nhave on the eventual outcome. We develop a methodology called Forking Paths Analysis (Sec. 2)\nin order to shed light on uncertainty dynamics and to empirically test for forking tokens. We find\ndramatic uncertainty dynamics in GPT-3.5 in many tasks commonly used for evaluation, including\nsingle tokens that cause the model to suddenly flip from low confidence to high confidence in a final\nanswer. This supports the Forking Tokens Hypothesis, and suggests that uncertainty dynamics in\nGPT-3.5 are considerably more chaotic than high confidence final answers might suggest.\nPut briefly, our primary contributions are:\n1. A novel hypothesis regarding the existence of 'forking tokens' that greatly impact the\noutcome of text generation. We propose the Forking Tokens Hypothesis, that there exist\nindividual tokens which, if sampled during decoding instead of an alternative token, lead to\ndramatic differences in subsequent text generation (Fig. 1).\n2. A novel approach to representing uncertainty at each token in next-word prediction.\nOur method aggregates text samples into time series and conditional distributions, revealing\nuncertainty dynamics invisible to prior work (Sec. 2.1, 2.2). We use change point detection\nmodels and survival analysis to empirically test our hypothesis and efficiently scale across\nhundreds of individual analyses (Sec. 2.3, 2.4).\n3. Our analysis shows striking text generation dynamics, including change points in many\nsequences and unexpected forking tokens such as space characters We examined text\ngeneration dynamics in GPT-3.5 using 7 different LLM evaluation tasks (Sec. 4). Our results\nsupport the Forking Tokens Hypothesis."}, {"title": "2 FORKING PATHS ANALYSIS", "content": "In text generation, exchanging a single token may drastically alter subsequent text. This is clearly true\nin reasoning. For example, if we ask \u201cWho is the current British head of state?\", the intermediate\nreasoning step \"The current year is (2021/2024) ... \" can lead to different final answers. This could\nalso occur in open-ended text generation, when a single token distinguishes topics. For example\n\u201cBilly woke up in a hotel . . . \u201d and \u201cBilly woke up in a spaceship . . . \u201d leads to very different stories.\nLLMs generate text one token at a time, and sampling a word such as \u201chotel\u201d instead of \u201cspaceship\u201d\ncould steer autoregressive text generation towards one path over another. Many possible tokens could\ncause forking if exchanged, e.g. if we manually set the token in the previous example to \u201ccoconut\u201d.\nHowever, we aim study the forking tokens that are most likely to be sampled during text generation.\nWe expect text generation to be relatively stable, in that most tokens an LLM is likely to sample\nwould only change surface features and would not affect the outcome or overall meaning. However,\nLLMs also show significant biases in producing certain words and phrases over others (McCoy et al.,\n2023), and adapting their semantic priors on the fly (Wei et al., 2023). We also might expect, then, to\nfind unexpected forking tokens with LLMs, for example if \"Billy woke up in a ...\" leads to a story\nabout a summer vacation and \u201cBilly woke up in the ... \" leads to a science fiction story. This could\noccur if an LLM were considering two different outcomes to a story (e.g. a summer vacation or a\nsci-fi story), but these outcomes were biased towards specific words or phrases such as \u201cBilly woke\nand \"Billy awoke. If we find unexpected forking tokens such as this, it might suggest that LLMs are\nnot planning their final responses before generating text, and instead they are effectively deciding\nwhat the outcome will be as each new token is sampled.\nWe predict that in text generation with LLMs, there will be individual tokens where text generation\n'forks' into multiple distinct outputs. More formally, we propose the Forking Tokens Hypothesis:\nthat a single token being exchanged for a probable alternate token can significantly impact the\noutcome of subsequent text generation in LLMs. To test this, we develop a method that includes\na multi-stage sampling pipeline (Section 2.1), aggregating text samples into outcome distributions\n(2.2), and applying statistical models to test our hypothesis (2.3, 2.4).\""}, {"title": "2.1 LANGUAGE MODEL SAMPLING PIPELINE", "content": "For our analysis, we collect a large number of output texts from an LLM for each individual question\nprompt using a 3-stage sampling pipeline (Fig. 2, Top). We begin by decoding a single base path\nresponse \\(x^*\\) to a given prompt. This sample includes, for each token \\(x_t\\) in sequence, the logit\nprobabilities \\(p(x_t = w|x_{<t})\\) for the top K tokens w at index t. Next, for each index t and each\nalternate token w with sufficiently high probability \\(p(x_t = w | x^*_{t})\\), we re-sample a batch of\nsamples \\({\\bf x}_{>t}^{(s)} \\) by prompting the LLM with the original input text appended to the base path\nresponse \\(x^*\\) up to time t, with the last token being \\(x_t = w\\). In the third step, we extract an outcome\nrepresentation for each sample \\(R(x^*{<t}, x_t = w, {\bf x}_{>t}^{(s)})\\). Described in the following section, R is a\nsemantic vector representation such as a one-hot encoding of the final answer.\nThese stages rely on token logit probabilities from the evaluated LLM, which are available in black-\nbox LLM APIs such as OpenAI and TogetherAI and can also be inferred for APIs without this feature\n(Morris et al., 2023). In our experiments, the third stage R uses a second LLM, which may be the\nsame or different from the main LLM being evaluated, and does not require logit probabilities. We\nprompt this model to extract the final answer from \\(x^*\\), and convert its response into a categorical\nvalue, such as \"A\" or \"B\"."}, {"title": "2.2 REPRESENTING TEXT GENERATION OUTCOMES", "content": "We construct outcome distributions for individual token indexes \\(o_t\\) and for token values \\(o_{t,w}\\). We\ndefine an outcome distribution o as the expected value of a semantic vector representation R, which\nvaries depending on the task and takes as input both the question prompt and model-generated\nresponse. In the case of a multiple-choice task, R is a one-hot encoding of the final answer in the\nresponse string (e.g. 'A', 'B', 'C', or 'D'). For open-ended tasks without final answers, R can be any"}, {"title": "2.3 BAYESIAN CHANGE POINT DETECTION", "content": "We now present a method for automatically identifying tokens where \\(o_t\\) changes suddenly, and for\nstatistically testing the hypothesis that such tokens exist. In simple terms, our goal is to test for\nwhether there are indexes t such that the outcome distribution changes substantially before and after\nt, i.e. \\(o_{>t} \\neq o_{<t}\\). Change Point Detection (CPD) is an area of statistics that models time series\ndata with abrupt changes to the line's intercept and/or slope. CPD models jointly infer what time\nt a change occurs at, as well as trend parameters \\(\\beta\\) for time segments before and after the change.\nWe suppose that there may be multiple forking tokens, or change points, in the outcome distribution\n\\(o_t(x^*)\\) for a single sequence \\(x^*\\). This leads us to use multiple CPD models, which assume there\nmay be multiple change points. However, inference with multiple CPD models can be exponentially\nmore expensive than with single CPD (Fearnhead, 2006). One solution to this complexity is using\nBayesian models for efficient approximate inference. Since multiple CPD with multivariate time\nseries (as in \\(o_t\\)) is a relatively young area of research (Cabrieto et al., 2017) with limited available\ntooling, we convert \\(o_t\\) into a univariate time series \\(y_t\\) using a semantic drift metric (Fig. 3, Top), as in\nKulkarni et al. (2015). Each time point in the univariate time series \\(y_t = d(o_0, o_t)\\) is the distance\nbetween the initial outcome distribution \\(o_0\\) and outcome distributions for subsequent time steps \\(o_t\\),\ngiven an arbitrary distance metric d. In our experiments, we use L2 distance for d.\nMore specifically, a CPD model decomposes a time series y into a set of m segments. Each segment\n\\(i \\in \\{0... m\\}\\) is fit by a regression model with intercept (i.e. abrupt change) \\(\\delta_i\\) and slope \\(\\beta_i\\), applied\nto time steps t between \\(\\tau_{i-1}\\) and \\(\\tau_{i}\\). In a Bayesian CPD model, \\(p(\\tau = t|y)\\) describes how likely it is\nthat a change point \\(\\tau\\) occurred at each time t in time series data y, and \\(p(m|y)\\) describes belief in\nthe number of change points m in a series. The beginning and end of a sequence are treated as fixed\nchange points (i.e. \\(\\tau_0 = 0\\) and \\(\\tau_{m+1} = T\\)) which are excluded from analysis of \\(p(\\tau = t | m)\\) and\n\\(p(m | y)\\). We test for statistically significant change points by comparing the hypothesis that there is\nno change point (m = 0) with the hypothesis that there is at least one (m > 1) (Aminikhanghahi\n& Cook, 2017). In Bayesian CPD, this entails model comparison between a model with no change\npoints and a model with at least one change point. We take a Bayes factor \\(p(m > 1|y) / p(m = 0|y)\\)\ngreater than 9 as supporting the hypothesis that there is at least 1 change point (Kass & Raftery,\n1995) *. We use an extremely efficient open-source implementation of Bayesian CPD which utilizes\nMonte Carlo Gibbs sampling to infer posterior distributions \\(p(m | y)\\) and \\(p(\\tau = t|y)\\) given a sequence\ny (Zhao, 2019). Further details of CPD model are provided in App. B."}, {"title": "2.4 SURVIVAL ANALYSIS", "content": "We perform a survival analysis to estimate on how likely it is that text generation would be greatly\nimpacted if a token in a base path \\(x^* = w^*\\) were instead sampled as w. This describes cases where\nan alternate token w leads an LLM down a very different path from \\(x^*\\), as in Fig 7 (Right). This\nforking may not appear as a stable change in \\(o_t(x^*)\\) (Sec 2.3) since w was not actually sampled and\ntherefore does not impact \\(o_{>t}(x^*)\\). We define the survival function S(t) as the probability that the\nbase path \"survives\u201d sampling alternate tokens w that would change the outcome distribution:\n\\begin{equation}\nS(t) = 1 - \\Pi_{t'=1}^{t} \\mathbb{E}_w \\left[\\mathbb{I}_{o_{t',w} \\neq o_{t',w^*}}\\right]\n= 1 - \\Pi_{t'=1}^{t} \\sum_w p(x_t = w / x^*_{<t'}) \\mathbb{I} \\left[d(o_{t',w}, o_{t',w^*}) > \\epsilon\\right]\n\\end{equation}\nS(t) is a discrete time survival function S(t) = 1 \u2212 \\(\\Pi_{t'=1}^{t}\\) h(t') where h(t) is the probability that\na failure (or hazard h) occurs at time t. In our case, a failure is when an alternate token causes the\noutcome distribution to shift significantly from the greedy, i.e. \\(o_{t,w} \\neq o_{t,w^*}\\), which we estimate\nby testing whether the distance between outcome distributions \\(d(o_{t,w}, o_{t,w^*})\\) is greater than some\nthreshold \\(\\epsilon\\). d is an arbitrary distance metric and we use L2 distance as d in our experiments. For\neach t, we compute the hazard rate h(t) as the sum of token logit probabilities \\(p(x_t = w|x^*_{<t})\\) for all\nforking tokens w (i.e. tokens where \\(d(o_{t,w}, o_{t,w^*}) > \\epsilon\\))."}, {"title": "5 DISCUSSION", "content": "Text generation with LLMs can be viewed as a branching tree of possible paths, where each word\nchoice determines what text will follow, akin to Borges' Garden of Forking Paths and other choose-\nyour-own-adventure stories (Borges, 1941; Bottou & Sch\u00f6lkopf, 2023; janus, 2021). Many of\nthese paths will follow similar trajectories and end in similar places, but some of them will hit\nforks which bifurcate into multiple distinct meanings. Our results support the Forking Tokens\nHypothesis by empirically demonstrating forking tokens in a state-of-the-art LLM applied to various\nreal world benchmarks, suggesting that LLMs are often just a single token away from producing\na very different answer. Forking Paths Analysis reveals dynamics unseen by prior approaches to"}, {"title": "A RELATED WORK", "content": "\u2022 Uncertainty estimation and calibration in LLMs Previous approaches to uncertainty\nestimation with LLMs have provided valuable insights (Geng et al., 2024; Xiong et al., 2024;\nKadavath et al., 2022; Tian et al., 2023; Guo et al., 2017; Ye et al., 2024). However, prior\nre-sampling based uncertainty estimation does not effectively capture the space of forking\npaths, for example paths that are very likely to branch off of the highest probability (i.e.\ngreedily decoded) branch. Final token probabilities or text-based uncertainty estimates (e.g.\n\"70%\") likely do not capture the full picture. It may be possible to develop approaches to more\neffectively sample the space of possible paths that are simpler and/or cheaper than Forking\nPaths Analysis. Finally, we find a parallel to our work in the area of conversation forecasting,\nwhich strives to estimate people's belief uncertainty over the course of a conversation (Sicilia\net al., 2024).\n\u2022 Semantic diversity in text generation Semantic diversity (Tevet & Berant, 2021; Han\net al., 2022; Kirk et al., 2023) measures the degree to which a language model generates\nmeaningfully distinct responses to the same input. We believe semantic diversity may be a\nkey cause of forking tokens, in that semantic diversity demands some degree of uncertainty\nin text generation. For example, diversity in CoT reasoning requires producing multiple\ndistinct proofs.\n\u2022 Chain of thought and similar reasoning techniques Chain-of-Thought (CoT) reasoning\n(Kojima et al., 2022; Wei et al., 2022) and related techniques prompt an autoregressive\nlanguage model to reason across the intermediate tokens it generates. One challenge in CoT\nreasoning is backtracking (Gandhi et al., 2024), where LLMs struggle to 'undo' a missed\nstep. The Forking Tokens Hypothesis describes this phenomenon more broadly, where a\nsingle token can trigger distribution shift such as one reasoning path over another. On the\nother hand, LLMs are not always faithful to their chains of reasoning in the token stream\n(Turpin et al., 2024). Forking Paths Analysis may be able to shed further light on these\ncases."}, {"title": "B CHANGE POINT DETECTION MODEL DETAILS", "content": "We use an extremely efficient implementation of Bayesian multiple CPD, the Bayesian Estimator\nfor Abrupt changes in Seasonality and Trends (BEAST). BEAST is described in Zhao (2019) and\navailable as an R package at https://cran.r-project.org/web/packages/Rbeast/\nindex.html. BEAST is implemented in C/C++, and we found it to be between 1 \u2013 10 thousand\ntimes faster than comparable packages for multiple CPD, most of which also did not support inference\nof p(my).\nWe use BEAST to infer the posterior probability of a change point at each time \\(p(\\tau = t|y)\\) as well\nas posterior of the number of change points in a time series p(my). To estimate these posteriors,\nBEAST iteratively draws Monte Carlo samples j for each of the following variables, in order: the\nnumber of change points \\(m \\sim p(m(i)|\\sigma^{(i-1)}, y)\\), change times \\(\\tau \\sim p(\\tau^{(i)}|m^{(i)}, \\tau^{(i-1)}, y)\\), segment\nparameters p\u03b2, \u03b4 ~ (\u03b2(i), (i)|\u2533(i), (i\u22121), y), and noise parameter o ~ p(\u03c3(i)|\u03b2(i), (i), (i),y).\nWe show plate notation for the structure of the BEAST CPD model in Fig. 8.\nAs described in Sec. 2.3, a CPD model decomposes a time series y into a set of m segments, and each\nsegment \\(i \\in \\{0. . . m \\}\\) is fit by a regression model with intercept (i.e. abrupt change) \\(\\delta_i\\) and slope \\(\\beta_i\\),\napplied to time steps t between \\(\\tau_{i-1}\\) and \\(\\tau_{i}\\). In our case, we assume linear models for each segment\n\\(y_t = \\beta_i t + \\delta_i \\quad t\\in \\{\\tau_{i-1},..., \\tau_i\\}\\), to match our assumption and qualitative observation that in \\(O_t\\)\nthere are stable regimes of uncertainty which continue for many tokens, until \\(O_t\\) abruptly changes\nto a new distribution. We also observe 'drift' in some cases, where \\(O_t\\) slowly changes from one\ndistribution to another, which in our model corresponds to different values of \\(\\beta_i\\). To our knowledge,\nour work is the first apply CPD to analyze neural network learning dynamics, either in-context (as\nin our case) or in-weights. (Hu et al., 2023) uses Hidden Markov Models to analyze in-weights\nlearning dynamics, which achieves a similar purpose as CPD *, but with less interpretable parameters.\nWe see an exciting direction for future work being to further understand which statistical modeling\nassumptions are most appropriate for describing uncertainty dynamics in text generation.\nOne challenge we found with using BEAST for CPD is a high false positive rate in cases where \\(O_t\\)\nhas fewer changes. In these cases, the drift \\(y_t\\) has a low magnitude overall, and so very small changes\nin \\(y_t\\) can show up as false positive change points when BEAST re-normalizes \\(y_t\\). To address this, we\nmanually tuned noise hyper-parameter a and slightly perturb \\(y_t\\) with Gaussian noise of variance .03.\nIn our CPD and survival analysis models, we used L2 distance. We also tested with L\u2081 distance\nand K-L divergence, but found that results with d = L2 most reliably corresponded to qualitative\njudgments of change points in \\(O_t\\) and \\(O_{t,w}\\)."}, {"title": "C COMPARING UNCERTAINTY ESTIMATION METHODS", "content": "We now compare out outcome distribution representation \\(o_t\\) to three static uncertainty estimation\nbaselines, inspired by work such as Xiong et al. (2024). (1) We estimate the outcome distribution\nby re-sampling N = 300 completions from the first token t = 0 and on. (2) We take the base path\n\\(x^*\\), append a brief string to the end \\\"Therefore, the answer is:\\\" and we take the logit probabilities\nfor the next token as the answer certainty estimate. (3) Given the model's output (greedily decoded\ntokens) in (2), we then prompt the model for its confidence by appending an additional prompt\nPercent confidence in final answer: For (3), we take the numeric % confidence estimate and\nassign that confidence to the greedy token output in (2), and all other confidence to a generic \u2018Other\u2019\noutcome.\nWe see in Figs. 9, 10 that in these cases with complex uncertainty dynamics and change points, the\nstatic confidence estimate (1) is significantly different from (2) and (3). This is easily explained by\nlooking at \\(o_t\\), since the outcome distribution at the beginning of the sequence \\(O_0\\) matches the first\nbaseline (1), and the outcome at the end of the sequence of approximately matches (2) and (3). We\nalso find that confidence estimates (2) and (3) assign very high certainty to a final answer, despite\nthere being substantial fluctuations in uncertainty over the course of text generation."}, {"title": "C.2 TOKEN LOGIT PROBABILITIES", "content": "A simple question one might ask about forking tokens, is whether these can simply be explained as\nlow-probability tokens which were unlikely, and when sampled caused the model to go off course.\nFor this reason we ran a correlation between the change point probability at a given token \\(p(\\tau = t | y)\\)\nand the token logit probability \\(p(x_t = w^*)\\) for the greedy token \\(w^*\\). As shown in Fig. 11, we\nfind minimal correlation between (log) change point probability and (log) token probability. The\nslight correlation we find is positive, contrary to the question above, and we note that many of the\nhighest-probability forking tokens also have high logit probabilities."}, {"title": "C.3 COMPARING CPD AND SURVIVAL ANALYSIS", "content": "Another simple question is how our two analysis methods, change point detection (Sec 2.3) and\nsurvival analysis (Sec 2.4), compare to one another. If our methods make similar predictions about\nwhich sequences have forking tokens, they might be redundant. In Sec. 2.4 we explain why survival\nanalysis of \\(O_{t,w}\\) may provide different results from change point detection. We test this by running\na correlation between the estimated number of change points predicted by out change point model\n(i.e. the .1 quantile of p(m | y)) and the final survival rate S(T) of a sequence. As shown in Fig. 12,\nwe find \u2248 0 correlation between which samples have low survival rates with which samples have\nmore change points. This suggests that our two methods are identifying different forking tokens in\nthe outcome distributions of \\(O_t\\) and \\(O_{t,w}\\). Though of and our change point detection models are given\nemphasis in the present work, further analysis of \\(O_{t,w}\\) may also be a promising direction for future\nwork."}, {"title": "D IMPROVING COMPUTATIONAL EFFICIENCY", "content": "As mentioned in Sec. 3, the main limitation of Forking Paths Analysis is that it is very costly in terms\nof number of tokens sampled. The approach we used has the token complexity: \\(O((\\vert x_{in} \\vert + \\vert x^* \\vert + \\vert x^{(s)} \\vert) * \\vert x_t = W\\vert * \\vert x^*\\vert * S)\\), where \\(\\vert X_{in} \\vert\\) is the input prompt, \\(\\vert x^* \\vert\\) is the length of the base path,\n\\(\\vert x^{(s)} \\vert\\) is the length of output completions, \\(\\vert x_t = w\\vert\\) is the number of alternate tokens at an index t,\nand S is the number of completions sampled for each of these.\nFor our experiments, we used S = 30 and sampled on the order of millions of tokens for each input\nand base path. However, one question we asked was whether a smaller number of samples might\nserve nearly as well to identify forking tokens. As shown in Fig. 13, we find that with 10-20 samples,\nthe number of change points our CPD model predicts is very similar to when we use S = 30. In other\nwords, our experiments could be run at half the cost and with similar results.\nAdditionally, we see a number of avenues for future work to improve the efficiency of Forking Paths\nAnalysis. By using prompt caching with open-source models *, the token sample complexity may\nbe reduced to O ((\\(\\vert x^* \\vert + \\vert x^{(s)} \\vert) * \\vert x_t = w\\vert * \\vert x^*\\vert * S) (i.e. samples will not scale by \\(x_{in}\\)). Next, it\nmay be possible to use statistical models to determine optimal tokens t and w to draw samples for.\nThis is very similar to the problem of Optimal Experiment Design (Banga & Balsa-Canto, 2008),\nwhich uses statistical models to determine which data should be collected to most efficiently test a\nhypothesis. More ambitiously, with open-source models we may be able to use hidden activations to\npredict forking tokens. Specifically, we can test whether hidden activations can predict token model\npredictions p(r = t | y), p(m | y) (for our CPD model), and S(T) (for our survival analysis). If this\nis possible, it may be possible to avoid the costly token sampling altogether, simply by analyzing\nmodel activations."}, {"title": "E ADDITIONAL ANALYSES", "content": "Visualizations for all examples x* in our dataset and their respective analyses are available online\nthrough an interactive dashboard: https://forking-paths.streamlit.app/.\nBelow, we include a subset of examples, and show ot, p(r = t|y), and highlighted text for each\nexample x*. Refer to Fig. 4 for how to interpret figures. Examples are hand-selected to demonstrate\ninteresting uncertainty dynamics, including change points. However, we also found many other\ninteresting examples not shown here."}, {"title": "COINFLIP", "content": "In the CoinFlip task (Fig. 15), most outcome distributions are static over the course of text generation.\nThis task is particularly easy for GPT-3.5, and from ot we conclude that, from the beginning of text\ngeneration, the LLM \u2018decides' for certain what its final response will be."}, {"title": "LASTLETTER", "content": "In the LastLetter task (Figs. 16, 17), we observe more change points than any other tasks. Many of\nthese follow a very consistent pattern: the outcome distribution ot (x*) remains uncertain until the\nfinal answer tokens, at which point it collapses to a single outcome."}, {"title": "AQUA", "content": "We find the most complex uncertainty dynamics in the Mathematical Reasoning domains, AQuA\n(Figs. 18, 19) and GSM8k (Figs. 20, 21).\nFor the AQUA examples shown here, we observe multiple changes, including changes at relatively\nunexpected tokens. In these examples, we also observe sharp changes which occur over the course of\na few tokens instead of a single token, e.g. the second change in AQuA-62 and the first change in\nAQUA-160."}, {"title": "GSM8K", "content": "For GSM8k (Figs. 20, 21), we similarly find complex uncertainty dynamics over text generation.\nFor the AQUA examples shown here, we observe multiple changes, including changes at relatively\nunexpected tokens. In these examples, we also observe sharp changes which occur over the course of\na few tokens instead of a single token, e.g. the second change in AQuA-62 and the first change in\nAQUA-160."}, {"title": "HOTPOTQA", "content": "We find dramatic uncertainty dynamics with change points in the HotpotQA examples shown in\nFigs. 4, 22. However, in HotpotQA we also observe a cases where different nearly identical outcomes\nare expressed with different words, as in Fig 23. While we tried to control for semantic variation\nby using a powerful LLM for R(\u00b7), gemini-1.5-flash-001, we see this as a general challenge with"}]}