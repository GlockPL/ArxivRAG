{"title": "THEORETICAL BARRIERS IN BELLMAN-BASED\nREINFORCEMENT LEARNING", "authors": ["Brieuc Pinon", "Rapha\u00ebl Jungers", "Jean-Charles Delvenne"], "abstract": "Reinforcement Learning algorithms designed for high-dimensional spaces often enforce the Bellman\nequation on a sampled subset of states, relying on generalization to propagate knowledge across\nthe state space. In this paper, we identify and formalize a fundamental limitation of this common\napproach. Specifically, we construct counterexample problems with a simple structure that this\napproach fails to exploit. Our findings reveal that such algorithms can neglect critical information\nabout the problems, leading to inefficiencies. Furthermore, we extend this negative result to another\napproach from the literature: Hindsight Experience Replay learning state-to-state reachability.", "sections": [{"title": "Introduction", "content": "One of the main goals of Artificial Intelligence is to devise universal algorithmic ideas to solve problems as efficiently\nas possible. One typical application is Automated Theorem Proving (ATP), which serves as a running example ap-\nplication here. In ATP, solving a problem involves finding a proof for a theorem from a set of axioms within the\nconstraints of a logic system. Since this must be done under constrained computational resources, algorithms must be\ndesigned to find a solution quickly.\nA promising direction to design efficient general-purpose algorithms is to blend search and learning, where learning\nis leveraged to accelerate the search process. These algorithms iteratively attempt to construct solutions, leveraging\nfeedback from previous attempts to learn to guide the next constructions.\nReinforcement Learning (RL) implements this paradigm to optimize objectives over sequences of decisions, where\neach decision-making point is associated with a state. This structure allows Dynamic Programming to be applied in\nthe form of the Bellman equation to learn a value function. RL algorithms typically sample sequences of states guided\nby the value function while simultaneously improving this value function by enforcing the Bellman equation on the\nsampled states. This approach forms the foundation of many RL algorithms, with various adaptations (Sutton & Barto,\n2018).\nThis paper examines the theoretical limitations of this RL approach to enhance our understanding and support the\ndevelopment of more effective algorithms.\nIntuitively, the results of this paper formalize the incapacity of these algorithms to efficiently \u201clearn from failure\u201d. For\ninstance, in ATP, the probability of proving a theorem with randomly sampled logical rules is often low, this leads to\nsparse rewards and might hinder the capacity of an algorithm to learn. While proving an incapacity to solve a problem\nwith sparse rewards without any other information is trivial -since any algorithm would resort to exhaustive search-\nwe analyze the non-trivial case: when the algorithm can leverage a priori information to help his search and learn from\nits failures."}, {"title": "Preliminaries", "content": "We note $[n] = \\{1, ..., n\\}$ the set of the n first natural numbers. For a vector $x \\in X^n$, with some set X and $n \\in \\mathbb{N}$,\nwe note $x_{<i}$($x_{\\leq i}$) the vector restricted to the first $i$($i-1$)th coordinates. An index list I over $n \\in \\mathbb{N}$ is a sequence\nof numbers in [n]. For I an index list over n, $x_I$ is the vector composed of the values of x at the coordinates in I. For\nx and y two vectors, we note $[x, y]$ their concatenation.\nOur counterexamples are based upon the Boolean satisfiability problem, we define here a classical form of this prob-\nlem. Given a vector of $n \\in \\mathbb{N}$ Boolean variables $x_i$ ($i \\in [n]$): a literal is one of these variables $x_i$ or its negation\n$\\neg x_i$; a clause is a set of literals joined by disjunctions $\\lor$. Finally, a conjunctive normal form satisfiability (CNF-SAT)\ninstance is a set of clauses over the n variables and a solution for the instance is a vector $x \\in \\{False, True\\}^n$ such\nthat all the clauses evaluate to True under the interpretation given by x. We denote this evaluation function checking\na binary vector x against a CNF-SAT instance p $Check(x; p)$."}, {"title": "Limitation on the Bellman Equation with Value Functions", "content": "We identify a limitation of Bellman equation-based algorithms when applied to solving CNF-SAT instances. Specifi-\ncally, we show that these algorithms can fail to exploit structure in aggregated instances. We demonstrate this through\nAlgorithm 1, which implements the Bellman equation to learn a value function and guide the search towards solutions\nfor a CNF-SAT instance. Theorem 3.6 establishes that the algorithm does not effectively decompose some counterex-\nample aggregated instances, resulting in an exponential runtime with respect to the number of instances."}, {"title": "The Algorithm", "content": "To solve a CNF-SAT instance using RL, we formulate the problem as a sequence of binary decisions. An instance with\nn binary variables is modeled as a sequence of n binary decisions that form a candidate solution. For each decision,\nthe state is defined as the SAT instance and the first i fixed variables, while the next action involves assigning a value\n(0 or 1) to the i + 1th variable. The RL algorithm seeks to optimize these decisions under the objective of maximizing\n1 for a solution and 0 otherwise.\nTo guide the generation toward a solution, our RL algorithm leverages value functions learned by enforcing the Bell-\nman equation. We now define these value functions, their optimality, and the Bellman equation."}, {"title": "Definitions", "content": "Definition 3.1. A value function $v(x_{<i};p)$ maps a CNF-SAT instance p over n variables and a partial assignment\n$x_{<i} \\in \\{0,1\\}^i$ with $i \\in [n]$ to $\\{0, 1\\}$.\nDefinition 3.2. A value function v is optimal if for any CNF-SAT instance p with n variables and any partial assign-\nment $x_{<i}, v(x_{<i}; p) = 1$ iff there exists an extension $y \\in \\{0,1\\}^{n-i}$ such that $Check([x_{<i}, y]; p)$ is True.\nIn our case, the Bellman equation applied on some value function v at some partial assignment $x_{<i}$ is:\n$v(x_{<i};p) = max\\{v([x_{<i}, 0]; p), v([x_{<i}, 1]; p)\\},\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1)$\nwhere $i \\in [n-1]$ and p is the CNF-SAT instance to solve. Given that our setup involves deterministic transitions, this\nexpression does not use expectations. Also, since the objective is binary, we use value functions with a binary output.\nThe Bellman equation must also enforce consistency with Check when evaluating a complete assignment. This addi-\ntional constraint reads $v(x; p) = Check(x;p)$ where x is any complete assignment.\nWe now define Algorithm 1, it iteratively generates candidate solutions guided by values' estimates while simultane-\nously improving these estimates through the Bellman equation.\nThe algorithm starts with an initial set of hypotheses for the value functions, denoted as $V^0$, and iteratively samples\nsequences of decisions to construct candidate solutions, guided by the current set $V^t$. At each step, the Bellman\nequation is applied to eliminate inconsistent value functions from $V^t$, continuing this process until a solution is found.\nThe initial hypothesis set $V^0$ encodes a prior over possible value functions. This prior can incorporate knowledge about\nsolving SAT instances, value functions excluded from $V^0$ reduce the set of possibilities and potentially accelerate the\nsearch. This knowledge may originate from some human-coded prior or from training on other SAT instances."}, {"title": "Counterexamples", "content": "With our algorithm defined, we now present a negative result by constructing examples of hard instances. To do so,\nwe introduce an operation that aggregates multiple CNF-SAT instances. This operation is formally defined as follows:\nDefinition 3.4. Let $p_1,...,p_K$ be CNF-SAT instances over $n_1,...,n_K$ variables, respectively. Additionaly, let\n$I_1, \\ldots, I_K$ be index lists over $n = n_1 + ... + n_K$ variables such that $|I_1| = n_1,\\ldots,|I_K| = n_K$, and all the ele-\nments in these lists are distinct. An aggregation of $p_1, ...,p_K$ with index lists $I_1,..., I_K$ produces a new CNF-SAT\ninstance p over n variables, where all the literals in the clauses are mapped according to the index lists. For example a\nclause $x_i \\lor \\neg x_j$ in $p_k$ becomes $x_{I_{k,i}} \\lor \\neg x_{I_{k,j}}$ in p.\nTo support our result, we also introduce a monotonic property for value functions.\nDefinition 3.5. A value function v is monotonic if, for any CNF-SAT instance p with n variables, any $x \\in \\{0,1\\}^n$,\nand any $i, j \\in [n]$ with $i < j$, the function satisfies: $v(x_{<i>;p) \\geq v(x_{<j};P)$.\nThis property implies that, from any partially constructed solution, each decision can only decrease the value function's\noutput. It is a characteristic of any optimal value function.\nWe now have all the necessary elements to state our result. Theorem 3.6 demonstrates that, under certain assumptions,\nan aggregated problem is computationally intractable for Algorithm 1. The proof leverages the fact that, while the\nBellman equation is sufficient to eventually identify an optimal value function and produce a solution, it does so\ninefficiently in this context.\nImportantly, the Bellman equation relies on the outputs of the value function to learn. When a solution attempt fails,\nthe output 0 of the value function (indicating failure) provides minimal feedback. This lack of informative feedback\nprevents the algorithm from understanding the reasons behind the failure and learning to anticipate similar failures.\nTheorem 3.6 constructs a problem with this difficulty by forcing Algorithm 1 to make several hard independent binary\ndecisions at the start. If any one of these decisions is incorrect, the attempt fails (value 0) and the algorithm struggles\nto attribute the failure to any particular decision.\nA representation of the aggregation operation used in our counterexamples is provided in Figure 1."}, {"title": "Limitation on the Bellman Equation with Universal Value Functions", "content": "Provided a goal to achieve, a common approach in RL is to sample trajectories using some initial exploration policy,\nthen improve the policy by reinforcing behaviors that lead to the goal. However, there is a common pitfall with this\napproach: the goal can be hard to achieve with the initial policy, leading to a lack of feedback to improve the policy.\nTo address this challenge, Hindsight Experience Replay (HER) was introduced (Andrychowicz et al., 2017). HER\ndeviates from traditional RL by learning a universal value function rather than focusing solely on the task-specific\nvalue function. A typical use of universal value functions is to estimate the reachability between states, predicting\nwhether any particular state b can be reached from any other state a. Then to reach a desired goal-state g, actions with\nhigh estimates of reaching g are taken.\nHowever, despite HER's design to create feedback in challenging environments with sparse rewards, it can still over-\nlook critical information in the problems. Moreover, under assumptions, the issue is independent of the prior used\nover universal value functions. Consequently, like classical value functions, improving the prior does not address the\ncore limitation.\nTo prove this limitation, we construct a counterexample similar to the one presented in the previous section. In this\ncounterexample, a search guided by classical value functions learned using the Bellman equation struggles to find a\nsolution. Furthermore, learning a universal value function in this scenario leverages no more feedback than learning a\nclassical value function, leading to the same struggles in finding a solution."}, {"title": "The Algorithm: Hindsight Experience Replay", "content": "As in the previous section, we work with CNF-SAT instances. We model the problem as taking a sequence of n binary\ndecisions with associated states, that incrementally build a candidate solution. To provide a clear goal-state for HER,\nwe introduce a final step with two possible outcomes: True or False. The state True indicates that the constructed\ncandidate satisfies the problem, while False signifies failure. Thus, the target goal-state for HER is g = True.\nThe state space S for a problem p with n variables is defined as $\\{0,1\\}^{[n]} \\cup \\{True, False\\}$, and the set of actions is\nbinary: $\\{0, 1\\}$.\nDefinition 4.1. Given a CNF-SAT instance p with n variables, its dynamics $D^p$ is as follows:\n*   Start in the initial state [].\n*   At each step (up to n), append the chosen binary action to the current state, forming a sequence of actions.\n*   For a state of length n represented by the binary vector x, transition to $Check(x; p)$.\nThe operator $D^p: S \\times \\{0,1\\} \\rightarrow S$ implements that dynamics for instance p, taking a state and a binary action as input\nand outputting the next state.\nWe now define universal value functions estimating state-to-state reachability."}, {"title": "Definitions", "content": "Algorithm 2 Learning universal value functions estimating state-to-state reachability with the Bellman equation\n(Hindsight Experience Replay) for CNF-SAT.\nCandidate solutions are iteratively sampled guided by a hypothesis set V of universal value functions to reach a given\ngoal-state g. Simultaneously, the Bellman equation is enforced on V for pairs of sampled states and g. The sampling\nprocess is performed according to Definition 4.4.\nInputs: $V^0$: an initial set of value functions; p : a CNF-SAT instance to solve with n variables; g : a final goal-state\nto reach\nfor t\u2190 0,1,... do\n$s_0, s_1,..., s_n, s_{n+1} \\sim \\pi_{V^t}(p, g)$\nif $s_{n+1} = g$ then\noutput $s_n$\n$V^{t+1} \\gets V^t$\nfor i \u2190 1,...,n do\n$V^{t+1} \\gets \\{v \\in V^{t+1}|v(s^i, g; p) = max\\{v(D^p(s^i, 0), g; p), v(D^p(s^i, 1), g; p)\\}\\}$\nfor j \u2190 i + 1, ..., n + 1 do\n$V^{t+1} \\gets \\{v \\in V^{t+1}|v(s^i, s^j; p) = max\\{v(D^p(s^i, 0), s^j; p), v(D^p(s^i, 1), s^j; p)\\}\\}$\nDefinition 4.2. A universal value function takes as input a CNF-SAT instance p, two states in S, and outputs a binary\nvalue. Moreover, it satisfies v(s, s; p) = 1 for any state s.\nDefinition 4.3. A universal value function is optimal if for any CNF-SAT instance p with n variables and any $s_1, s_2 \\in\nS, v(s_1, s_2; p) = 1$ iff there exists a sequence of actions from $s_1$ that leads to $s_2$ under the instance's dynamics\n(Definition 4.1).\nAs with classical value functions, the Bellman equation can be enforced to learn optimal universal value functions.\nFor a universal value function v, an instance p, distinct states a and b, and the dynamics operator $D^p$, the Bellman\nequation is:\n$v(a, b; p) = max\\{v(D^p(a, 0), b; p), v(D^p(a, 1), b; p)\\}.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(3)$\nAlgorithm 2 implements this equation. It is initialized with a set of universal value functions representing a prior for\nSAT-solving. The algorithm iteratively samples candidate solutions with this prior while refining it with the Bellman\nequation.\nThere are numerous pairs of states a, b with which Equation 3 can be enforced. Algorithm 2 enforces the equation for\npairs of states sampled in the same sequence and with the goal-state g. This is a key feature of HER that we replicate.\nTo guide the construction of candidate solutions, a policy is derived from the universal value function hypotheses. This\nprocess follows a rule similar to Definition 3.3.\nDefinition 4.4. Given a set V of universal value functions. The policy $\\pi_V$ maps a CNF-SAT instance p with n\nvariables and a goal g to a distribution over sequences of states $s_0,..., s_{n+1}$ in S.\nDefine $V_{s,g,p}$ as the number of universal value functions in V evaluating to 1 for inputs s, g, and p:\n$|\\{v \\in V v(s, g; p) = 1\\}|$.\nThe sequence of states is determined by the dynamics and the actions sampled at each state. The probability of taking\naction 0 at some state s is\n$\\frac{V_{D^p(s,0),g,p}}{V_{D^p(s,0),g,p} + V_{D^p(s,1),g,p}}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(4)$\nand the probability of action 1 is the complementary. If the denominator is zero, both actions are taken with equal\nprobabilities."}, {"title": "Counterexamples", "content": "We now define a monotonicity property for universal value functions, it parallels the one introduced earlier for classical\nvalue functions.\nDefinition 4.5. A universal value function v is monotonic if, for any CNF-SAT instance p with n variables, any state\ns \u2208 S, any $x \\in \\{0,1\\}^n$, and any $i, j \\in [n]$ with $i < j, v$ satisfies $v(x_{<i>, s; p) \\geq v(x_{<j}, s; p)$."}, {"title": "Discussion", "content": "Summary In this paper, we defined two algorithms for problem-solving with learning-based guidance in their search\nprocesses, Algorithms 1 and 2. Both of these algorithms represent an approach of the RL literature, they enforce the\nBellman equation on sampled states, one to learn a value function, the other a universal value function for state-to-state\nreachability (Hindsight Experience Replay).\nTo analyze these algorithms, we constructed counterexample problems that, by design, have a clear structure to ex-\nploit, yet remain provably challenging for the respective algorithms, Theorems 3.6 and 4.6. Our proofs demonstrate\nthat these algorithms struggle because they do not leverage rich feedback from their failed attempts to improve their\nguidance.\nLimitations One limitation of our theoretical analysis is its reliance on counterexamples, which, by nature, are\nspecific problems. This raises the question: do these counterexamples reflect broader limitations on the studied algo-\nrithmic approaches with practical problems?\nWe argue that they do. Our counterexamples and analysis are based on aggregating sub-problems into a larger prob-\nlem, an expected structure in practical applications. Also, although we use SAT instances to build minimal clean\ncounterexamples, our analysis is not tied to this problem, and we believe that the results generalize to other domains,\nsuch as Automated Theorem Proving.\nA second limitation of our theoretical analysis is its reliance on specific algorithmic implementations. While our\nalgorithms are straightforward implementations of each approach, they may inadvertently introduce specific imple-\nmentation choices not part of the original approaches and limit the scope of our findings.\nFor example, our algorithms use Bayesian Learning by iteratively updating a set of hypotheses according to the sam-\npled data. While Bayesian Learning is an idealized framework to describe a priori information and learning, it is not a\ncommon choice in the deep RL literature.\nAnother example of a specific choice in our algorithms is the use of universal value functions to model state-to-state\nreachability. In their original definitions, universal value functions and HER are more flexible, they can be used to\nmodel the reachability of some abstract goals not just future states. This additional flexibility could improve feedback\nand allow to avoid the studied limitation for algorithms developed in that direction.\nConclusion Despite the use of specific algorithms and counterexamples, we believe our findings highlight a broad\npractical limitation on algorithms that rely on the Bellman equation enforced at sampled states. These include model-\nfree Deep RL methods like Deep Q-learning (Mnih et al., 2013) and actor-critic methods (Schulman et al., 2017), as\nwell as some model-based methods like AlphaZero (Silver et al., 2018), all of which depend on the Bellman equation\nto learn to guide the search.\nOur work formalizes inefficiencies in these classical algorithmic ideas of RL. These insights can inform the develop-\nment of novel algorithms better leveraging learning to accelerate search, leading to more effective problem-solving\ntechniques."}, {"title": "Proofs of the Main Theorems", "content": "Theorem 3.6. Let p be a CNF-SAT instance over n variables, constructed by an aggregation of CNF-SAT instances\n$p_1,..., p_K$ using index lists $I_1, ..., I_K, ..., I_K$, where the first element of each $I_k$ is k.\nLet V and $V_1,..., V_K$ represent sets of value functions, and let $v^*$ denote an optimal value function.\nAssume the following:\n1.  The set V factorizes into $V_1, ..., V_K$; that is, v \u2208 V iff there exist $v_1$ \u2208 $V_1$,..., $v_k$ \u2208 $V_k$ such that for all\nx \u2208 $\\{0,1\\}^n$ and i \u2208 [n], $v(x_{<i>; p) = \\prod_{k \u2208 [K]} v_k (x_{I_k \\cap [i]}; p)$.\n2.  For all k \u2208 [K], either $v^*(0; p_k) = 1$ or $v^*(1; p_k) = 1$.\n3.  For all k \u2208 [K] and v \u2208 $V_k$, either $v(0; p_k) = 1$ or $v(1; p_k) = 1$.\n4.  For all k \u2208 [K], if $v^*(0; p_k) = 1$, then $\\pi_{V_k}(x_0 = 0; p_k) \\leq \\pi_{V_k}(x_0 = 1; p_k)$; otherwise, $\\pi_{V_k}(x_0 = 0; p_k) \\geq\n\\pi_{V_k}(x_0 = 1; p_k)$.\n5.  Any v \u2208 V is monotonic.\nUnder these assumptions, Algorithm 1, initialized with $V^0$ = V and p = p, runs for an expected time of at least $2^{K-1}$\nsteps.\nProof. By condition (2) each first variable of all instances $p_1,..., p_k$ must be either 0 or 1 to be a solution. Thus, any\nsolution to instance p must have its first K variables equal to some unique binary vector $y^*$ \u2208 $\\{0,1\\}^K$.\nWe derive an upper bound on the probability of generating this initial vector, then conclude from it the lower bound\non the running time stated in the Theorem.\nWe prove by induction on t (the number of loops performed) that the probability of generating $y^*$ is the smallest inside\nsome set of $2^K t$ binary sequences of length K as long as $y^*$ has not been generated.\nFor t = 0, by (1) and (4), $\\pi_{V^0}$ assigns the lowest probability on $y^*$ over the $2^K$ possibilities.\nAssuming the inductive hypothesis holds at some loop t for some set S of sequence with size $2^K$ \u2013 t and the sequence\n$x^k$ is not $y^*$. We look at the effect of the Bellman equations enforced on the set of value functions with two cases:\ni1) $v(x_{<i>;p) = 0$ then, by the monotonicity assumption (5), $v([x_{<i>, 0]; p) = 0$ and $v([x_{<i>, 1]; p) = 0; second,\nif $v(x_{<i>; p) = 1$ then, by (1) and (3), at least $v([x_{<i>, 0]; p) or $v([x_{<i>, 1]; p) must evaluate to 1. The equation is thus\nsatisfied in both possibilities.\nNow, in the second case, the Bellman equation is applied with i > K, including the condition ensuring consistency\nwith Check. Let v \u2208 $V^t$ be a value function that is removed by the Bellman equation for some i > K. We know\n$v(x_{<K}; p) = 1$ by the monotonicity assumption (5), otherwise the value function evaluates to 0 in all possible contin-\nuations and the Bellman equation is satisfied. Knowing this, by assumptions (1) and (3), the value function outputs 1\nfor $x^k$ and 0 for all the other sequences of length K. Consequently, from Definition 3.3, at most one element of S\ndecreases in probability, and all the others increase by a common normalizing factor. This positive factor preserves the\norder between the sequences' probabilities of being generated among this set, and thus the inductive hypothesis holds\nfor tt + 1.\nThis bound leads to an expected number of loops of at least $2^{K-1}$ to generate $y^*$ and thus a solution."}, {"title": "Positive Result for a Resolution-Based SAT Solver", "content": "Our main text presented a design to produce counterexamples for algorithms based on the Bellman equation approach\nwith classical value functions and universal value functions (HER). In this section, we prove that our design does not\nimply a limitation for SAT solvers based on the resolution operator. For these algorithms, the aggregation structure\ndesigned in our counterexamples does not incur an intractable computational cost. Enforcing the idea that being unable\nto decompose this structure is a limitation.\nWe note that this result applies to an algorithm constrained by a fixed variable assignment ordering, thus under the\nsame context as the negative results for Bellman-based RL.\nWe first define a resolution-based SAT solver, and then provide the positive result.\nOur SAT solver is a minimalistic version of modern conflict-driven clause learning (CDCL) SAT solvers\n(Silva & Sakallah, 1996; Biere et al., 2009; Knuth, 2015). At their core is a procedure that iteratively tries to gen-\nerate a solution, and, simultaneously to these attempts, learns from its failures. This working principle is thus very\nsimilar to RL algorithms, the main difference is found where learning happens. Here, learning consists in deducting\nnew logical formulas with the resolution operator.\nWe define this operator here, it allows us to learn a new clause by deduction.\nDefinition B.1. For some CNF-SAT instance, let $x_i$ be some variable and $l_1, . . ., l_m, l'_1, ..., l'_m$ be some literals. Given\ntwo clauses of the form $x_i \\lor l_1 \\lor ... \\lor l_m$ and $\\neg x_i \\lor l'_1 \\lor ..., l'_{m'}$, the resolution operator produces the new clause\n$l_1 \\lor ... \\lor l_m \\lor l'_1 \\lor ... \\lor l'_{m'}$"}]}