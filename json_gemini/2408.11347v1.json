{"title": "MULTIMODAL DATASETS AND BENCHMARKS FOR REASONING\nABOUT DYNAMIC SPATIO-TEMPORALITY IN EVERYDAY\nENVIRONMENTS", "authors": ["Takanori Ugai", "Kensho Hara", "Shusaku Egami", "Ken Fukuda"], "abstract": "We used a 3D simulator to create artificial video data with standardized annotations, aiming to aid\nin the development of Embodied AI. Our question answering (QA) dataset measures the extent to\nwhich a robot can understand human behavior and the environment in a home setting. Preliminary\nexperiments suggest our dataset is useful in measuring AI's comprehension of daily life.", "sections": [{"title": "Introduction", "content": "As Embodied AI continues to develop, understanding the time and place of actions in daily life becomes increasingly\nimportant [Posner and Fei-Fei(2020), Ahn et al.(2022), Blukis et al.(2022), Paolo et al.(2024), Davis et al.(2010)].\nDatasets and benchmarks have been created to support their development, and challenges have been presented\n[Deitke et al.(2022), Grauman et al.(2022), Ning et al.(2023)].\nMost of this data consists of recorded images of everyday life, annotations, and descriptions. The annotations were\nperformed manually and were imprecise; not everything in the room was annotated. The behavior of what the person\ntries to do needs to be fully described in these descriptions.\nNishimura et al. [Nishimura et al.(2021)] proposed PrimitiveActionOntology\u00b9 to abstract activity labels in recognition\ndatasets based on HomeOntology [Vassiliades et al.(2020)] and International Classification of Functioning, Disability\nand Health (ICF)2. They also proposed a HomeObjectOntology\u00b3 based on VirtualHome assets, objects defined in\nCharades [Sigurdsson et al.(2016)], and objects that occurred in the videos in the video archive called Elderly Behavior\nLibrary4.\nWe created artificial video data (MMDL: Multimodal Dataset of Daily Life) using a 3D VirtualHome-\nAIST [Ugai et al.(2024)] simulator, which is based on VirtualHome [Puig et al.(2018)] and, using Virtual-\nHome2KG [Egam et al.(2023)], created data describing what it is and where it is located for more objects. These data\nalso clarify what the data are from the scripts that are placed in the simulator to make the avatar work. The annotations\nare mechanically generated with a standard vocabulary based on PrimitiveActionOntology and HomeOntology, which\ncontributes significantly to the development of Embodied AI as they are consistent and free of contradictions."}, {"title": "MMDL: Simulation movie and detailed annotation", "content": "We also created a question answering (QA) dataset (MMQADL: Multimodal Question Answering Dataset of Daily\nLife) to measure the extent to which the robot could understand a person's daily life from a video. We offer various\ntypes of descriptive and quantitative questions for question answering (QA) to gather information on location, action,\nobject, time, and more. We also provide location-selective and descriptive QA examples for training and evaluation\ndata.\nThis paper presents the findings of initial experiments conducted using two generative Als, namely Video-\nLLaVa [Lin et al.(2023)] and Google's Gemini 1.5 Pro Vision. These AIs were fed with a combination of images,\nnatural language sentences, and QA that we created. The purpose of this experiment was to investigate AI's under-\nstanding of human behavior in a home environment. The results of the experiment indicate that our dataset is useful in\nmeasuring the AI's comprehension of human behavior and the surrounding environment in a home."}, {"title": "MMQADL: QA dataset for measuring daily life understanding", "content": "QA can pose different types of questions. They can be a choice (Listing 1) or a simple \u201cyes\u201d or \u201cno\u201d answer. The\nquestions were designed to gather information about the location, action, object, time, and combination of topics being\ndiscussed based on TempCompass [Liu et al.(2024)] and MVBench [Li et al.(2023)]. In addition, there are questions\nthat focus on the appropriate caption for a video, which can be either short or long."}, {"title": "Preliminary Experiment", "content": "In the Knowledge Graph Reasoning Challenge 2024, one of the strategies [Hirano et al.(2024)] is to complete the\nannotations provided in the Knowledge Graph from the video. If we can accurately fill in all the missing parts of the\nannotation, we can answer all the questions correctly. We have used video clips and questions about the missing actions,\nlocations, objects, and time as input. We tested the Large Language Models to answer the questions, and Table 1 is the\nresult of our experiment. Overall, Gemini performs well. In particular, the distinction between the four types of rooms\nis mostly accurate. Video-LLaVa, on the other hand, does not understand the time elapsed in the video."}, {"title": "Summary", "content": "This article discusses the creation of a dataset that supports the development of Embodied AI. The dataset includes\nartificial movie data and a QA dataset to measure the AI's comprehension of human behavior in a home environment.\nThe results of the initial experiments show that the dataset is useful for measuring the AI's understanding of human\nbehavior and the surrounding environment in a home. We are planning to organise a technology contest (Challenge) in\nthe future. All data is publicly available from https://github.com/KGRC4SI/DataSet"}]}