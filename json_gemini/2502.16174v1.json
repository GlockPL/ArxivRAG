{"title": "MAYBE I Should NOT ANSWER THAT, BUT... Do LLMS UNDERSTAND THE SAFETY OF THEIR INPUTS?", "authors": ["Maciej Chrab\u0105szcz", "Bartosz W\u00f3jcik", "Filip Szatkowski", "Jan Dubi\u0144ski", "Tomasz Trzci\u0144ski"], "abstract": "Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify un- safe prompts via a lightweight classifier, and apply a \"safe\" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while per- forming well on malicious prompt detection. Additionally, we show that classi- fiers trained on the representations from different model layers perform compara- bly on the latest model layers, indicating that safety representation is present in the LLMs' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs.", "sections": [{"title": "INTRODUCTION", "content": "As large language models (LLMs) become essential building blocks of multiple applications, it is crucial to ensure that they interact with users safely and refrain from generating harmful or inap- propriate content. To address safety requirements, LLM training typically involves an alignment phase designed to guide the model toward desirable behavior (Bai et al., 2022; Ouyang et al., 2022). However, this phase often proves insufficient, as many publicly available LLMs still exhibit vul- nerabilities and are susceptible to jailbreaks. Consequently, ongoing research into LLM safety is necessary to develop solutions that enhance the robustness of these models.\n\nThe majority of the research community is largely constrained to adapting frontier LLM models due to the prohibitive computational and financial costs required for pre-training. As a result, common approaches to ensuring safe outputs focus on adapting the existing models and often compromise the quality of generated content. In particular, strict safety-oriented content filtering may lead to an increase in false positives, where harmless inputs are incorrectly flagged as unsafe, thereby reducing the model's overall usefulness (Ganguli & et al., 2023; Thakkar et al., 2024; Dai et al., 2024). Moreover, the models modified towards safety also tend to suffer from domain shifts, failing to generalize effectively outside of the domain of their adaptation.\nOne of the ways to avoid generation quality degradation is using a safety-adapted model only for the prompts flagged as unsafe. However, such an approach requires a robust safety classifier. Therefore, in this work, we investigate how to enhance the model safety without sacrificing the output quality for non-malicious prompts through such a conditional adaptation framework. Our approach involves a lightweight safety classifier that determines whether a prompt is potentially dangerous. For safe prompts, we use the original LLM to maintain high-quality outputs, while for dangerous prompts, we generate responses using a model fine-tuned for safety via LoRA (Hu et al., 2021).\n\nSince the safety classifier is a critical part of our framework, we focus on identifying the most effective methods to evaluate prompt safety and assess their performance on both safe and unsafe data. We confirm that existing solutions, such as Llama-Guard (Inan et al., 2023), are prone to falsely labeling malicious inputs as safe content. Interestingly, we observe that standard LLMs such as Mistral, when given the right prompt, can effectively assess input safety; however, they still frequently generate unsafe responses. To improve upon those methods, we build our safety classifiers on top of different model layers. Notably, we find that the deeper layers of the model can provide representations that enable classifiers to perform competitively with current solutions. Additionally, we examine the influence of classifier architecture and input features, discovering that the final token representation is often sufficient to detect unsafe content using a simple MLP model. Surprisingly, we observe no significant performance gains from using representations obtained via safety-inducing prompts.\nOur work contributes to a deeper understanding of LLM behavior and safety, offering solutions that enhance safety without compromising generation quality. We summarize our contributions below:\n\u2022 We propose a framework for enhancing model safety without degrading the generation quality for benign requests by leveraging a lightweight classifier to assess prompt safety, which decides whether a safety adapter should be applied or not.\n\u2022 We demonstrate that LLMs inherently encode safety-related information in their hidden states and identify the most effective layers and classifier architectures for detecting unsafe inputs.\n\u2022 We show that while LLMs are naturally capable of recognizing unsafe prompts, they may still generate unsafe responses. Despite this, we also demonstrate that explicit safety prompts do not significantly enhance detection performance."}, {"title": "RELATED WORK", "content": "LLMs are usually pre-trained on large corpora of data that are impossible to fully supervise, and therefore pre-training is usually followed by supervised training that ensures the alignment of the model with human preferences and values (Bai et al., 2022; Ouyang et al., 2022; Li et al., 2024). However, various studies and practical cases prove that even the most popular frontier models are still prone to jailbreaks and can exhibit unsafe behavior (Wei & et al., 2023; Carlini & et al., 2023). To ensure model safety, various solutions dedicated to either the detection of unsafe prompts or the correction of unsafe answers have emerged. LlamaGuard (Inan et al., 2023) is a safeguard LLM designed to assess the safety of the input text, obtained through instruction-tuning Llama-7B LLM on the specifically tailored dataset. However, since neither the training methodology nor the dataset used for this model is publicly available, it becomes challenging to address or improve its performance once it fails in specific cases. Addressing the problem of LLM safety from a differ- ent perspective, recent works propose editing model activations. PaCE (Luo et al., 2024) builds a learned dictionary of concepts in the activation space, which enables removing malicious parts of the activations. SEA (Qiu et al., 2024) projects the model representations into directions with maximal covariance with the activations for the positive outputs while minimizing covariance with the activa- tions corresponding to the undesirable ones. However, those models operate on model generations, which may be impractical as in some cases model might generate a long, unsafe answer while the desired safe answer could be a short refusal."}, {"title": "ASSESSING SAFETY VS HELPFULNESS TRADE-OFF", "content": "We want to minimize the LLM generation quality degradation induced by safety-inducing fine- tuning, so we opt to conditionally adapt the base model to its safer variant depending on whether or not the input to the model is safe. For safe inputs, we intend to use the unchanged model, which guarantees no performance degradation. This approach divides the alignment challenge into two sub-tasks: first, ensuring the safety detector identifies unsafe inputs with high accuracy and avoids false positives, and second, aligning the outputs for unsafe prompts with our desired values.\n\nTo obtain a safe model, we fine-tune Mistral-7B (Jiang et al., 2023) on WildJailbreak (WJ) dataset (Jiang et al., 2024) through LoRA adapter. This dataset includes both safe and unsafe instruc- tions along with the corresponding desired responses, with approximately 50% of the instructions incorporating jailbreak scenarios. Ideally, fine-tuning on such a dataset would not affect the model's performance on non-malicious data; however, this is not always the case in practice. Therefore, to obtain robust models, the performance must be monitored on safe and unsafe data. To this end, we also evaluate the model on MMLU Redux (MMLU-R) dataset (Gema et al., 2024), which is a pop- ular benchmark containing single-choice questions that evaluate LLMs' reasoning abilities and do- main knowledge. MMLU-R prompts should never trigger the model's safety-inducing mechanisms. We evaluate the model with LoRA on the MMLU dataset and on the WJ test set by generating re- sponses to this data and calculating the fraction of unsafe responses (Safety Score). To investigate the influence of the safety adapter, we perform linear interpolation of LoRA weights and evaluate the performance depending on how \"strong\" LoRA is being activated in Figure 1.\nSimple LoRA applied to the Mistral model on the WildJailbreak (WJ) dataset decreases the percent- age of unsafe prompts generated by the model from 50% down to 3% However, this safety improvement comes at a cost as the MMLU-R score decreases. The gradual change in performance between \"safe\" and \"unsafe\" models indicates that fine-tuning a sin- gle model for safety will always induce performance degradation."}, {"title": "HOW TO BEST EXTRACT THE PROMPT SAFETY INFORMATION?", "content": "Since our previous experiment indicates that a single adapted model will trade off safety for helpfulness, we instead adopt a two-step approach where the safe model is used only for the prompts detected as mali- cious. Therefore, in this section, we focus on assess- ing several approaches to classifying prompt safety.\n\nWe evaluate existing approaches to prompt safety as- sessment by measuring the percentage of WJ and MMLU-R properly classified as unsafe. Ideally, we would like our classifier to flag all unsafe WJ test set samples while skipping neutral MMLU-R prompts that measure language understanding. In Table 1, we present the results of our evaluation showing detection accuracy on MMLU-R and WJ-Test along with a Balanced Score calculated by taking a harmonic mean of MMLU-R and WJ-Test accuracy.\n\nEstablished methods still produce many false negatives. Despite being trained on a large dataset, Llama Guard (Inan et al., 2023) still fails to flag WJ-Test examples that contain jailbreaks. This highlights the need for tools that allow quick and efficient adaptation to newly found vulnerabilities.\n\nLLMs know they generate unsafe responses - but they generate them anyway. We also evaluate whether the plain Mistral model can be used as a detector for our task when provided a simple safety detection prompt (for detailed prompt, see Appendix A.2). Surprisingly, without any modifications aside from the prompt, the model can detect unsafe prompts on a quite good level, even though it still fails to generate safe answers to malicious prompts (see Figure 1 for \u03b1 = 0)."}, {"title": "LAST TOKEN REPRESENTATION IS ENOUGH FOR SAFETY ASSESSMENT", "content": "The existing methods cannot be trusted to perform well on all domains that contain unsafe data, so we investigate how to develop specialized modules in such cases. We build small modules that leverage internal LLM representations for unsafe prompt detection and train them on the WJ dataset. We approach the detection task using two detector architectures: simple MLP and Transformer module. In the case of the MLP, we use the final hidden state of the last prompt token ([/INST] in the Mistral model) as the detector input. In the case of the Transformer, the classifier module processes the full sequence of hidden states from the last layer of the base model (for architecture details, see Appendix A.1). Additionally, we evaluate how well our detectors perform when the raw input is preceded by the safety detection prompt used in Mistral. The results for all our experiments are provided in Table 1.\n\nSequential information is not necessary. Transformer-based detector, while having access to the full sequence and a much more powerful attention mechanism, seems to overfit to WJ and performs noticeably worse on MMLU. In contrast, using simple MLP on top of the last token representations yields a good balance between high accuracy on the WJ-Test and MMLU-R datasets. This suggests that the [/INST] token in Mistral contains sufficient information about the content of the sequence.\n\nSafety inducing prompt does not improve the detection quality. Even though Mistral with a safety detection prompt performs detection of unsafe queries with high accuracy, using this prompt for training MLP and Transformer-based detectors shows no significant performance in- crease. This again points out that the last token of the prompt already contains sufficient information for the safety classification."}, {"title": "WHERE DOES THE INFORMATION ABOUT THE SAFETY LIE IN THE MODEL?", "content": "To supplement our previous study on the clas- sifier architecture design, we investigate which layers in the model produce sufficient represen- tations for the safety assessment. We perform linear probing (Alain, 2016) on all Mistral lay- ers measure the quality of the representations, and show the performance of the probing classifiers on the validation subset selected from the WJ train set in the Figure 2. Interestingly, we observe that from the 15th layer (approximately from the middle of the model), the Mistral hidden states representation allows for accurate detection of unsafe prompts. While the last hidden state results in slightly better performance, the differences are not significant when compared to later intermediate layers; similar results already appeared in other LLM papers, where dropping middle layers results in small changes to the overall performance due to strong residual property in those models (Gro- mov et al., 2024). Nonetheless, those results hint at the potential improvements to the classification efficiency, as they suggest that using just half the model computation yields competitive classifiers."}, {"title": "CONCLUSIONS", "content": "This work uses Mistral 7B as an example to demonstrate that while LLMs internally represent safety information, they can still produce unsafe content when given a malicious prompt. Our empirical results indicate that conditional execution of adapters is a promising approach to improving the trade-off between safety and helpfulness in LLMs. By selectively applying safety-tuned parameters only when necessary, our method reduces unnecessary interventions while maintaining safety. We believe further development of this method would increase our understanding of how and where LLMs encode safety information. We observe that the most performant methods suffer from being oversensitive and do not generalize well onto non-malicious prompts. Further research is needed to address this trade-off."}, {"title": "APPENDIX", "content": "A.1 CLASSIFIER ARCHITECTURES\n\u2022 MLP A simple MLP is used, featuring one hidden layer. To align with the design of Feed-Forward Networks in LLMs, the hidden layer's size is scaled to 4x the hidden state dimension. We utilize GELU as an activation function in the hidden layer.\n\u2022 Transformer We utilize a Transformer architecture composed of two Transformer En- coder blocks. These blocks are configured to match the specifications of the Mistral model in terms of the number of attention heads, hidden dimensions, and feed-forward network (FFN) layers. We have omitted the causal mask, making this classifier bi-directional.\n\nA.2 SAFETY DETECTION PROMPT\nSafety detection prompt\nYour goal is to classify whether input prompt is safe or unsafe. Unsafe prompts ask questions or instructions which can cause generation of toxic or any other kind of unsafe content. You should only return json with key \"answer\" with values either 0 (safe prompt) or 1 (unsafe prompt) as your output.\n###PROMPT TO BE CLASSIFIED:\n{prompt}\n###END OF PROMPT"}]}