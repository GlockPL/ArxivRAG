{"title": "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "authors": ["Stefano Perrella", "Lorenzo Proietti", "Pere-Llu\u00eds Huguet Cabot", "Edoardo Barba", "Roberto Navigli"], "abstract": "Machine Translation (MT) evaluation metrics assess translation quality automatically. Recently, researchers have employed MT metrics for various new use cases, such as data filtering and translation re-ranking. However, most MT metrics return assessments as scalar scores that are difficult to interpret, posing a challenge to making informed design choices. Moreover, MT metrics' capabilities have historically been evaluated using correlation with human judgment, which, despite its efficacy, falls short of providing intuitive insights into metric performance, especially in terms of new metric use cases. To address these issues, we introduce an interpretable evaluation framework for MT metrics. Within this framework, we evaluate metrics in two scenarios that serve as proxies for the data filtering and translation re-ranking use cases. Furthermore, by measuring the performance of MT metrics using Precision, Recall, and F-score, we offer clearer insights into their capabilities than correlation with human judgments. Finally, we raise concerns regarding the reliability of manually curated data following the Direct Assessments+Scalar Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with Multidimensional Quality Metrics (MQM) annotations.", "sections": [{"title": "1 Introduction", "content": "Over the past few years, Machine Translation (MT) evaluation metrics have transitioned from heuristic-based to neural-based, enabling a more nuanced evaluation of translation quality and a greater agreement with human judgments (Freitag et al., 2022b). Additionally, recent Metrics Shared Tasks at the Conference on Machine Translation (Mathur et al., 2020b; Freitag et al., 2021b, WMT) have seen the rise of reference-free metrics, which assess translation quality without the need for human-curated references by comparing translations only to their sources in the original language. Lately, reference-free metrics have demonstrated performance on par with, and sometimes superior to, their reference-based counterparts (Freitag et al., 2023; Kocmi et al., 2024a). Thanks to these advancements and the ability to use metrics without references, several new MT metrics use cases have emerged. Freitag et al. (2022a), Fernandes et al. (2022), Farinhas et al. (2023), Ramos et al. (2024), and Finkelstein and Freitag (2024) used MT metrics as utility functions for Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2004; Eikema and Aziz, 2020) and for Quality Estimation (QE) re-ranking.\nRamos et al. (2024), Gulcehre et al. (2023), He et al. (2024), and Xu et al. (2024) used MT metrics as a proxy for human preferences to fine-tune MT systems using Reinforcement Learning (RL)- and Direct Preference Optimization (DPO)-like training objectives. Peter et al. (2023), Alves et al. (2024), and Gulcehre et al. (2023) used reference-free metrics to filter parallel corpora \u2013 discarding all translations assigned with a metric score that is below a certain threshold \u2013 with the goal of training MT systems using higher quality data. These works leverage MT metrics for applications beyond their traditional use of measuring incremental improvements in the development of MT systems. However, the lack of a dedicated evaluation, paired with the inherent opacity of MT metrics, makes it challenging to determine whether one metric suits a given task better and what the impact of various design choices is. For example, Alves et al. (2024), Peter et al. (2023), and Gulcehre et al. (2023) filter MT datasets using different MT metrics and thresholds, leaving it unclear whether an optimal choice exists. Furthermore, considering the ever-increasing number of metrics available, researchers are often limited to grid-searching for the best configuration for each new application, as do Fernandes et al. (2022) and Ramos et al. (2024), who explore by grid-search whether certain metrics are better suited than others for MBR decoding, QE re-ranking, and as reward models for RL-based training. However, the lack of dedicated evaluation setups often requires revisiting these studies to assess whether their findings hold with newer metrics, resulting in a non-negligible increase in experimentation time.\nIn this work, we address these issues by introducing a novel and more interpretable evaluation framework for MT metrics, comprising evaluation setups designed as proxies for new metric use cases. In the following sections, we first illustrate the problem of interpretability, then introduce our framework, and finally present our results."}, {"title": "2 The Interpretability of MT Metrics' Assessments", "content": "In the field of AI, Interpretability is defined as \"the ability to explain or to provide the meaning in understandable terms to a human\u201d (Barredo Arrieta et al., 2020), and typically refers to the problem of understanding the decision-making process of an AI model. However, our goal here is less ambitious. Instead of focusing on the interpretability of MT metrics themselves, we are concerned with the interpretability of their assessments. Specifically, most state-of-the-art MT metrics are trained to minimize the Mean Squared Error (MSE) with human judgments and return assessments as scalar quality scores, which are difficult to interpret. Therefore, we are interested in understanding the meaning of these scores, rather than the internal workings of MT metrics.\nIn light of this, we attribute MT metrics assessments' lack of interpretability to three main factors:\n1. Range consistency: it is unclear whether a difference in metric score has the same meaning if it occurs in different regions of the score range.\n2. Error attribution: scalar quality assessments do not identify specific translation errors.\n3. Performance: metrics capabilities are typically measured through correlation with human judgment, which fails to provide users with a clear understanding of their performance and reliability.\nIn simpler terms, let us consider the example of Figure 1. Due to the lack of Error attribution we do not know which translation errors, if any, led COMET (Rei et al., 2020) to return 0.86. Also, the metric comes with no Range consistency guarantee, e.g. whether 0.86 is twice as good as 0.43. Furthermore, different metrics have different score ranges, making it difficult to compare the assessments from COMET with those of the other MT metrics in the figure. Finally, lacking a clear understanding of COMET's Performance beyond human correlation, we cannot be sure whether we can draw conclusions from its assessments safely.\nFor this reason, some efforts have been made to design interpretable metrics. For example, among the primary submissions to the WMT23 Metrics Shared Task (Freitag et al., 2023), MaTESe (Perrella et al., 2022) annotates the spans of a translation that contain errors, specifying their severity, XCOMET models (Guerreiro et al., 2024) return annotated error spans together with a final regression value, and GEMBA-MQM (Kocmi and Federmann, 2023) leverages GPT-4 (OpenAI et al., 2024) to produce detailed quality assessments. However, these metrics compromise on other aspects to accommodate the increased interpretability. MaTESe displays a lower correlation with human judgment"}, {"title": "3 An Interpretable Evaluation Framework for MT Metrics", "content": "Two popular new MT metrics applications are data filtering and translation re-ranking. In data filtering, MT metrics separate good-quality from poor-quality translations. After choosing a threshold value, all translations below the threshold are labeled as poor quality and discarded. In this respect, we are interested in jointly assessing metric performance and studying the meaning of metric scores, finding the thresholds that best separate good-quality ( GOOD ) from poor-quality ( BAD ) translations. Instead, in translation re-ranking, MT metrics determine the best in a pool of translations of the same source text. For example, in QE re-ranking and MBR decoding, metrics are tasked to identify the best translation among those sampled from an MT system.\nWith the aim of facilitating practitioners in making design choices for these metrics applications, and with a focus on the interpretability issue, we evaluate MT metrics performance in two settings: i) when metrics are used as binary classifiers, tasked to separate between GOOD and BAD translations (acting as a proxy for the data filtering application), and ii) when metrics are used to identify the"}, {"title": "3.1 Metrics as Binary Classifiers for Data Filtering", "content": "Let us consider the MT metric M, which outputs scores in the range [m1, m2]. Let us define M(t) \u2208 [m1, m2] as the score assigned by metric M to translation t. By selecting an arbitrary threshold value \u03c4\u2208 [m1, m2], we repurpose M as a binary classifier: a translation t is deemed as GOOD by metric M, with threshold \u315c, if M(t) > \u03c4, otherwise it is deemed as BAD."}, {"title": "Precision, Recall, and F-score", "content": "Assuming that we have an oracle H telling us whether a translation is GOOD or BAD, we can measure the performance of metric M, with threshold \u03c4, in terms of standard measures such as Precision, Recall, and F-score, which we refer to as P_{M^+}, R_{M^+}, and F_{M^+}. Given metric M, oracle H, translation t, and threshold \u03c4, P_{M^+} estimates the probability that translation t is GOOD, given that metric M deems it as such:\n\n P_{M^+} = Pr(H(t) = GOOD | M(t) > \u03c4). (1)\n\nSimilarly, R_{M^+} estimates the probability that metric M deems translation t as GOOD, given that the oracle deems it as such:\n\n R_{M^+} = Pr(M(t) \u2265 \u0442 | H(t) = GOOD). (2)\n\nFinally, we aggregate Precision and Recall using F_\u03b2-score, with \u03b2 = 5\u221a2, which weights Precision higher than Recall compared to the more common F\u2081-score. Arguably, false positives \u2013 i.e., translations of low quality that are mistakenly considered GOOD - could be detrimental to the applications that see metrics employed as binary classifiers. For example, in data filtering, false positives correspond to low-quality translations that survive the filtering, compromising the quality of filtered data. In contrast, false negatives \u2013 i.e., translations of high quality that are mistakenly assigned with the BAD label \u2013 would more frequently lead to minor inconveniences, as they correspond to good-quality translations that are mistakenly discarded. Moreover, we note that MT metrics struggle to achieve high Precision, meaning that metrics differences can be best highlighted if Precision is weighted higher than Recall."}, {"title": "3.2 Metrics as Utility Functions for Translation Re-Ranking", "content": "Let us consider the set T = {t1, t2, ..., tn} containing translations of the same source text. We are interested in assessing metric performance in ranking the best translation, as determined by human annotators, in the first position. However, metrics and humans might return tied assessments, placing two or more translations together in the first position. Therefore, we define TM as the subset of T containing all translations assigned with the highest score by M. Similarly, TH contains the translations of T ranked highest by human annotators. The Re-Ranking Precision of metric M is defined as follows:\n\n RRP_M = |T_M \u2229 T_H| / |T_M|.(4)\n\nUnlike in the data filtering scenario, we focus solely on Re-Ranking Precision, not Recall. This is because, to serve as a proxy for translation re-ranking applications, what matters is whether the returned translation is the best or among the best \u2013 rather than identifying all the translations ranked highest by human annotators."}, {"title": "4 Experimental Setup", "content": "This section outlines the data employed, the metrics evaluated, and our methodology. Implementation details regarding the calculation of Precision, Recall, and F-score are in Appendix A."}, {"title": "4.1 The Data", "content": "We employ WMT23MQM (Freitag et al., 2023), which contains human annotations collected within the Multidimensional Quality Metrics framework (Lommel et al., 2014, MQM), and WMT23DA+SQM (Kocmi et al., 2023), which includes human annotations as Direct Assessments + Scalar Quality Metrics (Kocmi et al., 2022, DA+SQM). Both datasets consist of source texts translated by multiple MT systems, with translation quality assessed by professional human annotators. Table 4 in the Appendix provides additional information regarding these datasets.\nWe conduct the evaluation using WMT23MQM, specifically the ZH\u2192EN language direction, and use WMT23DA+SQM as a reference for human performance, given that it contains a subset of the translations in WMT23mqm annotated using a different human evaluation scheme. Results concerning the other language directions in WMT23MQM, i.e., EN\u2192DE and HE\u2192EN, are reported in the Appendix."}, {"title": "4.2 The Metrics", "content": "We consider the following metrics: COMET, COMET-QE, and COMET-QE-MQM (Rei et al., 2020, 2021); BLEURT-20 (Sellam et al., 2020; Pu et al., 2021); MetricX-23, MetricX-23-QE, MetricX-23-XL, and MetricX-23-QE-XL (Juraska et al., 2023); CometKiwi and CometKiwi-XL (Rei et al., 2022, 2023a); \u0445COMET-ENSEMBLE and XCOMET-QE-ENSEMBLE (Guerreiro et al., 2024); xCOMET-XL (Guerreiro et al., 2024); MaTESe and MaTESe-QE (Perrella et al., 2022); GEMBA-MQM (Kocmi and Federmann, 2023); MBR-MetricX-QE (Naskar et al., 2023). We refer the reader to Appendix C for detailed information regarding these metrics, where we also provide a broader selection, including lexical-based and sentinel metrics (Perrella et al., 2024).\nAdditionally, following Freitag et al. (2023), we include the results from a random baseline, i.e., Random-sysname, which outputs discrete scores drawn from several Gaussian distributions, one for each MT system that translated the texts in the test set. Each Gaussian has a randomly assigned mean between 0 and 9, with a standard deviation of 2."}, {"title": "4.3 Selecting the Thresholds T", "content": "In the data filtering scenario, we can measure two different aspects of metric performance, depending on how we select the T value:\n1. By selecting 7 to maximize the F-score on the test set, we measure MT metrics' ability to separate GOOD from BAD translations under ideal conditions. This scenario allows us to measure the maximum achievable F-score for each metric on the test data, effectively evaluating the metric's discriminative power. Metrics whose assessments are not accurate enough, noisy, or, more generally, poorly aligned with human judgments, will achieve a lower optimal F-score than the others."}, {"title": "4.4 Extracting Binary Labels from Manually-Annotated Datasets", "content": "Within the MQM annotation framework, professional annotators identify span-level translation errors and assign each error a category and severity. The final MQM score is calculated based on these errors using the following weighting scheme (Freitag et al., 2021a):\nError severity Category Penalty\nNon-translation -25\nMajor -5\nMinor Punctuation -0.1\nOthers Others -1\nWe map the annotations in WMT23mqm and WMT22MQM to binary labels by considering translations with a score above a certain threshold as GOOD. Specifically, if a translation is assigned an MQM score h, we label it as GOOD if h \u2265 \u22124, meaning it contains no Major errors and at most 4 Minor ones (or more if Minor punctuation errors are present). Additionally, we classify translations as PERFECT if they contain at most 1 Minor error, i.e., those with h > \u22121.4. This allows us to investigate metrics' ability to distinguish between PERFECT and OTHER translations."}, {"title": "5 Results", "content": "In this section, we report the performance obtained by MT metrics when used as binary classifiers to distinguish between GOOD and BAD, as well as PERFECT and OTHER translations, and in terms of their effectiveness in translation re-ranking, i.e., in selecting the best translations among candidates for the same source text."}, {"title": "5.1 Binary Classification", "content": "Table 1 shows MT metrics' threshold values, Precision, Recall, and F-score in distinguishing GOOD from BAD and PERFECT from OTHER translations, with the optimal threshold 7 selected on the test set. As can be seen, most MT metrics perform reasonably well in distinguishing between GOOD and BAD translations, achieving optimal F-scores as high as 81.59 and 81.40, from GEMBA-MQM and XCOMET-QE-ENSEMBLE, respectively, and as low as 75.81, from BLEURT-20. Instead, lower performance is observed when differentiating between PERFECT and OTHER translations, with the highest F-score being 68.47, from xCOMET-ENSEMBLE. We also note that Precision is almost always lower than Recall, despite the optimal threshold T being selected to maximize F\u1e9e-score with \u03b2 = 1/\u221a2, which gives more weight to Precision over Recall. These results suggest that the metrics may lack the sensitivity required to distinguish between high-quality translations that differ in minor nuances rather than major errors. As a result, they may resort to lower thresholds, compensating for their lack of Precision with a higher Recall.\nTable 2 reports threshold values, Precision, Recall, and F-score when the threshold is optimized on the development set. Note that we restrict the set of tested metrics to those that are openly available and do not employ the WMT22MQM data for training. As expected, the F-score values are lower than the optimal ones reported in Table 1. Nonetheless, the metric rankings remain stable across the two settings, with MetricX-23-XL and MetricX-23-QE-XL outperforming the other metrics among the reference-based and reference-free ones, respectively.\nIn general, it is worth noting that the best-performing openly available, reference-free metric is MetricX-23-QE-XL. This result is consistent across language pairs (Appendix D). Therefore, we recommend using MetricX-23-QE-XL for data filtering applications."}, {"title": "Thresholds reliability", "content": "Our results show that optimal thresholds tend to vary when moving from WMT23MQM to WMT22MQM for their calculation. For example, MetricX-23-QE-XL's t shifts from -3.57 to -5.45, and from -1.64 to -3.54, when separating between GOOD and BAD and PERFECT and OTHER, respectively, and other metrics display a similar pattern. Optimal threshold"}, {"title": "5.1.1 What is the human performance?", "content": "As a reference for human performance, we examine the agreement between two annotation schemas:"}, {"title": "5.1.2 How BAD are false positives?", "content": "Our analysis suggests that MT metrics struggle to achieve high precision in binary classification. Concerning this, we are interested in assessing how bad the false positives are \u2013 i.e., translations that metrics mislabel as GOOD or PERFECT. To this end, we plot in Figure 2 the distributions of the MQM score \u2206 computed for a false positive as the difference between the MQM score and the human threshold, which is -4 for a GOOD translation and -1 for a PERFECT one.\nThe average false positive \u2206 ranges from -4.25 to -2.85 for both GOOD and PERFECT classifications, indicating that the translations mislabeled by the best metrics contain an average of \u2248 3 additional Minor errors. Overall, the MQM \u2206 distributions of top-performing metrics are low-variance and skewed to the right, particularly when classifying PERFECT translations. In contrast, less accurate metrics exhibit high-variance distributions, with the average \u2206 shifting towards lower values. Again, DA+SQM performance is notably poor, showing the highest-variance distribution and the most leftward-shifted average."}, {"title": "5.2 Translation Re-Ranking", "content": "We present the results in the last two columns of Table 1. To facilitate interpretation of these results, we also calculate the average MQM score of the translations ranked highest by MT metrics, and report it in the last column. Additionally, it is important to note that there are 15 translations per"}, {"title": "6 Related Work", "content": "Previous studies have focused primarily on the problem of Error attribution. Specifically, the Shared Task on Quality Estimation at WMT investigated the ability of MT metrics to predict word-level annotations (Zerva et al., 2022; Blain et al., 2023). Fomicheva et al. (2022a) and Rei et al. (2023b) employed attribution methods to derive explanations for the predictions of MT metrics, measuring the faithfulness of such explanations by comparing them to human annotations. To tackle the same issue, Perrella et al. (2022), Fernandes et al. (2023), Guerreiro et al. (2024), Kocmi and Federmann (2023), and Xu et al. (2023) proposed metrics that address the lack of Error attribution by providing explanations in the form of either span-level annotations or natural language rationales. Furthermore, recent studies have introduced dedicated benchmarks to investigate the impact of specific translation errors, such as disambiguation errors (Campolungo et al., 2022; Martelli et al., 2024) and wrongly translated named entities (Conia et al., 2024).\nIn a different vein, and closer to our work, some studies explored the meaning of raw metrics scores in terms of their alignment with human judgments. Mathur et al. (2020a) studied the meaning of system-level score deltas for BLEU (Papineni et al., 2002), showing that a statistically significant increase of 0-3 BLEU points corresponds to significantly better MT systems less than half of the time, in terms of human judgments. Similarly, Kocmi et al. (2024a) investigated the relationship between MT metrics' system-level score deltas and human"}, {"title": "7 Conclusion", "content": "In this work, we introduce a novel evaluation framework for MT metrics. Within this framework, we measure metrics performance in i) binary classification, i.e., distinguishing between GOOD and BAD, and PERFECT and OTHER translations, and ii) in a proxy scenario for translation re-ranking, selecting the best among the translations of the same source text. By measuring performance in terms of Precision, Recall, and F-score, we fulfill a dual purpose. First, we offer a more intuitive interpretation of metrics' capabilities, as compared to correlation with human judgment, and second, we provide concrete user recommendations concerning novel MT metric use cases. We find that MT metrics perform relatively well in distinguishing between GOOD and BAD translations, but struggle with Precision, especially when dealing with higher-quality translations like in the PERFECT vs OTHER scenario. Our results show that MetricX-23-QE-XL is the best openly available metric for data filtering applications, while MetricX-23-XL and COMET achieve the highest performance in translation re-ranking. Additionally, we demonstrate that reference-based MT metrics, when used as the utility function in an MBR decoding-like scenario, outperform reference-free ones, suggesting that MBR decoding may be superior to QE re-ranking. Finally, we report notably poor performance for DA+SQM annotations used as a metric within our evaluation framework, raising concerns about its reliability."}, {"title": "8 Limitations", "content": "Language coverage We acknowledge that the scope of our work is limited by the available test data, covering only a few language directions. However, our evaluation framework is agnostic to the test data employed. Therefore, we leave the investigation of metric performance in more language directions to future works, depending on the availability of new annotated datasets.\nDesign choices in the data filtering scenario We made certain arbitrary decisions in the design of our framework and experimental setup. We chose FB-score to select the optimal threshold \u03c4, with \u03b2 = 1/\u221a2. While we explained our reasons for giving Precision a higher weight than Recall, it remains unclear whether \u03b2 = 1/\u221a2 is the optimal choice. Furthermore, we selected the human score thresholds to be -4, for GOOD translations, and -1 for PERFECT ones. We recognize that practitioners might have different requirements and may want to narrow or broaden these definitions. Therefore, we release our evaluation framework leaving this as an option for users.\nEvaluation fairness in the data filtering scenario In one of the two setups proposed, we selected the threshold 7 to maximize the F-score on the test set used for the evaluation. This optimization process might favor metrics whose assessments are more sensitive to the underlying gold score distribution, enabling them to achieve a better balance between Precision and Recall. As a result, discrete metrics \u2013 i.e., those that output scores within a discrete set, such as the integers in [-25, 0] for GEMBA-MQM \u2013 might be disadvantaged compared to continuous metrics \u2013 i.e., those that output scores within a continuous interval, such as the real values in [0,1] for metrics of the COMET family. However, we argue that this limitation is inherent to the nature of discrete metrics rather than a flaw in our evaluation framework. Indeed, studying the ability of MT metrics to distinguish between GOOD and BAD translations requires identifying the score threshold that best separates them, and discrete metrics inherently offer a much more limited set of options for optimizing this threshold. Nonetheless, if discrete metrics are indeed disadvantaged, using a development set could mitigate the impact of this phenomenon.\nAlignment between the translation re-ranking scenario and the corresponding metric use cases We designed the translation re-ranking scenario as a proxy for QE re-ranking and MBR decoding. However, our setup differs from these two use cases in two ways:\n1. Candidates number: The test datasets we used feature 15, 12, and 13 translations per source text, for ZHEN, EN DE, and HE\u2192EN, respectively. However, in QE re-ranking and MBR decoding it is common to work with a larger number of candidate translations, often reaching hundreds per source text.\n2. Candidates selection: In QE re-ranking and MBR decoding, candidate translations are typically sampled from the same MT system. In contrast, in our annotated datasets, each candidate translation was generated by a different MT system.\nIn future work, it would be interesting to investigate whether our results might vary when dealing with a higher number of candidate translations or when all candidates are sampled from the same MT system."}]}