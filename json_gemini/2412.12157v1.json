{"title": "What Makes In-context Learning Effective for Mathematical Reasoning: A Theoretical Analysis", "authors": ["Jiayu Liu", "Zhenya Huang", "Chaokun Wang", "Xunpeng Huang", "Chengxiang Zhai", "Enhong Chen"], "abstract": "Owing to the capability of in-context learning, large language models (LLMs) have shown impressive performance across diverse mathematical reasoning benchmarks. However, we find that few-shot demonstrations can sometimes bring negative performance and their effectiveness on LLMs' reasoning abilities remains unreliable. To this end, in this paper, we aim to theoretically analyze the impact of in-context demonstrations on LLMs' reasoning performance. We prove that the reasoning efficacy (measured by empirical prediction loss) can be bounded by a LLM-oriented semantic similarity and an inference stability of demonstrations, which is general for both one-shot and few-shot scenarios. Based on this finding, we propose a straightforward, generalizable, and low-complexity demonstration selection method named LMS3. It can adaptively facilitate to select the most pertinent samples for different LLMs and includes a novel demonstration rejection mechanism to automatically filter out samples that are unsuitable for few-shot learning. Through experiments on three representative benchmarks, two LLM backbones, and multiple few-shot settings, we verify that our LMS3 has superiority and achieves consistent improvements on all datasets, which existing methods have been unable to accomplish.", "sections": [{"title": "Introduction", "content": "Mathematical reasoning is a critical task and serves as a milestone in assessing the progress of artificial intelligence (Zhang, Wang et al. 2020; Liu et al. 2023). Currently, many large language models (LLMs) have exhibited strong performance across various mathematical reasoning benchmarks (Hendrycks et al. 2021; Cobbe et al. 2021). A key capability of these LLMs is in-context learning (ICL) (Dong et al. 2022), which enables them to learn from a few examples to implement specific logical structures (Wei et al. 2022) or utilize codes (Chen et al. 2023) to improve reasoning accuracy. Based on this ability, they can adeptly address a wide variety of problems across different types and difficulty, ranging from elementary word problems to college-level algebra (Brown et al. 2020; Achiam et al. 2023).\nHowever, it remains an unresolved issue whether in-context learning truly enhances LLMs' mathematical reasoning abilities. To show this phenomenon, in Figure 1,"}, {"title": "Related Work", "content": "Mathematical Reasoning. Mathematical reasoning is a critical benchmark for assessing the level of artificial intelligence (Zhang, Wang et al. 2020; Liu et al. 2023). Early work in this area mainly focused on rule-based, template-based, and statistical machine learning methods for simple math word problems (Feigenbaum, Feldman et al. 1963; Fletcher 1985). With the development of large language models (LLMs), current mathematical reasoning work primarily falls into two categories. The first category improves the mathematical reasoning capabilities of general LLMs through techniques such as prompt engineering. On one hand, they endow LLMs with chain-like (Kojima et al. 2022), tree-like (Yao et al. 2024), or graph-like (Besta et al. 2024) reasoning processes, or require LLMs to generate code (Chen et al. 2023; Gao et al. 2023) to address potential numerical computation errors. On the other hand, they also involve providing the model with certain examples in the prompts through retrieval-augmented generation (Wei et al. 2022; Asai et al. 2024), allowing the model to solve problems based on similar approaches using its contextual learning abilities. The second category is to fine-tune a specific mathematical LLM using mathematical problem-solving data (Lewkowycz et al. 2022; Yue et al. 2024). This type of work addresses both the diversity of mathematical problems (e.g., range from elementary to university-level difficulties (Hendrycks et al. 2021), cover various types (Trinh et al. 2024), rephrase original corpus (Yu et al. 2024)) and the problem-solving process itself (e.g., supervise the training with the reasoning steps, rather than relying solely on the final answers (Lightman et al. 2023; Luo et al. 2023a)).\nIn-context Learning. In-context Learning (ICL) focuses on making LLMs learn and reason based on existing examples (Dong et al. 2022). Its advantage lies in the adaptability and flexibility for different tasks and scenarios. However, the selection of examples remain a central challenge, where current researches have developed supervised methods and unsupervised methods. This paper focuses on unsupervised methods, which can be grouped into three main categories. The first and currently most prominent method is called Similar-ICL (Liu et al. 2022; Luo et al. 2023b; Zhang et al. 2023; Fu et al. 2022), which aims to find examples with closest semantic representations to the test sample. The semantic representation approaches include TF-IDF, BM25 (Robertson, Zaragoza et al. 2009), T5 encoding (Raffel et al. 2020), BGE-M3 (Chen et al. 2024), OpenAI embedding, etc. The second line of methods calculate the impact of each demonstration on the test sample (Peng et al. 2024). Impact calculation approaches include influence function (Van, Wu et al. 2024; Chang and Jia 2023), mutual information (Sorensen et al. 2022), perplexity (Gonen et al. 2023), code-length (Wu et al. 2023), etc. The third category uses the feedback from LLMs to dynamically select demonstrations (Nguyen and Wong 2023; Qin et al. 2023). Regarding the underlying mechanisms of ICL, most existing research explored the impact of empirical factors such as the number of examples, gold labels, diversity, and types of LLMs from an experimental perspective (Pan et al. 2023; Peng et al. 2024; Min et al. 2022). Some theoretical explorations explain ICL from perspectives including meta-gradient updates (Dai et al. 2023), kernel regression (Han et al. 2023), and token reinforcement (Yan et al. 2024). In comparison, to the best of our knowledge, we are the first to theoretically quantify the impact of demonstrations on reasoning performance and identify when they are effective."}, {"title": "Theoretical Analysis", "content": "Notations. In in-context learning (ICL) setup, we have a demonstration pool $D$ and a test set $D_{test}$, which contain $M$ and $N$ mathematical problems respectively. The k-shot in-context learning is formulated as appending k demonstrations $\\{(X_1,Y_1), (X_2,Y_2), ..., (X_k, Y_k)\\} \\subseteq D$ with the test data $X_{test} \\in D_{test}$ in prompt to reason the solution\n$\\hat{y}_{test} \\stackrel{\\text{def}}{=} LLM((X_1,Y_1), (X_2,Y_2), ..., (X_k, Y_k), X_{test}),$ (1)"}, {"title": "", "content": "where $X_i, X_{test}$ represent the problem context and $y_i$ represents the labeled solution. The prediction loss on $X_{test}$ is denoted as $L(X_{test}, Y_{test})$. In the following, we omit the symbol $y$ and use $X$ to express each demonstration for brevity. To evaluate the influence of a demonstration $X$ on inferencing the answer of $X_{test}$, we use $h, h_{test} \\in \\mathbb{R}^d$ to denote the representation of problem $X$ and $X_{test}$. Then, the Transformer attention mechanism in ICL setting is denoted as:\n$F_{ICL}(h_{test}) = Attn(V, K, Q, h_{test})$\n$= \\frac{W_v [h, h_{test}]}{\\sqrt{d}} \\cdot softmax\\left(\\frac{(W_K [h, h_{test}])^T \\cdot W_Q h_{test}}{\\sqrt{d}}\\right),$ (2)\nwhere $W_Q, W_K, W_V$ are the projection matrices for computing the attention queries, keys, and values, respectively. Without loss of generality, we omit $W_Q$ in $F_{ICL}(h_{test})$ because we can redefine $W_K = W_K W_Q$. As a result, we only keep $W_K \\in \\mathbb{R}^{d \\times d}, W_V \\in \\mathbb{R}^{d' \\times d}$ in our setting, where $d'$ is the output dimension of layer $F_{ICL}$. Following Dai et al. (2023), we approximate the attention to linear attention by removing the softmax function:\n$F_{ICL}(h_{test})\\approx \\frac{W_v [h, h_{test}]}{\\sqrt{d}} \\cdot \\left(W_K[h, h_{test}]\\right)^T \\cdot h_{test}$\n$\\approx \\frac{W_v}{\\sqrt{d}}h(W_Kh)^T h_{test} + \\frac{W_v}{\\sqrt{d}}h_{test} (W_Kh_{test})^T h_{test}.$(3)\nAnalogy to Linear Optimization. We start our analysis of Eq. (3) by considering a linear function $F(z) \\stackrel{\\text{def}}{=} W \\cdot z, W \\in \\mathbb{R}^{d'\\times d}, z \\in \\mathbb{R}^{d}$. Specifically, given $F(z)$ with an initialized parameters $W_0$, assume we have a training data $z_0 \\in \\mathbb{R}^{d}$, then the gradient of loss $L(F)$ can be written as $\\Delta W = \\nabla F_{L(z_0, W_0)} \\cdot z$. Applying the gradient to parameter optimization, the prediction of a test sample $h_{test}$ is $F(h_{test}) = W_0h_{test} + \\nabla F_{L(z_0, W_0)}.zh_{test}$ Based on this idea, Eq. (3) can be interpreted as: 1) We have a linear function $F(z)$ with initialized parameters\n$W_0 = \\frac{W_v}{\\sqrt{d}}, h_{test} (W_Kh_{test})^T.$ (4)\n2) We introduce a training data $z_0 = W_Kh$ to optimize the parameters, with the gradient at $(z_0, W_0)$ satisfies:\n$\\nabla F_{L(z_0, W_0)} = \\frac{W_v}{\\sqrt{d}}h.$ (5)\n3) We finally apply the optimized parameters to calculate the result of test data $h_{test} \\in D_{test}$.\nUnder this setting, we aim to estimate the influence of the data $z_0 = W_Kh$ (corresponds to demonstration $X \\in D$) on loss $L(F(h_{test}))$. Before detailed derivation, we first give three mathematical annotations:\n$W \\stackrel{\\text{def}}{=} \\text{argmin}_W \\frac{1}{|D_{pre}|}\\sum_{z\\in D_{pre}}L(F(z))$\n$W_{\\epsilon, z_0} \\stackrel{\\text{def}}{=} \\text{argmin}_W \\frac{1}{|D_{pre}|}\\sum_{z\\in D_{pre}}L(F(z)) + \\epsilon \\cdot L(F(z_0))$ (6)\n$H_W \\stackrel{\\text{def}}{=} \\frac{1}{|D_{pre}|} \\sum_{z\\in D_{pre}} \\nabla^2_{WL}(z, \\widetilde{W}),$\nwhere $D_{pre}$ is the data for pre-training a LLM, and $H_W$ is the Hessian matrix which is positive definite by assumption (Van, Wu et al. 2024). It is worth noting that the pre-trained parameters $\\widetilde{W}$ are actually the initialized parameters"}, {"title": "", "content": "in our above setting, i.e., $W = W_0$. Taking $\\epsilon = \\frac{1}{|D_{pre}|}$, the testing loss on $h_{test}$ is represented as $L(h_{test}, W_{\\frac{1}{|D_{pre}|}, z_0})$.\nOn this basis, we derive the following theorem:\nTheorem 1. Assume $\\nabla F_L$ is Lipschitz continuous w.r.t F with constant $\\mu$. If inequality (7) holds true, then $L(h_{test}, W_{\\frac{1}{|D_{pre}|}, z_0}) < L(h_{test}, W_0, z_0)$, i.e., introducing the training sample $z_0$ (i.e., demonstration X) can reduce the testing loss on $h_{test}, \\lambda_{Add'}$ are the largest and smallest eigenvalues of $H_W$ respectively.\n$\\frac{\\lambda_{Add'}}{\\lambda_1} \\frac{1}{\\sqrt{d}}|\\frac{W_v}{}\\nabla_wL(h_{test}, W)| > ||h_{test} - z_0||(||h|| + \\mu C_1)$\n$C_1 = ||\\frac{W_v}{\\sqrt{d}}h_{test} || || W_Kh_{test} || ||h_{test} || $(7)\nWe refer the readers to Appendix 1 for the detailed proof and explanations, and present the sketch here.\nProof. With $W, W_{\\epsilon, z_0}$, the influence of upweighting $z_0$ on the empirical loss is (Ling 1984; Koh and Liang 2017):\n$I_{loss}(z) = \\frac{dL(h_{test}, W_{\\epsilon, z_0})}{d\\epsilon}|_{\\epsilon=0} = -\\nabla_wL(h_{test}, \\widetilde{W}). H\\nabla_wL(z_0, \\widetilde{W}).$ (8)\nThen, the testing loss $L(h_{test}, W_{\\frac{1}{|D_{pre}|}, z_0})$ can be evaluated by Taylor approximation since $\\frac{\\epsilon}{|D_{pre}|}$ is sufficiently small:\n$L(h_{test}, W_{\\frac{1}{|D_{pre}|}, z_0}) = L(h_{test}, W_0, z_0) + \\frac{1}{|D_{pre}|} \\nabla_WL(h_{test}, W). H\\nabla_wL(z_0, \\widetilde{W}).$(9)\nTherefore, now the question turns to evaluate\n$L_1 \\stackrel{\\text{def}}{=} (\\nabla_WL(h_{test}, W)^T. H\\nabla_wL(z_0, \\widetilde{W})$\n$= (\\nabla_wL(z_0, \\widetilde{W}) - \\nabla_WL(h_{test}, W)). H\\nabla_wL(h_{test}, \\widetilde{W})$\n$\\stackrel{L_{11}}{+} \\nabla_wL(h_{test}, W). H\\nabla_wL(h_{test}, \\widetilde{W})$\n$\\qquad\\qquad L_{12}$ (10)\nSince $H_W$ is positive definite, we denote $\\lambda_1 \\geq \\lambda_2 \\geq ... > \\lambda_{Add'} > 0$ are the eigenvalues of $H^{-1}_W$ and can prove that\n$L_{11} \\geq -\\lambda_1|\\nabla_wL(h_{test}, W)| \\cdot (|\\nabla F_{L(h_{test}, \\widetilde{W})} - \\nabla F_{L(z_0, \\widetilde{W})}|$\n$||h_{test}|| + ||\\nabla F_{L(z_0, \\widetilde{W})} || \\cdot ||h_{test} - z_0||),$ (11)\nSince $\\nabla F_L$ is Lipschitz continuous, we get $L_{11} \\geq -$\\n$\\lambda_1|\\nabla_wL(h_{test}, W) || (\\mu||\\widetilde{W}(h_{test} - z_0)|||h_{test}||+$\n$||\\nabla F_{L(z_0, \\widetilde{W})}||\\cdot ||h_{test} - z_0||)$ (12)\nApplying Eqs. (4) and (5) into Eq. (12), we have:\n$||\\widetilde{W}(h_{test} - z_0) || \\leq ||\\frac{W_v}{\\sqrt{d}}h_{test} ||\\cdot || W_Kh_{test} || \\cdot ||h_{test} - z_0||$ (13)"}, {"title": "", "content": "$||\\nabla F_{L(z_0, \\widetilde{W})} ||\\cdot ||h_{test} - z_0|| = ||\\frac{W_v}{\\sqrt{d}}h || ||h_{test} - z_0|| $(14)\nFor $L_{12}$, we similarly prove that:\n$L_{12} = \\sum \\lambda_i \\beta_i \\geq \\lambda_{Add'} ||\\nabla_wL(h_{test}, \\widetilde{W}) ||^2 $ (15)\nCombining Eqs. (12)-(15), we finally get:\n$L_1 >\\lambda_{Add'} ||\\nabla_wL(h_{test}, \\widetilde{W})||^2 - \\lambda_1 ||\\nabla_wL(h_{test}, \\widetilde{W})|| \\cdot (\\mu \\cdot C_1$\n$\\cdot ||h_{test} - z_0|| . ||h_{test} || + \\frac{1}{\\sqrt{d}}||h|| \\cdot ||h_{test} - z_0||) $. (16)\nAccording to Eq. (7), the right-hand side of Eq. (16) is greater than 0, which leads to the conclusion. \nExtension to k-shot setting. In Theorem 1, we only consider one demonstration X (i.e., the one-shot scenario). For the k-shot scenario, Eq (3) can be written as\n$F_{ICL}(h_{test}) \\approx \\frac{W_v}{\\sqrt{d}}h_{test} (W_Kh_{test})^T \\cdot h_{test}$\n$\\approx \\frac{W_v}{\\sqrt{d}} \\sum_{i=1}^k h_i (W_Kh_i)^T h_{test},$(17)\nwhere $h_1, ..., h_k$ are the representations of demonstrations $X_1, ..., X_k$. This formalization can be interpreted as introducing k training samples $z_1 = W_Kh_1, ..., z_k = W_Kh_k$ to optimize the linear function $F(z)$ simultaneously, where the gradient at each training sample $z_i$ satisfies\n$\\nabla F_{L(z_i, W_0)} = \\frac{W_v}{\\sqrt{d}}h_i. $(18)\nSimilar to the proof of Theorem 1, we derive the following Theorem 2 to illustrate the condition of these samples to ensure a reduction in the loss of testing data $X_{test}$, where\n$W_{\\epsilon, z_k} \\stackrel{\\text{def}}{=} \\text{argmin}_W \\frac{1}{|D_{pre}|}\\sum_{z\\in D_{pre}}L(F(z)) + \\epsilon \\cdot \\sum_{i=1}^k L(F(z_i))$ (19)\nTheorem 2. Assume $\\nabla F_L$ is Lipschitz continuous w.r.t F with constant $\\mu$. If inequality (20) holds true, then $L(h_{test}, W_{\\epsilon, z_k}) < L(h_{test}, W_0, z_k)$, i.e., introducing training samples $\\{z_1, ..., z_k\\}$ (i.e., demonstrations $X_1, ..., X_k$) can reduce the testing loss on $h_{test}$.\n$\\frac{\\lambda_{Add'}}{\\lambda_1} \\frac{1}{\\sqrt{d}}|\\frac{W_v}{}\\nabla_wL(h_{test}, \\widetilde{W})|| > \\sum_{i=1}^k||h_{test} - z_i||(||h_i|| + \\mu C_1)$ (20)\nTheorem 2 further indicates that the joint effect of different demonstrations follows an additive relationship. This implies that the selection of k different demonstrations can be approximately considered independently."}, {"title": "LMS3: Method Design", "content": "Based on Section 3, an ideal demonstration X needs to maximize the value of $L_1$ (i.e., minimize the empirical testing loss $L(h_{test}, W_{\\epsilon, z_0})$ in Eq. (9)). This is equivalent to minimize the right-hind side of Eq. (7) according to Eq. (16) and can be further divided into: 1) minimize the value\n$Sim(X) \\stackrel{\\text{def}}{=} ||h_{test} - \\frac{W_v}{\\sqrt{d}} W_Kh||,$ (21)"}, {"title": "", "content": "(recall $z_0 = W_Kh$ and $W_K$ is indeed $\\widetilde{W} \\cdot W_0$ in the aforementioned section), and 2) minimize the value\n$Stab(X) \\stackrel{\\text{def}}{=} ||\\frac{W_v}{\\sqrt{d}}h||.$ (22)\nSpecifically, $Sim(X)$ reflects a LLM-oriented Semantic Similarity between the demonstration X and the test data $X_{test}$. It goes beyond traditional methods by taking into account 1) the whole reasoning path of demonstrations (recall X includes both the problem context and the solution) and 2) the characteristics of the inference LLM itself, which is more consistent with intuition. The value of $Stab(X)$ is an evaluation of the Inference Stability of Demonstration X. Based on Eq (5), $Stab(X)$ is indeed the length of gradient of the loss function on X. If $Stab(X)$ is low, it indicates that the LLM has reached a stable prediction with minimal loss on X, and the parameters will not be excessively altered due to the introduction of this sample.\nSince it is hard to simultaneously achieve the minimum of $Sim(X)$ and $Stab(X)$, two intuitive approximations are to minimize a demonstration scoring function that calculates their sum or product as follows:\n$Score(X) = Sim(X) + \\lambda_1 \\cdot Stab(X),$ (23)\n$Score(X) = Sim(X) \\cdot Stab(X),$ (24)\nHowever, Eq. (23) requires considering the scale differences between the two objectives and adjusting the hyperparameter $\\lambda_1$ based on different LLMs and datasets, which is challenging to apply in practice. Therefore, we prefer Eq. (24) as the basic form of our scoring function. To implement k-shot in-context learning, we can select the top k samples with the highest Score(X) as demonstrations according to Theorem 2, which can ensure that the most relevant and stable samples are used to enhance the LLM's performance.\nFurthermore, we design a demonstration rejection mechanism, which is essential but has not yet been fully explored. For instance, it is possible that the examples with the highest Score(X) still do not satisfy Eq. (20). In such cases, unlike existing methods that always select top k examples, we tend to refuse to provide any demonstration and instead use a zero-shot approach, because our theorems suggests that providing examples in this case will have a negative effect. We control $Sim(X)$ to achieve this rejection mechanism, because if an example's $Sim(X)$ is already too large, $Sim(X) \\cdot \\mu C_1$ might have exceeded the left-hand side of Eq. (7). However, setting an absolute threshold for $Sim(X)$ is challenging since $\\mu$, $C_1$ is unknown, and calculating the gradient norm $||\\nabla_w L(h_{test}, \\widetilde{W})||$ is costly. Therefore, we adopt a simplified relative threshold. We expect that the $Sim(X)$ of an ideal demonstration should be as small as possible relative to all examples. Consequently, we rank $Sim(X)$ of all candidate examples. If a demonstration X ranked top-k in Score(X) does not have a $Sim(X)$ value within the top $\\Lambda$ smallest, we reject to select it.\nTheoretically, to compute Score(X), we need to input the concatenation of each \"(demonstration, testing data)\" pair (X, Xtest) into the LLM to obtain their semantic representations h, htest. However, in practice, this process requires"}, {"title": "Experiments", "content": "5.1 Experimental Setup\nDatasets. We use three datasets that cover a variety of types and difficulty levels. MAWPS (Koncel-Kedziorski et al. 2016) consists of 2,373 elementary-level math word problems. GSM8K (Cobbe et al. 2021) is composed of 8,792 high-quality, more challenging elementary math problems with a higher number of steps. MATH (Hendrycks et al. 2021) is collected from high school math competition, containing 12,500 problems across seven categories such as algebra, geometry, and number theory, and is currently one of the most widely used benchmarks. Dataset partition and statistics are presented in Appendix 3.\nBaselines. We use Llama2-13B (Touvron et al. 2023) and Llama3-8B (Meta 2024) as the backbones to validate our method (please see Appendix 4 for implementation details) and take 10 representative and SOTA baselines including:\n\u2022 Random randomly selects demonstrations from D.\n\u2022 Best-validate tests the performance of each data on a validation set, selecting the one with the highest accuracy, and some typical Similar-ICL methods:\n\u2022 TF-IDF represents each problem as a TF-IDF vector, and selects the nearest sample to the test data.\n\u2022 BM25 (Robertson, Zaragoza et al. 2009) selects demonstrations by retrieval method BM25.\n\u2022 T5 (Raffel et al. 2020) encodes problems with T5-large model and selects the most similar one.\n\u2022 BGEM3 (Chen et al. 2024) integrate multiple information retrieval functionalities in a unified embedding."}, {"title": "", "content": ""}]}