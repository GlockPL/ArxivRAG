{"title": "Generative AI-based Pipeline Architecture for Increasing Training Efficiency in Intelligent Weed Control Systems", "authors": ["Sourav Modak", "Anthony Stein"], "abstract": "In automated crop protection tasks such as weed control, disease diagnosis, and pest monitoring, deep learning has demonstrated significant potential. However, these advanced models rely heavily on high-quality, diverse datasets, which are often scarce and costly to obtain in agricultural settings. Traditional data augmentation techniques, while useful for increasing the volume of the dataset, often fail to capture the real-world variability and conditions needed for robust model training. In this paper, we present a new approach for generating synthetic images for improved training of deep learning-based object detection models in the context of intelligent weed control. The presented approach is designed to improve the data efficiency of the model training process. The architecture of our GenAI-based image generation pipeline integrates the Segment Anything Model (SAM) for zero-shot domain adaptation with a text-to-image Stable Diffusion Model, enabling the creation of synthetic images that can accurately reflect the idiosyncratic properties and appearances of a variety of real-world conditions. We further assess the application of these synthetic datasets on edge devices by evaluating state-of-the-art lightweight YOLO models, measuring data efficiency by comparing mAP50 and mAP50-95 scores among different proportions of real and synthetic training data. Incorporating these synthetic datasets into the training process has been found to result in notable improvements in terms of data efficiency. For instance, most YOLO models that are trained on a dataset consisting of 10% synthetic images and 90% real-world images typically demonstrate superior scores on mAP50 and mAP50-95 metrics compared to those trained solely on real-world images. This shows that our approach not only reduces the reliance on large real-world datasets but at the same time also enhances the models' predictive performance. The integration of this approach opens opportunities for achieving continual self-improvement of perception modules in intelligent technical systems.", "sections": [{"title": "1. Introduction", "content": "In the transformation towards more sustainable agriculture, the adoption of smart technology for crop protection is crucial to minimize the application of pesticides. Here, deep learning (DL)-based algorithms embedded in intelligent agricultural technology systems emerge as a promising frontier in the area of automated crop protection [1]. DL facilitates these tasks through a range of computer vision techniques, including image classification, object detection, and segmentation. A vast amount of data, however, is a prerequisite for the satisfactory performance of DL models.\nIn agricultural scenarios, high-quality labeled open-source datasets reflecting the heterogeneity present, for instance, in the various field conditions are yet scarce. Moreover, collecting and annotating images for new datasets is a highly labor-intensive and expensive process [2].To alleviate this data scarcity issue, data augmentation is a popular tool in DL to increase both the volume of and variation from the available datasets [3]. Different approaches to data augmentation exist. Classical operations such as flipping, rotating, and blurring of images are straightforward to implement and use but lack the important property of introducing real variability into the training data. This is, however, important to yield robustly working models generalizing well to unseen scenes. Recent approaches such as those proposed in [4] deal with generating artificial training data. We provide a brief overview in Section 2.\nThis article expands upon our previous work presented at the Architecture of Computing Systems (ARCS) conference [5], in which we developed a novel pipeline architecture for synthetic image generation adapted to weed detection, with applicability to other object detection tasks. Our approach combined the zero-shot transfer technique with the Segment Anything Model (SAM) [6] and the Stable Diffusion Model [7]. This integration aimed to generate synthetic image data that not only mimics the inherent style of real"}, {"title": "2. Background", "content": "This section Provides a brief overview of image augmentation (Sect. 2.1), the Segment Anything Model (SAM) (Sect. 2.2), diffusion models (Sect. 2.3), as well as the prominent YOLO models for object detection tasks (Sect. 2.4)."}, {"title": "2.1. Image Augmentation", "content": "Data augmentation methods in computer vision are categorized into model-free and model-based approaches [9]. Despite their effectiveness in downstream tasks, model-free techniques, such as blurring, occlusion, noise addition, and viewpoint changes, lack fidelity and natural variation [9]. In contrast, model-based methods, employing generative AI such as GANs [10], VAEs [11], and Diffusion Models (DMs) [12] offer greater natural variations and fidelity. Among these generative AI approaches, DMs showed superior image quality in comparison with the VAEs and GANs [13]. Despite their efficacy, DMs remain less utilized in agriculture. Recent studies however demonstrate their effectiveness in weed dataset augmentation [14]. Moreover, diffusion models have been found effective for augmenting plant disease datasets [15]. Furthermore, a recent study found that using synthetic images generated by the Stable Diffusion Model for image augmentation improved YOLO model performance in weed detection compared to traditional augmentation methods [16]."}, {"title": "2.2. Segment Anything Model (SAM)", "content": "SAM is a \u201czero-shot transfer\u201d method, which generally allows the segmentation of any object without the need for additional training or fine-tuning. It is trained on one billion image masks and 11 million images. SAM consists of three components, i.e., an image encoder, a prompt encoder, and a mask decoder. The image encoder is realized by a Vision Transformer (ViT), pre-trained by a Mask autoencoder (MAE) method. The prompt encoder has two types of acceptable prompts, sparse, e.g., points, boxes, and text, and dense e.g., masks. In the inference stage, SAM can operate in a \u201cManual Prompting\" and a \"Fully Automatic\" mode. For the former, manually created texts, boxes, or points are provided as a hint (i.e., conditioning) to predict the image masks. In the latter, Fully Automatic mode, SAM predicts the masks from the input image without any further conditioning input. SAM is available in three pre-trained model variants: ViT-B, ViT-L, and ViT-H, with 91M, 308M, and 636M learning parameters (neural network weights) respectively. ViT-H considerably outperforms the other models [6]. For weed detection in agriculture, SAM has accelerated annotation tasks, such as weed segmentation via bounding box and point inputs [17]."}, {"title": "2.3. Diffusion Models (DMs)", "content": "Amongst generative models, diffusion models are at the forefront and capable of producing realistic images, and high-quality data [18]. Inspired by non-equilibrium statistics, the diffusion process involves the use of two Markov chains, namely forward diffusion and reverse diffusion [12]. In the forward phase 1, each input image $x_0$ undergoes iterative transformation using forward transition kernels $F_t$ parameterized by noise levels $\\sigma_t$, leading to the formation of a prior distribution $F(x_0, \\sigma)$. This process involves the composition of transition kernels over time t, resulting in the gradual refinement of the distribution. Conversely, the reverse diffusion phase 2, reverses this transformation by iteratively applying so-called reverse transition kernels $R_t$ in backward order, guided by noise levels $\\sigma_t$. This phase aims to reconstruct the original images $x_T$ from the prior distribution, thus completing the reversal of the forward diffusion process.\n$F(x_0, \\sigma) = F_T(x_{T-1}, \\sigma_T) \\circ ... \\circ F_t(x_{t-1}, \\sigma_t) \\circ ... \\circ F_1(x_0, \\sigma_1)$ (1)\n$R(x_T, \\sigma) = R_1(x_1, \\sigma_1) \\circ ... \\circ R_t(x_t, \\sigma_t) \\circ ... \\circ R_T(x_T, \\sigma_T)$ (2)\nDuring inference time, new images are generated by the gradual reconstruction from white random noise, parameterized by a deep neural network [19], typically a U-Net [20]. In contrast to other diffusion models, so-called latent diffusion models (LDMs), as introduced by Rombach et al. (2022) [7], minimize computational costs and complexity by leveraging the latent space of a pre-trained autoencoder rather than operating within the pixel space. The training is divided into two phases: Firstly, an autoencoder is trained to create an efficient, lower-dimensional representation of the data or image space. Unlike previous methods, DMs are trained in this learned latent space, resulting in better scalability concerning spatial dimensions. LDMs thus allow for efficient image generation with a single network pass. Particularly, the autoencoding stage needs to be trained only once, enabling its reuse across multiple DM training or its transfer to different tasks. Additionally, this approach is extended to incorporate transformers into the DM's UNet backbone, facilitating various token-based conditioning mechanisms for image-to-image tasks [7]. The so-called Stable Diffusion Model, a text-to-image-based LDM, has been developed by researchers from CompVis, Stability AI, and LAION and was trained on the LAION-5B [21] dataset, the largest freely accessible multi-modal dataset, containing text-image pairs. The output image can be controlled by the prompted text through a classifier-free guidance [22] mechanism, ensuring Stable Diffusion for precise manipulation of desired visual attributes and generation of high-fidelity images. However, large text-to-image models lack the ability to replicate visual characteristics from reference sets and generate diverse interpretations [23]. For dealing with this issue, Ruiz et al. introduced Dreambooth [23], a few-shot fine-tuning method for personalizing text-to-image models, addressing subject-specific user needs. This involves embedding a pair of unique identifiers and subject classes, such as \u201ca HoPla Sugarbeet\u201d, into the text-to-image diffusion model's dictionary. Consequently, the model is enabled to learn the specific subject associated with the unique identifier, simplifying the need for verbose image descriptions. Even more importantly, utilizing the unique identifier, the model can learn to mimic the style of the input images. Thus, during the inference stage, an image can be produced by a descriptive text prompt, such as \u201ca [unique identifier] [subject classes] [context description]\". Using this unique approach, the subject can be placed in different backgrounds with realistic integration, including e.g., shadows and reflections. As we describe in the subsequent Section 3, we also make use of these various methods and integrate them into a pipeline architecture for the generation of synthetic high-fidelity weed images for data-efficient training of robust weed detection models.\""}, {"title": "2.4. You Only Look Once (YOLO) Models", "content": "In the domain of real-time object detection, YOLO models have gained significant traction due to their impressive speed and efficiency. YOLO approaches the object detection task as a regression problem by dividing the input image into an S \u00d7 S grid and predicting bounding boxes (B) and class probabilities (C) in a single pass. Each prediction consists of five regression values: $P_c$ (confidence score), $b_x$ and $b_y$ (center coordinates of the bounding box), and $b_h$ and $b_w$ (dimensions of the bounding box). The output is an $S \\times S \\times (B \\times 5 + C)$ tensor, which is further refined using non-maximum suppression (NMS) to eliminate duplicate detections [8].\nYOLO models are evaluated using commonly used metrics for assessing the performance of object detection models, such as precision, recall, F1 score, mAP50, and mAP50-95. Precision measures the proportion of correctly identified positive instances (see eq. 3); in the context of weed detection, high precision means that when the model identifies a weed, it is likely correct.\n$Precision = \\frac{True Positives}{True Positives + False Positives}$ (3)\nRecall measures the proportion of actual positives that are correctly detected (see eq. 4), in case of weed detection indicating the model's ability to identify all present weeds. Moreover, a high recall value in weed detection indicates that the model can effectively detect high proportions of actual weeds from the dataset [24].\n$Recall = \\frac{True Positives}{True Positives + False Negatives}$ (4)\nThe F1 score is calculated to provide a single metric that balances both precision and recall, representing the overall effectiveness of the detection model (see eq. 5).\n$F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$ (5)\nIntersection over Union (IoU) is an essential metric for object localization in detection tasks, quantifying the overlap between predicted and ground truth bounding boxes. Average Precision (AP) computes the area under the precision-recall curve, providing an overall measure of the model's performance, while Mean Average Precision (mAP) extends this concept by averaging the precision across all object classes. mAP50 calculates mAP at an IoU threshold of 0.50, while mAP50-95 computes mAP across various IoU thresholds from 0.50 to 0.95. For comprehensive performance evaluation with reduced localization error, mAP50-95 is typically preferred.\nYOLO models have been employed in a wide range of fields, including autonomous driving, medical applications, and autonomous weed, crop, and pest detection [25]. The latest models in the YOLO series, such as YOLOv8, YOLOv9, and YOLOv10 have further expanded the applicability and versatility of the approach in numerous domains in real-time detection [8]. The YOLO series comprises various models including nano, small, medium, large, and extra large, tailored to different hardware capabilities. Due to the limited resources of edge devices such as the Raspberry Pi and NVIDIA Jetson, DL models with a reduced number of parameters are often chosen for these platforms. Thus, for deployment on edge devices, the nano model is selected for highly resource-limited settings, and the small YOLO model is favored to maintain a balance between speed and accuracy. In terms of latency and mAP50-95 score, the YOLOv10 nano and small variants outperformed the corresponding variants of other state-of-the-art YOLO models (cf. Tab. 1).\nDifferent versions of YOLO nano and its modified variants have demonstrated their potential in various agricultural use cases, such as detection of color changes in ripening melons [26], real-time apple fruit detection [27], monitoring the stages of cabbage head harvest stages [28], detecting small strawberries [29], detection of weeds in sugar beet fields [30], [16]."}, {"title": "3. Methodological Approach", "content": "We consider the agricultural use case of weed detection a prerequisite for every smart spraying or autonomous weed control system (e.g., weeding robots). Accordingly, an image processing pipeline for generating synthetic images of weed-infested soil areas is developed, which is depicted in Figure 1. Our approach integrates the unique capabilities of the Segment Anything Model (SAM) and a generative Stable Diffusion Model. We proceed by describing the dataset from a current research project, the sensor used to collect it, as well as the resulting data modality. Subsequently, we detail the first phase of our synthetic image generation pipeline, which we call the data transformation phase. One crucial step in this phase is to leverage the universal segmentation feature of the foundation model SAM when applied to pre-annotated training data. For this initial paper, we assumed that the collected training data needs to be human-annotated. The subsequent image generation phase employing the Stable Diffusion Model is described afterward. After generating synthetic images, we employed a fine-tuned YOLOv8x model for label prediction and annotating these images. Later, the synthetic images were evaluated using no-reference IQA metrics against real-world images. In addition, we used a strategic substitution of real-world images with synthetic ones to evaluate the data efficiency in training the nano and small versions of the YOLOv8, YOLOv9, and YOLOv10 models.\nWe utilized an NVIDIA A100-SXM4-40GB GPU with 40 GB of VRAM and allocated 4 CPU cores and 12 GB of system memory on an AMD EPYC 75F3 32-core Processor for all stages of the Stable Diffusion training, image, and label generation, as well as YOLO models training and evaluation."}, {"title": "3.1. Dataset", "content": "The dataset was collected in the scope of a current research project (see Acknowledgement 6) at an experimental site in Rhineland Palatinate, Germany. The industrial camera sensor was attached to a herbicide sprayer mounted on a tractor which was operating at a speed of 1.5ms\u00af\u00b9. Within the field camera unit (FCU), a 2.3-megapixel RGB camera sensor with an Effective Focal Length (EFL) of 6mm was employed. This camera sensor was equipped with a dual-band filter tailored for capturing RED and near-infrared (NIR) wavelengths. The multiple FCUs attached at the sprayer's linkages maintained a constant height of 1.1 meters above the ground at a 25-degree off-vertical angle. The land machine carrying the camera sensors moves along a controlled, outdoor experimental setup where different crops and weeds are grown under various soil conditions in boxes built on euro pallets marked accordingly to identify the different weed and soil types. This method has been chosen to obtain well-balanced datasets for training robust weed detectors. Subsequently, pseudo-RGB images were derived from the raw RED and NIR bands after performing projection correction. The dataset was then manually labeled by domain expert annotators. The dataset comprises 2074 images primarily featuring sugar beet as the main crop class, alongside four weed classes: Cirsium, Convolvulus, Fallopia, and Echinochloa. The images have a resolution of 1752 \u00d7 1064 pixels. As exemplarily shown in Figure 2, our dataset exhibits distorted and unusual features. Most prominently the images are characterized by showing misaligned pallet frames, off-centered boxes, blurred pixels, and very small weeds. Despite that, we chose to utilize this dataset for our experiments due to the following reason: Such idiosyncratic features are commonly encountered in real-world settings and our objective is to demonstrate the robustness of our initial method regarding its general ability to capture and generate such idiosyncrasies effectively. In future research, we will extend our experiments by using several datasets."}, {"title": "3.2. Dataset Transformation", "content": "The original dataset was initially annotated for object detection tasks (step a) in Fig. 1). However, to refine the delineation of plants and weeds and eliminate unwanted backgrounds, we transformed bounding boxes into complex polygons using the Segment Anything Model (SAM). This conversion effectively converted the object detection dataset into an instance segmentation dataset, i.e., containing segmentation masks of the individual plants within an image. Given the complexity of the agricultural dataset and SAM's tendency for over-segmentation, achieving \u201cFully Automatic"}, {"title": "3.3. Image Generation", "content": "The image generation phase of the proposed pipeline comprises two main steps: (1) fine-tuning a Stable Diffusion Model and (2) text-to-image inference (cf. step d) in Fig. 1). Pursuing the creation of a diverse synthetic dataset, while still preserving important subject features (plants & weeds shapes), we choose subject-specific training to fine-tune the Stable Diffusion 1.5 model, employing the previously mentioned technique Dreambooth (cf. Sect. 2.3). This involves training the Stable Diffusion Model with embeddings representing \u201cHoPla,\u201d an Unique Identifier, and various Subject Classes, including Sugar beet, Cirsium, Echinchloa, Fallopia, Convolvulus, and the background plots. An example prompt would look like this: \u201cA Photo of HoPla Sugar beet with HoPla Fallopia on HoPla Plot\u201d. Although Dreambooth was originally designed to train one subject at a time, our dataset comprises five distinct plant/weed classes and a background soil class. Leveraging the diffusers library [32], we implemented a process called multi-subject Dreambooth to simultaneously train the model on multiple subject classes. We performed the fine-tuning of the Stable Diffusion Model by varying the model conditioning prompts, with a maximum of 60000 training steps. Additionally, we employed a batch size of 1 and utilized gradient checkpointing and accumulation techniques to enhance memory efficiency. Furthermore, we trained a text encoder to incorporate the Unique identifier and Subject classes into the diffusion model.\nAfter completing the fine-tuning of the generative model by the above-mentioned method, the inference stage offers controllability and customization regarding particular image generation through explicit prompting. This allows the optimization of different parameters, such as text prompt description, number of denoising steps, image resolutions, and guidance scale according to the user's needs. In this stage, we utilized the diffusion pipeline by making use of the diffusers library [32] once again. In our case, we generated images by optimizing various text prompts, number of inference steps, and guidance scale variants to assess image generation quality, inference time, and the alignment of the text prompt with the generated image, respectively, according to our requirements. The text prompt represents a typical description of images, while the number of inference steps indicates the number of iterative diffusion steps required to generate an image from the noise tensor. Generally, a higher number of inference steps leads to higher quality and better-detailed images, as it involves more iterations to reduce noise and enhance details [32]. The choice of the number of inference steps depends on factors such as desired image quality, available computational resources, and specific dataset characteristics. Finding the optimal number of inference steps depends on the practitioner's requirements and the computational resources available. In our research, we found that using 50 inference steps best suited our needs and hardware setup. The guidance scale determines the linkage of generated images to the text prompt. The default guidance scale is 7.5, however, the higher the value of guidance scale, the higher the generated images associated with the text prompt [32]. In our experiment, we kept the default setting of the guidance scale. Moreover, the quality and characteristics of generated images can be controlled by different schedulers. These schedulers control how denoising steps progress over time, influencing image quality and fidelity. As with other parameters in the pipeline, practitioners can independently experiment with various schedulers based on their needs, as there are no quantitative metrics available for evaluation. In our use cases, we have observed that the Euler Ancestral Discrete Scheduler consistently generated the desired quality of images in 20 - 30 denoising steps."}, {"title": "3.4. Label Generation", "content": "During the image generation phase (see Fig. 1(step d)), we initially categorized the weeds into four distinct species: Cirsium, Convolvulus, Fallopia, and Echinochloa. However, in the subsequent label generation and weed detection phase, we reclassified these species into two broader botanical categories to enhance practical applicability: dicotyledons (Cirsium, Convolvulus, and Fallopia) and monocotyledons (Echinochloa). This reclassification was performed to better align with the herbicides available on the market, which are designed to target specific botanical categories rather than individual species [33].\nWe fine-tuned a pre-trained YOLOV8x model, originally trained on the COCO dataset [34], using our real-world sugar beet dataset with three classes: Sugar beet, Monocotyledons (Monocot), and Dicotyledons (Dicot). The fine-tuned weight from the YOLOV8x model was utilized to annotate our synthetic images through an automated, model-guided annotation technique. This enhancement further optimizes our GenAI-based pipeline architecture for intelligent weed control systems, substantially reducing the time, cost, and labor associated with the annotation process."}, {"title": "3.5. Evaluation", "content": "The pursued goal of generating synthetic images is to augment the training database and to test the data-efficient training by replacing real-world images with synthetic images for a certain downstream task; in our case, object detection in an agricultural weed control setting. Due to its numerously demonstrated superior detection accuracy and precision, the state-of-the-art object detection model YOLO models [8] and its variants have been utilized in our study. Since YOLO models operate with a resolution of 640 \u00d7 640, we set the resolution of the synthetic images at 640 \u00d7 640. Since Stable Diffusion is a text-to-image model architecture, we used various text prompts to evaluate the weed diversity, fidelity, and relation to the real environment. To be able to address common data issues such as 'class imbalance' and 'lack of diversity', we split our image generation goal into two modes: fixed weed class and random generation. For instance, we prompted our model for the former fixed weed class with prompts such as \u2018A Photo of HoPla Echinochloa, HoPla Plot in the Background', and for the latter random generation case with \u2018A photo of random plants and weeds, HoPla Plot in the Background'. Samples of synthetic images from the fixed weed class, and for random generation are depicted in Figures 3, and 4, respectively."}, {"title": "4. Result", "content": "To quantitatively evaluate the quality of our synthetic images in comparison to real-world images", "follows": "brightness (0.385 \u00b1 0.186), noisiness (0.985 \u00b1 0.0184), and sharpness (0.958 \u00b1 0.052) (cf. Figure 7). Of particular interest are the natural and realism scores, where interestingly it can be observed, that synthetic images outscore real images. The mean natural and realism scores of synthetic images evaluate (0.417 \u00b1 0.174) and (0.946 \u00b1 0.058) respectively, surpassing their real-world counterparts, which score (0.369\u00b10.153) and (0.916 \u00b1 0.078) respectively. This would indicate a higher fidelity of the synthetic images. However, the interpretation of this observation is not straightforward. Since these metrics are no-reference metrics, and thus are calculated independently and not in pairs of a synthetic image with a reference real image, a conclusion that the synthetic images in our scenario have a higher fidelity than real images in general cannot be drawn. Additionally, the complexity score of synthetic images (0.187\u00b10.0916) aligns closely with that of real images (0.182\u00b10.0884) (cf. Figure 7). Following the statistical test, the results indicate significant differences in the brightness, noisiness, sharpness, natural, and realism values with p-values < 0.05; only the complexity values showed no significant differences (p > 0.05).\nTo evaluate the effectiveness of synthetic images in downstream models, we trained several versions of YOLO nano (YOLOv8n,YOLOv9t, YOLOv10n) and YOLO small (YOLOv8s, YOLOv9s, YOLOv10s) across a range of datasets containing both real and mixed (real and synthetic) data. The trained models were tested using real-world data to evaluate their performance in real-world environments and assess their data efficiency.\nThe mAP50 scores for the various YOLO models (see Tab. 3) trained on real and mixed datasets reveal clear trends, highlighting the impact of synthetic data integration on model performance. Introducing a small proportion of synthetic data generally improved model performance. For most models, the Syn10 Real90 dataset produced the highest mAP50 scores, suggesting that adding 10% synthetic data can enhance performance. For example, YOLOv8n achieved a mean mAP50 score of 0.883\u00b10.007 with 10% synthetic images, showing no significant differences from its performance in real data only (p > 0.05). This indicates that introducing 10% synthetic data does not negatively impact the model's ability to detect real-world data. However, as the proportion of synthetic data increased beyond 10%, YOLOv8n began to show declines in performance. Its performance dropped significantly on the Syn40 Real60 dataset, with a mean mAP50 score of 0.854 \u00b1 0.008.\nModels, such as YOLOv9s, effectively integrated synthetic data at moderate levels without a significant decline in performance. For example, YOLOv9s achieved a mean mAP50 score of 0.887 \u00b1 0.009 with 30% synthetic data, with no significant difference compared to its performance in real data only (p > 0.05). This demonstrates the model's ability to incorporate synthetic data effectively up to a certain threshold without experiencing a significant accuracy loss.\nWhile most models showed performance declines at higher synthetic data levels, the YOLOv10 model family exhibited high resilience to larger proportions of synthetic data. YOLOv10n maintained a mean mAP50 score of 0.732 \u00b1 0.027 with 90% synthetic data, showing no significant difference compared to real data only (p > 0.05). Similarly, YOLOv10s maintained strong baseline performance even with up to 80% synthetic data, achieving a mean mAP50 score of 0.790 \u00b1 0.019 without significant differences from real data alone. Furthermore, YOLOv10s outperformed other data set combinations in the Syn10 Real90 dataset, achieving a mean mAP50 score of 0.859 \u00b1 0.013, which was statistically superior to performance only with real data (p < 0.05).\nIn contrast, some models experienced significant performance drops as the proportion of synthetic data increased. For instance, the performance of YOLOv8s dropped significantly to 0.877 \u00b1 0.008 with just 20% synthetic data compared to its real data only0.892 \u00b1 0.000 counterpart (p < 0.05). A similar pattern was observed with YOLOv9t, where its performance dropped significantly (p < 0.05) from 0.902 \u00b10.000 with real data to 0.889 \u00b1 0.008 with just 10% synthetic data.\nTurning to the mAP50-95 metrics for the different YOLO models (see Tab. 4), a pattern similar to the mAP50 scores emerges, with the inclusion of a small proportion of synthetic data that generally does not hamper and in some cases improve model performance. Most YOLO models achieved their highest average mAP50-95 scores on datasets with 10% synthetic images (Syn10 Real90). For example, YOLOv8n achieved an observed mean mAP50-95 score of 0.713 \u00b1 0.006 on the Syn10 Real90 dataset, which was marginally higher than its performance on real data alone (0.687 \u00b1 0.000); however, this difference was not statistically significant (p > 0.05). Similarly, YOLOv8s exhibited a positive trend, with a mean mAP50-95 increasing from 0.691\u00b10.000 on real data to 0.730\u00b10.007 with 10% synthetic data, though this increase was also non-significant (p > 0.05). YOLOv9t followed a similar pattern with 10% of synthetic data, increasing from only real data 0.697\u00b10.000 to 0.714 \u00b1 0.007, but this difference was also not statistically significant (p > 0.05).\nIn contrast, YOLOv9s demonstrated a statistically significant performance improvement with the addition of 10% synthetic data, achieving a mean mAP50-95 increase from 0.684\u00b10.000 on real data alone to 0.735\u00b10.007 (p < 0.05). Similarly, YOLOv10s exhibited a statistically significant improvement in mAP50-95, increasing from 0.601 \u00b1 0.000 on real images to 0.699 \u00b1 0.018 with 10% synthetic data (p < 0.05).\nBeyond 10% synthetic data, most YOLO models maintained performance comparable to their real-data counterparts, even with moderate increases in synthetic data. For example, YOLOv8n performed similarly with the 30% of synthetic data with no statistically significant difference compared to the trained only with real data (p > 0.05). Similarly, YOLOv8s and YOLOv9t maintained strong performance with 50% and 20% synthetic data, respectively, with no significant difference from those trained with only real data (p > 0.05).\nYOLOv9s exhibited impressive resilience to increasing proportions of synthetic data, showing no statistically significant performance decline even when trained with up to 90% synthetic data compared to training exclusively on real data (p > 0.05). YOLOv9s achieved its highest mean mAP50-95 score of 0.735\u00b10.007 with 10% synthetic data and maintained similar performance up to 50% synthetic data, with no statistically significant differences observed (p > 0.05). Similarly, both YOLOv10n and YOLOv10 demonstrated strong resilience in synthetic data, maintaining performance consistent with baseline levels when trained with up to 90% synthetic data, without statistically significant differences from baseline with only real data (p > 0.05). Besides, when trained with 20% synthetic data, YOLOv10n significantly outperformed the baseline performance, achieving a mean mAP50-95 score of 0.640 \u00b1 0.018, compared to 0.590 \u00b10.000 for the model trained solely on real data (p < 0.05). Moreover, it maintained consistent performance up to 70% synthetic data, with no statistically significant differences observed the highest score with only 20% of synthetic data (p > 0.05). Similarly, YOLOv10s demonstrated strong adaptability to synthetic data, surpassing their performance with only real data. Specifically, YOLOv10s achieved a mean mAP50-95 score of 0.699 \u00b1 0.018 with 10% synthetic data, compared to 0.601 \u00b1 0.000 with real"}]}