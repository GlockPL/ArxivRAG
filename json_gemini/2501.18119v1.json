{"title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models", "authors": ["Qika Lin", "Tianzhe Zhao", "Kai He", "Zhen Peng", "Fangzhi Xu", "Ling Huang", "Jingying Ma", "Mengling Feng"], "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (i.e., tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as LLaMA (Touvron et al., 2023a,b) and GPT-4 (OpenAI, 2023), are initiating considerable transformations within the fields of artificial intelligence (AI) and natural language processing (NLP). They have achieved substantial success (Peng et al., 2023; Wang et al., 2024; Xu et al., 2024b), and thus, have been regarded as potential pathways towards achieving the ultimate goal of artificial general intelligence (Yang et al., 2024a). However, the specific training strategies employed by LLMs render them black-box models and struggle to retrieve the relevant facts necessary for the correct answer (Pan et al., 2024), resulting in low performance in complex reasoning scenarios (Xu et al., 2025, 2024a).\nFurthermore, knowledge hallucination becomes a serious issue, which may generate wrong statements that conflict with reality (Bang et al., 2023; Ji et al., 2023). It presents considerable risks, particularly in specialized fields like law (Cui et al., 2023) and healthcare (Lin et al., 2025; He et al., 2025).\nKnowledge Graphs (KGs), also known as knowledge bases, organizes massive amounts of factual knowledge in a structured and interpretable manner by the triple form of (subject, relation, object). They can serve as a vital supplement to LLMs (Pan et al., 2024), providing an alternative way to address hallucinations and generate more precise answers using continual fine-tuning (Zhang et al., 2024b; Hron et al., 2024) or retrieve-based reasoning (Sun et al., 2024; Tan et al., 2024; Zhang et al., 2024a). However, the KGs' structure is in a graph form, which markedly differs from the discrete token format of the natural language in LLMs. Thus, due to the presence of this natural representation gap, the effective integration of comprehensive structural information of KGs with LLMs has emerged as a significant question.\nAs shown in Figure 1 (a), one straightforward method involves converting relevant triples into textual prompts and then feeding them into LLMs, combined with semantic text. This simple strategy would necessitate a substantial number of tokens, causing an enormous resource burden. Supposing the average degree of an entity is d, the number of its neighbors grows exponentially and reaches dh in the h-hop. While certain sampling strategies such as random walk (Ko et al., 2024) and path pruning (Tan et al., 2024) have been introduced, a considerable computational load also exists. As shown in Figure 2, when only sample 20% 2-hop neighbors in FB15k-237 (Toutanova and Chen, 2015) dataset, the median and mean number of neighbors for entities are about 10 and 107, which requires median and mean tokens of about 300 and 3K, respectively. When with 30% sampling, even the median needed tokens reach about 2.5K per entity. Considering that KG tasks may involve multiple entities, even the most advanced long-context LLMs may face challenges in handling them. Meanwhile, employing KGs' substructures through sampling could disrupt the holistic modeling of the entire graph, potentially resulting in information loss and sub-optimal performance for downstream tasks.\nAnother alternate strategy involves integrating continuous KG embedding with LLMs by a learnable adapter (Zhang et al., 2024b), introducing new networks in the framework. It requires additional precise alignment between the different latent representation spaces of KG embeddings and LLMs. Considering the above context, we aim to explore the potential to bridge the natural gap between KG structure and natural language and then integrate KGs with LLMs. Inspired by the early fusion strategy in multimodal LLMs (Team, 2024), the general idea of this study is to learn compressed and discrete entity codes (i.e., tokens), rather than continuous embeddings, by quantized techniques to represent holistic structural and semantic information of entities in KGs. They have the same discrete form of natural language, e.g., the quantized codes in Figure 1 (b) align the format of language sentences. Thus, seamlessly integrating KGs with LLMs can be realized by directly inputting the learned codes into LLMs, merely requiring an expansion of the LLMs' tokenizer vocabulary and eliminating the need for any other framework modifications.\nAlthough several studies have conducted quantized representations on KGs (Galkin et al., 2022; Chen et al., 2023; Li et al., 2023), they universally employ an unsupervised approach to select anchors to represent entities, failing to the holistic structural and semantic modeling. In this study, we first introduce a self-supervised quantized representation for KGs, aiming to learn discrete codes for each entity that can reconstruct KG structures and align with semantic texts. A graph convolutional network (GCN) is used as an encoder to model neighbor structures of KGs, and vector quantization (Van Den Oord et al., 2017) is implemented for the KG quantized representation learning. Further, based on learned entity codes, we construct specific instructions for KG tasks, which can be seamlessly integrated with LLMs, presenting a new paradigm to employ LLMs in KG applications. In summary, our contributions lie in the following three folds:\n\u2022 We propose a self-supervised quantized representation (SSQR) method that is capable of acquiring both KG structural and semantic knowledge. To our knowledge, this is the first study for KG quantization learning in a self-supervised manner.\n\u2022 We propose the first study that utilizes the derived codes to seamlessly integrate KGs with LLMs, which is achieved by viewing codes as input features and designing KG instruction-following data. It has extensive potential applications, e.g., KG link prediction and triple classification.\n\u2022 From the experiment view, SSQR exhibits superior performance compared to current unsupervised quantized methods and the learned codes are more distinguishable. Besides, using only 16 codes for each entity, the fine-tuned LLaMA2 and LLaMA3.1 have superior performance on KG link prediction and triple classification tasks."}, {"title": "2 Quantized Representation for KGs", "content": "Formally, a KG can be represented as $G = \\{E, R, F\\}$, which is the combination of entities E, relations R, and triples $F\u2286 E \u00d7 R \u00d7 E$. Each triple is in the form of (h, r, t). For each entity e, it has the structural and semantic information, where we utilize the entity neighbors N(e) and its textual description Te to describe, respectively. Although here we only use one-order neighbors N(e)"}, {"title": "3 Tuning LLMs with SSQR", "content": "Employing the quantized representation, each entity in KGs can be illustrated by codes of length N. This can be perceived as the same form of natural language, thereby facilitating its seamless integration with LLMs. Every learned code can serve as a new token, necessitating only an expansion of the token vocabulary within the LLM's tokenizer."}, {"title": "4 Experiments and Analysis", "content": "To verify the effectiveness of the proposed SSQR and its ability to integrate with LLMs, We carry out experiments on the KG link prediction and triple classification tasks, where the popular datasets WN18RR (Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015) as well as FB15k-237N (Lv et al., 2022) are utilized. For SSQR, a 2-layer GCN is utilized as the encoder. \u03b2 is set to 0.25 in the experiment. The embedding dimension"}, {"title": "4.1 SSQR Results", "content": "We compare the performance of our SSQR with three unsupervised methods, i.e., NodePiece (Galkin et al., 2022), EARL (Chen et al., 2023), and random entity quantization (RandomEQ for short) (Li et al., 2023), for KG quantized representations. The results are given in Table 2.\nAs can be observed, SSQR achieves significant performance improvement against baselines, which has 9.28% and 7.84% improvements compared with the previous optimal performance on the WN18RR dataset. When at the FB15k-237 dataset, the improvements are even better, i.e., 16.45% and 8.57%. Although these unsupervised methods are simple and efficient for implementation, they fail to capture the structures of KGs. In contrast, our proposed self-supervised strategies would provide an effective way for quantized representations for KG structure learning."}, {"title": "4.2 SSQR Result Analysis", "content": "Ablation Studies. We carry out the ablation studies to verify the effectiveness of each module in SSQR as the bottom part of Table 2. Generally, the performance of link prediction degrades when GCN or semantic distilling is removed, but the extent of degradation varies across different datasets. It can be seen that the GCN encoder is more important for the FB15k-237 dataset (14.40% and 11.56% decline), while semantic information has more impact on WN18RR (7.45% and 9.86%). This may be due to the fact that FB15k-237 contains a rich KG structure which requires GCN to capture, while the semantic text is more important for WN18RR to make up for the defects caused by the lack of rich structural information.\nRelevance among Entity Codes. We also calculate the cosine similarity of quantized representation in Figure 4, including the original text embedding, SSQR, SSQR w/o GCN, and SSQR w/o semantics. When using only text embeddings, the similarities are all small positive values. SSQR w/o GCN has similarities that are all close to 1. These phenomena indicate that entity representations are in a small corner of the space (i.e., anisotropic), where the representation space is not fully utilized for efficient representation. SSQR solves this problem to a certain extent, with a greater range and variety of similarities. Removing semantic information would diminish that advantage.\nImpacts of M and N. The number of codebooks"}, {"title": "4.3 Quantized Representations with LLMs", "content": "Link Prediction. For fine-tuning, we utilize the pre-trained AdaProp (Zhang et al., 2023) to generate 20 candidates for each query as it has strong and balanced performance on most KG tasks. For comparison, we selected the current advanced embedding models, like TransE (Bordes et al., 2013), CompGCN (Vashishth et al., 2020), AdaProp (Zhang et al., 2023), MA-GNN (Xu et al., 2023), TCRA (Guo et al., 2024a), and DiffusionE (Cao et al., 2024). Besides, we include five advanced LLM-based methods for more direct comparison, including KICGPT (Wei et al., 2023), CSProm-KG-CD (Li et al., 2024), ARR (Chen et al., 2024), KG-FIT (Jiang et al., 2024), and MKGL (Guo et al., 2024b).\nThe results of link prediction are shown in Table 3. It can be observed that SSQR with LLaMA2 or LLaMA3.1 is obviously superior in KG link prediction against general embedding methods. Compared with the previous state-of-the-art MA-GNN, SSQR with LLaMA2 achieves about 4.60%, 8.09%, 4.39%, -0.88% and 18.47%, 32.62%, 18.31%, 4.92% improvement in two datasets, respectively. Compared with LLM-based methods, SSQR-LLaMA2 also shows competitive performance. It is better than KICGPT, CSProm-KG-CD, and ChatGPT. Even KICGPT achieves good results on the FB15k-237 dataset, it can also be raised by 8.98%, 14.37%, 9.60%, and 7.76%. For the KG-FIT (HAKE), it also has 6.87%, 12.30%, 3.87%, and -3.16% improvements on the WN18RR dataset. Although there is a slight deficiency in terms of Hits@10, improvements on other metrics are high. Meanwhile, SSQR-LLaMA3.1 is better than SSQR-LLaMA2, demonstrating that learned quantized representations can be used for a more powerful LLM to get better performance. From all the results, our methods generally achieve a greater improvement in the Hits@1 metric, which is caused by the candidate selection and ranking strategies we used in LLM fine-tuning. The candidate selection model may have limited ability, but our method has a strong ability to select the correct answer from all candidates. This demonstrates that our method has good scalability and can be further improved with more accurate candidate selection models.\nTriple Classification. Beyond the link prediction task, we conduct experiments on triple classification on the FB15k-237N dataset. The results are shown in Table 4, where our method outperforms general embedding methods and other LLM-based baselines. For the advanced KoPA (Zhang et al.,"}, {"title": "4.4 Insights of LLM Fune-tuning", "content": "Ablation Studies. We carry out ablation studies to verify the effectiveness of quantized representations for LLM tuning. The results are shown in Table 5 and the bottom part of Table 4, where w/o SSQR means only utilizing the entity's name for fine-tuning and removing learned entity codes. For the link prediction task, there is a large performance drop, especially in the MRR, Hits@1, and Hits@3 metrics. A similar pattern is also present in the triple classification task. We observe that when under the w/o SSQR setting, LLMs have overfitting issues, where their performance on training sets is very high but fails to generalize to valid and test sets. This demonstrates that the learned discrete codes are distinguishable and representative for different entities, thereby allowing their utilization as features to assist KG tasks in LLMs.\nImpacts of M and N for LLM Tuning. We explore the impacts of M and N for LLM tuning, the results are shown in Figure 7. First, we present the results of Original, which are the original results of AdaProp. It is shown that all other results are better than those of AdaProp, showing it is effective for LLM fine-tuning with quantized representations. The settings with N=16 and M=2048 have better results compared to 16-512 and 8-2048, indicating large values are needed to represent entity structural and semantic information, serving better features for LLMs. N is more important than M, which drops more performance, especially on the FB15k-237 dataset (16-512 even not dropping"}, {"title": "5 Related Work", "content": "For parameter-efficient embeddings on large KGs, NodePiece (Galkin et al., 2022) introduces an anchor-based method to learn a fixed-size entity vocabulary, where unsupervised strategies of Personalized PageRank (Page, 1999), node degree, and random are used for anchor selection. Each entity can be represented through k closest anchors and their respective distances. Further, EARL (Chen et al., 2023) randomly samples 10% entities as anchors and introduces connected relation information to match anchors' counterparts. To simplify the whole process, Li et al. (2023) introduces random entity quantization (RandomEQ) to randomly set anchor entities and randomly select relations for matching. The results show that RandomEQ achieves similar results compared to previous curated strategies and has more distinguishable ability. In general, these methods are all in an unsupervised learning manner, which could be efficient for large KG embedding but fails to model comprehensive structural and semantic information."}, {"title": "6 Conclusion and Potential Impacts", "content": "For seamlessly integrating KGs with LLMs, we introduce a self-supervised quantized representation method (SSQR). It compresses the structural and semantic information of entities in KGs to a discrete permutation of codewords, which has a similar format as the natural language and can be directly inputted to the LLMs. By specific instruction data and fine-tuning, LLMs can seamlessly learn KG's knowledge, which can be used in KG applications. To verify the effectiveness of our method, we implement experiments on KG link prediction and triple classification tasks, which demonstrate the superiority of our method. This innovative paradigm promises to usher in transformative techniques for KGs in the era of LLMs. In the future, we will explore more applications and make progress towards unified frameworks for multiple KG tasks, e.g., KG-based QA (Luo et al., 2024a), KG-based recommendation (Huang et al., 2023), and language modeling (Luo et al., 2024b)."}, {"title": "Limitations", "content": "Despite our SSQR method's capacity to facilitate the seamless integration of KGs with LLMs, our study encounters the generalization limitation due to the substantial computational burden associated with LLMs. In most recent and our studies, LLMs are fine-tuned for a specific KG and the corresponding task, which can not be applied to various KG tasks and largely limits the model generalization ability. In the future, we will try to construct unified LLMs for KGs by implementing quantization within the same discrete space."}, {"title": "A Statistics of WN18RR Dataset", "content": "Besides the statistic analysis of FB15k-237 in Figure 2, we also conduct the statistic analysis of WN18RR, which is shown in Figure 9. Specifically, we sample 200 entities from the whole KG and there are two settings (50% neighbor sampling and 100% neighbors). In the first setting of 50%, the median and mean of neighbors are 4.0 and 10.37, while the median and mean number of needed tokens are 61.5 and 185.84, respectively. For the setting of 100%, the median and mean of neighbors are 33.5 and 79.05, while the median and mean number of needed tokens are 623.5 and 1492.74, respectively. Compared to our SSQR, which only requires 16 tokens to represent each entity, both 50% and 100% settings demand a considerably higher number of tokens."}, {"title": "B Baselines", "content": "In this section, we give detailed descriptions of various baselines utilized in the paper."}, {"title": "B.1 Quantized Representations for KGS", "content": "\u2022 NodePiece (Galkin et al., 2022): The selection of quantized anchors relies on unsupervised strategies, including Personalized PageRank, node degree, and random approaches.\n\u2022 EARL (Chen et al., 2023): It randomly samples 10% entities as quantized anchors and introduces connected relation information to match anchors' counterparts.\n\u2022 Random entity quantization (RandomEQ for short) (Li et al., 2023): It randomly sets anchor entities and randomly selects relations for matching."}, {"title": "B.2 KG Link Prediction", "content": "\u2022 TransE (Bordes et al., 2013): The strategy of incorporating translational distance is utilized for learning representations of entities and relations.\n\u2022 CompGCN (Vashishth et al., 2020): Several entity-relation composition operations are proposed to combine the semantic information of neighbor entity-relation pairs in GNNs.\n\u2022 AdaProp (Zhang et al., 2023): An adaptive propagation path is learned to filter out irrelevant entities while preserving promising targets in the GNN framework.\n\u2022 MA-GNN (Xu et al., 2023): A dual-branch, multi-attention-based GNN model is employed to develop expressive entity representations.\n\u2022 TCRA (Guo et al., 2024a): A neuro-symbolic method that combines topological context learning with rule augmentation.\n\u2022 DiffusionE (Cao et al., 2024): Introducing diffusion process to KG embedding method."}, {"title": "B.3 KG Triple Classification", "content": "\u2022 TransE (Bordes et al., 2013): The strategy of incorporating translational distance is utilized for learning representations of entities and relations.\n\u2022 DistMult (Yang et al., 2015): It utilizes the semantic matching strategy, where the validity of a fact is depicted as the matching degree between the representation of entity and relation.\n\u2022 RotatE (Sun et al., 2019): It defines each relation as a rotation from the source entity to the target entity in a complex vector space.\n\u2022 Alpacazero-shot: It carries out zero-shot reasoning with Alpaca (Taori et al., 2023) with textual sequences for predicting the validity of a triple.\n\u2022 GPT-3.5 zero-shot: It carries out zero-shot reasoning with GPT-3.5 2 with textual sequences for"}, {"title": "C Experimental Details", "content": "Table 6: The statistics of WN18RR, FB15k-237, and FB15k-237N datasets. The former two are for link prediction. FB15k-237N dataset is for triple classification, where '/' splits the positive and negative samples.\nThe statistics of utilized datasets are shown in Table 6. For the SSQR learning, the default embedding dimension is set to 200. The GCN layer and dropout rate are 2 and 0.2. The training batch is 1024. For optimization, the learning rate is 0.0005 and the L2 regularization weight is 1e-8. For LLM tuning, we utilize 4 NVIDIA H100 GPUs and the learning rate is set to 2e-5 with 3% warmup ratio. In the link prediction experiment, we first tune LLMs on the instruction data of CompGCN's training split to initialize. Then, inspired by Wei et al. (2023) and Liu et al. (2024), we divide the valid set into two segments in a 9:1 ratio. The larger part is utilized to finetune LLMs to learn the ranking preference, while the smaller part is used for validation. In the triple classification experiment, we only update the embedding layer and the last four Transformer layers of LLMs for tuning efficiency. Meanwhile, M and N are set to 1024 and 16. In the training instruction data, we randomly select negative samples at a rate 16 times of positive ones. The instruction format of triple classification is shown in Table 7."}, {"title": "D Entropy and Jaccard Distance", "content": "As presented by Li et al. (2023), it is significant for the ability to distinguish different entities for"}, {"title": "E Additional Experimental Analysis", "content": ""}, {"title": "E.1 Training Process of SSQR", "content": "We display the training process SSQR in Figure 10, where w/o GCN and w/o sem denote ablations for the structural embedding and semantic distilling, respectively. The findings indicate that both structural embedding and semantic distilling contribute positively to the overall learning of quantized representation. The influence of semantic information on the FB15k-237 dataset is less significant when compared to its effect on GCN. Differently, semantic information is more important on the WN18RR dataset. This could be attributed to the varying levels of KGs' sparsity."}, {"title": "E.2 Relevance among Entity Codes on FB15k-237 Dataset", "content": "We also calculate the cosine similarity of quantized representation on the FB15k-237 dataset in Figure 12, which has the same setting as Figure 4. The contents presented in these two figures are also similar. When utilizing solely text embeddings, the corresponding similarities yield positive yet modest values. Moreover, the similarities associated with SSQR without the use of GCN are typically close to 1. These observations suggest that entity representations occupy a limited portion of the existing space, thus failing to maximize the efficiency of representation. SSQR addresses this issue to some degree by providing a broader range and diversity of similarities."}, {"title": "E.3 Impacts of M and N for LLM Tuning on FB15k-237 Dataset", "content": "We also explore the impacts of M and N for LLM tuning on the FB15k-237 dataset, the results are shown in Figure 11. It can lead to conclusions similar to Figure 7."}], "equations": ["e_{j}^{l+1} = W e_{j}^{l} + \\sum_{(e_{i}, r) \\in N(e_{j})} W m(e_{i}, r)", "m_{e_{i}, r} = e_{i}^{*} v_{r}", "Q(e) = x_{i}, \\text{ where } i = \\underset{m}{\\text{arg min}} ||e - x_{m}||_{2}", "q_{e} = W_{q}Q(e), Q(e) = [x_{q_{1}}, x_{q_{2}}, ..., x_{q_{N}}]", "L_{q} = ||sg[e^{l}] - q_{e}||_{2} + \\beta ||e^{l} - sg[q_{e}]||_{2}", "s(h, r, t) = [Flat(Conv(q_{h}|v_{r}))]^{T}W_{c}q_{t}", "L_{st}= \\frac{1}{|F|} \\sum_{i}[y_{i} \\text{log} \\hat{y_{i}}+(1-y_{i}) \\text{log}(1-\\hat{y}_{i})]", "L_{se} = \\frac{1}{|E|} \\sum_{i}||W_{a} t_{e_{i}} - a_{e_{i}}||^{2}", "L = L_{q} + L_{st} + L_{se}", "L_{llm} = - \\sum_{n=1}^{N} \\text{log}(x_{n}|x_{<n}, I)", "H = - \\sum p(\\text{Code}(e)) \\text{log} p(\\text{Code}(e))", "J = \\frac{1}{|E|} \\sum_{e_{i} \\in E} \\sum_{e_{j} \\in kNN(e_{i})} d(\\text{Code}(e_{i}), \\text{Code}(e_{j}))", "d(\\text{Code}(e_{i}), \\text{Code}(e_{j})) = \\frac{|CSet(e_{i}) \\cup CSet(e_{j})| - |CSet(e_{i}) \\cap CSet(e_{j})|}{|CSet(e_{i}) \\cup CSet(e_{j})|}"]}