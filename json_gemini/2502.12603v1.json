{"title": "Disentangling Long-Short Term State Under Unknown Interventions for Online Time Series Forecasting", "authors": ["Ruichu Cai", "Haiqin Huang", "Zhifan Jiang", "Zijian Li", "Changze Zhou", "Yuequn Liu", "Yuming Liu", "Zhifeng Hao"], "abstract": "Current methods for time series forecasting struggle in the online scenario, since it is difficult to preserve long-term dependency while adapting short-term changes when data are arriving sequentially. Although some recent methods solve this problem by controlling the updates of latent states, they cannot disentangle the long/short-term states, leading to the inability to effectively adapt to nonstationary. To tackle this challenge, we propose a general framework to disentangle long/short-term states for online time series forecasting. Our idea is inspired by the observations where short-term changes can be led by unknown interventions like abrupt policies in the stock market. Based on this insight, we formalize a data generation process with unknown interventions on short-term states. Under mild assumptions, we further leverage the independence of short-term states led by unknown interventions to establish the identification theory to achieve the disentanglement of long/short-term states. Built on this theory, we develop a Long Short-Term Disentanglement model (LSTD) to extract the long/short-term states with long/short term encoders, respectively. Furthermore, the LSTD model incorporates a smooth constraint to preserve the long-term dependencies and an interrupted dependency constraint to enforce the forgetting of short-term dependencies, together boosting the disentanglement of long/short-term states. Experimental results on several benchmark datasets show that our LSTD model outperforms existing methods for online time series forecasting, validating its efficacy in real-world applications.", "sections": [{"title": "Introduction", "content": "As one of the most fundamental tasks in time series analysis (Hamilton 2020; Liu et al. 2023b), time series forecasting (Zhou et al. 2021; Zeng et al. 2023; Kitaev, Kaiser, and Levskaya 2020; Liu et al. 2021; Wu et al. 2021; Zhou et al. 2021) plays a critical role in various fields such as finance (Clements, Franses, and Swanson 2004; Cao, Li, and Li 2019), and traffic (Lippi, Bertini, and Frasconi 2013). However, in the industry, since time series data often arrives sequentially and is accompanied by temporal distribution shifts (Wang et al. 2022; Li et al. 2024b), existing methods (Wu et al. 2021; Nie et al. 2022; Lopez-Paz and Ranzato 2017) that heavily rely on the mini-batch training paradigm can hardly adapt to these changing distributions, leading to suboptimal prediction results in the online scenario.\nTo solve this problem, several recent methodologies (Cai et al. 2023; Guo et al. 2024; Mejri, Amarnath, and Chatterjee 2024; Lin 1992) are proposed to adapt the short-term nonstationarity and long-term dependencies. FSNet (Pham et al. 2022) leverages the partial derivative to characterize the short-term information and an associative memory to preserve the long-term dependencies. To better combine long-term and short-term historical information, OneNet (Wen et al. 2024) uses a reinforcement learning-based to dynamically adjust the combination of temporal correlation and cross-variable dependency models. Recently, Zhang et al. (Zhang et al. 2024b) propose the Concept Drift Detection and Adaptation framework (D3A), which first detects the temporal distribution shift and then employs an aggressive manner to update the model. In summary, these methods aim to address online time series forecasting via two steps: 1) disentangling long/short-term states; and 2) adapting short-term states and reusing long-term states for forecasting. Please refer to Appendix A for further discussion about online time series forecasting and causal representation learning.\nAlthough current methods achieve non-trivial contributions on how to update short-term states or how to efficiently combine the long/short-term states, they implicitly assume that the long/short-term states have been well-disentangled from nonstationary time series data. However, this assumption is hard to meet, and without disentanglement of the long/short-term states, existing methods can hardly adapt to the nonstationary environments. Figure 1 provides a finance example, where the monetary exchange rate is influenced by the short-term variables (e.g. customs duties) and the long-term variables (e.g., financial revenue). Nonstationarity occurs due to unknown customs tariff policies. As shown in Figure 1 (a), when the long-term and short-term latent variables are not disentangled, the financial revenues are entangled with the customs duties. As a result, existing methods can be hard to effectively adapt to the changes in financial revenues and may obtain an inaccurate forecasting performance even if they use a masterly strategy to update the short-term states and preserve the long-term states.\nBased on the aforementioned example, we observe that nonstationarity is brought by the unknown interventions on short-term states. Moreover, to address the online forecasting task, it is intuitive to find that we should disentangle the long/short-term states from the time series with unknown interventions as shown in Figure 1 (b). Under this intuition, we first consider that the sequentially arriving data follow a data generation process in Figure 1 (c), where the latent short-term states are influenced by unknown interventions. Under mild assumptions, we establish disentanglement results on long/short-term states by leveraging the independence of intervened short-term states. To bridge the gap between theory and practice, we further develop a Long Short-Term Disentanglement model (LSTD) to solve the online time series forecasting problem. Specifically, the proposed LSTD model includes a minimal update constraint to preserve the long-term dependencies and an interrupted dependency constraint to enforce the forgetting of short-term dependencies, which facilitates the disentanglement of long-term and short-term latent states. Empirical results on several real-world benchmark datasets show that the proposed LSTD method outperforms existing state-of-the-art methods for online time series forecasting, highlighting its effectiveness in real-world applications."}, {"title": "Data Generation Process for Time Series Data", "content": "To show how to disentangle the long-term and short-term latent states in the online time series forecasting scenario, we first introduce the data generation process of time series data as shown in Figure 1 (c). Mathematically, we let X = {X\u2081,X\u2082 , \u2026\u2026\u2026, X\u209c,\u2026\u2026 } be time series data with discrete time steps, in which each observation X\u209c is generated from latent variables z\u209c through an invertible and nonlinear mixing function g as formalized in Equation (1).\nX\u209c = g(z\u209c).   (1)\nAt each time step t, z\u209c \u2208 \u211d\u207f are divided into the long-term latent states z\u02e2 \u2208 \u211d\u207f\u02e2 and short-term latent states z\u1d48 \u2208 \u211d\u207f\u1d48, and n = n\u209b + n\u1d48. Moreover, the i-th component of z\u1d48 is generated by some components of historical long-term latent states z\u02e2, with the time lag of \u03c4 via a nonparametric function as shown in Equation (2).\nz\u1d62\u1d48 = f\u1d48({z\u02e2\u209c\u208b\u03c4,\u2096|z\u02e2\u209c\u208b\u03c4,\u2096 \u2208 \ud835\udcaba(z\u1d62\u1d48)}, \u03b5\u209c,\u1d62),   with \u03b5\u209c,\u1d62~\ud835\udcab\u03b5  (2)\nwhere \ud835\udcaba(z\u1d62\u1d48) denotes the set of latent variables that directly cause z\u1d62\u1d48, and \u03b5\u209c,\u1d62 denotes the temporally and spatially independent noise extracted from a distribution \ud835\udcab\u03b5.\nMoreover, according to the observation in the example in Figure 1, we assume that the nonstationarity in time series data is led by the interventions on the short-term latent variables (e.g., the truncation between z\u02e2\u209c\u208b\u2081 and z\u1d48\u209c in Figure 1 (c)). It is noted that when the interventions occur is unknown. To illustrate the randomness of the interventions, we let I be an indicator to decide if an intervention occurs and I comes from a Bernoulli distribution \u212c(1, \u03b8) with the probability of \u03b8. When I = 0, it indicates no intervention, whereas when I = 1, it signifies intervention. When intervention occurs, the data is generated solely by noise. Formally, the generation process of the short-term latent variables is shown as follows:\nz\u1d62,\u2c7c\u1d48 = { f\u1d48({z\u02e2\u209c\u208b\u03c4,\u2096|z\u02e2\u209c\u208b\u03c4,\u2096 \u2208 \ud835\udcaba(z\u1d62,\u2c7c\u1d48)}, \u03b5\u209c,\u2c7c\u1d48), if I = 0 \u03b5, if I = 1  and I~\u212c(1, \u03b8),\nwhere \u03b5~\ud835\udcab\u03b5\u1d48     (3)"}, {"title": "Disentanglement of Long-Term and Short-Term States", "content": "To disentangle the long-term latent variables z\u02e2 and the short-term latent variables z\u1d48, we propose the block-wise identification theory in Theory 1. Mathematically, the block-wise identification means that for the ground-truth z, there exists  h\u2217  and an invertible function h\u2217 : \u211d\u207f \u2217 \u2192 \u211d\u207f \u2217, such that  = h\u2217(z). And \u2217 can be d or s.\nTheorem 1. (Subspace Identification of the long-term and short-term Latent Variables) Suppose that the observed data from long/short-term is generated following the data generation process in Figure1 (c), and we further make the following assumptions:\n\u2022 A1 (Smooth, Positive and Conditional independent Density:) (Yao, Chen, and Zhang 2022; Yao et al. 2021) The probability density function of latent variables is smooth and positive, i.e., p(Z\u209c\u208b\u03c4+1:\u209c|Z\u209c\u208b\u03c4) > 0 over Z\u209c\u208b\u03c4 and Z\u209c\u208b\u03c4+1:\u209c. Conditioned on Z\u209c\u208b\u03c4 each z\u1d62 is independent of any other z\u2c7c for i, j \u2208 1,...,n,i \u2260 j, i.e, logp(Z\u209c\u208b\u03c4+1:\u209c|Z\u209c\u208b\u03c4) = \u2211\u2096\u208c\u2081\u207f logp(z\u209c\u208b\u03c4+1:\u209c,\u2096|Z\u209c\u208b\u03c4)\n\u2022 A2 (non-singular Jacobian): (Kong et al. 2023b) Each generating function g has non-singular Jacobian matrices almost anywhere and g is invertible.\n\u2022 A3 (Linear Independence:) (Yao, Chen, and Zhang 2022) For any z\u1d48 \u2208 Z\u1d48\u209c\u208b\u03c4+1:\u209c \u2286 \u211d\u207f\u1d48, v\u209c\u208b\u03c4,\u2081, ..., v\u209c\u208b\u03c4,\u2099 \u1d48 as n\u1d48 vector functions in z\u209c\u208b\u03c4,\u2081, ..., z\u209c\u208b\u03c4,\u1d62, ..., z\u209c\u208b\u03c4,\u2099 \u1d48 are linear independent, where v\u209c\u208b\u03c4,\u2081 are formalized as follows:\nv\u209c\u208b\u03c4,\u2081 = \u2202\u00b2 log p(z\u1d48\u209c\u208b\u03c4+1:\u209c|Z\u209c\u208b\u03c4)\n\u2202 z\u1d48\u209c\u208b\u03c4+1:\u209c,\u2096  (4)\nSuppose that we learn (\u011d, f\u02e2, f\u1d48) to achieve Equation (1)-(3) with the minimal number of transition edge among short term latent variables z\u1d48,\u2026\u2026,z\u1d48\u209c,\u2026\u2026, then the long-term and short-term latent variables are block-wise identifiable.\nProof Sketch: The proof can be found in Appendix B. First, we construct an invertible transformation h\u2082 between the ground-truth latent variables and estimated ones. Sequentially, we prove that the ground truth of long-term latent variables is not the function of short-term latent variables by leveraging the pairing time series from different influences. Sequentially, we leverage sufficient variability of historical information to show that the short-term latent variables are not the function of the estimated long-term latent variables. Moreover, by leveraging the invertibility of transformation h\u2082, we can obtain the Jacobian of h\u2082 as shown in Equation (B.36), where B = 0 and C = 0, since the ground truth long-term latent variables are not the functions of short-term latent variables and the short-term latent variables are not the function of the estimated long-term latent variables.\nA := \u2202z \u2202z\u02e2, B := \u2202z \u2202z\u1d48 = 0\nC := \u2202z \u2202z\u02e2 = 0 , D := \u2202z \u2202z\u1d48 (5)\nDiscussion of the Identification Results: We would like to highlight that the theoretical results provide sufficient conditions for the identification of our model. That implies: 1) our model can be correctly identified when all the assumptions hold. 2) at the same time, even if some of the above assumptions do not hold, our method may still learn the correct model. From an application perspective, these assumptions rigorously defined a subset of applicable scenarios of our model. Thus, we provide detailed explanations of the assumptions, how they relate to real-world scenarios, and in which scenarios they are satisfied.\nSmooth, Positive and Conditional independent Density. This assumption is common in the existing identification results (Yao, Chen, and Zhang 2022; Yao et al. 2021; Yao, Chen, and Zhang 2022; Yao et al. 2021). In real-world scenarios, smooth and positive density implies continuous changes in historical information, such as temperature variations in weather data. To achieve this, we should sample as much data as possible to learn the transition probabilities more accurately. Moreover, The conditional independent assumption is also common in identifying temporal latent processes (Li et al. 2024a). Intuitively, it means there are no immediate relations among latent variables. To satisfy this assumption, we can sample data at high frequency to avoid instantaneous dependencies caused by subsampling.\nNon-singular Jacobian of g. This assumption is also common in (Kong et al. 2023b; Li et al. 2024b,c; Xie et al. 2023; Kong et al. 2023a). Mathematically, it denotes that the Jacobian from the latent variables to the observed variables is full rank. In real-world scenarios, it means that there is at least one observation for each latent variable. To meet this assumption, we can ignore such independent latent variables since they have no influence on the observations.\nLinear Independence.\nData generation process with unknown interventions. In real-world time series data, there are many unknown interventions that lead to nonstationarity like the financial example in Figure 1. Therefore, this assumption is reasonable. Besides, we need to impose discontinuities in the short-term components to break the symmetry between the long and short terms in the causal graph. This ensures that the long"}, {"title": "Long Short-Term Disentanglement Model", "content": "Model Overview\nIn this section, we introduce the implementation of the long/short-term disentanglement model as shown in Figure 2. Specifically, it uses a variational sequential autoencoder as a backbone architecture and further employs long-term and short-term prior architectures with smooth constraint and sparse dependency constraint for long-term and short-term latent variable disentanglement.\nVariational Sequential Autoencoder\nTo model the time series data, we follow the data generation process in Figure 1 (c) and derive the evidence lower bound (ELBO) as shown in Equation (6).\nELBO = \ud835\udd3cq(z\u2081:H|X\u2081:H) [ln \u2119(X\u2081:H|z\u2081:H, z\u0302\u2081:H)] - DKL(q(z\u2081:H|X\u2081:H)||\u2119(z\u2081:H)) - DKL(q(z\u0302\u2081:H|X\u2081:H)||\u2119(z\u0302\u2081:H)) (6)\nwhere LR and LP denote the reconstructed and prediction loss, respectively:\nLR = 1/L \u2211\u1d62\u208c\u2081\u1d38(z\u0302\u1d62 - z\u1d62)\u00b2, LP = 1/(H - L) \u2211\u1d62\u208c\u1d38\u208a\u2081\u1d34(z\u0302\u1d62 - z\u1d62)\u00b2 (7)\nDKL denotes the KL divergence. Specifically, q(Z\u2081:H|X\u2081:H), q(Z\u0302\u2081:H|X\u2081:H), which includes the encoder and the latent transition module in Figure 2, is used to approximate the prior distribution. \u2119(X\u2081:H|Z\u2081:H, Z\u0302\u2081:H) is used to reconstruct the historical observations and forecast the future values. The aforementioned two distributions can be formalized as follows:\nz\u0302\u2081:H, z\u0302\u2081:H = \u03c8(X\u2081:H), X\u0302\u2081:H = \u03c8(Z\u2081:H)   (8)\nWhere Z\u2081:H denotes the combination of z\u2081:H and z\u0302\u2081.H. For the implementation of \u03c8, we follow the backbone of FSNet (Pham et al. 2022). For the implementation of \u03c8, we employ an MLP (Multilayer Perceptron). Please refer to Appendix C for more implementation details of the LSTD model.\nLong-Term and Short-Term Prior Networks\nTo model the prior distribution of the long-term latent variables, we propose the long-term prior networks. Similar to the existing methods for causal representation learning (Yao et al. 2021; Yao, Chen, and Zhang 2022), we let {r\u1d62\u02e2} be a set of learned inverse transition functions that take the estimated long-term latent variables and output the noise term, i.e., \u03b5\u0302\u209c,\u1d62 = r\u1d62\u02e2(z\u0302\u209c,\u1d62|{z\u0302\u209c\u208b\u03c4}), and each r\u02e2\u1d62 is modeled with MLPs. Then we devise a transformation \u03ba := {z\u1d62, \u03b5\u0302\u209c,\u1d62} \u2192 ( z\u0302\u1d62, {1, diag}), and its Jacobian is J\u03ba := [\u2110  \ud835\udca9]\nwhere \u2133 denotes a matrix. By applying the change of variables formula, we have the following equation:\nlog \u2119(z\u0302\u209c\u208b\u2081, z\u0302\u209c) = log \u2119(z\u0302\u209c\u208b\u2081, \u03b5\u0302\u209c,\u1d62) + log |det(J\u03ba)|.  (9)\nSince we assume that the noise term in Equation (9) is independent with z\u0302\u209c\u208b\u2081, we can enforce the independence of the estimated noise and further have:\nlog \u2119(z\u0302\u02e2\u209c|z\u0302\u02e2\u209c\u208b\u2081) = log \u2119(\u03b5\u0302\u02e2\u209c,\u1d62) + \u2211\u1d62\u208c\u2081\u207f \u02e2 log |\u2202\u03b5\u0302\u209c,\u1d62/\u2202z\u0302\u02e2\u209c,\u1d62| (10)\nTherefore, the long-term prior can be estimated as follows:\nlog \u2119(z\u0302\u02e2\u2081:\u209c) = log(z\u0302\u02e2\u2081) + \u2211\u209c\u208c\u2082\u1d57(log(z\u0302\u02e2\u209c|z\u0302\u02e2\u209c\u208b\u2081) + \u2211\u1d62\u208c\u2081\u207f \u02e2 log |\u2202\u03b5\u0302\u209c,\u1d62/\u2202z\u0302\u02e2\u209c,\u1d62|) (11)\nwhere \u2119(\u03b5\u0302\u209c,\u1d62) follow Gaussian distributions. Similarly, we can further estimate the short-term prior as follows:\nlog \u2119(z\u0302\u1d48\u2081:\u209c) = log(z\u0302\u1d48\u2081) + \u2211\u209c\u208c\u2082\u1d57(log(z\u0302\u1d48\u209c|z\u0302\u1d48\u209c\u208b\u2081) + \u2211\u1d62\u208c\u2081\u207f \u1d48 log |\u2202\u03b5\u0302\u1d48\u209c,\u1d62/\u2202z\u0302\u1d48\u209c,\u1d62|) (12)\nSmooth Constraint for Long-Term Disentanglement\nTo preserve the long-term dependencies in the long-term latent variables, we propose the smooth constraint. Since the causal relationships of the long-term dependencies are stable, the association of the long-term dependencies is also stable. Based on this insight, we consider the attention weights as associations and extract the association with the help of the self-attention mechanism. Specifically, we first split the z\u2081:H into two equal-size segmentation z\u2081:H/2 and zH/2:H. And then the association of z\u2081:H/2 and zH/2:H can be formalized as follows:\n\ud835\udc9cz\u02e2 = Softmax( (Z\u2081:H/2\u02e2)\u1d40 (Z\u2081:H/2\u02e2)  / \u221an\u02e2 ),\n\ud835\udc9cz\u02e2 = Softmax( (ZH/2:H\u02e2)\u1d40 (ZH/2:H\u02e2)  / \u221an\u02e2),  (13)\nin which \ud835\udc9cz\u02e2 and \ud835\udc9cz\u02e2 denote the association matrices of the start half and the end half segments. Hence, we can restrict the long-term dependencies by restricting the similarity of these two matrices as shown in Equation (14)\n\u2112\u2098 = ||\ud835\udc9cz\u02e2 - \ud835\udc9cz\u02e2||\u2082, (14)\nwhere || ||\u2082 denotes the L2 norm of matrices.\nWe use the superscript symbol ^ to denote estimated variables."}, {"title": "Interrupted Dependency Constraint for Short-Term Disentanglement", "content": "Since the nonstationarity is assumed to be led by the interventions to the short-term latent variables, given z\u0302\u2081:H, if intervention occurs at \u03c4-th time step, and 2 < \u03c4 < H \u2212 1, then \u2202 z\u0302\u1d48\u03c4,\u1d62 / \u2202 z\u0302\u1d48\u03c4\u208b\u2081,\u2c7c = 0, where i, j\u2208 {1,\u2026,nd}. Based on this intuition, we aim to enforce the interruption of the estimated short-term dependencies to meet the unknown interventions. To achieve this, we propose the interrupted dependency constraint for the short-term variables. Specifically, given the estimated short-term variables z\u0302\u2081:H, we have:\n\u2112\u209b = \u2211 (i,j)\u2208{1,...,nd} \u2211 \u03c4\u2208 {2,\u2026, H-1} || \u2202 z\u0302\u1d48\u03c4,\u1d62 / \u2202 z\u0302\u1d48\u03c4\u208b\u2081,\u2c7c ||\u2081  (15)\nwhere || ||\u2081 denote the L1 norm.\nBy using the aforementioned interrupted dependency constraint, the intervention on the short-term latent variables can be automatically detected, which finally enforces the disentanglement of the short-term latent variables.\nModel Summary\nBy combining the aforementioned variational sequentially autoencoder with the restriction of smooth constraint and interrupted dependency constraint, we can finally formalize the total loss of the proposed LSTD model as follows:\n\u2112 = \u2112R + \u2112P + \u03b2\u2112\u1d0b +\u03b1\u2112\u2098 + \u03b3\u2112\u209b,   (16)\nwhere \u2112\u1d0b = \u2112s\u1d0b + \u2112\u1d48\u1d0b. And \u03b1, \u03b2, \u03b3 are hyper-parameters."}, {"title": "Experiment", "content": "Datasets\nTo evaluate the performance of our method, we consider the following datasets. ETT is an electricity transformer temperature dataset collected from two separate counties in China, which contains two separate datasets {ETTh2, ETTm1} for one hour level and minutes level, respectively. Exchange is the daily exchange rate dataset from eight foreign countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand, and Singapore ranging from 1990 to 2016. Weather is recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany. ECL is an electricity-consuming load dataset with the electricity consumption (kWh) collected from 321 clients. Traffic is a dataset of traffic speeds collected from the California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). For each dataset, we follow the standard preprocessing and setting in OneNet (Wen et al. 2024).\nBaselines\nWe consider nine state-of-the-art as follows: OneNet (Wen et al. 2024) which considered the temporal and feature relationships and used reinforcement learning to update their relationships in real-time. At the same time, we compared with a very excellent backbone model FSNet (Pham et al. 2022) which considered gradient updates to optimize fast new as well as retained information and be used in OneNet. Besides, we also compared the OneNet model with TCN as its backbone named OnetNet-TCN, and the regular usage of TCN named Online-TCN (Zinkevich 2003) for online learning. The Experience Replay (ER) (Chaudhry et al. 2019) stored the previous data in a buffer and interleaved with newer samples during learning. Meanwhile, ER has many advanced variants: TFCL (Aljundi, Kelchtermans, and Tuytelaars 2019) used a task-boundary detection mechanism and a knowledge consolidation strategy; MIR (Aljundi et al."}, {"title": "Quantitative Results and Discussion", "content": "Experiment results on each dataset are shown in Table 1 and Table 2. Since some methods report the best results on the original paper, we also show the best results on the aforementioned tables. Please refer to Appendix D for the experiment results with mean and variance over three random seeds. Our LSTD model significantly outperforms all other baselines on most online forecasting tasks. Specifically, our method outperforms the most competitive baselines by a clear margin of 44% on the Exchange, which verifies the example in the introduction. Moreover, our method also greatly reduced prediction errors in the WTH and ECL datasets. However, our method achieves the second-best but still comparable results in the ETT dataset, this might be because there are a few unknown interventions in the ETT datasets. How to address other types of nonstationarity will be an interesting future direction. In addition, we conduct performance analysis experiments and visualization in Appendix D. Compared with other models, we can find that the proposed LSTD has the best model performance and relatively good model efficiency."}, {"title": "Qualitative Results and Discussion", "content": "We further conduct visualization results in the WTH and Exchange dataset in Figure 4. Remarkably, our method detects interventions well and achieves better visualization results than that of OneNet and FSNet, which do not explicitly disentangle the short-term and long-term variables. This is because the long/short term variables of these methods might be entangled, hindering the rapid adaptation to the changing environment of the data streams, and finally resulting in suboptimal predictions. In the meanwhile, our method disentangles the long/short term variables by sparsity dependency constraint, and can efficiently adapt to the new environment. At the same time, the smooth constraint further maintains the long-term variables behind the time series data. Therefore, the prediction curve of our method can well align with the ground truth even if the prediction length is long."}, {"title": "Ablation Study", "content": "We further devise three model variants. a) LSTD-L1: we remove the interrupted dependency constraint for short-term disentanglement. b) LSTD-L2: we remove the smooth constraint for long-term disentanglement. c) LSTD-KL: we remove the long/short-term prior and the corresponding Kullback-Leibler divergence term. Experiment results on the Exchange dataset are shown in Figure 3. We find that 1) the performance of LSTD-L1 drops without an accurate forgetting of the information, implying that the accurate forgetting benefits the quickly adapting to changes in the data domain and improves the disentanglement and forecasting performance. 2) the performance of LSTD-L2 drops without re"}, {"title": "Summary", "content": "This paper presents a long/short-term state disentanglement model to address the challenges of online time-series forecasting in the presence of nonstationarity led by unknown interventions. Unlike existing methods, this model can theoretically identify both long-term and short-term latent variables, enhancing its relevance to real-world data. Technologically, the LSTD model employs the smooth constraint and sparse dependency constraint to enforce the disentanglement of long/short-term variables. In summary, this paper offers valuable insights into enhancing online time-series forecasting via causal representation learning."}, {"title": "Time Series Forecasting", "content": "Recently, various research studies focused on time series forecasting problems, and deep learning-based methods have been very successful in this field. More precisely, the deep learning-based methods can be divided into several classes. First, the model based on RNN utilizes a recursive structure with memory to construct hidden layer transitions over time points .(Graves and Graves 2012; Lai et al. 2018; Salinas et al. 2020). Second, TCN (Bai, Kolter, and Koltun 2018; Wang et al.; Wu et al. 2022) based approach to modeling hierarchical temporal patterns and extracting features using a shared convolutional kernel. Besides, there are simple but very effective methods as well, such as based on MLP (Oreshkin et al. 2019; Zeng et al. 2023; Zhang et al. 2022; Li et al. 2024b) and on states-base-model (Gu et al. 2022, 2021; Gu, Goel, and R\u00e9 2021). Above these methods, the Transformer methods are especially outstanding and get the great process on the time series forecasting task (Kitaev, Kaiser, and Levskaya 2020; Liu et al. 2021; Wu et al. 2021; Zhou et al. 2021). However, these existing methods are based on offline data processing, contrary to the mainstream ONLINE training methods of the significant data era. Since the above methods are unsuitable for direct application to online problems, a model that can be trained on online data and perform well is needed."}, {"title": "Online Time Series Forecasting", "content": "Due to the rapid increase in train data and the requirement for model updates online, online time series forecasting has become more popular than offline ones(Anava et al. 2013; Liu et al. 2016; Gultekin and Paisley 2018; Aydore, Zhu, and Foster 2019). (Pan et al. 2024) employs structural consistency regularization to capture a range of scenarios, using a representation-matching memory replay scheme to retain temporal dynamics and dependencies. (Luan et al. 2024) applies tensor factorization for streaming tensor time series prediction, updating the predictor in a low-complexity online manner to adapt to evolving data. (Mejri, Amarnath, and Chatterjee 2024) addresses online nonlinear time-series forecasting by mapping low-dimensional series to high-dimensional spaces for linear hyperdimensional prediction, adapting to temporal distribution shifts.Online time series forecasting is a widely used technique in the real world due to the continuity of the data and the frequent drift of concepts. In this approach, the learning process occurs over a series of rounds. The model receives a look-back window, predicts the forecasting window, and then displays the valid values to improve the model's performance in the next round. Recently, a brunch of online time series forecasting work got excellent results, including considering gradient updates to optimize fast new as well as retained information (Pham et al. 2022) and models that consider both temporal and feature dimensions (Wen et al. 2024). Nevertheless, the fast adaptation and information retention of the models mentioned above are simultaneous. It needs to decouple the long and short term, which can lead to confounding results and suboptimal predictions. To solve this problem, the LSTD decouples the data first to isolate the long and short-term effects on the prediction, with the long-term effects being used to preserve the characteristics of the historical data and the short-term effects being used to quickly adapt to changes in the data for better online prediction."}, {"title": "Continual Learning", "content": "Continual learning is a novel topic and aims to set up intelligent agency by learning the sequence of tasks to perform with restricted access to experience (Lopez-Paz and Ranzato 2017). A continual learning model must balance the knowledge of the current task and the prompt of the future learning process, as known in the stability-plasticity dilemma (Lin 1992; Grossberg 2013). Due to their connection to how humans learn, several neuroscience frameworks have prompted the development of various continual learning algorithms. The continual learning model corresponds to the requirement of online time series forecasting. The constant learning can enable real-time updates upon receiving the new data to adapt the data dynamics better, improving the model accuracy. The proposed LSTD incorporates continual learning into an online time series forecasting model, which mitigates the stability-plasticity problem by decoupling the long and short-term effects, retaining the knowledge of previous tasks through the long-term effects, and facilitating the learning of future tasks through the short-term effects."}, {"title": "Causal Representation Learning", "content": "To recover the latent variable with identification guarantees(Yao et al. 2023; Sch\u00f6lkopf et al. 2021; Liu et al. 2023a; Gresele et al. 2020), independent component analysis (ICA) has been used in a number of studies to determine the casual representation (Rajendran et al. 2024; Mansouri et al. 2023; Wendong et al. 2024; Li et al. 2024c). Conventional approaches presuppose a linear mixing function for latent and observable variables. (Comon 1994; Hyv\u00e4rinen 2013; Lee and Lee 1998; Zhang and Chan 2007). However, determining the linear mixing function is a difficult problem in real-world situations. For the identifiability, many assumptions are made throughout the nonlinear ICA process, including the sparse generation process and the usage of auxiliary variables(Zheng, Ng, and Zhang 2022; Hyv\u00e4rinen and Pajunen 1999; Hyv\u00e4rinen, Khemakhem, and Monti 2024; Khemakhem et al. 2020b; Li et al. 2023).\nSpecifically, Aapo et al.'s study confirms identifiability first. The exponential family is assumed to consist of latent sources in Ref. (Khemakhem et al. 2020a; Hyvarinen and Morioka 2016, 2017; Hyvarinen, Sasaki, and Turner 2019), where auxiliary variables such as domain indexes, time indexes, and class labels are added. Furthermore, Zhang et al.'s study (Kong et al. 2022; Xie et al. 2023; Kong et al. 2023a; Yan et al. 2024) demonstrates that the exponential family assumption is not necessary to accomplish component-wise identification for nonlinear ICA."}, {"title": "Identification", "content": "In this section", "assumptions": "n\u2022 A1 (Smooth", "Density": "Yao", "p(Z\u209c\u208b\u03c4+1": "\u209c|Z\u209c\u208b\u03c4) > 0 over z\u209c\u208b\u03c4 and Z\u209c\u208b\u03c4+1:\u209c. Conditioned on Z\u209c\u208b\u03c4 each z\u1d62 is independent of any other z\u2c7c for i", "logp(Z\u209c\u208b\u03c4+1": "\u209c|Z\u209c\u208b\u03c4) = \u2211\u2096\u208c\u2081\u207f logp(z\u209c\u208b\u03c4+1:\u209c", "Jacobian)": ""}]}