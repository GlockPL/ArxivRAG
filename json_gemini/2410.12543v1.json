{"title": "LLM-based Translation Inference with Iterative Bilingual Understanding", "authors": ["Andong Chen", "Kehai Chen", "Xuefeng Bai", "Yang Xiang", "Muyun Yang", "Tiejun Zhao", "Min Zhang"], "abstract": "The remarkable understanding and generation capabilities of large language models (LLMs) have greatly improved translation performance. However, incorrect understanding of the sentence to be translated can degrade translation quality. To address this issue, we proposed a novel Iterative Bilingual Understanding Translation (IBUT) method based on the cross-lingual capabilities of LLMs and the dual characteristics of translation tasks. The cross-lingual capability of LLMs enables the generation of contextual understanding for both the source and target languages separately. Furthermore, the dual characteristics allow IBUT to generate effective cross-lingual feedback, iteratively refining contextual understanding, thereby reducing errors and improving translation performance. Experimental results showed that the proposed IBUT outperforms several strong comparison methods, especially being generalized to multiple domains (e.g., news, commonsense, and cultural translation benchmarks).", "sections": [{"title": "Introduction", "content": "In the field of machine translation (MT), translations based on LLMs (LLM-MT) have become a research focus. Numerous studies have shown that the remarkable understanding and generation capabilities of LLMs significantly improve translation performance. One type of translation paradigm can be summarized as follows: the LLM first generates a contextual understanding of the sentence to be translated, and then, based on this understanding, performs a better translation (as shown in Figure 1(a)). However, our study found that when the LLM generates incorrect contextual understanding of the sentence to be translated, it negatively affects translation quality (as shown in Figure 1(b)). These generated understanding errors lead to the introduction of misleading information during the translation process, particularly when dealing with complex concepts such as commonsense and cultural domains. We refer to this issue as Understanding Distortion of LLMs. This study manually evaluated the Chinese-English test set in the commonsense domain. It found that Understanding Distortion makes up 40% of translation errors, highlighting the importance of this issue (\u00a7 6.2). To address this issue, we propose a new method called Iterative Bilingual Understanding Translation (IBUT). The IBUT method consists of four parts: Understanding Generation, Alignment Judgment, Iterative Refinement, and Understanding-Based Translation. For Under-"}, {"title": "Related Work", "content": "Machine Translation Based on Large Language Models (LLM-MT). Large language models, such as GPT-3 (Brown et al., 2020), have demonstrated their effectiveness in machine translation across various language pairs. Recent studies delve into the performance of LLM in machine translation, including control over formality in translation outputs, in-context translation abilities during pre-training, and the impact of LLM-based machine translation on culturally sensitive texts Additionally, a study has explored the bilingual capabilities of LLMs to enhance translation performance. For translation tasks requiring reasoning, multi-agent debates can effectively enhance the reasoning abilities of LLM-MT. These investigations further validate the research value of LLM-MT, offering diverse research directions for scholars.\nKnowledge-based Machine Translation. Extensive research indicates that incorporating knowledge enhances translation performance. This external knowledge includes bilingual dictionaries, probabilistic interpolation of dictionaries, data augmentation through back-translation , phrase memory decoding, and entity-based denoising pre-training . Additionally, researchers introduced domain and part-of-speech information during the inference phase and obtained multilingual translations of key terms through the NLLB translator , thereby enhancing the translation quality for low-resource languages. Regarding the introduction of internal knowledge into LLM-MT, LLM enhances the translation process by integrating keywords, themes, and examples similar to the given source sentences. LLM generate explanations for difficult words to enhance LLM-MT performance."}, {"title": "The Proposed Method", "content": "The incorrect understanding of translated sentences generated by LLMs leads to a decline in translation quality. To address this issue, we propose a new method called Iterative Bilingual Understanding Translation (IBUT). IBUT utilizes LLMs to generate bilingual contextual understanding of the source input and utilizes the dual learning of translation tasks to establish verbal feedback for iteratively refining this understanding. Finally, the iterative refinement reduces errors in bilingual contextual understanding, thereby enhancing translation performance. The IBUT consists of four parts: 1. Understanding Generation; 2. Alignment Judgment; 3. Iterative Refinement; 4.Understanding-Based Translation. We use MT to denote a translation model based on LLM, and lowercase letters s and t to represent sentences in the source language ($L_s$) and target language ($L_t$), respectively. That is, $s = (s[1], ..., s[n])$ and $t = (t[1], ..., t[m])$, where each $s[i]$ and $t[i]$ is a token.\n1. Understanding Generation. For the"}, {"title": "Experimental Setup", "content": "Dataset: We conduct experiments on four representative MT benchmarks, including the general news MT benchmarks (WMT22 and WMT23), commonsense MT, and cultural MT. The specific details of the dataset can be found in Appendix A.5.\nComparative Methods. In our evaluation, IBUT is compared with a range of translation methods, including Zero-shot, 5-shot, Rerank, Refine, MAD, and MAPS . To validate its generalizability, we utilize three LLMs, which include closed-source models such as ChatGPT and GPT-4, as well as open-source models like Alpaca-7B and Vicuna-7B . Details on comparative methods are in Appendix A.6.\nEvaluation Metrics. In evaluating our translation methodology, we initially employ COMET and BLEURT as automatic metrics, aligning with the established standards in LLM-based translation literature . For traditional translation evaluation, we use BLEU (Papineni et al., 2002). To further evaluate our translation method, we employ human evaluations to verify translation performance. Details on human evaluations are in Appendix B.7."}, {"title": "Experimental Results", "content": "5.1 Main Results\nThe effectiveness of IBUT in general news translation tasks. In the WMT22 general news tasks, as shown in Table 1 (WMT23 results in the Appendix B.4), IBUT outperforms other methods across 13 language pairs and 3 evaluation metrics (BLEU results in the Appendix B.3). Specifically, in the news domain, the IBUT method outperforms translations directly based on contextual understanding by +1.5 COMET and +1.4 BLEURT. This indicates that the IBUT method alleviates the issue of Understanding Distortion in the news domain.\nThe effectiveness of IBUT in low-resource tasks. We selected all low-resource tasks (Uk\u2194Cs, Ru\u2194Sah, Liv\u2194En, En\u2192Hr) from WMT22. As observed in Table 1, current low-resource tasks still pose challenges to LLMs. However, compared to baseline methods, IBUT achieved an average improvement of +2.6 COMET in these low-resource tasks, with increases of +4 and +6.5 COMET for Liv\u2194En, respectively.\nIBUT is effective across different language similarities. In WMT22, we validated the IBUT model using tasks with different language similarities. Specifically, Uk\u2194Cs represents closely related languages; En\u2192De and En\u2192Hr are from the same language family; Liv\u2194En, Ru\u2194Sah, and En\u2192Ja are categorized as distant language families. The experimental results, as shown in Table 1, demonstrate significant improve-ments across different language similarities due to IBUT. Notably, for the selected distant family languages, there was an average increase of +3.4 COMT, highlighting IBUT's potential to enhance translation tasks in distant language families.\n5.2 Generalizability of IBUT across multiple domains\nIBUT Adapts to Cultural MT. As shown in Table 2, IBUT outperforms other methods across all 6 language pairs. For translation corpora containing cultural-specific items, the IBUT method achieved an average increase of +2.02 and +1.6 COMET compared to the ChatGPT and MAPS methods. Notably, in the En\u2192Ta translation task, IBUT outperformed ChatGPT by +5.5 COMET. The experimental results above indicate that IBUT is suitable for translation tasks in the cultural domain."}, {"title": "Analysis", "content": "6.1 Automated Evaluation of Understanding Distortion and Translation Performance\nThis study explored the positive impact of reducing understanding distortion issues in bilingual con-textual understanding on translation performance using IBUT. We randomly selected a set of 200 Chinese\u2192English translation sentence pairs from the Commonsense MT dataset, which provides a test subset for lexical ambiguity. Based on the subset, IBUT iterated 8 times (max_iter = 8), saving the results of bilingual contextual understanding and translation COMET scores after each iteration.\nAs shown in Figure 3, the vertical axis represents the translation performance, measured as the COMET score. The horizontal axis represents the scores evaluated by GPT-4 for the quality of bilingual contextual understanding affected by understanding distortion issues, with a maximum score of 10. The score for the source language is $v_s$ and for the target language is $v_t$, while the overall score v is the average of the two ($v = \\frac{v_s + v_t}{2}$). Details on the evaluation prompt can be found in Appendix B.2.\nThe experimental results, as shown in Figure 3, demonstrate a positive correlation between the quality of contextual understanding and translation performance. Additionally, as the number of iterations increases, the quality of contextual understanding progressively improves, indicating that the IBUT method effectively reduces understanding distortion issues.\n6.2 Human Evaluation\nHuman Evaluation of Understanding Distortion Issue. In the human evaluation of understanding distortion issue, this study follows the method of Huang et al., 2024 and Chen et al., 2024b to assess translation outcomes from two main dimensions: accuracy in ambiguity resolution (commonsense domain) and the statistical results of understanding distortion issue (see Appendix B.7 for experimental setup details).\nThe experimental results are shown in Table 4. Understanding distortion issues accounts for a significant proportion (40%). Our method (IBUT) significantly addressed these failures, with a success rate of approximately 89%, demonstrating the effectiveness of our method. Additionally, in terms of ambiguity resolution accuracy, IBUT outperformed the baseline by 13 acc points, indicating that bilingual understanding and iterative refinement contribute to enhancing ambiguity resolution capabilities in translation tasks.\nTransaltion Quality. In human evaluation of translation quality, this study adopted the method (Liang et al., 2023) to validate translation quality on both the En\u2192Zh and Zh\u2192En test sets of the Cultural MT and the Commonsense MT dataset (Appendix B.7 for experimental setup details).\nThe experimental results are displayed in Figure 4. Within the Commonsense MT Dataset, IBUT performed best in terms of ambiguity resolution accuracy, thereby achieving higher human eval-uation scores compared to other methods. In the Cultural MT Dataset, IBUT received higher human"}, {"title": "Impact of Iterative Refinement on Translation Performance", "content": "To further verify the impact of the Iterative Refinement part on overall translation performance, we conducted experiments on Cultural MT (En\u2192Zh) and Commonsense MT (Zh\u2192En), comparing methods like MAD and Refine to iteratively enhance translation quality. We set the maximum number of iterations at 9 and required that each iteration in the Iterative Refinement part obtain a new translation COMET score, rather than allowing adaptive termination in the Alignment Judgment part.\nThe experimental results, as shown in Figure 5, first indicate that IBUT surpasses the comparative methods in translation performance with each iteration, further proving the effectiveness of the method. Secondly, compared to the comparative methods, IBUT progressively enhances its perfor-mance in each iteration, demonstrating that the dual learning of translation can provide positive supervision signals in each iteration."}, {"title": "Effectiveness of Bilingual Contextual Understanding and Ablation Experiments", "content": "The IBUT introduced bilingual contextual understanding based on the source sentence to improve translation performance. To evaluate the effects of bilingual contextual understanding, we designed 5 control methods: (a) LLM-MT directly translating (ChatGPT); (b) LLM generating contextual understanding based on the source language, translated by LLM-MT (SRC); (c) LLM generating contextual understanding based on the target language, translated by LLM-MT (TGT); (d) LLM generating contextual understanding for both source and target languages, translated by LLM-MT (SRC+TGT); (e) using the IBUT method described in section 3.\nThe effectiveness of Bilingual Contextual Understanding. The experimental results are shown in Figure 6. On the WMT22 and cultural MT datasets, first, the translation results based on contextual understanding surpass the baseline methods, demonstrating the validity of our research direction. Second, bilingual (SRC+TGT) contextual understanding significantly improves translation performance compared to monolingual (SRC or TGT) understanding, indicating that bilin-gual contextual understanding effectively enhances translation results. Additionally, experimental results show that target language understanding (TGT) contributes more to translation performance improvement than source language understanding (SRC).\nAblation Experiments on Different Parts of IBUT. As shown in Figure 6, \"SRC or TGT\" indicates the use of only the Understanding Generation part in IBUT, while \"SRC+TGT\" indicates that IBUT does not use iterative refinement. The experimental results demonstrate that the performance of the IBUT method surpasses these methods, thereby proving the rationality and effectiveness of the IBUT method design."}, {"title": "IBUT Demonstrates Generalizability in Model Selection", "content": "To validate the generalizability of the IBUT method on open-source models, we selected two open-source models (Alpaca and Vicuna) for experimental verification. The experimental results, as shown in Table 5, indicate that the overall performance trends of the two open-source models are consistent with those observed using the GPT3.5 model. This demonstrates the generalizability of the IBUT method in open-source models. Additionally, we further validated the effectiveness of the IBUT method in GPT-4. The results are shown in Appendix B.6."}, {"title": "Computational Resource Analysis", "content": "Since the IBUT method requires multiple iterative steps, it is necessary to discuss and analyze its resource consumption. For token consumption, we used the gpt-3.5-turbo tokenizer to tokenize and then calculated the token consumption of the comparative methods requiring iteration on the commonsense dataset. The results are shown in Table 6.\nThe experimental results show that the IBUT method increases token consumption by about 5 times compared to the 5-shot method, but it improves the COMET/BLEURT/BLEU translation metrics by +4.0/+4.1/+2.4, respectively, indicating a considerable performance gain. The IBUT method is on the same level as other strong methods (MAD and MAPS), achieving an average improvement of 2 points. For inference time analysis, refer to Appendix B.1. Additionally, we discuss this limitation in the Limitations section, noting it as a future research direction for the MT community."}, {"title": "Conclusion", "content": "In this paper, we introduce a new method called Iterative Bilingual Understanding Translation (IBUT), designed to address the inherent chal-lenges of decreased translation performance in LLM-MT due to error introduced by contextual understanding. The IBUT initially generates bilingual contextual understanding, and then constructs a supervisory signal based on the dual learning of the translation task, thereby iteratively refining bilingual contextual understanding to enhance LLM-MT performance. The method performs well in various scenarios involving general news translation tasks, commonsense MT, and cultural MT datasets, with human evaluations confirming its effectiveness."}, {"title": "Limitations", "content": "The IBUT method has several limitations. Firstly, models with stronger understanding and gener-ation capabilities will obtain better contextual understanding, thereby enhancing translation performance. Additionally, since our method requires multiple steps, it necessitates a significant amount of computational resources."}]}