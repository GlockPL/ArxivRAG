{"title": "PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency", "authors": ["Kenshin Abe", "Kaizaburo Chubachi", "Yasuhiro Fujita", "Yuta Hirokawa", "Kentaro Imajo", "Toshiki Kataoka", "Hiroyoshi Komatsu", "Hiroaki Mikami", "Tsuguo Mogami", "Shogo Murai", "Kosuke Nakago", "Daisuke Nishino", "Toru Ogawa", "Daisuke Okanohara", "Yoshihiko Ozaki", "Shotaro Sano", "Shuji Suzuki", "Tianqi Xu", "Toshihiko Yanase"], "abstract": "We introduce PLaMo-100B, a large-scale language model designed for Japanese proficiency. The model was trained from scratch using 2 trillion tokens, with architecture such as QK Normalization and Z-Loss to ensure training stability during the training process. Post-training techniques, including Supervised Fine-Tuning and Direct Preference Optimization, were applied to refine the model's performance. Benchmark evaluations suggest that PLaMo-100B performs well, particularly in Japanese-specific tasks, achieving results that are competitive with frontier models like GPT-4.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing (NLP) by enabling sophisticated solutions to complex linguistic tasks. As part of the Generative AI Accelerator Challenge (GENIAC) project in 2024, we developed PLaMo-100B, a 100 billion parameter LLM, which represents a major advancement in this field, particularly for its strong performance in both Japanese and English. Unlike models that are fine-tuned from pre-existing model weights like LLaMA-3 [Llama Team, AI @ Meta, 2024], PLaMo-100B was trained from scratch, tailored specifically for optimal performance in Japanese language tasks, while also delivering high proficiency in English.\nThe pre-training process for PLaMo-100B used 2 trillion tokens, with 1.5 trillion for initial pre-training and 0.5 trillion for continued pre-training. The dataset was carefully curated from diverse sources, including RefinedWeb [Penedo et al., 2023] and CommonCrawl2. The model's architecture is based on a decoder-only transformer [Vaswani et al., 2017], incorporating advanced techniques such as QK Normalization [Henry et al., 2020] and Z-Loss [Chowdhery et al., 2023]. The training process leveraged 3D parallelism [Shoeybi et al., 2020], Zero Bubble technique [Qi et al., 2024], and FP8 training to efficiently handle the massive scale of the model, using NVIDIA H100 GPUs for computation.\nPost-training involved a multi-stage approach to refine the model's capabilities. This included Supervised Fine-tuning (SFT) using a variety of instruction-following datasets, followed by multiple rounds of Direct Preference Optimization (DPO) [Rafailov et al., 2023, Xu et al., 2024] to align the model's outputs with human preferences. We also employed model merging techniques to combine different model checkpoints effectively. A key feature of the post-training process was the extensive use of synthetic data to expand the model's capabilities and address the scarcity of high-quality datasets, particularly for the Japanese language. PLaMo-100B has been evaluated using benchmarks such as Jaster [Han, 2024], Japanese MT-Bench [Zheng et al., 2023, Stability AI, 2023], and the Rakuda Benchmark [YuzuAI, 2023]. These assessments indicate"}, {"title": "2 Pre-Training", "content": "Our base pre-trained model, PLaMo-100B-Base, has been trained on 2 trillion tokens of text data in Japanese and English. The training corpus primarily consists of publicly available datasets, supplemented by data that we have curated through our web crawling efforts. In the following sections, we will delineate the training data and expound upon the model training process."}, {"title": "2.1 Dataset", "content": "For PLaMo-100B-Base, the training was conducted in two phases: the initial phase encompassed 1.5 trillion tokens, while the subsequent phase included 0.5 trillion tokens. \nThe English component required 1.3 trillion tokens, whereas the Japanese component necessitated 0.7 trillion tokens for pre-training. Notably, the collection of sufficiently high-quality Japanese datasets exclusively from publicly available sources posed significant challenges. Consequently, akin to the methodologies employed in CCNet [Wenzek et al., 2020] and RefinedWeb [Penedo et al., 2023], we constructed a Japanese dataset by using web-crawled data archived by CommonCrawl."}, {"title": "2.1.1 Japanese Dataset", "content": "Initially, we constructed the Japanese corpus from CommonCrawl using CCNet. However, for the latter portion consisting of 0.5 trillion tokens, we undertook a comprehensive preprocessing effort from the ground up, using 20 data dumps spanning from 2017 to 2024, thereby generating a dataset comprising approximately 460 billion tokens.\nThere are two main reasons for this approach:\n\u2022 The WET files processed by CCNet lack the structured information that is typically available in the HTML or Markdown format; consequently, we opted to process the WARC files directly.\n\u2022 Accumulated expertise during this project has bolstered our confidence that we can efficiently generate datasets independently.\nThe processing pipeline, akin to RefinedWeb [Penedo et al., 2023] and the Swallow Corpus\u00b3, managed the raw archived data stored in the WARC format through the following steps:\n1. Download WARC files while extracting Japanese HTML and text files.\n2. Convert the extracted data to Markdown if it is in HTML format."}, {"title": "2.2 Model Training Stabilization", "content": "Stabilizing model training is a significant challenge in the pre-training of LLMs. Chowdhery et al. [2023] indicated that as LLMs increase in size, their training processes tend to exhibit greater instability. In the training of PLaMo-100B-Base, we addressed this challenge using two primary strategies: the model architecture and the loss function.\nThe effectiveness of these strategies in our pre-training remains uncertain. Although the pre-training was executed without any observable instability, we cannot conclusively attribute this success to the aforementioned strategies; it is possible that the training setup possessed inherent stability. To disentangle these factors, it would be necessary to perform an additional pre-training session of equivalent scale, which is too costly. Nonetheless, given the complexities associated with re-running large-scale pre-training, we contend that the implementation of proactive measures was essential."}, {"title": "2.2.1 QK Normalization", "content": "The architecture of PLaMo-100B closely resembles that of Llama24 and Llama35. To enhance training stability, we implemented QK Normalization [Wortsman et al., 2024] because Wortsman et al. [2024] indicate that QK Normalization effectively stabilizes computations within self-attention layers and contributes to the overall stability of model training.\nIn preliminary experiments, we verified that QK Normalization does not adversely affect model performance. Additionally, findings in other studies, such as Jamba [Lieber et al., 2024] and Chameleon [Team, 2024], demonstrate that incorporating a normalization layer prior to the interaction between tokens enhances training stability. This observation suggests that QK Normalization may emerge as a standard technique for large-scale models."}, {"title": "2.2.2 Z-loss", "content": "Regarding the loss function, we incorporated z-loss, which enhances numerical stability within the softmax cross-entropy loss function, defined as\n$L(x) = \\left( \\log \\left( \\frac{\\exp (x[i])}{\\sum_{i=0}^{C} \\exp (x[i])} \\right) \\right)^2$ (1)\nwhere x \u2208 RC is the output of PLaMo-100B for the next token prediction and C is the vocabulary size. Similar to QK Normalization, Wortsman et al. [2024] demonstrated that z-loss contributes to the stabilization of training processes.\nIn preliminary experiments, we verified that z-loss does not adversely affect model performance, akin to QK Normalization. While it remains uncertain whether z-loss achieves its intended purpose of stabilizing training, we have yet to observe any negative impact from its implementation. Furthermore, z-loss has proven to be a valuable metric for monitoring training progress. In instances where training deviates from expectations (such as due to a bug), the changes in z-loss are often more significant than those observed in other loss functions or in downstream task performance. This characteristic facilitates the identification of whether observed changes are attributable to trial-to-trial variability or other factors."}, {"title": "2.2.3 Negative Results", "content": "While we have implemented two methods that contributed to the stabilization of training, we also explored additional techniques. In this section, we discuss those methods that either exhibited no discernible effect or had a counterproductive impact on training stability.\nParallel Layers: In the case of our previous pre-training model, PLaMo-13B [Preferred Networks, 2023], we implemented Parallel Layers [Black et al., 2022] as a technique to improve computational speed. Initially, there was negligible difference in performance with or without the implementation of Parallel Layers. However, as development advanced, the negative impact on model performance attributed to the use of Parallel Layers became increasingly apparent. As a result, we opted to revert to the standard Transformer architecture for PLaMo-100B-Base.\nNormalization of Embeddings: Dettmers et al. [2022] have reported that normalizing the output of embedding layers can stabilize training and enhance the performance of LLMs. We also evaluated this method, yielding mixed results:\n\u2022 Perplexity: A slight improvement in perplexity was observed.\n\u2022 Generation: We noted a degradation in performance for tasks requiring decoding.\nOne could argue that, in the context of pre-training, perplexity should be prioritized over word generation tasks, which are primarily relevant during instruction tuning. Nonetheless, we assessed the potential risks associated with this method to be too significant and ultimately decided against its adoption in our training process.\nSequence Length Warmup: Sequence length warmup [Li et al., 2022] may enhance training efficiency and improve model performance. This technique involves commencing training with shorter sequence lengths and gradually increasing the sequence length as the training progresses. Upon evaluation, we determined that sequence length warmup had little effect in our experiments. This method was originally proposed to address the challenges associated with large batch sizes. However, according to Kaplan et al. [2020], larger models are capable of using larger batch sizes without encountering significant issues. Therefore, in our context, the implementation of sequence length warmup was deemed unnecessary."}, {"title": "2.3 Performance Optimization", "content": "As of February 2024, when we began the pre-training of PLaMo-100B-Base, there were no established cases of training LLMs in the 100 billion parameter range using H100 GPUs. While employing H100 GPUs for training, minor overheads that were negligible with A100 GPUs are anticipated to become problematic. This is attributed to the H100's capacity for exceptionally high computational speeds (FLOP/s), which may cause non-computationally-bound operations to consume relatively more time.\nThroughout the training phase, we implemented various strategies aimed at enhancing training speed. Some of these strategies were informed by publicly available resources, such as the efficient utilization of FP8 TensorCore. However, we also encountered several challenges that could only be discerned during the actual training process. In this section, we present two approaches that emerged from our specific experiences."}, {"title": "2.3.1 ZeRo Bubble", "content": "For pre-training, we implemented 3D parallelism [Shoeybi et al., 2020], a method that integrates data parallelism with two types of model parallelism: tensor parallelism and pipeline parallelism to enable the training of large-scale models.\nAs for pipeline parallelism, we adopted Zero Bubble [Qi et al., 2024]. It is recognized that pipeline parallelism may encounter inefficiencies due to periods in which certain GPUs remain idle, known as \"bubbles.\" However, Zero Bubble aims to effectively minimize these idle periods to zero.\nWe did not implement the speculative parameter updates introduced by Zero Bubble for the following reasons:\n\u2022 The definition of one iteration becomes ambiguous in the Python script, complicating the debugging process.\n\u2022 Gradient clipping was consistently applied, resulting in very few iterations where speculative execution could be deemed effective.\nThe second point stands in contrast to the findings reported in the original paper, which suggested that gradient clipping is infrequently applied. We posit that this discrepancy may stem from differences in model size between our setup and that used in the study."}, {"title": "2.3.2 Numerical Precision of the lm-head (Linear Layer for Word Prediction)", "content": "LLMs, including PLaMo-100B-Base, are composed of repeated Transformer blocks but necessitate a final linear layer to predict the next token. In models provided via Hugging Face's Transformers library [Wolf et al., 2020], this layer is commonly referred to as the lm-head.\nInitially, we computed this layer in FP8 format to enhance training speed. Although we observed minimal issues concerning training loss, we experienced suboptimal performance in subsequent benchmark tasks. Through investigations conducted in smaller experimental settings, we discovered that the z-loss values were significantly elevated when using FP8 for the lm-head as shown by Figure 2, suggesting that the lm-head should be computed in a higher precision format than FP8 format. In our case, we used bfloat16 for the Im-head to mitigate this issue."}, {"title": "2.3.3 Runtime Performance", "content": "The pretraining of PLaMo-100B-Base achieved a computational speed of approximately 540 TFLOP/S/GPU, which is about 27% of the theoretical speed of 1979 TFLOP/s for FP8 on the H100. While a direct comparison is difficult due to differences in the number of GPUs used, we believe that our performance is comparable to that of Llama3 and the benchmarking by MosaicML6."}, {"title": "3 Post-Training", "content": "In our post-training phase, we employed Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) algorithms. As outlined in the subsequent sections, we implemented a training pipeline that consists of SFT followed by Iterative DPO. Furthermore, we adopted the Model Merging technique to enhance generalization performance post-training. A summary of the training pipeline is presented in Figure 3."}, {"title": "3.1 Post-training pipeline", "content": "In Supervised Fine-Tuning (SFT), the training process is guided by a predefined dataset comprising paired examples of \"questions posed to the LLM\" and their respective \"expected answers from the LLM\". This dataset encompasses a variety of tasks, each consisting of a question-answer pair. For instance, in the domain of mathematical problem-solving, each pair entails a problem and its respective solution. Similarly, in conversational settings, each pair consists of a user prompt and the preferred response. The objective of using a training dataset of high diversity and quality is to enhance the downstream task performance of the LLM.\nTraining in SFT is conducted using next-token prediction, analogous to the pre-training phase, albeit with an emphasis on fine-tuning the response generation component. To optimize this process, only the response portion of the input text is considered during loss calculation rather than the entire input sequence. Although our preliminary experimental evaluations revealed no significant performance difference between"}, {"title": "3.1.1 Supervised Fine-Tuning", "content": "this approach and that considers the entire input, this method was adopted to mitigate the risk of the model internalizing undesirable traits, such as aggressive expressions, potentially present in the questions.\nTraditionally, in SFT, various tasks' training datasets are aggregated and trained simultaneously. However, some prior studies, such as Nemotron-4 [Adler et al., 2024], have documented conflicts arising from concurrent learning of multiple tasks. Despite attempts to alleviate these conflicts through adjustments to the sample weighting ratios, attaining a substantial level of harmony was challenging, especially in coding tasks. Consequently, a two-stage SFT approach is proposed wherein the coding tasks are trained in the first stage of SFT, followed by the second stage of SFT, which focuses on more general tasks.\nIn our experiment, a similar trend was observed for mathematical questions. Therefore, we adopted the two-stage SFT method, initially segregating mathematical questions for the first stage and subsequently addressing various other tasks in the second stage."}, {"title": "3.1.2 Direct Preference Optimization", "content": "Direct Preference Optimization (DPO) is an algorithm proposed by Rafailov et al. [2023] that learns human preferences from labeled data, wherein pairs of responses to the same question are designated as either better (chosen) or worse (rejected). The model is encouraged to generate more preferable responses by leveraging this preference information.\nIn the learning process of DPO, several existing datasets, such as hh-rlhf [Bai et al., 2022] and HelpSteer2 [Wang et al., 2024], which include labeled responses generated by LLMs or human authors, are commonly used. Alternatively, data can also be synthesized by allowing the model to generate multiple response candidates, which can then be labeled accordingly. The former scenario, where the model producing the responses differs from the one being trained, is referred to as off-policy, while the latter scenario, in which the model being trained generates the responses, is termed on-policy.\nOn-policy training has demonstrated effectiveness in enhancing learning efficiency, as it provides preference feedback on the types of responses the model is more likely to generate [Tajwar et al., 2024]. A hybrid approach, known as SPIN [Chen et al., 2024], has also been introduced, which involves generating a dataset by pairing teacher responses from the SFT dataset with model-generated responses, under the assumption that the teacher's responses are more preferred.\nWhen employing on-policy data for training, it is possible to alternate between data generation and DPO training termed Iterative DPO. This method has been shown to yield superior results compared to a single round of DPO [Xu et al., 2024, Dong et al., 2024].\nThree different datasets are combined for our two-stage DPO training after SFT: (1) a publicly available dataset, (2) a dataset generated by labeling responses produced from a snapshot of PLaMo-100B, and (3) a dataset generated through the SPIN methodology. This approach enables us to take advantage of both high-quality publicly available preference datasets and the efficacy of Iterative DPO using on-policy datasets. The details of the data generation process are described in the data generation section."}, {"title": "3.1.3 Model Merging", "content": "Model merging is a technique that integrates multiple models to enhance overall performance [Izmailov et al., 2019, Wortsman et al., 2022]. Various methodologies exist for model merging; for instance, Llama-3.1 [Llama Team, AI @ Meta, 2024] has reported using the average of multiple models. We employed a straightforward model merging technique known as Spherical Linear Interpolation (SLERP) [Shoemake, 1985], which computes the midpoint between two models.\nThere were several DPO training results, depending on the combination of training data and other factors such as hyperparameters. By merging two of these models with distinct characteristics, we were able to create a model that incorporates the strengths of both models to some extent."}, {"title": "3.2 Post-training Data", "content": "To effectively conduct post-training, curating a diverse and high-quality dataset that describes the desired responses to user inquiries is necessary. In the early days of post-training, InstructGPT [Ouyang et al., 2022] engaged annotators to assemble a dataset that specifies the expected behavior of large language models in response to user questions. However, as LLM development has progressed, there have been efforts to have"}, {"title": "3.2.1 Public Data", "content": "High-quality post-training datasets suitable for commercial use are available, such as oasst2 [K\u00f6pf et al., 2023] or hh-rlhf [Bai et al., 2022] in English, along with ichikara-instruction7 in Japanese. Furthermore, the quantity of publicly accessible datasets is continuously increasing. We conducted experiments on a variety of these datasets to decide our instruction-tuning dataset collections."}, {"title": "3.2.2 Programatic Data Generation", "content": "To accurately address mathematical problems, we developed templates for various problem types that require calculations, subsequently generating datasets by varying the numerical values. Our mathematics dataset was manually constructed without machine learning techniques. Although there is a limit to the number of problem templates that can be created manually, and many data points would have only different numerical values, we decided it would be okay based on our previous empirical studies and the considerations outlined below.\nWhen an LLM produces a calculation result, the distribution of tokens tends to exhibit a deterministic quality, which aligns with our objectives during the training process. Furthermore, the textual content outside of the mathematical formulas is likely to adhere to a standardized format. Even when the only difference between data points is the numerical values, some degree of diversity in outcomes may still be observed. This includes distinctions such as whether carrying occurs during addition, the potential for simplification of fractional results, or the choice of which variable to eliminate in simultaneous linear equations.\nExisting datasets that do not rely on machine learning include the AMPS pretraining corpus [Hendrycks et al., 2021] and the work by Saxton et al. [2019]. However, these datasets feature artificial TEX representations for formulas, and their answers are restricted to numerical values, indicating potential areas for enhancement in post-training applications. To address this, we generated our own mathematical datasets aimed at augmenting the volume of Japanese mathematical data. We have used the math-related datasets for pre-training as well, but for post-training, we apply a different format, such as instruction-based responses, and combine different datasets with varying ratios, taking into account the characteristics of each dataset."}, {"title": "3.2.3 Synthetic Data Generation", "content": "For the question-answering dataset, we employed the Self-Instruct algorithm [Wang et al., 2023] as a foundation for data generation. However, rather than using the algorithm directly with GPT, we developed a method to facilitate the data generation using smaller LLMs like PLaMo-13B [Preferred Networks, 2023]. For instance, when attempting to generate a question sentence directly, the results were suboptimal, prompting us to incorporate an additional step to first generate a concise title.\nDuring the development of PLaMo-100B, we also focused on translating the collected and generated datasets into Japanese. The availability of post-training datasets for commercial use in Japanese is severely limited, creating challenges in acquiring a sufficient quantity and diversity of training data. Even when generating our own data, numerous open LLMs are primarily designed in English, complicating the generation"}, {"title": "3.2.4 Preference Data Generation", "content": "We generated preference data during the post-training process of PLaMo-100B. Referring to the work by Dong et al. [2024], we generated eight different responses for the same prompt using PLaMo-100B and evaluated their scores. The highest-scoring response was selected as the \"chosen\" response, while the lowest-scoring one was marked as \"rejected\". To evaluate response scores, we experimented with both the LLM-as-a-Judge method using open LLMs [Zheng et al., 2023, Verga et al., 2024] and the reward model.\nIn this data generation process, only the prompt is required, and a teacher response example is unnecessary. We can can datasets like chatbot_arena_conversations [Zheng et al., 2023], which only contains user prompts in a commercially usable license. During the response generation where LLM inference is required, we used VLLM [Kwon et al., 2023] for acceleration."}, {"title": "4 Evaluation Results", "content": "For evaluating the model, we used the g-leaderboard branch of the 1lm-leaderboard benchmark operated by Weights & Biases on the GENIAC 1.0, and measured Jaster and MT-Bench. Additionally, we also used in-house evaluation code to measure the Rakuda Benchmark."}, {"title": "4.1 Jaster", "content": "Jaster is a collection of Japanese benchmarks, to measure the ability of LLMs to understand Japanese. It is evaluated using the code from the 1lm-jp-eval repository. In the GENIAC project, it is evaluated on the g-leaderboard branch using a specific set of categories such as\n1. NLI: Natural Language Inference\n2. QA: Question Answering\n3. RC: Reading Comprehension\n4. MC: Multi-Choice QA\n5. MR: Math Reasoning\n6. FA: Fundamental Analysis.\nEach benchmark's score instead of the average score of each category is shown in Appendix A.\nThe performance of LLMs is assessed in both 4-shot and 0-shot settings with the question-answering task. In the 4-shot setting, examples of questions and answers are provided when asking questions, while in the 0-shot setting, no examples are given.\nAs shown in Table 2 and Table 3, the PLaMo-100B-Instruct model, which is obtained after post-training, significantly improved its performance compared to the base model, surpassing GPT-4's average score. Although the Jaster training dataset was not used in this experiment, the model learned how to use the knowledge gained during pre-training by learning how to answer various question formats during post-training [Longpre et al., 2023]. This capability contributes to the model's improved performance. The results confirm that the PLaMo-100B model, which is trained on a higher fraction of Japanese data, has a strong foundation in Japanese language understanding.\nThe only category where the model's performance fell short of GPT-4 was the Mathematical Reasoning (MR) category."}, {"title": "4.2 MT-Bench", "content": "MT-Bench [Zheng et al., 2023] serves as a benchmark for assessing the conversational response capabilities of LLMs, evaluating the quality of responses across eight categories: coding, extraction, humanities, math, reasoning, roleplay, stem and writing. Since the responses are free-form conversations, rule-based scoring methods are impractical. Instead, the LLM-as-Judge approach is employed, using models such as GPT-4 as evaluators to assign scores. While the original MT-Bench is presented in English, Stability AI has developed a Japanese version, which is publicly accessible as Japanese MT-Bench [Stability AI, 2023].\nWithin the GENIAC project, we have evaluated the scores of both the English and Japanese versions of MT-Bench and reported in Tables 4 and 5.\nBased on the evaluation, PLaMo-100B-Instruct managed to score an average of 7.781 in Japanese MT-Bench, coming close to GPT-3.5 in terms of score. Compared to the baseline model's score of 5.469, there is a significant improvement, demonstrating that the post-training process successfully enhanced the conversational response capabilities as intended.\nAnalyzing by category, we found that the model particularly excelled in the humanities, STEM, writing, and roleplay categories, with notable performance in responses requiring creative and open-ended dialogue. Table 10 in Appendix B illustrates an example response from the humanities category.\nOn the other hand, the pre-trained model demonstrated subpar performance in categories requiring scientific consideration such as math, coding, and reasoning. As indicated by the Jaster benchmark, enhancing the model's proficiency in these domains is likely essential during the pre-training phase. However, we investigated how much the model's scientific capabilities could be enhanced through post-training.\nAs previously noted, we developed a scalable data generation method to create a substantial number of high-quality mathematics and coding datasets for post-training. Following training on these datasets, the model's performance in mathematics, coding, and reasoning categories exhibited significant improvement,"}, {"title": "4.3 Rakuda Benchmark", "content": "The Rakuda Benchmark is designed to assess the performance of conversational responses to questions pertaining to Japanese domestic topics, including geography, politics, history, and society. In this study, we employed judge prompts from MT-Bench to conduct an absolute evaluation, scoring the responses on a scale of 10. Additionally, we performed a relative evaluation using prompts provided by the Rakuda Benchmark's official guidelines. However, we did not compute ratings based on pairwise comparisons of multiple models, which were done in official evaluations.\nThe maximum score for the absolute evaluation is 10 points. Notably, the results generated by PLaMo-100B-Instruct did not contain any errors detectable by the judge model. The primary variations in scores were attributed to the evaluation of the answer details. In the domain of geography, knowledge proved to be particularly significant, and it appears that the volume of Japanese data used during pre-training contributed to the wide coverage of this knowledge. Conversely, in the other three domains, the judge model frequently emphasized critical aspects such as \u201cperspective\u201d, \u201cimpact\u201d and \u201cchallenges\", which resulted in challenges in achieving high relative evaluation scores in comparison to GPT-4-0125-Preview.\nHere are some notes regarding the evaluation process. Response generation for PLaMo and the absolute evaluation were conducted using in-house implementations to facilitate faster inference, without altering the generation parameters. The judge model was specified as GPT-4, and the evaluation used the GPT-4-0613 version from Azure OpenAI, which was the latest version available at the time of the final update of the Rakuda Benchmark's official evaluation. The response data for GPT-4 0125-Preview was generated using the code provided in the Rakuda Benchmark repository. In contrast, the responses for GPT-4-0613 and\""}, {"title": "5 Conclusion", "content": "With the computational resources provided by GENIAC, we successfully completed the pre-training and post-training of PLaMo-100B, a large-scale language model. PLaMo-100B-Instruct achieved notable performance, surpassing GPT-4 in Japanese-specific benchmarks such as Jaster and Rakuda, though areas like mathematical reasoning and coding still need improvement.\nThe project also led to the creation of a scalable pipeline for dataset generation and training, which can be applied to other models beyond PLaMo. This provides a strong foundation for future model development and broader applications.\nFurthermore, we are actively considering the safety and ethical implications of large language models. For more details on our stance on responsible technology development, please refer to our statement \"Responsibility/\u8cac\u4efb\u3042\u308b\u6280\u8853\u958b\u767a\u306b\u5411\u3051\u3066\u201d10."}, {"title": "Author Contributions", "content": "Within each section, contributors are listed in alphabetical order by last name.\nPre-training team Yuta Hirokawa, Hiroyoshi Komatsu, Hiroaki Mikami, Shogo Murai, Daisuke Nishino, Shuji Suzuki, Tianqi Xu\nPost-training team Kenshin Abe, Kaizaburo Chubachi, Yasuhiro Fujita, Kentaro Imajo, Toshiki Kataoka, Tsuguo Mogami, Kosuke Nakago, Toru Ogawa, Yoshihiko Ozaki, Toshihiko Yanase\nOverall project management Daisuke Okanohara, Shotaro Sano"}]}