{"title": "YOLO-FEDER FUSIONNET: A NOVEL DEEP LEARNING ARCHITECTURE FOR DRONE DETECTION", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Stefan J\u00e4ger", "Tobias Koch"], "abstract": "Predominant methods for image-based drone detection frequently rely on employing generic object detection algorithms like YOLOv5. While proficient in identifying drones against homogeneous backgrounds, these algorithms often struggle in complex, highly textured environments. In such scenarios, drones seamlessly integrate into the background, creating camouflage effects that adversely affect the detection quality. To address this issue, we introduce a novel deep learning architecture called YOLO-FEDER FusionNet. Unlike conventional approaches, YOLO-FEDER FusionNet combines generic object detection methods with the specialized strength of camouflage object detection techniques to enhance drone detection capabilities. Comprehensive evaluations of YOLO-FEDER FusionNet show the efficiency of the proposed model and demonstrate substantial improvements in both reducing missed detections and false alarms.", "sections": [{"title": "1. INTRODUCTION", "content": "Robust drone detection systems play a vital role in enhancing security systems, protecting privacy and ensuring regulatory compliance [1]. Leveraging advanced computer vision techniques, image-based drone detection establishes a proactive mechanism to analyze visual data, facilitating early threat detection, and enabling effective mitigation measures. The widespread adoption of image-based detection techniques is primarily driven by the cost-effectiveness of camera sensors, their broad availability, and their seamless integration into established security systems [2].\nIn the field of drone detection, the processing of acquired image data typically relies on the application of generic object detection models (e.g., YOLOv5 [3] in various network configurations). These models enjoy widespread popularity due to their adeptness in balancing real-time processing speed and precision. Furthermore, generic object detection models exhibit notable effectiveness in detecting drones against homogeneous backgrounds (e.g., clear blue sky), or in scenarios where drones distinctly contrast with their surroundings [4]. However, their performance tends to decline considerably in scenarios where drones operate against complex and highly textured backgrounds [4, 5]. Our previous investigations specifically emphasized the substantial challenge inherent in detecting drones amidst or in close proximity to trees. The heterogeneous background composition, combined with difficult lighting conditions and the similarity between tree branches and drone rotor arms, facilitates a seamless integration of drones into their surroundings.\nThe resulting camouflage effect severely impedes the capacity of generic object detection systems to accurately identify and delineate the boundaries of drones, undermining the overall detection quality [5]. The phenomenon of camouflage effects extends beyond drone detection. For instance, it constitutes a considerable challenge in accurately detecting animals within their natural habitats, fostering the development of diverse camouflage object detection (COD) techniques [6].\nWhile COD techniques have demonstrated efficiency in animal detection, their direct transposition to drone detection is unexplored. Therefore, our study aims to assess the viability of leveraging insights from COD methods to enhance the reliability of generic drone detectors, especially in scenarios where they encounter limitations (cf. Figure 1). We introduce YOLO-FEDER FusionNet, a novel deep learning (DL) architecture that combines the strengths of generic object detection with the specialized capabilities of COD. Furthermore, we provide an examination of YOLO-FEDER FusionNet across diverse real-world datasets. This also entails a comparative analysis against established drone detection techniques, offering a robust evaluation of the effectiveness and performance enhancements achieved by our approach. Additionally, we introduce a simple techniques for false negative mitigation in image sequences.\nGiven the limited availability of annotated real-world data for drone detection and the problem-specific nature of existing datasets, our method strategically incorporates synthetic data a prevalent"}, {"title": "2. RELATED WORK", "content": "In the following, we discuss the latest advancements in image-based drone detection, focusing on their effectiveness in challenging environments. Additionally, we address the principle idea of COD along with prevalent COD techniques. Furthermore, we explore essential research and concepts related to generating synthetic data, addressing the inherent discrepancies between simulated and real-world scenarios in the context of drone detection.\nDrone Detection. Developing precise and reliable drone detection systems is a multidimensional challenge, accommodating various interpretations, sub-problems and strategic directions. Within the domain of image-based drone detection, considerable emphasis is directed towards the detection of small drones [10, 11] and accurately discerning them from other aerial entities, such as birds [10]. A notable deficiency persists in methodologies specifically tailored to the enhancement of drone detection amidst complex or highly textured backgrounds [5, 10]. Recent strategies addressing this challenge commonly entail the refinement of distinct modules within established object detection frameworks, such as YOLOv5 [10]. For instance, Lv et al. [10] introduce SAG-YOLOv5s \u2013 an adapted smaller version of YOLOv5s (denoting the small variant within the YOLOv5 series), specifically optimized for intricate environments. Their method integrates SimAM attention [12] and Ghost modules [13] into YOLOv5s' bottleneck structure, to elevate drone target extraction and refine background suppression during feature analysis. Concurrently, alternative methodologies seek to simplify the complexity of natural environments by dividing the process into two distinct stages. Typically, this involves eliminating background elements [4] and extracting moving objects [14], followed by a classification. Furthermore, some methodologies contemplate camera-based drone detection either as a preliminary stage in a tracking procedure or an integral part of a multi-sensor system [15]. This strategic consideration aims to offset potential shortcomings of a purely camera-based system, augmenting its robustness especially in complex environments. However, the challenge of camouflage effects induced by natural elements like trees remains unaddressed in current drone detection strategies.\nCamouflage Object Detection. The development of specialized methodologies dedicated to the detection of camouflage objects is an emerging field of research. Camouflage object detection (COD) embodies a class-independent detection task [6], particularly prevalent in the domain of animal detection. Its objective is the precise identification of objects that closely replicate the inherent characteristics of their surrounding, minimizing their visual contrast and distinctiveness. The majority of COD techniques address the challenges posed by intrinsic similarity and edge disruption through emulation of the human visual system [6, 16]. Only a small selection of approaches seeks to compensate for perception limitations by dis-assembling the camouflage scenario and emphasizing subtle distinguishing features [17]. A promising model belonging to the second algorithmic category is the feature decomposition and edge reconstruction (FEDER) model by He et al. [17].\nSimulation-Reality Gap. Leveraging synthetic data is a popular approach for training DL models in drone detection [5, 7-9] and other application domains [18] due to the high expenses tied to acquiring real-world data. Techniques like domain randomization [8] or game engine-based simulations [19] facilitate the generation of extensive, domain-specific datasets. These methods demonstrate cost-effectiveness through automated labeling processes, ensuring precise annotations, unlike manual labeling techniques. Furthermore, they enable the circumvention of real-world constraints (e.g., privacy regulations), fostering dataset diversification. However, transferring detection models exclusively trained on synthetic data to real-world applications frequently leads to performance degradation, attributed to the simulation-reality gap. The gap's severity is closely linked to the quality of both synthetic and real-world data, and is typically assessed through diverse quality measures such as mAP across various intersection over union (IoU) thresholds [7, 8]. Two primary strategies for narrowing this gap include fine-tuning with real-world data [8] and mixed-data training [14]."}, {"title": "3. FRAMEWORK", "content": "Given the complexity inherent to drone detection, the proposed YOLO-FEDER FusionNet strategically combines the benefits of generic object detection with the specific strengths of COD algorithms. The model relies on two essential components for feature extraction: the well-established YOLOv51 backbone architecture [3] and the specialized camouflage object detector FEDER [17]. YOLOv51 refers to the larger model configuration within the YOLOV5 series. As both algorithms yield complementary results, the YOLOv51 backbone and the FEDER algorithm operate as an ensemble system to extract significant features. This involves parallel processing an RGB image \\(X \\in \\mathbb{R}^{W \times H \times 3}\\) with \\(W = H\\), by both components (cf. Figure 2). The information from both components is fused at feature level within the network's neck, whose architectural design is inspired by YOLOv51 [3]. The feature maps issued by the neck are processed within the network's head to generate predictions for objects across three distinct sizes. Detailed descriptions of all network components are presented in the following sections.\nYOLOv51 Backbone. The employed YOLOv51 backbone [3] relies on CSPDarkNet53, which incorporates DarkNet-53 [20] in conjunction with an advanced CSPNet strategy [21]. The foundational architecture features a sequential arrangement of multiple CBS modules (consisting of convolutional, batch normalization, and SiLU activation layers) and C3 modules (comprising a CSP bottleneck with three convolutional layers). A spatial pyramid pooling fusion (SPPF) module [22] completes the backbone structure (see Figure 2, bottom left).\nFEDER Backbone. The feature decomposition and edge reconstruction (FEDER) model introduced by He et al. [17] consists of three main components: a camouflaged feature encoder"}, {"title": "4. EXPERIMENTAL SETUP", "content": "To assess the proposed framework, we employ the following experimental setup, incorporating diverse datasets and evaluation metrics.\n4.1. Datasets\nConsidering the scarcity of accessible data in the context of drone detection, we utilize self-captured real-world data sourced from a potential application site for evaluation. Concurrently, we leverage synthetically generated data, derived from physically-realistic simulations, to effectively train the proposed detection model. Table 1 provides an overview of the datasets employed in this study, with details discussed below. Both real and synthetically generated data have already been included in our prior work [5].\nReal-World Data. To acquire real-world data, we employ a fixed Basler acA200-165c camera system firmly stationed on the ground. The system is equipped with dual lenses (25mm and 8mm), enabling the capture of two distinct field of views from each vantage point. The selected recording environment replicates structural and environmental characteristics of a potential installation site for a drone detection system in an urban surveillance setting (cf. [5] for more details). The original RGB images are recorded at a resolution of 2040\u00d71086 pixels, leading to two distinct datasets R1 and R2 (cf. Table 1). While the background composition in dataset R1 is characterized by an increased prevalence of building structures, highly textured objects more precisely trees form a substantial part of the image backgrounds in dataset R2. Thus, R2 exhibits a greater level of complexity in comparison to R1. Given the model's constraint necessitating square images, a coarse cropping strategy is deployed, contingent upon the precise localization of the drone object within\nthe image frame. Subsequently, a random cropping technique is applied using distinct dimensions: 640\u00d7640 (YOLOv5l's default input size) and 1080\u00d71080. This procedure yields two different versions of each dataset: one set featuring images at a resolution of 640\u00d7640 pixels, and another set comprising images of size 1080\u00d71080 pixels (to amplify the informational content). Employing this systematic approach ensures the preservation of crucial information while concurrently enhancing dataset diversity. Alongside drone imagery, all datasets also include approximately 7-8% of background images.\nSynthetic Data. To generate synthetic training data, we employ the game engine-based data generation pipeline detailed in [19]. The pipeline harnesses the functionalities of the Unreal Engine version 4.27 [29] and Microsoft AirSim [30], enabling the efficient extraction of automatically labeled RGB images. Leveraging the Urban City environment [31], we aim to emulate essential attributes of the application scenario defined by R1 and R2. Data collection is performed from five unique camera perspectives, employing three distinct drone models (see [5] for more details). Aligned with the characteristics of R1 and R2, synthetic RGB images are initially captured at a resolution of 2040\u00d71086 pixels (leading to dataset S1, cf. Table 1). Subsequently, a cropping procedure is applied (in analogy to R1 and R2) to achieve a final resolution of 640\u00d7640 pixels. Dataset S1 also includes a small share of background images (7-8%).\n4.2. Evaluation Metrics\nEnsuring security against unauthorized drone intrusion necessitates precise early-stage detection. Thus, an exceptionally low false negative rate (FNR) is essential for a reliable detection system. However, in scenarios involving continuous data streams (e.g., commonly encountered in surveillance settings), detecting a drone in every frame of the sequence is not imperative. Extrapolating from adjacent frames enables (to a certain extent) partial inference of missed detections. Considering drone detection as an integral component of a comprehensive security framework, the reduction of false positives is also crucial for system credibility. This is akin to reducing the false discovery rate (FDR). Complementing the evaluation via FNR and FDR, we include the mean average precision (mAP) at an intersection over union (IoU) threshold of 0.5, due to its widespread adoption as key performance indicator for object detection models. As the requirement for precise bounding box localization can be alleviated in our application context and manually generated annotations exhibit notable variance in quality, we also consider mAP values at an IoU threshold of 0.25.\n4.3. Implementation Details\nYOLO-FEDER FusionNet is implemented in PyTorch, leveraging the foundation of the original YOLOv5 framework provided by [3]. It incorporates a YOLOv51 backbone pre-trained on the COCO benchmark dataset [32], as well as a FEDER network initialized"}, {"title": "5. RESULTS", "content": "In this section, we present the evaluation results of our proposed framework, providing an examination of its performance on real-world data, the mitigation of labeling biases through a post-processing strategy, and the assessment of the framework's effectiveness in an alarm scenario.\n5.1. Performance on Real-World Data\nExamining the performance of YOLO-FEDER FusionNet on real-world datasets R1 and R2 (with varying cutout sizes, cf. Section 4.1) reveals promising results, particularly in contrast to the original YOLOv51 model trained and assessed on 2040\u00d71086 images [5]. Specifically, YOLO-FEDER FusionNet exhibits a significant decline in FDR of 77.2% (from 0.5 to 0.114) and 86.8% (from 0.5 to 0.066) for dataset R1 (cf. Table 2). Additionally, an exceptional FDR reduction exceeding 90.0 % is observed for R2 across both\nimage sizes, with values decreasing from 0.29 to 0.029 and 0.007 (cf. Table 3). Furthermore, there is also a distinct reduction in FNRs. The direct comparison between YOLOv51 trained on un-cropped images of S1 and evaluated on un-cropped images of R1 \u2013 and YOLO-FEDER FusionNet reveals a variance in FNRs of 0.091 and 0.014, respectively. This disparity is further highlighted when contrasting the evaluation results of YOLO-FEDER FusionNet with those of YOLOv51 on cropped versions of R1 (cf. Table 2). A substantial discrepancy in FNRs becomes even more evident for dataset R2, where highly textured objects form a significant portion of the images' background (cf. Figure 1). While YOLOv51, evaluated on the original-sized R2 images (cf. 2040\u00d71086, Table 3), exhibits a FNR of 0.745, YOLO-FEDER FusionNet significantly reduces this rate. Specifically, when evaluated on 1080\u00d71080 image cutouts of R2, the FNR diminishes to less than half of its original magnitude. This observed trend remains consistent when contrasted with YOLOv51 SQ, trained similarly to YOLO-FEDER FusionNet on a cropped version of S1. However, owing to its consistently high FNRs and FDRs close to zero (cf. Tables 2 and 3), YOLOV51 SQ demonstrates a general inefficiency in the present context of drone detection. On the contrary, this highlights the performance advantages attained through the integration of YOLOv51 and FEDER within YOLO-FEDER FusionNet.\nAnalyzing the mAP values at an IoU threshold of 0.5 reveals distinct trends. Within dataset R1, there is a significant improvement in mAP, rising from 0.559 (cf. YOLOv51, 2040\u00d71086, Table 2) to 0.636 and 0.669 upon implementing YOLO-FEDER FusionNet. Conversely, a marginal decline in mAP values is evident within dataset R2 (cf. Table 3). Upon closer examination of the mAP values at an IoU threshold of 0.25, YOLO-FEDER FusionNet demonstrates superior performance when compared to YOLOv51. Specifically, YOLO-FEDER FusionNet exhibits mAP values ranging between 0.685 and 0.816, while YOLOv51 solely registers values below 0.572.\n5.2. Labeling Bias\nDespite the precise drone localization capabilities of YOLO-FEDER FusionNet (see Figure 4), a deeper analysis comparing predicted"}, {"title": "5.3. Drone Detection in an Alarm Scenario", "content": "Drone detection can also be seen as an integral component of a comprehensive security system, specifically targeting the identification of potential drone threats and the subsequent activation of warning mechanisms. Hence, inferring the existence of a drone within a video sequence does not necessarily mandate a frame-by-frame detection. Alternatively, the presence of a drone can be inferred based on a partial sequence of frames, where its appearance in at least one frame indicates its existence. This strategy leads to a decline of missed detections (cf. Table 6), albeit at the expense of an increased inference time."}, {"title": "6. CONCLUSION", "content": "In this work, we explored the effectiveness of integrating generic object detection algorithms with COD techniques for drone detection in environments with complex backgrounds. We introduced YOLOFEDER FusionNet, a novel DL architecture. Alongside the integration of dual backbones, we implemented a redesigned neck structure to enable seamless information fusion and facilitate the prioritization of essential features. We systematically evaluated the proposed detection model on a variety of real and synthetic datasets, characterized by different complexity levels. Our analyses demonstrated substantial improvements of YOLO-FEDER FusionNet over conventional drone detectors, especially in terms of FNRs and FDRs. Furthermore, we revealed a labeling bias originating from manually generated annotations in real-world data, adversely affecting mAP values. Addressing this bias via post-processing led to improvements w.r.t. mAP. We also showed that leveraging information from previous frames in a video stream can further reduce FNRs."}]}