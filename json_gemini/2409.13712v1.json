{"title": "Good Idea or Not, Representation of LLM Could Tell", "authors": ["Yi Xu", "Bo Xue", "Shuqian Sheng", "Cheng Deng", "Jiaxin Ding", "Zanwei Shen", "Luoyi Fu", "Xinbing Wang", "Chenghu Zhou"], "abstract": "In the ever-expanding landscape of academic research, the proliferation of ideas presents a significant challenge for researchers: discerning valuable ideas from the less impactful ones. The ability to efficiently evaluate the potential of these ideas is crucial for the advancement of science and paper review. In this work, we focus on idea assessment, which aims to leverage the knowledge of large language models to assess the merit of scientific ideas. First, we investigate existing text evaluation research and define the problem of quantitative evaluation of ideas. Second, we curate and release a benchmark dataset from nearly four thousand manuscript papers with full texts, meticulously designed to train and evaluate the performance of different approaches to this task. Third, we establish a framework for quantifying the value of ideas by employing representations in a specific layer of large language models. Experimental results show that the scores predicted by our method are relatively consistent with those of humans. Our findings suggest that the representations of large language models hold more potential in quantifying the value of ideas than their generative outputs, demonstrating a promising avenue for automating the idea assessment process.", "sections": [{"title": "Introduction", "content": "The rapid pace of scientific research in various disciplines has given rise to an overwhelming number of academic papers (Tabah, 1999; Bornmann and Mutz, 2015; Xu et al., 2023). Typically, ideas are commonly conveyed through these papers, which reviewers must carefully scrutinize to grasp the ideas authors present. However, amidst the vast volume of paper submissions, the review process becomes slow, labor-intensive, and less precise (Checco et al., 2021), making it a challenge to identify valuable ideas. Fortunately, with the advent of large language models (LLMs), we are presented with an unprecedented opportunity to revolutionize how we assess the merit of scientific ideas, and this work explores their use as a knowledge tool for such evaluation. In order to make idea more concrete, we take unpublished manuscripts or the latest papers as the research object.\nWhile the generative capabilities of LLMs have been widely recognized, their potential as idea (or paper) evaluative instruments has remained relatively underexplored. Recent studies (Yuan et al., 2022; Liang et al., 2023; Liu and Shah, 2023; Agarwal et al., 2024) have begun to harness LLMs for the automatic generation of paper reviews, aiming for outputs that are both informative and emulate human feedback. These efforts primarily utilize the text generation capability of LLM, which are highly dependent on the scale of model parameters. For instance, Liu and Shah (2023) proves that GPT-4 surpasses other open-source LLMs, such as LLaMA (Touvron et al., 2023), in generating reviews. Meanwhile, crafting intricate prompts containing specific commands and inquiries is essential for LLMs to produce meaningful reviews. Nonetheless, LLM-generated reviews can still reflect models' subjective biases and occasionally produce hallucinated contents (Zhang et al., 2023; Manakul et al., 2023). There is currently no research that quantitatively evaluate ideas in an objective way.\nAccording to Geva et al. (2021); Zou et al. (2023), the representations of different layers in LLM contain different semantic information, and in some tasks, the performance of the last layer is not the best. Based on this point, we suppose that the representations of LLMs encapsulate a detailed and nuanced comprehension of text, which can be leveraged to construct a systematic and objective framework for the assessment of ideas. Our research thus focuses on the quantitative evaluation of ideas through LLMs, an approach we argue is more objective than generative techniques. It is worth not-"}, {"title": "Related Work", "content": "Yuan et al. (2022) have explored the use of various NLP techniques to produce decisive and comprehensive reviews of academic papers from multiple perspectives. The work of ReviewerGPT (Liu and Shah, 2023) studies the application of LLMs as review assistants, focusing on tasks such as error identification, checklist verification, and comparative paper analysis. Another innovative approach by researchers (Liang et al., 2023) involves an automated system that employs GPT-4 to create review comments and suggestions for revisions, which are then benchmarked against feedback from human reviewers. What's more, LitLLM (Agarwal et al., 2024) equips Retrieval Augmented Generation (RAG) module to address the hallucination problem in review generation.\nIn addition to the methods mentioned above, some researches rely on external data to assess the quality of a paper. For instance, Thelwall et al. (2023) have developed a framework that predicts article quality scores using a range of bibliometric and metadata indicators, including citation counts, journal impact factors, and institutional rankings. Similarly, KQI (Wang et al., 2023) leverages the structure of citation networks to quantify the knowledge contribution of a paper. This kind of approaches, however, pertains to post-publication evaluation. Unlike our approach, which is based solely on the text of the paper itself, it does not require information about the paper's acceptance or publication status."}, {"title": "Idea Assessment", "content": "In this work, we focus on quantitative evaluation of ideas. Let $D = \\{d_1, d_2, ..., d_n\\}$ be a dataset consisting of n scientific manuscripts (papers), each representing a distinct scientific idea. The direct scoring of an idea involves mapping each manuscript $d_i$ to a quantitative score $s_i$ based on a predefined criterion $c \\in C$. The criterion set $C$ serves as the basis for the assessment and is essential for guiding the evaluation process. It can encompass various aspects of potential impacts of an idea, such as novelty, correctness, excitement, soundness, or alignment with current research trends."}, {"title": "Dataset", "content": "To ensure that the idea evaluator is well-calibrated, the benchmark idea dataset D should be representative of the scientific community and contain cutting-edge knowledge in academia. To this end, we have compiled a collection of 3,795 manuscripts that are available in PDF format from the International Conference on Learning Representations (ICLR) 2023. For the extraction of full texts from these PDFs, we employed GROBID (Lopez, 2009), a sophisticated tool for parsing academic PDF documents.\nAdditionally, the metadata of these papers includes comprehensive evaluation criteria from official reviewers, encompassing scores for overall quality, correctness, technical and empirical novelty, providing a rich ground truth for training and validation. It is possible that an idea is interesting but the paper score of a criterion such as correctness is low because there are flaws in the experiments."}, {"title": "Methodology", "content": "The purpose of our method is to train an evaluator $A_c$ to score ideas, which consists of four steps: consistency sorting, layer selection, token selection, and evaluator training. It should be highlighted that the steps of layer and token selection only exist in training process, which are determined during the inference process."}, {"title": "Consistency Sorting", "content": "In our scenario, we anticipate that models can learn the rating standard from human-rated data. Specifically, the human-assigned scores for each paper in the training set should exhibit a high level of consistency; that is, the more uniform the scores for each paper are (reflected by lower variance), the more suitable the data is for model training. Therefore, our method employs a consistency-based sorting mechanism to construct the training and testing sets. We commence by ordering the papers according to the variance in human scores for a given criterion c. Subsequently, based on a predetermined threshold for training set partitioning, papers that demonstrate high consistency (low variance) are allocated to the training set, while the remainder are designated for the testing set. This mechanism facilitates a more straightforward learning process for models to grasp the human rating standards."}, {"title": "Layer Selection", "content": "As claimed by Geva et al. (2021), lower layers of LLMs tend to capture shallow data patterns, while upper layers contain more semantic knowledge. This hierarchical processing of information within LLMs suggests that the utility of representations may vary across layers. Further to this, RepE (Zou et al., 2023) explores the relationship between layer depth and performance in utility estimation tasks, finding that middle-layer representations often yield the highest accuracy.\nInspired by these findings, our approach involves identifying the optimal layer within an LLM that provides the most effective representations for constructing an accurate evaluator. We hypothesize that a specific intermediate layer may offer the ideal balance between capturing fundamental linguistic features and the nuanced semantic understanding necessary for assessing the quality of scientific ideas. Our experiments are thus tailored to pinpoint this layer by analyzing the performance of given data across all layers. Then, we leverage its representations to enhance the performance of our idea evaluation framework."}, {"title": "Token Selection", "content": "Considering that a manuscript $d_i$ is composed of l sequential tokens, the semantic information of these token representations varies significantly. Due to the fact that most LLMs are auto-regressive models, the last token aggregates the attention information of all previous tokens (Zou et al., 2023). With a slight abuse of notation, by default, we use the last token representation $Rep(d_i, -1) \\in \\mathbb{R}^m$ to symbolize the entirety of the manuscript $d_i$.\nNevertheless, when dealing with lengthy input texts, such as full-text manuscript $d_i$, there are two issues with the default approach. For one thing, memory optimization mechanism such as VLLM (Kwon et al., 2023) should be adopted to prevent GPU from running out of memory. For another thing, the representation of the last token may become diluted or overly abstracted owing to the extensive accumulation of attention, potentially leading to a loss of specific semantic details pertinent to the overall idea assessment. To address these issues, we explore alternative strategies for token selection that aim to maintain the richness of semantic information while ensuring computational feasibility.\nWe consider a manuscript $d_i$ to be a composition of distinct sections. We select the last token representations from each section, and concatenate these to form a composite representation. The approach allows us to capture the essence of each section. Formally, if a manuscript $d_i$ is divided into r sections, and $Rep(d_{i,j, -1})$ represents the last token of the jth section, then the combined representation $Rep(d_i)$ is given by:\n$Rep(d_i) = \\bigoplus_{j=1}^{r} Rep(d_{i,j,-1}),$ \nwhere $\\bigoplus$ denotes the concatenation operation, and $Rep(d_i)$ is in $\\mathbb{R}^{r \\times m}$. Similarly, we can take into account the full length of the manuscript and divide it into equidistant segments based on a predefined length to obtain $Rep(d_i)$. By experimenting with these strategies, we aim to refine our approach to token selection and optimize the representation of manuscript for more accurate idea assessment."}, {"title": "Evaluator Training", "content": "In this part, we use the pre-processed $Rep(d_i)$ to train an idea evaluator $A_c$. Let $s_i$ be the average score given by humans for manuscript $d_i$, reflecting its overall quality according to the criterion c. The average score serves as the ground truth in our training process. The evaluator $A_c$ is instantiated as an Multilayer Perceptron (MLP) with one hidden layer. The MLP is tasked with learning the mapping from the representation space to the scalar scores, which takes as input the representation $Rep(d_i)$ for each manuscript $d_i$ and outputs a predicted score $\\hat{s_i}$. To optimize all parameters of the MLP, we employ the Mean Squared Error (MSE) loss function:\n$\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} (s_i - \\hat{s_i})^2.$\nBy minimizing $\\mathcal{L}$, the MLP learns to approximate the human-assigned scores as closely as possible. Through this training process, we aim to calibrate the evaluator $A_c$ such that it can reliably predict the value of new or unseen ideas based on their textual representations."}, {"title": "Experiments", "content": "This section presents a series of experiments to verify the performance of LLMs in the task of quantitative evaluation of ideas. Our main focus is on the mean value of the criteria overall quality, which is used as the training objective for the idea evaluator. Through our released benchmark and the experimental methodology, we answer the following four research questions (RQs):\n\u2022 RQ1: To what extent do the representations from LLMs correlate with human judgements in the evaluation of scientific ideas? Additionally, is the LLM generation method suitable for this task?\n\u2022 RQ2: What is the impact of choosing different layers and tokens for LLM representations on the performance of idea evaluation?\n\u2022 RQ3: How significantly does the consistency of human judgements influence the performance of LLM representations in this context?\n\u2022 RQ4: How does the size of training set impact the correlation between $A_c$ evaluations and human judgments in idea assessment?"}, {"title": "Baselines", "content": "We categorize the baselines into three distinct groups: LLM Generation, LLM Representation, and Human Evaluation. The first category involves LLMs generating numerical scores in response to textual descriptions of ideas. This category includes models such as GPT-3.5-turbo, LLaMa-2-7b-base (Touvron et al., 2023), and Baichuan-2-7b-base (Yang et al., 2023), which are fine-tuned using techniques like LoRA (Hu et al., 2021) or with full parameter updates. In the LLM Representation category, we evaluate models like BERT (Kenton and Toutanova, 2019), SciBERT (Beltagy et al., 2019), and RePE (Zou et al., 2023). For BERT and SciBERT, we also apply our proposed framework to quantify the value of ideas, with the primary distinction being in the token selection strategy. Specifically, we used the [CLS] token as the representation of an idea, and if the length of a section exceeds 512 tokens, we will divide it into equidistant subsections to apply the token selection strategy for BERT-like models. Moreover, we also analyze the performance of human evaluators through randomly selecting one score from the human-rated list against other scores."}, {"title": "Training Settings and Evaluation Details", "content": "In our implementation, our method employs LLaMA-2-7b-base as the foundational model. In order to make our experiments more solid and validate our framework is model-agnostic, we also use Baichuan-2-7b-base as the base model, the results of which are provided in Appendix C. We use the grid search to find appropriate sets of hyperparameters for baselines and our proposed method. For the configuration of the MLP evaluator, we choose a batch size of 32, a hidden layer dimension of 1024, a learning rate of 0.001, a dropout rate of 0.2, and employ the Adam optimizer. We limit the training to 20 epochs. All experiments are first conducted to evaluate the efficacy of our framework using the abstracts of papers for all research questions. We also explore the effects of using the full texts of papers as the training inputs for the token selection in RQ2.\nTo gauge the alignment of scores generated by"}, {"title": "Comparative Experiments (RQ1)", "content": "According to the principle of consistency sorting in Section 5.1, we construct training sets using the top 5% and top 30% ratios from ICLR23-low-std and ICLR23-all datasets respectively to preliminarily exclude the influence of dataset proportion on the conclusion, and take the rest of each dataset as the testing set. It can be observed that our proposed method achieves the best performance in all settings, where the performance on ICLR23-all is at most 30% higher than the second best method SciBERT. As expected, the correlation of ICLR23-low-std among human scores is close to 1, which is attributed to our data partitioning strategy. It should be noted that the correlation of our method on ICLR23-all dataset exceed the result of humans, when the training ratio is 30%, proving the feasibility of our method and its potential ability to be applied to real-world review scenarios. Moreover, in terms of different layers' performance, the middle and back layers of most models may achieve better results.\nFor the LLM Generation baselines, the fine-tuned LLaMA-2 is worse than Baichuan-2, especially for the LLaMA-2-Full-SFT, fine-tuned with full parameters, lacking the capability of effective evaluation since its pvalue > 0.05. Due to the inability of GPT-3.5 being fine-tuned, we adopt the zero-shot setting, which is only for sketchy reference. Overall, the LLM Generation methods are not competent for the quantitative evaluation of ideas. Furthermore, our experiment studies the degree of consistency between the predicted score and that"}, {"title": "Score Distribution (RQ1)", "content": "We also examine the difference (absolute error) between human-rated scores and the predicted scores on ICLR23-low-std dataset. We can see that 86.8% paper scores generated by our method are close to the human-rated scores, where the differences between them are lower than 2. Additionally, the distributions are shown in the right part of Figure 2. The distribution of scores predicted by our idea evaluator is normal distribution as expected while the human reviewers tend to give more higher or lower scores."}, {"title": "Influence of Layer Selection (RQ2)", "content": "We analyzed the representational efficacy across various layers of LLM and SciBERT. Given the nuanced role of layer-specific representations in the context of assessing the merit of scientific ideas, we propose a layer selection of representations for the task at hand. Specifically, we advocate for the utilization of representations from the layers situated in the last one-third of the model's depth. Such choice is informed by the empirical evidence suggesting that these layers strike a balance between retaining rich semantic content and providing the necessary abstraction for discriminative tasks."}, {"title": "Influence of Token Selection (RQ2)", "content": "In this part, we use the ICLR23-all dataset to investigate the influence of token selection. In terms of the paper abstract inputs, we first test the correlation results of using the last token. The findings, as presented in Table 4, indicate that solely relying on the last token yields superior results compared to combining it with the middle token. The latter approach appears to introduce a surplus of redundant information that may hinder the downstream performance of evaluator.\nWhen dealing with the full text of papers, we implement two token selection strategies outlined in Section 5.3: the amalgamation of last tokens from equidistant segments and the aggregation of last tokens from distinct paper sections. The ex-"}, {"title": "Analysis of Training Set (RQ3 & RQ4)", "content": "Figure 4 shows the Spearman correlations varying with layers under different training ratios on ICLR23 datasets. For ICLR23-low-std dataset, the human-rated scores in the dataset are highly consistent, and it is observed that the Spearman correlation tends to improve in tandem with increases in the training set size. Notably, when the proportion of data used for training surpasses the 50% threshold, the correlation between the scores predicted by our idea evaluator and those assigned by human experts becomes moderate, exceeding 0.4. Furthermore, our analysis reveals that even a relatively small subset of the data (with a training ratio of 5%) is capable of yielding positive performance.\nAs to ICLR23-all dataset, the outcomes indicate that an increase in the volume of training data does not necessarily correspond to a higher alignment with human evaluations in the testing set. This phenomenon can be attributed to the diminishing consistency of human scores as the dataset expands; that is, the variance in human-assigned scores grows with the size of the dataset. It becomes evident that while a larger training set generally provides more information, it also introduces a greater diversity of human judgment, which may not always be conducive to improving the ability of evaluator to mimic human scoring behavior."}, {"title": "Conclusion and Future Work", "content": "The study focuses on the quantitative evaluation of scientific ideas. We have reviewed existing methodologies for paper and idea evaluation and have broken new ground by focusing on the quantitative aspect of idea evaluation. Specifically, we first introduce a comprehensive benchmark dataset, accessible to the research community. Then, we develop a new framework that leverages the token representations of specific layers in LLM to quantify the value of ideas. Through rigorous experiments, we demonstrate that LLM representations correlate more strongly with human judgments compared to generative text outputs. Additionally, in our benchmark, the predicted scores of more than 80% papers are close to human-rated scores. In the future, we will broaden the scope of our research to encompass diverse disciplines with balanced data ratios, including the exact and social sciences, to further validate and refine our evaluative framework."}, {"title": "Limitations", "content": "Discipline\nThe scope of our research is confined to the field of computer science, which may restrict the broader applicability of our framework. The generalization performance of our model across different scientific disciplines remains an open question. Future research endeavors should aim to adapt and validate the framework in diverse fields, ranging from the exact sciences to the humanities.\nCriterion\nOur experiments have primarily focused on the overall quality score of manuscript papers, which is a composite yet somewhat abstract. Important aspects such as the correctness of the presented work and its novelty are equally critical in determining a paper's impact and significance. In forthcoming studies, we plan to dissect these individual criteria, developing a more granular approach to idea evaluation.\nModel Scale\nThe impact of model scale on performance is an aspect that has not been extensively explored in our research. The performance of LLMs is often closely tied to the number of parameters they contain; thus, models with different sizes may yield different results in the task of idea evaluation. Larger models may have the capacity to encode more nuanced representations of text, potentially leading to more accurate assessments of scientific ideas. Conversely, they may also introduce complexities that do not necessarily translate to better performance, such as overfitting or increased computational costs. The trade-offs between model size, accuracy, and efficiency are still an area ripe for exploration.\nEthics Statement\nThe dataset used in our study consists of publicly available academic papers. We have ensured that all data was collected and handled in a manner that respects the privacy and intellectual property rights of the authors. No personal data was used, and all information is attributed to its original source.\nWe are committed to transparency in our research process. To this end, we have made our benchmark dataset publicly available and have provided detailed descriptions of our methodologies and experimental setups to facilitate reproducibility by other researchers."}, {"title": "Case Study", "content": "We list four cases to show the performance and drawbacks of our method in Table 7. All these cases are selected from the domain of reinforcement learning. The first two cases are correctly predicted, and the human-rated scores and LLM representation scores are very close. As to the third case, although our method gives an overestimated score, the final score is not enough to make it acceptable. The fourth case is underestimated. One possible reason is that in such cases, our method may lack more contextual information to make a decision, such as tables and figures in papers, which is also something we need to consider in the future."}, {"title": "Domain Analysis", "content": "To see how our method rates ideas on popular topics or less trendy domains, we analyze the score distributions and differences between human-rated scores and LLM representation scores in 14 domains divided by ICLR-2023 program committee. On the whole, the differences in mean scores of most domains are less than 10%. However, there are three domains (Theory, Neuroscience and Cognitive Science, Infrastructure) where the mean values of human-rated scores are relatively higher than average, and the differences exceed 10%. We believe these three domains are distinguished from other domains since others frequently focus on Learning and Optimization, which makes our evaluator overfitting on these data. Therefore, it is necessary to train a domain-specific evaluator, while also maintaining a balance in the content of the dataset. We will address these issues in our future work."}, {"title": "Frequently Asked Questions", "content": "Numerical Processing Limitations in LLMS\nA critical issue faced by LLMs is their inherent difficulty in processing numerical data, such as digits. This limitation stems from the finite-sized vocabulary and tokenization strategies used by these models, affecting both encoder and decoder architectures. This impacts the ability to perform tasks that require precise numerical understanding. Therefore, we leverage the deep, contextual representations within LLMs to quantify the value of scientific ideas. These representations encapsulate rich semantic and contextual information that extends beyond the superficial token sequences.\nOur approach diverges significantly from the strategy of merely adding task-specific heads to the model. Instead, our approach involves a strategic selection of layers and tokens. By leveraging the hierarchical processing capabilities of LLMs, we can harness the most relevant and informative features for idea evaluation. This approach contrasts with using the entire weight set of the LLM (adding head to LLM), which might not be as efficient or effective for capturing the specific attributes necessary for assessing the value of scientific ideas.\nFramework\nThe current design of our framework shows considerable promise in the automated assessment of scientific ideas, yet there are avenues for further enhancing the evaluator's performance. The quantitative assessment of scientific ideas is inherently complex, involving a blend of objective metrics and subjective judgments. Our method, leveraging the representations of large language models, demonstrates the potential to approximate human judgment to a significant degree, which will provide human reviewers with an objective score, rather than replacing them to give subjective comments from multiple perspectives."}]}