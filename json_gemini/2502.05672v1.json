{"title": "On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers", "authors": ["Miroslav \u0160trupl", "Oleg Szehr", "Francesco Faccio", "Dylan R. Ashley", "Rupesh Kumar Srivastava", "J\u00fcrgen Schmidhuber"], "abstract": "This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) algorithms are designed to learn policies that choose optimal actions while interacting with an environment. The environment does not reveal optimal actions, but only provides higher rewards for taking better actions. This is in direct contrast with Supervised Learning (SL), where the correct output for each input is available for learning. Nevertheless, a series of algorithms have been proposed that attempt to solve reinforcement learning tasks using purely supervised learning techniques. Upside-Down Reinforcement Learning (Schmidhuber, 5 Dec 2019; Srivastava et al., 2019) (UDRL) inverts the traditional RL process by mapping desired returns/goals to actions, treating action prediction as a supervised learning problem. Goal-conditioned Supervised Learning (Ghosh et al., 2021) (GCSL) utilizes goal information to guide the model's learning process, and Online Decision Transformers (Zheng et al., 2022) (ODT) leverage transformer architectures to model entire trajectories, treating past states, actions, and rewards as sequences to predict optimal actions. Experiments have shown that in addition to being strikingly simple and scalable due to their dependence on SL, these algorithms can produce good results on several RL benchmarks (such as Vizdoom (Kempka et al., 2016), robotic manipulation (Ahn et al., 2020) and locomotion (Fu et al., 2020)). Their theoretical understanding, however, is limited to heuristics and the study of restrictive special cases. Through a rigorous analysis of convergence and stability, this work initiates the development of a theoretical foundation for algorithms that build on the broad idea of approaching RL via SL or sequence modeling. Two questions guide our investigation: 1) What can be said about the convergence of UDRL, GCSL and ODT assuming that an explicit model (transition kernel) for the underlying Markovian environment is given? What behavior can be expected of typical objects of interest, such as policies, state and action-values in the limit of infinite resources. 2) How stable are these quantities under perturbation or in the presence of errors in the environment model? Guarantees that ensure that algorithms reliably identify optimal solutions and remain stable under varying conditions are foundational for their practical deployment in real-world systems (Bertsekas and Tsitsiklis, 1996; Nocedal and Wright, 2006).\nStepping back to establish some basic background, it is notable that UDRL, GCSL, and ODT are very similar algorithms. Although architectural details vary, at their core stand common ideas about the acquisition of information by the learning agent. They all focus on directly predicting actions based on reward signals from trajectories, rather than learning value functions. The key shared ingredient is the interpretation of rewards, observations and planning horizons as task-defining inputs from which a command is computed for the learning agent. The agent's rule of action (the policy) is then updated through SL, to map previous trajectory observations and commands into actions, completing the learning process. More formally, suppose a number of trajectory samples have been collected by a learning agent that follows a certain rule of action $\\pi_{old}$. Given a trajectory segment that starts with a state-action pair $(s, a)$, has a length of $h$, and where the goal $g$ is a quantity that is computed from features of the segment (such as the sequences of states and rewards), one could reason that the action $a$ is useful for achieving $g$ from $s$ in $h$ steps. It then appears natural to interpret $(h, g)$ as a command for the agent, fitting the new rule of action $\\pi_{new}$ to the distribution a|s, h, g using SL,\n$\\pi_{new} = \\underset{\\pi}{\\mathrm{argmax}}  E[loss(\\pi(a|s, h, g))].$\nwhere $loss$ is an appropriate loss function and the expectation is computed over all segments in the trajectory sample. Learning proceeds iteratively by replacing $\\pi_{old}$ with $\\pi_{new}$, sampling new trajectories from $\\pi_{old}$, computing the achieved horizons and goals $(h,g)$, and finally using this information to update $\\pi_{new}$.\nOur analysis of the UDRL, GCSL, and ODT algorithms is conducted within the overarching framework of episodic UDRL (eUDRL), which is characterized by a specific structure of the goal. Specifically eUDRL assumes that a goal map $\\rho$ is applied to a segment's terminal state to evaluate whether the goal has been reached $g = \\rho(s_f)$. GCSL can be viewed as a slightly restricted version of eUDRL, as it focuses solely on state-reaching tasks and operates with a fixed horizon. Decision Transformer (Chen et al., 2021) (DT) essentially corresponds to one iteration of eUDRL aimed at offline RL, and ODT can be seen as a form of eUDRL with entropy regularization; see the background section 2 for details. Unlike standard RL policies which define an agent's action probabilities based on a given state, the ''policies'' $\\pi_{old}$ and $\\pi_{new}$ also condition these probabilities on the command $(h, g)$. This leads to the formalism of Command Extensions (cf. definition 1), a special class of Markov Decision Processes, where the command is included as part of the state, $s = (s,h,g)$. On the technical side this article develops the mathematics of Command Extensions, which provides a solid foundation for exploring RL through SL.\neUDRL's learning process only requires a single SL step, with no need for value function or policy proxies. While simplicity and high experimental performance coincide remarkably in the eUDRL algorithm, it is evident only in deterministic environments that eUDRL identifies the optimal policy. This limitation was acknowledged already in the original work (Schmidhuber, 5 Dec 2019), where an alternative approach was proposed for non-deterministic environments (e.g., by operating with expected returns rather than just actual returns). In practice, eUDRL's simplicity has led to its adoption in non-deterministic environments as well. In part this was based on the observation that many non-deterministic environments exhibit only minor non-determinism, i.e. they can be viewed as perturbed deterministic environments. This approach was explored in the case of eUDRL in the article (Srivastava et al., 2019), as well as concurrently in the case of GCSL in (Ghosh et al., 2021), demonstrating the algorithm's practical utility in various domains, including several MuJoCo tasks. However, these articles did not provide solid convergence guarantees. While Ghosh et al. (2021) showed that GCSL optimizes a lower bound on the goal-reaching objective, the guarantees regarding its tightness are limited. In fact, eUDRL's goal-reaching objective can be sensitive to perturbations of the transition kernel even in near-deterministic environments. We will discuss this behavior in several instances throughout the article illustrating it with specific examples and computations. The algorithm's stability is clearly a concern."}, {"title": "1.1 Outline", "content": "Section 2 provides the necessary background for understanding the article.\nSection 3 describes the eUDRL recursion in specific segment subspaces and discusses the connection to the algorithm Reward-weighted Regression.\nSection 4 describes sets of states where eUDRL is stable under environment perturbations across all iterations, which proves instrumental for the discussion of continuity of eUDRL-generated quantities in the subsequent sections.\nSection 5 proves that eUDRL converges to optimal policies in case of deterministic environments.\nSection 6 investigates the continuity of eUDRL-generated quantities at a finite number of iteration for deterministic environments.\nSection 7 investigates the continuity of sets of accumulation points of eUDRL-generated quantities for deterministic environments.\nSection 8 investigates the continuity of sets of accumulation points of quantities generated by regularized eUDRL in full generality.\nSection 9 discusses related work and section 10 concludes the presentation.\nTo make this article accessible to a wider audience several appendices are included.\nAppendix A contains details about the construction of segment distribution.\nAppendix B contains examples from the main text, worked out in full detail.\nAppendix C investigates the continuity of eUDRL-generated quantities at interior points (of the set of all transition kernels) for a finite number of iterations.\nAppendix D contains all details on lemmas and proofs about the regularized eUDRL recursion of section 8."}, {"title": "2 Background", "content": "This section provides the necessary background for understanding the article. It offers an orientation on the placement of our work within the \"theoretical landscape\", introducing fundamental theoretical concepts, particularly focusing on Markov Decision Processes of Command Extension type and distributions over segment space. Furthermore, this section outlines how our work fits within the existing literature, describing the integration of ODTs within the eUDRL framework. The innovative developments presented in this article will be introduced in the following sections.\nWe will be dealing exclusively with finite, real random variables. They are defined as maps $X : \\Omega \\rightarrow \\mathbb{R}$ with a measure space $(\\Omega, \\mathcal{F}, P)$, where $\\Omega$ is a finite set, $\\mathcal{F}$ is a $\\sigma$-algebra over $\\Omega$ and $P$ denotes a probability measure."}, {"title": "2.1 Markov Decision Process", "content": "Markov decision processes (MDPs) are a mathematical framework for sequential decision problems in uncertain dynamic environments (Puterman, 2014). Formally an MDP is a five-tuple $\\mathcal{M} = (S, A, \\lambda, \\mu, r)$, with a set $S$ of admissible states, a set of possible actions $A$, a transition probability kernel $\\lambda$ that defines the probability of entering a new state given that an action is taken in a given state, a distribution $\\mu$ over initial states, and a (random) reward $R$ that is granted through this transition. In MDPs an agent interacts iteratively with an environment over a sequence of time steps $t$ (where we add the subscript $t$ to random variables to emphasise that they belong to a specific point of time). Let the random variables $S_t : \\Omega \\rightarrow S$ describe the state of the MDP and the random variables $A_t : \\Omega \\rightarrow A$ describe the actions chosen by the agent. Beginning with an initial distribution over states $\\mu(s) = P(S_0 = s)$ at each step of the MDP the agent observes the current $s \\in S$ and takes a respective action $a \\in A$ according a policy $\\pi(a|s) = P(A_t = a|S_t = s)$, subsequently the environment transitions to $s'$ according to the probability $\\lambda(s'|s,a) = P(S_{t+1} = s' | S_t = s, A_t = a)$. A transition kernel $\\lambda$ is deterministic if for each $s,a \\in S \\times A$ the distribution $\\lambda(\\cdot|s, a)$ is deterministic, i.e. if there exists a state $s'_{s,a}$ such that $\\lambda(s'_{s,a}|s,a) = 1$. Similarly a policy is deterministic if for each $s \\in S$ the distribution $\\pi(\\cdot|s)$ is deterministic. Consecutive sequences of state-action transitions are commonly called trajectories of the MDP. In this article we will always assume deterministic rewards, i.e., the reward is given by $R_{t+1}(S_{t+1}, S_t, A_t) = r(S_{t+1}, S_t, A_t)$ with a deterministic function $r$. The return $G_t = \\sum_{k \\in \\mathbb{N}_0} R_{t+k+1}$ is the accumulated reward beginning with time $t$ over the entire MDP episode, where we do not discount future rewards. The performance of an agent following a policy $\\pi$ can be measured by means of the state value function $V^{\\pi}(s) = E[G_t | S_t = s; \\pi]$ and the action value function $Q^{\\pi}(s, a) = E[G_t | S_t = s, A_t = a; \\pi]$. There exist a unique optimal state value function $V^* = \\max_{\\pi} V^{\\pi}$ and a unique optimal action value function $Q^* = \\max_{\\pi} Q^{\\pi}$, where the maximization is taken over the set of all policies. A policy $\\pi^*$ for which $V^{\\pi^*} = V^*$ and consequently also $Q^{\\pi^*} = Q^*$ is called optimal. In what follows we will work with special types of MDPs called Command Extensions. In this article we will reserve the symbols $\\pi, V^{\\pi}, Q^{\\pi}$ for the respective quantities for MDPs of Command Extension type."}, {"title": "2.2 Markov Decision Processes of Command Extension Type", "content": "The aim of the training process in eUDRL is to achieve that the agent becomes better at executing commands. In eUDRL the command is provided to the agent in the form of a \"goal\" and a \"horizon\". The goal says which state the agent is supposed to achieve and the horizon says when this state should be achieved. A goal map $\\rho: S \\rightarrow G$ will be employed to evaluate if the command's goal has been reached at the command's horizon, i.e. if $\\rho(s)$ equals to the command's specified goal for the final state $s$. The codomain $G$ of the goal map is the set of all possible goals, weather they are reached in one specific trajectory or not. Notice that since $\\rho$ is defined on the entire state space $S$, every states at the horizon corresponds to a valid goal $\\rho(s)$. However, this goal might not be the specified target of the chosen command. The introduction of the goal map allows one to study prototypical eUDRL tasks within a unified formalism: The state reaching task is covered by $G = S, \\rho = id_S$ (the identity map on $S$), while goals related to return of the original MDP $\\mathcal{M}$ can be covered by extending the state by a component that accumulates rewards and defining $\\rho$ to be the projection on this component.\nApart from the MDP state the eUDRL agent takes an additional command input. Let the random variable $G_t: \\Omega \\rightarrow G$ describe the goal of the eUDRL agent and let $H_t : \\Omega \\rightarrow \\mathbb{N}_0 = \\{0, 1, ..., N\\}, N \\ge 1$, be a random variable that describes the remaining horizon. The eUDRL agent can be viewed as an ordinary agent operating in an MDP whose state space is extended by the command. In this context one has to provide an initial distribution over goals and horizons $P(H_0 = h,G_0 = g | S_0 = s)$ and with each transition of the extended MDP the remaining horizon decreases by 1 until a horizon of 0 is reached. At this stage the extended MDP enters an absorbing state (see below for a definition), from which no further evolution occurs. A reward is granted if the specified goal is reached when the horizon turns to 0. In summary we have the following definition of command extensions:\nDefinition 1 (Command Extension) A Command Extension (CE) of an MDP of the form $\\mathcal{M} = (S, A, \\lambda,\\mu,r)$ is an MDP of the form $\\mathcal{M} = (\\hat{S}, A, \\hat{\\lambda}, \\hat{\\mu},\\hat{r}, \\rho)$, where:\n$\\bullet$ A command is a pair $(g,h) \\in G \\times \\mathbb{N}_0$, where $G \\subset \\mathbb{Z}^{n_G}$ and $\\mathbb{N}_0 = \\{0,1,..., N\\}$. $N \\ge 1$ stands for the maximum horizon and $G$ for the goal set of the CE. The goal map $\\rho:S \\rightarrow G$ is used to evaluate if a goal $g \\in G$ has been reached.\n$\\bullet$ The extended state space is $\\hat{S} = S \\times \\mathbb{N}_0 \\times G$ and the extended state is a triple $s = (s,h,g) \\in \\hat{S}$ composed of an original MDP state $s$ and the command $(g,h)$.\n$\\bullet$ The initial distribution of $\\mathcal{M}$ is given by a product of a distribution of commands and the initial distribution of $\\mathcal{M}$\n$\\hat{\\mu}(s) = P(H_0 = h, G_0 = g | S_0 = s)\\mu(s)$.\n$\\bullet$ The transition kernel $\\hat{\\lambda}$ is defined for all $(s,h,g) \\in \\hat{S}$, all $s' \\in S$ and $a \\in A$ by\n$\\hat{\\lambda}((s', h -1,g) | (s,h,g), a) = \\lambda(s' | s, a) \\quad \\text{ if } h > 0,$\n$\\hat{\\lambda}((s',h,g) | (s,h,g), a) = \\delta_{ss'} \\quad \\text{ if } h = 0.$"}, {"title": "2.3 Trajectory and Segment Distributions of CEs", "content": "A key role in the eUDRL algorithm is played by segments of sampled CE trajectories. A CE trajectory contains a sequence of consecutive transitions comprised of extended states and actions. In full generality CE trajectories could be represented by infinite sequences of the form\n$\\tau = ((s_0", "T": "Omega \\rightarrow \\mathrm{Traj}$ a random variable with components $\\mathcal{T} = ((S_0, H_0, G_0), A_0, ..., (S_N, H_N,G_N))$ whose outcomes are CE trajectories. The probability of $T \\in \\mathrm{Traj}$ is given by\n$P(T=\\tau; \\pi) = \\mu(s_0) \\cdot \\prod_{t=1}^{l(T)} \\lambda(s_t | s_{t-1}, a_{t-1}) \\cdot \\prod_{t=0}^{N} \\pi(a_t|s_t).$\nEach segment contains a chunk of consecutive state-action transitions from a given trajectory and also the segment's initial horizon, goal and length. We will represent segments by tuples of the form\n$\\sigma = (l(\\sigma), s^0, h^0, g^0, a_0, s^1, a_1, \\cdot\\cdot\\cdot, s_{l(\\sigma)})"}]}