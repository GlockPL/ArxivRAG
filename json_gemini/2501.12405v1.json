{"title": "Scopes of Alignment", "authors": ["Kush R. Varshney", "Zahra Ashktorab", "Djallel Bouneffouf", "Matthew Riemer", "Justin D. Weisz"], "abstract": "Much of the research focus on Al alignment seeks to\nalign large language models and other foundation mod-\nels to the context-less and generic values of helpfulness,\nharmlessness, and honesty. Frontier model providers\nalso strive to align their models with these values. In this\npaper, we motivate why we need to move beyond such\na limited conception and propose three dimensions for\ndoing so. The first scope of alignment is competence:\nknowledge, skills, or behaviors the model must possess\nto be useful for its intended purpose. The second scope\nof alignment is transience: either semantic or episodic\ndepending on the context of use. The third scope of\nalignment is audience: either mass, public, small-group,\nor dyadic. At the end of the paper, we use the proposed\nframework to position some technologies and work-\nflows that go beyond prevailing notions of alignment.", "sections": [{"title": "1 Introduction", "content": "A new paradigm of artificial intelligence (AI) is upon us.\nLarge language models (LLMs) or foundation models, to\nrefer to the same concept irrespective of data modality\nmore generally, are finding uses across application domains\n(Bommasani et al. 2021). Such use is often to generate new\noutputs such as text, images, code, or molecules-based\non the training data and a prompt. Although there is ex-\ncitement for LLMs, and practical applications are seeing a\nreturn on investment, we must be aware of their risks and\nharms (Weidinger et al. 2022; Shelby et al. 2023). Tra-\nditional risks of AI, like lack of fairness, robustness, ex-\nplainability, transparency, and uncertainty quantification are\npresent in LLMs, but LLMs amplify the existing harms and\nintroduce new harms. Some new harms include hallucina-\ntion or lack of factuality, hateful speech, prompt injection,\ninformation leakage, copyright infringement, bullying and\ngaslighting. Thus, a major current endeavor in responsible\nAI beyond making LLMs as helpful as possible is to make\nthem as harmless and honest as possible (Sun et al. 2024).\nLLMs are being described in terms of their behaviors using\nthe language of psychology. Although we must be careful\nwith respect to anthropomorphism (Shneiderman and Muller\n2023), behavior is emerging as the lingua franca for speci-\nfying desired input-output relationships of LLMs and for\nevaluating their performance.\nValues are fundamental beliefs that guide behaviors. They\nindicate the importance of various things and actions to a\nperson or group of people, and determine the best ways to\nbehave. As shown in Fig. 1, there are many ways throughout\nthe AI application development lifecycle to get an LLM to\nbehave in a desired fashion. Data scraping, curation and pre-\ntraining an entire model from scratch offer an approach that\nis often prohibitively costly in terms of both data and com-\nputation. Safeguarding, through moderations or constrained\ndecoding based on classifiers or detectors for various harm\ndimensions (Achintalwar et al. 2024a), is lightweight in data\nand computing resources but may not entirely capture all\nsorts of desired behaviors.\nThe steps in the middle of the lifecycle between LLM\npre-training and safeguarding are known these days as align-\nment (Shen et al. 2023). Alignment steps are meant to em-\nbed behaviors and values in the AI system and may be\ndone over more than one round. (In the figure, the dot-\nted line implies that alignment is an optional step.) How-\never, alignment is an empty signifier': something without\nan underlying meaning (Kirk et al. 2023). Different parties\nhave appropriated the term to refer to various goals and ac-\ntions. Lipton quips (Lipton 2024): \u201cAlignment is now de-\nfined so broadly that all of AI, all of ML, and the entire\nhistory of technology is\u2014and always has been-'alignment\nresearch.\" Gilbert quips (Gilbert 2024): \u201cAlignment-the\nsensible kind anyway-is just human-centered computing.\"\nFrontier model providers typically refer to alignment as en-\ndowing LLMs with instruction-following behavior and help-\nful/harmless/honest values (Bai et al. 2022; Ouyang et al.\n2022).\nThere are several ways of carrying out the goal of align-\nment, each having a different cost profile. Full fine-tuning\nchanges all of the parameters of the LLM and can be done\nthrough supervised learning and/or reinforcement learning.\nParameter efficient fine-tuning changes a small number of\nparameters, often through low-complexity adapters. Prompt\nengineering does not change the LLM at all, but relies on\nprompts to achieve the desired behavior. Additionally, sys-\ntem prompts special prompts appended before all inputs\nare commonly developed and sometimes made public by"}, {"title": "2 Limitations of Alignment", "content": "There are limitations to the LLM lifecycle and how LLM\nalignment has been conceived and carried out to date.\nThe alignment methods and approaches of frontier model\nproviders lead to a singular set of values that are intended\nacross all contexts, all deployments, and all time. Many re-\ncent empirical studies show the set of knowledge and values\nwithin LLMs is sociopolitically biased toward dominant cul-\ntural sources (Durmus et al. 2023; Feng et al. 2023). The root\ncause is alignment, as much as, or more than pre-training (Qi\net al. 2023).\nMoral psychology is the key sub-area within psychol-\nogy that deals with values. Morality is the differentiation of\nbehaviors between those that are right (proper) and those\nthat are wrong (improper); it can be a body of princi-\nples derived from a particular religion or culture. It can\nbe highly personal as well. Morality captured by multi-\nlingual language models does not reflect cultural differences,\nbut is dominated by high-resource languages and cultures\n(Haemmerl et al. 2023; Scherrer et al. 2024). Such anal-\nsis has been organized around moral foundations theory,\nwhich sets up six dichotomies: care/harm, fairness/cheating,\nloyalty/betrayal, authority/subversion, sanctity/degradation,\nand liberty/oppression (Graham et al. 2013).\nClear patterns show up empirically through the lens of\nmoral foundations theory, but we may ask if there is some-\nthing more basic going on. The prevailing approach to align-\nment seems to espouse the silver rule do not do to oth-\ners what you would not like them to do to you and the\ngolden rule do to others what you would like them to do\nto you. The focus is completely on the person doing the ac-\ntion (agent) and their perception, not on the person to whom\nthe action is done (patient). In contrast, the platinum rule\nthat foregrounds the patient and their perception do to\nothers what they would like done to them - is currently not\nadequately incorporated in alignment. Yet, all three are im-\nportant.\nDyadic morality theory, an alternative to moral founda-\ntions theory, specifically considers moral issues as situa-\ntions where an intentional agent is perceived to cause dam-\nage to a vulnerable patient (Schein and Gray 2018). And,\nit is the differences in the perception of who or what is\nan intentional agent, who or what is a vulnerable patient,\nand what constitutes a causal relationship, that lead to dif-\nferences in morality. Are weather phenomena intentional\nagents? How about AI systems? Are unborn children vul-\nnerable patients? Can the saying of a mantra be the cause of\ndamage? Moreover, these perceptions may be different and\nchange based on the type of damage, the context, or other\nfactors. Thus, dyadic morality theory is less universal and\nmore contextually-scoped than the moral foundations theory\nthat undergirds prevailing views of alignment.\nIt is under this light of different perceptions of inten-\ntional agents causing damage to vulnerable patients that we"}, {"title": "3 Different Scopes of Alignment", "content": "Alignment should not be just one activity, technical ap-\nproach, or workflow. It should be more specific and scoped\nfor the different needs of different groups over time. Toward\nthis end, we propose the following three scopes of align-\nment: (1) competence, (2) transience, and (3) audience.\nBy the competence of alignment, we mean which kind\nof capability the LLM is being given further instruction\nor fine-tuning in: knowledge, skills, or behaviors. Knowl-\nedge is a topically-organized set of facts and information\nthat supports work-related performance. Skills what the\nmodel can do are about the model's proficiency and\nability to perform a job-related activity that contributes to\neffective performance. Some examples of skills displayed\nby LLMs include summarizing emails and converting texts\ninto hip-hop raps. Behaviors are the values, attitudes, and\ntemperament evidenced through the actions of LLMs. Be-\nhaviors can include verbosity, politeness, succinctness, ob-\njectivity, formality, inclusiveness, respecting social norms,\nnon-egregiousness, faithfulness, and non-profanity. In hu-\nman terms, the United States Army, as part of its Army\nTalent Attribute Framework, counts many different knowl-\nedge, skills, and behaviors (KSBs) as essential attributes that\n\"allow individuals to achieve goals of greater complexity\nand scale than they can achieve on their own\" (Homer and\nFleming 2023). Presumably they would also be essential at-\ntributes for Al systems to team with people to achieve their\ngoals.\nBy the transience of alignment, we mean the boundedness\nto time or context. Using terminology stemming from neu-\nroscience, we posit two categories of alignment transiences:\nepisodic and semantic. Episodic alignment is for particu-\nlar knowledge, skills or behaviors bound to a time, place,\nor other context. Semantic alignment is for general KSBs\nabout the world that applies to all times and contexts. Se-\nmantic knowledge includes concepts like an understanding\nof the law of gravitation, whereas episodic knowledge may\ninclude the state of things in a dynamic environment, such\nas who is in the building right now. Semantic behaviors may\ninclude general duties that are valued across contexts, such\nas not stealing. In contrast, episodic behaviors may include\nparticular behaviors from an organization's code of conduct,\nsuch as obtaining manager pre-approval before booking air\ntickets, or ones that are contextual based on relational ethics,\nsuch as an attorney not divulging incriminating conversa-\ntions with their client. The behavior transience dichotomy of\nLLMs is described using the terminology s\u0101dh\u0101ra\u1e47a-dharma\nand vi\u015be\u1e63a-dharma by (Varshney 2024). Skills may be simi-\nlarly dichotomized."}, {"title": "4 Motivating Example", "content": "A compelling example for demonstrating the need for dif-\nferent scopes of alignment is a (hypothetical) AI system de-\nsigned to support mental health counseling in the United\nStates and China, where cultural norms and regulatory en-\nvironments differ significantly.\nCompetence: In the United States, where openness and\nindividualized therapy are emphasized, AI alignment would\nfocus on competencies such as empathic communication, of-\nfering personalized strategies, and handling a wide array of\nmental health issues with flexibility. In China, however, A\u0399\ncompetence may need to align with collectivist values, pri-\noritizing solutions that emphasize family and community-\noriented approaches, as well as deference to social harmony\nand privacy regulations.\nTransience: In the U.S., episodic alignment might be pri-\noritized, allowing the AI to adapt dynamically to diverse\nindividual needs and new mental health practices as they\nemerge. In China, semantic alignment may be favored, as the\nAI system aligns more consistently with established prac-\ntices and cultural norms around mental health, which are\nmore stable and slower to change in response to new trends.\nAudience: The audience of alignment would also vary. In\nthe U.S., the AI system might be tailored for individuals in\nprivate settings, including direct interactions with therapists\nor mental health apps. In China, the audience of alignment\ncould target both individuals and local community health\ngroups, with the AI's output possibly needing to be acces-\nsible to family members or other community stakeholders in\nkeeping with collectivist support structures."}, {"title": "5 Implications", "content": "What is the use of defining these three scopes of alignment?\nThe first use is simply to be able to more precisely discern\nthat alignment is not a singular process; it contains many\npossibilities. Most of the literature focuses on mass seman-\ntic alignment for behaviors (Ji et al. 2023), but there are so\nmany other ways to conduct alignment.\nThe more important use is to clearly specify the technical\nneeds of alignment technologies and workflows. As Tan Zhi-\nXuan et al. submit, \u201cReward-based alignment\u2014and prefer-\nence matching more generally-is only appropriate for AI\nsystems with sufficiently local uses and scopes. In other\nwords, it is adequate for only the narrow or minimalist ver-\nsions of the value alignment problem, where the values and\nnorms at stake can be summarized as a reward function spe-\ncific to the system's scope\" (Zhi-Xuan et al. 2024).\nFor example, different competences of alignment may\nlead to different kinds of data requirements and optimiza-\ntion algorithms. Knowledge is often best learned by gen-\nerating a lot of questions and answers about content rather\nthan simply feeding in the plain content (Sudalairaj et al.\n2024). However, for skills, many examples and repetition is\nmore effective. Behaviors are better learned as preferences\nsupplemented with scenarios in which those behaviors oc-\ncur (Padhi et al. 2024). It is important to differentiate the\ntransience of alignment because while semantic alignment\nimplies (full) fine-tuning a model in advance and leaving it\nthat way, episodic alignment needs to be done at inference\ntime based on the context. Episodic alignment may be pos-\nsible with adapters learned using parameter efficient fine-\ntuning methods that can be applied on the fly. The audience\nof alignment determines whether the procedure is bidirec-"}, {"title": "6 Related Work", "content": "Alignment beyond mass semantic alignment for behaviors is\nnot a white space, and there are plenty of theory and methods\nthat exemplify other scopes. We recount a non-exhaustive\nlist for illustration purposes only. Along the audience di-\nmension, personalized alignment and mutual theory of mind\ntake us toward dyadic and small group alignment (Kirk et\nal. 2024; Wang et al. 2024). Alignment from unstructured\ntext takes us toward public alignment (Padhi et al. 2024;\nAchintalwar et al. 2024b). Along the transience dimension,\nwe have already discussed how low-rank adapters can con-\ntribute to episodic behaviors (Hu et al. 2022). For knowl-\nedge, there are emerging LLM architectures with episodic\nmemory (Das et al. 2024). Along the competency dimen-\nsion, we are seeing the emergence of some alignment\nmethodologies focused on knowledge and skills (Li et al.\n2024; Sudalairaj et al. 2024)."}, {"title": "7 Conclusion", "content": "In this paper, we submit a broader conception of LLM align-\nment that is organized around three scopes: competence,\ntransience, and audience. We do so not only to point out that\nthe literature is mostly focused on mass semantic alignment\nfor behaviors, but also to delineate some of the technical"}]}