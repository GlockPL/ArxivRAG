{"title": "Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation", "authors": ["Nolan Fey", "Gabriel B. Margolis", "Martin Peticco", "Pulkit Agrawal"], "abstract": "Achieving athletic loco-manipulation on robots requires moving beyond traditional tracking rewards-which simply guide the robot along a reference trajectory-to task rewards that drive truly dynamic, goal-oriented behaviors. Commands such as \"throw the ball as far as you can\" or \"lift the weight as quickly as possible\" compel the robot to exhibit the agility and power inherent in athletic performance. However, training solely with task rewards introduces two major challenges: these rewards are prone to exploitation (reward hacking), and the exploration process can lack sufficient direction. To address these issues, we propose a two-stage training pipeline. First, we introduce the Unsupervised Actuator Net (UAN), which leverages real-world data to bridge the sim-to-real gap for complex actuation mechanisms without requiring access to torque sensing. UAN mitigates reward hacking by ensuring that the learned behaviors remain robust and transferable. Second, we use a pre-training and fine-tuning strategy that leverages reference trajectories as initial hints to guide exploration. With these innovations, our robot athlete learns to lift, throw, and drag with remarkable fidelity from simulation to reality.", "sections": [{"title": "I. INTRODUCTION", "content": "General whole-body control comes naturally to animals after years of evolution, yet it remains a long-standing challenge in robotics. Fluid whole-body motion requires balancing multiple competing tasks and constraints that depend on both the robot's morphology and its environment [40]. Recent work [6, 32] demonstrates that sim-to-real reinforcement learning (RL), using methods such as Proximal Policy Optimization (PPO) [37], is a promising paradigm for learning these behaviors by leveraging parallel simulations [36].\nFor dynamic, goal-oriented loco-manipulation, it is natural to train robots with task rewards-commands like \"throw the ball as far as possible\" or \"lift the weight as quickly as possible\" that drive athletic behaviors. However, these task rewards pose two major challenges: (i) they are prone to reward hacking, where the policy exploits imperfections in the simulation, and (ii) the exploration process can lack sufficient guidance. To circumvent these issues, many works on sim-to-real transfer instead train whole-body controllers (WBCs) to track dense reference motions [4, 6, 12, 21, 32]. Dense tracking objectives provide strong regularization by constraining the policy to adhere to a reference trajectory-thereby reducing reward hacking\u2014and they offer a structured path for exploration. However, this strategy relies on defining high-quality reference commands a priori, which in turn demands access to high-quality reference data. For robots with non-human morphologies like legged manipulators, obtaining such data is particularly challenging, and the resulting reference commands may not capture the optimal, athletic strategies that a policy might otherwise discover.\nTo fully harness the benefits of task rewards, it is crucial to ensure that the simulation faithfully replicates real-world dynamics. Inaccurate simulation models allow policies to exploit imperfections, leading to reward hacking, particularly so when the reward is underspecified. Although techniques like domain randomization [46, 47, 49] and online system identification [14, 17, 22, 27, 29, 33] address this by sampling over parameter distributions, they rely on a priori assumptions that may not fully capture the complex dynamics of real hardware. For instance, harmonic drive actuators exhibit non-linear friction, hysteresis, and lag-behaviors that render traditional proxies like motor current unreliable for torque estimation.\nA promising alternative is to enhance the simulation's physics model directly with real-world data, focusing on accurately modeling the actuator dynamics. With this motivation, we introduce the Unsupervised Actuator Net (UAN), a framework for learning corrective actuator models without the need for torque sensors. UAN is trained using reinforcement learning to predict corrective torques, $\\delta\\tau = T_{UAN}(e)$, by minimizing discrepancies between simulated and real-world joint encoder measurements. In doing so, UAN effectively bridges the sim-to-real gap even for robots with complex transmission mechanisms and noisy or unavailable torque measurements.\nBuilding on this enhanced simulation environment, we address the challenge of guided exploration for athletic behaviors. Rather than enforcing strict adherence to a reference trajectory, we propose treating it as a hint to guide exploration. In our approach, a WBC is first pre-trained on random base velocities and end-effector pose commands to establish a strong motion prior. Then, to learn a new athletic behavior, we initialize the controller with a reference trajectory and fine-tune it using a task-specific reward-allowing the policy to depart from the reference when beneficial.\nIn summary, our paper presents an easy-to-use training pipeline for whole-body athletic behaviors that reliably transfer to reality. First, we employ the Unsupervised Actuator Net (UAN) to calibrate actuator dynamics and mitigate reward hacking, ensuring our simulator accurately reflects real-world physics. With this improved simulation environment, we then pre-train a whole-body controller (WBC) to establish fundamental motion skills and fine-tune it with task-specific rewards-using a reference trajectory merely as a hint to guide exploration. This integrated approach enables our robot to perform dynamic tasks such as throwing, lifting, and dragging with remarkable fidelity."}, {"title": "II. METHOD", "content": "Our training pipeline (see Figure 2) is separated into two phases: 1) real-to-sim calibration (Section II-A) and 2) WBC training (Sections II-B and II-C). The real-to-sim calibration phase involves collecting data on the real robot and training a UAN to close the sim-to-real gap for non-ideal actuation mechanisms. Similar to past work [7, 21, 31, 42], our WBC training is split into two distinct sub-phases: pre-training (Section II-B) and fine-tuning (Section II-C). After pre-training, the policy can track reference trajectories if provided as a sequence of base velocity and end effector pose commands. During the fine-tuning phase, the policy observes a reference task trajectory. This helps warm start exploration when learning a new task because the policy can simply track these commands to achieve reasonable task performance. Through training with the task reward itself rather than a tracking reward, the policy learns how to depart from the reference trajectory to achieve higher task performance. Our simulation environments for the pre-training and fine-tuning phases rely on the same strategies for sim-to-real transfer, including domain randomization (Section II-B) and a UAN (Section II-A).\nOur experiments consider a Unitree B2 quadruped with a modified Unitree Z1 Pro arm mounted on its back. The quadruped is 65 cm tall when standing and weighs 60 kg, while the arm is 74 cm fully extended and weighs 6.8 kg. The system has 19 actuated joints: 3 for each leg, 6 for the arm, and 1 for the gripper."}, {"title": "A. Unsupervised Actuator Net", "content": "Some actuators are challenging to model in simulation, especially when they have complex transmission mechanisms. In such cases, standard domain randomization and online system identification techniques may be insufficient, and instead, it is preferable to learn to model the actuator directly from hardware data. Previous approaches rely on output torque sensing [13], which is still uncommon in consumer hardware, to learn how to predict the motor's torque. Alternatively, we propose a method for matching the transition dynamics of the actuator such that\n$\\min_{f_{sim}} || f_{real} (S, \\tau) \u2013 f_{sim}(S, \\tau) ||$.    (1)\nTo influence the simulator dynamics, $f_{sim}$, we learn a residual model, $\\pi_{UAN}(e$, that observes a history of position and velocity errors, $e$, and outputs a corrective torque, $\\delta\\tau$, for the simulator such that\n$\\min_{T_{UAN}} || f_{real} (S, \\tau) \u2013 f_{sim} (S, \\tau + \\pi_{UAN} (e)) ||$. (2)\nThe corrective torques needed to minimize the transition error are unlabeled, so we parametrize $T_{UAN}$ as a neural network and train it with RL.\n1) Architecture and observation space: The network is designed as a 2-layer MLP with layer sizes [128, 128] and ELU activations. It is executed at every simulation time step (5 ms). Assuming each arm joint is identical, a single UAN is shared across all of the arm's actuators, with each actuator being processed independently by the shared network [13]. We constrain the observation space to include a history of the past 20 (equivalent to 100 ms) position and velocity errors for each relevant actuator. These design choices help prevent overfitting to other aspects of the training data, such as inertial coupling. Also, sharing the data across actuators improves data efficiency. For example, the actuator net is trained on data with various loads, as actuators closer to the robot's base generally experience more load than those near the gripper.\n2) Data collection: We collect data on the real hardware to construct a dataset of transitions {($S_t, \\tau_t, S_{t+1}$)}$_{i=0}^{N}$ from each actuator. Our intention during data collection was to sufficiently cover the state space to avoid overfitting. Thus, we opted not to use policy data, and instead, collected data with three types of action sequences: 1) square waves, 2) sine waves, and 3) gaussian noise. For the square and sine wave data, we passed torque commands to one actuator at a time, while keeping the rest of the actuators at a fixed position target. We swept 12 different combinations of amplitude and frequency for each wave, resulting in about 50 seconds of data for each actuator. For the gaussian noise data, we passed torque commands to all the robot's joints simultaneously. We sampled a new action from a gaussian distribution every 5 to 400 ms for about 5 minutes.\n3) Training Environment: We designed the training environment in Isaac Sim [30] with 4096 parallel environments. We train policies with the RSL-RL implementation [36] of PPO [37] with default hyperparameters, minus a few modifications (see Appendix A for the full list of learning algorithm hyperparameters). Following Radosavovic et al. [34], we apply a separate, fixed learning rate to the critic while using an adaptive learning rate for the actor. Additionally, we divide the data of each epoch into four mini-batches for the actor while using the entire batch for the critic, as we found that larger batch sizes produce more stable gradients and result in lower value function loss.\n4) Task Design: For each environment at each timestep, we uniformly sample a real-world transition, ($S_t, \\tau_t, S_{t+1}$)$_k$, and set the state of the simulator to match $s_t$ and the initial torque to $\\tau_t$. After policy inference, we modify the torque by adding the correction, $\\delta\\tau$, and then step the simulator. We then compute the reward as\n$r_{t}^{UAN} = r_{t}^{psim-to-real} + r_{t}^{smoothness}$     (3)\nwhere $r_{t}^{sim-to-real}$ aims to minimize the difference between the real joint position and the simulated joint position, and $r_{t}^{smoothness}$ biases exploration to gradual deviations. For a complete list of reward terms, please refer to Appendix A.\nEach training episode consists of a 20 s rollout executing the torque sequence from the hardware data from $\\bar{\\tau_t}$ to $\\bar{\\tau_{t+20s}}$. Through training on rollouts, the actuator net learns to remain stable across many simulation time steps."}, {"title": "B. Whole-body Controller Pre-training", "content": "Before training on task-specific behaviors, we pre-train the WBC to learn foundational trajectory-tracking skills. Our training scheme builds upon the method proposed in [6] by incorporating a strategy for learning to track an EE orientation command. As in Section II-A3, we designed the training environment in Isaac Sim with 4096 parallel environments and trained the policies with PPO [36, 37] (using separate learning rates and batch sizes for the actor and critic).\n1) Policy Architecture: The WBC is a control policy, $a_t = \\pi_{\\theta}(O_{t-H:t})$, where the action $a_t$ time t, $a_t$, is a vector of position targets for each of the robot's joints and $O_{t-H:t}$ is an observation history of length H = 10 timesteps (200 ms). We parameterize $\\pi_{\\theta}$ as a 3-layer multi-layer perceptron (MLP) with layer sizes [512, 512, 512] and ELU activations. The value function approximator network has the same architecture but does not share weights with the policy.\n2) Observation Space: The policy's observation space consists of proprioceptive readings from the robot's onboard sensors, including the gravity vector projected in the robot's body frame g, a base velocity command $v_{cmd}$, an end effector pose command $p_{cmd}$, the joint positions q, the joint velocities $\\dot{q}$, the previous actions $a_{t-1}$, and a timing variable $w_t = sin(2\\pi f t)$ with f = 2.2 Hz corresponding to the gait cycle frequency. Additionally, the observation includes a d-dimensional task embedding vector $z_t$ (set to zero during pre-training).\n3) Sim-to-Real Considerations: Our approach for bridging the sim-to-real gap uses a combination of domain randomization (DR) and real-to-sim calibration. To learn locomotion behaviors robust to terrain variations, we randomize terrain roughness, friction, and restitution. To account for inaccuracies in the robot's URDF, we randomize the mass and center of mass position of each of the robot's links. We also randomize the PD gains and stall torques for each actuator in the robot's legs, and the policy lag length to learn robustness to latencies observed on hardware. To encourage learning recovery behaviors, we randomize the initial joint and body states of the robot and periodically perturb it with external forces and torques at the base, hips, feet, and end-effector, following the approach proposed in [8]. The DR ranges used for both pre-training and fine-tuning are provided in Appendix A.\nInspired by [43], we clip the commanded motor torques $\\tau$ such that\n$\\tau \\geq \\tau_{max} \\cdot (1 - max \\cdot (min (\\frac{q}{\\dot{q}_{max}}, 0), -1))$,       (4)\n$\\tau \\leq \\tau_{max} \\cdot (1 - max \\cdot (min (\\frac{q}{q_{max}}, 0), -1))$,       (5)\nwhere $\\tau_{max}$ and $q_{max}$ are the maximum torques and velocities of the actuators, respectively. This clipping strategy enforces a physical motor constraint by ensuring that torque commands do not demand power beyond the motor's maximum output capacity. Furthermore, we clip the arm torques a second time to satisfy the constraint\n$|\\tau \\dot{q}| \\leq P_{max}$,       (6)\nwhere $P_{max}$ is the maximum total power of the arm joints, because we found experimentally this helps prevent the arm from entering a power protect state enforced by the robot's manufacturer.\nSince typical DR strategies were insufficient for athletic behaviors in the arm (which uses harmonic drives), we incorporate the UAN (Section II-A) for the arm actuators. Therefore, no DR is applied to arm joint properties.\n4) Task Specification: The pre-training task for the WBC is to track a desired base velocity and EE pose. The velocity command, $v_{cmd} = [v_{fwd}^{cmd}, v_{lat}^{cmd}, w_{yaw}^{cmd}]$, consists of a desired forward velocity $v_{fwd}^{cmd}$, a desired lateral velocity $v_{lat}^{cmd}$, and desired yaw-rate $w_{yaw}^{cmd}$. We command the EE pose in a yaw-rotated frame aligned with the robot's center of mass at a fixed height above the terrain. The choice of frame encourages the robot to coordinate with its legs to expand its workspace. The EE command $p_{cmd} = [P_{EE,t}, O_{EE,t}]^{cmd}$ comprises a cartesian position $P_{EE}^{t}$ and orientation $O_{EE}^{t}$ (provided as the first two columns of a rotation matrix).\n5) Reward Function: The reward function is split as $r_t = r_{track} + r_{aux}$, where $r_{track}$ include tracking terms (EE pose, base velocity) and gait terms, while $r_{aux}$ includes regularization and smoothing terms. The EE tracking term rewards minimizing the distance between four key points, where one key point is positioned at the frame's origin, and the others are positioned along each axis of the frame. Full details are provided in Appendix A.\n6) Command Sampling Scheme: We adopt the approach first proposed in [6] to sample commands during training. We sample a new base velocity command and a new goal end effector pose every 7 seconds of simulation time. Upon sampling, the command is linearly interpolated (over 2 to 5 seconds) from the previous command. While this sampling scheme suffices for foundational loco-manipulation skills, it may be too smooth for highly agile motions \u2013 this motivates our task-specific fine-tuning (Section II-C)."}, {"title": "C. Task-Specific Finetuning", "content": "After pre-training, the policy can track reference trajectories, but struggles on high-acceleration tasks. To address this, we fine-tune the policy directly with task rewards. The same WBC base policy weights can be reused for multiple task policies, thus avoiding repeated pre-training.\n1) Initialization: The policy weights are initialized to those learned during pre-training. To avoid policy collapse, we set a low initial learning rate ($1 \\times 10^{-5}$) for the actor and retain the standard deviation from pre-training. Additionally, we set the entropy coefficient in PPO to zero during fine-tuning to improve training stability.\n2) Reference trajectory and task embedding: During fine-tuning, the policy receives a task-specific reference trajectory and a one-hot task embedding to inform which phase of the task (e.g., set-up, execute, settle) is active. We hand-designed the reference trajectories through joint interpolation and forward kinematics, but they could also come from an expert policy or human demonstration.\n3) Fine-tuning with task reward: The environment for fine-tuning phase extends that of pre-training (same DR ranges, external pushes, etc.). The reward becomes $r_i + r_{aux}$, where $r_i$ is task-specific. Initially, the policy tracks the reference, aiding exploration; later, it learns to deviate to maximize task performance."}, {"title": "III. EXPERIMENTAL SETUP", "content": "We chose the Unitree B2 with Unitree Z1 Pro arm as our hardware platform, and we consider three athletic tasks: throwing, weight lifting, and sled pulling (see Section IV-C)."}, {"title": "V. LIMITATIONS", "content": "Our fine-tuning approach requires a task reference trajectory, which may not be available for all robot morphologies or tasks. It also necessitates per-task engineering of the training environment (reward functions, object simulation, etc.). Future work might employ generative models to automatically synthesize task references. Additionally, our unsupervised actuator net focuses on the arm actuators. Extending real-to-sim calibration to other robot subsystems and modeling structural integrity are promising future directions."}, {"title": "VI. RELATED WORK", "content": "Walking robots with arms present a formidable challenge for control due to their many degrees of freedom and complex dynamics. A typical paradigm is to implement a WBC that optimizes actuation to achieve control objectives considering a model of the robot's kinematics and dynamics [41]. WBC approaches based on offline trajectory optimization or online optimization with reduced-order models have achieved considerable success in dynamic walking and manipulation [1, 3, 28, 44]. Recently, reinforcement learning in simulation has enabled whole-body control that can naturally handle model uncertainty, e.g. uncertain terrain and robot properties [6]. In the case of reinforcement learning-based whole-body control, the controller is a neural network that is commanded with an input reference position [4, 6], force [32], or whole-body pose [5, 12, 23, 24] and outputs joint-space actions.\nIt is common to teleoperate legged-armed robots by parsing a reference trajectory from a human's movements in real-time and tracking it with a WBC; such an approach can accomplish expressive [4], forceful [32], or dexterous [7] tasks. One may also train a high-level policy to select reference trajectories or a latent representation autonomously in place of the tele-operator, using either learning from demonstration [7, 10] or reinforcement learning [21, 24]. However, some tasks may not be achievable by any choice of reference trajectory if they require a motion outside the training distribution of the WBC.\nIt is challenging to formulate a generic pre-training scheme for whole-body control that anticipates all kinds of tasks one might want to perform for humanoids, motion capture datasets can provide diverse feasible reference commands [23], but for quadruped manipulators, pre-training commonly defaults to tracking procedurally generated smooth trajectories within the workspace [6].\nTo avoid the reliance on high-quality pre-training, another possibility is to discard the explicit notion of reference trajectories altogether and directly train end-to-end policies for specific tasks such as fall recovery [25], door opening [38], or soccer [11, 15, 16]. This enables the policy to learn highly dynamic motions to optimize the task reward, but, in practice, these motions can be hard to find due to fundamental exploration challenges in RL. We address this challenge by initializing the policy with pre-trained WBC weights and a reference trajectory.\nPrior work proposed simulated athletic tasks as a benchmark for learned whole-body control [42, 24], though they left sim-to-real transfer as future work. In contrast, other studies have demonstrated sim-to-real transfer of athletic tasks on small robots with transparent actuators [11, 15, 16]. Achieving sim-to-real transfer for athletic behaviors on large robots with non-ideal actuators is especially challenging because even minor modeling discrepancies can lead to reward hacking. To address this, we introduce UAN, which leverages real-world data to bridge the sim-to-real gap.\nDR is a common strategy to mitigate discrepancies between simulation and reality [17, 18]. In the field of dynamic legged robots, common parameters to randomize include the proportional and derivative gains of each joint, the stall torques, the link masses and inertias, and terrain properties [18, 51]. Excessive DR can reduce peak performance if the policy cannot identify key parameters of the environment necessary to optimize its reward function. To overcome this challenge, previous work employed teacher-student frameworks, where a student policy learns to imitate an expert policy that has access to privileged observations related to its environment [6, 17, 18]. Alternatively, the policy may learn online system identification directly from an observation history. Some policy architectures (i.e., CNNs [20] and transformers [33]) have been shown to achieve in-context adaptation without relying on a teacher-student distillation.\nAccurate system identification can reduce reliance on DR by mitigating the sim-to-real gap directly. Methods for identifying inertial properties typically rely on least-squares estimation [2], including a notable approach that leverages insights about the geometric structure of the robot's dynamics to provide robustness against local optima [19]. This method was applied to identify the inertial parameters of the MIT Humanoid [39]. In our work, we rely on the inertial properties provided in the manufacturer's URDF file.\nActuator modeling methods traditionally rely on parameterized physics models to capture effects such as static friction, dynamic friction, and reflected inertia [8], the last of which can be set through the \"armature\" setting in physics simulations such as Isaac Sim [26] and MuJoCo [48]. This approach can be insufficient for actuators with complex transmission mechanisms. To address this, Hwangbo et al. [13] proposed learning an actuator net, which is a neural network trained to predict an actuator's output torque from a history of position and velocity errors. The actuator net was added to the simulator during policy training to reduce the sim-to-real gap in ANYmal's series elastic actuators. Their approach, however, relies on torque sensing, which is uncommon in consumer robotic hardware. Schwendeman et al. [39] avoided reliance on an output torque sensor when training an actuator net by measuring the torques from current. However, this is only accurate in low-reduction and low-torque-density actuators which are efficiently backdriveable and have minimal reflected inertia. In contrast, our approach, UAN, employs an actuator net without relying on torque data. Instead, we train the network to predict corrective torques for the simulator that minimize the discrepancy between the simulated and real-world transition dynamics.\nWhen ground-truth labels are unavailable (i.e., the robot's actuators lack torque sensing), they can be discovered through interaction to better match the real-world dynamics. For example, Zeng et al. [50] learned a residual model to better predict the ballistic motions of objects, enabling a manipulator to accurately throw them. Similarly, Gruenstein et al. [9] proposed learning residual actions for a simplified dynamics model for a legged microbot so that it transits to the same future states as a more complex dynamics model. In another study, Sontakke et al. [45] proposed learning a corrective external force policy to improve simulation accuracy for a buoyancy assisted legged robot. Mentee Robotics has publicly stated that they applied RL to train a delta action model using real-world data to overcome the sim-to-real gap on their humanoid, but the technical details of their approach remain unpublished [35]. While we also apply RL to correct our simulation model, we specifically target the sim-to-real gap for the robot's actuators with harmonic drives, which are notably hard to model. This focus leverages the parts of the simulator that are more accurate (i.e., rigid body mechanics) to reduce overfitting and also avoids reliance on a motion capture system."}, {"title": "VII. CONCLUSION", "content": "Legged manipulators promise enhanced strength and a larger workspace by coordinating arms and legs. We proposed a training pipeline that first pre-trains a whole-body controller and then fine-tunes it using task rewards, while simultaneously reducing the sim-to-real gap via our UAN. Our experimental results on ball throwing, dumbbell lifting, and sled pulling demonstrate the viability of this approach. Future work may extend the sim-to-real calibration to additional subsystems and incorporate structural integrity constraints directly in the training. Future work may extend the real-to-sim calibration to additional subsystems and incorporate structural integrity constraints directly in the training."}]}