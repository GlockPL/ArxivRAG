{"title": "Disentangling Hate Across Target Identities", "authors": ["Yiping Jin", "Leo Wanner", "Aneesh Moideen Koya"], "abstract": "Hate speech (HS) classifiers do not perform equally well in detecting hateful expressions towards different target identities. They also demonstrate systematic biases in predicted hatefulness scores. Tapping on two recently proposed functionality test datasets for HS detection, we quantitatively analyze the impact of different factors on HS prediction. Experiments on popular industrial and academic models demonstrate that HS detectors assign a higher hatefulness score merely based on the mention of specific target identities. Besides, models often confuse hatefulness and the polarity of emotions. This result is worrisome as the effort to build HS detectors might harm the vulnerable identity groups we wish to protect: posts expressing anger or disapproval of hate expressions might be flagged as hateful themselves. We also carry out a study inspired by social psychology theory, which reveals that the accuracy of hatefulness prediction correlates strongly with the intensity of the stereotype.", "sections": [{"title": "Introduction", "content": "The surge of interest in combating online hate led to increased efforts in creation of benchmark datasets and organization of shared tasks, and, as a consequence, rapid development of hate speech (HS) detection models (Caselli et al., 2020; Poletto et al., 2021). However, state-of-the-art HS detectors do not perform equally well across different datasets (Fortuna et al., 2021) and different target identities (Ludwig et al., 2022). These performance discrepancies have been attributed to diverging dataset annotations (Fortuna et al., 2020), out-of-domain distribution (Jin et al., 2023), spurious correlation between specific target identities and the labels (Ramponi and Tonelli, 2022), and specific topical focuses (Bourgeade et al., 2023). Unfortunately, none of these diagnoses resulted so far in a revision of the datasets or models, and, as a matter of fact, this is not surprising. Instead of treating HS detectors as black boxes and HS datasets as given commandments, the research community should understand what the datasets entail and how the models behave under different circumstances. Such insights can help us make practical"}, {"title": "Related Work", "content": ""}, {"title": "Hate Speech Detection Datasets", "content": "Early work in hate speech (HS) detection focused on specific phenomena such as \u201cracism\u201d, \u201csexism\u201d or \"xenophobia\u201d (Waseem and Hovy, 2016; Basile et al., 2019) or treated it as coarse-grained classification without explicitly stating the target identities involved (Davidson et al., 2017; Founta et al., 2018). However, ignoring the target identity and the difference among related concepts such as \u201cabusive\", \"offensive\u201d, and \u201ctoxic\" may cause HS detectors to learn frequently occurring patterns in a particular context and harm generalizability (Vidgen and Derczynski, 2020; Fortuna et al., 2020). Therefore, more recent datasets often provide additional contextual information. Zampieri et al. (2019) introduced the OLID dataset, where each offensive message is assigned a target \u2208 {\"individual\", \"group\", \"other\"}. Caselli et al. (2020) augmented the OLID dataset by adding new annotation dimensions like \"abusiveness\" and \"explicitness\". Ousidhoum et al. (2019) labeled five attributes for each post: directness (2), hostility (6), target attribute (6), target group (5), and sentiment of the annotator (7). Similarly, Mathew et al. (2021) provide rich annotation, including 18 fine-grained target groups related to race, religion, gender, sexual orientation, and rationale text spans on which the labeling decision is based. Due to data rarity, most HS detection datasets are collected using keywords, favoring explicit HS expressions (Poletto et al., 2021; Yin and Zubiaga, 2021; Rahman et al., 2021), which may also cause models trained on such datasets to be over-reliant on a specific set of keywords. To prevent an over-estimation of generalizable model performance, R\u00f6ttger et al. (2021) introduced HATECHECK, a suite of functional tests for HS detection models. They developed 29 functionalities representing challenges in tackling online hate through interviews with NGO workers. Then, they crafted test cases for each functionality consisting of short sentences with unambiguous labels. Templates such as \u201c[IDENTITY] are disgusting.\" are utilized to generate test cases at scale by replacing the special token \"[IDENTITY]\" with a specific target identity."}, {"title": "Bias Analysis and Mitigation", "content": "HS classifiers can absorb unintentional bias across different stages of model development, such as data sampling, annotation, and model learning (Fortuna et al., 2022). Classifiers also often have a superficial understanding of language and are heavily affected by spurious correlations. Wiegand et al. (2019) found that many top words strongly correlated with the hateful category are non-offensive topical words like \"football\" or \"commentator\". They argued that it is due to the narrow sampling strategy used to create the dataset. Park et al. (2018) observed that HS detectors are biased towards gender identities. For example, \"You are a good woman\" was classified as \u201csex-ist\". They proposed mitigation approaches including debiased word embeddings, gender swap data augmentation, and fine-tuning with a larger corpus to reduce the inequality measure. On the other hand, studies on dialectal/racial bias (Davidson et al., 2019; Sap et al., 2019; Mozafari et al., 2020) revealed that African American English (AAE) is much more likely to be predicted as offensive. Furthermore, Maronikolakis et al. (2022) studied the intersection of gender and racial attributes and showed that the bias could be amplified for certain attribute combinations (e.g., masculine and AAE). (Zhou et al., 2021) introduced ToxDect-roberta, focusing on mitigating lexical (e.g., swear words, identity mentions) and dialectal bias towards AAE. They explored debiased training (Clark et al., 2019) and data filtering (Le Bras et al., 2020; Swayamdipta et al., 2020) but obtained limited success. However, translating AAE to white-aligned English (WAE) automatically with GPT-3 and relabeling toxic AAE tweets whose WAE translation is predicted as non-toxic yields greater improvement for dialectal debiasing. Fraser et al. (2021) proposed an interpretation of stereotypes towards different target identities based on the Stereotype Content Model (SCM) (Fiske et al., 2002), which captures stereotypes along two primary dimensions: warmth and competence. Our stereotype analysis is inspired by Fraser et al. (2021). However, their work employed static word embedding models to study stereotypes expressed through unigram words. In contrast, we analyze stereotypes in natural language sentences by assigning scores along the \u201cwarmth\" and \"competence\" dimensions with an NLI model (He et al., 2021)."}, {"title": "Methodology", "content": "Datasets We use the HATECHECK (R\u00f6ttger et al., 2021) and GPT-HATECHECK (Jin et al., 2024) datasets to conduct our analyses, as these datasets provide additional diagnostic insights. Both datasets cover the same seven target identities and 24 functionalities (GPT-HATECHECK omitted the five functionalities related to spelling variations in HATECHECK). Below, we detail the models we experimented with: HateBERT, ToxDect-roberta, Perspective API, and Llama Guard 3. HateBERT (Caselli et al., 2021) and ToxDect-roberta (Zhou et al., 2021) are open-source models, while Perspective API is an industry-standard API developed by Jigsaw and Google's Counter Abuse Technology team to combat online toxicity and harassment. Llama Guard 3 (Inan et al., 2023) is a recent LLM safeguard model based on Meta's Llama 3 (Dubey et al., 2024)."}, {"title": "Disentangling Target Identity Mentions", "content": "We use examples from HATECHECK for a minimal set analysis to measure the models' bias towards target identity mentions. Previous work either compares the subset accuracy of posts from different populations (Davidson et al., 2019; Zhou et al., 2021) or performs perturbation to create adversarial examples (Fortuna et al., 2022). The first approach's drawback is that subsets differ not only in identity mentions but also in topics, styles, and vocabulary preference, among other factors, such that it is not possible to isolate the impact of identity mentions. The limitation of the second approach is that ad hoc perturbation often focuses on particular examples instead of the big picture. Using examples from HATECHECK yields two advantages: 1) It covers a wide variety of scenarios of hateful and non-hateful posts, and 2) Five annotators manually validated each example to ensure its validity and consistency with the gold-standard label. A large share of examples in HATECHECK is generated using templates. We select those templates in the dataset that instantiate exactly seven examples (one for each of the considered target identities). We exclude examples with no target identity, such as \u201cabusing objects\". We also exclude templates that use target-specific slur words (e.g., \"n*gger\", \"b*tch\") because they may entail different degrees of hatefulness. This leaves us with a total of 333 templates and 333\u00d77=2,331 examples. Since identity mentions are neutral, we assume that the hatefulness prediction for examples instantiated from the same template should not alter depending on the target identity, i.e., the predicted hatefulness score for \u201cI hate immigrants\" and \"I hate trans\" should be equal regardless of the absolute predicted score. Based on this assumption, we predict hatefulness scores for all 2,331 examples. Then, we calculate the normalized prediction for each example as the predicted score minus the median score of all examples instantiated from its corresponding template. Finally, we calculate the mean of the normalized predictions for each target identity and present different models' results in Figure 2. While the models show different degrees of bias towards identity mentions, the bias orientation is often the same: All models have a positive bias (predicting as more hateful) towards gays, black people, and Muslims and a negative bias towards women and disabled people. Surprisingly, ToxDect-roberta, which is trained explicitly to mitigate bias, possesses the largest bias towards identity mentions, reaching as high as +33.9% for black people. Comparing Llama Guard 3-1B and -8B, we observe that the larger LLM can better handle identity bias. We now focus on the impact of the identity mention bias on models' classification performance. For this experiment, we use GPT-HATECHECK because its examples are more realistic. We report each model's per-target-identity P/R/F\u2081 scores for the hateful category in Table 2. Perspective API performs consistently best among non-LLM baselines. ToxDect-roberta performs the worst, primarily due to its poor recall for the categories \u201cwomen\u201d, \u201ctrans\u201d, \u201cdisabled people\", and \"immigrants\". We hypothesize that these target identities are not well represented in the model's training dataset due to the significant performance discrepancy among different target identities. The Llama Guard 3 models obtained better recall scores than other baselines, showing LLMs' capability to catch more nuanced hateful expressions. While the larger 8B model performs better, it requires much more computation and consumes 30GB VRAM for inference only, which cannot fit into a current desktop GPU. Debiasing could potentially reduce the impact of the identity mention bias. However, an in-depth comparison of debiasing methods is beyond the scope of this paper. Therefore, we merely apply a na\u00efve debiasing method by subtracting the prediction by the model's target-identity bias. Target identities with strong negative bias in the minimum set experiment, such as \u201cwomen\u201d and \u201cdisabled people\", also have a much lower recall for the \"hateful\" category compared to other target identities. Subtracting the negative bias helped HateBERT and Perspective API improve the recall for these categories by a large margin with a much smaller sacrifice in precision. However, debiasing has little effect on ToxDect-roberta and Llama Guard 3 models because their predicted scores concentrate near 0 or 1 and are poorly calibrated, as shown in Appendix C."}, {"title": "Disentangling Emotions", "content": "Hateful and non-hateful posts entail distinct emotions, which may affect the accuracy of HS detectors. We want to study whether emotions are uniformly associated with different target identities We prompt GPT-4 (Achiam et al., 2023) to identify fine-grained emotions from posts in GPT-HATECHECK using the taxonomy proposed by Demszky et al. (2020), which contains 27 distinct emotions. We provide the full prompt in Appendix F. Figure 3 presents the detected emotions ranked by frequency. 4313 out of 4438 messages have emotions detected in them (97.2%). Hateful posts focus primarily on four emotions: disgust, disapproval, anger, and fear, while non-hateful posts demonstrate a much broader range of emotions, both positive and negative ones. Then, we analyze the distribution of target identities for each detected emotion and present the result in Figure 4. It is manifest that the emotions expressed towards each target identity have a unique composition. In hateful examples, the dominant emotions expressed towards Muslims and immigrants are \u201canger\u201d and \u201cfear\u201d, while the most prominent emotion towards black and disabled people is \"disgust\u201d. For non-hateful examples, \u201clove\u201d stands out for gays, \u201csadness\u201d for black people, and \"pride\" and \"approval\" for trans. In addition, we analyze the correlation between functionalities and emotions in Appendix D. Tabel 3 presents the fine-grained emotion level accuracy of each model for emotions with at least ten occurrences. The emotions with which models struggle the most are \u201cannoyance\u201d, \u201cdisapproval\", \"sadness\", and \"fear\". We further group the fine-grained emotions into positive (1), negative (-1), and ambiguous (0), based on Demszky et al. (2020)'s taxonomy and present the models' classification accuracy in the presence of emotions with different polarities in Table 4. The result is revelatory: All models can relatively accurately identify hateful posts with negative emotions and non-hateful posts with positive emotions. However, the accuracy degrades drastically for non-hateful posts with negative emotions, especially for HateBERT and ToxDect-roberta. This result is alarming since it suggests that HS detectors are entangled with emotion polarity, and some safe posts with negative emotions, such as counter-speech expressing disapproval or sadness, are likely marked as hateful, potentially silencing the voices of vulnerable groups."}, {"title": "Disentangling Stereotypes", "content": "Jin et al. (2024) motivated the use of LLMs with the generation of test cases that account for distinct stereotypes associated with different target identities (e.g., criminality for immigrants and sexuality for trans). However, they did not analyze which stereotypes are covered in their dataset and whether a distinction exists among target identities. We present an in-depth analysis of the stereotypes/counter-stereotypes in GPT-HATECHECK by 1) Interpreting stereotypes based on an established social psychology theory, 2) Analyzing the correlation between stereotypes and HS prediction accuracy, and 3) Extracting and qualitatively analyzing stereotypes/counter-stereotypes. Stereotypes Interpretation Fiske et al. (2002; 2007) proposed the Stereotype Content Model, which uses the universal dimensions \"warmth\" and \"competence\", to describe social perceptions and stereotypes. The model maps each stereotype onto interpretable semantic axes \u201cwarmth\u201d vs. \u201ccoldness\" and \"competence\" vs. \"incompetence\". We use a state-of-the-art NLI model (He et al., 2021) to assign \"warmth\u201d and \u201ccompetence\u201d scores to each example in the GPT-HATECHECK dataset. Inspired by Mathew et al. (2020), we derive the scores via semantic differentials of two opposite concepts (e.g., \u201cwarmth\u201d and \u201ccoldness\"). Specifically, we test four hypotheses for each example: \u2022 H\u2020: This message expresses warmth towards {target_identity}. \u2022 H: This message expresses coldness towards {target_identity}. \u2022 \ubc1c: This message expresses that {target_identity} are competent. \u2022 H2: This message expresses that {target_identity} are incompetent. The NLI model returns logit scores for the three classes: \"entail\", \"contradict\", and \"neutral\". We first take the softmax over the three classes and derive the score for \"warmth", "as": "S_{warmth} = P_{entail}(H+)+P_{contradict}(H-) - P_{contradict}(H+) \u2013 P_{entail}(H-)$ We derive Scompetence similarly by replacing H with H in Equation 1. Due to the softmax operation, Swarmth and Scompetence are both bounded in the range of [-2, 2]. Figure 5 plots the kernel density estimate (KDE) in the warmth-competence semantic space."}, {"title": "Conclusions and Future Work", "content": "We presented a comprehensive analysis of various factors that influence the behavior and accuracy of HS detectors. Empirical results revealed that popular industrial and academic HS classifiers are still prone to bias due to specific mentions of the target identity. They often confuse hatefulness and the polarity of the expressed emotions, and the stereotype intensity strongly impacts the classifiers' accuracy. While the result may seem pessimistic, our work opens up new venues for the NLP community to improve the robustness of HS detectors further and mitigate various biases. In future work, we plan to apply our method to more datasets and models and introduce an open-source evaluation benchmark to facilitate the future development of HS detectors."}, {"title": "Limitations", "content": "We conduct experiments on two functionality test datasets: HATECHECK (R\u00f6ttger et al., 2021) and GPT-HATECHECK (Jin et al., 2024). These datasets provide rich metadata such as the target identity and the type of hate expressions (functionality). The messages in these datasets were composed by crowd-source workers or LLMs. We chose not to use HS detection datasets sampled from social media platforms because 1) they usually do not provide fine-grained target identity information and 2) they do not provide detailed information on data sampling (Fortuna et al., 2022). Sampling examples for different target identities from different domains (e.g., subreddits) or using different keywords might introduce compounding factors and obscure the conclusions. Nevertheless, we demonstrate the utility of our framework by presenting preliminary experimental results on a multi-source social media dataset in Appendix B. The main contribution of our paper is the analysis of the impact of various factors in HS detection. The related problem of the analysis of bias mitigation methods was not in the focus of our work. While there exists an array of excellent surveys on bias mitigation methods (Meade et al., 2022; Kumar et al., 2023; Gallegos et al., 2024), including a comprehensive evaluation of bias mitigation methods would take up too much space and prevent us going into depth in the analysis. As we demonstrated in Section 3.1 and Appendix C, the na\u00efve debiasing method we use only helps when models predict well-calibrated probability-like scores. We claim neither the effectiveness nor the novelty of this method. Furthermore, we used LLMs to detect emotions and stereotype phrases and a pre-trained NLI model to score the two stereotype dimensions. This helped us develop a prototype and validate our hypotheses rapidly. Although we performed some prompt engineering and exploration, the accuracy was not perfect. If time and resources allow, hiring domain experts to relabel the examples would yield a more reliable result. Lastly, stereotypes and emotions towards target identities strongly depend on the cultural context. The examples in GPT-HATECHECK are written by LLMs, which align best with views of Western, educated, white, and younger population (Santy et al., 2023). Studying how the findings might alter under distinct socio-demographic backgrounds would be an exciting extension of this work."}]}