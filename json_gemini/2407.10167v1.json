{"title": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language Model", "authors": ["Xunyu Zhu", "Jian Li", "Yong Liu", "Can Ma", "Weiping Wang"], "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in mathematical reasoning tasks due to their extensive parameter counts and training on vast datasets. Despite these capabilities, deploying LLMs is hindered by their computational demands. Distilling LLM mathematical reasoning into Smaller Language Models (SLMs) has emerged as a solution to this challenge, although these smaller models often suffer from errors in calculation and semantic understanding. Prior work has proposed Program-of-Thought Distillation (PoTD) to avoid calculation error. To further address semantic understanding errors, we propose Key-Point-Driven Mathematical Reasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs by breaking down the problem-solving process into three stages: Core Question Extraction, Problem-Solving Information Extraction, and Step-by-Step Solution. This method is further divided into KPDD-CoT, which generates Chain-of-Thought rationales, and KPDD-PoT, which creates Program-of-Thought rationales. The experiment results show that KPDD-CoT significantly improves reasoning abilities, while KPDD-PoT achieves state-of-the-art performance in mathematical reasoning tasks. Our approach effectively mitigates misunderstanding errors, advancing the deployment of efficient and capable SLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) based on Transformer architectures represent a significant advancement in natural language processing. Notable models such as LLaMA (Touvron et al., 2023a), GPT-4 (OpenAI, 2023), and PaLM (Chowdhery et al., 2023) feature hundreds of billions of parameters. Trained on extensive text datasets, these models exhibit exceptional proficiency across a broad range of downstream tasks.\nRecent studies (Chen et al., 2023; Wang et al., 2023b,a; Liu et al., 2023) have enhanced the mathematical reasoning abilities of LLMs through Chain-of-Thought (CoT) prompting, which generates intermediate steps to solve complex problems. However, deploying these models remains challenging due to their size and computational demands. For instance, the GPT-3 model (Brown et al., 2020) requires at least 350GB of FP16 storage and multiple A100 GPUs with 80GB of memory each for efficient inference.\nRecent work (Magister et al., 2023; Shridhar et al., 2023; Ho et al., 2023; Fu et al., 2023) investigates distilling LLM reasoning into SLMs (under 1B parameters) for broader deployment. This involves using LLMs to create enriched datasets with detailed reasoning paths, which then fine-tune SLMs, endowing them with advanced reasoning abilities. For example, Chain-of-Thought Distillation (CoTD)(Ho et al., 2023) encapsulates the reasoning process into textual rationales. However, there is a significant performance gap between SLMs and LLMs. Prior work(Wei et al., 2022) identifies three main error types in CoT reasoning: (1) Calculation errors: Incorrect calculations leading to wrong answers. (2) Missing Step errors: Omissions of intermediate reasoning steps, especially in multi-step problems. (3) Semantic misunderstanding errors: Errors in understanding the problem or maintaining coherent reasoning, often due to insufficient model capability. To explore the reasons for the performance gap between SLMs and LLMs, we conducted the same error analysis on CoTD. Our preliminary experiments (shown in Figure 1) reveal numerous error combinations in CoTD, with calculation and semantic misunderstanding errors being the most prevalent. Prior work (Zhu et al., 2023a) proposed Program-of-Thought Distillation (PoTD) to mitigate calculation errors by formulating the reasoning process as a Python program executed by an external interpreter. This approach allows the SLM to focus on generating the program, avoiding calculation errors and improving reasoning performance. Given these circumstances, our paper focuses on addressing semantic misunderstanding errors in CoTD to further enhance the reasoning performance of SLMs.\nIn our paper, we propose a novel mathematical reasoning distillation method called Key-Point-Driven Mathematical Reasoning Distillation (KPDD) to enhance the mathematical reasoning performance of SLMs. KPDD breaks the reasoning process into three parts: (1) Core Question Extraction: Identifies the core question from the original problem. (2) Problem-Solving Information Extraction: Extracts relevant data and information needed to solve the problem. (3) Step-by-Step Solution: Uses the extracted key points to solve the problem in a step-by-step manner. The third part is further divided into two formats, KPDD-COT and KPDD-POT: (1) KPDD-CoT: Generates rationales in the form of Chain-of-Thought (CoT). This method focuses on reducing misunderstanding errors and explicitly illustrates the reasoning process, aiding in error analysis. (2) KPDD-PoT: Generates rationales in the form of Program-of-Thought (PoT). This approach not only reduces misunderstanding errors but also avoids calculation errors, further enhancing the SLM's mathematical reasoning performance.\nWe assessed KPDD across FlanT5 models from Small (0.06B) to Large (0.76B) on four mathematical reasoning datasets. The results show that KPDD-CoT significantly enhances SLMs' reasoning abilities, while KPDD-PoT enables SLMs to achieve state-of-the-art (SOTA) mathematical reasoning performance. For instance, with KPDD-CoT, FlanT5-Large achieved an average accuracy of 24.71% on these datasets, and KPDD-POT elevated FlanT5-Large to an average accuracy of 63.83%. Furthermore, our error analysis on KPDD confirms that KPDD effectively mitigates misunderstanding errors, thereby improving the mathematical reasoning performance of SLMs.\nOur contributions are summarized as follows:\n1. Our study reveals that misunderstanding errors and calculation errors are the major factors limiting CoTD's reasoning.\n2. We propose Key-Point-Driven Mathematical Reasoning Distillation (KPDD) to alleviate misunderstanding errors and effectively improve the reasoning performance of SLMs.\n3. Extensive experiments show that KPDD outperforms other methods across various benchmarks and achieves new state-of-the-art results on these mathematical reasoning datasets."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Mathematical Reasoning", "content": "Mathematical reasoning tasks, exemplified by benchmarks such as GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021), present a substantial challenge for LLMs. To enhance LLMs' performance in this domain, researchers have identified two primary strategies."}, {"title": "Chain-of-Thought Reasoning", "content": "LLMs' reasoning ability can be enhanced by prompting them to articulate intermediate steps towards a solution, as demonstrated by Wei et al. (Wei et al., 2022). This insight has spurred various advancements (Chen et al., 2023; Wang et al., 2023b,a; Liu et al., 2023) that refine reasoning paths: Chen et al. (Chen et al., 2023) prompt LLMs to generate executable code; Wang et al. (Wang et al., 2023b) use multiple reasoning paths with a voting mechanism; Wang et al. (Wang et al., 2023a) have LLMs create a plan before reasoning; Liu et al. (Liu et al., 2023) employ diverse reasoning prompts for problem-solving; Zhong et al. (Zhong et al., 2024) encourage LLMs to deeply understand problems and leverage key information for better reasoning. Building on these methods, our work introduces Key-Point-Driven Mathematical Reasoning Distillation (KPDD) to further enhance SLMs' mathematical reasoning."}, {"title": "Knowledge Distillation", "content": "Knowledge Distillation optimizes LLMs for practical use by transferring knowledge from larger models to smaller, efficient ones (Zhu et al., 2023b). Recent research (Magister et al., 2023; Shridhar et al., 2023; Ho et al., 2023; Fu et al., 2023) focuses on endowing compact models (< 1B parameters) like T5 (Raffel et al., 2020) and GPT-2 (Radford et al., 2019) with advanced reasoning capabilities from LLMs such as GPT-4 (OpenAI, 2023) and PaLM-2 (Anil et al., 2023). For instance, Ho et al. (Ho et al., 2023) fine-tune student models using accurate reasoning paths from LLMs, Shridhar et al. (Shridhar et al., 2023) train dual-model systems on sub-questions and solutions, and Fu et al. (Fu et al., 2023) propose scaling down general competencies of smaller models to enhance task-specific performance. Our work introduces a novel distillation approach where two SLMs independently extract the core question and key problem-solving information from an original question. These key points are then utilized to guide another SLM in solving the original question effectively."}, {"title": "3 Method", "content": "In this work, we introduce a novel distillation method for mathematical reasoning tasks called Key-Point-Driven Distillation (KPDD), structured into three stages: (1) Stage 1: KPDD distills the first SLM to extract the core question from the original question. (2) Stage 2: KPDD distills the second SLM to extract problem-solving information from the original question. (3) Stage 3: KPDD distills the third SLM to solve the original problem using the core question and problem-solving information. In Stage 3, we prompt the LLM to construct two types of reasoning datasets: (1) CoT Rationales: These are more comprehensible to both humans and LLMs, showcasing a detailed reasoning process. (2) PoT Rationales: These rationales delegate computational tasks to an external Python interpreter, thereby avoiding calculation errors."}, {"title": "3.1 Data Generation from LLMs", "content": "Our KPDD method begins by creating a mathematical reasoning dataset from LLMs, which is then used for SLM fine-tuning. In our paper, we use in-context learning (Dong et al., 2023; Min et al., 2022; Rubin et al., 2022) to prompt LLMs for constructing the reasoning dataset. Furthermore, in stage 3, our KPDD method employs two distillation approaches: one distills the SLM to generate CoT rationales for problem-solving, and the other distills the SLM to generate PoT rationales for problem-solving. In other words, our KPDD method can be divided into two approaches: KPDD-COT and KPDD-POT."}, {"title": "3.1.1 Data Generation for KPDD-COT", "content": "Given a mathematical dataset D, each entry (x, y) pairs a question x with its answer y. As illustrated in Figure 2, we select k samples {(x1,y1),..., (xk, yk)} from D and manually craft reasoning processes. Each reasoning process c includes a core question, problem-solving information, and rationales in CoT format. These elements are separated by HTML tags: \"<core>{core question}</core><info>{problem-solving information}</info><cot>{rationales in CoT format}</cot>\". These form contextualized instances {(x1, c1, y1), ..., (xk, ck, yk)}, compiled into a demonstration set Dc. We then prompt LLMs with the demonstration set Dc, a new question, and the instruction \"Firstly, let's extract the most comprehensive and detailed key question. Then, let's identify and list the most useful information related to the question. Finally, let's understand the key question and the problem-solving information, solve the question step by step, and show the answer.\" to generate the reasoning process for the new question. The KPDD-CoT dataset generation is formalized as:\n$c_i = f_M(x_i, D_c),$   (1)\nwhere M denotes the LLM, f is the decoding function, and i denotes the index in D. This yields the KPDD-COT dataset Dc, composed of triplets (x, c, y).\nData Filtering\u2014Upon generating the KPDD-CoT dataset with LLMs, we validate the reasoning process against the gold standard answer\u2014a crucial step to ensure the quality of our reasoning dataset Dc. Discrepancies between the generated reasoning process and the gold standard answer result in the exclusion of those entries from Dc. This meticulous filtering removes incorrect examples, thereby enhancing the dataset's overall quality. Finally, this refinement directly contributes to the improved performance of fine-tuned SLMs, due to the increased accuracy and reliability of the training data. By ensuring that only high-quality reasoning processes are included, we bolster the effectiveness of the SLMs in solving mathematical reasoning tasks."}, {"title": "3.1.2 Data Generation for KPDD-POT", "content": "Similar with KPDD-CoT, the initial phase in our KPDD-PoT entails creating a dataset from LLMs, setting the stage for SLM fine-tuning. For KPDD-CoT dataset generation, we also choose k samples {(x1,y1), (x2,y2), ..., (xk, yk)} from D and manually create reasoning processes p, where each reasoning process includes a core question, problem-solving information, and rationales in PoT format. These elements are also separated by HTML tags: \"<core>{core question}</core><info>{problem-solving information}</info><pot>{rationales in PoT format}</pot>\". These form contextualized instances {(x1, p1, y1), (x2, p2, y2), ..., (xk, pk, yk)}, which are compiled into a demonstration set Dp. We then prompt the LLM with the demonstration set Dp and a new question, and input the instruction \"Firstly, let's extract the most comprehensive and detailed key question. Then, let's identify and list the most useful information related to the question. Finally, let's understand the key question and the problem-solving information, and generate the python code (return ans) to solve the question.\" to generate a reasoning process for the new question. Figure 3 outlines this data generation process, and the KPDD-POT dataset generation is formalized as:\n$p_i = f_M(x_i, D_p),$   (2)\nwhere M is the LLM, f denotes the greedy decoding function, and i is represented as the index of the instance (x, y) in D. This yields a KPDD-PoT dataset Dp, organized as triplets (x, p, y).\nData Filtering\u2014Following KPDD-POT dataset generation by LLMs, each program in the reasoning process undergoes validation using an external Python interpreter, a vital step to ensure the quality of our initial dataset Dp. Programs that fail to compile or produce incorrect results are immediately discarded. This rigorous filtering process removes flawed instances, thus improving the dataset's quality."}, {"title": "3.2 Fine-tuning SLMs", "content": "After constructing these reasoning datasets, we use them to fine-tune the SLMs. In the KPDD, we fine-tune three SLMs: the first SLM, called KPDD-CoT/PoT-core, is used to extract the core question from the original problem, the second SLM, called KPDD-CoT/POT-info, extracts the problem-solving information, and the third SLM, called KPDD-CoT/PoT-solve, uses both the core question and problem-solving information to solve the original question."}, {"title": "3.2.1 Fine-tuning SLMs for KPDD-CoT", "content": "Firstly, we construct a core question subset from the KPDD-CoT dataset, denoted as DcC. Each sample in this subset can be represented as (x, cc), where x represents the original question and cc represents the core question. For each training instance (x, cc) from Dcc, we prepend the prompt Pcc \"Let's extract the most comprehensive and detailed core question.\" to the question x. This guides the KPDD-CoT-core in fine-tuning to accurately extract the corresponding core question cc. The fine-tuning loss function can be represented as follows:\n$L = - \\sum_{i=1}^{N} \\sum_{t=1}^{T} log P(c_{c_t} | c_{c_{<t}}, x^i, P_{cc}),$   (3)\nwhere N is the number of examples in DcC, Pcc is the prompt, and cc:T is the sequence of the core question.\nThen, we construct a problem-solving subset from the KPDD-CoT dataset, denoted as DCI. Each sample in this subset can be represented as (x, ci), where x represents the original question and ci represents the problem-solving information. For each training instance (x, ci) from Dci, we prepend the prompt pci \"Let's identify and list the most useful information related to the question.\" to the question x. This guides the KPDD-CoT-info in fine-tuning to accurately extract the corresponding problem-solving information ci. The fine-tuning loss function can be represented as follows:\n$L = - \\sum_{i=1}^{N} \\sum_{t=1}^{T} log P(c_{i_t} | c_{i_{<t}}, x^i, P_{ci}),$   (4)\nwhere N is the number of examples in DCI, Pci is the prompt, and ci:T is the sequence of the problem-solving information."}, {"title": "3.2.2 Fine-tuning SLMs for KPDD-PoT", "content": "In KPDD-PoT, aside from replacing the KPDD-CoT dataset with the KPDD-PoT dataset, the fine-tuning method for KPDD-PoT-core remains consistent with that of KPDD-CoT-core, and the fine-tuning method for KPDD-PoT-info remains consistent with that of KPDD-CoT-info. However, the fine-tuning method of KPDD-PoT-solve is different with KPDD-COT-solve. The main difference between them is the input instruction. Specifically, when fine-tuning KPDD-PoT-solve, the input instruction is: \"Let's understand the core question and the problem-solving information, and generate the python code (return ans) to solve the question.\" This instruction guides the model to not only understand the core question and the problem-solving information but also to generate Python code that can compute the answer. This approach leverages the model's ability to perform code generation, which can be particularly effective for solving mathematical problems programmatically.\nMoreover, the fine-tuning loss functions for the SLMs in KPDD-POT are identical to those in KPDD-CoT. This ensures that the optimization process remains consistent across both methods, focusing on minimizing the discrepancies between the model's output and the expected solutions."}, {"title": "3.3 Inference-time Predictions", "content": "Figure 4 illustrates the inference process of KPDD. After fine-tuning, the process for solving a given question involves three main steps:\n1. Core Question Extraction: First, we use the KPDD-CoT/PoT-core model to extract the core question from the original problem. This step isolates the essential part of the problem that needs to be addressed.\n2. Problem-Solving Information Extraction: Next, the KPDD-CoT/PoT-info model extracts the relevant problem-solving information. This model identifies and lists the necessary context and data required to solve the core question.\n3. Solution Generation: Finally, based on the original question, the core question, and the problem-solving information, the KPDD-CoT/PoT-solve model generates rationales in either CoT or PoT format to solve the original question. For KPDD-PoT, this involves generating Python code that can compute the answer."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "In our paper, the training dataset is derived from the GSM8K training set, which comprises diverse grade school math word problems (Cobbe et al., 2021). Additionally, the mathematical reasoning capabilities of the SLMs are evaluated using the GSM8K test set, along with other datasets including ASDiv, which contains diverse math word problems (Miao et al., 2020), SVAMP, which features math word problems with varying structures (Patel et al., 2021), and MultiArith, which consists of arithmetic word problems (Roy and Roth, 2015). The statistics of these datasets are summarized in Table 1. This comprehensive evaluation approach ensures that the SLMs' mathematical reasoning capabilities are thoroughly tested across a variety of problem types and structures, providing a robust assessment of their performance."}, {"title": "4.2 Implementation", "content": "We employ GPT-4 as the teacher LLM to construct our training dataset and utilize FlanT5 models-Small (60M), Base (250M), and Large (760M) (Chung et al., 2022)\u2014as student SLMs. We manually create 8 demonstrations to guide GPT-4 in generating 4 reasoning paths for each dataset (KPDD-CoT and KPDD-PoT). Fine-tuning of all student SLMs is conducted using the Huggingface library (Wolf et al., 2020) on an NVIDIA 3090 GPU with 24 GB RAM. The learning rate for fine-tuning is set to 5e-4, with a total of 10 fine-tuning epochs."}, {"title": "4.3 Baselines", "content": "Proprietary Large Language Models We present CoT prompting results from an array of SOTA LLMs, such as OpenAI's GPT-4, ChatGPT (gpt-3.5-turbo), Google's PaLM-2, and Anthropic's Claude-2.\nOpen-Source Large Language Models We present mathematical reasoning performance of Llama-2-7B, CodeLLaMA-7B, and their fine-tuned versions, such as Platypus-2, WizardMath, TORA.\nFine-tuned Small Language Models We present some works that try to fine-tune SLMs under 1B, such as Ho et al. (Ho et al., 2023) fine-tune GPT-3-ada, Fu et al. (Fu et al., 2023) fine-tune FlanT5, and Shridhar et al. (Shridhar et al., 2023) fine-tune GPT-2."}, {"title": "4.4 Main Results", "content": "Table 2 showcases our method's performance on four mathematical datasets, revealing key insights:\n1. KPDD-CoT Enhances Mathematical Reasoning: KPDD-CoT significantly improves the mathematical reasoning capabilities of SLMs, with absolute improvements ranging from 5.01% to 15.51% across tasks. Traditional baselines typically rely on CoTD, which involves generating numerous steps and performing extensive calculations. However, CoTD often encounters semantic misunderstanding errors that hinder the improvement of SLMs' mathematical reasoning abilities. In contrast, KPDD-CoT employs extra SLMs to extract key points (including the core question and problem-solving information) of the question and uses these key points to guide the SLMs' reasoning. This approach significantly reduces the semantic misunderstanding errors of CoTD, making KPDD-CoT better suited for improving the mathematical reasoning ability of SLMs.\n2. KPDD-POT Outperforms State-of-the-Art: KPDD-PoT surpasses previous state-of-the-art fine-tuned SLMs at all scales, with absolute improvements between 32.18% and 54.63% across tasks. Furthermore, KPDD-PoT's accuracy is higher than that of KPDD-CoT, highlighting the advantage of rationales in PoT format in enhancing SLMs' reasoning capabilities. Our analysis finds that the mathematical reasoning performance of CoTD is limited not only by semantic misunderstanding errors but also by calculation errors. PoTD converts rationales from CoT format into PoT format, formulating the reasoning process into a Python program and sending it to an extra Python interpreter to generate the final answer. This method transfers numerical computation from SLMs to a Python interpreter, avoiding calculation errors. Additionally, by extracting key points of the question, KPDD-PoT implicitly enhances the SLMs' understanding of the question, thereby improving their overall mathematical reasoning capabilities."}, {"title": "3. Importance of Model Size", "content": "The efficacy of mathematical reasoning distillation in SLMS is highly dependent on model size; larger models assimilate more reasoning knowledge, leading to superior performance. For instance, under KPDD-PoT, FlanT5-Small achieves 20.77% accuracy on GSM8K, FlanT5-Base reaches 34.57%, and FlanT5-Large attains 46.32%."}, {"title": "4. Strong Transferability of KPDD", "content": "KPDD exhibits strong transferability. The distillation dataset of KPDD is constructed based on the GSM8K training dataset, and we evaluate our SLMs on several mathematical reasoning datasets, including the GSM8K test dataset, ASDiv dataset, SVAMP dataset, and Multi-Arith dataset. Our experimental results show that KPDD not only achieves good reasoning performance on the GSM8K test dataset but also performs well on the ASDiv, SVAMP, and MultiArith datasets. These results demonstrate that KPDD has strong transferability and further corroborate that SLMs do not improve their reasoning performance through data leakage."}, {"title": "4.5 Effect of Different Components in KPDD", "content": "In this subsection, we delve into the impact of various components within KPDD. We have considered five distinct categories, which include: 1. Original SLMs without any fine-tuning; 2. SLMs with original CoT/PoT distillation; 3. SLMs with core distillation combined with CoT/PoT distillation; 4. SLMs with problem-solving information distillation combined with CoT/PoT distillation; 5. SLMs with KPDD. For each of the latter four categories, we have constructed corresponding reasoning datasets, each containing a single reasoning path per question. Following this, we have utilized FlanT5-base as our foundation for SLMs, and we have fine-tuned these models using the aforementioned reasoning datasets. To evaluate the reasoning capabilities of these SLMs, we have tested them on the GSM8K test dataset, as well as on the ASDiv, SVAMP, and MultiArith datasets.\nTables 3 and 4 present the results of our experiments, from which we make several observations: (1) We observe a significant performance improvement in Category 2 compared to original SLMs. Specifically, under CoT reasoning, Category 2 achieves an average accuracy gain of 4.61% across multiple datasets, while under PoT reasoning, it achieves a substantial average accuracy improvement of 32.91%. These experimental results indicate that CoTD and PoTD can markedly enhance the mathematical reasoning ability of SLMs. (2) We find that Categories 3 and 4 exhibit a further performance increase relative to Category 2. Specifically, in the context of CoT reasoning, Categories 3 and 4 achieve average accuracy gains of 0.55% and 0.45% respectively over Category 2 across multiple datasets. Under PoT reasoning, the gains are more pronounced with Categories 3 and 4 achieving average accuracy improvements of 4.21% and 6.13% respectively. This suggests that SLMs can deepen their understanding of questions by focusing on key points, thereby further enhancing their mathematical reasoning ability. (3) In Category 5, we combine the core questions with the problem-solving information to guide SLMs in addressing the questions. The results are promising: Category 5 achieves an average accuracy of 9.98% under CoT reasoning and a remarkable 45.20% under PoT reasoning across multiple datasets. This indicates that key points in questions play a crucial role in boosting the reasoning capabilities of SLMs, and that combining several key points provides richer information, leading to further improvements in their reasoning abilities."}, {"title": "4.6 Effect of SLM Quantity in KPDD", "content": "In this subsection, we investigate the impact of SLM Quantity in KPDD. We consider five distinct categories: I. Using one SLM to simultaneously extract the core question and problem-solving information, and solve the original question; II. Using one SLM to extract the core question and problem-solving information, and another SLM to solve the original question; III. Using one SLM to extract the core question, another SLM to extract the problem-solving information, and a third SLM to solve the original question; IV. Using one SLM to extract the problem-solving information, another SLM to extract the core question, and both to solve the original question; V. Using one SLM to extract the core question, another SLM to extract the problem-solving information, and a third SLM to solve the original question. For each category, we create corresponding reasoning datasets, each containing a single reasoning path per question. We utilize FlanT5-base as our base SLMs, fine-tuning them on these reasoning datasets. To assess their reasoning capabilities, we evaluate these SLMs on the GSM8K test dataset, as well as on the ASDiv, SVAMP, and MultiArith datasets.\nTables 5 and 6 present the results of our experiments, from which we make several observations: (1) Compared to other categories, Category I performed worse. For KPDD-CoT, Category I achieved an average accuracy of 7.16% across multiple datasets, while for KPDD-PoT, it achieved an average accuracy of 39.61%. This suggests that the limited model size of a single SLM hinders its performance across multiple tasks. (2) Category II outperformed Categories III and IV in reasoning performance. For KPDD-CoT, Category II achieved an average accuracy of 9.51% across multiple datasets, while for KPDD-PoT, it achieved an average accuracy of 41.80%. We attribute this result to the importance of the KPDD-CoT/POT-solve component, where using a single SLM for this phase yields the best reasoning performance. (3) For KPDD-CoT, Category V achieved an average accuracy of 9.98% across multiple datasets, while for KPDD-PoT, it achieved an average accuracy of 45.20%. This is the highest reasoning performance among all categories, indicating that our approach of using a separate SLM for each component maximizes the performance of each component, thereby maximizing the reasoning performance of KPDD."}, {"title": "4.7 Diverse Reasoning Paths Improve SLMs' Reasoning Performance", "content": "In this subsection, we fine-tune CodeT5-Base on our reasoning datasets, which are differentiated by the number of reasoning paths they contain, to analyze the effect of reasoning path multiplicity on the reasoning performance of SLMs. This examination aims to discern how the quantity of reasoning paths in training data influence the model's ability to perform reasoning tasks.\nFigure 5 presents the results of our experiments, which demonstrate that a variety of reasoning paths can bolster the reasoning performance of SLMs. For instance, CodeT5-Base, when trained on an KPDD-PoT dataset featuring four reasoning paths, attains a 34.57% accuracy on the GSM8K test dataset and a 52.29% accuracy on ASDiv. In contrast, CodeT5-Base trained on an KPDD-POT dataset with only one reasoning path achieves 49.33% accuracy on GSM8K test dataset and 46.1% accuracy on ASDiv. This suggests that the inclusion of multiple reasoning paths in training data can significantly enhance the model's performance, particularly in tasks requiring explanation generation."}, {"title": "4.8 Error Analysis", "content": "In this subsection, our aim is to verify whether KPDD can indeed reduce semantic misunderstanding errors. KPDD-PoT implicitly includes the reasoning process within its rationales, making it challenging to conduct error analysis on rationales in PoT format. Conversely, rationales in CoT format explicitly contain the reasoning steps, allowing us to clearly understand how the SLM solves the questions step by step, thus facilitating error analysis. Therefore, in this part, we focus on error analysis for rationales in CoT format. To achieve our goal, we randomly sample 100 examples from GSM8K/SVAMP and perform error analysis on the questions with incorrect answers. For a better understanding of KPDD's effect, we also consider three other scenarios: (1) vanilla CoTD, (2) reasoning that combines vanilla CoTD and core question extraction, and (3) reasoning that combines vanilla CoTD and problem-solving information extraction. Furthermore, to simplify our analysis, we use flanT5-base as our SLMs, and the corresponding reasoning datasets still contain a single reasoning path per question.\nThe detailed quantitative results are illustrated in Figure 6. By analyzing the experimental results, we found that: (1) Combination of Multiple Errors in SLMs: SLMs tend to exhibit combinations of multiple errors, with calculation errors having the most significant impact on reasoning performance. Specifically, vanilla CoTD on the GSM8K dataset showed 51 understanding errors, 79 calculation errors, and 34 step missing errors, resulting in a total of 164 errors. This number far exceeds the original number of problems, with calculation errors outnumbering other types of errors. Similar results were observed in the SVAMP dataset. This explains why PoTD achieves better reasoning performance than CoTD: PoTD converts vanilla rationales into Python programs, delegating the calculation process to an external Python interpreter to avoid calculation errors. (2) Reduction of Understanding Errors with Key Points: Introducing key points of the original questions effectively reduces understanding errors. Specifically, when core questions were introduced in vanilla CoTD, the number of understanding errors on the GSM8K dataset decreased to 50, and on the SVAMP dataset, it decreased to 53. When problem-solving information was introduced in vanilla CoTD, the number of understanding errors decreased to 48 on GSM8K and to 51 on SVAMP. These results indicate that key points of the original questions help SLMs better understand the questions, thereby reducing understanding errors and improving reasoning performance. (3) Further Reduction of Understanding Errors with Multiple Key Points: Combining multiple key points can further reduce understanding errors. Specifically, KPDD reduced the number of understanding errors to 46 on GSM8K and to 50 on SVAMP. This suggests that KPDD's method of integrating multiple key points can deepen SLMs\u2019 understanding of the original questions, further reducing understanding errors and enhancing reasoning performance."}, {"title": "5 Conclusion", "content": "In this paper, we propose Key-Point-Driven Distillation (KPDD) for enhancing mathematical reasoning in Small Language Models (SLMs). Our approach leverages the extraction of key points from questions to improve understanding and reduce errors in reasoning tasks. Experimental results demonstrate that KPDD significantly reduces understanding errors compared to conventional mathematical reasoning distillation method. However, PoTD implicitly embeds the reasoning process within the generated program, making it difficult to analyze misunderstandings. In the future, we will explore error analysis methods to facilitate POTD error analysis."}]}