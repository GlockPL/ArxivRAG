{"title": "A MAMBA FOUNDATION MODEL FOR TIME SERIES FORECASTING", "authors": ["Haoyu Ma", "Yushu Chen", "Wenlai Zhao", "Jinzhe Yang", "Yingsheng Ji", "Xinghua Xu", "Xiaozhu Liu", "Hao Jing", "Shengzhuo Liu", "Guangwen Yang"], "abstract": "Time series foundation models have demonstrated strong performance in zero-shot learning, making them well-suited for predicting rapidly evolving patterns in real-world applications where relevant training data are scarce. However, most of these models rely on the Transformer architecture, which incurs quadratic complexity as input length increases. To address this, we introduce TSMamba, a linear-complexity foundation model for time series forecasting built on the Mamba architecture. The model captures temporal dependencies through both forward and backward Mamba encoders, achieving high prediction accuracy. To reduce reliance on large datasets and lower training costs, TSMamba employs a two-stage transfer learning process that leverages pretrained Mamba LLMs, allowing effective time series modeling with a moderate training set. In the first stage, the forward and backward backbones are optimized via patch-wise autoregressive pre-prediction; in the second stage, the model trains a prediction head and refines other components for long-term forecasting. While the backbone assumes channel independence to manage varying channel numbers across datasets, a channel-wise compressed attention module is introduced to capture cross-channel dependencies during fine-tuning on specific multivariate datasets. Experiments show that TSMamba's zero-shot performance is comparable to state-of-the-art time series foundation models, despite using significantly less training data. It also achieves competitive or superior full-shot performance compared to task-specific prediction models. The code will be made publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series forecasting predicts future data based on historical chronological information, offering a valuable tool for anticipating changes, formulating strategies, and mitigating risks. This technique is widely used across various sectors, including energy, finance, healthcare, manufacturing, retail, and traffic management.\nGiven the dynamic and ever-evolving nature of real-world data, forecasting models should be capable of adapting to changing patterns. However, traditional supervised models trained or even designed for each individual dataset or tasks (referred to as specialized models hereinafter), which are commonly used for time series forecasting, are often static and struggle to accommodate evolving patterns. This issue stems from three main challenges: first, these models require specific datasets for training, yet relevant data for emerging patterns may be unavailable or difficult to collect; second, they lack the ability to generalize across different datasets or applications, making it expensive and time-consuming to adapt models from one domain to another; and third, they often exhibit low data efficiency, increasing the risk of overfitting when training data are limited.\nIn contrast, time series foundation models, which are pretrained on vast domains of data, have demonstrated strong generalization capabilities across a wide range of scenarios and tasks. These models also exhibit high data efficiency in fine-tuning, enabling them to adapt to specific datasets with minimal samples. Such advantages make them effective for forecasting emerging patterns in web data, even when relevant data are unavailable or scarce. \nThe development of foundational models for time series forecasting draws inspiration from the success of large language models (LLMs, e.g., Devlin et al. 2018; Brown et al. 2020; Touvron et al. 2023) in natural language processing (NLP), though it faces additional challenges.\nThe first challenge is the significant heterogeneity of time series data. Data from different domains exhibit diverse dependencies, and data collected at varying frequencies or sampling rates present distinct patterns. Additionally, data gathered from different devices show varying noise levels. Multivariate time series also differ in their channel characteristics, with each dataset potentially containing a different number of channels. In contrast, text data for NLP models generally do not involve concepts like frequencies or sampling rates and typically consist of a single channel.\nSecondly, acquiring large-scale time series data is more challenging than in NLP. For example, the Large-scale Open Time Series Archive (LOTSA, Woo et al. 2024), the largest publicly available time series dataset to our knowledge, contains around 27 billion time points, whereas large NLP datasets, such as RedPajama-Data-v2 Together (2023), include tens of trillions of tokens.\nLastly, training foundational models for time series on large datasets imposes enormous computational demands, leading to long training times and high resource consumption. As a result, the process is both time-intensive and costly, requiring a significant budget for computing power.\nMotivated by these challenges, some emerging time series foundation models (e.g., Zhou et al. 2023; Jin et al. 2024; Liu et al. 2024a;b) leverage existing LLMs and adapt them to time series tasks through cross-modality transfer learning. This approach allows these models to harness the knowledge acquired during LLM pretraining on vast datasets using significant computational resources. Other models are trained directly on large-scale time series datasets (e.g., Garza & Mergenthaler-"}, {"title": "2 RELATED WORK", "content": "Over the last decade, time series forecasting has evolved from traditional statistical approaches to more advanced deep neural network-based techniques. Traditional models such as Autoregressive (AR), ARIMA, and VAR (Box et al., 2015), as well as kernel methods (Chen et al., 2008) and Gaussian processes (Frigola & Rasmussen, 2014), have been widely used. However, the advent of deep learning, fueled by the rapid evolution of computing capabilities and neural network architectures, has marked a paradigm shift in this field.\nVarious deep neural network architectures, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), graph neural networks (GNNs), multi-layer perceptrons (MLPs), and Transformers, have been extensively applied to time series forecasting. RNNs (e.g., Hochreiter & Schmidhuber 1997; Qin et al. 2017; Rangapuram et al. 2018; Salinas et al. 2020), specifically designed for sequential data, were among the first deep learning models utilized in this domain. However, RNNs encountered challenges such as vanishing gradients Pascanu et al. (2012), which limited their effectiveness in capturing long-term dependencies. Similarly, CNNs, originally developed for image processing, were adapted to achieve state-of-the-art performance in time series forecasting (Bai et al., 2018; Borovykh et al., 2017; Sen et al., 2019; Wang et al., 2023; Wu et al., 2022; Donghao & Xue, 2024). While CNNs excel at identifying local patterns, their limited receptive field size constrained their ability to capture long-term dependencies. GNNs are increasingly being employed to enhance the recognition of both temporal and dimensional patterns in time series data (Wu et al., 2020; Cao et al., 2020). Additionally, recent advancements (Challu et al., 2023; Li et al., 2023b; Zeng et al., 2022; Das et al., 2023a; Ekambaram et al., 2023) suggest that MLP-based architectures remain competitive in forecasting tasks.\nFollowing the remarkable success of Transformers (Vaswani et al., 2017) in NLP (Kalyan et al., 2021), computer vision (Khan et al., 2021), and speech processing (Karita et al., 2019), Transformers have become a mainstream approach in time series forecasting, delivering promising results (Wen et al., 2022). Their attention mechanisms are highly effective at capturing long-term dependencies. However, Transformers suffer from quadratic complexity in both computation and memory. Additionally, their flexibility leads to a lack of certain inductive biases Ascoli et al. (2021), which are beneficial for extracting sequential temporal dependencies.\nVarious Transformer-based approaches have been proposed to simultaneously enhance forecasting performance and reduce computational costs. For example, Informer (Zhou et al., 2021) and Autoformer (Wu et al., 2021) reduce complexity to $O(L \\log(L))$, where L is the input length. Several improved Transformers (Wang et al., 2020; Ma et al., 2021; Xiong et al., 2021; Choromanski et al., 2021; Liu et al., 2022; Zhou et al., 2022; Chen et al., 2024) even achieve linear complexity. PatchTST Nie et al. (2023) applies the patching technique (Dosovitskiy et al., 2021; Bao et al., 2022; He et al., 2021) to the context of time series, dividing time series into overlapping or non-overlapping continuous patches and embedding each patch instead of individual time points. Although patching does not reduce the theoretical quadratic complexity, it substantially lowers the actual computational costs.\nThe success of foundation models in NLP, which utilize large-scale pre-training to tackle diverse tasks with minimal labeled data, has inspired similar strategies in time series forecasting. Although pretraining requires extensive computational resources, these models can be fine-tuned and deployed for specific prediction tasks with moderate training budgets.\nTo address the scarcity of large datasets for training time series foundation models, recent efforts have adapted pre-trained large language models (LLMs) to create a unified framework for various time series tasks, effectively leveraging the ability of transformer-based models to generalize across different domains. Among these efforts, Zhou et al. (2023) demonstrated that the self-attention mechanism functions similarly to PCA, offering a deeper understanding of the universality of transformer-based models. They leveraged a primarily frozen GPT-2 backbone Radford et al. (2019) to achieve competitive performance across a range of time series tasks. TIME-LLM (Jin et al., 2024) converts input time series data into text prototype representations and enhances input context by incorporating declarative prompts to effectively guide the LLM's reasoning process. Additionally, Chang et al. (2023) developed a two-stage fine-tuning approach to adapt the GPT-2 backbone model for time series forecasting tasks."}, {"title": "3 METHOD", "content": "This section applies the advanced state space model, Mamba, to construct a foundational model for time series forecasting, called TSMamba. We begin with an introduction to the time series forecasting problem, followed by a description of the Mamba model. Next, we outline the structure of the foundational model and propose a two-stage transfer learning approach to adapt the model to time series data across different domains and frequencies. Finally, we present the fine-tuning process designed to extract relationships within specific datasets, incorporating a compressed channel-wise attention module to leverage cross-channel dependencies."}, {"title": "3.1 THE TIME SERIES FORECASTING PROBLEM", "content": "We consider the multivariate time series forecasting problem, which involves predicting the future values of a time series based on historical data. The input series is denoted by $X_{1:L} = \\{x_1,..., x_L\\}$, where L represents the look-back window (input length). The value at the ith time step is $x_i \\in \\mathbb{R}^D$, where D is the number of channels. The model maps the input to the prediction $Y \\in \\mathbb{R}^{D\\times T}$, where T is the prediction length, also known as the target window. The goal of the model is to understand the input series and minimize the prediction error relative to the actual future values $X_{L+1:L+T}$."}, {"title": "3.2 PRELIMINARIES OF MAMBA", "content": "Originating from the classic Kalman filter model Kalman (1960), the state space model (SSM) has recently garnered significant interest. Among these methods, structured state space sequence models (S4) and Mamba represent a recent class of sequence models inspired by the following continuous system\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t).$\nThis system maps a one-dimensional function or sequence $x(t) \\in \\mathbb{R}$ to $y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N_{st}}$.\nThe system (1) can be discretized through a zero-order hold (ZOH) rule into\n$h_t = \\bar{A}h_{t-1} + \\bar{B}x_t,$\n$y_t = Ch_t,$\nwhere\n$\\bar{A} = exp(\\Delta A),$\n$\\bar{B}, = (\\Delta A)^{-1}exp(\\Delta A - I) \\cdot \\Delta B$\nand A is the time step.\nMamba improves upon S4 by making the parameters that affect interactions along the sequence input-dependent, enabling the model to selectively propagate or forget information based on the current inputs, thereby enhancing its capability to perform content-based reasoning.\nSpecifically, it sets\n$B = Linear_{N_1}(x),$\n$C = Linear_{N_c}(x),$\n$\\Delta = softplus(Parameter + Broadcast_{D_{mb}}(Linear_1(x)),$\nwhere $Linear_a$ is a parameterized projection to dimension d, and $D_{mb}$ is the model dimension.\nGu & Dao (2023) also designed the Mamba block, which integrates an SSM into the main branch of the Gated MLP block and serves as a foundational module for building LLMs of different scales.\nThe modifications in Mamba introduce some drawbacks in parallel computation. S4 can be computed in two ways: either by using the linear recurrence form (2), which is efficient for autoregressive inference, or by employing a global convolution mode, which can be parallelized effectively during training Gu et al. (2022). However, when Mamba introduces input-dependent parameters, it becomes incompatible with the convolution mode, making it difficult to fully utilize the strong parallel processing power of hardware accelerators such as GPUs.\nTo address this, Gu & Dao (2023) designed a hardware-aware parallel algorithm in recurrent mode. Dao & Gu (2024) further improves the Mamba model, enabling the implementation of tensor parallelism. Consequently, Mamba not only achieves theoretically linear scaling with sequence length but also benefits from fast training and inference in practice."}, {"title": "3.3 ARCHITECTURE OF THE FOUNDATION MODEL", "content": "The architecture of TSMamba is illustrated in Figure 2. The model encodes preprocessed data using a backbone comprised of forward and backward Mamba encoders. These encoders consist of homogeneously stacked Mamba blocks, interspersed with standard normalization and residual connections. While the forward encoder extracts sequential causal dependencies, the backward encoder enriches the representation by capturing inverse time relations from the flipped embedding. The backward representation is then flipped and processed through a temporal convolution module to align with the forward representation in the time dimension. Finally, the combined representations are mapped to the forecasting output by a prediction head."}, {"title": "3.4 Two-STAGE TRANSFER LEARNING APPROACH", "content": "To harness the knowledge of existing language models, enabling TSMamba to adapt to time series data across different domains and frequencies with low training costs, we designed a two-stage transfer learning approach for training the model.\nThe first stage involves refining the backbone and training the input embedding through autoregressive forecasting or backcasting tasks.\nThe backbone is initialized with the Mamba language model, specifically Mamba-130M, while the input embedding and prediction head are trained from scratch.\nThe second stage focuses on training the prediction head and further refining the other structures. The original TSMamba architecture is restored, with the backbone and input embedding loaded from the results of the first stage, while the prediction head is randomly initialized. This stage produces the TSMamba foundation model for forecasting, which can be used for zero-shot predictions directly or fine-tuned to further enhance performance on specific datasets.\nIn this stage, the newly initialized prediction head and temporal alignment module are trained with a larger learning rate, while the existing backbone and embedding are updated with a smaller learning rate to fully leverage the pretrained model from the first stage.\nThe two-stage transfer learning approach also extends to downstream tasks beyond forecasting, such as imputation, classification, and anomaly detection. In these cases, the first stage provides an input embedding and a refined backbone that produce a robust representation of the time series data. The second stage can then be adapted to accommodate specific tasks, achieving zero-shot performance using the information from the training sets."}, {"title": "3.5 FINE-TUNING WITH CROSS-CHANNEL RELATION EXTRACTION", "content": "Fine-tuning focuses on learning the unique dependencies of a specified dataset, typically with limited training data. During fine-tuning, we freeze the Mamba blocks in the backbone, which contain most of the model's parameters, to preserve the relationships extracted during pretraining and the two-stage transfer learning. The input embedding, RMSNorm, and prediction head are adjusted to learn from the new dataset. Additionally, we introduce a compressed channel-wise Transformer encoder to extract cross-channel relations."}, {"title": "4 EXPERIMENTS", "content": "We compare TSMamba with 16 different baselines, which represents state-of-the-art models in long-term forecasting. Our baselines could be divided into two parts: zero-shot forecasting and full-shot"}, {"title": "4.1 ZERO-SHOT FORECASTING", "content": "The zero-shot forecasting results assess the generalizability of foundation models by examining each model's ability to adapt to previously unseen data.\nThe models are evaluated on widely recognized long-term forecasting benchmarks that differ from those used during pre-training. As this work is in progress, TSMamba is evaluated on ETTm2 and Weather, two commonly used medium-sized datasets. For each dataset, we considered four different forecasting horizons: {96, 192, 336, 720}. Model performance is measured using two standard evaluation metrics: mean squared error (MSE) and mean absolute error (MAE). The results of other methods for comparison are obtained from (Shi et al., 2024).\nResults. Our brief zero-shot results are shown in table 1, comparing TSMamba with state-of-the-art models. While TSMamba does not consistently outperform all baselines, it excels at longer prediction lengths (336 and 720) and achieves competitive average performance. Notably, despite being pre-trained on significantly less data, TSMamba performs comparably to models with larger pre-training datasets, demonstrating its data efficiency and robustness in zero-shot forecasting."}, {"title": "4.2 FULL-SHOT FORECASTING", "content": "To assess TSMamba's adaptability to specific datasets through fine-tuning, we compare its forecasting results on three widely used datasets: ILI, ETTm2, and Weather. TSMamba is tested with four different prediction horizons {96, 192, 336, 720}. The results of other methods for comparison are obtained from (Nie et al., 2023; Zhou et al., 2023; Cai et al., 2024).\nResults. Our brief full-shot results are shown in Table 2, where TSMamba demonstrates superior performance across multiple datasets and prediction horizons. On average, our model achieves a 15% gain in performance compared to GPT4TS (Zhou et al., 2023), which is a recent LLM based on GPT2. Additionally, TSMamba outperforms the state-of-the-art task-specific time-series model PatchTST (Nie et al., 2023)."}, {"title": "5 CONCLUSIONS", "content": "This paper introduces a Mamba-based foundation model for time series forecasting. The model employs both forward and backward Mamba encoders to capture temporal dependencies. To meet the challenges of forecasting heterogeneous time series data across different domains and frequencies with limited training resources, it leverages knowledge from the Mamba language model and adapts to time series datasets through a two-stage transfer learning approach. While the foundation model processes each channel of multivariate time series independently, it can capture cross-channel dependencies during fine-tuning on specific datasets using an additional compressed cross-channel attention module. TSMamba performs comparably to state-of-the-art models in zero-shot and full-shot settings while requiring significantly less training data, underscoring its potential to enhance downstream forecasting tasks."}]}