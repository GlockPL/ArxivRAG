{"title": "INTEGRATING NEURAL OPERATORS WITH DIFFUSION MODELS\nIMPROVES SPECTRAL REPRESENTATION IN TURBULENCE\nMODELING", "authors": ["Vivek Oommen", "Aniruddha Bora", "Zhen Zhang", "George Em Karniadakis"], "abstract": "We integrate neural operators with diffusion models to address the spectral limitations of neural\noperators in surrogate modeling of turbulent flows. While neural operators offer computational\nefficiency, they exhibit deficiencies in capturing high-frequency flow dynamics, resulting in overly\nsmooth approximations. To overcome this, we condition diffusion models on neural operators\nto enhance the resolution of turbulent structures. Our approach is validated for different neural\noperators on diverse datasets, including a high Reynolds number jet flow simulation and experimental\nSchlieren velocimetry. The proposed method significantly improves the alignment of predicted energy\nspectra with true distributions compared to neural operators alone. Additionally, proper orthogonal\ndecomposition analysis demonstrates enhanced spectral fidelity in space-time. This work establishes a\nnew paradigm for combining generative models with neural operators to advance surrogate modeling\nof turbulent systems, and it can be used in other scientific applications that involve microstructure\nand high-frequency content.", "sections": [{"title": "1 Introduction", "content": "Understanding turbulence is the last frontier in classical Physics. The complexity arises due to the high Reynolds\nnumbers encountered in atmospheric and engineering turbulence, which induce an energy cascade producing spatiotem-\nporal scales spanning multiple orders of magnitude, rendering them difficult to resolve fully. Also, extreme sensitivity\nto important other parameters, such as body roughness and external flow perturbations have a compound effect. In\n1972, Howard Emmons, a distinguished researcher from Harvard, famously asserted that turbulence would never be\ncomputable. However, with the advent of supercomputers and developing dealiased spectral methods, a breakthrough\noccurred just a couple of years later. Steven Orszag of MIT, utilizing the NCAR's first supercomputer (CDC7600),\nconducted the first direct numerical simulation (DNS) of homogeneous turbulence, albeit at lower Reynolds numbers\n(Re).\nProgress has been made since then on simulating accurately several fundamental turbulent flows [1, 2, 3] but even\ntoday with exascale supercomputers and a speed-up of over $10^{10}$ over the CDC7600, DNS of turbulence is still limited\nto relatively low Reynolds numbers and simple-geometry flows. On the experimental side, methods such as particle\nimage velocimetry (PIV) and particle tracking velocimetry (PTV) can provide useful information but cannot lead to\nquantitative understanding as DNS does, e.g. compute all critical statistical quantities of interest, and they may have\ndifficulty resolving high wavenumbers and high frequencies.\nIt is clear, therefore, that a fundamentally different approach is needed to break the deadlock. What is new now that\nwas not available before is the in-situ diagnostic tools, such as PIV and PTV, that can provide at least some partial"}, {"title": "2 Results", "content": "In the context of modeling physical systems, a non-linear mathematical operator $N$ that governs the evolution of a\nspatio-temporal field $u(x, t)$, can be expressed as,\n$u(x, t + \\Delta t) = N(u(x,t))$                                                                   (1)\nUnlike the conventional discretization-based solvers, neural operators (G) that learn to approximate the underlying true\noperator ($N \\approx G$) from its solutions, have been shown to generalize well across the unseen initial conditions.\nHowever, the neural operators often learn only the low wave numbers and ignore the higher wave numbers leading\nto an overall smoother approximation of the true solution. The issue is rooted in the spectral bias associated with the\nconventional training of the neural networks. In this section, we present the results of the experiments we performed\nto investigate whether diffusion models can help mitigate the issue of spectral bias in neural operators. The detailed\nmethodology is explained in section 4 and is illustrated in Figure 1. All the neural operators and diffusion models used\nin this study have approximately 2 to 3 million trainable parameters."}, {"title": "2.1 Kolmogorov Flow", "content": "The unsteady two-dimensional incompressible Navier-Stokes equation for a viscous, incompressible fluid with Kol-\nmogorov forcing [43] in the vorticity form is given by:\n$\\begin{cases}\n\\frac{\\partial \\omega}{\\partial t} + u \\cdot \\nabla \\omega = \\nu\\Delta \\omega + f(x,y), & (x,y) \\in (0,2\\pi)^2, t \\in (0, t_{final}]\n\\\\f(x, y) = \\chi(sin(2\\pi(x + y)) + cos(2\\pi(x + y))), & (x, y) \\in (0,2\\pi)^2\n\\\\\\nabla \\cdot u = 0, & (x, y) \\in (0, 2\\pi)^2, t \\in (0, t_{final}]\n\\\\\\omega(x, y, 0) = \\omega_0, & (x, y) \\in (0,2\\pi)^2\n\\end{cases}$                                                                                      (2)\nwhere $x = 0.1$, w is the vorticity, u is the velocity field vector, v(= $10^{-5}$) is the kinematic viscosity, and \u2206 is the\ntwo-dimensional Laplacian operator. We consider periodic boundary conditions. The source term f is given by\n$f(x, y) = 0.1(sin(2\u03c0(x + y)) + cos(2\u03c0(x + y)))$, and the initial condition w(x) is sampled from a Gaussian random\nfield according to the distribution $N (0,141^{1/2}(\u2212\u2206 + 1961)^{-1.5})$. We used the publicly available pseudo-spectral solver\n[14] to generate a dataset with 1000 samples, and randomly split it to train, validation and test sets in the ratio 80:10:10.\nThe dataset originally generated with a spatial resolution of 512 \u00d7 512, was downsampled to 128 \u00d7 128 to train the\nneural operators and diffusion models.\nWe train the neural operator (G), to learn the mapping $w(x, y, t)|_{t \\in [0,10]} \u2192 w(x, y, t)|_{t \\in [10,t_{final}]}$ of the vorticity field\nupto t = 10 to $t_{final}$ = 20. We borrow the same problem setup as in [14], but with less smooth initial conditions\n(see the initial vorticity in Figure 2), causing higher spatial gradients in the resulting simulation as demonstrated in\nthe first row of Figure 2. We investigate the effectiveness of the proposed framework across different neural operator\narchitectures - Fourier Neural Operator (FNO) [14], UNet, and MATCHO [17]. The FNO accurately learns the dominant\nlow wave-number modes, but remains oblivious to the higher wavenumbers corresponding to the finer spatial features.\nAdditionally, the presence of high spatial gradients in the true solution causes the FNO predictions to exhibit Gibbs\noscillations. The UNet and MATCHO also show smoothening in their predictions.\nThe spectral characteristics of the images generated by diffusion models [44, 45] motivated us to utilize them to\nimprove neural operator predictions. Specifically, we train a score-based diffusion model [35] conditioned on the neural\noperator's output, to approximate the ground truth data distribution using denoising score-matching. Once trained, the\ndiffusion model uses annealed Langevin dynamics-based sampling to generate its prediction conditioned on the specific"}, {"title": "2.2 Buoyancy-driven Transport", "content": "Here we train a neural operator to learn the buoyancy-driven transport of a scalar concentration field (d) [46]. We\ncan define the buoyancy model with Boussinesq approximation using the incompressible Navier-Stokes equation\naccompanied by the advection transport of d and can be expressed in terms of the buoyancy factor & as,\n$\\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u = -\\frac{1}{\\rho}\\nabla P + \\nu\\nabla^2u + [0, 1]^T\\xi \\alpha \\quad s.t \\quad \\nabla \\cdot u = 0$                                                                     (3)\n$\\frac{\\partial d}{\\partial t} + u\\nabla d = 0$                                                                                                                                (4)\nwhere 0 \u2264 t \u2264 21, v = $10^{-2}$ and \u00a7 = 0.5. The system has Dirichlet boundary conditions for the velocity (v = 0)\nand Neumann boundary conditions for the concentration field ( $\\frac{\\partial d}{\\partial n} = 0$). We utilized the dataset from PDEArena [47]\ncomprising of 2080 train, 260 validation and 260 test trajectories, generated using flow [48]. Each simulation was\nsaved every At = 1.5, resulting in 14 time steps per trajectory.\nWe train a U-Net as the neural operator to learn the evolution of d. The neural operator takes the states at 4 previous\ntimesteps as the input and predicts the next time step as output. During training, the ground truth data is provided as the\ninput. But while validating and testing, the ground truth data at only t = 0, \u2206t, 2\u2206t, 3\u2206t is available. So the neural\noperator performs an auto-regressive rollout to infer the future states. The setup of the neural operator is consistent with\nthat demonstrated in [47].\nIn Figure 4, we visualize the ground truth simulation in the first row, predictions of the neural operator in the second\nrow, and that of the diffusion models conditioned on the neural operator's output in the third row. We compare the\nspectral distribution of each of the simulations in row 4. We can observe the over-smoothening in the fields predicted by\nthe neural operator. The diffusion model conditioned on the neural operator achieves better spectral similarity with\nthe ground truth, compared to the neural operator alone. Furthermore, we perform POD to analyze the energy decay\nspectrum and to visualize & compare the POD modes of the Ground Truth, UNet, and UNet + Diffusion Model in\nFigure A.2. Again, the proposed framework demonstrates a better alignment with true modes than the neural operator\nalone."}, {"title": "2.3 Turbulent airfoil wake", "content": "Here we consider a turbulent wake flow downstream of a NACA0012 airfoil with a chord length of Le at a Reynolds\nnumber of 23000, a free stream Mach number of 0.3 and an angle of attack of 6\u00b0 [49]. The flow displays coherent\nstructures linked to the Kelvin-Helmholtz instability over the separation bubble and von Karman vortex shedding in the\nwake, while preserving the broad complexity typical of turbulent flow [50]. This makes it an excellent platform for\nCFD validation, flow analysis, and the investigation of reduced-complexity models. We directly utilize the large eddy\nsimulation (LES) dataset generated by Towne et al. [50] from \"Deep Blue Data\", a publicly available database from the\nUniversity of Michigan. The flow was simulated using a finite-volume based compressible solver - CharLES [51]. The\nLES dataset is comprised of 16,000 time-resolved snapshots along the mid-span slice, collected at a constant convective\ntime increment of T = 0.0104\\frac{Lc}{U\u221e}. For more details on the problem setup and dataset generation, please refer to section\nVII in [50].\nFor training the neural operator we consider a subset of the streamwise velocity component (u) of the previously\ndescribed dataset, consisting of 1000 timesteps, at a time increment of 57, within a subdomain 4L \u00d7 2.5Lc. Specifically,\nwe train a U-Net-based neural operator, MATCHO [17], to learn the mapping $u(x,t) \u2192 u(x, t + \\Delta t)|_{\\Delta t \\in [0,50\\tau]}$. We\nsplit the 1000 timesteps into train, validation and test datasets in the ratio 80:10:10. From Figure 5, we observe that the\nneural operator predicts a smoothened representation of the true LES simulations. The diffusion model conditioned\non the neural operator's output can recover the finer features, with a better aligned power spectrum. Furthermore, we\nperform POD to analyze the energy decay spectrum and to visualize & compare the POD modes of the Ground Truth,\nMATCHO, and MATCHO + Diffusion Model in Figure A.3. The diffusion model conditioned on the neural operator\ndemonstrates a better alignment with true modes than the neural operator alone."}, {"title": "2.4 3D Turbulent Jet", "content": "A jet is a free shear flow consisting of a fast inner stream and a slower outer stream [50]. In a jet, a high-velocity stream\nof fluid is ejected through a nozzle into the surrounding medium where the mean velocity gradient is not affected by the\nwalls. Simulating the jet facilitates understanding the flow dynamics and optimizing designs in real-world applications"}, {"title": "2.5 Schlieren velocimetry of turbulent helium jet in air", "content": "So far our results demonstrate the effectiveness of using neural operators as priors of diffusion models, on numerically\nsimulated turbulent databases. In the last test case, we investigate if the proposed framework can be directly applied\nto real experimental setups. We train the surrogate network to model a turbulent helium jet in air from Schlieren\nvelocimetry data [52]."}, {"title": "3 Discussion", "content": "Mitigating spectral bias in neural operators\nTrained neural operators can serve as fast surrogates for modeling physical systems when subjected to unseen initial or\nboundary conditions, without the need for solving the system again using traditional discretization-based numerical\nsolvers. However, the states inferred by the neural operator often tend to suffer from over-smoothening. In this study,\nwe systematically compare the distribution of energy across the spectrum of Fourier modes, for the true solution and the\nstates predicted by the neural operator. The energy of the state at any time step can be considered as the square of the\nabsolute of the Fourier transform of the corresponding state. For all the cases reported in section 2, the energy spectrum\nanalysis of the states demonstrates that the energy of the solutions inferred by the neural operator aligns well with the\nground truth only for lower wave numbers. For higher wave numbers, the energy of neural operator prediction was\nconsistently lower than that of the true solution. This behavior is called the spectral bias of neural operators and often\nbecomes more prominent when the prediction window horizon is longer. The problem of oversmoothening arising from\nspectral bias of neural operator becomes a significant drawback while modeling turbulent flows using neural operators,\nbecause turbulent systems retains non-trivial energy even at higher wavenumber that cannot be approximated by the\nneural operators in the conventional setup. Our results demonstrate that a diffusion model conditioned on a neural\noperator as its prior can overcome this issue and provide estimates of the state with a better aligned energy spectrum.\nComputational Costs\nThe diffusion model has to autoregressively perform n denoising steps to estimate the state of the system at any time step\nas shown in Figure 3. Recent advances and improvements in the training and sampling of diffusion models [35] helped\nto bring down the number of sampling steps from n = 1000 to n = 32. For the Kolmogorov flow test case, the trained\nneural operator can predict a trajectory from an unseen initial state in roughly 0.02s, whereas, the diffusion model\nrequires 0.9s. Although the diffusion model is slower than the neural operator at the inference, it is significantly faster\nthan the traditional discretization-based solvers and achieves the true spectral properties unlike the neural operators. All\nthe neural operators and the diffusion models consist of \u2248 2M trainable parameters. The neural operators were trained\nusing the cosine annealing based learning rate scheduler, and the diffusion models were trained with a constant learning\nrate of $10^{-4}$ as recommended in [35]. Our diffusion model was implemented based on [35, 53].\nProper Orthogonal Decomposition Analysis\nThe energy spectra in Figure 2 to Figure 7 demonstrate that the diffusion model conditioned on the neural operator\nperforms better than the neural operator alone. However, these plots are constructed for each time step separately and\ndo not contain information about the cross-correlation of the states across time steps. We address this question by"}, {"title": "Future Directions", "content": "Summarizing, we propose a framework that utilizes neural operators as effective priors for diffusion models. Our results\ndemonstrate that the proposed framework that blends neural operators with diffusion models significantly improves the\nenergy spectrum compared to the neural operators alone and has been tested across different types of neural operator\narchitectures - FNO, U-Net, and MATCHO, over a variety of test cases. The framework utilized here is general, and\ntherefore, any advances in the quality, computational efficiency, or scalability of either the neural operator or the\ndiffusion models in the future can be applied seamlessly and could potentially improve the proposed framework.\nThe sampling routine of the score-based diffusion model we adopted consists of 32 denoising steps, corresponding\nto 32 function evaluations as against 1 function evaluation for the neural operator. Therefore, the diffusion model is\napproximately 32 times slower than the neural operator at inference. Nevertheless, inferring states of the system using\ndiffusion models still takes only an order of 1s on an NVIDIA H100 GPU, for all the cases we considered. Improving\nthe accuracy of longer rollouts, incorporating conservation of physical laws, and applying the methodology to problems\nin other scientific domains are interesting future research directions. We also believe it is worth investigating the\npotential of combining neural operators with other generative modeling techniques like GANs and normalizing flows.\nThe utility of this new method is in accelerating DNS and LES by interweaving small bursts of such expensive\nsimulations with large extrapolation time intervals from our method. This is similar to previous work we did for\nmaterials problem [19] using the neural operators alone. Moreover, for experimental work where a few snapshots are\nvisualized, the present method will enable a continuous in time evolution of the turbulent field at DNS-level fidelity."}, {"title": "4 Methods", "content": "4.1 Neural Operators\nNeural Operators are a class of architectures in deep learning, that maps between infinite dimensional functional\nspaces. Consider an operator G, that maps from the input function X to the output function Y, i.e., G : X \u2192 Y. Then\nany machine learning architecture that is used to approximate this functional mapping to predict y \u2208 Y for a given\nx \u2208 X can be defined as a neural operator. Neural operator is a popular framework for encoding dynamics and making\npredictions under some given conditions. But like any other neural network architectures, they suffer from spectral bias.\nLemma 4.1. Let F : X \u2192 Y be a functional mapping with X,Y \u2208 \u0397\u00b0(\u03a9), where H\u00b0(\u03a9) is the Sobelov space of\nfunctions defined on N. Let G(0NO) be a neural operator parameterized by @NO, that is approximating the functional\nspace F. Then G learns the lower frequency components of the functional space F more effectively.\nSketch of Proof: We want to approximate the functional space F(X) = Y, then if we take the Fourier transform for\nthis function, we have\n$\\hat{F}(\\xi) = \\int_{-\\infty}^{\\infty} F(X)e^{-2\\pi i \\xi X}dX,$                                                                                                                 (5)\nand\n$F(X) = \\int_{-\\infty}^{\\infty} \\hat{F}(\\xi)e^{2\\pi i \\xi}d\\xi,$                                                                                                                  (6)\nwhere $\\hat{F}(\\xi)$ represents the amplitude of the the frequency component \u00a7. Now let G(0no, X) is the operator approxi-\nmating F(X), typically the loss function L(0no) used for optimizing such networks is given by\n$L(\\theta_{NO}) = \\sum_{n=1}^{N} (F(x_n) - G(\\theta_{NO};x_n))^2,$                                                                                                                 (7)\nwhere $x_n$ \u2208 X, and n = 1,2,..., N are the discrete points where the functional evaluation are available for training.\nWe can also express G in terms of its discrete Fourier components as\n$\\hat{G}(\\theta_{\\pi o};\\xi) = \\frac{1}{N} \\sum_{n=1}^{N-1}G(\\theta_{\\pi o}; x_n)e^{-2\\pi i \\xi n/N},$                                                                                                                 (8)\nand\n$G(\\theta_{\\pi o}; x_n) = \\frac{1}{N} \\sum_{n=1}^{N-1} \\hat{G}(\\theta_{\\pi o}; \\xi)e^{2\\pi i \\xi n/N}.$                                                                                                                 (9)"}, {"title": "4.2 Score-based diffusion models", "content": "The overall goal of a score (s) based diffusion model is to estimate the score function defined by $s_{\\theta_D} (X) =\\nabla_x log p(X),$ where OD are the parameters of the diffusion model and p is the probability density of X. Since\nthe data distribution is not known explicitly, and there is a chance that the data might lie on a lower dimensional\nmanifold, the score function may be ill defined in regions where there is no data. To circumvent this issue, perturbation\nof the data is done by adding Gaussian noise, which ensures that the score is well defined across the entire space\nby smoothing the distribution. Hence, the score function gives us a direction to move towards the region of higher\nprobability. However the direct way to sample from the distribution is missing here. In order to fix this problem,\nLangevin Dynamics was used in [54]. Langevin Dynamics ensures the convergence of the generated samples to the true\nunderlying distribution. It balances the inherent dynamics between deterministic movement, which is driven by the\ngradient of log of the probability, with random exploration given by the noise levels. In our proposed method, we use\nthe score function, which is additionally conditioned on the output of the neural operator Y = G(ONO, X) given by\n$s_{\\theta_D} (X, \\sigma, Y) = \\nabla_x log p(X|Y, \\sigma),$                                                                                                                                    (13)\nwhere o is the noise level. The modified score function now considers the joint distribution of X and Y. This ensures\nthat the the generated samples from the diffusion model are consistent with both the structures defined by Y and the\ndata distribution that is learned by the diffusion model. The update rule for Langevin dynamics can be shown as\n$X_{i+1} = X_i + \\frac{\\epsilon}{2}s_{\\theta_D} (X_i, \\sigma_i, Y) + \\sqrt{\\epsilon}z_i,$                                                                                                                                         (14)\nwhere & is the step size and zi is the noise component. From Eqns 13 and 14, we can see that as \u03c3\u03b5 decreases, the score\nfunction sen focuses on recovering the high-frequency details which was smoothen out by the noise. In the frequency\ndomain Eqn 14 can be shown as\n$\\hat{X}_{i+1} = \\hat{X}_i + \\frac{\\epsilon}{2}\\hat{s}_{\\theta_D} (\\hat{X}_i, \\sigma_i, \\hat{Y}) + \\sqrt{\\epsilon}\\hat{z}_i,$                                                                                                           (15)\nwhere $\\hat{X}_i$, \u015d and 2 are the Fourier transform of the respective functions. Hence, after multiple iterations as the noise\ndiminishes becomes negligible and the score function amplifies the high frequency component as it acts a high pass\nfilter in reverse.\nLemma 4.2. Let u(X, t) \u2208 H(\u03a9) : {u \u2208 L\u00b2(N)|J\u00bau \u2208 L\u00b2(\u03a9), \u2200|a| \u2264 s} be the true solution discretized over N\nspatial grid points.\n$u(X, t) = \\sum_{n=1}^{\\infty} a_n(t)\\phi_n(X),$                                                                                                                         (16)\nwhere $\\phi_n(X)$ are the basis functions and $a_n(t)$ are the time-dependent coefficients. Let UNO(X, t) and UD(X, t)\nrepresents the output of the neural operator and the diffusion model respectively, such that\n$U_{NO}(X, t) + U_D(X, t) = \\sum_{n=1}^{\\infty} a_n(t)\\phi_n(X).$                                                                                                                                          (17)"}]}