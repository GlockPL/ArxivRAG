{"title": "A Collaborative PIM Computing Optimization Framework for Multi-Tenant DNN", "authors": ["Bojing Li", "Duo Zhong", "Xiang Chen", "Chenchen Liu"], "abstract": "Modern Artificial Intelligence (AI) applications are increasingly utilizing multi-tenant deep neural networks (DNNs), which lead to a significant rise in computing complexity and the need for computing parallelism. ReRAM-based processing-in-memory (PIM) computing, with its high density and low power consumption characteristics, holds promising potential for supporting the deployment of multi-tenant DNNs. However, direct deployment of complex multi-tenant DNNs on exsiting ReRAM-based PIM designs poses challenges. Resource contention among different tenants can result in sever under-utilization of on-chip computing resources. Moreover, area-intensive operators and computation-intensive operators require excessively large on-chip areas and long processing times, leading to high overall latency during parallel computing. To address these challenges, we propose a novel ReRAM-based in-memory computing framework that enables efficient deployment of multi-tenant DNNs on ReRAM-based PIM designs. Our approach tackles the resource contention problems by iteratively partitioning the PIM hardware at tenant level. In addition, we construct a fine-grained reconstructed processing pipeline at the operator level to handle area-intensive operators. Compared to the direct deployments on traditional ReRAM-based PIM designs, our proposed PIM computing framework achieves significant improvements in speed (ranges from 1.75\u00d7 to 60.43\u00d7) and energy(up to 1.89\u00d7).", "sections": [{"title": "I. INTRODUCTION", "content": "Processing-in-memory computing enables specific computations to be performed in memory, which reduces power consumption and processing latency by minimizing the overhead of data transmission between memory and processors [1]. One of the most promissing PIM techniques is the resistive random-access memory (ReRAM)-based PIM design. The resistive devices, a.k.a., memristors can represent data by changing their internal resistance level and the formed crossbar array structures can perform storage and matrix calculation. A variety of ReRAM-based PIM designs have been proposed for"}, {"title": "II. MOTIVATION", "content": "A. Resource Underutilization at the Operator Level\nIn ReRAM-based DNN computing, certain types of operators may have high computing area (or cycles) demands but few cycles (or area) requirements. This area-cycle imbalance may bottleneck the pipeline itself or block other parallel pipelines by occupying their computing area. This challenge is further amplified in a multi-tenant scenario, where accelerators are tasked with several simultaneous networks. These networks also construct their inter-layer computing pipelines. The resource underutilization introduced by computation-intensive or area-intensive operators becomes increasingly prominent in these highly parallelized multi-tenant DNNs.\nVarious operators need diverse computation times, among which computation-intensive operators may clog the pipeline and lead to resource underutilization. Unlike GPU-based deep learning frameworks where computation numbers across layers are relatively uniform, there can be up to millions of computations for computation-intensive layers and only one for other layers, leading to the 'Barrel Principle.' This principle implies that inefficiency in one stage limits overall system performance. As illustrated in Figure 2b, deploying operator(2,1) and (2,2) on a 4-unit area would waste 2 units per clock cycle.\nAt the operator level in multi-tenant DNNs, area-intensive operators usually have massive parameters and need large computing areas. Unlike convolutional layers requiring many computation clock cycles, a large on-chip area can update millions of parameters in one cycle. As Figure 2a illustrates, using additional clock cycles when the on-chip computing area is insufficient may lead to underutilization. This over-\nTherefore, we propose a joint optimization framework with multi-tenant DNN-aware hardware resource allocation and a fine-grained operator processing pipeline to address these challenges. The tenant-level hardware resource allocation and operator-level pipeline reconstruction interact dynamically: the hardware allocated to a single DNN (or tenant) affects the granularity of operator splitting, and the processing efficiency of the fine-grained pipeline post-reconstruction determines the hardware resource demands. This joint optimization framework significantly improves resource utilization and reduces computing latency. We employ a classic ReRAM-based PIM design (i.e., ISAAC [12]) as our deployment prototype. Compared to traditional deployment, our proposed optimization framework achieves speed improvements of up to 60.43\u00d7 and energy efficiency improvements of up to 1.89\u00d7."}, {"title": "B. Resource Competition at Tenant Level", "content": "A significant difference between ReRAM-based computing and conventional GPU/CPU is that computational resources are directly related to the allocated area in ReRAM platforms, which causes resource contention at the tenant level and extends the computation latency. At the tenant level in multi-tenant DNNs, improper area allocation can lead to resource contention and underutilization. When running multiple networks simultaneously on one ReRAM chip, overlapping rewrite and processing regions can cause contention, as shown in Figure 2c, where NN1 and NN2 compete for a unit area, leaving three units idle in two cycles. This leads to significant underutilization and reduced efficiency. The issue can be resolved by segregating parallel networks into separate areas to optimize computational time.\nIn a multi-tenant DNN scenario, the overall results consist of independent results from each tenant, which means the overall latency depends on the network with the highest processing time. Partitioning at the tenant level can mitigate resource contention but amplify the variances in processing time. Therefore, we need to partition computing resources according to the processing time of each tenant, which depends on the operator reconstruction at the operator level. In essence, whether at the operator level or the network level, optimizing in isolation may impact the optimization outcomes of the other level. This can result in a scenario where concentration shifts to local optimization rather than achieving global optimization. Hence, a joint optimization between operator level and tenant level is vital to schedule multi-tenant DNNs on ReRAM-based computing."}, {"title": "III. CROSS-LEVEL OPTIMIZATION FRAMEWORK", "content": "Figure 5 illustrate the proposed joint optimization framework, which include a profiler to analysis hardware computing performance, a intelligent and tenant aware hardware resource partition, and a fine-grained operators reconstruction.\nA. Classic Profiler and Inter-layer Parallelism\nAfter determining tenant architecture and accelerator topology, computation costs, latency, and energy are analyzed using the classic profiler, as described in [12]. With each clock cycle set to 100ns, the profiler calculates clock cycles for each operator on each tenant architecture. Figure 3 showcases this analysis, and processing time follows equation (1), with $C_i$ as clock cycles (100ns), $N_i$ as calculation bits, and $N_p$ as subsequent pooling cycles. Besides, for the energy calculation, consumption is determined by the power usage (Table II) of computational tiles (according to tenant architecture and accelerator topology) and corresponding processing time (as described above), while disregarding the static power consumption of idle components.\n$T = C_1[(1 + b) + (6 + N_b + N_p)]$\\\nThe classic profiler considers not only the calculation pipeline within each operator but also between operators. As shown in Figure 4, traditional deployment augments inter-layer parallelism to create a high-throughput pipeline by duplicating convolutional kernels. When sequentially deployed, an operator with a stride of 2 needs two clock cycles for an output value. Networks like VGGs include multiple stride-k operators, making deeper ones require k exponential power cycles for one output. Analyzing the architecture and stride reveals a proportional relationship for kernel duplication.\n$N_p = \\sum_{i=1}^{n} S_i$\\\nThe duplication count in inter-layer parallelism is determined using equation 2, where i is the current layer, n is the tenant's total layers, and si is the stride of the next pooling layer. By combining Eq.1 and Eq.2, classic profilers can assess tenant computation time, accounting for inter-layer parallelism and the processing pipeline within an operator.\nB. Intelligent Hardware Resource Partition\nOur main purpose at the tenant level is to divide separate regions for each tenant to avoid spatial resource contention and to intelligently partition computing area for each tenant that better suit their quantities of computation. The main steps are as follows. First, we divide the entire on-chip computing"}, {"title": "C. Fine-grained Operators Reconstrcution", "content": "In this section, we first introduce how to reconstruct im-balanced operators via Duplicator and Splitter. Then, we utilize the grid-search method to jointly optimize the duplicate and split parameters to obtain fine-grained reconstructed operators, referred to as re-operators. These re-operators meet the necessary criteria to compose our inter-layer pipeline, preventing underutilization caused by over-occupation and excessive computation.\nDuplicator: In the classic deployment of single neural networks on ReRAM-based chips, researchers determine the duplication ratio based on the architecture of networks and further construct inter-layer pipelines. Our research aims to create the most computationally efficient fine-grained operators for a given configuration through joint duplication and splitting at the operator level. Rather than having individual duplication ratios for each layer, we introduce an overall duplicate parameter,a, to adjust the duplication ratio while maintaining the original inter-layer ratio. Keeping this original inter-layer ratio has significant benefits. First, keeping the originally balanced parallelism can avoid pipeline stalls and ensure the pipeline's high throughput. Second, optimization of splitting and duplicating with separate replication ratios for"}, {"title": "IV. EXPERIMENTS", "content": "A. Experiments setup\nVarious sizes of Multi-tenant DNNs: As shown in Table I, we used eight different base networks which include both shallow networks and complex networks to evaluate the speedup and energy of our deployment method for different complexity of multi-tenant DNNs.\nComparation baseline: The joint optimization method presented in this paper does not rely on accelerators with a specific topology. Instead, it iteratively partitions at the tenant level and reconstructs fine-grained pipelines at the operator level, adapting to various accelerator architectures. Therefore, a simple and classic accelerator can minimize unrelated factors and focus on the optimization framework itself. We did all the experiments and analysis on ISAAC [12], using this classic accelerator to explain the operational principles, observe the acceleration effects, and provide insights for future researchers.\nInference setup: With the power/area value from [12] as is shown in Table II, we first verify the overall speedup of our joint optimization framework. Then we show the effectiveness of optimization at the tenant level and operator level respectively. We set eight distinct multi-tenant DNNs on three chip sizes to validate the optimization performance of our framework within the context of ReRAM-based computation. These computations span various complexities of multi-tenant DNNs and different accelerator topologies.\nB. Energy Consumption Analyze\nAfter using our joint optimization for deployment, we follow the original principle of accelerators to calculate energy as described in sec 3A. Static energy consumption is ignored for the idle part.\nTable III shows that in small-sized chips (chip1), 7 of 8 multi-tenant DNNs reduced energy use, owing to resource contention. Coarse-grained operators in a tenant hinder full utilization of on-chip resources, with smaller chips amplifying tenant-level contention and operator-level underutilization, thus increasing energy consumption. Our framework targets these issues by eliminating contentions at the tenant level and reconstructing operators to enhance the efficiency of on-chip resource utilization, these optimization reduces energy consumption. For medium and large-sized chips, a balance must be struck between underutilization from resource contention and accumulated resource waste from dividing into multiple independent regions. Allocating distinct computation areas for each tenant improves utilization but fails to reach 100% efficiency, leading to an increased waste of area with more tenants. Our framework can cut energy usage in medium-sized chip experiments (chip2), though less effectively than in smaller chips (chip1). In larger chips (chip3) with less contention, our framework doesn't yield energy savings.\nC. Speed-up Evaluation and Analysis\nFollowing the processing principles of original accelerators (section 2A), we get the result Table III. After normalizing ISAAC baselines into 1, our latency speedup varies between 1.78 and 60.43 across all experiments, which indicates that our cross-level framework can enhance the efficiency of on-chip resources. For a detailed analysis, we have two observations.\nFirst, multi-tenant DNNs composed of shallow networks (such as MT4 and MT5) demonstrate a better speedup on various sizes of ReRAM-based accelerators, with values ranging from 12.60 to 60.45. This might be because complex multi-tenant DNNs have a wider array of operators available for selection during deployment while shallow networks present fewer candidate operators for selection and depend more heavily on external tools like ours to alleviate this temporal-spatial imbalance. Second, four out of the five multi-tenant DNNs"}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "In this work, we proposed a cross-level ReRAM-based in-memory framework for multi-tenant DNNs. In our joint optimization framework, we initially allocate different computational areas iteratively for different tenants at the tenant level. Then, for each individual tenant based on the assigned computational resources, we perform a fine-grained operator reconstruction method which consists of splitters and duplicators to rebuild processing pipelines. In this way, we achieved a 1.75x acceleration and energy reduction in various topological structures of chips and different multi-tenant DNNs."}]}