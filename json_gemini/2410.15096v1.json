{"title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "authors": ["Oh Joon Kwon", "Daiki E. Matsunaga", "Kee-Eung Kim"], "abstract": "A critical component of the current generation of language models is preference alignment, which aims to precisely control the model's behavior to meet human needs and values. The most notable among such methods is Reinforcement Learning with Human Feedback (RLHF) and its offline variant Direct Preference Optimization (DPO), both of which seek to maximize a reward model based on human preferences. In particular, DPO derives reward signals directly from the offline preference data, but in doing so overfits the reward signals and generates suboptimal responses that may contain human biases in the dataset. In this work, we propose a practical application of a diversity-seeking RL algorithm called GFlowNet-DPO (GDPO) in an offline preference alignment setting to curtail such challenges. Empirical results show GDPO can generate far more diverse responses than the baseline methods that are still relatively aligned with human values in dialog generation and summarization tasks.", "sections": [{"title": "1 Introduction", "content": "The goal of language model (LM) alignment is to steer the model's generation to produce outputs deemed desirable to human needs and values. Reinforcement learning with human feedback (RLHF) is one such critical technique, as evidenced by notable applications such as ChatGPT (Achiam et al., 2023) and Claude (Ouyang et al., 2022). The classical RLHF pipeline involves training the reward model from human feedback and optimizing the policy with the learned reward model by RL, e.g. proximal policy optimization (PPO) (Schulman et al., 2017). Despite its effectiveness, this pipeline is known to be sample-inefficient and unstable. Moreover, its optimal performance hinges on the code-level details and meticulously tuned hyperparameters, making it difficult to reproduce its success with limited computational resources.\nTo simplify this complex RLHF pipeline, recent works have explored offline learning algorithms such as Direct Preference Optimization (DPO) (Rafailov et al., 2024b), which aims to improve the efficiency and stability of RLHF by leveraging human feedback data to derive reward signals directly. While convenient and compute-efficient due to the offline nature of its training, theoretical results suggest that DPO tends to overfit on the reward signal (Azar et al., 2023) and learns to reject undesired responses at a faster rate than it learns to accept desired responses, limiting the model's learning capacity (Feng et al., 2024). To overcome these challenges, other works (Azar et al., 2023; Xu et al., 2024; Zhao et al., 2023b) have proposed regularized objectives, but none directly aims to model the diversity of the distribution. Instead, they tend to settle around local modes in reward distributions, which may be suboptimal. This lack of diversity may hinder its applicability to creative use-cases (Castricato et al., 2022) or under-represent certain demographics in the LM's responses (Lahoti et al., 2023).\nIn this work, we directly tackle the goal of preference alignment from the perspective of Bayesian inference. In particular, we utilize GFlowNets (Bengio et al., 2023), which has recently been introduced as a principled method for amortized sampling of multimodal distributions in proportion to a given reward distribution. Sampling proportionally to the reward distribution results in diverse yet high-reward samples. While there has been an application of GFlowNets for tuning LLMs to induce a latent chain-of-thought (Hu et al., 2024), there is no established method for using GFlowNets in the context of offline alignment of LMs without relying on an explicit reward model.\nTo this end, we propose GFlowNet-Direct Preference Optimization (GDPO), providing an efficient offline method for language model alignment. Similar to DPO, GDPO learns the policy by"}, {"title": "2 Preliminaries", "content": "We define the token-wise Markov Decision Process (MDP) as a tuple (S, A, f, r, po), where the state space S consists of tokens generated so far, action space A is the vocabulary of tokens, transition f is the string concatenation, and the initial distribution po is the distribution over the prompt x. The episode ends when the model generates the end-of-sequence (EOS) token (denoted T), from which no future reward is given. The resulting trajectory after iterative sampling from the policy is the response y = y1:nT. For notational simplicity, we shall denote the initial state s0 := x and the terminal state sf := x; y."}, {"title": "2.1 Generative Flow Networks (GFlowNets)", "content": "GFlowNets offer a way to sample a compositional object from a high-dimensional distribution by taking a sequence of actions according to a learned policy, where the unnormalized probability distribution of the resulting objects converges to the reward distribution (Bengio et al., 2023). This positions GFlowNet at the intersection of Markov Chain Monte-Carlo (MCMC) methods and neural network-based generative models.\nThe policy interacts with an MDP, represented as a directed acyclic graph (DAG) augmented with some nonnegative function F called flow. The state with no parent is the initial state so, and there is exactly one such state in the network. The states with no children are terminal states referred to as sf, which result in the objects of interest. The reward is defined on terminal states, i.e. r : Y \u2192 R\u22650.\nThe flow is defined on a complete trajectory, \u03c4 := (s0 \u2192 s1 \u2192 ... \u2192 sn) \u2208 T, as F : T\u2192R>0. The state flow for any state F(s) = \u2211\u03c4:s\u2208\u03c4 F(\u03c4) is the total flow through a state and the edge flow F(s \u2192 s') = \u2211\u03c4:(s\u2192s')\u2208\u03c4 F(\u03c4) is the total flow through an edge. Note that every complete trajectory contains the initial state, hence one can define a total flow Z := F(T) = F(s0) which normalizes the flow to induce a probability measure on G.\nFrom here, a flow is defined to be Markovian if there is a distribution \u03c0(\u00b7 | s) over the children of a non-terminal state, Ch(s) where s \u2260 sf such that \\pi(\\tau) = \\prod_{t=1}^n \\pi(s_t | s_{t-1}) = F(\\tau)/Z. The distribution \u03c0(st+1 | st) is a forward policy, which can be used iteratively to sample complete trajectories from the flow network. Since every non-initial state can have multiple parent states, we also define a backward policy \u03c0\u0392(st | st+1). The forward and backward policies can be written in terms of flow if the flow is Markovian:\n\u03c0(st+1 | st) = F(st \u2192 st+1)/F(st) and \u03c0\u0392(st | st+1) = F(st \u2192 st+1)/F(st+1).\nA GFlowNet is a sampling algorithm with parameterizations and an objective function based on the balance conditions imposed on the network that define the Markovian flow F. Either of the following parameterizations can uniquely determine the Markovian flow: 1. edge flows F(s \u2192 s'), 2. total flow Z and forward policy \u03c0, and 3. total flow Z and backward policy \u03c0\u0392 (Bengio et al., 2023). Moreover, GFlowNets impose a boundary condition where F(s \u2192 sf) = r(s). Once the forward policy is learned to follow these conditions, we can iteratively sample from \u03c0 to approximately sample from the target distribution that is proportional to the reward. Crucially, this sampling can be done in a way that naturally balances reward-maximization and entropy, which becomes important for our aim to balance alignment and diversity in LMs."}, {"title": "2.2 RLHF", "content": "The conventional RLHF pipeline consists of three stages: (i) supervised fine-tuning with instruction data, (ii) learning the reward model based on the preference dataset sampled from generated responses, and (iii) optimizing the language model policy with the learned reward. We focus our discussion on the latter two stages."}, {"title": "2.2.1 Learning the reward model", "content": "In preference modeling, the preference distribution on a pair of responses (y, y') to some prompt x is\nP(y \u227b y' | x) = g(r(x, y) \u2212 r(x, y')), (1)\nwhere y \u227b y' denotes that y is preferred over y'. Here g: R\u2192 [0, 1] should be a monotonically non-decreasing function such that g(w) = 1 \u2212 g(-w), so it can map to a valid probability distribution. A common choice for g has been the sigmoid function, which results in the Bradley-Terry (BT)"}, {"title": "2.2.2 Policy Optimization", "content": "The main objective of RLHF is to maximize the expected KL-regularized reward, i.e.\narg max Ex\u223cD [r(x, y)]\n\u03c0 \u03a5\u03c0\n\u2212\u03b2KL(\u03c0(y | x)||\u03c0ref(y | x)), (2)\nwhere \u03c0ref is the base reference policy. Equation 2 can be solved either in an online formulation with policy gradient algorithms such as PPO (Ouyang et al., 2022) or offline via a classification loss (Rafailov et al., 2024b).\nWe can also rewrite Equation 2 directly in terms of a preference dataset:\narg max E x\u223cD [P(y \u227b y' | x,y)]\n\u03c0 y,y' \u03c0\n\u03b2KL(\u03c0(y | x)||\u03c0ref(y | x)), (3)\nHowever, for the first term, we need to compute the posterior, which is intractable:\nP(y \u227b y' | x, y)\n\u221d P(y | x, y \u227b y') \u2211 P(y \u2212 y', x | y). (4)\nx\nWhile previous alignment approaches have avoided using Equation 3 due to intractable posterior terms, we aim to solve it directly using GFlowNets."}, {"title": "3 Related Works", "content": "RLHF. The classical RLHF framework was introduced in Christiano et al. (2017); Ziegler et al. (2019) and subsequently refined by Ouyang et al. (2022). PPO (Schulman et al., 2017) has been the primary choice of algorithm for RLHF, though others such as REINFORCE (Williams, 1992) has been explored in language modeling settings without human preferences (Paulus et al., 2017). As previously noted, PPO often demands significant resources and efforts to tune effectively. Consequently, recent research has focused on creating alternatives to the PPO methodology, one of the most prominent being DPO (Rafailov et al., 2024b).\nDPO removes the need for training an explicit reward model and suggests that the tuned language model can parameterize the reward model. Concurrent works also show that DPO-aligned models learn token-wise dense rewards under mild assumptions despite DPO being formulated in a trajectory-level contextual bandit setting (Rafailov et al., 2024a; Zhong et al., 2024). Subsequent works (Azar et al., 2023; Feng et al., 2024) pointed out DPO overfits on reward signals from the data which may contain human bias. To mitigate this, identity preference optimization (IPO) (Azar et al., 2023) and other methods (Xu et al., 2024; Zhao et al., 2023b) suggest regularized objectives for a better and more efficient optimization. Another method, Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024) integrates the entire pipeline by jointly learning supervised fine-tuning"}, {"title": "4 Method", "content": "As aforementioned in Section 2.2.2, Equation 3 can be hard to deal with due to intractable posterior terms. In this section, we present GDPO, which is able to overcome these challenges via GFlowNets and provide an efficient offline method for aligning LLMs.\nDetailed balance. GFlowNets are optimized via objectives based on balance conditions. The balance conditions are imposed on the flow network to ensure that the flow is consistent with the underlying dynamics of the graph\u00b9. For the application in language modeling, we consider the detailed balance (DB) (Bengio et al., 2023) condition, which simplifies the objective and parameterizations.\nTo see this, we note that GFlowNets can be drastically simplified in the language modeling setting (Hu et al., 2024). Since all states in token MDPs are terminable (since EOS token probability is non-zero at most states), we can parameterize F in terms of \u03c0, i.e. F(s) = r(s)/\u03c0(sf | s), because the boundary condition r(s) := F(s \u2192 sf) = F(s)\u03c0(sf | s) holds for any terminating state. Furthermore, the backward transition becomes trivial, i.e. \u03c0\u03c1(s' | s) = 1.\nThe DB condition (Bengio et al., 2023) dictates that the transition flows must coincide, similar to the DB condition in Markov chains. This follows immediately from the definition of the forward and backward policies, and the detailed balance condition can be written as F(s)\u03c0(s' | s) = F(s')\u03c0\u0392(s | s'). The original DB objective in Bengio et al. (2023) is in the following form:\nLDB (\u03c0, \u03c0\u0392) = \u2211 log (F(s)\u03c0(s' | s))\nF(s')\u03c0\u0392(s | s').\nFollowing the LM formulation, the DB objective can be written in terms of reward and the forward policy 2. Letting \u03c0\u03b2(\u00b7) = 1 and F(s) = r(s)/\u03c0(sf | s), we have\nn-1\nLDB(\u03c0, \u03c0\u0392) = \u2211(1\u2212 log ( r(Yt | Y1:t\u22121)\u03c0(T | Y1:t+1) ))\nt=1\nr(Yt+1 | Y1:t)\u03c0(T | Y1:t)\n+ log (\u03c0(Yt+1 | Yt))\n2 (5)\nReward model. We define the token-wise reference log reward for the k-th token for each response in the pair as Equation 6. This ensures the model does not deviate too far from the reference model and learns to terminate at appropriate positions.\nWe temper the terminating log probability with hyperparameter \u03b3 \u2208 (0,1] to control the strength of the reward signal. We found it helpful to set \u03b3 \u2264 0.5. Without the terminating log probability reward, the generation may end abruptly because the flow function has been parameterized with the reward by assuming every state is terminable.\nlog rref(yk; x) := log \u03c0ref(yk | x, yk\u22121) + exp (\u03b3 log \u03c0ref(T | x, yk)) (6)\nGiven pairwise preference data (x, y, y'), we apply terminating flows to the preferred responses by setting p(y \u227b y' | x) = 1y>y', similar to the assumption made in the DPO objective. We expect that a GFlowNet-tuned language model (LM) policy will learn to assign credit to each token in a manner akin to DPO, as suggested by concurrent works (Zhong et al., 2024; Rafailov et al., 2024a). With \u03b1 \u2208 (0, 1], we define the total reward at the k-th token of a response y with the other response"}, {"title": "5 Experiments", "content": "Model and baselines. To investigate the scalability of the algorithm, we fully fine-tune and evaluate a series of OPT models, namely OPT -1.3b, and -2.7b (Zhang et al., 2022). We compare the proposed method against the following baselines: supervised finetuning (SFT), PPO (Schulman et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022), DPO (Rafailov et al., 2024b), Identity Preference Optimization (IPO) (Azar et al., 2023), Contrastive Preference Optimization (CPO) (Xu et al., 2024),"}, {"title": "6 Results", "content": "According to Figure 1 and 2, GDPO generates significantly more diverse outputs compared to other methods, demonstrating a clear advantage in encouraging creativity and variability in model outputs. Additionally, the method scales effectively to larger models, showing consistent performance improvements as model size increases. However, this increase in diversity is accompanied by a higher standard error in win rate. Depending on the task, GDPO performs on par with or, in some cases, worse than baseline methods on average. In particular, GDPO struggles with summarization tasks (Figure 2), likely due to the nature of these tasks, where consistency is prioritized over diversity in outputs. On the other hand, GDPO can still perform on par or slightly worse than the baselines in the"}, {"title": "7 Conclusion", "content": "We propose GDPO, a novel approach to language model alignment that leverages the strengths of GFlowNets to overcome the limitations of traditional RLHF and DPO methods. GDPO simplifies the alignment process by utilizing an offline prefer-"}, {"title": "8 Limitations", "content": "GFlowNets can be trained offline or online, with empirical evidence suggesting that the online approach may be more effective for certain tasks (Shen et al., 2023). However, in this work, we focused on addressing the limitations of current alignment methods while maintaining computational efficiency, and therefore, we did not explore the online setting. It would be valuable in future work to investigate how GFlowNets compares to RL methods such as PPO in an online setting, particularly in terms of computational cost, sample diversity, and overall efficiency. Additionally, experimenting with different reward structures (as discussed in Section 4) or model architecture and scale could potentially improve performance. Lastly, the relationship between diversity and alignment remains an open question that warrants further exploration, which we propose as a direction for future research."}, {"title": "9 Ethical Considerations", "content": "The alignment methods discussed in this work have significant implications, as they are used in production language models like ChatGPT. While GDPO aims to enhance the diversity of generated samples"}]}