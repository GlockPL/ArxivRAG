{"title": "GPT-Augmented Reinforcement Learning with Intelligent Control for Vehicle Dispatching", "authors": ["Xiao Han", "Zijian Zhang", "Xiangyu Zhao", "Guojiang Shen", "Xiangjie Kong", "Xuetao Wei", "Liqiang Nie", "Jieping Ye"], "abstract": "As urban residents demand higher travel quality, vehicle dispatch has become a critical component of online ride-hailing services. However, current vehicle dispatch systems struggle to navigate the complexities of urban traffic dynamics, including unpredictable traffic conditions, diverse driver behaviors, and fluctuating supply and demand patterns. These challenges have resulted in travel difficulties for passengers in certain areas, while many drivers in other areas are unable to secure orders, leading to a decline in the overall quality of urban transportation services. To address these issues, this paper introduces GARLIC: a framework of GPT-Augmented Reinforcement Learning with Intelligent Control for vehicle dispatching. GARLIC utilizes multiview graphs to capture hierarchical traffic states, and learns a dynamic reward function that accounts for individual driving behaviors. The framework further integrates a GPT model trained with a custom loss function to enable high-precision predictions and optimize dispatching policies in real-world scenarios. Experiments conducted on two real-world datasets demonstrate that GARLIC effectively aligns with driver behaviors while reducing the empty load rate of vehicles.", "sections": [{"title": "Introduction", "content": "The past decade has witnessed explosive growth in online car-hailing services, fundamentally transforming urban transportation. Central to this transformation is the role of vehicle dispatching (Shi et al. 2024), which serves as a pivotal component in reducing the waiting time of passengers, increasing the income of drivers, and facilitating daily transportation (Barrios, Hochberg, and Yi 2023; Rahman and Thill 2023; Sadrani, Tirachini, and Antoniou 2022). In recent years, reinforcement learning (RL) methods have emerged as outstanding performers in areas such as multi-agent control and sequential decision-making (Qiu et al. 2023; Ellis et al. 2024; Han et al. 2023). Therefore, many studies have leveraged RL techniques to enhance vehicle dispatching, treating it as a multi-agent sequential decision-making task (Guo et al. 2024; Huang et al. 2023).\nTo address the all above challenges, we propose a GPT-Augmented Reinforcement Learning with Intelligent Control framework, GARLIC, which utilizes an improved MARL approach. Specifically, we design a hierarchical traffic state representation module to integrate traffic features at different granularities, providing a comprehensive representation of real-time traffic conditions. Additionally, we quantify driving behavior through dynamic rewards using a contrastive learning method, aligning dispatching instructions with the intents of drivers. Given the complex analytical and understanding capabilities required for learning vehicle dispatching policies, we employ a Generative Pre-trained Transformer (GPT)-augmented model with a self-defined loss function to enhance the expression of the framework. To the best of our knowledge, our innovative framework offers a comprehensive solution to the core challenges in vehicle dispatching, setting a new benchmark in this field. Our main contributions can be summarized as follows:\n\u2022 Our proposed framework, GARLIC, combines hierarchical traffic state representation, dynamic reward generation, and GPT-augmented dispatching policy learning. To the best of our knowledge, this novel approach builds a complete GPT-enhanced MARL vehicle dispatching framework that has not been explored previously;\n\u2022 We utilize multiview graphs to depict the hierarchical traffic states in the road networks and establish a dynamic reward model for capturing driving behaviors, leading to better dispatching policy outcomes. These innovations contribute significantly to the improved performance of vehicle dispatching;\n\u2022 Extensive experiments on two real-world road networks against advancing baselines demonstrate the effectiveness and efficiency of GARLIC."}, {"title": "Preliminary", "content": "In this paper, we adopt a novel MARL method to optimize online car-hailing dispatching policies. This section outlines critical definitions for understanding our paper.\nVehicle Trajectory 7: This refers to a sequence of GPS points $(x_t, y_t)$ recorded over a time interval $t \\in [T]$, represented as $r = (x_1, y_1, t_1),\\ldots,(x_T, y_T, t_T)$. A vehicle can generate multiple trajectories based on different statuses (such as empty or occupied). We focus solely on empty vehicle trajectories to better understand driving behavior when drivers don't have a specific destination.\nmultiview Graph $G^i$: We define the multiview graph as $G^i = {V^i, E^i}$, where $i \\in {micro, meso, macro}$ presents different views, the node set $V^i$ represents various traffic zones, and the edge set $E^i$ indicates the connections among these zones. The features of each traffic zone at time t are denoted by $X_t \\in R^{|V| \\times m^i}$, capturing vehicle availability and order demand. For different views of graphs, we have different graph features: $m_{micro} \\neq m_{meso} \\neq m_{macro}$.\nMulti-Agent Reinforcement Learning for Vehicle Dispatching: In our model, each vehicle in the road network acts as an independent agent, with distinct driving behaviors and the ability to generate continuous trajectories and monitor local traffic conditions. For each agent (vehicle) u, we consider the following five essential elements:\n\u2022 Decision Time $[T]$: It is a set of all finite decision timesteps $[T] = {1,\\ldots,t,\\ldots, T}$. At each timestep t, the vehicle location and environment states are sampled.\n\u2022 Action $A_u$: $A_u = {a_u^1,\\ldots,a_u^t,\\ldots,a_u^T}$ represent the set of actions to balance the vehicle supply and demand. $a_t^u := {dis_t, deg_t}$ is an action performed by the vehicle u at time t, where $dis_t$ is the straight-line distance a vehicle needs to travel from time t to t + 1, and $deg_t$ means the azimuth angle between the target and current locations.\n\u2022 State $S_u$: $S_u = {S_u^1,S_u^2,\\ldots,S_u^T}$ represents the set of traffic states observed at each time t. Here $S_t$ is the concatenation of the state embedding matrix $Emb_{G,t}$ extracted from the traffic environment and the location embedding matrix $Emb_{loc,t}$ of the vehicle at time t.\n\u2022 Reward $R_u$: $R_u = {r_u^1,\\ldots,r_u^t,\\ldots, r_u^T}$ represents the set of rewards calculated by the reward function, and it is predefined according to the driver's driving behavior and the taxi fare. The total return is defined as $\\sum_t \\gamma^t . r_u^t$, where $\\gamma$ is a discount factor, $\\gamma \\in [0, 1]$.\n\u2022 Policy $\\pi_u$: $\\pi_u = \\pi(a|s)$ is a mapping from traffic states to dispatching actions of the u-th agent. The policy $\\pi$ determines the appropriate vehicle dispatch instructions a by analyzing the state s, which includes various features of the environment and the current status of the agent.\nWhile agents in the same area and close to each other may share the same multiview graphs $G^i$ of the road network and observe similar traffic features $X_t$, they exhibit unique driving behaviors that significantly influence their vehicle trajectories. To account for these behavioral differences, our study departs from conventional MARL frameworks with fixed rewards by employing a dynamic reward model. Additionally,"}, {"title": "The Proposed Framework", "content": "In this section, we first provide a framework overview of GARLIC. Then we introduce the hierarchical traffic state representation method to capture the real-time traffic states. Furthermore, we demonstrate a dynamic driving reward generation approach to score vehicle trajectories under different driving behaviors. Finally, a GPT-augmented dispatching policy learning model is applied to combine all of the components and learn the vehicle dispatching policy.\nFigure 2 provides an illustration of the overall vehicle dispatching framework, which is composed of three key modules: the hierarchical traffic state representation module, the dynamic reward generation module, and the GPT-augmented dispatching policy learning module.\nIn the first module, we employ a multiview Graph Convolutional Network (GCN) to represent the hierarchical traffic state by integrating traffic information gathered by various vehicles at different levels of granularity. By combining this with GeoHash-based vehicle location embeddings, we can accurately calculate the real-time traffic state of the specific region where each vehicle is located.\nThe second module utilizes a Gated Recurrent Unit (GRU)-based Recurrent Neural Network (RNN) to model driving behaviors, generating dynamic rewards that are weighted by the regional median carfare. This approach ensures that the reward system reflects both the temporal and spatial nuances of driver behavior.\nFinally, in the third module, we frame the training of the MARL-based vehicle dispatching task as a supervised learning process (Wang et al. 2024b; Yamagata, Khalil, and Santos-Rodriguez 2023). For each agent, the time-ordered states and rewards are utilized as inputs, and a GPT-augmented model is employed to produce high-precision actions for vehicle dispatching."}, {"title": "Hierarchical Traffic State Representation", "content": "Urban spatiotemporal data exhibits hierarchical characteristics (Ning et al. 2024), which cannot be directly represented using a single structured data format. For instance, features such as turning movements at a crossroad can only be captured from a micro-level view of the traffic environment, whereas the average travel time is a feature observable only from a macro-level perspective of the same environment. These features differ in sampling frequencies, dimensions, and units, necessitating specialized approaches to represent and integrate them accurately.\nTo address this issue, we present the road network as multiview honeycomb graphs, as shown in Figure 3(a). In this representation, the road network is divided into grids comprising square hexagons of varying radii, each representing a distinct view. Here, the hexagon-based grids ensure uniform distance from all adjacent neighbors to the central grid, facilitating more precise modeling of different spatial regions than square grid-based methods. To construct the multiview graph, each grid is treated as a node, and the traffic information in a grid is considered to be the node feature, with edges connecting adjacent grids, as shown in Figure 3(b).\nUnlike other road network modeling methods, we calculate distinct traffic indicators for different views of graphs. The micro-level graph primarily utilizes vehicle trajectory, road congestion status, and vehicle speed-data that can be directly obtained from the local environment (radii < 1km) of a vehicle. The meso-level graph considers factors such as traffic volume, average traffic speed, intersection performance, and parking availability, which require analysis of all vehicles passing through a set of certain traffic sections\u00b9 (1km < radii < 5km). Meanwhile, the macro-level graph includes features such as average travel time, road network connectivity, and overall traffic conditions, which necessitate a more comprehensive analysis of vehicles across a broader range (radii \u2265 5km) of the road network.\nTo extract refined the traffic embeddings $Emb_{G,t}^u$, a GCN-based model is then deployed for multiview graph representation for a vehicle u:\n$Emb_t^u = Concat(Emb_t^{u,i}),$\n$Emb_t^{u,i} = GCN(A^{u,i}, X_t^{u,i}) = A^{u,i} X_t^{u,i} W^i,$\nwhere $i \\in {micro, meso, macro}$, $W^i$ is the weight matrix that need to be trained, $A^{u,i}$ is the adjacency matrix of the graph $G^{u,i}$, under a specific view i, and $X_t^{u,i}$ is the traffic features related to the graph $G^{u,i}$, at time t.\nNote that a high-accuracy location embedding of real-time trajectories is essential for this task. We first use GeoHash (Morton 1966) to encode each real-time GPS point, based on latitude and longitude, in the trajectory:\n$Emb_{loc,t}^u = GeoHash(lat_t^{loc}, lon_t^{loc}),$\nwhere $t \\in {0,1,\\ldots,T}$ presents a specific timestep, and $loc_t := (lat_t^{loc}, lon_t^{loc})$ is the real-time GPS point of vehicle u. By combining this location embedding with the traffic state surrounding vehicle u at time t, we obtain the overall state embeddings of vehicle u:\n$s_t^u = Concat(Emb_t^{u}, Emb_{loc,t}^{u}).$"}, {"title": "Dynamic Reward Generation", "content": "The effective implementation of vehicle dispatching in real-world scenarios is largely influenced by driving behavior. However, early studies often ignored the quantification of driving behavior, focusing instead on minimizing vehicle imbalance or maximizing benefits in dispatching optimization (Wagenmaker and Pacchiano 2023). In this section, we propose a dynamic reward generation method that incorporates both driving behaviors and anticipated income. It quantifies the likelihood of drivers adhering to their driving habits by analyzing the vehicle trajectories in real-time.\nTo accurately capture the relationship between driving trajectories and corresponding driving behaviors in traffic embeddings, we deploy a GRU-based RNN model. This network calculates the probability $p_t^u$ whether a trajectory belongs to a given vehicle u.\n$z_t^u = \\sigma(W_{zx} Emb_{loc,t}^u + W_{zp} h_{t-1}^{u} + b_z),$\n$y_t^u = (W_{yx} Emb_{loc,t}^u + W_{yp} h_{t-1}^{u} + b_y),$\n$\\hat{p_t^u} = tanh (W Emb_{loc,t}^u + y_t^u W_p + b'),$\n$h_t^u = z_t^u h_{t-1}^u + (1 - z_t^u) \\odot \\hat{p_t^u},$\nwhere $t \\in {1,2,\\ldots,T}$, $h_0^u := Emb_{loc,0}^u$ is the location embedding at initial timestep (t = 0), $W_{GRU} = {W_{zx}, W_{zp}, W_{yx}, W_{yp}, W, W_p}$ is the set of weight matrices of GRU, and $b_{GRU} = {b_z, b_y, b'}$ is the set of bias. We employ a contrastive learning method to optimize the model parameters. Since different drivers exhibit distinct driving behaviors, trajectories generated by other vehicles are used as negative samples when modeling a specific driver's behavior, as illustrated in Equation (5).\n$Loss_{pre-training} = max \\sum_{u\\in[N]} \\sum_{t\\in[T]} q_t^u log p_t^u,$\nwhere N is the total number of online car-hailing vehicles, $q_t^u \\in {0,1}$ is the ground truth of the GPS point generated by the vehicle u at time t.\nAdditionally, when a vehicle is carrying passengers, the regional median carfare earned by a driver is another factor influencing vehicle dispatching. Therefore, we introduce the dynamic reward function, which incorporates both factors by introducing a hyperparameter $\\alpha$ to weigh them together.\n$r_t^u = \\alpha \\cdot p_t^u + (1 - \\alpha)\\cdot \\sigma(W_{fare} \\hat{x}_{fare,t}),$\nwhere $\\sigma(\\cdot)$ is the sigmoid activation function, $\\hat{x}_{fare,t} = \\sum_{t=0}^T x_{fare,t'}$, and $W_{fare}$ is the weight matrix."}, {"title": "GPT-Augmented Dispatching Policy Learning", "content": "As discussed in the framework overview, vehicle dispatching can be effectively modeled as a MARL problem, which can also be reformulated as a supervised learning task. In this context, states and rewards are treated as sequential input data, with the corresponding sequence of actions as the output data. However, the complexity of transportation systems, which requires the analysis of intricate traffic states and driving behaviors, demands advanced reasoning capabilities. Recently, the GPT model has demonstrated strong performance in handling long-sequence, context-dependent, and structured data. Therefore, we utilize a GPT-augmented model to address these challenges.\nNote that the core part of a GPT model is the transformer structure. The input of the transformer is a sequence of temporal data, and we assign different positional embeddings to the data at different timesteps. At each timestep, we mainly use two deep transformer decoder blocks to extract the probability of the next action. We use the expected reward at the current time step as the input of the first transformer decoder block, and get the output embedding to guide the subsequent transformer decoder block to calculate the result:\n$Emb_{r,t} = Decoder(P_{a,t-1}, r_t, t),$\n$P_{a,t} = Decoder(Emb_{r,t}, s_t, t),$\nwhere t starts from 1, and $P_{a,0} = O$ is initialized as the zero tensor at t = 0. $Decoder^{(k)}(x, y, z) = Decoder(Decoder^{(k-1)}(x, y, z))$ is a k-layer deep neural network of the transformer decoders. For each layer $Decoder^{(k)}(x, y, t) = Attention(x + Emb_{pos}(t), x + Emb_{pos}(t), y + Emb_{pos}(t))$, we add the same positional embedding $Emb_{pos}(t)$ to each input x and y. Here $Attention(x, y, z) = softmax((xW_q)(yW_k)^T) (zW_v),$\nwhere $\\sigma(\\cdot)$ is the GelU activation function.\nFinally, we use a Multi-Layer Perceptron (MLP) mapping the action probability tensor $P_{a,t}$ to a unique result $a'_t$ in the closed action set A as the action that a vehicle needs to perform in the current step:\n$a'_t = [a_t^{(1)'}, a_t^{(2)'}] = MLP_{wa}(P_{a,t}),$\nwhere $a_t^{(1)'}$ is the normalized distance from the current location, $a_t^{(2)} \\in [0^\\circ, 360^\\circ]$ is the direction that a vehicle headed to, and both of $a_t^{(1)'}$ and $a_t^{(2)'}$ make up the unique action that controls this vehicle, $W_a$ is the training parameters.\nNote that the difference between $359^\\circ$ and $1^\\circ$ is only 2 degrees when measuring angles. Most common loss functions (e.g., MAE Loss and MSE Loss) cannot describe this phenomenon well. To train our framework GARLIC effectively, we proposed a novel loss function, named Geospatial Loss (GeoLoss), to minimize the geospatial difference between the predicted action and the ground truth for this task, and our training target is to minimize the GeoLoss that we defined below:\n$min Loss(a, a_t) = min ((a_t^{(2)'} - a^{(2)})^2,\\newline (a_t^{(2)} + 360^\\circ - a^{(2)})^2) + (a_t^{(1)'} - a^{(1)})^2).$"}, {"title": "Experiments", "content": "This section conducts extensive experiments using 2 real-world datasets to evaluate the effectiveness of GARLIC. We first introduce the experimental settings. Next, we compare GARLIC with representative baselines. Finally, the ablation study and a case study are introduced to illustrate our framework better. More experiments are detailed in the Appendix."}, {"title": "Experimental Settings", "content": "Dataset. We use two datasets with different scales for experiments: one is located in lower and midtown Manhattan, New York City, USA, and the other larger dataset is the taxi trajectory data from the core area of Hangzhou, Zhejiang Province, CHN. More details can be found in Appendix A.\nMetrics. We use the Euclidean distance metric, Error, to assess the discrepancy between predicted actions and the driver's actual driving intentions. Additionally, the empty-loaded rate metric is employed to measure the efficiency of the car-hailing service, which is another widely used metric in transportation systems (Cao, Wang, and Li 2021).\nBaselines. We compare GARLIC with baselines from two different categories: (1) Online RL methods: MT (Robbennolt and Levin 2023) and FTPEDEL (Wagenmaker and Pacchiano 2023); (2) Offline RL methods: CQL (Kumar et al. 2020), TD3+BC (Fujimoto and Gu 2021), Decistion Transformer (DT) (Chen et al. 2021), RLPD (Ball et al. 2023), latent offline RL (Hong, Levine, and Dragan 2024), and SS-DT (Zheng et al. 2023). More details about these methods can be found in Appendix C."}, {"title": "Implementation Detail", "content": "Firstly, it's important to note that the correlation and influence between vehicles decrease exponentially with distance (Zhao et al. 2024). To avoid network congestion, we only allow V2V communications between vehicles in adjacent regions. In addition, we limit the waiting time for vehicles to broadcast and receive V2V multi-hop messages across different regions to 1 second, ignoring any timed-out transmissions. Typically, the delay for 1-hop 5G V2V data transmission ranges from 50ms to 200ms (Hakak et al. 2023; Boualouache and Engel 2023), enabling at least 5-hop communication within this limitation. As the macro-level graph shown in Figure 3, 5 communication hops allow vehicles to gather traffic information within about a 55km radius (city-level), which is sufficient for vehicle dispatching tasks.\nAdditionally, our experiments are simulated on the open-source software SUMO for secondary development and are deployed in a Linux server with two Intel(R) Xeon(R) Gold 6248R CPUs, 8 NVIDIA V100 32G graphics cards, and 800G memory. More details can be found in Appendix D."}, {"title": "Overall Performance", "content": "The performance of all the baselines in both two datasets is shown in Table 1, in terms of the two metrics we introduced before, i.e., Error and empty-loaded rate. We use M to present the Manhattan dataset and use H to stand for the"}, {"title": "Ablation Study", "content": "The effectiveness of multiview graph. To better understand the role of multiview graph learning in GARLIC, we divide the road network into regions with diameters of 1 km and 2 km (micro-level), 5 km (meso-level), and 10 km and 20 km (macro-level). We then extract 5 graphs with varying traffic features based on these granularities. Under the same 1-second V2V data transmission delay previously mentioned, we sequentially use various combinations of these graphs as inputs to conduct experiments. The results are presented in Figure 4(a).\nThe results indicate that the error in graph learning using a single view is significantly higher than that of multiview graph learning methods. Moreover, when comparing different scales, it is evident that model performance improves as the granularity of the scale decreases.\nTo further explore the relationship between computational latency (including V2V communication and model training time) and multiview graphs, we compared the training time of each model when achieving a scheduling error of 0.7 km, as shown in Figure 4(b). It also verifies that the multiview graph-based learning method could be more efficient than the single-view graph-based learning method.\nIn addition, by analyzing the average error in Figure 4(a) alongside the time cost in Figure 4(b), we selected three multiview graphs with diameters of 2 + 5 + 10 km to model the traffic of road networks efficiently."}, {"title": "The Influence of Driving Behavior", "content": "To verify the influence of driving behavior, we conduct experiments on this method alone via setting different hyperparameter $\\alpha$ defined in Equation (6). The experimental result is shown in Figure 5. When the weight of driving behavior increases, the error of the predicted vehicle trajectory decreases. At this time, the vehicle follows the path given by the offline data and cannot explore the path that can generate higher income in accordance with the vehicle's driving behavior. Conversely, when $\\alpha$ is close to 0, many passenger loading locations seriously deviate from the roads and regions familiar to the driver although the vehicle can receive more orders. These potential issues are more likely to cause traffic accidents. From Figure 5 we can see that the trajectory error significantly drops when $\\alpha$ is between 0.4-0.7. Therefore, in this paper, we set $\\alpha$ = 0.67 to let the dispatch strategy increase the driver's income as much as possible while satisfying each driver's driving behavior."}, {"title": "The efficiency of dynamic rewards", "content": "In this paper, we propose a dynamic reward function to score different vehicles' driving behavior and expected carfare. This reward function is also the core difference of GARLIC from vehicle"}, {"title": "Case Study", "content": "In this part, we randomly choose an online car-hailing car in Hangzhou as our experimental object. We simulated the vehicle dispatching routes using different methods to compare with the ground truth (Origin), as shown in Figure 7.\nWe selected a local area of Hangzhou for visualization. Different shades of red in honeycomb grids indicate the length of time the current vehicle stayed in one week of history. When the area has no color, the vehicle has not been to this area within a week. It represents the driver's familiarity with different regions and personalized driving behavior. The vehicle is currently located in the bright blue grid, and we visualize 4 dark blue areas with ride-hailing demand in the next 15 minutes. We use arrows of different colors to indicate the calculation results of different models.\nAs can be seen from Figure 7, Order 1 is farther from the departure point of the vehicle compared to Orders 2, 3, and 4. Since Order 4 is in the city center, most methods select this area as the vehicle pick-up point. However, there is a direct arterial road between Order 2 and the vehicle's departure location, so some methods choose to dispatch the vehicle to where Order 2 is located. Our method analyzes the driver's driving behavior and finds that the most suitable place to pick up passengers is the area where order 1 is located. Meanwhile, only the results calculated by our method are consistent with the actual vehicle trajectory, which shows the effectiveness of our proposed method."}, {"title": "Related Work", "content": "This section provides a concise overview of related research in vehicle dispatching. Unlike car-hailing order dispatching, vehicle dispatching focuses on relocating vehicles to ensure a future balance between supply and demand. Many previous studies have modeled this as a Markov decision process, which relies on explicitly fitted state transition probabilities (Zhang et al. 2024, 2023; Sun et al. 2024). Various approaches have been proposed to address the dynamic spatiotemporal distribution of passenger demand and its inherent uncertainties. For instance, Robbennolt et al. (Robbennolt and Levin 2023) introduced a novel maximum throughput policy (MT), designed to serve the maximum number of passenger requests while maintaining the stability of the global vehicle network. However, these methods that rely on global traffic state often face efficiency challenges, making it difficult to meet the real-time demands of vehicle dispatching and limiting their deployment in practical scenarios.\nFurthermore, driver behavior plays a crucial role in transportation analysis (Luo, Ge, and Qu 2023; Mohammed et al. 2023; Cui et al. 2024). Recent studies have begun to incorporate driving behavior into driving applications. For example, Li et al. (Li et al. 2022) used IL method to replicate human driving behavior, effectively transferring these strategies to autonomous vehicle scenarios. Jackson et al. (Jackson, Jesus Saenz, and Ivanov 2024) uses the powerful analysis and processing capabilities of LLAMA-7B to characterize driving behavior and then uses it for autonomous driving simulation. However, there is a relative scarcity of research that quantifies vehicle driving behavior to directly evaluate the rationality of vehicle dispatching orders. Consequently, there is an urgent need to design a more efficient and accurate driving behavior-based vehicle dispatching system."}, {"title": "Conclusion", "content": "In this paper, a novel framework called GARLIC is proposed to address the problem of vehicle dispatching while considering the driving behavior of drivers at the same time. Specifically, our framework can be divided into three modules, i.e., the hierarchical traffic state representation module for traffic state extraction, the dynamic reward generation module for driving behavior as well as carfare analysis, and the GPT-augmented dispatching policy learning module for balancing vehicle supply and passenger demand. The model achieves a response in seconds under multiple real datasets and has excellent performance. In the future, we hope to combine the Kafuka engine and cloud-edge collaboration technologies to further optimize the information transmission of each node in vehicle dispatching, achieve a quick response of hundreds of milliseconds, and improve the driver's order acceptance and user's riding experience."}, {"title": "Appendix A: Dataset", "content": "We use two datasets located in Manhattan and Hangzhou, and the road networks of these two cities are shown in Figure 8. The statistics of datasets we used in this paper are listed in Table 2. In the first dataset, we collect trajectories from 350 taxis within 4 hours and all car-hailing orders in this period that indicate the start and end locations. As for the second dataset, we collect trajectories from 9041 taxis within 1 month and also all car-hailing orders in this period. For all datasets, we partition the training, validation, and test sets in a 6:3:1 ratio based on chronological order."}, {"title": "Appendix B: Data Preprocessing", "content": "In this part, we introduce data preprocessing in detail, which mainly contains three parts: map matching, anomalous trajectory detection and filter, and trajectory segmentation.\nFirst, a statistically based anomaly trajectory identification method is applied to improve trajectory data quality. To be specific, We deploy map matching methods (Yu et al. 2022; Wu et al. 2020) to initially map the GPS points to the road segments to solve the problem of GPS drift. Next, we use the shortest path algorithm to calculate the actual trajectory between every two mapped points and add auxiliary GPS anchors at each intersection to ensure that the road segments of the trajectory are continuous.\nNext, we calculate the average speed of a vehicle traveling on a trajectory composed of time series GPS points and then remove the GPS points that cause the average vehicle speed to be 20% higher than the road speed limit. When modeling the driver's driving behavior, we only extract the trajectory data during the idling period of the vehicle for pre-training. When training the dynamic dispatching model, we use the complete trajectory data since the carfare and driving behavior jointly constitute the dynamic reward.\nAs mentioned in the third section, when we train the trajectory data in the form of sequence data, the cumulative error generated during the training process will reduce the model's convergence speed. This effect will increase exponentially with increasing sequence data step size. Taking a vehicle to generate a GPS track point every 30 seconds as an example, the trajectory length per month can reach 86400, which is a huge number. In order to reduce the impact of ultra-long sequence data on the model convergence speed, we adopt the method of position embedding coding and trajectory segmentation training: we define the maximum sequence data length $Leng$ ($Leng$ = 1024 is used in this paper). Then we randomly select multiple trajectory sequences (1000-10000 samples) with any random length less than or equal to $Leng$ from each vehicle trajectory as the model training sample set."}, {"title": "Appendix C: Baseline Details", "content": "The details of baseline models are described as follows:\nMT (Robbennolt and Levin 2023). It uses statistical methods to build an optimization model of vehicle dispatching, which builds on the dynamic queuing model design of Kang and Levin which provides a maximum stability dispatch policy for SAVs.\nFTPEDEL (Wagenmaker and Pacchiano 2023). This method is formulated as an integer linear program that minimizes the total travel time of SAVs while also minimizing a penalty related to vehicle imbalance in the network.\nCQL (Kumar et al. 2020). It is an offline reinforcement learning method that addresses the limitations of overestimating values induced by the distributional shift between the dataset and the policy.\nTD3+BC (Fujimoto and Gu 2021). This method is an efficient offline reinforcement learning method. It only adds a regular term of behavior cloning (BC) to the value function and normalizes the state to achieve the same accuracy as other complex offline reinforcement learning methods.\nDecistion Transformer (DT) (Chen et al. 2021)). This method is the pioneering work of RL Transformer. It regards the RL problem as a sequence modeling problem, abandons the calculation modes such as Policy Gradient in the traditional RL method, and improves the calculation efficiency.\nRLPD (Ball et al. 2023). This method builds upon standard off-policy RL algorithm and incorporates symmetric sampling, layer normalization, sample-efficient learning, and large ensembles.\nLatent offline RL (Hong, Levine, and Dragan 2024). This method utilizes Conservative Q-Learning (CQL) with a low-dimensional representation learning to obtain human behavior, allowing the agent to adapt its strategy as the human behavior changes over time.\nSS-DT (Zheng et al. 2023). It introduces a semisupervised offline reinforcement learning approach that leverages both labeled and unlabeled data to train reinforcement learning agents, and we use DT as its base RL model."}, {"title": "Appendix D: Implementation Detail", "content": "We have three parts in the proposed framework, such as the multiview graph representation, dynamic reward modeling,"}, {"title": "Appendix E: Training & Testing Process", "content": "In this subsection, we introduce the training process of the GARLIC. As shown in Algorithm 1, we deploy a two-stage sequential Learning framework since the GPT-based model usually has the limitation of slow convergence. The training process can be divided into two parts and every agent shares the same model parameters: traffic state representation & dynamic reward learning, and a GPT-augmented model for policy learning.\nThe first part extracts sequences of states, actions, and rewards for each vehicle agent, shown in lines 1-10. The second part is used to learn the relationship between the action and the environment state, as well as the reward. It aims to predict the expected action under the environmental state at the current moment, shown in lines 11-19."}, {"title": "Appendix F: Additional Ablation Study", "content": "Dispatching Performance. As mentioned in the previous study (Fujimoto and Gu 2021), the advantage of the offline reinforcement learning method is that its training time could be much shorter than that of the online reinforcement learning method, and it can make full use of valuable features in the training data. To observe this phenomenon more intuitively, we visualize all methods' training errors with training epoch increases, as shown in Figure 9. We can see that online RL-based methods, such as MT, have the"}, {"title": "Appendix G: Visualization", "content": "As an extension of the case study, we visualize the supply and demand of the multi-view road network before/after our algorithm is deployed in this section, as shown in Figure 11. In"}]}