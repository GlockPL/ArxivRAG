{"title": "Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models", "authors": ["Xiaochen Zhu", "Georgi Karadzhov", "Chenxi Whitehouse", "Andreas Vlachos"], "abstract": "Diffusion models have shown promise in text generation but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion overlooks word-order dependencies and enforces short output windows, while passage-level diffusion struggles with learning robust representation for long-form text. To address these challenges, we propose Segment-Level Diffusion (SLD), a framework that enhances diffusion-based text generation through text segmentation, robust representa-tion training with adversarial and contrastive learning, and improved latent-space guidance. By segmenting long-form outputs into separate latent representations and decoding them with an autoregressive decoder, SLD simplifies diffusion predictions and improves scalability. Experiments on XSum, ROCStories, DialogSum, and DeliData demonstrate that SLD achieves competitive or superior performance in fluency, coherence, and contextual compatibility across automatic and human evaluation metrics comparing with other diffusion and autoregressive baselines. Ablation studies further validate the effectiveness of our segmentation and representation learning strategies.", "sections": [{"title": "1 Introduction", "content": "Transformer-based autoregressive (AR) language models have become the prevailing standard in natural language generation (Vaswani et al., 2017; Zhao et al., 2023). However, the nature of next-token prediction inherently makes them prone to error propagation and incorrect handling of long-term dependency, while also complicating controllable generation (He et al., 2021; Wu et al., 2018). Diffusion models, which are non-autoregressive (NAR) generative models widely successful in image and video generation, have also shown promise in text generation (Ho et al., 2020; Radford et al., 2021; Singer et al., 2023). Li et al. (2022) pioneered the application of diffusion models to discrete text generation by predicting continuous word embeddings. Building on this work, Lin et al. (2023) introduced GENIE, a pre-trained diffusion language model that enhances semantic understanding through continuous paragraph-level de-noising. These approaches fall into token-level diffusion, as they directly generate word embeddings. In contrast, Lovelace et al. (2023) proposed latent diffusion for text generation (LD4LG), encoding text into latent representations, applying diffusion to high-level semantic structures, and decoding them into text using an AR decoder. Chen and Yang (2023) leveraged diffusion models for controllable dialogue generation, operating on high-level discourse representations to enable precise control over semantically rich dialogue structures, such as dialogue actions. However, existing diffusion language models face challenges in generating long, fluent, and coherent text. Token-level diffusion requires pre-training with larger output windows to handle longer texts, which increases computational cost, or relies on time-consuming iterative diffusion sampling. Additionally, it ignores word-order dependencies, often resulting in ungrammatical or incoherent output. Generating longer latent representations for passages with multiple sentences is also more complex, as learning a smooth latent distribution is challenging (Vahdat et al., 2021; Zhang et al., 2023), which can lead to abrupt meaning changes in the decoded text, as predicted latent representations are highly sensitive to small noise. To address these limitations, we propose a novel approach for diffusion-based text generation, Segment-Level Diffusion (SLD), illustrated in Figure 1. Inspired by the concept of image patches (Ding et al., 2023), we use a diffusion model to perform high-level semantics and structural plan-ning, generating a latent representation for each segment (e.g., sentences in paragraphs, utterances in dialogues), instead of handling long texts with a single latent representation. Then, an AR decoder decodes predicted representations to texts. To enhance the quality of the generated text, we integrate adversarial training (Miyato et al., 2017) and contrastive learning (Gao et al., 2021) to smooth the latent representation distribution and optimize the AR decoder with respect to the diffusion process. Additionally, we incorporate extra loss signals from both decoding and latent space reconstruction to further strengthen control, improving coherence and fidelity in text generation. We compare our SLD model against three diffusion models, GENIE (Lin et al., 2023), LD4LG (Lovelace et al., 2023), Diffuse-CG (Chen and Yang, 2023), and an autoregressive baseline, Flan-T5 (Chung et al., 2024), across multiple tasks. The evaluation includes summarization (XSum, Narayan et al. 2018), title-to-story generation (ROCStories, Mostafazadeh et al. 2016), summary-to-dialogue generation (DialogSum, Chen et al. 2021), and multiparty decision-making dialogue generation (DeliData, Karadzhov et al. 2023). Evaluation by both automatic and human metrics indicates that SLD generates text that is more coherent and fluent, better aligns with the provided input, and produces outputs that more closely match ground-truth distributions."}, {"title": "2 Related Work", "content": "Token-Level Diffusion Li et al. (2022) adapted diffusion models for discrete text generation by operating in the continuous space of learned word embeddings. The architecture iteratively de-noises sampled Gaussian noise into a sequence of word vectors. A rounding method is then applied to map predicted embeddings into the nearest learned embeddings. Extending this work, Gong et al. (2023) applied token-level diffusion to sequence-to-sequence generation tasks. Lin et al. (2023) further advanced this approach by incorporating pre-training, which enhanced semantic and syntactic coherence by training diffusion decoders to reconstruct clean text from corrupted embeddings in a paragraph. These models achieve sequence-to-sequence generation using encoded text as classifier-free guidance (Ho and Salimans, 2022). However, token-level diffusion has notable limitations: the NAR generation process ignores word order dependencies, often resulting in text that lacks grammatical correctness and fluency. Furthermore, the fixed output shape constrains the generation of longer texts, making it computationally expensive and inefficient for shorter outputs as unused capacity is wasted. Passage-Level Diffusion Lovelace et al. (2023) built on the concept of latent space diffusion (Rombach et al., 2022) by compressing and predicting texts using high-level semantic representations for output texts in the latent space, rather than directly predicting fine-grained token representations. Such compression is beneficial for both performance and efficiency, as it provides a length-independent representation and removes information not needed for diffusion prediction, in contrast to representations from traditional language encoders. A separate AR decoder is employed to ensure the fluency of the generated text. However, this approach primarily focuses on short text generation, as learning robust latent representations for long passages remains challenging, and it is crucial to ensure the smoothness of the learned distribution for high-quality generation (Vahdat et al., 2021). Without proper regularization, the learned distribution may be susceptible to abrupt semantic changes even from small perturbations, increasing the difficulty of the prediction of the diffusion model. Although Zhang et al. (2023) proposed techniques to improve the distributional"}, {"title": "3 Segment-Level Diffusion", "content": "To address the challenges faced by diffusion language models in controllable long-form generation, we propose a novel approach, Segment-Level Diffusion (SLD), which consists of three key improvements: (1) text generation in segments, (2) robust latent representation and decoder learning, and (3) strengthened guidance in high-dimensional latent spaces. In this section, we provide an overview of the language generation process using diffusion models in latent space, as depicted in Figure 1, followed by of our improvements, with an detailed overview of SLD in Figure 2.", "subsections": [{"title": "3.1 Formulation", "content": "Given an input text sequence i = {i\u00b9, i2, ..., in} consisting of n tokens and an output sequence o = {01, 02, . . ., om} consisting of m tokens, we model the conditional probability p(oli) using a learnable diffusion model R(;0R). We follow Lovelace et al. (2024) by introducing additional encoding and decoding components to convert texts into continuous latent representations z\u2208 Rk\u00d7hrep. The diffusion model operates on continuous latent variables z across T time steps, modelled as a Markov chain (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020), which consists of two processes: a backward process for inference and a forward process for training. Inference The backward process generates the latent representation of the estimated output text \u00f4 by iteratively removing noise from an initial noisy sample. Starting with a variable \u017c\u2020 ~ N(zT; 0, I), the diffusion model with parameters OR predicts the de-noised variable 2t-1 at each time step t as follows:\np(2t-1|2t; 0R) = N(2t-1; \u03bc\u03b8Rt-1, \u03c3\u03b8R2)\nwhere \u03bc\u03b8r and \u03c3\u03b8\u20a8 are predicted mean and variance. For simplicity, we denote the sampled estimated representation as 2t\u22121 using a function R(; 0R), typically a transformer (Peebles and Xie, 2023), conditions on the input sequence i with a separate frozen language encoder Encctx(;0ctx) using classifier-free guidance as below.\n2t-1 = R(2t, t, Encctx (i; 0ctx); 0R)\nThe model keeps refining the noisy sample \u017cy with respect to the input sequence i to recover 20 which will be converted to text. The predicted latent representation 20 is passed to a function parameterised by @g, which reconstructs it to match the input dimensions of an AR decoder with parameters Odec for decoding:\ng(20;09) \u2208 Rm\u00d7him, \u00f4 = Dec(g(20; 0g); 0dec).\nTraining The diffusion model R(;0R) is trained by minimizing a regression loss to predict the noise added during the forward process. In the forward process, an original representation zo of encoded o from a sampled observation (i, o) ~ D is gradually corrupted into Gaussian noise over T time steps. The encoding process consists of an encoder with parameters benc that encodes the output texts:\nEnc(0; \u03b8enc) \u2208 Rm\u00d7him\nand a compression function with parameters of that projects encoder outputs into a length-independent latent space using:\nz = f(Enc(0; \u03b8enc);0f) \u2208 Rkxhrep\nHere, we reduce the dimension of encoder outputs to a fixed-length representation with k < m and hrep \u226a him. The corruption is modelled as:\nq(Zt|Zt-1) = N(zt; \u221a 1 \u2013 \u1e9etzt\u22121, \u03b2\u0399)\nwhere Bt controls the variance of the added noise at each step. The objective is to minimize the distance between the predicted representation 2\u0165 and the true posterior zt which is computed in closed form by sampling from the forward process.\nL(OR) = \u2211 E || 2t - Zt ||2\nt=1q(Zt/Zo)"}, {"title": "3.2 Segmented Text Generation", "content": "Inspired by the concept of image patches (Ding et al., 2023), our approach divides long outputs into smaller segments, such as sentences or dialogue utterances, rather than projecting the entire output into a single latent space representation. This segmentation effectively reduces the size and complexity of each latent representation, simplifying diffusion predictions and enabling greater flexibility for scaling, allowing the model to handle long-form text more efficiently. Formally, we construct P = {p\u00b9, ..., p'}, where each p\u00b9 corresponds to a non-overlapping contiguous segment of tokens in o. This process yields a set of latent representations Z = {z\u00b9, ., z\u00ed}, establishing a one-to-one correspondence between each utterance (patch) and its respective latent representation."}, {"title": "3.3 Learning Latent Representations for Robust Decoding", "content": "As mentioned earlier, performing diffusion in latent space for text generation requires learnable components for encoding, compression, reconstruction, and decoding. To train these components, a straightforward approach is to use the loss incurred during decoding \u00f4p = Dec(g(z; 0g); 0dec), where z = Enc(f(p; 0f); \u03b8enc), for a patch of text p = {01,..., Op}. We denote the parameters collectively as din = {0enc, 0f} for the encoding and compression, lout = {09, 0dec} for reconstruction and decoding, and Orep = 0in Ubout for latent representation parameters. The parameters are trained using the standard cross-entropy loss as below.\np\nLcnv(Orep) = \u2211log p(\u03bf\u03b9|\u03bf1=1\nHowever, learning a robust latent representation is non-trivial. Zhang et al. (2023) highlighted that a good latent representation should ensure low latent-to-text conversion error and smoothness in the latent distribution. Small perturbations to the latent representation should not significantly affect decoding, while textual segments with similar meanings should be distributed relatively close together in the latent space. To achieve this, we augment the cross-entropy loss with contrastive learning and adversarial training as regularization techniques. Contrastive Representation Learning In our framework, we operate in latent spaces for patches of text p, which are relatively short compared to the paragraphs in Zhang et al. (2023). This allows us to easily obtain meaningful positive examples (e.g.,"}, {"title": "3.4 Diffusion for Semantic Planning", "content": "We use the diffusion model as a segment-level semantic planner that leverages learned segment representations to plan and generate meaningful passages consisting of a sequence of segments. We truncate or pad patched texts to n segments using a special empty segment embedding. Given a context i and collated output texts segments P, we derive the corrupted latent representation of patches Zt \u2208 Rn\u00d7k\u00d7hrep at time t. Positional encoding is applied to the flattened representation with respect to n \u00d7 k. A transformer-based model is used for de-noising, defined as:\nZt-1 = R(Zt, t, Encctx(i; ctx); 0R),\nwhere Encctx(; 0ctx) is an independent pre-trained language encoder with parameters typically frozen, serving as the cross-attention target for the transformer de-noiser, enabling conditioning in classifier-free guidance. We define the input-output de-noising loss as:\nLnoise (OR) = E ||R(Zt, t, Encctx(i; ctx)) \u2013 Zt-1||2.\n P,i,t\nTo strengthen the guidance and ensure fluency in the final decoded text, we add post-diffusion training strategy, which incorporates loss signals from the reconstruction and decoding processes. This strategy effectively teaches the diffusion model how to use g(; 0g) and Dec(; 0dec), further enhancing the quality of the generated text. Similar to Zhang et al. (2024)'s pixel level guidance, we freeze the reconstruction and decoding parameters and define the additional objectives for \u03b8\u20a8 as follows :\nLrec(OR) = E ||9(2;09) - g(Zt;09)||2, \n P,i,t\np\nLdec(OR) = 1P||E log P(01/02\nCombining these losses with hyperparameters A3 and 4 as weighting factors, we define the final loss function as below.\nLdiff(OR) = Lnoise(OR)+13Lrec(OR)+14Ldec(OR)\nDetailed training pipeline of the entire model is outlined in Appendix as Algorithm 1."}]}, {"title": "4 Evaluation", "content": null, "subsections": [{"title": "4.1 Datasets and Baselines", "content": "We evaluate our implementation on datasets with an increasing number of utterances to assess its performance across various tasks. We start with the XSum dataset (Narayan et al., 2018), consisting of BBC news articles paired with concise, one-sentence summaries, to compare our model against baseline short-form diffusion models. We then scale up to longer outputs using the ROCStories dataset (Mostafazadeh et al., 2016) for title-to-story generation, and the DialogSum dataset (Chen et al., 2021) for summary-to-dialogue generation. These datasets allow us to evaluate the model's capability for long-form generation. Additionally, we test our model on dialogue continuations simulated with DeliData (Karadzhov et al., 2023). DeliData consists of multi-party problem-solving dialogues where participants propose solutions which they revise and are scored for their correctness. By comparing the predicted user score trajectories against the ground truth and identifying hallucinations, we analyse the effectiveness of applying control to the model's generation. We compare our model against a range of baselines. Specifically, we use LD4LG (Lovelace et al., 2023) as the diffusion baseline and Flan-T5 Large (Chung et al., 2024) as the autoregressive baseline. For the XSum dataset, we also compare against the token-level diffusion model GENIE (Lin et al., 2023). For the DialogSum dataset, we include comparisons with the dialogue-level diffusion model Diffuse-CG (Chen and Yang, 2023)."}, {"title": "4.2 Evaluation Metrics", "content": "We use ROUGE as the primary evaluation metric to assess the quality and similarity of generated text with respect to the gold output. While ROUGE provides a baseline for lexical overlap, we acknowledge its limitations in capturing semantic fidelity, coherence, and conversational nuances, particularly in controlled long-form generation. To address this, we extend human evaluation guidelines from Clark et al. (2023), assessing repetition, fluency, coherence, compatibility (ROCStories/DialogSum), and hallucination (DeliData):\n\u2022 Repetition: Check for repetitive tokens or utterances that affect meaning.\n\u2022 Fluency/Grammar: Assess grammatical correctness and fluency.\n\u2022 Coherence: Evaluate logical flow and naturalness of interactions.\n\u2022 Compatibility: Ensure alignment with the story title/dialogue summary.\n\u2022 Hallucination: Detect impossible choices or non-existent participants.\nHuman scores range from 0 to 3, with higher scores indicating better performance. Details and examples are in Appendix C. Following the evaluation metrics in the literature, we also evaluate the perplexity of generated text using GPT-2 Large (Radford et al., 2019) as teacher model, and record the average length of generated texts."}, {"title": "4.3 Implementation Details", "content": "We build upon the backbone design of Latent Diffusion for Language Generation (LD4LG) proposed by Lovelace et al. (2023), using Flan-T5 Base (Chung et al., 2024) to initialize our encoder and decoder. We incorporate the Perceiver Resampler (Alayrac et al., 2022) as the compression and re-construction unit and employ a pre-LayerNorm transformer as the de-noising model (Vaswani et al., 2017). For contrastive learning targets, we use Llama-3-8B-Instruct (Llama Team, 2024) to generate paraphrases for each text segment. For XSum, we sampled out-of-domain (OOD) texts from Movie-Dic (Banchs, 2012) dataset as hard negative targets. For other datasets, we sample from CNN/Daily Mail (See et al., 2017)."}]}, {"title": "5 Results", "content": "We present the results of our model against the baselines on XSum, ROCStories, DialogSum and DeliData in Table 1. For shorter texts (<50 tokens), in XSum and ROCStories, our model demonstrates on-par performance compared with other baselines. For longer generations, in DialogSum and Deli-Data, our model shows better overall performance, especially in repetition, fluency and compatibility. XSum: SLD achieves competitive performance compared to other methods. While Flan-T5 achieves the highest ROUGE-1, ROUGE-2, and ROUGE-L scores, SLD provides a comparable ROUGE-L score of 27.77 while maintaining a length close to the gold reference. Importantly, the results indicate the importance of an AR decoder. For Flan-T5, LD4LG and our SLD that models p(01/0, i), they have a substantially higher fluency score than GENIE which uses diffusion models to decode token level embeddings directly, mod-"}, {"title": "6 Analysis", "content": "We further analyse the impact of our representation learning methods and text segmentation through additional ablation studies. Our approach demonstrates faster convergence in latent representation learning, compared to other models. Moreover, our method improves the decoding process during the de-noising stage of diffusion predictions.", "subsections": [{"title": "Representation to Text", "content": "To assess representation learning, we tested various autoencoder-decoder configurations using BLEU (Papineni et al., 2002) to compare input text with recovered text after encoding. Using DialogSum utterances as segments, we evaluated LD4LG, ML-Planner, and SLD with and without contrastive learning. For consistency, segments were limited to 64 tokens, represented in a latent space of 32 \u00d7 64. An LD4LG baseline with longer dialogues (up to 512 tokens) and latent dimensions of 256 \u00d7 64 was also included. Figure 3 shows that ML-Planner failed to converge within five epochs, while LD4LG achieved a BLEU score of 1.00, indicating perfect recovery. However, LD4LG's performance degraded when scaled to longer texts, highlighting limitations in generalization. Without contrastive learning, our model occasionally corrupted words, altering meaning. Incorporating contrastive learning enabled meaningful paraphrases instead of semantic corruption, as demonstrated in Table 2, emphasizing its role in enhancing representation quality."}, {"title": "Decoding after De-noising", "content": "We further investigated how latent representations behave under perturbations to evaluate their robustness during the de-noising process. We randomly selected 100 sentences from the ROC dataset, along with their paraphrases and OOD sentences sampled from the CNN/Daily Mail dataset. We visualized a 2D PCA projection of learned representations, detailed in Appendix B. Without contrastive learning, the representations of original sentences and OOD sentences showed significant overlap, increasing the risk of abrupt semantic changes during decoding. In contrast, representations learned with contrastive training were better clustered and distinct, providing improved robustness for diffusion predictions. To further validate, we sampled the de-noise trajectory of a test sentence and analysed the decoded text along the trajectory. Representations trained with both contrastive learning and adversarial training produced text that was more robust to noise and less prone to abrupt semantic shifts. This robustness facilitates smoother and more reliable predictions during the diffusion process."}]}, {"title": "7 Conclusion", "content": "We propose Segment-Level Diffusion (SLD) for controllable long-form text generation using latent space diffusion. Key innovations include text segmentation, robust representation learning with adversarial and contrastive training, and improved latent-space guidance. SLD addresses challenges in generating fluent, coherent, and contextually accurate long-form text, bridging the gap between latent diffusion models and practical demands of long-form generation, offering a scalable framework for applications such as story and dialogue generation. Our results validate the potential of SLD as a new paradigm for controllable text generation, providing insights for future research in diffusion-based generative models for language."}, {"title": "Limitations", "content": "This work focuses exclusively on text generation in English, leaving the model's potential for multilingual tasks unexplored. Furthermore, our experiments and evaluations did not involve real-world use cases, limiting insights into practical applicability. Future research could extend our approach to multilingual and application-oriented scenarios such as outline-controlled generation (Li et al., 2024; Lee et al., 2024). Additionally, we did not explicitly examine the relationship between the reduced dimensionality of the length-independent latent representations and the original dimensionality of encoded text segments of varying lengths. These hyperparameters were chosen empirically, without a systematic exploration of their impact. Future work could leverage principles from information theory (Tishby and Zaslavsky, 2015) to quantify the information capacity of these representations and to balance compression and utility more effectively. Developing a generalized framework to streamline hyperparameter selection across diverse datasets and pre-training tasks would also enhance the scalability of our method. Finally, our modular training approach, where individual components are optimized separately, may introduce suboptimal performance during inference due to error propagation and misalignment between training and inference objectives. Future work could explore end-to-end training strategies to jointly optimize all components, reducing such discrepancies and improving overall performance."}, {"title": "Ethics Statement", "content": "No personally identifiable information (PII) was collected or processed during the human evaluation, and all data handling adhered to the General Data Protection Regulation (GDPR) and the University's research guidelines\u00b9. Participants were recruited from within the University on a voluntary basis and were assigned anonymized random IDs to ensure their privacy during the evaluation process. Additionally, all data points presented to annotators were manually reviewed to ensure fairness and accuracy in assessing our methods and to minimize potential bias or harm to participants. This approach reflects our commitment to ethical research practices and to safeguarding the well-being and integrity of participants throughout the"}, {"title": "Appendix", "content": null, "subsections": [{"title": "A Training", "content": "All experiments were conducted on a single NVIDIA A100-SXM-80GB GPU. Training for latent representation learning took approximately 8-10 hours per dataset, while diffusion training required 36-60 hours per dataset. Including inference and other experiments, the total GPU usage amounted to around 500 hours. The detailed training algorithm is presented in Algorithm 1."}, {"title": "B Ablation Study: Latent Representation", "content": "We conducted an ablation study to evaluate how different representation learning methods affect the noised latent representations during the diffusion process. The \"Vanilla\" configuration corresponds to the original LD4LG implementation. As shown in Figure 6 and Table 5, our method achieves a smoother latent distribution and more robust representations, resulting in improved diffusion predictions and decoding."}, {"title": "C Human Evaluation", "content": "For ROCStories, DialogSum, and DeliData, we recruited 5 participants to evaluate 25 dialogues generated by LD4LG, Flan-T5, and SLD for each dataset. Annotators also rated the gold dialogue output as a reference. In total, we collected 3 \u00d7 25 \u00d7 4 \u00d7 5 = 1,500 data points across 4 evaluation criteria. Before presenting the dialogues, annotators were provided with instructions (example shown in Table 3)."}, {"title": "D Case Study: DeliData", "content": "To evaluate the controlled generation capabilities of our model, we performed a fine-grained score trajectory analysis on DeliData. This involved comparing the distribution of performance gains at the end of group discussion dialogues, based on users' choices before and after revisions. Following the guidelines of Karadzhov et al. (2023), we observed that dialogue continuations generated by our model produced a performance gain distribution closer to the ground truth, as shown in Figure 4."}, {"title": "E Sampled Generations", "content": "We provide sample output comparisons in Tables 6, 7, and 8."}]}]}