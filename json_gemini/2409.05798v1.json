{"title": "Enhancing Preference-based Linear Bandits via Human Response Time", "authors": ["Shen Li", "Yuyang Zhang", "Zhaolin Ren", "Claire Liang", "Na Li", "Julie A. Shah"], "abstract": "Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences (\u201ceasy\u201d queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.", "sections": [{"title": "1 Introduction", "content": "Interactive preference learning from human binary choices is crucial in many systems and applications, including recommendation systems [10, 20, 31, 57], assistive robots [55, 66], assortment optimization [1, 54], and fine-tuning large language models [6, 43, 46, 47, 60]. This form of human feedback is widely adopted for its simplicity and minimal cognitive load on users [36, 73, 75]. Preference learning from binary choices is typically modeled as a preference-based bandit problem [8, 30], where a system queries users with pairs of options and refines its understanding of their preferences. However, while choices help estimate preferences, they provide limited information on preference strength [78]. To address this limitation, researchers have incorporated additional explicit feedback mechanisms, such as ratings [50, 59], labels [75], and slider bars [6, 73], but these approaches often increase interface complexity and the cognitive burden on users [35, 36].\nIn this paper, we propose leveraging an implicit form of human feedback\u2014response time\u2014to provide additional information about preference strength. Unlike explicit feedback, response time is typically effortless and unobtrusive to measure [16] and provides valuable complementary information beyond what choices alone can offer [3, 15]. For instance, consider an online retailer that repeatedly presents users with two options: purchase or skip a recommended product [34]. Most users skip products most of the time [32], resulting in a probability of skipping close to 1 across most products. As a result, the choices themselves don't convey how strongly a product is liked or disliked, making it difficult for the retailer to identify the user's preferences. In such cases, response time provides a useful solution. Psychological research has shown an inverse relationship between response time"}, {"title": "2 Problem setting and preliminaries", "content": "We consider preference-based bandits with a linear utility function [30], extending both dueling bandits [79] and logistic bandits [4]. The learner is given a finite set of options (referred to as \u201carms\u201d), each represented by a feature vector in $\\mathcal{Z} \\subset \\mathbb{R}^d$, and a finite set of binary comparison queries, where the features are the differences between pairs of arms, denoted by $\\mathcal{X} \\subset \\mathbb{R}^d$. For instance, if the learner can query any pair of arms, the query set is $\\mathcal{X} = \\{z - z' : z, z' \\in \\mathcal{Z}\\}$. In the online retailer scenario from section 1, the query set becomes $\\mathcal{X} = \\{z - z_{\\text{skip}} : z \\in \\mathcal{Z}\\}$, where $z$ represents the option to purchase a product and $z_{\\text{skip}}$ represents the option to skip (often set to 0). For each arm $z \\in \\mathcal{Z}$, the human utility is defined as $u_z := z^T\\theta^*$, where $\\theta^* \\in \\mathbb{R}^d$ represents the parameters of human preference. For any query $x \\in \\mathcal{X}$, the utility difference is then defined as $u_x := x^T\\theta^*$.\nGiven a query $x := z_1 \u2013 z_2 \\in \\mathcal{X}$, we model human choices and response times using the difference-based EZ-Diffusion Model (dEZDM) [9, 68], integrated with a linear utility structure. (For a detailed comparison with other models, see appendix B.1.) This model views human choice-making as"}, {"title": "3 Utility estimation", "content": "In this section, we address the problem of estimating human preference $\\theta^*$ from a fixed dataset, denoted by $\\{x, \\mathcal{X}, C_{x,s_{x,i}}, t_{x,s_{x,i}} \\}_{x \\in \\mathcal{X}_{\\text{sample}}, i \\in [n_x]}$. Here $\\mathcal{X}_{\\text{sample}}$ denotes the set of queries contained in the dataset, $n_x$ denotes the number of samples for the query $x \\in \\mathcal{X}_{\\text{sample}}$, and $s_{x,i}$ denotes the episode where we sample $x$ for the $i$-th time. Samples from each query $x$ are i.i.d., while samples from different queries are independent. Section 3.1 introduces a new estimator, called the \u201cchoice-decision-time estimator,\u201d which uses both choices and response times, alongside the widely used \"choice-only estimator\" that relies solely on choices [4, 30]. Sections 3.2 and 3.3 provide a theoretical comparison of the two estimators, examining both their asymptotic and non-asymptotic performance, respectively, and highlighting the benefits of incorporating response times. Later, section 5.1 provides empirical comparisons, confirming our theoretical analysis."}, {"title": "3.1 Choice-decision-time estimator vs. choice-only estimator", "content": "The choice-decision-time estimator is based on the following relationship between human utilities, choices, and response times, derived from eqs. (1) and (2):\n$\\forall x \\in \\mathcal{X} : x^T\\theta^* \\approx \\alpha \\frac{\\mathbb{E} [C_x]}{\\mathbb{E} [t_x]}$.\nIntuitively, if a human facing query $x$ provides consistent choices (i.e., large $|\\mathbb{E}[C_x]|$) and makes decisions quickly (i.e., small $\\mathbb{E}[t_x]$), the query is easy, indicating a strong preference (i.e., large $|x^T\\theta^*|$). This identity reframes the estimation of $\\theta^*$ as a linear regression problem. Accordingly, the choice-decision-time estimator computes the empirical means for both choices and response times, aggregates the ratios across all sampled queries, and applies ordinary least squares (OLS) to estimate $\\theta^*/\\alpha$. Since the ranking of arm utilities based on $\\theta^*/\\alpha$ matches that based on $\\theta^*$, estimating $\\theta^*/\\alpha$ is sufficient for identifying the best arm. This estimate, denoted by $\\hat{\\theta}_{CH.DT}$, is formally calculated as:\n$\\hat{\\theta}_{CH.DT} := \\Big(\\sum_{x \\in \\mathcal{X}_{\\text{sample}}} n_x x x^T\\Big)^{-1} \\sum_{x \\in \\mathcal{X}_{\\text{sample}}} n_x x \\frac{\\frac{1}{n_x} \\sum_{i=1}^{n_x} C_{x,s_{x,i}}}{\\frac{1}{n_x} \\sum_{i=1}^{n_x} t_{x,s_{x,i}}}.$\nIn contrast, the choice-only estimator is based on eq. (1), which shows that for each query $x \\in \\mathcal{X}$, the random variable $(C_x + 1)/2$ follows a Bernoulli distribution with mean $1/[1 + \\exp(-2\\alpha x^T\\theta^*)]$. Similar to the choice-decision-time estimator, the parameter $2\\alpha$ does not affect the ranking of the arms, so identifying $2\\alpha\\theta^*$ is sufficient for best-arm identification. Estimating $2\\alpha\\theta^*$ becomes a logistic regression problem [4, 30], which can be solved using the following MLE:\n$\\hat{\\theta}_{CH} := \\arg \\max_{\\theta \\in \\mathbb{R}^d} \\sum_{x \\in \\mathcal{X}_{\\text{sample}}} \\sum_{i=1}^{n_x} \\log \\mu(C_{x,s_{x,i}} x^T\\theta),$\nwhere $\\mu(y) := 1/[1 + \\exp(-y)]$ is the standard logistic function. This MLE lacks a closed-form solution but can be efficiently solved in practice using methods such as Newton's algorithm [23, 44]."}, {"title": "3.2 Asymptotic normality of the two estimators", "content": "The choice-decision-time estimator from eq. (4) satisfies the following asymptotic normality result, as proven in appendix C.2:\nTheorem 3.1 (Asymptotic normality of $\\hat{\\theta}_{CH,DT}$). Suppose the learner has an i.i.d. dataset $\\{x, C_{x,s_{x,i}}, t_{x,s_{x,i}} \\}_{i \\in [n_x]}$ for every $x \\in \\mathcal{X}_{\\text{sample}}$, where $\\sum_{x \\in \\mathcal{X}_{\\text{sample}}} n_x > 0$, and the datasets for different $x \\in \\mathcal{X}_{\\text{sample}}$ are independent. Then, for any $y \\in \\mathbb{R}^d$, as $n \\to \\infty$, the following holds:\n$\\sqrt{n} y^T (\\hat{\\theta}_{CH.DT,n} - \\theta^*/\\alpha) \\to \\mathcal{N}(0, \\zeta^2/\\alpha^2)$.\nHere, the asymptotic variance $\\zeta^2$ is a problem-specific constant that is upper bounded as follows:\n$\\zeta^2 \\leq ||y||^2 (\\sum_{x \\in \\mathcal{X}_{\\text{sample}}} M_{CH,DT}^{asym} x x^T)^{-1}$,\nwhere $M_{CHDT}^{asym} := \\min_{x \\in \\mathcal{X}_{\\text{sample}}} m_{CHDT}^{asym}(x^T\\theta^*)$ and $m_{CHDT}^{asym}(x^T\\theta^*) := \\frac{1}{\\frac{V [C_x]}{\\alpha^2} + \\frac{\\mathbb{E}[C_x]^2}{\\mathbb{E}[t_x]^4} V [t_x]}$.\nThe constants $\\mathbb{E} [t_x]$, $\\mathbb{E} [C_x]$, $V [C_x]$, and $V [t_x]$ are detailed in appendix C.1 and depend only on the utility difference $x^T\\theta^*$ for any fixed barrier $\\alpha$. In this asymptotic variance upper bound, each sampled query is weighted by the common factor $\\min_x m_{CHDT}^{asym}(x^T\\theta^*)$. Intuitively, the true human utilities are not observed directly but are inferred through the signals provided by choices and response times. This weight represents how much information from each query $x$ is preserved in these signals. Larger weights indicate more retained information, resulting in lower variance and more accurate estimation.\nIn contrast, the choice-only estimator from eq. (5) has the following asymptotic normality result as described in Gourieroux and Monfort [28, proposition 4] and McFadden [42, theorem 3]:\nTheorem 3.2 (Asymptotic normality of $\\hat{\\theta}_{ch}$). Suppose the learner has an i.i.d. dataset $\\{x, C_{x,s_{x,i}}, t_{x,s_{x,i}} \\}_{i \\in [n_x]}$ for every $x \\in \\mathcal{X}_{\\text{sample}}$, where $\\sum_{x \\in \\mathcal{X}_{\\text{sample}}} n_x > 0$, and the datasets for different $x \\in \\mathcal{X}_{\\text{sample}}$ are independent. Then, for any $y \\in \\mathbb{R}^d$, as $n \\to \\infty$, the following holds:\n$\\sqrt{n} y^T (\\hat{\\theta}_{CH,n} \u2013 2\\alpha\\theta^*) \\to \\mathcal{N} (0,4\\alpha^2\\sigma_{CH}^{asym,2})$,\nwhere $\\sigma_{CH}^{asym,2} := y^T (\\sum_{x \\in \\mathcal{X}_{\\text{sample}}} m_{CH}^{asym} (x^T\\theta^*) x x^T)^{-1} y$ and $m_{CH}^{asym} (x^T\\theta^*) := 4\\alpha^2 \\mu'(2\\alpha x^T \\theta^*)$.\nHere, $\\mu(\\cdot)$ denotes the first-order derivative of the function $\\mu(\\cdot)$ defined in eq. (5). In this asymptotic variance, each sampled query $x$ is weighted by its own factor $m_{CH}^{asym} (x^T\\theta^*)$, which represents the amount of information from each query $x$ that is preserved in the choice signals.\nThe weights in theorems 3.1 and 3.2 highlight how choice and response time signals retain different amounts of information about human preferences. The choice-decision-time estimator applies a single \u201cmin-weight\u201d across all queries, denoted by $\\min_x m_{CHDT}^{asym}(x^T\\theta^*)$, with $m_{CHDT}^{asym}(\\cdot)$ illustrated by the orange curves in fig. 2a. In contrast, the choice-only estimator assigns each query $x$ its own weight, $m_{CH}^{asym} (x^T\\theta^*)$, shown as the gray curves in fig. 2a. As illustrated, first, as the query becomes harder\u2014when the utility difference $x^T\\theta^*$ approaches 0\u2014both $m_{CHDT}^{asym}(x^T\\theta^*)$ and $m_{CH}^{asym} (x^T\\theta^*)$ increase, indicating that harder queries carry more information than easier ones. Second, as the barrier $\\alpha$ increases from 0.5 to 1.5, the orange curve, representing $m_{CHDT}^{asym}(\\cdot)$, rises across all queries, regardless of utility difference. This leads to a lower asymptotic variance upper bound and improved estimation. Intuitively, a larger barrier suggests more conservative human decision-making, resulting in longer response times and more consistent choices, which provide stronger signals for estimation. By contrast, as $\\alpha$ increases, the gray curve, representing $m_{CH}^{asym} (\\cdot)$, rises for hard queries but falls for easy ones. This indicates that the choice-only estimator gathers more information from hard queries but less from easy queries as the human becomes more conservative.\nFinally, comparing the weights of the two estimators reveals our key insight. As shown in fig. 2a, the orange curves, representing $m_{CHDT}^{asym}(\\cdot)$, are consistently higher than the gray curves, representing $m_{CH}^{asym} (\\cdot)$. However, the choice-decision-time estimator\u2019s min-weight, $\\min_x m_{CHDT}^{asym}(\\cdot)$, can be either larger or smaller than the choice-only estimator\u2019s weights, $m_{CH}^{asym} (\\cdot)$, depending on the query set. For instance, when the barrier $\\alpha$ is small (e.g., $\\alpha = 0.5$), the choice-decision-time estimator\u2019s min-weight is likely smaller than the choice-only estimator\u2019s weights, suggesting that in this case, incorporating response times may not improve performance. On the other hand, when $\\alpha$ is large (e.g., $\\alpha = 1.5$) and the queries are easy, the choice-only estimator\u2019s weights shrink significantly, while the choice-decision-time estimator\u2019s min-weight remains relatively large. This demonstrates that incorporating response times makes easy queries more useful, resulting in improved estimation performance."}, {"title": "3.3 Non-asymptotic concentration of the two estimators for utility difference estimation", "content": "In this section, we consider the simplified problem of estimating the utility difference for a single query, without aggregating data from multiple queries. Comparing the non-asymptotic concentration bounds of both estimators in this scenario provides insights similar to those discussed in section 3.2. A non-asymptotic analysis for estimating the human preference vector $\\theta^*$ is left for future work.\nGiven a query $x \\in \\mathcal{X}$, the simplified problem focuses on estimating its utility difference $u_x := x^T\\theta^*$ using the i.i.d. dataset $\\{(C_{x,s_{x,i}},t_{x,s_{x,i}})\\}_{i \\in [n]}$. Applying the choice-decision-time estimator from eq. (4) yields the following estimate (for details, see appendix C.3.1):\n$\\hat{u}_{x,CH,DT} := \\frac{\\sum_{i=1}^{n_x} C_{x,s_{x,i}}}{\\sum_{i=1}^{n_x} t_{x,s_{x,i}}}.$\nIn contrast, applying the choice-only estimator from eq. (5) yields the following estimate (for details, see appendix C.3.2):\n$\\hat{u}_{x,CH} := \\mu^{-1} (\\frac{1}{n_x} \\sum_{i=1}^{n_x} \\frac{C_{x,s_{x,i}} + 1}{2}),$\nwhere $(C_{x,s_{x,i}} + 1)/2$ is the one-zero-coded binary choice and $\\mu^{-1}(p) := \\log (p/(1 \u2013 p))$ is the logit function, the inverse of $\\mu$ introduced in eq. (5). As with the estimators in section 3.1, the choice-decision-time estimator in eq. (6) estimates $u_x/\\alpha$, while the choice-only estimator in eq. (7) estimates $2\\alpha u_x$. Interestingly, the choice-only estimator in eq. (7) aligns with the drift estimator for the EZ-diffusion model, as proposed in Wagenmakers et al. [68, eq. (5)]. Additionally, the estimators in Xiang Chiong et al. [74, eq. (6)] and Berlinghieri et al. [9, eq. (7)] can be viewed as combinations of the two estimators presented in eqs. (6) and (7). In section 5, we apply Wagenmakers et al. [68, eq. (5)] and Xiang Chiong et al. [74, eq. (6)] to the full bandit problem and empirically demonstrate that they are outperformed by our estimator proposed in eq. (4).\nAssuming the utility difference is nonzero, the choice-decision-time estimator in eq. (6), has the following non-asymptotic concentration result, proved in appendix C.3.1:\nTheorem 3.3 (Non-asymptotic concentration of $\\hat{u}_{x,CH,DT}$). For each query $x \\in \\mathcal{X}$ with $u_x \\neq 0$, suppose that the learner has an i.i.d. dataset $\\{(C_{x,s_{x,i}},t_{x,s_{x,i}})\\}_{i \\in [n]}$. Then, for any $\\epsilon > 0$ satisfying $\\epsilon \\leq \\min \\{|u_x|/(\\sqrt{2}\\alpha), (1 + \\sqrt{2}) \\alpha|u_x|/\\mathbb{E} [t_x]\\}$, we have the following:\n$P(\\vert\\hat{u}_{x,CH,DT} - \\frac{u_x}{\\alpha}\\vert > \\epsilon) \\leq 4 \\exp(-\\frac{m_{CH,DT}^{non-asym} (x^T\\theta^*) \\cdot n_x [\\epsilon \\cdot \\alpha]^2}{u_x^2}),$\nwhere $m_{CH,DT}^{non-asym}(x^T\\theta^*) := \\mathbb{E} [t_x]^2 / [(2 + 2\\sqrt{2})^2 \\alpha^2]$.\nIn contrast, the choice-only estimator in eq. (7) has the following non-asymptotic concentration result, which can be directly adapted from Jun et al. [30, theorem 1]:"}, {"title": "4 Interactive learning algorithm", "content": "In this section, we introduce the Generalized Successive Elimination (GSE) algorithm [2, 4, 76] for fixed-budget best-arm identification in preference-based linear bandits. We outline several options for each component of GSE, which will be empirically compared in section 5.\nGSE's pseudo-code is presented in algorithm 1. There is one hyper-parameter, $\\eta$, that controls the number of phases, the budget per phase, and the number of arms eliminated in each phase. GSE starts by dividing the budget B evenly into phases according to $\\eta$ and reserves a buffer in each phase to prevent over-consuming the budget (line 3). In each phase, GSE first computes an experimental design, a probability distribution $\\Lambda$ over the query space, to determine which queries to sample during that phase. We consider two designs: the transductive design [22], $\\Lambda_{\\text{trans}}$ (line 4), and the hard-query design [30], $\\Lambda_{\\text{hard}}$ (line 5). Both designs minimize the worst-case variance of the estimated utility difference between surviving arms. The transductive design assigns equal weight to all queries. In contrast, the hard-query design assigns more weight to hard queries, taking advantage of the choice-only estimator's advantage\u2014compared to the choice-decision-time estimator\u2014in extracting information from hard queries, as discussed in section 3."}, {"title": "5 Empirical results", "content": "This section empirically compares the GSE variations introduced in section 4: (1) $(\\Lambda_{\\text{trans}}, \\hat{\\theta}_{CH,DT})$: Transductive design with our choice-decision-time estimator. (2) $(\\Lambda_{\\text{trans}}, \\hat{\\theta}_{ch})$: Transductive design with the choice-only estimator. (3) $(\\Lambda_{\\text{hard}}, \\hat{\\theta}_{CH})$: Hard-query design with the choice-only estimator."}, {"title": "5.1 Estimation performance using synthetic data", "content": "We benchmark the estimation performance of these GSE variations using the \"sphere\" synthetic problem from the linear bandit literature [19, 41, 62]. In this problem, the arm space $\\mathcal{Z} \\subset \\{z \\in \\mathbb{R}^5: ||z||_2 = 1\\}$ contains 10 randomly generated arms. To determine $\\theta^*$, we select the two arms $z$ and $z'$ with the closest directions, i.e., $(z, z') \\in \\arg \\max_{z,z' \\in \\mathcal{Z}} z^Tz'$, and define $\\theta^* = z + 0.01(z' \u2212 z)$. In this way, $z$ is the best arm. The query space $\\mathcal{X} := \\{z \u2212 z' : z \u2208 Z\\}.\nEstimation performance, as discussed in section 3, depends on the utility difference $x^T\\theta^*$ and the barrier $\\alpha$. To adjust the utility differences, we scale each query by scaling each arm $z$ to $c_z \\cdot z$. We"}, {"title": "5.2 Fixed-budget best-arm identification performance using real datasets", "content": "This section compares the bandit performance of six GSE variations. The first three are as previously defined: $(\\Lambda_{\\text{trans}}, \\hat{\\theta}_{CH,DT})$, $(\\Lambda_{\\text{trans}}, \\hat{\\theta}_{CH})$, and $(\\Lambda_{\\text{hard}}, \\hat{\\theta}_{CH})$. The fourth variation evaluates performance when the non-decision time $t_{\\text{nondec}}$ (assumed to be known in section 2) is unknown. By replacing each query's decision time $t_x$ with the response time $t_{RT,x}$ in the choice-decision-time estimator from Eq. (4), we obtain a new GSE variation, denoted by $(\\Lambda_{\\text{trans}}, \\hat{\\theta}_{CH,RT})$. The fifth variation is based on Wagenmakers et al. [68, eq. (5)], which states that $x^T \\cdot (2\\alpha\\theta^*) = \\mu^{-1}(P[c_x = 1])$, where the logit function $\\mu^{-1}(p) := \\log (p/ (1 \u2212 p))$. This leads to the following choice-only estimate of $2\\alpha\\theta^*$:\n$\\hat{\\theta}_{CH,\\text{logit}} := (\\sum_{x \\in \\mathcal{X}_{\\text{sample}}} n_x x x^T)^{-1} \\sum_{x \\in \\mathcal{X}_{\\text{sample}}} n_x x \\cdot \\mu^{-1} (\\hat{C}_x,i),$"}, {"title": "6 Limitations and conclusion", "content": "Limitations We have demonstrated the benefits of incorporating response times to accelerate prefer-ence learning. However, one key limitation is that high-quality response time data requires participants to remain focused on the task without distraction [45], which can be a challenge in crowdsourc-ing environments, such as fine-tuning large language models [6, 43, 46, 47, 60]. One potential solution is to integrate human eye movements into the DDM framework, as seen in attentional DDMs [24, 37, 38, 58, 77], to detect lapses in attention. In cases where attention is lost, the system can disregard the response times. From a technical standpoint, a limitation of our work is the lack of full analysis for algorithm 1, which could provide insights into optimizing the elimination parameter $\\eta$. Both limitations present promising directions for future research.\nConclusion In this work, we investigated the incorporation of human response times to enhance fixed-budget best-arm identification in preference-based linear bandits. We proposed an estimator that combines human choices and response times, comparing it against an estimator that uses only choices. Our theoretical and empirical analyses revealed that incorporating response times makes easy queries more useful. We then deployed these estimators in a bandit learning algorithm and demonstrated the superior performance of incorporating response times across simulations based on three datasets."}, {"title": "A Broader impacts", "content": "Incorporating human response times in human-interactive AI systems provides significant benefits, such as efficiently eliciting user preferences, reducing cognitive loads on users, and improving accessibility for users with disabilities and various cognitive abilities. These benefits can greatly improve recommendation systems, assistive robots, online shopping platforms, and fine-tuning for large language models. However, using human response times also raises concerns about privacy, manipulation, and bias against individuals with slower response times. Governments and law enforcement should work together to mitigate these negative consequences by establishing ethical standards and regulations. Businesses should always obtain user consent before recording response times."}, {"title": "B Literature review", "content": "B.1 Bounded accumulation models for choices and response times\nBounded Accumulation Models (BAMs) represent a class of models for human decision-making, typically involving an accumulator (or sampling rule) and a stopping rule [71]. For binary choice tasks, such as two-alternative forced choice tasks, a widely used BAM is the drift-diffusion model (DDM) [51], which consists of a Brownian-motion accumulator and a stopping rule based on two fixed boundaries. To capture the empirical differences in human response times when choosing the correct versus incorrect answer, Ratcliff and McKoon [51] allows the drift, starting point, and non-decision time to randomly vary across trials. Later, Wagenmakers et al. [68] proposed the EZ-diffusion model (EZDM), a simplified version of DDM, offering closed-form expressions for choice and response time moments to simplify the parameter estimation procedure and improve the statistical robustness. EZDM assumes that the drift, starting point, and non-decision time are deterministic and remain fixed across trials and that the starting point is equidistant from the upper and lower boundaries. Recently, Berlinghieri et al. [9] focused on the parameter estimation for the difference-based EZDM, a variant where the drift is the difference in utilities between two options. Formally, for each binary query with arms $z_1$ and $z_2$, the drift is modeled as the utility difference $u_{z_1} - u_{z_2}$, where $u_{z_1}$ and $u_{z_2}$ are the utilities of $z_1$ and $z_2$.\nOur work extends the difference-based EZDDM by parameterizing human utility as a linear function. Specifically, we model each arm's utility $u_z := z^T\\theta^*$, where $\\theta^*$ represents the human preference vector. This linear parameterization is supported by both the bandit and psychology literature. In the bandit domain, linearly parameterized utility functions can scale effectively in systems with a large number of arms [14, 40]. In psychology, linear combinations of attributes are commonly used to model human multi-attribute decision-making, where individuals focus on different attributes during the decision-making process [24, 65, 77]. One of our empirical studies on food-gamble preferences [58] highlights the potential of our approach for learning human preferences through multi-attribute decision-making.\nIn a similar vein, recently, Shvartsman et al. [56] parameterize the human utility function as a Gaussian process and propose a moment-matching-based Bayesian inference procedure to estimate the latent utility using both choices and response times. Unlike our work, their focus is solely on estimation, without addressing the bandit optimization aspect. Exploring how their estimation techniques could be integrated into bandit optimization is a promising direction for future research.\nAnother widely used BAM is the race model [11, 67], which naturally extends to queries with more than two options. In race models, each option has its own accumulator, and the decision-making process ends when any accumulator reaches its threshold. BAMs can also capture human attention during the decision-making process. For example, the attentional-DDM [37, 38, 77] jointly models human choices, response times, and eye movements across various options or attributes in a query. Similarly, Thomas et al. [63] introduce the gaze-weighted linear accumulator model to explore gaze bias effects at the trial level. To incorporate human learning effects, Pedersen et al. [49] integrate reinforcement learning (RL) with DDM to account for the drift's non-stationarity. In their work, the human uses RL to update their drift, while in our work, the AI agent uses RL to make decisions and interact with the human.\nBAMs have theoretical connections to human Bayesian RL models. For example, Fudenberg et al. [25] propose a model where humans face a fixed cost for spending additional time thinking and solve a Bayesian optimal stopping problem to balance this cost with the decision accuracy. Their work shows that this model is equivalent to a DDM with time-decaying boundaries.\nNeurophysiological evidence supports BAMs. For instance, EEG recordings demonstrate that neurons exhibit accumulation processes and decision thresholds [71]. Additionally, diffusion processes have been used to model neural firing rates [53]."}, {"title": "B.2 Parameter estimation for bounded accumulation models", "content": "BAMs typically lack closed-form density functions, making hierarchical Bayesian inference a common approach for parameter estimation [72]. While flexible, these methods are computationally intensive and impractical for real-time use in online learning systems. Fast-computing estimators [9, 68, 74] usually estimate parameters for single option pairs without aggregating data across multiple"}, {"title": "B.3 Various uses of response times", "content": "Response times can serve a variety of objectives, as explored by Clithero [16", "15": "demonstrated that the EZDM [68", "3": "showed that response times can enhance the identifiability of human preferences compared to relying on choices alone.\nResponse times also provide insights into human decision-making processes. Castro et al. [13", "18": ".", "81": "proposed a framework that leverages human planning time to infer their targeted goals.\nResponse times can also enhance AI decision-making. In dueling bandits and preference-based RL [8"}, {"3": ".", "z_2": 1, "3.2": "."}, {"z_2": "sigma^{\\ddagger}(u_{z_1}, u_{z_2})$, where $\\sigma^{\\ddagger}$ could be a complicated function that depends separately on $u_{z_1}$ and $u_{z_2}$, lacking a closed form. If we fix $u_{z_2}$ and vary only $u_{z_1}$, the function $\\sigma^{\\ddagger} (\\cdot, u_{z_2})$ is known as a psychometric function, typically exhibiting an \u201cS\u201d shape, as discussed in Strzalecki [61, the text above fig. 1.1"}]}