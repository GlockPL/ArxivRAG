{"title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy", "authors": ["Hyunjong Kim", "Suyeon Lee", "Yeongjae Cho", "Eunseo Ryu", "Yohan Jo", "Suran Seong", "Sungzoon Cho"], "abstract": "The increasing demand for mental health services has led to the rise of Al-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.", "sections": [{"title": "1 Introduction", "content": "In modern society, the issue of mental health is emerging as a critical problem, with an increasing demand for mental health services. This has led to a shortage of mental health workers available to meet the growing demand (Butryn et al., 2017). Thus, there has been considerable expansion in the investigation of AI-driven chatbots providing mental health assistance (Inkster et al., 2018; Mousavi et al., 2021). However, there are many challenges in training such chatbots. It suffers from privacy issues, requires a considerable amount of cost and time for collecting data, and crucially, needs expertise in mental health. Cho et al. (2023) pointed out that many mental health chatbots are designed without any underlying theory for psychotherapy, casting doubt on their utility for mental health support.\nAs a theoretical basis for enhancing expertise, Motivational Interviewing (MI) has been attracting increasing attention in developing mental health support chatbots (Park et al., 2019; He et al., 2022; Brown et al., 2023). MI is a client-centered counseling technique to elicit behavior change by helping clients explore and resolve ambivalence (Miller and Rollnick, 2002). MI has been an active area of research intersecting the disciplines of psychotherapy and Natural Language Processing (NLP), due to its effectiveness and well-structured behavioral coding systems such as MITI (Motivational Interviewing Treatment Integrity) (Moyers et al., 2003, 2014).\nFew works have proposed MI datasets with annotations from MI behavioral coding systems. P\u00e9rez-Rosas et al. (2016) developed an MI dataset using automatic captioning of YouTube and Vimeo videos. However, the dataset is currently not publicly available due to ethical reasons. Welivita and Pu (2022) proposed an alternative of scraping data from online peer support forums. Although these dialogues are available in abundance, they have shown significant differences from those collected from professional counselors in terms of MI, thus lacking expertise. In line with this problem, Wu et al. (2023) presented AnnoMI, a dataset of 133 MI conversations that were professionally transcribed from MI demonstration videos and further annotated by experienced MI practitioners. Despite its decent transcription and annotation quality, it significantly lacks in quantity due to the limited video sources and the intensive human labor required.\nIn short, the mentioned datasets contain limitations to be used for training MI-boosted chatbots, such as lack of expertise or insufficient quantity. Also, transcripts often include many onomatopoeic words and instances of stammering, which are undesirable in chatbots. Furthermore, previous works have predominantly focused on English. While increasing language coverage is crucial in addressing the challenges of mental health support (Cho et al., 2023), research on other languages and cultures has been relatively understudied.\nTo address these issues, we propose a novel framework to generate synthetic dialogues that simulate MI sessions using Large Language Models (LLMs). We carefully design two agents, the therapist simulator and the client simulator, which alternately take turns to generate utterances based on few-shot in-context learning. To incorporate the expertise of professional therapists, we train an MI forecaster model, which predicts the next-turn therapist behavior. Also, we focus on a non-English language, Korean in our case, and ground the dialogues in real-world contexts that reflect actual Korean circumstances. Consequently, we present KMI, a dataset of 1,000 Korean Motivational Interviewing dialogues. As illustrated in Figure 1, KMI is the only dataset that overcomes the previously mentioned limitations of existing resources. KMI covers a wide range of concerns and anxieties common among Koreans, while successfully integrating the MI strategy of professional therapists simultaneously. Each therapist utterance within the"}, {"title": "2 Related Work", "content": "Dialogue Generation Using LLMs Recent research has increasingly focused on using LLMs to generate dialogues for various applications. Kim et al. (2022) proposed a human-machine collaborative framework to build a large-scale dialogue dataset for training conversational agents to handle problematic content appropriately. Chen et al. (2023a) used a small number of expert-written conversations as in-context examples to create synthetic multi-party conversations. Chen et al. (2023b) proposed LLM prompting methods to generate mixed-initiative dialogues. Kim et al. (2023) created a large-scale social dialogue dataset by distilling conversations from LLMs. Macina et al. (2023) paired human teachers with an LLM student to generate teacher-student tutoring dialogues grounded in math reasoning problems.\nNLP Applications in MI MI was developed as a technique to assist individuals in resolving ambivalence and committing to change (Miller, 1983), representing an evolution of client-centered therapy. Along with the advancement in NLP, considerable research efforts have been underway to apply NLP techniques in the field of MI. Several studies have been proposed to automatically classify a given utterance into one of the MI behavioral codes. While earlier approaches utilize linguistic features (P\u00e9rez-Rosas et al., 2017) and recurrent neural network architectures (Tanana et al., 2015; Xiao et al., 2016; Cao et al., 2019; Gibson et al., 2019), recent approaches make use of pre-trained language models such as ROBERTa (Liu et al., 2019; Tavabi et al., 2021; Welivita and Pu, 2023). Some works adopt a multimodal approach, leveraging additional information such as speech features (Tavabi et al., 2020) or facial features (Nakano et al., 2022).\nWelivita and Pu (2023) demonstrated the potential of LLMs in boosting dialogues using the MI strategy. They observed that among the MI dataset curated from online platforms (Welivita and Pu, 2022), 92.86% of the advice given by peers fall into the Advise without permission category, which is MI non-adherent. To make the dataset more MI-consistent, they fine-tuned BlenderBot (Roller et al., 2021) and GPT-3 (Brown et al., 2020) to rephrase these responses into more MI-adherent Advise with permission responses. Although this work demonstrated the possibility of leveraging LLMs in MI, the impact of rephrasing remains marginal, considering that its capability is restricted to revising the manner of speech and has limitations in modifying the content itself. In our work, we further exploit the generative ability of LLMs to generate the entire dataset from scratch."}, {"title": "3 KMI: A Dataset of Korean Motivational Interviewing Dialogues", "content": "This section introduces a novel framework to generate realistic motivational interviewing dialogues. The overall framework is illustrated in Figure 2. First, we collect context data (Section 3.1) that reflects the actual concerns and anxieties experienced by Koreans. Each context serves as the topic of each dialogue. Next, we simulate an MI session (Section 3.2) using a therapist simulator and client simulator. The resulting KMI dataset is presented in Section 3.3.\nTo construct context data for generating realistic dialogues, we web-crawl posts from a Korean psychological counseling platform Mindcafe\u00b3, which contain common concerns of Koreans. We collect a total of 7,530 posts from seven categories: mental health, interpersonal relationships, ego & personality, career & employment, academic & examination, addiction & obsession, and family. Then, using GPT-4 API\u2074 (Achiam et al., 2023), we score these posts on a scale from 1 to 3 in terms of the post's specificity and its suitability as a topic for an MI session. Among the 3,098 posts that received a score of 3, a total of 1,000 posts were sampled with a predetermined quantity for each category. Details regarding the collection, filtering, and sampling of context data can be found in Appendix A.\nFor each context, we simulate an MI session using an LLM-based therapist simulator (Section 3.2.1) and client simulator (Section 3.2.2). They alternately take turns to generate utterances. The conversation begins with a general open question by the therapist, such as \u2018Hello, what concerns have brought you here today?'. For both simulators, we utilize GPT-4 API for high-quality generation.\nWe define eight labels derived from MITI code 2.0 (Moyers et al., 2003) and 4.2.1 (Moyers et al., 2014) to categorize therapist behaviors. Each therapist utterance in the final KMI dataset is annotated with one of these eight labels, which helps capture the nuances of therapeutic conversations and enhances the dataset's utility for various NLP applications.\nUsing MI-consistent techniques and blending them skillfully helps motivate clients to change (Moyers et al., 2014). Thus, it is crucial to follow the"}, {"title": "3.1 Collecting Context Data", "content": "To construct context data for generating realistic dialogues, we web-crawl posts from a Korean psychological counseling platform Mindcafe\u00b3, which contain common concerns of Koreans. We collect a"}, {"title": "3.2 Motivational Interviewing", "content": "For each context, we simulate an MI session using an LLM-based therapist simulator (Section 3.2.1) and client simulator (Section 3.2.2). They alternately take turns to generate utterances. The conversation begins with a general open question by the therapist, such as \u2018Hello, what concerns have brought you here today?'. For both simulators, we utilize GPT-4 API for high-quality generation.\nWe define eight labels derived from MITI code 2.0 (Moyers et al., 2003) and 4.2.1 (Moyers et al., 2014) to categorize therapist behaviors. Each therapist utterance in the final KMI dataset is annotated with one of these eight labels, which helps capture the nuances of therapeutic conversations and enhances the dataset's utility for various NLP applications.\nUsing MI-consistent techniques and blending them skillfully helps motivate clients to change (Moyers et al., 2014). Thus, it is crucial to follow the"}, {"title": "3.2.1 Therapist Simulator", "content": "Using MI-consistent techniques and blending them skillfully helps motivate clients to change (Moyers et al., 2014). Thus, it is crucial to follow the behavioral choices of professional therapists to generate high-quality MI dialogues. In this work, this is achieved by first predicting the next-turn therapist behavior via MI forecaster and decision module, and then generating the utterance based on the predicted behavior by prompting a LLM. If one could simulate a realistic therapist with expertise, LLM-based simulation could be a significant breakthrough in the field of MI, alleviating the issues of extensive human workload and privacy concerns.\nMI Forecaster MI forecaster aims to predict the next-turn therapist behavior, which is one of the MI labels listed in Table 1, given a dialogue history. We fine-tune T5-base (Raffel et al., 2020) with a converted dataset of AnnoMI (Wu et al., 2023), which is preprocessed and converted for a forecasting task. AnnoMI is an expert-annotated MI dialogue dataset. As the original dataset doesn't include Affirm among its utterance labels, we add Affirm label to the dataset using the RoBERTa-based MI classifier developed in Welivita and Pu (2023). More details regarding the preprocessing of AnnoMI can be found in Appendix B.\nThen we construct pairs of input text and output text, where input text is the dialogue history prefixed with the task instruction (Predict next therapist's dialogue act) and output text is the next-turn therapist behavior label. We explore various modeling settings to find the model with the best predictive power: (1) we test history window sizes from 1 to 8 and (2) try inserting the MI behavior label of each therapist utterance in the dialogue history to provide additional information to the model. An example of the converted dataset is"}, {"title": "Decision Module", "content": "In each therapist's turn, among the three labels the MI forecaster predicted, the decision module decides the final therapist label to generate. Based on two simple rules, it complements the MI forecaster from a broader perspective: (1) The same therapist label cannot appear three times in a row. (2) Either open or closed, the therapist cannot ask questions three times in a row. The first rule is devised to keep the dialogue from being too homogeneous. The second rule is a direct implementation of a clinical guideline that advises against asking three questions in a row (Miller and Rollnick, 2002). Such behavior might direct the client into a passive, question-answering role, which should be avoided in MI. In the order of top-1, top-2, and top-3 predictions, the decision module checks each label against these two rules. The first prediction that complies with both rules is determined to be the final therapist's behavior for the next turn."}, {"title": "Utterance Generation", "content": "Once the label is determined, we generate the therapist's utterance by prompting an LLM to generate an utterance based on the determined label and dialogue history. We leverage in-context learning for this purpose, providing the definition and three examples of the corresponding label. The definition and examples are excerpted from Korean MI textbooks (Schumacher and Madson, 2017; Shin and Cho, 2016; Shin, 2019), which could be considered among the most credible sources available. These examples contain speech patterns and linguistic expressions actually used by professional Korean counselors. In all prompts, we specify predefined constraints to generate consistent and natural utterances. Also,"}, {"title": "3.2.2 Client Simulator", "content": "We generate client utterances by directly prompting a LLM. We provide the context and instruct the LLM to generate responses based on the context and dialogue history. In order to depict the evolving states of clients throughout the MI session, we adopt a simple but effective approach to instruct the LLM to generate change talk, a client language that indicates movement toward a particular change (Miller and Rollnick, 2023), if interaction with the counselor inspired a willingness to change and speaking about change appears suitable within the dialogue context. As in the therapist simulator, we provide the definition and four examples of change talk excerpted from Korean MI textbooks (Schumacher and Madson, 2017; Shin and Cho, 2016; Shin, 2019) for in-context learning. The four examples each include one from the four types of preparatory change talk\u2014desire, ability, reasons, and need (often abbreviated as DARN) (Miller and Rollnick, 2023)\u2014encompassing various types of change talk. Descriptions of each type can be found in Appendix D. Also, we specify predefined constraints in each prompt. The prompt for generating a client utterance, which includes instructions, constraints, definition, and examples of change talk,"}, {"title": "3.3 Statistics", "content": "As shown in Table 2, KMI consists of 1,000 long-turn dialogues, with an average of 18.12 turns per dialogue. Each utterance by therapists is assigned one of the MI labels. As detailed in Table 2, Complex Reflection emerges as the most frequent label within KMI, accounting for 36% of all therapist utterances, followed by Open Question and Simple Reflection. Although there are no strict guidelines for the proportion of MI labels, MITI coding manual (Moyers et al., 2014) provides summary scores to measure clinicians' competence in using MI. Reflection-to-Question Ratio (R:Q) being 1:1 is considered 'fair', while 2:1 is considered 'good'. KMI yields a ratio of 1.8:1, implying that it meets the standard of professional clinicians. An example dialogue in KMI is illustrated in Figure 4."}, {"title": "4 Evaluation", "content": "We evaluate the quality of the generated dataset (Section 4.2) and the dialogue model fine-tuned with it (Section 4.3). We compare our dataset with CounselGPT, the only Korean counseling dataset to date that is created using OpenAI API, and An-noMI. We translate AnnoMI into Korean using Up-"}, {"title": "4.1 Evaluation Criteria", "content": "We evaluate the datasets and dialogue models from two perspectives: MI and general dialogue systems.\nMI Quality We aim to measure how closely the dialogues adhere to the principles of MI. To achieve this goal, we propose novel metrics derived from MI theory: (1) Partnership, (2) Acceptance, (3) Compassion, (4) Evocation, (5) Similarity, and (6) Effectiveness. (1)~(4) are derived from the fundamental spirit of MI (Miller and Rollnick, 2012). Similarity measures how closely the generated therapist's utterances resemble those of an actual therapist. Effectiveness measures the overall efficacy of the MI session.\nGeneral Quality Following previous works (Wan et al., 2022; Chen et al., 2023a), we use three evaluation criteria to measure the general quality of the dialogues: (1) Consistency, (2) Fluency, and (3) On-topic. Consistency examines whether the entire dialogue and utterances between turns are consistent. Fluency assesses the flow of the dialogue and the naturalness and fluidity of each utterance. On-topic evaluates whether the dialogue is relevant to the provided context."}, {"title": "4.2 Evaluation of Dataset", "content": "Evaluation Framework We assess the MI quality and general quality of KMI based on human evaluation, particularly by experts. We recruited four Korean psychological counseling experts experienced in MI. For evaluation, we randomly sample 100 dialogues from each of KMI, Counsel-GPT, and AnnoMI. The experts are then requested to evaluate the dialogues based on the criteria defined in Section 4.1, using a Likert scale ranging from 1 to 5. The median score is calculated by applying the majority vote approach.\nMI Quality As demonstrated in Table 3, KMI surpasses CounselGPT and AnnoMI across all evaluation criteria of MI quality. CounselGPT, which"}, {"title": "4.3 Evaluation of Dialogue Model", "content": "Training Dialogue Models We assess dialogue models fine-tuned with these datasets to verify the utility of KMI as a training dataset for chatbots. We fine-tune komt-llama2-7b-v111, a LLaMA 2-7B (Touvron et al., 2023) model further tuned with Korean multi-task instruction tuning, using each of KMI, CounselGPT, and translated AnnoMI. Finetuning details are stated in Appendix G.\nEvaluation Framework We recruited four native Korean crowdworkers to participate in conversations with the dialogue models. Given context data, they are asked to interactively converse with the model, as if they were the writer of the given context. They engage in 30 conversations with each dialogue model, using an identical set of 30 contexts for each model. Among the collected data, context data not previously used for data generation is used in this step. The completed dialogues are then evaluated by experts12 based on the aforementioned criteria, using a 5-point Likert scale.\nMI Quality Results in Table 5 show that the dialogue model fine-tuned on KMI outperforms other models across all MI quality criteria by a substantial margin. This highlights the value of KMI for building mental health chatbots, especially those specialized in MI. We speculate that the relatively low scores for similarity and effectiveness are related to the capability of the base model, komt-llama2-7b-v1, since it might be challenging for a 7B-sized model to proficiently role-play a professional therapist or lead an effective MI session from a professional perspective. The scores of CounselGPT and AnnoMI are mostly below 3,"}, {"title": "5 Discussion and Conclusion", "content": "In this paper, we introduced a novel framework to generate synthetic motivational interviewing dialogues, along with KMI, the resulting dataset of 1,000 Korean MI dialogues. Through comprehensive evaluations, we demonstrated its quality and utility for chatbot development. For meaningful evaluation, we also proposed novel metrics derived from MI theory.\nOur dataset has three main applications. First, it could be used for developing mental health chatbots grounded in MI theory. We trained a Korean chatbot in this paper, where expert evaluation results and usage examples (see Table 22) show that our dataset is capable of building effective chatbots. Second, it could function as a labeled dataset of (Utterance, MI label) pairs for classification and forecasting tasks. MI label accuracy of 96.0% demonstrates the reliability of the labels. Third, it could serve as a reference for MI practitioners. Full dialogues of therapy sessions are usually private, while MI textbooks provide only segments of dialogues. Despite being synthetic, KMI can offer high-quality, full dialogues covering diverse topics.\nIn addition, our generation method is potentially generalizable to other languages. Korean-specific resources required in our framework are: (1) context data, (2) 3~4 high-quality examples for each MI label for few-shot prompting, and (3) a Korean-to-English translation model. If these resources are available, our framework could be further used to boost non-English resources for psychotherapy."}, {"title": "Limitations", "content": "Error Analysis While our framework generally performs well, we found some cases where the dialogue doesn't sufficiently reflect the contents and details of the context data. These cases are discussed below:\n(1) When the client simulator generates an utterance, it considers both the context and dialogue history. The ideal scenario is when it responds to the therapist while incorporating the details of the context at the same time, but sometimes the counselor might lead the conversation in a direction that doesn't align with the context data. In such cases, the client simulator primarily focuses on responding to the therapist's last utterance, potentially resulting in somewhat shallow or meandering dialogues that don't contain the details of the context data.\n(2) As the context data is collected from an online counseling platform, some of it contains extreme or violent content. We found that GPT-4 automatically filters such content when generating utterances, removing problematic elements from the final KMI dataset. Dialogues generated based on such context might be less specific and realistic.\nMI Forecaster Trained on English Dataset As there were no public Korean MI resources available prior to our work, we used AnnoMI, an English MI dialogue dataset, to train the MI forecaster. An-noMI consists entirely of English dialogues, most of which are based on Western cultures. Thus, the predictions made by the MI forecaster might be biased toward the practices of therapists from these cultural backgrounds. However, we suppose that the core principles of MI such as partnership, acceptance, compassion, and evocation remain universal regardless of the language and culture. It is known that MI is well-suited to working with different languages and culturally diverse populations (Miller and Rollnick, 2023). Also, McMaster and Griva (2015) discovered that over 50 groups of health professionals across North America, Europe, Africa, and Asia generated remarkably similar responses when asked what they would consider good practice and bad practice in their own settings. This indicates that good MI practice would still remain good MI practice in different cultures, supporting the validity of our approach.\nEffect of Translation Two kinds of translations are involved in our research. First, in the generation process of KMI, we translate the generated dialogue history into English to use it as the input for the MI forecaster. Second, we translate AnnoMI into Korean during the evaluation phase13. In both cases, we utilized Upstage SOLAR (Kim et al., 2024), one of the most recent translation models, to preserve the meaning of sentences, but some degradation is inevitable. We consider the latter more critical since slight variations in nuance or expression can have a substantial impact in the context of psychotherapy, possibly resulting in lower scores for AnnoMI. We believe the effect of translation is more trivial in the former case, as the MI forecaster considers the overall content and flow of the dialogue history to make predictions, rather than focusing on detailed expressions.\nEthics Statement In the Korean psychological counseling platform Mindcafe, individuals are anonymous and they recognize that their writing is archived on the site unless they delete it. We collected only publicly available data and there was no interaction with the Mindcafe users.\nHuman Evaluation For human evaluation, we recruited four professional counselors. They were compensated with 50,000 won per hour. For the evaluation of dialogue models, we also recruited four crowdworkers. They were compensated with 10,000 won per hour, which is higher than the Ko-rean minimum wage at the time they worked.\nMental Health Support Chatbots The KMI dataset was designed to facilitate the advancement of chatbots that adhere to MI principles, particularly for mental health support. However, despite the promising capabilities of recent AI models, AI-driven chatbots may still pose risks. The unpredictability of generative models can lead to unintended consequences, especially in sensitive conversations involving emotional distress. Therefore, any deployment of chatbots trained on KMI should be approached with caution, and human supervision is essential to ensure that the responses are appropriate."}, {"title": "D Change Talk", "content": "Change talk is a client language that indicates movement toward a particular change (Miller and Rollnick, 2023). Four types of change talk\u2014desire, ability, reasons, and need (often abbreviated as DARN)-are called preparatory change talk because you hear them when people are considering whether to do something. Descriptions and examples of each type are stated below.\n\u2022 Desire language is a way of saying, \u201cI want.\" It signals some inclination toward action. (\"I want to quit smoking.\")\n\u2022 Ability language provides information about how confident people are that they would be able to take the action in question. (\"I think it's possible for me to quit.\")\n\u2022 Reason language states specific reasons for doing something. The reason might be a possible advantage of change or a disadvantage of not changing. (\"My children are begging me to quit.\")\n\u2022 Need language has an imperative quality emphasizing some urgency of change. It implies that a change is important without specifying why. (\"I've got to quit smoking.\")"}]}