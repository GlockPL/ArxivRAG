{"title": "Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem", "authors": ["Declan Campbell", "Sunayana Rane", "Tyler Giallanza", "Nicol\u00f2 De Sabbata", "Kia Ghods", "Amogh Joshi", "Alexander Ku", "Steven M. Frankland", "Thomas L. Griffiths", "Jonathan D. Cohen", "Taylor W. Webb"], "abstract": "Recent work has documented striking heterogeneity in the performance of state- of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks \u2013 such as counting, localization, and simple forms of visual analogy \u2013 that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.", "sections": [{"title": "Introduction", "content": "Recent progress in training large-scale neural networks on internet-scale datasets has led to the creation of Al systems with capabilities rivaling human performance across a broad range of complex tasks. Most recently, this has given rise to an array of vision language models (VLMs), including multimodal language models such as GPT-4v that can generate text descriptions of multimodal text and image inputs [1], and text-to-image models such as DALL-E 3 that can generate images from natural language descriptions [24]. However, despite the considerable success of VLMs across many tasks, these models still perform poorly on several surprisingly simple multi-object reasoning tasks such as counting [23, 25, 40], relational image generation [7], relational scene understanding [15, 31], and simple visual analogy tasks [20, 38] on which humans achieve near perfect accuracy.\nDrawing from theoretical work both in cognitive science and neuroscience, we turn to the binding problem [9, 10, 29, 33, 36] as a potential explanation for these limitations. \u2018Binding' refers to the ability to associate one feature of an object (e.g., its color) with the other features of that object (e.g., its shape and location), and the 'binding problem' refers to the question of how the brain accomplishes this without interference between the features for different objects. It is widely recognized that the"}, {"title": "Visual Search", "content": "Extensive prior work in cognitive psychology has tested how people process scenes involving multiple objects and under what conditions their performance degrades. These studies demonstrate that performance is not driven solely by the number of objects present in a scene, but also depends on the likelihood of interference among objects given the specific distribution of features and feature conjunctions from which they are composed. This can be seen most directly in research on visual search, where participants are typically tasked with identifying a specific object within a multi-object array. A classic pattern of results arises from a comparison of two conditions: disjunctive and conjunctive search [33]. In disjunctive search (depicted on the left side of Figure 1), the array consists of distractor objects that share one feature with the target (e.g., the distractors are all circles) but differ in a second feature (e.g., the distractors are all red circles in the 2D task variant). Since one of the feature values (the color green) is uniquely assigned to the target object, the distractors present little interference and therefore task performance is invariant to the number of distractors. This condition is therefore sometimes referred to as \"popout\" search, as the target immediately stands out from the distractors, and the task can thus be performed rapidly without the need for serial processing. Conversely, in conjunctive search (depicted in the middle of Figure 1), there are two types of distractor objects that each share one feature with the target (e.g., half of the distractors are red L-shapes and the other half are green T-shapes). In this case, the target (a green L-shape) possesses no unique feature that easily distinguishes it from the distractors, leading to a significant degree of interference between the distractors and the target. One way to mitigate this is the use of serial search to identify the target. This is suggested by ubiquitiously observed increases in reaction time as a function of the number of distractors, as well as the observation that when participants are prevented from engaging in serial search (e.g., by forcing participants to respond quickly), task performance degrades rapidly as more objects are added to the scene."}, {"title": "Methods", "content": "We tested the extent to which VLMs demonstrate similar capacity constraints to humans in visual search tasks. We evaluated four multimodal language models \u2013 GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5 \u2013 on a task involving disjunctive and conjunctive search conditions. We generated datasets involving either 2D sprites or 3D scenes created in Blender [6] (similar to those found in the CLEVR dataset [13]). The datasets were designed to evaluate the ability of the model to detect the presence of a target object among multiple distractors. In half of the images, a target was present, while in the other half, no target was present.\nEach image contained between 4 and 50 distractors. For the disjunctive search task, these consisted of non-overlapping red circles (for the 2D dataset) or green spheres (for the 3D dataset) of a uniform size. Half of the images additionally contained a target object, which was a green circle (for the 2D dataset) or a red sphere (for the 3D dataset). For the conjunctive search task, the 2D dataset consisted of images in which the distractors were either red L-shapes or green T-shapes (randomly selected with equal probability). Half of the images additionally contained a target object, which was a green L-shape. The 3D dataset consisted of images in which the distractors were either green spheres or red cubes (randomly selected with equal probability). Half of the images additionally contained a target object, which was a red sphere. Each of the datasets (2D disjunctive, 2D conjunctive, 3D disjunctive, 3D conjunctive) contained 1000 images."}, {"title": "Results", "content": "We measured the performance of each model by calculating, for each condition, how detection accuracy varied as a function of the number of distractors. The results indicate that performance in the disjunctive search (i.e., popout) condition was perfect, and invariant to the number of distractors. That is, regardless of the number of distractors, all models achieved perfect accuracy in this condition.\nIn contrast, in the conjunctive search condition performance was inversely related to the number of objects: for 5 objects, all models displayed an accuracy of ~90%, but as the number of objects increased, performance dropped substantially. These results were consistently observed for both the 2D (top panel of Figure 1) and 3D (bottom panel of Figure 1) datasets. These results were also replicated in an alternative version of the disjunctive search task, in which target and distractor colors were varied between trials (Supplementary Figure 7).\nThe results of these experiments suggest that multimodal language models demonstrate human-like capacity constraints in their ability to perform visual search in multi-object settings. It is important to emphasize that these capacity constraints are not driven solely by the number of objects present within a scene. Like humans, these models demonstrate capacity constraints only in the task conditions that are impacted by interference between the target and distractor objects, consistent with the hypothesis that these capacity constraints arise as a consequence of the binding problem."}, {"title": "Numerical Estimation", "content": "To assess the generality of the human-like capacity constraints observed for VLMs in visual process- ing, we investigated a simple numerical estimation task (i.e., counting) that has been widely studied in cognitive psychology. Although human observers can precisely count a very large number of items when allowed to explicitly process those items one at a time, their ability to rapidly estimate the number of items in a display is subject to a severe capacity constraint. Studies have found that the number of objects that can be reliably estimated without explicit serial counting (sometimes referred to as \"subitizing\u201d) is somewhere between 4 and 6 [14, 17, 19, 27, 34]. To determine whether VLMs are subject to similar constraints, we evaluated both multimodal language models (GPT-4v, GPT-40, Gemini Ultra 1.5, Claude Sonnet 3.5, and Llava 1.5) and text-to-image models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse) on a numerical estimation task involving variations of both the number and type of objects. We found that VLMs, across a variety of stimulus and model types, display strikingly similar quantitative capacity limits to those observed in human vision. We also found that these capacity constraints were strongly affected by the variability of features present in an image. This effect is consistent with the hypothesis that these constraints arise due to representational interference: given that objects are represented with a shared set of representational resources, greater feature variability leads to less overlap in the use of these resources, and therefore less opportunities for interference and binding errors."}, {"title": "Methods", "content": "We generated datasets involving both 2D sprites and 3D objects, varying the number of objects per image between 1 and 20. We explored four conditions with varying levels of feature entropy (i.e., feature variability): a low-entropy condition in which all objects in an image had the same color and shape; two medium-entropy conditions in which all objects in an image had the same shape but unique colors, or vice versa; and a high-entropy condition in which all objects in an image had unique colors and shapes. We prompted the multimodal language models to describe the image and then state the number of objects present in it. To test the text-to-image models, we generated a dataset comprising 100 distinct categories, evenly split between common foods (50 categories) and animals (50 categories). We tasked these models with generating images from each category, for which the number of instances of each object ranged from 1 to 10. To assess their ability to generate images with the exact number of objects requested, we conducted a human evaluation study. Participants were asked to count and report the number of objects visible in each generated image. The collected human judgments were then used to quantify the model's accuracy. See Appendix C for further details on human evaluations."}, {"title": "Results", "content": "We measured performance by calculating, for each condition, how accuracy varied with the number of objects present in the scene. The results indicated that, regardless of the type of stimuli used (2D vs. 3D shapes, or animals vs. food), and across two fundamentally different types of vision language model (multimodal language models and text-to-image models), VLMs displayed human-like capacity limits (Figure 2). For both multimodal language models and T2I models, accuracy was very high for scenes involving a relatively small number of objects (1-5), but dropped sharply for scenes involving 6 or more objects. Moreover, the multimodal language models exhibited performance consistent with our hypothesis that capacity limits arise due to representational interference across objects (i.e., the binding problem), with overall performance highest in the high-entropy condition (lowest interference), lowest in the low-entropy condition (highest interference), and intermediate in between these two extremes in the medium-entropy conditions. Though there are slight differences between the capacity limits exhibited by these two classes of models, it is striking that they both fall within the subitizing limit of human vision, especially when considering the significant differences in both architecture and training procedures. Furthermore, the effect of feature entropy on these capacity"}, {"title": "Scene Description", "content": "Theoretical accounts of the binding problem [9, 33] posit that capacity limits in rapid visual processing arise as a consequence of interference between representations. Given a scene containing multiple objects, and a set of shared features with which to represent those objects, the likelihood of interference will tend to increase as a function of the number of objects in the scene (without the availability of a mechanism for binding features together, e.g., serial processing). However, as emphasized in our experiments on visual search and numerosity estimation, interference is not driven solely by the number of objects, but is also strongly influenced by the specific feature conjunctions present within a scene.\nWe developed a novel scene description task to further investigate the extent to which VLM per- formance is driven by representational interference. The task is illustrated in Figure 3a. For each image, the likelihood of representational interference was quantified as the number of feature triplets present in that image. A feature triplet is defined as any set of three objects for which one pair shares a feature, and another pair shares a different feature. For instance, {green X, green triangle, yellow triangle} is a feature triplet, because the feature 'green' is shared by two objects (the green X and the green triangle), and the feature 'triangle' is shared by two objects (the green triangle and the yellow triangle). Without the ability to accurately bind these features together at the level of objects, such feature triplets create opportunities for representational interference, and thus lead to illusory conjunctions. For instance, the feature triplet {green X, green triangle, yellow triangle} may lead to the erroneous identification of a yellow X. We studied the extent to which the presence of such feature triplets can account for scene description performance in VLMs."}, {"title": "Methods", "content": "As in the previous tasks, we generated datasets involving either 2D sprites or 3D objects. Each scene contained a variable number of objects (10-15 objects for the 2D dataset and 8-12 objects for the 3D dataset), and we systematically varied the number of feature triplets present in each scene. For example, the scene depicted in Figure 3a contains three feature triplets (illustrated by the dashed lines). VLMs (GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5) were prompted to provide a description of the objects in JSON format (see Appendix B for more details). We also generated prompts describing similar scenes (but involving real-world objects) and tested the ability of the T2I models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse) to accurately generate these scenes (as assessed by human evaluation; see Appendix C for more details). To obtain a representative sampling of scenes with different triplet counts, we systematically varied the diversity of colors and shapes across trials. This approach ensured adequate sampling of trials with different feature combinations and their associated triplet counts. To ensure reliable performance estimates, we excluded from analysis any triplet counts represented by fewer than 20 trials across all conditions. For the T2I experiments, we additionally excluded trials where models generated more than three extraneous objects not specified in the prompt, as these represented significant deviations from the intended scene structure."}, {"title": "Results", "content": "We measured scene description performance by calculating how the number of errors (quantified as the edit distance between the true description of the scene and the model's description of the scene) varies as a function of the number of objects present in the scene, and the number of feature triplets. The results (Figure 3) confirmed our prediction that performance should vary as a function of the number of triplets. Across multiple stimulus types (2D and 3D objects), and model types (both multimodal language models and text-to-image models), the largest number of errors occurred in the trials where the risk of binding errors was highest (i.e., the trials with the largest number of feature triplets), consistent with the hypothesis that errors would be driven primarily by the formation"}, {"title": "Visual Analogy", "content": "An open question in studying the performance of VLMs is the extent to which these models can solve analogical reasoning tasks. These tasks are of particular interest given their centrality in human higher-order cognition [11] and their use as measures of human intelligence [30]. Recent work has demonstrated that LLMs have an impressive ability to solve a range of text-based analogical reasoning tasks [37], but initial tests of VLMs have suggested that they often struggle to solve comparable visual forms of these tasks, sometimes performing well below human participants [20, 38].\nThis leads to the question of why, given the success of LLMs on text-based problems in this domain, VLMs do not display comparable success in solving analogy tasks. One possible explanation,"}, {"title": "Methods", "content": "We generated 200 trials from a simple relational match-to-sample (RMTS) task (Figure 4) using the same 2D sprite stimuli from the previous experiments. We selected a subset of 8 easily recognizable shapes and colors to generate a set of 64 stimuli. For each trial, we chose the source pair by sampling two objects that shared at least one of the two feature dimensions. We then selected the target pairs by sampling two pairs of objects: one which matched the source pair exactly along its relations (the correct target) and one which shared only one of the relations with the source pair (the incorrect target). We manipulated visual processing demands by investigating two conditions, one in which the source and target pairs were presented in a single image (the \u201cunified\u201d condition), and one in which the source and target pairs were presented as separate images presented in sequence (the \"decomposed\" condition), thereby reducing the chance of binding errors.\nWe assessed the performance of four VLMs (GPT-4v, GPT-40, Gemini Ultra 1.5, and Claude Sonnet 3.5) in four tasks: identification of the correct target pair in the full RMTS task (Analogy), decoding of single features from specific individual objects (Single Feature Decoding Task), comprehensive decoding of all features in a given problem (Full Feature Decoding), and decoding of relations between object pairs (Relation Decoding)."}, {"title": "Results", "content": "We found that performance on this task was highly variable aross VLMs, with some models (Claude Sonnet) showing nearly perfect performance on all tasks (Table 3) and other models (Gemini Ultra) showing poor performance on most tasks (Table 4). These results are consistent with recent work showing mixed success on visual analogy problems [38], and at odds with work claiming that VLMs have no capacity for visual analogy [20]. Interestingly, we also found that many models also struggled on more basic tasks such as identifying the features of the objects present in the image, or identifying the relations for individual pairs of objects."}, {"title": "Discussion", "content": "We have presented a series of experiments aimed at understanding the limits of vision language models in processing multi-object scenes. Our results suggest that these limitations can all be understood as arising from an inability to manage the binding problem, a fundamental problem associated with compositional coding identified by classic work in cognitive science [10, 33].\nRecent theoretical work has formalized this problem within a normative framework [9], sug- gesting that it arises due to a tension between the learning of compositional representations, and the shared use of such representations to encode multiple objects at the same time. To illustrate this, consider two different schemes for representing multi-object scenes: a conjunc-"}, {"title": "Limitations & Future Directions", "content": "This study has several limitations that should be considered when interpreting the results. First, we limited our analysis to a relatively small set of tasks. The tasks in our study were selected to illustrate the different settings in which the binding problem may impact performance, while grounding our analysis in well known tasks from cognitive science that have been used to index such capacity constraints in humans. Future work may examine a broader set of tasks such as matrix reasoning tasks [3, 26, 39] that are more diagnostic of the reasoning failures arising due to issues with binding. Second, we primarily investigated propietary VLMs, for which we do not have detailed knowledge of the training data or architecture, or the ability to directly investigate internal representations. We chose to focus on these models because they reflect the best-performing current set of VLMs (our experiments with the open-source Llava-1.5 yielded very poor performance on all tasks), but continued progress in the development of open-source VLMs should make it possible to investigate open-source models in future work. Finally, our work is focused particularly on characterizing the capacity constraints of VLMs arising due to issues with feature binding. While we propose a naive approach for improving performance by selectively processing sub-images independently, future work may explore more flexible methods for decomposing complex, multi-object reasoning tasks, especially by exploiting methods for object-centric representation learning [4, 5, 10, 16]."}, {"title": "Appendix: Prompts for Vision-Language Model Experiments", "content": ""}, {"title": "Numerical Estimation", "content": "You are presented with an image containing several objects. Your task is to accurately count the number of objects in the image. Follow these instructions carefully:\n1. Begin by describing each object in the image.\n2. Conclude your response by providing the total count of objects as an integer enclosed in square brackets. Only the number should be enclosed in square brackets."}, {"title": "Visual Search", "content": "You are presented with an image containing several shapes. Your task is to determine if there are any green shapes in the image. Follow these steps carefully:\n1. Describe each shape in the image, noting their color.\n2. Conclude your response by stating (True) if there are any green shapes, or ( False) if there are none. Enclose your final answer in square brackets, as shown."}, {"title": "Scene Description", "content": "The following image contains multiple simple, colored objects.\nThe possible shapes that may be present in the image are: <airplane, triangle, cloud, X-shape, umbrella, pentagon, heart, star, circle, square, spade, scissors, infinity check mark, right-arrow>.\nThe possible colors that may be present in the image are: <red, magenta, salmon green, lime, olive, blue, teal, yellow, purple, brown, gray, black, cyan, orange >.\nDescribe each object in the image in the form of a JSON object detailing the color and shape of each item.\nYou must answer only with the json array of objects, without any additional information or text.\nFor example, if the image contains a purple check mark, two green scissors, one orange right-arrow, and a teal infinity sign you would write:\n[\n{\"shape\": \"check mark\", \"color\": \"purple\"},\n{\"shape\": \"scissors\", \"color\": \"green\"},\n{\"shape\": \"scissors\", \"color\": \"green\"},\n{\"shape\": \"right-arrow\", \"color\": \"orange\"},\n{\"shape\": \"infinity\", \"color\": \"teal\"}\n]"}, {"title": "Relational Match to Sample (RMTS)", "content": "The following image depicts a trial of a relational match to sample task with two features: shape and color.\nThere are three pairs of objects relevant to your task: the source pair (the top pair of objects), target pair #1 (the pair of objects on bottom left), and target pair (the pair of objects on the bottom right).\nNow, given the source pair and the two target pairs, identify the matching target pair. To accomplish this task, you can use the following steps:\n1. Identify the features of the objects in each pair (i.e. shape and color).\n2. Identify the relations over the features of each pair.\n3. Determine which target pair shares the same relations with the source pair - exactly one target pair will share the same relations with the source.\n4. Conclude with the integer of the correct target pair wrapped in square brackets. For example, if target pair #1 matches the source pair, return [1]."}, {"title": "NeurIPS Paper Checklist", "content": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: All claims made in the abstract and introduction are supported by behavioral evaluation or models and comparisons to previously published work in cognitive science.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper."}, {"title": "Limitations", "content": "Question: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: We added a dedicated limitations paragraph to the Discussion (Section 6).\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations."}, {"title": "Theory Assumptions and Proofs", "content": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\nAnswer: [NA]\nJustification: Our work does not involve any novel assumptions that require formal justifica- tion.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced.\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced."}, {"title": "Experimental Result Reproducibility", "content": "Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: All details regarding reproducing our experiments and controls are thoroughly described in the methods sections in the text and the Appendix. Furthermore, code for reproducing our experiments in the zip file attached to the OpenReview submission.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results."}, {"title": "Open access to data and code", "content": "Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material?\nAnswer: [Yes]\nJustification: All experiment code is provided in the submission's attached zip file. We will provide a documented version of the code upon publication.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.\n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \"No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted."}, {"title": "Experimental Setting/Details", "content": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\nAnswer: [NA]\nJustification: All methods for testing models is specified in the text and Appendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material."}, {"title": "Experiment Statistical Significance", "content": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We report error bars and significance tests for all experiments.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions)."}, {"title": "Experiments Compute Resources", "content": "Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\nAnswer: [Yes]\nJustification: Details on compute are listed in the Appendix (Section C).\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper)."}, {"title": "Code Of Ethics", "content": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We reviewed the Code of Ethics and confirm that this paper conforms with it.\nGuidelines:\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction)."}, {"title": "Broader Impacts", "content": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\nAnswer: [Yes]\nJustification: These are discussed in Section 6.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations."}, {"title": "Safeguards", "content": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g.", "datasets)?\nAnswer": ["NA"], "nJustification": "The paper poses no such"}]}