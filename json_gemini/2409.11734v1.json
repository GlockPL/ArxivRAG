{"title": "InverseMeetInsert: Robust Real Image Editing via Geometric Accumulation Inversion in Guided Diffusion Models", "authors": ["Yan Zheng", "Lemeng Wu"], "abstract": "In this paper, we introduce Geometry-Inverse-Meet-Pixel-Insert, short for GEO, an exceptionally versatile image editing technique designed to cater to customized user requirements at both local and global scales. Our approach seamlessly integrates text prompts and image prompts to yield diverse and precise editing outcomes. Notably, our method operates without the need for training and is driven by two key contributions: (i) a novel geometric accumulation loss that enhances DDIM inversion to faithfully preserve pixel space geometry and layout, and (ii) an innovative boosted image prompt technique that combines pixel-level editing for text-only inversion with latent space geometry guidance for standard classifier-free reversion. Leveraging the publicly available Stable Diffusion model, our approach undergoes extensive evaluation across various image types and challenging prompt editing scenarios, consistently delivering high-fidelity editing results for real images.", "sections": [{"title": "1. Introduction", "content": "In recent years, advancements in image editing techniques, particularly those utilizing text-guided diffusion models, have made significant strides in real-world image editing. Our work introduces a new image editing method and framework that offers exceptional control and flexibility.\n The diffusion model has been instrumental in advancing the generation of complex and detailed images. This model stands out for its ability to incorporate various types of information at each step of the image denoising process. One noteworthy implementation of this model is the text-to-image diffusion model, which excels at creating images that closely match natural language inputs, often referred to as 'text prompts.' Control techniques, developed based on these models, play a crucial role in enhancing image quality and relevance to better meet user preferences. These techniques often involve modifying textual prompts to refine the direction of image generation or using tools like bounding boxes, masks, and sketches for precise spatial guidance.\n However, the application of these control techniques to real-world image editing is a relatively unexplored territory. One challenge arises from the inherent nature of diffusion models, which generate images from noisy Gaussian inputs. Identifying the corresponding noisy latent space that can accurately reconstruct real images has proven to be a formidable task. Although Denoising Diffusion Implicit Model (DDIM) helps convert real images into a suitable noisy latent space for text-to-image conditional diffusion models, image quality often suffers due to cumulative errors introduced by the basic ODE solver. This limitation hampers the direct application of established diffusion control techniques to practical image editing tasks.\n Another issue in inversion-based techniques is stability. While classifier-free sampling is known to produce higher-quality images, incorporating null text weights into DDIM inversion can lead to instability. Null-text inversion and similar methods optimize text embedding rather than the text conditioning component to achieve better reconstruction. However, this optimization is computationally expensive for each new image, and the reconstruction of unedited parts remains unstable after inserting complicated text prompts.\n To address these challenges, we introduce a novel concept called the 'geometric accumulative loss' for inversion. This loss leverages predicted image direction from the starting point back to an intermediate step. Instead of relying solely on text-based noise prediction, our geometric accumulative loss incorporates classifier-free guidance for the initial approximation. This approach capitalizes on the insight that achieving a precise and stable inverse path under classifier-free guidance for any real image would enhance editing stability.\n The geometric accumulative loss finds a balance between these considerations by considering the input-encoded image latent as a reference point for fitting predictions, aiding the inversion process in retaining the geometric features of the input image. Our method allows for preliminary pixel-"}, {"title": "2. Related Work", "content": "Text Condition Image Generative model The field of condition generative models has undergone a remarkable transformation, marked by a shift from early GAN-based and VAE-based models to the more sophisticated diffusion-based models. Initially, GAN based methods [4, 16, 30] set the foundation, offering impressive diversity and quality in image generation, yet they often fell short in accurately translating complex textual descriptions into images. This gap was significantly bridged by diffusion-based models [5, 11\u201313, 23, 25\u201328]. These newer models excel in synthesizing photorealistic images that closely describe user prompts, thanks to advancements in deep learning and their training on extensive data sets. Beyond their enhanced fidelity to text prompts, these models have also expanded the horizons of generative applications, finding use in diverse fields such as 3D modeling, novel view synthesis, and even music generation. This evolution not only represents a leap in the technical capabilities of text-to-image synthesis but also reshapes the potential of artificial intelligence in creative domains, providing tools that can transform text into vivid, accurate visual representations with unprecedented ease and flexibility, thereby setting new standards and opening new possibilities in the realm of digital art and beyond.\n Diffusion Model for Image Editing In recent years, the development of diffusion models has introduced a more flexible design space for image editing tasks compared to Generative Adversarial Networks (GANs). These diffusion models offer a simpler training setup, exemplified by methods like SDEdit [17] and ILVR [2].\n Some approaches, exemplified by Textual Inversion [7] and Dream-Booth [22], have showcased their proficiency in generating diverse images characterized by unique object attributes. They achieve this by fine-tuning diffusion models using multiple images, effectively identifying the inverse semantic latent representations that capture the distinct characteristics of objects in the embedding space. Similarly, Imagic[14] and UniTune[29], leveraging the powerful Imagen model [43], have showcased impressive editing performance. However, a common limitation of these methods lies in their requirement for restrictive fine-tuning of pre-trained models, which can hinder their ability to fully harness the generalization potential of the models, often leading to issues such as overfitting or language drift.\n Another category of methods, such as those represented by [1, 20], rely on user-provided masks to guide the diffusion process. While effective, this requirement for user-provided masks can limit the interactivity of these methods.\n In the pursuit of text-only interactive editing, recent developments have given rise to optimization-free methods. For instance, Prompt-to-Prompt[8] and DiffEdit[3] have been proposed to automatically infer masks before initiating the editing process.\n Prompt-to-Prompt (PTP) [8] achieved comprehensive text-guided image editing without the need for diffusion model refinement. This method encompassed local editing even without a predefined mask. However, PTP primarily focused on generated image editing, asserting that the straightforward step-by-step inversion process is less reliable for real images, particularly when employing larger classifier-free guidance scales. Null-text inversion (NTI) [19] introduced a strategy involving the modification of constant null-text embeddings into image-specific optimal embeddings. This approach was designed to achieve precise image reconstruction and subsequently applied Prompt-to-Prompt (PTP) techniques for real image editing. Its successor, Negative Prompt Inversion [18], attempted to expedite the null-text tuning phase of NTI for faster editing. However, both approaches essentially address the same optimization target, inheriting the constraints associated with null-text inversion. These constraints demand careful word selection in editing prompts, potentially limiting robustness.\n In our method, we adopt an inversion-based technique for image editing while incorporating an image prompt derived directly from user-level pixel editing. We demonstrate"}, {"title": "3. Method", "content": "Our objective is to take a real-world image, referred to as I, and edit it to meet user specifications, yielding a modified image I*. Additionally, as the editing requirements are not rigid and should allow for creative generativity, our method aims to produce a set of images that adhere to these editing criteria, collectively labeled as {I* | i = 1, 2, . . ., n}."}, {"title": "3.1. Background and Preliminaries", "content": "Text-guided Latent Diffusion Models. Consider a text-guided diffusion model that initiates with a textual embedding C and a random Gaussian noisy image, denoted as IT. Note that C = T(P) is the embedding of text prompt P in natural language projected by the text encoder T and $I_T \\in \\mathbb{R}^{H\\times W\\times C}$ is characterized by Gaussian i.i.d pixel values.\n The goal of the diffusion model is to progressively denoises the image, resulting in a sequence IT, \u0406T\u22121,..., I0 such that Io corresponds to the text prompt P. But to allow the diffusion model to operate on a more compact representation, which reduces both time complexity and memory usage, Latent Diffusion Models (LDMs) [21] uses an encoder $\\mathcal{E}$ to map a given image I into a latent embedding z. Subsequently, a decoder $\\mathcal{D}$, is employed to reconstruct the input image from z, such that $\\mathcal{D}(\\mathcal{E}(I)) \u2248 I$. Therefore, we only need to replace the image I with its latent embedding in the follwing algorithm.\n zt is a sample with added standard Gaussian noise \u03f5, where the noise is introduced according to a time-dependent schedule at. The relation is given by $z_t = \\sqrt{a_t}z_0 + \\sqrt{1 \u2013 a_t}\\epsilon$, corresponding to a forward diffusion process:\n$q(x_t|x_{t-1}) := N(x_t; \\sqrt{a_t}x_{t-1}, (1 \u2212 a_t)I)$ (1)\nTo learn the reverse process, the mean value $\u03bc_\u03b8(xt,t)$, parameterized by \u03b8, is predicted:\n$P_\u03b8(X_{t-1}|X_t) := N(x_{t-1}; \u03bc_\u03b8(xt, t), \u03a3_\u03b8)$ (2)\nA denoiser module, parameterized by the network $\u03b5_\u03b8$, is utilized for fitting the objective, which is equivalent to fitting the mean value prediction in the reverse process:\n$\\min_\u03b8 \\mathbb{E}_{z_0,\\epsilon \u223c \\mathcal{N}(0,I), t \u223c \\mathcal{Uniform}(1,T)}||\\epsilon - \\epsilon_\u03b8(z_t, t,C)||^2$. (3)\nDuring the inference phase, the model can employ either stochastic sampling as in DDPM [10], which introduces noise at each sampling step, or deterministic sampling as in DDIM [24], which follows an ODE-like deterministic trajectory. Specifically, we utilize the deterministic DDIM sampling approach to leverage the inverse characteristics of the ODE path:\n$z_{t-1} = \\frac{\\sqrt{a_{t-1}}}{\\sqrt{a_t}}z_t + (\\frac{1}{\\sqrt{a_t}}\\sqrt{1-a_t}) \\epsilon_\u03b8 (z_t, t, C) - (\\frac{\\sqrt{a_{a-1}}}{\\sqrt{a_t}} \\sqrt{1-a_t})\\epsilon_\u03b8 (z_t, t, C)$ (4)\nDDIM inversion. The challenge in diffusion-based inversion lies in transforming a real image into its Gaussian noisy"}, {"title": "3.2. Insert Meet Inverse", "content": "The task of editing images in pixel space has a well-established history with conventional methods, offering greater flexibility. This is due to the ability to employ pixel-level operations such as manual drawing, pasting, layer masking, merging, and smoothing. To this end, we propose GEO, in which we suggest that pixel-level editing can be efficiently integrated into the noisy latent space using a diffusion model-based inversion technique. This integration is then reversed back into the real image pixel space, resulting in more natural and semantically enhanced pixel-level edits."}, {"title": "3.2.1 Insert in Pixel Space: Editing Proposal in Pixel Space", "content": "We suggest various editing methods capable of creating initial editing proposals in pixel space that match the user-provided text prompts. Our method uniquely avoids modifying the text encoder [6] and cross-attention components in U-net [8, 18, 19]. Consequently, there are no restrictions on the length or content of the text prompts, unlike methods based on attention mixture.\n In the context of image editing, our approach begins with an unaltered real-world image, denoted as I. The user is then asked to provide two distinct text prompts: P, which aims to describe the original image I as accurately as possible, and P*, which outlines the desired modifications. Subsequently, the user can engage in pixel-level editing, employing a variety of elemental operations:\n Brush Stroke: This function enables users to select specific regions within the image for editing. It is particularly useful for tasks such as altering colors or swapping backgrounds. The brush stroke tool offers a straightforward and intuitive interface for these modifications, allowing for precise control over the editing area.\n Image Paste: Users can also incorporate elements from external sources into the image I. This involves selecting an object from a separate image and seamlessly integrating it into the original picture. The merging process can either be executed manually, involving steps like image mask cropping and layer overlapping, or automated through advanced mask segmentation tools, such as Segment Anything [15]. This feature is instrumental in enhancing the creative flexibility of the editing process, enabling users to blend elements from diverse sources effectively.\n SDEdit: The Stochastic Differential Equation (SDE) editing method is particularly adept at generating intricate object details or styles that are challenging to manually draw or derive from other images. Initially, the input image is converted into a latent space representation through an encoder, denoted as z = $\\mathcal{E}$(I). This latent representation, z, is then subjected to the addition of independent standard Gaussian noise, expressed as $z_t = \\sqrt{a_t}z + \\sqrt{1 \u2013 a_t}\u03f5$. The denoising process commences not from the initial step T but from an intermediate step t. This approach constitutes a stochastic inversion process that does not reliably reconstruct z from zt when t is close to T. Consequently, the reversed image, while not directly suitable as an output for editing results, can be superimposed onto the original image I with appropriate opacity to provide visual guidance from the pixel space.\n It is important to note that these three operations \u2013 Brush Stroke, Image Paste, and SDE Editing \u2013 can be combined in various ways to achieve more refined and tailored editing results. This flexibility allows for an extensive range of creative possibilities, offering users the ability to fine-tune their edits to align closely with their vision and the guidance provided by the text prompts."}, {"title": "3.2.2 Inverse in Latent Space: Geometric Accumulation Inversion", "content": "A distinguishing feature of the denoising process is evident from Eq.(2), which enables a direct estimation of zo from the model's denoising direction $\u03b5_\u03b8$:\n$z_0 = \\frac{1}{\\sqrt{a_t}}(z_t - \\sqrt{1 \u2013 a_t}\u03b5_\u03b8(zt, t))$ (7)"}, {"title": "4. Applications", "content": null}, {"title": "4.1. Experiment Set Up", "content": "In our experimental setup, we employ the text-conditional Latent Diffusion Model, also referred to as Stable Diffusion, which has been trained with 890 million parameters on the LAION-5B dataset at a resolution of 512 x 512. For the DDIM schedule, we adhere to a regimen consisting of 50 steps while retaining the original hyperparameter settings of Stable Diffusion. Notably, our inversion procedure can be completed within a duration of one minute and thirty seconds when executed on a single A100 GPU."}, {"title": "4.2. Object Editing/ Style Editing", "content": "As depicted in Figure 2, our approach initiates the editing process by first modifying the input real image at the pixel level, utilizing it as an image prompt. Subsequently, we provide the user with guided, unlimited edit prompts in the form of natural language guidance for our inversion procedure. Our method excels at seamlessly swapping objects within an image, transitioning from one object, such as a 'cat,' to another, like a 'tiger.' Furthermore, our method demonstrates versatility in style editing, capable of adjustments at both local and global scales.\n For instance, consider the 'A cat on the beach' image. After introducing the 'oil painting' guidance in the edit prompt, the style of the entire image is successfully transformed. This transformation smoothly merges the roughly brushed strokes with the remaining photorealistic portions.\n It is noteworthy that our method not only accurately captures color information in pixel editing but also possesses the ability to discern correct semantic information from rough or ambiguous images. This capability is exemplified in the second example, where we transform an initially unrealistic 'smiling woman' image into a 'crying woman.' We achieve this by generating an initially unconvincing crying woman's face using SDEdit, which we then superimpose onto the original input image. We accomplish this by segmenting out the woman's face and adjusting the opacity of the overlaid face as an image prompt hint. Although the input image prompt may appear unrealistic to human observers, our method, when provided with the appropriate guidance, accurately steers the inversion process toward the correct semantic interpretation of 'crying.'\n While conducting local edits, our method excels in preserving the background due to its specific geometric accumulative design, which retains the geometric information of the original input image. In contrast, Null-text Inversion can sometimes maintain the background but lacks the fine control exhibited by our approach. Additionally, Null-text Inversion is sensitive to the prompt, as observed in the 'gundam' case where the background undergoes a complete transformation. In other cases, such as 'two cats' and 'A cat on the beach,' the cat's appearance is semantically accurate but unsuitable and unreal. SDEdit generates images with more stable styles; however, its level of detail is less conducive to robust editing applications."}, {"title": "4.3. Multiarea Editing", "content": "As evident from the examples presented in Figure 3, our method incorporates an image prompt that offers flexible editing proposals derived from the image space, allowing for multi-area editing. For instance, this capability enables simultaneous editing of multiple regions within a single image, such as distinct materials for two cats and background, as well as the addition of glass and cloth elements. This addresses a long-standing issue prevalent in CLIP-based text-guided models, namely, the problem of contamination.\n For illustration, consider the editing result of the 'silver sculpture cat and golden sculpture cat' example using null-text inversion. In this case, the presence of the word 'silver' inadvertently affects both cats, rather than producing two cats composed of separate materials. We contend that the self-amplification effect inherent in text-conditioned models can be harnessed to enhance the semantic information conveyed by the image prompt.\n In other words, while the initial manual editing originating from the pixel space may fall short of achieving a realistic and natural appearance, during the inversion process, as we progress to an intermediate step and subsequently reverse our modifications, the less desirable aspects of the image prompt can be rectified under the guidance of the editing prompt. This correction process allows us to refine and align the image prompt more closely with the intended editing prompt."}, {"title": "4.4. Customize Editing", "content": "Another advantageous aspect of incorporating the image prompt is its ability to offer users greater flexibility in customizing features such as precise color matching or seamlessly inserting objects from other images into the edited image. This flexibility is demonstrated in Figure 4, where our method excels at preserving the user-provided colors specified in the image prompt. Notably, it seamlessly integrates the Joker mask from a film poster onto a human face without distorting the original facial details. In contrast, both SDEdit and NTI introduce distortions to the facial features, whereas our results maintain the natural appearance of the original face."}, {"title": "5. Ablations", "content": "Our method initializes the Geometric Accumulation Inversion using the DDIM backward step. To evaluate the effectiveness of our proposed loss, we conducted ablation experiments as shown in Figures 5 and 6.\n In Figure 5, we focus on the preservation of background details. It is evident that without our loss, the details of the hand and the donut in the background easily distort during the reverse generation process. However, when our loss is applied, the hand remains faithful to its original appearance after the reverse generation.\n In Figure 6, we delve into the impact of the predicted latent space, visualized as a blurred image in the first row. It becomes clear that without proper preservation of shape information in the blurred image path, the resulting reconstructed image deviates from the desired outcome aligned with the guidance provided by the text prompt. Our method enhances the geometric memory within the blurred image path, resulting in a reconstructed image of higher quality that better corresponds to the editing prompt."}, {"title": "6. Limitations", "content": "Our current image prompt design relies on manual adjustments, which can introduce errors during pixel space masking. Additionally, the manual nature of image prompt creation limits the scalability of our method for batch processing, impacting quantitative metrics."}]}