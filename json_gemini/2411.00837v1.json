{"title": "Longitudinal Mammogram Exam-based Breast\nCancer Diagnosis Models: Vulnerability to\nAdversarial Attacks", "authors": ["Zhengbo Zhou", "Degan Hao", "Dooman Arefan", "Margarita Zuley", "Jules Sumkin", "Shandong Wu"], "abstract": "In breast cancer detection and diagnosis, the longitudinal\nanalysis of mammogram images is crucial. Contemporary models excel\nin detecting temporal imaging feature changes, thus enhancing the learn-\ning process over sequential imaging exams. Yet, the resilience of these\nlongitudinal models against adversarial attacks remains underexplored.\nIn this study, we proposed a novel attack method that capitalizes on the\nfeature-level relationship between two sequential mammogram exams of\na longitudinal model, guided by both cross-entropy loss and distance\nmetric learning, to achieve significant attack efficacy, as implemented us-\ning attack transferring in a black-box attacking manner. We performed\nexperiments on a cohort of 590 breast cancer patients (each has two se-\nquential mammogram exams) in a case-control setting. Results showed\nthat our proposed method surpassed several state-of-the-art adversarial\nattacks in fooling the diagnosis models to give opposite outputs. Our\nmethod remained effective even if the model was trained with the com-\nmon defending method of adversarial training.", "sections": [{"title": "1 Introduction", "content": "Mammography-based AI is at the forefront of medical AI research and many AI\nproducts are being translated for clinical deployment. The cyber-security of such\nmodels is therefore becoming a paramount need to ensure AI integrity. Although\nmedical IT system is relatively closed, but cyber-attacks still occur, quite often,\nthrough infiltration of the IT systems [1] or by internal hackers [2]. For example,\nhospitals had to pay ransom when their data are hacked [3]. In fact, 94% of U.S.\nhospitals are affected by healthcare data breaches [4], showing high vulnerability\nand risks to adversarial attacks. As elucidated in [5], adversarial attacks can\noccur with real world bad intentions for data-based ransom, insurance fraud,\nclinical trial effect manipulation, etc."}, {"title": "2 Related Work", "content": "Adversarial Attack Methods: Szegedy et al [11] showed that classifiers may\nconfidently make incorrect predictions when subjected to imperceptible pertur-\nbations. Kurakin et al. [12] showed that classifiers remain vulnerable to adver-\nsarial samples even in the physical world. Goodfellow et al. [13] propose FGSM,"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Task and Model Architecture", "content": "Task. In this study, we implemented our novel method on the task of diagnos-\ning breast cancer on mammograms (i.e., classification of cancer cases vs. normal\ncontrols). The diagnosis can be based on a single time-point exam or multi-\nple longitudinal exams that leverage temporal information to improve diagnosis\naccuracy. Thus, we designed two models: Source model (using a single time-\npoint mammogram exam, denoted as Current) and Target model (using two\nsequential exams, denoted as Prior and Current, respectively, which are taken\nat two different time points). The Source model is useful when a patient does\nnot have Priors available. It should be pointed out that the Prior exams are nor-\nmal/healthy mammograms that are stored as historical data of a patient in the\nelectronic health records. The use of normal Priors in longitudinal models con-\nforms to radiologists' clinical practice to improve lesion detection and diagnosis\nin addition to using the Current exam.\nModel Architecture. As illustrated in Figure 1, in the Source model, features\nextracted by the backbone model are utilized for diagnosis, which also serve\nas the basis for generating adversarial samples to attack the target longitudi-\nnal model i.e., (the attack transferring). For the Target longitudinal model, the\nimaging features are derived from both the Prior and Current exams, referred\nto as Xprior and Xcurrent, respectively. The inputs to the cross-view module are\ncurrent exam Xcurrent and the changes between the prior and current exam,\ni.e., Xprior-current = Xprior - Xcurrent. Here Xprior-current aims to capture and em-\nphasize the changes between the two time points. Subsequently, both feature\nvectors are fed to the cross-view module. In this module, we use multi-head\nattention mechanisms to identify and stress the information in the subtracted\nfeature (xprior-current) that is relevant to the current feature (xcurrent). The core of\nmulti-head attention is a scaled dot-product attention[10] mechanism that com-\nputes attention scores between queries and keys, which are then used to linearly\ncombine the associated values. In our setting, we reshape the embedded feature\nmaps for the target phase (xcurrent Or Xprior-current) into a query matrix and re-\nshape the feature maps for the source phase into a key matrix. The outcome\nof this attention process produces Xcurrent-attention and X (prior-current)-attention.\nTo form new attention-enhanced feature vectors, we integrate these represen-\ntations with the original vectors, resulting in x(prior-current)-cross = Xprior-current +\nX(prior-current)-attention and xcurrent-cross = Xcurrent +Xcurrent-attention. These refined\nfeature vectors are then concatenated, serving for the ultimate prediction."}, {"title": "3.2 Adversarial Attack", "content": "Rationale: Our study aims to attack the Target model that makes diagnosis\nusing the temporal relationships/changes of the Prior and Current exams. In\nour black-box attacking scenario, attackers do not have access to the model\narchitecture and parameters of the Target model, particularly the mechanisms in"}, {"title": "Adversarial Sample Generation.", "content": "Let f(s) denote an arbitrary deep neural\nnetwork which takes s (s \u2208 R", "follows": "n$\n\u00a7 = \\underset{||s-s||p\u2264e}{argmax} l(s,t)\n$\n(1)\nwhere t and l(,) denote the label of x and the loss function used to train the\nmodel respectively. In our experiments, we investigate the effects of using the\ncross entropy loss and distance metric learning for attacking the Target model.\nTo optimize Equation (1), the Iterative Fast Gradient Sign Method (I-FGSM)\nis employed. Initially, the input x is scaled to the range into [-1,1] and we\nset xo = x. Then, we compute the gradients of loss function with respect to\ninput x. Following this, the adversarial samples undergo several iterations of\nupdates. In each step, we apply the sign function to the calculated gradients\nand then clip the updated adversarial samples to the range [-1,1] to maintain\nthem as valid images. At every iteration, the generated samples are designed to\ncross the decision boundary, directed by the gradient data, and are retained for\nsubsequent sampling. Ultimately, adversarial samples are generated by adding"}, {"title": "Knowledge-guided Adversarial Sample Selection Method.", "content": "As illustrated\nin Figure 2, the I-FGSM method can generate multiple adversarial samples aim-\ning at traversing the decision boundary. We propose a novel criteria to select\nmost impacting adversarial samples using distance metric learning. This criteria\nis designed based on the knowledge of: (1) the longitudinal model relies on the\nrelationship of Prior (always normal) and Current (could be cancer or normal)\nexams to make a diagnosis, and (2) the Euclidean distance between Prior and\nCurrent is smaller for a normal patient (because of less variation across two\nnormal exams), but larger for a cancer patient (because of larger variation from\nnormal transitioning to cancer development). Specifically the criteria differ be-\ntween Normal and Cancer cases: in Cancer cases, we select the adversarial sample\nPatienti,adv current that is closest from Patienti,prior, while in Control cases,\nwe choose the sample that is furthest to Patienti,prior. For Cancer cases, the ob-\njective is to coax the model into misclassifying the adversarial sample as Normal\nby leveraging the proximity to a Patienti,prior image labeled as normal. This\nproximity strategy enhances the likelihood that the longitudinal model, which\nrelies on temporal relationships for classification, erroneously assesses the sample\nas Normal. Conversely, for Normal cases, the intent behind selecting adversarial\nsamples with the greatest distance from the Prior image is to challenge the model\ninto a false diagnosis of Cancer. Given the substantial separation between the\nadversarial sample and a Prior image, the longitudinal model is more inclined\nto misclassify these as belonging to Normal cases. This novel adversarial sample\nselection approach takes advantages of the longitudinal model's dependencies on\nthe relationship between Patienti,prior and Patienti,current exams."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Study Cohort", "content": "This study received institutional review board approval and we used a dataset\nof 590 subjects in a case-control study setting, with 293 breast cancer cases and\n297 breast cancer-free controls (i.e., normal/negative). Each subject has a Cur-\nrent mammogram exam and a Prior exam taken at approximately 1 year apart.\nAll diagnosis outcomes are biopsy-proven and based on the Current exams. All\nPrior exams are normal/negative. For the cancer cases, we exclusively utilized\nthe biopsied breast for. For the controls, either the right or left breast was ran-\ndomly chosen for a subject to avoid modeling from shortcut learning. To ensure\nuniformity in the orientation of a breast, we applied a horizontal flip for a right-\nside breast to appear like a left-side breast. We specifically opted for using the\ncraniocaudal view of the mammogram images."}, {"title": "4.2\nImplementation Details", "content": "The dimensions of the input mammogram images were standardized to 350x400\npixels to maintain consistency across the subjects. We employed a Swin Trans-"}, {"title": "In addition, we implemented two more novel methods that used distance metric\nloss to alter the relationship between Prior and adversarial Current. Specifically,\nwe use the distance loss in Equation (2) as a penalty term aiming to more\nefficiently guide the search directions. We named this regularization method as\n'Distance Reg.'", "content": "(Equation (3)) as another way to combine the cross entropy\nloss and distance metric learning. We compared our proposed knowledge-guided\nmethod to this alternative method (A is experimentally determined as 0.05).\n$\n\u00a7 = \\underset {||s-s||p<e}{argmax} l(s, t) + A. loss_{distance} (s', t)\n$\n(3)\nWe utilized a patient-wise 5-fold cross-validation and Area Under the ROC Curve\n(AUC) reporting Mean AUC \u00b1 Standard Deviation (Std) of the various meth-\nods. We also assessed the model's performance after incorporating adversarial\ntraining as defence method, where the model underwent retraining with a combi-\nnation of clean data and adversarial samples generated by BIM with perturbation\nsize of 0.01 and batch size of 32. All computational tasks were performed on an\nNVIDIA TESLA V100 GPU, provided by our local supercomputing facility."}, {"title": "5 Results", "content": "As shown in Table 1, the Source and Target models show baseline AUCs of 0.670\nand 0.704, respectively. The Target model has a higher AUC than the Source"}, {"title": "model, indicating the usefulness of the use of Prior exams for diagnosis. With\nadversarial attacks, all methods lowered the AUCs to a range of 0.205 to 0.531,\nshowing all the attacks successfully fooled the model to give opposite (AUC<0.5)\nor random (AUC=0.5) diagnosis. Our proposed attack led to the lowest AUC of\n0.205, outperforming all the other compared methods.", "content": "Table 1 last column shows the effects of after incorporating adversarial train-\ning to retrain the Target models. It shows that the Target model achieved an\nAUC of 0.685 when testing with clean data, which is slightly lower than the AUC\n(0.704) without adversarial training - this shows a trade-off between increased\nadversarial robustness and slightly-decreased performance on clean data. As can\nbe seen, the Target model becomes more resilient to all the adversarial attacks\nas the AUC increased to higher values (range 0.548 to 0.634) compared to those\n(0.205 to 0.531) without adversarial training. This partially indicates the adver-\nsarial training is useful to mitigate the attacks to certain extent, but still, the\nmodel performance remains much lower than the normal performance (0.685),\nwhich means the attacks can still substantially fool the diagnosis model. Here,\nagain, our proposed attack method achieved the best attacking effects.\nThe results also show that both the cross-entropy loss (FGSM, I-FGSM),\nPGD and distance-guided learning (Distance-guided FGSM, Distance-guided I-\nFGSM) can independently degrade model performance. The Distance Regularization-"}, {"title": "based methods perform better than the distance-guided methods, but are less\neffective than our proposed approach - this is potentially due to that the distance\nmetric loss is not necessarily able to ensure that, in altering the relationship be-\ntween the adversarial Current and Prior, the adversarial Current will breach the\ndecision boundary, a task typically however can be influenced by cross-entropy\nloss. This suggests that our knowledge-guided sample selection method used to\ngenerate adversarial samples is a more effective method for attacks.", "content": "It should be noted that the generated adversarial samples are supposed to\nbe able to fool the Source model (even though the real intention is to attack\nthe Target model). This is verified in Table 1 from the low AUCs (0 means\nan opposite diagnosis of the entire cases) of the Source model. In addition, for\nthe knowledge described for sample selection, our experiments also provided\nquantitative statistics supporting the validity of the distance knowledge: the\naverage Euclidean distance between Prior and Current is 0.38 (smaller) and 0.52\n(larger) on the normal and cancer patients, respectively.\nTable 2 presents the performance of various attack methods using VGG as\nthe backbone for the source model, while keeping the target models (includ-\ning the adversarially trained model) unchanged. The results indicate that the\noverall attack performance patterns are consistent with those observed in Table\n1, suggesting that our method is effective across different model architectures,\nincluding non-Transformer-based models.\nFrom Fig. 3, our method consistently outperforms the other four meth-\nods (I-FGSM, C&W, MI-FGSM, PGD) across a range of parameter values,\nboth with and without adversarial training. Notably, as the epsilon value in-\ncreases indicating stronger adversarial perturbations\u2014AUC values for all meth-\nods decrease, as expected."}, {"title": "6 Conclusion", "content": "In this study, we delved into the medical imaging AI model robustness against\nadversarial attacks on longitudinal models, with a particular focus on breast\ncancer diagnosis. Our research topic is novel because studies on adversarial at-\ntacks to longitudinal models are rare, yet such models are gaining popularity\nin medical applications. We proposed a novel attacking method that combines\ncross-entropy loss and knowledge-guided distance metric learning, showing much\nsuperior effects in terms of fooling the diagnosis model, and outperforming sev-\neral compared state-of-the-art methods. Our method remained effective even\nafter incorporating the defensing method of adversarial training. Future work\nincludes further evaluation on different deep learning structures and develop-\ning effective defense methods. Our study highlights the importance and urgency\nof adversarially robust medical diagnosis models towards delivering safe AI to\npatient care."}]}