{"title": "Unraveling Indirect In-Context Learning Using Influence Functions", "authors": ["Hadi Askari", "Shivanshu Gupta", "Terry Tong", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "abstract": "This work introduces a novel paradigm for generalized In-Context Learning (ICL), termed Indirect In-Context Learning. In Indirect ICL, we explore demonstration selection strategies tailored for two distinct real-world scenarios: Mixture of Tasks and Noisy Demonstrations. We systematically evaluate the effectiveness of Influence Functions (IFs) as a selection tool for these settings, highlighting the potential for IFs to better capture the informativeness of examples within the demonstration pool. For the Mixture of Tasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU, BigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining BertScore-Recall (BSR) with an IF surrogate model can significantly improve performance, leading to average absolute accuracy gains of 0.37% and 1.45% for 3-shot and 5-shot setups when compared to traditional ICL metrics. In the Noisy Demonstrations setting, we examine scenarios where demonstrations might be mislabeled. Our experiments show that reweighting traditional ICL selectors (BSR and Cosine Similarity) with IF-based selectors boosts accuracy by an average of 2.90% for Cosine Similarity and 2.94% for BSR on noisy GLUE benchmarks. In sum, we propose a robust framework for demonstration selection that generalizes beyond traditional ICL, offering valuable insights into the role of IFs for Indirect ICL.", "sections": [{"title": "1 Introduction", "content": "In-Context Learning (ICL) has emerged as a powerful method for utilizing large language models (LLMs) to handle novel tasks at inference (Mann et al., 2020; Min et al., 2022). Unlike traditional approaches that require task-specific fine-tuning, ICL allows a single model to adapt to different tasks without additional training, relying solely on the demonstrations provided in the context. This flexibility not only reduces the cost of task adaptation but also offers a transparent and easily customizable way of guiding the model's behavior (Liu et al., 2021a; Wei et al., 2022). By leveraging the context provided in prompts, ICL has been shown to improve both generalization across diverse tasks and reasoning abilities (Anil et al., 2022; Drozdov et al., 2022). Despite its advantages, the success of ICL is closely tied to the choice of demonstrations used in the prompt. Even slight variations in these demonstrations can significantly influence the model's performance, as shown in numerous studies (Zhao et al., 2021; Liu et al., 2021a; Lu et al., 2022).\nTraditional ICL makes numerous assumptions that restrict its applicability to real-world problem domains. For instance, traditional ICL (Min et al., 2021; Conneau, 2019; Halder et al., 2020) assumes that demonstrations to be selected are directly and accurately annotated for the end-task. However, this is not always the case for low-resource, sparse, or specialized domains, end-task information and labeled demonstrations might not be available. Similarly, when LLMs are deployed as services, the user query or the end task itself could be unknown beforehand, let alone providing direct demonstrations at inference. Thus, in this paper, we explore a more generalized setting for ICL, which we refer to as Indirect ICL. In indirect ICL, we aim to provide indirect (or incidental) supervision (Yin et al., 2023; Li et al., 2024) by selecting demonstrations from a pool of examples where the majority are not directly suited to the end task due to severe distribution shifts. This includes selecting demonstrations from a pool that predominantly consists of examples belonging to other tasks, with few demonstrations from the end task possibly included. Additionally, the demonstration set may be mislabeled by humans (Yan et al., 2014; Zhu et al., 2022) or LLMs (Wu et al., 2023). Since the effectiveness of ICL heavily relies on the quality of demonstrations selected (Kossen et al., 2024; Wu et al., 2022; Wang et al., 2024), selecting the most helpful indirect demonstrations becomes imperative in these situations.\nDespite these potential issues with the demonstration set, we wish to pave the way for extracting maximal benefit from any type of annotated dataset, irrespective of label purity or task relatedness. Thus, in order to combat the aforementioned issues with sub-optimal datasets for ICL, we leverage Influence Functions (IFs) (Hampel, 1974; Cook and Weisberg, 1980). IFs offer a formal method for assessing how individual training data points affect model predictions. They have proven effective in a range of downstream machine learning tasks, including mislabeled data detection (Koh and Liang, 2017; Pruthi et al., 2020), optimal subset selection (Feldman and Zhang, 2020; Guo et al., 2020; Xia et al., 2024), model interpretation (Han et al., 2020; Grosse et al., 2023; Chhabra et al., 2024b), data attribution (Bae et al., 2024), data valuation (Choe et al., 2024) and analyzing model biases (Wang et al., 2019; Kong et al., 2021).\nTraditional (direct) ICL methods that use metrics such as BertScore-Recall (BSR; Gupta et al. 2023a) and cosine similarity (Reimers, 2019) inherently rely on the semantic similarity between demonstrations and test samples. In this paper, we posit that IFs can be a reasonable measure of affinity between the end task and any (indirect) demonstrations. We show that it is practical to use IF to identify candidate demonstrations that represent a close inductive bias with the end that, and utilize this information for highly accurate demonstration selection in the challenging indirect ICL setting. As our experiments and results will demonstrate, this is indeed the case, and we find that IFs can aid in improved performance when simple semantic similarity is insufficient for demonstration selection.\nIn sum, our work advances ICL demonstration selection and makes the following key contributions and findings:\n\u2022 This work formalizes a new and general paradigm for ICL, namely Indirect In-Context Learning, where we benchmark demonstration selection for two distinct and real-world settings: (a) Mixture of Tasks and (b) Noisy Demonstrations.\n\u2022 We propose the use of Influence Functions (IFs) as a viable approach for demonstration selection in these generalized ICL settings.\n\u2022 For Mixture of Tasks, combining an IF Surrogate model with BertScore-Recall (BSR) can lead to a 0.37 and 1.45 average absolute increase in performance for k = 3 and k = 5 shots compared to the best performing traditional ICL metric.\n\u2022 For Noisy Demonstrations, we observe that undertaking a weighted average selection using traditional ICL selectors (BSR and Cosine Similarity) and IF based selectors increases the absolute accuracy on average by 2.90% for Cosine and 2.94% for BSR."}, {"title": "2 Preliminaries", "content": "We hereby introduce preliminaries of ICL and IF.\n2.1 Traditional In-Context Learning\nBefore we define the more generalized problem of Indirect ICL, we first define traditional ICL.\nIn-Context learning. ICL allows LLMs to solve test inputs from novel tasks by presenting a few examples of the task in the prompt. Formally, given a set of input $x$ and output $y$ pairs ${(x_i, y_i)}_{i=1}^k$, prompt template $T$, and the test input $X_{test}$, ICL using an LLM involves prompting it to conditionally generate the test output $Y_{test}$ according to the following distribution:\n$Y_{test} \\sim P_{LLM}(\\cdot | T(x_1, y_1, ..., x_k, y_k, X_{test}))$\nDemonstration Selection. In this work we study the problem of selecting k in-context examples from a pool of $N \\gg k$ labeled candidates. This is often necessary due to context length limits and cost considerations (Rubin et al., 2021; Gupta et al., 2023a). Formally, the goal is to select a subset $S \\subseteq {(x_i, y_i)}_{i=1}^N$ of size k that maximizes the probability of generating the desired $Y_{test}$ when the LLM is conditioned on $X_{test}$ and $S$. It is noteworthy that prior studies mainly consider a task-dependent ICL scenario and assume that candidate demonstrations all directly match the end task (Min et al., 2021; Conneau, 2019; Halder et al., 2020).\n2.2 Indirect In-Conext Learning\nNow, we describe two scenarios of indirect ICL, one where the candidate pool comprises of demonstrations from various tasks and the other where the demonstrations may have noisy labels.\nMixture of Tasks. Unlike traditional ICL, where candidate demonstrations match the end task at inference, we consider the more generalized indirect ICL setting where the demonstration pool is task-agnostic. In practice, this setting would allow for pooling annotated demonstrations from various accessible tasks. Formally, given a set of input $x$ and output $y$ pairs ${(x_i, y_i)}_{i=1}^k$, where the pairs $(x_i, y_i)$ may originate from different tasks than the test input $X_{test}$, the model is prompted to maximize performance across test tasks.\nNoisy Demonstrations. To further generalize the problem of indirect ICL, we also consider noisy supervision that is likely existing in the pool of demonstrations. Formally, let $D = {(x_i, y_i)}_{i=1}^N$ represent the training dataset, where $x_i \\in X$ is the input and $y_i \\in Y$ is the corresponding binary label. We randomly select 20% of the data points from D and flip their labels.\nOnce the noisy dataset is generated, we use it for ICL. Formally, given the noisy set of input-output pairs ${(x_i, y_i)}_{i=1}^N$ and a test input $X_{test}$, the goal is to conditionally generate the test output $Y_{test}$ based on the noisy training data."}, {"title": "2.3 Influence Functions", "content": "Here we formally define how we will use IFs to perform Generalized Indirect ICL.\nLet the input space be $X$ and the label space be $Y$. The training dataset is denoted as $D = {(x_i, y_i)}_{i=1}^n$, where $x_i \\in X$ and $y_i \\in Y$ are the input and label of the i-th data point. Given a loss function $l$ and a parameter space $\\Theta$, the empirical risk minimization problem is defined as:\n$\\theta^* = arg \\min_{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^n l(y_i, f_\\theta(x_i)),$\nwhere $f_\\theta : X \\rightarrow Y$ is the model parameterized by $\\theta \\in \\Theta$. The gradient of the loss for the i-th data point with respect to a vector $\\eta$ is denoted as:\n$\\nabla_{\\eta} l_i = \\nabla_{\\eta} l(y_i, f_\\theta(x_i)).$\nThe IF evaluates the effect of individual training data points on the estimation of model parameters (Hampel, 1974; Cook and Weisberg, 1980; Martin and Yohai, 1986). It measures the rate at which parameter estimates change when a specific data point is up-weighted.\nSpecifically, for $k \\in [n]$ and $\\epsilon \\in R$, we consider the following $\\epsilon$-weighted empirical risk minimization problem:\n$\\theta^{(k)}(\\epsilon) = arg \\min_{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^n l(y_i, f_\\theta(x_i)) + \\epsilon l(y_k, f_\\theta(x_k)).$\nHere, the loss function $l(y, f_\\theta(x))$ is assumed to be twice-differentiable and strongly convex in $\\theta$ for all $(x, y) \\in X \\times Y$, the empirical risk minimizer (model weights) $\\theta^*$ is well-defined, and the influence of the k-th data point $(x_k, y_k) \\in D$ on the empirical risk minimizer (model weights) $\\theta^*$ is defined as the derivative of $\\theta^{(k)}(\\epsilon)$ at $\\epsilon = 0$:\n$I_{\\theta^*}(x_k, y_k) := \\frac{d \\theta^{(k)}}{d \\epsilon}|_{\\epsilon=0} = -H(\\theta^*)^{-1} \\nabla_{\\theta} l(y_k, f_{\\theta}(x_k)).$\nwhere $H(\\theta) := \\nabla_{\\theta}^2 (\\sum_{i=1}^n l(y_i, f_\\theta(x_i)))$ is the Hessian of the empirical loss.\nThe $I_{\\theta^*}(x_k, y_k)$ on the empirical risk minimizer $\\theta^*$ is generalized to assess its effect on prediction loss (Koh and Liang, 2017). Given a validation dataset $D^V := {(x_i^V, y_i^V)}_{i=1}^m$, the influence of $(x_k, y_k)$ on the validation loss is defined as:\n$I(x_k, y_k) := (\\frac{1}{m} \\sum_{i=1}^m \\nabla_{\\theta} l(y_i^V, f_{\\theta}(x_i^V)))^T \\times I_{\\theta^*}(x_k, y_k).$"}, {"title": "3 Proposed Approach", "content": "In this section, we describe our approach to select demonstrations in both sub tasks.\n3.1 Selecting within Mixture of Tasks\nIn this scenario, we develop influence-based methods for demonstration selection. Specifically, for each validation example, we compute influence values to identify the most impactful examples from a pool of training examples containing a mixture of tasks. Two approaches are employed to calculate these influence scores:\n\u2022 A surrogate-model based method, where a lightweight surrogate model such as ROBERTa (Liu, 2019) is fine-tuned on the candidate demonstrations to compute influence.\n\u2022 A pretrained-gradient based method where the samples are passed through the LLM itself. We then compute the IF using the extracted gradients.\nFormally, for each validation example $(x_{val}, y_{val})$, we compute the influence of each training example $(x_i, y_i) \\in D_{train}$, where $D_{train}$ is the set of the training examples. The influence score $I((x_i, y_i), (x_{val}, y_{val}))$ quantifies the effect of $(x_i, y_i)$ on the loss function evaluated at $(x_{val}, y_{val})$. Using these computed influence values, we select the top k most influential demonstrations.\nWe compare two versions of computing the IF after extracting the gradient, DataInf (Kwon et al., 2023) and TracIn (Pruthi et al., 2020). DataInf is based on an easy-to-compute closed-form expression, leading to better computational and memory complexities than other IF computation methods. TracIn traces how the loss on the test point changes during the training process simply using an inner product of training set gradients. Since it does not compute the Hessian matrix, it is faster than DataInf, but at the cost of lower estimation performance.\nAdditionally, we compare the influence-only methods with well-performing approaches BertScore-Recall (BSR; Gupta et al. 2023a) and Cosine Similarity (Reimers, 2019). These methods excel at capturing semantic similarity between validation and training examples. We also compare with a well performing sparse information retrieval algorithm, BM25 (Jones et al., 2000).\nLastly, we combine the previously described approaches by implementing a two-stage selection process. First, we perform an initial pruning of the demonstration pool using either BSR or Cosine Similarity. Specifically, for a given number of desired demonstrations k, we prune the dataset to select 2k candidates from the original set of labeled examples ${(x_i, y_i)}_{i=1}^N$.\nWe then apply the IF-based methods to re-rank these remaining examples based on their influence on the validation loss. The final selection of k in-context demonstrations is performed by selecting the top k examples from the re-ranked subset.\n3.2 Selecting Noisy Demonstrations\nIn this setting, we utilize IFs to identify noisy samples within the dataset. Formally, let $D = {(x_i, y_i)}^n_1$ represent the training dataset. First, we employ IFs to prune the dataset by detecting and removing noisy examples, following which"}, {"title": "4 Experiments", "content": "Here, we expand upon our experimental setup to conduct the experiments and analyze the results.\n4.1 Experimental Setup\nWe discuss our, dataset details and model used to conduct the experiments.\nEvaluation Data. For Mixture of Tasks, we collect a generalized pool of examples from different tasks such that the input x and output y pairs ${(x_i, y_i)}_{i=1}^k$ do not necessarily correspond to the same task as the test input $X_{test}$. The evaluation task pool contains three samples each from 28 different tasks from MMLU (Hendrycks et al., 2020), BigBench (Srivastava et al., 2022), StrategyQA (Geva et al., 2021) and CommonsenseQA (Talmor et al., 2018). We evaluate the ICL accuracy, using this train set, on 12 different tasks from MMLU and BigBench.\nFor Noisy Demonstrations, we employ the noisy dataset framework from Kwon et al. (2023). In their work, the four binary classification GLUE datasets (Wang, 2018) MRPC, QQP, QNLI, and SST2 are utilized. To simulate a scenario where a portion of the data samples are noisy, 20% of the training data samples are randomly selected and their labels are flipped. We use these noisy datasets as the candidate pool in our experiments and evaluate the ICL accuracy.\nBase LLM. In Mixture of Tasks, for k = 3 shots, we conduct ICL experiments on Llama-2-13b-chat (Touvron et al., 2023), Mistral-7b-v0.3 (Jiang et al., 2023) and Zephyr-7b-beta (Tunstall et al., 2023). For k = 5 shots we conduct experiments on Llama-2-13b-chat. We extend on the framework designed by Gupta et al. (2023a,b). The temperature is set to 0 for inference. For Noisy ICL, we conduct experiments on Llama-2-13b-chat. All of our experiments run on 8\u00d7 NVIDIA RTX 6000 Ada GPUs.\n4.2 Method and Baseline Configurations\nHere we expand on the methods and baselines we use for our experiments in both settings.\nMixture of Tasks. We construct 4 IF-only methods. 2 based on the Surrogate Model based approach, SUR and 2 based on the Pretrained LLM weights based approach, PRE. We test Data-Inf and TracIn based versions of these approaches, namely, Surrogate Model-DataInf SURD, Surrogate Model-TracIn SURT, Pretrained Model-DataInf PRED and Pretrained Model-TracIn PRET. As mentioned before, SURD and SURT use RoBERTa as the surrogate model, whereas PRED and PRET use Llama2-13b-chat as the pretrained LLM.\nAdditionally, we test traditional semantic approaches, such as BSR and, Cosine Similarity (Cos), as well as retrieval based approaches, such as BM25, as baselines. Finally, we test the combination of the aforementioned traditional and IF methods as well.\nNoisy Demonstrations. As elaborated in Section 3.2, we explore two approaches, IF Pruning and IF Averaging, for the task of selecting the best demonstrations. We only use the surrogate model-based IF method in this setting, and we employ an additional method of computing IFs, LiSSA (Koh and Liang, 2017). We experiment with different levels of pruning and IF weights (a) as hyperparameters, namely 10% pruning and 0.5\u03b1. Furthermore, we also create a random pruning baseline for BSR and Cosine Similarity as well.\nFor baselines, we again compare our IF based approaches with BSR, Cosine Similarity and BM25.\n4.3 Results on Mixture of Tasks\nWe present the results on Mixture of Tasks in Figure 2, the full results for multiple number of shots and multiple LLMs can be seen in Appendix A.1. Additional results for the TracIn IF method are provided in Appendix A.2. Results for Pretrained Gradients combined with BertScore and Cosine Similarity are presented in Appendix A.3.\nCombining Surrogate Model DataInf with BertScore results in the best performance. As we can clearly observe in Figure 2 and Table 8, SUR[D,BSR] method had the highest average performance across the tasks, in both 3 and 5 shots. This shows the benefit of combining IF with BertScore as performance increased by 0.56 in k = 3 shots and by 1.52 in k = 5 shots. The results also show that the maximal benefit of IF methods is gained in combination with the semantic similarity methods. This is due to the fact that IF can leverage the model's inductive bias to re-rank the retrieved demonstrations effectively, but the initial 2k pruning via BSR is critical to shorten the candidate pool to demonstrations that are semantically relevant enough.\nSurrogate Models outperform Pretrained Gradients. We see that surrogate models outperformed Pretrained Gradients in demonstration selection for the Mixture of Tasks setting in both the k = 3 and k = 5 shots. The fine-tuning of the surrogate model leads it to better capture the test task affinity of the demonstration pool.\nDataInf is better than TracIn as an IF method. The speed gains of TracIn come at a cost of performance as the DataInf method of IF computation routinely outperformed TracIn. TracIn likely underperforms because it does not utilize critical second order gradient information since the Hessian $H (\\theta^*)$ is assumed to be the identity matrix. This trend has also been observed in past work on IF methods (Chhabra et al., 2024a).\n4.4 Results on Noisy ICL\nIn this section we provide analysis on the aforementioned General Noisy ICL setting. We also conduct two ablations on varying IF a weight in IF Averaging and varying the Noise level in the datasets.\nIF Averaging works better than other baselines. Table 1 and Figure 3 clearly show that doing a weighted average between the surrogate model IF and both Cosine and BertScore leads to performance boosts. Atleast one and if not both of the highest performing methods in each of the datasets we tested were from the averaging method. We see that LiSSA and DataInf are similarly effective, with DataInf being more computationally efficient.\nPruning hurts not helps performance. We can see that pruning actively hurts performance as Figure 3 shows that all 3 types of BertScore pruning and all 3 types of Cosine pruning had lower average scores than BertScore and Cosine Similarity. This might be due to the fact that we are removing potentially helpful samples from the demonstration pool, even if they might have noisy labels.\nVarying IF a weight. We perform an analysis on the effect of varying alpha on the MRPC dataset with 20% noise added. We found that generally alpha ranges between 0.4 to 0.6 give the most optimal results as Figure 4 indicates. An explanation is that assigning higher weights to IF scores leads to missing out on semantically relevant samples and assigning higher weights to semantic selection methods leads to including several noisy samples.\nVarying Noise level. We also test whether our method can perform well on varying noise levels in the dataset. To test this, we create 2 datasets of MRPC with 10% and 30% noise added. As seen in Table 3, in both the cases IF Averaging outperformed other baselines. With the DataInf configuration performing better for the 10% noise dataset and the LiSSA configuration performing better for the 30% noise dataset.\n4.5 Computational Complexity\nWe present the worst case time complexity for inference of ours and related method in Table 4. As can be observed, our methods are comparable, if not more efficient than the other baselines. Note that the SUR methods require an additional fine-tuning step on a smaller surrogate model before the gradients are extracted, which the PRE methods do not. Furthermore, note that TracIn as an influence method is much faster than Hessian-based approaches (e.g. DataInf) as it assumes that the Hessian is the identity matrix. While this leads to more efficient influence computation, it comes at the cost of lower estimation performance, as our results with TracIn also show."}, {"title": "Limitations", "content": "The main limitation of Influence Functions is that they are costly to compute, especially for large datasets and LLMs with lots of parameters. This is why we opted to fine-tune a smaller model such as ROBERTa and use pretrained LLMs for our methods. Further performance gains can be attained at the cost of computational speed if fine-tuned LLMs are employed instead. As research on influence estimation methods for LLMs is currently ongoing, faster influence functions developed in the future can also be utilized with our methods for highly efficient and accurate ICL performance."}, {"title": "5 Related Work", "content": "In-Context Learning (ICL). Following the scaling of model sizes and learning resources (Mann et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023), LLMs have gained emergent abilities for efficient inference-time adaptation via ICL (Mann et al., 2020). However, ICL is critically sensitive to demonstration pool examples (An et al., 2023; Liu et al., 2021a; Zhang et al., 2022) and selection strategies (Rubin et al., 2021; Mavromatis et al., 2023). One line of work studies example scoring and retrieval, utilizing model-agnostic heuristic metrics like perplexity (Gonen et al., 2022), mutual information (Sorensen et al., 2022), semantic similarity (Liu et al., 2021a; Gupta et al., 2023b), etc. to select demonstrations. Another line of work optimizes selection based on empirically verified desirable features a priori, e.g. diversity (Su et al., 2022; Ye et al., 2023), coverage (Gupta et al., 2023a), etc. However, prior work assumes that the demonstration distribution is aligned with task distribution, which is not always the case (Chatterjee et al., 2024). Our work serves as a first to investigate ICL demonstration selection in the task and dataset quality shifts in the ICL settings.\nInfluence Functions. Influence functions (IFs) comprise a set of methods from robust statistics (Hampel, 1974; Cook and Weisberg, 1982) that have been recently proposed for deep learning data valuation and can provide a conceptual link that traces model performance to samples in the training set. For gradient-based models trained using empirical risk minimization, IFs can be used to approximate sample influence without requiring actual leave-one-out retraining. For deep learning models, the seminal work by Koh and Liang (2017) utilized a Taylor-series approximation and LiSSA optimization (Agarwal et al., 2017) to compute sample influences. Follow-up works such as Representer Point (Yeh et al., 2018) and Hydra (Chen et al., 2021) sought to improve IF performance for deep learning models, constrained to vision applications. More recently, efficient influence estimation methods such as DataInf (Kwon et al., 2023), Arnoldi iteration (Schioppa et al., 2022), and Kronecker-factored approximation curvature (Grosse et al., 2023) have been proposed which can be employed for larger generative language models, such as LLMs. Some other simpler IF approaches simply consider the gradients directly as a measure of influence (Pruthi et al., 2020; Charpiat et al., 2019), followed by some ensemble strategies (Bae et al., 2024; Kim et al., 2024). Recent work has also found that self-influence only on the training set can be a useful measure for detecting sample influence (Bejan et al., 2023; Thakkar et al., 2023).\nIFs have been utilized with great success in a number of application scenarios (e.g. classification (Chhabra et al., 2024a; Koh and Liang, 2017), generative models (Kwon et al., 2023; Schioppa et al., 2022; Grosse et al., 2023), active learning (Chhabra et al., 2024b; Liu et al., 2021b), etc.). Moreover, while some recent works have considered using influence for selecting direct demonstrations (Nguyen and Wong, 2023; Van et al., 2024), neither of them has consider their effect on inductive bias selection in the indirect ICL setting, which is the focus of our work."}, {"title": "6 Conclusion", "content": "We formalize a new paradigm for generalized In-Context Learning, which we call Indirect In-Context Learning. We analyze two different real-world Indirect ICL settings and propose effective demonstration selection strategies for these scenarios. We explore using Influence Functions (IFs) to leverage the informativeness of the samples in the demonstration pool and the models' task inductive bias. We find that combining a surrogate model-based IF approach with BertScore performs better when there are an overwhelming majority of irrelevant tasks in the candidate pool. We also find that reweighting the surrogate model-based IF scores with traditional metric scores can be helpful in the case where noisy demonstrations are present. Future work will aim to augment the Pretrained Gradient approach by using better/larger LLMs or finetuning the LLMs."}]}