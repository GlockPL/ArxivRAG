{"title": "Quantifying Behavioural Distance Between Mathematical Expressions", "authors": ["Sebastian Me\u017enar", "Sa\u0161o D\u017eeroski", "Ljup\u010do Todorovski"], "abstract": "Existing symbolic regression methods organize the space of candidate mathematical expressions primarily based on their syntactic, structural similarity. However, this approach overlooks crucial equivalences between expressions that arise from mathematical symmetries, such as commutativity, associativity, and distribution laws for arithmetic operations. Consequently, expressions with similar errors on a given data set are apart from each other in the search space. This leads to a rough error landscape in the search space that efficient local, gradient-based methods cannot explore. This paper proposes and implements a measure of a behavioral distance, BED, that clusters together expressions with similar errors. The experimental results show that the stochastic method for calculating BED achieves consistency with a modest number of sampled values for evaluating the expressions. This leads to computational efficiency comparable to the tree-based syntactic distance. Our findings also reveal that BED significantly improves the smoothness of the error landscape in the search space for symbolic regression.", "sections": [{"title": "1. Introduction", "content": "Mathematical modeling of an observed phenomenon is omnipresent in many scientific and engineering fields. Addressing the task requires human experts to follow tedious and time-consuming trial-and-error endeavors, which include a combination of reasoning from basic principles and parameter fitting. Equation discovery, also referred to as symbolic regression, addresses the task of finding mathematical equations that fit a given data set well and allows for the automation of these endeavors. The significant advantage of equations over other models obtained from data by using machine learning methods is their ability to provide a concise, human-readable, and interpretable way to represent complex relationships between the observed variables.\nSymbolic regression methods typically employ combinatorial search through the space of candidate mathematical expressions, i.e., model structures, together with numerical optimization for fitting the values of the model parameters to data. Methods mainly differ in how the space of equations is ordered and searched. However, most symbolic regression methods, if not all, assume a search space where syntactically similar expressions are close to each other. The prevailing paradigm, evolutionary optimization (Schmidt & Lipson, 2009) use genetic operators of crossover and mutation, which apply a sequence of edits to the expression trees to generate new expression from existing ones. The structure of the search space of symbolic regression methods thus follow the Levenshtein distance (Levenshtein, 1965), which calculates the number of edits required to transform one expression into another. However, syntactically similar expressions can significantly differ when their behavior, i.e., evaluations, are considered. For example, expressions $c + x$ and $cx$, where $c$ and $x$ denote a constant parameter and an observed variable, respectively, are syntactically similar (note that the Levenhstein distance between them is minimal, i.e., equals 1), but their evaluations for given values of $c$ and $x$ differ significantly. This also means that the error of similar expressions on a given data set will vary substantially, which renders simple, local search methods, like gradient descent, inappropriate for searching the space of equations (Tanevski et al., 2020).\nIn this paper, we propose a distance measure between mathematical expressions that captures the nuances of their behavior. The proposed measure should cluster expressions"}, {"title": "2. Related work", "content": "The distance between mathematical expressions is often measured by the Levenshtein distance (Levenshtein, 1965) between the corresponding string representations of the expressions in the in-fix notation. Alternatively, the distance between expression trees can be measured by using tree-edit distance (Zhang & Shasha, 1989). Both measures calculate the distance as the minimal number of elementary edit operations (add, remove, or edit a sequence symbol or a tree node) needed to transform one expression into another. These distances operate on the symbolic representations of the mathematical expressions as sequences or trees. In the more general context of code similarity (Walenstein et al., 2007), they are considered instances of the prevailing paradigm of representational or syntactic similarity.\nIn contrast, semantic or behavioral similarity aims to assess the similarity of the functions that the compared programs implement. Computational methods for aspects of semantics that can be captured by specific program representations, such as control-flow graphs (Nair et al., 2020). Another relevant aspect of code similarity, not related to program representation, is measuring the fraction of inputs for which two programs produce the same output (Walenstein et al., 2007). While this is considered an important aspect of code similarity when identifying code clones (Juergens et al., 2010), it would also be very relevant for comparing mathematical expressions. To this end, to the best of our knowledge, there are no readily available computational methods for assessing it.\nDespite the variety of techniques for searching through the space of mathematical expressions used in symbolic regression, the search space itself is commonly structured around the syntactic similarity between expressions. Take, for example, the prevailing approach in symbolic regression, i.e, genetic programming (Koza, 1994; Schmidt & Lipson, 2009; Cranmer, 2023). It represents mathematical expressions as expression trees and performs simple operations on the latter to transform the expressions from the current population into new expressions. The operations, such as crossover and mutation, are, in fact, sequences of elementary edits of the expression trees and, therefore, closely align with the concept of the syntactic tree-edit distance. Similarly, grammar-based methods search through the space of parse trees, where the most straightforward parse tree is considered first, and more complex parse trees are generated systematically by simple revisions of the existing ones based on the grammar rules (Todorovski & Dzeroski, 1997). Consequently, parse trees close to each other in the search space correspond to syntactically similar expressions. Similarly, process-based modeling (Bridewell et al., 2008) systematically explores the space of candidate models by exhaustively enumerating all expressions with simple edit operations of adding or changing a process.\nIn recent years, a growing body of research has explored the use of generative neural networks to embed expressions into a latent space where expressions with similar syntactic structures occupy close proximity (Kusner et al., 2017; G\u00f3mez-Bombarelli et al., 2018; Me\u017enar et al., 2023). These approaches then employ optimization algorithms to navigate this latent space and generate new expressions with similar syntactic properties. While these approaches do not explicitly use edit distance or tree-edit distance metrics during training, the underlying structure of the latent space aligns with these measures, effectively clustering syntactically similar expressions together. DSO (Petersen et al., 2021) further extends this concept by incorporating reinforcement learning and evolutionary algorithms to guide the search for new expressions. However, this approach's spatial relationships between expressions still largely mirror the neighborhoods defined by edit and tree-edit distance metrics. Finally, Symbolic-Numeric Integrated Pre-training, SNIP (Meidani et al., 2023) diverges from this trend by employing two transformer-based encoders, one for symbolic and one for numeric data. Note, however that SNIP requires a subsequent fine-tuning stage that utilizes syntactic similarity measures."}, {"title": "3. Behavioural distance between expressions", "content": "In the first part of this section, we introduce the challenges associated with the quantification of behavioural distance. In the second part, we present our solution to these challenges by defining the behaviour-aware expression distance, BED."}, {"title": "3.1. Challenges with behavioural distance", "content": "Mathematical expressions are often used as a discrete representation of a curve in continuous space. We can think of expressions in two ways, depending on what we want to do with them. On the one hand, we can represent them as a sequence of symbols in infix notation, which is a concise and precise way to write them down. This representation is convenient for evaluating expressions with specific values and conveying their general characteristics. On the other hand, we can visualize mathematical expressions as curves on a graph. This visualization helps us understand how an expression behaves and how similar it is to another expression. In this paper, we try to quantify this behavioural distance between two expressions in order to ease exploration of the space of expressions.\nBehavioural distance between expressions has several aspects. The first is the presence of constants with unknown values within expressions. Two expressions with identical structure can exhibit distinct behavior if their constants differ in value. Consider the expression $c \\cdot x^2$, where $c$ represents a constant. The behavior of this expression varies considerably depending on the value of $c$. For instance, when $c = 1$, the expression represents a simple parabola, while when $c = -1$, it becomes a downward-facing parabola. This poses a significant challenge for the metric in the context of symbolic regression, where generating expressions with known values of the constants is more difficult and produces worse results than generating the structure of an expression and subsequently replacing placeholder constant symbols with specific values through numeric optimization. Consequently, symbolic regression models generate expressions with unknown constants between which it is hard to define a distance.\nAnother aspect is the domain of the expression variables. Two expressions might exhibit similar behaviour over one domain, but diverge significantly when observed over another domain. For example, the expressions $x$ and $sin x$ exhibit similar behaviour over the domain [-0.5, 0.5] but diverge significantly over the domain [-100, 100]. This fact makes us believe that behavioural distance between expressions must be data driven, as different domains drive expression closer or further apart.\nBuilding on these insights, we introduce the behavior-aware expression distance (BED), an unsupervised yet data-driven metric for quantifying the distance between mathematical"}, {"title": "3.2. Behaviour-aware expression distance", "content": "Consider a specific input value of the expression variable(s) $x$. An expression $u$ without constants evaluates to a single output value $y = u(x)$. However, expressions involving constants with unknown values can be evaluated multiple times with different values of the constants, resulting in a set $Y = \\{u(x)\\}$ of output values. This set, along with the frequency of each output value, represents a probability distribution over the possible expression evaluations for a given $x$. This distribution captures the uncertainty associated with the expression's output caused by varying the values of the\nconstants. In other words, we can model the output value of the expression $u$ at $x$ as a random variable $U(x)$ with the cumulative distribution function $F_{U(x)} (y)$.\nBy treating expressions' evaluations (outputs) at a specific input value as random variables and probability distributions, we can employ distance metrics from probability theory to quantify their dissimilarity. One possible choice is the $p$-Wasserstein distance, also known as Earth Mover's Distance, EMD (Villani, 2009). The Wasserstein distance measures the minimum amount of work required to transform one distribution into another. This option is particularly well-suited for our problem because it considers the relative importance of different output values rather than simply comparing their means or variances. Additionally, it has been shown to effectively capture human perception of similarity (Rubner et al., 2000). The $p$-Wasserstein distance is defined by the Eq. (1), where $\\Gamma(F, G)$ represents the set of all couplings between random variables $E$ and $G$, i.e., pairs of random variables $(D, H)$, such that $D$ and $H$ have the same distributions as $E$ and $G$, respectively.\n$W_p(E, G) = \\inf_{\\pi \\in \\Gamma(E,G)} \\{\\int_{\\mathbb{R} \\times \\mathbb{R}} ||D - H||^P d\\pi\\}^{\\frac{1}{p}}$ (1)\nIf we assume that the expressions evaluate to real numbers, we can simplify the calculation of the p-Wasserstein distance using the quantile function (inverse cumulative distribution function) of the corresponding random variable, denoted by $Q_{U(x)} (q) = F_{U(x)}^{-1} (y)$. Specifically, we use Eq. (2) (Ramdas et al., 2017), which directly relates the distance between distributions to their quantile functions. This approach offers computational advantages as it allows us to focus on a single coupling, rather than evaluating all possible couplings and selecting the one with the minimum value. The simplified equation also provides a more intuitive interpretation of the distance, as it directly relates to the differences in the quantile functions of the distributions.\n$W_p(E,G) = \\{\\int_0^1 |Q_E(q) - Q_G(q)|^p dq\\}^{\\frac{1}{p}}$ (2)\nWe can now employ Eq. (2) to quantify the distance between two expressions $u$ and $v$ at a specific value of their variables $x$. However, to comprehensively assess their overall resemblance, we need to consider their behavior across the entire input domain X. To achieve this, we integrate the $p$-Wasserstein distance over the entire domain and normalize the result by the volume of the domain. Eq. (3) embodies this approach, where $Vol(X)$ represents the volume of the domain X, and $F_{U(x)}$ the cumulative distribution of the output for expression u at input x.\n$BED_p(u, v) = \\frac{1}{Vol(X)} \\int_{X} W_p(F_{U(x)}, F_{V(x)}) dx$ (3)\nTo overcome the computational intractability of directly evaluating Eq. (3), we follow an approximation strategy outlined here. We first restrict the domain of the expressions' constants to a finite interval $[a, b]$, which encapsulates domain knowledge or biases regarding the acceptable range of the constants. For each expression, we employ Latin Hypercube Sampling, LHS (McKay et al., 1979) to generate a predefined number of distinct sets of constants' values $C_i$ from the given interval. LHS ensures that the selected constants are more representative of the entire range of possible values, compared to random sampling. We similarly sample points $x$ from a predefined domain $X$ using LHS.\nWe approximate the 1-Wasserstein distance using Eq. (4). In the equation, $Y_{u,x} = \\{u(x)\\}$ represents the set of output values we obtain by evaluating expression $u$ at $x$ using constant values $C_i$, $Y_x = Y_{u,x} \\cup Y_{v,x}$, $y_k$ is the k-th smallest value in the set $Y$, and $d_k = y_{k+1} - y_k$. The approximation of the cumulative distribution function $F_u$ is defined by using Eq. (5).\n$W_1(u,v,x) = \\sum_{k=1}^{|Y_x|-1} d_k |F_{U(x)} (y_k) - F_{V (x)} (y_k)|$ (4)\n$F_{U(x)} (y) = \\begin{cases} 0; & y < y_1\\\\ \\frac{k}{|Y_u,x|}; & y_k \\leq y < y_{k+1} \\\\ 1; & otherwise \\end{cases}$ (5)\nSince we do not have guarantees that the sets $Y_{u,x}$ and $Y_{v,x}$ are non-empty (due to failures of expression evaluation for given values of the variables and constants), two border cases can occur. In the first case, when both sets are empty, we can assume the distance between them is 0. In the other border case, when only one of the two sets is empty, we set the distance between $u$ and $v$ at a point $x$ to a predefined high value of, e.g., $10^{20}$.\nFinally, we calculate the approximation of Eq. (3) using Eq. (6). Practically, we evaluate the distance between the two expressions as the average 1-Wasserstein distance at the sampled points from the domain $X$.\n$BED (u, v) = \\frac{1}{|X|} \\sum_{x \\in X} W_1 (u, v, x)$ (6)"}, {"title": "4. Evaluation", "content": "In this section, we report on the results of assessing the proposed metric's validity and effectiveness. In the first part, we demonstrate the metric's ability to produce consistent results, ensuring repeated evaluations yield similar rankings of expressions. The second part focuses on the smoothness of the error landscape, demonstrating that expressions close together in terms of our metric also exhibit similar fit to a given data set, validating the metric's ability to capture meaningful relationships between expressions."}, {"title": "4.1. Metric consistency", "content": "Consistency is a crucial aspect of a metric. Non-stochastic metrics inherently exhibit consistency as they consistently produce the same value. However, our proposed metric incorporates stochasticity. This necessitates demonstrating that the metric's outputs remain relatively consistent when the distance between a pair of expressions is calculated multiple times. Due to the varying magnitudes of distance values, simply presenting the consistency of the metric as the average standard deviation across different runs might be misleading. To address this, we recognize that the actual value of the distance metric itself is not of primary interest (it doesn't matter if the distance is 1 or 1000). Instead, we focus on the rankings of expressions produced by these distances. Accordingly, we assess the consistency of our metric by calculating the average Spearman's rank correlation coefficient (Spearman, 1904) between rankings across multiple experiments.\nTo assess the consistency of our metric across different parameter settings, we examine its performance on a set of 100 expressions with at most two variables. For additional experiments for expressions with at most 1, 4, 6, 8, and 10 variables, check Appendix A. For a given parameter configuration, we evaluate consistency as follows. First, we calculate the distance between each pair of expressions 100 times and store the distances in 100 distance matrices.\nFor each distance matrix, we sample different values of the variables ($x$) and constants in the expressions. Next, we rank the expressions within each row (i) of the matrices based on their distance to the i-th expression. Then, we calculate the Spearman's rank correlation coefficient between each pair of rankings corresponding to the same row index across different matrices. Finally, we represent the consistency of our metric for a particular parameter set as the average of the computed Spearman's rank correlation coefficients."}, {"title": "4.2. Smoothness of the error landscape", "content": "The central claim of this paper is that BED will increase the smoothness of the error landscape of the symbolic regression search space. Neighboring expressions in the restructured search space should have similar errors on a given data set. To test the validity of this conjecture, we perform experiments with 20,000 randomly generated expressions and eleven data sets from the Feynman symbolic regression benchmark (Udrescu & Tegmark, 2020) that include two variables. We compare BED against three other distance"}, {"title": "5. Discussion and conclusions", "content": "This paper introduces a novel metric for quantifying the distance between mathematical expressions based on their evaluation behavior. By treating expressions as probability distributions of their output evaluations, we leverage established metrics from probability theory, specifically adapting the 1-Wasserstein distance to the domain of expressions"}, {"title": "A. Metric consistency for different number of variables", "content": "In Section 4.1, we explore the consistency of our metric when expressions contain at most 2 variables. However, we want to highlight that our metric achieves consistency even on expressions that contain more/of fewer variables. Figure 6 shows results on expressions with different number of maximum variables (1, 2, 4, 6, 8, and 10) that are obtained using the same methodology as for Figure 2. We can see that consistency of our metric does not change significantly with the number of maximum variables."}, {"title": "B. Error landscape smoothness with additional aggregations", "content": "In Section 4.2 we present the smoothness of the error landscape using max as aggr\u2081 and mean as aggr2. As the choice of these aggregation functions is arbitrary, we present the results using different aggregation functions in this section.\nFigure 7 presents the difference in error between expressions in the close neighborhood when both aggregation functions (aggr\u2081 and aggr\u2082) are set to median. This aggregation strategy results in a slightly higher difference in error for our proposed metric, consistently exceeding the optimal metric on most datasets. However, it still exhibits a significant performance drop (higher error curve relative to the optimal metric) on data sets I.26.2 and II.11.28, where the data domain in the dataset deviates considerably from the sampling domain. Notably, we sample points from the interval [1,5], while the domain of the first variable in I.26.2 and both variables in II.11.28 is restricted to [0,1].\nFigure 8 illustrates the difference in error between expressions in the close neighborhood when the aggregation functions are set to max (aggr\u2081) and median (aggr\u2082). As observed, our metric closely aligns with the optimal metric on most datasets. The only exception is data set II.11.28, where the error difference for our metric deviates significantly from the optimal metric. On this dataset, our metric performs similarly to the edit distance."}]}