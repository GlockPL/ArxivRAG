{"title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications", "authors": ["Alon Halfon", "Shai Gretz", "Ofir Arviv", "Artem Spector", "Orith Toledo-Ronen", "Yoav Katz", "Liat Ein-Dor", "Michal Shmueli-Scheuer", "Noam Slonim"], "abstract": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance on downstream tasks. However, choosing the appropriate setting of tuning hyperparameters (HPs) is a labor-intensive and computationally expensive process. Here, we provide recommended HP configurations for practical use-cases that represent a better starting point for practitioners, when considering two SOTA LLMs and two commonly used tuning methods. We describe Coverage-based Search (CBS), a process for ranking HP configurations based on an offline extensive grid search, such that the top ranked configurations collectively provide a practical robust recommendation for a wide range of datasets and domains. We focus our experiments on Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a total of > 10,000 tuning experiments. Our results suggest that, in general, Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that for both models and tuning methods, exploring only a few HP configurations, as recommended by our analysis, can provide excellent results in practice, making this work a valuable resource for practitioners.", "sections": [{"title": "1 Introduction", "content": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance by adapting them to specific domains and tasks (Shi et al., 2023). This approach is particularly valuable in real-world enterprise scenarios, where there is often a need to address specific downstream tasks using available data, such as the company's proprietary data.\nRecently released base-models, such as Gemma (Team et al., 2024), Llama (Touvron et al., 2023a,b), and Mistral (Jiang et al., 2023), claim ease of fine-tuning across various tasks (Zhao et al., 2024). However, comprehensive studies of these models in the context of fine-tuning are still limited, leaving several important questions less explored. In this paper, we focus on the role of hyperparameter optimization (HPO) in the fine-tuning process of LLMs, and provide detailed and concrete recommendations for HP values, aiming to save practitioners time and computational resources. We present Coverage-based Search (CBS), which leverages an extensive grid search for highlighting an effective HP recommendation, as well as the ability to expand to a few promising HP configurations that collectively suggest high performance across diverse datasets and tasks.\nFor the purpose of providing these recommendations we conduct a comprehensive systematic study, focusing on practical scenarios where relatively small training data are available for tuning. We examine prominent tasks such as classification, summarization, and contextual question-answering (CQA) across various domains. Our study considers two leading LLMs, Llama-3-8B (AI@Meta, 2024) and Mistral-7B-v0.3 (Jiang et al., 2023), as well as two commonly used fine-tuning methods: full fine-tuning (FFT) and LoRA (Hu et al., 2021). Our main contributions are as follows:\n1. Recommended HP configurations for tuning, optimized per model and tuning method.\n2. Analysis of the differences between Llama-3-8B and Mistral-7B-v0.3, as well as between LORA and FFT, across 3 real-world tasks in practical scenarios.\n3. Analysis of the potential gain, accumulated by considering additional HP configurations suggested by our analysis."}, {"title": "2 Related Work", "content": "HPO is an established research area, known for its critical role in enhancing model performance (Yu and Zhu, 2020; Jin, 2022). The most straightforward approach for HPO is a grid search over the exponential space of HP values (Bergstra and Bengio, 2012). Since grid search is computationally demanding, a large volume of research has been focused on developing and evaluating more efficient HPO methods (Bergstra et al., 2011; Swersky et al., 2013; Snoek et al., 2012; Liu and Wang, 2021), while others consider over which HPs one should focus on (Gkouti et al., 2024; Zhang and Duh, 2020; Huang and Li, 2024). A few recent studies, described next, aimed to provide concrete recommendations for HP settings. However, these works typically considered a limited collection of datasets, tasks, or HPs. J et al. (2024) who fine-tuned the Llama-2 model on RAG and Code generation tasks, compared FFT and LoRA, and provided some general recommendations; however, their evaluation was limited to a single dataset per task, considering a small test set, and exploring a limited set of HP configurations. Zhang et al. (2024) examined the effects of scaling model size, data size, and PEFT parameters on machine translation and multilingual summarization, and found that larger data size improved performance, while scaling PEFT parameters was ineffective. Tribes et al. (2024) evaluated Llama-2 7B using LoRA tuning, exploring configurations of rank, alpha, dropout, and learning rate on instruction datasets. Utilizing black box optimization techniques, they identified that a learning rate around $10^{-3.5}$ yielded the best results, while other HPs showed no decisive optimal values.\nIn contrast, the present work considers both FFT and LoRA, for two SOTA models, using a comprehensive grid search across a large number of HP configurations, for a wide range of datasets, covering multiple domains and tasks. Thus, we expect the recommendations suggested here to provide a significant added value on top of previous research in this area."}, {"title": "3 Experimental Setup", "content": "Our experimental setup is concisely depicted in Figure 1. For each pair of model and tuning method we consider 3 tasks, multiple datasets, and 2 training sizes, as well as several HPs. For each of these HPs we consider multiple values, and apply a grid search over all the resulting HP configurations to identify the best one. Next, we dive deeper into each part of this setup."}, {"title": "3.1 Tasks and Datasets", "content": "We consider 3 tasks: text classification, text summarization, and contextual question answering (CQA). For text classification, we use 5 multi-class datasets from various domains, with class counts ranging from 6 to 100. For text summarization, we utilize 5 datasets from different domains, featuring diverse input and output lengths. For CQA, we include 3 datasets, one of which (DoQA) consists of 3 sub-datasets, one per domain, resulting in a total of 5 datasets for this task.\nWe adhere to the original train/validation/test splits when available. Otherwise, we create train and validation splits, allocating a portion (which differs from dataset to dataset) of the training data to validation. Full details of the datasets are provided in Appendix A."}, {"title": "3.2 Models", "content": "There is a plethora of models that can be tuned over labeled data, and naturally not all can be covered with limited resources. Thus, we considered representatives of two of the most popular families of open-source models, restricting their size to 8B or less for practical reasons: Llama-3-8B and Mistral-7B-v0.3."}, {"title": "3.3 Fine-Tuning Methods", "content": "We explore two tuning techniques commonly used by practitioners: FFT and LoRA. FFT updates all model parameters, offering potential higher performance gain at a greater computational cost. LoRA reduces the number of learnable parameters by approximating weight matrices, significantly lowering computational overhead. Our study aims to identify recommended HP configurations for each approach. In addition, we compare the two methods to evaluate their effectiveness in terms of performance gains across different models and tasks."}, {"title": "3.4 HP Search Space", "content": "For FFT, we tune 4 key HPs: learning rate (LR), learning rate scheduler (LR Scheduler), effective batch size\u00b9 (Batch), and number of epochs (Epochs). For LoRA fine-tuning, we additionally explore LoRA-specific HPs: the rank (LORAR) and scaling factor (LoRA), and fixing the scheduler.\nWe define a separate search space for each model and tuning method based on preliminary experiments, assuming that optimal settings vary across different models and tuning techniques. The full search space is detailed in the upper part of Table 1. This comprehensive search involves 96 \u2013 288 configurations for each tuple of model, tuning method, and dataset. Thus, in total, the results reported in this study are based on > 10,000 fine-tuning experiments.\u00b2"}, {"title": "3.5 Coverage-based Search (CBS)", "content": "Next, we outline our approach to obtain recommended HP configurations using grid search results across multiple datasets. Our strategy leverages the diversity of tasks and domains, aiming for good coverage for unseen datasets. For each model and tuning method, we evaluate all D datasets across the 3 tasks, considering results obtained on training sizes M = m1, m2, using a set of possible HP configurations C. The goal is to identify a ranking of HP configurations such that the top configurations yield consistently good results for most datasets and training sizes.\nFirst, for a given model, tuning method, dataset d, and training size m, we denote by $s(c)$ the score obtained by HP configuration c. The score is normalized w.r.t to the maximum score on d using the same model, tuning method, and training size:\n$S_{n}(c) = \\frac{s(c)}{\\max _{c \\epsilon C} s(c)}$\nFor a given dataset d and training size m, we denote the top configurations, $T_{c}^{d,m}$, as the configurations that receive a score that is at most 3 percent lower than the best configuration:\u00b3\n$T_{C}(d, m) = \\{c \\epsilon C | s_{n}(c) > 0.97 \\}$\nThe effective batch size is a product of actual batch size, number of GPUs, and gradient accumulation steps.\nIncluding preliminary experimentation and reproducing results.\nWe chose 3% since it provided a good balance between having in TC too common and too unique configurations.\nWe define $T_{C}^{*}$ as the union of $T_{C}(d, m)$ for all (d, m) in D \u00d7 M. The score of each configuration c in $T_{C}^{*}$ is defined as follows -\n$S_{n}(c) = \\sum_{(d,m)\\epsilon D\\times M, c \\epsilon T_{C}(d,m)} S_{n}(c)$\nThis score essentially counts the number of times c was selected in the top configurations $T_{C}^{*}$. Finally, we sort $T_{C}^{*}$ according to the value of $S_{n}(c)$ in descending order.\nHP configurations that work well for some datasets may not be optimal for other datasets. To take that into account, while still providing the practitioner with a small set of recommended HP configurations, we take the following approach. First, we define RankedAbove(c) as the set of configurations c1 \u2208 C s.t. $S_{n}(c1) > S_{n}(c)$. Next, when iterating over $T_{C}^{*}$ we calculate for each c:\n$coverage(c) = \\{(d,m) \\epsilon D \\times M | c \\epsilon T_{C}(d,m), c_{1} \\notin T_{C}(d,m) \\} \\forall c_{1} \\epsilon RankedAbove(c)$\nThat is, the set of (d, m) for which c provides a good result (i.e., \u0441 \u2208 $T_{C}(d,m)$) while higher-ranked configurations do not. We finally sort $T_{C}^{*}$ by the size of coverage(c) in descending order, to obtain a ranking over the HP configurations."}, {"title": "4 Evaluation Details", "content": "We focus on practical scenarios where training data is typically limited. Thus, for each dataset we evaluate two variants of training data sizes: 100 and 1000, sampled at random. The validation and test set sizes are 1000 for classification, 500 for summarization, and 329 \u2013 500 (depending on availability) for CQA. Note, both training data sizes are used to identify the recommended HP configurations, and we do not optimize these recommendations for specific training sizes. For downloading and processing the datasets we use the Unitxt library (Bandel et al., 2024): a collaborative framework for unified textual data processing and evaluation which allows easy formatting, sharing, and reproducibility of LLM evaluation results. For classification, we ensure each class has at least one train sample to prevent missing classes in the train set."}, {"title": "5 Results and Analysis", "content": "We report micro-f1 for text classification datasets, and rougeL for summarization and CQA datasets. For reporting the performance on each task we report macro-average over the respective datasets."}, {"title": "5.1 HP Recommendations", "content": "Table 2 presents the average performance of CBS compared to the Default configuration and the upper bound for FFT. The results show that CBS_1 outperforms the Default method by a large margin across all tasks, models, and train sizes.\nThe results of the same experiment with LORA are shown in Table 3. Here, for LLama-3-8B, the performance of CBS_1 is either comparable or slightly better than the Default configurations, indicating that the recommendations published for Llama-3-8B are beneficial. In contrast, for Mistral-8B-v0.3, CBS outperforms the baseline recommendation by a large margin in all cases. Thus, for both models, the CBS_1 configuration can be considered a new HP recommendation for both FFT and LORA in the considered region of small training size."}, {"title": "5.2 Upper Bound vs. CBS_1", "content": "In Tables 2 and 3 we consider the gap between the recommendation provided by our CBS_1 approach, in LOO mode, compared to selecting the best configuration found via a comprehensive HP search over the validation set of the individual dataset. Evidently, for Llama-3-8B our approach is quite close to the upper bound, while for Mistral-7B-v0.3 the gap is more evident. However, we note that in practice, full HP search over the validation set is often not feasible."}, {"title": "5.3 Tuning Methods", "content": "In general, FFT is known to demand more computational resources compared to LoRA. This is particularly true in our experimental setting with relatively smaller train data size. Thus, a pertinent question arises: what is the performance gain from FFT, and is it worth the increased computational cost? An examination of the results in Tables 2 and 3 reveals that, across all configurations of Default, CBS, and the upper bound, in most cases, there is no significant performance gain with FFT. This observation aligns with findings previously reported in Zhang et al. (2024) under different settings. Based on this analysis, when using small training data, our recommendation is to use LoRA, as it requires lower hardware resources while delivering similar or even superior performance compared to FFT, for both models and across all tasks."}, {"title": "5.4 Models", "content": "Overall, there is a clear advantage for LLama-3-8B over Mistral-7B-v0.3 across all dimensions. This finding is in line with the ranking in the Fine-tuning Leaderboard where Llama-3-8B and Mistral-7B-v0.3 are ranked first and fifth, respectively (Zhao et al., 2024)."}, {"title": "5.5 Train Data Size", "content": "As expected, moving from 100 to 1000 train samples improves the results across all tasks, models, and tuning methods. Notice, that despite the differences in performance scores, the overall trends between the considered configurations (Default, CBS, upper bound) are qualitatively similar across the data sizes."}, {"title": "5.6 Impact of Exploring Multiple CBS Recommendations", "content": "Next, we examine the impact of exploring more than one configuration from our recommended ranked list. To that end, we expand the evaluation of CBS described in Section 4.3 to consider additional HP recommendations beyond the top one. From a practical perspective. the budget is therefore defined as the number of configurations in $T_{C}^{*}$ we evaluate.\nFor each dh (the held-out dataset) and training size m, we iterate over c \u2208 $T_{C}^{*}$ according to the ranking induced by coverage(c). Assuming we have a budget of size k, we consider the top k configurations in $T_{C}^{*}$, and evaluate s(c) of each configuration on the validation set of hd. We mark the configuration with the highest s(c) as $C_{best}$. Finally, we calculate sn($C_{best}$) (the normalized score, see Section 3.5) on the test set of dh. We then average these scores over all held-out datasets and training sizes."}, {"title": "5.7 Recommendation for the Practitioner", "content": "Based on our experiments, we created practical HP recommendations for each model and tuning method. Table 4 shows the 4 top-ranked configurations, and we suggest to use these HP configurations in order, according to the available budget. As shown in Figure 2, using these 4 configurations, or even less, is expected to yield results nearly equivalent to full grid search over the HP space."}, {"title": "6 Conclusions", "content": "To effectively fine-tune LLMs, it is essential to use a proper HP configuration, aligned with the model and tuning method at hand. Our work aims to contribute to the understanding of this aspect by providing practitioners with recommended HP configurations for two leading models and two tuning methods. These recommendations represent the outcome of the analysis of the results of more than 10, 000 fine-tuning experiments, across a large collection of datasets, representing different tasks and domains. Furthermore, we provide comparative analysis between Llama-3-8B and Mistral-7B-0.3, and between LoRA and FFT, indicating in both cases the advantage of the former option. Taken together, we believe our results should be of significant practical value for practitioners in the field.\nIn future work, we plan to expand our analysis by considering additional HPs such as warmup ratio and weight decay. We will also compare our CBS approach with more advanced HPO algorithms. Finally, we intend to periodically update this work with new recommendations for HP configurations for additional models and tuning methods, aiming to further establish this work as a valuable resource in practice."}]}