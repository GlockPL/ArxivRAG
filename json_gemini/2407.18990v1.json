{"title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications", "authors": ["Alon Halfon", "Shai Gretz", "Ofir Arviv", "Artem Spector", "Orith Toledo-Ronen", "Yoav Katz", "Liat Ein-Dor", "Michal Shmueli-Scheuer", "Noam Slonim"], "abstract": "Fine-tuning Large Language Models (LLMs)\nis an effective method to enhance their perfor-\nmance on downstream tasks. However, choos-\ning the appropriate setting of tuning hyperpa-\nrameters (HPs) is a labor-intensive and compu-\ntationally expensive process. Here, we provide\nrecommended HP configurations for practical\nuse-cases that represent a better starting point\nfor practitioners, when considering two SOTA\nLLMs and two commonly used tuning methods.\nWe describe Coverage-based Search (CBS), a\nprocess for ranking HP configurations based on\nan offline extensive grid search, such that the\ntop ranked configurations collectively provide\na practical robust recommendation for a wide\nrange of datasets and domains. We focus our\nexperiments on Llama-3-8B and Mistral-7B, as\nwell as full fine-tuning and LoRa, conducting\na total of > 10,000 tuning experiments. Our\nresults suggest that, in general, Llama-3-8B\nand LoRA should be preferred, when possi-\nble. Moreover, we show that for both models\nand tuning methods, exploring only a few HP\nconfigurations, as recommended by our anal-\nsis, can provide excellent results in practice,\nmaking this work a valuable resource for prac-\ntitioners.", "sections": [{"title": "1 Introduction", "content": "Fine-tuning Large Language Models (LLMs) is\nan effective method to enhance their performance\nby adapting them to specific domains and tasks\n(Shi et al., 2023). This approach is particularly\nvaluable in real-world enterprise scenarios, where\nthere is often a need to address specific downstream\ntasks using available data, such as the company's\nproprietary data.\nRecently released base-models, such as Gemma\n(Team et al., 2024), Llama (Touvron et al.,\n2023a,b), and Mistral (Jiang et al., 2023), claim\nease of fine-tuning across various tasks (Zhao et al.,\n2024). However, comprehensive studies of these\nmodels in the context of fine-tuning are still limited,\nleaving several important questions less explored.\nIn this paper, we focus on the role of hyperparame-\nter optimization (HPO) in the fine-tuning process\nof LLMs, and provide detailed and concrete rec-\nommendations for HP values, aiming to save prac-\ntitioners time and computational resources. We\npresent Coverage-based Search (CBS), which lever-\nages an extensive grid search for highlighting an\neffective HP recommendation, as well as the ability\nto expand to a few promising HP configurations\nthat collectively suggest high performance across\ndiverse datasets and tasks.\nFor the purpose of providing these recommen-\ndations we conduct a comprehensive systematic\nstudy, focusing on practical scenarios where rela-\ntively small training data are available for tuning.\nWe examine prominent tasks such as classification,\nsummarization, and contextual question-answering\n(CQA) across various domains. Our study con-\nsiders two leading LLMs, Llama-3-8B (AI@Meta,\n2024) and Mistral-7B-v0.3 (Jiang et al., 2023), as\nwell as two commonly used fine-tuning methods:\nfull fine-tuning (FFT) and LoRA (Hu et al., 2021).\nOur main contributions are as follows:\n1. Recommended HP configurations for tuning,\noptimized per model and tuning method.\n2. Analysis of the differences between Llama-3-\n8B and Mistral-7B-v0.3, as well as between\nLORA and FFT, across 3 real-world tasks in\npractical scenarios.\n3. Analysis of the potential gain, accumulated\nby considering additional HP configurations\nsuggested by our analysis."}, {"title": "2 Related Work", "content": "HPO is an established research area, known for its\ncritical role in enhancing model performance (Yu\nand Zhu, 2020; Jin, 2022). The most straightfor-\nward approach for HPO is a grid search over the\nexponential space of HP values (Bergstra and Ben-\ngio, 2012). Since grid search is computationally\ndemanding, a large volume of research has been fo-\ncused on developing and evaluating more efficient\nHPO methods (Bergstra et al., 2011; Swersky et al.,\n2013; Snoek et al., 2012; Liu and Wang, 2021),\nwhile others consider over which HPs one should\nfocus on (Gkouti et al., 2024; Zhang and Duh,\n2020; Huang and Li, 2024). A few recent studies,\ndescribed next, aimed to provide concrete recom-\nmendations for HP settings. However, these works\ntypically considered a limited collection of datasets,\ntasks, or HPs. J et al. (2024) who fine-tuned the\nLlama-2 model on RAG and Code generation tasks,\ncompared FFT and LoRA, and provided some gen-\neral recommendations; however, their evaluation\nwas limited to a single dataset per task, considering\na small test set, and exploring a limited set of HP\nconfigurations. Zhang et al. (2024) examined the\neffects of scaling model size, data size, and PEFT\nparameters on machine translation and multilin-\ngual summarization, and found that larger data size\nimproved performance, while scaling PEFT param-\neters was ineffective. Tribes et al. (2024) evaluated\nLlama-2 7B using LoRA tuning, exploring config-\nurations of rank, alpha, dropout, and learning rate\non instruction datasets. Utilizing black box opti-\nmization techniques, they identified that a learning\nrate around $10^{-3.5}$ yielded the best results, while\nother HPs showed no decisive optimal values.\nIn contrast, the present work considers both FFT\nand LoRA, for two SOTA models, using a compre-\nhensive grid search across a large number of HP\nconfigurations, for a wide range of datasets, cover-\ning multiple domains and tasks. Thus, we expect\nthe recommendations suggested here to provide a\nsignificant added value on top of previous research\nin this area."}, {"title": "3 Experimental Setup", "content": "Our experimental setup is concisely depicted in Fig-\nure 1. For each pair of model and tuning method\nwe consider 3 tasks, multiple datasets, and 2 train-\ning sizes, as well as several HPs. For each of these\nHPs we consider multiple values, and apply a grid\nsearch over all the resulting HP configurations to\nidentify the best one. Next, we dive deeper into\neach part of this setup."}, {"title": "3.1 Tasks and Datasets", "content": "We consider 3 tasks: text classification, text summa-\nrization, and contextual question answering (CQA).\nFor text classification, we use 5 multi-class datasets\nfrom various domains, with class counts ranging\nfrom 6 to 100. For text summarization, we utilize 5\ndatasets from different domains, featuring diverse\ninput and output lengths. For CQA, we include 3\ndatasets, one of which (DoQA) consists of 3 sub-\ndatasets, one per domain, resulting in a total of 5\ndatasets for this task.\nWe adhere to the original train/validation/test\nsplits when available. Otherwise, we create train\nand validation splits, allocating a portion (which\ndiffers from dataset to dataset) of the training data\nto validation. Full details of the datasets are pro-\nvided in Appendix A."}, {"title": "3.2 Models", "content": "There is a plethora of models that can be tuned\nover labeled data, and naturally not all can be cov-\nered with limited resources. Thus, we considered\nrepresentatives of two of the most popular fami-\nlies of open-source models, restricting their size to\n8B or less for practical reasons: Llama-3-8B and\nMistral-7B-v0.3."}, {"title": "3.3 Fine-Tuning Methods", "content": "We explore two tuning techniques commonly used\nby practitioners: FFT and LoRA. FFT updates all\nmodel parameters, offering potential higher perfor-\nmance gain at a greater computational cost. LoRA\nreduces the number of learnable parameters by ap-\nproximating weight matrices, significantly lower-\ning computational overhead. Our study aims to\nidentify recommended HP configurations for each\napproach. In addition, we compare the two meth-"}, {"title": "3.4 HP Search Space", "content": "For FFT, we tune 4 key HPs: learning rate (LR),\nlearning rate scheduler (LR Scheduler), effec-\ntive batch size\u00b9 (Batch), and number of epochs\n(Epochs). For LoRA fine-tuning, we additionally\nexplore LoRA-specific HPs: the rank (LORAR) and\nscaling factor (LoRA), and fixing the scheduler.\nWe define a separate search space for each model\nand tuning method based on preliminary experi-\nments, assuming that optimal settings vary across\ndifferent models and tuning techniques. The full\nsearch space is detailed in the upper part of Table 1.\nThis comprehensive search involves 96 \u2013 288 con-\nfigurations for each tuple of model, tuning method,\nand dataset. Thus, in total, the results reported\nin this study are based on > 10,000 fine-tuning\nexperiments.2"}, {"title": "3.5 Coverage-based Search (CBS)", "content": "Next, we outline our approach to obtain recom-\nmended HP configurations using grid search results\nacross multiple datasets. Our strategy leverages the\ndiversity of tasks and domains, aiming for good\ncoverage for unseen datasets. For each model and\ntuning method, we evaluate all D datasets across\nthe 3 tasks, considering results obtained on training\nsizes M = m1, m2, using a set of possible HP con-\nfigurations C. The goal is to identify a ranking of\nHP configurations such that the top configurations\nyield consistently good results for most datasets\nand training sizes.\nFirst, for a given model, tuning method, dataset\nd, and training size m, we denote by s(c) the score\nobtained by HP configuration c. The score is nor-\nmalized w.r.t to the maximum score on d using the\nsame model, tuning method, and training size:\n$S_n(C) = \\frac{s(c)}{\\text{max}_{c \\in C} s(c)}$\nFor a given dataset d and training size m, we\ndenote the top configurations, $T_C^{d,m}$, as the configu-\nrations that receive a score that is at most 3 percent\nlower than the best configuration:3\n$TC(d, m) = {c \\in C | sn(c) > 0.97}$ .\nThat is, the set of (d, m) for which e provides a\ngood result (i.e., \u0441 \u2208 TC(d,m)) while higher-\nranked configurations do not. We finally sort TC*\nby the size of coverage(c) in descending order, to\nobtain a ranking over the HP configurations."}, {"title": "4 Evaluation Details", "content": "We focus on practical scenarios where training data\nis typically limited. Thus, for each dataset we eval-\nuate two variants of training data sizes: 100 and\n1000, sampled at random. The validation and test\nset sizes are 1000 for classification, 500 for summa-\nrization, and 329 \u2013 500 (depending on availability)\nfor CQA. Note, both training data sizes are used to\nidentify the recommended HP configurations, and\nwe do not optimize these recommendations for spe-\ncific training sizes. For downloading and process-\ning the datasets we use the Unitxt library (Bandel\net al., 2024): a collaborative framework for unified\ntextual data processing and evaluation which al-\nlows easy formatting, sharing, and reproducibility\nof LLM evaluation results. For classification, we\nensure each class has at least one train sample to\nprevent missing classes in the train set."}, {"title": "4.2 Training", "content": "We use the SFTTrainer from HuggingFace Trans-\nformers library (Wolf et al., 2020) with PyTorch\nFSDP. Each tuning and inference process utilizes\neither a single NVIDIA A100 with 80GB or a pair\nof them, operating at FP16 precision."}, {"title": "4.3 Methods to Select HP Configurations", "content": "For each model and tuning method, we aim to pro-\nvide HP recommendations for the practitioner. To\ndetermine the quality of these recommendations,\nwe consider the following recommendation alterna-\ntives.\nDefault. A common practice is to retrieve HP\nrecommendations from publicly available sources.\nFor LORA, we evaluate official HP recommenda-\ntions for tuning Llama and Mistral.6 For FFT, we\ncould not find HP recommendations in our litera-\nture and online search. Thus, we use the default\nparameters provided by HuggingFace.7 The HPs\ndefined by each Default configuration can be found\nat the bottom of Table 1.8\nCBS Leave-one-dataset-out (LOO). We evalu-\nate the approach presented in Section 3.5 in a LOO\nfashion, to simulate the benefit of its recommended\nHP configuration on new datasets. Note that we\nconsider only the single top configuration entailed\nby the CBS ranking. We denote this method as\nCBS_1. For each held-out dataset, dh, we calcu-\nlate the top HP configuration obtained by running\nCBS on D\\dh. We then take the score achieved by\nusing this configuration on the test set of dh.\nUpper Bound. We optimize the HPs of each\ndataset separately on its validation set, and report\nthe score on the test set. In other words, this is\na full grid search for each dataset, which is quite"}, {"title": "5 Results and Analysis", "content": "5.1 HP Recommendations\nTable 2 presents the average performance of CBS\ncompared to the Default configuration and the up-\nper bound for FFT. The results show that CBS_1\noutperforms the Default method by a large margin\nacross all tasks, models, and train sizes.\nThe results of the same experiment with LORA\nare shown in Table 3. Here, for LLama-3-8B, the\nperformance of CBS_1 is either comparable or\nslightly better than the Default configurations, in-\ndicating that the recommendations published for\nLlama-3-8B are beneficial. In contrast, for Mistral-\n8B-v0.3, CBS outperforms the baseline recommen-\ndation by a large margin in all cases. Thus, for\nboth models, the CBS_1 configuration can be\nconsidered a new HP recommendation for both\nFFT and LORA in the considered region of small\ntraining size.\n5.2 Upper Bound vs. CBS_1\nIn Tables 2 and 3 we consider the gap between the\nrecommendation provided by our CBS_1 approach,\nin LOO mode, compared to selecting the best con-\nfiguration found via a comprehensive HP search\nover the validation set of the individual dataset. Ev-\ndently, for Llama-3-8B our approach is quite close\nto the upper bound, while for Mistral-7B-v0.3 the\ngap is more evident. However, we note that in prac-\ntice, full HP search over the validation set is often\nnot feasible.\n5.3 Tuning Methods\nIn general, FFT is known to demand more computa-\ntional resources compared to LoRA. This is particu-"}, {"title": "5.4 Models", "content": "Overall, there is a clear advantage for LLama-3-8B\nover Mistral-7B-v0.3 across all dimensions. This\nfinding is in line with the ranking in the Fine-tuning\nLeaderboard where Llama-3-8B and Mistral-7B-\nv0.3 are ranked first and fifth, respectively (Zhao\net al., 2024)."}, {"title": "5.5 Train Data Size", "content": "As expected, moving from 100 to 1000 train sam-\nples improves the results across all tasks, models,\nand tuning methods. Notice, that despite the dif-\nferences in performance scores, the overall trends\nbetween the considered configurations (Default,\nCBS, upper bound) are qualitatively similar across\nthe data sizes."}, {"title": "5.6 Impact of Exploring Multiple CBS Recommendations", "content": "Next, we examine the impact of exploring more\nthan one configuration from our recommended\nranked list. To that end, we expand the evalua-\ntion of CBS described in Section 4.3 to consider\nadditional HP recommendations beyond the top\none. From a practical perspective. the budget is\ntherefore defined as the number of configurations\nin TC* we evaluate.\nFor each dh (the held-out dataset) and training\nsize m, we iterate over c \u2208 TC* according to the\nranking induced by coverage(c). Assuming we\nhave a budget of size k, we consider the top k\nconfigurations in TC*, and evaluate s(c) of each\nconfiguration on the validation set of hd. We mark\nthe configuration with the highest s(c) as Cbest. Fi-\nnally, we calculate sn(Chest) (the normalized score,\nsee Section 3.5) on the test set of dh. We then av-\nerage these scores over all held-out datasets and\ntraining sizes."}, {"title": "5.7 Recommendation for the Practitioner", "content": "Based on our experiments, we created practical\nHP recommendations for each model and tuning\nmethod. Table 4 shows the 4 top-ranked configu-\nrations, and we suggest to use these HP configura-\ntions in order, according to the available budget. As\nshown in Figure 2, using these 4 configurations, or\neven less, is expected to yield results nearly equiv-\nalent to full grid search over the HP space."}, {"title": "6 Conclusions", "content": "To effectively fine-tune LLMs, it is essential to\nuse a proper HP configuration, aligned with the\nmodel and tuning method at hand. Our work aims\nto contribute to the understanding of this aspect by\nproviding practitioners with recommended HP con-\nfigurations for two leading models and two tuning\nmethods. These recommendations represent the\noutcome of the analysis of the results of more than\n10, 000 fine-tuning experiments, across a large col-\nlection of datasets, representing different tasks and\ndomains. Furthermore, we provide comparative\nanalysis between Llama-3-8B and Mistral-7B-0.3,\nand between LoRA and FFT, indicating in both\ncases the advantage of the former option. Taken\ntogether, we believe our results should be of signif-\nicant practical value for practitioners in the field.\nIn future work, we plan to expand our analysis by\nconsidering additional HPs such as warmup ratio\nand weight decay. We will also compare our CBS\napproach with more advanced HPO algorithms. Fi-\nnally, we intend to periodically update this work\nwith new recommendations for HP configurations\nfor additional models and tuning methods, aiming\nto further establish this work as a valuable resource\nin practice."}, {"title": "A Datasets", "content": "The datasets considered in this work are presented\nin Table 5."}]}