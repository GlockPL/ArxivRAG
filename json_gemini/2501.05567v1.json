{"title": "Approximate Supervised Object Distance Estimation on Unmanned Surface Vehicles", "authors": ["Benjamin Kiefer", "Yitong Quan", "Andreas Zell"], "abstract": "Unmanned surface vehicles (USVs) and boats are increasingly important in maritime operations, yet their deployment is limited due to costly sensors and complexity. LiDAR, radar, and depth cameras are either costly, yield sparse point clouds or are noisy, and require extensive calibration. Here, we introduce a novel approach for approximate distance estimation in USVs using supervised object detection. We collected a dataset comprising images with manually annotated bounding boxes and corresponding distance measurements. Leveraging this data, we propose a specialized branch of an object detection model, not only to detect objects but also to predict their distances from the USV. This method offers a cost-efficient and intuitive alternative to conventional distance measurement techniques, aligning more closely with human estimation capabilities. We demonstrate its application in a marine assistance system that alerts operators to nearby objects such as boats, buoys, or other waterborne hazards.", "sections": [{"title": "I. INTRODUCTION", "content": "Unmanned surface vehicles (USVs) and autonomous boats promise efficient autonomous navigation and enhanced safety. However, the development and operational efficiency of USVs are hampered by the limitations of current sensing and perception technologies [1].\nTraditional sensors for distance estimation, such as LiDAR, radar, sonar, and depth cameras, play a crucial role in navigation and object detection [2]. LiDAR exhibits high costs and sparsity of data. It requires complex processing to interpret sparse point clouds, making it less effective in dynamic, cluttered maritime environments.\nRadar systems, while robust in harsh weather conditions, suffer from their own drawbacks. While less expensive than LiDAR, they are still orders of magnitude more expensive than monocular cameras. Their primary technical limitation is the inherent noise in radar signals, which leads to challenges in accurately detecting and classifying small or distant objects [3]. This limitation is particularly critical in busy maritime channels where the ability to detect small objects can be crucial for collision avoidance.\nSonar sensors are less effective for detecting surface-based obstacles due to their primary design for water depth estimation [4]. Besides their costliness and limited utility in surface object detection, sonar systems also raise environmental concerns. The acoustic emissions from sonar are known to be disruptive to marine life [5]. These sound waves, often loud, can disturb, disorient, or even harm aquatic animals, especially marine mammals that rely on sound for navigation and communication.\nDepth cameras offer a potential alternative, providing rich spatial data about the surroundings. However, they require precise calibration and are constrained by the lack of specialized datasets tailored for marine environments [2].\nMonocular depth estimation leverage neural networks to estimate depth from single images (for each pixel). While promising in theory, this technique is hindered by the quality and variety of available datasets. Existing datasets are predominantly land-based and do not represent the unique challenges of maritime environments, such as water reflections, varying lighting conditions, and diverse objects [6].\nLastly, trigonometry-based approaches by estimating the orientation of the camera via an onboard IMU (or horizon finding methods), and subsequent ray-casting yield poor results due to the difficulties of obtaining a precise own-orientation in the presence of many different acceleration forces [1] and as this projection is ill-posed with such acute viewing angles [7]."}, {"title": "II. RELATED WORK", "content": "Research in the intersection of computer vision and USVs has seen progress in areas, such as horizon estimation [8], [9], [10], [11], [12], [13], [14], [15], semantic segmentation [16], [17], [18], [19], [20], panoptic segmentation [21], [22], [23], anomaly/obstacle detection [24], [19], [25], heading estimation [26], [27], and more. Please see the survey or workshop papers on the current progress of maritime computer vision [28], [29], [30], [31], [32], [33].\nThe field of object detection has matured within the last years from CNN-based two-stage detectors [34], [35], [36], one-stage detectors [37], [38], [39], up to transformer-based detectors [40], [41], [42]. While there are attempts on making transformer-based detectors embedded-friendly [43], [44], [45], the default choice for embedded detectors are CNN-based one-stage detectors, such as the YOLO series [46]. In our experiments, we mostly rely on these as they are the most prevalent in this domain. In the maritime domain, object detection has often been connected to obstacle detection for downstream obstacle avoidance [47], [48], [28]. We refer to [29] for a discussion on challenges in maritime object detection.\nSince many vessels don't employ an automatic identification system (AIS) and it also only transponds signals slowly, the need of distance estimation through other means is inevitable [49]. There are many radar-based approaches, focusing on reducing the noise or classifying radar blobs [50], [51], [52]. However, radar suffers from the usual challenges of reflectivity and lack of interpretability [53]. LiDAR-based approaches are investigated as well, but lack the resolution for wider distances [54], [55], [56]. Both sensor types are considerably more expensive than cameras.\nHence, vision-based solutions are in the main focus of current USV-based computer vision research. Traditional approaches are triangulation-based, e.g. stereo-vision approaches were investigated in [57], [58], [59], where depth information is obtained by comparing the disparity between two horizontally displaced cameras. This method provides accurate distance estimation for objects at varying ranges, but the accuracy diminishes with increasing distance due to reduced disparity. E.g., experiments on KITTI show a poor performance for distances beyond ~100m [60]. Also, the maritime domain has mostly poor features, often only showing moving water and sky. Furthermore, stereo cameras are prone to misconfiguration and very sensitive to calibration.\nTraditional monocular approaches rely on geometric triangulation based on knowing the ego-pose and camera intrinsics [61], [26]. However, these approaches are ill-posed with small variations in self-orientation estimation leading to large variations in pixel space [62]. [63], [64] show that parallax-based approaches can outperform stereo vision for greater distances, but it remains inaccurate for larger distances.\nLately, fully monocular depth estimation approaches by means of end to end neural networks have been investigated as a means of gauging distances [65], [66], [6]. While promising, these approaches rely on large amounts of accurate depth maps, either from stereo cameras, lidar, or synthetic data. Furthermore, they tend to work in indoor scenes with limited distances only [67].\nContrary to these approaches, we focus on approximate object-based supervised distance estimation [68]. The maritime domain has unique challenges and our approach has the benefit to work with larger distances, can be trained fully end to end without any considerable overhead and is simple to integrate into existing object detection architectures. Please see Section V for an analysis and comparison of different methods.\nLastly, to facilitate this research direction, we collected and annotated a large dataset with bounding box labels accompanied with distances. Current maritime CV datasets focus on object detection, multi-object tracking, semantic segmentation [69] or focus on other sensor modalities [70]. However, CV-based distance estimation is a relatively young domain without any publicly available datasets."}, {"title": "III. METHODOLOGY", "content": "We employ the YOLOv7 and YOLOv9 series of object detectors for our task. These detectors are known for being on the Pareto front of accuracy and real-time performance [71]. To adapt these models for distance estimation, we modify the architecture to predict an additional output for distance as part of each anchor as shown in Figure 2.\nThe adjusted model architecture integrates distance prediction by including an extra output neuron for each anchor. This additional output allows the model to predict the distance to the detected object directly. This formulation allows us\nHowever, these networks predict numbers that are \"well-behaved\", in that they are mostly centered around zero and are of low magnitude [71]. Directly predicting the metric distances would result in unstable training or very poor performance. Hence, we experimented with various normalization strategies for the distance prediction branch, including linear scaling, logarithmic scaling, and hybrid approaches to handle different ranges and distributions of distances.\nWe explored four main normalization strategies for the distance prediction:\n1) Linear Normalization: The linear normalization scales the actual distance d to a normalized value y in the range [0, 1]:\n$y = \\frac{d}{d_{max}}$\nDuring inference, we recover the predicted distance d from the network's output y:\n$\\hat{d} = y \\times d_{max}$\n2) Logarithmic Normalization: For logarithmic normalization, we apply a logarithmic transformation and scale to [0, 1]:\n$y = \\frac{log(d + \\epsilon)}{log(d_{max} + \\epsilon)}$\nHere, $\\epsilon$ is a small positive constant to avoid taking the logarithm of zero (e.g., $\\epsilon$ = 1).\nDuring inference:\n$\\hat{d} = exp (y \\times log(d_{max} + \\epsilon)) \u2013 \\epsilon$\n3) Linear Negative Normalization: To map distances to the range [-1,1]:\n$y=2(\\frac{d}{d_{max}})-1$\nDuring inference:\n$d = (\\frac{y+1}{2}) \\times d_{max}$\n4) Logarithmic Negative Normalization: Combining logarithmic scaling with mapping to [-1,1]:\n$y = 2 \\frac{log(d + \\epsilon)}{log(d_{max} + \\epsilon)} -1$\nDuring inference:\n$\\hat{d} = exp ((\\frac{y+1}{2}) \\times log(d_{max} +\\epsilon)) - \\epsilon$\nBy applying these normalization strategies, we ensure that the network's distance predictions y are within ranges suitable for neural network outputs. During training, the loss is computed between the normalized predicted distances and the normalized ground truth distances. During inference, the inverse transformations are applied to map the network's outputs back to actual distance values d.\nThe model is trained using a composite loss function that balances several components:\n*   Objectness Loss: Penalizes incorrect predictions of object presence.\n*   Classification Loss: Ensures accurate classification of detected objects.\n*   Localization Loss: Measures the accuracy of bounding box predictions.\n*   Distance Loss: Evaluates the accuracy of the distance predictions.\nWe leave the loss functions for the first three losses unchanged, and use $L_1$ as the distance loss."}, {"title": "E. Dataset Collection and Annotation", "content": "Our evaluation dataset includes images captured from USVs, manually annotated with bounding boxes and distances. We captured the images using rented boats on the east coast of the US. For ground truth evaluation (only for that - for our proposed method, we use manually labelled data), we determined distances to static objects such as buoys and docked boats using the USV's GPS position and heading and verified manually with known chart positions. For the chart data, we manually downloaded and fused buoy data for the US from NOAH.\nOur evaluation dataset consists of 1,000 images captured from USVs in various maritime environments, including open waters, harbors, and coastal regions. Each image is annotated with bounding boxes around objects such as boats, buoys, and other obstacles, along with their respective distances from the USV. Distances are measured using GPS positioning for static objects. This comprehensive dataset ensures robust training and evaluation of our model.\nWe created our own labeling tool, which combines all these components: bounding box labeling, accompanying chart data integration, and association labelling. See Figure 5 for a screenshot from its UI. We're making it publicly available. Also see Figure 3 for distance histogram."}, {"title": "F. Human Labeling For More Data", "content": "To avoid the need for metadata alongside the image data, we also explored the approach of having human annotators gauge distances to objects. For this, we asked a professional labeling service company to label bounding boxes and alongside of it distances. With this approach, we obtained another 5,000 images with pseudo-distances. We'll explore in the experiment section how useful this data is."}, {"title": "IV. EVALUATION METRICS", "content": "To evaluate the performance of our object detection and distance estimation model, we use two primary metrics: mean Average Precision (mAP) and a novel weighted distance error metric. We calculate mean Average Precision (mAP) using an Intersection over Union (IoU) threshold of 0.5 to determine correct detections.\nThe weighted distance error metric evaluates the accuracy of the distance predictions while considering the confidence of each detection. For each detected object i, let $d_i$ be the ground truth distance, $\\hat{d_i}$ be the predicted distance, and $c_i$ be the confidence score of the detection. The distance error $e_i$ is given by $e_i = |d_i - \\hat{d_i}|$.\nTo weight the distance error by the confidence, we define the weighted distance error E as:\n$E = \\frac{\\sum_{i=1}^{M} c_i e_i}{\\sum_{i=1}^{M} c_i}$\nwhere M is the total number of detections. This metric ensures that higher confidence detections have a greater impact on the overall error, reflecting their presumed accuracy. Similar to the mAP calculation, we only consider distance errors for detections whose IoU with the ground truth exceeds a predefined threshold. This ensures that only sufficiently accurate object detections contribute to the weighted distance error metric."}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate the performance of our proposed approach for approximate distance estimation on unmanned surface vehicles (USVs). We conduct experiments to assess both object detection accuracy and distance estimation precision. The evaluation includes comparisons between different model architectures, analysis of distance estimation errors across distance intervals and object types, and benchmarking against other distance estimation methods. All experiments are performed using our maritime dataset described in Section III."}, {"title": "A. Object Detection Performance", "content": "We first evaluate the object detection capabilities of our modified YOLO models. Table I presents the precision, recall, mean Average Precision at IoU threshold 0.5 (mAP@0.5), mean Average Precision across IoU thresholds from 0.5 to 0.95 (mAP@0.5:0.95), and inference speed measured in frames per second (FPS). The models are tested"}, {"title": "B. Distance Estimation Accuracy", "content": "To assess the distance estimation performance, we analyze the weighted distance error across different distance intervals and object types. Before selecting the normalization strategy for the distance prediction, we conducted preliminary tests with the different normalization methods described in Section III, including linear normalization, logarithmic normalization, linear negative normalization, and logarithmic negative normalization. We found that only the basic linear normalization yielded satisfactory results. The other methods either did not converge during training or resulted in unstable predictions. Therefore, we proceeded with the basic linear normalization in our experiments.\nTable II presents the weighted distance errors (in meters) for boats and buoys over distance ranges from 0 to 500 meters (clipped), using different model architectures.\nThe results indicate that the distance estimation error increases with the distance to the object, which is expected due to reduced visual cues at greater distances. Our models consistently perform better on boats than buoys, likely because boats are larger and have more distinct features. Among the models, YOLOv7 and YOLOv9-M achieve lower average distance errors, demonstrating the effectiveness of our approach in accurately estimating distances."}, {"title": "C. Comparison with Other Methods", "content": "We compare our proposed method with traditional triangulation-based distance estimation (based on orientation via pitch and roll) and state-of-the-art monocular depth estimation models (dense distance values are projected into bounding box by taking the median distance within a box). Table III summarizes the mean distance error (MDE) in meters, the percentage of outliers, and the inference speed in FPS for each method.\nOur proposed method achieves the lowest mean distance error and the smallest percentage of outliers while maintaining real-time performance. Traditional triangulation methods and monocular depth estimation models either have higher"}, {"title": "D. Loss Balancing and Ablation Studies", "content": "We conducted ablation studies to understand the impact of the distance loss gain on the model's performance for both object detection (OD) and distance estimation. As shown in Table IV, the distance loss gain significantly affects the balance between OD accuracy and distance estimation precision.\nThe results indicate that increasing the distance loss gain improves distance estimation accuracy (lower MAE) but significantly reduces OD performance (lower mAP). A lower distance loss gain (e.g., 0.001) maintains higher OD accuracy but at the cost of less precise distance predictions. Balancing these loss components is crucial for optimizing both OD and distance estimation tasks."}, {"title": "E. Smoothing the Distance Estimates", "content": "To handle distance estimation in video sequences, where objects move and change positions over time, we use the Simple Online and Realtime Tracking (SORT) algorithm [72]. SORT tracks objects across frames by combining object detection results with a Kalman filter and the Hungarian algorithm for data association, ensuring that each detected object maintains a consistent identity across frames.\nAlongside tracking, we keep a running average of the distance estimates for each tracked object. This simple averaging technique helps to smooth the distance predictions over time, filtering out outliers caused by sudden detection errors or rapid changes in object appearance. The approach is computationally efficient and provides stable distance estimates, which are crucial for downstream tasks like collision avoidance in dynamic maritime environments.\nBy combining SORT with a running average for distance smoothing, we achieve robust and real-time performance on Unmanned Surface Vehicles (USVs), ensuring that distance predictions are both accurate and temporally consistent. Table V shows how tracking and smoothing reduces the Mean Distance Error (MDE) and the percentage of outliers."}, {"title": "F. Evaluation of Human-Labeled Distances", "content": "To assess the accuracy of human-labeled distances compared to chart-derived ground truth data, we examine human estimations of distances to boats and buoys in maritime settings. The errors in human distance estimation generally increase with the distance to the object."}, {"title": "VI. DISCUSSION AND LIMITATIONS", "content": "Our method provides a cost-effective alternative to traditional sensors for USVs, enhancing safety and operational efficiency. However, there are limitations to consider.\nA key limitation is the sensitivity of our distance estimation to changes in the camera's field of view (FoV) and other camera parameters like focal length and mounting position. The model learns to associate object sizes with distances based on the training FoV. If deployed with a different FoV or zoom level, the model could produce inaccurate estimates, as it cannot inherently differentiate between varying FoVs. For instance, a narrower FoV might make distant objects appear closer, leading to erroneous distance predictions. Future work could address this by incorporating FoV information or calibration steps to ensure consistent distance estimation across different camera setups."}, {"title": "VII. CONCLUSION", "content": "We have introduced a novel approach for approximate distance estimation on USVs using supervised object detection, providing a cost-efficient alternative to traditional sensors. Our method achieves solid performance in both object detection and distance estimation while maintaining real-time speeds suitable for USV applications. Future work may focus on handling varying camera parameters and environmental conditions to enhance robustness."}]}