[{"title": "AudioGenX: Explainability on Text-to-Audio Generative Models", "authors": ["Hyunju Kang", "Geonhee Han", "Yoonjae Jeong", "Hogun Park"], "abstract": "Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio to-ken level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.", "sections": [{"title": "Introduction", "content": "Text-to-audio generation models (TAG) (Kreuk et al. 2023;\nZiv et al. 2024; Yang et al. 2023; Liu et al. 2023; Schnei-\nder et al. 2023) have emerged as a pivotal technology in\ngenerative AI, enabling textual content to be transformed\ninto an auditory experience. Although models such as Au-\ndioGen (Kreuk et al. 2023) excel at generating high-quality\naudio based on textual prompts, a critical challenge remains:\nthe lack of transparency in how each textual input affects the\ngenerated audio. Consequently, users may struggle to trust\nthe model, making it essential to provide explanations for\nthe TAG task. Explainability provides several key advan-\ntages. First, it enhances awareness of how input tokens af-\nfect the model's outputs, enabling users to ensure that the\nmodel emphasizes the correct aspects of the text. Second, it\nprovides actionable insights to support the decision-making\nabout which elements to modify and to what extent in the au-\ndio editing process. Third, analyzing generated explanations\ncan aid with debugging and identifying potential biases. Ac-\ncordingly, this study argues that the ability to quantify the\nimportance of textual inputs in TAG models is crucial to\nbeing able to unambiguously assess and communicate their\nvalue.\nWhile approaches specifically tailored for explaining\nTAG models are limited, recent research has explored\nmethodologies for calculating the importance of input\ntokens in large-scale transformer-based models. Cross-\nattention layers in multi-modal architectures, such as those\nin TAG models, are widely regarded as critical for inte-\ngrating textual and auditory information, while also enhanc-\ning explainability by revealing how information from one\nmodality influences another. A notable method (Abnar and\nZuidema 2020) utilizes attention weights and aggregates\nthem across all layers to approximate the importance of each\ninput token. However, attention scores alone are not con-\nsidered reliable for causal insights, as they do not directly\nindicate how perturbation to specific inputs influences the\noutput. Recently, AtMan (Deiseroth et al. 2023) introduced\na perturbation method that suppresses the attention score of\none token at a time to observe the impact of each input on\noutput prediction. This single-token perturbation approach,\nhowever, may overlook interactions between multiple to-\nkens. Consequently, it provides less reliable explanations in\nscenarios where the model heavily relies on the contextual\nrelationships between multiple tokens, leading to an over-\nsimplification of the model's behavior.\nTo address the challenge of faithful explanations, causal\ninference theory, encompassing factual and counterfactual"}, {"title": "Related Work", "content": "Text-to-Audio Generation Models. Recent text-to-audio\ngeneration models can be categorized into two model ar-\nchitectures: Transformer-based (Kreuk et al. 2023; Ziv et al.\n2024) and Diffusion-based (Yang et al. 2023; Liu et al. 2023;\nSchneider et al. 2023). Transformer models, such as Audio-\nGen (Kreuk et al. 2023), employ autoregressive Transform-\ners to predict discrete audio tokens, while MAGNET (Ziv\net al. 2024) enhances efficiency through masked genera-\ntive modeling in a non-autoregressive scheme. Diffusion-\nbased approaches such as Diffsound (Yang et al. 2023)\ngenerate discrete mel-spectrogram tokens, whereas models\nlike AudioLDM (Liu et al. 2023) and Mo\u00fbsai (Schneider\net al. 2023) directly predict continuous mel-spectrograms or\nwaveforms. Despite architectural differences, these models\ncommonly use cross-attention mechanisms, making Audio-\nGenX a model-agnostic explainer for TAG models that use\ncross-attention in audio generation.\nExplainable AI. Explainability involves methods that\nhelp to understand the importance of each input token with\nrespect to output predictions. These methods generally fall\ninto two categories: gradient-based methods (Selvaraju et al.\n2017; Sundararajan, Taly, and Yan 2017; Nagahisarchoghaei\net al. 2023) and perturbation-based methods (Ribeiro, Singh,\nand Guestrin 2016; Lundberg and Lee 2017). Gradient-\nbased explanation methods trace gradients from the target\nlayers to the predictive value, using the calculated gradients\nas a measure of importance. While effective, these methods\nrequire substantial memory resources to store the values of\neach targeted layer. In contrast, perturbation-based methods,\nsuch as SHAP (Lundberg and Lee 2017), are more memory-\nefficient, calculating feature importance by comparing pre-\ndictions with and without specific features. Similarly, our\nmethod adopts a perturbation-based approach to effectively\ngenerate explanations.\nExplainability on Audio Processing Models. Existing\nexplainability approaches (Akman and Schuller 2024) on\naudio processing models have extended generic explana-\ntion methods. For instance, one study (Becker et al. 2018)\nemploys Layer-wise Relevance Propagation (LRP) to ex-\nplain the model trained on raw waveforms and spectrograms\nfor spoken digit and speaker gender classification. Another\nstudy applied DFT-LRP (Frommholz et al. 2023) to audio\nevent detection, assessing the significance of time-frequency\ncomponents and guiding input representation choices. Sim-\nlarly, audioLIME (Haunschmid, Manilow, and Widmer\n2020) extends LIME (Ribeiro, Singh, and Guestrin 2016)\nto explain music-tagging models by perturbing audio com-\nponents derived from source separation. However, since the\nabove methods focus on explaining audio continuously and\nsequentially, they are not directly applicable to the unique\nchallenges posed by TAG models, which require techniques\nthat address the complex interactions between text inputs\nand generated audio outputs.\nExplainability on Transformer. With the widespread use\nof Transformers, the demand for explainability has grown.\nPrimarily, Rollout (Abnar and Zuidema 2020) primarily ag-\ngregates attention weights in all layers to track informa-\ntion flow but struggles to integrate cross-attention weights\nin multi-modal models with differing domain dimension-\nalities. Another recent work (Chefer, Gur, and Wolf 2021)\nleverages Layer-wise Relevance Propagation (LRP) (Samek,\nWiegand, and M\u00fcller 2017) to calculate class-specific rel-\nevance scores based on gradients of attention weights in"}, {"title": "The Proposed AudioGenX", "content": "AudioGenX addresses the challenge of explaining TAG\nmodels, where the goal is to quantify the importance of tex-\ntual input corresponding to the generated audio. To achieve\nthis within a sequence-to-sequence framework, we decom-\npose the explanation target, represented as sequential audio,\ninto individually non-sequential audio tokens. Since the out-\nput is sequential data, calculating gradients across the entire\nsequence, from the first to the last token, is computationally\nexpensive and time-consuming. To overcome these issues,\nwe redefine the explanation target as individual audio to-\nkens, rather than the entire sequence. This modification en-\nables parallel computation of generating an explanation for\neach token, significantly speeding up the process. Finally,\nAudioGenX integrates these individual token-level explana-\ntions to provide a comprehensive understanding of the entire\naudio sequence. An overview of AudioGenX is illustrated in\nFigure 2.\nDefinition of Masks as Explanations\nWe quantify the importance of the t-th audio token zt within\nthe audio sequence using a mask as the explanation. The\nsoft mask is denoted as $M_{u,z\u0142} \u2208 R^{L\u00d71}$, where each element\n$m_{ui,zt} \u2208 M_{U,zt}$ represents the importance of the i-th tex-\ntual token with respect to the t-th audio token zt. Each value\nlies in the range [0,1], where a value close to 1 indicates\nthat the corresponding textual token is highly important for\ngenerating the target audio token, while a value closer to 0\nindicates lower importance. To serve as a soft mask repre-\nsenting the importance of each text token, AudioGenX opti-\nmizes the Explainer to predict the mask $M_{u,z\u0142}$ as the expla-\nnation. The Explainer consists of Multi-Layer Perceptrons\n(MLPs) with a sigmoid and gumbel-softmax (Jang et al.\n2017) function to constrain values within the range [0,1]\nwithout additional scaling and to enforce the values close\nto either 0 or 1, thereby highlighting relatively distinguished\ncontribution. Using the soft mask, we apply perturbation to\nmodify the inner computational steps of the cross-attention\nlayers, altering the attention score of the given textual in-\nput. Consequently, we measure the perturbation effect on the\nprediction at the layer $f(U, z_{t-1}) = e_t$, observing how la-\ntent representation vector $e_t$ for the audio token z\u0142 changes\nunder these perturbations. In the following section, we detail\nhow we optimize Explainer to predict the mask as expla-\nnations based on both factual and counterfactual reasoning.\nFormulating Factual Explanations\nFactual reasoning (Tan et al. 2022; Ali et al. 2023; Kenny\net al. 2021) aims to find sufficient input that can approx-\nimately reproduce the original prediction. To quantify the\nsufficiency of textual tokens, we employ a perturbation-\nbased method using the soft mask, interleaving the computa-\ntion to measure the impact of changes. Specifically, we mask\nout attention scores in the cross-attention layers where tex-\ntual information is fed into the TAG model. We formulate\nthe perturbation in factual reasoning as:\n$\\hat{g}(Q, K, V, M_{u,zz}) = (\u03c3(\\frac{QK^T}{\\sqrt{d_k}}) \\odot M_{u,ze})V$,", "latex": ["M_{u,z\u0142} \u2208 R^{L\u00d71}", "m_{ui,zt} \u2208 M_{U,zt}", "f(U, z_{t-1}) = e_t", "\\hat{g}(Q, K, V, M_{u,zz}) = (\u03c3(\\frac{QK^T}{\\sqrt{d_k}}) \\odot M_{u,ze})V"]}, {"title": "Objective Function for AudioGenX", "content": "Along with factual and counterfactual explanation objective\nfunctions, we add the regularization term to generate the ex-\nplanation mask in a simple and efficient manner. Therefore,\nwe incorporate additional regularization in our final objec-\ntive function for the Explainer, which is formulated as:\n$L = L_F + L_{CF} + a\\cdot L_1(M_{U,zt}) + \u03b2\\cdot L_2 (M_{U,zt})$.\nHere, $L_1$ and $L_2$ represent the $L_1$-Norm and L2-Norm, re-\nspectively, as regularization terms to minimize the mask\nsize. This prevents a trivial solution where the Explainer\ngenerates an explanation mask assigning equal importance\nto all values. At the same time, adhering to Occam's Ra-\nzor principle, we favor simpler and more effective explana-\ntions (Tan et al. 2022; Blumer et al. 1987). Hence, according\nto the objective function in Equation (6), AudioGenX opti-\nmizes the Explainer generating faithful explanation masks\nin the audio-token level.\nProviding Audio-Level Explanations\nIn this section, we aggregate audio token-level explanations\nto provide a comprehensive understanding of the entire au-\ndio sequence. The aggregation is performed by averaging\nthe mask values across all audio tokens as follows:\n$M_{U,Z} = \\frac{1}{T}\\sum_{t=1}^{T}M_{U,Zt}$,\nwhere t refers to the step, and T represents the total length\nof generated audio. Additionally, it is possible to focus on\na specific interval of interest within the audio, defined be-\ntween a starting step s and an ending step n. This is denoted\nas $M_{U,z} = \\frac{1}{n-s+1}\\sum_{t=s}^{n}M_{U,zt}$, which provides granular\nexplanations based on the user's request. This flexible ap-\nproach enables users to discover patterns within specific in-\ntervals, as AudioGenX can effectively capture and explain\nauditory content in targeted regions of the audio sequence.", "latex": ["L = L_F + L_{CF} + a\\cdot L_1(M_{U,zt}) + \u03b2\\cdot L_2 (M_{U,zt})", "M_{U,Z} = \\frac{1}{T}\\sum_{t=1}^{T}M_{U,Zt}"]}, {"title": "Experimental Setup", "content": "Dataset. We use AudioCaps (Kim et al. 2019) as the source\nof textual prompts. For each prompt, we generate a 5-second\naudio clip using AudioGen, pairing each prompt with its cor-\nresponding generated audio. For hyperparameter tuning, we\nselect 100 validation captions, while the test dataset consists\nof 1,000 randomly selected captions.\nEvaluation Metrics. We evaluate explanations based on\ntwo metrics: Fidelity and KL divergence, both derived from\nthe classification probabilities of a pre-trained audio classi-\nfier. Specifically, we utilize PaSST (Cai et al. 2022), a clas-\nsifier trained on the AudioSet dataset, which is also used\nin the evaluation of AudioGen. Its classification probabili-\nties are likely to provide meaningful insights into the rela-\ntionship between textual prompts and generated audio. Fi-\ndelity (Yuan et al. 2021; Ali et al. 2023), a core evaluation\nmetric in the field of XAI, measures the change in top-1 label\nprediction probabilities of the generated audio after apply-\ning factual and counterfactual explanation masks, denoted\nas FidF and FidCF, respectively.\nIn addition, KL divergence (Kilgour et al. 2018), origi-\nnally used to evaluate audio generative models (Kreuk et al.\n2023; Yang et al. 2023; Huang et al. 2023), measures the\ndifferences of label distribution between generated and ref-\nerence audio. For explanation evaluation, we introduce new\nmetrics KLF and KLCF, which measure the conceptual\nchange in the generated audio after applying explanation\nmasks in factual and counterfactual reasoning, respectively.\nIn factual evaluation, the generated audio should closely\nmatch the original audio, making lower values Fide and\nKLF desirable. In contrast, in counterfactual evaluation,\nhigher values of FidCF and KLCF indicate a more effec-\ntive explanation. Additionally, we include the average mask\nsize as part of our evaluation.\nBaselines. We compare our method with five baselines.\nRandom-Mask is a mask with randomly assigned values\nranging between 0 and 1. Grad-CAM (Selvaraju et al.\n2017) is evaluated in two variations: Grad-CAM-a and\nGrad-CAM-e. Specifically, Grad-CAM-a computes the gra-\ndients of the latent representation vector of the t-th audio\ntoken et with respect to the generated audio sequence zt,\nwhile Grad-CAM-e computes the gradients of the last cross-\nattention map to the zt. We also include the AtMan (Deis-\neroth et al. 2023) and the method proposed by Chefer et\nal. (Chefer, Gur, and Wolf 2021) as baselines.\nExperimental Setting. The Explainer model includes\na linear layer that reduces the text token embeddings from\n1536 to 512 dimensions, followed by a PReLU activation\nfunction. The 512-dimensional text token embeddings are\nthen mapped to a single value through another linear layer\nand a sigmoid function, producing a value in the [0, 1] range.\nA Gumbel-Softmax function is subsequently applied to push\nvalues closer to 0 or 1, representing the importance of each\ntext token. The Explainer is trained for 50 epochs with a\nlearning rate as \u00d710-3 using the Adam optimizer. Hyperpa-\nrameters are set as a = 1\u00d710-3 and \u03b2 = 1\u00d710-\u00b9 as coeffi-\ncients for the explanation objective function. Hyperparame-\nter sensitivity analysis and detailed experimental settings are\nboth provided in the Appendix. Our code is available at the\nfollowing link 1."}, {"title": "Experimental Results", "content": "RQ 1: Does AudioGenX Generate Faithful\nExplanations?\nWe evaluate the generated explanations by AudioGenX\nbased on factual and counterfactual reasoning, as presented\nin Table 1. AudioGenX achieves the best performance across\nthe metrics FidF, FidCF, KLF, and KLCF, while also\nmaintaining the smallest size (Size), demonstrating that our\nexplanations are both simple and effective. The baseline, de-\nnoted as Naudio = 5, generates audio conditioned on the\nsame textual input five times to observe the inherent vari-\nance, serving as the lower bound for FidF, KLF. Audio-\nGenX's factual audio nearly reaches the lower bound, indi-\ncating high performance. Furthermore, significant changes\nin FidCF and KLCF under counterfactual perturbations\nconfirm that the explanations are both sufficient and neces-\nsary. The AudioGenX with factual and counterfactual losses\nin Eq.(6), outperforms the variants AudioGenX w/ Eq. (3)\nand AudioGenX w/ Eq. (5), which apply only factual or\ncounterfactual loss with a regularization term. This indicates\nthat the two losses complement each other, enhancing over-\nall performance. Furthermore, we evaluate AudioGenX w/\nEq. (7) using an averaged explanation mask, showing the ro-\nbustness of explainability in describing the entire audio. In\ncontrast, other baselines fail to generate meaningful coun-\nterfactual audio, lacking the optimization properties needed\nto enforce counterfactual explanations.\nThe strong performance highlights the effectiveness of\nleveraging latent embedding vectors to generate explana-\ntions. While most baselines are designed to explain super-\nvised learning models, they rely on vectors that represent\nthe probability distribution of the final audio token. This ap-\nproach, however, does not align well with the inference pro-\ncess of audio generation models. In extreme cases, such as\ntop-k sampling (k=250), the 250-th audio token could be\nsampled, leading to significant discrepancies between the\ngradients or probability-related information the token most\nlikely predicted by the model. In contrast, our approach\navoids dependency on the sampling process, allowing the\nmodel to produce more faithful explanations.\nRQ 2: How Well Do the Explanations from\nAudioGenX Reflect the Generated Audio?\nWe visualize the explanations generated by AudioGenX and\nother baselines, as shown in Figure 3. AudioGenX demon-\nstrates a clear advantage in focusing on key audio ele-\nments. Unlike other baselines, which often assign relatively\nhigh importance scores to less important tokens like 'A'\nand 'with', AudioGenX consistently assigns higher impor-\ntance scores to crucial tokens such as 'ticktocks' and 'mu-\nsic'. For instance, AudioGenX assigns a notably high im-\nportance score of 0.96 to 'music', emphasizing its ability to\nfocus on significant input tokens. In contrast, other models\nlike Grad-CAM-e and AtMan distribute importance more\nbroadly, including less relevant tokens. These results show\nthat AudioGenX consistently provides faithful explanations,\naligning the generated audio with the essential components\nof the input text.\nFurthermore, when generating audio from a prompt con-\ntaining multiple concepts, some words may be less promi-\nnently reflected. In such case, AudioGenX provides ad-\nequate explanations for each specific audio, indicating\nwhether each word from the prompt has been incorporated\ninto the generated audio. As illustrated in Figure 4, the\ndifference between the two audios is that bird sounds are\npresent in Figure 4-(a) but absent in Figure 4-(b). Audio-\nGenX effectively describes the audios by assigning high im-\nportance scores of 0.98 and 0.99 to the token 'Water,' which\nis the primary sound in both audios. AudioGenX assigns a"}, {"title": "RQ 3: How Can Explanations Help Understand AudioGen Behavior?", "content": "We explore the output patterns of AudioGen using the ex-\nplanations generated by AudioGenX. First, we investigate\nwhether AudioGen can effectively handle sentences contain-\ning negations and double negations, as shown in Figure 5.\nThe explanations of the generated audios are presented in\nresponse to input prompts containing 'without thunder' and\n'without no thunder.' In both cases, the generated audio in-\ncludes the sound of thunder along with the rain. Using Au-\ndioGenX, we observe that 'without' and 'without no' have\nlower importance compared to 'thunder' in the explanations.\nWe hypothesize that this occurs because the training dataset\nlacks sufficient examples of negation and double negation.\nAn examination of the AudioCaps dataset reveals a scarcity\nof such cases. Additionally, by aggregating tokens from the\nexplanations, we identify the top and bottom 50 tokens in\nTable 3 in the Appendix. Tokens with high importance are\npredominantly nouns, such as 'thunder,' while those with\nlow importance include sound descriptors like 'distant,' as\nwell as sequential expressions like 'before.' Such analyses\ncould be used to debug TAG models or to identify potential\ninherent biases in their behavior."}, {"title": "RQ 4: Does AudioGenX Generate Explanations Efficiently?", "content": "We evaluate the efficiency of explanation methods based on\nthe average time and total GPU memory usage per explana-\ntion, as shown in Table 2. For GPU memory efficiency, the\nresults rank in the following order: AtMan, Grad-CAM-e,\nAudioGenX, Grad-CAM-a, and Chefer et al. For time ef-\nficiency, the order is AtMan, Grad-CAM-e, Chefer et al.,\nGrad-CAM-a, and AudioGenX. Although AtMan is the\nmost efficient, its performance remains subpar due to its sim-\nplistic approach. Grad-CAM-e demonstrates greater mem-\nory efficiency compared to Grad-CAM-a and Chefer et al.,\nas it tracks a shallower layer. While AudioGenX requires\nadditional computational time to train explanation masks, it\nachieves memory efficiency by reducing GPU storage and\noperates with O(Lk) complexity, ensuring linear scalability\nfor large-scale tasks."}, {"title": "Conclusion", "content": "AudioGenX quantifies the importance of textual tokens cor-\nresponding to generated audio by leveraging both factual\nand counterfactual reasoning frameworks. This approach en-\nables the generation of faithful explanations, providing ac-\ntionable insights for users to edit audio and assisting de-\nvelopers in debugging. Consequently, AudioGenX enhances\nthe transparency and trustworthiness of TAG models."}, {"title": "Appendix", "content": "Evaluation metrics\nWe detail the four evaluation metrics discussed in the main\nmanuscript. Z represents the audio token sequence gener-\nated by a text-to-audio model given the i-th text prompt in\nthe dataset. The sequences (ZF); and (ZCF); are produced\nby applying factual and counterfactual masks, respectively,\nas defined in Eqs. (2) and (4). These masks serve as expla-\nnations for both Zi and its associated text prompt. To eval-\nuate these explanations, we use the pre-trained audio classi-\nfier PaSST (Koutini et al. 2021), denoted as q, which gener-\nates a prediction probability distribution q(Z). Specifically,\nq(Z)y; represents the prediction probability for class yi. Fi-\nnally, N is the total number of data points, L is the number\nof text tokens in the text prompt, and T is the total length\nof the audio token sequence. The evaluation metrics FidF,\nFidCF (Yuan et al. 2021, 2022; Ali et al. 2023) are defined\nas:\n$Fidr = \\frac{1}{N} \\sum_{i=1}^{N}q(Z_i)_{yi} - q((Z_F)_i)_{yi}$,\n$FidcF = \\frac{1}{N} \\sum_{i=1}^{N}q(Z_i)_{yi} \u2013 q((Z_{CF})_i)_{yi}$,\nwhere yi is the predicted class for q(Z), defined as Yi\n= arg maxcec q(Zi)c, and C is the set of all classes that q\npredicts. Moreover, KLF, KLCF (Kreuk et al. 2023; Yang\net al. 2023; Huang et al. 2023), and Size are defined as:\n$KLF = \\frac{1}{N}\\sum_{i=1}^{N}DKL (q(Z_i) || q((Z_F)_i))$,\n$KLCF = \\frac{1}{N}\\sum_{i=1}^{N}DKL (q(Z_i) || q((Z_{CF})_i))$,\n$Size = \\frac{1}{NLT}\\sum_{i=1}^{N}\\sum_{l=1}^{L}\\sum_{t=1}^{T}(muz,z)$\nHere, DKL refers to the KL divergence. bsectionDetails on\nbaseline setting While there is no existing XAI model specif-\nically designed for explaining text-to-audio generative mod-\nels, we adopt Transformer explanation methods for evalua-\ntion.\nRollout (Abnar and Zuidema 2020), a method for\nexplaining Transformers, proposes aggregating attention\nscores recursively by multiplying attention maps in all lay-\ners. The proposed method named rolling out the attention\nweights is formulated as below:\n$A(l) = \\begin{cases} A(l_i)A(l_{i-1}) & \\text{if } i > j \\\\ A(l_j) & \\text{if } i = j, \\end{cases}$\nwhere A means attention rollout. A(li) represents i-th raw\nattention map. However, applying Rollout in models with\ncross-attention blocks designed to handle multi-modality is\nchallenging because the dimensions of the attention maps\ndo not match. Therefore, we exclude Rollout from our base-\nlines.", "latex": ["Fidr = \\frac{1}{N} \\sum_{i=1}^{N}q(Z_i)_{yi} - q((Z_F)_i)_{yi}", "FidcF = \\frac{1}{N} \\sum_{i=1}^{N}q(Z_i)_{yi} \u2013 q((Z_{CF})_i)_{yi}", "KLF = \\frac{1}{N}\\sum_{i=1}^{N}DKL (q(Z_i) || q((Z_F)_i))", "KLCF = \\frac{1}{N}\\sum_{i=1}^{N}DKL (q(Z_i) || q((Z_{CF})_i))", "Size = \\frac{1}{NLT}\\sum_{i=1}^{N}\\sum_{l=1}^{L}\\sum_{t=1}^{T}(muz,z)", "A(l) = \\begin{cases} A(l_i)A(l_{i-1}) & \\text{if } i > j \\\\ A(l_j) & \\text{if } i = j, \\end{cases}"]}, {"title": null, "content": "Grad-CAM (Selvaraju et al. 2017) computes the gradi-\nents of the output activations from the target layer with re-\nspect to the final prediction. The importance map is then cal-\nculated as:\n$\\hat{A} = \\mathbb{E}((V_A \\odot A)^+)$,\nwhere A represents the output activations of the target layer,\nand VA represents the gradient of these activations with re-\nspect to the prediction. Specifically, VA comprises the gra-\ndients computed for each selected codebook, which are aver-\naged afterward. denotes element-wise multiplication, and\n(.)+ extracts positive values. Grad-CAM-a calculates gra-\ndient of the latent representation vector et, corresponding\nto the t-th audio token. The 1,536 dimensions of the au-\ndio token embedding are treated as separate channels, and\ntheir mean is used to compute the token importance score.\nGrad-CAM-e derives the gradient of the last cross-attention\nmap, and their mean is used as the token importance score.\nAtMan (Deiseroth et al. 2023) extracts important tokens\nby perturbation of a single token. For all cross-attention lay-\ners and all heads, we multiply 1 k with the pre-softmax\nattention scores for the target text tokens to introduce per-\nturbation. The value k is consistently set to 0.9, following\nthe configuration in (Deiseroth et al. 2023). To quantify the\ninfluence of tokens, we calculate the difference in cross-\nentropy for each codebook and use the sum of these differ-\nences as the token importance.\nChefer et al. (Chefer, Gur, and Wolf 2021) calculates the\nrelevance score, following attention layers. In our experi-\nments, we follow (Chefer, Gur, and Wolf 2021). However,\nsince we do not consider the influence of the text encoder,\nwe replace it with an identity matrix.\nTo scale importance values between 0 and 1, we apply\nMax scaling for each sequence for all baselines except At-\nMan, For AtMan, which includes negative values, we use\nMin-Max scaling.\nExperimental Setting\nIn our experiments, we used the following packages and\nhardware:\n\u2022\n\u2022\nPython 3.9.18\nspacy==3.5.2\n\u2022 torch>=2.1.0\n\u2022 torchaudio>=2.1.0\n\u2022 Transformers>=4.31.0\nAll computations were performed using a single NVIDIA\nA100 GPU.\nHyperparameter Sensitivity Analysis\nWe conduct a hyperparameter sensitivity analysis using var-\nious combinations of hyperparameters on the validation\ndataset. The validation dataset is randomly sampled from the\nvalidation set of AudioCaps (Kim et al. 2019). As illustrated\nin Figure 6-(a), when the value of a decreases from 0.1 to\n0.001, Fide significantly decreases from 0.398 to 0.138.\nFurthermore, when \u00df is adjusted from 0.1 to 0.001 while\nkeeping a = 0.001, FidF shows a slight increase from", "latex": ["\\hat{A} = \\mathbb{E}((V_A \\odot A)^+)"]}, {"title": null, "content": "be emphasized. To correct this", "follows": "n$M_{U_{Zt"}]}, {}]