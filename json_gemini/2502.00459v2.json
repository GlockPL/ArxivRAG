{"title": "AudioGenX: Explainability on Text-to-Audio Generative Models", "authors": ["Hyunju Kang", "Geonhee Han", "Yoonjae Jeong", "Hogun Park"], "abstract": "Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio to-ken level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.", "sections": [{"title": "Introduction", "content": "Text-to-audio generation models (TAG) (Kreuk et al. 2023; Ziv et al. 2024; Yang et al. 2023; Liu et al. 2023; Schneider et al. 2023) have emerged as a pivotal technology in generative AI, enabling textual content to be transformed into an auditory experience. Although models such as AudioGen (Kreuk et al. 2023) excel at generating high-quality audio based on textual prompts, a critical challenge remains: the lack of transparency in how each textual input affects the generated audio. Consequently, users may struggle to trust the model, making it essential to provide explanations for the TAG task. Explainability provides several key advantages. First, it enhances awareness of how input tokens affect the model's outputs, enabling users to ensure that the model emphasizes the correct aspects of the text. Second, it provides actionable insights to support the decision-making about which elements to modify and to what extent in the audio editing process. Third, analyzing generated explanations can aid with debugging and identifying potential biases. Accordingly, this study argues that the ability to quantify the importance of textual inputs in TAG models is crucial to being able to unambiguously assess and communicate their value.\nWhile approaches specifically tailored for explaining TAG models are limited, recent research has explored methodologies for calculating the importance of input tokens in large-scale transformer-based models. Cross-attention layers in multi-modal architectures, such as those in TAG models, are widely regarded as critical for integrating textual and auditory information, while also enhancing explainability by revealing how information from one modality influences another. A notable method (Abnar and Zuidema 2020) utilizes attention weights and aggregates them across all layers to approximate the importance of each input token. However, attention scores alone are not considered reliable for causal insights, as they do not directly indicate how perturbation to specific inputs influences the output. Recently, AtMan (Deiseroth et al. 2023) introduced a perturbation method that suppresses the attention score of one token at a time to observe the impact of each input on output prediction. This single-token perturbation approach, however, may overlook interactions between multiple tokens. Consequently, it provides less reliable explanations in scenarios where the model heavily relies on the contextual relationships between multiple tokens, leading to an over-simplification of the model's behavior.\nTo address the challenge of faithful explanations, causal inference theory, encompassing factual and counterfactual reasoning, is often utilized (Pearl 2009). These two approaches aim to identify impactful input information in different ways. Factual reasoning focuses on identifying critical input information that reproduces the original prediction, whereas counterfactual reasoning (Tan et al. 2022; Ali et al. 2023; Kenny et al. 2021) seeks to determine crucial input information that, if absent, would change the prediction. Given their differing assumptions, these reasoning approaches can be employed together as complementary frameworks to generate more faithful explanations. However, prior research has yet to investigate the feasibility of applying factual and counterfactual reasoning within TAG models.\nTo provide faithful explanations for the TAG model, we introduce AudioGenX, a perturbation-based explainability method leveraging factual and counterfactual reasoning. Our approach utilizes the latent representation vectors in TAG models to observe the effects of factual and counterfactual perturbations. These perturbations are applied in the cross-attention layer using a soft mask, enabling the simultaneous perturbation of multiple tokens' attention scores. More importantly, the mask itself serves as an explanation, with its values quantifying the importance of the textual input. We optimize the mask through a gradient descent method guided by our proposed factual and counterfactual objective functions. To mitigate the high computational cost of calculating gradients for the entire sequential audio, we enhance efficiency by decomposing the explanation target into individual audio tokens. This approach enables us to customize the explanation range of generated audio interactively, providing comprehensive explanations for the entire audio or more granular explanations for specific segments of interest, depending on user demand. For instance, in Figure 1, (a) provides a comprehensive explanation for the entire audio, indicating a strong relation to vehicle motion. By focusing on a specific interval in (b) and (c), AudioGenX captures the different contexts of each audio segment and delivers contextually accurate explanations accordingly. Extensive experiments demonstrate the faithfulness of our explanations and benchmark their performance against recent techniques using proposed evaluation metrics for audio generation tasks.\nContributions. We summarize our contributions as follows: 1) We propose a faithful explanation method for text-to-audio generation models, grounded in factual and counterfactual reasoning to quantify the importance of text tokens to the generated audio. 2) We offer a framework that provides both holistic and granular audio explanations based on user requests, enabling tailored insights. 3) We introduce new evaluation metrics for text-to-audio explanations and demonstrate the effectiveness of AudioGenXthrough extensive experiments compared to existing methods. 4) We present case studies demonstrating how AudioGenX provides valuable insights to support the understanding of model behavior and editing tasks."}, {"title": "Related Work", "content": "Text-to-Audio Generation Models. Recent text-to-audio generation models can be categorized into two model architectures: Transformer-based (Kreuk et al. 2023; Ziv et al. 2024) and Diffusion-based (Yang et al. 2023; Liu et al. 2023; Schneider et al. 2023). Transformer models, such as AudioGen (Kreuk et al. 2023), employ autoregressive Transformers to predict discrete audio tokens, while MAGNET (Ziv et al. 2024) enhances efficiency through masked generative modeling in a non-autoregressive scheme. Diffusion-based approaches such as Diffsound (Yang et al. 2023) generate discrete mel-spectrogram tokens, whereas models like AudioLDM (Liu et al. 2023) and Mo\u00fbsai (Schneider et al. 2023) directly predict continuous mel-spectrograms or waveforms. Despite architectural differences, these models commonly use cross-attention mechanisms, making Audio-GenX a model-agnostic explainer for TAG models that use cross-attention in audio generation.\nExplainable AI. Explainability involves methods that help to understand the importance of each input token with respect to output predictions. These methods generally fall into two categories: gradient-based methods (Selvaraju et al. 2017; Sundararajan, Taly, and Yan 2017; Nagahisarchoghaei et al. 2023) and perturbation-based methods (Ribeiro, Singh, and Guestrin 2016; Lundberg and Lee 2017). Gradient-based explanation methods trace gradients from the target layers to the predictive value, using the calculated gradients as a measure of importance. While effective, these methods require substantial memory resources to store the values of each targeted layer. In contrast, perturbation-based methods, such as SHAP (Lundberg and Lee 2017), are more memory-efficient, calculating feature importance by comparing predictions with and without specific features. Similarly, our method adopts a perturbation-based approach to effectively generate explanations.\nExplainability on Audio Processing Models. Existing explainability approaches (Akman and Schuller 2024) on audio processing models have extended generic explanation methods. For instance, one study (Becker et al. 2018) employs Layer-wise Relevance Propagation (LRP) to explain the model trained on raw waveforms and spectrograms for spoken digit and speaker gender classification. Another study applied DFT-LRP (Frommholz et al. 2023) to audio event detection, assessing the significance of time-frequency components and guiding input representation choices. Similarly, audioLIME (Haunschmid, Manilow, and Widmer 2020) extends LIME (Ribeiro, Singh, and Guestrin 2016) to explain music-tagging models by perturbing audio components derived from source separation. However, since the above methods focus on explaining audio continuously and sequentially, they are not directly applicable to the unique challenges posed by TAG models, which require techniques that address the complex interactions between text inputs and generated audio outputs.\nExplainability on Transformer. With the widespread use of Transformers, the demand for explainability has grown. Primarily, Rollout (Abnar and Zuidema 2020) primarily aggregates attention weights in all layers to track information flow but struggles to integrate cross-attention weights in multi-modal models with differing domain dimensionalities. Another recent work (Chefer, Gur, and Wolf 2021) leverages Layer-wise Relevance Propagation (LRP) (Samek, Wiegand, and M\u00fcller 2017) to calculate class-specific relevance scores based on gradients of attention weights in self- and cross-attention layers. Nevertheless, AtMan (Deiseroth et al. 2023) raises the issue of excessive memory usage and introduces a scalable explanation method that employs single-token perturbation to observe the change of loss in the response. While intuitive and memory-efficient for large-scale models, this method is limited in its ability to account for the interrelationship of input tokens."}, {"title": "Preliminaries", "content": "AudioGen (Kreuk et al. 2023), a representative TAG model, consists of three key components: a text encoder (Raffel et al. 2020), an autoregressive Transformer decoder model (Vaswani et al. 2017), and an audio decoder (D\u00e9fossez et al. 2023). The Transformer decoder serves as the core model responsible for generating the audio sequence, while the text encoder processes the input text and the audio decoder post-processes the generated audio token sequence into audio. Given a text prompt, it is converted into a tokenized representation vector, denoted as \\(U = [u_1, ..., u_L], U \\in R^{L \\times d_U}\\), where L denotes number of textual tokens and \\(d_U\\) represents a dimension of the textual token representation vector. The generated audio can be expressed in a discrete form, as EnCodec (D\u00e9fossez et al. 2023) converts the audio into either discrete tokens or continuous token representations. The tokenized audio sequence is denoted as \\(Z = [z_1, ..., z_T], Z \\in N^{T \\times d_V}\\), where T denotes the length of the audio sequence and \\(d_V\\) indicates the number of codebooks \\(d_V\\). In detail, the codebook is a structured set of discrete audio tokens used in multi-stream audio generation to produce high-quality audio. For more comprehensive information on multi-streaming audio generation, we refer to the original AudioGen paper (Kreuk et al. 2023).\nFor the generation of an audio sequence, the Transformer-decoder model (Vaswani et al. 2017), denoted as h, generates \\(z_t\\) as t-th order audio token in the sequence, following the formulation \\(h(U, Z_{t-1}) = z_t\\). For brevity, we omit the detailed notation of other components and the top-p or top-k sampling process in the Transformer. Instead, we focus on the attention layers, including cross-attention, which are crucial components of the model, denoted as f. The computation within these layers is expressed in a simplified version as \\(f(U, Z_{t-1}) = e_t\\), where \\(e_t\\) represents the latent representation vector corresponding to the t-th audio token. In the absence of ground truth and class labels, the latent embedding vector \\(e_t\\) in the audio token space provides information on how perturbation impacts subsequent generations. Particularly, the cross-attention layer is essential to fuse the textual information with auditory information in layers f, we denote the cross-attention layers as:\n\\(g(Q, K, V) = \\sigma(\\frac{QK^T}{\\sqrt{d_k}})V,\\) (1)\nwhere \\(sigma\\) indicates a softmax function, Q, K, V, \\(d_k\\) refers to query, key, values, and the number of vector dimensions in the k-th layer, respectively. In detail, Q refers to previously generated audio tokens, representing the query information, whileK and V correspond to the textual tokens."}, {"title": "The Proposed AudioGenX", "content": "AudioGenX addresses the challenge of explaining TAG models, where the goal is to quantify the importance of textual input corresponding to the generated audio. To achieve this within a sequence-to-sequence framework, we decompose the explanation target, represented as sequential audio, into individually non-sequential audio tokens. Since the output is sequential data, calculating gradients across the entire sequence, from the first to the last token, is computationally expensive and time-consuming. To overcome these issues, we redefine the explanation target as individual audio tokens, rather than the entire sequence. This modification enables parallel computation of generating an explanation for each token, significantly speeding up the process. Finally, AudioGenX integrates these individual token-level explanations to provide a comprehensive understanding of the entire audio sequence. An overview of AudioGenX is illustrated in Figure 2.\nDefinition of Masks as Explanations\nWe quantify the importance of the t-th audio token \\(z_t\\) within the audio sequence using a mask as the explanation. The soft mask is denoted as \\(M_{U,z_t} \\in R^{L \\times 1}\\), where each element \\(m_{ui,z_t} \\in M_{U,z_t}\\) represents the importance of the i-th textual token with respect to the t-th audio token \\(z_t\\). Each value lies in the range [0,1], where a value close to 1 indicates that the corresponding textual token is highly important for generating the target audio token, while a value closer to 0 indicates lower importance. To serve as a soft mask representing the importance of each text token, AudioGenX optimizes the Explainer to predict the mask \\(M_{U,z_t}\\) as the explanation. The Explainer consists of Multi-Layer Perceptrons (MLPs) with a sigmoid and gumbel-softmax (Jang et al. 2017) function to constrain values within the range [0,1] without additional scaling and to enforce the values close to either 0 or 1, thereby highlighting relatively distinguished contribution. Using the soft mask, we apply perturbation to modify the inner computational steps of the cross-attention layers, altering the attention score of the given textual input. Consequently, we measure the perturbation effect on the prediction at the layer \\(f(U, z_{t-1}) = e_t\\), observing how latent representation vector \\(e_t\\) for the audio token \\(z_t\\) changes under these perturbations. In the following section, we detail how we optimize Explainer to predict the mask as explanations based on both factual and counterfactual reasoning.\nFormulating Factual Explanations\nFactual reasoning (Tan et al. 2022; Ali et al. 2023; Kenny et al. 2021) aims to find sufficient input that can approximately reproduce the original prediction. To quantify the sufficiency of textual tokens, we employ a perturbation-based method using the soft mask, interleaving the computation to measure the impact of changes. Specifically, we mask out attention scores in the cross-attention layers where textual information is fed into the TAG model. We formulate the perturbation in factual reasoning as:\n\\[\\hat{g}(Q, K, V, M_{U,z_t}) = (\\sigma(\\frac{QK^T}{\\sqrt{d_k}}) \\odot M_{U,z_t})V,\\] (2)"}, {"title": "", "content": "where \\(sigma\\) denotes the softmax activation function and the mask \\(M_{U,z_t}\\) controls the amount of information corresponding to the text token. When the mask value \\(m_{ui,z_t}\\) approaches 0, the attention score is suppressed, meaning the information corresponding to the textual token is not fully propagated to the subsequent layer. Conversely, as the mask value approaches 1, the original value is fully preserved. To distinguish this process from the original layer \\(f(U, z_{t-1})\\), we denote the layer applying perturbation with the factual mask as \\(f(U, z_{t-1}, M_{U,z_t})\\).\nWhen the mask sufficiently serves as a factual explanation, the perturbed output remains approximately the same as the original prediction. To evaluate the impact of perturbation, we measure the resulting changes in the latent representation vector within the audio token space. Since the latent vector encodes rich and implicit information, we expect that two vectors close to each other indicate a similar auditory meaning, which is likely to result in similar audio generation. By leveraging this vector similarity, we can effectively measure the influence of perturbation and formulate the objective function for Explainer as:\n\\[L_F = -cos(f(U,z_{t-1}), \\hat{f}(U, Z_{t-1}, M_{U,z_t})),\\] (3)\nwhere cos function refers to cosine similarity, which measures how similar factual result \\(\\hat{f}(U, Z_{t-1}, M_{U,z_t})\\) is to the original prediction \\(f(U,z_{t-1})\\) in the audio token space. Since the objective function involves negative cosine similarity, minimizing the loss function corresponds to maximizing the similarity. Hence, following the objective function, the Explainer generates the factual explanation mask, ensuring that the two representations or predictions are as close as possible in the audio token space.\nFormulating Counterfactual Explanations\nCounterfactual reasoning (Tan et al. 2022; Ali et al. 2023; Kenny et al. 2021) aims to identify necessary inputs that can significantly alter the original prediction when it is perturbed or removed. This perturbation operates in the opposite direction of factual explanations, removing the important input to observe the counterfactual result. We formulate the perturbation method in counterfactual reasoning as:\n\\[\\hat{g}(Q, K, V, 1 - M_{U,z_t}) = (\\sigma(\\frac{QK^T}{\\sqrt{d_k}}) \\odot (1 - M_{U,z_t}))V,\\] (4)"}, {"title": "", "content": "where \\(1 \\in R^{1 \\times T_U}\\) is a vector of ones and 1-\\(M_{U,z_t}\\) subtracts the importance of the corresponding textual tokens. Consequently, the more important a textual token is, the more its attention score is suppressed in proportion to its importance. This perturbation operates under a counterfactual assumption as the What-If scenario (Tan et al. 2022; Ali et al. 2023; Kenny et al. 2021): What happens if the important textual token does not exist? After applying the perturbation in Equation (4), the counterfactual result is observed as \\(f(U, z_{t-1}, 1 - M_{U,z_t})\\). If the counterfactual result significantly differs from the original prediction, it indicates that the counterfactual mask is necessary to explain the original prediction. Conversely, if the change is trivial, the counterfactual mask is unnecessary to explain the causal relationship with the prediction.\nGenerally, counterfactual explanations in supervised settings aim to find the important inputs that change the prediction with minimal perturbation. However, no class labels or guidance are available in our task of audio generation. Instead, we measure the change of meaning in latent space leveraging the cosine similarity function after counterfactual perturbation. Thus, the counterfactual explanation objective function is formulated as:\n\\[L_{CF} = cos(f(U,z_{t-1}), \\hat{f}(U, z_{t-1}, 1 - M_{U,z_t})),\\] (5)\nwhere cos function measures how dissimilar counterfactual result \\(f(U, z_{t-1}, 1 - M_{U,z_t})\\) is to the original prediction in latent space. As the cosine similarity decreases, the objective function minimizes the similarity. Consequently, the Explainer generates the counterfactual explanation mask to ensure that the two representations or predictions are as far as possible in the audio token space after counterfactual perturbation.\nObjective Function for AudioGenX\nAlong with factual and counterfactual explanation objective functions, we add the regularization term to generate the explanation mask in a simple and efficient manner. Therefore, we incorporate additional regularization in our final objective function for the Explainer, which is formulated as:\n\\[L = L_F + L_{CF} + \\alpha \\cdot L_1(M_{U,z_t}) + \\beta \\cdot L_2 (M_{U,z_t}).\\] (6)\nHere, \\(L_1\\) and \\(L_2\\) represent the \\(L_1\\)-Norm and \\(L_2\\)-Norm, respectively, as regularization terms to minimize the mask size. This prevents a trivial solution where the Explainer generates an explanation mask assigning equal importance to all values. At the same time, adhering to Occam's Razor principle, we favor simpler and more effective explanations (Tan et al. 2022; Blumer et al. 1987). Hence, according to the objective function in Equation (6), AudioGenX optimizes the Explainer generating faithful explanation masks in the audio-token level.\nProviding Audio-Level Explanations\nIn this section, we aggregate audio token-level explanations to provide a comprehensive understanding of the entire audio sequence. The aggregation is performed by averaging the mask values across all audio tokens as follows:\n\\[M_{U,Z} = \\frac{1}{T}\\sum_{t=1}^{T} M_{U,z_t},\\] (7)\nwhere t refers to the step, and T represents the total length of generated audio. Additionally, it is possible to focus on a specific interval of interest within the audio, defined between a starting step s and an ending step n. This is denoted as \\(M_{U,Z} = \\frac{1}{n-s+1}\\sum_{t=s}^{n} M_{U,z_t}\\), which provides granular explanations based on the user's request. This flexible approach enables users to discover patterns within specific intervals, as AudioGenX can effectively capture and explain auditory content in targeted regions of the audio sequence."}, {"title": "Experimental Setup", "content": "Dataset. We use AudioCaps (Kim et al. 2019) as the source of textual prompts. For each prompt, we generate a 5-second audio clip using AudioGen, pairing each prompt with its corresponding generated audio. For hyperparameter tuning, we select 100 validation captions, while the test dataset consists of 1,000 randomly selected captions.\nEvaluation Metrics. We evaluate explanations based on two metrics: Fidelity and KL divergence, both derived from the classification probabilities of a pre-trained audio classifier. Specifically, we utilize PaSST (Cai et al. 2022), a classifier trained on the AudioSet dataset, which is also used in the evaluation of AudioGen. Its classification probabilities are likely to provide meaningful insights into the relationship between textual prompts and generated audio. Fidelity (Yuan et al. 2021; Ali et al. 2023), a core evaluation metric in the field of XAI, measures the change in top-1 label prediction probabilities of the generated audio after applying factual and counterfactual explanation masks, denoted as FidF and FidCF, respectively.\nIn addition, KL divergence (Kilgour et al. 2018), originally used to evaluate audio generative models (Kreuk et al. 2023; Yang et al. 2023; Huang et al. 2023), measures the differences of label distribution between generated and reference audio. For explanation evaluation, we introduce new metrics KLF and KLCF, which measure the conceptual change in the generated audio after applying explanation masks in factual and counterfactual reasoning, respectively. In factual evaluation, the generated audio should closely match the original audio, making lower values Fide and KLF desirable. In contrast, in counterfactual evaluation, higher values of FidCF and KLCF indicate a more effective explanation. Additionally, we include the average mask size as part of our evaluation.\nBaselines. We compare our method with five baselines. Random-Mask is a mask with randomly assigned values ranging between 0 and 1. Grad-CAM (Selvaraju et al. 2017) is evaluated in two variations: Grad-CAM-a and Grad-CAM-e. Specifically, Grad-CAM-a computes the gradients of the latent representation vector of the t-th audio token et with respect to the generated audio sequence zt, while Grad-CAM-e computes the gradients of the last cross-attention map to the zt. We also include the AtMan (Deiseroth et al. 2023) and the method proposed by Chefer et al. (Chefer, Gur, and Wolf 2021) as baselines.\nExperimental Setting. The Explainer model includes a linear layer that reduces the text token embeddings from 1536 to 512 dimensions, followed by a PReLU activation function. The 512-dimensional text token embeddings are then mapped to a single value through another linear layer and a sigmoid function, producing a value in the [0, 1] range. A Gumbel-Softmax function is subsequently applied to push values closer to 0 or 1, representing the importance of each text token. The Explainer is trained for 50 epochs with a learning rate as \u00d710-3 using the Adam optimizer. Hyperparameters are set as a = 1\u00d710-3 and \u03b2 = 1\u00d710-\u00b9 as coefficients for the explanation objective function. Hyperparameter sensitivity analysis and detailed experimental settings are both provided in the Appendix. Our code is available at the following link 1."}, {"title": "Experimental Results", "content": "RQ 1: Does AudioGenX Generate Faithful Explanations?\nWe evaluate the generated explanations by AudioGenX based on factual and counterfactual reasoning, as presented in Table 1. AudioGenX achieves the best performance across the metrics FidF, FidCF, KLF, and KLCF, while also maintaining the smallest size (Size), demonstrating that our explanations are both simple and effective. The baseline, denoted as Naudio = 5, generates audio conditioned on the same textual input five times to observe the inherent variance, serving as the lower bound for FidF, KLF. Audio-GenX's factual audio nearly reaches the lower bound, indicating high performance. Furthermore, significant changes in FidCF and KLCF under counterfactual perturbations confirm that the explanations are both sufficient and necessary. The AudioGenX with factual and counterfactual losses in Eq.(6), outperforms the variants AudioGenX w/ Eq. (3) and AudioGenX w/ Eq. (5), which apply only factual or counterfactual loss with a regularization term. This indicates that the two losses complement each other, enhancing overall performance. Furthermore, we evaluate AudioGenX w/ Eq. (7) using an averaged explanation mask, showing the robustness of explainability in describing the entire audio. In contrast, other baselines fail to generate meaningful counterfactual audio, lacking the optimization properties needed to enforce counterfactual explanations.\nThe strong performance highlights the effectiveness of leveraging latent embedding vectors to generate explanations. While most baselines are designed to explain supervised learning models, they rely on vectors that represent the probability distribution of the final audio token. This approach, however, does not align well with the inference process of audio generation models. In extreme cases, such as top-k sampling (k=250), the 250-th audio token could be sampled, leading to significant discrepancies between the gradients or probability-related information the token most likely predicted by the model. In contrast, our approach avoids dependency on the sampling process, allowing the model to produce more faithful explanations.\nRQ 2: How Well Do the Explanations from AudioGenX Reflect the Generated Audio?\nWe visualize the explanations generated by AudioGenX and other baselines, as shown in Figure 3. AudioGenX demonstrates a clear advantage in focusing on key audio elements. Unlike other baselines, which often assign relatively high importance scores to less important tokens like 'A' and 'with', AudioGenX consistently assigns higher importance scores to crucial tokens such as 'ticktocks' and 'music'. For instance, AudioGenX assigns a notably high importance score of 0.96 to 'music', emphasizing its ability to focus on significant input tokens. In contrast, other models like Grad-CAM-e and AtMan distribute importance more broadly, including less relevant tokens. These results show that AudioGenX consistently provides faithful explanations, aligning the generated audio with the essential components of the input text.\nFurthermore, when generating audio from a prompt containing multiple concepts, some words may be less prominently reflected. In such case, AudioGenX provides adequate explanations for each specific audio, indicating whether each word from the prompt has been incorporated into the generated audio. As illustrated in Figure 4, the difference between the two audios is that bird sounds are present in Figure 4-(a) but absent in Figure 4-(b). AudioGenX effectively describes the audios by assigning high importance scores of 0.98 and 0.99 to the token 'Water,' which is the primary sound in both audios. AudioGenX assigns a score of 0.54 to 'birds,' while it assigns a score of 0.14, accurately reflecting the different audio characteristics in each case. These results show that AudioGenX can provide explanations that are well-suited to the corresponding audio. Furthermore, these explanations serve as valuable insights for editing generated audio to better align with user intention."}, {"title": "RQ 3: How Can Explanations Help Understand AudioGen Behavior?", "content": "We explore the output patterns of AudioGen using the explanations generated by AudioGenX. First, we investigate whether AudioGen can effectively handle sentences containing negations and double negations, as shown in Figure 5. The explanations of the generated audios are presented in response to input prompts containing 'without thunder' and 'without no thunder.' In both cases, the generated audio includes the sound of thunder along with the rain. Using AudioGenX, we observe that 'without' and 'without no' have lower importance compared to 'thunder' in the explanations. We hypothesize that this occurs because the training dataset lacks sufficient examples of negation and double negation. An examination of the AudioCaps dataset reveals a scarcity of such cases. Additionally, by aggregating tokens from the explanations, we identify the top and bottom 50 tokens in Table 3 in the Appendix. Tokens with high importance are predominantly nouns, such as 'thunder,' while those with low importance include sound descriptors like 'distant,' as well as sequential expressions like 'before.' Such analyses could be used to debug TAG models or to identify potential inherent biases in their behavior."}, {"title": "RQ 4: Does AudioGenX Generate Explanations Efficiently?", "content": "We evaluate the efficiency of explanation methods based on the average time and total GPU memory usage per explanation, as shown in Table 2. For GPU memory efficiency, the results rank in the following order: AtMan, Grad-CAM-e, AudioGenX, Grad-CAM-a, and Chefer et al. For time efficiency, the order is AtMan, Grad-CAM-e, Chefer et al., Grad-CAM-a, and AudioGenX. Although AtMan is the most efficient, its performance remains subpar due to its simplistic approach. Grad-CAM-e demonstrates greater memory efficiency compared to Grad-CAM-a and Chefer et al., as it tracks a shallower layer. While AudioGenX requires additional computational time to train explanation masks, it achieves memory efficiency by reducing GPU storage and operates with O(Lk) complexity, ensuring linear scalability for large-scale tasks."}, {"title": "Conclusion", "content": "AudioGenX quantifies the importance of textual tokens corresponding to generated audio by leveraging both factual and counterfactual reasoning frameworks. This approach enables the generation of faithful explanations, providing actionable insights for users to edit audio and assisting developers in debugging. Consequently, AudioGenX enhances the transparency and trustworthiness of TAG models."}, {"title": "Appendix", "content": "Evaluation metrics\nWe detail the four evaluation metrics discussed in the main manuscript. Z represents the audio token sequence generated by a text-to-audio model given the i-th text prompt in the dataset. The sequences (ZF); and (ZCF); are produced by applying factual and counterfactual masks", "as": "n\\[Fidr = \\frac{1"}, {"q((Z^F)_i)_{y_i},\\": 8, "q((Z^{CF})_i)_{y_i},\\": 9, "as": "n\\[KLF = \\frac{1"}, {"q((Z^F)_i)),\\": 10, "q((Z^{CF})_i)),\\": 11, "mu_{l,z_t})\\": 12, "below": "n\\[A(l) = \\begin{cases"}, "A(l_i)A(l_{i-1}) & if i > j\\\\A(l_i) & if i = j,\\end{cases}\\"], "as": "n\\[\\tilde{A} = E((V_A \\odot A)^+),\\] (14)\nwhere A represents the output activations of the target layer, and VA represents the gradient of these activations with respect to the prediction. Specifically, VA comprises"}