{"title": "Supervised Score-Based Modeling by Gradient Boosting", "authors": ["Changyuan Zhao", "Hongyang Du", "Guangyuan Liu", "Dusit Niyato"], "abstract": "Score-based generative models can effectively learn the distribution of data by estimating the gradient of the distribution. Due to the multi-step denoising characteristic, researchers have recently considered combining score-based generative models with the gradient boosting algorithm, a multi-step supervised learning algorithm, to solve supervised learning tasks. However, existing generative model algorithms are often limited by the stochastic nature of the models and the long inference time, impacting prediction performances. Therefore, we propose a Supervised Score-based Model (SSM), which can be viewed as a gradient boosting algorithm combining score matching. We provide a theoretical analysis of learning and sampling for SSM to balance inference time and prediction accuracy. Via the ablation experiment in selected examples, we demonstrate the outstanding performances of the proposed techniques. Additionally, we compare our model with other probabilistic models, including Natural Gradient Boosting (NGboost), Classification and Regression Diffusion Models (CARD), Diffusion Boosted Trees (DBT), and Bayesian neural network-based models. The experimental results show that our model outperforms existing models in both accuracy and inference time.", "sections": [{"title": "Introduction", "content": "Generative artificial intelligence (GAI) models aim to create highly realistic simulations or reproductions of data by learning from the characteristics of the original data. Recently, Score-based models (diffusion models) have become the most powerful families of GAI due to their excellent generation performance. In these models, a series of noises is first added to the original data through a forward diffusion process. The model then learns the added noise and continuously denoises it to generate new data that follows the distribution of the original dataset. At present, many researchers have applied score-based models to generation tasks in many fields, including image generation, audio generation, video generation, etc., all with excellent performance. Supervised learning is another important research area in machine learning different from generative learning. In supervised learning, a model is trained on labeled data with the goal of learning a mapping from inputs to outputs, allowing it to predict the output for unseen data accurately."}, {"title": "Related Work", "content": "Score-based generative models have demonstrated powerful learning capabilities for complex data distribution in many fields. There are generally two categories of such models: diffusion-based and score matching-based models. Diffusion-based models consist of a forward diffusion process, which adds Gaussian noises into the original data, making it converge to the standard normal distribution, and a denoising process, which removes the added noises from a random variable following normal distribution. The score matching-based model can be regarded as an energy-based model, which aims to learn explicit probability distributions of data by a score function. Denoising score matching is a state-of-the-art approach for score estimation, which also estimates the score function from the disturbed dataset and then denoises through Langevin dynamics. From a general perspective, These two types of models can also be viewed as the discretization of stochastic differential equations.\nThe GBM is a powerful ensemble learning technique that builds models sequentially, with each new model aiming to correct the errors of its predecessors. By combining the predictions of multiple weak learners, GBM creates a strong predictive model that can handle complex data patterns and interactions. XGBoost, LightGBM, and NGBoost are typical decision trees-based gradient boosting techniques that extend the foundational principles of GBM to enhance efficiency and performance. XGBoost and LightGBM focus on computational speed and scalability, while NGBoost introduces probabilistic predictions, offering a novel approach to uncertainty estimation in gradient boosting models.\nRecently, via setting input data in supervised learning as conditions, conditional score-based models have been utilized to solve supervised learning problems. proposed a score-based generative classifier for classification tasks. This model predicts classification results through maximum likelihood estimation of different label generation distributions and target values instead of direct prediction. In , the authors proposed CARD, a score-based model for classification and regression. They emphasized its ability to solve uncertainty in supervised learning and compared its performance with existing Bayesian neural networks. The DBT model, a diffusion boosting paradigm, connects the denoising diffusion generative model and GBM via decision trees. Through experiments on real-world regression tasks, this approach demonstrates the potential of integrating GBM with score-based generative models."}, {"title": "Background", "content": "Gradient Boosting Machine\nThe GBM is a supervised learning framework that combines several weak learners into strong learners in an iterative way. In GBM, a boosted model F(x) is a weighted linear combination of the weak learners, which can be formulated as\n$$F(x) = F_0(x) + \\sum_{k=1}^{m} \\alpha_k h_k(x),$$\nwhere $F_0(x)$ is the initial prediction, $\\alpha_k$ is the weight coefficient, and $h_k(x)$ represents weak learner. Let $l(y, F(x))$ represent a measure of data error at the observation $(y, x)$ for the differentiable loss function 1. To train a GBM, the goal is to explore a function F that minimizes the expected loss $L(F(x)) = E_{(x,y)}l(y, F(x))$ where the expectation is taken over the input dataset of $(y, x)$. Since the loss function $l$ is differentiable, the optimal solution $F^*(x)$ can be represented as\n$$F^* (x) = F_0(x) + \\sum_{k=1}^{m} \\alpha_k\\cdot(-g_k(x)),$$\nwhere $g_k (x) = \\nabla_{F_{k-1}(x)}L(F_{k-1}(x))$ is the gradient at optimization step k. Therefore, weak learners are trained to estimate the negative gradient term to approximate the optimal solution. Given a trained GBM and input, the predicted result can be obtained via the iterative equation Eq. (1).\nLangevin Dynamics\nLangevin dynamics is a mathematical model that describes the dynamics of molecular systems. For a continuously differentiable probability distribution $p(x)$ where $x \\in R^d$, Langevin dynamics uses the score function, i.e., log-likelihood estimate $\\nabla_x \\log p(x)$, iteratively to obtain samples that fit the distribution $p(x)$. Given a certain step size $\\epsilon > 0$ and any prior distribution $\\pi(x)$, the Langevin dynamics can be expressed as\n$$X_t = X_{t-1} + \\frac{\\epsilon}{2} \\nabla_x \\log p(x_{t-1}) + \\sqrt{\\epsilon} z_t,$$\nwhere $t \\geq 0$, $x_0 \\sim \\pi(x)$, and $z_t \\sim N(0,I)$. As a Markov chain Monte Carlo (MCMC) technique , the Langevin equation takes gradient steps based on the score function and also injects Gaussian noise to capture the distribution $p(x)$ instead of the maximum point of the log-likelihood estimate $\\log p(x)$. When $\\epsilon \\rightarrow 0$ and $t \\rightarrow \\infty$, the distribution of $x_t$ will converge to the target distribution $p(x)$ under some regularity conditions.\nDenoising Score Matching for Score Estimation\nSince the Langevin equation in Eq. (3) only relies on the score function $\\log p(x)$, estimating the score of the target"}, {"title": "Supervised Score-based Model via Denoising Score Matching and Gradient Boosting", "content": "In supervised learning, the training set D comprises multiple input-target pairs, i.e., $D = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i \\in R^m$ and $y_i \\in R^d$. The model $f : R^m \\rightarrow R^d$ trained on this training set aims to predict the response variable y given an input variable x, i.e., $f(x) \\approx y$. For a regression problem, the response variable y is a continuous variable, whereas it is a categorical variable for classification.\nFor supervised learning problems, given an input $x_i$, unlike estimating the distribution of output, we pay more attention to predicting the value of $y_i$, or a special single-point distribution, i.e. $p(y) = I_{y_i}(y)$, where $I(\\cdot)$ is an indicator function. It is meaningless to estimate the score function of a single point distribution since its score function is only defined at value $y_i$. To address this issue, denoising score matching provides a feasible method via perturbing the single point distribution with noises. Specifically, when adding a Gaussian noise with mean 0 and variance $\\sigma$, the perturbed distribution will become a normal distribution with mean $y_i$ and variance $\\sigma$, i.e., $q(\\tilde{y}) = \\int p(y)N(\\tilde{y}|y, \\sigma^2I)dy = N(y_i, \\sigma^2)$, whose score function is well defined. Therefore, to estimate the value or the single point distribution of $Y_i$, we aim to train an NCSN to estimate the score function of input-target pairs $(x_i, Y_i) \\in D$ which uses an input variable $x_i$ as another condition, i.e., $s_\\theta(y, \\sigma_j,x_i) \\in R^d$, where $\\sigma_j \\in \\{\\sigma_i\\}_{i=1}^L$. For a given $(x_i, Y_i) \\in D$, $s_\\theta(y, \\sigma_j,x_i)$ denotes the score function of $y_i$, which is the score network of our framework SSM.\nAccordingly, the objective in our framework is,\n$$l(\\theta; \\sigma, x, y) = \\frac{1}{2}E_{p(y)} E_{\\tilde{y} \\sim N(y,\\sigma^2I)} [|| s_\\theta(\\tilde{y}, \\sigma , x) + \\frac{\\tilde{y}-y}{\\sigma^2} ||^2].$$\nThen for all noise levels $\\{\\sigma_i\\}_{i=1}^L$ and dataset D, the loss is,\n$$L(\\theta; \\{\\sigma_i\\}_{i=1}^L) = \\frac{1}{L} \\sum_{i=1}^L \\lambda(\\sigma_i)E_{(x,y)\\in D}l(\\theta;\\sigma_i, x,y).$$\nDue to the multi-step denoising characteristic of score-based generative models and GBM models, analyzing the relationship between the two is more helpful in deploying score-based generative models in supervised learning tasks. Additionally, the choice of model parameters greatly affects the performance of the score-based generative model and the GBM model . As discussed in , we need to design many parameters to ensure the effectiveness of training and inference, including (i) the choices of noise scales $\\{\\sigma_i\\}_{i=1}^L$; (ii) the step size $\\epsilon$ in Langevin dynamics; (iii) the inference steps t in Langevin equation. Therefore, in the following, we provide theoretical analysis to ensure the performance of the SSM on regression and classification problems.\nGradient Boosting\nFirstly, we consider the connection between SSM and GBM. For a given input, the denoising score match of SSM provides a perturbed distribution in the solution space whose maximum log-likelihood estimation point is the target value. To predict the maximum log-likelihood estimation point, we can set the loss function $l$ in GBM as the negative log-likelihood estimation, i.e., $-\\log q(F(x))$, where $q(x|x)$ is the perturbed distribution. Thus, the optimal solution in Eq. (2) will be the maximum log-likelihood estimation point of the perturbed distribution. Particularly, the iterative equation of GBM in Eq. (1) can be converted to a noise-free Langevin"}, {"title": "Experiment", "content": "Toy examples\nTo demonstrate the effectiveness of SSM and all improved Techniques, we first perform experiments on 5 selected toy examples (linear regression, quadratic regression, log-log linear regression, log-log cubic regression, and sinusoidal regression) proposed in . We add unbiased normal distribution noise to the original models to obtain noisy data for training, increasing model complexity. We aim to predict accurate regression results for pure input data in the inference process. To evaluate the impact of various technologies, we have designed an ablation experiment that considers three variables: the use of original Langevin dynamics, the implementation of the fast sampling technique as described in Technique 2, and the application of different coefficients $\\lambda(\\sigma_i)$ in the loss function, where L1 and L2 represent the degree of noise levels of 1 and 2, respectively. The experiment results with the Root Mean Squared Error (RMSE) and inference time about different model settings are shown in Table 1 and 2, respectively. The scatter plots for 5 examples of both true and generated data following the first setting, i.e., L1, w/o noise fast, are shown\nRegression\nWe further evaluate our model on 10 UCI regression tasks . We employ the same experimental settings as those used in the CARD model. We compare our model with NGboost,"}, {"title": "Conclusion", "content": "In this paper, we proposed Supervised Score-based Model (SSM), score-based gradient boosting model using denoising score matching for supervised learning. First, we analyzed the connection between SSM and GBM and showed that the iterative equation of GBM can be converted to a noise-free Langevin equation. Then, we provided a theoretical analysis on learning and sampling for SSM to balance inference time and prediction accuracy with noise-free Langevin sampling. Furthermore, we showed the effectiveness of the proposed techniques on selected toy examples. Lastly, we compared SSM with existing models, including NGboost, CARD, DBT, and Bayesian neural network-based models, for several regression and classification tasks. The experimental results show that our model outperforms in both accuracy and inference time."}, {"title": "Analysis of SSM", "content": "Equation with the above loss function, i.e.,\n$$F_k(x) = F_{k-1}(x) + \\beta_k \\cdot \\nabla \\log q(F_{k-1}(x)).$$\nMoreover, we plan to train one conditional score network with different noise levels as the conditions, similar to the uniform score network in the score-based generative model. The different noise level conditions represent various weak learners in GBM. Specifically, given an input $x_1$ and a noise level $\\sigma_j$, and a trained score network $s_\\theta(y, \\sigma_j, x_1)$, the converted noise-free Langevin equation is\n$$Y_t = Y_{t-1} + a_j \\cdot s_\\theta(Y_{t-1},\\sigma_j,X_I),$$\nwhere $a_j = \\epsilon\\cdot \\sigma^3/\\sigma^2$ is the step size.\nNext, we analyze the convergence results of the noise-free Langevin equation. Assume that we have a well-trained score network that can estimate the gradient accurately. For a prediction pair $(x_1, y_1)$, the noise-free Langevin equation is,\n$$Y_t = Y_{t-1} - r_L \\cdot (Y_{t-1} - Y_I),$$\nwhere $r_L = \\epsilon/\\sigma^2$ is called the refinement rate. Similar to GBM, it iteratively reduces the difference between $y_t$ and the target $y_I$, and then lets $y_t$ converge to $y_I$ as $t\\rightarrow \\infty$. Simultaneously, the estimation of score network $s_\\theta(y, \\sigma_j, x_I)$ is the error estimation weighted by. We call the inference algorithm following Eq. (11) as error refinement."}]}