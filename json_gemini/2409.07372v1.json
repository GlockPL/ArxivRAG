{"title": "Awaking the Slides: A Tuning-free and Knowledge-regulated Al Tutoring System via Language Model Coordination", "authors": ["Daniel Zhang-Li", "Zheyuan Zhang", "Jifan Yu", "Joy Lim Jia Yin", "Shangqing Tu", "Linlu Gong", "Haohua Wang", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li"], "abstract": "The vast pre-existing slides serve as rich and important materials to carry lecture knowledge. However, effectively leveraging lecture slides to serve students is difficult due to the multi-modal nature of slide content and the heterogeneous teaching actions. We study the problem of discovering effective designs that convert a slide into an interactive lecture. We develop Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring system that can (1) effectively convert an input lecture slide into a structured teaching agenda consisting of a set of heterogeneous teaching actions; (2) create and manage an interactive lecture that generates responsive interactions catering to student learning demands while regulating the interactions to follow teaching actions. Slide2Lecture contains a complete pipeline for learners to obtain an interactive classroom experience to learn the slide. For teachers and developers, Slide2Lecture enables customization to cater to personalized demands. The evaluation rated by annotators and students shows that Slide2Lecture is effective in outperforming the remaining implementation. Slide2Lecture's online deployment has made more than 200K interaction with students in the 3K lecture sessions. We open source Slide2Lecture's implementation in https://anonymous.4open.science/r/slide2lecture-4210/.", "sections": [{"title": "1 Introduction", "content": "The vast amount of learning materials available on the Internet, thanks to the sharing and dissemination of knowledge by educators, has enabled countless individual learners to achieve personal development through self-study [3]. However, despite the availability of numerous course slides and textbooks, students often experience confusion during self-learning, which requires additional clarification to address these diverse learning needs caused by different knowledge backgrounds. Without personalized guidance and explanations from teachers and assistants in traditional classrooms [6], individuals cannot interact directly with educational materials, making self-study a difficult and challenging endeavor [16].\nExisting Studies. Despite a few previous efforts to build Intelligent Tutoring Systems (ITS) that provide self-learners with adaptive learning experiences [13, 14, 28, 31], these approaches often require significant investment and extensive model training. With the rapid development of Large Language Models (LLMs), researchers have attempted to create ITS by abstracting complex teaching behaviors with LLM workflows [11, 18], which remains largely theoretical and challenging to implement without actual course materials. Nevertheless, constructing personalized ITS from specific teaching slides involves numerous technical challenges:\n\u2022 Multi-Modal and Structured Teaching Material Understanding. Online massive teaching materials encompass multi-modal [7] and hierarchically structured knowledge [4], such as relationships between pages and sections. To ensure that the explanations of the lecture slides are accurate, informative, and seamless, understanding the slides must cover multiple modalities\u00b9 and different levels of knowledge content\u00b2 Therefore, designing systems"}, {"title": "2 Preliminaries", "content": "In this section, we first introduce the background techniques for building the proposed system and then formalize the task."}, {"title": "2.1 Background Techniques", "content": "The construction of an interactive and knowledge-regulated intelligent tutoring system (ITS) involves joint collaboration of intelligent tutoring techniques and powerful large language models.\nIntelligent Tutoring System. The investigation into the architecture of intelligent tutoring systems commenced during the initial era of computer-assisted instruction (CAI) [1, 9, 10, 31]. The advent of contemporary machine learning methodologies has catalyzed recent endeavors to develop systems that are more interactive, personalized, emotionally supportive, and tailored to individual learning styles [19, 36]. The introduction of ChatGPT [22] has had a profound impact on the educational domain [2]. In particular, some researchers have discovered that, with adequate training, language models are capable of effectively addressing student inquiries [5, 13]. Furthermore, integrating language models with graph-based techniques has demonstrated substantial promise in mapping learning trajectories for specific concepts [14].\nLanguage Model Coordination. Large Language Models (LLMs) demonstrate advanced functionalities that have profoundly impacted various domains [8], particularly in increasing interactivity within intelligent tutoring systems. Owing to ongoing discoveries in multi-agent and tool-utilization studies [24, 26, 27], novel planning and interaction techniques for ITS have been devised. A recent investigation [11] has also revealed the potential of LLMs to"}, {"title": "2.2 Problem Formulation", "content": "Definition 1. Lecture Slide, denoted as $F$, is a file rich in diverse form of knowledge $(t_{1~n})$ that could be used for teaching.\nDefinition 2. Intelligent Tutoring System is a system that takes the given interaction history, $H^{t}$, to generate a set of interactions, $S_{t}$, in response to user query, $U_{t}$, where t is the count of user queries.\nProblem 1. Slide Knowledge-regulated Intelligent Tutoring For a given lecture slide $F$, the task is to create a set of interactions $S_{t}$ in response to the last user query $U_{t}$, while ensuring that the interaction history $H_{t}$ is within the scope of knowledge of slide file $t_{1~n}$."}, {"title": "3 Slide2Lecture", "content": "In this section, we first overview the design principle and the workflow of Slide2Lecture, and then elaborate on its implementation."}, {"title": "3.1 Framework Overview", "content": "Figure 1 shows the overview of Slide2Lecture, which consists of three subsystems, Read, Plan, and Teach. The Read and Plan jointly serve during pre-class (Algorithm 1) and the Teach subsystem is responsible for providing real-time in-class interactions. The detailed workflow of these three subsystems is illustrated as follows:\n(1) Read: Multi-Modal Knowledge Extraction. This subsystem is designed to extract rich and diverse knowledge within the slide. We aim to retain the hierarchical structure of the course slides. Leveraging the advanced capability to understand multi-modal inputs of modern LLMs [29, 34], this subsystem not only extracts the visual and textual contents of the slide but also consists of a pipeline to rebuild a tree-formed agenda to extract the structure knowledge. These outputs serve as uniform input knowledge to augment and regulate the generation of later subsystems. In addition, this design also potentially enables developers to transform the implementation to input materials other than lecture slides.\n(2) Plan: Heterogeneous Teaching Action Generation. This subsystem is implemented to formalize the extracted knowledge into a set of heterogeneous teaching actions, such as ShowFile, ReadScript, and AskQuestions, which will be practiced during the lecture. The Plan subsystem includes a modular implementation to handle each type of teaching action and will output a serialized action queue, which can be further used in the subsequent teaching process. The"}, {"title": "3.2 Read: Multi-Modal Knowledge Extraction", "content": "The function of multi-modal knowledge extraction is to prepare the multi-modal long-formed slides for generating teaching actions. Slide2Lecture extracts both content and structure knowledge from the given slide file to simplify and unify the input."}, {"title": "3.2.1 Content Knowledge", "content": "To extract information from the given slide, we focus primarily on extracting two types of content: textual (t) and visual (v). For the textual content, we use python - pptx to extract all the texts within each slide page. For visual content, we convert each slide page into an image using libreoffice and pymupdf. The content knowledge for each slide is organized into textual and visual information for further processing."}, {"title": "3.2.2 Structure Knowledge", "content": "In addition to the content knowledge, we further organized the slides into a tree-formed structure. The structured knowledge is extracted in an iterative manner, dividing the input slide into sections based on the content knowledge of each page, where each section contains multiple pages $(\\{p_{i}, p_{i+1},...,p_{j}\\})$. This extraction process aims to segment the slide into a set of sequences, where each sequence is continuous and represents a section. The extract structure knowledge offers two main advantages. First, it helps preserve the section and ordering of the slides, ensuring that knowledge points are positioned within the overall lecture, and maintaining the hierarchy of knowledge, such as prerequisite relationships. Second, the tree-formed structure benefits the process of teaching action generation. For example, questions and exercises can be inserted at the end of each section to facilitate student learning. In Slide2Lecture, the structure knowledge of the slide is extracted using two components: Description Generation and Slide File Segmentation."}, {"title": "3.3 Plan: Heterogeneous Teaching Action Generation.", "content": "The objective of teaching action generation is to take the extracted knowledge of the slide pages and generate a set of teaching actions regulated by this knowledge that can be further used in the interactive teaching part. Our design focuses on two major concerns: (1) How should teaching actions be represented to capture the diverse teaching activities? (2) How should heterogeneous teaching actions be generated within the model context?"}, {"title": "3.3.1 Teaching Action Representation", "content": "The teaching activities (e.g. lecturing, giving quizzes) are abstracted into teaching actions in Slide2Lecture. Specifically, teaching actions (t) are represented"}, {"title": "3.3.2 Teaching Action Generation", "content": "As the teaching actions are related to specific slide pages, we generate and store teaching actions as lists within the agenda nodes of the slide pages, represented as node.actions = [t1, t2, ...]. In Slide2Lecture, we provide the implementation of the following three key teaching actions:\n(1) ShowFile ($f_{SF}$) is used to show the pages of the slides to the students in order. In Slide2Lecture, the function ShowFile is generated first. For simplicity, we loop over the page directly in each sequence and use the page id as the value for the action, denoted as (ShowFile, $p_{i}.id$). Therefore, when Slide2Lecture performs ShowFile, the next page of the slides will be displayed.\n(2) ReadScript ($f_{RS}$) is involved in introducing the content within each slide page. For each slide page, we generate a corresponding teaching script. We aim to ensure that the scripts cover the details of the respective slide page and maintain a consistent tone across different pages. Therefore, we (1) recall the original visual ($v_{i}$) and textual ($t_{i}$) information extracted from the source slide page ($p_{i}$), (2) supplement it with the scripts generated for the previous k pages, and (3) concatenate this information and prompt the LLM to generate a script in the teacher's tone. The generated ReadScript action is denoted as (ReadScript, scripti). The script will be used in the future by the agent teacher to introduce the knowledge described in the slide page.\n(3) AskQuestion ($f_{AQ}$) is used to instruct the teacher agent to give quizzes to the student and provide explanations in response to the student's solution. We leverage the structure of the agenda and generate questions at the end of the sections with at least k slide pages to ensure that questions are generated only for sections with sufficient content. LLMs are provided with the current script (scripti) as the direct source to generate questions and corresponding answers. The recently generated scripts from previous slide pages ([scripti-k,..., scripti-1]) are also included as additional support to improve the quality of questions. The AskQuestion action is denoted as (AskQuestion, $qa_{i}$). We consider both single choice and multiple-choice questions in the current system, which leads to the value of the AskQuestion action to contain four variables, $qa_{i}$ = (question, question_type, options, answer)."}, {"title": "3.4 Teach: Knowledge-regulated Interactive Tutoring", "content": "The Teach subsystem is responsible for providing interactive tutoring. It generates responses and manages an interactive lecture for the student user's queries (U) while ensuring the flow of the lecture is aligned with each teaching action (t). This subsystem is composed of two modules: Interacting Agents and Scene Controller."}, {"title": "3.4.1 Interacting Agents", "content": "Slide2Lecture considers three types of interactions in the classroom: (1) giving lectures and providing explanations to the student user; (2) ensuring that the lecture follows the safety guidelines and handles unsafe inputs; (3) providing other interactions that are important for the lecture process. Three LLM-based agents are implemented for each interaction: For the first type of interaction, we iconize a teacher agent to deliver tutoring and answer user's questions. For the second type of interaction, the teaching assistant agent is crafted and optimized to manage the discipline of the classroom. To determine the termination of the teaching action and iconize other forms of interactions, Slide2Lecture also includes the system agent to control the system and determine the termination of the teaching actions. The system agent is unaware to users, but serves crucial interactions, such as displaying a slide page and terminating a teaching action."}, {"title": "3.4.2 Scene Controller", "content": "Following previous works that utilize a managing agent to control the multi-agent scenarios [32, 35, 36], we introduce the Scene Controller to select the appropriate agent to handle the lecture. To create diverse interaction experiences for different actions, the behavior of the Scene Controller will vary from different teaching actions, which is described below:\n(1) ShowFile Controller controls the system agent to change the displayed slide page followed by the termination of the action.\n(2) ReadScript Controller would first control the teacher agent to provide the script to the user. Then, if the user chooses to interact with the system, the controller would iteratively select the appropriate agent to generate a response, until it selects the user or the system agent to terminate an action.\n(3) Ask Question Controller would first call the teacher agent to post the question. Then, it would wait for the user's response. Once the user submits a solution, the controller will provide the"}, {"title": "4 Experiments", "content": "In this section, we design experiments to analyze the characteristics of Slide2Lecture. To test our design, we first include offline evaluation and user studies to evaluate the overall performance of the pre-class subsystems (Section 4.1) and in-class subsystem (Section 4.2). We then conduct detailed observations of the components to study the performance of the system in detail (Section 4.3). Finally, we deployed Slide2Lecture as an online learning platform at a university, where 556 student users participated. We demonstrate the statistical results of these users on the platform (Section 4.4).\nImplementation. We set the value k as 3 (number of previous pages in Section 3.2) for all experiments. For pre-class generation, we employ GPT-4V as the LLM for course content generation. For real-time interactions during in-class sessions, in accordance with the ethical considerations of the partnering university and the existing collaboration agreement, we utilized GLM4 [15]. We input a maximum of 12 utterances of chat history during in-class generation. Most hyperparameters are set to default, but max_tokens was set to 4, 096 for GPT-4V (Appendix D). In Appendix F, we also provide the system prompts to reproduce our work."}, {"title": "4.1 Pre-Class Stages Evaluation", "content": "We perform offline evaluations to demonstrate the effectiveness of the method for the pre-class subsystems. With the approval of the corresponding teachers, we include lecture slides drawn from the course Towards Artificial General Intelligence (TAGI) and How to Study in University (HSU) to conduct offline experiments. The two slides contain 46 and 61 pages, respectively. In the following paragraphs, we employ crowd-source annotators to assess the numerous ReadScript actions (Section 4.1.1). Subsequently, we engage experts to evaluate the less frequent AskQuestion actions (Sec 4.1.2)."}, {"title": "4.1.1 ReadScript", "content": "We first include two baseline setups to compare with Slide2Lecture implementation: (1) We use the original instructions to reproduce Script2Transcript [20], denoted as S2T, which uses the titles to provide global information about the slide. (2) We then reproduce Self-Critique Prompting [37], noted by SCP, which includes a self-critique and refinement process to generate scripts. Since SCP uses the GPT-4 [21] in its original literature, we include visual input to ensure that the performance is fully reproduced. In contrast, we exclude visual inputs for S2T as its original paper uses LLaMA [30], which does not support out-of-box visual input. To examine the influence of including visual and contextual (content in neighboring pages) information in the system, we also include two additional ablation setups, where the visual and contextual inputs are removed from Slide2Lecture's default implementation. We use GPT-4V for all setups to ensure that the method performance is not affected by the capability of the backbone model. We further include the scripts revised by the teachers and the TAs based on the slides as a human expert baseline.\nMetrics. We include 4 different metrics during the evaluation of the generated scripts, each with a 5-point Likert scale, where 1 stands for unacceptable and 5 for optimal: (1) Tone stands for whether the script adopts the appropriate tone of a teacher delivering a lesson. (2) Clarity indicates whether the script is clear and easy to understand; (3) Supportive measures whether the script includes emotional support; (4) Matching grades how much the script matches the content of the slide page. We compute the overall performance by averaging the scores for all metrics.\nWe run the pre-class pipeline for each baseline and ask the annotators to rate the scripts. To reduce bias between annotators, we ensure that each slide is labeled with 3 annotators, where each of them is required to provide ratings for all settings of the same slide.\nResults. As shown in Table 1, Slide2Lecture achieves the highest overall score of 4.00, surpassing all baselines. We observe that: (1) Visual input is important for script generation, as both S2T and Slide2Lecture without visual inputs obtain poor matching scores; (2) Contextual information is crucial for the quality of the scripts. Contents of previous pages not only improve the Clarity of the current page but also the Supportive and Matching, as they provide the model with coherent information. (3) Surprisingly, our method slightly outperformed the human baseline across three dimensions. As LLMs have demonstrated a strong capability to follow instructions, they are inclined to strictly follow the slide contents with a more encouraging tone for the users. In contrast, human instructors tended to diverge more from the content, incorporating their own style and expanding on topics more freely."}, {"title": "4.1.2 Ask Question", "content": "We tested the performance of Slide2Lecture in course question generation, by evaluating the quality and timing of the questions generated based on the lecture slides, considering that both of them are crucial in an open classroom setting. Since preliminary studies have not focused on question-generation techniques with teaching slides and scripts, this evaluation focuses on observing performance change in ablation settings, where the visual and contextual inputs are removed respectively. As the frequency"}, {"title": "4.2 In-Class Stage Evaluation", "content": "We conduct a series of user studies to evaluate the design for the in-class Teach subsystem. Specifically, our human evaluation team consists of 22 students. In the experiment, all students are asked to use the automatically generated lecture plans from Section 4.1 to assess the system. Each student is required to interact freely with the system and go through at least 40 pages of each lecture.\nWe also conduct a parallel experiment as the ablation study during this evaluation. To replicate the traditional meta-agent strategy, we eliminate the role descriptions of the system agent and omit the prompt injection for the teacher while generating responses. This setting can help us to observe whether the design of the Teach module is effective from the user's perspective.\nMetric. Students are required to evaluate the system by assigning several 1 to 5 scores for each lecture, and the metrics include: (1) Learning New Concepts: whether the system can effectively introduce and teach new concepts to students; (2) Deepen Understanding: how in-depths can the system reach when teaching concepts; (3) AI Teacher Correct: the ability of the teacher agent to provide correct explanation after the student has provided a solution; (4) Question Appropriation: whether the questions generated and posted by the system are in good timing and appropriate contents.\nResults. As shown in Figure 4, we plot the distributions of the student ratings in the two settings for each course and each evaluation metric. The data indicates a clear pattern: the ratings for the system's ability to deepen understanding and provide accurate responses to student solutions through the AI teacher agent were generally higher in the Slide2Lecture setting. Specifically, these plotted results produced several observations:\n(1) For the TAGI course, the complete setting of Slide2Lecture demonstrated greater consistency and lower variance in the ratings for the \"Question Appropriation\u201d, highlighting the effectiveness of the system in maintaining relevance and engagement.\n(2) In contrast, for the HSU course, which is a social study subject, the performance was similar between the ablation and Slide2Lecture settings. This similarity can be attributed to the nature of the content of the HSU course, which already includes interactive elements within the provided PowerPoint slides. These built-in interactions are reflected during the generated scripts, helping to maintain engagement and understanding, which resulted in minimizing the differences observed between the two settings.\n(3) From an alternative viewpoint, this also indicates that the pre-class subsystems methodically formalized the knowledge within the slide to enhance such interactions. The ablation condition exhibited greater variability, notably in the \"Deepen Understanding\" and \"Question Appropriation\" metrics, implying that the excluded features may result in a less effective experience."}, {"title": "4.3 Module-Wise Evaluation", "content": "To get a better understanding of performance, in this subsection, we conduct detailed observations of the components."}, {"title": "4.3.1 Slide File Segmentation", "content": "To better understand the performance of the Read module, we evaluate the segmented agenda via annotating the node relation accuracy.\nMetric. After gathering the intermediate agendas produced in Section 4.1.2, we introduced two metrics to be evaluated on a scale from 1 to 5: (1) Precision: We assess the precision of segmentation by labeling the appropriateness of creating a new section and noting any missed or incorrectly detected segmentation; (2) Matching: The annotator also evaluates the correspondence between each page and the section name of the page. Additionally, we include an option to flag correctly detected unnecessary segmentations, allowing us to exclude such cases when segmentation accuracy is not relevant.\nResults. The results are summarized in Table 2, highlighting the importance of visual and contextual information to achieve high segmentation precision and content matching accuracy. First, the TAGI course achieved the highest scores in the full setting, with a segmentation precision of 3.89 and a matching score of 4.72. Second, removing visual input decreased performance to 3.40 and 4.54. This decline suggests that visual cues, such as layout changes and titles, are essential to accurately identify new sections. Similarly, the lack of contextual information caused even greater drops, with scores falling to 3.00 in segmentation precision and 2.98 in content matching for TAGI. Third, a similar pattern is also observed in the HSU course, where the complete set-up scores are 3.54 and 4.34, respectively, but drop to 2.77 and 3.95 without visual input, and then to 2.71 and 2.26 without contextual input."}, {"title": "4.3.2 Scene Controller Precision", "content": "In Section 4.2, the user-study provided observations when degrading Teach module implementation from a user's perspective. To gain a more systematic understanding of performance, we randomly sample 100 LLM calls for each setting during the scene controller and label the precision to observe the effect of excluding the role description.\nAs demonstrated in Figure 5, statistical analysis indicates that removing the role descriptions for each agent considerably decreases the classifier performance. We observe that although the LLM can still identify the correct agent, possibly by highlighting partial behavior from the chat history in certain instances, the inclusion of additional descriptions still significantly improves performance."}, {"title": "4.4 Online Evaluation", "content": "We implemented Slide2Lecture in an online environment to collect user feedback. A total of 556 students volunteered to participate in the study, with 214 completing the assigned lectures. The lecturing process for these students has already involved 214, 238 interactions in 3, 103 lecture sessions, consuming a total 49, 267 LLM calls for in-class generations. The participants were divided into two groups: one for the HSU course, which consisted of 405 pages for 7 lectures, and the other for the TAGI course, which included 6 lectures with 351 pages. Upon completing all lectures, students were asked to complete a post-class survey to rate various aspects of the system on a scale of 1 to 5. The clarity of the teacher agent received a solid rating of 4.12. In addition, the students appreciated the ability of the system to create a free and easy learning environment, giving it a score of 4.15. Furthermore, the system was rated 4.14 for making students feel relaxed while studying, indicating a positive user experience with Slide2Lecture."}, {"title": "5 Conclusion", "content": "In this paper, we present a tuning-free framework Slide2Lecture, to generate and provide intelligent tutoring regulated by slide knowledge. This framework offers a workflow including pre-class lecture planning (Read and Plan subsystems) and in-class lecture giving (Teach subsystem) with modularized teaching actions, providing a convenient toolkit for developers to customize an intelligent tutoring system. We perform extensive experiments including offline evaluation, user studies, module-wise evaluation, and online evaluation to demonstrate the effective performance of Slide2Lecture. However, at the same time, the proposed Slide2Lecture suffers from limitations as it focuses mainly on the slide file, while the vast existing Internet materials, such as textbooks and videos, have not been fully explored in coordination with the slide file. We hope that our work could serve as the ground for developing various forms of knowledge-regulated and AI-driven education systems."}, {"title": "A Offline Labeling Guidelines", "content": "We present the labeling guidelines provided to the annotators during offline evaluation.\nTone. What is the tone of the speech?\n1 The tone does not resemble a teacher at all, lacking authority or friendliness.\n2 The tone has significant issues, with parts of the script unsuitable for a teacher.\n3 The tone is generally suitable for a teacher, but some parts could be improved.\n4 The tone is good, most of the script sounds like a teacher, but minor adjustments could be made.\n5 The tone is perfect for a teacher, it can be read out directly in the classroom.\nClarity. How clear is the script?\n1 The script is very difficult to understand, students might not understand at all.\n2 The script has significant issues, with parts difficult for students to understand.\n3 The script is generally clear, but some parts could be made simpler and more direct.\n4 The script is very clear, most students can understand, but minor improvements are possible.\n5 The script is very clear, students have no difficulty understanding it.\nSupportive. How emotional supportive is the script?\n1 The script is not supportive at all, and may even have a negative impact.\n2 The script lacks supportiveness, students might not feel encouraged.\n3 The script is neutral, with no obvious encouragement or negative impact.\n4 The script is very supportive, with most parts conveying positive messages.\n5 The script is extremely supportive, conveying a lot of positive energy and support.\nMatching. How well does the script match the PPT content?\n1 The script does not match the PPT content at all.\n2 The script deviates significantly from the PPT, but some parts are relevant.\n3 The script generally matches the PPT, but there are some inconsistencies.\n4 The script matches the PPT well, with most parts being consistent.\n5 The script matches the PPT perfectly, with no deviations."}, {"title": "B User-Study Survey", "content": "We present the survey used for students to rate system performance during the user-study (Section 4.2).\nLearning New Concept. I believe the Slide2Lecture platform can help me learn new concepts.\n1 Strongly disagree\n2 Disagree\n3 Neutral\n4 Agree\n5 Strongly agree\nDeepen Understanding. I believe the Slide2Lecture platform can help me deepen my understanding for concepts.\n1 Strongly disagree\n2 Disagree\n3 Neutral\n4 Agree\n5 Strongly agree\nAI Teacher Correct. I believe the explanations given by the AI teacher after I answer the system's questions are correct.\n1 Strongly disagree\n2 Disagree\n3 Neutral\n4 Agree\n5 Strongly agree\nQuestion Appropriation. I believe the questions asked by the teacher agent are appropriate.\n1 Strongly disagree\n2 Disagree\n3 Neutral\n4 Agree\n5 Strongly agree"}, {"title": "C Example Teaching Actions", "content": "We present the translated teaching action examples below."}, {"title": "D Hyperparameters", "content": "We introduce the hyperparameters used during the LLM generations. For Read and Plan subsystems, all LLM calls are made to call GPT4V (gpt-4-vision-preview). We use the default value for most hyperparameters, but set the max_tokens to 4, 096. The detailed hyperparameter values for calling GPT4V are shown in Table 4."}, {"title": "E Deployment Details", "content": "Server. Slide2Lecture is deployed on a cloud server with 16 vCPUs, 64GB memory, and 500GB SSD storage. The bandwidth is set to 100 Mbps. We use MongoDB as the database and use Minio as the distributed file system to elastically adapt to larger storage in future needs. Because LLM calls could take several seconds to generate a response that requires a thread to be used for the user query but idling for the network return, we implement the Teach module to improve memory usage efficiency. This is handled using RabbitMQ as the task queue and distributor. We divide a single response, which may contain multiple interactions, and is handled during multiple steps. Each step is only allowed to make at most one LLM call, it will then save its status. The idle process will continuously check for LLM calls and, once a call is complete, will resume processing. During the user study, we degraded to an 8vCPU and 31GB server, where we observed that the Slide2Lecture implementation only takes less than 20% of memory but serves all students to assess the system."}, {"title": "F System Prompt", "content": "We translate and present the prompts used during generation."}, {"title": "F.1 Read", "content": "Table 6 presents the system prompt used to generate the description for each page. The set of results is then handled by the system prompt presented in Table 7, where the LLM iteratively constructs a tree-shaped agenda."}, {"title": "F.2 Plan", "content": "We present the system prompt used for ReadScript generation in Table 8 and the system prompt used for AskQuestion generation in Table 9."}, {"title": "F.3 Teach", "content": "We present the system prompt designed for ReadScript controller in Table 10. The system prompt used for the teacher agent and the teaching assistant agent is presented in Table 11 and Table 12."}, {"title": "G Other Results", "content": ""}, {"title": "G.1 Token Usage", "content": "We present the amount of token usage for pre-class evaluation in Table 13 and Table 14."}, {"title": "G.2 Network Usage", "content": "We present the change in network usage over time in Figure 7. We observe that more traffic occurs during the daytime. Then again, the fact that to a certain point, the recorded value dropped significantly. This is the day that the teachers announced that it is the deadline to complete the course on the system. The recorded values slowly raised backup again as we reopened the system because student volunteer users made multiple requests stating that they were interested in completing the course and using the system regardless of not being able to receive a bonus due to not completing all the lectures in time."}, {"title": "G.3 Disk Usage", "content": "We present the disk access rate recorded in the online deployment in Figure 8 and Figure 9. We observe that there are much more write requests as Slide2Lecture records a set of detailed interactions and intermediate results for system engineers to analyze."}, {"title": "G.4 CPU Usage", "content": "As shown in Figure 10, the low CPU usage demonstrated in Slide2Lecture is implemented efficiently."}]}