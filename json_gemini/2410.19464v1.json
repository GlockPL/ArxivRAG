{"title": "LOCAL: Learning with Orientation Matrix to Infer Causal Structure from Time Series Data", "authors": ["Yue Cheng", "Jiajun Zhang", "Weiwei Xing", "Xiaoyu Guo", "Xiaohui Gao"], "abstract": "Discovering the underlying Directed Acyclic Graph (DAG) from time series observational data is highly challenging due to the dynamic nature and complex nonlinear interactions between variables. Existing methods often struggle with inefficiency and the handling of high-dimensional data. To address these research gap, we propose LOCAL, a highly efficient, easy-to-implement, and constraint-free method for recovering dynamic causal structures. LOCAL is the first attempt to formulate a quasi-maximum likelihood-based score function for learning the dynamic DAG equivalent to the ground truth. On this basis, we propose two adaptive modules for enhancing the algebraic characterization of acyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and Dynamic Graph Parameter Learning (DGPL). ACML generates causal masks using learnable priority vectors and the Gumbel-Sigmoid function, ensuring the creation of DAGs while optimizing computational efficiency. DGPL transforms causal learning into decomposed matrix products, capturing the dynamic causal structure of high-dimensional data and enhancing interpretability. Extensive experiments on synthetic and real-world datasets demonstrate that LOCAL significantly outperforms existing methods, and highlight LOCAL's potential as a robust and efficient method for dynamic causal discovery. Our code will be available soon.", "sections": [{"title": "1 INTRODUCTION", "content": "Exploration of the underlying causal generation process of dynamic systems is an important task [11, 19] for trustworthy machine learning. Unfortunately, it is unethical, impossible due to technical reasons, or expensive to conduct intervention experiments on the dynamic systems of certain domains [6, 25]. Another challenge is to infer about the structure which may be high dimensional and nonlinear. Some recent works [15, 18, 34, 37] have made significant efforts by employing dynamic Bayesian networks (DBNs) with observational and interventional data: among dynamic systems, as illustrated in Figure 1. The variable xi at timestept is affected by which variables xj at the same time step (instantaneous dependency) and which variables xj at the previous timestep (lagged dependency)? This question highlights the crucial roles of those algorithms in the interpretable performance of the trained models.\nIn order to study the nonparametric DBN, DYNOTEARS [34] (i.e., a score-based approach to learning DBNs with a differentiable DAG constraint [43]) was proposed as a proxy to capture the learn"}, {"title": "2 RELATED WORK", "content": "Learning causal structure from data using constraint and score-based techniques is a classic approach [29], but the major drawback is the computational inefficiency. Many recent works have made great efforts to promote computability and practicality: NOTEARS [43] employed a least squares objective to speed up the process, but required augmented Lagrangian to satisfy hard DAG constraints. Based on this, DYNOTEARS [34] first tried to recover the causal structure of time series data. Golem [32] exploits the likelihood-based function to make the objective unconstrained. Those likelihood-based works, however, still struggled with the DAG constraint."}, {"title": "2.1 Score-Based Objective for Causal Discovery", "content": ""}, {"title": "2.2 Constraint Free Causal Discovery", "content": "Many efforts have been made to emancipate constraints DAGs, focusing on designing more tractable optimization constraints [3, 7, 17, 40] on static data. In contrast to these approaches, Yu et al. [41] introduced NOCURL, which separated the topological ordering from the adjacency matrix, thereby eliminating the necessity to optimize the DAG constraint within the objective function. Inspired by NOCURL, a series of causal masks based on topological structures appeared [1, 28, 33]. For example, MCSL-MLP [33] sampled DAGs from a real matrix U \u2208 Rd\u00d7d. BayesDAG [1] inherited NOCRUL's grad matrix and directly sampled DAGs from the posterior of grad matrix p(u, v). By exploiting the temperature sigmoid function to p(u, v), COSMO [28] provides a fast implementation method when samples DAGs. However, DAG masks for the time domain datasets have rarely been studied."}, {"title": "2.3 Matrix Decomposed for Causal Discovery", "content": "The matrix decomposition approach is widely employed to address high-dimensional problems [10, 16, 28], with applications spanning large-scale parameter tuning [22], image restoration [9], and trajectory prediction [2]. While many of these works focus on learning intrinsic dimensionality to achieve efficient optimization in parameter space, VI-DP-DAG [8] adopts adjacency matrix decomposition Was W = IUI, while LoRAM [14] integrates low-rank matrix factorization with sparsification mechanisms for continuous optimization of DAGs. Those works do not conduct an in-depth exploration of the physical meaning of the decomposition matrix, and the decomposition form is not concise enough."}, {"title": "3 LOCAL: LEARNING CAUSAL DISCOVERY FROM TIME SERIES DATA", "content": ""}, {"title": "3.1 Problem Definition", "content": "We target the causal discovery from a dynamic system. Consider multitudinous stationary time series that contains d causal univariate time series represented as X := [Xt] \u2208 RT\u00d7d, where Xi is the recording of i-th component of Xt at time step t, our target is to simultaneously infer the instantaneous dependency W\u00b9 = {Wij : Xj \u2192 X\u2081| i, j \u2208 _1,\u2026\u2026,d;i \u2260 j} and the lagged dependency A = {A: X-p \u2192 X\u2081| i, j \u2208 1,\u00a8\u00a8\u00a8 ,d; p \u2208 Z\u207a} of X, from which we may recovery the dynamic causal structure. Following the practice in the dynamic causal structure discovery, we formulate the problem of learning a dynamic Bayesian network (DBN) that captures the dependencies in the X:\nXt = XW +YA+N\ns.t. h(W) = 0"}, {"title": "3.2 Quasi-Maximum Likelihood-Based Objective", "content": "Due to the hard acyclicity constraint [34], learning a DAG equivalent to the ground truth DAG through directly optimizing Eq. (1) is difficult. To circumvent this issue, we reformulate a score-based method to maximize the data quasi-likelihood of a dynamic linear Gaussian model.\nThe overall objective is defined as follows:\nmin S(W, A; Xt) = L(W, A; Xt) +11h(W) + 12 (Rsparse(A) +Rsparse(W))\nwhere L(W, A; Xt) is the quasi-maximum likelihood estimator (QMLE) of X, Rsparse is the penalty term encouraging sparsity, i.e. having fewer edges.\nASSUMPTION 1. Suppose N follows the multivariate normal distribution N(0, \u03c3\u0399). Further assumes that the noise variances are equal, i.e. f = \u03c32. Let X = (X,j \u2260 i) denote the responses of all variables except for the i-th variable. Motivated by the least squares estimation (LSE) method [45], we have X} = E{X\\X\u00a1\u00b9} = \u00b5\u00a1 + \u2211j\u2260i &i,j(X \u2013 \u03bcj), where\ndij = (Wij + wji) \u2013 Ek WkiWkj/ 1 + Ek wi\nand \u03bc\u2081 = E(X).\nInspecting Eq. (3), we can find that for the i-th variable, the causal relationship are related to its first- and second-order parents or children, where the first-order parents or children are collected by {j: Wij \u2260 0 or wji \u2260 0}, and the second-order parents or children are collected by {j: \u03a3k WkiWkj \u2260 0}. Based on the conditional expectation and Eq. (3), we now derive that:\n\u0160\u012f \u2212 \u03bc\u2081 = (1 + ||W.i||\u00b2)\u22121(W.\u2081 +W? \u2013 W?Wii)(xt \u2013 \u03bc)\n= (1 + ||W.\u00a1||\u00b2)\u22121{(W.; + W \u2013 WWii) (x \u2212 \u03bc) \u2212 (X \u2212 \u03bc\u2081)} + (X \u2013 \u03bc\u03af)\nBy defining D := {I+diag(WTW)}-1 and S := I \u2013 W, we could verify the least squares objective function as\nXS - Y'A = ||{XS - YA} DST ||\nFollowing the multivariate Gaussian distribution assumption, we now formulate a score-based method to maximize the data quasi-likelihood of the parameters in Eq. (2). Omitting additive constants, the log-likelihood is:"}, {"title": "3.3 Asymptotic Causal Mask Learning", "content": "Although QMLE can be statistically efficient, the computational cost could be expensive due to the matrix exponential operation of h() function [43]. Most recent work in causal structure learning deploys an alternative continuous DAG constraint h(W) to characterize acyclic graphs. Although decoupling the causal structure into a causal mask and a real matrix may help to give more explanatory results for instantaneous causal effects, it means that the matrix needs to learn changes from d x d to d x (d + 1). To solve the issue, we propose an Asymptotic Causal Mask Learning (ACML) module to asymptotic infer the causal relationship from data automatically. The ACML module initializes a learnable priority vector p \u2208 Rd, which is initialized as a vector with all elements set to 1, and a strictly positive threshold w > 0. Then the smooth orientation matrix of the strict partial order is the binary orientation matrix M\u1ff3(p)u,v \u2208 {0,1}d\u00d7d such that:\n\u039c\u03c4,\u03c9 (p)u,\u03c5 = \u03c3\u03c4,\u03c9 (Pu \u2013 Pu)\nwhere \u03c3\u03c9 can be formulated as the w-centered Gumbel-Sigmoid:\n\u03c3\u03c4,\u03c9 (\u03a1\u03c5 \u2013 Pu) = exp((Po - Pu - \u03c9) + j) /\u03c4/exp((Pv - Pu \u2013 \u03c9) + j)/\u03c4 + exp(\u0434/\u0442)\nwhere \u03c3\u03c9 is formulated as the w-centered Gumbel-Sigmoid, \u0121 and \u00ff are two independent Gumbel noises, and \u03c4\u2208 (0,\u221e) is a temperature parameter. The threshold w adjusts the midpoint of the sigmoid function and disrupts the symmetry when two variables have roughly equal priority. Finally, the ACML enhanced DBN can be formulated as:\nXt = Xt (W\u03bf \u039c\u03c4,\u03c9 (p)u,v) + YA + Z"}, {"title": "3.4 Dynamic Graph Parameter Learning", "content": "Another problem lies in the existing dynamic causal structure models, which require directly learning p + 1 d \u00d7 d-dimensional matrices. There are two main methods to capture dynamic causal effects: 1) Designing a complexity neural network, which specifically customizes a neural network with DAG constraint [18, 30]. 2) With the help of the Granger causal mechanism, [37] introduces the idea of Neural Granger's component network [38] to provide each variable design independent neural network. However, these approaches are quite unintuitive. Overly complex network design or component Neural Granger [12, 44] methods are overparameterized, but most works in practice only exploit the first-layer weights of the network to characterize causal effects, which increases the optimization cost and has to adopt a second-order optimizer L-BFGS-B. Besides, over-parameterization methods are prone to overfitting when dealing with unsupervised problems such as causal structure discovery, leading to considerable biases in the causal structure.\nTo solve the issue, we propose a Dynamic Graph Parameter Learning (DGPL) module to infer the dynamic causal graphs from observed data and interventional data automatically. The DGPL module first randomly initializes 2 learnable dynamic node embedding dictionaries Eso \u2208 R(p+1)\u00d7d\u00d7k, Eto \u2208 R(p+1)xdxk for all nodes, where each row of Eso represents the source embedding of a node, each row of Eto represents the target embedding of a node. Then, we can infer the causal structure between each pair of nodes by multiplying Eso and Eto:\nW = Eso (t)E(t)\nA = [Eso (t \u2212 1)E(t \u2212 1)|\u00b7\u00b7\u00b7|Eso (t \u2212 p)E(t \u2212 p)]\nwhere W is instantaneous dependency, A is lagged dependency. During training, Eso and Eto will be updated automatically to learn the causal structure among different variables. Compared with the CNN-based work in [37], the DGPL module is simpler and the learned embedding matrices have better interpretability (see Section 4.6 for more details). Finally, the DGPL enhanced DBN can be formulated as:\nXt = Xt(Eso(t)E(t) \u03bf \u039c\u03c4,\u03c9 (p)u,v) + YAso \u00a9 Ato + Z"}, {"title": "3.5 Nonlinear Form and Model Training", "content": ""}, {"title": "3.5.1 Nonlinear Form.", "content": "In practice, the interactions among variables can be highly nonlinear, increasing the difficulty in modeling. To alleviate this issue, we adopt 1D Convolutional Neural Networks (CNN), a classical nonlinear model for temporal data, to capture nonlinear interactions. We replace the 3D convolutional kernel with two consecutive kernels, which are estimates of Eso and Eto. Formally, a convolutional kernel in a 1D CNN is 3D tensor W \u2208 RK\u00d7d\u00d7d, where K = p + 1 is the number of convolution kernels and d is the input feature dimension. Let Xt\u2212p:t \u2208 RB\u00d7T\u00d7d be the input temporal data, then the output is defined as:\nX\u2081 = WP+1 * (WP * ... (W\u00b9 * Xt\u2212p:t)) = 0T Xt-p:t\ns.t. W\u00b0 = E(E)T,\u2200c \u2208 {1,\u2026, p + 1}\nwhere We represents the c-th filter, E, is to learn whether the variable Xj is the root cause of other variables in the causal structure, Er is to learn whether the variable Xj is affected by other root causes, while Eso and Eto are denoted as collections of Eco and E. \"*\" is the convolution operation.\nSimilar to [37], The first layer of each CNN is a 1D convolution layer with p + 1 kernels, stride equal to 1, and no padding, where the last kernels p + 1 represent instantaneous dependency. Finally, the CNN enhanced LOCAL (LOCAL-CNN) can be formulated as:\n\u0176t = CNN(Xt\u2212p:t; 0(9))\ns.t. WP+1(9) := WP+1(q) \u03bf \u039c\u03c4,\u03c9 (p(q)),0)"}, {"title": "3.5.2 Model Training.", "content": "In this part, we combine the ACML and DGPL modules with QMLE, then the overall loss function S (W, A; Xt) becomes:\nS(W, A; X\u2081) = S(Eso, Eto, p; Xt)\n= log(||DST {SX, \u2013 YA} ||\u00b2) \u2013 log |det(S)| + 12 (Rsparse(A) + Rsparse(W))\nwhere S := I \u2013 Eso (t)E\u0142o (t) \u03bf \u039c\u03c4,\u03c9 (p)u,v, A := Aso \u00a9 Ato, W := Eso(t)E(t) \u03bf \u039c\u03c4,\u03c9 (p)u,v\u00b7\nFor the nonlinear form, the overall objective becomes:\nS(W, A; X\u2081) = S(Eso, Eto, p; Xt)\n= log(||DST {Xt \u2013 CNN(Xt-p:t; 0(9))}||\u00b2) \u2013 log |det(S)| + 2(Rsparse(A) +Rsparse(W))"}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to answer the following research questions:\n\u2022 RQ1: As a framework based on temporal causal discovery, can LOCAL accurately recover causal structures according to the DBN from Eq. (1)?\n\u2022 RQ2: Can LOCAL accurately recover causal structures based on complex nonlinear fMRI dataset?\n\u2022 RQ3: Can causal structures discovered by LOCAL align with expert knowledge on real-world dataset?\n\u2022 RQ4: How do the key designs and parameters in LOCAL affect performance?\n\u2022 RQ5: What are the physical meanings of source embedding and target embedding matrices?"}, {"title": "4.1 Experimental Setup", "content": "This section provides an overview of the datasets, hyper-parameters setting, evaluation metrics, and compared baselines."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce an easy-to-implement variant to the dynamic causal discovery family, LOCAL, particularly tailored for"}]}