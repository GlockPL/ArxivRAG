{"title": "Advancing Content Moderation: Evaluating Large Language Models for Detecting Sensitive Content Across Text, Images, and Videos", "authors": ["Nouar AlDahoul", "Myles Joshua Toledo Tan", "Harishwar Reddy Kasireddy", "Yasir Zaki"], "abstract": "The widespread dissemination of hate speech, harassment, harmful and sexual content, and violence across websites and media platforms presents substantial challenges and provokes widespread concern among different sectors of society. Governments, educators, and parents are often at odds with media platforms about how to regulate, control, and limit the spread of such content. Technologies for detecting and censoring the media contents are a key solution to addressing these challenges. Techniques from natural language processing and computer vision have been used widely to automatically identify and filter out sensitive content such as offensive languages, violence, nudity, sex, and addiction in both text, images, and videos, enabling platforms to enforce content policies at scale. However, existing methods still have limitations in achieving high detection accuracy with fewer false positives and false negatives. Therefore, more sophisticated algorithms for understanding the context of both text and image may open rooms for improvement in media content censorship to build a more efficient and accurate censorship system. In this paper, we evaluate existing large language model-based content moderation solutions such as OpenAI moderation model and Llama-Guard-3 and study their capabilities to detect sensitive contents. Additionally, we explore recent large language models (LLMs) such as generative pre-trained transformer (GPT), Google Gemini, and Meta Llama in identifying inappropriate contents across media outlets. Various textual and visual datasets like X tweets, Amazon reviews, news articles, human photos, cartoons, sketches, and violence videos have been utilized for evaluation and comparison. The results demonstrate that LLMs outperform traditional techniques by achieving higher accuracy and lower false positive and false negative rates. This highlights the potential to integrate LLMs into websites, social media platforms, and video-sharing services for regulatory and content moderation purposes.", "sections": [{"title": "1. Introduction", "content": "The widespread dissemination of hate speech, harassment, harmful and sexual content, and violence across websites and media platforms presents substantial challenges and provokes widespread concern among different sectors of society [1]. This type of content, readily available via the websites, social media, and other digital channels, affects individuals of all ages, with particularly profound and frequently negative impacts on children and adolescents [2]. The use of pornography among adolescents has been rising steadily, and the age at which they first encounter sexually explicit materials has been decreasing [3]. This issue is complex, intersecting with mental health concerns, societal norms, legal ramifications, and the technical hurdles associated with the surveillance and restriction of such materials [4]. First and foremost, the exposure of minors to sexually explicit, violent, and otherwise inappropriate content can detrimentally impact their psychological development and overall well-being [4]. Media violence exposure has been associated with heightened aggression, a numbing towards violence, and increased fear, leading to a skewed perception of reality and social norms [5]. From a legal perspective, the distribution and consumption of certain content types, especially child pornography, constitute criminal offenses across numerous jurisdictions [6]. The advent of the digital age has introduced complexities in detecting, monitoring, and prosecuting these offenses [7]. Law enforcement and legal systems face challenges in adapting to the swift technological advancements, resulting in a regulatory gap in online content oversight [8]. On the technological front, identifying and filtering out explicit, violent,"}, {"title": "2. Related Work", "content": "and sexual content involves intricate tasks that demand advanced algorithms and machine learning techniques [9]. The creation of this technology needs to strike a delicate balance between ensuring precision and effectiveness and upholding privacy rights and freedom of speech [10]. The issues of false positives (erroneously identifying innocuous content as inappropriate) and false negatives (failing to recognize harmful content) pose significant challenges. The former may lead to undue censorship, while the latter risks exposing vulnerable populations to potentially harmful material. Despite these limitations, advancements in AI, such as improved image recognition models and more sophisticated algorithms for understanding human intent and context, hold promise for more efficient and accurate content moderation [11, 12, 13, 14, 15]. However, existing methods still have limitations in achieving high detection accuracy with fewer false positives and false negatives. Therefore, this work opens rooms for improvement in media content censorship to build a more efficient and accurate censorship system.\nIn this study, our proposed content censorship solutions utilize state-of-the-art LLMs to identify inappropriate text data and the vision capability of these LLMs to recognize sensitive contents in the images and videos. Against this backdrop, we summarize our contributions as follows:\n\u2022 We explored LLM-based content moderation solutions such as OpenAI moderation model and Llama-Guard-3.\n\u2022 We unveiled the potential of using LLMs such as Google Gemini 1.5, GPT-40, and Llama-3 to detect inappropriate text such as tweets, reviews, and articles.\n\u2022 We demonstrated a vision capability in LLMs and employed it in the task of visual content censorship, such as detection of images and videos containing nudity, pornography, violence, child abuse, alcohol, and drug abuse.\n\u2022 We formulated a visual content censorship task as a visual question answering task using various LLMs.\n\u2022 We utilized various textual and visual datasets containing text, images, and videos for evaluation and comparison.\nTo this end, we compile a visual and textual datasets including violence videos, images with nudity and pornography content, news articles from multiple news outlets, Amazon reviews, and X tweets, and examine violence,"}, {"title": "2.1. Identifying inappropriate textual content", "content": "The challenges of detecting and tracking hate speech domains, e.g., cyberbullying, abusive language, discrimination, sexism, extremism, and radicalization in text, are increasingly pressing for society, individuals, policymakers, and researchers. Several efforts have been made to employ automated techniques for detection and monitoring [16]. Studies in literature have most widely used the SVM algorithm [16, 17, 18, 19] for classification. On the other hand, traditional feature representation methods such as TF-IDF [17, 20], bag of words [21, 22], and N-gram [23, 24] have been demonstrated widely. Additionally, different kinds of word embedding word2Vec [25, 26], GloVe [19, 27], FastText [19, 27], and ELMO [28, 29] with CNN [30, 31] and RNN [31, 32] architectures have been explored in the literature. Moreover, several works claimed BERT's [33, 34, 29, 35] outperforming CNN and RNN models.\nThe increased connectivity has also facilitated the rapid dissemination of harmful and violence-inciting content. Several studies thoroughly investigate violence-inciting text using a diverse range of machine learning and deep learning models [36, 37, 38].\nDetecting content of profanity in speech or audio files for foul language censorship purposes is also an active research topic that has been explored using CNNs and RNNs [39, 40].\nThe censorship of online sexual predatory behaviors and abusive language on social media platforms has become a critical area of research [41, 42]. LSTM and BERT language models have been used for early sexual detection in chats and dialogue [42, 43, 44].\nThe detection of hate speech, violence-inciting, and sexual contents in textual data has various impacts on media outlets such as tweets [45, 46, 47], news articles [48, 49], social media posts [50], and messaging platforms [51]. Previously mentioned deep learning methods have outperformed traditional techniques in detecting hate speech, harmful, violence-inciting, and sexual contents in textual data. However, there is still room for improvement to enhance accuracy and reduce false positive and false negative rates. As such, this paper aims to improve textual content censorship using the large language models."}, {"title": "2.2. Identifying inappropriate visual content", "content": "The recognition of inappropriate visual content such as nudity and pornography in images and videos has seen significant advancements through various studies. This recognition is primarily categorized into four methods: color-based, shape-based, local feature-based, and machine learning-based. Color-based methods utilize pixel colors to identify skin regions, as discussed in several studies [52, 53]. Shape-based methods are subdivided into techniques such as contour-based methods [54, 55], moments [56, 57], geometric constraints [58], color segments [59], and MPEG7 features [60], which each offer a unique approach to recognizing inappropriate content. Local feature-based methods leverage tools like the scale invariant feature transform (SIFT) [61], probabilistic Latent Semantic Analysis (pLSA) [62], and bag of words (BoW) model [63, 64] to enhance detection accuracy. In recent years, machine learning-based techniques have become prevalent in the detection of adult content in images. Multiple-instance learning has been applied with notable success [65, 66].\nRecent developments have seen a surge in deep learning-based solutions for pornography and nudity detection, employing convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to enhance accuracy and efficiency [67, 68]. These approaches leverage the strengths of deep learning to interpret complex patterns in image and video content, significantly improving detection capabilities. The use of CNNs has been extensively researched, demonstrating their effectiveness in nudity detection [69, 70, 71, 72, 73, 74]. In addition, [75, 76, 77] has worked on the pornography detection in cartoon and sketch images. Moreover, an ensemble of CNNs has also been employed to improve detection rates [78]. Additionally, the incorporation of attention mechanisms has been explored to further refine the accuracy of nudity detection in images [79]. These advancements highlight the dynamic nature of research in this field and the continuous efforts to improve content recognition technologies.\nViolence detection is implemented across both surveillance and non surveillance environments, employing a spectrum of feature extraction methodologies that span traditional techniques to advanced deep learning approaches. Within traditional frameworks, methodologies such as scale-invariant feature transform (SIFT) [80], speeded-up robust features (SURF) [81], and bag of words (BoW) [82] have been pivotal. Conversely, the deep learning paradigm leverages specialized recurrent neural networks (RNNs), notably long short-term memory networks (LSTMs) [83], deep neural networks (DNNs) [84], Convolutional neural networks [85, 86], and CNN-LSTM models [87, 88] to enhance the efficacy and accuracy of violence detection. Specifically, in the context of detecting bloody content, a tripartite feature extraction strategy is employed, encompassing static, motion, and audio features [89] to provide a comprehensive analysis. This multifaceted approach underscores the evolving landscape of violence detection methodologies, reflecting a transition from traditional techniques to sophisticated, deep learning-driven strategies for robust and effective violence detection.\nPreviously mentioned deep learning methods have outperformed traditional techniques in detecting nudity, sexual contents, and violence. However, there is still room for improvement to enhance accuracy and reduce false positive and false negative rates. As such, this paper aims to improve visual content censorship in general and particularly nudity, sexual content, and violence detection using visual language models."}, {"title": "2.3. Emerging role of LLMs in text and image processing", "content": "Large Language Models (LLMs) have garnered significant attention for their impressive performance across various natural language tasks such as summarization [90], classification [91], code generation [92], and data generation [93], particularly following the release of chatGPT in November 2022 [94]. Their ability to understand and generate language in a general-purpose manner is achieved by training billions of model parameters on vast amounts of text data.\nLLMs have opened a new opportunity to address online sexual predatory chats and abusive texts by fine-tuning Llama2 [41]. They can also detect hate speech in text, according to a study [95], which investigated reactions of seven state-of-the-art LLMS (LLAMA 2, Vicuna, LLaMA 3, Mistral, GPT-3.5, GPT-4, and Gemini Pro). In this study, they also discussed strategies to mitigate hate speech generation by LLMs, particularly through fine-tuning and guideline guardrailing.\nThe vision capability in large language models refers to the extension of the transformer architecture, originally designed for natural language processing (NLP) tasks, to computer vision, enabling models to process and understand visual information in a manner similar to text [96]. While the original GPT models were focused on understanding and generating text [97], the concept of \"GPT for Vision\" [98, 99, 100] involves adapting this architecture to process and understand visual data. The key innovation in applying the GPT architecture to vision tasks lies in treating pixels or patches of an image as sequences, similar to how words or tokens are treated in NLP tasks.\nLLMs have also been integrated with traditional vision models in tasks like visual question answering (VQA) [101], where models are trained to respond to questions based on image content. This highlights the synergy between LLMs and image recognition, requiring a deep understanding of both visual and textual data [101].\nAlthough there have been few attempts to detect hate speech and sexually explicit content using large language models, to the best of our knowledge, no prior works have specifically targeted the detection of inappropriate content in customer reviews and news articles using LLMs. Additionally, identifying nudity, violence, child abuse, and drug or alcohol abuse using large language models (LLMs) has not yet been explored. This presents a unique opportunity to expand the application of LLMs to these areas, addressing both textual and visual content across diverse contexts."}, {"title": "3. Research Motivation", "content": "Integrating Large Language Models (LLMs), including Google Gemini 1.5 [102, 103], GPT-40 [104, 105], and Llama-3 [14] into censorship tasks has the potential to greatly enhance the performance and functionality of deep learning models. This paper explores recent state-of-the-art large language models to tackle complex challenges of natural language processing and computer vision, such as detecting inappropriate textual and visual contents, including violence, harassment, harm, sex, nudity, and hate speech. By leveraging the advanced understanding and generation capabilities of LLMs, the approach aims to improve textual and visual recognition in these domains.\nThe direct applications of LLMs in tackling such complex challenges of identifying inappropriate media contents such as tweets, posters, customers' reviews, articles, images, and videos are still an evolving area of research and practice. The current solutions provided by LLMs in various applications demonstrate, through their advanced comprehension and generative abilities, their potential to act as complementary\u2014or even alternative\u2014approaches to traditional and deep learning methods for censoring both textual and visual media. By harnessing the advanced language processing and contextual analysis capabilities of LLMs, researchers can significantly enhance the accuracy, efficiency, and flexibility of technologies employed in media content moderation and censorship."}, {"title": "4. Materials and Methods", "content": "This section describes the datasets used in the experiments conducted to evaluate the LLMs performance in textual and visual content censorship tasks. Moreover, the section discusses the baseline methods usually used in the literature and highlights our proposed LLM-based content censorship solutions."}, {"title": "4.1. Dataset Overview", "content": ""}, {"title": "4.1.1. Textual Datasets", "content": "In this section, we discuss the textual datasets used to evaluate and explore LLMs for textual content censorship tasks. These datasets contain text from various sources, such as news articles, Amazon reviews, X tweets, articles' descriptions, and text that has inappropriate contents such as hate speech, violence, adult contents, harassment, and harmful contents."}, {"title": "4.2. Methods", "content": "This paper examines harmful content across both text and visual modalities, offering a thorough foundation for identifying the limitations of existing content censorship systems. It addresses these gaps by exploring the potential of LLM-based content moderation, which enhances detection accuracy and minimizes false positive and negative rates through the integration of vision-language contexts.\nThe proposed solution for media censorship is an AI system that combines language and visual processing to improve content understanding and generation. This system is designed to detect inappropriate content in all types of media, such as text, images, and videos, based on provided prompts. We employed large-language models to harness their natural language processing abilities, enabling contextual interpretation and analysis within images. Figure 1 presents the block diagram illustrating the proposed solution for textual and visual content censorship.\nWe utilized the models covered in this section to assess the performance of large-language models in detecting inappropriate textual and visual content, which is central to this research."}, {"title": "4.2.1. OpenAI moderation model with Textual and Visual Date", "content": "OpenAI's moderation system found by OpenAI serves as a foundational framework, leveraging machine learning models designed to classify inputs (text and/or image) if potentially harmful across several categories [11]. \u201cOmni-moderation-latest\u201d model is the latest OpenAI moderation model that supports more categorization options and multi-modal inputs [11]. The categories contain harassment, harassment/threatening, hate, hate/threatening, illicit, illicit/violent, self-harm, self-harm/intent, self-harm/instructions, sexual, sexual/minor, violence, and violence/graphic [11].\nIn this study, we utilized OpenAI's content moderation tools and particularly the \"Omni-moderation-latest\" model to explore and evaluate the LLM-based content moderation in its capability to detect inappropriate content, such as sexuality, violence, hate speech, and other harmful materials [11]. First for text moderation, we employed the \u201comni-moderation-latest\u201d model [11] to automatically evaluate text input such as articles, Amazon customers' reviews, and X tweets by assigning a confidence score based on the likelihood that the content fits into one of several harmful categories, including hate speech, adult contents, and violence. These confidence scores are used to flag content that may need intervention or filtering [11].\nSecond, for visual content moderation, the \u201comni-moderation-latest\u201d model analyzes visual content such as images and videos, detecting harmful content like nudity, pornography, violence, child abuse, and drug and alcohol abuse. \u201cOmni-moderation-latest\u201d model incorporates object and scene recognition to classify images based on the same harmful content categories used in text moderation [11]."}, {"title": "4.2.2. Llama-Guard-3 with Textual and Visual Date", "content": "Llama-Guard-3 is an LLM-based safeguard model developed by Meta and available on Hugging Face. It is designed to handle multi-modal tasks by combining natural language processing with visual recognition capabilities [13, 14]. Llama-Guard-3 is also designed for human-AI conversational scenarios. The model features a safety risk taxonomy, which serves as an effective tool for categorizing specific safety risks identified in LLM prompts, enabling prompt classification [13].\nLlama-Guard-3 model uses multi-modal transformers to interpret visual and textual inputs concurrently [13], making them well-suited for our study's focus on detecting inappropriate content across text and image modalities. Llama-Guard-3-8B, with 8 billion parameters, exhibited strong performance in language tasks [113]. It is particularly useful in scenarios where computational resources are limited while still maintaining effective moderation capabilities.\nOn the other hand, the Llama-Guard-3-11B-Vision is a larger version with enhanced capabilities in both text and image recognition [114]. This model leverages its 11 billion parameters to provide more accurate and robust classification of inappropriate content in images and associated textual descriptions. The model was specifically tested for its ability to detect harmful imagery, such as explicit scenes or depictions of violence, and align these detections with accompanying textual cues for a comprehensive understanding of content."}, {"title": "4.2.3. Llama Instruct", "content": "Llama 3.1 is an auto-regressive language model, developed by Meta, built on an optimized transformer architecture [115]. It consists of multilingual LLMs that include pre-trained and instruction-tuned generative models, designed for text input and text output. Llama-3.1-8B-Instruct is a fine-tuned version that employs supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to better align the model with human preferences for helpfulness and safety [115].\nLlama-3.2-11B-Vision-Instruct is built upon the Llama 3.1 text-only model [116] and comprises multi-modal generative models, capable of processing both text and images as input and generating text output. Llama-3.2-11B-Vision-Instruct is an instruction-tuned model, specifically optimized for tasks such as visual recognition, image reasoning, captioning, and answering general questions about images [116]. For image recognition tasks, Llama-3.2-11B-Vision-Instruct incorporates a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model.\nIn this study, we evaluated both Llama-3.1-8B-Instruct [115] model and Llama-3.2-11B-Vision-Instruct [116] to complement our effort to detect inappropriate content, such as sexuality, violence, and hate speech, within both text- and vision-based data. The model is able to ensure that harmful content is filtered out based on a combination of object recognition in images and semantic analysis in text."}, {"title": "4.2.4. Gemini 1.5", "content": "The Gemini 1.5 model, developed by Google, integrates advanced vision and natural language processing techniques, making it particularly effective in understanding both text and images [102]. It also incorporates Google's advancements in instruction-following tasks, ensuring that specific content moderation guidelines were adhered to when evaluating both text and image inputs [117].\nWith its sophisticated architecture, Gemini 1.5 Pro excels in scenarios requiring high-level contextual understanding of images in conjunction with text. On the other hand, Gemini 1.5 Flash is a lightweight, smaller, and faster model, optimized for speed and efficiency [102].\nThe Gemini API provides safety settings that can be adjusted. It has several levels of blocking content, such as no block, only high probability of unsafe content, only medium or high probability, all low, medium, or high probability [118]. The Gemini API's safety filters cover the following categories: harassment, hate speech, sexual explicit, and dangerous content.\nIn this study, we further evaluated Gemini 1.5 Pro and Gemini 1.5 Flash to assess their capabilities in detecting inappropriate textual and visual contents that are usually available in media outlets."}, {"title": "4.2.5. GPT-4o", "content": "GPT-4o is an OpenAI's advanced language model builds upon the robust architecture of earlier GPT models, expanding its proficiency in both text and vision-language tasks [105]. GPT-4o employs an extensive parameter count and deep learning techniques that enable it to recognize text and visual content based on complex contextual understanding. Its vision-language fusion allows it to interpret both image-based and textual data concurrently. Leveraging GPT-4o's instruction-following capabilities, the model was fine-tuned for the specific tasks, ensuring adherence to predefined guidelines [105].\nIn this study, we evaluated GPT-4o [105, 104], specifically in the context of its multi-modal capabilities for detecting inappropriate textual and visual contents such as adult content, violence, and hate speech. This was particularly beneficial in scenarios where inappropriate visual elements were paired with text."}, {"title": "5. Results and Discussion", "content": "This section presents the evaluation of several LLMs on tasks related to textual and visual content censorship. It also demonstrates the comparison results in terms of their accuracy, recall, precision, F1 score, false positive rate (FPR), which is a proportion of negative instances incorrectly classified as positive, and false negative rate (FNR), which is a proportion of positive instances incorrectly classified as negative.\nState-of-the-art LLMs such as Llama-Guard-3, OpenAI moderation model, GPT-4o, Gemini 1.5, and Llama-3 are utilized and compared to tackle the above challenging censorship tasks. Additionally, we compared the LLM-based solutions with existing state-of-the-art methods.\nBoth Gemini 1.5 and OpenAI moderation model can detect multiple categories for the same instance. While OpenAI moderation model provides a score for each detected category, Gemini 1.5 assigns a Low, Medium, or High level for the detected category. Other LLMs used in our experiments generate only one category, as determined in the prompt given.\nSince Gemini 1.5 lacks specific categories for \u2018harm', \u2018violence', and 'graphic violence', these categories are grouped under the broader \u201cdangerous\u201d category.\nWe carried out multiple experiments to assess the capabilities of various LLMs for textual and visual content censorship, with a specific focus on identifying hate speech, sexual, and violent contents. Formulating the censorship application as a visual question-answer task allows to leverage the LLMs with their capabilities of understanding and processing both the image and associated text."}]}