{"title": "Unveiling Provider Bias in Large Language Models for Code Generation", "authors": ["Xiaoyu Zhang", "Juan Zhai", "Shiqing Ma", "Qingshuang Bao", "Weipeng Jiang", "Chao Shen", "Yang Liu"], "abstract": "Large Language Models (LLMs) have emerged as the new recom-mendation engines, outperforming traditional methods in both capability and scope, particularly in code generation applications. Our research reveals a novel provider bias in LLMs, namely without explicit input prompts, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). This bias holds sig-nificant implications for market dynamics and societal equilibrium, potentially promoting digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. This paper presents the first comprehensive empirical study of provider bias in LLM code generation. We develop a systematic methodology encompassing an automated pipeline for dataset generation, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Our analysis encompasses over 600,000 LLM-generated responses across seven state-of-the-art models, uti-lizing approximately 500 million tokens (equivalent to $5,000+ in computational costs). The study evaluates both the generated code snippets and their embedded service provider selections to quan-tify provider bias. Additionally, we conduct a comparative analysis of seven debiasing prompting techniques to assess their efficacy in mitigating these biases. Our findings demonstrate that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users' requests. Notably, we observe discrepancies between providers rec-ommended in conversational contexts versus those implemented in generated code. The complete dataset and analysis results are avail-able at https://anonymous.4open.science/r/InvisibleHand-7566.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommendation systems (RS), represented by search engines, have been widely used to retrieve information and complete various tasks in people's daily life [21, 78]. However, it harbors dangers. RS can be biased due to intentional design (e.g., favoring spon-sors' products), resulting in the prioritization of items and content from stakeholders in recommendation results, ultimately leading to serious consequences of market monopoly, and even threatening users' personal safety and social order. For example, in the past ten years, Google, the most popular search engine provider, has been sued multiple times for search monopoly, and the EU alone has fined it more than $8 billion [11, 12]. Baidu Inc., which operates the largest search engine in China, has also been investigated by the government for prioritizing the recommendation of illegal medical advertisements that led to the death of users [20]. Researchers have also revealed that bias in recommendation and search results can influence elections [33]. With the development and application of Artifact Intelligence (AI) technology, Large Language Model (LLM) has become one of the most important channels and means for people to retrieval in-formation and knowledge. According to OpenAI's report, ChatGPT, which is one of the most popular LLMs, serves and impacts over 100 million users weekly [5]. LLMs, as recommendation systems, excel capabilities of traditional techniques. One of the most promising and active applications is code recommendation, which returns code snippets based on users' demands. Recent reports [28] show that LLMs have significantly reduced the activity of traditional programming information retrieval and question-answering plat-forms like StackOverflow. As the new recommendation engine in the AI era, there is an urgent need to study and explore potential new types of bias brought by this new capability and its security consequences. In this paper, we introduce a new type of bias in LLMs for code generation, provider bias, which is the preference for spe-cific service providers. We observe that the code snippets gener-ated by LLMs frequently use the services from specific providers (e.g., Google Speech Recognition) and rarely use services of other providers, despite their popularity and market shares in reality. LLMs can even silently modify user code without user request, replacing the source services with the services from preferred providers (Fig. 1). Provider bias potentially not only impairs users' autonomy decision-making, but also promotes unfair market com-petition and digital monopolies, leading to serious legal conse-quences. For example, the Sherman Antitrust Act in the United States investigates whether digital platforms lead to monopolistic behaviors [25]. The European Union has implemented the Digi-tal Services Act (DSA), prohibiting various digital platforms and services from 'deceiving or nudging recipients of the service (i.e., users) and distorting or impairing the autonomy decision-making, or choice of the recipients of the service' [34]. The U.S. Federal Trade Commission Act also prohibits deceptive or manipulative acts or practices in commerce, including those digital platforms [1]. However, existing LLM security research mainly focuses on the social biases related to gender and race in LLM question-answering and code generation [15, 41, 52, 71]. To the best of our knowledge, there is no prior work to explore the provider bias in LLM for code generation and its risks. To fill the gap, we conduct the first large-scale experiments to study the provider bias in seven state-of-the-art (SOTA) LLMs for code generation, including GPT-3.5, GPT-40 [60], Claude-3.5-Sonnet [19], Gemini-1.5-Flash [39], Qwen-Plus [18], DeepSeek-V2.5 [53], and Llama-3.1-405b [32]. Concretely, we aim to under-stand LLM's preferences for various service providers and study and reveal the impact and threat of provider bias. The main challenge to be addressed by the study is how to build the first comprehensive dataset covering a variety of real-world tasks and scenarios to dis-cover and assess LLM provider bias. To overcome the challenge, we first investigate and collect commonly used coding tasks from real-world LLM users, as well as various application scenarios that need use third-party services and APIs in code. We then construct an au-tomated pipeline to generate various input prompts. As a result, our dataset contains 17,014 items of input prompts, covering 6 distinct coding task categories and 30 verified real-world application sce-narios. Subsequently, we utilize this dataset to evaluate LLMs and record their responses to different tasks and scenarios. Finally, we extract the embedded services and corresponding providers from the code snippets of LLM responses and evaluate LLM provider bias and its impact on user's input code. Concretely, our study answers the following research questions:\n\u2022 RQ1: Do LLMs have provider bias in code generation? To intuitively explore the provider preferences of LLMs, we analyze 20,026 valid LLM responses across 30 scenarios where users only provide requests but not prototype code. We observe that LLMs exhibit significant provider preferences in code generation and achieve an average Gini coefficient of 0.80 across scenarios, close to the max-imum value of 1 (i.e., preferring and only using the services from a single provider). Google and Amazon are the most preferred providers across seven LLMs, whose services achieve the highest usage rate in 46.67%-56.67% of scenarios across different LLMs.\n\u2022 RQ2: Do LLMs have provider bias in code modification? To evaluate the impact and modification of provider bias on user code snippets and reveal potential threats, we analyze 571,057 LLM re-sponses across 5 coding tasks (where code appears in input prompts) and calculate the ratio of LLM modifying source services in the user code. Notice that the user prompts do not ask for the modification of the source services. We observe that LLMs are susceptible to provider bias and then modify the services in user input, with a total of 11,582 modification cases across different models. In modification cases, the code snippets using Microsoft services are modified the most, and the most common (i.e., the most preferred) target provider is Google. It illustrates the risks that provider bias promotes unfair competition in digital markets. It may also cause financial loss to users who have already purchased services from the source provider (if they do not manually check the code snippets generated by LLMs).\n\u2022 RQ3: Is LLM provider bias consistent with the internal knowledge of LLMs? To obtain the preference for providers in the internal knowledge of LLMs, we directly ask LLMs to rank their preference for different service providers in various scenarios. Then, we investigated the consistency between the provider preference of LLMs in the conversations and the preference in actual code generation. We observe that in over 90% of scenarios, there is no sig-nificant correlation between the ranking from the internal knowledge of LLMs and the ranking from the preference in actual code genera-tion, which demonstrates the difference between LLM knowledge and actual biased behaviors.\n\u2022 RQ4: Can prompt engineering methods effectively mitigate provider bias? Considering that most SOTA LLMs are closed-source and provided as APIs, we conduct an experiment to study the effectiveness of seven debiasing prompting techniques in mitigating LLM provider bias from the users' perspective. The experimental results on 56,000 LLM responses show that prompting techniques are difficult to mitigate provider bias in LLM code generation without introducing large overhead. Furthermore, asking LLMs to maintain source/specific services in generation can significantly mitigate the LLM's modification of services in user code. In summary, we reveal a new type of threats in LLMs as recom-mendation systems, provider bias. That is, LLMs prefer to use the"}, {"title": "2 MOTIVATION", "content": "Our study on LLM provider bias is motivated by a real-world case encountered by one of our authors. The author is developing a speech recognition tool in Python to convert audio commands into actionable tasks for smart home devices. The tool utilizes the open-source framework DragonFly, which supports multiple backends, including Dragon Speech Recognition (DSR) and Windows Speech Recognition (WSR), providing both scalability and portability. Lever-aging DSR and WSR support within our organization, the tool can use these speech recognition services for free to fulfill functional requirements without additional charges. During development, a critical bug arose due to missing several lines of code that define the variable grammars and load the light control rules ( self.light_rule ) to the DragonFly engine. To resolve this, the author queries the Gemini-1.5-Flash model , providing relevant code snippets and expecting the model to identify and fix the bug. The prompt is as follows. Please review and debug the following Python code that is used to perform the Voice Command for Smart Home scenario of the Speech Recognition task. The given Python code can: 'Create a program that listens for specific voice commands to control various smart home devices, such as lights, thermostat, and security systems, by processing and recognizing spoken instructions'. However, the response from Gemini-1.5-Flash deviated signifi-cantly from expectations. Instead of identifying and fixing the bug, the model fundamentally alters the functions and classes in the input code snippet. Specifically, it replaces the intended DragonFly service with Google Speech Recognition, as illustrated in the red box on Lines 18 and 19 of Fig. 1(b). Google Speech Recognition, a proprietary service developed by Google, requires a paid API with usage-based charges. Notably, the author does not mention Google Speech Recognition service in the input prompt and does not in-tend to use this service in the code. Adopting the generated code snippet would abandon the source services (i.e., WSR) supported by our organizations, thereby increasing development and main-tenance costs, which is contrary to the author's intent to utilize a cost-effective, open-source solution. In contrast, GPT-3.5-Turbo, another state-of-the-art LLM, accurately identifies and fixes the bug when querying with the same inputs, as shown in Fig. 1(c). The corrections made by GPT-3.5-Turbo are marked in green. Detailed results are available in our repository [14]. Such service modifications of LLMs are neither isolated incidents nor rare corner cases. Our further experiments on other LLMs reveal that the LLMs under test are all biased and often exhibit preferences for specific service providers during code generation and recommendation. In some cases, they even alter user-provided code to integrate services from preferred providers without explicit user requests. We define this new type of bias in LLM code generation and recommendation as LLM provider bias. Definition 2.1 (LLM provider bias). LLM provider bias refers to the systematic preference towards specific service providers and producers in LLM responses. This bias not only leads to high expo-sure of services from specific providers in recommendation results, but could also introduce unsolicited modifications to user input code, steering users away from their original choices. Provider bias can lead to serious security and ethical concerns. Similar to biases in traditional RS, LLM provider bias can be deliber-ately manipulated to increase the visibility of services from specific providers (e.g., sponsors) in code recommendations and generation, suppressing competitors and leading to unfair market competi-tion and digital monopolies. More critically, LLM provider bias may introduce unauthorized service modifications to user code. Careless users may not thoroughly review the LLM outputs [10] and unknowingly adopt altered code snippets, thereby being de-ceived and making controlled decisions, increasing development costs, and potentially violating organizational management policies (e.g., unauthorized use of competitors' services). Our human study reveals that 87% of participants are unable to detect the service modifications in LLM responses and choose to accept the modified code snippets. Furthermore, after being informed of these modifi-cations, 60% expressed concern that it undermined their autonomy in decision-making (\u00a7A.1.3). Admittedly, some vigilant users can identify these modifications, the provider bias still diminishes the perceived intelligence of LLMs and erodes user trust, hindering the adoption and application of models. Additionally, users are forced to invest extra time and resources to rewrite biased code snippets. According to our study, 46% of participants agree that this modification negatively impacts their experience. In this paper, we present the first large-scale empirical study to take a step further to investigate the provider bias and security consequences in seven widely used state-of-the-art LLMs, namely GPT-3.5, GPT-40, Claude-3.5-Sonnet, Gemini-1.5-Flash, Qwen-Plus, DeepSeek-V2.5, and Llama-3.1-405b. Our study introduces an auto-mated pipeline to construct a comprehensive dataset comprising 6 coding tasks and 30 real-world application scenarios, to analyze LLM's preference for specific service providers. Furthermore, we ex-amine the impact of LLM provider bias on user code across diverse coding tasks. From this extensive investigation, we distill seven key findings that illuminate challenges and opportunities for advancing research on AlI security and ethics from a new perspective."}, {"title": "3 METHODOLOGY", "content": "Our methodology consists of two stages. In constructing dataset, we summarize six categories of coding tasks and collect 30 real-world scenarios, along with 145 detailed scenario requirements"}, {"title": "3.1 Constructing Dataset", "content": "To construct a comprehensive dataset for evaluating LLM provider bias in code generation, our prompt generation pipeline needs to meet two requirements. Covering various code application sce-narios where code snippets need to call specific APIs or services to complete given functional requirements. For example, the 'Speech Recognition' scenario in Fig. 1 typically requires calling third-party speech recognition services (e.g., Dragonfly) or paid API (e.g., Google Speech Recognition). Covering a variety of coding tasks that users ask or prompt LLMs to perform (e.g., the debugging task in Fig. 1). Collecting Scenarios. We first collect diverse code application examples and corresponding detailed functional requirements from the open-source community [4, 13]. Then, we group requirements that utilize similar types of APIs and services into unified scenarios, while distinguishing scenarios that require fundamentally differ-ent services or APIs. For example, requirements such as 'Voice Command for Smart Home' and 'Transcribing Meetings' both in-volve speech-to-text conversion (for commands or meeting logs). These requirements can be fulfilled using the Dragonfly service, as illustrated in Fig. 1, and are thus categorized under the 'Speech Recognition' scenario (Table 3). We invite two co-authors with ex-pertise in software engineering (SE) and artificial intelligence (AI) security to independently verify and categorize the collected sce-narios. For the inconsistency in the classification, a third co-author organizes discussions until all participants reach a consensus on the categorization. This process results in a final collection of 30 sce-narios encompassing 145 subdivided requirements. Table 3 shows a subset of these scenarios and their associated requirements. For each scenario, we manually collect a minimum of five third-party services or APIs from different providers. Our analysis shows that Python is the programming language with the most compre-hensive support (e.g., various libraries and interfaces) from these services, and Java ranks second. Consequently, our dataset focuses on Python code snippets. We systematically collect the features of different services (i.e., URL templates, keywords, and library names), which can be used for subsequent result labeling. To illustrate, using the Dragonfly service in Fig. 1 typically needs to load the 'dragonfly' library. Therefore, 'dragonfly' is one of the features for Dragonfly service. Code snippets that use Amazon web services often use URLs with 'aws' in them (e.g., https://xxx.amazonaws.com), mak-ing such a URL template one of the features for Amazon services. The collected scenarios, services, and features are in our repository. Generating Prompts. To generate diverse input prompts and cover various coding tasks developers query LLMs to perform, we first collect coding tasks from open-source community and then design a questionnaire for developers in our organization with expertise in the fields of computer science. Based on the responses from 39 developers who self-reported 'using LLMs to assist in the development of at least two projects', we obtain six categories of coding tasks that developers commonly use on LLMs in follows. We design the corresponding prompt templates based on relevant blogs in the open source community [7, 8, 66], as shown in Table 1.\n\u2022 Generation. Users prompt LLMs to generate Python code snippets directly from scenario description and requirement without any intial code reference.\n\u2022 Debugging. Users query LLM to review and debug the user code to fix bugs such as missing neccesary variables in the code snippets (Fig. 1 provides an example).\n\u2022 Translation. Users request LLM to translate the initial Python code snippets into Java code snippets based on the given scenario and requirements. We have verified that services embedded in these initial code snippets maintain Java lan-guage support."}, {"title": "3.2 Labeling Responses", "content": "Using the constructed dataset, we evaluate 7 representative LLMs from different companies and label their responses in this stage (i.e., 5 closed-sourced commercial models and 2 open-sourced mod-els), including GPT-3.5-Turbo, GPT-40, Claude-3.5-Sonnet, Gemini-1.5-Flash, Qwen-Plus, DeepSeek-V2.5, and Llama-3.1-405b. These models are selected based on their state-of-the-art performance in text generation and code generation tasks and their widespread adoption in the community [17, 54]. More details of models are in \u00a7A.1.4. Our evaluation budget varies by task category. For the prompts in the 'generation' task without initial code, we repeatedly query LLM 20 times with each prompt to get the different services used in the code snippets generated by LLMs for each scenario and requirement. For the other coding tasks containing code snippets, we perform 5 queries for each prompt to manage cost. Finally, we obtain a total of 610,715 recorded LLM responses. To effectively identify and extract service providers from the generated code snip-pets in LLM responses, we develop a feature-based labeling pipeline, including two steps.\n\u2022 Step 1: Filtering. The pipeline first identifies and removes invalid re-sponses that lack code snippets. These invalid responses are usually refusal responses or non-code content like purely conceptual coding suggestions. Invalid responses are detected by the absence of essen-tial syntax elements (e.g., 'def' and 'return' in Python). This filtering process eliminates 19,632 invalid responses, with their distribution and root causes illustrated in Fig. 2. Our analysis reveals that Qwen-Plus generates the highest proportion of invalid responses (81.66%), while Llama-3.1-405b produces the lowest (0.38%). Notably, 86.56% of invalid responses result from overly restrictive content filtering and alignment mechanisms. This finding highlights the critical need for improving model capabilities and optimizing content filtering mechanisms in future LLM applications.\n\u2022 Step 2: Labeling. Our pipeline identifies services in generated code by matching against previously collected features of services in the scenario. For instance, in the 'Speech Recognition' scenario, when the code snippet imports the 'dragonfly' library, the pipeline identifies it as using the Dragonfly service. To ensure accuracy, the pipeline restricts service matching to only those services relevant to the scenario in the input prompt, preventing false matches across multiple services and providers. Notably, we have not observed"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "4.1 Setup Metrics. We implement two metrics to evaluate and measure LLM provider bias on different coding tasks in our experiments.\n\u2022 Gini Index (GI) (i.e., Gini coefficient) is widely used to measure the degree of unfairness and inequality in traditional RS [35, 38, 55, 76]. Our experiment uses GI to measure LLM's preference for service providers involved in the 'generation' task (without code snippets in inputs) across different scenarios, as shown in the following."}, {"title": null, "content": "GI = \\frac{\\sum_{i=1}^{n}(2i - n - 1)x_i}{n \\sum_{i=1}^{n} x_i}"}, {"title": null, "content": "where $x_i$ represents the number of times the service of provider $i$ is used in LLM responses, and n represents the number of distinct providers that have appeared in all model responses in this scenario. When the LLM uses services of different providers equally, it has $x_i = \\frac{\\sum_{i=1}^{n} x_i}{n}$, and GI takes its minimum value of 0. When the LLM prefers a specific provider and uses only their service in a certain scenario, GI takes its maximum value of 1.\n\u2022 Modification Ratio (MR) evaluates the provider bias of LLMs in the code modification tasks where input prompts include code snippets (i.e., 'debugging', 'translation', 'adding unit test', 'adding functionality', and 'dead code elimination'). In these tasks, the initial code snippets in user prompts already utilize services from specific providers to meet the functional requirements of a given scenario. However, in some cases, LLMs may silently alter the services in the initial code snippets, replacing them with services from other providers. These occurrences are referred to as modification cases. For clarity, we define the service/provider in the initial code snippet as the source service/provider, and the one introduced in the LLM response as the target service/provider. We propose MR to quantify this behavior by calculating the proportion of modification cases $N_m$ to the total number of queried cases N, as expressed below."}, {"title": null, "content": "MR = \\frac{N_m}{N} \\times 100%"}, {"title": null, "content": "The value of MR ranges from 0 to 1 (i.e., 100%), with a higher value indicating a greater impact of LLM provider bias on user code and intended services. An MR value of 1 signifies the most severe case, where the LLM modifies the services in all input prompts, replacing them entirely with services from other providers (e.g., preferred providers). This indicates that the model completely tamper with the user's original intent. Statistical Strategy. To enhance the robustness and reliability of our analysis across different LLMs, tasks, and scenarios, we employ"}, {"title": "4.2 RQ1: Do LLMs have provider bias in code generation?", "content": "Experiment Design: To evaluate the provider bias and identify the providers whose services are utilized in LLM responses for the 'generation' task, we first analyze the Python code snippets gener-ated by each LLM and obtain the services used in the code snippets and corresponding providers. In total, we collect 20,026 valid re-sponses across seven LLMs and extract associated providers from these responses. Based on these results, we analyze the distribution of services from different providers used by LLMs and calculate the Gini Index (GI) for each model across different scenarios to quan-tify provider bias in the 'generation' task. Additionally, to further understand LLM preferences, we count the most frequently used providers (i.e., the preferred provider in the following sections) for each scenario, highlighting those whose services are predominantly utilized in the LLM-generated code snippets. Analysis of LLMs: The distribution of GI values for different models across various scenarios is shown in Fig. 3. Red and yellow separately mark the median and mean GI values for each LLM. The results indicate that all LLMs under test frequently exhibit high GI values, with a median of 0.80, underscoring significant bias and unfairness toward specific service providers. Among the models, DeepSeek-V2.5 achieves the highest average GI of 0.82. Notably, it has achieved a maximum GI up to 0.94 in the 'Speech Recognition' scenario. In this case, 98.60% of its responses choose to use Google's services (i.e., Google Speech Recognition) to fulfill the functional requirements, while only 1.40% of generated code snippets use Nuance's speech recognition services. Qwen-Plus and"}, {"title": "Finding 1", "content": "LLMs generally exhibit bias and unfairness towards different providers in code generation, and the provider bias is not correlated with the model generation capability. Among them, DeepSeek-V2.5 shows the strongest provider bias, while GPT-3.5-Turbo demonstrates the best fairness."}, {"title": "Analysis of Scenarios", "content": "We observe that the distribution of GI values varies significantly across different scenarios. In some sce-narios, multiple LLMs exhibit severe provider bias, resulting in most generated code snippets relying on services from a specific provider. Specifically, LLM provider bias is most severe in the 'Speech Recog-nition' scenario, where the average GI across the seven models reaches 0.91. In this scenario, between 78.70% and 98.60% of the code snippets generated by these models utilize Google's services to fulfill speech recognition requirements. Similarly, scenarios such as 'Translation', 'Text-to-Speech', and 'Weather Data' show high GI values of 0.88, 0.87, and 0.84, respectively. For 'Translation' and 'Weather Data', all seven LLMs exhibit a strong preference for the services from Google and Open Weather, which are used in over 89.80% and 72.90% of the generated code snippets, respectively. In contrast, in the scenarios of 'Authentication & Identity Manage-ment' and 'File Storage & Management', LLMs achieve relatively fair results, with average GI values of 0.66 and 0.69, respectively. In these scenarios, no single provider's service is applied in more than 50% of the generated code snippets across all models. Significant discrepancies in provider bias can also occur among different LLMs within the same scenario. For example, in the 'Email Sending - Email Marketing' scenario, GPT-40 and Llama-3.1-405b"}, {"title": "Finding 2", "content": "LLMs exhibit varied provider bias in different sce-narios. Provider bias is most severe in the 'Speech Recogni-tion' and 'Translation' scenarios, and all seven LLMs exhibit a preference for a specific provider (i.e., Google). In contrast, in 'Authentication & Identity Management' and 'File Storage & Management' scenarios, LLMs can generally provide relatively fair recommendation results for different providers."}, {"title": "Analysis of Popular Providers", "content": "We first identify the most com-monly used providers for each LLM across different scenarios (ex-cept 'None' provider). Our analysis reveals that Google is the most frequently used provider, with the highest usage in 26.67% to 43.33% of scenarios. It is followed by providers such as Amazon and Mi-crosoft, as illustrated in Fig. 6. This frequent usage of Google's services may stem from their broader applicability, as Google's services support 28 scenarios, compared to Amazon and Microsoft, whose services support 20 and 18 scenarios, respectively. To further explore LLMs' preferences for these popular providers (i.e., Google, Amazon, and Microsoft), we analyze their responses in 15 scenarios that are supported by all three providers (e.g., 'Cloud Hosting' and 'Text-to-Speech'). The distribution of the preferred providers is shown in Fig. 5. Our findings show that LLMs generally favor Amazon in the majority of these scenarios, followed by Google. Only Gemini-1.5-Flash and Llama-3.1-405b demonstrate a stronger preference for Google over Amazon. This is particularly evident for Gemini-1.5-Flash, which favors Google's services in 8 out of the 15 scenarios. In addition, despite its global prominence as a leading provider, these LLMs rarely prefer Microsoft's services across different scenarios. Fig. 12 shows the distribution of popular providers in the generated code snippets, further supporting the above observations. The services from Amazon and Google are the most widely used in LLM code generation, while 34.00% of code snippets generated by Gemini-1.5-Flash utilize Google's services."}, {"title": "Finding 3", "content": "Different LLMs exhibit varying preferences for popular providers in code generation. In 15 scenarios covered by all three popular providers, five LLMs demonstrate a strong preference for Amazon, followed by Google and Microsoft. Only Gemini-1.5-Flash and Llama-3.1-405b show the strongest preference for Google."}, {"title": "4.3 RQ2: Do LLMs have provider bias in code modification?", "content": "Experiment Design: To explore LLM provider bias in code mod-ification and assess its impact on user code and the embedded services, we collect code snippets from LLM responses across five coding tasks that contain initial code in input prompts (i.e., 'debug-ging', 'translation', 'adding unit test', 'adding functionality', and 'dead code elimination' in Table 1). We then extract the services and corresponding providers used in the generated code snippets."}, {"title": null, "content": "Specifically, we obtain 571,057 valid responses containing code snippets generated by seven LLMs. Subsequently, we compare the service providers in the responses with the service providers of the input code snippets (recorded during dataset construction) and count the number of modification cases, where LLMs alter the ser-vice and its provider in the input code. Finally, we calculate the MR to quantify the impact of LLM provider bias on user code. Analysis of Modification Cases: We have identified a total of 11,582 modification cases, with an average MR of 2.00% across all seven models. Fig. 7 illustrates the distribution of modification cases for different LLMs across various coding tasks. Among seven LLMs, Claude-3.5-Sonnet has achieved the highest MR of 3.90%, indicating a tendency to modify the source services users expect to use and replace them with services from different providers. This MR is significantly higher than that of the other models (p < 0.05). In contrast, Deepseek-V2.5 and Llama-3.1-405b are hardly affected by provider bias, with the lowest MR of only 1.40%. This indicates they can focus on the given coding task, rather than completely rewriting the user's code snippets and altering the user's intended services. We also evaluate the correlation between the number of modification cases and each model's code generation capability (see \u00a74.2). The results show that the Spearman coefficient reaches 0.49, indicating that the number of modification cases does not significantly correlate with the model's code generation capability. Regarding coding tasks, 'translation' and 'debugging' are most susceptible to provider bias and modify the source service in user code, as marked in green and purple of Fig. 7. They achieve an average MR of 5.20% and 3.00% on seven LLMs, respectively, which is significantly higher than other coding tasks. Our analysis shows that these two tasks often involve modification or even restructur-ing of the user's input code, leading to the complete replacement of the source service. In contrast, 'adding unit test' and 'adding func-tionality' are the least affected by provider bias, with an MR of only 0.30%. In these two categories of coding tasks, LLMs usually only"}, {"title": "Finding 4", "content": "Claude-3.5-Sonnet is the most likely to modify services in user code snippets, highlighting the significant im-pact of provider bias on LLM code recommendation results and interference with user intent. In contrast, DeepSeek-V2.5 and Llama-3.1-405b show the lowest MR, indicating less provider bias in code modification. There is also no significant correla-tion between the model capability and the number of modified cases. Additionally, the coding task 'translation', which often requires substantial code revisions, is the most likely to trigger modification cases."}, {"title": "Analysis of Providers in Modification Cases", "content": "We further an-alyze the distribution of the source providers being modified and the target providers used in the LLM responses in the collected modification cases. Our analysis shows that the distribution of tar-get providers across different scenarios in modification cases is not significantly correlated with the distribution of providers in the 'generation' task in \u00a74.2 (chi-square test). In general, modification cases involve a diverse set of target providers. The target provider with the highest ratio in modification cases (i.e., most commonly used) is Google, accounting for 14.90% across seven LLMs, signifi-cantly higher than the ratio of Apache (6.90%) and Amazon (2.10%) and other Python libraries. Note that Apache and Spring framework (i.e., 13.00% and 10.70%) achieve a ratio close to Google (13.80%) in the 'translation' task, likely due to their strong support for the Java programming language, enabling LLMs to learn more code snippets involving Apache and Spring in their training corpus. For the source providers modified by LLMs, Microsoft accounted for the largest proportion, reaching 11.50% across different models, respectively. Fig. 8 uses a Sankey diagram to show the proportion of source and target providers in modification cases on Claude-3.5-Sonnet. To further understand LLMs' preferences for popular service providers in modification cases, similar to \u00a74.2, we compare the distribution of preferred providers in the source and target provider across 15 scenarios, as shown in Fig. 9. Purple indicates scenarios where LLMs exhibit no modification cases. More details of the distri-bution of service providers are in \u00a7A.2.3. The results reveal Google's dominant position as the most preferred provider in modification cases. LLMs rarely heavily modify code snippets that use Google's"}, {"title": "Finding 5", "content": "In modification cases, the distribution of target providers is not significantly correlated with the distribution of providers in the 'generation' task. Google is the preferred target provider in many scenarios, which means that LLM tends to modify other services in the input code snippets to Google's services. Code snippets using services from providers such as Microsoft and Nuance are heavily modified by LLMs, reflecting LLMs' discrimination against these providers."}, {"title": "4.4 RQ3"}]}