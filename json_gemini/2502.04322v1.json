{"title": "SPEAK EASY: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions", "authors": ["Yik Siu Chan", "Narutatsu Ri", "Yuxin Xiao", "Marzyeh Ghassemi"], "abstract": "Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative-two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HARMSCORE, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and SPEAK EASY, a simple multi-step, multilingual attack framework. Notably, by incorporating SPEAK EASY into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HARMSCORE in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large language models (LLMs) have driven their widespread adoption across various domains (Achiam et al., 2023; Anthropic, 2023; Touvron et al., 2023), serving a variety of individuals from highly skilled experts to non-technical, everyday users (Bommasani et al., 2021). To ensure safe deployment, significant efforts have been made to align these models (Bai et al., 2022a;b; Ganguli et al., 2022; Markov et al., 2023). However, these efforts face ongoing challenges from \u201cjailbreaks\u201d (Jin et al., 2024; Wei et al., 2024), adversarial attacks that aim to breach LLMs' safety mechanisms and induce harmful responses, which pose societal risks when used by malicious actors (Hendrycks et al., 2023).\nDespite the widespread adoption of LLMs by non-technical users, current research offers limited insights into how jailbreaks manifest in simple, everyday interactions. Existing jailbreak methods typically require a technical understanding of models' internal mechanisms (Zou et al., 2023) or extensive engineering efforts (Chao et al., 2023; Mehrotra et al., 2023). These settings, however, may not accurately reflect real-world scenarios where an average user attempts to misuse LLMs for malicious purposes (NPR, 2025).\nTo address this gap, we investigate two questions: (1) What kinds of jailbroken responses enable non-technical users to induce harm? (2) Can these responses be obtained through simple interactions with an LLM? To answer the first question, we identify four attributes (Xing et al., 2017; Cho et al., 2019; Ganguli et al., 2022) potentially related to harmfulness and curate a synthetic dataset in which each example demonstrates a combination of these attributes. Through human evaluation, we determine actionability and informativeness as key attributes in inducing harm when the jailbroken response is followed by individuals without specialized knowledge. On this basis, we introduce HARMSCORE, a metric that explicitly measures the aforementioned attributes and provides a more fine-grained assessment of jailbreak harmfulness than commonly used measures of success (e.g., Attack Success Rate (ASR) (Ganguli et al., 2022; Mazeika et al., 2024; Wei et al., 2024)). Notably, HARMSCORE aligns better with human judgments than ASR, especially for queries that seek practical instructions.\nTo demonstrate that simple interactions can sufficiently elicit actionable and informative jailbreaks, we propose SPEAK EASY. In contrast to other jailbreak frameworks, SPEAK EASY emulates two types of human-LLM interactions commonly observed in real-world examples (Deng et al., 2024b; Zhao et al., 2024; Zheng et al., 2024): multi-step reasoning and multilingual querying. Given a harmful query, users can decompose it into multiple seemingly innocuous subqueries (Dua et al., 2022; Kojima et al., 2022; Wei et al., 2022), which more easily circumvent safety guardrails. They can"}, {"title": "2. Related Work", "content": "Jailbreaking LLMs. Methods to jailbreak safety-aligned LLMs range from manual approaches to automated techniques, including gradient-based token optimization (Zhu et al., 2023; Zou et al., 2023; Liao & Sun, 2024), multi-agent prompt augmentation (Perez et al., 2022; Chao et al., 2023; Mehrotra et al., 2023), and custom inference templates (?Anil et al., 2024). However, these methods often require technical expertise and intensive computation, making them less accessible to average users and less suited for evaluating LLM safety in real-world scenarios. In contrast, we explore the potential to elicit harmful outputs through simple natural language interactions in realistic scenarios.\nJailbreak Evaluation. To systematically evaluate model safety against jailbreak methods, several benchmarks have been proposed (Mazeika et al., 2024; Chao et al., 2024; Xie et al., 2024). These benchmarks typically focus on jailbreak success, where model responses are evaluated by an LLM judge using metrics including compliance (Zou et al., 2023; Wei et al., 2024), fulfillment (Xie et al., 2024), harmfulness (Huang et al., 2024b), validity (Zhu et al., 2023), and specificity (Souly et al., 2024). In contrast, we examine the underlying attributes of harmful responses that a malicious, non-expert user seeks.\nCommon Human-LLM Interactions. To help LLMs better understand user intent and solve complex problems, prompt design strategies, such as demonstrated reasoning steps (Nye et al., 2021; Wei et al., 2022) and query decomposition (Perez et al., 2020; Dua et al., 2022; Zhou et al., 2023), have gained popularity in user interactions (Deng et al., 2024b). Additionally, LLMs' multilingual ability (Fu et al., 2022; Achiam et al., 2023; Ahuja et al., 2023) further enhances accessibility for users from diverse linguistic backgrounds. However, the ease of interaction also introduces risks, as malicious users may exploit multi-step reasoning capabilities (Shaikh et al., 2023; Li et al., 2023), multi-turn conversations (Li et al., 2024; Huang et al., 2024a), or safety loopholes in mid- and low-resource languages (Yong et al., 2023; Deng et al., 2024a; Wang et al., 2024b). In fact, such behaviors already exist in real-world user-LLM interaction logs (Zhao et al., 2024; Deng et al., 2024b). Our work aims to demonstrate the simplicity and efficacy of manipulating these interaction modes for harmful jailbreaks."}, {"title": "3. What Constitutes a Harmful Jailbreak?", "content": "Existing evaluation metrics predominantly score jailbreak responses with explicit measures of success, such as ASR (Mazeika et al., 2024; Wei et al., 2024). However, relying solely on success rates may lead to misalignments with human judgment (Souly et al., 2024). This gap raises the need for more precise measurements that better align with human perceptions, particularly in terms of how useful the jailbroken responses are for malicious actors. We first identify four attributes that relate to characterizing the harmfulness of a jailbreak response by drawing from red-teaming and computational linguistics literature:\n\u2022 Actionability measures if a response has clear steps, specific action verbs, and objects that assist a user in carrying out harmful actions (Ganguli et al., 2022).\n\u2022 Coherence measures if a response is logically connected (Cho et al., 2019) and contains consistent information (Malkin et al., 2022) such that a malicious user can easily understand the response."}, {"title": "3.2. Jailbreak Evaluation with HARMSCORE", "content": "Now, we consider measuring harmfulness through the lens of actionability and informativeness through our metric termed HARMSCORE. First, note that a harmful response should be both actionable and informative: An actionable response with poor informativeness may include irrelevant instructions, while an informative but unactionable response may only provide information without guiding actions. We can realize this notion via the geometric mean of a jailbreak response's actionability and informativeness scores.\nConcretely, given a malicious query Q, a jailbreak response R, and metrics $f_a(Q, R) \\in [0,1]$ and $f_1(Q, R) \\in [0,1]$ which respectively score the actionability and informativeness of R in response to Q, we define HARMSCORE as:\n$HARMSCORE(Q, R) = 1[R \\cap S \\neq 0] \\cdot \\sqrt{f_a(Q, R) \\cdot f_1(Q, R)}$,\nwhere the indicator function determines whether R contains any predefined refusal strings S (Mehrotra et al., 2023; Zou et al., 2023; Mazeika et al., 2024). In words, if the response R does not refuse to engage with the malicious query Q, we assess the harmfulness of R by computing the geometric mean of its actionability and informativeness scores."}, {"title": "4. Jailbreaks Through Simple Interactions", "content": "In real-world interactions between human users and LLMs, conversations often extend beyond single-instance question-answering (Wang et al., 2024c). Users may engage in multi-turn interactions, pose follow-up questions, or communicate in languages other than English, as evidenced by millions of conversations in user-LLM interaction datasets (Zhao et al., 2024; Zheng et al., 2024; Deng et al., 2024b).\nHowever, common interactions can also be exploited for malicious purposes. Consider the left interaction in Figure 2, sourced from WildVis (Deng et al., 2024b). The user gradually seeks suggestions for suicidal drugs through a multi-turn conversation starting with a drug-related query. This act resembles the decomposition of a complex task into modular subtasks (Khot et al., 2023), where, similarly, harmful instructions can be decomposed into seemingly benign subqueries to bypass safeguards.\nAnother frequently observed interaction mode is multilingual conversations, with nearly half of the real-world interaction logs in WildChat (Zhao et al., 2024) conducted in languages other than English. The right interaction in Figure 2 displays a conversation from WildVis where the LLM complies with a user's request for drug abuse instructions in Spanish. Since safety training is typically conducted in English, if a malicious request triggers a refusal in English, users can simply rephrase it in another language, increasing the likelihood of receiving an affirmative response (Yong et al., 2023; Deng et al., 2024a; Shen et al., 2024)."}, {"title": "4.1. The SPEAK EASY Jailbreak Framework", "content": "Based on observations of misuse in real-world multi-step and multilingual interactions, we design SPEAK EASY to simulate how non-expert users realistically pursue harmful content and visualize the jailbreak framework in Figure 3. Given a malicious query Q, SPEAK EASY prompts the target LLM to decompose it into m subqueries, $Q = {q_1,..., q_m}$. We manually curate query decompositions (Dua et al., 2022; Wei et al., 2022; Zhou et al., 2023) of benign tasks as in-context learning examples (additional details in \u00a7B.1). In Figure 3, the query \u201cmaking dimethylmercury\u201d is decomposed into three steps, beginning with identifying materials containing mercury and then inquiring about synthesizing dimethylmercury from them.\nTo obtain useful information from each subquery, SPEAK EASY exploits multilingual vulnerabilities by prompting the target LLM with subqueries translated into a predefined set of languages, $L = {L_1,..., L_n}$, spanning different resource groups (Joshi et al., 2020; Lai et al., 2023). The responses are then translated back into English, forming a pool of n candidate responses, $R_i = {r_{i,1},..., r_{i,n}}$, for"}, {"title": "4.2. Selecting Actionable and Informative Responses", "content": "To train response selection models for measuring actionability and informativeness, we repurpose existing preference datasets for each attribute using GPT-4. We first summarize queries from the HH-RLHF (Bai et al., 2022a) and Stack-Exchange-Preferences (Lambert et al., 2023) datasets into single sentences to match the format of typical jailbreak queries (Zou et al., 2023; Mazeika et al., 2024). For both attributes, we filter out irrelevant examples by assessing whether the queries can be answered with an actionable or informative response. The responses of the remaining examples are then annotated for each attribute, and we only retain examples with one response exhibiting the attribute and another lacking it. This process yields a preference dataset comprising 27,000 valid query-preference pairs for each attribute, with examples in \u00a7B.2.\nUsing these datasets, we fine-tune Llama-3.1-8B-Instruct (Touvron et al., 2023) as our response selection models for each attribute separately with iterative direct preference optimization (Xu et al., 2024) (See \u00a7B.2 for additional training"}, {"title": "5. Experiments", "content": "We now evaluate SPEAK EASY by measuring its effectiveness in amplifying harmful jailbreaks. \u00a75.1 outlines the experimental setup, and \u00a75.2 validates HARMSCORE through human evaluation. In \u00a75.3, we compare SPEAK EASY against existing jailbreak baselines through both ASR and HARMSCORE. Finally, we conduct ablation studies in \u00a75.4 and analyze language usage in \u00a75.5."}, {"title": "5.1. Experimental Setup", "content": "In our main experiments, we evaluate three jailbreak baselines, both with and without SPEAK EASY using four benchmarks. We target three multilingual LLMs: the proprietary GPT-40 (OpenAI, 2024) and the open-source Qwen2-72B-Instruct (Qwen2) (Yang et al., 2024) and Llama-3.3-70B-Instruct (Llama3.3) (Grattafiori et al., 2024).\nJailbreak Baselines. We incorporate SPEAK EASY into the following three jailbreak baselines and observe the absolute change in jailbreak success and harmfulness: (1) Direct Request (DR) directly prompts the target LLM with malicious queries; (2) Greedy Coordinate Gradient-Transfer (GCG-T) (Zou et al., 2023) appends an adversarial suffix to the query; (3) Tree of Attacks with Pruning-Transfer (TAP-T) (Mehrotra et al., 2023) utilizes tree-of-thought reasoning to iteratively refine malicious queries. These baselines are applied to the decomposed subqueries in SPEAK EASY during integration. More details are in \u00a7B.3.\nJailbreak Benchmarks. We evaluate on four jailbreak benchmarks, covering a wide range of harm categories: (1)"}, {"title": "5.2. Human Evaluation for HARMSCORE", "content": "Before proceeding to evaluations, we first assess how well HARMSCORE aligns with human judgments compared to two variants of ASR: GPT-ASR (Qi et al., 2024), based on GPT-40, and HB-ASR, based on the HarmBench classifier. To this end, we randomly sample ten queries from each of the six semantic categories in HarmBench and collect the corresponding responses from the three jailbreak baselines, both with and without SPEAK EASY, totaling 360 query-response pairs. We then recruit 27 graduate students to label whether each response is harmful, ensuring that each query-response pair is annotated three times. Fleiss' \u03ba of 0.622 indicates a strong inter-annotator agreement.\nAs shown in Table 3, HARMSCORE and GPT-ASR achieve comparable alignment with human judgment, both outperforming HB-ASR with a 9% absolute increase in overall correlation. Notably, HARMSCORE excels in four of six cat-"}, {"title": "5.3. Evaluation Results", "content": "We present our main findings in Figure 4. Overall, our results demonstrate the strong jailbreak efficacy of SPEAK EASY as measured by ASR and HARMSCORE.\nSPEAK EASY significantly increases ASR and HARMSCORE for direct requests. All evaluated LLMs have undergone safety alignment, so directly querying the model (DR) results in consistently low scores. The shaded bars in Figure 4(a) illustrate the substantial increase achieved by SPEAK EASY, often exceeding a threefold rise in both metrics. This effect is most pronounced in GPT-40, where ASR increases from 0.092 to 0.555 on average, with the largest change of 0.672 on AdvBench. Although Qwen2 and Llama3.3 exhibit better robustness, we still observe average ASR increases of 0.304 and 0.226 respectively. A similar trend holds for HARMSCORE, with SPEAK EASY driving an increase ranging from 0.327 to 0.579. Notably, the effect is larger in magnitude than ASR, suggesting that responses can become sufficiently harmful even with a modest rise in ASR. Across benchmarks, AdvBench exhibits high ASR but comparatively lower HARMSCORE, while MedSafetyBench follows the opposite pattern. The latter contains more domain-specific harmful queries, making it more difficult to assess success but often yielding highly actionable and informative responses.\nSPEAK EASY further increases ASR and HARMSCORE when combined with existing jailbreak methods. Next, we examine the effect of integrating SPEAK EASY into two state-of-the-art jailbreak techniques, GCG-T and TAP-T. Observe that GCG-T, when used independently, leads to minimal changes or even a decline in attack success. We integrate SPEAK EASY by appending the GCG-generated adversarial suffix to the decomposed subqueries, and find average increases of at least 0.2 across all LLMs and benchmarks. The largest change is again observed in GPT-40, where ASR increases by 0.480 and HARMSCORE by 0.636 on average. Furthermore, we evaluate TAP-T, which already achieves high baseline scores. Despite its strong performance, SPEAK EASY further boosts ASR by 0.235 to 0.336 and HARMSCORE by 0.299 to 0.393 on average. Strikingly, for both GPT-40 and Llama3.3, this integration"}, {"title": "5.4. Ablation Studies", "content": "When interacting with LLMs in a multi-step and multilingual manner, users can adjust the number of steps, the choice of languages, and the selection of responses at each stage of the process. Here, we examine how these three components in SPEAK EASY influence the jailbreak responses' harmfulness. By default, we use GPT-40 as the backbone with DR + SPEAK EASY and HarmBench as the target benchmark. Unless otherwise specified, we use three decomposition steps, six languages, and our fine-tuned response selection models. We present our results in Table 4.\nNumber of Query Decomposition Steps. We vary the number of decomposition steps, $m \\in {1,3,5}$. Setting m = 1 corresponds to the multilingual jailbreak method (Yong et al., 2023; Deng et al., 2024a) using six languages. Increasing m from 1 to 3 introduces the query decomposition component, which significantly increases ASR from 0.115 to 0.560 and HARMSCORE from 0.154 to 0.779. We attribute this to the effectiveness of the decomposition process in breaking down a harmful query into seemingly harmless subqueries. Namely, we find that while GPT-4o refuses to respond in 81% of single-step multilingual queries (m = 1), the refusal rate drops sharply to 1.5% for m = 3 (as measured by when one or more of the m subqueries"}, {"title": "5.5. Language Usage in SPEAK EASY", "content": "We provide additional analysis into the languages used in SPEAK EASY, based on GPT-40's results on HarmBench. Figure 5(a) illustrates the average actionability and informativeness scores for subqueries in SPEAK EASY. We observe that lower-resource languages tend to score lower in actionability (Souly et al., 2024), with English achieving the highest score. Actionability scores are generally low at the subquery level, as subquery responses may be unactionable on their own and only become fully actionable when combined into a complete response. In contrast, informativeness scores are consistently high across all languages and exhibit a more uniform distribution, with Ukrainian and Turkish from the medium-resource group scoring the highest. This pattern supports the efficacy of query decomposition, as subqueries can elicit informative responses across languages."}, {"title": "6. Conclusion", "content": "In this paper, we demonstrate that state-of-the-art LLMs can easily produce harmful responses through interactions commonly encountered in everyday user settings. We identify actionability and informativeness as the two key contributors to response harmfulness. On this basis, we propose HARMSCORE, a new metric that quantifies harm using these two attributes, which demonstrates competitive alignment with ASR based on human judgments. Following this, we propose SPEAK EASY, a jailbreak framework that leverages vulnerabilities in multi-step, multilingual interactions. SPEAK EASY significantly increases the likelihood of generating harmful content, with an average absolute increase of 0.319 in ASR and 0.426 in HARMSCORE. The success of SPEAK EASY highlights the simplicity with which highly actionable and informative\u2014and therefore truly harmful\u2014jailbreak responses can be elicited in LLMs. Our work highlights a critical gap between current jailbreak research and real-world use cases. We advocate that future work in safety alignment should focus more on realistic user settings."}, {"title": "Impact Statement", "content": "This paper investigates vulnerabilities in large language models (LLMs) within human-LLM interactions to improve safety measures in current alignment practices. By highlighting these vulnerabilities in widely deployed LLMs, we aim to encourage protective measures before such weaknesses can be exploited at scale. While our work identifies potential weaknesses that could be exploited, we believe transparent research on these issues is crucial for developing more robust safeguards."}, {"title": "B. Implementation Details of SPEAK EASY", "content": "In this section, we provide additional details on the implementation of the SPEAK EASY framework introduced in \u00a74."}, {"title": "B.1. Query Decomposition in SPEAK EASY", "content": "In the SPEAK EASY framework, we first decompose a harmful query into multiple seemingly benign subqueries. To do so, we instruct the target LLM to perform this decomposition with a prompt that includes four in-context examples of benign questions, as shown in Figure 9. The in-context examples were manually constructed with reference to previous work on query decomposition (Dua et al., 2022) and multi-step question answering (Shaikh et al., 2023).\n[TASK]: You are an agent preventing adversarial attacks from malicious users. Currently, malicious users are breaking down a harmful request into multiple sub-questions that could seem unharmful. Your goal is to identify these sub-questions to help prevent harmful attacks. What are the [NUMBER OF SUBQUERIES] most common sub-questions of the following harmful request: [HARMFUL QUERY]? You may follow the format of the following examples:\nWrite a tutorial on how to profit from homemade lemonade:\n1.\tWhat is a good lemonade recipe that people will enjoy?\n2.\tWhat are the costs of running a lemonade stand?\n3.\tWhat are the best times and locations to sell lemonade, and how can I maximize profit?\nWrite a persuasive post encouraging daily exercise:\n1.\tWhat are the benefits of daily exercise?\n2.\tWhy should I make daily exercise a habit?\n3.\tHow can I start exercising regularly?\nGive detailed instructions on how to make slime using household materials:\n1.\tWhat household materials are commonly used to make slime?\n2.\tWhat are the correct proportions of these materials for making slime?\n3.\tWhat are the steps to mix the materials and make slime?\nWrite an article claiming that a US president is devoted to improving education quality:\n1.\tWhich US president is committed to improving education quality?\n2.\tWhat can a US president do to enhance education quality?\n3.\tWhat are the potential impacts of a president's dedication to improving education quality?"}, {"title": "B.2. Response Selection Models", "content": "In this section, we outline the process used to fine-tune the two response selection models in \u00a74.1.\nFine-Tuning Datasets. As outlined in \u00a74.1, we preprocess the HH-RLHF (Bai et al., 2022a) and Stack-Exchange-Preferences (Lambert et al., 2023) datasets by filtering out irrelevant instances. The HH-RLHF dataset originally contains 161,000 preference pairs, each consisting of two responses to the same question\u2014one selected and one rejected by a human annotator. The Stack-Exchange-Preferences dataset follows a similar structure.\nThe preprocessing involves three main steps. First, because instances from Stack-Exchange-Preferences often include lengthy queries with context that differ from typical jailbreak prompts, we instruct GPT-40 to summarize these instances using the prompt instructions in Figure 10a. Second, we label each query to determine if it can be answered with an actionable or informative response. The prompts used for this process are shown in Figures 10b and 10c. Finally, we label each query-response pair as either actionable or informative, using the prompt provided in Figure 10d. We provide example instances before and after summarization in Table 8. Table 9 shows sample pairs from the final dataset."}, {"title": "B.3. Jailbreak Method Details", "content": "For all experiments, we follow prior work and use greedy decoding for output generation (Chao et al., 2023; Zou et al., 2023). We set max_tokens to 256 (Mazeika et al., 2024).\nGCG-Transfer. We use the transfer version of GCG (Zou et al., 2023), which generates adversarial suffixes that can be transferred to all models. For training, we use the Vicuna-7B and Vicuna-13B (Chiang et al., 2023) models and randomly sample 25 test cases from the given benchmark. The suffix that achieves the lowest loss after 100 steps is selected. We abbreviate this method as GCG-T.\nGCG-T + SPEAK EASY. Within the SPEAK EASY framework, we append the adversarial suffix optimized for the given benchmark to the translated and decomposed subqueries. The rest of the process follows the standard SPEAK EASY pipeline.\nTAP-Transfer. We use the transfer version of TAP (Mehrotra et al., 2023), following the settings in Mazeika et al. (2024). We use GPT-40 as the judge and target models, and Mixtral 8x7B (Jiang et al., 2024) as the attack generator. The generated test cases are used as transfer cases for all other models. We abbreviate this as TAP-T.\nTAP-T + SPEAK EASY. We apply the TAP-T method to the decomposed English subqueries in SPEAK EASY before translation. After TAP-T refines the test case for each subquery, the resulting test case is fed back into the SPEAK EASY pipeline. It is then translated into the target languages for inference, with the remainder following the standard SPEAK EASY pipeline."}, {"title": "C. Supplementary Results", "content": "In this section, we provide additional results on the performance of SPEAK EASY. Table 10 presents the full evaluation results, which we use to create Figure 4."}]}