{"title": "VIZECGNET: VISUAL ECG IMAGE NETWORK FOR CARDIOVASCULAR DISEASES\nCLASSIFICATION WITH MULTI-MODAL TRAINING AND KNOWLEDGE DISTILLATION", "authors": ["Ju-Hyeon Nam", "Seo-Hyung Park", "Su Jung Kim", "Sang-Chul Lee"], "abstract": "An electrocardiogram (ECG) captures the heart's electrical\nsignal to assess various heart conditions. In practice, ECG\ndata is stored as either digitized signals or printed images.\nDespite the emergence of numerous deep learning models\nfor digitized signals, many hospitals prefer image storage\ndue to cost considerations. Recognizing the unavailability\nof raw ECG signals in many clinical settings, we propose\nVizECGNet, which uses only printed ECG graphics to de-\ntermine the prognosis of multiple cardiovascular diseases.\nDuring training, cross-modal attention modules (CMAM) are\nused to integrate information from two modalities - image and\nsignal, while self-modality attention modules (SMAM) cap-\nture inherent long-range dependencies in ECG data of each\nmodality. Additionally, we utilize knowledge distillation to\nimprove the similarity between two distinct predictions from\neach modality stream. This innovative multi-modal deep\nlearning architecture enables the utilization of only ECG im-\nages during inference. VizECGNet with image input achieves\nhigher performance in precision, recall, and F1-Score com-\npared to signal-based ECG classification models, with im-\nprovements of 3.50%, 8.21%, and 7.38%, respectively.", "sections": [{"title": "1. INTRODUCTION", "content": "Cardiovascular disease ranks as the second leading cause of\ndeath, following cancer. Physicians employ electrocardio-\ngrams (ECGs) as a diagnostic tool to monitor the heart's elec-\ntrical activity. During an ECG, electrodes are placed on the\nskin, typically on the chest, arms, and legs, to detect and\nrecord the heart's electrical impulses. Healthcare profession-\nals analyze these signals to diagnose various heart conditions,\nincluding arrhythmias, myocardial infarction (heart attack),\nand heart failure. Despite its utility, manual analysis of ECG\nsignals is complex and prone to human error, potentially lead-\ning to the oversight of subtle yet critical diagnostic patterns.\nWith advancements in deep learning and signal process-\ning, researchers have strived to automate cardiovascular dis-\nease diagnosis. For instance, [1] introduced a binary classi-\nfication model for arrhythmic fibrillation signals, utilizing a\n1D convolutional neural network (CNN) based on single-lead\nECG data. Subsequently, RhythmNet [2] and Stacked CNN-\nLSTM [3] addressed the periodic nature of normal signals by\nincorporating recurrent neural networks (RNN and LSTM) to\ndetect abnormalities based on long-term dependencies. Addi-\ntionally, [4] proposed a method that extracts multi-scale fea-\ntures to classify diseases from abnormal ECG signals.\nIn clinical practice, physicians generally prefer utilizing\nmulti-lead ECG signals, which are gathered from multiple\nelectrodes, over single-lead ECG signals for diagnostic pur-\nposes. Consequently, numerous researchs have focused on\ndeveloping diagnostic tools based on multi-lead signals. For\nexample, [5] employed residual learning and recurrent neu-\nral networks to classify seven abnormal signals from 12-lead\nECG data. Similarly, [6] classified nine abnormal signals\nby combining new features obtained by experts from 12-lead\nECG signals with features extracted from 1D CNN. Address-\ning the challenge posed by the concurrent manifestation of\nmultiple cardiovascular diseases in a 12-lead ECG signal, 1D\nRANet [7] tackles a multi-label classification task.\nWe observed that ECG signal classification models pri-\nmarily rely on digitized signals for identifying abnormalities.\nOur primary motivation arises from the unavailability of dig-\nitized ECG signals, particularly in smaller clinics, due to\nfactors such as the high maintenance costs of databases and\nthe use of legacy devices. With this motivation, we introduce\nVizECGNet, a model for classifying cardiovascular diseases\nusing printed ECG graphics (images). We utilized features\nextracted from both the image and the signal. Recognizing\nthat features from each modality exhibit distinct character-\nistics, we apply a cross-modal attention module (CMAM)\nto fuse these features. Additionally, a self-modal atten-\ntion module (SMAM) refines the extracted features across\nheterogeneous modalities, emphasizing discriminative fea-\ntures crucial for distinguishing normal and abnormal sig-\nnals. These modality-specific features are then integrated\nin fully-connected layers, with knowledge distillation ap-"}, {"title": "2. METHOD", "content": "VizECGNet is a composition of 1D and 2D CNN for multi-\nlabel classification of 12-lead ECG signals. Our model is\na novel structure that combines multi-modal learning and\nknowledge distillation techniques. For multi-modal learning,\nwe adopt a self-attention mechanism between different and\nthe same modality, called CMAM and SMAM. The features\nof each modality are forwarded through fully-connected lay-\ners for knowledge distillation. Only ECG images are used for\npredicting cardiovascular disease during the inference phase\nin 12-lead ECG signals. Fig. 1 illustrates the overall structure\nof VizECGNet."}, {"title": "2.1. Cross- and Self-Modal Attention Modules", "content": "The main goal of VizECGNet is to extract the correlations\nbetween different modalities and extract discriminative fea-\ntures via cross- and self-modal attention modules. To achieve\nthis goal, we extract features from the 12-lead ECG signals\n$\\mathbf{X}^{s} = {\\mathbf{x}_1, . . . , \\mathbf{x}_{12}}$ and image $\\mathbf{X}^{i}$ using CNN-based feature\nextractors $f$ and $g$, respectively. For each $l$-th single-lead sig-\nnal $\\mathbf{x}_{l} = [x_{l,1},...,x_{l,T}]$ with time length $T$, we use twelve\ndifferent 1D CNN-based feature extractor $f = {f_1,..., f_{12}}$\nwith sharing parameters to efficiently fuse each signal fea-\ntures as follows:\n$z_{a v g}=\\frac{1}{12} \\sum_{l=1}^{12} f_{l}\\left(\\mathbf{x}_{l}\\right) \\in \\mathbb{R}^{C^{s} \\times T}$.\nSimilar to ECG signal $\\mathbf{X}^{s}$, we extract the feature maps from\n12-lead ECG image $\\mathbf{X}^{i}$ using 2D CNN-based feature extrac-\ntor $g$ as follow:\n$\\mathbf{z}^{i}=g\\left(\\mathbf{X}^{i}\\right) \\in \\mathbb{R}^{C^{i} \\times H^{i} \\times W^{i}}$.\nwhere $C^s = C^i = 512$. Subsequently, we apply cross-\nmodal attention module between two extracted features $\\mathbf{z}_{avg}$\nand $\\mathbf{z}^{i}$. For clarify, let assume we use self-attention on modal-\nity $m$ based on modality $n$. Then, we can write CMAM as\nfollows:\n$\\mathbf{z}^{m \\leftarrow n}=\\operatorname{Softmax}\\left(\\mathbf{Q}^{n} \\mathbf{K}^{n T}\\right) \\mathbf{V}^{m}$\nwhere $\\mathbf{Q}^{n}=\\mathbf{W}_{Q^{n}} \\mathbf{z}^{n}, \\mathbf{K}^{n}=\\mathbf{W}_{K^{n}} \\mathbf{z}^{n}, \\mathbf{V}^{m}=\\mathbf{W}_{V^{m}} \\mathbf{z}^{m}$.\nThen, each refined features $\\widetilde{\\mathbf{z}}^{s \\leftarrow i}$ and $\\widetilde{\\mathbf{z}}^{i \\leftarrow s}$ contain different"}, {"title": "2.2. Knowledge Distillation", "content": "Knowledge distillation is a technique commonly used in ma-\nchine learning to transfer knowledge from a complex model\n(teacher model) to a simple model (student model). To utilize\nonly printed ECG signal to classify abnormal signals during\ninference and acquire knowledge about 12-lead ECG signals,\nwe adopt knowledge distillation from signal stream into im-\nage stream. First, predicting the probability distributions $p^{s}$\nand $p^{i}$ for each class is performed using a classifier for each\nmodality stream as follows:\n$p^{m}=\\mathrm{MLP}_{m}\\left(\\mathrm{GAP}\\left(\\widetilde{\\mathbf{z}}^{m}\\right)\\right)$\nwhere $\\mathrm{MLP}_{m}(\\cdot)$ is a multi-layer perceptron (MLP) for\neach signal and image modality stream. For each prediction,\nthe classification loss function $\\mathcal{L}_{c l s}$ for the same label $t$ is\ncomputed as follows:\n$\\mathcal{L}_{c l s}=\\sum_{c=1}^{C}\\left(\\mathcal{L}_{B C E}\\left(t_{c}, p_{c}^{s}\\right)+\\mathcal{L}_{B C E}\\left(t_{c}, p_{c}^{i}\\right)\\right)$\nwhere $C$ is a number of classes and $\\mathcal{L}_{B C E}$ is the binary\ncross-entropy loss function. Since the dataset used in this\npaper can have multiple diseases for a single ECG signal,\na binary classification is performed for each class. Finally,\nwe calculate knowledge distillation loss $\\mathcal{L}_{K D}$ between two\nmodality streams to reduce the difference in probability dis-\ntribution $p^{s}$ and $p^{i}$ as follows:\n$\\begin{aligned}\\mathcal{L}_{K D}\\left(p^{s}, p^{i}\\right) &=\\sum_{c=1}^{C} \\mathcal{L}_{K L}\\left(p_{c}^{s} \\| p_{c}^{i}\\right) \\\\&=\\sum_{c=1}^{C} \\sum_{x \\in \\mathcal{X}} p_{c}^{s}(x) \\log \\left(\\frac{p_{c}^{s}(x)}{p_{c}^{i}(x)}\\right)\\end{aligned}$\nwhere $\\mathcal{L}_{K L}$ is a Kullback-Leibler Divergence to calcu-\nlate the difference between two probability distributions $p_{c}^{s}$\nand $p_{c}^{i}$ for each class $c$. The final loss function $\\mathcal{L}_{t o t a l}=$\n$\\lambda_{1} \\mathcal{L}_{c l s}+\\lambda_{2} \\mathcal{L}_{K D}$ is used for updating parameters of each\nmodality stream. To reduce the sensitivity of hyperparame-\nters, we fix $\\lambda_{1}=\\lambda_{2}=1$."}, {"title": "3. EXPERIMENTAL RESULTS", "content": ""}, {"title": "3.1. Experimental Settings and Implementation Details", "content": "We implemented VizECGNet in Pytorch 1.11 and Python\n3.8. The large-scale 12-lead ECG signal dataset [15] used\nin this paper is multi-label (1dAVb, RBBB, LBBB, SB, AF,\nST). The ECG signal of each lead is composed of 4096\ntime lengths. In this paper, to eliminate the trend of each\nsignal, the average voltage for the entire time is set to zero\nmean, and we apply a detrending process. Finally, we convert\nthe 12-lead ECG signals using the ecg_plot Python library\nfor multi-modal learning on the images. We compared our\nVizECGNet with five signal-based models (InceptionTime\n[8], XResNet1D-101 [9], Transformer [10], ACNet [11], and\n1D RANet [7]), two image-based models (ResNet18 [12] and\nMobileNetV3 [13]), and multi-modal models (MHM [14]).\nSince the use of default training settings generally performs\npoorly in ECG dataSet, for fair comparison, we optimized\nthe parameters to work best on ECG dataset. We train all\nmodels in an end-to-end manner using the Adam optimizer.\nThe initial learning rate starts from $10^{-3}$ and is decreased to\n$10^{-6}$ using the cosine annealing learning rate scheduler [16],\nand the training settings were set to a batch size of 16 and\nepochs of 300 till the loss functions of all models converged.\nFor evaluation, we used three metrics (Precision, Recall, and\nmacro-averaged F1-Score) to measure the performance of\neach model. To efficiently extract features from signals and\nimage, we utilize ResNet18 as feature extractor."}, {"title": "3.2. Results Analysis", "content": "As shown in Table 1, VizECGNet outperforms single- and\nmulti-modality models on all performance evaluation metrics\nin both inference data types. When predicting using ECG\nsignals, VizECGNet achieve 9.58%, and 19.68% higher F1-\nScore compared with 1D RANet, and MHM, respectively.\nFurthermore, when using printed ECG images, VizECGNet\nachieve 17.21%, and 24.06% higher F1-Score compared with\nResNet18, and MHM, respectively. Note that MobileNetV3\nand ResNet18 used only simple ECG images for training and\nevaluation. This training strategy makes two image-based\nmodels unable to understand the abnormal signal characteris-\ntics of each lead. However, although 1D RANet receives 12-\nlead ECG signal data with complex data structures, its perfor-\nmance is low. These reasons indicate that the interpretability\nof 1D RANet for complex data is still poor. On the other hand,\nVizECGNet achieves high classification performance because\nit exchanges information based on multi-modal learning and\ndistills the knowledge of abnormal signals generated by sub-\ntle differences in 12-lead ECG signals into an image modality\nstream during learning.\nWe also examined extrapolation to actual ECG printed\nimages to confirm the utility of the model (Fig. 2). In\naddition, the inference results of two image-based models"}, {"title": "3.3. Ablation Study", "content": "In this section, we analyze the effectiveness of two attention\nmodules (CMAM and SMAM). In Table 2, the results of the\nablation study on attention modules are listed with four con-\nfigurations. Basically, attention is used to extract discrimi-\nnative features from messy features. In this paper, important\ninformation can be additionally extracted by focusing on each\nmodality or between modalities. It can be seen from the ac-\ntual experimental results that when the mode is paid attention\nto, all performance evaluation indicators have achieved high\nperformance."}, {"title": "4. CONCLUSION", "content": "We propose VizECGNet, which applies multi-modal learning-\nbased knowledge distillation techniques to classify abnormal\nelectrical signals in ECG signals. Experimental results on\na large-scale ECG dataset demonstrate that VizECGNet\nperforms better than traditional heart disease classification\nmodels. In the multi-modal case, cross- and self-modality\nattention modules (CMAM and SMAM) enable us to focus\non discriminative features between different modalities and\napply knowledge distillation techniques to prevent the per-\nformance drop when only images are used during inference.\nThese results prove that the model can be fully exploited in\ndeveloping countries, which only have access to ECG printers\nwithout undergoing the refinement process. To further verify\nthe generalization ability of VizECGNet, we plan to train\nand evaluate it on various 12-lead datasets, make it into an\napplication, and test it in a real clinical setting."}]}