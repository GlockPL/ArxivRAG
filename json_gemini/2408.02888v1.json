{"title": "VIZECGNET: VISUAL ECG IMAGE NETWORK FOR CARDIOVASCULAR DISEASES CLASSIFICATION WITH MULTI-MODAL TRAINING AND KNOWLEDGE DISTILLATION", "authors": ["Ju-Hyeon Nam", "Seo-Hyung Park", "Su Jung Kim", "Sang-Chul Lee"], "abstract": "An electrocardiogram (ECG) captures the heart's electrical signal to assess various heart conditions. In practice, ECG data is stored as either digitized signals or printed images. Despite the emergence of numerous deep learning models for digitized signals, many hospitals prefer image storage due to cost considerations. Recognizing the unavailability of raw ECG signals in many clinical settings, we propose VizECGNet, which uses only printed ECG graphics to determine the prognosis of multiple cardiovascular diseases. During training, cross-modal attention modules (CMAM) are used to integrate information from two modalities - image and signal, while self-modality attention modules (SMAM) capture inherent long-range dependencies in ECG data of each modality. Additionally, we utilize knowledge distillation to improve the similarity between two distinct predictions from each modality stream. This innovative multi-modal deep learning architecture enables the utilization of only ECG images during inference. VizECGNet with image input achieves higher performance in precision, recall, and F1-Score compared to signal-based ECG classification models, with improvements of 3.50%, 8.21%, and 7.38%, respectively.", "sections": [{"title": "1. INTRODUCTION", "content": "Cardiovascular disease ranks as the second leading cause of death, following cancer. Physicians employ electrocardiograms (ECGs) as a diagnostic tool to monitor the heart's electrical activity. During an ECG, electrodes are placed on the skin, typically on the chest, arms, and legs, to detect and record the heart's electrical impulses. Healthcare professionals analyze these signals to diagnose various heart conditions, including arrhythmias, myocardial infarction (heart attack), and heart failure. Despite its utility, manual analysis of ECG signals is complex and prone to human error, potentially leading to the oversight of subtle yet critical diagnostic patterns.\nWith advancements in deep learning and signal processing, researchers have strived to automate cardiovascular disease diagnosis. For instance, [1] introduced a binary classification model for arrhythmic fibrillation signals, utilizing a 1D convolutional neural network (CNN) based on single-lead ECG data. Subsequently, RhythmNet [2] and Stacked CNN-LSTM [3] addressed the periodic nature of normal signals by incorporating recurrent neural networks (RNN and LSTM) to detect abnormalities based on long-term dependencies. Additionally, [4] proposed a method that extracts multi-scale features to classify diseases from abnormal ECG signals.\nIn clinical practice, physicians generally prefer utilizing multi-lead ECG signals, which are gathered from multiple electrodes, over single-lead ECG signals for diagnostic purposes. Consequently, numerous researchs have focused on developing diagnostic tools based on multi-lead signals. For example, [5] employed residual learning and recurrent neural networks to classify seven abnormal signals from 12-lead ECG data. Similarly, [6] classified nine abnormal signals by combining new features obtained by experts from 12-lead ECG signals with features extracted from 1D CNN. Addressing the challenge posed by the concurrent manifestation of multiple cardiovascular diseases in a 12-lead ECG signal, 1D RANet [7] tackles a multi-label classification task.\nWe observed that ECG signal classification models primarily rely on digitized signals for identifying abnormalities. Our primary motivation arises from the unavailability of digitized ECG signals, particularly in smaller clinics, due to factors such as the high maintenance costs of databases and the use of legacy devices. With this motivation, we introduce VizECGNet, a model for classifying cardiovascular diseases using printed ECG graphics (images). We utilized features extracted from both the image and the signal. Recognizing that features from each modality exhibit distinct characteristics, we apply a cross-modal attention module (CMAM) to fuse these features. Additionally, a self-modal attention module (SMAM) refines the extracted features across heterogeneous modalities, emphasizing discriminative features crucial for distinguishing normal and abnormal signals. These modality-specific features are then integrated in fully-connected layers, with knowledge distillation ap-"}, {"title": "2. METHOD", "content": "VizECGNet is a composition of 1D and 2D CNN for multi-label classification of 12-lead ECG signals. Our model is a novel structure that combines multi-modal learning and knowledge distillation techniques. For multi-modal learning, we adopt a self-attention mechanism between different and the same modality, called CMAM and SMAM. The features of each modality are forwarded through fully-connected layers for knowledge distillation. Only ECG images are used for predicting cardiovascular disease during the inference phase in 12-lead ECG signals. Fig. 1 illustrates the overall structure of VizECGNet."}, {"title": "2.1. Cross- and Self-Modal Attention Modules", "content": "The main goal of VizECGNet is to extract the correlations between different modalities and extract discriminative features via cross- and self-modal attention modules. To achieve this goal, we extract features from the 12-lead ECG signals $X = \\{x_1, . . ., x_{12}\\}$ and image $X^i$ using CNN-based feature extractors f and g, respectively. For each l-th single-lead signal $x_l = [x_{l,1},...,x_{l,T}]$ with time length T, we use twelve different 1D CNN-based feature extractor $f = \\{f_1,..., f_{12}\\}$ with sharing parameters to efficiently fuse each signal features as follows:\n$z_{avg} = \\frac{1}{12} \\sum_{l=1}^{12} f_l(x_l) \\in \\mathbb{R}^{C^s \\times T}$.\nSimilar to ECG signal $X^s$, we extract the feature maps from 12-lead ECG image $X^i$ using 2D CNN-based feature extractor g as follow:\n$z^i = g(X^i) \\in \\mathbb{R}^{C^i \\times H^i \\times W^i}$.\nwhere $C^s = C^i = 512$. Subsequently, we apply cross-modal attention module between two extracted features $z_{avg}$ and $z^i$. For clarify, let assume we use self-attention on modality m based on modality n. Then, we can write CMAM as follows:\n$z^{m\\leftarrow n} = \\text{Softmax} \\left( Q_n K_n^T \\right) V_m$\nwhere $Q_n = W_{Q_n} z^n, K_n = W_{K_n} z^n, V_m = W_{V_m} z^m$. Then, each refined features $z^{s\\leftarrow i}$ and $z^{i\\leftarrow s}$ contain different"}, {"title": "2.2. Knowledge Distillation", "content": "Knowledge distillation is a technique commonly used in machine learning to transfer knowledge from a complex model (teacher model) to a simple model (student model). To utilize only printed ECG signal to classify abnormal signals during inference and acquire knowledge about 12-lead ECG signals, we adopt knowledge distillation from signal stream into image stream. First, predicting the probability distributions $p^s$ and $p^i$ for each class is performed using a classifier for each modality stream as follows:\n$p^m = MLP_m \\left( GAP \\left( z^m \\right) \\right)$\nwhere $MLP_m(\\cdot)$ is a multi-layer perceptron (MLP) for each signal and image modality stream. For each prediction, the classification loss function $\\mathcal{L}_{cls}$ for the same label t is computed as follows:\n$\\mathcal{L}_{cls} = \\sum_{c=1}^{C} \\left( L_{BCE} (t_c, p_c^s) + L_{BCE} (t_c, p_c^i) \\right)$\nwhere C is a number of classes and $L_{BCE}$ is the binary cross-entropy loss function. Since the dataset used in this paper can have multiple diseases for a single ECG signal, a binary classification is performed for each class. Finally, we calculate knowledge distillation loss $\\mathcal{L}_{KD}$ between two modality streams to reduce the difference in probability distribution $p^s$ and $p^i$ as follows:\n$\\mathcal{L}_{KD}(p^s, p^i) = \\sum_{c=1}^{C} L_{KL} \\left( p_c^s || p_c^i \\right)$\n$= \\sum_{c=1}^{C} \\sum_{x \\in X} \\left( p_c^s(x) \\log \\left( \\frac{p_c^s(x)}{p_c^i(x)} \\right) \\right)$\nwhere $L_{KL}$ is a Kullback-Leibler Divergence to calculate the difference between two probability distributions $p_c^s$ and $p_c^i$ for each class c. The final loss function $\\mathcal{L}_{total} = \\lambda_1 \\mathcal{L}_{cls} + \\lambda_2 \\mathcal{L}_{KD}$ is used for updating parameters of each modality stream. To reduce the sensitivity of hyperparameters, we fix $\\lambda_1 = \\lambda_2 = 1$."}, {"title": "3. EXPERIMENTAL RESULTS", "content": null}, {"title": "3.1. Experimental Settings and Implementation Details", "content": "We implemented VizECGNet in Pytorch 1.11 and Python 3.8. The large-scale 12-lead ECG signal dataset [15] used in this paper is multi-label (1dAVb, RBBB, LBBB, SB, AF, ST). The ECG signal of each lead is composed of 4096 time lengths. In this paper, to eliminate the trend of each signal, the average voltage for the entire time is set to zero mean, and we apply a detrending process. Finally, we convert the 12-lead ECG signals using the ecg_plot Python library for multi-modal learning on the images. We compared our VizECGNet with five signal-based models (InceptionTime [8], XResNet1D-101 [9], Transformer [10], ACNet [11], and 1D RANet [7]), two image-based models (ResNet18 [12] and MobileNetV3 [13]), and multi-modal models (MHM [14]). Since the use of default training settings generally performs poorly in ECG dataSet, for fair comparison, we optimized the parameters to work best on ECG dataset. We train all models in an end-to-end manner using the Adam optimizer. The initial learning rate starts from $10^{-3}$ and is decreased to $10^{-6}$ using the cosine annealing learning rate scheduler [16], and the training settings were set to a batch size of 16 and epochs of 300 till the loss functions of all models converged. For evaluation, we used three metrics (Precision, Recall, and macro-averaged F1-Score) to measure the performance of each model. To efficiently extract features from signals and image, we utilize ResNet18 as feature extractor."}, {"title": "3.2. Results Analysis", "content": "As shown in Table 1, VizECGNet outperforms single- and multi-modality models on all performance evaluation metrics in both inference data types. When predicting using ECG signals, VizECGNet achieve 9.58%, and 19.68% higher F1-Score compared with 1D RANet, and MHM, respectively. Furthermore, when using printed ECG images, VizECGNet achieve 17.21%, and 24.06% higher F1-Score compared with ResNet18, and MHM, respectively. Note that MobileNetV3 and ResNet18 used only simple ECG images for training and evaluation. This training strategy makes two image-based models unable to understand the abnormal signal characteristics of each lead. However, although 1D RANet receives 12-lead ECG signal data with complex data structures, its performance is low. These reasons indicate that the interpretability of 1D RANet for complex data is still poor. On the other hand, VizECGNet achieves high classification performance because it exchanges information based on multi-modal learning and distills the knowledge of abnormal signals generated by subtle differences in 12-lead ECG signals into an image modality stream during learning.\nWe also examined extrapolation to actual ECG printed images to confirm the utility of the model (Fig. 2). In addition, the inference results of two image-based models"}, {"title": "3.3. Ablation Study", "content": "In this section, we analyze the effectiveness of two attention modules (CMAM and SMAM). In Table 2, the results of the ablation study on attention modules are listed with four configurations. Basically, attention is used to extract discriminative features from messy features. In this paper, important information can be additionally extracted by focusing on each modality or between modalities. It can be seen from the actual experimental results that when the mode is paid attention to, all performance evaluation indicators have achieved high performance."}, {"title": "4. CONCLUSION", "content": "We propose VizECGNet, which applies multi-modal learning-based knowledge distillation techniques to classify abnormal electrical signals in ECG signals. Experimental results on a large-scale ECG dataset demonstrate that VizECGNet performs better than traditional heart disease classification models. In the multi-modal case, cross- and self-modality attention modules (CMAM and SMAM) enable us to focus on discriminative features between different modalities and apply knowledge distillation techniques to prevent the performance drop when only images are used during inference. These results prove that the model can be fully exploited in developing countries, which only have access to ECG printers without undergoing the refinement process. To further verify the generalization ability of VizECGNet, we plan to train and evaluate it on various 12-lead datasets, make it into an application, and test it in a real clinical setting."}]}