{"title": "Leveraging Knowledge Graph Embedding for Effective Conversational Recommendation", "authors": ["Yunwen Xia", "Hui Fang", "Jie Zhang", "Chong Long"], "abstract": "Conversational recommender system (CRS), which combines the techniques of dialogue system and recommender system, has obtained increasing interest recently. In contrast to traditional recommender system, it learns the user preference better through interactions (i.e. conversations), and then further boosts the recommendation performance. However, existing studies on CRS ignore to address the relationship among attributes, users, and items effectively, which might lead to inappropriate questions and inaccurate recommendations. In this view, we propose a knowledge graph based conversational recommender system (referred as KG-CRS). Specifically, we first integrate the user-item graph and item-attribute graph into a dynamic graph, i.e., dynamically changing during the dialogue process by removing negative items or attributes. We then learn informative embedding of users, items, and attributes by also considering propagation through neighbors on the graph. Extensive experiments on three real datasets validate the superiority of our method over the state-of-the-art approaches in terms of both the recommendation and conversation tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems have been widely applied in our daily life, including e-commerce, music and movies, restaurants, and online news, etc. Traditional recommender systems infer user's preference based on user's interaction history or the history of similar users, which generally suffers from the cold-start problem and faces the challenge of dynamic user preference. In this case, [5] proposed conversational recommender system (CRS), which integrates the techniques of dialogue system and recommender system, and has attracted numerous research interest in recent years. That is, compared to traditional recommender systems, CRS learns user's preference through several turns of communication (i.e., conversations), and then makes effective recommendations. Such a framework allows the platform (or recommendation agent) to make recommendations relatively easily for users without interaction history or users who dynamically change their preferences. On the other hand, compared with task-oriented dialogue system [6] which helps users search for ideal items in a domain-related database/system, CRS can better understand users' needs based on interaction history and the history of similar users.\nOverall, previous work on CRS can mainly be classified into two categories. One category [4, 15, 16, 18, 34] is about Natural Language Processing (NLP) in CRS, which attempts to understand users' utterances and merge each recommended item into a textual sentence generated by a CRS system. Thus, it allows users to express their questions more freely and strives to make the system's responses more like human languages. Conversely, the other category [5, 13, 14, 20, 27, 32, 35] focuses on \"strategy\" in CRS. Here, \"strategy\" refers to what action to conduct in each turn (conversation), such as \"whether to recommend\", \"which items to recommend\", or \"which attributes to ask\". Most studies in this category choose to simplify (e.g., using template-based sentences) or just skip the NLP module. Our study falls into this category and considers that it is feasible to add the NLP module derived from the first category of studies into our CRS system if needed.\nThe dialogue \"strategy\u201d has not been well discussed among the first category of studies, but it is of great importance for CRS. An effective \"strategy\" is expected to let CRS perform well in both recommendation task and conversation task, i.e., recommend satisfactory items to users while avoid asking useless questions about attributes to infer users' preferences (i.e., maintain fewer rounds of conversations).\nAs it is difficult to define the best \"strategy\u201d in CRS, reinforcement learning (RL) is adopted in the framework [13, 14, 20]. One of the key issues in these methods is how to represent users, items, and attributes, as these representations are the basis of both recommendation and \"strategy\", and will directly affect the results of recommendation and conversation tasks. Take user representation as an example, a user is profiled by his/her interaction history with items, thus learning the corresponding user representation should also be entangled with the interaction history. Previous work [13, 14, 20] mainly uses user representation to predict users' possible future interactions and argued that the learned user representation might implicitly capture the historical interactions. However, such kind of implicit modeling might lead to difficulty in well capturing the relationships among users, items and attributes, and the learning process would be further deteriorated by the data imbalance issue. That is, the interaction intensities among different users, items, and attributes are quite dissimilar. Therefore, we argue that it is worthwhile to explicitly encode the interaction history to provide better guidance and explanations for representation learning.\nIn this view, we propose a Knowledge Graph based Conversational Recommender System (KG-CRS), which adopts knowledge graph embedding techniques to explicitly encode the connections among users, items and attributes for more accurate representations, effective recommendations, and efficient conversations. In particular, we combine the user-item graph (identified from users' historical interactions) and item-attribute graph together, then learn user representations, item representations and attribute representations by also considering the propagation according to the graph. This provides guidance and explanations for the learned representations and greatly facilitates the representation learning of users, items and attributes, which are the basis of the recommendation and conversation tasks.\nBesides, to further enhance the conversation process, we dynamically update the whole graph by removing negative items or attributes identified by the previous conversations. Extensive experiments on three real-world datasets validate the superiority of our method over the state-of-the-art approaches in terms of both the recommendation and conversation tasks.\nThe contribution of our work is threefold:"}, {"title": "2 RELATED WORK", "content": "In this section, we mainly discuss two folds of related work: (1) conversational recommender system; and (2) knowledge graph (KG)-based recommender system."}, {"title": "2.1 Conversational Recommender System", "content": "As introduced before, the existing studies on CRS can be concluded into two categories, since the work of [5].\nThe first line focuses on the understanding and generation of conversations from the perspectives of NLP and considers the semantic information in CRS [4]. Specifically, it attempts to understand users' utterances and merge recommended items into textual sentences generated by CRS. Thus, users are allowed to express their questions more freely, meanwhile the system's responses are more in a human language form. For example, Li et al. [15] and Radlinski et al. [18] collected datasets consisting of real-world conversations to facilitate the NLP research on CRS. The work of [16] considered the item-attribute graph for multiple recommendation tasks (e.g., both hotel and restaurant recommendations), while Zhou et al. [34] semantically fused both item-oriented graph and word-oriented graph for CRS. Yu et al. [28] further combined NLP with visual information.\nThere are two main drawbacks with existing studies of this category: (1) the training process relies heavily on the manually collected natural language data, which might hinder its usability to a new domain where new data should be collected to retrain the corresponding methods from scratch. Besides, it ignores to well explore the \"strategy\" in CRS; and (2) user's interaction history is not well utilized in the previous studies in this category. As a result, a CRS system may generate similar conversations for different users, which is thus not personalized.\nAnother line of research emphasizes the effective \"strategy\" in CRS, which includes four main sub-tasks: \"when to recommend\", \"which items to recommend\", \"when to ask\", and \"which attributes to ask\". For example, Zou et al. [35] improved the solution on \"which items to recommend\" via a novel matrix factorization model, and the task \"which attributes to ask\" was addressed through Generalized Binary Search (GBS). For the other two sub-tasks, it chooses to keep asking until the size of candidate items is smaller than the size of recommendation list or the maximum number of questions is reached. Such \"strategy\" is similar to Max Entropy in [20] and the method in [2, 36-38], the main difference is the recommendation module. Bi et al. [2] utilized the negative feedback during the conversation to improve the"}, {"title": "2.2 KG-based Recommender System", "content": "Traditional recommender system has been widely studied. These studies might be helpful for building an effective CRS, where the recommendation task is of great importance. We only explore the KG-based ones as they are most relevant to our study. Those studies that utilize KG to improve the performance of CRS [14, 16, 27, 34] have been introduced in Section 2.1, thus, their recommendation module will not be repeatedly discussed here. We merely review the KG-related traditional recommender systems here.\nSome studies [10, 24, 30] directly utilize mature Knowledge Graph Embedding (KGE) methods to integrate side information for recommendation. For example, Zhang et al. [30] combined the structural data (graph), textual data and visual data to improve the item representations, where the structural data is explored using TransR [17]. Wang et"}, {"title": "3 PROPOSED MODEL", "content": "In this section, we present our KG-CRS model in great detail by first simply formulating the multi-round recommendation scenario."}, {"title": "3.1 Problem Formulation", "content": "The same as [13], we focus on the multi-round recommendation scenario where a CRS interacts with a user multiple times (together arranged as a session) by either asking attributes or recommending items until the user is satisfied with the recommendation or chooses to leave the session without a satisfactory recommendation. It should be noted that, similar to [13, 14, 20, 32], the conversation process adopts \"System ask, User Respond\" pattern. That is to say, a conversation session is mostly driven by the system, except for the beginning and the end of the session. More detailed process is shown in Figure 1, and the main notations are summarized in Table 1.\nSpecifically, at the beginning of each session, user u firstly specifies a favorable attribute p (from the set of attributes P describing items on the platform, $p \\in P$), and the CRS shortlists the candidate items containing attribute $V_p \\subset V$. Next, in each round (turn), the CRS conducts an action $a \\in A$: asking attribute or making recommendation. That is, for the action of making recommendation, if the user accepts the recommendation, the session will be terminated at the corresponding round. Otherwise, the recommended items are considered as negative items, and we move to the next round. On the other hand, for the action of asking attribute, if the user considers the asked attribute in this round is irrelevant to her preference, the attribute will be marked as negative attribute and we also move to the next round, otherwise the attribute is considered as positive attribute. The action space |A| is equal to Ng + 1, where Ng represents"}, {"title": "3.2 The KG-CRS Framework", "content": "The framework of our KG-CRS is demonstrated in Figure 2. Specifically, it consists of a dynamic knowledge graph and two modules, a conversation module and a recommendation module. The dynamic graph consists of three types of entities (namely user, item, and attribute) and three types of corresponding relations (denoted as user-item, item-attribute, and user-attribute). Its content will change (i.e. removing negative items and negative attributes) during a conversation session and will be reset when a new session starts. The conversation module encodes the current state and decides an action (asking attribute or making recommendation) according to the current state, while the recommendation module will give recommendations from the candidate items when the conversation module decides to make recommendation. Both conversation and recommendation modules rely on the informative embedding learned from the dynamic graph, i.e., to encode the current state in conversation module, and to calculate the recommendation score of each candidate item. In this framework, a conversation session is both started and ended by a user, and the user will give responses when the CRS agent asks questions. Noted that, similar to [13, 14], we skip the Natural Language Understanding (NLU) and Natural Language Generation (NLG) in our model."}, {"title": "3.2.1 Graph Embedding Learning", "content": "As shown in Figure 3, there are three types of entities (namely user, item, and attribute) and three types of corresponding relations (denoted as user-item, item-attribute and user-attribute) in the graph $G = \\{(h, r, t) | h, t \\in E, r \\in R\\}$, where $h, t$ represent entities (head and tail respectively), and $r$ represents relations. Every (entity, relation, entity) triple means that there is a relationship between entity $h$ (e.g., user) and entity $t$ (e.g., item), for example, user 'Neil' was interested in item 'Baby Cakes'. Inspired by [8, 25, 26], the learning process of embedding is divided into two stages, node feature and information propagation. The design of dynamic graph is to prevent negative items and negative attributes from affecting other related entities' representation during the information propagation process.\nNode Feature. This stage is to learn the vector representation of each entity and relation in the graph. We use a translation-based method, TransD [7, 11], as it performs better on describing the structure of knowledge graph. TransD assumes that $e_h' + e_r \\approx e_t'$ if triple $(h, r, t)$ exists in the graph, where $e_h', e_t'$ are the embedding of entity $h, t$, and $e_r$ is the embedding of relation $r$. Such assumption is achieved by optimizing the score function:\n$f(h, r,t) = ||e_h' + e_r \u2013 e_t'||^2_2$   (1)\nwhere $e_h' = M_{rh}e_h$ and $e_t' = M_{rt}e_t$. $M_{rh}$ and $M_{rt}$, as defined in Equation 2, are the projection matrices, which assure that each type of entity have different representations for different relations. $e_h, e_t$ are the embedding of entity $h, t$ without projection, respectively. The score $f(h, r, t)$ is low if there is a triple $(h, r, t)$, and high otherwise.\n$M_{rh} = m_r m_h^T + I; M_{rt} = m_r m_t^T + I $  (2)\nwhere $m_h, m_t$ and $m_r$ represent the projection vectors to determine corresponding projection matrices, and $I$ is the identity matrix to initialize the projection matrices."}, {"title": "3.2.2 Recommendation Module", "content": "The loss function of this stage is defined as:\n$L_{graph} = \\sum \\text{max}(f(h, r, t) \u2013 f(h, r, t'), -\\lambda)$ (3)\nwhere $\\lambda$ is the margin parameter. Triple $(h, r, t)$ is the positive triple that exists in the graph, while triple $(h, r, t')$ is a negative triple generated by randomly sampling an irrelevant entity $t'$ for $h,r$.\nInformation Propagation. This stage is to enrich the entity embedding by recursively updating based on each entity's neighbourhood. As illustrated in Figure 3, user 'Jamie' may be interested in item 'Hot Yoga Wellness' because both item 'Arizona Biltmore Golf Club' and item 'Hot Yoga Wellness' have attribute 'Active Life'. Therefore, it is necessary to propagate the information about item 'Arizona Biltmore Golf Club' to the user 'Jamie' while learning her embedding. The neighbourhood $N_h$ of entity $h$ is defined as:\n$N_h = \\sum a_{h,r,t} e_t$  (4)\n$(h,r,t) \\in G$\nwhere $e_t$ is the embedding of tail entity $t$ in the triple $(h, r, t)$ without projection. $a_{h,r,t}$ is the attention weight, which is computed according to the distance between $h$ and $t$:\n$a_{h,r,t} = e_t'^T tanh(e_r' + e_r)$ (5)\nWe recursively update the embedding of entity $h$ as:\n$e_h^{(k)} = g(e_h^{(k-1)}, N_h^{(k-1)}) = ReLU(W^k (e_h^{(k-1)} + N_h^{(k-1)}) + b^k)$ (6)\nwhere $W^k$ is the parameter matrix, $b^k$ is a bias and $k = 1, 2, ..., K$. $e_h^{(k-1)}$ and $N_h^{(k-1)}$ represent the embedding of entity $h$ and the information of neighbourhood $N_h$ at step $k \u2212 1$, respectively. $e_h^{(1)} = e_h, N_h^{(1)} = N_h$.\nThe final embedding of entity $h$ (Equation 7) is the concatenation of $e_h^{(k)}$ at each step. In the following subsections, we will use u, v and p to represent the final embedding of the three different types of entities (user, item and attribute) in the graph. And, the calculation of the three representations all refers to Equation 7.\n$h = [e_h^{(1)}, e_h^{(2)}, ..., e_h^{(K)}]$  (7)"}, {"title": "3.2.2 Recommendation Module", "content": "The main objective of this module is to obtain recommendation scores of candidate items ($V_{cand}$) based on the graph embedding, and simultaneously measure the loss function for graph embedding"}, {"title": "Learning", "content": "during the training process. Specifically, it generates a score of each candidate item, sorts the items in descending order in terms of the scores, and finally recommends the top K items to the target user. It should be noted that negative items and negative attributes will not be utilized explicitly when calculating the score. The removal of these negative entities will only change the embedding obtained in the information propagation stage. The score of each item $v \\in V_{cand}$ is calculated as Equation 8:\n$Y_0 = u_0 v_0 + \\frac{1}{|P_u|} \\sum v_1 p_1$ (8)\nwhere $u_0 = M_{0u}u, v_0 = M_{0v}v, v_1 = M_{1v}v$, and $p_1 = M_{1p}p$. Specifically, (1) u, v and p are the embedding of user u, item v and positive attribute p, respectively; (2) $M_{0u}, M_{0v}, M_{1v}$ and $M_{1p}$ are the projection matrices in Equation 2 to assure item v to be capable of having different representations for different relations, namely, user-item (i.e., 0 in the subscript, and $10 in Algorithm 1) and item-attribute (i.e., 1 in the subscript, and #1 in Algorithm 1\u00b9), respectively.\nThe loss function in this module is defined as:\n$L_{item} = \\sum log (y_0) \u2013 log (y\u2019)$   (9)\nwhere $y_0$ is the recommendation score of positive item $v$ that user $u$ has interacted with in the history, and $y\u2019$ is the recommendation score of negative item $v\u2019$.\nThe training process of graph embedding learning and recommendation module is detailed in Algorithm 1. As elaborated, graph embedding is first pre-trained with $L_{graph}$ (line 1), and then optimized with $L_{graph}$ and $L_{item}$ alternately (lines 2-19). It should be noted that the graph embedding and other aforementioned parameters are learned in advance, and then kept fixed during the training process of the conversation module. They are only updated while there occurs negative items during the conversation session (see line 25 in Algorithm 2)."}, {"title": "3.2.3 Conversation Module", "content": "Conversation module encodes current state and make decisions, including asking attributes or making recommendation, according to the current state. The policy network, which is used to make decisions, is a multi-layer perceptron and optimized via the policy gradient method of reinforcement learning [22]. We will first introduce the design of the state vector in our KG-CRS, and then explain the details of the policy network. It is to be noted, as we have stated, graph G is dynamic during the conversation process, which might directly affect the result of Equation 7, and then further influence the recommendation results and the state vector in conversation module.\nIn particular, in triples $(h, r, t)$, negative entities in Pneg and Vneg are removed from G, so that the information of these negative entities will not influence the recommendation and conversational results. For example, considering two candidate items $v1 and $v2 in the current session, and item $v1 is much more similar to item $v3 than $v2. On the other hand, item $v3 was chosen by user u in the history, but rejected by u in the current session. In this case, if item $v3 is still involved in the graph G, item $v1 may get a higher score than $v2 because it is more similar to item $v3 in the history. Therefore, in order to address this issue, we update graph G each turn by removing the negative entities (e.g., item $v3) and resume it for every new session. In this example, we only remove item $v3 but its related attributes will still exist in the graph. Consequently, for every related attribute p, it only loses one relation with $v3 in embedding learning, but can still propagate its information by other relations.\nState Vector. As defined in Equation 10, state vector consists of four components, entropy-based probability $s_{ent}$, user-based probability $s_{user}, conversation-based probability $s_{conv} and dialogue feature $s_{dial}. The first three parts calculate the probability of which attribute to ask from three different perspectives. The fourth part records the situation"}, {"title": "Algorithm 1: Offline Training of Graph Embedding and Recommendation Module", "content": "$s = [s_{ent}, s_{user}, s_{conv}, s_{dial}]$ (10)\nwhere $s_{ent}$ is a vector composed of the entropy of all attributes, whose size equals to the size of attributes. The entropy information of each attribute is calculated as Equation 11, which will be changed as the candidate items change. The agent can narrow the candidate set of items quickly through asking attributes with large entropy. This vector is one component of the state vector in [13], and we keep it as it has a good ability for the selection of attributes to ask.\n$H(x) = -\\sum p(x) log p(x)$ (11)\nwhere p(x) represents the probability of attribute x in the set of candidate items.\n$s_{user} is a vector composed of the scores of all attributes, and the size of the vector is equivalent to the number of attributes. The score of an attribute p, $y_p$, is calculated according to the embedding of user u and attribute p, as shown in Equation 12. The corresponding embedding, $u_2 and $p_2$, is obtained via graph embedding learning. That is, $u_2 = M_{2u}u, $p_2 = M_{2p}p, where u, p are the embedding of user u and attribute p measured using Equation 7, and $M_{2u}, M_{2p} are the projection matrices in terms of the user-attribute relation in Equation 2. The motivation of component $s_{user}$ is that user u may keep the same preference as demonstrated in her historical interactions. As the graph embedding is pre-trained before the conversation module (see Algorithm 1), attributes that user u often care will obtain a higher score, which can"}, {"title": "Algorithm 2: Training of Conversation Module", "content": "guide the policy networks to select more appropriate attributes.\n$y_p = u^T p_2$ (12)\n$s_{conu}$ is also a vector composed of the scores of all attributes while the score of each attribute p, $y_p$, is calculated according to the embedding of all known positive attributes $P_u$ and candidate attribute p, as shown in Equation 13. Specifically, $P_{u1} = M_{1p}P_u, p_1 = M_{1p}p, where $p_u, p are the embedding of attribute $p_u and attribute p calculated using Equation 7, and $M_{1u} are the projection matrices in terms of item-attribution relation in Equation 2. Pu represents the set of attributes that has been confirmed by the user in the previous rounds of the conversation session. The objective of this component is to guide the agent on selecting an attribute to ask based on the immediate user preference in the session. That is, the more relevant $p_u and p are, the higher the score is. Its size is still the same as the size of attributes.\n$Y_p = \\sum P_u^T P_{u1}$ (13)\n$s_{dial}$ is a vector that encodes the situation of current conversation, which mainly considers the length of candidate items and the dialogue history. In this work, we follow its design as [13], and leave its variant as our future work. Specifically, the length of candidate items is encoded as a one-hot vector with the motivation that it is easy for the agent to recommend successfully when the length of candidate items is short, unsuccessfully otherwise. The dialogue history is represented as a fixed-length (i.e., the maximum number of turns T) vector and each number in this vector represents the situation of the corresponding dialogue turn, where 1 means asking a positive attribute, 0 means asking an irrelevant attribute, and -1 means making an unsuccessful recommendation. The number at the position exceeding the current dialogue turn t is also represented as 0.\nReward. After the agent takes an action according to current state, user will respond and correspondingly get a numerical reward. As shown in Table 2, user u's responses can be concluded into five categories, and we specifically adopt two different designs of the reward function (i.e., coarse-grained reward (CG reward) and fine-grained reward (FG reward)) to address these responses."}, {"title": "Case, it might be unnecessary for the CRS agent to ask u about attribute p", "content": "And, the soundness of this intuition is also dependent on whether the recommendation module has precisely learned preference of user u. Therefore, under these considerations, rimp is designed as shown in Equation 14. Specifically, $loc_t refers to the ideal item position in a turn, where the ideal item means a ground-truth item user u will interact with, and its position means the ranking order given by the recommendation module. $loc_t means the ideal item position before action at (the action conducted at turn t), while $loc_{t+1}$ refers to the position after at. In this case, $\\frac{loc_t -loc_{t+1}}{loc_t}$ measures the impact of action at. Namely, a more necessary question can lead to greater position change of the idea item. $\\beta$ is a hyper-parameter that balances the impact of rimp and other rewards, while the fine-grained reward framework reduces to the coarse-grained reward with $\\beta = 0$.\n$rimp = \\beta * \\frac{loc_t - loc_{t+1}}{loc_t}$  (14)\nPolicy Network. The policy network is a multi-layer perceptron, which takes the state vector s as the input and outputs a probability distribution of all actions. The dimension of the probability distribution is $Nq + 1. The reward rt, as shown in Table 2, is the summation of all related rewards at turn t, and the policy network is optimized according to Equation 15.\n$\\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} log \\pi_{\\theta} (a_t | s_t) ( \\sum_{t\u2019=t,...,T} \\gamma^(t\u2019\u2212t) r_t\u2019)$  (15)\nwhere $\\theta represents the policy network's parameter, $\\eta is the learning rate, $\\pi_{\\theta}(at st) denotes the probability of action at given state st, and $\\gamma is a discount factor. $T\u2019 is the total number of turns in this session. The training process of the entire conversation module is detailed in Algorithm 2."}, {"title": "3.3 Time Complexity Analysis", "content": "The main operations in our method are listed as follows:\n\u2022 Operation 1: node feature learning. It locates in lines 5-7 in Algorithm 1\n\u2022 Operation 2: information propagation and score calculation. It lies in lines 9-16 in Algorithm 1 and line 6 in Algorithm 2.\n\u2022 Operation 3: updating weight ah,r,t. It refers to lines 2 and 19 in Algorithm 1.\n\u2022 Operation 4: encoding state vector. This operation locates in line 7 in Algorithm 2.\n\u2022 Operation 5: taking action. It refers to line 8 in Algorithm 2.\n\u2022 Operation 6: updating graph structure (i.e., removing negative items and negative attributes). It lies in line 29 in Algorithm 2.\nThe time complexity of these operations are summarized in Table 3, where N = |U| + |V| + |P|, m is the embedding size, n is the batch size, nhid is the hidden size of the policy network and other notations are shown in Table 1. The experiments are conducted with NVIDIA Tesla K80 and 2 CPU core (2.20GHZ)."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments on three real-world datasets to validate the effectiveness of our proposed KG-CRS, with the goal of answering three research questions:\n\u2022 RQ1: How does our KG-CRS perform compared with other state-of-the-art approaches in terms of both recom- mendation task and conversation task?\n\u2022 RQ2: How do different components of KG-CRS contribute to the performance?"}, {"title": "4.1 Experiment Settings", "content": "4.1.1 Datasets. We conduct experiments on three datasets: Yelp2, LastFM\u00b3 and Amazon\u2074. All these datasets provide rich information about users and items, including user's interaction history with items, item's price, and item's categories."}, {"title": "4.1.2 User Simulator and Conversation Setting", "content": "As our CRS is dynamic and the consideration of real users is costly, we follow [13] to create a user simulator to replace real users for the model training and testing. The user simulator randomly samples an attribute of the ideal item as the beginning of the conversation and gives a response according to the question or the recommendation in the following turns. We use the multi-round recommendation setting, which means that the user will not quit when the recommendation fails until the predefined maximum number of rounds T reaches. Same as [13, 14], the agent will recommend K items (i.e., K = 10) each time. T is set as 15 if not specifically identified in the following experiments.\nAs illustrated in [13], there are generally two kinds of questions in CRS: binary question and enumerated question, which are also referred to as yes/no question and open question in some other literature [27] respectively. The former one may ask questions like 'Are you in Washington?' while the latter one may ask like 'Which city are you in?'. Platforms can determine to use which kind of question setting according to their engineering and business needs. For fair comparisons, we follow [13] to test our model with both two settings. Specifically, the experiments on Yelp is conducted with enumerated question, while on LastFM and Amazon is binary question. Moreover, 590 attributes on Yelp are categorized into 29 facets given the enumerated setting, which means Nq on Yelp is equivalent to 29, namely the action space equals to 30. Given the binary setting on LastFM and Amazon, one question is related to one attribute and Ng on LastFM and Amazon thus equals 33 and 115 respectively, and thus the action space on is equal to 34 and 116 respectively."}, {"title": "4.1.3 Baselines", "content": "Although there are numerous works [5, 13-15, 18, 20, 27, 32, 34, 35] on CRS, as discussed in Section 2, many of them [15, 18, 27, 32, 34, 35] have different settings or focuses from our study. For example, [18] constructed a system which interacts with users like a chat-bot rather than a task-oriented system. [15, 34] focused on the NLP and semantic information. Thus, we compare our performance with the following most related baselines since their settings and research focus are much closer to ours:\n\u2022 Abs Greedy [5]: The main idea of the dialogue strategy for this method is to keep making recommendations without asking any questions. To better compare this rule-based dialogue strategy with the trained policy network in KG-CRS, the recommendation module in this method is the same as that in KG-CRS.\n\u2022 Max Entropy [20]: This method is also designed for the dialogue strategy and chooses to ask the attribute that has the maximum entropy among the candidates in each turn. The recommendation is randomly triggered by the length of the candidate item list. While the length gets shorter, it becomes more likely to do the recommendation. Similar to Abs Greedy, the recommendation module in this method is also the same as that in KG-CRS.\n\u2022 CRM [20]: The dialogue strategy in this method is decided by a neural network using a reinforcement learning framework, and the recommendation module is fulfilled by Factorization Machine(FM). It is originally designed for the single-round recommendation scenario. We adapt it to the multi-round recommendation scenario by following the process of [13].\n\u2022 EAR [13]: Similar to CRM, the conversation module and the recommendation module of this method also rely on reinforcement learning and FM respectively. Different from CRM, it is applied in the multi-round setting and proposes a three-stage solution called Estimation-Action-Reflection framework which updates the recommendation module according to the conversation module.\n\u2022 SCPR [14]: This is the state-of-the-art method on CRS under the multi-round setting. It is built on EAR and further considers the graph-based conversational path reasoning to help narrow the candidate set of attributes and items."}, {"title": "4.1.4 Parameters Setup", "content": "Following [13, 20", "31": "choose to let the policy network mimic the strategy of Max Entropy, whereas in our KG-CRS, besides the Max-Entropy strategy, we also propose to let the policy network mimic the ground truth of this problem. Specifically, in policy network pretrained phase, as shown in Figure 4, assuming that user 'Alice' wants to find item 'Lauryn Hill' through the CRS, it is better if the agent can ask attributes relating to 'Lauryn Hill' precisely and then make the recommendation. As the unstable performance of reinforcement learning, we consider this strategy can better initialize the parameters for the policy network in KG-CRS.\nIn KG-CRS, in graph embedding and recommendation module, the propagation step and embedding size of each step are set as 4 and 64 on Yelp, LastFM and Amazon, while the corresponding learning rate is 0.001 with the SGD optimizer. Following the parameter setting in [7"}]}