{"title": "Explaining Categorical Feature Interactions Using Graph Covariance and LLMS", "authors": ["Cencheng Shen", "Darren Edge", "Jonathan Larson", "Carey E. Priebe"], "abstract": "Modern datasets often consist of numerous samples with abundant features and associated timestamps. Analyzing such datasets to uncover underlying events typically requires complex statistical methods and substantial domain expertise. A notable example, and the primary data focus of this paper, is the global synthetic dataset from the Counter Trafficking Data Collaborative (CTDC) a global hub of human traffick-ing data containing over 200,000 anonymized records spanning from 2002 to 2022, with numerous categorical features for each record. In this paper, we propose a fast and scalable method for analyzing and extracting significant categorical feature interactions, and querying large language models (LLMs) to generate data-driven insights that explain these interactions. Our approach begins with a binarization step for categorical features using one-hot encoding, followed by the computation of graph covariance at each time. This graph covariance quantifies temporal changes in dependence structures within categorical data and is established as a consistent dependence measure under the Bernoulli distribution. We use this measure to identify significant feature pairs, such as those with the most frequent trends over time or those exhibiting sudden spikes in dependence at specific moments. These extracted feature pairs, along with their timestamps, are subsequently passed to an LLM tasked with generating potential explanations of the underlying events driving these de-pendence changes. The effectiveness of our method is demonstrated through extensive simulations, and its application to the CTDC dataset reveals meaningful feature pairs and potential data stories underlying the observed feature interactions.", "sections": [{"title": "1. Introduction", "content": "Modern datasets often contain numerous samples with abundant features (also called attributes or di-mensions), each associated with a timestamp. For example, the publicly available Counter-Trafficking Data"}, {"title": "2. Main Method", "content": "Given a dataset represented as $(X_{iu}, T_i)$, where $i = 1,...,n$ indexes the samples, $u = 1,...,p$ indexes the features, and $t_i \\in {1,...,T}$ denotes the timestamp of the i-th sample. Suppose the u-th feature has values in the range [0, Ku], where Ku is a positive integer, and the feature is binary if and only if Ku = 1. Let m represent the desired number of feature pairs to be extracted and analyzed. The method proceeds as follows:\n\u2022 Step 1 (Transform Categorical to Binary): For each categorical feature u that is not binary, we transform the feature into binary format using one-hot encoding. Specifically, compute $Z_u \\in [0,1]^{n \\times (K_u+1)}$ as:\n$Z_u (i, k + 1) = 1$ if and only if $X_{iu} = k$\nfor each i. This transformation ensures that each row of Zu contains all zeros except for a single element. After this step, the u-th column is removed from X, and Zu is appended. This process is repeated for all categorical features, resulting in a transformed X with p\u2081 binary features, where p1 \u2265 p. Additionally, a label vector Y \u2208 RP1 is created to track the transformation, where Yi = u indicates that the i-th binary feature originated from the u-th categorical feature.\n\u2022 Step 2 (Compute Temporal Graphs Per Timestamp): For each timestamp t = 1, ..., T, calcu-late the total number of samples at t as:\n$N_t = \\sum_{i=1,...,n} 1(T_i = t)$.\nNext, compute the graph At for each timestamp t, defined as:\n$A_t(u, v) = \\frac{\\sum_{T_i=t} 1(X_{iu} = X_{iv} = 1)}{N_t} \\in [0, 1]$,\nwhere u, v = 1,...,p1. In this graph, At (u, v) represents the normalized co-occurrence frequency of binary features u and v at timestamp t.\n\u2022 Step 3 (Compute Graph Covariance): For each timestamp t, calculate the p1 \u00d7 p1 covariance matrix as:\n$\\Sigma_t(u, v) = A_t(u, v) \u2013 A_t (u, u)A_t(v, v) \\in [-1,1]$\nfor all (u, v) such that Yu \u2260 Yv. Each \u03a3t(u, v) quantifies the dependence between features u and v at timestamp t.\n\u2022 Step 4 (Determine 95% Significance Level): For each timestamp t, compute a significance level Et using:\n$0.025 = 2Prob(Normal(0, 0.25) \\leq \\epsilon_t\\sqrt{n_t}$\nThis can be efficiently computed using the inverse cumulative distribution function.\n\u2022 Step 5.1 (Detect Common Interactions): For each pair (u, v), calculate:\n$\\delta_1 (u, v) = \\sum_{t=1}^T |\\Sigma_t(u, v)|1(|\\Sigma_t(u, v)| > \\epsilon_t)/T$\nand identify the timestamps where the interaction is significant, along with the trend information (positive or negative), into the set:\n$t_s (u, v) = {t \u00b7 1(\\sum_t(u, v) > \\epsilon_t) \u2212 t \u00b7 1(\\sum_t(u, v) < \u2212\\epsilon_t)}$.\nSelect the top m pairs with the largest \u03b4\u2081 (u, v) and store them in the set A1 = {(u, v, ts(u, v))}.\n\u2022 Step 5.2 (Detect Spiked Interactions): For each pair (u, v), calculate:\n$\\delta_2 (u, v) = \\sum_{t=1}^T |\\Sigma_t(u, v)|1(|\\Sigma_t(u, v)| > \\epsilon_t)/|t_s(u,v)|$.\nSelect the top m pairs with the largest \u03b4\u2082(u, v) and store them in the set A2 = {(u, v, ts(u, v))}.\n\u2022 Step 6 (LLM Query on Set Information): For each element in A\u2081 and A2, query an LLM with the following details: basic dataset information, the feature pair (u, v) along with their respective textual descriptions, the time range ts(u,v) during which the pair (u, v) is significant, and the positive or negative trend information. Then, prompt the LLM to identify potential underlying events or contexts that could explain the significant relationship between u and v within the specified time range.\nDetailed Discussions\nStep 1 employs the one-hot encoding scheme, transforming all categorical features into binary features. After this step, X contains only binary features, with n samples and p\u2081 binary columns. This transformation preserves the categorical information by expanding each category into its own column, a technique funda-mental in neural networks and machine learning [2, 15]. More recently, it has also been applied in hypothesis testing [17] and graph embedding [22].\nIn Step 2, At measures how often two features are both equal to 1, forming a graph of size P1 \u00d7 P1. Numerically, this computation is highly efficient due to the binary nature of the features. Specifically, for timestamp t, extracting $X(t) = {X_{iu}|T_i = t} \\in \\mathbb{R}^{n_t \\times P_1}$ allows the matrix At to be computed using matrix multiplication as:\n$A_t = X_{(t)}^T X_{(t)} \\in [0, 1]^{P_1 \\times P_1}$.\nStep 3 centers the weighted graph computed in Step 2, resulting in the graph covariance matrix \u03a3t \u2208 [-1,1]P1\u00d7P1. While \u03a3t can still be interpreted as a weighted graph, it also serves as a quantitative measure of dependence between binary features. Importantly, when both features u and v originate from the same categorical variable, \u03a3t(u, v) is set to 0 by default. This adjustment is necessary because one-hot encoding makes features from the same categorical variable mutually exclusive. As a result, computing the graph covariance for such pairs would always yield At (u, v) = 0 and \u03a3t (u, v) as negative. Including these pairs would cause the algorithm to detect spurious negative trends arising solely from the binarization process.\nIn Step 4, we use a 95% significance level based on the asymptotic null distribution derived in Theorem 1, which assumes the largest possible variance. This level can be adjusted for different levels of sensitivity. For instance, a 99% significance level would increase et, while a 90% significance level would reduce it, allowing for more or less extreme level on the detected interactions.\nStep 5 introduces two thresholding schemes to identify significant feature pairs: one for detecting the most common interactions (81) and another for capturing spikes in dependence (d2). The distinction lies"}, {"title": "3. Mathematical Theory", "content": "In this section, we quantify the proposed graph covariance using Bernoulli random variables and prove that the graph covariance is a consistent statistic for measuring dependence between binary features.\nWithout loss of generality, we focus on the graph element At (u,v) and the graph covariance \u03a3t (u, v) between two binary variables. Suppose $(X_{iu}, X_{iv}) \\overset{i.i.d}{\u223c} F_{X_{ut}X_{vt}}$ for each i where Ti = t, with each marginal"}, {"title": "4. Simulation Study", "content": "In this section, we present a series of simulations to demonstrate the ability of the proposed graph covariance method to detect dependence between features and capture dynamic interactions for both stable and shifting relationships over time.\nWe considered t 1,2,..., 8 time steps, with nt = 5000 samples generated for each t. Eight binary features were simulated, each exhibiting varying degrees of dependence on a reference feature, X1t. The generation process for these features is defined as follows:\n\u2022 X1t ~ Bernoulli(0.2) for all t, serving as the baseline feature for comparison.\n\u2022 X2t ~ Bernoulli(0.1) for all t, remaining independent of X1t across all time points.\n\u2022 X3t ~ Bernoulli(0.9) * X1t for all t, exhibiting strong dependence on X1t throughout time.\n\u2022 X4t ~ Bernoulli(0.1 + 0.1t) * X1t for all t, with increasing dependence on X1t as time progresses.\n\u2022 X5t ~ Bernoulli(0.9 \u2013 0.1t) * X1t for all t, showing decreasing dependence on X1t over time.\n\u2022 X6t ~ Bernoulli(0.1 + 0.2(t \u2212 4)) * X1t for all t, demonstrating shifting dependence, decreasing from t = 1 to 4 and increasing from t = 4 to 8.\n\u2022 X7t ~ Bernoulli(0.1) for t < 8, and X7t ~ Bernoulli(0.9).X1t for t 8, remaining independent of X1t until a dependence spike at t = 8.\nThese configurations simulate various scenarios where pairwise feature dynamics change over time, provid-ing a comprehensive test of the graph covariance method's capability to capture different types of feature interactions."}, {"title": "5. Exploring the CTDC Dataset", "content": "We applied the proposed method to the Counter-Trafficking Data Collaborative (CTDC) global synthetic dataset. After excluding approximately 30,000 records that lacked a recorded year, the dataset comprised n = 206,582 records, each containing p = 26 categorical features along with the year of registration, and missing features (NaN) for samples were simply set to 0.\nAll categorical features were transformed into binary format using one-hot encoding, resulting in P1 158 binary features. The graph covariance matrix, \u03a3\u03c4\u2208 [\u22121,1]158\u00d7158, was computed for each year from 2002 to 2022, spanning T = 21 time steps. Using m = 10 feature pairs for both \u25b3\u2081 (most common interactions) and A2 (spiked interactions), we identified the top significant pairs. These feature pairs were then passed to an LLM to generate detailed data insights, including information about the feature pairs, their textual descriptions, and their time-specific significance trends (positive or negative). The entire computation, excluding the LLM query, was completed in under 2 seconds."}, {"title": "5.1. Computing Graph Covariance", "content": "Figure 2 presents the graph covariance matrices Et for the years 2003, 2009, 2015, and 2021, illustrating the dependence structures that enable the extraction of significant feature pairs. While certain patterns remain stable, noticeable variations in the dependence structures are observed across different years, par-ticularly between earlier and later time frames. These visualizations emphasize the evolving relationships"}, {"title": "5.2. Extracting Significant Feature Pairs", "content": "Table 1 presents the top 10 feature pairs in A1, representing the most significant and persistent trends, while Table 2 showcases the top 10 pairs in A2, capturing spiked interactions. Although there is some overlap between the sets, \u25b3\u2081 predominantly includes pairs significant over multiple years, whereas A2 includes several noteworthy spiked dependencies. Most relationships exhibit positive trends, with a few negative trends indicated by a negative sign preceding the years."}, {"title": "5.3. Explain Interactions using LLM Insights", "content": "While the graph covariance and feature extraction yield intriguing feature pairs and corresponding times-tamps, and the visualizations in Figure 3 are valuable for further analysis, understanding and explaining the observed interactions across years often requires significant effort to investigate underlying events, whether global or local. Some pairs, especially those in A1, are relatively straightforward to interpret. However, pairs in A2, due to their spiked nature, are more challenging to explain without historical context. These spikes could stem from data collection anomalies, such as a surge in records during a particular year, or from local or geopolitical events.\nLeveraging LLMs, our tool generates immediate data insights for any feature pair, whether from \u03941, \u03942, or user-specified queries. For example, we queried the following into an LLM: \"In the Counter-Trafficking Data Collaborative dataset, after year 2015, there are more reported cases based on SexualExploit and USA Country of Exploitation. Please analyze the potential causes, global and local events, and geopolitical factors that may explain this trend.\" This query is straightforward and can be customized by replacing the bolded text with any other feature pair. The LLM typically produces a detailed analysis spanning 1-2 pages, often concluding with a summary. Here, we report only the final conclusions generated with a GPT-4 LLM instance:"}, {"title": "6. Conclusion", "content": "In this paper, we proposed a fast and scalable method for statistical analysis and data insight generation using large language models (LLMs) for large-scale categorical datasets with timestamps, focusing on the CTDC global synthetic dataset as a case study. The method's primary strengths lie in its scalability, computational efficiency, and ability to construct correlation matrices, detect trends, and identify outliers in temporal data. By integrating graph covariance with LLM queries, the approach enables actionable insights by uncovering potential events and trends from dependence shifts, facilitating real-time analysis and interpretation of complex datasets.\nTheoretical analysis established that graph covariance is a consistent measure of dependence between binary random variables, effectively quantifying the strength of dependence through probability differences. Simulation studies validated its accuracy in capturing dynamic dependence patterns, and the application to the CTDC dataset demonstrated its capability to deliver rapid, interpretable analysis. By combining graph covariance results with LLM-generated insights, the method sheds light on socio-political and economic factors driving observed trends, supporting enhanced decision-making and providing a richer understanding of complex interactions."}, {"title": "APPENDIX", "content": "Appendix A. Proofs\nTheorem 1. At any given timestamp t:\n\u2022 The graph covariance satisfies the law of large numbers such that:\n$\\Sigma_t(u, v) \\xrightarrow{n_t \\to \\infty} \\mu_t$\nwhere\n$\\mu_t = Prob(X_{ut} = X_{vt} = 1)$\n$Prob(X_{ut} = 1)Prob(X_{vt} = 1)).$\n\u2022 The graph covariance satisfies the central limit theorem such that:\n$\\sqrt{n_t}(\\Sigma_t(u, v) \u2013 \\mu_t) \\xrightarrow{dist} Normal(0, \\sigma^2_t)$\nwhere the variance o satisfies:\n$\\sigma_t^2 \\leq \\frac{1}{4} + \\frac{1}{16\\sqrt{n_t}}$\n\u2022 The graph covariance converges to 0, i.e., \u00b5\u2081 = 0, if and only if Xut and Xvt are independent.\nProof. (i): Based on basic probability and the law of large numbers, the normalized graph element At(u, v) satisfies:\n$A_t(u, v) \\xrightarrow{n_t} Prob(X_{ut} = X_{vt} = 1)$.\nMoreover, we have\n$A_t(u, u) \\xrightarrow{n_t} Prob(X_{ut} = 1)$,\n$A_t(v, v) \\xrightarrow{n_t} Prob(X_{vt} = 1)$.\nIt follows that:\n$\\Sigma_t(u, v) = (A_t(u, v) \u2013 A_t(u, u)A_t(v, v))$\n$\\to Prob(X_{ut} = X_{vt} = 1) \u2013 Prob(X_{ut} = 1)Prob(X_{vt} = 1)$\n= \u03bc\u03c4.\n(ii): Applying the central limit theorem, it is clear that\n-$\\sqrt{n_t}(A_t(u, v) \u2013 Prob(X_{ut} = X_{vt} = 1)) \\xrightarrow{dist} Normal(0, Prob(X_{ut} = X_{vt} = 1)(1 \u2013 Prob(X_{ut} = X_{vt} = 1)))$.\nSimilarly, we have\n$\\sqrt{n_t}(A_t(u, u) \u2013 Prob(X_{ut} = 1)) \\xrightarrow{dist} Normal(0, Prob(X_{ut} = 1)(1 \u2013 Prob(X_{ut} = 1)))$,\n$\\sqrt{n_t}(A_t(v, v) \u2013 Prob(X_{vt} = 1)) \\xrightarrow{dist} Normal(0, Prob(X_{vt} = 1)(1 \u2013 Prob(X_{vt} = 1)))$.\nIt follows that:\n$\\sqrt{n_t}(\\Sigma_t(u, v) \u2013 \\mu_t) = \\sqrt{n_t}(A_t(u, v) \u2013 A_t (u, u)A_t(v, v) \u2013 \\mu_t) \\xrightarrow{dist} Normal(0, \\sigma^2_t)$.\nwhere\n$\\sigma_t^2 < Prob(X_{ut} = X_{vt} = 1)(1 \u2013 Prob(X_{ut} = X_{vt} = 1))$\n+ Prob(X_{ut} = 1)(1 \u2013 Prob(X_{ut} = 1))Prob(X_{vt} = 1)(1 \u2013 Prob(X_{vt} = 1))/\\sqrt{n_t}$\n$\\leq \\frac{1}{4} + \\frac{1}{16\\sqrt{n_t}}$\n(iii): It suffices to prove that:\n$Prob(X_{ut} = X_{vt} = 1) = Prob(X_{ut} = 1)Prob(X_{vt} = 1)$\nUt\nif and only if Xut and Xv\u2081 are independent.\nThe if direction is trivial. To prove the only if direction, since Xut and Xv\u2081 are binary, their independence is equivalent to proving the following four equalities:\n$Prob(X_{ut} = X_{vt} = 1) = Prob(X_{ut} = 1)Prob(X_{vt} = 1)$\n$Prob(X_{ut} = 0, X_{vt} = 1) = Prob(X_{ut} = 0)Prob(X_{vt} = 1)$\n$Prob(X_{ut} = 1, X_{vt} = 0) = Prob(X_{ut} = 1)Prob(X_{vt} = 0)$\n$Prob(X_{ut} = X_{vt} = 0) = Prob(X_{ut} = 0)Prob(X_{vt} = 0)$.\nThe first equality is given and holds by assumption. The second equality holds because:\n$Prob(X_{ut} = 0, X_{vt} = 1)$\n= Prob(Xv\u2081 = 1) \u2013 Prob(Xu\u2081 = Xvt = 1)\n= Prob(Xv\u2081 = 1) \u2013 Prob(Xu\u2081 = 1)Prob(Xv\u2081 = 1)\n= Prob(Xv\u2081 = 1)(1 \u2013 Prob(Xut = 1))\n= Prob(Xut = 0)Prob(Xv\u2081 = 1).\nBy the same argument, but switching Xu\u2081 and Xv\u2081, we obtain the third equality. The fourth equality follows from:\n$Prob(X_{ut} = X_{vt} = 0)$\n= Prob(Xv\u2081 = 0) \u2013 Prob(Xu\u2081 = 0, Xvt = 0)\n= Prob(Xvt = 0) \u2013 Prob(Xut = 1)Prob(Xv\u2081 = 0)\n= Prob(Xv\u2081 = 0)(1 \u2013 Prob(Xut = 1))\n= Prob(Xut = 0)Prob(Xv\u2081 = 0).\nThus, we have proven the only if direction.\nAppendix B. Additional Simulations\nFigure El replicates the experiment from Figure 1, using distance correlation and the Hilbert-Schmidt Independence Criterion (HSIC). While these methods produce quantitatively similar results to the graph covariance shown in Figure 1, they are significantly slower in computation time: 0.01 seconds for graph covariance, 6 seconds for distance correlation, and 2 minutes for HSIC.\nAppendix C. LLM Query Details on CTDC Data\nHere, we present a few LLM-generated data insights for additional feature pairs not included in the main paper. Although any feature pair can be queried, and the LLM outputs are far more detailed than the summaries provided here, these examples are included for illustrative purposes.\nAppendix C.1. LLM for Feature Pair (MMR, IDN)\nQuery: \"In the Counter-Trafficking Data Collaborative dataset, at year 2015, there are more reported cases based on Myanmar Citizen and Indonesia Country of Exploitation. Please analyze the potential causes, global and local events, and geopolitical factors that may explain this trend.\"\nOutput Summary: \"The increase in reported trafficking cases involving Myanmar citizens and Indonesia as the country of exploitation in 2015 is likely due to a combination of local, regional, and global factors, including: Myanmar's political and economic instability, which led to increased migration and vulnerability to trafficking. Indonesia's demand for migrant labor, particularly in the fishing industry, domestic work, and agriculture, where exploitation was rampant. The Rohingya crisis exacerbating migration flows from Myanmar, increasing vulnerability to trafficking. The global focus on human trafficking and increased media attention to exploitative industries in Indonesia, such as fishing. Improved victim identification and better"}]}