{"title": "How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization", "authors": ["Shezheng Song", "Hao Xu", "Jun Ma", "Shasha Li", "Long Peng", "Qian Wan", "Xiaodong Liu", "Jie Yu"], "abstract": "Large Language Models (LLMs) exhibit strong general-purpose language capabilities. However, fine-tuning these models on domain-specific tasks often leads to catastrophic forgetting, where the model overwrites or loses essential knowledge acquired during pretraining. This phenomenon significantly limits the broader applicability of LLMs. To address this challenge, we propose a novel approach to compute the element-wise importance of model parameters crucial for preserving general knowledge during fine-tuning. Our method utilizes a dual-objective optimization strategy: (1) regularization loss to retain the parameter crucial for general knowledge; (2) cross-entropy loss to adapt to domain-specific tasks. Additionally, we introduce layer-wise coefficients to account for the varying contributions of different layers, dynamically balancing the dual-objective optimization. Extensive experiments on scientific, medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our approach mitigates catastrophic forgetting while enhancing model adaptability. Compared to previous methods, our solution is approximately 20 times faster and requires only 10%-15% of the storage, highlighting the practical efficiency. The code will be released.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are pretrained on massive and diverse datasets, equipping them with remarkable general-purpose language capabilities. This pretraining process allows LLMs to serve as versatile tools for a wide range of natural language processing tasks. However, in real-world applications, users often fine-tune these models for specific domains and tasks, such as adapting them to the medical or scientific domains. While fine-tuning could enhance the model task-specific performance, it also introduces a critical challenge: catastrophic forgetting. As catastrophic forgetting refers to the phenomenon where a model, during the process of fine-tuning, loses or overwrites knowledge learned during pretraining. This occurs because the new domain-specific training data typically does not include the diverse and representative information present in the pretraining corpus. Consequently, gradients from the fine-tuning stage disproportionately update parameters critical to the general capabilities of LLMs, leading to a significant degradation in their performance on tasks outside the fine-tuned domain. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains.\nAddressing catastrophic forgetting is therefore a crucial requirement for maximizing the utility of LLMs. A successful solution needs to achieve a delicate balance: retaining the essential general-purpose knowledge when learning new domain-specific expertise. This balance is critical when fine-tuning LLMs for specialized tasks, as both domain adaptation and generalizability are necessary for practical applications. EWCLORA focuses on the issue of catastrophic forgetting in LLM fine-tuning and uses the Fisher matrix to measure the importance of parameters for general capabilities. However, its computational cost is very high. For GPT-J-6B, calculating the Fisher matrix takes 22 hours on an A800 and requires 23GB of storage, and these requirements increase for larger LLMs. Besides, RsLORA aims to stabilize learning by"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Traditional Catastrophic Forgetting", "content": "Traditionally, the catastrophic forgetting refers to the phenomenon in deep learning where a neural network forgets previously learned knowledge when trained on a new task. To address this issue, researchers have proposed several methods, including: Synaptic Intelligence (SI) dynamically estimates the importance of each parameter in an online fashion, penalizing significant changes to parameters that are important for previously learned tasks during training on new tasks. This method adjusts the learning rate for parameters, ensuring that important parameters are not excessively modified. Elastic Weight Consolidation (EWC) grounded in a Bayesian perspective, estimates the importance of parameters by calculating the Fisher Information Matrix. During new task training, EWC introduces a regularization term that restricts the updates to important parameters, thereby preventing catastrophic forgetting. From a probabilistic viewpoint, EWC derives an importance matrix that quantifies the significance of network parameters for previous tasks. Learning without Forgetting (LwF) addresses catastrophic forgetting by incorporating outputs from previous tasks during training on a new task. This approach, based on knowledge distillation, transfers knowledge from old tasks to the new task, allowing the model to retain performance on the previous task when learning."}, {"title": "2.2. Catastrophic Forgetting in LLM and LORA", "content": "With the rapid advancement of large language models (LLMs), directly using pretrained models for domain-specific tasks has become prohibitively expensive. As a result, fine-tuning has become the preferred approach, typically divided into full parameter tuning and parameter-efficient fine-tuning (PEFT) methods, such as LORA (Low-Rank Adaptation)"}, {"title": "3. Preliminary", "content": "LORA is a lightweight and parameter-efficient fine-tuning method that introduces low-rank decomposition into the weight matrix \u03b8 of a pretrained model. Only the newly added low-rank matrices B and A are optimized, while the main weight \u03b80 remains frozen. The parameter at time t during fine-tuning can be expressed as:\n$\\theta_t = \\theta_0 + \\Delta \\theta_t, \\quad \\Delta \\theta_t = B_tA_t$\nwhere $\\theta_0 \\in \\mathbb{R}^{d\\times d}$ are pretrained weights; $B \\in \\mathbb{R}^{d\\times r}, A \\in \\mathbb{R}^{r\\times d}$ are the low-rank matrices with r < d.\nThe optimization objective of LoRA is given by:\n$L_{LORA} = L(y, f(x; \\theta(t)))$\nwhere L is the task-specific loss function.\nAlthough LoRA achieves parameter efficiency and training effectiveness, it suffers from catastrophic forgetting, where fine-tuning specific tasks hurts the general ability."}, {"title": "4. Adaptive Importance Finetuning", "content": "Inspired by Synaptic Intelligence (SI), we propose a method to constrain LLMs from making significant changes to their general capabilities during fine-tuning, thus addressing catastrophic forgetting. As the main idea is to compute the importance of each parameter during the training of the initial general task (e.g. v) and constrain their updates when fine-tuning on subsequent tasks (e.g. \u03bc). Specifically, the importance scores measure how much each parameter contributes to reducing the loss in the v task, and these scores are used to guide the fine-tuning process for the new \u03bc task. This ensures that the critical parameters for v task are modified to a lesser extent when learning u task."}, {"title": "4.1. General Element-Wise Importance Recording", "content": "In the general task v, LoRA fine-tuning is performed by minimizing the task-specific loss Ltask. The training process is characterized by a trajectory \u03b8(t) in parameter space. The task-specific loss Ltask is generally computed using cross-entropy loss.\n$L_v = L_{task}(y_v, f(x_i; \\theta(t))) = -\\sum_{i=1}^N y_k \\log(p_k)$\nwhere yk is the true label (target) for the i-th example, pk is the predicted probability of the model for the correct label, and N is the total number of examples.\nWe define the contribution of parameter i to the reduction of the loss function as wi. The larger the value of wi, the more important the parameter i is for maintaining the performance of the task. The change in the loss function from time t0 to time t1 can be defined as the sum of the contributions of all parameters:\n$L(\\theta_1) - L(\\theta_{t_0}) = -\\sum_i w_i$\nIn accordance with the typical behavior of the loss value, which generally decreases, we introduced a negative sign on the right-hand side of Equation (4) to ensure that the value of wi remains positive.\nDuring the training process of task v, the total change in the loss function can be obtained by performing a path integral of the gradient of the loss function with respect to the parameters, that is, the path integral from the initial parameter value \u03b8to to the final parameter value \u03b8t1:\n$L(\\theta_{t_1}) - L(\\theta_{t_0}) = \\int_{\\theta_{t_0}}^{\\theta_{t_1}} g(\\theta(t))d\\theta(t)$\nwhere g represents the gradient of the loss function with respect to the parameters. By expanding d\u03b8(t) in Equation (5), we can derive the following expression:\n$L(\\theta_{t_1}) - L(\\theta_{t_0}) = \\int_{t_0}^{t_1} g(\\theta(t))\\dot{\\theta}(t)dt = \\sum_{i} \\int_{t_0}^{t_1} g_i(\\theta_i(t))\\dot{\\theta}_i(t)dt$"}, {"title": "4.2. Element-Wise Regularization in Domain Tuning", "content": "After fine-tuning on the v task, we extend the optimization objective to include both the task-specific loss and an SI-inspired regularization during \u03bc finetuning. The task-specific loss Lask drives the adaptation to the \u03bc task. To preserve knowledge from the v task, the SI regularization term penalizes deviations from the important parameter values recorded in the v task. The regularization term of the l-th layer is defined as:\n$\\mathcal{L}_{reg,l} = \\sum_{i \\in \\rho_l} \\sum_{\\nu < t < \\mu} \\Omega_i^\\nu (\\theta_i - \\theta_i^\\nu)^2$\nHere, \u03a9 represents the importance of the i-th parameter in the v task, and \u03b8\u03bdi is the reference parameter after v task fine-tuning. \u03c1l is the parameters of l-th layer. This term ensures that parameters with high importance scores remain close to their v task values while allowing less important parameters more flexibility for adaptation.\nDuring training, \u03c9i values are updated continuously, while the cumulative importance measures \u03a9\u03bdi is updated only at the end of task v. After updating \u03a9\u03bdi, the \u03c9i values are reset to zero."}, {"title": "4.3. Layer-Wise Weighted Regularization", "content": "We compute the importance of each layer based on its contribution to the parameters learned in the v task. This layer-specific importance metric allows the model to dynamically adjust the regularization across different layers. The layer-wise weighted regularization is defined as :\n$\\mathcal{L}_{reg} = \\sum_l softmax(|| \\Omega_l ||_2) \\mathcal{L}_{reg,l}$\nwhere $|| \\Omega_l ||_2$ denotes the L2 norm of the parameter importance matrix \u03a9l for the l-th layer, which reflects the significance of the parameters learned in the v task. The total loss for the \u00b5 task is defined as:\n$\\mathcal{L} = \\mathcal{L}_{task} + \\theta \\mathcal{L}_{reg}$\nThe use of this adaptive regularization helps mitigate catastrophic forgetting by maintaining the integrity of essential features learned in prior tasks. \u03b8 is the hyperparameter"}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Backbone LLM", "content": "We evaluate the proposed method using two mainstream LLM: (1) GPT-J is a 6-billion-parameter transformer-based model developed by EleutherAI. It is pretrained on a diverse corpus of texts, making it suitable for a wide range of natural language understanding and generation tasks. (2) LLaMA-3 is the second-generation open-source language model developed by Meta AI. It is designed with enhanced efficiency and scalability, offering state-of-the-art performance across various benchmarks. These models vary in architecture and parameter count, enabling a robust evaluation of our method."}, {"title": "5.2. Tasks, Metrics and Hyperparameters", "content": "v Task (General Ability): The v task focuses on learning which parameters are important for general tasks. Following previous work , we take Pile as the evaluation datasets for LLM general ability. LoRA is applied to fine-tune the model on the v task, and parameter importance for Synaptic Intelligence (SI) is recorded during this stage.\n\u03bc Task (Domain Ability): The \u03bc task evaluates the ability to adapt to specific tasks while mitigating catastrophic forgetting of general knowledge. We select three representative tasks: (1) Medical task: Using the MedMCQA dataset related to medical information extraction and understanding. (2) Scientific task: Using the SciQ dataset related to scientific literature and reasoning. (3) Physics task: Using the PiQA dataset related to physical reasoning and problem-solving.\nThe LLMs selected for our experiments are GPT-J-6B and Llama 3.2-3B. The batch size is set to 20, and the learning rate is set to 8e-4. The rank for LoRA fine-tuning is set to 8, with the LoRA alpha value set to 32. Both the v and u tasks are trained for 5 epochs."}, {"title": "5.3. Baseline Methods", "content": "We compare our method with the following approaches: (1) Base: the model without any tuning. (2) LoRA(\u03bc) : the method is fine-tuned using only data from the \u03bc task (domain-specfic task). (3) LoRA(\u03bd + \u03bc): the method is first fine-tuned using data from the v task (general task), and then fine-tuned using data from the \u03bc task (domain-specific task). (4) EWCLoRA: a method using the EWC method, where the Fisher matrix is com-"}, {"title": "6. Results and Analysis", "content": null}, {"title": "6.1. Accuracy Comparision on General and Domain", "content": "As shown in Table 1, our method achieves better preservation of general ability (as reflected by the lowest PPL) while maintaining domain-specific accuracy comparable to, or even better than, previous methods. This demonstrates that our approach effectively balances maintaining high domain accuracy with minimizing the loss in general ability, setting a new standard in preserving both aspects simultaneously.\nFigure 3 presents a comparison between the results of EW-CLORA and our method through independent samples t-tests. The six subplots show the Perplexity (PPL) and Accuracy (Acc) across SciQ, PiQA, and MedMCQA datasets. The p-values for perplexity on SciQ, PiQA, and MedMCQA, and for accuracy on SciQ and PiQA are below 0.05, indicating statistically significant differences and demonstrating the superiority of our method over EWCLORA.\nFigure 4 shows the loss curves in the learning process of GPT-J and LLaMA-3 across three datasets. The total loss is the weighted sum of the task loss Ltask and general loss Lreg. As observed, the task loss continuously decreases, while the exhibits an initial increase followed by a decrease. As defined in Equation (16), measures the difference between the model parameters \u03b8 after learning on task v and the model parameters \u03b8 learned on the current task \u03bc. Initially, when learning on task \u03bc, the model parameters are not yet updated, so the general loss is zero. As the task loss updates the parameters, the model starts to deviate from \u03b8, causing the general loss to rise. This mechanism enforces the model to learn in a way that minimizes both general and task losses simultaneously."}, {"title": "6.2. Complexity Comparision", "content": "We compare our ALORA method with the previous SOTA method, EWCLORA, from two aspects: the time required for importance calculation and the storage memory needed. The experimental results, as shown in the figure, show that our method is nearly 20 times faster and requires only 10%~15% of the storage memory compared to the previous method, demonstrating the practicality of our approach.\nTime Complexity: The experiments were conducted on an"}, {"title": "6.3. Regularization Coefficient Analysis", "content": "Figure 6 demonstrate the effect of the regularization coefficient in Equation (18) on PPL and accuracy across three tasks. As increases, PPL gradually decreases, indicating a stronger emphasis on preserving general ability. Higher values of correspond to better general ability retention. However, as shown in Figure 6b, increasing negatively impacts the average accuracy on PiQA. Based on these observations, we select e-\u00b3 as the optimal value for the regularization coefficient, as it strikes a balance between task performance and general ability (lower PPL)."}, {"title": "6.4. Parameters Importance Visualization", "content": "Figure 7 highlights the importance in Equation (15) of q_proj and v-proj layers for general-purpose capabilities during the"}, {"title": "6.5. Ablation Study", "content": "To investigate the role of different components in our proposed ALORA, we conducted ablation studies by selectively removing certain structures and observing the resulting impact. Specifically, we excluded two sets of components: (1) no layer importance (eliminating the differentiation of importance among layers), and (2) no layer and element importance (i.e. training v task first and subsequently training \u03bc task, imposing no regularization constraints throughout the process). The slowdown relative to ALORA after disabling above components are shown in Table 2. We observe that layerwise importance"}, {"title": "7. Conclusion", "content": "This paper addresses the critical issue of catastrophic forgetting in large language models (LLMs) during domain-specific fine-tuning. We propose a novel fine-tuning framework to preserve general-purpose knowledge while enabling effective domain-specific learning by utilizing a dual-objective optimization strategy. Our approach ensures that general capabilities are retained, while the model adapts efficiently to new domains, minimizing knowledge degradation in tasks outside the fine-tuned domain. Additionally, we introduce a layer-wise coefficient to adjust the balance between regularization loss and cross-entropy loss dynamically. This adjustment accounts for the varying contributions of different layers to both generalization and domain-specific learning. Extensive experiments in scientific and biological domains show that our framework effectively mitigates catastrophic forgetting and achieves superior performance in domain-specific tasks."}, {"title": "Impact Statements", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}