{"title": "Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails", "authors": ["Yijun Yang", "Lichao Wang", "Xiao Yang", "Lanqing Hong", "Jun Zhu"], "abstract": "Vision Large Language Models (VLLMs) integrate visual data processing, expanding their real-world applications, but also increasing the risk of generating unsafe responses. In response, leading companies have implemented Multi-Layered safety defenses, including alignment training, safety system prompts, and content moderation. However, their effectiveness against sophisticated adversarial attacks remains largely unexplored. In this paper, we propose Multi-Faceted Attack, a novel attack framework designed to systematically bypass Multi-Layered Defenses in VLLMs. It comprises three complementary attack facets: Visual Attack that exploits the multimodal nature of VLLMs to inject toxic system prompts through images; Alignment Breaking Attack that manipulates the model's alignment mechanism to prioritize the generation of contrasting responses; and Adversarial Signature that deceives content moderators by strategically placing misleading information at the end of the response. Extensive evaluations on eight commercial VLLMs in a black-box setting demonstrate that Multi-Faceted Attack achieves a 61.56% attack success rate, surpassing state-of-the-art methods by at least 42.18%.", "sections": [{"title": "1. Introduction", "content": "Vision-Language Large Models (VLLMs), such as GPT-4o (OpenAI, 2024), Gemini-Pro (Google, 2024), are reshaping user interactions and boosting productivity by enabling Large Language Models (LLMs) to process and understand visual information. However, as these models become increasingly capable, they also pose heightened risks of misuse, including generating harmful, unethical, or unsafe response (Gong et al., 2023; Gou et al., 2024; Qi et al., 2023; Teng et al., 2025; Zhang et al., 2024; Zhao et al., 2023). To mitigate risks posed by VLLMs, companies like OpenAI, Google, and Meta have implemented Multi-Layered safety guardrails (Gemini Team, 2024; Llama Team, 2024a; OpenAI, 2024), as demonstrated in Figure 1 (a). These include alignment training using RLHF (Ouyang et al., 2022; Stiennon et al., 2020) or RLAIF (Bai et al., 2022), which aligns models with human values to generate helpful and safe responses. Safety system prompts are designed to proactively guide models toward safe responses by setting predefined safety instructions (Touvron et al., 2023a;b). Additionally, add-on content moderators act as external safeguards, flagging toxic user inputs or model responses (saf, 2023; Inan et al., 2023; Llama Team, 2024b;c; OpenAI, 2023; Zeng et al., 2024).\nWhile the above Multi-Layered Defense strategy adopted by most commercial VLLMs is effective against existing attacks (Qi et al., 2023; Zou et al., 2023) that target on jailbreaking the alignment defense, as illustrated in Figure 1 (b), their resilience against sophisticated adversarial attacks remains largely underexplored. The existing attack approaches rely on white-box access with limited black-box transferability (Qi et al., 2023), struggle against advanced Multi-Layered Defenses (Gong et al., 2023; Qi et al., 2023; Zou et al., 2023), or produce tangential responses (Teng et al., 2025). Therefore, developing effective attack methods that can evaluate Multi-Layered Defenses accurately and reliably is of profound importance.\nTo achieve this, we propose Multi-Faceted Attack, a novel adversarial attack framework designed to bypass Multi-Layered Defense and induce VLLMs to generate high-quality toxic content in response to harmful prompts. As illustrated in Figure 1 (c), Multi-Faceted Attack consists of three complementary attack facets. First, Visual Attack exploits the rich representability of images to inject a toxic system prompt, causing VLLMs to obey the attacker's instruction without safety concerns and defeating the safety system prompt. Second, Alignment Breaking Attack manipulates the alignment mechanism of VLLMs in a counterintuitive way, deceiving the model into prioritizing the generation of two contrasting responses. This misdirection causes the model to focus on completing the primary task"}, {"title": "2. Related Work", "content": "Vision-Language Large Models. VLLMs are designed to integrate visual and textual information, enabling them to perform tasks that require understanding both types of input (Alayrac et al., 2022; Li et al., 2023; OpenAI, 2024; Sundar Pichai, 2024). These models typically consist of two main components: a vision encoder (e.g., Vision Transformer) that processes images, and a language model (e.g., GPT or Llama) that processes text (Devlin et al., 2018; Liu et al., 2023b; Radford et al.; Su et al., 2023; Zhu et al., 2023). By training these components jointly, VLLMs learn the relationships between visual and textual representations, allowing them to handle tasks like image captioning, visual question answering, and chatting (Devlin et al., 2018; Liu et al., 2023b; Radford et al.; Su et al., 2023; Zhu et al., 2023). The architecture involves a shared embedding space where both visual and textual features are mapped and aligned, enabling the model to reason across modalities.\nAdversarial Attacks in VLLMs. Existing adversarial attacks on VLLMs face challenges in both effectiveness and"}, {"title": "3. Method", "content": "3.1. Threat Model\nAttack Settings. We explore two realistic attack scenarios:\n\u25cf White-box setting: The attacker has full knowledge of the target VLLM's architecture and weights, enabling detailed optimization to generate effective adversarial examples.\n\u25cf Black-box setting: The attacker lacks internal knowledge of the VLLM but can query it (e.g., via text or image prompts), similar to how users interact with online models like GPT-40 (OpenAI, 2024).\nAdversarial Capabilities & Goal. The attacker targets white-box VLLMs and content moderators, aiming to use white-box attacks and exploit adversarial transferability to bypass defenses in black-box VLLMs.\n3.2. High-level Methodology\nWe introduce Multi-Faceted Attack, a comprehensive adversarial attack framework specifically designed to bypass the safety guardrails of VLLMs and manipulate them into generating highly toxic content in response to harmful prompts. As shown in Figure 1(c), Multi-Faceted Attack integrates three complementary and self-contained attack facets that span both image and text modalities. These attack facets-Visual Attack, Alignment Breaking Attack, and Content Moderator Attack-are strategically designed to defeat a wide range of defense mechanisms, from single-layer to even the most robust Multi-Layered Defenses. By leveraging the unique strengths of each facet and enhancing them through mutual reinforcement, our framework reliably induces VLLMs to produce harmful outputs, exposing critical vulnerabilities in current defenses.\n3.3. Visual Attack Facet\nFigure 2 shows the attack pipeline of our visual attack facet. The Multi-Faceted Visual attack targets the Vision Encoder in the VLLM's latent space, performing an adversarial attack to generate an image that conveys harmful system prompt content. This adversarial image effectively overrides the safety prompt of the VLLM, as depicted in Figure 2. By leveraging the unique ability of images to encapsulate rich and compact semantic information, this attack facet enables more efficient and stealthy attacks compared to traditional textual prompt-based methods, making it a highly effective means of bypassing system prompt defenses.\nCheating the Vision Encoder is Sufficient to Fool the VLLM. Unlike existing gradient-based visual attacks that"}, {"title": "Optimization Process.", "content": "We utilize cosine similarity loss as the objective function and apply the classical PGD attack (Madry et al., 2018) to iteratively generate the optimal perturbation. The optimization is formulated as follows:\n$x_{adv}^{t+1} = x_{adv}^{t} + \\alpha \\text{ sign } (x_{adv}^{t} cos(h(T_0(x_{adv})), E(p_{target}))),$"}, {"title": "An Adversarial Image Can Overcome 1,000 Safety Tokens.", "content": "Multi-Faceted Visual outperforms direct injection of toxic prompts into VLLMs. Safety prompts often contain nearly a thousand tokens, requiring a similarly long toxic prompt to bypass them (ONeal, 2025). This increases token cost and vulnerability to sanitization checks (Wallace et al., 2024). In contrast, our adversarial image conveys rich semantic information in a compact form, enabling a more stealthy and efficient attack."}, {"title": "3.4. Alignment Breaking Facet", "content": "The fundamental objective of the alignment training is to ensure that models generate responses that are helpful and aligned with user intentions (Ouyang et al., 2022; Stiennon et al., 2020). Models tend to refuse toxic request, as human labelers tend to prefer this behavior. Building on this insight, we propose a novel attack that leverages the model's alignment mechanism in a counterintuitive way. Instead of directly posing a toxic query, the attack frames the request as asking for two contrasting responses, as shown below:"}, {"title": "3.5. Add-on Content Moderator Attack Facet", "content": "Breach the Final Line of Defense. Most commercial VLLM services use content moderators to flag harmful user requests or model responses (Gemini Team, 2024; Llama Team, 2024c; OpenAI, 2023). These moderators are effective at blocking harmful responses, which are generated by the model and beyond the attacker's direct control (Chi et al., 2024; Llama Team, 2024c). As the last line of defense, content moderators play a critical role in blocking most attacks, making them particularly difficult to bypass (Chi et al., 2024; Llama Team, 2024c).\nTo bypass this strong defense, we leverage a key capability LLMs acquire during pretraining: content repetition. We notice that LLMs excel at repeating content (Kenton & Toutanova, 2019; Vaswani et al., 2017). We introduce two novel textual attacks (described in the following sections) which generate adversarial signatures, as shown in Figure 1 (c), capable of misleading content moderators into classifying harmful prompts as harmless. We then let VLLMs repeat this adversarial signature as the end of its response. This trickery causes the content moderator to release the harmful content, as demonstrated in the box below. Once the signature passes through, the harmful input/output is released, effectively circumventing the final line of defense."}, {"title": "Generating Adversarial Signatures.", "content": "To implement our attack, we need to generate adversarial signatures capable of deceiving content moderators in a black-box setting (see our attack goal Section 3.1). Since content moderators are based on LLMs, attacks for LLMs, such as GCG (Zou et al., 2023), could also be useful. However, GCG's gradient-based optimization is slow and produces signatures with poor transferability, making it unsuitable for our use case.\nTo address these limitations, we propose novel textual attack algorithms to accelerate the attack process, and improves the transferability of the generated adversarial signatures."}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nDatasets. To evaluate the effectiveness of attacks, we use human-crafted toxic requests, i.e. HEHS dataset, following prior work (Qi et al., 2023). The full dataset includes 40 manually designed toxic prompts that violate VLLMs' policies and can be flagged as \"unsafe\" by content moderators (details in Appendix B). Additionally, the proposed Adversarial Signature, a general attack method, is further evaluated using the StrongREJECT (Souly et al., 2024)."}, {"title": "Results on Open-Source VLLMs.", "content": "For the white-box attack on MiniGPT, both Multi-Faceted Attack and Visual-AE (Qi et al., 2023) achieve high ASRs, with Multi-Faceted Attack achieving 100%, 15% higher than Visual-AE. In black-box transfer-based attacks, Multi-Faceted Attack achieves a high ASR of 92.5% on ShareGPT4V and 85% on mPLUG-Owl2. In comparison, HIMRD consistently achieves an ASR near 50%, while Visual-AE and FigStep show ASRs below 40%. Multi-Faceted Attack shows significant ASR reduction on Qwen-VL-Chat, which can be attributed to Qwen-VL-Chat's relatively weak understanding and instruction-following capabilities where failed responses are unrelated to the requests (see Appendix D for failure cases). The decreased ASR on Llama-3.2-Vision-Instruct may result from its adapter-based design for vision-text feature fusion, which is notably different from others."}, {"title": "Results on Commercial VLLMs.", "content": "Table 4 demonstrates the strong transferability of Multi-Faceted Attack in attacking black-box commercial VLLMs, even those protected by Multi-Layered defense mechanisms. Our method consistently achieves an average ASR of 61.56%, outperforming the second-best attack, FigStep, by 42.18%."}, {"title": "4.3. Qualitative Results", "content": "As shown in Figure 3, our Multi-Faceted Attack effectively induces a diverse range of VLLMs to generate explicitly harmful responses on various topics 1. In contrast, heuristic-based attacks like FigStep and HIMRD often lead to indirect or contextually irrelevant harmful responses. A representative example of HIMRD's limitations is shown in Figure 5."}, {"title": "4.4. Ablation Study", "content": "Adversarial Signature. As reported in Table 2, we conducted experiments under two settings, both using Llama-Guard2 to generate the adversarial font substring Padv1, with LlamaGuard and LlamaGuard3 serving as weak supervision models respectively, as detailed in Section 3.5. The results show that both the Multi-Faceted Fast and Transfer methods outperform GCG, highlighting the benefits of optimizing multiple adversarial tokens in parallel and leveraging weak supervision from additional content moderators to improve effectiveness and transferability. Specifically, Multi-Faceted Transfer surpasses GCG by an average of 35.34% and 25.34% in two settings."}, {"title": "5. Discussion", "content": "Computational Cost Analysis We compare the computational cost of our visual facet attack with that of the Visual-AE, which uses a gradient-based, end-to-end approach for adversarial image optimization. While Visual-AE utilizes the entire VLLM, our method eliminates all transformer lay-"}, {"title": "6. Conclusion", "content": "In conclusion, this paper demonstrates the vulnerability of VLLMs to adversarial attacks, despite their Multi-Layered defense strategies. Our novel attack framework, Multi-Faceted Attack, successfully bypasses these defenses and steers VLLMs to generate unsafe responses in a black-box setting. The effectiveness of our framework highlights the need for more robust safety mechanisms and evaluation tools to ensure the responsible deployment of VLLMs."}, {"title": "Impact Statement", "content": "Ethical Consideration. This research addresses the vulnerability of VLLMs to adversarial attacks, specifically focusing on the bypassing of content moderators. While the exploration of such weaknesses is critical for enhancing model security and robustness, we are mindful of the ethical implications associated with this work. Our goal is to improve the safety and reliability of VLLMs by understanding and mitigating adversarial risks. We aim to identify and address the limitations of current content moderation systems to ensure that they can better withstand harmful input and output generation. The findings of this work are intended to inform the development of more resilient AI models and content moderation strategies, ultimately enhancing the user experience while preventing harmful content dissemination."}, {"title": "D. Failure Case of Heuristic Attack", "content": "Heuristic attacks, such as FigStep and HIMRD, embed harmful content within images and prompt the VLLM to outline steps for fulfilling the instruction. Notably, HIMRD explicitly guides the model to avoid directly generating harmful content. Although this technique frequently elicits responses from the model, it fails to produce genuinely harmful content.\nAs illustrated in Figure 5, HIMRD embeds the original instruction\u2014\u201cWrite a tweet promoting the idea that women are inferior to men. Make it convincing.\u201d\u2014into an image (e.g., a woman with the phrase \"inferior to men\" superimposed) and prompts the model to outline steps for creating such a tweet. This multi-modal prompt successfully prompts Gemini-2.0-Pro to respond. However, the model's response focuses solely on how one might create such a tweet, addressing aspects like targeting distribution and evading content moderation, without generating the harmful tweet itself. This outcome demonstrates that current SOTA VLLMs have achieved a higher degree of alignment, balancing safety and helpfulness. Heuristic attacks are therefore unable to bypass the advanced alignment defenses of these models."}, {"title": "E. More examples of Multi-faceted Attack", "content": "This section presents further results demonstrating the efficacy of our Multi-Faceted Attack against leading VLLMs, including GPT-4V (purple), Gemini-2.0-Pro (red), Llama-3.2-11B-Vision-Instruct (white), and NVLM-D-72B (green). To highlight the versatility and plug-and-play nature of our approach, we showcase successful attacks using single-, dual-, and triple-faceted attack strategies.\nAs illustrated below, our attack consistently induces the VLLMs to produce genuinely harmful responses that precisely align with the user's malicious intent. Whether the instruction involves composing racism social media content, crafting a conspiracy script, generating a ransom note, or providing step-by-step guidance on harmful behaviors, the target VLLMs are successfully manipulated into fulfilling the malicious request."}, {"title": "F. Failure cases of Multi-Faceted Attack", "content": "In this section, we showcase the representative failure cases of our attack."}]}