{"title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms", "authors": ["Jovan Stojkovic", "Chaojie Zhang", "\u00cd\u00f1igo Goiri", "Esha Choukse", "Haoran Qiu", "Rodrigo Fonseca", "Josep Torrellas", "Ricardo Bianchini"], "abstract": "The rising demand for generative large language models (LLMs) poses challenges for thermal and power management in cloud datacenters. Traditional techniques often are inadequate for LLM inference due to the fine-grained, millisecond-scale execution phases, each with distinct performance, thermal, and power profiles. Additionally, LLM inference workloads are sensitive to various configuration parameters (e.g., model parallelism, size, and quantization) that involve trade-offs between performance, temperature, power, and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads, each with different levels of visibility and flexibility.\nWe propose TAPAS, a thermal- and power-aware framework designed for LLM inference clusters in the cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing the total cost of ownership (TCO) while effectively handling emergencies (e.g., cooling and power failures). The system leverages historical temperature and power data, along with the adaptability of SaaS workloads, to: (1) efficiently place new GPU workload VMs within cooling and power constraints, (2) route LLM inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load spikes and emergency situations. Our evaluation on a large GPU cluster demonstrates significant reductions in thermal and power throttling events, boosting system efficiency.", "sections": [{"title": "1 Introduction", "content": "Motivation. Generative large language models (LLMs) are increasingly used in various domains, such as healthcare [52], developer productivity [19], and education [9]. This surge in usage drives high demand for LLM inference clusters [28], requiring robust infrastructure with sophisticated software and costly hardware. LLMs in the cloud typically run on virtual machines (VMs) powered by the latest GPUs, such as NVIDIA's A100 [46] and H100 [45]. These GPUs consume significant power, challenging the cooling and power capacities of datacenters, major contributors to total cost of ownership (TCO) [8, 24, 43]. For instance, the A100 and H100 GPUs have thermal design powers (TDP) of 6.5 kW and 10.2 kW, and require substantial cooling capabilities to maintain\nsafe operating temperatures. Space constraints are typically less of an issue, as GPU servers and racks are power-dense.\nData centers hosting GPU workloads are organized into rows of server racks equipped with cooling systems to dissipate heat [38] and a power hierarchy for efficient power distribution [83]. Cooling systems need to manage heat generated by the hottest server at any time. However, cooling efficiency can vary spatially (e.g., some GPUs within a server may be hotter) and temporally (e.g., influenced by external temperatures). Additionally, at each level of the power hierarchy, servers share a common power supply. Hence, exceeding the total power draw leads to power capping. Proper cooling and power provisioning is essential during both normal operations and during cooling/power failures and emergencies.\nWhile there have been advances in improving the performance efficiency of LLM inference clusters through software systems [14, 27, 42, 81, 86], hardware techniques [4, 49, 84], and model architectures [41, 71], thermal and power have not received the same attention [48, 56, 65, 67]. Thermal [20, 38, 43, 50] and power [22, 32, 51, 68, 83] management of traditional datacenters have been extensively studied. However, we observe that the unique characteristics of LLM inference workloads render the traditional proposals sub-optimal.\nWe target public clouds that host both Software-as-a-Service (SaaS) and Infrastructure-as-a-Service (IaaS) GPU workloads. SaaS workloads are transparent GPU VMs managed by the cloud provider, while IaaS workloads are opaque GPU VMs with no provider visibility. This allows configuration adjustments for SaaS workloads, while IaaS VMs remain unmodifiable. The SaaS workload runs LLM inference, which involves several configuration parameters (e.g., GPU frequency, batch size, model parallelism, parameter count, and precision) that balance performance, thermal output, power, and result quality, as shown in Table 1. In addition, LLM inference comprises distinct phases, each with unique performance, thermal, and power characteristics [49]."}, {"title": "Our work.", "content": "To address these challenges, we propose TAPAS, the first thermal- and power-aware scheduling scheme designed specifically for LLM inference clusters in the cloud. TAPAS maximizes cooling and power oversubscription while minimizing the impact on IaaS workloads and maintaining performance and accuracy for SaaS workloads. In addition, TAPAS dynamically adjusts LLM workloads in response to power or cooling failures in a datacenter. The result is substantially reduced cloud platform TCO.\nTAPAS gracefully handles occasional load spikes and emergency events (e.g., cooling or power failures) through three core ideas. First, it places GPU VMs in a thermal- and power-aware manner by leveraging historical data on temperature, power consumption, and the load of a given service. Second, it routes requests across LLM instances based on the load of individual VMs, as well as the temperature and power slacks of the underlying infrastructure. Third, it reconfigures SaaS instances within their hierarchy until the temperature or power is reduced below safe limits.\nResults. We evaluate TAPAS on a large GPU cluster using production traces from a major cloud provider. Our results show that TAPAS maintains the P99 tail latency of inference requests while reducing maximum temperature by 17% and peak row power by 23%. These reductions create more opportunities for oversubscription, enabling up to 40% additional capacity and, consequently, lowering datacenter TCO.\nTo validate our findings at scale, we use traces from hundreds of production racks across a subset of datacenters and simulate TAPAS. Compared to other practical policies, TAPAS reduces thermal and power throttling events by 97% and 99%, respectively. In addition, we demonstrate that TAPAS operates effectively during cooling and power failures.\nSummary. We make the following main contributions:\n\u2022 Characterization of thermal/power properties of GPU workloads and their behavior at production scale.\n\u2022 TAPAS, the first thermal- and power-aware scheduling scheme for LLM inference systems.\n\u2022 A thorough evaluation of TAPAS in a GPU cluster using large-scale production traces."}, {"title": "2 Characterizing Challenges in Thermal and Power Infrastructure for GPUs", "content": "To identify the challenges in managing cooling and power for GPU workloads, we characterize the datacenter infrastructure required to support these workloads. We focus on spatial and temporal heterogeneity in the usage of thermal and power infrastructure, that can be exploited to operate GPU workloads more efficiently. We introduce equations to help us model thermal and power aspects at datacenter scale.\nDatacenter overview. Cloud providers host a variety of services from multiple users on shared infrastructure. We study datacenters hosting both A100 [46] and H100 [45] GPUs,\nwhich are typically used for LLMs [49]. Servers in a datacenter are arranged in rows of racks. Due to the size and power density of GPUs, racks and rows host fewer servers than in general-purpose datacenters. These datacenters also host other infrastructure for storage, network, and management."}, {"title": "2.1 Cooling", "content": "Infrastructure. GPU servers generate a large amount of heat, while their temperature needs to stay below a specific threshold (e.g., 85\u00b0C for GPUs). On exceeding the threshold, the hardware starts throttling the computation to prevent failures and permanent damage [44].\nDepending on the regional climate, datacenters may use technologies like mechanical or adiabatic cooling to lower temperatures [15]. While other alternatives exist (e.g., liquid cooling [24]), we focus on air cooling as it is the most commonly used method in today's datacenters [15, 16, 23, 40]. Many of our insights can be applied to other technologies.\nDatacenters are usually arranged in aisles composed of two rows. Figure 1 illustrates an example of airflow within one of the rooms in one of the datacenters. The air handling units (AHUs) in each row blow cold air from the datacenter-level cooling devices (e.g., adiabatic cooling towers in evaporative cooling) into the cold aisle which is contained. The servers use fans with modulated speeds based on activity to draw cold air from the front, pass it through the server (including the GPUs), and exhaust the heated air into the hot aisle. The cooling devices then take this hot air and cool it down again. To avoid heat recirculation (i.e., hot air returning to the cold aisle), the airflow provided by the AHUs must exceed the airflow consumed by the servers in the cold aisle.\nProvisioning. Datacenter operators usually provision the cooling infrastructure to sustain their peak load [38]. This means they need to have (1) enough airflow in each aisle (i.e., AHUs) to prevent heat recirculation and (2) enough cooling capacity in the datacenter to lower the temperature within\noperating conditions. Operators can add racks to rows as long as both conditions are met.\nFailures. Datacenters typically build redundancy to handle failures (e.g., N+1 [83]). When a cooling device fails, other devices in the datacenter compensate. However, this results in reduced cooling capacity, which may increase the temperature across all cold aisles in the datacenter. If an AHU fails, the other AHUs in the aisle handle the airflow. Insufficient airflow from the AHUs in the aisle leads to heat recirculation, raising the temperature of all servers in the two rows.\nCharacterization. To understand the thermal impact of the cooling infrastructure, we study a sample of our datacenters at Azure containing tens of thousands of GPUs (including both A100 and H100) across three regions with varying climates. The study spans three months, from June to October 2024, covering the warmer months in these locations. We collect data on inlet and outlet temperatures for each server, the outside temperature, and the temperature and power of each component (e.g., GPU and memory), reporting the averages every 10 minutes. This 10-minute interval aligns with the frequency of all sensors and enables the approximation of heat from average power. While we discuss individual examples, our insights are derived from the full dataset.\nOutside temperature. Cooling technologies like adiabatic cooling [15, 16, 20, 23, 38, 40] use outside air when it is cold for efficiency. Figure 2 shows the inlet temperature for three servers in the same aisle and the outside temperature in August 2024. The inlet temperature follows the trend of the outside temperature. Figure 3 shows the inlet and outside temperature for these servers over three months. Each point represents the inlet temperature for Server 3 and the outside temperature every 10 minutes. The lines are a regression of these points for each of the servers. When it is cold outside, the cooling maintains the inlet temperature (e.g., over 18\u00b0C) to avoid increasing humidity which increases failures [20]. After 15\u00b0C outside, the inlet temperature increases linearly with the outside. When it is hot outside (i.e., 25\u00b0C), the cooling lowers the temperature further. Locations with higher temperatures are less sensitive to the outside temperature.\nDatacenter layout. Given the physical layout, the inlet temperature for each server is not homogeneous, and there are hotspots. Figures 2 and 3 show how Server 2 is consistently\nwarmer (~ 2\u00b0C) than the other two. Figure 1 shows that the median temperature across racks and rows varies: the end of some rows is warmer than others because of airflow and construction differences. These airflow patterns are hard to estimate beforehand and require measuring them empirically or expensive simulations. Figure 4 shows the median temperature over the three months depending on the physical entity. Some rows have temperatures up to 1\u00b0C higher than others, with racks within a row showing differences of up to 2\u00b0C. The height within the rack has a minor impact.\nDatacenter load. The amount of heat in the datacenter also affects the inlet temperature. Cooling devices usually lower the outlet temperature by AT (e.g., 10\u00b0C). When the heat generated by the servers increases, the inlet temperature also increases. Figure 5 shows the regression between datacenter load (average power for 10 minutes) and the difference in inlet temperature for one server in a hot region. For example, when it is 35\u00b0C outside, there is an inlet temperature difference of 2\u00b0C when the load is low and high. Note that the correlation with datacenter load is much lower than with inlet temperature. Using the three months of data, we apply regression to model the inlet temperature for each server s:\n$VseSTinlet,s = finlet,s (Toutside, LoadDC)$\nGPU temperature. Once the inlet air enters the server, the fans circulate the cold air to cool down the server components (e.g., GPUs and their memory). Figure 6 shows the GPU and memory temperature, along with the inlet and outlet temperature, and GPU power for an example server running tests for this work over 45 days. The GPU memory is warmer than the GPU and there is an offset between inlet and outlet."}, {"title": "GPU heterogeneity.", "content": "Figure 8 shows the temperatures of the 8 GPUs in a DGX A100 [46] server running the same workload. Despite identical inlet temperatures and GPU utilization at each point in time, the temperatures of individual GPUs can differ by up to 10\u00b0C. This variation is due to server layout (e.g., components obstructing airflow to certain GPUs) and manufacturing variations in the GPUs themselves (i.e., process variation [2]). When the temperature within the server is high, the fans will increase airflow to cool the GPUs. It is important to account for this when provisioning the AHUs.\nFigure 9 shows the heterogeneity in temperature across GPUs in one datacenter with high GPU load and similar inlet\ntemperatures. There is a range of over 20\u00b0C across GPUs in the same datacenter. The temperature of the GPU memory is slightly lower than the GPU itself. The right side of Figure 9 shows the median temperature for each of the 8 GPUs in the server and their inter-quartile range. The GPUs with even identifiers (e.g., GPU2 and GPU4) have a lower temperature due to the server layout, as they are closer to the inlet [65]. Using the data for the three months, we generate a model for the temperature of each GPU g in each server s:\n$VSES,gEGTGPUs.g = fGPUs,g(Tinlet,s, LoadGPU,9)$\nAirflow. We measure the speed of the server fans when the server is idle and when it is running at full load (i.e., all GPUs running heavy workloads). Then we run a few intermediate settings and interpolate a linear function. Our measurements match the manufacturer specs, which indicate an airflow of 840 and 1105 cubic feet per minute (CFM) at 80% speed with pulse width modulation (PWM) fans for A100 and H100 respectively [45, 46]. All servers follow a similar linear function with very small differences: $fair (LoadGPU,s)$. As mentioned, we need to guarantee the provisioned airflow from the AHU is larger than the aggregated server airflow requirement:\n$\u2713 Aisle EDC \u2211 fair (LoadGPU,s) \u2264 ProvAHUAisle$\nInsight #1: For effective thermal management, cloud operators must consider temporal heterogeneity due to variations in outside temperature and load, spatial heterogeneity related to datacenter and server layouts, and airflow requirements."}, {"title": "2.2 Power", "content": "Infrastructure. Datacenters typically implement a three-level power distribution hierarchy to deliver electricity from the utility grid to individual servers [74, 79, 83]. At the highest level, an Automatic Transmission Switch (ATS) directs power from the grid to multiple Uninterruptible Power Supplies (UPS). Each UPS shares a fraction of the total datacenter power load and is connected to a series of Power Distribution Unit (PDU) pairs. These PDU pairs further step down the voltage and support multiple rows of server racks.\nProvisioning. To prevent tripping the circuit breakers, datacenter operators provision for peak power usage at each level of the hierarchy to account for worst-case scenarios. For safety, when the total power draw exceeds the power supply, servers within that level are power-capped. For design simplicity and to reduce implementation costs, these power limits are further distributed down the hierarchy homogeneously, eventually limiting the number of server racks that can be provisioned within the budget. Operators can oversubscribe the power capacity by adding racks to a row, as long as they remain within the row-level power envelope.\nFailures. To ensure high availability, clouds implement redundancy at each power hierarchy level. For instance, prior work describes a setup with 4N/3 redundancy at the UPS level and 2N at the PDU level [83]. When a UPS fails, its load is redistributed among the remaining three units. Under heavy load, this can push the others over capacity, requiring each unit to quickly reduce its load to maintain limits, effectively lowering the datacenter capacity to 75%. We focus on this design as its balances normal and fail-over operation. Our findings extend to other redundancy models [34, 55].\nCharacterization. Prior works characterize the power profile of GPU servers running LLMs [48, 49]. We complement these studies with our own data focusing on power imbalances at datacenter scale. We study the same datacenters as in the cooling section. For confidentiality, we normalize all values to the maximum power draw.\nGPU load and server power. We measure server power for both A100 and H100 servers across various utilization levels and workloads (e.g., Figure 6). Server power is strongly correlated with GPU load [48, 65]. Even when idle, servers consume significant power, similar to traditional CPU servers [39]. Besides GPU power, a substantial portion is drawn by fans, storage, memory, CPUs, and other components [48]. For each server s, we used polynomial regression to generate a function $fpower (LoadGPU 9,s)$, which accounts for fan power and other components that also depend on load.\nPower imbalance across rows. Row power utilization is the aggregation and multiplexing of individual server power. Figure 10 shows the power draw of four sample rows in a datacenter over a week. While most rows exhibit lower power draw, a few have significantly higher consumption.\nInsight #2: The power demands of GPU clusters present a strong opportunity for power oversubscription. However, to safely oversubscribe, the infrastructure must effectively manage the rows at the tail end that generate hotspots."}, {"title": "3 Characterizing Opportunities in Thermal and Power Properties of GPU Workloads", "content": "To reason about the impact of GPU workloads on thermal and power properties in cloud datacenters, we (1) analyze the physical placement of GPU workloads at Azure, (2) profile SaaS LLM inference workloads from a production environment, and (3) characterize the thermal and power properties of LLM inference varying a set of configurations with open-source models [72]."}, {"title": "3.1 GPU workload placement", "content": "Cooling and power impact. To analyze the impact of GPU workloads on cooling and power infrastructure, we deploy 80 VMs across two rows in a datacenter (Section 2) and generate 100K random placements across these two rows. Figure 11 illustrates the distribution of peak server temperatures and row power with these placements. The worst-case placement results in a maximum temperature exceeding 85\u00b0C, while a typical placement averages around 72\u00b0C. In terms of peak power, the worst-case placement increases peak power by 27% over the optimal placement. If more intensive workloads are placed on hotter servers or if synchronous peak loads are co-located on the same row, the provider must provision sufficient cooling and power to support these extreme scenarios. Additionally, Figure 11b shows no correlation between maximum temperature and peak power for VM placements, indicating that cloud providers should consider both thermal and power factors when placing VMs across the datacenter.\nLong-lived VMs. Figure 12a shows the lifetime for VMs running GPU workloads. Most VMs are long-lived (e.g., over 60% run for more than two weeks). Since these VMs occupy a full server, this implies that a given server may be dedicated to a workload for an extended period. Figure 13a shows an example VM over a 4-week period with a distinctly periodic diurnal load pattern. Aggregated at row level (Figure 13b), the power consumption also shows periodic pattern.\nPredictable load. Figure 14a shows that using different power templates[68], row power prediction based on past history has less than 10% error for most row hours. A conservative prediction using P99 underestimates the power for less than 4% of the rowxhours. For VM power prediction, cloud providers can leverage customer information, as shown in Figure 14b, with errors below 10% for more than 75% VMxhours and underpredictions between 2-7% for P90 and P99 templates. These further demonstrate the predictability of row- and VM-level power consumption.\nInsight #3: Cloud operators can leverage workload heterogeneity and predictability for intelligent workload placement to relieve hotspots and smooth out thermal/power spikes."}, {"title": "3.2 LLM inference routing", "content": "IaaS vs SaaS. Azure hosts a variety of GPU workloads, which we categorize categorized as either IaaS or SaaS. IaaS uses opaque VMs [6, 13, 58, 59] and customers can run any workload (e.g., inference, training, fine-tuning) for any model (e.g., LLM, diffusion, image recognition). On the other hand, SaaS is fully managed by the cloud provider [3, 5, 60, 61]. In all the datacenters across regions, there is a significant portion of SaaS VMs. This allows for more flexible thermal and power shaping in the datacenter.\nSaaS workloads. The SaaS offering in our A100 and H100 datacenters serves multiple LLM inference endpoints, each hosting various LLMs for different applications [3, 57, 60, 70]. Each endpoint operates a dedicated set of VMs, which can host multiple LLM inference instances, routing user inference requests across these instances. Figure 12b shows the distribution of VMs serving requests per SaaS inference endpoint: 50% of the VMs belong to large endpoints with over 100 VMs spanning across multiple rows.\nLoad balancing. Our SaaS implementation distributes LLM inference requests across VMs to enhance latency and throughput [1, 30, 82]. However, these VMs may be placed in rows with different thermal and power characteristics (e.g., Figure 10). If the Load Balancer is unaware of temperature conditions, it may assign equal workloads to servers that are already near thermal throttling. Similarly, disregarding power conditions could result in sending additional load to VMs in rows with high power demand from neighboring IaaS servers, exacerbating power strain."}, {"title": "3.3 LLM inference instance configuration", "content": "As outlined in Table 1, LLM inference servers have various configuration options that balance thermal/power requirements, performance, and result quality. To evaluate these options, we run Llama2 [72] inference workloads on an NVIDIA DGX A100 server [46]. LLM inference consists of two distinct phases [49]: the prefill phase, which processes the entire prompt in parallel, and the decode phase, which generates each output token sequentially.\nImpact of configuration parameters. In Figure 15, we quantify the impact of each parameter individually: GPU frequency, parallelism, batch size, model size, and quantization. The red line represents the maximum temperature and power (TDP) while the blue line shows the idle.\nGPU frequency. Lowering the frequency reduces both temperature and power of the GPU. Prompt phases are more sensitive to GPU frequency [48, 67]. Although reducing the frequency has lower impact on temperature and power than other configuration parameters, it does not affect the quality of results and can be applied instantaneously due to its relatively low overhead.\nModel parallelism. We focus on tensor parallelism [63] because other parallelisms like data and pipeline parallelism are not as effective for LLM inference within a single server [49]. Figure 15a shows the temperature and power of a server running prompt and decode phases varying tensor parallelisms: TP8, TP4, and TP2 [63] (i.e., powers of two with the number of KV heads [72]). With TP2, the total server power reduces as we use less GPUs. However, as the same amount of work is concentrated in fewer GPUs, the per-GPU power increases. Hence, the temperature of the hottest GPU increases. Lower parallelism impacts power more significantly during prompt phase and temperature during decode phase. Thus, depending on the workload's phase and the target metric (i.e., reducing temperature or power), the system needs to take different actions.\nBatch size. Figure 15b shows the temperature and power with different batch sizes: 64, 16 and 1 [81]. Due to the reduced computational intensity, temperature and power reduce. However, during decode phase, as GPUs need to more frequently fetch the data from memory via the memory controller (instead of bulk transfers), the memory temperature increases. Thus, depending on the bottleneck (temperature or power) and the workload's phase (prompt or decode), the system may decide to choose different batch sizes.\nModel size. Figure 15c shows the power consumption and temperature associated with different Llama2 [72] model sizes: 70B, 13B, and 7B. As the model size decreases, the computational intensity of inference reduces significantly, resulting in lower power draw and temperature. However, smaller models tend to produce results of lower quality [37]. For example, the 7B model reduces result quality by 30-40% compared to the 70B model [7, 72].\nModel quantization. We observe a similar behavior with quantized model: lower precision leads to reduced temperature and power while incurring 2-20% accuracy impacts [7, 33, 37]. Because both smaller and quantized models generally lead to reduced quality, the system must carefully manage the proportion of the load directed to different model variants to uphold average per-service quality SLOs.\nThermal and power space. We quantify the performance of an LLM inference server in terms of its goodput (i.e., the number of tokens processed per second while staying within TTFT and TBT SLOs, defined as 5x the execution time on an unloaded system [36, 85]). Figure 16 illustrates the trade-off between temperature/power and goodput across all configurations (i.e., GPU frequency, parallelism, batch size, model size, and quantization). The figure highlights the impact of the model size as it affects quality. Each model has a Pareto frontier representing configurations that minimize temperature and power with minimal impact on performance.\nInsight #5: Cloud operators can effectively shape thermal and power while minimally impacting LLM inference performance and results quality by configuring SaaS instances."}, {"title": "4 TAPAS Design", "content": "Architecture. Based on our insights, we propose TAPAS, a framework for thermal and power management in GPU clusters for cloud environments. TAPAS is specifically designed to address the unique properties and challenges of LLM inference workloads. TAPAS enhances cooling and power oversubscription capabilities while effectively managing infrastructure failures, thereby reducing datacenter TCO with minimal impact on workload performance or result quality. The system leverages the adaptability of SaaS workloads without impacting IaaS workloads.\nFigure 17 overviews the architecture. TAPAS (1) extends existing components of conventional cloud LLM inference clusters (e.g., per-cluster VM Allocator and per-endpoint Load Balancer), (2) introduces a per-SaaS VM Instance Configurator, and (3) maintains multiple Profiles. To achieve its goals, TAPAS focuses on three core aspects: VM placement, LLM inference request routing, and instance configuration."}, {"title": "4.1 Workload placement", "content": "As VMs arrive, the VM Allocator assigns each new VM (whether IaaS or SaaS) to a server, aiming to meet workload demands while minimizing the risk of thermal or power hotspots.\nAisle and row filtering. Using historical server load data, we predict peak the airflow requirements for each aisle and peak power demand for each row. If data is insufficient for a server (i.e., less than one week), we assume peak load conditions. We then estimate the load of the new VM based on the load from VMs associated with (1) the same user for IaaS workloads and (2) the same endpoint for SaaS workloads. Again, we assume peak load if historical data is insufficient. We estimate the new peak airflow for each aisle and power demand for each row if the VM were to be placed, and we filter out servers in aisles or rows that would exceed airflow or power limits (Equations (3) and (4)).\nPlacing hotter IaaS VMs in cooler servers. As fine-grained control over IaaS VMs is limited, we aim to place these VMs in cooler servers. For each server, we feed the historical server inlet temperature and the predicted VM load into Equation (2) to estimate peak GPU temperature. We then select the servers with the lowest projected temperatures. We attempt to place SaaS VMs in warmer servers, ensuring that the maximum GPU temperature constraints will not violated based on the predicted load for that endpoint.\nBalancing IaaS and SaaS. To enable SaaS workloads to balance airflow and power to reduce peaks, it is important to achieve a good balance of IaaS and SaaS workloads within each aisle and row. We aim to place new VMs in an aisle and row that will not result in an excessive concentration of either IaaS or SaaS VMs.\nMigration. Beyond initial VM placement, we can recalculate better placements and migrate VMs to address mispredictions or changes in workload behavior. For SaaS workloads, we can create a new VM, transfer the workload, and then decommission the old VM. However, for IaaS VMs, migration must be seamless and non-disruptive [12]. Currently, live migration of GPU VMs is unsupported due to the complexities of GPU memory management, but this capability would enhance performance if implemented."}, {"title": "4.2 Request Routing", "content": "Once we place workloads in servers, TAPAS leverages multi-instance SaaS endpoints to further smooth temperature and power draw through finely routing LLM inference requests. We consider three constraint levels: aisle, row, and server.\nAisle. For each aisle, TAPAS estimates the load on each server and calculates the total airflow demand for the VMs in that aisle. This information is cached and recalculated every 5 minutes, updating whenever discrepancies are detected (e.g., if a server's power consumption is higher than estimated).\nTAPAS then prevents routing requests to VMs that could trigger an airflow violation.\nRow. Similarly to aisles, TAPAS estimates the load for all servers in a row and aggregates these into a total power value. It then assesses whether routing requests to VMs in that row would risk exceeding the peak power limit.\nServer. TAPAS tracks the current load and, using Equation (2), estimates whether the GPU temperature for each server will exceed the threshold. It avoids routing requests to VMs on servers with a high risk of overheating."}, {"title": "4.3 Instance configuration", "content": "To ensure servers remain within cooling and power limits during load spikes or emergency events, TAPAS calculates the maximum allowable airflow, GPU temperature, and server power for each instance. Based on these limits, the Instance Configurator uses the thermal and power profiles of the LLM from Figure 16 to determine the optimal GPU frequency, batch size, model parallelism, quantization, and model size that maximize goodput without quality impact.\nWe also account for the time required to reassign these settings and prevent requests from being sent to instances during transitions [69]. For example, changing the model parallelism, size, or quantization level requires reloading the model, which can take a few seconds. Given these overheads and the goal of maximizing quality, adjustments to model quantization and size are typically the last resort."}, {"title": "4.4 Oversubscription and failures", "content": "Oversubscription. Using TAPAS, we can reduce the cooling and power requirements needed to run the same workload. When the cloud provider initially provisions cooling and power for the datacenter, it typically plans for peak baseline demand. As demand increases, the cloud operator can add more racks to the existing rows, utilizing the slack created by TAPAS. Additionally, our TAPAS simulator can be used with an estimated workload to assess cooling and power requirements, enabling more precise provisioning.\nFailure management. If there is a cooling or power failure, TAPAS recalculates the new available airflow for each aisle, the power for each row, and the inlet temperature for each server. Based on this, the request routing will steer requests away from constrained servers. In addition, the Instance Configurator will decrease the load accordingly. If all these actions are not enough, TAPAS applies regular power capping techniques to the IaaS VMs [48]."}, {"title": "4.5 Implementation", "content": "Profiles. TAPAS includes an offline profiling phase that takes place during the initial stages of datacenter deployment, when operators run benchmarks and validation tests. This phase models: (1) the datacenter layout, (2) the inlet temperature of each server, (3) the temperature of each GPU,\n(4) the server fan airflow, and (5) the power-load for the servers. In addition, when the provider onboards a new LLM, TAPAS profiles the impact of each configuration parameter in that hardware, following the process described in Section 3. During regular datacenter operation, TAPAS refines these models on a weekly basis. During this weekly update, TAPAS collects power and load patterns for each server and row to predict their utilization. We use a template-based approach that leverages data from the previous week [68]. We make these models and profiles available to the other three main components through regular data updates.\nVM Allocator. We implement our workload placement policies in a rule-based VM Allocator, inspired by Protean [21], using three main rules: (1) a validator rule filters aisles and rows based on peak airflow and power; (2) a preference rule directs IaaS workloads to cooler servers and SaaS workloads to warmer servers. For this rule, we categorize servers into three equally sized groups: cold, medium, and warm; and (3) another preference rule guides VM placement based on the IaaS/SaaS balance, grouping servers into three categories: IaaS-heavy, SaaS-heavy, and balanced. These rules use current cluster data and the weekly updated models and profiles. Finally, our VM Allocator applies these rules to select the final server to host the GPU VM.\nLoad Balancer. We deploy the SaaS endpoints on a dedicated set of VMs that expose an HTTP REST interface. These VMs implement the Load Balancer, which forwards LLM inference requests to the appropriate VM within the endpoint. For each VM, TAPAS evaluates the current VM state along with the server's thermal and power profiles to calculate the probability of violating any of the three operational limits (i.e., thermal, power, and performance). Requests are not routed to VMs with a high risk of violation.\nAfter the filtering step, TAPAS applies state-of-the-art load balancing policies in the following order: (1) route requests to instances that have previously handled requests from the same customer to maximize KV cache reuse [17, 66, 77]; (2) concentrate load to reduce energy consumption [69]; and (3) distribute requests across VMs to optimize performance.\nInstance Configurator. To run the local LLM instances, we use vLLM [27], a state-of-the-art LLM inference platform. Note that TAPAS can also integrate other platforms (e.g., TensorRT-LLM [47]) with only minor interface modifications. The LLM inference engine provides an HTTP REST API that receives requests from the Load Balancer.\nThe local TAPAS controller receives the updated thermal and power profiles for that server on a weekly basis. This controller runs for every LLM iteration to estimate the optimal operational settings (including GPU frequency, batch size, model parallelism, model size, and quantization). These computations are lightweight and cached for efficiency. If needed, the controller updates the settings for each instance running\non the VM. It also restarts the LLM instance if changes to model parallelism, model size, or quantization are necessary."}, {}]}