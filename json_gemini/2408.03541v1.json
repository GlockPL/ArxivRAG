{"title": "EXAONE 3.0 7.8B Instruction Tuned Language Model", "authors": ["LG AI Research"], "abstract": "We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning. With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct.", "sections": [{"title": "Introduction", "content": "EXAONE stands for Expert AI for EveryONE, a vision that LG is committed to realizing in order to democratize access to expert-level artificial intelligence capabilities. Our objective of Expert AI is twofold: to help the general public achieve expert-level competency in various fields and to assist experts in attaining even higher levels of proficiency. This aligns with LG AI Research's mission to integrate advanced AI into everyday life, making expert knowledge and capabilities accessible to a broader audience.\nIn August 2024, LG has announced the release of EXAONE 3.0 models with enhanced performance and equipped with the Enterprise AI Agent service enabled by the models. EXAONE 3.0 models will be supplied for commercial purposes, mainly to LG affiliates and partners as before, but among them, the 7.8B instruction-tuned model is made publicly available for non-commercial, research purposes. This release aims to support the broader AI community by providing access to a high-performance language model, thereby fostering innovation and collaboration. This technical report covers the performance of EXAONE 3.0's 7.8B instruction-tuned model which is competitive in English and excellent in Korean compared to other similar-sized recently-released large language models (LLMs)."}, {"title": "Model Training", "content": "In this section, we provide an overview of the model training process for EXAONE 3.0, which encompasses several critical stages, including the detailed architecture design, efficient tokenization for bilingual support, extensive pre- training on a diverse dataset, and advanced post-training techniques to enhance instruction-following capabilities. These steps ensure the model's robust performance in real-world scenarios and adherence to strict data compliance standards."}, {"title": "Model Architecture", "content": "In line with recent trends, EXAONE language model is based on the decoder-only transformer architecture [39]. Its maximum context length is 4,096 tokens, and it uses Rotary Position Embeddings (RoPE) [36] and Grouped Query Attention (GQA) [2]. The model architecture is shown in detail in Table 1."}, {"title": "Tokenizer", "content": "The design choices for a tokenizer has a significant impact on the efficiency of training and generation. It is essential to take into account the supporting languages in order to ensure optimal performance. EXAONE 3.0 7.8B is a bilingual model to support two languages: English and Korean. Due to the heterogeneous linguistic features of the two, we especially considered the agglutinative feature in Korean to pre-tokenize Korean corpora using MeCab [21]. Then, we trained on BBPE (byte-level byte-pair encoding) tokenizer [40] from scratch with a vocabulary size of 102,400. It results in a similar compression ratio in English but a lower compression ratio in Korean over existing tokenizers as in Table 2. A lower compression ratio indicates that the tokenizer generates fewer tokens per word, which can be beneficial as it reduces the likelihood of over-tokenization. This is particularly important for Korean language due to its agglutinative structure, where words can be formed by combining multiple morphemes, thus leading to improved model performance and generation."}, {"title": "Pre-training", "content": "There has been a trend in pre-training to utilize trillions of tokens (Table 3) far beyond the data-optimal scaling laws [18]. Furthermore, the importance of data quality becomes more significant in cost-effective training [13, 43]. Following the trend, several researchers put efforts to make the large amount of web-crawled data accessible at hands and study the behavior of the model by controlling the quality and diversity of the data [29, 30].\nIn order to create the best-fit data and training regime to train EXAONE language models from scratch, we made sure to enhance the overall quality of the data, acknowledged the potential legal issues, and set an adequate curation strategy to boost expert knowledge.\nData Processing To construct the data pool at first, we collected a comprehensive combination of large-scale web-crawled, publicly-available, and internally-constructed corpora. Then, we applied the de facto standard which includes rule-based filtering, machine learning based filtering, URL-based filtering, fuzzy deduplication, and removal of personally identifiable information (PII) to our pool. We not only adhered to the established methods but also implemented data-specific processing strategies to increase the depth of knowledge. In addition to handling the data quality, we excluded the data sources that posed potential legal risks. The detailed information is described in Section 2.6."}, {"title": "Post-training", "content": "To improve the instruction-following ability of EXAONE language models, we performed a two-stage post-training: supervised fine-tuning (SFT) [41] and direct preference optimization (DPO) [31].\nAs a first stage, creating high-quality instruction tuning data is crucial for performance as it helps the model generalize to new tasks. However, challenges arise from the difficulty in gathering sufficiently good-quality data. To address this, we developed a broad spectrum of instruction types to enhance diversity and coverage. To cover a broad range of service-oriented instructions, we defined various topics and instructional functionalities. Using the definitions, we created multi-turn datasets that are diverse and closely mimic authentic user interactions, providing a realistic reflection of genuine user experiences as in Table 4.\nThe second stage is to align the model with human preferences using human feedback, which is known as Direct Preference Optimization (DPO). Language models were trained to maximize differences in reward between chosen and rejected responses in preference datasets. There are two methods in DPO: offline DPO and online DPO, and we applied them in sequence. The offline DPO is a technique for training models using pre-built preference data, as shown in Table 5. On the other hand, the online DPO configures prompts to have data distributions similar to those learned through the offline DPO, enables the model to generate responses, evaluates them against preferences using reward models, labels responses to chosen or rejected, and uses the results for training again."}, {"title": "Training Costs", "content": "EXAONE language models were trained using Google Cloud Platform and a cluster powered by NVIDIA H100 GPUs and NVIDIA NeMo Framework. Then, they were optimized by NVIDIA TensorRT-LLM. The total amount of computation used for model training was about 4 \u00d7 1023 FLOPS."}, {"title": "Data Compliance", "content": "AI model development requires a large amount of data, and the acquisition and utilization of this data can lead to various legal issues, such as copyright infringement, intellectual property infringement, and personal information protection violations. If these issues are ignored or addressed inadequately, it can have a significant impact on the company, as well as the general users and businesses that use the AI model.\nTo minimize these risks, LG AI Research conducts AI Compliance reviews throughout the entire process of data collection, AI model training, and information provision. The team responsible for data collection uses a checklist to identify potential problems before they occur. If a problem arises, the relevant department is consulted. When acquiring data through ownership or licensing agreements, the relevant team negotiates with the data owner and, if necessary, consults with legal professionals to ensure proper data acquisition or licensing.\nEach training dataset is subjected to a licensing review process. After this review, the AI model is trained using the approved data. Subsequently, a data risk assessment is conducted to establish the criteria for the AI model's distribution.\nThe language model, developed pursuant to this robust compliance system, distinctly omits legally precarious data such as news articles and books."}, {"title": "Evaluation", "content": "EXAONE 3.0 7.8B is a bilingual model trained mainly on English and Korean. To evaluate performance in English and Korean, well-known public benchmark datasets and in-house benchmark datasets were used. See Table 18 in Appendix for more details on the benchmark datasets and the methods used to evaluate the models with them.\nThe results of model's English and Korean performance against the benchmarks summarized in Table 6. The models used for performance comparison are the latest models of similar size that support both English and Korean, for which we obtained all the performance data by measuring performance ourselves. There are some differences between the performance results that we measured and the reported numbers, but most of them did not show significant differences."}, {"title": "English Capability", "content": "The results of this performance comparison show that our model has a competitive overall performance in English against the comparison models."}, {"title": "Real-world Use Cases", "content": "EXAONE aims to be an Expert AI, so achieving comprehensive performance in real-world use cases is crucial. However, evaluating comprehensive performance through benchmarks that only measure single tasks has its limitations. Often, there's a discrepancy between responses perceived as satisfactory by users and the actual benchmark scores. Therefore, LMSYS Chatbot Arena [26], which reflects actual human evaluations, has gained attention. To verify performance in real-world use cases, we measured four benchmarks that have a high correlation with LMSYS Chatbot Arena as in Table 7. Like well-known stylistic preference for longer responses (a.k.a. verbosity bias) in MT-bench [45], each benchmark inherently exhibits certain biases by design. Therefore, we advocate using multiple benchmarks to ensure comprehensive and accurate real-world evaluations.\nBased on Table 7, EXAONE 3.0 7.8B instruction-tuned model demonstrates significantly better performance compared to other models on MT-Bench, one of the benchmarks prominently featured in LMSYS Chatbot Arena. Specifically, the MT-Bench score of 9.01 is remarkably high. In the Arena-hard-auto full leaderboard [4], only models with at least 70B parameters have achieved a score of 46.8 or higher as of today. The WildBench score of 48.2 is also the highest among models with less than 10B parameters. Lastly, the AlpacaEval 2.0 LC benchmark score of 45.0 surpasses the GPT-4-0314 model's score of 35.3, as listed on the leaderboard [3]. Overall, as evidenced by the average scores, our model outperforms other similar-sized open models in real-world use cases."}, {"title": "Math", "content": "To assess performance in math capabilities, we measured two benchmarks: GSM8K and MATH. GSM8K is used to measure grade school math word problems, and MATH is used to measure challenging competition mathematics problems. As shown in Table 8, EXAONE 3.0 7.8B instruction-tuned model performed well on both benchmarks, and as evidenced by the average scores, it demonstrates superior math capability compared to other models."}, {"title": "Coding", "content": "To evaluate coding capabilities, we measured the performance on popular benchmarks for Python code generation, focusing on relatively simple, self-contained functions. HumanEval measures functional correctness for synthesizing Python programs from docstrings, and MBPP (The Mostly Basic Programming Problems) measures models' ability to synthesize short Python programs. As shown in Table 9, EXAONE 3.0 7.8B instruction-tuned model's performance in HumanEval stands out compared to other models, and it also shows competitive performance in MBPP. Consequently, the average scores indicate that our model demonstrated superior coding capability compared to other models."}, {"title": "Reasoning", "content": "To evaluate the reasoning capability, we measured two benchmarks: ARC-C (AI2 Reasoning Challenge - Challenge Set) and GPQA (General-Purpose Question Answering) as in Table 10. ARC-C focuses on the model's higher-order reasoning capabilities, particularly in solving challenging science exam questions that require the application of scientific knowledge and logical thinking. GPQA assesses the model's ability to answer a wide range of questions across various domains, testing the breadth and accuracy of its knowledge. Together, these benchmarks provide a comprehensive assessment of the models' performance in both complex reasoning tasks and general knowledge.\nBased on Table 10, EXAONE 3.0 7.8B instruction-tuned model ranks third in performance on both benchmarks."}, {"title": "General", "content": "Due to recent issues with benchmark contamination, the reliability of evaluation scores from traditional benchmarks has decreased. To address this problem, Open LLM Leaderboard 2 [14] was released. It includes IFEval (Instruction Following Evaluation), BBH (Big-Bench Hard), MATH Level 5, GPQA (Google-Proof QA), MuSR (Multistep Soft Reasoning), and MMLU-Pro. These benchmarks are designed to test models on complex reasoning, long-range context parsing, and instruction-following abilities, providing a more rigorous evaluation than traditional benchmarks."}, {"title": "Korean Capability", "content": "To measure the model's general capability, we adopted the Open LLM Leaderboard 2 for comparative evaluation. As shown in Table 11, EXAONE 3.0 7.8B instruction-tuned model demonstrated competitive general capability compared to other models."}, {"title": "Real-world Use Cases", "content": "To evaluate the comprehensive performance of models, similar to real-world use cases in Section 3.1.1, we selected two Korean benchmarks: KoMT-Bench and LogicKor. KoMT-Bench is an in-house dataset created by translating the MT-Bench dataset into Korean and modifying the content to reflect the characteristics and cultural nuances of the Korean language. Examples are shown in Table 19 in Appendix. The categories and number of questions are identical to those of the original MT-Bench dataset. LogicKor is a similar benchmark to MT-Bench, consisting of 42 multi-turn prompts across six categories (reasoning, mathematics, writing, coding, comprehension, and Korean language).\nAs shown in Table 12, EXAONE 3.0 7.8B instruction-tuned model surpassed the comparison models in both benchmarks. In this experiment, we found that, even when responses in the KoMT-Bench were generated in a language other than Korean, GPT-4-0613, acting as the judge, continued to award high scores. To handle such cases, we adopt a square root penalty which applies the square root to the score of non-Korean responses in order to adjust for this discrepancy5."}, {"title": "General", "content": "To conduct a comprehensive evaluation, we utilized public Korean benchmarks as given in Table 13. In accordance with the English general benchmarks in Section 3.1.5, we adopted similar benchmarks KMMLU [34] and KoBEST [20]. Furthermore, we included Korean subset of Belebele [6] benchmark which is a multiple-choice multilingual machine reading comprehension benchmark. The overall results demonstrate that our model outperformed other models on most benchmarks."}, {"title": "Responsible AI", "content": "We follow the LG AI Ethics Principles [23] to ensure the responsible development and deployment of the EXAONE 3.0 7.8B. Considering the model's capabilities, we assessed potential social and ethical issues and identified solutions to address them. We focus on improving the model's safety and maintaining high ethical standards throughout the development process."}, {"title": "Benefits", "content": "EXAONE 3.0 7.8B is the open model designed to offer robust performance in bilingual environments, with particular strength in Korean. We believe this broad access to our model can open new avenues for researchers and developers within the AI community. This accessibility encourages innovation and collaboration, enabling users to explore a wide range of application possibilities.\nOne of the key benefits of EXAONE 3.0 7.8B is its advanced capabilities allow for comprehensive instruction fine-tuning, which supports a wide range of developer needs. This flexibility enhances the model's utility in creating specialized applications. These applications can be tailored to suit various industries and domains, making EXAONE 3.0 7.8B a valuable tool in diverse professional settings.\nHowever, with the release of this model, we emphasize the importance of responsible use to prevent malicious activities. By doing so, we aim to foster a safe and innovative AI research environment, thereby making a positive contribution to the global AI community."}, {"title": "Risks and Mitigations", "content": "Open model brings significant benefits to the AI community. However, we are aware that they also come with significant challenges for responsible deployment such as malicious misuse, unintended outcomes like discriminatory bias, and harmful content. Through AI Ethical Impact Assessment, we identified several risks and improved the model's safety.\nA primary concern is malicious misuse from by bad actors. Open access to model weights allows anyone to fine-tune and deploy the model without substantial oversight. This accessibility increases the risk of misuse, such as generating misinformation and disinformation, influencing public opinion, and enabling scams or phishing attempts, similar to risks associated with prior language models [38, 42]. While it is challenging to completely prevent misuse, a combination of technical constraints and educating developers and end-users can mitigate these risks. Additionally, users are encouraged to report misuse and adhere strictly to the ethical and safe use guidelines outlined in the model license agreement restrictions (see Appendix 8.3). These restrictions are designed to ensure responsible use of the model.\nThe varied downstream use of this model raises key concerns. When deployed across different industries and user groups, the model interacts with varied data types, user inputs, and operational contexts. This diversity can lead to unexpected model behavior, such as generating inaccurate, inappropriate, or harmful outputs. Despite extensive training on large datasets with careful selection, discriminatory biases or unsafe content may still exist within the model. These outputs can result not only from the complexity of the tasks but also from differences in cultural norms, legal standards, and ethical expectations across user groups. We recommend continuous monitoring of the model's responses to identify and address these issues effectively.\nOur comprehensive strategy for mitigating the risks associated with open model deployment encompasses technical safeguards, educational initiatives, usage monitoring, and legal restrictions. Furthermore, to anticipate and prepare for potential hazards, we conduct regular assessments using both internal and external red teaming. The outcomes of these red teaming activities are documented below, underscoring LG AI Research's commitment to continuous research and development in a safe and responsible way."}, {"title": "Red Teaming", "content": "We have conducted comprehensive evaluations of EXAONE 3.0 7.8B to assess the ethics and security using both in-house and third-party datasets. Ethical evaluations focus on detecting hate, bias, and illegal content, while security assessments address potential information hazards, such as the use of private data in training.\nThe internal evaluation is executed by an in-house team and tested using carefully crafted question-answer datasets, designed to cover a wide range of unethical and insecure scenarios. Team members labeled the system's responses as either \"Pass\u201d or \u201cFail\u201d, providing reasons for their assessments, and labeled as \"Skipped\" if the language model provided off-topic responses. The results are presented in Table 14.\nAbout 10% of the test cases failed, likely because EXAONE 3.0 7.8B is tuned to increase the helpfulness of its responses, i.e., the model is trained to provide the user with as much information as possible. Thus, the model did not refuse to answer and generated inappropriate responses at times. Please refer to Table 15 for some examples."}, {"title": "Limitations", "content": "EXAONE 3.0 7.8B, like all existing language models, has certain limitations and may occasionally generate inap- propriate responses. The language model generates responses based on the output probability of tokens, and it is determined during learning from training data. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses. Please note that the text generated by EXAONE language model does not reflect the views of LG AI Research.\n\u2022 Inappropriate answers may be generated, which contain personal, harmful or other inappropriate information.\n\u2022 Biased responses may be generated, which are associated with age, gender, race, and so on.\n\u2022 The generated responses rely heavily on statistics from the training data, which can result in the generation of semantically or syntactically incorrect sentences.\n\u2022 Since the model does not reflect the latest information, the responses may be false or contradictory.\nLG AI Research strives to reduce potential risks that may arise from EXAONE language model. Users are not allowed to engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of inappropriate outputs violating LG AI's ethical principles when using EXAONE language model."}, {"title": "Deployment", "content": "Section 8.3 in Appendix provides license information for using the EXAONE 3.0 7.8B. Understanding the license information is essential for the legal utilization of the language model."}, {"title": "Conclusion", "content": "In this technical report, we premiered EXAONE 3.0 7.8B instruction-tuned language model, our first open LLM in EXAONE model family. Demonstrating its excellence in Korean and competency in English among models of comparable size, we expect that stellar performance across real-world scenarios facilitates diverse open innovations. For notable instance, this model serves foundations for our enterprise AI agent that optimizes business workflow, boosting both efficiency and productivity.\nWhile we are currently releasing the cost-effective EXAONE 3.0 7.8B instruction-tuned model exclusively for non- commercial and research purposes, we are optimistic that witnessing diverse applications of the 7.8B will further open access to additional models in the future."}, {"title": "Contributors", "content": "All authors are listed in alphabetical order by last name.\nCore Contributors Eunbi Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen,\nHyeongu Yun\nContributors Soyoung An, Kyunghoon Bae, Stanley Jungkyu Choi, Yemuk Choi, Yeonjung Hong, Gerrard Jeongwon\nJo, Jiyeon Jung, Yountae Jung, Euisoon Kim, Hyosang Kim, Youchul Kim, Edward Hwayoung Lee, Honglak Lee,\nMoontae Lee, Seungjun Lee, Woohyung Lim, Sooyoun Park, Yongmin Park, Boseong Seo, Sihoon Yang, Kyungjae Yoo"}, {"title": "Benchmarks", "content": "By default, the chat templates were not used in benchmark tests, but for benchmarks that require the instruction- following capability (marked with an asterisk), we used the chat templates. In these cases, we applied the chat template that includes the system role but did not use the system prompt for evaluations."}]}