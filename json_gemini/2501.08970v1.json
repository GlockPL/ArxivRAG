{"title": "Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography", "authors": ["Ilia Shumailov", "Daniel Ramage", "Sarah Meiklejohn", "Peter Kairouz", "Florian Hartmann", "Borja Balle", "Eugene Bagdasarian"], "abstract": "We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.", "sections": [{"title": "What are TCMEs?", "content": "In this paper we contend that recent advancements in machine learning enable a new paradigm for private inference. Fundamentally, the need for many cryptographic primitives stems from the fact that we don't have trusted third parties, thus requiring mutually untrusted participants to interact in a way that avoids revealing their data to each other but where they can nevertheless agree on a result. In this paper we argue that a capable machine learning model, in some settings, can play the role of a trusted third party (Abadi, 2004; Anderson, 2010). We propose Trusted Capable Model Environments, a setting where an individual machine learning model or a number of models initiate an interaction with additional constraints in the input and output to ensure that private data cannot leave the TCME. Consider for example the classical millionaires problem, where a pair of individuals are trying to figure out who has more money without disclosing how much money they have. Cryptographically, this can be solved using a secure two-party computation by Yao, or subsequent protocols that are more efficient (Ioannidis and Grama, 2003; Lin and Tzeng, 2005; Yao, 1982). With TCME, both individuals agree on\n\u2022 A model, e.g., Gemma (Gemma-Team, 2024);\n\u2022 A prompt, e.g., \"Say \"first\" if A is bigger than B and \"second\" otherwise? A={} and B={}.\";\n\u2022 Input constraints, e.g., A and B are 32-bit integers;\n\u2022 Output constraints, e.g., the only approved outputs are 'first' or 'second'.\nIf the environment can be trusted to both avoid leaking the private information provided by each party and to reliably output the correct answer, this presents an alternate approach to enabling this computation. While a viable cryptographic solution exists for this simple example, for the applications we discuss below it is currently computationally infeasible to rely on cryptographic solutions due to the unstructured nature of the computation. In contrast, we argue that securely performing these computations is entirely feasible with a new inference paradigm utilising machine learning models.\nThis paradigm for private computations enables analysis and collaboration for tasks that were previously infeasible. For example, programming the task is no longer limited to a highly technical specification, where all possible states have to be modeled; instead, it is possible to use human language directly by non-specialists.\nThere are three fundamental properties that we need to satisfy in order for models to be trusted:\n1. Statelessness \u2013 the model should be incapable of memorising, learning, or retaining any state based on the data with which it interacts. That way, it is clear with each interaction what (private) data influences the output, ensuring confidence in that post-invocation no private data can leak from the model and the model can't discriminate the user based on prior interactions.\n2. Explicit Information Flow Control\n\u2022 Information Flow \u2013 the model and the underlying system should have an explicit and frozen information flow that can be defined in a coordinated (and verifiable) way.\n\u2022 Verifiability \u2013 users of the TCME need an explicit mechanism for verifying that the correct model, correct prompt, and input/output constraints are respected.\n3. Trustworthy and Capable Model(s) we assume the use of trustworthy model(s) that are capable of solving a given user task, and that the model(s) are aligned in their performance with the expectations of the involved parties.\nAll of the properties above are currently only partially achievable, as we expand on further in Section 3.1. Below, we first formally specify the setting and describe our expectations of TCME. Next, we compare and contrast TCMEs with cryptographic solutions, namely multi-party computation (MPC) and zero-knowledge proofs (ZKPs). Finally, we provide a number of TCME-enabled applications.\nAs a hypothetical scenario to understand what TCME can be, imagine that you could have a model with explicit integrity guarantees and a precise information flow control mechanism.\nFor example, it could be a hardware implementation of a given open model. You ensure that the model integrity is preserved by imaging it with radiography and comparing to blueprints; you also ensure that it is incapable of maintaining state, e.g., utilize volatile memory with explicit (hardware-based) erasure protocol; and leaking state by placing in a Faraday cage; you also can power up and down the system at will to explicitly reset its state; finally you program the output and input constraints; e.g., the model can't respond with anything but a number 1\u201310. That way, even if private data is supplied to the model, it will be incapable of memorising it or further leaking private information. The model also has no alternative data access beyond what the user supplied in the input, making it impossible to identify and discriminate a given user. The model is verified either empirically or theoretically as being sufficiently capable of solving a task specified by the user.\nWith explicit mechanisms for statelessness, information flow control, and trustworthiness described above, the model becomes a trusted third party."}, {"title": "Trusted Capable Model Environments", "content": "We can start formulating a TCME in the same way as a secure multi-party computation (MPC): a set of $n$ parties $P_1, ..., P_n$ hold respective private data $x_1,..., x_n$, and want to compute an output $y = F(x_1,...,x_n)$ of some pre-agreed function $F(\\cdot)$. The parties want to compute this function in a way that is (1) correct, meaning $y$ really is the output of $F$ on the private data of each party, and (2) private, meaning no party learns any information about the private data of any other (honest) party.\nIn a TCME, we additionally consider a machine learning model $M$ that is capable of computing the function $F$; i.e., that given inputs $x_1, . . ., x_n$ can output $F(x_1, . . ., x_n)$ in an accurate and efficient way, to a degree that is acceptable to the users. The model must furthermore be run in an environment that is trustworthy, meaning that the environment (1) prevents unauthorized access to the model i.e. provides model integrity and protects intermediate state; (2) ensures the model operates in a stateless manner; and (3) ensures the model respects a pre-defined information flow control policy.\nIn addition to the function $F$, we thus consider that the parties interacting with a TCME also need to agree on the information flow control policy, the model being used, and the model's input and output constraints.\nThe goals of TCMEs are the same as the goals of multi-party computation, in terms of achieving correctness and privacy. The main difference is in how the computation is carried out: rather than parties interacting among themselves, in the proposed operation of a TCME each party provides their private input to the environment, which computes the function $F$ itself and outputs the response. Correctness is achieved following the capability of the model, and in particular its ability to compute the function accurately, and privacy is achieved following the strict information flow controls in place and the statelessness of the model. In particular, the ability of a TCME to prevent unauthorized access to the model implies that privacy can be achieved even with respect to the party running the environment.\nThe following components and properties are assumed to be trusted and secure:\n\u2022 TCME: The TCME itself is assumed to be secure and isolated, preventing unauthorized access and ensuring that the model operates in a stateless manner according to the predefined information flow control. This includes ensuring proper input sanitization, output filtering, and secure communication channels.\n\u2022 Information Flow Control Mechanism: The mechanism enforcing the information flow within the TCME is assumed to be correctly implemented and tamper-proof.\n\u2022 Initial Model Vetting and Continuous Monitoring: While we consider the possibility of a compromised model, we assume that an initial vetting process has been performed to ensure the model is free of known vulnerabilities and backdoors, and can perform the task at hand to an acceptable degree of performance at the time of deployment within the TCME. We also assume that the model can be further deployed with continuous monitoring tools that can terminate executing in case integrity is violated or an adversary is detected.\nWhy is this different from classical cryptographic approaches?"}, {"title": "Practical implementation", "content": "We envision that today it is possible to construct practical TCMEs that rely on TEEs to deliver private computations:\n\u2022 Model Hosting and Operation: Currently, there is no established standard for who runs the TCME. One approach is to treat the model as a \"trusted third party.\" This could involve having a neutral party host and run the model, ensuring that it operates according to the agreed-upon rules and does not favor any particular participant. The key is to select a model host that all participants trust to act fairly and impartially. Clear agreements between participants and the model host are crucial, explicitly covering data handling, access controls, and conflict resolution. Furthermore, TEE-style guarantees can enhance trust in the model hosting environment. These guarantees may include attestation to ensure the model's software integrity, secure enclaves to protect the model and its data from unauthorized access, and remote attestation capabilities for participants to verify the model's environment. Finding a trustworthy host and ensuring ongoing compliance require robust monitoring and auditing mechanisms.\n\u2022 Input and Output Constraints: Input constraints can be enforced through input validation and sanitization procedures. Similarly, output constraints can be implemented by filtering and transforming the model's output before it is released to the participants. Formal regular languages can be employed to define these constraints precisely, as explicit procedures deployed within separate TEEs.\n\u2022 Secure Communication: Secure communication channels with cryptography should be used to protect the confidentiality and integrity of data transmitted between the parties and the TCME.\n\u2022 Statelessness: The statelessness of the model can be enforced by resetting the model's state after each computation or by using specialized hardware that prevents state persistence. Formal verification techniques can be used to ensure that the statelessness property is maintained.\n\u2022 Error Handling and Fault Tolerance: Robust error handling and fault tolerance mechanisms are crucial for ensuring the reliability and availability of the TCME. This includes handling unexpected inputs, model failures, and hardware errors."}, {"title": "Limitations", "content": "While TCMEs promise to enable a number of previously impossible applications, several limitations and areas for future research warrant consideration. These limitations are discussed in the context of privacy and correctness, model trustworthiness and capability, and scalability and complexity."}, {"title": "Examples", "content": "We now turn to providing a number of practical examples that are enabled by TCMEs, but were infeasible with prior primitives."}, {"title": "Practical Example 1: Multi-agent non-competition", "content": "Setting: It often happens in academic research that multiple groups pursue the same research question. This can lead to challenges in publication and potential interpersonal conflicts, especially when students are involved and they require publications for graduation. Traditionally, senior researchers within these groups, often acquainted with each other, would convene to ensure non-competition and potentially instead foster collaboration. However, in rapidly expanding fields like machine learning, such coordination becomes increasingly difficult.\nThis scenario serves as an excellent illustration of a problem well-suited for TCME. That is because:\n\u2022 Unstructured Input: The problem domain is inherently open-ended and unstructured. This makes multi-party computation problematic since inputs are not well defined.\n\u2022 Abstraction: The protocol must function effectively at varying levels of abstraction.\n\u2022 Information Leakage: Defining and controlling information leakage is inherently challenging for unstructured inputs.\nSolution: We envision a solution where machine learning models are executed within a shared Trusted Execution Environment between a number of groups. Constraints on prompts, inputs, and outputs are defined in advance. For example, all input ideas might be represented as a list of text, with a single Boolean output. Encrypted private knowledge bases are loaded and decrypted within the TCME. The models then communicate to determine a shared answer to the query or terminate communication if agreement cannot be reached. Third-party trusted models, launched locally within the TEE (e.g., on H100/200), are employed to execute the solution. A separate TEE is used to ensure that output constraints are satisfied. That way, group members can submit their list of ongoing projects and learn if they are in competition with each other."}, {"title": "Practical Example 2: Audit for confidentiality violations", "content": "Setting: Consider a regulator who wants to ensure that the protection promises described by a business are honest and correct; e.g., that it does not store any passwords in an unencrypted state. At the same time, the business owner wants to make sure that none of the business secrets get leaked.\nSolution: We describe TCME that can work for this setting. The business owner and regulator agree on a machine learning model and a specific prompt. These are then hardcoded into the system, along with a predefined output template. For instance, the system might be designed to output only \"YES\" if insecure handling of PII is detected. The model is granted access to code describing the system and the database access. The only allowed outputs are \"YES\" and \"NO,\" indicating whether PII is mishandled. The input itself is restricted such that no state changing transition can be made. The model prompt is predefined, such as: \"Output YES only if private user data is stored in a way that would endanger the customer in case of compromise\", with the approval of both business owner and the regulator. Both the regulator and the business owner are notified if the output is \"YES.\"\nThis approach balances the confidentiality of the business and enables the regulator to perform an automated check. TCME alerts only in case violations are detected, avoiding unnecessary intrusion."}, {"title": "Practical Example 3: Damage to business property", "content": "Setting: Consider a landlord who wants to ensure that their business property is not damaged while preserving the privacy of their renters. The landlord requires a mechanism to monitor the condition of the property without infringing on the business renters's privacy by continuously observing their activities within the space.\nSolution: We describe TCME that can work for this setting in Figure 1. The business owner and landlord agree on a machine learning model and a specific prompt. These are hardcoded into the system, along with an output template. For instance, the system might be designed to output \"YES\" if significant damage is detected.\nThe model is granted access to camera recordings at the end of the day. The only allowed outputs are \"YES\" and \"NO,\" indicating whether damage has occurred. The input is restricted to the recordings only. The model prompt is predefined, such as: \"Output YES only if the space is severely damaged.\", with the approval of both landlord and the tenant. Both the landlord and the tenant are notified only if the output is \"YES.\"\nThis approach balances the landlord's need to protect their property with the business renter's right to privacy. The model only alerts the landlord if significant damage is detected, avoiding unnecessary intrusion."}, {"title": "Practical Example 4: Private Code Auditor in TEE Attestation", "content": "Setting: Consider a setting in which a user wants to perform TEE attestation where some of the code involved is proprietary and cannot be shared, yet we still want to provide some guarantee that code does not violate user expectations.\nSolution: We argue TCME can be used in this setting. We present the overall flow in Figure 2. Here, a public model can be used during the attestation process to perform checking of the private parts of the code against concerns of the user. Here concerns get codified by the user together with the attestation provider, which are then applied and used as part of the attestation. This way a soft guarantee can be provided to the user that private parts of the code are not violating some constraints."}, {"title": "Understanding the Trade-Off with Cryptography", "content": ""}, {"title": "Comparing with MPC", "content": "Yao's millionaire problem described in the introduction is an unlikely application for TCME, due to the relatively ease with which it can be solved using a two-party computation. Here we provide a more complex problem that better illustrates the boundary between problems that can be efficiently solved using cryptography and problems for which we might require TCMEs.\nTwo companies want to determine if their clientele overlap significantly in terms of age ranges. Each company has a list of age ranges they target (e.g., 18-24, 25-34, etc.). We want to compute the number of overlapping age ranges (or, thinking more broadly, potentially compare other attributes) without revealing the specific ranges each company targets. Each company represents its targeted age ranges as a binary vector. For example, if there are five possible age ranges, a company targeting the first two ranges (18-24 and 25-34) would have the vector [1,1,0,0,0]. The circuit takes two binary vectors as input and computes the number of overlapping age ranges. The circuit consists of AND gates for each corresponding age range and a summation gate to count the number of overlaps.\nGarbling and Evaluation: The typical technique used in two-party computations is garbled circuits. One company garbles the circuit, encrypting the truth table of each gate and encrypting its input. The other company obtains the garbled circuit and encrypted input. It uses oblivious transfer to obtain the keys corresponding to its input vector without revealing the vector itself, thus obtaining its own garbled input. This company then evaluates the garbled circuit, obtaining the encrypted output (the number of overlapping ranges).\nOutput Decryption: The first company provides the decryption key for the output to the second company, who decrypts the result.\nThe same task can be performed with TCME. Both parties agree on:\n\u2022 A model, e.g., Gemma (Gemma-Team, 2024);\n\u2022 A prompt, e.g., \u201cOutput the overlap across the ages of the clients according to the annotation scheme: {}. Only output the overlap as a number\";\n\u2022 Input constraints, e.g., list of ages represented as integer;\n\u2022 Output constraints, e.g., the only approved output is a float representing the overlap.\nThe communication and computational costs of using garbled circuits are linear in the size of the circuit and the size of the input(s). For small circuits, the costs associated with garbled circuits are likely lower than the costs of running a machine learning model in a TCME. However, as circuit complexity increases, because of, for example, finer discretization of age ranges or the addition of other sensitive attributes, the costs will increase. This scaling challenge is less pronounced in TCMEs, which operate on a different level of abstraction with a relatively constant associated cost."}, {"title": "Comparing with ZKPs", "content": "To illustrate again the boundary between problems where cryptography is the most suitable solution and ones where we might want or need TCMEs, we consider another classical problem: that of proving knowledge of a valid 3-coloring of a graph. In this problem, a graph is available to a prover and a verifier, and the prover is in possession of a valid 3-coloring. Their goal is to prove knowledge of this to the verifier without revealing any information about the 3-coloring itself.\nTo determine the capability of modern LLMs in verifying solutions to this problem, we sampled a thou- sand random graphs of sizes 5\u201325 with edge probability of 0.1. We then asked the Gemini-1.5-Flash model to verify that the solution is correct with the following prompt:\nYou are an agent that receives a graph that is represented by the adjacency matrix.\nYou job is to verify the coloring of the graph with 3 colors\nsuch that no two adjacent nodes have the same color.\nOnly produce YES if coloring is correct, otherwise output NO.\nColor scheme is a dict where each node is mapped to its color 1 to 3.\nFor example, {{0:1, 1:2, 2:2, 3:0, 4:1,5:1, 6:2, 7:0, 8:2, 9:2}}\nmeans node 0 is color 1, node 1 is color 2, and so on."}]}