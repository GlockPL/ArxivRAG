{"title": "DARD: A Multi-Agent Approach for Task-Oriented Dialog Systems", "authors": ["Aman Gupta", "Anirudh Ravichandran", "Ziji Zhang", "Swair Shah", "Anurag Beniwal", "Narayanan Sadagopan"], "abstract": "Task-oriented dialogue systems are essential for applications ranging from customer service to personal assistants and are widely used across various industries. However, developing effective multi-domain systems remains a significant challenge due to the complexity of handling diverse user intents, entity types, and domain-specific knowledge across several domains. In this work, we propose DARD (Domain Assigned Response Delegation), a multi-agent conversational system capable of successfully handling multi-domain dialogs. DARD leverages domain-specific agents, orchestrated by a central dialog manager agent. Our extensive experiments compare and utilize various agent modeling approaches, combining the strengths of smaller fine-tuned models (Flan-T5-large & Mistral-7B) with their larger counterparts, Large Language Models (LLMs) (Claude Sonnet 3.0). We provide insights into the strengths and limitations of each approach, highlighting the benefits of our multi-agent framework in terms of flexibility and composability. We evaluate DARD using the well-established MultiWOZ benchmark, achieving state-of-the-art performance by improving the dialogue inform rate by 6.6% and the success rate by 4.1% over the best-performing existing approaches. Additionally, we discuss various annotator discrepancies and issues within the MultiWOZ dataset and its evaluation system.", "sections": [{"title": "1 Introduction", "content": "In recent research, significant efforts have been made to build systems that involve planning and communication between various specialized agents to perform complex tasks [1, 2, 3]. These agents are, in turn, backed by instruction-tuned open-source LLMs, external APIs, or other simpler tools. Various tasks such as logical reasoning [4, 5], societal simulations [6, 7], software development [8] have seen remarkable improvement in performance using these multi-agent framework methods."}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 MultiWOZ 2.2 Dataset", "content": "The MultiWOZ 2.2 [13] dataset is an improved version of MultiWOZ 2.1 [12] correcting DST annotations of 17.3% of utterances and ontology issues associated with some of the slot values. The dataset contains 10,437 conversations divided between train (8,437), validation (1,000), and test (1,000) sets. Each conversation contains alternating turns between the user and the system utterances. Although the dataset comprises 7 domains, the domains of hospital and police are only present in the training data. Following the work of [19, 20], we remove the conversation of these domains from the training set. The domains of restaurant, hotel, taxi, and train are bookable as the users can ask the system about booking these for them. The dataset also contains an external database for each domain, which contains a list of entries and their attributes. The system responses are supposed to suggest and offer a reservation for only these entries. Ground truth data comprises detailed annotations of dialog states, system responses, and conversation goals, which are used to compare and evaluate the model-predicted responses."}, {"title": "2.2 Scoring Metrics", "content": "We follow the standard and the widely used metrics for the tasks of DST and Response Generation presented in the work\u2074 [16].\nDST The DST performance is measured by Joint State Accuracy (JSA) between the predicted and the ground-truth states. Each slot consists of a triplet of (domain, slot key, and slot value), and multiple such slots can be present in a given dialog history. Some of these slots like hotel-stars are categorical in nature so can take only a fixed set of values while others like hotel-name can take any value based on conversation history. The binary JSA values are computed at each user turn of the conversation and involve comparison between all predicted states and the ground truth states. The domain name and slot key must match exactly, but a fuzzy margin is applied when comparing slot values to ignore minor syntactic differences.\nEnd-to-end Response Generation The overall TODS pipeline, including DST and response generation, is evaluated using Inform, Success, and BLEU scores. The Inform rate is a conversation-level binary variable that indicates whether the user is presented with the correct venues according to their constraints. For example, for the conversation in Figure, if the system provides the user with a restaurant located in the center of town that serves Chinese food, it will be considered a correctly informed conversation. A multi-domain conversation will be considered correctly informed if the right venues are presented in each domain. Furthermore, a conversation has a Success rate of one if, firstly, the conversation has an inform rate of one (the user was provided the right venue) and secondly, the user was presented with the right attributes about the venue. For instance, in the above example, if the user was provided with the correct restaurant and later during the conversation, the user asks about the restaurant's phone number, postcode, etc., the conversation would be considered successful if this information was presented to the user and unsuccessful otherwise. Like the inform rate, success metric is also binary and is computed at a conversation level. To evaluate the quality of the generated text, BLEU scores are calculated between delexicalized predicted responses and ground truth system responses. This delexicalization process ensures that model predictions are not unfairly penalized when they provide information about a suitable venue that differs from the specific venue chosen in the ground truth response, as multiple venues may satisfy user constraints. The following definition of the combined score is used to compare approaches in the MultiWOZ benchmark.\nCombined Score = $\\frac{(Inform + Success)}{2}$ + BLEU Score\nAdditionally, some metrics to compare textual richness are also compared."}, {"title": "2.3 Approach", "content": ""}, {"title": "2.3.1 DST Prediction", "content": "We start by building a DST pipeline for MultiWOZ, which takes the dialog history as input and outputs the slots present in the context. We run all of our experiments with Flan-T5-large, Mistral-7B, and Claude Sonnet 3.0 as our base models, as they represent a comprehensive range of model sizes, spanning from sub-1B parameters to mid-sized 7B models and larger LLMs, allowing us to assess performance across varying scales.\nSingle fine-tuned Agent This approach fine-tunes a language model to generate the list of slots given the input. We used the models Mistral-7B and Flan-T5-large for this approach. For model training, we treat each segment of the dialogue history up to each user utterance as a separate example, instead of updating a running dialogue state with each new utterance. This approach offers two key advantages: i) It prevents the accumulation of errors that could occur with subsequent dialogs ii) It better handles scenarios present in MultiWOZ datasets where previous dialogue slots are cleared, a situation where continuously updating dialogue states would fail.\nPrompted LLM In this approach, we presented the Claude Sonnet 3.0 model with detailed instructions for the DST task. These instructions include a list of possible slots to track, all possible values categorical slots can take, explicit output format, and some in-context examples from which the models can learn. We present Claude with 50 random in-context examples selected at random from the training set.\nMulti-Agent DARD In this approach, we fine-tune domain-specific DST models. We train a separate distinct model for each domain, focusing exclusively on tracking the slots relevant to that domain. To prepare the training data, we segregate the slots by domain; for instance, if a training sample contains slots from both the attraction and train domains, we add that sample to the training data of both domain models, with outputs consisting of slots from the respective domain only. During the testing phase, we first pass the context to a dialog manager agent, which outputs all domains whose slots are present. We then invoke the respective domain models to obtain the final set of slots. We use a prompted Claude 3.0 Sonnet LLM as a dialog manager agent to inform us about the domains present in the dialog context."}, {"title": "2.3.2 Response Generation Pipeline", "content": "The response generation pipeline involves predicting the delexicalized system utterance given the conversation context up to the previous user utterance. Similar to the DST pipeline, we choose to experiment with a single-agent approach and a domain-specific multi-agent approach.\nSingle fine-tuned Agent This approach fine-tunes a single model to handle conversations across all domains, providing it with both the dialog context and details of venues meeting user criteria. These venue details, obtained by querying the database using predictions from the best DST pipeline, include the number of matching venues and specifics of one of them, if any. Adding these venue details allows the model to tailor responses based on the number of available options, whether suggesting a single match, asking for more preferences when multiple venues fit, or informing the user when no matches are found.\nMulti-Agent DARD For this approach, we again use models that are specialized to respond to queries from specific domains. We experimented with fine-tuned models(Mistral-7B & Flan-T5-Large) and instructed Claude Sonnet 3.0 as our domain agents. The Claude-based agent is prompted with detailed information about the list of possible delexicalized tags to be used, generic instructions on how to respond, and some in-context examples. The generic instructions on how to respond were designed using manual observation of training data from each domain. We use the Sonnet 3.0 dialog manager agent to determine which domain agent will be best equipped to respond based on the conversation context. We then pass the conversation context and venue details to the delegated agent to generate the system response. Since we can choose any type of agent for each domain, we also experiment with and select the best-performing agent from among Claude Sonnet 3.0, Flan-T5-Large, and Mistral-7B agents based on their combined scores on the validation set."}, {"title": "3 Results", "content": "Table compares the results of our DST experiments with the existing best-performers [21, 22, 23]. We divide the existing work primarily into those that use fine-tuned models and those that use prompting LLM methods. We observe that for the fine-tuned Flan-T5-Large model, the performance is much better with domain-specific agents than with a single agent. However, for a fine-tuned Mistral-7B, the performance is nearly the same with the two approaches, and the single model performs a little better. We notice that Claude's performance was poorer than that of the fine-tuned models. Our approach of using a single fine-tuned Mistral-7B model performs better than most existing works, second only to the method followed by [22]. [22] fine-tune a LLaMA-2 7B model on the DST dataset, first on the original training data itself(LUASR), which leads to similar performance as that of fine-tuned Mistral-7B, and then on both original training data and augmentations (LUASR+G), producing marginally better performance than a fine-tuned Mistral-7B models. The prompt-based approaches described in [23] performed better than our prompting-based Claude Sonnet method. We believe this stark difference in performance is due to the fact that the work in [23] employs a single-slot return approach, where the model predicts each slot individually for each conversation context, resulting in 30 calls in total for each sample's complete prediction. This approach simplifies the task for the LLM, as it only needs to check for one slot in the context at a time, unlike our method, which requires consideration of all possible slots. However, the single-query method requires roughly 30 times more LLM calls."}, {"title": "4 Discussions", "content": "In this section, we compare and present some key insights based on various experiments that we tried."}, {"title": "4.1 Analysis of Claude's DST performance", "content": "We conducted a thorough analysis of errors made by Sonnet 3.0 and discovered that most mistakes stemmed from its tendency to track slots in both user and system utterances, whereas the ground truths typically only include slots from user utterances. We define over-prediction as cases where groundtruth slots are a subset of predicted slots, under-prediction where groundtruth slots are a superset of predictions, both mismatch when neither is a subset or superset of the other, and value match error when predictions have an incorrect value for any slot. The table reveals that in over half (54%) of cases, predicted responses contain more slots than the groundtruth. Examining about 100 random samples from this set showed that this occurs because Sonnet responses track slots from system utterances as well. Further investigation uncovered annotation inconsistencies in the MultiWOZ 2.2 dataset itself, with some annotators only tracking slots from user utterances while others included slots from both user and system utterances. This issue is also highlighted in [31], which offers a detailed analysis of the extent of this problem in the MultiWOZ 2.2 dataset. To address this inconsistency, [31] corrected the DST labels in over 70% of the dialogs in the dataset"}, {"title": "4.2 Single Agent Vs Multi-Agent Approaches", "content": "DST We observe that the JSA of the domain-specific fine-tuned Flan-T5-Large model is 4.6% better than that of a single model. However, for the fine-tuned Mistral-7B, the difference is not as big, and the single-agent approach performs better. We attribute this observation to the fact that Mistral-7B is a more powerful and larger model (~ 7B params) compared to Flan-T5-Large (~780M params), hence it is more capable of modeling slots of all domains in a single model. While having domain specialization is useful for the smaller Flan model, it brings no advantages for the bigger Mistral-7B model. A possible drawback of the domain-specific agents is that they are trained on a smaller amount of data than a single agent, and they also do not support cross-domain transfer learning.\nOverall TODS Just like the DST pipeline, we get similar observations for the fine-tuning-based approaches, i.e., we get large (31.5%) improvements with domain-specific agents for Flan-T5-Large while Mistral-7B sees a slight decrease (2.7%) in performance. In terms of textual richness, we notice that multi-agent approaches have great lexical diversity. Intuitively, this aligns with our expectations as domain-specific agents are more likely to learn and use domain-specific corpora, leading to higher overall diversity.\nOverall, the DARD-based multi-domain approach offers significant flexibility and composability for TODS. This framework serves as a versatile plug-and-play environment for various domain agents, allowing us to select the most effective agents for each domain. For instance, in the case of the MultiWOZ dataset, Claude 3.0 Sonnet worked best for the attraction, hotel, and restaurant domains, while Mistral-7B worked best for the train domain, and Flan-T5-Large worked best for the taxi domain. The multi-agent structure improves interpretability and simplifies the development of improved domain-specific agents. Throughout the development process, we can monitor domain-wise accuracies, focusing on improving underperforming domains without affecting others. Additionally, the domain-expert approach enables runtime performance optimization by employing smaller, faster models for simpler domains while utilizing larger language models for more complex ones. However, these advantages rely on the crucial assumption that the dialog manager agent can accurately assign the appropriate agent to each task. While this assumption held true for the MultiWOZ dataset due to its distinct and exclusive domains, it may not universally apply to TODS with overlapping domains in real-world scenarios."}, {"title": "4.3 Fine-tuned Vs Prompted Models", "content": "The BLEU score of prompted Claude is considerably lower compared to that of fine-tuned models and existing works. However, the opposite is true for the inform and success rates. The low BLEU score of Claude's responses can be explained by the fact that LLM-generated responses follow different speaking styles and vocabulary. Additionally, the model prompt contains only 8-10 (<1%) in-context examples from the training data, while the fine-tuned models are trained on the entire dataset (100%). Hence, the fine-tuned models have a better understanding of responding as an annotator resulting in a higher BLEU score. While the responses from prompted LLMs may have low BLEU scores these responses are preferred by human evaluators [32] [33]. These studies suggest that the low BLEU scores is due to lack of grounding with the dataset.  We also analyzed conversations where fine-tuned agents failed to achieve inform and success rates, but Claude agents succeeded. In 52% of these cases, the fine-tuned model failed to inform because none of its responses contained any suggested venues. We found that this issue arises due to another annotator disagreement in the dataset: while some annotators preemptively suggest venue names in earlier utterances, others ask users about additional preferences first before suggesting them with a particular venue.  For the same query (museums in the center) with 11 matching venues, annotator-1 immediately recommended a specific venue, while annotator-2 informed the user about the available choices and asked if they had a particular venue in mind. Both methods are valid, but fine-tuned models sometimes adopt the preference-asking way when the ground truth expects an immediate suggestion. As a result, in subsequent interactions, the model assumes a venue name was already mentioned based on the ground truth and doesn't repeat it. Consequently, no responses contain a venue name, leading the evaluation system to conclude that the agent failed to suggest any venues. This often causes the fine-tuned models to miss out on making any suggestions. This issue does not occur with Claude agents, as they are prompted to do both: make a suggestion while also asking for user preferences."}, {"title": "5 Conclusion", "content": "In this work, we propose a DARD (Domain Assigned Response Delegation), a multi-agent framework to build Task-Oriented Dialogue Systems (TODS). The DARD framework includes a central dialog manager agent that assigns tasks to domain-specific agents, which then handle and solve these tasks. We evaluate this approach using the Dialogue State Tracking (DST) and Response Generation tasks of the MultiWOZ dataset. For our domain agents, we utilize Flan-T5-Large, Mistral-7B, and Claude Sonnet 3.0 models.\nOur findings show that the effectiveness of the multi-agent DARD depends on the type of model used. Smaller models, like Flan-T5-Large, experience significant performance improvements with the multi-agent framework, while larger models, such as Mistral-7B, see a slight decrease in performance. Despite this, DARD offers notable benefits in terms of flexibility, composability, and interpretability, making it a valuable framework for developing more efficient TODS. We also observe that while Claude-based agents achieve state-of-the-art inform and success rates, they have lower BLEU scores due to a lack of alignment with the training samples. However, Claude-generated responses are much more lexically diverse than those from other models. Additionally, our work highlights two significant annotator discrepancies in the MultiWOZ 2.2 dataset. The first discrepancy involves the tracking of dialog slots: while some annotators track only the slots from user utterances, others track slots from both user and system utterances. This inconsistency leads to overprediction when using language models like Claude for Dialogue State Tracking (DST). The second discrepancy relates to the conversation policy. Some annotators preemptively suggest the name of a possible venue to the user, while others first ask for user preferences before suggesting a name. This inconsistency causes fine-tuned models to assume that a venue name was already suggested in a previous utterance, leading them not to mention any name in their predicted responses. As a result, these models often fail to achieve high inform and success rates in conversations."}, {"title": "6 Future Work & Limitations", "content": "While our work tests DARD on the MultiWOZ benchmark, additional evaluations on more complex task-oriented dialogue (TOD) datasets, such as the Schema Guided Dataset [10] and SMD [34], are needed for more conclusive insights into the advantages of multi-agent frameworks. Moreover, it would be beneficial to evaluate DARD-based TOD systems within an interactive evaluation framework. Currently, model responses are assessed statically based on the existing conversation, which can be problematic since user messages often correspond to prior ground truth system messages rather than the predicted responses. To enhance DARD's performance, one possible approach is to provide domain agents with selective context instead of the entire conversation. In multi-domain scenarios, such as switching from the attraction domain to the taxi domain, the inclusion of irrelevant prior utterances may hinder the new agent's performance. A dialog manager could mitigate this issue by determining the active domain and selecting only the relevant context."}]}