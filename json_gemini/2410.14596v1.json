{"title": "Teaching Models to Balance Resisting and Accepting Persuasion", "authors": ["Elias Stengel-Eskin", "Peter Hase", "Mohit Bansal"], "abstract": "Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates. We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model's performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.", "sections": [{"title": "1 Introduction", "content": "Persuasion is a core component of our ability to interact successfully and productively with each other, allowing one individual to change the beliefs of another. Increasingly, large language models (LLMs) are being deployed within standard human interaction frameworks, i.e. interacting in dialogues with people (Yi et al., 2024) as well as with other LLMs (Chen et al., 2024; Liang et al., 2023; Du et al., 2024b). LLMs have broadly revealed themselves to be easily persuaded in ways that can hurt their usability; for example, models can be persuaded to reveal private data or generate harmful text (Zeng et al., 2024) and simply questioning the correctness of model outputs often causes them to change their answers (Laban et al., 2023). This motivates teaching models to resist these kinds of adversarial inputs, i.e. to make models less easily persuaded. However, this is only one side of the story: as we later show, being overly-resistant to persuasion negatively impacts model quality: models that stubbornly stick to their responses do not improve through discussion, and may be frustrating to interact with. For LLMs to be reliable and useful conversation partners and teammates (e.g. in multi-agent debate, human-model interaction, etc.), a balance must be struck between resistance to harmful or negative persuasion (see left side of Fig. 1) and acceptance of beneficial (or positive) persuasion (see Fig. 1, right side); in other words, models should be persuaded when appropriate.\nPast work (Zeng et al., 2024; Xu et al., 2024; Laban et al., 2023) has primarily focused on measuring negative persuasion, analyzing existing models and finding that they perform poorly when faced with an adversary who persuades the model to change its answer to be incorrect or undesireable in some other way (e.g. unsafe, offensive, etc.). We argue that, while LLMs should be hardened against negative persuasion (which we do in our experiments), real-world models will be presented with a heterogenous mix of negative and positive persuasion, and thus must also be able to change their outputs to improve their responses or answers (e.g. by adopting a correct answer, as the model on the right does in Fig. 1). This introduces a new challenge, as models must learn to assess differences between their knowledge and claims from their interlocutor in order to recognize when they should \u2013 or should not \u2013 accept persuasion.\nTo tackle this challenge, we introduce Persuasion-Balanced Training, or PBT, which teaches models to appropriately accept and resist persuasion. We first create preference-based training data using a multi-agent, recursive tree-based paradigm. Our data is sourced from a question-answering (QA) setting where two LLMs debate each other, acting as both speakers and listeners to create a dialogue tree encoding different ways a conversation could go. By comparing responses counterfactually, we can evaluate different ways the dialogue could have gone and thereby obtain data for both positive and negative persuasion, which we can use to train LLMs via a balanced preference-based RLHF objective. We compare models trained with PBT \u2013 which balances resisting negative persuasion and accepting positive persuasion \u2013 to resist-only and accept-only models.\nUsing these models, we address three key research questions. First, we ask: (1) What effect does training have on resistance to misinformation and flipflopping? We find that training models to resist negative persuasion allows models to maintain performance when faced with adversarial prompts trying to misinform the agent or flip its answer, with lower misinformation and flipflopping rates. However, as discussed above, models must also be amenable to positive persuasion, so we also ask: (2) What effect does training have on a balanced mix of positive and negative persuasion? Here, we find that only PBT training consistently improves both positive and negative persuasion, with resist-only and accept-only training over-correcting and having negative effects on the other direction. Finally, evaluating models as conversational partners, we ask (3) How does the persuadability of individual models affect a multi-agent team's performance? Here, we team models up via multi-agent debate, measuring their accuracies at the start and end of the dialogue. We find a troubling trend: without PBT, the performance of the team depends heavily on which model goes first, with the weaker model often persuading the stronger one and dragging it down. Crucially, we find that PBT greatly reduces the ordering effect, with similarly high scores regardless of which model goes first.\nMore specifically, we evaluate resistance to misinformation on the FARM dataset (Xu et al., 2024), which persuades models to adopt misinformation, and use Laban et al. (2023)'s \u201cAre you sure?\u201d evaluation to measure flipflopping. PBT applied to Llama-3.1-70B leads to a 38.13% absolute reduction in the misinformation rate and completely eliminates flipflopping. While resist-only training also leads to improvements on misinformation and flipflopping, when we evaluate on a balanced dataset of positive and negative persuasion, we find that it leads to over-resisting on all examples and thus poor performance. PBT balances resistance and acceptance, with the best overall performance across Mistral-7B, Llama-3.1-8B, and Llama-3.1-70B, obtaining an average accuracy of 63.88% across models (compared to the base models' 48.87%). Finally, in the team setting, we pair a strong Llama-3.1-70B model with a weaker Llama-3.1-8B model in a multi-agent debate, finding that base model performance depends on which agent goes first, with accuracy dropping by an absolute 8.7% when the wrong agent starts. PBT improves average team performance from 71.7% to 74.2% and largely eliminates order dependence, leading to similarly high performance with both agent orders.\nFinally, we also analyze features influencing a PBT model's decision to accept or reject an answer. We find that whether a model is persuaded is driven by the plausibility of the model's answer and the alternative answer being proposed as opposed to the perceived confidence of the responses or the uncertainty of the base model; when the model's probability on the alternative is high and the probability on the current answer is low, the model switches to the alternative. In other words, PBT training teaches the model to compare the likelihood of different answers and adopt the most likely one. We also compare qualitative examples"}, {"title": "2 Related Work", "content": "Persuasion in LLMs Recent work has focused on negative persuasion, showing that LLMs can be overly persuadable. For models deployed in dialogue settings, simply asking whether a model is sure often leads the model to change its answer, a behavior known as \"flipflopping\" (Laban et al., 2023). Other studies show that adversarial users can systematically persuade models of clearly false claims (Xu et al., 2024) or jailbreak them by using specific persuasion strategies like emotional appeals (Zeng et al., 2024). These behaviors make LLMs less effective and less safe. We show that PBT results in improved performance on Laban et al. (2023) and Xu et al. (2024)'s settings after training models to resist negative persuasion. Moreover, we introduce positive persuasion and show that balancing resistance to negative persuasion with also accepting positive persuasion is central to overall model performance and team performance. Khan et al. (2024) use best-of-N sampling to vary persuasiveness w.r.t a judge model in an LLM debate; in contrast, we create data for persuasion and train models, and perform debate without a judge model, more directly measuring the models' ability to persuade each other (as opposed to a judge).\nKnowledge Updating and Conflict Our work also relates to work that studies how LLMs respond to new textual evidence (Longpre et al., 2021; Wang et al., 2023; Xie et al., 2023; Du et al., 2024a) and to perceived confidence (Stengel-Eskin et al., 2024). Specifically, our work connects to knowledge conflict, where information that conflicts with a model's parametric knowledge is given in the model's context. Wan et al. (2024) find that model outputs are influenced by text provided in-context that is relevant but not credible (according to human credibility notions). Wu et al. (2024) show that models are more likely to adopt more plausible information from their contexts. In our analysis, we find that PBT teaches models to rely on answer plausibility to decide when to adopt answers in a dialogue setting."}, {"title": "3 Methodology", "content": "3.1 PBT Data Creation via Multi-Agent Trees\nWe introduce a multi-agent method for automatically creating persuasion data that resembles tree search algorithms like Monte-Carlo Tree Search (Coulom, 2006). Our method is detailed in Fig. 2; broadly, we create preference data by unrolling dialogues from agents with multiple different roles, storing their respective responses in a tree. This allows us to recursively score dialogue turns (based on how many correct answers they eventually lead to) and compare different counterfactual continuations, i.e. how the dialogue would have gone if an agent had produced a different response.\nWe begin with a set of questions and their corresponding reference answers, prompting two LLM agents to discuss each question and produce a final answer. Agents are assigned different roles and prompts. In the persuader role, following Xu et al. (2024), we prompt agents to argue based on logical reasoning, emotional appeal, or establishing credibility. In the persuadee role, agents are instructed to be acceptant or resistant. Agents take turns, alternating between persuader and persuadee turns. At each turn, the agent generates a separate response from each prompt, leading to a tree structure (seen in Fig. 2) with the parent node being the previous agent's turn and the children representing alternative responses. We follow Stengel-Eskin et al. (2024) and extract a final answer from each turn using a few-shot extraction prompt. More formally, let $y_i$ be a node with the response and answer from agent i at turn t, and let $a(y_i)$ be the parent to $y_i$. When generating a response, each agent is conditioned on the dialogue history given by its ancestors, i.e. it receives as context $[a(y_i), a(a(y_i)), a(a(a(y_i))), . . .]$. We terminate a branch when both agents agree on their answer. Note that the first two turns deviate from this structure, as we ask each agent to respond independently of each other to encourage disagreement; we find that this is necessary because base models tend to agree with each other when their first turns are conditioned on each other, i.e. the second model generally adopts the answer of the first model, even if it would give a different answer when prompted independently.\nFor each question, we expand the dialogue tree until a maximum number of turns is reached or all branches are terminated by agreement. We then score the nodes; a node receives a point if its answer is in the reference set. We recursively aggregate these scores up the tree, s.t. the parent node receives its own accuracy score, plus the aggregated accuracies of its children. Let $c(y_i)$ be the set of children of node $y_i$, and let $correct(y_i)$ be a function that returns 1 if the answer expressed in node $y_i$ is correct. We define the score for a node as: $s(y_i) = correct(y_i) + \\sum_{y' \\in c(y_i)} s(y')$. In other words, nodes are scored not only by whether they express the right answer, but also by whether they lead to more correct answers downstream. For example, in Fig. 2, the generation \u201cI disagree, it's definitely Shirley Bassey\u201d receives a high score because it leads to two downstream correct answers by resisting the negative persuasion in the turn", "I disagree\" and then later provide a correct answer.) Before comparing scores, we filter to ensure that the answers expressed by $y_i^\u0142$ and $y_i^\u0142$ actually differ by prompting a separate LLM. By filtering for real disagreement, we ensure that the trees contain examples of both positive and negative persuasion, with correct agents resisting negative persuasion and incorrect agents accepting positive persuasion.\nWe use TriviaQA (Joshi et al., 2017) as our source of questions and answers, sampling questions from the training split, and use two different LLMs for the two agents (Mistral-7B-v0.2-Instruct and Llama-3.1-8B) to introduce answer diversity. Dialogues are limited to four turns. All prompts are in Appendix C, with further details on data creation and train/dev/test split size in Appendix B.\"\n    },\n    {\n      \"title\": \"3.2 PBT: Persuasion-Balanced Training\",\n      \"content\": \"PBT involves training models to maximize the margin between positive and negative examples ($y_w$ and $y_\u00b9$ in Fig. 2), where $y_w$ and $y_\u00b9$ are continuations to a dialogue. Note that the pairs can encode both resisting negative persuasion (the first example in Fig. 2) or accepting positive persuasion (the second example). Moreover, for PBT we balance the training data, downsampling resistance examples (as these are more common). Before training with a DPO loss (Rafailov et al., 2023) as given by the equation in Fig. 2, we first perform supervised fine-tuning on the positive side of the preference pairs. We train with LoRa (Hu et al., 2022), selecting the best model based on dev performance (details in Appendix D). For accept-only and resist-only, the dev set only includes accept or resist examples. For PBT, the dev set is balanced; the test set is always balanced. We use instruction-tuned models as they have been finetuned on chat data.\"\n    },\n    {\n      \"title\": \"3.3 Experimental Setup: Models and Metrics\",\n      \"content\": \"Models. We examine three models: Mistral-7B-v0.2-Instruct (Jiang et al., 2023) and Llama 3.1 8B and 70B Instruct (AI@Meta, 2024). All models are run across three random seeds; we report mean performance and the standard deviation.\nMetrics for Resisting Misinformation. To measure each model's ability to resist negative persuasion, we use the FARM dataset (Xu et al., 2024), which measures how easily models are misinformed. FARM consists of questions from popular QA benchmarks (TruthfulQA (Lin et al., 2021), Natural Questions (Kwiatkowski et al., 2019), and BoolQ (Clark et al., 2019)) paired with counterfactual answers that contain misinformation.2 A target model is asked to answer a question and an adversary attempts to persuade the target to adopt a misinformed belief across multiple rounds. We use the \\\"Logical": "trategy, which gives a logical argument for why the misinformation is true and is generally the most effective, and use Xu et al. (2024)'s evaluation, measuring the rate at which the target is misinformed (lower is better).\nMetrics for Resisting Flipflopping. Laban et al. (2023) demonstrate that challenging models often leads them to flip their answers and reduces overall accuracy. We apply this to a 1000-question subset of the TriviaQA validation split, where we first prompt models to answer a question and then add \u201cI don't think so. Are you sure?", "Ok, so what's your final answer?": "e use the same extraction prompt as in Section 3.1, reporting initial and final accuracy.\nMetrics for Balancing Positive and Negative Persuasion. Our first two evaluations only measure resistance; to get a more balanced view, we test on data with both positive and negative persuasion. Specifically, we construct held-out data following"}, {"title": "4 Results", "content": "4.1 RQ1: Resisting Negative Persuasion\nResisting Misinformation. Table 1 shows the average misinformation rate of models on the FARM dataset; lower is better. We show only the Llama-3.1-70B numbers here, with similar trends on other models in Appendix A. First, resist-only training reduces the rate at which models are misinformed, reducing the average rate by 45.69% (absolute). Moreover, combined training also reduces the rate substantially by 38.13%, and even beats resist-only training on NQ for Llama-3.1-70B. This indicates that training on our data generated from TriviaQA transfers well to other datasets. Finally, as expected, accept-only training over-accepts and results in higher rates compared to the untrained baseline.\nResisting Flipflopping. Table 2 shows the accuracy of different models using the \u201cAre you sure?\u201d prompt from Laban et al. (2023); we report results from Llama-3.1-70B with similar trends on other models in Appendix A. Base model accuracy decreases when the model is questioned, dropping by 33.00%. Training models to resist negative persuasion eliminates this decrease, with only a 0.40% drop. However, the resist-only accuracy is also much lower (43.87% vs 73.10%), with high variance between runs; we find that some runs of resist-only lead to a local optimum where the model refuses to answer questions, leading to low accuracy. Similarly, accept-only training lowers the accuracy, although it actually results in a smaller drop of 9.50% compared to the baseline. Crucially, PBT's balanced training consistently leads to the highest accuracies after the model is challenged, with the 70B model in fact improving slightly by 0.23%. In other words, PBT gives us the best of both worlds: high accuracy and resistance to flipflopping."}, {"title": "4.2 RQ2: Addressing Positive Persuasion", "content": "We argue that resistance to negative persuasion is only one half of the picture: models should not only be resistant to wrong answers but should also be able to accept right answers, as outlined in Fig. 1. Moreover, being excessively focused on resisting negative persuasion may lead to models that over-correct, i.e. become impossible to persuade. Table 3 quantifies this, evaluating on a balanced dataset of positive (\u2212 \u2192 +) and negative (+ \u2192 -) persuasion. PBT consistently performs best in overall accuracy, which is balanced between positive and negative. For both Llama models, PBT leads to the highest performance on all metrics. The fact that data from weaker 7B and 8B models improves Llama-3.1-70B is particularly promising. In general, resist-only training helps negative persuasion but destroys the model's ability to accept positive persuasion, leading to lower overall scores. The opposite holds for accept-only, which generally increases the model's ability on positive persuasion but hampers its resist ability."}, {"title": "4.3 RQ3: Building Effective LLM Teams", "content": "We pair one strong model (Llama-3.1-70B) with a weaker model (Llama-3.1-8B) to examine how persuasion affects performance when there are strength imbalances on an LLM team. Fig. 3 shows the average accuracy on a 1000-question subset of TriviaQA validation questions for different teams. We vary the 70B model, holding the weaker model fixed, and within each pair we vary which model responds first. The blue and black lines indicate each model's accuracy before discussion, i.e. the baseline or \"solo\" - accuracy of each model.\nBase-Base and Base-Accept have variable team performance. When evaluating two base models, we find that the order of the models has a substantial effect: when the stronger model goes second, it brings the weak model up to its level, but when the weaker model goes second, it brings the stronger model down. The gap is shown in more detail in Fig. 4, where we see a major drop between the team columns for base-base when the order is changed. We also report the gap as a fraction of the initial difference between the untrained base models (left-most blue columns in Fig. 4), with 0% meaning no drop from the 70B model and 100% meaning a drop all the way down to the weaker 8B's model's performance. The initial difference represents by how much the weaker model could lower the stronger model's performance, and we report the fraction of that total that is realized. For the base-base pair, the gap between orderings (8B first vs. 70B first) represents a 82.1% of the initial difference; for base-accept, it is 50.8%. This is troubling, as it means choosing the wrong model to go first can drastically hurt performance, and it puts the onus of choosing models on the user. The choice may be further complicated by the fact that there may not always be a single stronger model. Note that this \"second model\" trend follows from the design of our dialogues, since we have both models answer the question before discussing. Thus, (given models A and B), the first turns from A and B are independent, but the second turn from A (third overall turn) is conditioned on B. In other words, the first model is also influenced first.\nBase-resist has weak performance. As in Table 2 and Table 3, resist-only training leads to poor overall accuracy, meaning the Llama-3.1-70B model is actually weaker than the Llama-3.1-8B model and consistently pulls it down. Because of this, we exclude it from Fig. 4. Qualitatively, the resist agent typically derails the dialogue due to the fact that it always disagrees and sometimes refuses to answer the question, leading to lower accuracy.\nPBT improves team performance and reduces variability. When pairing a weaker 8B model with a 70B model trained with PBT, we obtain the best average team performance of 74.1%. Moreover, regardless of which model goes first, the 70B model pulls up the 8B model, with the smallest gap. In Fig. 4, the Base-PBT team has the highest average team performance across both orders, and the \"70B first\" team is closest to the 70B solo performance. Nevertheless, there is a decrease in the 70B accuracy when it goes first, with a 2.1% drop from the baseline; this gap only represents 17.8% of the difference between the baseline models' performance and is much smaller than the base-base and base-accept gaps (82.1% and 50.8%). These results are promising in that they help alleviate the burden of choosing the first model and indicate that PBT creates more robust teammates."}, {"title": "5 Discussion and Analysis", "content": "How does the model know when to flip? An open question is what features of the model \u2013 and the argument it is presented with \u2013 influence whether the PBT model will accept or reject the answer. Here, we explore different signals that the model might be exploiting in its decision to flip its answer or not. We take turns from the balanced test data and filter for triples in the following answer format: A, B, B, where A is the target model's answer and B is the other model's answer (i.e. target model flips), and A, B, A, where the target maintains its initial answer exactly. Using Llama-3.1-8B with PBT, we extract the following features of the model: (1) Ans. H, the entropy of its answer distribution, computed by sampling the base model 20 times with temperature and binning the answers. (2) log Porig., the model's probability on the original answer A, extracted via MiniCons (Misra, 2022) by forced-decoding the answer after the tokens Final answer:. (3) log Palt., the model's probability on the alternate answer B. We also add the following external features: (4) Conf.orig., the perceived confidence of the previous turn, extracted following Stengel-Eskin et al. (2024). (5) Conf.alt., the perceived confidence of the alternate turn. (6) Acc., whether B is correct.\nWe train and evaluate a logistic regression model on these models to predict whether the answer is flipped with 10-fold cross-validation. The average accuracy of the model is 96.36%. The feature weights are given in Table 4; the only significant features are the probabilities, and the model performs similarly with just these two features (95.91%). Thus, the model is learning to rely on answer plausibility under its own language distribution to determine when to switch; this plausibility correlates with correctness. Even when the model fails to generate the correct answer, it can discriminate between correct and incorrect answers, paralleling past findings (Naor, 1996; Gu et al., 2023).\nQualitative examples. Fig. 5 shows examples of positive and negative persuasion. In the first example (negative persuasion) both the PBT and resist-only model correctly resist and maintain their correct answer, whereas the accept-only falsely accepts the wrong answer. In the second example (positive persuasion) the accept-only model correctly accepts, while the resist-only model falsely resist, maintaining an incorrect answer. The PBT model correctly accepts the correction, and is the only model that is right on both examples.\nDiscussion. A large body of work has explored persuasion in human interactions and language (Petty and Cacioppo, 1986; Durmus and Cardie, 2018, 2019). Broadly speaking, we see certain parallels in behavior between model teams and human teams, which can also be susceptible to \u201canchoring biases\" whereby information observed first holds disproportionate sway over the conversation (Sox et al., 2024; Stasser and Titus, 1985). The modular nature of the prompts in Section 3 means that future work might adopt insights about conversational strategies to mitigate these \u2013 and other \u2013 negative biases and thereby improve teamwork. Given that LLMs are models of human language, we expect that many of the interventions that help people might also trigger models to engage in better conversations. One particularly promising connection is to Woolley et al. (2010), who argue that group intelligence is driven more by social sensitivity, diversity, and turn-taking than by the group members' individual intelligence. This, in turn, suggests that aligning models to be good teammates is a potential way to improve performance and that even weak models can improve (and be improved by) teams.\""}, {"title": "6 Conclusion", "content": "We focus on the problem of persuasion in LLMs, finding that LLMs are too easily persuaded. We also note the importance of accepting persuasion when it can improve the model's answer. By automatically creating preference data through LLM dialogue trees, we show how to align models to accept persuasion when appropriate, leading to LLMs that resist misinformation and flipflopping while still accepting corrections."}, {"title": "Limitations", "content": "To measure persuasion, we extract and compare closed-form answers to questions. This allows us to scalably create training data for persuasion and automatically evaluate model performance but also leads to two limitations. Like past work (Joshi et al., 2017; Stengel-Eskin et al., 2024) we are limited to domains where such answers are available (e.g. trivia) and languages like English for which such data has been annotated.\nWe also note that the question of whether LLMs can have beliefs is unresolved (Hofweber et al., 2024) and we aim to avoid claims about the beliefs that LLMs may or may not have, focusing on what we can observe: the beliefs expressed in their outputs. Past work has found that models tend towards sycophancy (Sharma et al., 2023), i.e. reporting beliefs that are in agreement with their interlocutor, even when the model might more consistently report different beliefs when questioned in a neutral context. Without access to the belief state of an agent, we cannot truly know if it has been persuaded and changed its belief, or whether it is simply paying lip service to its interlocutor. This problem exists also in evaluating human beliefs, where past work has found self-reported beliefs to be inconsistent (Nisbett and Wilson, 1977) and biased towards beliefs that might be perceived favorably by others (Podsakoff et al., 2012), and has documented persistent gaps between beliefs and behavior (Fishbein and Ajzen, 1975, 2011).\nFinally, PBT trains models to accept and resist persuasion as appropriate, with the goal of improving factual beliefs about trivia questions, i.e. beliefs about how things are. While we do not foresee any particular risks associated with this domain, and making models resistant to persuasion makes them robust to misinformation (improving safety), it could also reduce their controllability, i.e. make them more \"stubborn\"."}]}