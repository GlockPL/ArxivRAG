{"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "authors": ["Tu Vu", "Kalpesh Krishna", "Salaheddin Alzubi", "Chris Tar", "Manaal Faruqui", "Yun-Hsuan Sung"], "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their\noutput due to the high costs of human evaluation. To make progress towards better LLM autoraters,\nwe introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large\nand diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and\nstandardized using publicly released human evaluations from previous research. FLAMe significantly\nimproves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary\ndata like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful\nstarting point for further downstream fine-tuning, using reward modeling evaluation as a case study\n(FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the\ntop-performing generative model trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-40 (84.7%). Additionally, we explore a more computationally efficient\napproach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for\nreward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while\nrequiring approximately 25\u00d7 less training datapoints. Overall, our FLAMe variants outperform all popular\nproprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks,\nencompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our\nanalysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr\nautorater bias benchmark, while effectively identifying high-quality responses for code generation.", "sections": [{"title": "1. Introduction", "content": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their\noutput due to the high costs of human evaluation. To make progress towards better LLM autoraters,\nwe introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large\nand diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and\nstandardized using publicly released human evaluations from previous research. FLAMe significantly\nimproves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary\ndata like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful\nstarting point for further downstream fine-tuning, using reward modeling evaluation as a case study\n(FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the\ntop-performing generative model trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-40 (84.7%). Additionally, we explore a more computationally efficient\napproach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for\nreward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while\nrequiring approximately 25\u00d7 less training datapoints. Overall, our FLAMe variants outperform all popular\nproprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks,\nencompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our\nanalysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr\nautorater bias benchmark, while effectively identifying high-quality responses for code generation."}, {"title": "2. Related Work", "content": "Below, we discuss existing literature in the space of autoraters, drawing connections to FLAMe.\nTraditional metrics like BLEU and ROUGE assess the lexical overlap between model output and human references. In the BERT era, several methods use pretrained models to measure the distributional similarity or token probabilities. A line of work explores statistical methods to measure the divergence between two text distributions. Other work fine-tunes pretrained models on human ratings to create automatic evaluation metrics for specific tasks, including machine translation, text summarization, question answering, and text simplification. Unlike these task-specific evaluation\nmetrics, FLAMe is trained on various fine-grained quality assessment tasks and can be prompted\nduring inference to tackle novel tasks.\nWith the advent of LLMs like ChatGPT, recent work has used these\nmodels as judges to evaluate LLM capabilities on various benchmarks, including AlpacaEval, MT-Bench, and"}, {"title": "3. The FLAMe Collection", "content": "At a high level, we fine-tune instruction-tuned LLMs on our multitask mixture of standardized human\nevaluations (102 tasks, 5.3M human judgments). This data collection is meticulously curated to\nencompass human evaluations across a broad spectrum of LLM capabilities (Section 3.2-3.3). We\nmanually crafted task definitions and evaluation instructions, reformatting all tasks into a unified\ntext-to-text format (Section 3.4).\nWe use the term \u201ctask\u201d to refer to a specific assignment for the model, which involves presenting a text\n(e.g., a machine-generated summary) alongside its context (the original article) and instructing the\nmodel to evaluate one or more aspects of the text based on provided evaluation criteria (see Figure 2).\nEach task has distinct definitions and evaluation guidelines. It is possible to derive different tasks\nfrom the same dataset. For example, HelpSteer includes human annotations\nfor different attributes of model responses such as Helpfulness, Correctness, Coherence, Complexity,\nand Verbosity, allowing us to create distinct tasks, each focused on a specific attribute. Additionally,\ntasks with similar definitions and evaluation criteria but sourced from different datasets are treated\nas distinct tasks. Based on this definition, the FLAMe collection has a total of 102 distinct tasks."}, {"title": "3.2. Principles for Data Collection", "content": "We adhere to the following principles while choosing our datasets:\nFor transparency and reproducibility, we use only permissively\nlicensed datasets from HuggingFace Datasets, TensorFlow Datasets, or the\noriginal authors' GitHub repositories.\nWe exclusively use datasets with human-labeled annotations, avoid-\ning those generated by models like GPT-4 due to potential inaccuracies and legal concerns raised in\nrecent research\nTo enhance the generalizability of our models, we gather datasets from a\ndiverse range of task types. These include (see breakdown in Figure 3):\nTasks that involve comparing two responses at a time to determine a\npreference (e.g., \u201cWhich response, A or B, is more helpful?\u201d).\nTasks that involve evaluating specific attributes of individual responses\nindependently (e.g., \u201cPlease rate the overall coherence of the response on a 5-point Likert scale.\u201d).\nTasks that involve categorizing individual responses into predefined categories\n(e.g., \u201cDoes the model output follow the instructions? (Yes/No)\").\nTasks that require free-form, unrestricted answers (e.g., \u201cIs the\nsummary fully attributable to the source article? Provide a short explanation.\u201d).\nWe choose datasets from literature that assess diverse LLM capabili-\nties, including factuality, safety, reasoning, instruction-following, long-form generation, creativity,\nattribution, coding, etc. (see Section 3.3).\""}, {"title": "3.3. LLM Capabilities Covered by FLAMe", "content": "Following the principles outlined in Section 3.2, we curated a comprehensive data collection of\n5.3M datapoints, spanning 102 training tasks (with an additional 53 tasks reserved for evaluation,\nas detailed in Section 5.1). Appendix 8 contains the list of datasets used in our study. Our data\ncollection encompasses key capabilities of contemporary LLMs, as outlined below (see breakdown in\nFigure 4).\nTo evaluate LLM response quality, we use various datasets that measure\nattributes like helpfulness, coherence, fluency, creativity, complexity, and verbosity. These include:\nSummary Comparisons (SummFeedback) LMSYS Chatbot Arena conversa-\ntions WebGPT (SummEval News Summary Evaluation BeaverTails Helpfulness SEAHORSE HelpSteer etc. Additionally, to measure LLM instruction-following capabilities, we include datasets\nlike GENIE InstruSum and riSum (Skopek et al., 2023).\nTo address the increasing importance of measuring hallucinations in gener-\nated LLM responses, we incorporate several datasets that evaluate the factual accuracy of responses\nand their grounding, measuring whether claims are supported by source documents. These include:\nXSum Hallucination WikiBio Hallucination FRANK FactScore VitaminC HaluEval and MNLI etc.\nWe construct datasets to help FLAMe differentiate between correct\nand incorrect solutions to mathematical problems. We leverage PRM800K and extract human vs incorrect LLM-generated solutions, as well as pairs of (correct, incorrect)\nLLM-generated solutions."}, {"title": "3.4. Unified Task Format", "content": "After carefully selecting our training datasets (Section 3.2-3.3), we process and standardize them\ninto a unified text-to-text format. This preprocessing step typically takes about 3-4 hours per dataset\nand involves several key tasks:\nWe carefully review the associated re-\nsearch and additionally consult with the original authors to clarify ambiguities or inconsistencies.\nWe collect all relevant data files from the corresponding HuggingFace Datasets,\nTensorFlow Datasets, or GitHub repositories.\nWe identify and extract specific data fields containing quality assessments\nconducted by human annotators.\nWe meticulously create detailed task defini-\ntions and evaluation instructions for each quality assessment task, ensuring consistency and\nstandardization. To maintain alignment with the original evaluation criteria, we adhere to any\navailable instructions provided to the original human annotators. Our instructions help the\nmodel identify the input and output formats, as well as understand the specific aspects it should\nassess.\nFinally, we reformat all tasks as text-to-text tasks (see\nFigure 2). Task definitions, evaluation instructions, and desired output fields are listed under\nan INSTRUCTIONS block. Input field values and target field values are placed under CONTEXT\nand EVALUATION blocks, respectively. This flexible text-to-text format is easily adaptable to a\nwide range of quality assessment tasks."}, {"title": "4. Model", "content": "We now leverage our large and diverse multitask mixture of quality assessment tasks to train general-purpose LLM autoraters, which can be prompted during inference to perform various tasks. We train\nthree model variants: FLAMe, which is trained with examples-proportional mixture weights FLAMe-RM, which is initialized with FLAMe and slightly fine-tuned on a balanced\nmixture of four pairwise evaluation datasets, spanning chat, reasoning, and safety (Section 4.2);\nand FLAMe-Opt-RM, which is trained with reward modeling optimized mixture weights, determined\nusing a tail-patch fine-tuning strategy (Section 4.3).\nWe start with a baseline training approach by using supervised multitask training to train an instruction-tuned PaLM-2-24B model on our multitask mixture for a fixed number of 30K training steps. We\nemploy examples-proportional mixture weights, capped at a maximum of 216 per task to avoid\noversampling large datasets. Our resulting FLAMe model significantly improves generalization to a\ndiverse array of held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many\ntasks (see Figure 1 and Table 1). These findings support our hypothesis that large-scale multitask\ninstruction tuning effectively equips the model with general-purpose quality assessment capabilities.\nHowever, we find that this approach is not optimal for specialized downstream applications like reward\nmodeling evaluation, which motivates our approaches targeting specific downstream distributions\n(Section 4.2 and Section 4.3)."}, {"title": "4.2. Fine-tuning FLAMe for Reward Modeling Evaluation (FLAMe-RM)", "content": "Motivated by our findings with FLAMe, we delve deeper into the potential of FLAMe as a powerful\nstarting point for further fine-tuning on specific downstream applications. We focus on reward\nmodeling evaluation as a case study. We create FLAMe-RM by fine-tuning FLAMe on a mixture of four\npairwise evaluation datasets, equally mixed, spanning chat, reasoning, and safety. These include:\nHelpSteer PRM800K CommitPack and HH-RLHF Harmlessness. Since FLAMe is already trained on these\ndatasets, we only fine-tune it for 50 steps. The resulting FLAMe-RM model significantly improves the\noriginal FLAMe's RewardBench overall score from 86.0% to 87.8% accuracy. Remarkably, FLAMe-\nRM-24B is the top-performing generative model trained exclusively on permissively licensed data,\nsurpassing both GPT-4-0125 (85.9%) and GPT-40 (84.7%); see Figure 1 and Table 1."}, {"title": "4.3. Optimizing FLAMe Multitask Mixture for Reward Modeling Evaluation (FLAME-Opt-RM)", "content": "While our vanilla FLAMe mixture with examples-proportional mixing performs well across many\ntasks, it requires extensive training to attain strong performance on certain specialized downstream\napplications, for example, RewardBench (see Figure 5). We attribute this to suboptimal mixture\nweights that undersample beneficial tasks during training. To address this, we introduce a novel tail-patch ablation strategy that analyzes the impact of each dataset on targeted distributions. This allows\nus to find the optimal proportions of individual datasets in our multitask mixture, efficiently optimizing\nall mixing weight hyperparameters at once. By fine-tuning the initial instruction-tuned PaLM-2-24B\ncheckpoint on this optimized mixture for only 5000 steps, we achieve competitive RewardBench\nperformance (87.0%) with our baseline FLAMe approach (86.0%) while using approximately 25\u00d7\nless training datapoints.\nHere, we directly optimized our multitask mixture based on RewardBench performance changes\ndue to its lack of a development set. In early experiments, we observed weak correlations between\nRewardBench performance and performance on our other held-out tasks across model variants,\npreventing us from creating a reliable \u201cproxy\u201d development set. We emphasize that our goal here is\nnot to achieve state-of-the-art RewardBench results but instead to demonstrate how our multitask\nmixture can be optimized for targeted distributions. We found that longer training and/or additional"}, {"title": "Tail-patch Ablations to Determine Beneficial Tasks:", "content": "Setting the right mixing weight for each\nindividual training task in our multitask mixture is non-trivial due to the large number of tasks.\nInstead, we examine the impact of each task on targeted distributions, and then use this information\nfor weight assignment. First, we select a checkpoint that is partially trained on our vanilla mixture,\nshowing decent but not optimal performance across RewardBench categories.\u201d Then, we perform\na brief fine-tuning stage (\u201ctail-patch\") exclusively on each individual training task, limited to 3000\ntraining steps. We posit that training on a beneficial task would bridge the gap between fair and\noptimal performance. We note that this is a one-time procedure per downstream application and can\nbe done with smaller models to further reduce computational costs.\""}, {"title": "A Re-weighted Mixture Based on Tail-patch Ablations:", "content": "After training a tail-patch on each task,\nwe rate how helpful each training task is to each category of RewardBench using one of four rat-\nings: Helpful (+2, performance significantly improves and remains stable), Somewhat helpful (+1,\nperformance slightly improves), No clear effect (0, performance is nearly unchanged), Harmful (-1,\nperformance is significantly worse). We then organize tasks into seven bundles: Generally helpful\n(tasks with the highest total ratings, \u2265 5 in our study), Category-specific, one for each of the five\nRewardBench categories (most beneficial tasks for a specific category where performance crosses a\nthreshold \u03c4),8 and Others for the remaining tasks.\nWe assign a fixed mixing weight for each bundle: \\(w_{general}=100K\\) for Generally helpful, \\(w_{specific}=30K\\)\nfor each Category-specific bundle, and \\(w_{others}=3K\\) for Others. A task can belong to more than one"}, {"title": "4.4. Training Details", "content": "We initialize both FLAMe and FLAMe-Opt-RM with the PaLM-2-24B model instruction-tuned on the Flan collection and train for 30K\nand 5K steps, respectively. FLAMe is then further fine-tuned for 50 steps to create FLAMe-RM. Our\nmodels are trained using T5X with the Adam optimizer, a learning rate of 0.0001, and a dropout rate of 0.05. FLAMe is trained on 256 Cloud TPU chips with\na batch size of 32, whereas FLAMe-RM and FLAMe-Opt-RM use 128 Cloud TPU chips with a batch\nsize of 8.9"}, {"title": "5. Experiments", "content": "Having discussed our FLAMe variants and their implementations in Section 3, we now present our\nmain experiments. We compare FLAMe to several popular LLM-as-a-Judge autoraters (Section 5.2)\nusing an evaluation suite that includes 12 autorater benchmarks: 1 held-in and 11 held-out, covering\na total of 53 quality assessment tasks (Section 5.1). Overall, we find that our FLAMe variants, trained\nexclusively on permissively licensed data, outperform LLMs trained on proprietary data like GPT-4\nand Claude-3 on 8 out of 12 benchmarks (Section 5.3).\nOur goal is to measure general quality assessment capabilities of FLAMe. As such, we evaluate our\nmodels using a diverse set of held-in and held-out tasks. We cast each task into our unified text-to-text\nformat (Section 3.4) and prompt our models to perform the task. For benchmarks with multiple\ncategories (e.g., RewardBench and LLM-AggreFact), we use the same prompt instructions across\ncategories. To reduce model API costs, we randomly sample 256 examples per evaluation task, 10\nexcept for RewardBench, where we report results on the full evaluation sets."}, {"title": "5.1.1. Held-in Evaluations", "content": "HelpSteer We assess FLAMe's performance on rating helpfulness, correctness,\ncoherence, complexity, and verbosity, using the HelpSteer validation set."}, {"title": "5.1.2. Held-out Evaluations", "content": "RewardBench RewardBench is a widely used benchmark for assessing\nreward models, focusing on their capabilities and safety. It involves pairwise preference tasks where\nreward models choose the better response between two options based on a given prompt. Reward-\nBench encompasses four main categories aimed at evaluating specific desired capabilities in LLMs:"}, {"title": "5.2. Evaluated Models", "content": "We evaluate several popular LLM-as-a-Judge models as baselines, including: Llama-3-70B-Instruct, Mixtral 8\u00d77B Claude-3-Opus GPT-3.5-turbo-0125 (OpenAI, 2024a), GPT-4-0125 and OpenAI's current flagship model GPT-40 (OpenAI, 2024c). We also compare our results with several models on the official RewardBench leaderboard, notably Gemini-1.5-Pro Prometheus-2-8\u00d77B and NVIDIA's\nNemotron-4-340B-Reward and Llama-3-70B-SteerLM-RM.\nWe evaluate all our three FLAMe variants: FLAMe, FLAMe-RM, and FLAMe-Opt-RM, as described in\nSection 4.1-Section 4.3. Additionally, we include the initial instruction-tuned PaLM-2-24B checkpoint,\nwhich has not been trained on our FLAMe data, to separate the impact of instruction tuning and\nFLAMe training."}, {"title": "5.3. Main Results", "content": "Table 1 shows our main results across all evaluation benchmarks. RewardBench and LLM-AggreFact\nresults are shown in Table 2 and Table 3, respectively. Below, we first provide an overview of these\nresults before analyzing them in more detail:\nOur\nresults in Table 1 suggest that FLAMe variants, despite being trained solely on permissively licensed\ndatasets, perform strongly across various evaluation benchmarks. Remarkably, our models outperform\nall state-of-the-art LLM-as-a-Judge models trained on proprietary data on 8 out of 12 benchmarks.\nFLAMe variants exceed the next-best model by a large margin on several held-out benchmarks,\nincluding: ContrSearch RankGen"}, {"title": "6. Further Analysis of FLAMe", "content": "In this section, we provide an analysis to elucidate some interesting aspects of our models. We depart\nfrom the usual focus on analyzing the effect of factors like model size, data size, and data quality in\nmultitask learning, which have been extensively studied in recent work on multitask learning and\ninstruction tuning Instead, we explore potential biases\ninherent in our LLM autoraters. Additionally, we demonstrate the potential utility of FLAMe for AI\ndevelopment, such as sampling high-quality responses.\nA common criticism of LLM-as-a-Judge autoraters involves their bias towards certain judgments . In this section, we evaluate FLAMe variants\non the CoBBLEr autorater bias benchmark. We find that our models are significantly\nless biased than other popular LLM-as-a-Judge autoraters.\nCOBBLEr measures six types of biases in LLM autoraters:\nDoes the autorater have a preference towards the response position?\nDoes the autorater's judgment change when the response-generating LLM's actual\nname, such as \u201cGPT-4\u201d, is used instead of aliases like \"Model A\"?\nDoes the autorater have a preference for longer or shorter outputs?\nDoes the autorater have a preference for outputs generated by itself?\nDoes the autorater get swayed by sentences like \u201c90% people prefer response A\u201d?\nDoes the autorater get distracted by irrelevant context, such as \u201cResponse A is about\ncats.\"?\""}, {"title": "6.2. Using FLAMe to Re-rank Decoded Outputs", "content": "Finally, we explore the application of our LLM autoraters in selecting optimal outputs from multiple\nresponses, a method known as \u201cBest-of-N\u201d sampling Using\nFLAMe for re-ranking, we assess its impact on code generation performance with the HumanEval\nPython programming benchmark . We conduct experiments by re-ranking 10\ncode samples generated by three models: OpenAI's davinci-002, InCoder-6B and CodeGen-16B using a round-robin competition, and then measuring\nperformance with the top-ranked code sample.15 Results in Table 5 show that FLAMe provides\nsignificant gains in pass@1 accuracy across all three models. Notably, FLAMe improves CodeGen-\n16B's pass@1 from 21.2 to 31.1, closing nearly 40% of the gap to the Oracle ranker (46.9)."}, {"title": "7. Conclusion", "content": "We introduce FLAMe, a family of foundational autorater models that can perform various quality\nassessment tasks. FLAMe is trained on a large and diverse collection of curated and standardized\nhuman evaluations derived exclusively from permissively licensed datasets. We demonstrate FLAMe's\nstrong zero-shot generalization abilities, outperforming models trained on proprietary data like GPT-4\nand Claude-3 on many held-out tasks. FLAMe can also effectively serve as a powerful starting point\nfor further downstream fine-tuning. Our FLAMe-RM variant, which is fine-tuned for reward modeling\nevaluation, is among the top-performing generative models on RewardBench, despite being trained\nsolely on permissively licensed data, outperforming both GPT-4-0125 and GPT-40. Additionally, we\npresent a more computationally efficient approach using a novel tail-patch fine-tuning strategy to\noptimize our FLAMe multitask mixture for targeted distributions, offering competitive performance\nwith significantly less compute. Our FLAMe variants outperform popular proprietary LLM-as-a-\nJudge models across 8 out of 12 autorater evaluation benchmarks, covering 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe exhibits\nsignificantly lower bias compared to popular LLM-as-a-Judge models on the CoBBLEr autorater bias\nbenchmark, while effectively identifying high-quality responses for code generation."}, {"title": "Limitations and Future work", "content": "Evaluating LLMs is challenging due to evolving evaluation standards and the need to assess new LLM\ncapabilities. Expanding our data collection with open-source contributions could address this issue.\nAdditionally, our models, trained primarily on English data with a context length of 2048 tokens,\nmight not perform well on multilingual or long-context quality assessment tasks. In future releases, we plan to include training on more\nmultilingual datasets with longer context lengths. Finally, in this work, we train our models in a\nsupervised multitask fashion. Exploring alternative training approaches such as RLHF and DPO is a\npromising direction for future work."}, {"title": "Ethical Considerations and Risks", "content": "All considerations and risks outlined by prior work for pretrained and instruction-tuned LLMs apply to LLM autoraters. We recommend following standard\npractice for responsible development of these models . Additionally, LLM autoraters raise new risks due to increased quality assessment\ncapabilities. First, our models can inherit and amplify biases from human evaluations, leading to\nunfair or discriminatory outcomes. For instance, the model may replicate biases related to race,\ngender, or other sensitive attributes from the training data, potentially harming certain groups.\nSecond, overreliance on LLM autoraters risks automating decisions that need human understanding\nand empathy. To mitigate these risks, transparency in model development and use, along with robust\nmeasures like bias audits, data anonymization, and incorporating diverse perspectives, is essential for\npromoting fairness, accountability, and trustworthiness."}, {"title": "Appendix", "content": "Table 7 shows the list of datasets used in our study."}, {"title": "9. Analyzing Length and Token Bias in RewardBench", "content": "In this section, we provide an analysis of length (Appendix 9.1) and token (Appendix 9.2) bias issues\nidentified in the RewardBench benchmark. Given these issues, we encourage future work to evaluate\nLLM autoraters on a wide variety of benchmarks (such as our evaluation suite in Section 5), rather\nthan relying solely on RewardBench."}, {"title": "9.1. Length Bias in RewardBench", "content": "highlights length bias in RewardBench. Overall, RewardBench shows significant imbalance\nacross categories regarding length: Chat Hard, Math, and Coding favor shorter outputs, while Chat\nleans towards longer outputs. An adversarial submission might strategically select longer or shorter\noutputs based on prompt categories to achieve higher scores, without necessarily reflecting a genuinely\nstrong preference model."}, {"title": "9.2. Token Bias in RewardBench", "content": "Besides length bias, we identified token bias in the Math and Safety categories of RewardBench. In\nSafety, favored responses significantly leaned towards phrases like \u201cI'm sorry\u201d, which suggest hedged\nresponses. The word \u201csorry\u201d appeared nearly 23% more frequently in preferred responses compared\nto non-preferred ones. Similarly, the Math split exhibited token bias, where tokens such as \u201ci\u201d, \u201ccan\u201d,\n\u201cneed\u201d, \u201cto\u201d, \u201cfind\u201d were predominantly found in rejected responses."}]}