{"title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "authors": ["Tu Vu", "Kalpesh Krishna", "Salaheddin Alzubi", "Chris Tar", "Manaal Faruqui", "Yun-Hsuan Sung"], "abstract": "As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on our large and diverse collection of 100+ quality assessment tasks comprising 5M+ human judgments, curated and standardized using publicly released human evaluations from previous research. FLAMe significantly improves generalization to a wide variety of held-out tasks, outperforming LLMs trained on proprietary data like GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a powerful starting point for further downstream fine-tuning, using reward modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative model trained exclusively on permissively licensed data, outperforming both GPT-4-0125 (85.9%) and GPT-40 (84.7%). Additionally, we explore a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for reward modeling evaluation (FLAMe-Opt-RM), offering competitive RewardBench performance while requiring approximately 25\u00d7 less training datapoints. Overall, our FLAMe variants outperform all popular proprietary LLM-as-a-Judge models we consider across 8 out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals that FLAMe is significantly less biased than these LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation.", "sections": [{"title": "1. Introduction", "content": "The increasing power and versatility of large language models (LLMs) bring with them a growing challenge: How can we reliably evaluate their long-form outputs? Recent research suggests a promising solution: these models themselves, after undergoing large-scale multitask instruction tuning, can generalize to follow new human instructions (Chung et al., 2024; Longpre et al., 2023; Mishra et al., 2022; Sanh et al., 2022; Wei et al., 2022), making them suitable for use as autoraters of model outputs. This is particularly appealing because human evaluation, though crucial for assessing model performance, is limited by subjectivity (Krishna et al., 2023a), variability among raters (Karpinska et al., 2021), and the high costs of extensive evaluations (Min et al., 2023; Vu et al., 2023; Wei et al., 2024).\nTo align LLM autoraters with human preferences, training on human judgments is crucial (Ouyang et al., 2022). However, obtaining these judgments is both costly and time-consuming. Collecting existing human evaluations from previous research seems promising but faces challenges such as lack of standardization, diverse evaluation criteria, inadequate documentation, data privacy, and proprietary concerns. Alternatively, using model outputs for autorater training offers consistency (Jiang et al., 2024b; Kim et al., 2024b) but also carries with risks, including reinforcing biases and hallucinations (Gudibande et al., 2023; Muennighoff et al., 2023). Additionally, it may violate terms of use for proprietary LLM services, which prohibit using their models' outputs to develop competing models.\nTo address these limitations, we curated and standardized human evaluations from prior research to create FLAMe, a collection of 102 quality assessment tasks comprising more than 5.3M total human judgments (Section 3). FLAMe spans a wide variety of task types, from assessing machine translation quality to evaluating how well AI assistants follow user instructions. We hypothesized that training on this large and diverse data collection would enable LLM autoraters to learn robust, generalized patterns of human judgment, minimizing the impact of noisy or low-quality human judgments.\nFor transparency and reproducibility, we use only publicly available human evaluation data with permissive licenses from previous studies (Section 3.2). To overcome challenges in collecting such data, which rarely adhere to a particular standard and often lack documentation, we thoroughly examined the associated research (Section 3.4) and additionally consulted with the original authors to address ambiguities or inconsistencies (spending 3-4 hours per dataset).\nWe train LLM autoraters using supervised, multitask fine-tuning on our data collection. Inspired by T5's unified task format (Raffel et al., 2020), we convert all our quality assessment tasks into a text-to-text format with manually crafted task definitions and evaluation instructions. All training examples are formulated as input-target pairs, where the input includes task-specific context, and the target contains the expected human evaluations (see Figure 2). This approach facilitates effective transfer learning across tasks, enabling our models to interpret and respond to various tasks consistently. Additionally, our task format is simple, intuitive, and easily accommodates new tasks.\nOur approach can be viewed as developing general-purpose LLM autoraters that can perform various quality assessment tasks. We demonstrate that training an instruction-tuned LLM, i.e., PaLM-2-24B (Anil et al., 2023), on our FLAMe collection significantly improves generalization to a wide variety of held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many tasks. This suggests that our large-scale multitask instruction tuning approach effectively equips the model with general-purpose quality assessment capabilities.\nMotivated by these results, we further investigate the impact of using FLAMe as a powerful starting point for fine-tuning on targeted downstream applications, using reward modeling evaluation as a case study (FLAMe-RM). Specifically, we further fine-tune FLAMe for only 50 steps on a mixture of"}, {"title": "3. The FLAMe Collection", "content": "At a high level, we fine-tune instruction-tuned LLMs on our multitask mixture of standardized human evaluations (102 tasks, 5.3M human judgments). This data collection is meticulously curated to encompass human evaluations across a broad spectrum of LLM capabilities (Section 3.2-3.3). We manually crafted task definitions and evaluation instructions, reformatting all tasks into a unified text-to-text format (Section 3.4)."}, {"title": "3.1. Task Definition", "content": "We use the term \u201ctask\u201d to refer to a specific assignment for the model, which involves presenting a text (e.g., a machine-generated summary) alongside its context (the original article) and instructing the model to evaluate one or more aspects of the text based on provided evaluation criteria (see Figure 2). Each task has distinct definitions and evaluation guidelines. It is possible to derive different tasks from the same dataset. For example, HelpSteer (Wang et al., 2023b) includes human annotations for different attributes of model responses such as Helpfulness, Correctness, Coherence, Complexity, and Verbosity, allowing us to create distinct tasks, each focused on a specific attribute. Additionally, tasks with similar definitions and evaluation criteria but sourced from different datasets are treated as distinct tasks. Based on this definition, the FLAMe collection has a total of 102 distinct tasks."}, {"title": "3.2. Principles for Data Collection", "content": "We adhere to the following principles while choosing our datasets:\nPublic, Open-source Datasets: For transparency and reproducibility, we use only permissively licensed datasets from HuggingFace Datasets (Lhoest et al., 2021), TensorFlow Datasets, or the original authors' GitHub repositories.\nHuman-labeled Annotations: We exclusively use datasets with human-labeled annotations, avoiding those generated by models like GPT-4 due to potential inaccuracies and legal concerns raised in recent research (Gudibande et al., 2023; Muennighoff et al., 2023).\nVarious Task Types: To enhance the generalizability of our models, we gather datasets from a diverse range of task types. These include (see breakdown in Figure 3):\nPairwise Evaluation: Tasks that involve comparing two responses at a time to determine a preference (e.g., \u201cWhich response, A or B, is more helpful?\u201d).\nPointwise Evaluation: Tasks that involve evaluating specific attributes of individual responses independently (e.g., \u201cPlease rate the overall coherence of the response on a 5-point Likert scale.\u201d).\nClassification: Tasks that involve categorizing individual responses into predefined categories (e.g., \u201cDoes the model output follow the instructions? (Yes/No)", "Evaluation": "Tasks that require free-form, unrestricted answers (e.g., \u201cIs the summary fully attributable to the source article? Provide a short explanation.", "Capabilities": "We choose datasets from literature that assess diverse LLM capabilities, including factuality, safety, reasoning, instruction-following, long-form generation, creativity, attribution, coding, etc. (see Section 3.3)."}, {"title": "3.3. LLM Capabilities Covered by FLAMe", "content": "Following the principles outlined in Section 3.2, we curated a comprehensive data collection of 5.3M datapoints, spanning 102 training tasks (with an additional 53 tasks reserved for evaluation, as detailed in Section 5.1). Appendix 8 contains the list of datasets used in our study. Our data collection encompasses key capabilities of contemporary LLMs, as outlined below (see breakdown in Figure 4).\nGeneral Response Quality: To evaluate LLM response quality, we use various datasets that measure attributes like helpfulness, coherence, fluency, creativity, complexity, and verbosity. These include: Summary Comparisons (SummFeedback) (Stiennon et al., 2020), LMSYS Chatbot Arena conversations (Zheng et al., 2023), HH RLHF Helpfulness (Bai et al., 2022a), WebGPT (Nakano et al., 2021), SummEval (Fabbri et al., 2021), News Summary Evaluation (Goyal et al., 2022), SHP (Ethayarajh et al., 2022), BeaverTails Helpfulness (Ji et al., 2023), SEAHORSE (Clark et al., 2023), HelpSteer (Wang et al., 2023b), etc. Additionally, to measure LLM instruction-following capabilities, we include datasets like GENIE (Khashabi et al., 2022), InstruSum (Liu et al., 2024), and riSum (Skopek et al., 2023).\nFactuality/Attribution: To address the increasing importance of measuring hallucinations in generated LLM responses, we incorporate several datasets that evaluate the factual accuracy of responses and their grounding, measuring whether claims are supported by source documents. These include: XSum Hallucination (Maynez et al., 2020), QAGS (Wang et al., 2020), WikiBio Hallucination (Manakul et al., 2023), FRANK (Pagnoni et al., 2021), FactScore (Min et al., 2023), VitaminC (Schuster et al., 2021), HaluEval (Li et al., 2023a), Q\u00b2 (Honovich et al., 2021), FaithDial (Dziri et al., 2022a), DialFact (Gupta et al., 2022), BEGIN (Dziri et al., 2022b), and MNLI (Williams et al., 2018), etc.\nMathematical Reasoning: We construct datasets to help FLAMe differentiate between correct and incorrect solutions to mathematical problems. We leverage PRM800K (Lightman et al., 2024) and extract human vs incorrect LLM-generated solutions, as well as pairs of (correct, incorrect) LLM-generated solutions."}, {"title": "3.4. Unified Task Format", "content": "After carefully selecting our training datasets (Section 3.2-3.3), we process and standardize them into a unified text-to-text format. This preprocessing step typically takes about 3-4 hours per dataset and involves several key tasks:\nComprehensive Review and Author Consultations: We carefully review the associated research and additionally consult with the original authors to clarify ambiguities or inconsistencies.\nData Collection: We collect all relevant data files from the corresponding HuggingFace Datasets, TensorFlow Datasets, or GitHub repositories.\nData Extraction: We identify and extract specific data fields containing quality assessments conducted by human annotators.\nTask Definitions and Evaluation Instructions: We meticulously create detailed task definitions and evaluation instructions for each quality assessment task, ensuring consistency and standardization. To maintain alignment with the original evaluation criteria, we adhere to any available instructions provided to the original human annotators. Our instructions help the model identify the input and output formats, as well as understand the specific aspects it should assess.\nText-to-Text Format Conversion: Finally, we reformat all tasks as text-to-text tasks (see Figure 2). Task definitions, evaluation instructions, and desired output fields are listed under an INSTRUCTIONS block. Input field values and target field values are placed under CONTEXT and EVALUATION blocks, respectively. This flexible text-to-text format is easily adaptable to a wide range of quality assessment tasks."}, {"title": "4. Model", "content": "We now leverage our large and diverse multitask mixture of quality assessment tasks to train general-purpose LLM autoraters, which can be prompted during inference to perform various tasks. We train three model variants: FLAMe, which is trained with examples-proportional mixture weights (Raffel et al., 2020); FLAMe-RM, which is initialized with FLAMe and slightly fine-tuned on a balanced mixture of four pairwise evaluation datasets, spanning chat, reasoning, and safety (Section 4.2); and FLAMe-Opt-RM, which is trained with reward modeling optimized mixture weights, determined using a tail-patch fine-tuning strategy (Section 4.3)."}, {"title": "4.1. Training General-purpose LLM Autoraters (FLAMe)", "content": "We start with a baseline training approach by using supervised multitask training to train an instruction-tuned PaLM-2-24B model on our multitask mixture for a fixed number of 30K training steps. We employ examples-proportional mixture weights, capped at a maximum of \\(2^{16}\\) per task to avoid oversampling large datasets. Our resulting FLAMe model significantly improves generalization to a diverse array of held-out tasks, outperforming models like GPT-4, Claude-3, and Llama-3 on many tasks (see Figure 1 and Table 1). These findings support our hypothesis that large-scale multitask instruction tuning effectively equips the model with general-purpose quality assessment capabilities. However, we find that this approach is not optimal for specialized downstream applications like reward modeling evaluation, which motivates our approaches targeting specific downstream distributions (Section 4.2 and Section 4.3)."}, {"title": "4.2. Fine-tuning FLAMe for Reward Modeling Evaluation (FLAMe-RM)", "content": "Motivated by our findings with FLAMe, we delve deeper into the potential of FLAMe as a powerful starting point for further fine-tuning on specific downstream applications. We focus on reward modeling evaluation as a case study. We create FLAMe-RM by fine-tuning FLAMe on a mixture of four pairwise evaluation datasets, equally mixed, spanning chat, reasoning, and safety. These include: HelpSteer (Wang et al., 2023b), PRM800K (Lightman et al., 2024), CommitPack (Muennighoff et al., 2023), and HH-RLHF Harmlessness (Bai et al., 2022a). Since FLAMe is already trained on these datasets, we only fine-tune it for 50 steps. The resulting FLAMe-RM model significantly improves the original FLAMe's RewardBench overall score from 86.0% to 87.8% accuracy. Remarkably, FLAMe-RM-24B is the top-performing generative model trained exclusively on permissively licensed data, surpassing both GPT-4-0125 (85.9%) and GPT-40 (84.7%); see Figure 1 and Table 1."}, {"title": "4.3. Optimizing FLAMe Multitask Mixture for Reward Modeling Evaluation (FLAMe-Opt-RM)", "content": "While our vanilla FLAMe mixture with examples-proportional mixing performs well across many tasks, it requires extensive training to attain strong performance on certain specialized downstream applications, for example, RewardBench (see Figure 5). We attribute this to suboptimal mixture weights that undersample beneficial tasks during training. To address this, we introduce a novel tail-patch ablation strategy that analyzes the impact of each dataset on targeted distributions. This allows us to find the optimal proportions of individual datasets in our multitask mixture, efficiently optimizing all mixing weight hyperparameters at once. By fine-tuning the initial instruction-tuned PaLM-2-24B checkpoint on this optimized mixture for only 5000 steps, we achieve competitive RewardBench performance (87.0%) with our baseline FLAMe approach (86.0%) while using approximately 25\u00d7 less training datapoints.\nHere, we directly optimized our multitask mixture based on RewardBench performance changes due to its lack of a development set. In early experiments, we observed weak correlations between RewardBench performance and performance on our other held-out tasks across model variants, preventing us from creating a reliable \u201cproxy\u201d development set. We emphasize that our goal here is not to achieve state-of-the-art RewardBench results but instead to demonstrate how our multitask mixture can be optimized for targeted distributions. We found that longer training and/or additional"}, {"title": "5. Experiments", "content": "Having discussed our FLAMe variants and their implementations in Section 3, we now present our main experiments. We compare FLAMe to several popular LLM-as-a-Judge autoraters (Section 5.2) using an evaluation suite that includes 12 autorater benchmarks: 1 held-in and 11 held-out, covering a total of 53 quality assessment tasks (Section 5.1). Overall, we find that our FLAMe variants, trained exclusively on permissively licensed data, outperform LLMs trained on proprietary data like GPT-4 and Claude-3 on 8 out of 12 benchmarks (Section 5.3)."}, {"title": "5.1. Evaluation Datasets", "content": "Our goal is to measure general quality assessment capabilities of FLAMe. As such, we evaluate our models using a diverse set of held-in and held-out tasks. We cast each task into our unified text-to-text format (Section 3.4) and prompt our models to perform the task. For benchmarks with multiple categories (e.g., RewardBench and LLM-AggreFact), we use the same prompt instructions across categories. To reduce model API costs, we randomly sample 256 examples per evaluation task, except for RewardBench, where we report results on the full evaluation sets."}, {"title": "5.1.1. Held-in Evaluations", "content": "HelpSteer (Wang et al., 2023b): We assess FLAMe's performance on rating helpfulness, correctness, coherence, complexity, and verbosity, using the HelpSteer validation set."}, {"title": "5.1.2. Held-out Evaluations", "content": "RewardBench (Lambert et al., 2024): RewardBench is a widely used benchmark for assessing reward models, focusing on their capabilities and safety. It involves pairwise preference tasks where reward models choose the better response between two options based on a given prompt. Reward-Bench encompasses four main categories aimed at evaluating specific desired capabilities in LLMs:"}, {"title": "5.2. Evaluated Models", "content": "We evaluate several popular LLM-as-a-Judge models as baselines, including: Llama-3-70B-Instruct (Meta, 2024), Mixtral 8\u00d77B (Jiang et al., 2024a), Claude-3-Opus (Anthropic, 2024), GPT-3.5-turbo-0125 (OpenAI, 2024a), GPT-4-0125 (OpenAI, 2024b), and OpenAI's current flagship model GPT-4o (OpenAI, 2024c). We also compare our results with several models on the official RewardBench leaderboard, notably Gemini-1.5-Pro (Reid et al., 2024), Prometheus-2-8\u00d77B (Kim et al., 2024b), and NVIDIA's Nemotron-4-340B-Reward and Llama-3-70B-SteerLM-RM (Wang et al., 2024).\nWe evaluate all our three FLAMe variants: FLAMe, FLAMe-RM, and FLAMe-Opt-RM, as described in Section 4.1-Section 4.3. Additionally, we include the initial instruction-tuned PaLM-2-24B checkpoint, which has not been trained on our FLAMe data, to separate the impact of instruction tuning and FLAMe training."}, {"title": "5.3. Main Results", "content": "Table 1 shows our main results across all evaluation benchmarks. RewardBench and LLM-AggreFact results are shown in Table 2 and Table 3, respectively. Below, we first provide an overview of these results before analyzing them in more detail:\nFLAMe Variants Outperform all LLM-as-a-Judge baselines on 8 out of 12 benchmarks: Our results in Table 1 suggest that FLAMe variants, despite being trained solely on permissively licensed datasets, perform strongly across various evaluation benchmarks. Remarkably, our models outperform all state-of-the-art LLM-as-a-Judge models trained on proprietary data on 8 out of 12 benchmarks. FLAMe variants exceed the next-best model by a large margin on several held-out benchmarks, including: ContrSearch (69.9 vs 57.5 for GPT-4o/GPT-3.5-turbo-0125), RankGen (69.5 vs 66.0 for GPT-4o), AlpacaFarm (58.2 vs 55.5 for GPT-3.5-turbo-0125), SummFeedback (53.1 vs 50.8 for Llama-3-70B-Instruct), and RewardBench (87.8 vs 85.9 for GPT-4-0125). Unsurprisingly, our models also obtain the best held-in performance on HelpSteer (48.4 vs. 41.3 for Claude-3-Opus). On the other hand, FLAMe variants lag behind proprietary models on several benchmarks, including HHH (91.4 vs 94.6 for GPT-4-0125/Claude-3-Opus), LitTrans (69.5 vs 72.7 for GPT-4o), and LFQAEva (74.2 vs 77.0 for GPT-4-0125), suggesting that these proprietary models may have been optimized for these capabilities. Interestingly, GPT-4-0125 outperforms GPT-4o on 6 out of 12 benchmarks, including RewardBench, despite GPT-4o achieving a higher rank on the official LMSYS leaderboard (Chiang et al., 2024). Finally, FLAMe provides significant gains over the initial instruction-tuned PaLM-2-24B across almost all benchmarks, highlighting the benefits of FLAMe training. Overall, our results demonstrate FLAMe's robust generalization to held-out tasks, showcasing its effectiveness as a versatile LLM autorater.\nFLAMe Variants Are Among The Most Powerful Generative Models on RewardBench: Our results in Table 2 indicate that FLAMe variants are among the top-performing generative models on the official RewardBench leaderboard, achieving strong performance across all categories: Chat, Chard Hard, Safety, and Reasoning. Notably, FLAMe-RM-24B achieves an overall score of 87.8%, the best performance among generative models trained solely on permissively licensed data, surpassing both GPT-4-0125 (85.9) and GPT-40 (84.7). As of July 15, 2024, FLAMe-RM-24B ranks second among generative models (below Gemini-1.5-Pro) and sixth among all models (spanning various model types such as custom classifier, generative, sequence classifier, and DPO) on the official RewardBench leaderboard. While RewardBench is a widely used benchmark for evaluating reward models, we identified issues with length and token bias during our evaluations. We provide an analysis of bias in RewardBench in Appendix 9.\nFLAMe Attains the Best Performance on LLM-AggreFact: Finally, Table 3 presents our attribution results on LLM-AggreFact (Tang et al., 2024), categorized into four common use-cases: 1) LLM-FactVerify: fact verification of LLM-generated responses, 2) Wiki-FactVerify: evaluating correctness of Wikipedia claims, 3) Summarization: assessing faithfulness of summaries, and 4) Long-form QA: evaluating long-form answers to questions. FLAMe variants outperform all other models in three out of"}, {"title": "6. Further Analysis of FLAMe", "content": "In this section, we provide an analysis to elucidate some interesting aspects of our models. We depart from the usual focus on analyzing the effect of factors like model size, data size, and data quality in multitask learning, which have been extensively studied in recent work on multitask learning and instruction tuning (Longpre et al., 2023; Raffel et al., 2020). Instead, we explore potential biases inherent in our LLM autoraters. Additionally, we demonstrate the potential utility of FLAMe for AI development, such as sampling high-quality responses."}, {"title": "6.1. Autorater Bias Analysis", "content": "A common criticism of LLM-as-a-Judge autoraters involves their bias towards certain judgments (Bai et al., 2023; Liu et al., 2023a,b; Panickssery et al., 2024). In this section, we evaluate FLAMe variants on the CoBBLEr autorater bias benchmark (Koo et al., 2023). We find that our models are significantly less biased than other popular LLM-as-a-Judge autoraters.\nCOBBLEr measures six types of biases in LLM autoraters:\nOrder: Does the autorater have a preference towards the response position?\nCompassion: Does the autorater's judgment change when the response-generating LLM's actual name, such as \u201cGPT-4\u201d, is used instead of aliases like \"Model A\"?\nLength: Does the autorater have a preference for longer or shorter outputs?\nEgocentric: Does the autorater have a preference for outputs generated by itself?\nBandwagon: Does the autorater get swayed by sentences like \u201c90% people prefer response A\u201d?\nAttention: Does the autorater get distracted by irrelevant context, such as \u201cResponse A is about cats.\"?\nWe leverage the original (prompt,response) pairs from Koo et al. (2023) and reformat them into our unified FLAMe format (Figure 2). We compare FLAMe variants to other LLM-as-a-Judge autoraters reported in Koo et al. (2023), including GPT-4.\""}, {"title": "6.2. Using FLAMe to Re-rank Decoded Outputs", "content": "Finally, we explore the application of our LLM autoraters in selecting optimal outputs from multiple responses, a method known as \u201cBest-of-N\u201d sampling (Krishna et al., 2022; Nakano et al., 2021). Using FLAMe for re-ranking, we assess its impact on code generation performance with the HumanEval Python programming benchmark (Chen et al., 2021). We conduct experiments by re-ranking 10 code samples generated by three models: OpenAI's davinci-002, InCoder-6B (Fried et al., 2023), and CodeGen-16B (Nijkamp et al., 2023) using a round-robin competition, and then measuring performance with the top-ranked code sample. Results in Table 5 show that FLAMe provides significant gains in pass@1 accuracy across all three models. Notably, FLAMe improves CodeGen-16B's pass@1 from 21.2 to 31.1, closing nearly 40% of the gap to the Oracle ranker (46.9)."}, {"title": "7. Conclusion", "content": "We introduce FLAMe, a family of foundational autorater models that can perform various quality assessment tasks. FLAMe is trained on a large and diverse collection of curated and standardized human evaluations derived exclusively from permissively licensed datasets. We demonstrate FLAMe's strong zero-shot generalization abilities, outperforming models trained on proprietary data like GPT-4 and Claude-3 on many held-out tasks. FLAMe can also effectively serve as a powerful starting point for further downstream fine-tuning. Our FLAMe-RM variant, which is fine-tuned for reward modeling evaluation, is among the top-performing generative models on RewardBench, despite being trained solely on permissively licensed data, outperforming both GPT-4-0125 and GPT-4o. Additionally, we present a more computationally efficient approach using a novel tail-patch fine-tuning strategy to optimize our FLAMe multitask mixture for targeted distributions, offering competitive performance with significantly less compute. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models across 8 out of 12 autorater evaluation benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe exhibits significantly lower bias compared to popular LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark, while effectively identifying high-quality responses for code generation."}, {"title": "Limitations and Future work", "content": "Evaluating LLMs is challenging due to evolving evaluation standards and the need to assess new LLM capabilities. Expanding our data collection with open-source contributions could address this issue. Additionally, our models, trained primarily on English data with a context length of 2048 tokens, might not perform well on multilingual (Freitag et al., 2021) or long-context (Karpinska et al., 2024; Kim et al., 2024c) quality assessment tasks. In future releases, we plan to include training on more multilingual datasets with longer context lengths. Finally, in this work, we train our models in a supervised multitask fashion. Exploring alternative training approaches such as RLHF and DPO is a promising direction for future work."}, {"title": "Ethical Considerations and Risks", "content": "All considerations and risks outlined by prior work for pretrained and instruction-tuned LLMs (Anil et al., 2023; Chowdhery et al., 2022) apply to LLM autoraters. We recommend following standard practice for responsible development of these models (Achiam et al., 2023; Gemini et al., 2023; Reid et al., 2024). Additionally, LLM autoraters raise new risks due to increased quality assessment capabilities. First, our models can inherit and amplify biases from human evaluations, leading to unfair or discriminatory outcomes. For instance, the model may replicate biases related to race, gender, or other sensitive attributes from the training data, potentially harming certain groups. Second, overreliance on LLM autoraters risks automating decisions that need human understanding and empathy. To mitigate these risks, transparency in model development and use, along with robust measures like bias audits, data anonymization, and incorporating diverse perspectives, is essential for promoting fairness, accountability, and trustworthiness."}, {"title": "Appendix", "content": "8. List of Training Datasets in FLAMe\nTable 7 shows the list of datasets used in our study.\n9. Analyzing Length and Token Bias in RewardBench\nIn this section, we provide an analysis of length (Appendix 9.1) and token (Appendix 9.2) bias issues identified in the RewardBench benchmark. Given these issues, we encourage future work to evaluate LLM autoraters on a wide variety of benchmarks (such as our evaluation suite in Section 5), rather than relying solely on RewardBench.\n9.1. Length Bias in RewardBench\nTable 6 highlights length bias in RewardBench. Overall, RewardBench shows significant imbalance across categories regarding length: Chat Hard, Math, and Coding favor shorter outputs, while Chat leans towards longer outputs. An adversarial submission might strategically select longer or shorter outputs based on prompt categories to achieve higher scores, without necessarily reflecting a genuinely strong preference model.\n9.2. Token Bias in RewardBench\nBesides length bias, we identified token bias in the Math and Safety categories of RewardBench. In Safety, favored responses significantly leaned towards phrases like \u201cI'm sorry\u201d, which suggest hedged responses. The word \u201csorry\u201d appeared nearly 23% more frequently in preferred responses compared to non-preferred ones. Similarly, the Math split exhibited token bias, where tokens such as \u201ci\u201d, \u201ccan\u201d, \u201cneed\u201d, \u201cto\u201d, \u201cfind\u201d were predominantly found in rejected responses."}]}