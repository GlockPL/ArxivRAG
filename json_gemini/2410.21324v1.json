{"title": "Mathematical Derivation Graphs: A Task for Summarizing Equation Dependencies in STEM Manuscripts", "authors": ["Vishesh Prasad", "Brian Kim", "Nickvash Kani"], "abstract": "Recent advances in natural language processing (NLP), particularly with the emergence of large language models (LLMs), have significantly enhanced the field of textual analysis. However, while these developments have yielded substantial progress in analyzing textual data, applying analysis to mathematical equations and their relationships within texts has produced mixed results. In this paper, we take the initial steps toward understanding the dependency relationships between mathematical expressions in STEM articles. Our dataset, sourced from a random sampling of the arXiv corpus, contains an analysis of 107 published STEM manuscripts whose inter-equation dependency relationships have been hand-labeled, resulting in a new object we refer to as a derivation graph that summarizes the mathematical content of the manuscript. We exhaustively evaluate analytical and NLP-based models to assess their capability to identify and extract the derivation relationships for each article and compare the results with the ground truth. Our comprehensive testing finds that both analytical and NLP models (including LLMs) achieve ~40-50% F1 scores for extracting derivation graphs from articles, revealing that the recent advances in NLP have not made significant inroads in comprehending mathematical texts compared to simpler analytic models. While current approaches offer a solid foundation for extracting mathematical information, further research is necessary to improve accuracy and depth in this area.", "sections": [{"title": "1 Introduction", "content": "In natural language processing, the summarization task asks an algorithm to reduce a long piece of text into its most pertinent components. While this problem has been studied extensively for natural language texts (El-Kassas et al., 2021; Gambhir and Gupta, 2017), summarizing math-heavy texts remains a challenge for modern NLP techniques (Davis, 2024; Wu, 2024; Sundaram et al., 2022). These algorithms are rarely trained on symbolic mathematics central to math and physics manuscripts, necessitating new approaches to effectively process and summarize mathematical texts."}, {"title": "2 Problem Statement", "content": "Although the mathematical context in scholarly articles has been previously studied, significant limitations remain in effectively capturing and understanding mathematical relationships within these texts (Greiner-Petter et al., 2020; Chandrasekaran et al., 2020; Jo et al., 2021). Extracting and comprehending these relationships through derivation graphs is critical for tasks like representation learning and mathematical information retrieval (Pape and Tchoshanov, 2001; Van Nguyen et al., 2022; Gururaja et al., 2023; Aizawa and Kohlhase, 2021).\nThis research aims to understand the relationships between key equations presented in many STEM articles by extracting their derivation graphs. An example of an extracted derivation graph is shown in Figure 1, where nodes represent \"key math equations\" from the manuscript and directed edges between nodes, (u, v), indicate that equation v was derived from equation u within the text.\nSince manuscripts often contain hundreds of mathematical equations, many of which may be uninteresting (e.g., \u201c$k_b = 1.38 \\cdot 10^{-23} J \\cdot K^{-1}$\"), we limit our derivation graph to only contain the most essential equations within a manuscript. We define key math equations as those explicitly labeled with a reference number in the manuscript, as seen in Figure 2. Qualitative analysis has shown that these numbered equations help illustrate the manuscript's primary focus, making them a reliable indicator for inclusion in the derivation graph.\nA derivation is defined as to originate, develop, or be obtained from another object. In this context, a derivation graph is defined as a directed acyclic graph (DAG) where the set vertices represent the labeled key math equations and the set of edges represents the derivation relationships between equations, as seen in Figure 1. Specifically, a directed edge from node i to node j indicates that equation j is derived, as defined above, from node i.\""}, {"title": "3 Dataset", "content": "To facilitate our investigation into the extraction of mathematical relationships using derivation graphs, we developed a hand-labeled corpus"}, {"title": "4 Methods", "content": "Several algorithms were investigated to extract derivation graphs from STEM articles, including analytic, machine-learning-based, and LLM models. Further analysis of each successful algorithm is presented in Section 5, and additional explanations of all methods are found in Appendix B.\nThe first method explored is the brute force algorithm. This algorithm exploits the fact that the most obvious edges between equations are explicitly stated in the text. Thus, the brute force algorithm looks for textual references to other equations in the sentence(s) immediately preceding and following an equation, as seen in Figure 2.\nNext, the token similarity algorithm constructs a derivation graph by analyzing the number of tokens shared between two equations. The algorithm compares the percentage of tokens of one equation that appear in another, regardless of their position within the string. When the token similarity between two equations exceeds a predetermined threshold (see Appendix B.2), a derivation dependency is"}, {"title": "5 Results", "content": "From Table 1., it is observed that the brute force and LLM algorithms perform the best with the highest F1 scores. Though all four algorithms have high accuracy metrics, accuracy is found not to be helpful in differentiating between the performances of the algorithms due to the fact that a derivation graph is sparse in nature and, therefore, the number of true negatives (no edge) vastly outnumbers the relatively fewer number of possible true positives (edges that exist).\nFurther inspection of the metrics shows that the brute force algorithm has the highest precision, with approximately 50% of the edges it identifies also labeled as true edges in the ground truth. The precision not being higher can be attributed to a significant number of false positives, where equations are mentioned in the surrounding context but do not have a direct derivation relationship.\nFrom the experimentation, it is noted that the Gemini LLM had the highest recall, meaning that out of the ground truth edges labeled, the Gemini LLM could identify the highest number of them. This suggests that the model performs marginally better than the brute-force algorithm at capturing derivation relationships, specifically those not explicitly stated in the surrounding text.\nThe token similarity and Naive Bayes performed the worst compared to the other two algorithms. Token similarity, which only considers equation content without contextual information, struggles to differentiate between derivation chains, leading to an increased number of false positives. Naive Bayes, while effective given its limited training data, lacks the ability to incorporate external information, unlike the Gemini LLM, resulting in a lower F1 score. Further discussion and hyper-parameter testing are discussed in Appendix B."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel task: summarizing mathematical literature by reconstructing dependency graphs that capture the derivation relationships between mathematical equations. We have authored a dataset of extracted derivation graphs of published arXiv manuscripts and used it to evaluate the performance of various language processing algorithms. LLM and analytic brute-force models both achieve a best F1 score of ~48% when tested, marking a promising first step in this challenging area of research and underscoring the need for further improvements to advance the field."}, {"title": "A Dataset Details", "content": "Given the diversity in the dataset, further research could be conducted into how specific article structures for each category could be analyzed and utilized to improve the algorithms. An adjacency list of its derivation graph was stored in a singular JSON file, along with auxiliary information for each article. Examples of dataset entries can be seen in Figure 3. The dataset can be found at\nDue to the large data set size, the authors labeled the articles individually. To ensure validity, they checked each other's labeling both after the dataset was created and when performance numbers were unusual."}, {"title": "B Further Algorithms Discussion", "content": "The following section offers details about the variety of algorithms used for extracting derivation graphs. These algorithms assess whether there is a derivative relationship between two equations.\nThe section also investigates hyper-parameter testing where necessary."}, {"title": "B.1 Brute Force", "content": "The algorithm used to test the brute force method can be found in Algorithm 1. In implementing the brute force algorithm, it is found that there is a clear pattern present within these sentences that contains the obvious edge. Many, if not all, of these sentences end with very predictable words/phrases. Some of these words include \"yields,\" \"becomes,\u201d \u201cproves,\u201d \u201cget,\u201d \u201ctakes the form,\" \"find that,\u201d and \u201cwrite as.\u201d All these phrases share a similar trait in that they introduce the next phase in the author's line of logical reasoning. Further research could be conducted in specifying the direction of the link by further analyzing the context around an equation."}, {"title": "B.2 Token String Similarity", "content": "The algorithm utilized for token similarity can be found at Algorithm 2. For the token similarity algorithm, three hyper-parameters were explored. The first hyper-parameter is the strictness of the threshold used in the implementation. With options of either zero (0), one (1), or two (2), the value of this hyper-parameter determined how many of the threshold checks need to return true for an edge to be inserted. The zero (0) option adds an edge between every node, while the other two options are more selective. The second hyper-parameter was the direction of the inserted edge. This hyper-parameter had two options, \u201cgreater\u201d, or \u201clesser\u201d. Given that the previous threshold check was passed, it is necessary to establish which direction to insert the edge between the two nodes. For two nodes (node 1 and node 2) and their similarity variables (percentage_1_in_2 and percentage_2_in_1), if percentage_1_in_2 > percentage_2_in_1, then the \u201cgreater\u201d option results in the edge node 1 \u2192 node 2, and the \u201clesser\" option results in the edge node 2 \u2192 node 1. In essence, the \u201cgreater\" option makes the source node be the node that is a greater percentage of the other node, and the target node the node that is a lesser percentage of the other node. The \u201clesser\u201d option flips this relationship.\nThe third and final hyper-parameter for our implementation is the percentage threshold that must be passed by the strictness check in order for an edge to be determined. As seen in Figure 4, the fine tuning of these hyper-parameters is explored."}, {"title": "B.3 Naive Bayes", "content": "Label and feature construction are necessary to train the Naive Bayes classifier. For a label and\nfeature between equation i and equation j (where i < j), we would construct the following:\nlabeli,j =\n{\n+1, edge: i\u2192 j\n0 , no edge\n-1, edge: i\u2190 j\nfeaturei,j = equationi + \u2211 words_between_equations(k \u2212 1, k) + equationj\nk=i+1\nThe philosophy behind constructing the feature in this manner was the belief that any relevant information from the article that would dictate if an edge existed between two equations would exist in the MathML alttext notation of the two equations and any of the text that occurs between the two equations. It is noted from the construction of features and labels that if an article had n equations, there would be Sn = n(n-1) labels constructed and required by the model. It is to also be noted that\nthe Naive Bayes model utilized 90% of the dataset for its model training to maximize the F1 score, however lower training set percentages resulted in only a small decrease in correctness metrics."}, {"title": "B.4 Large Language Models", "content": "The Gemini 1.5 Flash model utilized for testing was the out-of-the-box model provided by Google. The model was not fine-tuned specifically for the dataset. A future study could be conducted on correctness if a custom pre-trained model was utilized.\nPrompt construction for an LLM is integral to obtaining the consistent desired response by providing sufficient context to answer given questions. For this experiment, the prompt used was of the following form:\n\"I have the following article that contains various mathematical equations:\n\\n\"+{total_article_text}+\u201c\\n From this article, I have extracted the list of equations, numbers as follows: \\n\"+{equation_list}+\u201c\\n Analyze the context of the article to identify which equations are derived from each equation. Provide the output as a list and nothing else, with the format: w -> x, y, z;\\n x -> h, t;\\n ... If no equations are derived from a certain equation, return an empty list with the format: t ->;\\n\".\nOut of initial prompt testing, the above prompt provided the best-performing results. It also provided the highest consistency regarding the formatting of the result, with errors less than ~7% of the time on average."}]}