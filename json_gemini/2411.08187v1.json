{"title": "TractoEmbed: Modular Multi-level Embedding framework for white matter tract segmentation", "authors": ["Anoushkrit Goel", "Bipanjit Singh", "Ankita Joshi", "Ranjeet Ranjan Jha", "Chirag Ahuja", "Aditya Nigam", "Arnav Bhavsar"], "abstract": "White matter tract segmentation is crucial for studying brain structural connectivity and neurosurgical planning. However, segmentation remains challenging due to issues like class imbalance between major and minor tracts, structural similarity, subject variability, symmetric streamlines between hemispheres etc. To address these challenges, we propose TractoEmbed, a modular multi-level embedding framework, that encodes localized representations through learning tasks in respective encoders. In this paper, TractoEmbed introduces a novel hierarchical streamline data representation that captures maximum spatial information at each level i.e. individual streamlines, clusters, and patches. Experiments show that TractoEmbed outperforms state-of-the-art methods in white matter tract segmentation across different datasets, and spanning various age groups. The modular framework directly allows the integration of additional embeddings in future works.", "sections": [{"title": "1 Introduction", "content": "Diffusion MRI (dMRI) [1,2] facilitates the non-invasive examination of the brain's white matter (WM) microstructural organization. A crucial component of the dMRI analysis pipeline is fiber tractography [3,22,23], which tracks fibers or streamlines under anatomical constraints from the dMRI signal received from the scanner (refer to Section 3). Tract Segmentation involves dividing the streamlines into distinct, anatomically meaningful tracts, with each tract corresponding to a specific white matter pathway. These tracts can be broadly grouped into 3 types based on structural connectivity, i.e. Association, Commissural, and Projection Fibers. Each type is further subdivided based on its specific structural connectivity and function, allowing for more granular distinctions. Through the segmentation process, it becomes possible to conduct quantitative studies of white matter (WM), which is important in understanding neurological disorders such as Alzheimer's, and Parkinson's [16], the effect of tumors on segmenting fiber streamlines, etc.\nIn addition, tract segmentation is also crucial for preoperative neurosurgical planning [14], as it helps identify eloquent white matter areas and determine optimal surgical approaches that minimize post-operative damage. Tract segmentation is also extensively used to visualize particular segments for focused examination by clinicians. However, this process is typically performed by expert Neuroanatomists using their knowledge of brain anatomy to divide fibers into multiple bundles. As a result, it is very time-consuming and can vary between experts, affecting the consistency and reliability of the results.\nTaking challenges associated with manual tract segmentation, various techniques have been developed over the years. These techniques range from classical methods to ATLAS-based and distance-based algorithms [8, 9,21,25] (refer to Section 2). An ATLAS refers to a standardized reference that allows spatial mapping of neuroimaging data from different studies (refer to Table 1) and modalities. They approximate the shape, location, and brain region boundaries in a common coordinate space, facilitating the comparison of brain structure and function across individuals.\nThese methods require significant manual intervention and are prone to age-related brain changes, also their effectiveness depends on the alignment and quality of the ATLAS. Considering the limitations of manual and classical methods, as well as the importance of tract segmentation, machine learning, and deep learning-based frameworks have been proposed for automatic tract segmentation [4,28,33]. Deep learning algorithms can learn information from shape, structure, relative location, fiber orientations, etc.\nHowever, a notable drawback is that these models often fail in classifying streamlines that are linear in shape due to over-reliance on shape, such as striato-thalamo-pallido projection fibers, which in existing methods, require global reference along with streamlines [7]. Additionally, when neurosurgeons are concerned with segmenting only a specific set of streamlines, global tractography can become a computational overhead. Due to these complexities in streamline classification-based tract segmentation, each method inherently has a certain drawback.\nTo address this, we propose TractoEmbed, a modular framework that combines multi-level embeddings extracted from hierarchical data representations specifically at streamline, patch, and cluster levels (refer to Fig. 1). Our approach surpasses state-of-the-art (SOTA) results in tract segmentation. In this work, we present an approach with the following major contributions:\n1. We introduce TractoEmbed, a novel modular multi-embedding framework, which leverages learning task-specific encoders to embed data representations, and generate embeddings. Where the encoders and their hyperparameters are selected after rigorous experimentation.\n2. We propose novel hierarchical and descriptive streamline data representations. These representations includes spatial information about regional patches, neighboring streamlines and the streamline itself, providing"}, {"title": "2 Related Work", "content": "In recent years, a plethora of classical and deep learning methods have been developed for tract segmentation, capable of performing in diverse conditions and data formats with minimal supervision from the skilled medical practitioners. Among these methods, clustering and distance-based methods, QuickBundles [9] and RecoBundles [8] are fast algorithms that utilize clustering approaches. QuickBundles is known for its speed in grouping streamlines based on their similarity, while RecoBundles excels in identifying parent anatomical bundles of streamlines. RecoBundles achieves this by recognizing and clustering similar streamlines based on their shape and spatial location, meanwhile leveraging a model of known white matter anatomy for accurate segmentation. Additionally, the Fast Streamline Search (FSS) [21] is a highly accurate distance-based search method. FSS indexes streamlines in a spatial data structure, enabling efficient retrieval of similar streamlines in tractography data.\nOther notable methods include GeoLab [25], a tract segmentation framework for analyzing the geometry, topology, and structural connectivity of white matter fiber bundles. Classifyber [4] is a linear classifier that uses distance-based embeddings with local and global streamlines and regions of interest (ROIs) in the brain, concatenated into a weight vector, which serves as a hybrid of distance-based and learning-based algorithms. TractSeg [28], one of the seminal works, uses a 2D U-Net model that directly works on fODF peaks [23] to segment tracts, without the need for parcellation and registration. In Deep-WMA [33], shape information of a single streamline is used to feed a FiberMap to a simple CNN model, preserving local information. BrainSegNet [10] employs bi-directional LSTMs, while FS2Net [11] uses an LSTM-based model to develop a rotation-invariant segmentation model. TRAFIC [12] uses geometry and 265 landmarks to accurately label, classify, and clean the traced paths of streamlines in streamline space.\nXue et al. use the PointNet model to classify streamlines using a local-global data representation, and Wang et al. [27] utilize a transformer encoder for fiber segmentation by incorporating features related to fiber shape and position. In [13], a graph convolution (GCNN)-based framework, Spectral GCNN extracts geometry-invariant features. In FIESTA, [5] Dumais et al. segment tracts in latent space, via an autoencoder-based segmentation algorithm."}, {"title": "3 Diffusion MRI Data", "content": "In this section, we discuss how diffusion MRI data (refer to Table 1) is acquired and processed to generate input and labels for the proposed framework. Additionally, we explain how the data is divided for training and testing purposes and converted to variations of Point Cloud Data before feeding to encoders.\n3.1 Data Preparation\nDiffusion MRI data [2] (refer to Table 1) is acquired by applying magnetic diffusion gradients and measuring the resulting signal attenuation, which depends on the local tissue microstructure. This diffusion MRI is preprocessed using standardized algorithms [22, 23], followed by streamlines tracking using a tractography algorithm [3,15]. ATLAS based labelling of streamlines is performed in the parcellation process. ATLAS registration on different brains can be inconsistent, non-scalable, knowledge intensive, dataset-specific and time-consuming because they are created by expert neuroanatomists. Hence there is a need for algorithms to automate tract segmentation.\nFor Tractography, we utilize the Unscented Kalman Filter (UKF) [15], which estimates microstructural parameters and fiber orientations from diffusion MRI data to track neuronal paths from multiple seed points. After tractography, the extracted streamlines are bundled into parcels or clusters, where these parcels are mapped to anatomically meaningful tracts using ATLAS, as discussed below.\nParcellation refers to the division of the brain into anatomical regions based on ATLAS and clustering techniques into parcels. Initially, division of hemisphere in streamline space, registration on ATLAS, and transformations are performed to align current brain with the ATLAS.\nThrough this process, we obtain 800 parcels that are appended to anatomical tracts and labeled along with quality control. These parcels are further refined using the ATLAS and diffusion measurements to separate outlier streamlines from each parcel, dividing each parcel into 2, resulting in 800 outlier parcels and 800 plausible parcels.\nUsing ATLAS, we club and label all 800 Outlier parcels to \"Other\" label, and 800 plausible parcels to 42 anatomical tracts. This entire procedure can be executed using whitematteranalysis package [17,18,32], which follows these steps from tractography streamlines to parcellation. This method ensures consistency across subjects and datasets. ATLAS used in this paper was derived from mean of 100 registered tractography of young healthy adults in the Human Connectome Project (HCP) [24].\n3.2 Training and Testing Data\nAfter sequentially performing fiber tractography, parcellation, and labeling of each parcel, we obtain a total of 1 million labeled streamlines from 100 out of 120 subjects (refer to Table 1), where each subject contains 10,000 streamlines. And 20 subjects out of 120 subjects are kept aside for real world test cases, and not included in data splits.\nFrom a total corpus of 100 subjects, we obtain an array consisting of 1M streamlines of shape (1000000, 15, 3) (refer Table 1), where (15,3) streamline array is derived from feature data in RAS (Right, Anterior, Superior) coordinate space (refer Supplementary Material). This dataset is subsequently partitioned into train, validation, and test sets in a ratio of 70 subjects for training, 10 subjects for validation, and 20 subjects for testing. Data is split subject-wise, where a subject will only belong to one data split at a time [30].\nThe dataset encompasses 43 tract classes: 42 anatomically significant tracts spanning the entire brain, and one category labeled as \"other\", which includes anatomically implausible outlier streamlines identified during the parcellation process (refer Section 3.1). Here PCD is an acronym for Point Cloud Data.\n3.3 Model Input Data\nTraining and Testing Data (refer Section 3.2), is in the form of a three-dimensional array that contains (number of streamlines, points per streamline, number of features) and is in unusable form for most encoders. Hence, to make it suitable for encoder-specific preprocessing methods, we represent data in 3 forms (as mentioned in Fig 1). which would be utilised by encoders in sections 4.14.24.3 and in results section 5."}, {"title": "4 Methodology", "content": "TractoEmbed utilizes a modular framework to fuse learnable embeddings trained on hierarchical streamline data representations, as detailed in the following subsections: 4.1, 4.2, and 4.3.\nIn the Methodology section we describe pre-processing, training method, model architecture, and output embedding for each encoder.\n1. Streamline Encoder essentially is any model that preserves intra-streamline information, its order of points, shape, and geometry amidst the random shuffling of data points in other models. To preserve intra-streamline spatial information we chose CNN-based method due CNN's inherent capability of learning local and global features from a 2D array with channels. One can argue LSTM encoder for auto-regressive sequential information but LSTM struggles to encode spatial information (refer Section 5)\n2. Cluster Encoder, should encode the shape, inter-streamline dependencies, and information of a cluster to resemble the target tract. Based on our evaluation PointNet is imperative in understanding spatial features from a cluster of points or point cloud. We did mild variations in kernel sizes and layers. We found the simple PointNet [19] model's ability to discern intricate patterns and dependencies better than others.\n3. Objective of Patch Encoder is to learn regional information in a hyperlocal streamline point cloud, to embed origin and termination region information in the point cloud through regional patches. We chose a combination of mini-pointnet and Discrete Variational Autoencoder (dVAE) [20] to reconstruct point cloud patches and learn regional generative features. Patches are used to embed regional information as attention across only points fails due to minimal information in a single point and high compute requirements [31].\nBroadly, three types of encoders are pre-trained or finetuned for classification downstream tasks. Embeddings from these encoders are combined to assist the classifier MLP (as illustrated in Fig. 2) in achieving accurate classification.\n4.1 Streamline Encoder\nPreprocessing: The streamline(15,3) is passed through the Fiber Descriptor to get streamline representation for streamline encoder. Fiber Descriptor is a"}, {"title": "4.2 Cluster Encoder", "content": "Preprocessing: From interpolated (40,3) streamlines we sample $klocal$ local neighbour streamlines using MDF Distance from QuickBundles. After finding local streamlines for each streamline, we sample hyperlocal streamlines using FSS [21]. FSS uses barycenter of streamlines and distance parameters like radius to accurately find similar streamlines. Streamlines sampled from FSS are highly probable to belong to the same class which allow us to merge these streamlines creating hyperlocal streamline data (6,40,3) where $khyperlocal$=5 and 1 streamline, resulting in (240,3) point cloud, from which $n_c$ points are sampled for the input to PointNet Model (refer Table 3) for the Cluster Embedding.\nCluster Embedding is a 1024 dimensional output embedding of a PointNet Model (refer Table 3 [19]), which takes in Cluster data (refer 4.2) as input ($n_c$,3), where $n_c$ is the number of points in the total point cloud.\nCluster Encoder architecture is mentioned in Table 3, all values are Xavier initialized, and the Cluster Embedding (1024 dimensional) is extracted at final"}, {"title": "4.3 Patch Encoder", "content": "Patch Data refers to a 3D patch on the Cluster data. We use patch data to capture information across and among regional patches on a group of streamlines (Cluster). Enabling us to classify streamlines that are structurally linear in shape, and are very similar to other streamlines (like projection fibers). Regional Patches will learn to focus on Origin and Termination ROIs to help better segment difficult tracts.\nPreprocessing: Patches are created by iteratively sampling, $p_f$ farthest points using FPS, Farthest Point Sampling, over the point cloud. we then use kNN to sample $p_{local}$ nearest points per patch. We get $p_f$ = 64 patches, where every patch has $P_{local}$ = 16 points making patch data dimensions to be (64, 16, 3) from an input cluster data of ($n_c$, 3), randomly sampled with replacement. (refer Ablation Study 6 to see effects on variation in $n_c$) Patch Embedding is a 1024 dimensional output of dVAE decoder (refer: dVAE architecture used Table 4 and Subsection 4.3) This dVAE model contains an encoder and a decoder trained to reconstruct patch data [31] using Chamfer and KL Divergence loss. The objective of the encoder is to create an 8192-dimensional token embedding for each patch, then the decoder scales down each token embedding to a 256-dimensional token embedding passing to the MLP to reconstruct the input patches. (For more details refer to Supplementary Material) We extract Patch Embedding at 1024 dimensional linear layer, and concat with other embeddings at MECL."}, {"title": "4.4 Training Strategy", "content": "TractoEmbed concatenates all three embeddings-Streamline, Cluster, and Patch at the MECL (Multi-Embedding Concat Layer) to feed the classifier"}, {"title": "5 Results & Discussions", "content": "In this section, we present extensive ablation studies, and comparative results highlighting the effectiveness of our embeddings and data representations across different datasets. We evaluated all the results on the test split containing 20 subjects from a sample of 100 subjects. Classification Report with Accuracy and F1 scores for each class is described in the Supplementary Material.\nWe present a comparison with several models, including DeepWMA [33], DCNN++ [29], basic PointNet [19], and DGCNN, using Single Streamline data. In Local PCD, TractoEmbed outperforms both variations of TractCloud [30]. In Hyperlocal PCD, we see that with only similar streamlines in the point cloud, TractoEmbed performs better than its performance in Local PCD, due to extensive focus on learning shape information of streamlines, as shown in Table 6.\npresented in Table 6 reveals the effectiveness of combining multiple embeddings used by TractoEmbed. As the neighboring point cloud becomes sparser, the representations need to be denser, increasing the need for more embeddings. Conversely, when the neighboring point cloud has a higher density of points, a pair of embeddings, cluster and streamline embedding, can achieve satisfactory performance. These findings ascertain the importance of incorporating dense streamline data representations from various perspectives/levels, including self, region, and neighbors.\nThe efficacy of a combination of embeddings can further be proven vital in increasing streamline classification accuracy as individual encoders perform poorly when compared to a combination of these embeddings (see Tables 7 6). Diving even further, there are slight improvements in F1 scores for projection fibers, striato-thalamo-pallido bundles, as observed in the Classification Report (refer to Supplementary Material), indicating that Patch Embedding can effectively make information-dense patches of an input point cloud. Having multiple embeddings decreases the over-reliance on one knowledge representation, and makes TractoEmbed robust to changes in either of the representations. Explicit addition of Streamline Embedding containing information on the order of points and intra-streamline spatial information makes TractoEmbed robust to point cloud perturbations in Cluster Encoder.\nIn summary, TractoEmbed demonstrates effectiveness in hyperlocal point clouds (regional examinations) and time-critical settings where a specific 3D brain segment is considered. It is also effective particularly for classifying structurally similar, minor, and projection fibers, achieving increased F1 scores and improved overall accuracy compared to LSTM-based approaches. TractoEmbed emphasizes the significance of fusing dense representations, incorporating various perspectives, including self, regional patches, and neighboring streamlines, which is crucial for extracting multiple types of information from low-fidelity streamline data. Future research can explore additional encoders, refined embedding combinations, optimal hyper-parameters, and different data representations. Also,"}, {"title": "6 Conclusion", "content": "With TractoEmbed we introduce an innovative method for Tract Segmentation, characterized by substantial accuracy, robustness, and modularity improvements. Our method integrates multiple embeddings from task-specific encoders to provide rich representations of streamlines, enabling a reduction in spatial input data requirements. It also demonstrates effectiveness in special cases, classifying structurally similar, minor, and projection fibers, by incorporating various data perspectives and minimal reliance on a single embedding. TractoEmbed also gives researchers the freedom to directly experiment with embeddings and data representations to get even better results. With its spatially minimal data requirements, TractoEmbed can be useful for focused ROI-specific, and time-sensitive clinical settings. Code will be made available upon request."}]}