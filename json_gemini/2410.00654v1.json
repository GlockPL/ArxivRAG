{"title": "Explainable Multi-Stakeholder Job Recommender Systems", "authors": ["Roan Schellingerhout"], "abstract": "Public opinion on recommender systems has become increasingly\nwary in recent years. In line with this trend, lawmakers have also\nstarted to become more critical of such systems, resulting in the\nintroduction of new laws focusing on aspects such as privacy, fair-\nness, and explainability for recommender systems and AI at large.\nThese concepts are especially crucial in high-risk domains such as\nrecruitment. In recruitment specifically, decisions carry substan-\ntial weight, as the outcomes can significantly impact individuals'\ncareers and companies' success. Additionally, there is a need for\na multi-stakeholder approach, as these systems are used by job\nseekers, recruiters, and companies simultaneously, each with its\nown requirements and expectations. In this paper, I summarize\nmy current research on the topic of explainable, multi-stakeholder\njob recommender systems and set out a number of future research\ndirections.", "sections": [{"title": "1 BACKGROUND AND CONTEXT", "content": "With recommender systems being one of the most widespread forms\nof machine learning used, they tend to be under heavy scrutiny\nby the public [14]. These systems are extensively utilized across\nvarious domains such as e-commerce, social media, and content\nstreaming, making their impact on daily life significant. Conse-\nquently, concerns about privacy, bias, and transparency have be-\ncome more pronounced [13]. Oftentimes, recommender systems\nare even distrusted, with users and representatives being wary of\npotential manipulation being performed by the system to nudge\nthem into certain beliefs or behaviors [16].\nOne way to address such suspicions is through the use of explain-\nable artificial intelligence (XAI). By allowing users (and lawmakers\nalike) to gain insights into how specific recommendations came to\nbe, we can enable them to understand the system better, leading to\nmore trust in its efficacy and less suspicion of foul play [4]. Explain-\nable AI can be critical for gaining user trust, as well as compliance\nwith regulations such as the GDPR and the EU AI Act [8, 10, 11].\nWhile considerable research has been done on using explainable\nAl to aid system developers and users, further prominent stakehold-\ners (e.g., advertising companies, item providers, lawmakers) should\nnot be ignored. This need for a multi-stakeholder approach requires\na nuanced approach, as it makes recommending and explaining\nitems more complex [1-3, 5]. All stakeholders want recommenda-\ntions and explanations optimized for their needs, but they often\nhave conflicting interests. This balancing act becomes even more\ncomplex in high-risk domains, such as recruitment. Job recom-\nmender systems (JRSs), which match job seekers with potential\nemployment opportunities, can have a considerable impact on indi-\nviduals' lives [9]. Job recommender systems generally have three\nmain stakeholders, all of which fall under the three main recom-\nmender system stakeholder types identified by Abdollahpouri et al.\n[1]: candidates - the people looking for a job (i.e., consumers); com\u0440\u0430-\nnies - businesses offering positions of employment (i.e., providers);\nand recruiters - people whose job it is to match candidates and\nvacancies (i.e., system). Each of these stakeholders has different\nneeds and priorities, making a multi-stakeholder approach to gen-\nerating explanations crucial [1, 9]. For example, candidates need\nto trust the system and understand why a job is suitable for them\nbefore making such an impactful decision [15]. Proper explanations\ncan also help mitigate biases and ensure fair treatment of all can-\ndidates, as it enables the system to be scrutinized. Furthermore,\nrecruiters can use explanations to understand why certain candi-\ndates are recommended, allowing them to focus more efficiently on\npromising matches. Companies, on the other hand, can be enabled\nto quickly find the most relevant candidate from a large pool of\noptions, increasing their productivity.\nThis leads us to formulate the following research question for my\nPhD project: How can we create an explainable, multi-stakeholder job\nrecommender system that supports targeted explanations for different\nstakeholders?\nTo assist in answering this research question, we consider the\nfollowing sub-questions:\nSQ1: What are the stakeholder-specific explanations require-\nments and preferences of candidates, recruiters, and compa-\nnies respectively?\nSQ2: How can we design an explainable multi-stakeholder job\nrecommender system that outperforms state-of-the-art sys-\ntems in user- and provider-side performance metrics?\nSQ3: To what extent does the inclusion of explainability into a\nreal-world job recommender system improve its perceived\nusefulness, transparency, and trust?\nIn the rest of this paper, we first summarize the research con-\nducted so far. Then, we set out multiple directions for future work."}, {"title": "2 COMPLETED RESEARCH", "content": "The research conducted during the PhD project up until this point\nhas focused on SQ1 (finding stakeholder requirements and pref-\nerences) and SQ2 (building an explainable multi-stakeholder job\nrecommender system). In this section, we describe the specific ex-\nperiments we have performed so far."}, {"title": "2.1 Stakeholder preferences (SQ1)", "content": "To get an initial indication of the explanation preferences of the\nthree main stakeholder types, we conducted a small-scale user\nstudy wherein we interviewed 6 participants while we exposed\nthem to different examples of possible explanation types [15]. We\nused this user study as a starting point to get an indication of\nwhat explanation types were most promising to explore in more\ndepth in future work. While we used a relatively small sample size,\nthis allowed us to spend a considerable amount of time with each\nparticipant (around 1 hour per person), which enabled us to have\nthe participants co-design the explanation types.\nWe found considerable preference differences, both between and\nwithin stakeholder types. Candidates and recruiters strongly pre-\nferred textual explanations over visual explanations, mentioning\nthat those were easier to grasp and had a more 'personal' feel to\nthem. Company representatives initially gravitated towards the\ntextual explanations too, as those were easiest to understand at first.\nHowever, they indicated a preference towards visual, graph-based\nexplanations once they had spent some time trying to grasp those.\nOnce they understood how the graph-based explanations should be\ninterpreted, they mentioned how such visualizations allowed them\nto get a comprehensive overview of the complex relations in the\ndata at a glance. This difference between stakeholders could largely\nbe attributed to the fact that company representatives tended to\nhave more experience with working with charts and graphs as part\nof their day-to-day job.\nHowever, there was significant disagreement between members\nof each stakeholder type as well. For example, recruiters disagreed\non how comprehensive the explanations should be - either prefer-\nring long texts allowing them to provide sufficient detail to their\nclients when trying to convince them of potential matches, or pre-\nferring more limited explanations to offer them an initial indication\nof suitability, after which they could use their expertise to come\nto a more honed-in decision. To allow users to cater the expla-\nnation environment to their personal preferences and needs, we\ndetermined that interactive interfaces are crucial for job recom-\nmendation, as those allow individuals to access the data they find\nimportant, while not getting overwhelmed by information they do\nnot consider useful."}, {"title": "2.2 Mock-up System Experiment (SQ1)", "content": "After having co-designed the different explanation types, we cre-\nated a prototypical explanation environment wherein users could\nbrowse multiple recommended items and their accompanying ex-\nplanations (Fig. 1). We tested this explanation environment with 30\nparticipants in total; 10 of each stakeholder type. When interacting\nwith the environment, both subjective (perceived usefulness, trust,\nand transparency) and objective (correctness and efficiency) met-\nrics were collected. Participants were tasked to select what they\nconsidered to be the best option from the list of items twice - once\nafter having seen real explanations generated by a graph neural\nnetwork, and once after having seen random explanations (they\nwere shown the different explanations in random order). Due to the\nnature of the data available to us at the time, this system did not\nallow users to get recommendations for their personal CV or va-\ncancy, but rather had them read a pre-selected CV or vacancy before\nseeing the recommended items, after which they were instructed\nto decide as if they were the person/company whose CV/vacancy\nthey just read.\nWe found that preferences largely stayed the same, with candi-\ndates and recruiters strongly gravitating towards the textual expla-\nnations, and company representatives having a more diverse range\nof preferences. However, regardless of which explanation type the\nparticipants preferred, the difference in metrics between the ran-\ndom and real explanations was very limited. I.e., whether a user was\nshown a 'nonsensical' random explanation or a genuine explanation,\ntheir opinion of the system barely changed. While the subjective\nmetrics trended upward with the real explanations, this trend was\nnot statistically significant. Correctness, on the other hand, even\nwent down when participants were using real explanations to come\nto a decision. One contributing factor to this lack of difference is\nthat the most commonly preferred explanation type, text, lends it-\nself quite poorly to indicating minor differences between examples\n(i.e., it is hard for users to spot discrepancies in phrasing between\nexplanations, e.g., 'somewhat' instead of 'strongly'). Furthermore,\nthe decrease in correctness indicates that users do not actively en-\ngage with the explanations and instead apply their own reasoning\nto the situation. Even if the system gives a specific argument for\nwhy an item is a good match, users often come up with widely\ndifferent reasons for their decision, even when agreeing with the\nmodel. This lack of engagement leads to a slight benefit for the\nrandom explanations, as those are less likely to create 'friction',\nallowing participants to always apply their own reasoning to their\ndecision without feeling like they disagree with the model.\nBased on these findings, we determine that we should instead\nprovide decision-support to the users of a job recommender system.\nWhile most XAI research focuses on persuasive explanations, we\nfind that trying to persuade the stakeholders of the model's cor-\nrectness is futile, as they will apply their domain expertise to the\ndecision-making process regardless. As such, it is better to sup-\nport them in this process, rather than trying to steer them in a\ncertain direction. This also addresses another concern in job rec-\nommendation: ground truth values are often generated manually\nby human recruiters - as a result, they are not objective truths. At-\ntempting to force users to agree with the system can therefore be\ncounterproductive, as their decision could, in theory, be preferable\nover the one determined to be 'correct'. Furthermore, during the\nexperiment, multiple participants indicated wanting a clear, direct\nrelation between the explanation and the 'source material' (i.e.,\nthe CV or vacancy). If the explanation contained information that\nwas not (directly) present in the CV/vacancy (e.g., work experi-\nence stored in the data, but not written in the CV), participants\ntended to get confused, wondering where that new information\ncame from. Therefore, we conclude that the arguments used by the\ndecision-support system should be clearly grounded in the source\nmaterial."}, {"title": "2.3 Explainable Graph Neural Network (SQ2)", "content": "The aforementioned papers made use of a rather low-quality, but\npublicly available dataset. As a result, the performance of the rec-\nommender system, and as a consequence its explanations, had\nsignificant room for improvement. To address this shortcoming,\nwe collaborated with a large, international recruitment agency in\norder to gain access to a high-quality, manually labeled, proprietary\njob recommendation dataset. To determine the efficacy of graph\nneural networks (GNN) on this dataset, we implemented a novel\nexplainable GNN, the Occupational Knowledge-based Recommender\nusing Attention (OKRA). We then compared OKRA to multiple state-\nof-the-art job recommendation models; both text-only and other\ngraph-based models.\nOur experiments showed that graph-based models strongly out-\nperformed text-only models. Considering the majority of JRS re-\nsearch focuses primarily on text-based solutions, this finding could\nhave considerable consequences on the field. While most research\nfocuses on utilizing transformer-based models to compare CV and\nvacancy texts to find matches, our findings indicate that this leaves\na significant amount of predictive power unused. Compared to\nstate-of-the-art graph-based models, OKRA performed significantly\nbetter due to its ability to make stakeholder-specific decisions (as\ncandidates and companies are not necessarily always in agreement),\nbut at the cost of increased training time. Additionally, we found\nthat most state-of-the-art models are slightly biased against both\nrural candidates and companies, indicating a need for the consider-\nation of regional fairness in the field of job recommendation.\nWhile OKRA is inherently explainable, the focus of this paper\nwas on its recommendation performance rather than its explain-\nability. Due to the architecture we used, OKRA is able to generate\nmultiple explanations for a single prediction, meaning it can sep-\narate 'positive' and 'negative' contributions to a decision. While\nthis theoretically lends OKRA's explanations to a decision support\nsystem, we did not evaluate the model's explainability component,\nleaving it for future work."}, {"title": "3 FUTURE WORK", "content": "During the rest of the project, we will primarily focus on improving\nthe model and its explanations so that they conform to the stake-\nholders' demands as much as possible. In the rest of this section,\nwe set out multiple avenues for future research related to each\nsub-question."}, {"title": "3.1 SQ1: Designing desirable explanations", "content": "Based on the research we have conducted for sub-question 1 so\nfar, we already have a general understanding of the preferences\nand needs of the different stakeholders. However, we have also\nidentified multiple shortcomings with our previous approach that\nneed to be addressed. To conclusively answer SQ1, we will focus\non alleviating these shortcomings in future work, so that we can"}, {"title": "3.1.1 Improving explanation coherence", "content": "One of the main difficulties\nfaced by the stakeholders in our mock-up experiment was that\nthey struggled to connect the explanations to the source material.\nConsidering the explanations were generated using both the source\nmaterial and structured data, parts of the explanation based on the\nstructured data did not necessarily align with what was shown in\nthe CV or vacancy (e.g., it highlighted a skill that someone did not\nlist on their CV). This led to confusion and made it more difficult\nfor participants to understand the explanations. To address this\nshortcoming, we will attempt to integrate the CVs/vacancies and\nthe structured data so that their contents are more aligned.\nOne possible approach is to make use of automated knowledge\ngraph construction from text [7, 17, 18]. For this, we would create a\nmachine learning pipeline capable of automatically finding entities\nand their underlying relations in a given text. The graphs generated\nfrom this text could then be linked to the rest of the structured data,\nso that for every piece of structured data, there exists a link to a\nword or phrase in the source material. This would not only improve\nthe coherence of the textual explanations but also allow the model\nto more directly integrate the information stored in the CV/vacancy\ninto the recommendation process. However, this approach would\nrequire some type of training data, as zero-shot learning is likely\nto be insufficiently integrated into the existing ontology. Alterna-\ntively, we could apply untrained clustering algorithms to cluster\nthe embeddings of different tokens, so that different tokens refer-\nring to the same concept can be coalesced. While this approach\ndoes not require training data, it is presumed to be less reliable, as\nmismatches between the structured and unstructured data could\nstill occur. When only using textual data to generate the knowledge\ngraph, it is certain that all the information in the graph is also stored\nin the text, however, when combining structured and unstructured\ndata, even with clustering, some tokens/concepts come up in one\ndata type, but not the other (e.g., work experience that is stored in\nthe structured data, but not mentioned in the CV)."}, {"title": "3.1.2 Clarifying textual explanations", "content": "Furthermore, we found that\nsubstantive differences in attention weights can lead to rather\nminute differences in the textual explanations. As mentioned above,\nthis made it difficult for users to differentiate between textual expla-\nnations with different contents correctly. While an attention weight\nof 0.7 instead of 0.2 stands out immediately, properly communi-\ncating this difference without referring to the exact values (since\nreferring to numeric values directly was indicated as complicated\nand overwhelming by stakeholders) can be difficult for LLMs. For\nexample, while describing these values as 'moderately high' and\n'fairly low' is correct, such formulations do not stand out immedi-\nately, which causes users to easily gloss over them. To solve this,\nwe intend to fine-tune an LLM, such as GPT-4, on a collection of\nexplanations that have been manually verified as 'clear' or 'under-\nstandable'. To determine what constitutes a clear explanation, we\nwill conduct an experiment wherein participants will be asked to\npick a preferred option between two versions of the same explana-\ntion, but with the value of one textual feature (such as word count,\nword complexity, level of formality, etc.) altered. By repeating this\nmatch-up multiple times, each time with different features being"}, {"title": "3.2 SQ2: Improving model performance", "content": null}, {"title": "3.2.1 Exploiting linked data", "content": "One major benefit of using knowl-\nedge graphs is that they are capable of easily combining data from\nmultiple sources [6]. While regular databases can be difficult to\ncombine, primarily due to differences in (naming) conventions and\nhigher-order relations being hard to implement using relational\nalgebra, knowledge graphs easily allow data from multiple sources\nto be combined. One major aspect of job recommendation where\nthis can make a large difference, is in the initial creation of node\nembeddings. Currently, all node embeddings used by OKRA were\ninitialized randomly, except for those based on CVs or vacancies.\nThe CV and vacancy nodes had a starting embedding based on the\ntext embedding value created by a transformer-based model. By\nincorporating linked data, node types that currently do not have\nany text related to them, such as function titles and codes (e.g.,\ninternational standard classification of occupation,\u00b9 or ISCO, codes),\ncan be linked to their respective nodes in existing graphs like that\nof WikiData2 and DBpedia.\u00b3 These linked data sources often have\nextensive descriptions of the functions/codes, allowing the model\nto use those descriptions to create starting embeddings. Further-\nmore, these data sources contain significantly more data than most\ndomain-specific datasets, using which the knowledge graph can be\nmade more exhaustive, enabling more high-level relations in the\ndata to be identified."}, {"title": "3.3 SQ3: Evaluating the system as a whole", "content": null}, {"title": "3.3.1 Evaluating in a real-world context", "content": "The explanation evalu-\nations we have done so far have all been conducted using non-\npersonal data. The CVs and vacancies used in our mock-up system\nwere examples using which the participants had to role-play. While\nthis was sufficient for the scope of this study, using such data can\nlead to some bias, as it is more difficult to make a decision on be-\nhalf of someone else than for oneself. Logically, we aim to address\nthis in future work by creating a live version of the environment,\nwherein users will be able to submit their own vacancy/CV and\npersonal data, so that the model can generate a personalized list\nof recommended items for them. This will enable the users to go\nthrough a more natural decision-making process, as they do not\nhave to bear the additional cognitive load of having to remember a\nCV/vacancy that is not theirs. As a result, we will be able to evaluate\nthe system in a more holistic manner using this live version of the\nenvironment; users can interact with the system as they would in a\nreal-world scenario as well, making it possible for us to determine\nto what extent the users interact with the explanations (rather than\nsimply reading the recommended vacancies/CVs like they would\nwith a non-explainable environment). Furthermore, by having a live,\nworking system, we will be able to more easily experiment with a\nlarge sample size, as participants will be able to interact with the"}, {"title": "3.3.2 List-wise explanations", "content": "Lastly, a significant challenge we an-\nticipate when shifting towards decision-support-focused explana-\ntions is having to present explanations related to a list of recom-\nmendations. When users are shown the pros and cons of different\nrecommended items, understanding why one item is ranked higher\nthan another can be complex. While providing pair-wise compar-\nisons is relatively straightforward, offering clear and comprehen-\nsive explanations in a list-wise context is much more challenging\n[12]. This difficulty arises from the need to show the intricate rela-\ntionships and trade-offs among multiple items simultaneously. To\naddress this issue, we will explore ways to effectively communi-\ncate the advantages and disadvantages of multiple items in future\nresearch. This could involve developing new comparative visualiza-\ntion techniques, interactive interfaces, or summary metrics that can\nhelp users grasp the overall ranking rationale and make informed\ndecisions based on the recommendations provided."}]}