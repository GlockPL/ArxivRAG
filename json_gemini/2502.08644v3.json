{"title": "Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks", "authors": ["Hoony Kang", "Wolfgang Losert"], "abstract": "The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic\nthat artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms\nof the mechanical structures of neural cells, we developed a learning paradigm that is based on\noscillations in link strengths and associates learning with the coordination of these oscillations.\nWe find that this paradigm yields rapid adaptation and learning in artificial neural networks.\nLink oscillations can rapidly change coordination, endowing the network with the ability to sense\nsubtle context changes in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture capable of predicting\ndynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to\nnever-seen-before contexts. These capabilities make our learning paradigm a powerful starting\npoint for novel models of learning and cognition. Furthermore, learning through link coordination\nis agnostic to the specifics of the neural network architecture, hence our study opens the door\nfor introducing rapid adaptation and learning capabilities into leading AI models.", "sections": [{"title": "1 Introduction", "content": "Much of the dynamics that govern the natural world experience drifts in internal parameters, leading\nto non-stationary dynamics with statistics that change in time. Examples are changes in atmospheric\nchemistry, tectonic plate movement, or the transition of a tumor's state from benign to malignant. The\neffects of nonstationarity can be dramatic enough to alter its entire probability space, rendering simple\ntools such as mean-centering of the data insufficient. Although the initial impact of system parameter\nchanges is often barely discernible in the data, detecting them may allow us to anticipate more\nsevere long-term consequences such as climate change, earthquakes, or metastases. Therefore, the\nclassification and prediction of dynamics that experience parameter drifts have garnered considerable\nattention across disciplines.\nWhile machine learning has been proven to be a powerful tool in predicting the dynamics of\ncomplex systems in the absence of mathematical models, most methods optimize prediction for tasks\noriginating from the same context, i.e., stationary probability distribution. Learning is implemented\nby having the link strengths of the network monotonically increase or decrease during training,\nuntil an optimal configuration is found. 1,2 Furthermore, to predict nonstationary data, one must\nfirst classify the different states present in the data, and then feed these contextual tokens into the\nmachine so that it can understand how the context of the training data changes over time. Here, we\ndefine 'states' as sets of trajectories that belong to the same stationary probability distribution, such\nas those of an attractor or some other set with invariant long-term statistical properties. Because\neach state represents a distinct stationary probability distribution, a state can be considered the"}, {"title": "1.1 A new learning paradigm", "content": "Inspired by the mechanochemical interactions of astrocytes and the neuronal synapse, we conjecture\na new paradigm of neuroplasticity. Recent work has suggested that learning may be implemented\nwith biomechanics, since squeezing of neuronal synapses strengthens synaptic transmission.5 Our\nlearning paradigm is inspired by this potential role of biomechanics and harnesses it in two parts.\nFirst, we propose that learning involves rhythmic variations in link strength. This is inspired by\nthe recent findings of our team and others that biomechanics exhibits spontaneous rhythms.6-9\nSecond, we propose that learning occurs via coordination of the phases of these rhythmic varia-\ntions. Astrocytes inspire this key element of our algorithm: Astrocytes, a type of glia abundantly\npresent in the brain that wrap around thousands of synapses, exhibit rhythmic hotspots of\nbiomechanical activity, which may enable the coordination among synapses.\n10\nThese two concepts work together to generate a powerful learning paradigm: We implement\nthese rhythms as slow oscillations in link strength of some or all links, and allow their phases to\nbe different for each link (fig. 1A). Therefore information flowing through the network can utilize\nmultiple subnetworks, depending on the phases of the oscillations at a given time (fig. 1B).\nThe second part of the paradigm is that the phases of each link can change depending on the\ninformation it processes, e.g., different states, as illustrated in fig. 1C. We propose that links that are\nactive in processing a given state can adjust their phases rapidly to synchronize to other links. This\nsynchronization is mediated by objects that can integrate information from subsets of links (inspired\nby the integrative role of astrocytes and denoted as stars in 1C).\nBecause input of a different state results in active link changes, state changes manifest as col-\nlective changes to synchronization of the link phases as illustrated in 1C. Therefore, link phase\nsynchronization also functions as a classification token of each state.\nWe implement our algorithm on an artificial neural network and demonstrate that it can rapidly\nsense dynamic state changes in a range of dynamical systems including key model systems of chaos,\nand serve as a digital twin for prediction and state targeting."}, {"title": "2 Results", "content": ""}, {"title": "2.1 Unsupervised state estimation with rhythmic sharing", "content": ""}, {"title": "2.1.1 Detecting simple state changes", "content": "We choose the 3D Thomas' cyclically symmetric system11 as our first example input because it hosts\nnumerous states with different values of its internal parameter, Thomas.\nWe simulate data where Thomas starts at 0.18, which results in a chaotic trajectory, and then\nsuddenly switches to 0.29, which results in a periodic orbit. This data is then sent frame-by-frame\ninto the nodes of a neural network whose connection weights and link phases are initially random.\nThe evolution of the link phases as data enters the neural network is then tracked, as shown in fig.\n2A. The collective dynamics of the link phases are characterized by their order parameter R\u2208 [0,1]\nand the mean phase (\u03a6) \u2208 [-\u03c0, \u03c0). R(t) measures the degree of global synchronization of link phases:\nR = 0 indicates uniform dispersion, i.e., complete decoherence, of the phases, while R = 1 indicates\ncomplete synchronization of the phases.\nWe emphasize two observations about R(t). First, R converges to some equilibrium value whenever\nthe input trajectory belongs to a stationary state. For instance, R evolves from its initial condition 0\nand reaches a steady state value 1 within roughly two \u2018cycles' of the chaotic state (AThomas = 0.18) and\nmaintains this value hereafter. This means that the links detected an invariant measure underlying\nthe trajectory, even when the trajectory is chaotic; in other words, the links detected that a stationary\nprobability distribution underlies the time evolution of the trajectory. Second, each state gives rise\nto a unique equilibrium value of R. Therefore, the evolution of R mimics the evolution of Thomas.\nThe dynamics of the mean phase (\u03a6)(t) are not as interpretable yet, as they do not respond\nuniquely to how Thomas evolves in time. However, they will be critical in the ability of the network\nto recall and predict various states (section 2.2)."}, {"title": "2.1.2 Sensing continuous parameter changes on systems with memory", "content": "The future evolution of a system may have a nontrivial dependence on its history. These systems are\nparticularly abundant in biology, such as the immune and nervous systems.12,13 One such example\nis the 1D Mackey-Glass system.14 For this system, the internal parameter Mackey that induces state\nchanges represents the time delay, or memory, of the dynamics. We vary this parameter in a sinusoidal\nmanner when simulating data.\nFig. 2B shows the response of the links to this input data. While R is shakier, likely due to the\nsystem's complexity and the continuous nature of the parameter change, it still successfully mimics\nthe evolution of Mackey."}, {"title": "2.1.3 Detecting warning signs of catastrophic phenomena", "content": "Finally, we study a system where changes to its internal parameters are not immediately apparent or\nmeasurable with current methods, but whose detection is critical to prevent the impending collapse\nof a system's dynamics.\nA well-known chaotic system that exhibits this behavior is the 3D Lorenz system.15 Its internal\nparameter Lorenz is responsible for state changes. The system exhibits a chaotic trajectory when\nLorenz = 24.5. When Lorenz abruptly changes to 23.5, the chaotic trajectory shifts into another\nsimilar, but transient chaotic trajectory until it suddenly collapses into a dead signal after some\nprobabilistic time. 16 Indeed, fig. 3C shows the close similarity in the spectral properties of the two\nchaotic trajectories; to resolve the differences in the spectral peaks (0.1-0.2 Hz), at least 50-100 cycles\nof this frequency band must be observed.\nThe behavior of R(t) of fig. 3C demonstrates that the links sensed the moments when the states\nwere switched: from the first chaotic trajectory to the chaotic transient, and then to the dead signal."}, {"title": "2.2 State transformation and recall by rerouting information flow with\nrhythmic sharing", "content": "A powerful feature of a neural network is its ability to store multiple dynamical states. If states were\ntagged with contextual tokens in the training data, one may recall a specific state by inputting the\nassociated token entry. Without these tokens, recall is very difficult.\nWhile R seems to fulfill the role of the required contextual token, the link phases' mean dynamics\nare also governed by (\u03a6), both of which shape the 'neural representation' of the input. Therefore, we\ncannot neglect its specification when attempting to recall a state. Yet the latter's role is not visually\nclear from fig. 2, as it does not seem to respond to state changes. To assess this, we will observe how\nthe neural output changes in response to changes to Rand (\u03a6).\nTo obtain an output from the network, we train the network as a reservoir computer (RC),\na form of a recurrent neural network that has been noted to perform well for chaotic dynamics\nprediction. 17-20 The training of an RC involves finding the optimal combination of nodes such that\ntheir combined output best recovers the true value of the input time series at the next timestep.\nWe reuse the Thomas system, as its large repository of states is well suited for our demonstration\nof state targeting. We train the RC with simulated data that alternates between two values of\nThomas, with the goal being to predict the dynamics of each state independently (fig. 3A). We will"}, {"title": "3 Conclusion", "content": "Motivated by neuro-glial interactions of the brain, we developed a learning paradigm consisting of\ntwo key elements: the slow oscillations of the links and the ability to change their coordination. We\nimplemented our model into an artificial neural network to probe the role of these mechanisms in\nneural representation.\nIn our model, links rapidly adjust their synchronization to identify the current state dynamics.\nThus, the synchrony of links identifies the current state of the dynamics. By self-adjusting\nlink strength rhythms for different states, the network is capable of learning nonstationary dynamics.\nThis is a powerful feature that regular machine learning algorithms cannot handle without user-\nprovided contextual tokens, despite the ubiquity of nonstationary dynamics in real-world data. Our\nresults demonstrate that the network is capable of rapidly identifying anomalous events in data and\nsignaling warning cues for impending disasters. Furthermore, this is achieved in real time because\nthe network did not require prior training.\nSlow link strength rhythms enable extrapolation of dynamics of numerous unseen states, and\nby training the network endowed with rhythmic sharing, we can predict the stationary dynamics of\neach state. This asset may be used to harness the network as a digital twin, capable of adapting its\nprediction when the physical twin experiences state changes.\nWe restricted the application of our algorithm to systems whose nonstationarity is induced by\na single parameter. This is because state changes brought on by more than one parameter cannot\nbe causally differentiated from the R(t) plot alone, which will combine all sources of change to the\nsystem. A future direction would be to design a model where multiple clusters can individually track\nthe nonstationary evolution of different internal parameters of a system, thereby creating small-world\nnetworks of links."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 The input systems", "content": ""}, {"title": "4.1.1 Thomas' cyclically symmetric system", "content": "The following equations govern the 3D Thomas system:11\n$\\begin{aligned}\n\\dot{x} &= \\sin(y) - bx \\\\\n\\dot{y} &= \\sin(z) - by \\\\\n\\dot{z} &= \\sin(x) - bz\n\\end{aligned}$                                                         (1)\nThe parameter b induces bifurcations in the system, and is recast as the parameter Thomas in\nthe main text."}, {"title": "4.1.2 Mackey-Glass system", "content": "The normalized form of the 1D Mackey-Glass system is given by:14\n$\\dot{x(t)} = \\beta \\frac{x(t - \\tau)}{1 + x(t - \\tau)^n} - \\gamma x(t)$ \nHere, the exact time dependence of the state variable x is provided for clarity. T is the time\ndelay parameter responsible for inducing bifurcations of the attractor (parameterized with phase\nspace coordinates (x(t), x(t \u2212 \u03c4)), and is recast as the parameter Mackey in the main text. We fix\n\u03b2 = 0.2, \u03b3 = 0.1, and n = 10, while we oscillate T sinusoidally between T = 20 and 7 = 24 with a\nslow angular frequency (= 0.007) during the simulation of our data."}, {"title": "4.1.3 Lorenz system", "content": "The following equations govern the 3D Lorenz system:15\n$\\begin{aligned}\n\\dot{x} &= \\sigma(y - x) \\\\\n\\dot{y} &= x(\\rho - z) - y \\\\\n\\dot{z} &= xy - Bz\n\\end{aligned}$                                                       (2)\nHere, the parameter \u03c1 is responsible for bifurcations of the system and is recast as the parameter\nALorenz in the main text. We fix the other parameters to values \u03c3 = 10, \u03b2 = 8/3 for the duration of the\nentire simulation, while we switch the value of \u03c1 abruptly as in the case for the Thomas system. For\np\u2265 24.06, the system permits a globally stable strange attractor that coexists with two stable fixed\npoints, but the former's trajectories never hit the latter's basin boundary. For 13.93 \u2264 p\u2264 24.06,\nthe strange attractor turns into a chaotic saddle, which also coexists with the fixed points, but\ntrajectories of the saddle will hit the saddle boundary after some probabilistic time. 16\nWe switch \u03c1 from 24.5 to 23.5 in our simulated data. Our choice of \u03c1 = 23.5 is due to its proximity\nto the bifurcation threshold at \u03c1 \u2248 24.06. By minimizing this difference, we minimize visual changes\nto the trajectory as the chaotic attractor transitions into a saddle."}, {"title": "4.2 Rhythmic sharing", "content": "We model the link interactions with the following governing equation:\n$\\frac{d\\varphi}{dt} = \\omega_0 + (\\epsilon_1 + \\epsilon_2Qn^*) \\odot sin(\\Psi - \\Phi + \\gamma)$                                        (3)\nwhere \u2299 denotes the Hadamard product. \u03a6 is a vector of phases [\u03c61...\u03c6Ni]T with Ni being the total\nnumber of links. wo is a vector of initially imposed natural frequencies of the links. The vector of\nlocal mean fields \u03a8(t) \u2208 RN\u0131 that each link is coupled to is given by\nr(t) ei\u03a8(t) =  A\u03c6ei\u03a6(t)                                                                   (4)\nwhere r\u2208 RN\u0131 is a vector of local order parameters whose elements describe the degree of synchro-\nnization across the particular local mean field that each link is coupled to.  A\u03c6i = A\u03c6. /A\u03c6\u2022 .\nwhere A\u03c6 \u2208 RNI \u00d7 Ni is a random phase adjacency matrix, binary for simplicity, whose elements equal\n1 if the links are connected and 0 otherwise, and the normalization factor A\u03c6 is the L\u00b9 norm\nof the i-th row of A\u03c6. The density of non-zero elements of A\u03c6 matrix is a tuneable hyperparame-\nter. We note in passing that while we limit the focus of this study to 1:1 synchronization, arbitrary\norders of p: q synchronization (for clusters) may be readily implemented by modifying the argument\nof the right-hand side of the eq. 3 by taking sin(\u03a8 \u2013 \u03a6) \u2192 sin(p \u2022 \u03a8 - qo\u03a6), where p and q are\nvectors of positive integers whose elements qi, pi denote the desired synchronization order between\nthe phase \u03c6i and its connected mean field \u03a8. However, careful engineering of the structure of A\u03c6\nwill most likely be needed to minimize competition of different synchronization orders between the\nlinks. Finally, y is a phase-lag vector (treated as another hyperparameter) whose role is akin to that\nof the Sakaguchi-Kuramoto model in promoting asymmetric communication. 21"}, {"title": "4.2.1 Oscillation death for isolated links", "content": "In Kuramoto-like models, 24 every oscillator is independently coupled to a subset of other oscillators\nin the network. If an oscillator is not coupled to any other oscillator, its phase will evolve with its\nunperturbed natural frequency wo.\nHowever, our choice to couple each oscillating link to the mean field of its connections endows\nthe system with the property that any isolated, active link will eventually quench its oscillations for\nwo < \u03b5. To see how oscillation death happens in our model, let us consider a single oscillator, \u03c6\n(where we drop indices for brevity), that evolves without any link-to-link interaction:\n$\\dot{\\varphi} = \\omega_0 + \\epsilon sin(\\Psi_0 - \\varphi)$                                                                     (10)\nwhere \u03a8 is the constant atan2(0./0.), which is mathematically undefined yet computationally yields\nsome numerical value due to floating point accuracy. \u03b5 = 81 + 82(QT n*) is the strength of the\ninteraction for this particular oscillator, and which is bounded in general between 81 \u00b1 \u00a32. Here, & is\nfrozen as a constant for simplicity and to obtain an analytic solution.\nBy a translation, \u03c6 \u2192 \u03c6 \u2013 \u03a8\u03bf, eq. 10 reduces to\n$\\dot{\\varphi} = \\omega_0 + \\epsilon sin(-\\varphi)$                                                       (11)\nBy use of a half-tan substitution, the solution can be easily shown to be:\n$\\varphi(t) = 2tan^{-1} \\bigg[\\frac{\\epsilon}{\\omega_0} + \\sqrt{\\frac{1 - \\Delta/\\omega_0}{1 + \\Delta/\\omega_0}} tan(\\frac{\\Delta t}{2} + \\xi_0) \\bigg]$                                 (12)\nwhere $ \\Delta = \\sqrt{\\omega_0^2 - \\epsilon^2}$ and $\\xi_0 = 2tan^{-1}[-\\frac{\\epsilon}{\\omega_0} + \\frac{\\Delta}{\\omega_0} tan(\\frac{\\phi_0}{2}) ] $ with \u03c6 being the initial phase. It suffices\nto show that if the argument of the tan\u00af\u00b9 has a real limit for large time given small w (compared\nto the order of \u025b, which will be on the order of unity for synchronization), then tan-1 (and therefore\n4) will converge to a real value as t \u2192 \u221e.\nFor wo| < |\u03b5o, \u2206 and \u03be\u03bf are purely imaginary, making both the amplitude and the argument of\nthe nested tan in the expression for \u03c6(t) also purely imaginary. As such, \u03c6(t) evolves as 2tan-1[a+\nbtanh(ct + d)], where a, b, c, d are real constants. Since tanh(ct + d) possesses a real limit for large\nt, \u03c6(t) converges to a real limit, i.e.,the oscillation has quenched.\nOn the other hand, consider |wo| > |8o|. Then A and go are purely real, and \u03c6(t) evolves as\n2 tan-\u00b9[a+btan(ct + d)], where a, b, c, d are reused to denote arbitrary real constants. Because the"}, {"title": "4.3 Overview of reservoir computing with echo state networks", "content": "This section provides a brief review of how training and predicting are conducted for an echo state\nnetwork."}, {"title": "4.3.1 Training", "content": "First, a reservoir is initiated into a 'warm-up phase' for Twarm steps of At of the input. As a reservoir\ncomputer has leaky memory, this stage is required to provide sufficient time for a reservoir to forget\nits initial state (usually a blank slate n = 0). Here, the input u is the training data u(t) = utrain(t) so\nas to entrain n(t) to the dynamics of the input data without any feedback error. As this stage is used\nsolely to calibrate the nodes' state space with that of the data, these reservoir states are discarded\nand not used for training.\nUpon completion, the reservoir enters the 'training phase' for Ttrain steps of At. The now-calibrated\nstates are once more entrained by u(t) = utrain(t), but all the reservoir states during this phase\nn(to + \u2206t), n(to + 2\u2206t), ..., n(to + Ttrain\u2206t) are recorded. For notational brevity, twarm = Twarm\u2206t\nmarks the end of the warm-up period. Using the concatenated state responses across time, the optimal\noutput matrix Wout is then calculated via L2 regression by minimizing:\n$\\underset{Wout}{arg min} \\sum_{t=t_{warm}+\\Delta t}^{t_{warm}+T_{train} \\Delta t} ||W_{out}n(t) - u_{train}(t)||^2 + \\beta tr(W_{out} W_{out}^T)$                                    (13)\nwhere \u03b2 is the regularization hyperparameter."}, {"title": "4.3.2 Predicting", "content": "Equipped with the trained Wout, we are now ready to commence the prediction phase. A portion of\nthe test data utest is sent into the reservoir (which can either start from a blank slate again or from\nits last state during the training phase) for another warm-up period to forget its previous state and\nre-calibrate it to the new data to be predicted. We note in passing that the warm-up period for the\ntest data need not be the same as that of training. The last frame of the test data in the warm-up\nperiod utest (twarm) is now used as the initial condition to generate the first prediction, \u0169(twarm + \u2206t).\nThis output is then fed back as the new input to obtain the reservoir's next state, forming a closed-\nloop prediction for the next timestep. This process is iterated until the reservoir has advanced up to\nour desired prediction window."}, {"title": "4.3.3 State targeting and prediction by steering (R, (\u03a6))", "content": "For our modified architecture, we perform the same procedure as above, but modify the warm-up\nstage of the prediction slightly if we wish to predict the time series of a single state, as it was done\nin fig. 3C. As the network warms up to the test data, the links re-initialize their order parameter to\nthe test state. Once the order parameter has reached its steady state (which we denote here as Ro),\nwe freeze R by imposing that the phases evolve in any arbitrary manner according to a prescribed\nvector-valued forcing g satisfying\n$\\begin{aligned}\n\\dot{\\Phi}(t) &= g(\\Phi, t), \\\\ &= g(t) \\mathbf{1}_{N_i} \\\\\n(\\mathbf{\\Phi}_{t_{RO}}) &= \\mathbf{\\Phi}_{RO} \\\\\n\\end{aligned} $                                                   (14)\nwhere g is some scalar-valued function, 1v\u2081 = (1,1, .., 1) \u2208 Ri denotes a vector of 1s of size Ni, \u03a6Ro\ndenotes as the vector of phases at the first instance t when R = Ro, which we denote as tro. This is\nbecause the system of equations satisfies the constraint R(t) = Ro for t > tro, where tro <twarm,\nsince the forcing does not depend on the phase, and so the Lyapunov exponent of the system is 0\n(the phases do not diverge from each other in time).\nWe choose g = \u03a9t, with \u03a9 = \u03c9\u03bf, such that the states can evolve slowly until the target mean\nphase (\u03a6) = (\u03a6), can be obtained. Once our desired (\u03a6), is reached, we replace g 0 to freeze the"}, {"title": "4.4 Data availability", "content": "The dataset of the Thomas system used in this study is provided on GitHub, 25 and the equations\nand information necessary to generate the other simulated data are provided in the manuscript."}]}