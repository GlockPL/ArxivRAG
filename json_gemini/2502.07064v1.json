{"title": "Contextual Thompson Sampling via Generation of Missing Data", "authors": ["Kelly W. Zhang", "Tiffany (Tianhui) Cai", "Hongseok Namkoong", "Daniel Russo"], "abstract": "We introduce a framework for Thompson sampling contextual bandit algorithms, in which\nthe algorithm's ability to quantify uncertainty and make decisions depends on the quality of\na generative model that is learned offline. Instead of viewing uncertainty in the environment\nas arising from unobservable latent parameters, our algorithm treats uncertainty as stemming\nfrom missing, but potentially observable, future outcomes. If these future outcomes were all\nobserved, one could simply make decisions using an \"oracle\" policy fit on the complete dataset.\nInspired by this conceptualization, at each decision-time, our algorithm uses a generative model\nto probabilistically impute missing future outcomes, fits a policy using the imputed complete\ndataset, and uses that policy to select the next action. We formally show that this algorithm is\na generative formulation of Thompson Sampling and prove a state-of-the-art regret bound for\nit. Notably, our regret bound i) depends on the probabilistic generative model only through the\nquality of its offline prediction loss, and ii) applies to any method of fitting the \"oracle\" policy,\nwhich easily allows one to adapt Thompson sampling to decision-making settings with fairness\nand/or resource constraints.", "sections": [{"title": "1 Introduction", "content": "Recent advances in machine learning have transformed our ability to develop high quality predictive\nand generative models for complex data. This work introduces a framework for developing decision-\nmaking algorithms, specifically for contextual bandit problems, that can take advantage of these\nmachine learning advances. By design, we assume the algorithm developer is able to effectively\napply these machine learning techniques (e.g., minimize a loss via gradient descent) and employ\nthese methods as subroutines in our decision-making algorithm. Moreover, our theory formally\nconnects the quality of effective (self-)supervised learning via loss minimization to the quality of\ndecision-making.\nClassically, contextual Thompson sampling algorithms form a parametric model of the envi-\nronment and consider the decision-maker's uncertainty as arising from unknown latent parameters\nof that model [51]. In this classical perspective, the primitive operations that are assumed to be\nfeasible (at least approximately) include i) the ability to specify an informative prior for the latent\nparameter using domain knowledge, ii) the ability to sample from the posterior distribution of the\nlatent parameter, and iii) the ability to update the posterior distribution as more data is collected.\nUnfortunately, it is well known that all three of these primitive operations are non-trivial to perform\nwith neural networks [20, 61].\nBuilding on our previous work [8] which focuses on multi-armed bandits without contexts, we\nview missing, but potentially observable, future outcomes as the source of the decision-maker's\nuncertainty. This perspective allows us to replace the primitive operations required in the classical\nview with new primitives that are more compatible with neural networks: i) the ability to effectively\nminimize an offline sequence prediction loss, ii) the ability to autoregressively generate from the\noptimized sequence model, and iii) the ability to fit a desired policy given access to a complete\ndataset (outcomes from all actions and decision-times)."}, {"title": "2 Problem formulation", "content": "2.1 Meta contextual bandit problem\nWe consider a meta contextual bandit problem where bandit tasks \u03c4 are sampled from an unknown\ntask distribution p*:\n\u03c4 ~ p*.\n(1)\nEach bandit task \u03c4 consists of prior information Z\u03c4, an action space A\u03c4, a sequence of context vec-\ntors X1:T = {X1, ..., XT}, and a table of potential outcomes\u00b9 {Yt(a)}a\u2208At = {Yt(1),..., Yt(|A\u03c4|)}a\u2208At:\nT = {Z\u03c4, X1:T, {Yt(a),..., Yt(a)}a\u2208A\u03c4}.\nInformally, the agent's objective is to select actions to maximize the total expected reward for each\nencountered task. At the start of a task, the agent observes prior information Z\u03c4. For each decision\ntime t \u2208 [1: T], the agent observes the context Xt, selects an action At \u2208 A\u03c4, observes the outcome\nYt = Yt(At), and computes the reward R(Yt), for a fixed, known function R that takes values in\n[0,1]. We use Ht to denote the history, which includes the current context:\nHt = {Z\u03c4, (X1, A1, Y1), . . ., (Xt\u22121, At\u22121, Yt\u22121), Xt}.\nThe agent is able to learn both online within a single task meaning over the T total decision times,\nas well as meta-learn across different tasks (e.g., learning how task prior information Z\u03c4 may inform\nthe distribution of {Yt(a)}a\u2208At)."}, {"title": "2.2 Environment assumptions", "content": "The defining quality of our problem formulation is that we do not make explicit assumptions about\nthe distribution of outcomes Yt conditional on contexts Xt and prior information Z\u03c4. It is common\nin the meta bandit literature to assume a known parametric model class that accurately captures\nthe distribution {Yt(a)}a\u2208At | (Xt, Z\u03c4) [9, 10, 28, 62]; Typically, there is an unknown environment\nparameter that varies between tasks. We instead allow this distribution to be general. Our algo-\nrithm's decision-making quality depends on how accurately the agent models this distribution, as\nwell as the policy fitting procedure the algorithm designer chooses. Rather than relying on strong\nassumptions on the environment structure, we put the onus on the algorithm designer to i) learn\na model that accurately captures the environment structure of the meta-bandit task at hand, and\nii) choose a meaningful method for fitting a desired \u201coracle\u201d policy, assuming access to a complete\ndataset. The motivation for this comes recognizing that such offline learning problems are routinely"}, {"title": "2.3 Definition of regret", "content": "Policy fitting. We assume that the algorithm designer specifies a procedure for fitting a desired\n\"oracle\" policy given access to a complete bandit task dataset \u03c4. This policy fitting procedure\noutputs policies in a function class \u03a0* where each \u03c0* \u2208 \u03a0* defines a mapping from contexts Xt\nto an action a \u2208 A\u03c4, that does not vary over time. For notational simplicity, the policies in \u03a0*\nare assumed to be non-stochastic. Note that we do not require that this policy class is necessarily\n\"correct\u201d. For a particular task \u03c4, we use \u03c0*(\u00b7;\u03c4) to denote a \u201cbest-fitting\u201d policy \u03c0* \u2208 \u03a0*, where"}, {"title": "3 Key conceptual idea: Missing data view of uncertainty quan-\ntification", "content": "In this work, we view missing data as the source of the decision-maker's uncertainty. This contrasts\nthe classical approach of considering unknown model parameters as the source of uncertainty. As\nwe will explore in the following sections, the missing data viewpoint is very amenable to modern\ndeep learning methods, which can be used to train models that are able to impute missing data\nprobabilistically in a calibrated fashion."}, {"title": "3.1 Posterior sampling via imputing missing data", "content": "To convey the missing data viewpoint, we first consider an idealized setting in which we have access\nto the meta task distribution p* (we discuss how to approximate p* in Section 4). Using p* we can\nform an exact posterior sample for task outcomes \u03c4\u0302 = {Z\u03c4, X1:T, {Yt(a)}} given the history Ht:\n\u03c4\u0302t ~ p* (\u03c4\u2208\u00b7 | Ht).\n(3)\nAbove we probabilistically generate values in \u03c4 that have not yet been observed in the history Ht;\nThis consists of future contexts, future outcomes, and outcomes from previous timesteps for actions\nthat were not selected."}, {"title": "3.2 Regret: Thompson sampling via generation with p*", "content": "This section presents a regret bound for Algorithm 1 with the perfect imputation model, p* from\n(1). Our work develops a novel analysis of contextual Thompson sampling, which is applicable\nto infinite policy classes \u03a0* with finite VC dimension. Our VC dimension bound resembles those\nfrom adversarial bandits, but for the first time, we show we can derive this using an information\ntheoretic analysis."}, {"title": "4 Thompson sampling via generation under an imperfect imputa-\ntion model", "content": "In the previous section, we introduced a generative version of Thompson sampling for contextual\nbbandits under the assumption we have access to a perfect imputation model p*. In this section,\nwe discuss how to practically approximate such an algorithm. First, we pretrain an autoregressive\nsequence model to predict successive outcomes (Y's) on historical data Doffline. Then, at decision\ntime, recommendation decisions are made by imputing the missing outcomes in \u03c4 with by generating\noutcomes (\u0176's) autoregressively from the pretrained sequence model. The offline pretraining allows\nthe algorithm to \"meta-learn\" a good model for imputing missing outcomes using data from previous\ntasks."}, {"title": "5 Regret of generative Thompson sampling with an imperfect im-\nputation model", "content": "In this section, we present a generalization of the regret bound for the Thompson sampling via\ngeneration algorithm from Section 3.2. Our generalization is notable because the sequence model\nonly affects the regret bound through its offline prediction loss, which means any sequence model\nclass can be used even sequence models that are not exactly exchangeable. Moreover, our result\nshows that the lower offline prediction loss of the sequence model p\u03b8 translates into a better re-\ngret guarantee. Our result effectively reduces a difficult online decision-making problem to one of\ntraining an accurate sequence prediction model.\nSpecifically, our regret bound will depend on the following population-level version of the train-\ning loss from (5) (the expectation below averages over the task distribution p*):\nl(p\u03b8) = \u2212E [ \u2211a\u2208A\u03c4 \u2211t=1 T log p\u03b8(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) )].\n(6)\nTheorem 1 (Regret bound for generative Thompson sampling with an imperfect imputation\nmodel). Let Assumptions 1, 2, and 3 hold. Under generative Thompson sampling (Algorithm 1)"}, {"title": "6 Related work", "content": "Decision-making with sequence models. Many recent methods use sequence models in decision-\nmaking involve imitation learning, i.e., from demonstrations learn to mimic an expert's actions\n[12, 23, 24]. Lee et al. [32] discuss how these approaches can be used even without access to expert\ndemonstrations, as long as one is able to fit an approximate \"oracle\" policy from offline bandit"}, {"title": "7 Experiments", "content": "Problem settings. We consider two meta-bandit settings. In both, T = 500, |A| = 10, outcomes\nY are binary, and R(y) = y. Our SYNTHETIC setting uses a Bayesian logistic regression data\ngenerating process with a 2-dimensional task vector Z and 5-dimensional contexts X. Our SEMI-\nSYNTHETIC setting is designed to mimic a cold-start news recommendation setting and uses the\nMIcrosoft News Dataset [65]; Here Z consists of the article headlines, contexts X are 5-dimensional\nuser features, and Y represents whether or not the user clicked on a recommended article. See\nAppendix B.1 for more details.\nBandit algorithms. The sequence model used in generative Thompson sampling (TS-Gen) is a\nsimple multi-layer perceptron-based sequence model which takes as input the prior information Z, a\nsummary statistic for the history, and the current context X, and outputs a distribution over Y. In\nthe SEMI-SYNTHETIC setting, we additionally embed the article text Z using DistilBERT [53] before\nfeeding it into our sequence model. For the policy class \u03a0*, we choose a simple logistic policy class\nfor our SYNTHETIC setting and a XGBoost-like, tree based policy class for our SEMI-SYNTHETIC\nsetting. We compute regret against the best fitting policy in each class for the respective setting.\nWe further examine the choice of policy class in App. B.5."}, {"title": "8 Discussion", "content": "We introduce a generative Thompson sampling algorithm for contextual bandits that is compatible\nwith any generative model with low offline prediction loss and any policy fitting procedure. We\nprove a regret bound for our algorithm that allows for misspecification of the generative model,\nand also provides insights into information theoretic analyses for contextual bandits, which may be\nof independent interest.\nDirections for future investigation include i) developing methods to guide how one might choose\nan appropriate policy class [17], ii) quantifying how much offline data is needed to train a high\nquality generative sequence model (which includes settings where the offline data is collected by\na behavior policy), iii) investigating the impact of approximating the context distribution when it\nis unknown, iv) exploring if the generative approach to modelling uncertainty can be extended to\nmore difficult decision-making settings, like Markov decision processes."}, {"title": "A Theory", "content": "A.1 Relationship to Thompson sampling with misspecified priors and lower\nbounds.\nIn our per-period regret bound from Theorem 1, the \"penalty\" for using a suboptimal sequence\nmodel p\u03b8 does not vanish as T grows. Since frequentist regret for Thompson sampling do not incor-\nporate such non-vanishing terms, one might interpret this as indicating our result is not tight. This\ninterpretation is significantly mistaken. Standard frequentist regret bounds for Thompson sam-\npling critically assume diffuse, non-informative priors [2, 3], which ensure that each arm is explored\nsufficiently. It turns out that Thompson Sampling can be highly sensitive to misspecification in the\nprior, especially if under the prior the probability of the optimal action being the best is too low,\nso the algorithm has a high probability of under exploring the best action. Specifically, previous\nwork has shown that the per period regret may be non-vanishing for a worst case environment and\nchoice of prior [36, 58]. Additionally, Cai et al. [8] show that for a multi-arm (non-contextual) ban-\ndit version of the generative Thompson sampling algorithm that the penalty for using an imperfect\nsequence model depends on \u221al(p\u03b8) \u2212 l(p*) in a way that is in general unavoidable.\nA.2 Notation\nWe first introduce some notation we use throughout this Appendix.\n\u2022 Recall that by definition\n\u0394(ATS-Gen (p\u03b8)) = EATS-Gen (p\u03b8)[ 1/T \u2211t=1 T {R(Yt(*(Xt;\u03c4))) \u2212 R(Yt(At))}\nThroughout proof, we will omit the ATS-Gen(p\u03b8) subscript on the expectation, i.e., we use\nE[\u00b7] := EATS-Gen (p\u03b8)[\u00b7].\n\u2022 Additionally, throughout this proof we use Et to denote expectations conditional on Ht and\nX1:T, i.e., we use\nEt [\u00b7 ] = E [\u00b7 | Ht, X1:T] .\n(8)\nNote that this means E1 [\u00b7] = E [\u00b7 | H1, X1:T] = E [\u00b7 | Z, X1:T].\n\u2022 We use H(Y) to denote the entropy of a discrete random variable Y, i.e., H(Y) = \u2211 P(Y =\ny) log P(Y = y)dy. We also use Ht(Y) = H(Y | Ht, X1:T) to denote the entropy of Y\nconditional on Ht and X1:T; Note that is standard in information theory, Ht(Y) is not a\nrandom variable, rather, it marginalizes over Ht and X1:T:\nHt(Y) := H(Y | Ht, X1:T) = E [\u2211 P(Y = y | Ht, X1:T) log P(Y = y | Ht, X1:T)dy ] ;\ny\nAbove, the outer expectation marginalizes over the history Ht and X1:T.\n\u2022 We also use I(Z; Y) to denote the mutual information between some random variables Z and\nY, i.e., I(Z;Y) = \u222b \u222byP(Z = z,Y = y) log P(Z=z,Y=y) dzdy. We further use It(Z;Y) to\nP(Z=z)P(Y=y)\ndenote the mutual information between Z and Y conditional on Ht and X1:T (where again"}, {"title": "A.3 VC Dimension", "content": "Lemma 1 (VC dimension bound on entropy). For any binary\u00b9 action policy class \u03a0*,\nH (\u03c0* (X1:T) | Z\u03c4, X1:T) \u2264 H (\u03c0*(X1:T) | X1:T) = O(VCdim(\u03a0*) log T).\nProof. The first inequality H (\u03c0*(X1:T) | Z\u03c4, X1:T) \u2264 H(\u03c0*(X1:T) | X1:T) holds by the chain rule\nfor entropy.\nThe second big O equality result holds by the Sauer-Shelah lemma [54, 57]. Specifically, by the\nSauer-Shelah lemma if a function class has VC dimension k, then that function class can produce\nk\nmost \u2211i=0 (1) = O(Tk) different labelings of any T points. Thus, since a coarse upper bound on\nthe entropy of a random variable is the log of the number of unique values that variable can take,\nVCdim(\u03a0*) ( T )\nwe get that H (\u03c0*(X1:T) | X1:T) <log\n= O(VCdim(\u03a0*) log T)."}, {"title": "A.4 Lemma 2: To minimize loss p\u00e5 needs to approximate p*.", "content": "The next lemma is a standard result connecting the excess expected loss of a sequence model p\u03b8\nto its KL divergence from the true sequence model p*. Recall, the expected loss of a sequence\nmodel p\u03b8 is denoted l(p\u03b8), defined in (6). To (nearly) minimize loss, p\u03b8 the learner needs to closely\napproximate the true sequence model p*.\nLemma 2 (Decomposing loss under p\u03b8). Under Assumptions 1 and 3, for the loss l as defined in\n(6),\nl(p\u03b8) = l(p*) + |A|\u00b7E [DKL (p* (Y1:T(a) | Z\u03c4, X1:T) || p\u03b8 (Y1:T(a) | Z\u03c4, X1:T))].\nProof. By the definition of the expected loss in (6),\nl(p\u03b8) \u2212 l(p*) = E [\u2211a\u2208A\u03c4\u2211t=1 T {log p\u03b8(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) ) \u2212 log p*(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) )}]\n(a)\n=|A|\u00b7E[\u2211t=1T 1/|A| {log p\u03b8(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) ) \u2212 log p*(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) )} ]\n=|A|\u00b7E[\u2211t=1T [DKL (p*(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) ) || p\u03b8(Yt(a) | Z\u03c4, X1:t, Y1:t\u22121(a) ))] ]."}, {"title": "A.5 Lemma 3: Action selection under perfect vs. imperfect imputation models.", "content": "Lemma 3 (KL Divergence in next action distribution). Under Assumption 3, for any t,\nE[DKL (Pt (\u03c0*(Xt; \u03c4) = \u00b7) || Pt (At = \u00b7))] \u2264 |A\u03c4|\u00b7 {l(p\u03b8) \u2212 l(p*)}.\nProof. Note the following:\nE[DKL (Pt (\u03c0*(Xt; \u03c4) = \u00b7) || Pt (At = \u00b7))]\n(a)\nE [DKL (Pp* ({Yt(a)}a\u2208A\u03c4 | X1:T, Ht) || Pp\u03b8 ({Yt(a)}a\u2208A\u03c4 | X1:T, Ht))]\nE [DKL (Pp* ({Yt(a)}a\u2208A\u03c4 | X1:T, Z\u03c4) || Pp\u03b8 ({Yt(a)}a\u2208A\u03c4 | X1:T, Z\u03c4))]\n(b)\n(c)\nAE [DKL (PP* (Y1:T(a) | X1:T, Z\u03c4)\n| Pp\u03b8 (Y1:T(a) | X1:T, Z\u03c4))] \u2264 {l(p\u03b8) \u2212 l(p*)}.\n(d)\nAbove,\n\u2022 Inequality (a) holds because \u03c0*(Xt;\u03c4) and At are both are derived by applying the same\nfunction to the outcomes {Yt(a)}a\u2208A\u03c4 .\n\u2022 Inequality (b) holds because by the chain rule for KL divergence,\nDKL (Pp* ({Yt(a)}a\u2208A\u03c4 | X1:T, Z\u03c4) || Pp\u03b8 ({Yt(a)}a\u2208A\u03c4 | X1:T, Z\u03c4))\n= DKL (Pp* ({Yt(a)}a\u2208A\u03c4 | X1:T, Ht) || Pp\u03b8 ({Yt(a)}a\u2208A\u03c4 | X1:T, Ht))\n+ DKL (Pp* (Ht | X1:T, Z\u03c4) || Pp\u03b8 (Ht | X1:T, Z\u03c4)),\nand the KL divergence is non-negative.\n\u2022 Inequality (c) holds by Assumption 2 (Independence across actions).\n\u2022 Inequality (d) holds by Lemma 2 (Decomposing loss under p\u03b8)."}, {"title": "A.6 Lemma 4: Mutual information equivalency.", "content": "Lemma 4 (Mutual information equivalency).\nIt (\u03c0*(Xt; \u03c4); (Y(At), At))\n= E [\u2211 \u03a3 Pt(At = a) Pt (\u03c0*(Xt; \u03c4) = A) \u00b7 DKL (Pt(Yt(a) | \u03c0*(Xt; \u03c4) = A) || Pt(Yt(a)))]\n\u03b1, A\u2208A\u03c4\nProof. Note that\nIt (\u03c0*(Xt; \u03c4); (Y(At), At)) = It (\u03c0*(Xt; \u03c4); Yt(At) | At)\n(a)\n=E [\u2211 Pt(At = a) It (\u03c0*(Xt; \u03c4); Yt(a)\n\u03b1\u2208A\u03c4\n= a)]\n(b)\n=E [\u2211 Pt(At = a) I(\u03c0*(Xt; \u03c4); Yt(a))]\n\u03b1\u2208A\u03c4\n(c)\n=E [\u2211 Pt(At = a) \u2211 Pt(\u03c0*(Xt; \u03c4) = A)DKL (Pt(Yt(a) | \u03c0*(Xt; \u03c4) = A) || Pt(Yt(a)))].\n(d)\nAbove, equality (a) holds since \u03c0*(Xt;\u03c4) and At are independent conditional on Ht, X1:T. Equality\n(b) holds by the definition of conditional mutual information. Equality (c) holds because Yt(a) and\n\u03c0*(Xt; \u03c4) are independent of At conditional on Ht, X1:T. Equality (d) holds by the KL divergence\nform of mutual information."}, {"title": "A.7 Lemma 5: Bounding sum of mutual information terms.", "content": "Lemma 5 (Bounding sum of mutual information terms).\n\u2211t=1 T I(\u03c0*(Xt;\u03c4); (Y(At), At)) \u2264 H(\u03c0*(X1:T) | Z\u03c4, X1:T).\nProof.\n\u2211t=1 T I(\u03c0*(Xt;\u03c4); (Y(At), At)) = \u2211i=1 T Ht(\u03c0*(X1:T))\u2212Ht(\u03c0*(X1:T) | (Y(At), At)t=1)\n\u2264 H1(\u03c0*(X1:T))\n(iii)\n\u2264 H(\u03c0*(X1:T) | Z\u03c4, X1:T).\n(iv)\nAbove, equality (i) holds by the chain rule for mutual information. Equality (ii) holds by the\nentropy formulation of mutual information. Equality (iii) holds since the entropy is always non-\nnegative. Equality (iv) holds by the definition of H1, which is the entropy conditional on H1 = {Z\u03c4}\nand X1:T."}, {"title": "A.8 Proof of Theorem 1", "content": "Theorem 1 (TS-Gen regret bound). We use l as defined in (6). Under Assumptions 1, 2, and 3,\nthe regret of the TS-Gen algorithm is bounded as follows:\n\u0394(ATS-Gen (p\u03b8)) \u2264 1/ 2T [|A\u03c4|\u00b7 H (\u03c0*(X1:T) | Z\u03c4, X1:T) + \u221a2{l(p\u03b8) \u2212 l(p*)} ]\nRegret bound for Thompson sampling\nPenalty for sub-optimal prediction\nRecall from (4) that \u03c0*(X1:T) := {\u03c0*(Xt; \u03c4)}t=1T.\nProof. Note that by the law of iterated expectations,\n\u0394(ATS-Gen) = E [ 1/T \u2211t=1 T (R(Y(*(Xt;\u03c4))) \u2212 R(Yt(At)))] = E[1/T \u2211t=1 T Et [R(Yt(*(Xt;\u03c4))) \u2212 R(Yt(At))]]\nConsider the following for any t \u2208 [1: T]:\nEt [R(Yt(*(Xt;\u03c4))) \u2212 R(Yt(At))]\n=\u2211a\u2208A\u03c4 Pt(\u03c0*(Xt; \u03c4) = a) \u00b7 Et [R(Yt(a)) | \u03c0*(Xt; \u03c4) = a] + \u2211a\u2208A\u03c4 Pt(At = a)\u00b7 Et [R(Yt(a)) | At = a]\n(i)=\u2211a\u2208A\u03c4 Pt(\u03c0*(Xt; \u03c4) = a) \u00b7 Et [R(Yt(a)) | \u03c0*(Xt; \u03c4) = a] + \u2211a\u2208A\u03c4 Pt(At = a)\u00b7 Et [R(Yt(a))]\n=\u2211a\u2208A\u03c4 \u221aPt(\u03c0*(Xt; \u03c4) = a)Pt(At = a) (Et [R(Yt(a)) | \u03c0*(Xt; \u03c4) = a] \u2212 Et [R(Yt(a))])\n+\u2211a\u2208A\u03c4 (Pt(\u03c0*(Xt; \u03c4) = a) \u2212 Pt(At = a))\n(ii)\n(\u221aPt(\u03c0*(Xt; \u03c4) = a)Et [R(Yt(a)) | \u03c0*(Xt; \u03c4) = a] + \u221aPt(At = a)Et [R(Yt(a))])\n\u2211a\u2208A\u03c4 \u221aPt(\u03c0*(Xt; \u03c4) = a)Pt(At = a) (Et [R(Yt(a)) | \u03c0*(Xt; \u03c4) = a] \u2212 Et [R(Yt(a))])\u00b2\n+ \u2211a\u2208A\u03c4 [Pt(\u03c0*(Xt; \u03c4) = a) \u2212 Pt(At = a)\n(iii)\n\u2264\u2211a\u2208A\u03c4 / 2|At| Pt(At = a) \u2211A\u2208A\u03c4 Pt(\u03c0*(Xt; \u03c4) = A) (Et [R(Yt(A)) | \u03c0*(Xt; \u03c4) = A] \u2212 Et [R(Yt(A))])\u00b2\n+ 1/2\u22c5 DKL (Pt(\u03c0*(Xt; \u03c4) = \u00b7 ) || Pt(At = \u00b7))\n(iv)\u2264\u2211a\u2208A\u03c4 Pt(At = a) \u2211A\u2208A\u03c4 Pt(\u03c0*(Xt; \u03c4) = A) \u00b7 DKL (Pt(Yt(a) | \u03c0*(Xt; \u03c4) = A) || Pt(Yt(a)))\n(v)\n+ 1/2\u22c5 DKL (Pt(\u03c0*(Xt; \u03c4) = \u00b7 ) || Pt(At = \u00b7))\nAbove, equality (i) holds since conditional on Ht, the action At and the outcome Yt(a) are inde-\npendent. Inequality (ii) uses Cauchy-Schwartz inequality in the first term and uses that R takes\nvalues in [0, 1] in the second term. Inequality (iii) uses an elementary equality of summation in the"}, {"title": "A.9 Proof of Proposition 1", "content": "Proposition 1 (Regret for Thompson sampling via generation with a perfect imputation model).\nUnder Assumptions 1 and 3, Thompson sampling via generation (Algorithm 1) with the imputation\nmodel p* has regret that is bounded as follows:\n\u0394(ATS-Gen(p*)) \u22641/ 2T [|A|\u00b7 H (\u03c0*(X1:T) | Z\u03c4, X1:T)]\nProof. This proposition holds as a direct corollary of Theorem 1."}, {"title": "B Experiment details", "content": "B.1 Data generating process\nSynthetic setting We evaluate our method on a synthetic contexutal bandit setting. The\ntask features Z for a given bandit task consist of one feature per action, i.e. Z = {Zt(a)}a\u2208A\u03c4,\nwhere only Zt(a) affects the reward for action a. For simplicity, R(y) = y. For taskt, ac-\ntion a, and timestep t, with action features Zt(a), context features Xt, and unknown coefficients\nU(a) := (Uconst(a), Uz(a), UX(a), Ucross), let \u03c3(w) := (1 + exp(\u2212w))\u22121, and define\nWt(a) = Uconst(a) + Uz(a)Zt(a) + UX(a)Xt + XtUcrossZt(a)\nwhere Yt(a) \u223c Bernoulli(\u03c3(Wt(a))) i.i.d.\n(10)\nAll of the random variables above are generated i.i.d. for each task \u03c4. All of the random variables\nindexed by action a above are also generated i.i.d. across actions a \u2208 A\u03c4.\nWe generate each Zt(a) \u223c N(02, I2) and Xt \u223c N(05, I5) as multivariate Gaussians. The unob-\nserved coefficients are also drawn as multivariate Gaussians: Uconst(a) \u223c N(0,1), Uz(a) \u223c N(12, I2 \u22c5\n0.252), UX(a) \u223c N(I5, I5 \u22c5 0.252). The last coefficient Ucross is drawn as a random diagonal matrix,\nwhere the diagonal entries are each drawn independently as i.i.d. N(1, 0.252).\nUnless otherwise specified, the training dataset consists of 10k independently drawn actions,\nand the validation set also consists of 10k independently drawn actions. For bandit evaluations, sets\nof 10 actions are drawn independently for each bandit environment.\nSemi-synthetic setting We extend our synthetic experiment setting to a semi-synthetic news\nrecommendation setting in which we use text headlines Zt(a) for arm a, so that the sequence model\nrequires feature learning. We define\nWt(a) = Uconst(a) + Uz(a)\u03a6z(Zt(a)) + UX(a)\u03a6x(Xt) + \u03a6x(Xt)Ucross\u03a6z(Zt(a))\nwhere Yt(a) \u223c Bernoulli(\u03c3(Wt(a))) i.i.d.\n(11)\nThis is similar to the synthetic setting in Equation (10), except that the data-generating process\nuses \u03a6x(Xt) and \u03a6z(Zt(a)) instead of Xt, Zt(a), respectively, where \u03c6x and \u03c6z are nonlinear. This\nincreases the difficulty of the learning task for the sequence model. The rest of the data generation\nis the same, aside from Zt(a) being text headlines and using \u03a6x(Xt) and \u03c6z(Zt(a)), is the same.\nMore specifically, the headlines Zt(a) are sampled randomly (without replacement) from the\nMIND large dataset [65] (training split only). The headlines are split into training, validation, and\nbbandit evaluation sets, where headlines are disjoint between these three datasets. The training and\nvalidation sets are used to train and perform model selection for sequence models, and the bandit\nevaluation set is solely for evaluating regret. We generate one draw of one action (i.e. Wt(a)) for\neach headline. Unless otherwise specified, the training set has 20k headlines, validation has 10k,\nand the bandit set is everything left over, which is about 74k headlines.\nAdditionally, \u03a6z(Zt(a)) is a two-dimensional vector, where the first dimension is the probability\noutput of a pre-trained binary [55] evaluated on Zt(a), and the second dimension is the probability\noutput of a binary pre-trained formality classifier on Zt(a) [4] with outputs normalized to have mean\n0 and variance 1. Both models were obtained from huggingface.com. Next, \u03a6x(Xt) is as follows:\nas Xt \u2208 R5 as defined in the synthetic setting, \u03a6X(Xt)t,1:4 = Xt,1:4 \u00b7 sign(Xt,5), i.e. \u03c6x multiplies\nthe first four dimensions of Xt by the sign of the fifth dimension."}, {"title": "B.2 Offline training", "content": "B.2.1 Resampling historical data\nIt is uncommon in to have access to all potential outcomes for all actions in realistic scenarios.\nInstead, it is more common to have access to the outcome corresponding to the action that was\ntaken. Under the assumption that the contexts Xt are exchangeable, and that the actions chosen\nhistorically were chosen at random, then for each action a, we can consider the contexts Xt for\ntimesteps t for which this action was taken, and the corresponding outcomes Yt(a). We assume\nthat we have 1000 such timesteps per action. During training, in every epoch, we sample without\nreplacement from this set of (Xt, Yt(a))'s to form a sequence of length 500; the sequence model p\u03b8\nis then trained on such sequences of data.\nB.2.2 Sequence model architecture"}, {"title": "B.2.3 Additional sequence model training details", "content": "Synthetic setting We train (and validate)"}]}