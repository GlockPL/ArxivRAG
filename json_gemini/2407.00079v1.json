{"title": "Mooncake: Kimi's KVCache-centric Architecture for LLM Serving", "authors": ["Ruoyu Qin", "Zheming Li", "Weiran He", "Mingxing Zhang", "Yongwei Wu", "Weimin Zheng", "Xinran Xu", "Moonshot AI", "Tsinghua University"], "abstract": "Mooncake is the serving platform for Kimi, a leading LLM service provided\nby Moonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the underutilized\nCPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated\ncache of KVCache. The core of Mooncake is its KVCache-centric scheduler,\nwhich balances maximizing overall effective throughput while meeting latency-\nrelated Service Level Objectives (SLOs). Unlike traditional studies that assume all\nrequests will be processed, Mooncake faces challenges due to highly overloaded\nscenarios. To mitigate these, we developed a prediction-based early rejection policy.\nExperiments show that Mooncake excels in long-context scenarios. Compared to\nthe baseline method, Mooncake can achieve up to a 525% increase in throughput\nin certain simulated scenarios while adhering to SLOs. Under real workloads,\nMooncake's innovative architecture enables Kimi to handle 75% more requests.", "sections": [{"title": "1 Introduction", "content": "With the rapid adoption of large language models (LLMs) in various scenarios [1, 2, 3, 4], the work-\nloads for LLM serving have become significantly diversified. These workloads differ in input/output\nlength, frequency and distribution of arrival, and, most importantly, demand different kinds of Service\nLevel Objectives (SLOs). As a Model as a Service (MaaS) provider, one of the primary goals of\nKimi [5] is to solve an optimization problem with multiple complex constraints. The optimization\ngoal is to maximize overall effective throughput, which directly impacts revenue, while the constraints\nreflect varying levels of SLOs. These SLOs typically involve meeting latency-related requirements,\nmainly the time to first token (TTFT) and the time between tokens (TBT).\nTo achieve this goal, a prerequisite is to make the best use of the various kinds of resources available in\nthe GPU cluster. Specifically, although GPU servers are currently provided as highly integrated nodes\n(e.g., DGX/HGX supercomputers [6]), it is necessary to decouple and restructure them into several\ndisaggregated resource pools, each optimized for different but collaborative goals. For example, many\nresearchers [7, 8, 9] have suggested separating prefill servers from decoding servers because these\ntwo stages of LLM serving have very different computational characteristics, in which the KVCache\nshifts with requests moving from prefill to decoding servers.\nBuilding on this idea, we found that the scheduling of KVCache is central to LLM serving scheduling.\nTo improve overall throughput, there are typically two general approaches: 1) reuse KVCache as\nmuch as possible to reduce the required computation resources; and 2) maximize the number of\ntokens in each batch to improve the Model FLOPs Utilization (MFU). However, reusing KVCache"}, {"title": "1.2 Design and Results of Mooncacke", "content": "In the following sections of this paper, we first present an overview of Mooncake's architecture,\nincluding its main components and the typical workflow for processing a request (\u00a73). Then, we\ndescribe the main design choices made during its implementation, especially those not covered in\ncurrent research.\nFirst, in \u00a74, we discuss how to implement a separate prefill node pool that seamlessly handles the\ndynamic distribution of context length. We employ a chunked pipeline parallelism (CPP) mechanism\nto scale the processing of a single request across multiple nodes, which is necessary for reducing the\nTTFT of long-context inputs. Compared to traditional sequence parallelism (SP) based solutions, CPP\nreduces network consumption and simplifies the reliance on frequent elastic scaling. This mechanism\nis further supplemented with layer-wise prefill that enables stream transferring of KVCache to overlap\nlatency.\nNext, in \u00a75, we detail our KVCache-centric request scheduling algorithm, which balances instance\nloads and user experience as measured by TTFT and TBT SLOs. This includes a heuristic-based\nautomated hot-spot migration scheme that replicates hot KVCache blocks without requiring precise\npredictions of future KVCache usage. Experimental results show that our cache-aware scheduling can\nsignificantly lower TTFT in real-world scenarios. In end-to-end experiments using public datasets,\nsimulated data, and real workloads, Mooncake excels in long-context scenarios. Compared to the\nbaseline method, Mooncake can achieve up to a 525% increase in throughput while meeting SLOs.\nUnder real workloads, Mooncake enables Kimi to handle 75% more requests.\nFinally, unlike existing work on LLM serving that assumes all requests will be processed, Mooncake\nconsistently faces overload due to Kimi's rapid growth in user requests. Thus, Mooncake's scheduling\ninvolves determining whether to accept or reject incoming requests based on the system load. In \u00a76,\nwe discuss our implementation of a unique early rejection policy that reduces wasted computational\nresources in overloaded scenarios. We further explore the load fluctuation problem caused by\nstraightforward early rejection and how predicting future load can mitigate this issue.\nMooncake is currently the primary platform for serving Kimi and has successfully handled exponential\nworkload growth, proving its effectiveness in scaling out to large and highly overloaded workloads.\nHowever, many more problems need to be explored, and these future directions are also included in\nthe paper.\nTo protect proprietary information and facilitate reproducibility, all the ex-\nperimental results reported in this paper are based on replayed traces of real\nworkloads, but using a dummy model that follows the same architecture as\nLLaMA2-70B. The trace includes only the timing of request arrivals, the number\nof input tokens, and the number of output tokens, without any real user content.\nThe trace will be open-sourced later after following certain internal procedures."}, {"title": "2 Preliminary and Problem Definition", "content": "Modern large language models (LLMs) are based on the Transformer architecture, which utilizes\nattention mechanisms and multilayer perceptrons (MLP) to process input. Popular Transformer-based\nmodels, such as GPT [10] and LLaMA [11], employ a decoder-only structure. Each inference request\nis logically divided into two stages: the prefill stage and the decoding stage.\nIn the prefill stage, all input tokens are processed in parallel. This stage generates the first output\ntoken while storing intermediate results of computed keys and values, referred to as the KVCache.\nThe decoding stage then uses this KVCache to autoregressively generate new tokens, adding new keys\nand values from the computation to the KVCache. The ability to process input tokens simultaneously\nin the prefill stage typically makes it computationally intensive, except for short requests. Since\nthe computational complexity of attention networks scales quadratically with input length while"}, {"title": "3 Overview of Mooncake's Disaggregated Archtecture", "content": "As depicted in Figure 1, Mooncake employs a disaggregated architecture that not only separates prefill\nfrom decoding nodes, but also groups the CPU, DRAM, SSD, and RDMA resources of the GPU\ncluster to implement a disaggregated KVCache. This disaggregated cache harnesses underutilized"}, {"title": "4 Implementation of the Prefill Pool", "content": "Unlike the inviolable decoding nodes, the necessity and best practices for designing a separate and\nelastic prefill pool remain under debate. For example, although many researchers [7, 8, 9] share our\nintuition to use a disaggregated architecture, it is worth discussing whether this separation is still\nnecessary with the introduction of chunked prefill [15]. Chunked prefill divides the input tokens into\nmultiple small chunks that join the continuous batch process. This approach has two clear benefits:\n1) Without separation, all nodes are treated equally, making scheduling easier; 2) Inlining chunked\nprefill into the decoding batch can improve the computational intensity of the decoding batch, leading\nto better MFU.\nHowever, after careful consideration, we decided to maintain Mooncake's disaggregated architecture.\nA request's prefill is inlined into the decoding batch only when it can be forwarded without chunking\nand without compromising the TBT SLO. There are two main reasons for this decision: 1) Prefill\nnodes require different cross-node parallelism settings to handle long contexts (\u00a74.1). 2) It presents a\nunique opportunity to save VRAM (\u00a74.2)."}, {"title": "4.1 Multi-node Prefill", "content": "The available context length of recent LLMs is increasing rapidly, from 8k to 128K and even 1M [16].\nTypically, for such long context requests, the input tokens can be 10 to 100 times larger than the\noutput tokens, making optimizing the TTFT crucial. Due to the abundant parallelism in long context\nprefill, using more than a single 8x GPU node to process them in parallel is desirable. However,\nextending tensor parallelism (TP) across more than one node requires two expensive RDMA-based\nall-reduce operations per layer, significantly reducing the MFU of prefill nodes."}, {"title": "4.2 Layer-wise Prefill", "content": "Beyond computational power, the limited size of VRAM is also a precious resource, and we aim to\nminimize the VRAM occupation by states, primarily the KVCache. Theoretically, if the KVCache\nsize of a request is S and the processing time is T, its occupation cost is S * T. If a request is chunked\nand the processing of each chunk is inlined with other decoding requests in chunked prefill, T will\nincrease, leading to a larger occupation cost.\nMoreover, since prefill is processed layer-by-\nlayer and is computation-bound, it is possible\nto overlap the transferring and dumping of KV-\nCache with computation, further reducing its\noccupation cost. In Mooncake, KVCache load-\ning and storing are executed asynchronously via\nlaunch and wait operations. Before each layer's\nattention computation begins, the model waits\nfor the asynchronous loading of that layer's KV-\nCache to complete and triggers the next layer's\nasynchronous KVCache loading. After the at-\ntention calculation is complete, asynchronous\nstorage of that layer's KVCache is launched.\nOnce all layers' computations are finished, the\nprocess waits for the completion of all asyn-\nchronous storage operations. Transfer overlap-\nping allows the prefill instance's execution time\nto be roughly equivalent to either the KVCache\nloading time or the standard prefilling time, de-\npending on the prefix cache proportion relative"}, {"title": "5 KVCache-centric Scheduling", "content": "In this section, we mainly discuss how Conductor schedules the requests and KVCache blocks under\nnormal conditions, leaving the discussion on overload scenarios for the next section."}, {"title": "5.1 Prefill Global Scheduling", "content": "Previous research on LLM serving typically uses a load-balancing strategy that evaluates the load\non each instance based on the number of assigned requests. In Mooncake, however, the selection\nof prefill instances considers additional factors-not just load but also the prefix cache hit length\nand the distribution of reusable KVCache blocks. While there is a preference to route requests to"}, {"title": "5.2 Cache Load Balancing", "content": "In our Mooncake cluster, each prefill machine manages its own set of local prefix caches. The usage\nfrequency of these caches varies significantly. For example, system prompts are accessed by almost\nevery request, whereas caches storing content from a local long document may be used by only one\nuser. As discussed in \u00a75.1, Conductor's role is crucial in achieving an optimal balance between\ncache matching and instance load. Thus, from the perspective of the distributed cache system, load\nbalancing also plays an important role. Specifically, it involves strategizing on how to back up caches\nto ensure that global prefill scheduling can achieve both high cache hits and low load.\nA straw-man solution to this KVCache scheduling problem could be collecting the global usages of\neach block, using a prediction model to forecast their future usages, and making scheduling decisions\naccordingly. However, unlike the estimation of prefill time, workloads are highly dynamic and change\nsignificantly over time. Especially for a MaaS provider experiencing rapid growth in its user base,\nit is impossible to accurately predict future usages. Thus, we propose a heuristic-based automated\nhot-spot migration scheme to enhance cache load balancing.\nAs previously noted, requests may not always be directed to the prefill instance with the longest prefix\ncache length due to high instance load. In such cases, the conductor forwards the cache's location\nand the request to an alternative instance if the estimated additional prefill time is shorter than the\ntransfer time. This instance proactively retrieves the KVCache from the holder and stores it locally.\nMore importantly, we prefer to compute the input tokens if the best remote prefix match length is no\nlarger than the current local reusable prefix multiplied by a threshold. Both strategies not only reduce\nthe prefill time for requests but also facilitate the automatic replication of hot-spot caches, allowing\nfor their broader distribution across multiple machines."}, {"title": "6 Overload-oriented Scheduling", "content": "Most existing work on LLM serving assumes that all requests will be processed, optimizing the\nthroughput or the TTFT and TBT of requests accordingly. However, in real scenarios, processing\nevery incoming request is neither economical nor realistic. For commercial inference services facing\nrapidly increasing volumes of user requests, the growth rate of the cluster's inference resources is far\nslower than the increase in incoming requests. As a result, overload is a common issue in current\nLLM serving, especially during peak times.\nTo balance costs and user experience, the system should process as many requests as possible until the\nsystem load reaches a predefined threshold. After this point, remaining requests will be either directly\nrejected or deferred for later retry. Mooncake, implemented as a disaggregated inference system,\nallows for more flexible scheduling strategies but also confronts unique scheduling challenges not\npresent in non-disaggregated systems and not mentioned in previous works[7, 8, 9].\nIn this section, we describe an early rejection policy designed specifically for a disaggregated\narchitecture and address the load fluctuation caused by this approach. We then explore how predicting\nthe generation length is necessary to mitigate these problems."}, {"title": "6.1 Scheduling in Overload Scenarios", "content": "In scenarios where system overload occurs, scheduling involves determining whether to accept or\nreject incoming requests based on the system load. A critical aspect of this process is defining what\nconstitutes the \"system load\", as this definition influences the threshold at which requests are rejected.\nIn conventional coupled systems, the prediction of TTFT and TBT can be complicated by interference\nbetween the prefill and decoding stages. Therefore, the load is often measured simply by the ratio of\nthe number of requests being processed to the system's maximum capacity.\nIn contrast, Mooncake, with its disaggregated architecture, processes the prefill and decoding stages\nindependently. Thus we use SLO satisfaction as a direct load measurement. Specifically, we define\n$l_{ttft}$ and $l_{tbt}$ as the TTFT and TBT SLO constraints for requests, respectively. The load for prefill\nand decoding instances is then determined by comparing the predicted maximum TTFT and TBT on\nan instance against $l_{ttft}$ and $l_{tbt}$. With these two criteria, Mooncake's scheduling requires two key\ndecisions: first, whether to accept the prefill stage based on the prefill instance's load, and second,\nwhether to proceed with the decoding stage depending on the decoding instance's load."}, {"title": "6.2 Early Rejection", "content": "In practice, the individual load on prefill or decoding instances does not accurately reflect the actual\nnumber of requests processed by the system. This discrepancy arises due to a time lag between\nscheduling prefill and decoding instances for a single request. If a request is rejected by the decoding\ninstance due to high load after the prefill stage has been completed, the computational resources\nexpended during the prefill stage are wasted. Consequently, the actual number of successfully\nprocessed requests during prefill is less than that indicated by the load metric.\nTo address this issue, it is natural to advance the load assessment of the decoding instance to precede\nthe beginning of the prefill stage. We refer to this strategy as Early Rejection. Upon the arrival\nof a request, Conductor evaluates whether to accept the request based on the greater load between\nthe prefill and decoding pools. Early Rejection significantly reduces ineffective computations from\nrejected requests and enhances load balancing."}, {"title": "6.3 Load Fluctuation Caused by Early Rejection", "content": "However, Early Rejection introduces new challenges. Figure 7 shows the observed real-world\ninstance load over a 20-minute period in a cluster of 20 machines after using the Early Rejection\nstrategy. It highlights significant anti-phase fluctuations between prefill and decoding machines. This\nphenomenon becomes more pronounced in clusters with fewer prefill machines and in scenarios\nwhere the prefill stage takes longer.\nUpon further exploration, we found that this load fluctuation problem is rooted in the time lag between\npredicting the decoding load and its actual execution. Scheduling based on the current decoding\nload is inherently delayed. This delay causes fluctuations and phase staggering between the loads\non prefill and decoding instances, as illustrated in the theoretical example described in Figure 8a.\nThe green curve represents the load of prefill instances (scaled from 0 to 1), and the yellow curve\nrepresents the load of decoding instances.\nIn Stage 1, the load on both prefill and decoding instances is low, so Conductor accepts a large\nnumber of requests until the load on prefill instances reaches its limit. In Stage 2, requests processed\nby prefill instances are scheduled to decoding instances, causing the load on decoding instances to be\nhigh. Consequently, Conductor rejects incoming requests, leading to a lower load on prefill instances.\nIn Stage 3, no new requests enter the decoding stage, resulting in a decreased load. At this point,\nConductor again accepts a large number of requests until the prefill instances are fully loaded. In\nStage 4, as the load on decoding instances increases, Conductor rejects requests, causing a low load\non prefill instances. This severe fluctuation in load between prefill and decoding instances results in\npoor resource utilization of the inference cluster."}, {"title": "6.4 Early Rejection Based on Prediction", "content": "To solve the load fluctuation problem, we propose a framework of Early Rejection Based on Prediction\nto address scheduling challenges in overload scenarios for disaggregated LLM serving systems like\nMooncake. As illustrated in Figure 8b, this framework predicts the decoding load after the prefill\nstage of incoming requests and uses this prediction to decide whether to accept the requests, which\nhelps mitigate the fluctuation problem. The core component of this strategy is the accurate prediction\nof the decoding load for the subsequent period. We introduce two approaches for this:\nRequest level: Previous work highlights a significant challenge in predicting loads for LLM serving:\nthe unknown output length of each request. If we could determine the output length in advance, it\nwould be possible to estimate the TTFT and TBT much more accurately. This, in turn, would help\npredict the number of requests a decoding instance can complete and the number of new requests that\nwill be added after a specified time, thereby obtaining the load at that time. However, predicting each\nrequest's output length is challenging due to high costs [9] or low accuracy, especially under overload\nconditions where resources are scarce and accurate predictions are necessary, making request-level\npredictions particularly difficult.\nSystem level: In contrast to request-level predictions, system-level predictions do not attempt to\npredict the completion time for individual requests. Instead, they estimate the overall batch count or\nthe TBT status for instances after a specified time. This type of prediction is ongoing and requires\nless precision, making it more appropriate for overload scenarios.\nIn Mooncake, we currently utilize a system-level prediction strategy: we assume that each request's\ndecoding stage takes a uniform time $t_a$. First, for a given moment $t$, requests that can be completed\nby the prefill instances at $t$ are added to the uniform decoding instances. Next, requests that will be\ncompleted (i.e., their execution time exceeds $t_a$) before $t$ are removed from the decoding instances.\nFinally, the average TBT ratio of all decoding instances to $l_{tbt}$ is calculated to predict the load. The\nexploration of request-level prediction is left for future work."}, {"title": "7 Evaluation", "content": "This section evaluates the end-to-end performance of Mooncake under different datasets and various\nworkloads. As stated before, to protect proprietary information and facilitate reproducibility, all\nthe experimental results reported in this paper are based on a dummy model that follows the same\narchitecture as LLaMA2-70B.\nTestbed During the experiments, the system was deployed on a high-performance computing node\ncluster to test performance. Each node in the cluster is configured as follows: 8 NVIDIA-A800-\nSXM4-80GB GPUs, each with 80GB HBM, connected by NVLINK; equipped with RDMA network\ncards that supporting up to 800 Gbps of interconnect bandwidth between nodes. Each node deploys\neither a prefill instance or a decoding instance according to the startup parameter.\nDataset and Workload Building upon previous research [15, 8, 14], we selected or designed the\ndatasets as outlined in Table 1. In addition to utilizing public datasets, we generated a batch of\nsimulated data featuring predefined lengths and prefix cache ratios for our experiments. To examine\nperformance in real-world scenarios, we constructed a dataset consisting of 23,000 real request traces,\neach annotated with an arrival timestamp. Experiments involving real request traces were conducted\nby replaying these requests according to their actual arrival times. For other scenarios, we simulated\nrequests using a Poisson arrival process and controlled the request rate through RPS (Requests per\nSecond).\nMetric In the experiments, we focus on the throughput performance of various systems under\ndefined SLOs. We measure the TTFT and TBT across different RPS rates, where a higher RPS\nsignifies improved throughput. To assess whether the majority of requests satisfy the SLOs, we use\nthe 90th percentile (P90) values of TTFT and TBT as the ultimate metrics. As mentioned in \u00a72,\nthe thresholds for TTFT and TBT are set by multiplying the lowest observed RPS values by factors\nof 10 and 5, respectively. Exceeding these thresholds indicates a failure to meet the SLOs and the\ncorresponding consumed resources are considered as wasted. For ease of comparison, we normalize\nall TTFT and TBT values against these upper limits, establishing a baseline of 1.0.\nBaseline We employ vLLM, one of the state-of-the-art open-source LLM serving systems, as our\nexperimental baseline. vLLM incorporates continuous batching and PagedAttention technologies,\nsignificantly boosting inference throughput. Despite its strengths, vLLM's design, which couples the\nprefill and decoding stages of inference requests, can cause disruptions during decoding in scenarios\ninvolving long contexts."}, {"title": "7.1.1 Public Datasets", "content": "This section evaluates the performance of Mooncake and vLLM in end-to-end tests on public datasets\nusing ArXiv Summarization and L-Eval. We establish a baseline using a cluster of four vLLM\ninstances, denoted as vLLM-[4M]. In contrast, Mooncake is configured in two distinct setups: one\ncluster consists of three prefill instances and one decoding instance, labeled Mooncake-[3P+1D],\nand the other has two prefill and two decoding instances, labeled Mooncake-[2P+2D]. The results,\ndepicted in Figure 9, demonstrate that on the ArXiv Summarization and L-Eval datasets, Mooncake-\n[3P+1D] achieves throughput improvements of 20% and 40%, respectively, over vLLM-[4M] while\nsatisfying SLOs. Moreover, Mooncake's throughput on the L-Eval dataset is further enhanced by\nprefix caching, which significantly reduces prefill time. However, despite having lower TBT latency,\nMooncake-[2P+2D] does not perform as well on the TTFT metric compared to Mooncake-[3P+1D]\nand vLLM-[4M]. This discrepancy arises from an imbalance in the load between prefill and decoding\ninstances. In real-world clusters, the demand for prefill and decoding instances generally remains\nstable over certain periods, with only minor temporary imbalances. Thus, the proportion of prefill\nand decoding instances can be preset. Future research will explore more flexible deployment and\nconversion methods."}, {"title": "7.1.2 Simulated Data", "content": "In this section, we employ simulated data for an end-to-end experiment. The cluster configuration is\nthe same as in \u00a77.1.1, utilizing Mooncake configurations of [3P+1D], [2P+2D], and vLLM-[4M].\nNotably, the long-context requests in simulated data significantly disrupt the decoding stage of vLLM.\nTo counteract this, vLLM processes requests individually, rather than in batches. The results of the\nexperiment are presented in Figure 10. Although Mooncake employs batch processing, its two-stage\ndisaggregation design effectively minimizes the impact of the prefill stage on the decoding stage,\nensuring it never breaks the TBT SLO. Mooncake demonstrates significantly higher throughput,\nwith enhancements ranging from 50% to 525%, while adhering to the same TTFT and TBT SLO\nconstraints compared to vLLM."}, {"title": "7.1.3 Real Workload", "content": "We further utilize 10 prefill instances and 10 decoding instances, labeled Mooncake-[10P+10D], along\nwith 20 instances of vLLM, referred to as vLLM-[20M], to replay real request traces and conduct\nload tests on both Mooncake and vLLM. In this experimental setup, the upper limit for the TTFT is\nset at 30 seconds, while the TBT threshold is capped at 0.1 seconds per token. Figure 11 presents the\nCDF (Cumulative Distribution Function) plots for the TTFT and TBT for the two systems. The TTFT\ndistributions for both Mooncake-[10P+10D] and vLLM-[20M] are nearly identical, with almost\n100% of requests meeting the TTFT SLO. However, while approximately 100% of the requests for\nMooncake-[10P+10D] satisfy the TBT SLO, only 57% of the requests for vLLM-[20M] meet this\ncriterion, with some requests exhibiting extremely high TBTs. In this experiment, Mooncake can\nprocess approximately 75% more requests while adhering to the SLOs."}, {"title": "7.2 Performance in Overload Scenarios", "content": "In this section, we evaluate performance under overload scenarios, focusing on the maximum number\nof requests the system can handle, as discussed in \u00a76. The baseline strategy, which rejects requests\nbased on load before both stages start, leads to resource wastage by rejecting requests already\nprocessed in the prefill stage. In contrast, we propose the Early Rejection and Early Rejection based\non Prediction strategies, detailed in \u00a76.2 and \u00a76.4, respectivly. These strategies take the system's load\ninto comprehensive consideration, and hence reducing unnecessary request rejections.\nSpecifically, We built a Mooncake cluster with 8 prefill instances and 8 decoding instances and tested\nit using real traces from 23,000 requests. To simulate overload scenarios, we increased the replay\nspeed to 2x."}, {"title": "8 Related Work", "content": "Significant efforts have been dedicated to enhancing the efficiency of LLM serving systems through\nscheduling, memory management, and resource optimization. Production-grade systems like Faster-\nTransformer [28], TensorRT-LLM [29], and DeepSpeed Inference [30] are designed to significantly\nboost throughput. Orca [12] employs iteration-level scheduling to facilitate concurrent processing\nat various stages, while vLLM [13] leverages dynamic KV cache management to optimize memory.\nFlexGen [31], SARATHI [15], and FastServe [32] incorporate innovative scheduling and swapping\nstrategies to distribute workloads effectively across limited hardware, often complementing each\nother's optimizations.\nOur design of Mooncake builds on these developments, particularly drawing from the open-source\ncommunity of vLLM, for which we are deeply appreciative.\nMoreover, recent research shares our insight into separating the prefill and decoding stages, leading to\na disaggregated architecture that enhances system throughput. The arXiv publication of Splitwise [7]\nis at the early stage of the development of Mooncake, which further motivated our progress. Many\nconcurrent works corroborate our findings, including DistServe [8], which optimizes resource allo-\ncation and parallel strategies for each stage to maximize GPU goodput, and TetriInfer [9], which\nincorporates both chunked prefill and two-stage disaggregation along with a predictive two-stage\nscheduling algorithm to optimize resource utilization.\nPrefix caching is also widely adopted to enable the reuse of KV caches across multiple requests,\nreducing computational overhead in LLM inference systems [29, 13]. Prompt Cache [33] precomputes\nand stores frequently used text KV caches on inference servers, facilitating their reuse and significantly\nreducing inference latency. SGLang [34] leverages RadixAttention, which uses a least recently used\n(LRU) cache within a radix tree structure to efficiently enable automatic sharing across various reuse\npatterns.\nAmong these approaches, AttentionStore [35], a concurrent work with us, proposes a hierarchical KV\ncaching system that utilizes cost-effective memory and storage media to accommodate KV caches\nfor all requests. The architecture of Mooncake shares many design choices with AttentionStore.\nHowever, in long-context inference, the KV cache becomes extremely large, requiring high capacity\nand efficient data transfer along with KVCache-centric global scheduling. Additionally, Mooncake is\nnot a standalone cache service, it incorporates both a memory-efficient cache storage mechanism and\na cache-aware scheduling strategy, further improving prefix caching efficiency.\nFurthermore, recent research [36] has started exploring the scheduling of prompts, which is essentially\nKVCache-centric scheduling. We corroborate many results in this area, although the real reusability in\nour online traces is much smaller than results reproduced by open-source benchmarks. Theoretically,\nup to only 50% of the KVCache can be reused in our current workloads, even if we assume both\nthe capacity of storage and the TTFT SLO are infinite. However, this reusability highly depends on\nthe application scenario and can be as large as 90% for certain scenarios, such as our chat-to-paper"}, {"title": "9 Future Work", "content": "Disaggregating different parts of LLM serving into dedicated resource pools is key to Mooncake's\nhigh resource utilization. In the future, we plan to explore more opportunities along this path,\nparticularly the potential use of heterogeneous accelerators. Current flagship accelerators balance"}, {"title": "7.1 End-to-end Performance", "content": "Dataset\nArXiv Summarization [26]\nL-Eval [27]\nSimulated Data\nReal Data\nAvg Input Length\n8088\n19019\n16k, 32k, 64k, 128k\n7955\nAvg Output Length\n729\n72\n512\n194\nCache Ratio\n~0%\n>80%\n50%\n~50%\nArrival Pattern\nPoisson Process\nPoisson Process\nPoisson Process\nTimestamp-based"}, {"title": "7.1 End-to-end Performance", "content": "Baseline\nNumber of rejected requests\nEarly Rejection\n4183\nEarly Rejection\n3771\nEarly Rejection\n3589"}]}