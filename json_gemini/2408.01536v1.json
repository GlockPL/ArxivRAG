{"title": "ACTIVE LEARNING FOR NEURAL PDE SOLVERS", "authors": ["Daniel Musekamp", "Marimuthu Kalimuthu", "David Holzm\u00fcller", "Makoto Takamoto", "Mathias Niepert"], "abstract": "Solving partial differential equations (PDEs) is a fundamental problem in engineering and science. While neural PDE solvers can be more efficient than established numerical solvers, they often require large amounts of training data that is costly to obtain. Active Learning (AL) could help surrogate models reach the same accuracy with smaller training sets by querying classical solvers with more informative initial conditions and PDE parameters. While AL is more common in other domains, it has yet to be studied extensively for neural PDE solvers. To bridge this gap, we introduce AL4PDE, a modular and extensible active learning benchmark. It provides multiple parametric PDEs and state-of-the-art surrogate models for the solver-in-the-loop setting, enabling the evaluation of existing and the development of new AL methods for PDE solving. We use the benchmark to evaluate batch active learning algorithms such as uncertainty- and feature-based methods. We show that AL reduces the average error by up to 71% compared to random sampling and significantly reduces worst-case errors. Moreover, AL generates similar datasets across repeated runs, with consistent distributions over the PDE parameters and initial conditions. The acquired datasets are reusable, providing benefits for surrogate models not involved in the data generation.", "sections": [{"title": "INTRODUCTION", "content": "Partial differential equations describe numerous physical phenomena such as fluid dynamics, heat flow, and cell growth. Because of the difficulty of obtaining exact solutions for PDEs, it is common to utilize numerical schemes to obtain approximate solutions. However, numerical solvers require a high temporal and spatial resolution to obtain sufficiently accurate numerical solutions, leading to high computational costs. This issue is further exacerbated in settings like parameter studies, inverse problems, or design optimization, where many iterations of simulations must be conducted. Thus, it can be beneficial to replace the numerical simulator with a surrogate model by training a neural network to predict the simulator outputs (Takamoto et al., 2022; Lippe et al., 2023; Brandstetter et al., 2021; Gupta & Brandstetter, 2023; Li et al., 2020b). In addition to being more efficient, neural surrogate models have other advantages, such as being end-to-end differentiable.\nOne of the main challenges of neural PDE surrogates is that their training data is often obtained from the same expensive simulators they are intended to ultimately replace. Hence, training a surrogate provides a computational advantage only if the generation of the training data set requires fewer simulations than will be saved during inference. Moreover, it is non-trivial to obtain training data for a diverse set of initial conditions and PDE parameters required to train a surrogate model with sufficient generalizability. For instance, contrary to training foundation models for text and images, foundation models for solving PDEs require targeted and expensive data generation to generalize well.\nActive learning is a possible solution to these challenges as it might help to iteratively select a smaller number of the most informative and diverse training trajectories, thereby reducing the total number of simulations required to reach the same level of accuracy. Furthermore, AL may also improve the reliability of the surrogate models by covering challenging dynamical regimes with enough training data, which may otherwise be hard to find through hand-tuned input distributions. However,"}, {"title": "BACKGROUND", "content": "We seek the solution $u : [0,T] \\times X \\rightarrow \\mathbb{R}^{N_c}$ of a PDE with a D-dimensional spatial domain X,\n$x = [x_1,x_2,...,x_D]^\\top \\in X$, temporal domain $t \\in [0,T]$, and $N_c$ field variables or channels c\n(Brandstetter et al., 2021):\n$\\partial_t u = F (\\Lambda, t, x, u, \\partial_x u, \\partial_{xx} u, ...), \\quad (t, x) \\in [0, T] \\times X$ (1)\n$u(0,x) = u_0(x), \\quad x \\in X;\\quad \\mathcal{B}[u](t, x) = 0, \\quad (t, x) \\in [0,T] \\times \\partial X$ (2)\nHere, the boundary condition $\\mathcal{B}$ (Eq. 2) determines the behavior of the solution at the boundaries $\\partial X$\nof the spatial domain X, and the initial condition (IC) $u^0$ defines the initial state of the system (Eq. 2).\nThe vector $\\Lambda = (\\lambda_1,..., \\lambda_k) \\in \\mathbb{R}^k$ with $\\lambda_i \\in [a_i, b_i]$ denotes the PDE parameters which influence\nthe dynamics of the physical system governed by the PDE such as the diffusion coefficient in Burgers'\nequation. In the following, we only consider a single boundary condition (periodic) for simplicity,"}, {"title": "RELATED WORK", "content": "Neural surrogate models for solving parametric PDEs is a popular area of research (Takamoto et al., 2023; Kapoor et al., 2023; Lippe et al., 2023; Hagnberger et al., 2024; Cho et al., 2024). Most existing works, however, often focus on single or uniformly sampled parameter values for the PDE coefficients and improving the neural architectures to boost the accuracy. In the context of neural PDE solvers, AL has primarily been applied to select the collocation points of PINNs. A typical approach is to sample the collocation points based on the residual error directly (Arthurs & King, 2021; Gao & Wang, 2023; Mao & Meng, 2023; Wu et al., 2023a). While this strategy can be effective, it differs from standard AL since it uses the \u201clabel\u201d, i.e., the residual loss, when selecting data points. Aikawa et al. (2023) use a Bayesian PINN to select points based on uncertainty, whereas Sahli Costabal et al. (2020) employ a PINN ensemble for AL of cardiac activation mapping.\nPestourie et al. (2020) use AL to approximate Maxwell equations using ensemble-based uncertainty quantification for metamaterial design. Uncertainty-based AL was also employed for diffusion, reaction-diffusion, and electromagnetic scattering (Pestourie et al., 2023). In multi-fidelity AL, the optimal spatial resolution of the simulation is chosen (Li et al., 2020a; 2021; Wu et al., 2023b). For instance, Li et al. (2023) use an ensemble of FNOs in the single prediction setting. Wu et al. (2023c) apply AL to stochastic simulations using a spatio-temporal neural process. Bajracharya et al. (2024) investigate AL to predict the stationary solution of a diffusion problem. They consider AL using two different uncertainty estimation techniques and selecting based on the diversity in the input space. Pickering et al. (2022) use AL to find extreme events using ensembles of DeepONets (Lu et al., 2019). Next to using AL, it is also possible to reduce the data generation time using Krylov Subspace Recycling (Wang et al., 2023) or by applying data augmentation techniques such as Lie-point symmetries (Brandstetter et al., 2022). Such symmetries could also be combined with AL using LADA (Kim et al., 2021).\nIn recent years, several benchmarks for neural PDE solvers have been published (Takamoto et al., 2022; Gupta & Brandstetter, 2023; Hao et al., 2023; Luo et al., 2023; Liu et al., 2024). For instance, PDEBench (Takamoto et al., 2022) and PDEArena (Gupta & Brandstetter, 2023) provide efficient implementations of numerical solvers for multiple hydrodynamic PDEs such as Advection, Navier-Stokes, as well as recent implementations of neural PDE solvers (e.g., DilResNet, U-Net, FNO) for standard and conditioned PDE solving. Similarly, CODBench (Burark et al., 2023) compares the performance of different neural operators. WaveBench (Liu et al., 2024) is a benchmark specifically"}, {"title": "AL4PDE: AN AL FRAMEWORK FOR NEURAL PDE SOLVERS", "content": "The AL4PDE benchmark consists of three major parts: (1) AL algorithms, (2) surrogate models, and (3) PDEs and the corresponding simulators. It follows a modular design to make the addition of new approaches or problems as easy as possible (Fig. 2). The following sections describe the AL approaches, including the general problem setup, acquisition and similarity functions, and batch selection strategies. Moreover, we describe the included PDEs and surrogate models."}, {"title": "PROBLEM DEFINITION AND SETUP", "content": "AL aims to select the most informative training samples so that the model can reach the same generalization error with fewer calls to the numerical solver. We measure the error using test trajectories on random samples from an input distribution $p_\\tau$. Fig. 1 shows the full AL cycle. Since it requires retraining the NN(s) after each round, we use batch AL with sufficiently large batches. Specifically, in each round, a batch of simulator inputs $S_{\\text{batch}} = {\\psi_1,\\dots, \\psi_{N_{\\text{batch}} } }$ is selected. It is then passed to the numerical solver, which computes the output trajectories using numerical approximation schemes. The new trajectories are then added to the training set, and the cycle is repeated.\nWe implement pool-based active learning methods, which select from a set of possible inputs\n$S_{\\text{pool}} = {\\psi_1,\\dots, \\psi_{N_{\\text{pool}} } }$ called \u201cpool\u201d. The selected batch is then removed from the pool, simulated, and added to the training set $S_{\\text{train}}$:\n$S_{\\text{pool}} \\gets S_{\\text{pool}} \\setminus S_{\\text{batch}}, \\quad S_{\\text{train}} \\gets S_{\\text{train}} \\cup \\text{solve}(S_{\\text{batch}}).$ (4)\nWe sample the pool set randomly from a proposal distribution $\\pi$. In our experiments, we sample pool and test set from the same input distribution $\\pi = p_\\tau$, although $p_\\tau$ might not always be known in practice. Following common practice, the initial batch is selected randomly. Besides pool-based methods, our framework is also compatible with query-synthesis AL methods that are not restricted to a finite pool set. Several principles are useful for the design of AL methods (Wu, 2018): First, they should select highly informative samples that allow the model to reduce its uncertainty. Second, selecting inputs that are representative of the test input distribution at test time is often desirable. Third, the batch should be diverse, i.e., the individual samples should provide non-redundant information. The last point is particular to the batch setting, which is essential to maintain acceptable runtimes. In the following, we will investigate batch AL methods that first extract latent features or direct uncertainty estimates from the neural surrogate model for each sample in the pool and subsequently apply a selection method to construct the batch."}, {"title": "UNCERTAINTIES AND FEATURES", "content": "Since neural PDE solvers provide high-dimensional autoregressive rollouts without direct uncertainty predictions, many AL methods cannot be applied straightforwardly. In the AL4PDE framework, we"}, {"title": "BATCH SELECTION STRATEGIES", "content": "Given uncertainties or features, we need to define a method to select a batch of pool samples. As a generic baseline, we compare to the selection of a (uniformly) random sampling of the inputs according to the input distribution, $\\psi \\sim p_\\tau(\\psi)$.\nUncertainty-based selection strategies. When given a single-sample acquisition function $a$ such as the ensemble uncertainty, a simple and common approach to selecting a batch of $k$ samples is Top-K, taking the $k$ most uncertain samples. However, this does not ensure that the selected batch is diverse. To improve diversity, Kirsch et al. (2023) proposed stochastic batch active learning (SBAL). SBAL samples inputs $\\psi$ from the remaining pool set $S_{\\text{pool}}$ without replacement according to the probability distribution $P_{\\text{power}}(\\psi) \\propto a(\\psi)^m$, where $m$ is a hyperparameter controlling the sharpness of the distribution. Random sampling corresponds to $m = 0$ and Top-K to $m = \\infty$. The advantage of SBAL is that it selects samples from input regions that are not from the highest mode of the uncertainty distribution and encourages diversity.\nFeature-based selection strategies. In the simpler version of their Core-Set algorithm, Sener & Savarese (2018) iteratively select the input from the remaining pool with the highest distance to"}, {"title": "PDES", "content": "We consider 1D and 2D parametric PDEs, all with periodic boundary conditions. The first 1D PDE is the Burgers' equation from PDEBench (Takamoto et al., 2022) with kinematic viscosity $v$: $\\partial_t u + u \\partial_x u = (v / \\pi) \\partial_{xx} u$. Secondly, the Kuramoto-Sivashinsky (KS) equation,\n$\\partial_t u + u \\partial_{xx} u + \\partial_{xxxx} u + v \\partial_{xxxx} u = 0$, from Lippe et al. (2023) demonstrates diverse dynamical\nbehaviors, from fixed points and periodic limit cycles to chaos (Hyman & Nicolaenko, 1986). Next to\nthe viscosity $v$, the domain length $L$ is also varied. Thirdly, to test a multiphysics problem with more\nparameters, we include the so-called combined equation (CE) from Brandstetter et al. (2021) where\nwe set the forcing term $d = 0$: $\\partial_t u + \\partial_x (\\alpha u^2 - \\beta \\partial_x u + \\gamma \\partial_{xx} u) = 0$.\nDepending on the value of\nthe PDE coefficients $(\\alpha, \\beta, \\gamma)$, this equation recovers the Heat, Burgers, or the Korteweg-de-Vries\nPDE. For 2D, we use the compressible Navier-Stokes (CNS) equations from PDEBench (Takamoto\net al., 2022), $\\partial_t \\rho + \\nabla \\cdot (\\rho v) = 0$, (11a)\n$\\rho(\\partial_t v + v \\cdot \\nabla v) = - \\nabla p + \\eta \\triangle v + (\\zeta + \\frac{ \\eta}{3} ) \\nabla (\\nabla \\cdot v)$, (11b)\n$\\partial_t (\\epsilon + \\rho v^2 / 2 ) + \\nabla \\cdot [ (p + \\epsilon + \\rho v^2 / 2) v - v \\cdot \\sigma'] = 0$, (11c)\nwhere $\\sigma'$ is the viscous tensor. The equation has four channels (density $\\rho$, velocity x-component $v_x$\nand y-component $v_y$ as well as the pressure $p$. The spatial domain is set to $x \\in [0, 1] \\times [0, 1]$ We\nuse the JAX simulator and IC generator from PDEBench (Takamoto et al., 2022) for CNS equations.\nThe PDE parameters are drawn in logarithmic scale as in Eq. (7). The IC generator for the pressure,\ndensity, and velocity channels is also based on the superposition of sinusoidal functions. However,\nthe velocity channels are renormalized so that the IC has a given input Mach number. Secondly, we\nconstrain the density channel to be positive by\n$u_\\rho = \\rho_0 (1 + \\Delta_\\rho u_\\rho / \\max_{x} (|u(x)|) )$ (12)\nwhere $\\rho_0$ is sampled from $[0.1, 10)$ and $\\Delta_\\rho$ from $[0.013, 0.26)$. The pressure channel $p$ is similarly\ntransformed using $\\Delta_p \\in [0.04, 0.8)$. The offset $\\tau_0$ is defined relatively to $\\rho_0$ as $\\tau_0 = T_0 \\rho_0$ with\n$T_0 \\in [0.1, 10)$. The compressibility is reduced using a Helmholtz-decomposition (Takamoto et al.,\n2022). A windowing is applied with a probability of 50% to a channel."}, {"title": "NEURAL SURROGATE MODELS", "content": "Currently, the benchmark includes the following neural PDE solvers: (i) a recent version of U-Net (Ronneberger et al., 2015) from Gupta & Brandstetter (2023), (ii) SineNet (Zhang et al., 2024), which is an enhancement of the U-Net model that corrects the feature misalignment issue in the residual connections of modern U-Nets and can be considered a model with state of the art accuracy, specifically for advection-type equations, and (iii) the Fourier neural operator (FNO, Li et al., 2020b)."}, {"title": "SELECTION OF EXPERIMENTS", "content": "We investigate (i) the impact of AL methods on the average error, (ii) the error distribution, (iii) the variance and reusability of the generated data, (iv) the temporal advantage of AL, and (v) conduct an ablation study concerning the different design choices of SBAL and LCMD. We use a smaller version of the modern U-Net from Gupta & Brandstetter (2023). We train the model on sub-trajectories (two steps) to strike a balance between learning auto-regressive rollouts and fast training. The training is performed with a cosine schedule, which reduces the learning rate from $10^{-3}$ to $10^{-5}$. The batch size is set to 512 (CNS: 64). We use an exponential data schedule, i.e., in each AL iteration, the amount of data added is equal to the current training set size (Kirsch et al., 2023). For 1D equations, we start with 256 trajectories (2D: 128). The pool size is fixed to 100,000 candidates. For Burgers, we choose the parameter space $v \\in [0.001, 1)$ and sample values uniformly at random but on a logarithmic scale. For the KS equation, besides the viscosity $v \\in [0.5, 4)$, we vary the domain length $L \\in [0.1, 100)$ as"}, {"title": "CONCLUSION", "content": "This paper introduces AL4PDE, an extensible framework to develop and evaluate AL algorithms for neural PDE solvers. AL4PDE includes four PDEs, surrogate models including U-Net, FNO, and SineNet, and AL algorithms such as SBAL and LCMD. An initial study shows that existing AL algorithms can already be advantageous for neural PDE solvers and can allow a model to reach the same accuracy with up to four times fewer data points. Thus, our work shows the potential of AL for making neural PDE solvers more data-efficient and reliable for future application cases. However, the experiments also showed that stable model training can be difficult depending on the base architecture (CNS). AL is especially impacted by such issues, since the model is trained repeatedly with different data sets and the data selection relies on the model. Hence, more work on the reliability of the surrogate model training is necessary. Another general open issue of AL is the question of how to select hyperparameters which work sufficiently well on the growing, unseen datasets during AL. To be closer to realistic engineering applications, future work should also consider more complex geometries and non-periodic boundary conditions, as well as irregular grids. AL could be especially helpful in such settings due to the inherently more complex input space to select from."}]}