{"title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "authors": ["Mengru Wang", "Yunzhi Yao", "Ziwen Xu", "Shuofei Qiao", "Shumin Deng", "Peng Wang", "Xiang Chen", "Jia-Chen Gu", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.", "sections": [{"title": "1 Introduction", "content": "Knowledge is the cornerstone of intelligence and the continuation of civilization, furnishing us with foundational principles and guidance for navigating complex problems and emerging challenges (Davis et al., 1993; Choi, 2022). Throughout the extensive history of evolution, we have dedicated our lives to cultivating more advanced intelligence by utilizing acquired knowledge and exploring the frontiers of unknown knowledge (McGraw and Harbison-Briggs, 1990; Han et al., 2021).\nAs we know, Large language models (LLMs) are also renowned for encapsulating extensive parametric knowledge (Roberts et al., 2020; Sung et al., 2021; Cao et al., 2021; Zhong et al., 2021; Kandpal et al., 2023; Heinzerling and Inui, 2020; Petroni et al., 2019; Qiao et al., 2023; Kritharoula et al., 2023; He et al., 2024a), achieving unprecedented progress in application. However, the knowledge mechanisms in LLMs for learning, storage, utilization, and evolution still remain mysterious (Gouldet al., 2023a). Extensive works aim to demystify various types of knowledge in LLMs through knowledge neurons (Dai et al., 2022; Chen et al., 2024a) and circuits (Elhage et al., 2021; Yao et al., 2024; Zou et al., 2024), yet these efforts, scattered across various tasks, await comprehensive review and analysis.\nAs shown in Fig 1, this paper pioneeringly reviews the mechanism across the whole knowledge life cycle. We also propose a novel taxonomy for knowledge mechanisms in LLMs, as illustrated in Fig 2, which encompasses knowledge utilization at a specific time and knowledge evolution across all periods of LLMs 1. Specifically, we first introduce preliminaries of this field (\u00a72) and review knowledge utilization mechanism from a new perspective (\u00a73). Then, we delve into the fundamental principles for knowledge evolution (\u00a74), discuss challenges of knowledge utilization, and posit some promising hypotheses to explore potential avenues for developing powerful and trustworthy"}, {"title": "2 Preliminary", "content": "2.1 Knowledge Scope\nKnowledge is an awareness of facts, a form of familiarity, awareness, understanding, or acquaintance (Zagzebski, 2017; Hyman, 1999; Mahowald et al., 2023; Gray et al., 2024). It often involves the possession of information learned through experience and can be understood as a cognitive success or an epistemic contact with reality. We denote a diverse array of knowledge as set K, wherein each element k \u2208 K is a specific piece of knowledge, which can be expressed by various records, e.g., a text record \"The president of the United States in 2024 is Biden\" (denoted as rk).\n2.2 Definition of Knowledge in LLMs\nGiven a LLM denoted as F, we formulate that F master knowledge k if F can correctly answer the corresponding question rk\\t:\nF(r_{k\\t}) = t\\\\\nt \\in T \\Rightarrow F \\text{ masters knowledge } k,\nF is a LLM, rk\\t is a record about knowledge k that lacks pivot information t. Take an example for illustration: rk\\t is \u201cThe president of the United States in 2024 is __\u201d, t is \u201cBiden\u201d. Besides, rk\\t can be represented by the above textual statement, captured through a question-answering pair (\u201cWho is the President of the United States in 2024?\u201d, or conveyed by audio, video, image 2, and other equivalent expressions. The correct answer for r'k\\t can be expressed by various fomarts, which are formulated as T = {\u201cBiden\u201d, \u201cJoe Biden\u201d, \u2026\u2026\u2026 }. t is an element from T.\n2.3 The Architecture of LLMs\nAn LLM F consists of numerous neurons, which work systematically under a specific architecture.\nTransformer-based architecture. The prevailing architecture in current LLMs is the Transformer. Specifically, a transformer-based LLM F begins with token embedding, followed by L layers transformer block, and ends with token unembedding"}, {"title": "2.4 Knowledge Analysis Methods", "content": "Knowledge analysis method M aims to reveal precise causal mechanisms from inputs to outputs (Bereska and Gavves, 2024). Furthermore, if components C of F accurately infer t through analysis method M, it is assumed that the knowledge k is presented by C:\nt \\leftarrow M_{C\\subseteq F} (r_{k\\t}, C), \n(3)\nt \\in T \\Rightarrow C \\text{ represents knowledge } k,\nThe elements in set C may be individual neurons, MLPs, attention heads, a transformer block layer,"}, {"title": "3 Knowledge Utilization in LLMs", "content": "According to Bloom's Taxonomy of cognition levels (Bloom et al., 1956; Keene et al., 2010; Fadul, 2009), we categorize knowledge representation and utilization within LLMs into three levels (as shown in Fig 3): memorization, comprehension and application, and creation. Note that these mechanistic analyses are implemented via methods in \u00a72.4. We further evaluate the applicability, advantages, and limitations of different methods in \u00a73.4.\n3.1 Memorization\nKnowledge memorization aims to remember and recall knowledge, e.g., specific terms (entities), grammar, facts, commonsense and concepts (Allen-Zhu"}, {"title": "3.2 Comprehension and Application", "content": "Knowledge comprehension and application focus on demonstrating the understanding of memorized knowledge and then solving problems in new situations, e.g., reasoning (Hou et al., 2023) and planning (McGrath et al., 2021). Merrill et al. (2023) denote the transition from memorization to comprehension and application as grokking, and suggest that the grokking derives from two largely distinct subnetworks competition. Intuitively, only knowledge that is correctly memorized (Prashanth et al., 2024) in \u00a73.1 can be further applied to solving complex tasks. Therefore, we posit Reuse Hypothesis from two knowledge memorization perspectives."}, {"title": "3.3 Creation", "content": "Knowledge creation (Runco and Jaeger, 2012; Sternberg, 2006; Gaut, 2010) emphasizes the capacity and process of forming novel and valuable things, rather than the existing ones (i.e., LLMs have seen) discussed in \u00a73.1 and \u00a73.2. The creations encompass two levels: 1) LLMs create new terms following the current world's principles comprehended by LLMs, such as new proteins (Shin et al., 2021; Madani et al., 2023), molecules (Bagal et al., 2022; Fang et al., 2023; Edwards et al., 2022), code (DeLorenzo et al., 2024), video (Kondratyuk et al., 2023), models (Zheng et al., 2024), names for people and companies, written stories (G\u00f3mez-Rodr\u00edguez and Williams, 2023; Buz et al., 2024), synthetic data (Stenger et al., 2024; Mumuni et al., 2024; Abufadda and Mansour, 2021), etc. These novel items operate according to the existing rules, e.g., law of conservation of energy, reasoning logic (Wang et al., 2024a), or principles of probability theory. 2) LLMs may generate new rules, such as mathematical theorems, and the resulting terms will operate according to the new rules. We posit that the knowledge creation of LLMs may derive from the Extrapolation Hypothesis."}, {"title": "3.4 Comparison of Different Mechanism Analysis Methods", "content": "The above four Hypotheses are achieved by Observation-based and Intervention-based methods. These two methods are typically combined to trace knowledge in LLMs (Mossing et al., 2024; Ghandeharioun et al., 2024). Most knowledge analysis methods are architecture-agnostic and can be adapted to various models.\nEach method is suitable for different scenarios. Specifically, the Modular Region Hypothesis can be analyzed using either Observation-based or Intervention-based methods. In contrast, the Connection Hypothesis, which examines inter-regional connectivity, generally necessitates Intervention-based methods. However, the results of knowledge mechanism analysis depend heavily on different methods and are sensitive to evaluation metrics and implementation details. Hence, Huang et al. (2024b) propose a dataset, RAVEL, to quantify the comparisons between a variety of existing interpretability methods. They suggest that methods with supervision are better than methods with unsupervised featurizers. Later, Zhang and Nanda (2023) further systematically examine the impact of methodological details in intervention-based methods. For corrupted run, they recommend Symmetric Token Replacement (e.g., \"The Eiffel Tower\"\u2192\"The Colosseum\") (Sharma et al., 2024; Wang et al., 2023c; Vig et al., 2020) instead of Gaussian Noising (Meng et al., 2022), which disrupts the model's internal mechanisms. Besides, they advocate using the logit lens over probes for evaluation metric E due to its fine-grained control over localization outcomes."}, {"title": "4 Knowledge Evolution in LLMs", "content": "Knowledge in LLMs should evolve with changes in the external environment. We introduce the Dynamic Intelligence Hypothesis for knowledge evolution in individuals and groups.\n4.1 Individual Evolution\nImmersed in a dynamic world, individuals mature through an iterative process of memorization, forgetting, error correction, and deepening understanding of the world around them. Similarly, LLMs dynamically encapsulate knowledge into parameters through the process of conflict and integration.\nIn the pre-training phase, LLMs start as blank slates, facilitating easier acquisition for new knowledge. Consequently, numerous experiments demonstrate that LLMs accumulate vast amounts of knowledge during this stage (Cao et al., 2024b; Zhou et al., 2023a; Kaddour et al., 2023; Naveed et al., 2023; Singhal et al., 2022). Aky\u00fcrek et al. (2022) delve further into identifying which training examples are instrumental in endowing LLMs with specific knowledge. However, contradictions during the pre-training stage may induce conflicts among internal parametric knowledge. On the one hand, the false and contradictory information in training corpus propagate and contaminate related memories in LLMs via semantic diffusion, introducing broader detrimental effects beyond direct impacts (Bian et al., 2023). On the other hand, LLMs tend to prioritize memorizing more frequent and challenging facts, which can result in subsequent facts overwriting prior memorization, significantly hindering the memorization of low-frequency facts (Lu et al., 2024). In other words, LLMs struggle with balancing and integrating both low and high-frequency knowledge.\nAfter pre-training, LLMs are anticipated to refresh their internal knowledge to keep pace with the evolving world during post-training stage. Although LLMs seem to absorb new knowledge through continued learning, follow user instructions via instruct tuning (Zhang et al., 2023c), and align with human values through alignment tuning (Ziegler et al., 2019), Ji et al. (2024a) have noted that LLMs intrinsically resist alignment during the"}, {"title": "4.2 Group Evolution", "content": "Besides individual learning, social interaction plays a pivotal role in the acquisition of new knowledge and is a key driver of human societal development (Baucal et al., 2014). LLMs, also known as agents, collaborate to accomplish complex tasks during group evolution, each bearing unique knowledge"}, {"title": "4.3 Comparison of Different Evolution Strategies", "content": "Individuals and groups achieve dynamic intelligence primarily through two strategies: updating internal parametric knowledge (Zhou et al., 2023a; Qiao et al., 2024) and leveraging external knowledge 4 (Huang and Huang, 2024; Xie et al., 2024). These two strategies are usually used together in applications (Yang et al., 2024b).\nUpdating internal parametric knowledge necessitates high-quality data for parameter adjustments (Vashishtha et al., 2024). Data proves pivotal when fine-tuning models to acquire new knowledge. Ovadia et al. (2024) also posit that the continued training of LLMs via unsupervised tuning generally exhibits suboptimal performance when it comes to acquiring new knowledge. Note that updating internal parametric knowledge requires resolving conflicts among internal parameters. The crux of effective internal knowledge updating lies in preserving the consistency of the model's parameter knowledge before and after tuning. In contrast, leveraging external knowledge requires managing conflicts within the external knowledge itself 5 as well as conflicts between external and internal knowledge (Xu et al., 2024b; Liu et al., 2024a). Besides, parametric knowledge compresses extensive information, promoting grokking and enhancing generalization (Wang et al., 2024a). In contrast, leveraging external knowledge avoids high training costs but necessitates substantial maintenance and retrieval costs for every user query. Therefore, the combination of these two strategies is promising. An attempt for combination (Yang et al., 2024b) suggests employing RAG for low-frequency knowledge and parametric strategy for high-frequency knowledge."}, {"title": "5 Discussion", "content": "5.1 What Knowledge Have LLMs Learned?\nCritics question whether LLMs truly have knowledge or if they are merely mimicking (Schwarzschild et al., 2024), akin to the \u201cStochastic Parro\" (Bender et al., 2021) and \"Clever Hans\" (Shapira et al., 2024). We first review the doubts from the following three levels through observation phenomena: 1) Memorization: LLMs primarily rely on positional information over semantic understanding (Li et al., 2022) to predict answers. Additionally, LLMs may generate different answers for the same question due to different expressions. 2) Comprehension and application: Allen-Zhu and Li (2023b) argue that LLMs hardly efficiently apply knowledge from pre-training data, even when such knowledge is perfectly stored and fully extracted from LLMs. Therefore, LLMs struggle with diverse reasoning tasks (Nezhurina et al., 2024; Guti\u00e9rrez et al., 2024) as well as the reverse curse (Berglund et al., 2023). Besides, LLMs are not yet able to reliably act as text world simulators and encounter difficulties with planning (Wang et al., 2024d). 3) Creation: Although LLMs are capable of generating new terms, their quality often falls below that created by humans (Raiola, 2023). Even though LLMs possess knowledge, some critics argue that current analysis methods may only explain low-level co-occurrence patterns, not internal mechanisms. The primary criticism asserts that the components responsible for certain types of knowledge in LLM fail to perform effectively in practical applications (Hase et al., 2023). In addition, the components responsible for specific knowledge within LLMs vary under different methods. For these criticisms, Chen et al. (2024f,d) propose degenerate neurons and posit that different degenerate components indeed independently express a fact. Chen et al. (2024e) delineate the differences in the mechanisms of knowledge storage and representation, proposing the Query Localization Assumption to response these controversies. Zhu and Li (2023) further observe that knowledge may be memorized but not extracted due to the knowledge not being sufficiently augmented (e.g., through paraphrasing, sentence shuffling) during pretraining. Hence, rewriting the training data to provide knowledge augmentation and incorporating more instruction fine-tuning data in the pre-training stage can effectively alleviate the above challenges and criticisms.\nDespite considerable criticism, the mainstream view is that current LLMs may possess basic world knowledge but hardly master principle knowledge of underlying rules for reasoning and creativity. In other words, LLMs master knowledge memorization (discussed in \u00a73.1). Although LLMs possess the foundational ability to compre-"}, {"title": "5.2 Improper Learning or Misuse?", "content": "The knowledge learned by LLMs is fragile, leading to issues of hallucination, knowledge conflicts, and secure risk 6. Hallucination denotes content generated by LLMs that diverges from real-world facts or inputs (Huang et al., 2023b; Xu et al., 2024d; Farquhar et al., 2024; Chen et al., 2024c). On the one hand, factuality hallucination underscores the disparity between generated content and real-world knowledge. On the other hand, faithfulness hallucination describes the departure of generated content from user instructions or input context, as well as the coherence maintained within the generated content. Knowledge Conflict inherently denotes inconsistencies in knowledge (Xu et al., 2024b; Kortukov et al., 2024). On the one hand, internal memory conflicts within the model cause LLMs to exhibit unpredictable behaviors and generate differing results to inputs which are semantically equivalent but syntactically distinct (Xu et al., 2024b; Wang et al., 2023a; Feng et al., 2023b; Raj et al., 2022). On the other hand, context-memory conflict emerges when external context knowledge contradicts internal parametric knowledge (Jin et al., 2024b; Yao et al., 2024; Hoffmann et al., 2022;"}, {"title": "5.3 Dark Knowledge Hypothesis for LLMS?", "content": "The distribution and quality of data are vital for knowledge acquisition and robust operation within the model (machine). Imagine an ideal scenario where we have access to all kinds of data to train the machine. The data includes all possible modalities, such as text, image, audio, video, etc. Models can also interact with each other and the external environment. In this long-term development, will there still be unknown dark knowledge for intelligence to human or model (machine)?\nWe hypothesize that there will still exist dark knowledge for intelligence in the future. As shown in Fig 4, dark knowledge describes knowledge unknown to human or machine from the following three situations: 1) knowledge unknown to human & known to machine (UH, KM). Machines leverage vast amounts of data to explore internal patterns, whereas humans struggle with processing such data due to physiological limitations on data processing capacity and computational limits (Burns et al., 2023; McAleese et al., 2024). (UH, KM) includes gene prediction, intelligent transportation systems, and more. Specifically, the structural elucidation of proteins remains mysterious to humans for a long time. Cryo-electron microscopy, through capturing millions of images, first reveals the three-dimensional structures of proteins. Now, neural models can directly predict protein properties with high efficiency and accuracy (Pak et al., 2023). 2) knowledge known to human & unknown to machine (UH, KM). On the one hand, some scholars claim that machine can possess a \u201cTheory of Mind\u201d capability (Zhu et al., 2024) and emotions (Normoyle et al., 2024). On the other hand, critics contend that machine lacks sentient and merely probabilistically generates tokens (Wang et al., 2024d). Specifically, some abstract knowledge like hunger, happiness, loneliness, and others cannot be fully represented by discrete data points. The above knowledge akin to NP-hard problems (Hochba, 1997), is also exceedingly complex and nearly impossible to model perfectly with current algorithms. Therefore, opponents argue that no matter how many parameters machine possesses, it cannot learn all the knowledge that human has mastered. 3) knowledge unknown to human & unknown to machine (UH, UM) is beyond our cognition. Generally, Dark knowledge extends beyond current data and model architectures (Tseng et al., 2024). (UH, UM) necessitates human-machine col-"}, {"title": "5.4 How to Benefit from Interdisciplinarity?", "content": "How can LLMs continuously narrow the boundaries of dark knowledge and achieve higher level intelligence by leveraging the human experience of perpetual knowledge exploration throughout history? We draw inspirations from the following interdisciplinary studies.\nNeuroscience studies the structure and function of the brain at molecular, cellular, neural circuit, and neural network levels (Squire et al., 2012). Generally, both mechanism analysis in LLMs and neuroscience utilize observation and intervention methods to investigate the basic principles of knowledge learning and memory, decision-making, language, perception, and consciousness. The biological signals of the human brain and the internal activation signals in LLMs are capable of reciprocal transformation (Caucheteux et al., 2023; Feng et al., 2023a; Mossing et al., 2024; Flesher et al., 2021). Benefiting from advancements in neuroscience (Jamali et al., 2024; de Schotten et al., 2022; Lee et al., 2022a), mechanism analysis in LLMs has identified analogous function neurons and regions (Zhao"}, {"title": "6 Future Directions", "content": "6.1 Parametric VS. Non-Parametric Knowledge\nLLMs can be conceptualized as parametric knowledge stores, where the parameters of the model\u2014typically the weights of the neural network\u2014encode a representation of the world's knowledge. This parametric approach to knowledge storage means that the knowledge is implicitly embedded within the model's architecture, and it can be retrieved and manipulated through the computational processes of the neural network (Allen-Zhu and Li, 2023b). In contrast, non-parametric knowledge storage involves methods where the knowledge is explicitly represented and can be directly accessed. Examples of non-parametric knowledge storage include knowledge graphs, databases, and symbolic reasoning systems, where knowledge is represented as discrete symbols or facts. Parametric knowledge enables LLMs to deeply compress and integrate information (Huang et al., 2024c; Shwartz-Ziv and LeCun, 2024), allowing them to generalize and apply this"}, {"title": "6.2 Embodied Intelligence", "content": "The current LLM still cannot be regarded as a truly intelligent creature (Bender and Koller, 2020; Bisk et al., 2020). The process of human language acquisition is not merely a passive process of listening to language. Instead, it is an active and interactive process that involves engagement with the physical world and communication with other people. To enhance the current LLM's capabilities and transform it into a powerful agent, it is necessary to enable it to learn from multimodal information and interact with the environment and humans.\nMultimodal LLMs. The integration of multiple modalities is a critical challenge in the field of LLMs and embodied AI. While LLMs have demonstrated impressive capabilities when processing language data, their ability to seamlessly incorporate and synthesize information from other modalities such as images, speech, and video is still an area of active research. However, the current multi-modal model faces challenges, particularly in complex reasoning tasks that require understanding and integrating information from both text and images.\nRecent studies (Huang et al., 2024a; Chen et al., 2024b) have highlighted the discrepancy between the model's performance in language tasks and its ability to integrate knowledge from different modalities effectively. These findings suggest that current models often prioritize linguistic information, failing to fully exploit the synergistic potential of multimodal data (Wang et al., 2024c). There are some pioneering efforts in this direction (Pan et al., 2024a), aiming to uncover the mechanisms by which multi-modal models store and retrieve information. Despite these advancements, there is still a need for further exploration to deepen our understanding of multi-modal knowledge storage.\nSelf-evolution. As discussed in the previous part, current language models are mainly based on tuning to gain knowledge, which requires a lot of training and high-quality data. These learnings are passive whereas, to be a human, evolution usually also undergoes communication and interaction. As an intelligent agent, the models should be able to learn through interactions and learn by themselves spontaneously. Recently, some work has attempted to enable the model to learn by themselves (Zhang et al., 2024b) or learn by interaction with the environment (Xu et al., 2024a; Xi et al., 2024). By"}, {"title": "6.3 Domain LLMs", "content": "The success of general-purpose LLMs has indeed inspired the development of domain-specific models that are tailored to particular areas of knowledge, such as biomedicine (Yu et al., 2024a; Moutakanni et al., 2024), finance (Yang et al., 2023), geoscience (Deng et al., 2023), ocean science (Bi et al., 2024), etc. However, unlike human language, the knowledge of these different domains bears specific characteristics. It remains unclear whether LLMs can acquire complex scientific knowledge or if such knowledge still resides within the realm of current dark knowledge. Furthermore, does domain-specific knowledge such as mathematics share the same underlying mechanisms as textual knowledge (Bengio and Malkin, 2024), or does it exhibit more intricate mechanisms of knowledge acquisition? Currently, there is a relative lack of research focusing on the mechanism of these domain-specific knowledge and there is an increasing recognition of the importance of developing a deeper understanding of these mechanisms.\nData sparsity and diversity in domain-specific models pose another challenge. Sparsity is usually caused by confidentiality, privacy, and the cost of acquisition in specialized fields. As for diversity, the presentation of knowledge varies across different fields. For instance, in the biomedical domain, knowledge includes complex biological concepts such as the structure and function of proteins and molecules. This requires models to integrate understanding that extends beyond natural language, often involving graphical representations like chemical structures, which cannot be directly expressed in text. Similarly, in fields such as finance and law (Lai et al., 2023), models must engage in sophisticated reasoning and decision-making processes based on domain-specific knowledge. Hence, the critical tasks of collecting high-quality data for domain-specific models (including synthetic data generation) and effectively embedding domain knowledge into LLMs require immediate attention."}, {"title": "6.4 Trustworthy LLMs", "content": "Wei et al. (2023) delve into the safety of LLM and reveal that the success of jailbreak is mainly due to the distribution discrepancies between malicious attacks and training data. Geva et al. (2022b) and Wang et al. (2024b) further discover that some parameters within LLMs, called toxic regions, are intrinsically tied to the generation of toxic content. Ji et al. (2024a) even conjecture that LLMs resist alignment. Therefore, traditional aligned methods, DPO (Rafailov et al., 2023) and SFT, seem to merely bypass toxic regions (Lee et al., 2024; Wang et al., 2024b), making them susceptible to other jailbreak attacks (Zhang et al., 2023d).\nInspired by the knowledge mechanism in LLMs, a promising trustworthy strategy may be designing architecture and training process during the pre-training phase to encourage monosemanticity and sparsity, which make the reverse engineering process more tractable (Jermyn et al., 2022; Bricken et al., 2023; Liu et al., 2024b; Zhang et al., 2021; Tamkin et al., 2023). Yet, maintaining sparsity for a vast amount of world knowledge requires substantial resources, and whether monosemantic architecture can support advanced intelligence remains elusive. Besides, machine unlearning (Nguyen et al., 2022; Tian et al., 2024) aims to forget privacy or toxic information learned by LLMs. However, these unlearning methods suffer over-fitting, forgetting something valuable. Another alternative technique is knowledge editing, precisely modifying LLMs using few instances during the post-training stage (Mazzia et al., 2023; Yao et al., 2023; Wang et al., 2023d; Hase et al., 2024; Qian et al., 2024b). Extensive experiments demonstrate that knowledge editing has the potential to detoxify LLMs (Yan et al., 2024). Specifically, (Wu et al., 2023) and Geva et al. (2022b) deactivate the neurons related to privacy information and toxic tokens, respectively. However, knowledge editing also introduces side effects, such as the inability of the modified knowledge to generalize to multi-hop tasks (Zhong et al., 2023; Li et al., 2023c; Cohen et al., 2023; Kong et al., 2024) and the potential to impair the model's general capabilities (Gu et al., 2024). Therefore, recent efforts aim to manipulate intermediate representation of LLMs during inference stage instead of editing parameters of LLMs (Zou et al., 2023; Turner et al., 2023; Zhou et al., 2023b; Zhu et al., 2024). These representations can track and address a wide range of safety-relevant problems, including"}, {"title": "7 Conclusion", "content": "In this paper, we propose a novel knowledge mechanism analysis taxonomy and review knowledge evolution. We further discuss knowledge utilization issues, as well as unexplored dark knowledge. We hope these insights may inspire some promising directions for future research and shed light on more powerful and trustworthy models."}, {"title": "Limitations", "content": "This work has some limitations as follows:\nHypothesis Despite reviewing a large body of literature and proposing several promising hypotheses, there are still some limitations. On the one hand, there may be other hypotheses for knowledge utilization and evolution in LLMs. On the other hand, the accuracy of these hypotheses requires further exploration and validation over time.\nKnowledge There are various forms of knowledge representation. However, due to current research constraints, this paper does not delve into space (Li et al., 2024d), time (Gurnee and Tegmark, 2023), event-based knowledge, and geoscience (Lin et al., 2024).\nReference The field of knowledge mechanisms is developing rapidly and this paper may miss some important references. Additionally, due to the page limit, we have omit certain technical details. We will continue to pay attention to and supplement new works.\nModels Despite mentioning artificial neural models in this paper, knowledge mechanism analysis focuses on LLMs. We will continue to pay attention to other modal models progresses. Besides, all existing work has not considered models larger than 100 billion parameters. Whether the knowledge mechanisms within large-scale models are consistent with smaller ones remains to be studied."}, {"title": "Ethics Statement", "content": "We anticipate no ethical or societal implications arising from our research. However, we acknowledge that the internal mechanisms of large language models might be exploited for malicious purposes. We believe such malicious applications can be prevented through model access and legislative regulation. More critically, a transparent model contributes to the development of safer and more reliable general artificial intelligence."}, {"title": "A Comparison of Methods for Knowledge Evolution", "content": "Note that due to the page limit, \u00a74 does not provide a detailed enumeration of various techniques and details, such as machine unlearning and knowledge augmentation. Hence, we briefly outline common methods during post-training stage in this section and illustrate their associations and differences (Zhang et al., 2024a) in Fig 5.\n\u2022 Continual Learning aims to continually acquire new skills and learn new tasks while retaining previously acquired knowledge.\n\u2022 Parameter-efficient Fine-tuning (PET) (Zhang et al., 2019) only updates a minimal set of parameters instead of full fine-tuning. A promising strategy is LoRA (Hu et al., 2022).\n\u2022 Knowledge Augmentation is proposed to assist the model in handling unknown knowledge for LLMs (Zhang et al., 2019; Han et al., 2022). RAG (Huang and Huang, 2024) is the is the most prevalent methods. Beside, knowledge augmentation also includes prompt engineering (Gu et al., 2023; Kraljic and Lahav, 2024; Liang et al., 2023b) and in-context learning (Luo et al., 2024a).\n\u2022 Machine Unlearning (Nguyen et al., 2022; Tian et al., 2024; Liu et al., 2024c) focuses on discarding undesirable behaviors from LLMs.\n\u2022 Editing, including knowledge editing (Zhang et al., 2024a) and representation editing (Wu et al., 2024), aims to enable quick and precise modifications to the LLMs. Usually, editing first identifies the knowledge location in LLMs and then precisely modifies model behavior through a few instances."}, {"title": "B Universality Intelligence", "content": "To validate the hypotheses in this paper across different architectures, we first introduce other popular model architectures in \u00a7B.1, and observe the generalizability of our hypotheses across other model architectures in \u00a7B.2. Besides, recent work further claims that models trained with different data, modalities, and objectives are converging to shared representation spaces (Huh et al., 2024). Ar-"}, {"title": "B.1 Model Architecture", "content": "B.1.1 Transformer\nMLP The Multilayer Perceptron (MLP) is a crucial component in neural networks, usually comprising multiple fully connected layers. Within the Transformer architecture, the MLP plays a vital role in applying nonlinear transformations to the input hidden states, thereby enriching the model's capacity for expression. More precisely, every MLP block involves two linear transformations separated by a point-wise activation function \u03c3:\nMLP (h^{l}) = \\sigma (W_{up}^{l}h^{l-1}) W_{down}^{l},\n(6)\nwhere \u03c3 is the point-wise activation function, typically a non-linear function such as ReLU or GELU. Wlup is the weight matrix for the first linear transformation in the l-th layer, mapping the input hidden state hl\u22121 to an intermediate representation. Wldown is the weight matrix for the second linear transformation in the l-th layer, transforming the intermediate representation to the output of the MLP block.\nAttention is a mechanism in neural networks, especially in models like Transformers, that captures dependencies between different positions within"}, {"title": "B.1.2 SSM", "content": "Mamba introduced by Gu and Dao (2023), is a recent family of autoregressive language models based on state space models (SSMs). Mamba employs a unique architecture called MambaBlock, which replaces the attention and MLP blocks used in Transformer layers.Specifically, Mamba maps a sequence of tokens x = [x1,x2,...,x\u03c4 ] to a probability distribution over the next token y. Each token xi is first embedded into a hidden state of size d as h(0)i , which is then transformed sequentially by a series of MambaBlocks. The hidden state h(l)i after the l-th MambaBlock is computed as follows:\nh^{(l)}_i = h^{(l-1)}_i + o^{(l)}_i\n(9)\nThe output o(l)i of the l-th MambaBlock for the i-th token is a combination of s(l)i (from Conv and SSM operations) and g(l)i (a gating mechanism):\no^{(l)}_i = MambaBlock([h^{(l-1)}_0, h^{(l-1)}_1,..., h^{(l-1)}_i])\n= W^{(l)} [s^{(l)}_i ; g^{(l)}_i],\n(10)\nHere, \u2299 denotes element-wise multiplication.The calculation of s(l)i is as follows:\ns^{(l)}_i = W^{(l)}h^{(l-1)}_i,\nc^{(l)} = \\text{Conv1D}([a^{(l)}_0,a^{(l)}_1,...,a^{(l)}_i]),\n(12)\nc^{(l)} = \\text{selective-SSM}([c^{(l)}_0, c^{(l)}_1,..., c^{(l)}_i])\n(13)\nThe operations in Equations (12) and (13) correspond to Conv and SSM operations, respectively.The gating mechanism g(l)i is given by:\ng^{(l)}_i = \\text{SiLU} [W^{(l)}h^{(l-1)}_i].\n(14)\nThe formulas and concepts used here are adapted from Sharma et al. (2024).\nCompared to Transformer, Mamba\u2019s design enables more efficient parallel training and effectively captures dependencies in sequences, making it suitable for various natural language processing tasks."}, {"title": "B.1.3 Vision and Multi-modal Models", "content": "In the realm of vision and multi-modal models, various architectures have emerged, each with its unique approach to tackling complex visual tasks. For example, GANs (Generative Adversarial Nets) (Goodfellow et al., 2014) consist of two neural networks: a generator and a discriminator. Through adversarial learning, the generator aims to produce realistic data samples (such as images), while the discriminator attempts to distinguish between real and generated data. Diffusion Model (Li et al., 2023b; Sohl-Dickstein et al., 2015) is a powerful tool for generating high-quality images and data. It simulates a diffusion process by gradually adding and removing noise to achieve data"}, {"title": "B.2 Knowledge Mechanisms in Other Architectures", "content": "Surprisingly, similar mechanisms as those found in transformer-based LLMs have also been discovered in other architectural models. Specifically, Mamba employs the knowledge memorization mechanism similar to Transformer (Sharma et al., 2024). Vision and multi-modal architectures also adopt function region (Modular Region Hypothesis) for knowledge utilization (Pan et al., 2023a; Schwettmann et al., 2023; Koh et al., 2020; Bau et al., 2017), e.g., multi-modal neuron regions are responsible for multi-modal tasks. Besides, the connections hypothesis between neurons is found in vision architecture models (Olah et al., 2020). Olah et al. (2020) further suggest that different types of knowledge reuse partial components, e.g., cars and cats reuse the same neurons (Reuse Hypothesis). As for the Dynamic Intelligence Hypothesis, it inherently focuses on entire artificial neural models."}, {"title": "B.3 Machine and Human", "content": "Analogous to the Hominidae Family in biological taxonomy, artificial neural models can be regarded as Neural Model Family:\n\u2022 Family: Neural Model, likened to \"Hominidae\".\n\u2022 Genus: Transformer architecture, Mamba architecture, etc., likened to \"Homo\" and \"Pan\".\n\u2022 Species: BERT, GPT, Llama, Mistral, Mamba, etc., likened to \"Sapiens\", \"Pan troglodytes\", 'Pan paniscus\u201d.\nMetaphorically, Llama-7B, Llama-13B, Llama-70B, etc., can be viewed as the infancy, childhood, adulthood of humans. Shah et al. (2024) further find that, regardless of model size, the developmental trajectories of PLMs consistently exhibit a window of maximal alignment with human cognitive development. Therefore, we hypothesize that artificial neural networks (machine) and biological neural networks (human) tend to converge to universality intelligence. In other words, huamn and machine share similar features and circuits.\nSpecifically, extensive evidences demonstrate that machine and human share the same mechanism of knowledge memorization, i.e., modular region and connection (de Schotten et al., 2022). The activations of modern language models can also linearly map onto the brain responses to speech (Caucheteux et al., 2023). Caucheteux et al. (2023) pioneer the explanation via predictive coding theory: while transformer-based LLMs are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. The above phenomenon indicates that machine and human share similar underlying mechanisms of knowledge (Sucholutsky et al., 2023; Chan et al., 2023; Kornblith et al., 2019), irrespective of their specific configurations, process and comprehend information. This could be due to inbuilt inductive biases (S\u00e6b\u00f8 and Brovold, 2024) in neural networks or natural abstractions (Chan et al., 2023) \u2013 concepts favored by the natural world that any cognitive system would naturally gravitate towards (Bereska and Gavves, 2024)."}, {"title": "C Tools for Mechanism Analysis", "content": "Numerous tools exist for interpreting knowledge mechanisms in LLMs. TransformerLens (Nanda and Bloom, 2022) is a library for the mechanistic interpretability using observation and intervention. TransformerLens allows users to cache, remove, or replace internal activations during model running. XMD (Lee et al., 2023a) provides various forms of feedback via an intuitiveness, which enable explanations align with the user feedback. NeuroX (Dalvi et al., 2023) implements various interpretation methods under a unified API then provides interpretability of LLMs. PatchScope (Ghandeharioun et al., 2024) is a tool developed by Google that employs a novel model to elucidate the hidden states in the original model. Transformer Debugger (Mossing et al., 2024), an interpretability tool from OpenAI, utilizes GPT-4 and sparse auto-encoders to explain language neurons. Gao et al. (2024a) leverage sparse auto-encoders to extract interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Transcoders (Dunefsky et al., 2024) decomposes model computations involving MLPs into interpretable circuits."}]}