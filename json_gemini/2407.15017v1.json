{"title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "authors": ["Mengru Wang", "Yunzhi Yao", "Ziwen Xu", "Shuofei Qiao", "Shumin Deng", "Peng Wang", "Xiang Chen", "Jia-Chen Gu", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.", "sections": [{"title": "1 Introduction", "content": "Knowledge is the cornerstone of intelligence and the continuation of civilization, furnishing us with foundational principles and guidance for navigating complex problems and emerging challenges (Davis et al., 1993; Choi, 2022). Throughout the extensive history of evolution, we have dedicated our lives to cultivating more advanced intelligence by utilizing acquired knowledge and exploring the frontiers of unknown knowledge (McGraw and Harbison-Briggs, 1990; Han et al., 2021). As we know, Large language models (LLMs) are also renowned for encapsulating extensive parametric knowledge (Roberts et al., 2020; Sung et al., 2021; Cao et al., 2021; Zhong et al., 2021; Kandpal et al., 2023; Heinzerling and Inui, 2020; Petroni et al., 2019; Qiao et al., 2023; Kritharoula et al., 2023; He et al., 2024a), achieving unprecedented progress in application. However, the knowledge mechanisms in LLMs for learning, storage, utilization, and evolution still remain mysterious (Gould et al., 2023a). Extensive works aim to demystify various types of knowledge in LLMs through knowledge neurons (Dai et al., 2022; Chen et al., 2024a) and circuits (Elhage et al., 2021; Yao et al., 2024; Zou et al., 2024), yet these efforts, scattered across various tasks, await comprehensive review and analysis. As shown in Fig 1, this paper pioneeringly reviews the mechanism across the whole knowledge life cycle. We also propose a novel taxonomy for knowledge mechanisms in LLMs, as illustrated in Fig 2, which encompasses knowledge utilization at a specific time and knowledge evolution across all periods of LLMs 1. Specifically, we first introduce preliminaries of this field (\u00a72) and review knowledge utilization mechanism from a new perspective (\u00a73). Then, we delve into the fundamental principles for knowledge evolution (\u00a74), discuss challenges of knowledge utilization, and posit some promising hypotheses to explore potential avenues for developing powerful and trustworthy models (\u00a75). Finally, we also provide some future directions (\u00a76) and tools for knowledge mechanism analysis (\u00a7C). Our contributions are as follows: \u2022 To the best of our knowledge, we are the first to review knowledge mechanisms in LLMs and provide a novel taxonomy across the entire life. \u2022 We propose a new perspective to analyze knowledge utilization mechanisms from three levels: memorization, comprehension and application, and creation. \u2022 We discuss knowledge evolution in individual and group LLMs, and analyze the inherent conflicts and integration in this process. \u2022 We suspect that the prevalent transformer architecture may impede creativity, data distribution and quantity may contribute to the fragility of parametric knowledge, leading hallucinations and knowledge conflict. Besides, dark knowledge will exist for a long time. Comparison with Existing Surveys Previous interpretability surveys typically aim to investigate various methods for explaining the roles of different components within LLMs from the global and local taxonomy (Ferrando et al., 2024; Zhao et al., 2024a; Luo and Specia, 2024; Murdoch et al., 2019; Bereska and Gavves, 2024; Vilas et al., 2024; Singh et al., 2024). In contrast, this paper focuses on knowledge in LLMs. Hence, our taxonomy, oriented from target knowledge in LLMs, reviews how knowledge is acquired, stored, utilized, and subsequently evolves. Additionally, previous taxonomy mostly explore the explainability during the inference stage (a specific period), while ignoring knowledge acquisition during the pre-training stage and evolution during the post-training stage (R\u00e4uker et al., 2023; Luo et al., 2024b; Apidianaki, 2023; Jiao et al., 2023; R\u00e4uker et al., 2023; Rai et al., 2024). Our taxonomy aims to explore the dynamic evolution across all periods from naivety to sophistication in both individual and group LLMs. In contrast to the most similar survey (Cao et al., 2024a) that introduces knowledge life cycle, our work focuses on the underlying mechanisms at each stage. Generally, this paper may help us to explore and manipulate advanced knowledge in LLMs, examine current limitations through the history of knowledge evolution, and inspire more efficient and trustworthy architecture and learning strategy for future models. Note that most hypotheses in this paper are derived from transformer-based LLMs. We also validate the generalizability of these hypotheses across other architectural models then propose universality intelligence in \u00a7B."}, {"title": "2 Preliminary", "content": "2.1 Knowledge Scope Knowledge is an awareness of facts, a form of familiarity, awareness, understanding, or acquaintance (Zagzebski, 2017; Hyman, 1999; Mahowald et al., 2023; Gray et al., 2024). It often involves the possession of information learned through experience and can be understood as a cognitive success or an epistemic contact with reality. We denote a diverse array of knowledge as set K, wherein each element k \u2208 K is a specific piece of knowledge, which can be expressed by various records, e.g., a text record \"The president of the United States in 2024 is Biden\" (denoted as rk). 2.2 Definition of Knowledge in LLMs Given a LLM denoted as F, we formulate that F master knowledge k if F can correctly answer the corresponding question rk\\t: \\begin{equation} F (r_{k\\t}) = t \\\\ t\\in T \\Rightarrow F \\text{ masters knowledge } k, \\tag{1} \\end{equation} F is a LLM, rk\\t is a record about knowledge k that lacks pivot information t. Take an example for illustration: rk\\t is \u201cThe president of the United States in 2024 is __\u201d, t is \u201cBiden\u201d. Besides, rk\\t can be represented by the above textual statement, captured through a question-answering pair (\u201cWho is the President of the United States in 2024?\u201d, or conveyed by audio, video, image 2, and other equivalent expressions. The correct answer for r'k\\t can be expressed by various fomarts, which are formulated as T = {\u201cBiden\u201d, \u201cJoe Biden\u201d, \u2026\u2026\u2026 }. t is an element from T. 2.3 The Architecture of LLMs An LLM F consists of numerous neurons, which work systematically under a specific architecture. Transformer-based architecture. The prevailing architecture in current LLMs is the Transformer. Specifically, a transformer-based LLM F begins with token embedding, followed by L layers transformer block, and ends with token unembedding"}, {"title": "2.4 Knowledge Analysis Methods", "content": "Knowledge analysis method M aims to reveal precise causal mechanisms from inputs to outputs (Bereska and Gavves, 2024). Furthermore, if components C of F accurately infer t through analysis method M, it is assumed that the knowledge k is presented by C: \\begin{equation} t \\leftarrow M_{C \\subset F} (r_{k\\t}, C), \\tag{3} \\\\ t\\in T \\Rightarrow C \\text{ represents knowledge } k, \\end{equation} The elements in set C may be individual neurons, MLPs, attention heads, a transformer block layer, or knowledge circuit (Yao et al., 2024). Inspired by Levels of Analysis (Yurdusev, 1993), these methods can be classified into two main categories: observation and intervention (Bereska and Gavves, 2024). Observation-based methods. These methods aim to observe the internal information of F, directly projecting the output of component C into human-understandable forms by E: \\begin{equation} t \\leftarrow E_{C \\subseteq F} (r_{k\\t}, C, F), \\tag{4} \\end{equation} E is a evaluation metric, which can be a probe, logit lens, or a sparse representation. Probe is a meticulously trained classifier, and its classification performance is used to observe the relationship between model's behavior and the output of C (R\u00e4uker et al., 2023; Belinkov, 2022; Elazar et al., 2021; McGrath et al., 2021; Gurnee et al., 2023). Logit lens usually translate output of C into vocabulary tokens via token unembedding (nostalgebraist, 2020; Geva et al., 2022b; Belrose et al., 2023; Pal et al., 2023; Din et al., 2024; Langedijk et al., 2023). Sparse representation disentangles the output of C into basic features via sparse autoencoder (Gao et al., 2024b; Sharkey et al., 2022; Cunningham et al., 2023; Lee et al., 2006) or sparse dictionary learning (He et al., 2024b; Olshausen and Field, 1997; Yun et al., 2021). Each basic feature represents an interpretable unit, and the output of C is the combination (Elhage et al., 2022; Bricken et al., 2023) of these basic features. Intervention-based methods. These methods allow for direct corruptions in LLMs to identify the critical C via intervention strategies I. Note that C, encompassing various neuron combinations, correlates with specific model behaviors: \\begin{equation} C\\leftarrow I (r_{k\\t}, F), r \\in R_{k}, \\\\ t\\leftarrow E (r_{k\\t}, C, F) \\tag{5} \\end{equation} I is also known as causal mediation analysis (Vig et al., 2020), causal tracing (Meng et al., 2022), interchange interventions (Geiger et al., 2022), activation patching (Wang et al., 2023c; Zhang and Nanda, 2023), and causal scrubbing techniques. For intervention-based methods, E typically refers to token unembedding used for predicting answer tokens. Under evaluation metric of E, there exists a causal relationship between C and specific behavior of LLMs F. Generally, I consists of the following three steps. 1) Clean run: perform an input with F and obtain the answer. 2) Corrupted run: corrupt the performing process by noisy input or activations of F (Meng et al., 2022; Goldowsky-Dill et al., 2023; Stolfo et al., 2023; Yao et al., 2024; Conmy et al., 2023; Mossing et al., 2024; Lepori et al., 2023; Huang et al., 2023a). 3) Restoration run: recover the correct answer using C obtained from the above corrupted run (Meng et al., 2022; Vig et al., 2020; Wang et al., 2023c; Zhang et al., 2017; Nanda, 2023)."}, {"title": "3 Knowledge Utilization in LLMs", "content": "According to Bloom's Taxonomy of cognition levels (Bloom et al., 1956; Keene et al., 2010; Fadul, 2009), we categorize knowledge representation and utilization within LLMs into three levels (as shown in Fig 3): memorization, comprehension and application, and creation. Note that these mechanistic analyses are implemented via methods in \u00a72.4. We further evaluate the applicability, advantages, and limitations of different methods in \u00a73.4. 3.1 Memorization Knowledge memorization aims to remember and recall knowledge, e.g., specific terms (entities), grammar, facts, commonsense and concepts (Allen-Zhu and Li, 2023a; Yu et al., 2023a; Mahowald et al., 2023; Zhu and Li, 2023; Allen-Zhu and Li, 2023b, 2024; Cao et al., 2024a). We posit knowledge memorization from Modular Region and Connection Hypothesis by reviewing existing research. Hypothesis 1: Modular Region Knowledge is Encoded in Modular Regions. Similar to the function regions of human brain (Zhao et al., 2023a), this modular region hypothesis simplifies knowledge representation in transformer-based models into isolated modular region, e.g., MLPs or attention heads. Knowledge is encoded via MLPs. Geva et al. (2021) posit that MLPs operate as key-value memories and each individual key vector corresponds to a specific semantic pattern or grammar. Based on the above finding, Geva et al. (2022b,a) reverse engineer the operation of the MLPs layers and find that MLPs can promote both semantic (e.g., measurement semantic including kg, percent, spread, total, yards, pounds, and hours) and syntactic (e.g., adverbs syntactic including largely, rapidly, effectively, previously, and normally) concepts in the vocabulary space. Miller and Neo (2024) find a single MLP neuron (in GPT-2 Large) capable of generating \u201can\u201d or \u201ca\u201d. Subsequently, fact (Dai et al., 2022; Meng et al., 2022) and commonsense knowledge (Gupta et al., 2023) are found. Advanced language-specific neurons (Tang et al., 2024), linguistic regions (Zhao et al., 2023a), entropy neurons (Stolfo et al., 2024), abstract conceptual (Wang et al., 2024e) and unsafe (Wang et al., 2024b; Wu et al., 2023) knowledge, are also observed in MLPs. In addition to MLP, knowledge is also conveyed by attention heads (Geva et al., 2023; Gould et al., 2023b). Hoover et al. (2020) explain the knowledge each attention head has learned. Specifically, attention heads store evident linguistic features, positional information, and so on. Besides, fact knowledge (Yu et al., 2023b; Li et al., 2023a) and bias (Hoover et al., 2020) are mainly convey by attention heads. Jiang et al. (2024b) further observe that LLMs leverage self-attention to gather information through certain tokens in the contexts, which serve as clues, and use the value matrix for associative memory. Later, Zhu et al. (2024) also find that attention heads can simulate mental state and activate \"Theory of Mind\u201d (ToM) capability. However, Hypothesis 1 ignores the connections between different regions. Inspired by advancements in neuroscience (de Schotten et al., 2022), Hypothesis 2 asserts that the connection of different components integrates knowledge, rather than the isolated regions in Hypothesis 1. Hypothesis 2: Connection Knowledge is Represented by Connections. Geva et al. (2023) outline the encoding of factual knowledge (e.g., \"The capital of Ireland is Dublin\") through the following three steps: (1) subject (Ireland) information enrichment in MLPs, (2) the relation (capital of) propagates to the last token, (3) object (Dublin) is extracted by attention heads in later layers. This claim is supported by Li et al. (2024c). Similarly, Lv et al. (2024) conclude that task-specific attention head may move the topic entity to the final position of the residual stream, while MLPs conduct relation function. Moreover, the recent prominent knowledge circuit framework (Nainani, 2024; Yao et al., 2024; He et al., 2024b; Elhage et al., 2021; Marks et al., 2024) advocates leveraging a critical computational subgraph among all components to explore internal knowledge within LLM parameters. The competencies for indirect object identification and color object tasks are discovered to be embedded in specialized knowledge circuits (Conmy et al., 2023; Wang et al., 2023c; Merullo et al., 2023a; Yu et al., 2024c). Lan et al. (2024) also identify number-related circuits that encode the predictive ability of Arabic numerals, number words, and months. More importantly, experimental evidence demonstrates that various types of knowledge, including linguistic, commonsense, factual, and biased information, are encapsulated in specific knowledge circuits (Yao et al., 2024). Interestingly, knowledge encoded by specific circuits can rival or even surpass that of the entire LLM. This may be because knowledge circuits memorized the relevant knowledge, while noise from other components might impede the model's performance on these tasks.\""}, {"title": "3.2 Comprehension and Application", "content": "Knowledge comprehension and application focus on demonstrating the understanding of memorized knowledge and then solving problems in new situations, e.g., reasoning (Hou et al., 2023) and planning (McGrath et al., 2021). Merrill et al. (2023) denote the transition from memorization to comprehension and application as grokking, and suggest that the grokking derives from two largely distinct subnetworks competition. Intuitively, only knowledge that is correctly memorized (Prashanth et al., 2024) in \u00a73.1 can be further applied to solving complex tasks. Therefore, we posit Reuse Hypothesis from two knowledge memorization perspectives. Hypothesis 3: Reuse LLMs Reuse Certain Components during Knowledge Comprehension and Application. From the Modular Region Perspective, knowledge utilization reuses some regions. These regions might include a few neurons, attention heads, MLPs, or partial knowledge circuits. Generally, basic knowledge (position information, n-gram pattern, syntactic features) tends to be stored at earlier layers, while sophisticated knowledge (mental state, emotion, and abstract concept, e.g., prime number, Camelidae, and safety) is located at later layers (Zhu et al., 2024; Jin et al., 2024a; Wang et al., 2024b,e; Men et al., 2024; Kobayashi et al., 2023). Therefore, neurons of earlier layers related to basic knowledge tend to be reused (Kang and Choi, 2023; Zhao et al., 2024a; Kandpal et al., 2023). Various math reasoning tasks also utilize the attention mechanism in initial layers to map input information to the final token positions, subsequently generating answers using a set of MLPs in later layers (Stolfo et al., 2023; Hanna et al., 2023; Langedijk et al., 2023). Besides, some specific function regions are also reused. Specifically, retrieval heads (Li et al., 2023a) are reused for Chain-of-Thought (CoT) reasoning and long-context tasks. These retrieval heads are found in 4 model families, 6 model scales, and 3 types of fine-tuning. Subsequently, induction heads, identified in Llama and GPT, are claimed to be responsible for in-context learning (ICL) tasks Olsson et al. (2022); Crosbie and Shutova (2024). Moreover, Lv et al. (2024) find that an attention head can map country names to capital cities. Li et al. (2023a) demonstrate that some attention heads (in Alpaca, Llama, and Vicuna) exhibit the capability for truthful answers instead of hallucinations. Tang et al. (2024) find language-specific neurons (in Llama and BLOOM) that process multiple languages, such as English, French, Mandarin, and others. Zhao et al. (2023a) further reveal linguistic regions (in Llama) correspond to linguistic competence, which is the cornerstone for performing various tasks. From the Connection Perspective, knowledge utilization shares partial knowledge circuits. For instance, similar tasks share subgraphs (computational circuits) with analogous roles (Lan et al., 2024). Besides, knowledge circuits (in GPT2) are reused to solve a seemingly different task, e.g., indirect object identification and colored objects tasks (Merullo et al., 2023a). Wang et al. (2024a) further observe that two-hop composition reasoning tasks reuse the knowledge circuits from the first hop. Yao et al. (2024) also believe that this reuse phenomenon exists in factual recall and multi-hop reasoning. Specifically, sub-circuits are reused in similar factual knowledge, such as tasks related to \u201ccity_in_country\", \"name_birth_place\u201d, and \u201ccountry_language\u201d. Besides, Dutta et al. (2024) demystify LLMs how to perform CoT reasoning, i.e., Llama facilitates CoT tasks via multiple parallel circuits enjoying significant intersection."}, {"title": "3.3 Creation", "content": "Knowledge creation (Runco and Jaeger, 2012; Sternberg, 2006; Gaut, 2010) emphasizes the capacity and process of forming novel and valuable things, rather than the existing ones (i.e., LLMs have seen) discussed in \u00a73.1 and \u00a73.2. The creations encompass two levels: 1) LLMs create new terms following the current world's principles comprehended by LLMs, such as new proteins (Shin et al., 2021; Madani et al., 2023), molecules (Bagal et al., 2022; Fang et al., 2023; Edwards et al., 2022), code (DeLorenzo et al., 2024), video (Kondratyuk et al., 2023), models (Zheng et al., 2024), names for people and companies, written stories (G\u00f3mez-Rodr\u00edguez and Williams, 2023; Buz et al., 2024), synthetic data (Stenger et al., 2024; Mumuni et al., 2024; Abufadda and Mansour, 2021), etc. These novel items operate according to the existing rules, e.g., law of conservation of energy, reasoning logic (Wang et al., 2024a), or principles of probability theory. 2) LLMs may generate new rules, such as mathematical theorems, and the resulting terms will operate according to the new rules. We posit that the knowledge creation of LLMs may derive from the Extrapolation Hypothesis. Hypothesis 4: Extrapolation LLMs May Create Knowledge via Extrapolation. The expression of knowledge is diverse; some knowledge is inherently continuous. Therefore, it is difficult, if not impossible, to represent certain knowledge using discrete data points (Spivey and Michael, 2007; Penrose; Markman, 2013). LLMs utilize insights into the operational principles of the world to extrapolate additional knowledge from known discrete points, bridging gaps in knowledge and expanding our understanding of the world(Heilman et al., 2003; Douglas et al., 2024; Park et al., 2023b; Kondratyuk et al., 2023). Drawing inspiration from research on human creativity (Haase and Hanel, 2023), the physical implementation of knowledge extrapolation relies on the plasticity of neurons (Mukherjee and Chang, 2024). Specifically, plasticity refers to LLMs changing activations and connectivity between neurons according to the input (Coronel-Oliveros et al., 2024). However, from a statistical perspective, the intricate connections and activations between neurons, though not infinite, resist exhaustive enumeration. In terms of value, not all creations are valuable. Obtaining something valuable with an exceedingly low probability is impractical, as even a monkey could theoretically print Shakespeare's works. How do LLMs ensure the probability of generating valuable creations? What are the mechanisms underlying the novelty and value of creation? A prevalent conjecture posits that novelty is generated through the random walk (S\u00e6b\u00f8 and Brovold, 2024). However, intuitively, current LLMs themselves seem unable to evaluate the value of creations due to architectural limitations (Chakrabarty et al., 2024). Because, once the next token is generated, there is no intrinsic mechanism for accepting or rejecting the creations. This hinders the evaluation of the usefulness and value of proposed novelties, as humans do, by bending, blending, or breaking biases (S\u00e6b\u00f8 and Brovold, 2024). Some works assume that each token is indeed valuable and meets long-term expectations. However, the well-known hallucination problem (Xu et al., 2024d) of LLMs refutes this assumption. Besides, the transformer architecture struggles with long context (Li et al., 2024a), despite the existence of many variants for addressing this issue. More importantly, MLPs of Transformer may also work contrary to creativity, i.e., the increased attentions narrow the conditional distribution for token prediction (S\u00e6b\u00f8 and Brovold, 2024)."}, {"title": "3.4 Comparison of Different Mechanism Analysis Methods", "content": "The above four Hypotheses are achieved by Observation-based and Intervention-based methods. These two methods are typically combined to trace knowledge in LLMs (Mossing et al., 2024; Ghandeharioun et al., 2024). Most knowledge analysis methods are architecture-agnostic and can be adapted to various models. Each method is suitable for different scenarios. Specifically, the Modular Region Hypothesis can be analyzed using either Observation-based or Intervention-based methods. In contrast, the Connection Hypothesis, which examines inter-regional connectivity, generally necessitates Intervention-based methods. However, the results of knowledge mechanism analysis depend heavily on different methods and are sensitive to evaluation metrics and implementation details. Hence, Huang et al. (2024b) propose a dataset, RAVEL, to quantify the comparisons between a variety of existing interpretability methods. They suggest that methods with supervision are better than methods with unsupervised featurizers. Later, Zhang and Nanda (2023) further systematically examine the impact of methodological details in intervention-based methods. For corrupted run, they recommend Symmetric Token Replacement (e.g., \"The Eiffel Tower\"\u2192\"The Colosseum\") (Sharma et al., 2024; Wang et al., 2023c; Vig et al., 2020) instead of Gaussian Noising (Meng et al., 2022), which disrupts the model's internal mechanisms. Besides, they advocate using the logit lens over probes for evaluation metric E due to its fine-grained control over localization outcomes."}, {"title": "4 Knowledge Evolution in LLMs", "content": "Knowledge in LLMs should evolve with changes in the external environment. We introduce the Dynamic Intelligence Hypothesis for knowledge evolution in individuals and groups. Hypothesis 5: Dynamic Intelligence Conflict and Integration Coexist in the Dynamic Knowledge Evolution of LLMs. 4.1 Individual Evolution Immersed in a dynamic world, individuals mature through an iterative process of memorization, forgetting, error correction, and deepening understanding of the world around them. Similarly, LLMs dynamically encapsulate knowledge into parameters through the process of conflict and integration. In the pre-training phase, LLMs start as blank slates, facilitating easier acquisition for new knowledge. Consequently, numerous experiments demonstrate that LLMs accumulate vast amounts of knowledge during this stage (Cao et al., 2024b; Zhou et al., 2023a; Kaddour et al., 2023; Naveed et al., 2023; Singhal et al., 2022). Aky\u00fcrek et al. (2022) delve further into identifying which training examples are instrumental in endowing LLMs with specific knowledge. However, contradictions during the pre-training stage may induce conflicts among internal parametric knowledge. On the one hand, the false and contradictory information in training corpus propagate and contaminate related memories in LLMs via semantic diffusion, introducing broader detrimental effects beyond direct impacts (Bian et al., 2023). On the other hand, LLMs tend to prioritize memorizing more frequent and challenging facts, which can result in subsequent facts overwriting prior memorization, significantly hindering the memorization of low-frequency facts (Lu et al., 2024). In other words, LLMs struggle with balancing and integrating both low and high-frequency knowledge. After pre-training, LLMs are anticipated to refresh their internal knowledge to keep pace with the evolving world during post-training stage. Although LLMs seem to absorb new knowledge through continued learning, follow user instructions via instruct tuning (Zhang et al., 2023c), and align with human values through alignment tuning (Ziegler et al., 2019), Ji et al. (2024a) have noted that LLMs intrinsically resist alignment during the post-training phase. In other words, LLMs tend to learn factual knowledge through pre-training, whereas fine-tuning 3 teaches them to utilize it more efficiently (Gekhman et al., 2024; Zhou et al., 2023a; Ovadia et al., 2024). Ren et al. (2024) also posit that instruction tuning is a form of self-alignment with existing internal knowledge rather than a process of learning new information. We conjecture that the debate on whether these processes truly introduce new knowledge stems from information conflicts. For example, the conflict between outdated information within LLMs and new external knowledge exacerbates their difficulty in learning new information. To mitigate information conflicts, Ni et al. (2023) propose first forgetting old knowledge then learning new knowledge. Another technique, retrieval-augmented generation (RAG) (Huang and Huang, 2024), while avoiding conflicts within internal parameters, still needs to manage conflicts between retrieved external information and LLMs' internal knowledge (Xu et al., 2024b). RAG also attempt to efficiently and effectively integrate new knowledge across passages or documents using multiple retrieval (Yang et al., 2024a) and hippocampal indexing (Guti\u00e9rrez et al., 2024). Besides, editing technologies, including knowledge and representation editing, exhibit promising potential for knowledge addition, modification, and erasure. Specifically, knowledge editing (Zhang et al., 2024a; Wang et al., 2023d; Mazzia et al., 2023) aims to selectively modify model parameters responsible for specific knowledge retention, while representation editing (Zou et al., 2023; Wu et al., 2024) adjusts the model's conceptualization of knowledge to revise the stored knowledge within LLMs. Note that the other strategy for knowledge editing adds external parameters or memory banks for new knowledge while preserving models' parameters. We also provide the comparison of the above methods in \u00a7A for better understanding."}, {"title": "4.2 Group Evolution", "content": "Besides individual learning, social interaction plays a pivotal role in the acquisition of new knowledge and is a key driver of human societal development (Baucal et al., 2014). LLMs, also known as agents, collaborate to accomplish complex tasks during group evolution, each bearing unique knowledge that may sometimes contradict each other. Therefore, contrary to individual evolution, group evolution encounters intensified conflicts, such as conflicts in specialized expertise among agents, competing interests, cultural disparities, moral dilemmas, and others. To achieve consensus and resolve conflicts, agents must first clarify their own and others' goals (beliefs) through internal representations in models (Zhu et al., 2024; Zou et al., 2023). Agents then discuss, debate, and reflect on shared knowledge through various communication methods (Soltoggio et al., 2024), e.g., prompt instructions, task and agent descriptions, parameter signals (activation and gradient), and representations of models. However, conformity of agents, which tends to believe the majority's incorrect answers rather than maintaining their own, hinders conflict resolution during group evolution (Zhang et al., 2023a). Note that the group also struggles with automating moral decision-making when facing moral conflicts. Specifically, agents in the group miss ground truth for moral \"correctness\" and encounter dilemmas due to changes in moral norms over time (Hagendorff and Danks, 2023). Generally, when, what, and how to share knowledge in the communication process to maximize learning efficiency and long-term expectations are still open questions in group evolution. Through debate and collaboration, groups integrate more knowledge and can surpass the cognition of individual units (Liang et al., 2023a; Qian et al., 2023; Qiao et al., 2024; Talebirad and Nadiri, 2023; Zhang et al., 2023a). This derives from the assumption that each individual unit can contribute to and benefit from the collective knowledge (Soltoggio et al., 2024; Xu et al., 2024c). In addition, \u201cWhen a measure becomes a target, it ceases to be a good measure\u201d, which implies that optimizing one objective on a single individual will inevitably harm other optimization objectives to some extent. Hence, it is unrealistic for an individual to learn all knowledge compared to group optimization. Interestingly, LLM groups also follow the collaborative scaling law (Qian et al., 2024a), where normalized solution quality follows a logistic growth pattern as scaling agents. Moreover, some works (Huh et al., 2024; Bereska and Gavves, 2024) propose that knowledge tends to converge into the same representation spaces among the whole artificial neural models group with different data, modalities, and objectives."}, {"title": "4.3 Comparison of Different Evolution Strategies", "content": "Individuals and groups achieve dynamic intelligence primarily through two strategies: updating internal parametric knowledge (Zhou et al., 2023a; Qiao et al., 2024) and leveraging external knowledge 4 (Huang and Huang, 2024; Xie et al., 2024). These two strategies are usually used together in applications (Yang et al., 2024b). Updating internal parametric knowledge necessitates high-quality data for parameter adjustments (Vashishtha et al., 2024). Data proves pivotal when fine-tuning models to acquire new knowledge. Ovadia et al. (2024) also posit that the continued training of LLMs via unsupervised tuning generally exhibits suboptimal performance when it comes to acquiring new knowledge. Note that updating internal parametric knowledge requires resolving conflicts among internal parameters. The crux of effective internal knowledge updating lies in preserving the consistency of the model's parameter knowledge before and after tuning. In contrast, leveraging external knowledge requires managing conflicts within the external knowledge itself 5 as well as conflicts between external and internal knowledge (Xu et al., 2024b; Liu et al., 2024a). Besides, parametric knowledge compresses extensive information, promoting grokking and enhancing generalization (Wang et al., 2024a). In contrast, leveraging external knowledge avoids high training costs but necessitates substantial maintenance and retrieval costs for every user query. Therefore, the combination of these two strategies is promising. An attempt for combination (Yang et al., 2024b) suggests employing RAG for low-frequency knowledge and parametric strategy for high-frequency knowledge."}, {"title": "5 Discussion", "content": "5.1 What Knowledge Have LLMs"}]}