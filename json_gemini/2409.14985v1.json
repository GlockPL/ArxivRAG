{"title": "Sparse-to-Dense LiDAR Point Generation by LiDAR-Camera Fusion for 3D Object Detection", "authors": ["Minseung Lee", "Seokha Moon", "Seung Joon Lee", "Jinkyu Kim"], "abstract": "Accurately detecting objects at long distances remains a critical challenge in 3D object detection when relying solely on LiDAR sensors due to the inherent limitations of data sparsity. To address this issue, we propose the LiDAR-Camera Augmentation Network (LCANet), a novel framework that reconstructs LiDAR point cloud data by fusing 2D image features, which contain rich semantic information, generating additional points to improve detection accuracy. LCANet fuses data from LiDAR sensors and cameras by projecting image features into the 3D space, integrating semantic information into the point cloud data. This fused data is then encoded to produce 3D features that contain both semantic and spatial information, which are further refined to reconstruct final points before bounding box prediction. This fusion effectively compensates for LiDAR's weakness in detecting objects at long distances, which are often represented by sparse points. Additionally, due to the sparsity of many objects in the original dataset, which makes effective supervision for point generation challenging, we employ a point cloud completion network to create a complete point cloud dataset that supervises the generation of dense point clouds in our network. Extensive experiments on the KITTI and Waymo datasets demonstrate that LCANet significantly outperforms existing models, particularly in detecting sparse and distant objects.", "sections": [{"title": "I. INTRODUCTION", "content": "iDAR sensors are among the most widely utilized devices for capturing three-dimensional (3D) representations of real-world environments. However, the 3D point clouds generated by LiDAR sensors often contain sparse point distributions, which can result in insufficient semantic information for object identification. This sparsity arises because the device emits laser pulses in a circular pattern with discrete angular intervals, leading to dense point captures near the sensor and sparse point captures at farther distances. Consequently, this sparsity hinders the accurate recognition of distant objects, degrading performance in 3D detection tasks. Objects in close proximity to the ego vehicle typically have well-formed point clouds, making them easier to identify. In contrast, objects located at greater distances are represented by sparser point clouds, leading to challenges in detection. This suggests that by making sparse points denser and ensuring they retain their original shape, detection performance can be improved, thereby addressing the inherent limitations of LiDAR sensors."}, {"title": "II. RELATED WORK", "content": "3D Object Detection. 3D object detection involves localiz- ing objects within a three-dimensional scene, typically rep- resented as point cloud data. Point-based 3D object detectors [9]-[11] extract features directly from raw points and detect 3D objects based on these features. Grid-based methods address some of the computational challenges by dividing the 3D scene into a grid structure, such as voxels, with [1], [5], [12], [13] being prominent examples of this approach. These methods partition the scene into voxels or pillars, and process the results with 2D or 3D convolution layers. Some works, such as [14], [15], combine both point-based and voxel-based methods. For the sensor fusioning area, early research efforts [16], [17] explored fusing camera and Li- DAR data at an early stage of the model by decorating points with semantic features derived from images. Other studies, such as [18], [19], demonstrate significant improvements by concatenating features extracted from different modalities, rather than decorating points at the input stage. In contrast, more recent approaches [20]\u2013[24] propose fusion methods based on the bird's-eye view (BEV) space, using LSS [25] to project 2D features into BEV space with dense semantic signals and fuse them with 3D features.\nPoint Generation for 3D Object Detection. In raw point- based methods, [2] and [3] incorporate points within regions of interest (RoI) to generate dense point clouds, providing supplementary spatial supervision for 3D object detection. Specifically, [2] employs a point cloud completion network [26] to complete the point cloud. Similarly, [4] generates additional points using a GCN-based [27] point cloud com- pletion network. It utilizes farthest point sampling (FPS) for downsampling within the RoIs and generates multi- resolution feature maps [10] to reconstruct the missing parts of the point cloud, all while preserving the original points' spatial structure. Meanwhile, [4] utilizes RoI-pooled features rather than raw point features within the RoIs. This method first pools multi-scale voxel features of the RoIs from the 3D backbone and subsequently generates additional points that encapsulate more contextual information surrounding the Rols. However, these methods exclusively rely on 3D point clouds, where sparse background points may be misidentified as foreground objects, leading to erroneous completions. In contrast, our proposed method LCANet captures richer se- mantic information by incorporating camera image features, which facilitate the generation of semantically meaningful points."}, {"title": "III. LIDAR-CAMERA AUGMENTATION NETWORK", "content": "A. Semantic Points Encoding (SPE)\nLiDAR-only detectors often generate false positive and false negative proposals in regions with sparse points due to the lack of semantic priors. Inspired by works such as [16], [17], which demonstrated significant improvements in 3D object detection by augmenting points with semantic priors or image features, we project 3D points onto the image and merge the corresponding image features with the 3D point features for joint embedding. First, we extract image fea- tures using a Swin-Transformer [28] and a Feature Pyramid Network (FPN) [29]. The Semantic Points Encoding (SPE) module then maps raw LiDAR points into the image feature space using a calibration matrix. Bilinear interpolation is then employed to extract image features at these projected points and these interpolated features are subsequently concatenated with corresponding points' features, followed by multilayer perceptrons (MLPs) for channel reduction, thereby enriching each point with semantic information. As a result, the seman- tic points can generate more semantically accurate region proposals. Additionally, SPE module is utilized in the final step of point generation to incorporate semantic information into the detection process.\nB. Cross-modal Feature Fusion (CMFF)\nRegion Proposal Generation. Most recent 3D detectors [4], [5], [13]\u2013[15] initialize their backbone networks with SEC-"}, {"title": "Feature Extraction and Fusion", "content": "Initially, region proposals generated by the RPN are subdivided into $G \\times G \\times G$ subvoxels, with the centers of these subvoxels serving as grid points. The voxel features surrounding these grid points are aggregated using the approach outlined in Voxel-RCNN [5]. Specifically, the grid feature for $g_i$ is computed by aggregating the features of sampled voxels ${v_1, v_2, ..., v_K}$ within the vicinity of the grid point $g_i$ through the PointNet++ [10] module:\n$f_{g_i} = \\underset{k=1}{\\text{MaxPool}} \\{ \\text{MLP} (v_k - g_i; f_{v_k} ) \\}$ ,\nwhere $f_{v_k}$ denotes the voxel features corresponding to the voxel $v_k$, and $v_k \u2013 g_i$ represents the relative coordinates from grid point $g_i$ to voxel $v_k$.\nImages contain rich contextual information that is often absent in LiDAR point clouds, making it crucial to capture this information. Each $G \\times G \\times G$ grids are projected onto the image using calibration matrices and image features are bilinear interpolated on theses points to capture local grid image features $f_I^g$. In addition, inspired by methods for generating 2D bounding boxes on the KITTI dataset [7], [19], we project the eight corners of the 3D region proposals onto the image plane using calibration matrices"}, {"title": "C. Rol Points Generation", "content": "We follow the same process of generating additional points [4]. However, previous methods [2]\u2013[4] only extract features from only LiDAR point derived methods, our method can give more semantic information with the SPE module and CMFF network. However, these fused grid features lack interaction between each others. Therefore, Transformer en- coder [30] is used to refine the fused features to capture long-range dependencies between grids of RoIs and can be written as: $f^{Fus} = T(f^{Fus}_{g_i}, d_{g_i})$, where $T$ is the Transformer encoder [30]. Positional encoding is generated with feed-forward network (FFN) and can be formulated as $g_i = \\text{FFN}([g_i - r^0; g_i \u2013 r^1; g_i \u2013 r^2;\u00b7\u00b7\u00b7; g_i \u2013 r^8])$, where $r^0$ is the center and $r^{1,2,...,8}$ are the eight corners of the 3D bounding box. Finally, MLPs generate points' semantic features and offsets from the center of the grid $g_i$: i.e., $[o_i; f_s^i] = \\text{MLP}(f^{Fus}_{g_i})$.\nThe offsets of the points from the grid centers are ge- neated, therefore, actual point's coordinates $p_i = (X_i, Y_i, Z_i)$ are calculated as $g_i + o_i$. In addition, foreground scores $S_i$ for generated points are calculated by MLPs and a sigmoid function $\\sigma$, which can be written as $s_i = \\sigma(\\text{MLP}(f_p^i))$."}, {"title": "D. Detection Head", "content": "To optimize, we minimize $L_{RPG} = L_{score} + L_{offset}$. $L_{offset}$ ensures that the generated points are geometrically closer to the complete points of objects. We use Chamfer distance described as follows:\n$L_{offset} = \\frac{1}{N_p} \\sum_{r=1}^{N_p} [ \\frac{1}{|S_1|} \\sum_{x \\in S_1} \\underset{y \\in S_2}{min} ||x - y||^2 + \\frac{1}{|S_2|} \\sum_{y \\in S_2} \\underset{x \\in S_1}{min} ||x - y||^2]$ (2)\nwhere $N_p$ is the number of positive proposals, and $S_1$ and $S_2$ represent the generated points and the complete object points of the r-th foreground proposal, $r = 1,2,..., N_p$.\nFurther, $L_{score}$ ensures that the generated points reside within the ground truth (GT) bounding boxes. The FPS algo- rithm [10] is used to sample the generated points and reduce computational costs. $L_{score}$ can be described as follows:\n$L_{score} = - \\frac{1}{N_s} \\sum_j ( (1 - s_i) \\log \\epsilon ) $ , (3)\nwhere $N_s$ is the number of sampled points.\nTo effectively capture the local spatial information of the generated points, these points are first transformed into the canonical space. Nonetheless, it is noteworthy that the canon- ical transformation may lead to a loss of global context. To mitigate this, the depth $d_i = \\sqrt{x_i^2 + y_i^2 + z_i^2}$ is incorporated into the feature set [11]. Additionally, the point score $s_i$ is appended to the feature vector to quantify the significance of each point. The MLPs are then applied to extract canonical features from the transformed points, as described by the following equation:\n$f_c^p = \\text{MLP} ([x_i^c, y_i^c, z_i^c, d_i, s_i])$. (4)\nwhere $(x_i^c, y_i^c, z_i^c)$ are the coordinates obtained from the canonical transformation, which is performed based on re- gion proposal prediction. Since generated points may lose detailed semantic features as the network goes deeper, we utilize our SPE module that produces generated image fea- tures $f^p_I$ before feeding them into the point encoder to ensure that the generated points are enriched once more time, providing them with enhanced semantic meaning. Subse- quently, the spatial features $f_c^p$ and semantic features $f^p_I$ are concatenated and processed by the PointNet++ encoder [10] to extract comprehensive features for the Region of Interest (RoI), which are descibed as :\n$f_{roi} = \\text{PointEncoder} ([p_i; f_I^{p_i}; f_c^{p_i}]) $. (5)"}, {"title": "Loss Function", "content": "Concretely, we minimize the following loss function $L_{total}$:\n$L_{total} = L_{RPN} + L_{RCNN} + L_{RPG}$ (6)\nwhere $L_{RPN}$ is the RPN loss, $L_{RCNN}$ is the proposal refine- ment loss, and $L_{RPG}$ is the point generation loss. $L_{RPN}$ and $L_{RCNN}$ are fundamentally similar, predicting Rols' classes and bounding boxes. For bounding box regression, only positive predictions that exceed the intersection-over-union (IoU) threshold participate in loss calculation using the smooth-L1 loss. For class classification, focal loss [31] is used for the RPN, and binary cross-entropy is used for the detection head."}, {"title": "E. Dense Dataset Construction", "content": "Most LiDAR datasets, do not have complete object shapes. Therefore, synthesized point clouds from sources such as ShapeNet [32] are often utilized for point cloud completion in methods like [2] and [3]. [33] offers a complete point cloud dataset for KITTI [7] dataset by manually combining the point sets of the best-matching objects within the dataset, without relying on external sources. However, some objects remain incomplete due to point cloud sparsity that cannot be matched. Instead of making dense and complete dataset manually or rule-based method, we employed a point cloud completion network (PCN) [26] that leverages both LiDAR points and image features to reconstruct a more complete dataset from [33]. For the Waymo [8] dataset, we aggregate points from the same objects across several sequences, as it provides object IDs. Additionally, we aggregate flipped points for vehicles and cyclists, assuming they are symmetric. These augmented objects, with a sufficiently large number of points, are used as a training dataset to supervise the PCN in generating additional points, thereby building a more comprehensive ground truth (GT) database."}, {"title": "IV. EXPERIMENTS", "content": "Datasets. We conducted experiments using the KITTI [7] dataset, which provides 7,481 training and 7,518 test samples for the following three classes: car, pedestrian, and cyclist. Following the conventions [7], the training samples were split into 3,712 training and 3,769 validation samples. Addi- tionally, we utilized the Waymo Open Dataset (WOD) [8], a large-scale autonomous driving dataset consisting of 798 training and 202 validation sequences for the three classes (i.e., vehicle, pedestrian, and cyclist), captured by five surrounding-view cameras.\nImplementation Details. For the KITTI dataset [7], the detection range is set to [0m, 70.4m] along the X-axis, [- 40m, 40m] along the Y-axis, and [-3m, 1m] along the Z- axis, as it provides annotations only for objects visible in the camera images. The voxel size is set to (0.05m, 0.05m, 0.1m). We follow common data augmentation strategies, including random flipping along the X-axis, global scaling with a factor in the range [0.95, 1.05], and global rotations about the Z-axis within the range [\u2212\u03c0/4 , \u03c0/4]. We also adopt ground truth (GT) sampling [13], [34] for multi-modal data, cropping and pasting image bounding boxes corresponding to the sampled objects, where image retains original size. We build our codebase on OpenPCDet [35] and train our method using the Adam optimizer [36] with a one-cycle policy. We trained for 80 epochs with an initial learning rate of 0.01. We used 4 NVIDIA RTX 3090 GPUs for a batch size of 16, and the training time was less than 6 hours.\nFor the Waymo dataset [8], the detection range is set to [-75.2m, 75.2m] along both the X-axis and Y-axis, and [- 2m, 4m] along the Z-axis. The voxel size is set to (0.1m, 0.1m, 0.15m). We employed the same global rotation and scaling augmentation strategies as described for the KITTI dataset, but GT sampling was not used in this case. Other settings are consistent with the KITTI training details, with the number of epochs set to 30 for conventional cases. Due to GPU resource limitations, we used 4% of the training dataset and 20% of the validation dataset, and images are downsampled by 4x. Training was conducted on 4 NVIDIA RTX A6000 GPUs, with a total training time of less than 6 hours."}, {"title": "Evaluation Details", "content": "For KITTI [7] dataset, we evaluated our method using mean average precision (mAP) for 40 recall positions (R40), with IoU thresholds set to 0.7, 0.5, and 0.5 for cars, pedestrians, and cyclists, respectively. For Waymo [8] dataset, mAPH (mean average precision by heading) is also used and our evaluation was conducted for all difficulty level: i.e., Level 1 (L1) and 2 (L2), where L1 refers to ground truth boxes with more than 5 points, and L2 refers to boxes with fewer or equal to 5 points."}, {"title": "A. Bechmark Results", "content": "Our method is based on PG-RCNN [4], and we reproduced the results from the original code implementation (marked by *)"}, {"title": "B. Qualitative Results", "content": "Fig. 6 illustrates the qualitative results on KITTI validation set comparing the baseline [4] with our method. The baseline suffers from false-positive detections due to the lack of dense semantic priors. In contrast, our method incorporates image features, which helps reduce the number of false positives. Additionally, the baseline [4] struggles to detect distant, sparse objects, whereas our model effectively handles long- range detection and captures sparse objects. Only generated points with confidence scores greater than 0.1 are visualized."}, {"title": "C. Ablation Studies", "content": "Effect of Each Component. As shown in Table VI, we evaluated the performance of each module: SPEpre, CMFF, and SPEPost. The results show that utilizing all three modules improves performance across Easy, Moderate, and Hard difficulty levels.\n(A) Excluding CMFF and using only SPEpre and SPEPost results in lower performance, particularly for Moderate and Hard categories, indicating that cross-modal feature fusion is critical for combining image and point cloud data. (B) Adding CMFF boosts performance, especially in the Moderate category, by enriching point generation with image features. (C) The full configuration further improves results, with SPEpost refining points generated by CMFF, particularly benefiting Easy and Hard categories. (D) Our proposed method achieves the highest performance across all categories, especially in the Hard category, confirming the effectiveness of combining SPEpre, CMFF, and SPEPost for semantic enrichment and spatial refinement."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduced the LiDAR-Camera Aug- mentation Network (LCANet), a novel approach to en- hance 3D object detection by generating points through the integration of 2D image features with 3D point clouds. Our method, which includes SPE modules and the CMFF network, addresses the limitations of sparse and incomplete point clouds\u2014common challenges in LiDAR-based detec- tion\u2014by utilizing image features to generate semantically enriched points. To train our method, we employed a point cloud completion network to construct a complete point cloud dataset, which is used to supervise the generation of dense point clouds for the KITTI and Waymo datasets. Through extensive experiments on these datasets, we demon- strated that our method outperforms existing point gener- ation models, particularly in detecting distant and sparse objects, effectively compensating for LiDAR's limitations in detecting distant objects. We believe that our proposed point generation method will play a crucial role in overcoming the inherent limitations of LiDAR sensors. Future work will focus on further enhancing the generalization capabilities of our method across diverse environments and exploring its potential for real-time applications."}]}