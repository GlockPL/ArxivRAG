{"title": "A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism", "authors": ["Yu Xue", "Chenchen Zhu", "MengChu Zhou", "Mohamed Wahib", "Moncef Gabbouj"], "abstract": "Neural architecture search (NAS) enables researchers to automatically explore vast search spaces and find efficient neural networks. But NAS suffers from a key bottleneck, i.e., numerous architectures need to be evaluated during the search process, which requires a lot of computing resources and time. In order to improve the efficiency of NAS, a series of methods have been proposed to reduce the evaluation time of neural architectures. However, they are not efficient enough and still only focus on the accuracy of architectures. In addition to the classification accuracy, more efficient and smaller network architectures are required in real-world applications. To address the above problems, we propose the SMEM-NAS, a pairwise comparison relation-assisted multi-objective evolutionary algorithm based on a multi-population mechanism. In the SMEM-NAS, a surrogate model is constructed based on pairwise comparison relations to predict the accuracy ranking of architectures, rather than the absolute accuracy. Moreover, two populations cooperate with each other in the search process, i.e., a main population guides the evolution, while a vice population expands the diversity. Our method aims to provide high-performance models that take into account multiple optimization objectives. We conduct a series of experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets to verify its effectiveness. With only a single GPU searching for 0.17 days, competitive architectures can be found by SMEM-NAS which achieves 78.91% accuracy with the MAdds of 570M on the ImageNet. This work makes a significant advance in the important field of NAS.", "sections": [{"title": "I. INTRODUCTION", "content": "ONVOLUTIONAL neural networks (CNNs) have achieved great success in various machine learning tasks [1]\u2013[4]. However, in order to achieve promising per- formance, traditional CNNs are usually designed manually by human experts with extensive domain knowledge and experience. Not every interested user has such expertise, and even for experts, designing CNNs is also a time-consuming and error-prone process. Therefore, in order to facilitate and automate the design of deep convolutional neural networks, Zoph et al. first proposed the concept of neural architec- ture search (NAS) [5] and developed a well-known neural architecture search method for CNNs called NASNet [6], which can obtain neural architectures with highly competitive performance compared to hand-crafted neural architectures. In recent years, researchers have developed many NAS methods, which has attracted increasing attention from both industry and academia in a variety of learning tasks [7], [8], such as object detection [9], semantic segmentation [10], and natural language processing [11].\nIn addition to the accuracy of neural networks, real-world applications also require NAS methods to find computationally efficient network architectures, e.g., low power consumption in mobile applications and low latency in autonomous driv- ing. Generally speaking, the classification accuracy usually continuously increases with the complexity of the network architectures (i.e., the number of layers, the number of chan- nels, etc.) [12]. This implies that maximizing the accuracy and minimizing the network complexity are two competing and conflicting objectives, thus requiring multi-objective opti- mization for NAS. Existing NAS algorithms can be classified into three categories including reinforcement learning (RL) based [6], [13], evolutionary algorithm (EA) based [14], [15] and gradient-based methods [16], [17]. Despite recent advances in RL-based and gradient-based NAS methods, they are still not easily applicable to multi-objective NAS. Gradient- based methods in which the search space is continuous rely on gradient descent to optimize the neural architectures, while other objectives, such as complexity, can not be optimized by the loss function. So, multiple objectives are not easy to be optimized and the diversity of architectures could be missed in gradient-based methods. RL-based methods are more costly and time-consuming than the other two methods. Evolutionary algorithm is a search method based on evolutionary principles that searches for an optimal solution through the evolution and selection [14]. When dealing with multi-objective problems in"}, {"title": "NAS", "content": "field, evolutionary algorithms are more adaptable and flexible.\nRegarding multi-objective NAS, researchers have proposed many methods for finding high-performance architectures that satisfy multiple metrics simultaneously [18]. For example, MnasNet [19] can be used to design a multi-objective neural architecture search approach that optimizes both accuracy and real-world latency on mobile devices. MixNet [20] can be used to design a new mixed depthwise convolution with accu- racy and computation as the optimization metrics. But these methods have lower efficiency in optimizing architectures. In addition to the problem of search efficiency, there is a lack of improvement in the search strategy.\nDuring the search process, there is often a lack of di- versity in the generated architecture, leading to premature convergence to suboptimal solutions. This phenomenon can be attributed to several factors, including the selection operation favouring one particular objective to retain individuals and the inadequate evolutionary operators. These constraints often hinder the exploration of the entire Pareto front, thereby limiting the quality of the obtained architecture [21]. For example, NSGANetV1 [15] tends to search for architectures within a small range around a certain FLOPs value, which may lead to the convergence of the local optimal solution. Therefore, how to maintain the diversity of the population without sacrificing performance is a challenging problem [22], [23]. There exist a number of diversity preservation approaches in existing multi-objective evolutionary algorithms, which usually select diverse architectures from the current generation and put them to the next generation to increase the diversity of the population during the search [21]. Inspired by these methods, which have not been well applied in NAS, we improve the selection operation and design a multi-population mechanism to generate more diverse architectures.\nNevertheless, regardless of the search space and search strat- egy used, one bottleneck in this field is the need to evaluate a large number of network architectures during the search process, which requires huge computational resources and time [24]. Many researchers have proposed lots of methods to improve the efficiency of NAS algorithms, including weight sharing [13], [17], population memory [14], early stopping mechanism [25], and so on. But most of these methods still have to explicitly or implicitly select numerous architectures and then perform training to evaluate them. Recently, some researchers have established surrogate models to virtually eval- uate network architectures, which can execute the environment selection with the predicted accuracy, significantly reducing the evaluation time [26]. However, one of the key challenges in obtaining reliable surrogate models is that obtaining train- ing samples (pairs of architecture and accuracy) is compu- tationally expensive in some extend. In existing methods, sample utilization of surrogate models is not efficient. For example, PRE-NAS [27] trains and evaluates a large number of architectures to be as training samples for the surrogate model. But, these training samples are without special data processing and the quantity is very limited. Moreover, we find that surrogate models are not necessary to produce reliable accuracy estimates (accuracy in the absolute sense), as long as"}, {"title": null, "content": "the predicted results are consistent with the true performance ranking of architectures. In the evolutionary algorithm, the selection operation needs to be performed according to the fitness values of the individuals, and only the individuals with high fitness are selected for subsequent evolution. Therefore, it is more important to obtain the ranking of the candidate architectures than to predict the true accuracy.\nTo address the above problems, we propose an efficient algorithm, called SMEM-NAS, which is a surrogate-assisted multi-objective evolutionary algorithm with a multi-population mechanism for NAS in this work. Firstly, we establish an efficient surrogate model based on pairwise comparison re- lations to obtain the accuracy ranking of candidate architec- tures. Then we conduct practical training on the selected top- ranked architectures. Furthermore, to consider other perfor- mance metrics and increase the diversity of the algorithm, we propose a multi-objective evolutionary algorithm based on a multi-population mechanism. Our approach aims to provide a set of high-performance architectures that take into account multiple optimization objectives. We validate the effectiveness of the proposed method on an image classification task with standard datasets, CIFAR-10, CIFAR-100 and ImageNet. The computational results show that our method outperforms most state-of-the-art NAS methods. This work intends to make the following contributions to advance the field of NAS:\n1) It proposes a method to optimize both the accuracy and network complexity for NAS. We choose the number of multiply-add operations (MAdds) as the second objective. It can facilitate the ENAS to achieve high-quality architec- tures with low computational cost.\n2) A more efficient surrogate model based on pairwise com- parison relations is proposed. Different from existing sur- rogate models that focus on absolute accuracy, ours focuses more on the relative rankings among network architectures. It is further able to shorten the time consumption of the search process compared to previous surrogate models.\n3) A multi-objective evolutionary algorithm based on a multi- population mechanism is proposed, in which a main popu- lation guides the evolution and a vice population assists the evolution. The mechanism can greatly prevent the algorithm from falling into local optima and speed up the convergence of the algorithm.\nThe remainder of this paper is organized as follows: Section II presents the related works and background. Section III describes our proposed method in detail. We present the experimental design and results to verify the effectiveness and efficiency of our method with respect to its peers in Section IV. Finally, the conclusions and future works are drawn in Section V."}, {"title": "II. RELATED WORK", "content": "In recent years, with deep neural networks widely de- ployed in different applications and computing environments, researches and explorations for NAS are particularly attractive. In the following, Section II-A and Section II-B present studies related to multi-objective NAS and surrogate models for NAS. The introduction and usage of the supernet are presented in Section II-C."}, {"title": "A. Multi-objective NAS", "content": "In the early methods of NAS, most studies only focus on the accuracy of neural networks. This often leads to that the searched networks suffer from excessive computation time and model size. On the other hand, the accuracy is rarely the only metric to be considered in real-world applications. It is often necessary to consider other additional and conflicting goals specific to the deployment scenario in the real world, such as model size, model computation, inference latency, and so on. Thus, some researchers have tried to take these secondary objectives into account while searching for architectures with high accuracy. For example, Lu et al. consider both classifica- tion accuracy and model computation, and then use the non- dominated sorting genetic algorithm II (NSGA-II) as the multi- objective optimization method [15], [28]. Besides, Xue et al. propose a multi-objective evolutionary algorithm with a prob- ability stack for NAS, which considers accuracy and time consumption [29]. Wang et al. adopt a multi-objective parti- cle swarm optimization algorithm to simultaneously optimize classification accuracy and FLOPs [30]. Du et al. further refine the multi-objective optimization process in NAS with the ref- erence point-based environment selection [31]. Most of these existing approaches rely on the evaluation of architectures, and are not efficient enough and still time-consuming [32], [33]. There lacks the improvement of multi-objective evolutionary algorithms (MOEAs) in these methods, which tends to choose the existing multi-objective evolutionary algorithms such as NSGA-II [34]. There exists the phenomenon of small model traps during the search process [35]. Specifically, during the early search, smaller models with fewer parameters but higher test accuracy tend to dominate larger models that are less accurate but have the potential for higher accuracy [36]. More- over, unsuitable evolution operators or conflicting objective characteristics may also lead to low diversity of the population and the imbalance between diversity and convergence, which may cause the search results to fall into the local optima [37]."}, {"title": "B. Surrogate Models", "content": "The main computational bottleneck in NAS is the per- formance evaluation of architectures. In recent years, many surrogate-based methods have been proposed to accelerate the evaluation process in evolutionary algorithms [38]. The PNAS proposed by Liu et al. from Google uses long short-term memory (LSTM) as a surrogate model to replace the training of deep neural networks [39]. Afterwards, Luo et al. propose a neural network architecture optimization method, NAO, by using MLP as a surrogate model which works better than PNAS [16]. E2EPP [40] uses an offline surrogate model based on random forest (RF) to predict the performance directly from encodings of architectures. The online surrogate model is further improved in NSGANetV2 [34], which uses the architecture search to guide the construction of the accuracy predictor and significantly reduces the number of training samples. Guo et al. exploit a ranking loss function to learn a predictor model that predicts the relative score of architec- tures [41]. They can directly score and rank the performance of architectures to assist the evaluation process in NAS. However,"}, {"title": null, "content": "accuracy predictors constructed and surrogate models adopted in most existing methods are simple and could not make full use of training samples. Inaccurate accuracy predictions also tend to lead to rank disorder of architecture performance, which affects the environment selection. Recently, Wang et al. propose a neural architecture search method based on parti- cle swarm optimization, which uses support vector machine (SVM) as a surrogate model to conduct the preliminary exploration on the individual comparison relationship [42]. Despite the impressive progress, they search in a specific space constructed by themselves rather than in a general search space. That makes their method difficult to compare with other related methods and the method is not general enough."}, {"title": "C. SuperNet", "content": "To further improve the search efficiency of the proposed algorithm, we choose to adopt the weight sharing technique. At first, we need a supernet that includes all searchable archi- tectures which are sub-networks. In this paper, the supernet we use is OnceForAll [43]. OnceForAll supports variations in four factors: depth, width, convolutional kernel size, and image resolution. In OnceForAll, an overall network is firstly trained by using a large amount of GPU resources, i.e., a maximal network is constructed by taking all the searched hyperparameters of the architectures (depth, width and kernel size) to their maximal values. Then the supernet is trained with the progressive shrinking algorithm [43], in which the largest network is fine-tuned to support sub-networks and they are added to the sampling space by sharing weights. The weights inherited from the trained supernet are used as a warm-up for the gradient descent algorithm during our architecture search."}, {"title": "III. THE PROPOSED METHOD FOR EVOLUTIONARY MULTI-OBJECTIVE NEURAL ARCHITECTURE SEARCH", "content": "This section presents the details of our proposed pairwise comparison relation-assisted multi-objective evolutionary al- gorithm for NAS. We firstly present the framework of the proposed algorithm in Subsection III-A, and the details of the proposed search space and encoding, surrogate model, and multi-objective algorithm with a multi-population mechanism are presented in Subsections III-B to III-D."}, {"title": "A. Overall Framework", "content": "An overview of the proposed algorithm is illustrated in Fig. 1. As shown in Fig. 1, our algorithm generally follows the basic process of genetic algorithm. Compared to existing ENAS methods, it mainly differs in the following two aspects: one is that a surrogate model is constructed to assist in the evaluation. Another is to add a multi-population mechanism into the process of generating new individuals.\nAlgorithm 1 shows the pseudo code of the proposed al- gorithm. Firstly, in order to construct an efficient surrogate model, some evaluated architectures are needed to construct the training dataset. So before the search process starts, an empty archive A to store training samples should be initialized (line 3). N individuals are randomly sampled from the search"}, {"title": null, "content": "space and then decoded to train. As mentioned above, we employ the weight sharing during the training. The weights inherited from the trained supernet are used as a warm up for the stochastic gradient descent (SGD) algorithm to improve the search efficiency (lines 4-9). Afterwards, we enter the main search loop of the algorithm (line 10). The surrogate model is the first to be constructed using the archive A (line 11). A detailed description of the surrogate model is given later in subsection III-C. Then we define a set to store individuals to be evaluated. Next, we use the proposed multi- objective evolutionary algorithm with multi-population mecha- nism (MP-MOEA for short) combine with our surrogate model to optimize both accuracy (Acc) and MAdds, i.e., maximize classification accuracy and minimize model complexity (line 12). This process is described in detail later in subsection III-D. To improve the sample efficiency of our search, we train the surrogate model using an online learning approach. The retained individuals of each generation are decoded and trained, and these retained offspring are used as new training samples to update the surrogate model (line 14-18). The above steps are repeated until the conditions for the end of the procedure are satisfied. Finally, after the multi-objective evolutionary search, the optimal architectures are selected from the pool of architectures based on the non-dominated sorting (line 21)."}, {"title": "B. Search Space and Encoding", "content": "The search process of NAS starts with constructing a search space that can contain most CNNs for image tasks. In this paper, the backbone structure is based on MobileNetV3 [44]. The structure consists of three stages. The start stage extracts features, and the final stage outputs categories. These two parts do not need to be searched. The middle stage needs to be searched and consists of five sequentially connected MBConvBlocks [43] that progressively decrease the feature map size and increase the number of channels. Each block is composed of a series of layers, and every layer adopts the"}, {"title": null, "content": "inverted bottleneck structure [45]: first, a 1 \u00d7 1 convolution, which is used to convert the input channel to the expansion channel; second, a depth-wise convolution, which is the ex- pansion channel and contains the parameter stride; at last, a 1 \u00d7 1 convolution, which is used to convert from the expansion channel to the output channel. In addition, only the first layer"}, {"title": null, "content": "uses stride 2 if the feature map size decreases and all the other layers in the block use stride 1. The number of layers of each convolution block (depth) is selected from {2,3,4}; for each layer, we search the expansion rate of the first 1 \u00d7 1 convolution and the kernel size of the depth-wise separable convolution. The expansion rate is selected from {3,4,6}, and the kernel size is selected from {3,5,7}. Moreover, we also allow the CNNs to obtain multiple input image size (resolution), ranging from 192 to 256 with a stride of 4. For the encoding strategy, we use a fixed 46-bit integer string. If the encoding length of the architecture with fewer layers is less than 46 bits, the fixed length is achieved by padding with zeros.\n inverted bottleneck structure "}, {"title": "C. Surrogate Model", "content": "Unlike most of the existing surrogate-assisted methods used to predict the classification accuracy of architectures, we use the surrogate model to obtain the performance ranking of candidate architectures. In the evolutionary algorithm, the selection operation needs to be carried out according to the fitness values of individuals, and individuals with high fitness will be selected for the subsequent evolution. Therefore, it is more important to obtain the ranking of the candidates than to predict the true accuracy. We propose a surrogate model based on pairwise comparison relations, which is constructed on the basis of a classification model. Its inputs are vectors of the encoding combinations of pairwise architectures, and its outputs are labels indicating which one is the better solution. In this paper, we try to study the comparison relation between two individuals and transform the comparison relation learning problem into a binary classification problem. Binary classification models can be trained better with fewer samples. For example, n(n-1)/2 training samples can be obtained from a dataset containing n labelled samples. Given a small number of samples, the surrogate model with better performance can be obtained through the extended training samples.\nIn this paper, SVM is used as the surrogate model. In order to train the surrogate model, we should firstly construct the training dataset. The construction method is the same as that proposed in EffPNet [42]. The specific process is as follows: firstly, the data used to train needs to be built from the archive A. The n individuals in the archive A are matched in pairs sequentially. In a pair of individuals, if the fitness value of the first individual is better than the other, the class label will be 1. Otherwise, the class label will be 0. Then n(n - 1)/2 training samples can be obtained to feed into the SVM for training. The above data processing transforms the direct prediction of architecture performance into a binary classification task comparing the performance of a pair of architectures. We take a pair of encodings of two architectures as the input of the surrogate model, and the output is label \u201c1\u201d or \u201c0\u201d as described above. To make it more intuitive, an example is provided in Fig. 4 to present the concatenation of the encodings of two architectures, as well as the output. Label \"1\" indicates that architecture  Arc_1  has better performance, while label \u201c0\u201d indicates that architecture  Arc_2  has better performance.\nThrough the above pairwise relations prediction, we can"}, {"title": null, "content": "obtain the performance ranking of all architectures. The details are as follows: assuming that there are n architectures, we set a score for each architecture corresponding to its performance. Firstly, we match the first architecture with the other n 1 architectures and use the trained surrogate model to predict the relationship of these pairs of architectures. For each pair of architectures, if the first architecture outperforms the second one, the score of the first architecture is added by 1, otherwise the score of the second architecture is added by 1. Then, the second architecture is paired one by one with the remaining architectures after removing the first architecture, and the above process is repeated until the last architecture is left. Finally, each architecture is compared n 1 times and the score ranges from 0 to  n  - 1. The number of comparisons in total is n(n - 1)/2. The larger the value of the score, the more times the corresponding architecture wins in the overall comparisons, which implies that the architecture has a superior performance. The results from pairwise relations prediction will be used in the non-dominated sorting for the environment selection during the subsequent evolutionary process."}, {"title": "D. Multi-objective Evolutionary Algorithm Based on Multi- population Mechanism", "content": "In multi-objective evolutionary algorithms, diversity needs to be considered as well as convergence. During the practical evolution process, it is easy to fall into the local optimum without exploring the region of the optimal solution. To over- come this problem, we propose a multi-objective evolution- ary algorithm based on the multi-population mechanism, see Algorithm 2 for details. The multi-population mechanism is mainly used in the selection operation to select parents for the next generation during the evolution process. Firstly, two sub- populations are constructed from the initial population Pt (line 1). The main population is Et, and the vice population is Ft. Specifically, the initial population Pt is non-dominated sorted according to the predicted architectural rankings obtained from the above surrogate model and the calculated MAdds. After that, the non-dominated solutions of the first level are taken as the main population Et (line 2-5). Then, the individuals in the population Et are removed from the population Pt, and the crowding distances of the remaining solutions are calculated. K individuals with high crowding ranking are selected to form the vice population F\u2081 (line 6). We set a threshold based on the number of generations, and when this threshold is less than 9, Both parent p1 and p2 come from the main population Et (line 13). If this threshold is greater than 0, parent P1 comes from the primary population and parent p2 comes from the secondary population (line 15). A number of offspring architectures are generated until the number of offspring is smaller than m in the current generation. m is a number used to limit the number of offspring. The threshold is a value related to the evolution generation, which is generated by the following equation:\n$$Threshold =\\begin{cases}  random(d, 0.7) & g < \\frac{1}{3} G\\\\  random(0,5) & \\frac{1}{3} G \\leq g \\leq \\frac{2}{3} G\\\\ random(d, 1) & g > \\frac{2}{3} G  \\end{cases}$$"}, {"title": null, "content": "where d is a randomly generated number between [0,1], g is the number of generation, and G is the total number of generations.\nIntegers are used to encode the architectures in this paper. The crossover operator uses two-point crossover. Because the traditional polynomial mutation is based on real numbers, we rewrite it to fit the integer encoding. In the early stage of the search, we should focus on the convergence and diversity. It is vital to keep the whole population evolving in the right direction, so the main population should be the dominant one, and the vice population should play an assisting role in the exploration. In the middle stage of the search, more consideration is given to convergence, and the parents should"}, {"title": null, "content": "be the best individuals. So, parents are all selected from the main population Et, which helps to achieve a fast convergence rate. In the later stage, the population evolution tends to be stable, we use the vice population to explore more solution regions and increase the diversity of the population to avoid the algorithm falling into local optima. In the main loop of the algorithm, the main and vice populations will be iteratively updated, and they will collaborate to achieve optimization of the correlation metric with higher accuracy and lower network complexity."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct a series of experiments to val- idate the effectiveness of our proposed method SMEM-NAS. Specifically, we firstly show our experimental configurations in Section IV-A. Next, Section IV-B presents and analyses our experimental results on benchmark datasets. Then we evaluate the effectiveness of the multi-population mechanism in Section IV-C. The comparison results with other algorithms in terms of accuracy and complexity, search cost is also discussed. Finally, Section IV-D show some ablation experiments for surrogate models and the initial population."}, {"title": "A. Experimental Configurations", "content": "Regarding the benchmark datasets used in the experiments, we use CIFAR-10, CIFAR-100 and ImageNet datasets consis- tent with the existing ENAS works. In this study, the process of searching for architectures is performed separately on these three datasets. The performance of the final searched CNN architectures is also evaluated on three datasets. We evaluate the effectiveness of different architectures in terms of both classification accuracy and computational complexity.\nFor each dataset, we start with 100 randomly sampled architectures from the search space as the initial population. Then the evolutionary search is performed, running for a total of 25 iterations. Except for the initial population, for each subsequent iteration, we retain and evaluate eight architectures from generated offspring. They are used for the training of the surrogate model and for the next iteration. So, in the whole search process, we evaluate 300 architectures in total, which are also used to train the surrogate model. Because OnceForAll is based on the ImageNet dataset, we fine-tune the weights inherited from the supernet for five epochs when searching on CIFAR-10 and CIFAR-100. All our experiments are performed on a single Nvidia RTX 3090 GPU card. Finally, we select three promising architectures from the obtained non- dominated solutions and then train them for 200 epochs by the standard SGD optimizer with momentum, where the initial learning rate, the momentum rate and the batch size are set to 0.01, 0.9 and 128."}, {"title": "B. Results on Standard Datasets", "content": "In this section, we show the results of our method on benchmark datasets to validate its effectiveness. We conduct the search and train the final architectures separately on three datasets. This is different from existing NAS methods that"}, {"title": null, "content": "mostly use a transfer learning setup. We think the transferred architectures maybe not optimal on another dataset. Then, in"}, {"title": null, "content": "order to validate the effectiveness of the proposed method, we compare the non-dominated architectures obtained by SMEM-NAS with those obtained by other state-of-the-art NAS methods. The selected peer methods can be broadly divided into three categories: manually designed by human experts, EA-based, and non-EA-based (e.g., RL and GD). In addition, we give the searched results by SMEM-NAS without the multi-population mechanism."}, {"title": null, "content": "Results on CIFAR-10: After completing the search using our method, we selected three promising architectures, and for ease of representation we named them SMEMNAS-S/M/L (sorted by MAdds). Fig. 5 and Table I show the results of SMEM-NAS and the comparison with other methods on CIFAR-10. Fig. 5 visualizes the dominant relationship be- tween the network models in terms of accuracy and model complexity. It is obvious that our models always outperform most models or our models are comparable to them. Firstly, from the perspective of time, our method takes 1.25 GPU days and significantly reduces the search time compared with methods based on EA and RL. The surrogate model we used effectively accelerates the search process. Compared with the"}, {"title": null, "content": "accuracy.\nResults on ImageNet: Different from the conclusion on the CIFAR datasets, there is no need for pre-training before searching on the ImageNet dataset. Table III shows the results of the proposed method on the ImageNet dataset, SMEM-NAS outperforms other methods and achieves the highest accuracy of 78.91% with the MAdds of 570M. In addition, our proposed surrogate model effectively reduces the search time, and only takes 4 hours to complete the search.\nIn summary, the results on the three benchmark datasets validate the effectiveness of the proposed method in terms of the architectural performance and time consumption."}, {"title": "C. Performance of Multi-population Mechanism", "content": "The multi-population mechanism is an important strategy to increase the diversity of the solutions while ensuring convergence in this study. In order to verify its effectiveness, further experiments are conducted on the CIFAR-100 dataset in this section. We separately conduct the architecture search with and without the multi-population mechanism, and the comparison between the two search processes is shown in Fig. 7. The figures show the changes of the pareto front and the distribution of new individuals generated during the evolutionary process. Fig. 7a shows the search process without the multi-population mechanism and Fig. 7b shows the search process using the multi-population mechanism. Fig. 7 shows the distribution of the architectures searched in the archive, which have been realistically evaluated. Specifically, in both figures, the blue dots represent individuals at the initial and early stages of the evolutionary process, and the corresponding blue line is the pareto front at the early stage. In Fig. 7a, it is obvious that except for the randomly initialized individuals, the new individuals generated in the later evolution are all almost distributed in the same solution region, and there is no obvious change in the pareto front. However, in Fig. 7b, we can see that the new individuals generated are uniformly distributed unlike without the multi-population mechanism. The pareto front is also further updated in the excellent direction (lower error rate, smaller model size) and more promising individuals are"}, {"title": null, "content": "explored. Evidently, the multi-population mechanism is better able to ensure the diversity of solutions and prevent them from falling into the local optima. Additionally, we show changes of the pareto front during the search process by using our method in Fig. 8. As we can see that the pareto front has stabilized in the early stage of the search, and is constantly changing and updating with the increase of iterations. In general, that is the same as our expected effect, accelerating the convergence speed while maintaining the diversity."}, {"title": "D. Ablation Study", "content": "Comparison of classification models: In order to construct more accurate surrogate models, we consider four classifica- tion models: SVM, random forest (RF), k-nearest neighbors (KNN), and multi-layer perceptron (MLP). We perform four groups of experiments on the CIFAR-100 dataset. For more intuitive comparisons, we compute the correlation of 300 fully trained and evaluated architectures during the search process with each surrogate model. We choose the Kendall's Tau"}, {"title": null, "content": "coefficient (KTau) to show the correlation between their true rankings and predicted rankings. Fig. 9 shows the searched results using different models and the corresponding KTau. Architectures existing in the archive are used to train surrogate models. After that, new K architectures are retained in each generation and used for testing. The figure shows the results of the last generation. From the KTau and the distribution of individuals in the search process, it can be seen that SVM is the best classifier with very limited training samples. The new individuals generated during the evolution when using KNN and RF as surrogate models are worse and non-uniformly distributed, which may be caused by the inaccurate prediction at the early stage. In addition, the performance of MLP is similar to SVM as shown in the figure. But, SVM can take less time for training and achieve better results than MLP with the limited samples. Therefore, we choose SVM as the surrogate model."}, {"title": null, "content": "Efficiency of the surrogate model: According to our experimental setup (initial population with 100 individuals, evolving for 25 generations), approximately thousands of offspring would be generated during the whole search process. If all these architectures are truly evaluated, it may take more than 10 GPU days. By employing the proposed surrogate model, the search time is significantly reduced.\nCompared to traditional surrogate models, we conduct data augmentation, expanding from n samples to n(n-1)/2, which greatly increases the training data. Increased training data can effectively improve the reliability of the surrogate model. Table IV shows the performance comparison of the proposed surrogate model with the traditional regression surrogate mod- els. We use 300 samples for training models, and in each experiment each surrogate model is trained with the same data. We train three models with the proposed classification-based method (SVM, RF, and KNN), and we train another three models with the traditional regression-based method (MLP, Decision Tree, and AdaBoost)."}, {"title": null, "content": "Hyper-parameters: The initial population is important for both the training of the surrogate model and the subsequent evolution. In order to achieve a better balance between search efficiency and search quality, we test the appropriate value of the initial population size, N, by conducting the following"}, {"title": null, "content": "small-scale experiments on the CIFAR-10. We set different sizes of the initial population"}, {"title": null, "content": "surrogate model has low reliability, the search results are prone to be less stable. Therefore, after comprehensive consideration, we choose N = 100 as the size of the initial population."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose an evolutionary multi-objective algorithm by using a multi-population mechanism and a surro-"}, {"title": null, "content": "gate model based on pairwise comparison relations to perform neural architecture search. The proposed method utilizes a surrogate model to predict accuracy rankings of architectures without focusing on accuracy values. The proposed surrogate model makes full use of the limited training samples and can greatly reduce the search overhead. Besides, a multi- population mechanism is used to improve the diversity in multi-objective NAS, which not only increases the population diversity in the evolution process, but also accelerates the convergence of the algorithm. We have conducted a series of experiments on benchmark datasets to verify the effectiveness of SMEM-NAS. The final architectures found by SMEM-NAS outperform several existing search methods.\nIn this work, our surrogate model only considers the accu- racy and ignores other indicators of networks. Single-objective surrogate models are not effectively adapted to multi-objective search frameworks. Out next work plans to explore the multi- objective surrogate model to predict domination relations of architectures. Besides, it can be observed that the initial pop- ulation is vital to the results of evolutionary algorithms. The proposed method simply uses random sampling to initialise the population, and the sampled architectures are non-uniformly distributed in the objective space, which may not be a proper approach for subsequent evolutionary exploration. Therefore, we intend to investigate efficient sampling methods to initialize the population by considering a given problem\u2019s features."}]}