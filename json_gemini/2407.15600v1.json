{"title": "A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism", "authors": ["Yu Xue", "Chenchen Zhu", "MengChu Zhou", "Mohamed Wahib", "Moncef Gabbouj"], "abstract": "Neural architecture search (NAS) enables researchers to automatically explore vast search spaces and find efficient neural networks. But NAS suffers from a key bottleneck, i.e., numerous architectures need to be evaluated during the search process, which requires a lot of computing resources and time. In order to improve the efficiency of NAS, a series of methods have been proposed to reduce the evaluation time of neural architectures. However, they are not efficient enough and still only focus on the accuracy of architectures. In addition to the classification accuracy, more efficient and smaller network architectures are required in real-world applications. To address the above problems, we propose the SMEM-NAS, a pairwise comparison relation-assisted multi-objective evolutionary algorithm based on a multi-population mechanism. In the SMEM-NAS, a surrogate model is constructed based on pairwise comparison relations to predict the accuracy ranking of architectures, rather than the absolute accuracy. Moreover, two populations cooperate with each other in the search process, i.e., a main population guides the evolution, while a vice population expands the diversity. Our method aims to provide high-performance models that take into account multiple optimization objectives. We conduct a series of experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets to verify its effectiveness. With only a single GPU searching for 0.17 days, competitive architectures can be found by SMEM-NAS which achieves 78.91% accuracy with the MAdds of 570M on the ImageNet. This work makes a significant advance in the important field of NAS.", "sections": [{"title": "I. INTRODUCTION", "content": "CONVOLUTIONAL neural networks (CNNs) have achieved great success in various machine learning tasks [1]-[4]. However, in order to achieve promising per- formance, traditional CNNs are usually designed manually by human experts with extensive domain knowledge and experience. Not every interested user has such expertise, and even for experts, designing CNNs is also a time-consuming and error-prone process. Therefore, in order to facilitate and automate the design of deep convolutional neural networks, Zoph et al. first proposed the concept of neural architec- ture search (NAS) [5] and developed a well-known neural architecture search method for CNNs called NASNet [6], which can obtain neural architectures with highly competitive performance compared to hand-crafted neural architectures. In recent years, researchers have developed many NAS methods, which has attracted increasing attention from both industry and academia in a variety of learning tasks [7], [8], such as object detection [9], semantic segmentation [10], and natural language processing [11].\nIn addition to the accuracy of neural networks, real-world applications also require NAS methods to find computationally efficient network architectures, e.g., low power consumption in mobile applications and low latency in autonomous driv- ing. Generally speaking, the classification accuracy usually continuously increases with the complexity of the network architectures (i.e., the number of layers, the number of chan- nels, etc.) [12]. This implies that maximizing the accuracy and minimizing the network complexity are two competing and conflicting objectives, thus requiring multi-objective opti- mization for NAS. Existing NAS algorithms can be classified into three categories including reinforcement learning (RL) based [6], [13], evolutionary algorithm (EA) based [14], [15] and gradient-based methods [16], [17]. Despite recent advances in RL-based and gradient-based NAS methods, they are still not easily applicable to multi-objective NAS. Gradient- based methods in which the search space is continuous rely on gradient descent to optimize the neural architectures, while other objectives, such as complexity, can not be optimized by the loss function. So, multiple objectives are not easy to be optimized and the diversity of architectures could be missed in gradient-based methods. RL-based methods are more costly and time-consuming than the other two methods. Evolutionary algorithm is a search method based on evolutionary principles that searches for an optimal solution through the evolution and selection [14]. When dealing with multi-objective problems in"}, {"title": "II. RELATED WORK", "content": "In recent years, with deep neural networks widely de- ployed in different applications and computing environments, researches and explorations for NAS are particularly attractive. In the following, Section II-A and Section II-B present studies related to multi-objective NAS and surrogate models for NAS. The introduction and usage of the supernet are presented in Section II-C."}, {"title": "A. Multi-objective NAS", "content": "In the early methods of NAS, most studies only focus on the accuracy of neural networks. This often leads to that the searched networks suffer from excessive computation time and model size. On the other hand, the accuracy is rarely the only metric to be considered in real-world applications. It is often necessary to consider other additional and conflicting goals specific to the deployment scenario in the real world, such as model size, model computation, inference latency, and so on. Thus, some researchers have tried to take these secondary objectives into account while searching for architectures with high accuracy. For example, Lu et al. consider both classifica- tion accuracy and model computation, and then use the non- dominated sorting genetic algorithm II (NSGA-II) as the multi- objective optimization method [15], [28]. Besides, Xue et al. propose a multi-objective evolutionary algorithm with a prob- ability stack for NAS, which considers accuracy and time consumption [29]. Wang et al. adopt a multi-objective parti- cle swarm optimization algorithm to simultaneously optimize classification accuracy and FLOPs [30]. Du et al. further refine the multi-objective optimization process in NAS with the ref- erence point-based environment selection [31]. Most of these existing approaches rely on the evaluation of architectures, and are not efficient enough and still time-consuming [32], [33]. There lacks the improvement of multi-objective evolutionary algorithms (MOEAs) in these methods, which tends to choose the existing multi-objective evolutionary algorithms such as NSGA-II [34]. There exists the phenomenon of small model traps during the search process [35]. Specifically, during the early search, smaller models with fewer parameters but higher test accuracy tend to dominate larger models that are less accurate but have the potential for higher accuracy [36]. More- over, unsuitable evolution operators or conflicting objective characteristics may also lead to low diversity of the population and the imbalance between diversity and convergence, which may cause the search results to fall into the local optima [37]."}, {"title": "B. Surrogate Models", "content": "The main computational bottleneck in NAS is the per- formance evaluation of architectures. In recent years, many surrogate-based methods have been proposed to accelerate the evaluation process in evolutionary algorithms [38]. The PNAS proposed by Liu et al. from Google uses long short-term memory (LSTM) as a surrogate model to replace the training of deep neural networks [39]. Afterwards, Luo et al. propose a neural network architecture optimization method, NAO, by using MLP as a surrogate model which works better than PNAS [16]. E2EPP [40] uses an offline surrogate model based on random forest (RF) to predict the performance directly from encodings of architectures. The online surrogate model is further improved in NSGANetV2 [34], which uses the architecture search to guide the construction of the accuracy predictor and significantly reduces the number of training samples. Guo et al. exploit a ranking loss function to learn a predictor model that predicts the relative score of architec- tures [41]. They can directly score and rank the performance of architectures to assist the evaluation process in NAS. However,"}, {"title": "C. SuperNet", "content": "To further improve the search efficiency of the proposed algorithm, we choose to adopt the weight sharing technique. At first, we need a supernet that includes all searchable archi- tectures which are sub-networks. In this paper, the supernet we use is OnceForAll [43]. OnceForAll supports variations in four factors: depth, width, convolutional kernel size, and image resolution. In OnceForAll, an overall network is firstly trained by using a large amount of GPU resources, i.e., a maximal network is constructed by taking all the searched hyperparameters of the architectures (depth, width and kernel size) to their maximal values. Then the supernet is trained with the progressive shrinking algorithm [43], in which the largest network is fine-tuned to support sub-networks and they are added to the sampling space by sharing weights. The weights inherited from the trained supernet are used as a warm-up for the gradient descent algorithm during our architecture search."}, {"title": "III. THE PROPOSED METHOD FOR EVOLUTIONARY MULTI-OBJECTIVE NEURAL ARCHITECTURE SEARCH", "content": "This section presents the details of our proposed pairwise comparison relation-assisted multi-objective evolutionary al- gorithm for NAS. We firstly present the framework of the proposed algorithm in Subsection III-A, and the details of the proposed search space and encoding, surrogate model, and multi-objective algorithm with a multi-population mechanism are presented in Subsections III-B to III-D."}, {"title": "A. Overall Framework", "content": "An overview of the proposed algorithm is illustrated in Fig. 1. As shown in Fig. 1, our algorithm generally follows the basic process of genetic algorithm. Compared to existing ENAS methods, it mainly differs in the following two aspects: one is that a surrogate model is constructed to assist in the evaluation. Another is to add a multi-population mechanism into the process of generating new individuals.\nAlgorithm 1 shows the pseudo code of the proposed al- gorithm. Firstly, in order to construct an efficient surrogate model, some evaluated architectures are needed to construct the training dataset. So before the search process starts, an empty archive A to store training samples should be initialized (line 3). N individuals are randomly sampled from the search"}, {"title": "B. Search Space and Encoding", "content": "The search process of NAS starts with constructing a search space that can contain most CNNs for image tasks. In this paper, the backbone structure is based on MobileNetV3 [44]. The structure consists of three stages. The start stage extracts features, and the final stage outputs categories. These two parts do not need to be searched. The middle stage needs to be searched and consists of five sequentially connected MBConvBlocks [43] that progressively decrease the feature map size and increase the number of channels. Each block is composed of a series of layers, and every layer adopts the inverted bottleneck structure [45]: first, a 1 \u00d7 1 convolution, which is used to convert the input channel to the expansion channel; second, a depth-wise convolution, which is the ex- pansion channel and contains the parameter stride; at last, a 1 \u00d7 1 convolution, which is used to convert from the expansion channel to the output channel. In addition, only the first layer uses stride 2 if the feature map size decreases and all the other layers in the block use stride 1. The number of layers of each convolution block (depth) is selected from {2,3,4}; for each layer, we search the expansion rate of the first 1 \u00d7 1 convolution and the kernel size of the depth-wise separable convolution. The expansion rate is selected from {3,4,6}, and the kernel size is selected from {3,5,7}. Moreover, we also allow the CNNs to obtain multiple input image size (resolution), ranging from 192 to 256 with a stride of 4. For the encoding strategy, we use a fixed 46-bit integer string. If the encoding length of the architecture with fewer layers is less than 46 bits, the fixed length is achieved by padding with zeros."}, {"title": "C. Surrogate Model", "content": "Unlike most of the existing surrogate-assisted methods used to predict the classification accuracy of architectures, we use the surrogate model to obtain the performance ranking of candidate architectures. In the evolutionary algorithm, the selection operation needs to be carried out according to the fitness values of individuals, and individuals with high fitness will be selected for the subsequent evolution. Therefore, it is more important to obtain the ranking of the candidates than to predict the true accuracy. We propose a surrogate model based on pairwise comparison relations, which is constructed on the basis of a classification model. Its inputs are vectors of the encoding combinations of pairwise architectures, and its outputs are labels indicating which one is the better solution. In this paper, we try to study the comparison relation between two individuals and transform the comparison relation learning problem into a binary classification problem. Binary classification models can be trained better with fewer samples. For example, $n(n-1)/2$ training samples can be obtained from a dataset containing n labelled samples. Given a small number of samples, the surrogate model with better performance can be obtained through the extended training samples.\nIn this paper, SVM is used as the surrogate model. In order to train the surrogate model, we should firstly construct the training dataset. The construction method is the same as that proposed in EffPNet [42]. The specific process is as follows: firstly, the data used to train needs to be built from the archive A. The n individuals in the archive A are matched in pairs sequentially. In a pair of individuals, if the fitness value of the first individual is better than the other, the class label will be 1. Otherwise, the class label will be 0. Then $n(n - 1)/2$ training samples can be obtained to feed into the SVM for training.\nThe above data processing transforms the direct prediction of architecture performance into a binary classification task comparing the performance of a pair of architectures. We take a pair of encodings of two architectures as the input of the surrogate model, and the output is label \u201c1\u201d or \u201c0\u201d as described above."}, {"title": "D. Multi-objective Evolutionary Algorithm Based on Multi-population Mechanism", "content": "In multi-objective evolutionary algorithms, diversity needs to be considered as well as convergence. During the practical evolution process, it is easy to fall into the local optimum without exploring the region of the optimal solution. To over- come this problem, we propose a multi-objective evolution- ary algorithm based on the multi-population mechanism, see Algorithm 2 for details. The multi-population mechanism is mainly used in the selection operation to select parents for the next generation during the evolution process. Firstly, two sub- populations are constructed from the initial population $P_t$ (line 1). The main population is $E_t$, and the vice population is $F_t$. Specifically, the initial population $P_t$ is non-dominated sorted according to the predicted architectural rankings obtained from the above surrogate model and the calculated MAdds. After that, the non-dominated solutions of the first level are taken as the main population $E_t$ (line 2-5). Then, the individuals in the population $E_t$ are removed from the population $P_t$, and the crowding distances of the remaining solutions are calculated. K individuals with high crowding ranking are selected to form the vice population $F_t$ (line 6). We set a threshold based on the number of generations, and when this threshold is less than 9, Both parent $p_1$ and $p_2$ come from the main population $E_t$ (line 13). If this threshold is greater than 0, parent $P_1$ comes from the primary population and parent $p_2$ comes from the secondary population (line 15). A number of offspring architectures are generated until the number of offspring is smaller than m in the current generation. m is a number used to limit the number of offspring. The threshold is a value related to the evolution generation, which is generated by the following equation:\n$$Threshold =\\begin{cases}\nrandom(d, 0.7) & g< \\frac{G}{3} \\\\\nrandom(0,5) & \\frac{G}{3} \\leq g \\leq \\frac{2G}{3}\\\\\nrandom(d, 1) & g > \\frac{2G}{3}\n\\end{cases}$$\nwhere d is a randomly generated number between [0,1], g is the number of generation, and G is the total number of generations."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we conduct a series of experiments to val- idate the effectiveness of our proposed method SMEM-NAS. Specifically, we firstly show our experimental configurations in Section IV-A. Next, Section IV-B presents and analyses our experimental results on benchmark datasets. Then we evaluate the effectiveness of the multi-population mechanism in Section IV-C. The comparison results with other algorithms in terms of accuracy and complexity, search cost is also discussed. Finally, Section IV-D show some ablation experiments for surrogate models and the initial population."}, {"title": "A. Experimental Configurations", "content": "Regarding the benchmark datasets used in the experiments, we use CIFAR-10, CIFAR-100 and ImageNet datasets consis- tent with the existing ENAS works. In this study, the process of searching for architectures is performed separately on these three datasets. The performance of the final searched CNN architectures is also evaluated on three datasets. We evaluate the effectiveness of different architectures in terms of both classification accuracy and computational complexity.\nFor each dataset, we start with 100 randomly sampled architectures from the search space as the initial population. Then the evolutionary search is performed, running for a total of 25 iterations. Except for the initial population, for each subsequent iteration, we retain and evaluate eight architectures from generated offspring. They are used for the training of the surrogate model and for the next iteration. So, in the whole search process, we evaluate 300 architectures in total, which are also used to train the surrogate model. Because OnceForAll is based on the ImageNet dataset, we fine-tune the weights inherited from the supernet for five epochs when searching on CIFAR-10 and CIFAR-100. All our experiments are performed on a single Nvidia RTX 3090 GPU card. Finally, we select three promising architectures from the obtained non- dominated solutions and then train them for 200 epochs by the standard SGD optimizer with momentum, where the initial learning rate, the momentum rate and the batch size are set to 0.01, 0.9 and 128."}, {"title": "B. Results on Standard Datasets", "content": "In this section, we show the results of our method on benchmark datasets to validate its effectiveness. We conduct the search and train the final architectures separately on three datasets. This is different from existing NAS methods that mostly use a transfer learning setup. We think the transferred architectures maybe not optimal on another dataset. Then, in order to validate the effectiveness of the proposed method, we compare the non-dominated architectures obtained by SMEM-NAS with those obtained by other state-of-the-art NAS methods. The selected peer methods can be broadly divided into three categories: manually designed by human experts, EA-based, and non-EA-based (e.g., RL and GD). In addition, we give the searched results by SMEM-NAS without the multi-population mechanism.\nResults on CIFAR-10: After completing the search using our method, we selected three promising architectures, and for ease of representation we named them SMEMNAS-S/M/L (sorted by MAdds). Fig. 5 and Table I show the results of SMEM-NAS and the comparison with other methods on CIFAR-10. Fig. 5 visualizes the dominant relationship be- tween the network models in terms of accuracy and model complexity. It is obvious that our models always outperform most models or our models are comparable to them. Firstly, from the perspective of time, our method takes 1.25 GPU days and significantly reduces the search time compared with methods based on EA and RL. The surrogate model we used effectively accelerates the search process. Compared with the GD-based method, although there are some deficiencies in time, the architecture we searched has better performance. Our model achieves the accuracy of 98.13%, surpassing almost all peer competitors. Among them, our model is slightly lower than FairNAS-A, but has nearly 100M lower MAdds and the search time is much less than FairNAS. In general, SMEM- NAS is superior to or consistent with other models in different metrics.\nResults on CIFAR-100: Similar to the settings on CIFAR- 10, we also conducted experiments on CIFAR-100, and Fig. 6 and Table II show the results of our model compared to networks from other works. We also provide three models on this dataset, large, medium and small, in which the highest accuracy can be 87.95%. From Fig. 6, it is clear that our architectures dominates most methods in terms of accuracy and complexity. Table II shows more details. On this dataset, although our results are slightly inferior to EfficientNet, the hand-crafted model requires significant expertise and labour. Additionally, we can see that the architecture with the highest accuracy searched on the CIFAR-10 dataset transferred to CIFAR-100 does not perform optimally, achieving 87.16% accuracy.\nResults on ImageNet: Different from the conclusion on the CIFAR datasets, there is no need for pre-training before searching on the ImageNet dataset. Table III shows the results of the proposed method on the ImageNet dataset, SMEM-NAS outperforms other methods and achieves the highest accuracy of 78.91% with the MAdds of 570M. In addition, our proposed surrogate model effectively reduces the search time, and only takes 4 hours to complete the search.\nIn summary, the results on the three benchmark datasets validate the effectiveness of the proposed method in terms of the architectural performance and time consumption."}, {"title": "C. Performance of Multi-population Mechanism", "content": "The multi-population mechanism is an important strategy to increase the diversity of the solutions while ensuring convergence in this study. In order to verify its effectiveness, further experiments are conducted on the CIFAR-100 dataset in this section. We separately conduct the architecture search with and without the multi-population mechanism, and the comparison between the two search processes is shown in Fig. 7. The figures show the changes of the pareto front and the distribution of new individuals generated during the evolutionary process. Fig. 7a shows the search process without the multi-population mechanism and Fig. 7b shows the search process using the multi-population mechanism. Fig. 7 shows the distribution of the architectures searched in the archive, which have been realistically evaluated. Specifically, in both figures, the blue dots represent individuals at the initial and early stages of the evolutionary process, and the corresponding blue line is the pareto front at the early stage. In Fig. 7a, it is obvious that except for the randomly initialized individuals, the new individuals generated in the later evolution are all almost distributed in the same solution region, and there is no obvious change in the pareto front. However, in Fig. 7b, we can see that the new individuals generated are uniformly distributed unlike without the multi-population mechanism. The pareto front is also further updated in the excellent direction (lower error rate, smaller model size) and more promising individuals are explored. Evidently, the multi-population mechanism is better able to ensure the diversity of solutions and prevent them from falling into the local optima. Additionally, we show changes of the pareto front during the search process by using our method in Fig. 8. As we can see that the pareto front has stabilized in the early stage of the search, and is constantly changing and updating with the increase of iterations. In general, that is the same as our expected effect, accelerating the convergence speed while maintaining the diversity."}, {"title": "D. Ablation Study", "content": "Comparison of classification models: In order to construct more accurate surrogate models, we consider four classifica- tion models: SVM, random forest (RF), k-nearest neighbors (KNN), and multi-layer perceptron (MLP). We perform four groups of experiments on the CIFAR-100 dataset. For more intuitive comparisons, we compute the correlation of 300 fully trained and evaluated architectures during the search process with each surrogate model. We choose the Kendall's Tau coefficient (KTau) to show the correlation between their true rankings and predicted rankings. Fig. 9 shows the searched results using different models and the corresponding KTau. Architectures existing in the archive are used to train surrogate models. After that, new K architectures are retained in each generation and used for testing. The figure shows the results of the last generation. From the KTau and the distribution of individuals in the search process, it can be seen that SVM is the best classifier with very limited training samples. The new individuals generated during the evolution when using KNN and RF as surrogate models are worse and non-uniformly distributed, which may be caused by the inaccurate prediction at the early stage. In addition, the performance of MLP is similar to SVM as shown in the figure. But, SVM can take less time for training and achieve better results than MLP with the limited samples. Therefore, we choose SVM as the surrogate model.\nEfficiency of the surrogate model: According to our experimental setup (initial population with 100 individuals, evolving for 25 generations), approximately thousands of offspring would be generated during the whole search process. If all these architectures are truly evaluated, it may take more than 10 GPU days. By employing the proposed surrogate model, the search time is significantly reduced.\nCompared to traditional surrogate models, we conduct data augmentation, expanding from n samples to $n(n-1)/2$, which greatly increases the training data. Increased training data can effectively improve the reliability of the surrogate model. Table IV shows the performance comparison of the proposed surrogate model with the traditional regression surrogate mod- els. We use 300 samples for training models, and in each experiment each surrogate model is trained with the same data. We train three models with the proposed classification-based method (SVM, RF, and KNN), and we train another three models with the traditional regression-based method (MLP, Decision Tree, and AdaBoost)."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose an evolutionary multi-objective algorithm by using a multi-population mechanism and a surro- gate model based on pairwise comparison relations to perform neural architecture search. The proposed method utilizes a surrogate model to predict accuracy rankings of architectures without focusing on accuracy values. The proposed surrogate model makes full use of the limited training samples and can greatly reduce the search overhead. Besides, a multi- population mechanism is used to improve the diversity in multi-objective NAS, which not only increases the population diversity in the evolution process, but also accelerates the convergence of the algorithm. We have conducted a series of experiments on benchmark datasets to verify the effectiveness of SMEM-NAS. The final architectures found by SMEM-NAS outperform several existing search methods.\nIn this work, our surrogate model only considers the accu- racy and ignores other indicators of networks. Single-objective surrogate models are not effectively adapted to multi-objective search frameworks. Out next work plans to explore the multi- objective surrogate model to predict domination relations of architectures. Besides, it can be observed that the initial pop- ulation is vital to the results of evolutionary algorithms. The proposed method simply uses random sampling to initialise the population, and the sampled architectures are non-uniformly distributed in the objective space, which may not be a proper approach for subsequent evolutionary exploration. Therefore, we intend to investigate efficient sampling methods to initialize the population by considering a given problem's features."}]}