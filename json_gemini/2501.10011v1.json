{"title": "Mitigating Hallucinations on Object Attributes using Multiview Images and Negative Instructions", "authors": ["Zhijie Tan", "Yuzhi Li", "Shengwei Meng", "Xiang Yuan", "Weiping Li", "Tong Mo", "Bingce Wang", "Xu Chu"], "abstract": "Current popular Large Vision-Language Models (LVLMs) are suffering from Hallucinations on Object Attributes (HOOA), leading to incorrect determination of fine-grained attributes in the input images. Leveraging significant advancements in 3D generation from a single image, this paper proposes a novel method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled from generated 3D representations as visual prompts for LVLMs, thereby providing more visual information from other viewpoints. Furthermore, we observe the input order of multiple multiview images significantly affects the performance of LVLMs. Consequently, we have devised Multiview Image Augmented VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule capable of simultaneously eliminating the influence of input image order and aligning visual information from multiview images with Large Language Models (LLMs). Besides, we designed and employed negative instructions to mitigate LVLMs' bias towards \"Yes\" responses. Comprehensive experiments demonstrate the effectiveness of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "Current popular Large Vision-Language Models (LVLMs) [1]\u2013[6] are suffering from hallucinations [7], [8]. These hallucinations manifest as inconsistencies between the textual responses generated by LVLMs and the semantic content of input images [9]. Specifically, these hallucinations can be categorized into three types [7]: a.) Hallucination on Object Existence (HoOE), wherein errors occur in judgments regarding the presence of objects, such as when non-existent objects are included in the descriptions generated by LVLMs; b.) Hallucination on Object Attributes (HOOA), wherein errors arise in describing the attributes of objects, including shape and color attributes, as exemplified by LVLMs describing a red apple as green; and c.) Hallucination on Object Relationships (HoOR), wherein errors occur in describing relationships between different objects, such as describing a person in front of a sofa as being behind it [7]. Notably, benchmarks designed for assessing LVLMs' hallucinations, such as M-HalDetect [10], MMHal-Bench [11], and AMBER [12], include multiple objects that exhibit issues related to existence, attributes, and relationships simultaneously. Consequently, these three types of problems are strongly coupled in current benchmarks, posing significant challenges for analyzing their individual causes. For instance, in addressing the HOOA, the presence of multiple objects introduces hallucinations related to HoOE and HOOR, thereby complicating the analysis. More specifically, as illustrated in Figure 1, LLaVA-1.5 [13] provides correct answers to questions within the red box above the dashed line. However, in the image below the dashed line, LLaVA-1.5 determines that there is a person wearing glasses and dressed in black. This constitutes a HoOE problem. However, such hallucinations might be caused by the LVLMs failing to correctly understand the \u201cblack\u201d attribute, which corresponds to a HoOA problem. In such complex test scenarios, it is challenging to decouple these different types of hallucinations and address them separately, making it difficult to accurately assess the true capabilities of LVLMs.\nTherefore, it is necessary to design individual benchmarks for each type of hallucination that can exclude interference from other hallucinations. This article demonstrates how to utilize face captioning as a foundational task to design a benchmark for the HoOA problem. Face captioning is a crucial multimodal task widely employed in downstream applications such as facial recognition [14] and text-to-face applications [15]. The CelebAText-HQ [16] dataset, manually annotated with facial attributes, provides detailed descriptions for each face, including shape, color, and other facial attributes. CelebAText-HQ exclusively offers detailed descriptions for individual objects (faces), thereby allowing us to design a benchmark that excludes issues related to HoOE and HOOR, facilitating a more accurate evaluation of the HOOA problem. In constructing this benchmark, we employ common techniques used in standard evaluation metrics, such as POPE [8], CIEM [17], and NOPE [18], to transform the generative task into a discriminative task. Each manually annotated description is converted into a question posed to LVLMs, with occurrences of \u201cYes\u201d responses tallied to calculate accuracy. It is noteworthy that all converted questions yield \u201cYes\u201d answers. However, as mentioned in previous studies [17], [19], there exists a tendency in current LVLMs to favor \u201cYes\u201d responses disproportionately. Consequently, to assess whether LVLMs recognize the attributes of the images, we designed questions for which the answer is \u201cNo\u201d for the same image. For clarity, we term questions with correct \"Yes\" answers as positive questions, and those with \u201cNo\u201d answers as negative questions. Ultimately, we observed a near-opposite performance of LVLMs on positive and negative questions.\nTraining high-quality LVLMs requires addressing both training data and model design aspects. Therefore, we analyze potential causes of HoOA from both data and model perspectives. From the data perspective, the emergence of the HoOA problem can be ascribed to two causes: a.) Insufficient information in single images to enable LVLMs to generate correct responses. b.) Popular LVLMs often undergo instruction tuning with a high proportion of positive visual instructions.\nIn response to cause a.), prior studies have found that introducing richer image descriptions [10] or spatial information [20] can effectively mitigate hallucinations in LVLMs. An intuitive approach is to introduce additional depth maps to significantly improve HOOR problems. Consider the relationship between a person and a sofa: introducing a depth map as additional information can effectively resolve HoOR problems based on the different depths of the sofa and the person. However, when considering the HoOA problem, solely utilizing semantic segmentation or depth maps from the current viewpoint would evidently overlook fine-grained attribute information from other viewpoints. This loss of attribute information leads to two results. Firstly, certain fine-grained details from the current viewpoint may be incomplete. In cases where questions are posed about these potentially incomplete details, the possibility of LVLMs producing hallucinations exists. Secondly, fine-grained attribute information from other viewpoints is almost certainly incomplete. When questions are posed about these inherently incomplete details, LVLMs are highly likely to produce hallucinations. Therefore, generating 3D representations for current objects can effectively mitigate such HoOA problems. Benefiting from the considerable advancements in generating 3D representations from single images, images from other viewpoints can be sampled from the 3D representation [21]\u2013[23]. From the perspective of visual prompt learning [24]\u2013[26], these sampled images can be regarded as visual prompts. These visual prompts provide more visual information for the same attributes, thereby enhancing the robustness of LVLMs' responses. As for cause b.), prior works have introduced additional negative visual instructions during instruction tuning to enhance overall performance [19], [20]. Drawing inspiration from these approaches, we adopt the aforementioned negative questions as negative instructions to teach the LVLMs to respond \u201cNo\u201d to HoOA problems.\nFrom the model perspective, we have observed that cause c): Directly inputting multiple images to LVLMs may lead to HoOA problems. Similar to LLMs' sensitivity to the order of multiple prompts [27]\u2013[29], LVLMs also exhibit sensitivity to the order of multiview images. For causes c), we designed a submodule named Multiview Attributes Perceiver (MAP) and integrated it into a model called Multiview Image Augmented VLM (MIAVLM) to align the multiview images with the LLM and mitigate the impact of the input order.\nIn summary, the contributions of this paper are as follows: 1.To ascertain the presence of the HoOA problem while eliminating interference from HoOE and HoOR problems, we propose the HoOA benchmark. 2. To mitigate the HoOA problem, we propose utilizing multiview images of current objects as visual prompts. Furthermore, we design a novel network module called MIAVLM, integrating a MAP sub-module capable of eliminating the influence of input image order and aligning visual information from multiview images with LLMs. Additionally, we designed and employed negative instructions to mitigate LVLMs' bias towards \"Yes\" responses. 3. To validate the effectiveness of our algorithms, we conducted comprehensive experiments on the HoOA benchmark."}, {"title": "II. METHOD", "content": "We propose Multiview Image Augmented Vision-Vanguage Model (MIAVLM), a model for generating more comprehensive and reliable results from multiple inputs.\nModel Architecture\nThe overview of the MIAVLM model is shown in Figure 2, in which we propose a Multiview Attributes Perceiver (MAP) to bridge the gap between a frozen image encoder and a frozen LLM (Flan-T5-large [30]). Firstly, the input image is processed by a Multiview Generator (HFGI3D [31]) to generate multiview images of the input. Secondly, the image encoder (ViT-L/16 [32]) encodes multiview images into dense embeddings and passes the projected embeddings through the MAP to get the aggregated representation of all the inputs. Finally, the frozen LLM then accepts the output from the MAP and produces the final text outputs. The inner structure of the MAP is shown in Figure 4, including a Visual Extractor and a Multihead Sampler. The details will be further discussed."}, {"title": "B. Visual Extractor", "content": "Following the Vanilla transformer, we design the Visual Extractor as a pile of transformer decoder blocks (6 blocks). In cross attention, the input soft prompts are regarded as the queries, and the image embeddings are regarded as keys and values to inject the visual information into the soft prompts.\nEngaging soft prompts in cross-attention with image embeddings encourages the prompt to interact with vision information and better extract the information for downstream tasks. Since there are multiple input image embeddings, the Visual Extractor performs cross-attention on the soft prompts with each image embedding separately. Formally, we denote the multiple input image embeddings as \\(E = \\{e_1, e_2, ..., e_n\\}\\), where \\(e_i\\) is the i-th input embedding. We denote the soft prompts as \\(P\\) containing \\(l\\) soft tokens, \\(P \\in \\mathbb{R}^{l \\times d}\\), where \\(d\\) is the LLM's model dimension (l=32 and d=1024). Denote the output from the Visual Extractor as \\(O_{VE}\\), the mapping matrix as \\(W_Q\\), \\(W_K\\) and \\(W_V\\), then \\(O_{VE}\\) can be computed as follows:\n\\[O_{VE} = \\{softmax(\\frac{(P W_Q)(e_i W_K)^T}{\\sqrt{d}})e_i W_V | e_i \\in E\\}\\]\n\\[E = \\{e_1, e_2, ..., e_n\\}; W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\]\nIn Equation (1), \\(P\\) denotes the soft prompts and \\(e_i\\) is the image embedding of the i-th input. Noticing that different input has different contributions to the final output, the outputs in \\(O_{VE}\\) are weighted and summed according to the weights computed by the Multihead Sampler. Besides, we compute each cross-attention output in parallel and separately instead of using the previous output as the next query in Equation (1). This is because in most conditions the input images have no order and we're supposed to compute their relation to the soft prompts separately."}, {"title": "C. Multihead Sampler", "content": "The Multihead Sampler is used for computing weights for the weighted sum of the Visual Extractor's outputs \\(O_{VE}\\) in Equation (1). To further decompose the visual information in the input image embeddings, a Decomposer consisting of a 2-layer MLP is used to map the [CLS] token of the input embedding into \\(m\\) (\\(m = 4\\)) extra tokens, and the same number of attention heads are applied to compute the attention weights over each decomposed token and the soft prompts. This design aims to introduce multiple experts in the form of attention heads to focus on different features in the inputs.\nAs shown in Figure 3, the soft prompts serve as the queries and the decomposed image embeddings serve as the keys to compute the attention weights. Note that only the attention weights are computed and the means over the query's dimension are taken as the output of each head. Denote \\(e_j^i\\) as the j-th decomposed token of the i-th input embedding, each head's output weights \\(\\text{weights}_j^i\\) can be written as follows:\n\\[\\text{score}_j^i = \\text{head}_j(P, E^i); P \\in \\mathbb{R}^{l \\times d}, E^i = [e_1^i, e_2^i, ..., e_n^i] \\in \\mathbb{R}^{n \\times d}\\]\n\\[\\text{weights}_j^i = \\text{mean}(\\text{score}_j^i); \\text{score}_j^i \\in \\mathbb{R}^{l \\times n}; \\text{weights}_j^i \\in \\mathbb{R}^n\\]\nIn Equation (2), \\(d\\) is the model dimension, \\(P\\) is the soft prompts and \\(\\text{head}_j\\) means the computation of attention score over \\(P\\) and \\(E^i\\). \\(l\\) is the number of tokens in soft prompts. The \\text{mean} operation takes the averaged sum over the number of tokens in \\(P\\). The averaged sum of weights from each head is taken as the output of the Multihead Sampler:\n\\[W_{MS} = \\frac{1}{m} \\sum_{j=1}^m \\text{weights}_j; W_{MS} \\in \\mathbb{R}^n\\]\nIn Equation (3), \\(m\\) (\\(m = 4\\)) is the number of decomposed extra tokens and the number of attention heads in MS. MS aims to further capture the fine-grained visual features in the input image embeddings by applying different attention heads for different decomposed embeddings of the inputs."}, {"title": "D. Multiview Attributes Perceiver", "content": "As shown in Figure 4, after getting the weights from the Multihead Sampler, the final output of MAP is computed through the weighted sum of \\(O_{VE}\\) over \\(W_{MS}\\). Assume \\(W_{MS} = \\{w_1, w_2, ..., w_n\\}\\) and the output of Visual Extractor \\(O_{VE} = \\{o_1, o_2, ..., o_n\\}\\), the outputs of MAP can be formulated as follows:\n\\[\\sum_{i=1}^n w_i o_i; o_i \\in O_{VE}, w_i \\in W_{MS}\\]\n\\[O_{VE} = \\{o_1, o_2, ..., o_n\\}, W_{MS} = \\{w_1, w_2, ..., w_n\\}\\]\nIn Equation (4), \\(w_i\\) is the corresponding weight of the i-th input in \\(W_{MS}\\). Note that in the design of MAP, the number of image inputs is not restricted and this design enables the proposed MIAVLM model to accept any number of image inputs. The form of a weighted sum in the outputs also ensures that the input order has no influence on the final output, making the model more robust and reliable in practice."}, {"title": "III. EXPERIMENTS", "content": "Benchmark Settings and Implementation Details\nBenchmark Settings. The HoOA benchmark is generated from the CelebAText-HQ [16] dataset. In the original dataset, each image contains manually annotated descriptions of facial attributes such as ear shapes, colors, and various other attributes. Based on these descriptions, we used the Yi-CHAT-34B [33] model to rewrite them into general questions. These questions are called positive questions since all their answers are 'Yes'. To generate negative questions, we use Yi-CHAT-34B [33] to replace the original attributes in the questions with their opposite words to generate adversarial question sets. Finally, we sampled 1,430 images and obtained 14,291 positive questions and 14,291 negative questions. During instruction tuning, they were respectively employed as 14,291 positive instructions and 14,266 negative instructions for MIAVLM. Throughout the instruction tuning process, these instructions were divided into training and testing sets in a 9:1 ratio. We define the model's average accuracy on positive and negative questions as the HoOA metric.\nImplementation Details. The Language Modeling loss is used for training MIAVLM and we apply Adam optimizer with lr = 0.001 for optimization. The whole model is trained for 20 epochs with a cosine annealing scheduler. A single NVIDIA 3090 GPU was used for training."}, {"title": "B. The Performance of LVLMs on HoOA Benchmark", "content": "We compared MIAVLM (ours) with BLIP3 [34], four versions of OpenFlamingo [35], OPERA [36], Idefics2 [37] and LLaVA-UHD [2] on the HoOA benchmark. Among these LVLMs, both LLaVA-UHD and OPERA claim to have made improvements specifically targeting the hallucination problem based on LLaVA-1.5 [13]. All LVLMs utilize two input modes: 1. Using only the original image. 2. Using the original image along with eight generated images as input. For LVLMS like BLIP3, LLaVA-UHD, and OPERA, which only support single-image input, mode 2 involves combining the nine images into a single image (9in1). Overall, we observed that current popular LVLMs generally have a tendency to respond \"Yes\" to questions. In contrast, our model demonstrates a more balanced approach. Given that our designed MAP can efficiently process multiple images simultaneously and utilizes a lightweight LLM, our model also has a significant advantage in terms of efficiency. By comparing the performance of different LVLMs across the two input modes, we observed that using the 9in1 image as input did not improve results. This may be due to the fact that the 9in1 image is more challenging to interpret compared to the original image. In contrast, models that used nine separate multiview images as input showed overall performance improvements.\nTo demonstrate the importance of negative instructions, we only used positive instructions to tune MIAVLM. The results are shown in Table II. It can be observed that using negative instructions effectively enhances the model's performance on negative questions. However, using negative instructions also leads to performance degradation on positive questions."}, {"title": "C. The Influence of Multiview Images Input Order on LVLMs", "content": "We used OpenFlamingo [35] for comparison, along with our MIAVLM, both using 9 images as input from positive questions. We shuffled the order of these 9 images five times and recorded the results of both models. As shown in Figure 5, MIAVLM are not affected by any input order. However, any version of OpenFlamingo [35] is influenced by the order."}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduce a new benchmark to confirm the significant presence of HOOA problems in popular LVLMs. To mitigate HoOA, we propose MIAVLM, a LVLM that leverages multiview images of the current object as input and employs a novel MAP module to eliminate the influence of input image order. Additionally, negative instructions are utilized to suppress LVLMs' tendency to answer \u201cYes\u201d excessively."}]}