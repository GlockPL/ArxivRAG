{"title": "WAS: Dataset and Methods for Artistic Text Segmentation", "authors": ["Xudong Xie", "Yuzhe Li", "Yang Liu", "Zhifei Zhang", "Zhaowen Wang", "Wei Xiong", "Xiang Bai"], "abstract": "Accurate text segmentation results are crucial for text-related generative tasks, such as text image generation, text editing, text removal, and text style transfer. Recently, some scene text segmentation methods have made significant progress in segmenting regular text. However, these methods perform poorly in scenarios containing artistic text. Therefore, this paper focuses on the more challenging task of artistic text segmentation and constructs a real artistic text segmentation dataset. One challenge of the task is that the local stroke shapes of artistic text are changeable with diversity and complexity. We propose a decoder with the layer-wise momentum query to prevent the model from ignoring stroke regions of special shapes. Another challenge is the complexity of the global topological structure. We further design a skeleton-assisted head to guide the model to focus on the global structure. Additionally, to enhance the generalization performance of the text segmentation model, we propose a strategy for training data synthesis, based on the large multi-modal model and the diffusion model. Experimental results show that our proposed method and synthetic dataset can significantly enhance the performance of artistic text segmentation and achieve state-of-the-art results on other public datasets.", "sections": [{"title": "1 Introduction", "content": "Text segmentation is dedicated to finely segmenting the strokes of text from complex scene images, discriminating whether each pixel belongs to the text foreground or the background. Accurate text segmentation results are the foundation for text-related generative tasks. For instance, tasks such as text image generation [5], text style transfer [17,20], and text removal [35,36] can produce excellent and practical generative outcomes based on text masks. However, although existing models have achieved outstanding performance in regular text"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text Segmentation Method", "content": "Some early text segmentation methods relied on thresholding [28], low-level features [33], or Markov Random Fields (MRF) [23] to segment foreground text from the background, but these methods could only achieve limited success in document processing. With the continuous advancement of deep learning, the corresponding text segmentation methods have shown great potential in complex scenes [3, 12, 30]. For instance, SMANet [3] employs the encoder-decoder architecture of PSPNet [50] and achieves a multi-scale attention module for text segmentation. PGTSNet [43] employs a pre-trained detector to ground out text regions before segmentation, further enhancing the accuracy of segmentation. TexRNet [42] incorporates character recognition and attention-based similarity checking to aid the model in segmenting text. Building on these methodologies, Yu et al. [45] developed a model featuring a lightweight detection head and a Text-Focused Module, elevating text segmentation performance in complex scenes to a new level. Nevertheless, recent high-performance text segmentation methods either utilize extra bounding box annotations and rely on text detection, or employ character-level supervision. Also, these methods lack specialized design for artistic text."}, {"title": "2.2 Text Segmentation Dataset", "content": "The construction of text segmentation datasets has not received enough attention in academia. ICDAR13 [14] and Total-Text [10] provide high-quality, pixel-level annotations for text segmentation, but their quantities are very limited with only 462 and 1,555 images, respectively. To address the issue of insufficient quantity, researchers have proposed a dataset COCO-TS [2] (14,690 images) based on COCO-Text [31] for text segmentation. Similarly, MLT_S [3]"}, {"title": "2.3 Segmentation Dataset Generation", "content": "The construction of synthetic segmentation datasets plays an important role in studying visual perception problems. DatasetGAN [49] and BigDatasetGAN [15] only use a small number of manually labeled samples for each category to train the decoder and generate a large amount of new data. Diffumask [38] extends the text-driven image synthesis to semantic mask generation in Stable Diffusion [26] to create a high-resolution and class-discriminative pixel-wise mask. Dataset diffusion [25] leverages the pre-trained diffusion model and text prompts to generate segmentation maps corresponding to synthetic images. DatasetDM [37] is a generic dataset generation model that decode the latent code of the diffusion model as accurate perception annotations. MosaicFusion [40] is a diffusion-based data augmentation method that does not require training and does not rely on any label supervision, especially for rare and new categories. Then SegGen [44] integrates Text2Mask and Mask2Img synthesis to generate training data, improving the performance of state-of-the-art segmentation models in various segmentation tasks. These advanced methods often fail to align artistic text with their masks in images. In this paper, we avoid having the model generate both images and mask annotations. Instead, we pre-render the text masks and use ControlNet [47] to generate mask-conditioned images."}, {"title": "3 Dataset", "content": "As artistic text in the real world is incredibly diverse, we propose two new datasets: WAS-R composed of real-world text images, and WAS-S composed of synthetic text images. These multi-purpose artistic text datasets aim to bridge the gap between artistic text segmentation and real-world applications, accommodating the rapid advances in text vision research."}, {"title": "3.1 WAS-R Image Collection", "content": "The WAS-R dataset is composed of 7,100 images sourced from a variety of contexts, including posters, cards, covers, logos, goods, road signs, billboards, digital designs, and handwritten text. Among these, 4,100 images serve as the training"}, {"title": "3.2 WAS-R Image Annotation", "content": "The WAS-R dataset stands out due to its comprehensive annotations, surpassing existing datasets. Specifically, WAS-R provides minimum quadrilateral detection boxes with distinct segmentation mask labels for each word. It also provides text transcription for each word mask. Moreover, we annotates the word effects such as shadow, glow, 3D, which play a crucial role in distinguishing artistic text from conventional scene text and significantly impacts text segmentation."}, {"title": "3.3 WAS-S Synthetic Dataset Construction", "content": "Fig. 2 shows the pipeline of generating synthetic text images. The core idea is that we build a text image generation model which can generate aligned text images from text masks and input prompts. To this end, we construct the training pipeline as illustrated in Fig. 2 (a). Specifically, we first generate diverse and informative captions from the text images in the training set of WAS-R to obtain training triplets <caption, mask, image>. Following that, we train a Control- Net [47] with these triplets for generating diverse images that are pixel-wisely aligned with the input text mask. During inference, as shown in Fig. 2 (b), we first construct diverse text masks using Mask Render, then use GPT-4 to extend the texts in the mask into a scene description caption. The constructed text mask and caption are send to our trained ControlNet to generate the synthetic text image. We describe the modeling details below."}, {"title": "4 Methodology", "content": "In this section, we introduce our artistic text segmentation model WASNet. We first present the overall architecture, followed by detailed descriptions of the local and global designs."}, {"title": "4.1 Overall Architecture", "content": "The overall framework of WASNet is shown in Fig. 4. We take an excellent semantic segmentation model Mask2Former [8] as the meta-architecture. It is a mask classification architecture that directly predicts multiple binary masks and corresponding category labels, instead of performing per-pixel classification. We add a skeleton-assisted head and improve the Transformer decoder with a mechanism"}, {"title": "4.2 Transformer Decoder with Layer-wise Momentum Query", "content": "Artistic text segmentation faces the challenge of the local stroke shapes being flexible and changeable. Due to designers using hundreds of different artistic fonts and applying various text effects, the local strokes of the same character can differ significantly. This results in some slender strokes spanning across other areas, as well as twisted ligatures leading to complex text edges. In contrast, normal scene text typically utilizes regular printed fonts without special designs, and the stroke shapes are almost invariant. Therefore, it is necessary for the decoder to pay attention to these special local strokes.\nFirst, we use the masked attention mechanism [8], constraining cross-attention to within the local text mask region for each query, instead of attending to the"}, {"title": "4.3 Skeleton-Assisted Head", "content": "Different from regular text and general objects, the global topological structure of artistic text is very complex, and there are many holes and intricate connections inside. This presents new challenges for the segmentation task. The model needs to capture the global structure of the text object rather than just a region. Inspired by DeepSkeleton [27] and DeepFlux [34], we found that the skeleton is an effective representation to describe the shape and topology of text because it can extract the central axis of the object. Therefore, we use skeletons to assist text segmentation.\nAs shown in Fig. 4, we add a skeleton-assisted head to WASNet, enabling the model to simultaneously predict the mask and the skeleton, thus endowing it with the capability to perceive the global topological structure. Since the binary mask is a finely annotated label for semantic segmentation, the ground truth for the skeleton can be obtained by processing the mask with the classic Zhang- Suen [48] skeleton extraction algorithm. The algorithm progressively removes pixels that satisfy certain template structural conditions through an iterative process, until no more pixels meeting the conditions are deleted."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Implementation Details", "content": "Our experiments are mainly based on the MMSegmentation [11] toolbox. The overall hyperparameter configuration is the same as [8]. The pixel decoder is a multi-scale deformable attention Transformer [51] with 6 layers. The Transformer decoder consists of 9 layers, each with an auxiliary loss. We use the AdamW [19] optimizer and the poly [6] learning rate schedule with an initial learning rate"}, {"title": "5.2 Results of Artistic Text Segmentation", "content": "To verify the superiority of our method in the task of artistic text segmentation, we trained several representative models on our WAS-R dataset, including six semantic segmentation models and two text segmentation models. We use the officially released code for TexRNet [42], DDP [13], and TextFormer [45], and the code reproduced by MMSegmentation [11] for other models. For a fair comparison, we did not apply the character-level glyph discriminator for TexRNet.\nThe experimental results in Tab. 1 indicate that our WASNet outperforms all of these advanced models. Moreover, when we train the baseline models and WASNet with the synthetic dataset WAS-S, their performance can be further improved. Our final results have achieved a significant SOTA performance."}, {"title": "5.3 Results of Scene Text Segmentation", "content": "To further verify the generalizability of WASNet, we also conducted experiments on three publicly available scene text segmentation datasets [2, 10, 42], as shown in Tab. 2. We can draw the same conclusion as in Sec. 5.2 regarding the effectiveness of WASNet and our synthetic dataset. It is worth mentioning that character-level annotations were used to train TexRNet on TextSeg. Extra bounding box labels were used to train the text detection module of Text Former on all three datasets. However, we only use binary mask labels of the full images. Despite this, we still achieved competitive or state-of-the-art results. Due to the highly inaccurate annotation quality of COCO_TS [2] and the fact that Total-Text [10] contains only 300 test images, the conclusions drawn from the evaluation results of the models on these two datasets may be inconsistent.\nFurthermore, we directly evaluate the performance of WASNet on the three datasets using the model trained on the synthetic and real WAS datasets, as shown in the last row of Tab. 2. Note that the results in this row have not been fine-tuned on specific datasets but are still competitive. Therefore, to simplify the experimental paradigm and evaluation process of text segmentation models, we encourage researchers to train on WAS and test directly on other datasets."}, {"title": "5.4 Ablation Study", "content": "In this section, we conduct the ablation study on the artistic text segmentation dataset WAS. We first validate the effectiveness of our proposed modules and"}, {"title": "5.5 Further Analysis", "content": "To further verify the effectiveness of our proposed WASNet, we visualize the inference outputs of our baseline model Mask2Former [9] and WASNet in Fig. 5. According to Fig. 5 (a), it is evident that WASNet can capture special-shaped stroke regions such as slender tails or twisted ligatures. This is attributed to our Transformer decoder with the layer-wise momentum query. Additionally, according to Fig. 5 (b), WASNet exhibits good scale adaptability. It can achieve fine results for both large-scale and small-scale text with complex structures. This is because the skeleton-assisted head can obtain the global topological structure of the text through the thinning operation, guiding fine segmentation.\nOnce accurate text stroke masks are obtained, downstream text-related generative tasks can demonstrate excellent results. The application effects of text removal, text background replacement, and text style transfer are shown in the supplementary materials."}, {"title": "5.6 Limitation", "content": "Although the proposed synthetic dataset can improve the performance of text segmentation models, the enhancement is limited and does not significantly increase. Even when we further increased the amount of synthetic data, the performance remained unchanged. This could be caused by the bottleneck encountered in the diversity and realism of the synthetic images. In the future, we are considering designing more advanced generative models."}, {"title": "6 Conclusion", "content": "The paper focuses on a new challenging task of artistic text segmentation. We propose a real dataset for this task to train the models and benchmark the performance. We also construct a synthetic dataset to further enhance the accuracy and generalization ability. In order to meet the challenges of this task, we introduce the layer-wise momentum query to handle the changeable local strokes and the skeleton-assisted head to capture the complex global structure. Experimental results have demonstrated the effectiveness and superiority of our method in the tasks of artistic text segmentation and scene text segmentation. We hope that more researchers can focus on this task in the future and that the dataset we propose can change the experimental paradigm of text segmentation."}, {"title": "A More Details on Synthetic Dataset Construction", "content": "As stated in the paper, we first construct the training pipeline for a text image generation model, learning to generate text images spatially aligned with text masks. Then we construct an inference pipeline to input new masks and prompts into the trained generation model, generating new text images. Here we add the details of prompt generation in the training and inference pipelines."}, {"title": "A.1 Training Pipeline", "content": "To train the text image generation model such as ControlNet [47], it is necessary to obtain training data of <caption, text mask, text image> triplets. Text masks and text images are from our proposed real dataset. Captions should be detailed descriptions of the text images. To this end, we utilize a large multi-modal model, Monkey [18], to caption the images. Monkey is an open-source model and can handle vision-language tasks with high-resolution input and detailed scene understanding. It performs well on Image Captioning and various Visual Question Answering (VQA) tasks. Therefore, we feed a text image and a prompt \"generate the detailed caption in English\" to Monkey and let it output a detailed description. The examples of the generated captions are shown in Fig. 6. We found that, in many cases, Monkey is able to recognize and describe the text in images. To ensure the accuracy of the descriptions and to highlight the importance of the text, we add a sentence after each caption: This image contains the text \"text in the image\"."}, {"title": "A.2 Inference Pipeline", "content": "During the inference phase, we first need to produce new binary masks of text through the Mask Render introduced in the paper. Moreover, it is crucial to generate new prompts that describe more complex scenes. Combining the masks with rich descriptions of scenes, the trained model can generate new and realistic text images. We use GPT-4 [4] to generate the prompts. To ensure that the new prompts and the training prompts are in the same domain, and avoid domain gaps in the images generated by the model, we first provide GPT-4 with 50 caption examples produced by Monkey. Then we ask GPT-4 to mimic the style of these captions and synthesize new prompts. The instruction is Please follow the above caption examples and generate a similar caption, which must contain some double-quoted spaces \"\". Next, we insert the text corresponding to each new mask into the quotation marks in the new prompt, forming the final prompt. The generated <prompt, mask, image> triplet is shown in Fig. 3."}, {"title": "B Applications", "content": ""}, {"title": "B.1 Text Removal", "content": "Text removal refers to the process of erasing or deleting text regions from an image. The finer the text mask, the better the erasing performance, as it pre-"}]}