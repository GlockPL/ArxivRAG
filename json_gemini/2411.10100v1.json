{"title": "Multi-Task Adversarial Variational Autoencoder for Estimating Biological Brain Age with Multimodal Neuroimaging", "authors": ["Muhammad Usman", "Azka Rehman", "Abdullah Shahid", "Abd Ur Rehman", "Sung-Min Gho", "Aleum Lee", "Tariq M. Khan", "Imran Razzak"], "abstract": "Despite advances in deep learning for estimating brain age from structural MRI (sMRI), incorporating functional MRI (fMRI) data presents significant challenges due to its complex data structure and the noisy nature of functional connectivity measurements. To address these challenges, we present the Multitask Adversarial Variational Autoencoder (M-AVAE), a bespoke deep learning framework designed to enhance brain age predictions through multimodal MRI data integration. The M-AVAE uniquely separates latent variables into generic and unique codes, effectively isolating shared and modality-specific features. Additionally, integrating multitask learning with sex classification as a supplementary task enables the model to account for sex-specific aging nuances. Evaluated on the OpenBHB dataset-a comprehensive multisite brain MRI aggregation-the M-AVAE demonstrates exceptional performance, achieving a mean absolute error of 2.77 years, surpassing conventional methodologies. This success positions M-AVAE as a powerful tool for metaverse-based healthcare applications in brain age estimation.", "sections": [{"title": "INTRODUCTION", "content": "The advent of multimodal neuroimaging, which combines functional magnetic resonance imaging (fMRI) for the assessment of functional connectivity and structural magnetic resonance imaging (sMRI) for cortical morphology, offers a nuanced approach to the detection of cognitive impairment and the prediction of brain age [10]. However, the exploration of anatomical and functional differences in the brain between sexes using multimodal imaging for the estimation of brain age remains underexplored.\nSex differences play a vital role in the brain's ageing process, with notable anatomical and functional variations between male and female brains [11]. Incorporating sex information into age estimation models improves accuracy and has shown promise in deep learning applications [12]. Our research addresses this gap by integrating sex considerations in a multimodal imaging framework within the metaverse context, aiming to improve the accuracy and applicability of brain age predictions in personalised healthcare.\nSpecifically, we propose a novel metaverse-based AI application for brain age estimation: the Multi-Task Adversarial Variational Autoencoder (M-AVAE). This innovative model merges adversarial learning and variational auto-encoding capabilities within a multitask learning framework, aiming for simultaneous estimation of brain age and prediction of sex from multimodal MRI data, including both sMRI and fMRI. The design of M-AVAE meticulously segregates the latent features of each imaging modality into distinct components, effectively disentangling the shared and unique attributes across modalities. This method not only improves the accuracy in capturing commonalities, but also minimises interference during data fusion, presenting a novel approach to multimodal neuroimaging analysis suitable for integration into metaverse platforms.\nOur major contributions can be summarised as follows.\n\u2022 We introduce a novel multimodal framework for the estimation of brain age within the metaverse ecosystem for healthcare. Integrating our AI model into a metaverse environment, may enable continuous, real-time updates and interactions, enhancing the precision and reliability of brain age estimations and may allow personalised, predictive healthcare.\n\u2022 Our approach is unique in creating a disentangled representation of brain imaging data by applying both adversarial and variational principles within a single architecture. This disentanglement allows for the clear differentiation of shared versus modality-specific information, paving the way for more nuanced interpretations of neuroimaging data within a metaverse platform.\n\u2022 Through rigorous evaluation of publicly available datasets, our extensive experiments validate the efficacy and robustness of the proposed framework, thereby establishing a new benchmark for brain age estimation models suitable for metaverse integration."}, {"title": "RELATED WORK", "content": "Most studies on brain age estimation have utilised T1-weighted sMRI scans [9], while recent research highlights the potential of functional magnetic resonance imaging (fMRI) to predict cognitive variables [24], [25]. The ability of fMRI to capture intricate patterns of brain activity makes it valuable for the prediction of brain age. Several studies have explored multimodal MRI to improve prediction accuracy [26]."}, {"title": "PROPOSED METHOD", "content": "This section delineates the Multitask Adversarial Variational Autoencoder Network (M-AVAE), which leverages multimodal inputs\u2014specifically sMRI and fMRI scans-to predict biological age accurately. Our approach begins with feature extraction from both modalities, followed by integration into the M-AVAE. The model comprises dual autoencoders and a combination of loss functions that work to optimise the accuracy of brain age estimation. The architecture is depicted in Fig. 1, and the following subsections break down the components of the proposed model."}, {"title": "Feature Extraction Process", "content": "Given the high-dimensional nature of MRI data and limited dataset sizes, direct input of these features into neural networks can lead to overfitting. Thus, feature selection is a critical preprocessing step. Feature selection methods are broadly classified into filter, wrapper and embedded methods [53]. Consistent with prior work [23], we use the filter method"}, {"title": "Encoder Branches and Latent Variables", "content": "For each modality, a multilayer perceptron (MLP) serves as the encoder, denoted $Enc_i$ for $i = 1,2$. The encoder transforms the input feature vectors into latent representations:\n$z_i = Enc_i(x_i),$ (2)\nwhere $z_i$ is divided into a shared component $Gen(z_i)$ and a unique component $Unq(z_i)$ to capture the shared and modality-specific information. This separation ensures that the encoder can reconstruct the latent representation of its components while maximising the similarity of shared codes between modalities and distinguishing unique codes."}, {"title": "Cross Encoder Reconstruction", "content": "An MLP acts as a decoder $Dec_i$ for each modality. Given $Z_i = [Gen(z_i), Unq(z_i)]$, the decoder reconstructs $x_i$:\n$x'_i = Dec_i(Gen(z_i), Unq(z_i)).$ (3)\nAdditionally, the shared codes $Gen(z_i)$ are used to reconstruct the inputs from the other modality, ensuring that:\n$x'_i = Dec_i (Gen(z_j), Unq(z_i))$ for $i \u2260 j,$ (4)\nwhich enforces the separation of shared and unique information."}, {"title": "Age and Sex Prediction Strategy", "content": "Disentangling each modality's latent variable into generic $Gen (Enc_i(x_i))$ and unique $Unq(Enc_i (x_i))$ codes, we form $M(x_1, x_2)$ as:\n$M(x_1, x_2) = (Gen_{1,2}, Unq(Enc_1(x_1)), Unq(Enc_2(x_2))),$ (5)\nwhere $Gen_{1,2} = \\sum_{i=1}^{2} W_i Gen(Enc_i(x_i))$ with $w_1 = w_2 = 0.5$. Two MLPs, a regressor $P$ and a classifier $C$, predict age and gender from $M(x_1,x_2)$, respectively. Our approach integrates adversarial autoencoders (AAE) [55] with variational loss [56] for each modality, facilitating disentanglement and information fusion in cross-reconstruction. A shared MLP discriminator $D_z$ enforces adversarial regularisation on $z_i$, guiding it toward a predefined distribution. The weights of $Enc_i$, $Dec_i$, $D_z$, $P$, and $C$ are learnt together with specific loss functions."}, {"title": "Objective Function", "content": "1) Adversarial Loss: Let the prior distribution imposed on the generic part of the latent variable be denoted by $p(z_i)$, the encoding distribution by $q(z_i|x_i)$, and the decoding distribution by $p(x_i|z_i)$. The Adversarial Autoencoder (AAE) aims to learn the data distribution $p_d(x_i)$ by training an autoencoder with a regularised latent space. This involves ensuring that the aggregated posterior distribution $q(z_i) = \\int_{x_i} q(z_i|x_i)p_d(x_i)dx_i$ matches the predefined prior $p(z_i)$. Regularisation is achieved through an adversarial process involving the discriminator $D_z$, leading to a minimax problem: The total adversarial loss is the sum of the adversarial losses from both modalities:\n$L_{adv} = L_{adv}^{1} + L_{adv}^{2}.$ (6)\nThe encoder's goal is to make the generic part of the posterior distribution indistinguishable by the discriminator $D_z$ from the prior distribution $p(z_i)$, while $D_z$ aims to differentiate between $q(z_i)$ and $p(z_i)$. In this study, a Gaussian prior distribution $N(\\mu_i(x_i), \\sigma_i(x_i))$ is used for $z_i$, applying the reparameterisation trick for efficient backpropagation.\n2) Variational Loss: To regularise unique and generic codes without significantly increasing computational complexity, we integrate the Variational Autoencoder (VAE) approach, known for its ability to impose prior distributions through KL divergence, as an alternative to using multiple discriminator networks. This approach, referred to as variational loss, enforces different prior distributions $p(z_1)$ and $p(z_2)$ for the unique codes of the sMRI and fMRI modalities, respectively.\nThe variational loss aims to regularise the unique part of the latent space. The encoding distribution for the unique latent variables is $q(z_u|x_i)$, and the decoding distribution is $p(x_i|z_u)$. The VAE approximates the true posterior $p(z_u|x_i)$ with $q(z_u |x_i)$, employing the re-parameterisation trick for efficient optimisation.\nThe variational loss, represented as $L_{var}$, is formulated as the Kullback-Leibler (KL) divergence $D_{KL}(q(z_u|x_i)||P(Z_u))$, which serves as a regularisation term to align $q(z_u|x_i)$ with the prior $p(z_u)$, typically a standard Gaussian $N(0, 1)$.\n$L_{var} = D_{KL} (q(z_u|x_i)||P(Z_u)),$ (7)\nThe overall variational loss is the sum of the losses from both modalities:\n$L_{var} = L_{var}^{1} + L_{var}^{2}$ (8)\nThis integration of variational loss facilitates learning rich and nuanced representations through a hybrid approach, combining the strengths of adversarial and probabilistic modelling for effective generative capabilities and structured latent space interpretation.\n3) Generic-Unique Distance Ratio Loss: The distance ratio loss, $L_{Dist}$, emphasises the disentanglement of latent variables by balancing the distances between the generic (shared) and unique components of the embeddings of two modalities. It is defined as follows:\n$L_{Dist} = \\frac{L_{Dist}^{Gen}}{L_{Dist}^{Unq}}$ (9)\nwhere,\n$L_{Dist}^{Gen} = E_{x_1,x_2} || Gen (Enc_1 (x_1)) - Gen (Enc_2 (x_2))||_2,$ (10)\nand\n$L_{Dist}^{Unq} = E_{x_1,x_2} || Unq (Enc_1 (x_1)) - Unq (Enc_2 (x_2))||_2.$ (11)\n4) Regression Loss: The regression loss employs the L2 norm to measure the discrepancy between predicted and actual values:\n$L_{reg} = E_{x_1,x_2} ||y - P (M (X_1, X_2))||_2.$ (12)\n5) Classification Loss: The classification loss is formulated using Binary Cross-Entropy to evaluate the accuracy of binary classification tasks:\n$L_{class} = y \\log (C (M (X_1, X_2)))$\n$+ (1 -y) \\log (1 - C (M (X_1, X_2))).$ (13)\n6) Reconstruction Loss: The reconstruction loss, designed for cross-modality reconstruction, excludes any computation from missing data to accommodate incomplete neuroimage datasets:\n$L_{recon} = \\sum_{i=1}^{2} \\sum_{j=1}^{2}E_{x_i{\\sim}P_d(x_i)}$\n$||x_i - Deci (Generic (Encj (x_j)), Unq (Enc_i (x_i)))||$ (14)\n7) Full Objective: The full objective function integrates all individual losses with corresponding trade-off parameters, aiming to optimize the model components cohesively:\n$LD = Ladv,$\n$L_{Enci,Deci,P,C} = \\lambda_1L_{reg} + \\lambda_2L_{class} + \\lambda_3L_{dist}$\n$+ \\lambda_4L_{recon} + \\lambda_5 L_{adv} + \\lambda_6L_{var},$ (16)\nwhere $\\lambda_1$, $\\lambda_2$, $\\lambda_3$, $\\lambda_4$, $\\lambda_5$, and $\\lambda_6$ are the trade-off parameters. The multitask adversarial variational autoencoder (M-AVAE) framework first updates the discriminator Dz to differentiate between true and generated samples, followed by updating the encoder Enci, decoder Deci, and predictor P based on the combined objective function, catering to both modality-specific and shared representations."}, {"title": "Datasets", "content": "Our experiments leverage the OpenBHB dataset [57], a comprehensive collection comprising 5,330 3D brain MRI scans from 71 different acquisition sites. Of these, 3,984 scans are publicly accessible, distributed across 3,227 training and 757 validation instances. OpenBHB spans 10 datasets, featuring subjects of European-American, European, and Asian descent, ensuring a diverse range of genetic backgrounds. For our analysis, we focused on subsets containing both sMRI and fMRI scans, specifically using two datasets referenced in [58] and [59], comprising 66 and 315 scans, respectively. These datasets were merged to facilitate our study, and we applied the preprocessing pipeline outlined in Section to both datasets."}, {"title": "Model Architecture and Training Strategy", "content": "The Multitask Adversarial Variational Autoencoder (M-AVAE) architecture, utilised in our experiments, is illustrated in Fig. 3. The batch size was established at 20, with the latent variable dimensionality set to 120. Furthermore, the dimensions of the generic and unique codes were determined to be 50 and 70, respectively, based on empirical tests. These dimensionality settings apply equally to all AAE models employed in our study, as represented in Fig. 3. For the purpose of single-task learning analysis, the gender classifier was deactivated. The training employed the Adam optimiser with an initial learning rate of 0.001. Learning rate adjustments were made by reducing it to one-quarter upon failing to observe performance enhancements on the validation set after nine epochs. To mitigate overfitting, an early stop mechanism was implemented. The models were developed using Keras with the TensorFlow back-end and trained on an NVIDIA RTX 4090 GPU. The number of parameters in our model is 1,144,372 and it took almost 12 and 1.5 hours to train and test the model, respectively."}, {"title": "RESULTS AND DISCUSSION", "content": "In this section, we evaluated the efficacy of our proposed Multitask Adversarial Variational Autoencoder (M-AVAE) model by comparing it against a diverse set of regression methodologies. These included four model-agnostic methods: Random Forest (RF), Support Vector Regression (SVR), Gaussian Process Regression (GPR), and Partial Least Squares Regression (PLSR); two model-based methods: Multiple Kernel Learning (MKL) [63] and Incomplete Multi-Source Fusion (iMSF) [64]; and two approaches based on Adversarial Autoencoders (AAEs). The MKL method integrates dual kernels applied to sMRI and fMRI features to derive an optimal regression kernel, whereas iMSF focuses on learning shared feature sets with sparse regression across varying data source availability. The AAE methodologies take advantage of latent variables from sMRI and fMRI data for the prediction of brain age, with a variant that also predicts the gender of the patient (M-AAE)."}, {"title": "Robustness Analysis", "content": "To assess the robustness of the proposed M-AVAE model, we conducted a 10-fold cross-validation to examine the consistency of the model's performance across the entire dataset. Scatter plots juxtaposing the predicted ages with the chronological ages, generated by M-AVAE and other models evaluated under identical experimental conditions. This comparison illuminates the relative performance and robustness of each model.\nThe analysis reveals that the model-agnostic methods (RF, SVR, GPR, and PLSR) and the model-based techniques (MKL and iMSF) exhibit comparable mean performance metrics. However, PLSR and iMSF demonstrate enhanced confidence intervals, indicating more reliable performance predictions. Among these, AAE-based models significantly outperform in terms of confidence intervals, highlighting the robustness of AAE-based approaches. In particular, the multitask AAE model surpasses the single task AAE variant, underscoring the value of incorporating gender information through multitask learning. Our M-AVAE model achieves superior performance, exemplifying the effectiveness of our approach in creating a disentangled latent space. This is achieved by enforcing three distinct prior distributions, leveraging both adversarial and variational losses, which signifies the robustness and adaptability of the M-AVAE model.\nFurther examination of prediction accuracy across various age groups. The analysis presents differential performance across age segments. In particular, all models exhibit improved accuracy in younger age groups (below 25 years), with MAE values ranging from 4.78 to 2.66 years. However, performance disparities become more pronounced in older age segments, particularly within the 35 ~ 45 and 45 ~ 55 year ranges, where performance metrics vary significantly-highlighting a decline in model accuracy for older individuals. Despite these variations, AAE-based models maintain consistent performance, reinforcing their robustness. Within this category, the Multitask AAE model consistently outperforms its single-task counterpart, further validating the benefit of integrating gender information through multitask learning. Ultimately, the M-AVAE model distinguishes itself by delivering the most consistent and accurate predictions across all age groups, affirming the efficacy of our method in generating a disentangled latent space through the application of adversarial and variational principles."}, {"title": "Multi-Modality Analysis", "content": "The impact of multi-modality fusion on model performance was scrutinized by comparing unimodal (sMRI or fMRI alone) and multimodal (combined sMRI and fMRI) data approaches. The analysis revealed that uni-modal models trained exclusively on fMRI yielded the least favourable results, with those trained on sMRI performing moderately better. In contrast, models that used a multimodal strategy, incorporating sMRI and fMRI scans, demonstrated superior performance, underscoring the benefits of such an integrative approach.\nSpecifically, the Mean Absolute Error (MAE) for age prediction using only sMRI data ranged from 6.06 to 3.15 years. In contrast, models that relied on fMRI data alone showed MAEs between 8.32 and 3.58 years. Using multimodal data, MAEs improved, covering 4.82 to 2.77 years. This incremental improvement from sMRI-based unimodal to multimodal strategies was consistently observed in all models, including the proposed M-AVAE. This trend suggests that while traditional multimodal fusion methods might introduce additional noise from fMRI data, thus reducing accuracy compared to using only sMRI data, the M-AVAE approach effectively mitigates this issue. In particular, M-AVAE reduced MAE from 3.15 years with sMRI data alone to 2.77 years by employing the multimodal strategy, highlighting the model's ability to integrate multimodal information."}, {"title": "CONCLUSIONS", "content": "In this study, we introduced the Multi-Task Adversarial Variational Autoencoder (M-AVAE), a framework designed for brain age estimation, with potential applications in the evolving landscape of the Metaverse and healthcare. Our approach utilises multimodal Magnetic Resonance Imaging (MRI) data, combining structural and functional MRI to create a more comprehensive latent representation. Through the integration of techniques such as cross-reconstruction, adversarial and variational learning, and the introduction of a distance ratio loss, M-AVAE aims to disentangle latent variables into shared and modality-specific components. This process facilitates more effective data fusion and improves prediction accuracy, especially with the incorporation of sex information as an auxiliary task, acknowledging biological factors in age estimation. The performance of M-AVAE was evaluated on the OpenBHB dataset, where it demonstrated promising results, achieving a mean absolute error of 2.77 years in predicting brain age. Although these findings suggest that the M-AVAE framework can provide significant improvements over some traditional methods, further validation is necessary across different datasets and scenarios to fully understand its broader applicability. We believe that the insights gained from this work could contribute to the development of more refined diagnostic tools in virtual healthcare environments, although further research and exploration are essential to realise their full potential."}]}