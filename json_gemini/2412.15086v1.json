{"title": "Learning Disentangled Equivariant Representation\nfor Explicitly Controllable 3D Molecule Generation", "authors": ["Haoran Liu", "Youzhi Luo", "Tianxiao Li", "James Caverlee", "Martin Renqiang Min"], "abstract": "We consider the conditional generation of 3D drug-like\nmolecules with explicit control over molecular properties such\nas drug-like properties (e.g., Quantitative Estimate of Druglikeness or Synthetic Accessibility score) and effectively binding\nto specific protein sites. To tackle this problem, we propose\nan E(3)-equivariant Wasserstein autoencoder and factorize the\nlatent space of our generative model into two disentangled\naspects: molecular properties and the remaining structural context of 3D molecules. Our model ensures explicit control over\nthese molecular attributes while maintaining equivariance of\ncoordinate representation and invariance of data likelihood.\nFurthermore, we introduce a novel alignment-based coordinate loss to adapt equivariant networks for auto-regressive de-novo 3D molecule generation from scratch. Extensive experiments validate our model's effectiveness on property-guided and context-guided molecule generation, both for de-novo 3D molecule design and structure-based drug discovery against protein targets.", "sections": [{"title": "1 Introduction", "content": "Although a lot of progress has been made in AI-powered\n2D molecule design (Bengio et al. 2021; Jin, Barzilay, and\nJaakkola 2018; Shi et al. 2019; Xie et al. 2021), generating\ndesirable 3D molecules with specific attributes, e.g., druglike properties (e.g., QED and SA Score) and the ability to\nbind with targeted proteins, is still challenging and crucial\nin drug discovery. Given the vastness of the chemical space,\nestimated to be in the order of 10^{33} (Polishchuk, G, and\nVarnek 2013), conducting an exhaustive search is impractical.\nTherefore, our goal is to develop computational methods\nfor direct generation of novel and valid drug-like molecules\nconditional on desired properties.\nTo perform conditional 3D molecule generation, exist-\ning auto-regressive models such as G-SchNet (Gebauer,\nGastegger, and Sch\u00fctt 2019) and G-SphereNet (Luo and Ji\n2022) can be fine-tuned using subsets of favorable data, se-\nlected based on specific threshold criteria. However, these\nmodels are limited by their inability to generate molecules\nwith desired property values such as asphericity, QED, SA\nScore, and binding to target proteins. On the other hand,\nequivariant diffusion models (Hoogeboom et al. 2022; Qiang\net al. 2023; Xu et al. 2023) address this limitation by directly\nincorporating property values as additional inputs. Neverthe-\nless, they encounter a different challenge due to lacking an\nexplicit latent space for direct manipulation of latent vari-\nables, which is required for assigning or anchoring distinct\nproperties to the noise/latent space. Therefore, they are un-\nable to explicitly control molecule attributes in the generation\nprocess. However, explicit control over 3D molecule genera-\ntion is crucial in real-world drug design scenarios. Specialists\noften need to optimize certain properties while maintaining\nothers. For instance, improving a drug's synthetic accessi-\nbility without altering its binding affinity to a target. This\nprecision demands a generation process with a manipulable\nlatent space, enabling targeted property modifications.\nTo achieve explicit control over 3D molecule generation,\nwe propose to incorporate disentangled representation learn-\ning (Bengio, Courville, and Vincent 2013; Higgins et al. 2016;\nKim and Mnih 2018) into this task. Specifically, we factorize the latent space of our generative model into two disentangled aspects: one representing the desired molecular\nproperties, and the other capturing the remaining structural\ncontext related to 3D molecular validity. Notably, almost\nall existing methods only focus on the property-guided side\nof 3D molecule generation, neglecting the control over the\nremaining component of the latent space, i.e., the complemen-\ntary factors to molecular properties. In contrast, our model\nsupports two distinct generation modes: property-targeting\ngeneration, for which we generate 3D molecules with spe-\ncific, desirable properties for property-targeting 3D gener-\nation, and context-preserving generation, for which we\ngenerate 3D molecules allowing optimization of targeted\nmolecular properties while maintaining the molecule's fun-\ndamental architecture. These two capabilities enabled by our\nmodel are essential for many real-world drug design sce-\nnarios, where minor modifications can critically impact a\ndrug's efficacy and safety. Especially, the unique capability\nfor context-preserving generation distinguishes our model\nfrom all existing 3D molecule generation methods.\nTo realize these capabilities, we propose an E(3)-\nequivariant Wasserstein autoencoder (E3WAE) model ar-"}, {"title": "2 Related Works", "content": "De-novo Molecular Generation in 3D. Recently, there has\nbeen considerable progress in the field of 3D molecule gen-\neration using deep learning techniques. Various approaches\nhave been explored to accurately represent and generate 3D\nmolecular structures. Some have focused on learning joint\ndistributions of geometric features (Garcia Satorras et al.\n2021; Luo and Ji 2022). Others, such as G-SchNet (Gebauer,\nGastegger, and Sch\u00fctt 2019), have employed auto-regressive\nmodels with symmetry constraints to sample 3D molecules\ndirectly in their spatial configurations. A notable advance-\nment is the use of equivariant diffusion models (Hooge-\nboom et al. 2022; Qiang et al. 2023; Xu et al. 2023), which\nhave shown significant promise in generating high-quality\nmolecules. These techniques have been further applied in\nspecific generation tasks with external conditioning, such as\nreference ligands (Adams and Coley 2023; Liu et al. 2022;\nPowers et al. 2022; Guan et al. 2023) and linkers (Huang\net al. 2022; Igashov et al. 2022; Imrie et al. 2020). Despite\nthese advancements, a common limitation in most existing\nmodels is the lack of explicit control over the attributes of\nthe generated molecules. This poses a challenge for appli-\ncations where there is a need to modify certain attributes of\nthe molecules, such as a specific chemical property, while\npreserving other aspects like overall shape and composition.\nOur research aims to address this gap by explicitly controlling\nthe generation of molecules, where higher-level semantics\ncan be precisely manipulated to meet specific criteria.\nDisentangled Representation Learning. The design of\ndisentangled representation learning (DRL) tasks varies\nbased on the nature of data involved. For instance, DRL tasks\ninvolve decoupling \u201cstyle\u201d and \u201ccontent\u201d in text (Cheng et al.\n2020) or images (Kotovenko et al. 2019), or distinguishing\nbetween \"static\" and \"dynamic\" components in videos (Zhu\net al. 2020; Han et al. 2021). Such disentangled representa-\ntions are pivotal in applications like style transfer (Cheng et al.\n2020; Kotovenko et al. 2019; Lee et al. 2018) and conditional\ngeneration (Denton et al. 2017; Zhu et al. 2018). To achieve\ndisentanglement, many regularization techniques are devel-\noped. These techniques include managing the capacity of\nlatent space (Burgess et al. 2018; Higgins et al. 2016), imple-\nmenting adversarial loss (Deng et al. 2020), employing cyclic\nreconstruction (Lee et al. 2018), imposing mutual informa-\ntion constraints (Chen et al. 2016; Cheng et al. 2020; Zhu et al.\n2020), and integrating self-supervising auxiliary tasks (Zhu\net al. 2020). In molecular science, DRL has been applied to\nmolecule generation using SMILES sequences (Mollaysa,\nPaige, and Kalousis 2020) and 2D molecular graphs (Du et al.\n2022). However, to the best of our knowledge, its application\nin de-novo 3D molecule generation and structure-based drug\ndesign has not been explored."}, {"title": "3 Preliminaries", "content": "Problem Formulation. To generate large-scale drug-like\nmolecules, our method adopts an auto-regressive, fragmentbased 3D generation approach. Let G be the space of 3D\nmolecular graphs, where each 3D molecular graph G\u2208 G\nconsists of the fragment node set V, the edge set E, and the\nfragment coordinate matrix R. Specifically, each fragment\nrepresents a combination of several atoms and bonds, e.g.,\na benzene ring could be a fragment that includes six carbon\natoms and aromatic bonds. Correspondingly, each fragment\ni\u2208V is associated with a node feature vi and a position\nvector ri, which corresponds to the i-th line of R and rep-\nresents the center coordinates of the fragment. Following\nfragment-based generation (Bengio et al. 2021; Jin, Barzilay,\nand Jaakkola 2018; Qiang et al. 2023), we use edge eij \u2208 E\nto indicate that two fragments i, j \u2208 V share a bond/atom.\nThis fragmentization inherently includes assembled rules for\nneighboring fragments and enables us to model a substantial\nportion of the chemical space with a reasonably sized frag-\nment vocabulary. The generation process is iterative; at each\nstep t, the next graph state is predicted as G_{t+1} = M (G_t),\nwhere M is the auto-regressive generation model. The frag-\nmentization details are in Appendix 7. Building upon this,\nour model is designed for explicit control over molecular at-\ntributes while ensuring both E(3)-invariance and equivariance\nproperties.\nExplicit Control. We define a generative model with\nexplicit control as M : Y_1 \u00d7 Y_2 \u2192 X, where V\u2081 and V\u2082 rep-"}, {"title": "4 Methodology", "content": "We propose an E(3)-equivariant Wasserstein autoencoder\n(E3WAE) model, where the generation of the 3D molecule\nis factorized into two disentangled factors: the property and\nthe structural context. The former variable comprises the\nchemical property of the 3D molecule while the latter refers\nto all other 3D structure patterns that do not relate to the prop-\nerty yet reflect the chemical constraints within the molecular\nchemical space. This factorization allows for explicit control\nduring the generation process. In addition, we introduce a\nnovel coordinate prediction loss as part of the reconstruction\nloss to enable auto-regressive 3D molecule generation with\nequivariant networks. An overview of our model framework\nis shown in Figure 1."}, {"title": "4.1 E(3)-Equivariant Disentangled Encoder", "content": "We use two separate E(3)-equivariant GNN encoders to ex-\ntract invariant and equivariant latent variables, respectively:\nZ_{h,e}, Z_{v,e} = \\Theta_e(G),\n(1)\nwhere e = {p, s} represents \u201cproperty\u201d or \u201cstructural con-\ntext\". The invariant latent variable associate with node i,\nz_i^{h,e} \u2208 z^{h,e}, has a dimensionality of dh. Correspondingly,\nthe equivariant latent variable z_i^{v,e} \u2208 z^{v,e} is dimensioned in\nR^{d \u00d73}.\nIn this paper, we opt for E(3)-equivariant Vector Neurons\nMultilayer Perceptron (VN-MLP) (Deng et al. 2021) and\nMixed-Features Message Passing (MF-MP) (Huang et al.\n2022) as building blocks for both branches of encoders \u0398e.\nThese choices are due to their proven effectiveness in inte-\ngrating both invariant and equivariant features, ensuring the\nproperty space's invariance and the coordinate space's equiv-\nariance. However, it is important to note that our model is\ncompatible with any E(3)-equivariant architecture.\nFor the property branch, the property latent variables zh,p\nand zv,p are first derived by the property encoder \u0398p(G). Sub-\nsequently, a Readout function is used to obtain graph-level\nrepresentations. The Readout function is implemented as ei-\nther the average or the summation of all node embeddings.\nIn order to ensure that these latent variables carry informa-\ntion related to the property of 3D molecules, we introduce\nan auxiliary prediction head Hprop that takes the aggregated\nzh,p as input and predicts the target property value of the 3D\nmolecule:\n\u0177 = H_{prop}(Readout(z_{h,p})),\n(2)\nFor the structure context branch, the context latent vari-\nables zh,s and zv,s are encoded using the structural encoder"}, {"title": "4.2 Disentanglement of the Latent Space", "content": "To achieve the disentanglement between property and con-\ntext latent variables, we introduce a Wasserstein autoencoder\nregularization loss following Tolstikhin et al. (2018). This\napproach involves minimizing the Maximum Mean Discrepancy (MMD) between the distribution of latent variables and\nan isotropic multivariate Gaussian prior, denoted as z ~ Pz.\nSpecifically, for the invariant latent variables z\u0127 ~ Q\u266d, where\nZh = concat(zh,p, zh,s), and an isotropic Gaussian prior\nPh = N (0, I^{2d}), we compute the disentanglement loss for\ninvariant variables as\nL_{Dis}^h = MMD(P_h, Q_h).\n(3)\nFor the equivariant latent variables, z = concat(\u0396\u03c5,\u03c1, \u0396\u03c5,\u03c2)\nand zv \u2208 R^{2d\u00d73}, we maintain independence along the 2dv\naxis while allowing for covariance along the remaining di-\nmension. To this end, we sample three isotropic Gaussian\npriors from P = N(0, I^{2d_v}) and calculate the correspond-\ning disentanglement Wasserstein loss as:\nL_{Dis}^v = MMD(P_v, Q_v),\n(4)\nwhere P\u2208 R^{2d_v\u00d73} is the combined Gaussian priors and\nQu\u2208 R^{2d_v\u00d73} is the distribution of the equivariant latent\nvariables z. The total disentanglement loss is then calculated\nas the sum of these components: LDis = LDis + LDis\nFor MMD estimation, we take an input batch of latent vari-\nables {zi}i=1,\u2026,m with batch size m. Corresponding samples\n{\u017ei}i=1,\u2026,m are randomly drawn from the Gaussian prior,\nmaintaining the same sample size. The MMD is then calcu-\nlated using the linear time unbiased estimator (Gretton et al.\n2012; Sutherland et al. 2017) as\nMMD(Pz, Qz) = \\frac{1}{\\left[m / 2\\right]} \\sum_{i=1}^{\\left[m / 2\\right]}\\left[k\\left(Z_{2 i-1}, Z_{2 i}\\right)\n+k\\left(\\tilde{Z}_{2 i-1}, \\tilde{Z}_{2 i}\\right)-k\\left(Z_{2 i-1}, \\tilde{Z}_{2 i}\\right)-k\\left(Z_{2 i}, \\tilde{Z}_{2 i-1}\\right)\\right] .\n(5)\nwhere k is the kernel function, implemented using a radial\nbasis function (RBF) kernel with \u03c3 = 1.\nMinimizing this loss aligns the joint distribution of the\nlatent embeddings with the isotropic normal distributions,\nso that the property and context latent variables are indepen-\ndent. Additionally, the Gaussian shape of the latent space\nfacilitates smooth interpolation, effective regularization, and\nenhanced generation diversity (Kingma and Welling 2014;\nTolstikhin et al. 2018). A theoretical analysis supporting the\ndisentanglement guarantee is provided in Appendix 8."}, {"title": "4.3 Decoder and 3D Molecule Graph\nReconstruction", "content": "The decoder D(G|zh, zv) reconstructs 3D molecule graph G\nfragment-by-fragment in an auto-regressive manner. Specif-\nically, we employ another E(3)-equivariant GNN as the de-\ncoder. We incorporate the connectivity rules between frag-\nments by masking out invalid edges. For the auto-regressive\nreconstruction, our approach is inspired by the focus and ex-\npand molecule generation procedure in previous works (Cey-\nlan and Gutmann 2018; Huang et al. 2022). Notably, our\nmethod is significantly different from previous works. We\nuse fragments as nodes and introduce a novel coordinate\nprediction loss (see Section 4.4) specifically designed to opti-\nmize equivariant networks. This approach enables the genera-\ntion of molecules using equivariant networks without relying\non any external conditioning, such as reference structures\nlike linker designs and pocket-based generation. A detailed\nalgorithm for this decoding process is in Appendix 9.\nSpecifically, the decoding process includes the following\nsteps: (i) Node type prediction: Initially, fragment types\n{x}_1^n for all n nodes in G are determined from latent vari-\nables zh and zv. The fragment type logits are obtained from\nl atent variables through a self-attention mechanism and an\nMLP, and these logits are then used to sample all fragment\ntypes. Once the node types are sampled, their embeddings\nare concatenated with the corresponding latent variables zh\nfor subsequent steps. (ii) Focus queue initialization: A fo-\ncus queue Q is then initialized with a randomly chosen node\nfrom G. (iii) Focus and expand iterations: For each focus\nnode f popped from Q, the decoder predicts an expand edge\nconnecting f to another node. If the connected node u is\nnot a stop node or being linked for the first time, its coordi-\nnate (Xu, Yu, zu) is predicted. Node u is then added to Q if\nunvisited. This process repeats until a stop node is reached,\nmarking f as visited. The order of node focus and edge con-\nnection is determined using Breadth-First Search, facilitating\nteacher-forcing training. The reconstruction process ends\nwhen Q is empty, ensuring that all nodes in the connected\ncomponent of G have been considered for expansion.\nConcretely, in each iteration t of the focus and expand\nphase, we have the currently reconstructed subgraph Gt =\n(Vt, Et, Rt). Initially, the latent node embeddings are updated\nas 2h, zu with a MF-MP layer. Then the edge logits between\nthe focus node f and any node i are obtained with\ne_{f, i}=\\Phi\\left(\\left|\\left|z_f^h\\right|\\right|, \\left|\\left|z_i^h\\right|\\right|, m_{f, i}, \\sum_{j \\in V_t} z_j^v, \\sum_{j \\in V_t} z_j^v \\right)\n(6)\nwhere I is a feed forward network and mf,i \u2208 {0,1} indi-\ncates whether fragments f and i can be connected, based\non feasible chemical bonding rules. The softmax function is\napplied to the edge logits to determine the probabilities for\neach edge. Node u is then determined by identifying which\nnode i has the highest probability.\nTo predict the coordinate for the newly connected node\nu, we take the geometric center rt of current subgraph Gt\nas a reference point and predict a displacement of node u\nrelated to the reference point. Initially, we compute two sets\nof pair-wise interactions in the current connected graph as\nP_{i j}^{(k)}=\\Gamma(\\tilde{z}_{j,i}^{1}, \\tilde{z}_{j,i}^{1},\\left(W^{(k)},W^{(k)}\\right)), k=1,2\n(7)"}, {"title": "4.4 Training Objectives", "content": "where I is a feed forward network and W(k) is a learnable\nlinear transformation. The displacement of node u is then\ncalculated by:\nd_u = \\sum_{j \\in V_t} P_{i j}^{(1)}\\left(r_j - r_t\\right) + \\Omega_1\\left(\\sum_{j \\in V_t} P_{i j}^{(2)}\\right)\n(8)\nwhere 21 and 22 are VN-MLP layers. Finally, the predicted\ncoordinates of node u is obtained by ru = du + rt.\nThe proposed E3WAE network is trained by minimizing a\nweighted sum of three individual losses:\nL_{Total} = L_{Prop} + \\alpha L_{Dis} + \\beta L_{Recon},\n(9)\nwhere a and \u00df are the trade-off weights for the losses. Specif-\nically, LDis is introduced in Section 4.2. In addition, for the\nauxiliary property prediction head Hprop attached to prop-\nerty encoder Op in Section 4.1, we adopt the L1 loss as the\nproperty prediction loss:\nL_{Prop} = ||y - \u0177||_1,\n(10)\nwhere y denotes the ground truth value of the target property.\nFinally, for the decoder in Section 4.3, we adopt a reconstruc-\ntion loss which consists of three parts:\nL_{Recon} = L_{NodeType} + L_{Edge} + L_{Coords}.\n(11)\nInitially, a cross-entropy loss LNodeType is used for a clas-\nsification task to accurately determine the types of nodes.\nFollowing this, another cross-entropy loss, LEdge, is applied\nto predict the edges in each iteration of the process. Finally,\nwe incorporate a log-MSE loss (Yu 2020) as the coordinate\nprediction loss Loords at each iteration t:\nL_{coords} = log(\\sum_{i=1}^n f_i ||\\tilde{r_i}-r_i ||^2)/(\\sum f_i),\n(12)\nwhere fi is a binary flag indicating the presence of a newly\nadded node in the i-th subgraph G_i^{'} that is not a stop node.\nHowever, we cannot apply the log-MSE loss directly.\nSpecifically, for the first iteration t = 1, there are only\ntwo nodes in the current subgraph, in which ||Vt||= 2 and\nRt \u2208 R^{2\u00d73}. The only constraint to the current 3D structure\nis the distance between the two existing nodes. Thus, the\nground truth coordinates can be considered as any point on a\nsphere with distance dt = ||ru \u2013 rt|| to the reference point\ncoordinate rt = rf, where ru denotes the ground truth co-\nordinate of newly added node u and rf is the coordinate of\nthe focus and the only node f in previous subgraph. More-\nover, for the situation that there are three nodes in the current\nsubgraph, where ||Vt ||= 3 and Rt \u2208 R^{3\u00d73}, the ground truth\ncoordinate is considered equivalent to any point with a dis-\ntance dt = ||ru \u2013 \u0155t || to \u0159t and a angle 0t = arccos(St, Suf).\nHere, we use st, Suf to denote the unit vectors of ru - rt and\nru - rf, respectively. Therefore, if we use the log-MSE coor-\ndinate loss directly, all these situations will be neglected and\nthe symmetric-invariance property of the geometric space\nwill be violated."}, {"title": "4.5 Generation with Explicit Control", "content": "To adapt E(3)-equivariant networks to de-novo molecule\ngeneration task without external reference structure, we pro-\npose to align the coordinates with Kabsch algorithm (Kabsch\n1976) and then calculate the coordinate loss with transformed\ncoordinates for the case when there are less than or equal to\nthree nodes in any samples in this batch. Concretely, we cal-\nculate a rotation matrix R \u2208 SO(3) and a translation vector\nT\u2208 R\u00b3 for the maximum alignment between generated coor-\ndinates Rt = [ri]iev, and ground truth coordinates Rt. Then\ntransformed coordinates of generated nodes are obtained by\n\u0159i = Rri + T,\nVi \u2208 Vt.\n(13)\nThus, for samples with three or fewer nodes at the current\niteration, the coordinate loss is:\nL_{Coords} = \\sum_{i=1}^t \\sum_{j \\in V_{t,i}} ||\u0159_j \u2013 r_j ||^2.\n(14)\nIn the generation phase, we sample latent variables zh and\nz and set the maximum number of fragments to N. Note\nthat the exact number of fragments might be smaller than\nN. Then the generation process follows the same decoding\nprocess introduced in Section 4.3 but differs by operating\nwithout teacher forcing, relying solely on the model's selfguided predictions. Notably, a unique feature of our E3WAE\nmodel is the disentangled latent space, which enables explicit control over two key aspects of molecule generation:\n(i) property-targeting generation: In this approach, predefined property latent variables zh,p, Zv,p are combined with\nsampled or template context latent variables zh,s, Zv,s. This\ncombination is then fed into the decoder, producing new\nmolecules G = D(G|zh,p, Zv,p) with targeted properties; (ii)\ncontext-preserving generation: Conversely, this mode uses\npredefined context latent variables zh,s, Zu,s combined with\neither sampled or template property latent variables zh,s, Zv,s.\nUsing such latent variables, the decoder can generate new\nmolecules G = D(G|zh,s, zv,s) that refine certain properties\nwhile maintaining the core molecule framework. The predefined latent variables are obtained either by directly using\nor by performing interpolation or extrapolation with latent\nvariables from existing molecules. For simplicity, this work\nadopts the first approach.\nAtom Conformation Assembling. Once the 3D molec-\nular graph is complete at the fragment level, including all\nfragment types, links, and center coordinates, we proceed to\ndetermine the atom-level coordinates. Following Qiang et al.\n(2023), we first randomly pick a fragment and explore all\nfeasible links to its neighbors, selecting the one nearest to our\npre-defined fragment center. We then use RDKit (Landrum\n2016) to model each potential connection, evaluating their\nsuitability based on root-mean-square deviation (RMSD)\nfrom the center. This procedure is iteratively applied to build\nthe structure fragment by fragment. Finally, we align these\nlocal structures within the molecular framework using the\nKabsch algorithm (Kabsch 1976), adjusting the RDKit coor-\ndinates to match target positions."}, {"title": "5 Experiments", "content": "In our work, we focus on generating large-scale drug-like\nmolecules. We follow the experimental setup in Qiang et al.\n(2023) and conduct experiments mainly on two benchmark\ndatasets for 3D molecule generation: GEOM-Drugs (Axelrod\nand Gomez-Bombarelli 2022) and CrossDocked2020 (Fran-\ncoeur et al. 2020). GEOM-Drugs contains 304k drug-like\nmolecules while Cross Docked2020 consists of about 100k\n3D ligand structures, each derived from a protein-ligand com-\nplex. Our model is trained using a subset of 50,000 molecular\nstructures from each dataset, specifically selecting those with\nthe lowest energy conformations for each molecule. We com-\npare our model with three well-established baseline methods,\nincluding one auto-regressive method, G-SphereNet (Luo\nand Ji 2022) and two equivariant diffusion-based methods,\nEDM (Hoogeboom et al. 2022) and HierDiff (Qiang et al.\n2023). Hyperparameters and other implementation details are\nprovided in Appendix 11."}, {"title": "5.1 Property-targeting 3D Generation", "content": "In AI-enhanced drug discovery, a crucial focus is the genera-\ntion of 3D molecules with specific, desirable properties. This\ntask is commonly referred to as property-targeting 3D gener-\nation. Following the experiment settings in HierDiff (Qiang\net al. 2023), we select asphericity, QED, SAS, and logP as key\nproperties for conducting practical drug discovery-related\nconditional generation. Asphericity assesses how much a\nmolecule's shape deviates from a sphere, influencing its bio-\nlogical interactions. Quantitative Estimate of Drug-likeness\n(QED) gauges a molecule's similarity to known drugs, indi-\ncating its potential efficacy as a drug candidate. The Synthetic\nAccessibility Score (SAS) evaluates the ease of synthesizing\na molecule, a crucial factor in drug development. Lastly, logP\nmeasures a compound's lipophilicity, which is essential for\nunderstanding the compound's absorption and distribution in\nthe body. Each of these properties plays a significant role in\ndetermining a molecule's suitability for drug development,\nmaking them ideal choices for our focused properties.\nFor this task, as introduced in Section 4.5, we combine\nthe property latent embeddings of a reference molecule with\nanother molecule's context latent embeddings to generate\nnew molecules. To ensure a fair comparison and assess gen-\neralization, we use the properties of molecules in the test set\nas the target when generating new molecules for all baseline\nmethods and our model."}, {"title": "De-novo Molecule Generation", "content": "We evaluate the mean\nsquared error (MSE) and mean absolute error (MAE) be-\ntween the input properties and the real properties of the gen-\nerated 3D molecules. The results are shown in Table 1, and\nthe detailed standard deviation of experiment results are pro-\nvided in Appendix 11.3. The baseline results of Asphericity\nand QED on the GEOM-Drugs dataset are taken from Hi-\nerDiff (Qiang et al. 2023) while the others are reproduced\nby ourselves using their official implementation. The results\nshow that our model achieves the best performance on 7\nout of 8 properties across two datasets. While we do not\nachieve the top ranking in the logP task, we attain results\nthat closely approximate the current state-of-the-art method.\nThe results demonstrate the effectiveness of our method in\nproperty-targeting generation for 3D drug-like molecules,\nshowing that our model can serve as a useful tool in compu-\ntational drug design. We provide results for general genera-\ntion quality, i.e., drug-likeness metrics and 3D conformation\nquality metrics, in Appendix 11.4."}, {"title": "Structure-based Drug Design", "content": "In real-world drug de-\nsign applications, optimizing specific properties of binding\nmolecules is a pivotal scenario. For instance, enhancing the\nsynthetic accessibility score of a binding ligand can preserve\nits binding capability while simplifying the manufacturing\nprocess. Thus, we propose a property-targeting generation\ntask for structure-based drug design. For this task, we train\nthe model by taking pocket-ligand complex structures as in-\nput and performing ligand generation conditioned on a given\nproperty latent vector. Through this method, we generate 3D\nmolecules while considering specific target binding sites and\ndesired property values. We evaluate the generated molecules\nfrom targeted binding affinity and molecular properties. Fol-\nlowing previous works (Guan et al. 2023; Luo et al. 2021), we\nemploy AutoDock Vina (Eberhardt et al. 2021) to estimate\nthe target binding affinity. We use the MSE and MAE be-\ntween generated and reference properties as property-related\nmetrics. The results of are shown in Table 2. The results\ndemonstrate that our model excels in generating binding\nligands for target proteins with enhanced property values,\nsurpassing the performance of the baseline model. This un-\nderscores the practical utility of our model in real-world\nstructure-based drug design."}, {"title": "Multi-property Targeting Generation", "content": "Additionally, we\napply our model to disentangle multiple target properties and\nthe structure context. Specifically, we conduct experiments\nwith asphericity and QED properties using the GEOM-drug\ndataset. The results are shown in the following table. We\ncan observe that the performance of multi-property targeting\ngeneration slightly dropped. Theoretically, asphericity and\nQED cannot be completely disentangled, but our model can\nstill achieve a certain degree of explicit control on the multi-\nproperty-targeting generation."}, {"title": "5.2 Context-Preserving 3D Generation", "content": "Our model enables a novel structural context-preserving 3D\ngeneration task, which optimizes targeted molecular proper-\nties while preserving the molecule's fundamental architecture.\nThis capability plays a crucial role in enhancing the efficacy,"}, {"title": "5.3 Ablation Study", "content": "We perform ablation studies to justify our model design on\ntwo key components of E3WAE: (1) model architecture and\ndisentangled representation learning objectives, comparing"}, {"title": "5.4 Visualization of Disentangled Latent Space", "content": "To more explicitly show the disentangling ability of our\nmodel, we provide a t-SNE visualization (Van der Maaten\nand Hinton 2008) of the property and structure embeddings in\nFigure 2, focusing on asphericity and logP as examples. The\ncolor coding corresponds to various ground truth property val-\nues. We observe that for each molecule, property labels can\nbe well-represented by the property latent variables but not\nthe context latent embeddings. The observed entanglement\nin the context latent space reflects the inherent independence\nof structural aspects from selected property labels, in line\nwith the principles of disentangled representation learning.\nThis distinct separation in the property space demonstrates\nour model's ability to differentiate molecular properties and\nalign them systematically, thereby enabling explicit control\nover generating 3D molecules with desired characteristics."}, {"title": "6 Conclusion"}]}