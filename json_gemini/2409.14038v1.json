{"title": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang", "Jing Jiang"], "abstract": "Hallucinations of large language models (LLMs) commonly occur in domain-specific downstream tasks, with no exception in ontology matching (OM). The prevalence of using LLMs for OM raises the need for benchmarks to better understand LLM hallucinations. The OAEI-LLM dataset is an extended version of the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate LLM-specific hallucinations in OM tasks. We outline the methodology used in dataset construction and schema extension, and provide examples of potential use cases.", "sections": [{"title": "1. Motivation", "content": "Large language models (LLMs) have shown incredible capabilities in natural language generation (NLG) and question answering (QA). In the Semantic Web community, LLMs have recently been used in ontology matching (OM). Although LLMs offer a strong background knowledge base for OM, they can sentiently generate a number of false mappings and miss some true mappings, resulting in lower precision or recall. This phenomenon has been observed in several recent papers in [1, 2, 3, 4, 5]. It is mainly caused by LLM hallucinations, where LLMs tend to generate synthesised answers when they do not have sufficient background knowledge or have biased domain knowledge.\nFor example, the conference track [6] is one of the Ontology Alignment Evaluation Initiative (OAEI) [7] campaigns that evaluates OM tasks in conference organisation. It includes several reference alignments between different ontologies related to research conference management. LLM hallucinations can cause an incorrect match of \u201chttp://cmt#Paper\u201d and \u201chttp://confOf#Paper\u201d. LLMs lack domain knowledge to infer that \"paper\" in conference submission (e.g. in the ConfOf ontology) may have a narrower intended meaning like \"full-version paper\u201d that is distinguished from other submission types such as \u201cabstract", "poster\u201d. The correct match in this case is \u201chttp": "cmt#PaperFullVersion\u201d with \u201chttp://confOf#Paper\u201d. LLMs may also experience hallucinations when they encounter a missing match of \u201chttp://cmt#Chairman\u201d and \u201chttp://conference#Chair\u201d. It is very difficult for LLMs to understand polysemous words, such as where the word \u201cchair", "issues": "the former mismatches the entity \"paper\" with the entity \u201cpaper\u201d, while the latter assumes that the entity \u201cchairman\u201d does not match the entity \"chair\". It seems feasible to mitigate LLM hallucinations using fine-tuning, but there are few datasets that can be used to provide baseline references.\nEstablished in 2004, OAEI has become the largest community of specified ontology matching chal- lenges. Over the years, OAEI has developed several multi-ontology tracks across a wide range of topic domains. Almost all tracks use expert systems or machine learning-based systems to generate ground-truth reference mapping results. We observe that no tracks have covered the topic of LLM"}, {"title": "2. Methodology", "content": "Figure 1 illustrates the procedure of constructing the dataset. The original OAEI datasets provide three files: the source ontology (Os), the target ontology (Ot), and their OAEI reference (Roaei). We implement an LLM-based OM system to generate the alignment file LLM-Alignment (Allm). The system takes Os and Ot as inputs and generates a set of predicted mappings. The matching assessment procedure detailed below will compare Roaei with Allm and find their differences."}, {"title": "2.1. Dataset Construction", "content": "For each pair of mappings (e1, e2) \u2208 Roaei and (e1', e2') \u2208 Allm, we consider that there are no LLM hallucinations if (e1, e2) = (e1', e2'). We only deal with LLM hallucinations that occur when (e1, e2) \u2260 (e1', e2\u2032). These can be categorised into two different categories: (1) Missing: For a specific pair (e1, e2), there does not exist any pair (e1', e2') satisfies (e1 = e1') \u2228 (e2 = e2\u2032). (2) Incorrect: For a specific pair (e1, e2), there exists a pair (e1', e2') satisfies (e1 = e1' \u2227 e2 \u2260 e2') \u2228 (e1 \u2260 e1' \u2227 e2 = e2'). For missing mappings, additional details on the mapping that occurs in Roaei or Allm are provided. For incorrect mappings, additional details on the misaligned entity and type of hallucinations are provided.\nThe matching assessment determines the following types of hallucinations in incorrect mappings:\n\u2022 Align-up: LLMs map the entity to its intended superclass.\n\u2022 Align-down: LLMs map the entity into its intended subclass.\n\u2022 False-mapping: LLMs map the entity to an irrelevant entity.\n\u2022 Disputed-mapping: LLMs map the entity to a relevant entity, but it does not align with the OAEI reference. We call this mapping type \u201cdisputed\u201d because we cannot guarantee the OAEI reference is always correct. LLMs may discover a more precise matching entity. OAEI-LLM also aims to use LLMs to help with evolving more precise reference alignments for OM tasks.\nThe matching assessment could be done via a semi-automatic process (e.g. human-in-the-loop evaluation) or a fully automated process (e.g. multiple LLM agent negotiation). The final results will be used to generate the OAEI-LLM dataset."}, {"title": "2.2. Schema Extension", "content": "The OAEI tracks use a general alignment format [8] derived from the Expressive and Declarative Ontology Alignment Language (EDOAL) [9]. A simple example to represent a valid match of \u201cchairman\u201d and \"chair\" is illustrated as follows: Each mapping is recorded in a tag <Cell>. The schema uses the tags <entity1> and <entity2> to record the pair of entities, the tag <measure> to present their similarity, and the tag <relation> to indicate their relations.\nWe extend the current mapping schema to record the new information related to LLM hallucinations. For each mapping, different types of hallucinations can occur in different LLMs. Each of them is recorded in the tag <hallucination>. In the extended version of the example (highlighted in blue), the first segment depicts the LLM hallucinations that occurred in Llama-3.1, which is a case of missing mapping on the LLM side. The second segment illustrates the LLM hallucinations that occurred in GPT-40, indicating an incorrect mapping to the entity \u201cconference chair\u201d. The matching assessment classifies this mapping as \"disputed mapping\"."}, {"title": "3. Potential Use Cases", "content": "Different LLMs can achieve different performance on the same OM task. However, running an LLM- based matcher on a traditional OAEI dataset can only observe the differences in precision, recall, and F1 score. Using the extended version of OAEI-LLM, we can additionally quantify the LLM errors made by counting each of the different types of errors we have identified. Therefore, it becomes possible to understand the tendency of LLMs to generate incorrect answers or to provide relevant but not precise answers. This could also be a valuable supplement to enhance an LLM leaderboard for OM tasks. Indeed, this same additional information on matching failures could also be helpful if adopted more widely by non-LLM ontology matchers."}, {"title": "3.1. Benchmarking LLMs for OM tasks", "content": "Different LLMs can achieve different performance on the same OM task. However, running an LLM- based matcher on a traditional OAEI dataset can only observe the differences in precision, recall, and F1 score. Using the extended version of OAEI-LLM, we can additionally quantify the LLM errors made by counting each of the different types of errors we have identified. Therefore, it becomes possible to understand the tendency of LLMs to generate incorrect answers or to provide relevant but not precise answers. This could also be a valuable supplement to enhance an LLM leaderboard for OM tasks. Indeed, this same additional information on matching failures could also be helpful if adopted more widely by non-LLM ontology matchers."}, {"title": "3.2. A dataset for fine-tuning LLMs used in OM tasks", "content": "Fine-tuning is a common approach used in LLMs to inject domain knowledge, but a significant amount of data typically needs to be provided. Unlike the traditional OAEI dataset, which only provides information on matched entities, OAEI-LLM provides extensive information identifying when and how LLM hallucinations occur, and even the mismatched entities caused by LLM hallucinations. This could be used to generate high-quality training data to fine-tune LLMs."}, {"title": "Ethical Considerations", "content": "According to the OAEI data policy (date accessed: 2024-06-30), \u201cOAEI results and datasets, are pub- licly available, but subject to a use policy similar to the one defined by NIST for TREC. These rules apply to anyone using these data.\" Please find more details from the official website: https: //oaei.ontologymatching.org/doc/oaei-deontology.2.html"}, {"title": "Acknowledgments", "content": "The authors thank the Commonwealth Scientific and Industrial Research Organisation (CSIRO) for supporting this project."}, {"title": "References", "content": "[1] Y. He, J. Chen, H. Dong, I. Horrocks, Exploring Large Language Models for Ontology Alignment,\n2023. URL: https://arxiv.org/abs/2309.07172. arXiv:2309.07172.\n[2] S. Hertling, H. Paulheim, OLaLa: Ontology Matching with Large Language Models, in: Proceedings\nof the 12th Knowledge Capture Conference 2023, K-CAP\u201923, Association for Computing Machinery,\nNew York, NY, USA, 2023, p. 131-139. doi:10.1145/3587259.3627571.\n[3] Z. Qiang, W. Wang, K. Taylor, Agent-OM: Leveraging LLM Agents for Ontology Matching, 2024.\nURL: https://arxiv.org/abs/2312.00326. arXiv:2312.00326.\n[4] H. B. Giglou, J. D'Souza, F. Engel, S. Auer, LLMs4OM: Matching Ontologies with Large Language\nModels, 2024. URL: https://arxiv.org/abs/2404.10317. arXiv:2404.10317.\n[5] R. Amini, S. S. Norouzi, P. Hitzler, R. Amini, Towards Complex Ontology Alignment using Large\nLanguage Models, 2024. URL: https://arxiv.org/abs/2404.10329.arXiv:2404.10329.\n[6] OAEI Community, OAEI Conference Track, 2024. URL: https://oaei.ontologymatching.org/.\n[7] OAEI Community, Ontology Alignment Evaluation Initiative (OAEI), 2024. URL: https://oaei.\nontologymatching.org/.\n[8] INRIA, A format for ontology alignment, 2024. URL: https://moex.gitlabpages.inria.fr/alignapi/\nformat.html.\n[9] J. Euzenat, J. David, M. Atencia, N. Guillouet, EDOAL: Expressive and Declarative Ontology Align-\nment Language, 2014. URL: https://ns.inria.fr/edoal/1.0/."}]}