{"title": "Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization", "authors": ["Junlin He", "Jinxiao Du", "Wei Ma"], "abstract": "Self-supervised learning (SSL) has rapidly advanced in recent years, approaching the performance of its supervised counterparts through the extraction of representations from unlabeled data. However, dimensional collapse, where a few large eigenvalues dominate the eigenspace, poses a significant obstacle for SSL. When dimensional collapse occurs on features (e.g. hidden features and representations), it prevents features from representing the full information of the data; when dimensional collapse occurs on weight matrices, their filters are self-related and redundant, limiting their expressive power. Existing studies have predominantly concentrated on the dimensional collapse of representations, neglecting whether this can sufficiently prevent the dimensional collapse of the weight matrices and hidden features. To this end, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weight matrices, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances the performance of SSL methods across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures. Our code will be released at https://github.com/Umaruchain/OR_in_SSL.git.", "sections": [{"title": "Introduction", "content": "Self-supervised learning (SSL) has established itself as an indispensable paradigm in machine learning, motivated by the expensive costs of human annotation and the abundant quantities of unlabeled data. SSL endeavors to produce meaningful representations without the guidance of labels. Recent developments have witnessed joint-embedding SSL methods achieving, or even exceeding the supervised counterparts (Misra & Maaten 2020, Bardes et al. 2022, Caron et al. 2020, Chen, Fan, Girshick & He 2020, Chen, Kornblith, Norouzi & Hinton 2020, Chen & He 2021, Dwibedi et al. 2021, HaoChen et al. 2021, He et al. 2020, He & Ozay 2022, Jing et al. 2021, Li, Zhou, Xiong & Hoi 2020, Jing et al. 2020, Balestriero et al. 2023, Grill et al. 2020, Zbontar et al. 2021, Chen et al. 2021). The efficacy of these methods hinges on two pivotal principles: 1) the ability to learn augmentation-invariant representations, and 2) the prevention of complete collapse, where all inputs are encoded to a constant vector.\nTo prevent dimensional collapse of representations, as depicted in Figure 1, existing methods include modifying representations in downstream tasks (He & Ozay 2022), whitening representations directly (i.e. removing the projector) (Jing et al. 2021), incorporating regularizers on representations during pretraining (Huang et al. 2024, Hua et al. 2021). However, whether they sufficiently prevent the dimensional collapse of weight matrices and hidden features remains unknown (i.e., no theoretical guarantee) (Pasand et al. 2024). In Appendix A.1, we further demonstrate that whitening representations directly to eliminate the dimensional collapse of representations cannot adequately remove the dimensional collapse of weight matrices.\nTo address these challenges, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. It is natural that OR prevents the dimensional collapse of weight matrices as it ensures weight matrices orthogonality, keeps the correlation between its filters as low as possible, and lets each filter have a norm of 1. For features (e.g. hidden features and representations), orthogonal weight matrices can promote uniform eigenvalue distributions and thus prevent the domination of eigenspaces by a"}, {"title": "Related Work", "content": "Self-supervised Learning (SSL) aims to learn meaningful representations from unlabeled data. Existing SSL methods can be broadly classified into two categories: generative and joint embedding methods. This paper concentrates on joint-embedding methods, which learn representations by aligning the embeddings of different augmented views of the same instance. Joint-embedding methods further subdivide into contrastive and non-contrastive methods. Contrastive methods, such as those proposed by He et al. (2020), Chen, Kornblith, Norouzi & Hinton (2020), Chen et al. (2021), treat each sample as a distinct class and leverage the InfoNCE loss (Oord et al. 2018) to bring representations of positive pairs closer together while distancing those of negative pairs in the feature space. These methods generally require a substantial number of negative samples for effective learning. In contrast, non-contrastive methods eschew the use of negative samples. They instead employ various techniques such as self-distillation (Caron et al. 2021, Grill et al. 2020, Chen & He 2021), clustering (Caron et al. 2018, 2020, Pang et al. 2022) and feature whitening (Bardes et al. 2022, Zbontar et al. 2021, Weng et al. 2022, 2023). Our empirical findings indicate that incorporating OR enhances the performance of both contrastive and non-contrastive SSL methods. The exploration of its effects on generative methods remains for future work.\nDimensional collapse plagues both generative and joint embedding SSL methods (Zhang et al. 2022, Jing et al. 2021, Zhang et al. 2021, Tian et al. 2021). To prevent the dimensional collapse of representations, existing work has typically focused on imposing constraints on the covariance matrix of the representations, including modifying representations in downstream tasks (He & Ozay 2022), removing the projector (Jing et al. 2021), incorporating regularizers on representations during pretraining (Huang et al. 2024, Hua et al. 2021). However, these strategies face challenges such as performance degradation upon removing the projector, not addressing collapse during pre-training, and failing to prevent dimensional collapse in hidden features and weight matrices within the encoder (referred to Appendix A.1). This motivates us to regularize the weight matrices of the DNNs directly in SSL."}, {"title": "Orthogonality Regularization", "content": "Orthonormality regularization, which is applied in linear transformations, can improve the generalization and training stability of DNNs (Xie et al. 2017, Huang et al. 2018, Saxe et al. 2013). OR has demonstrated its effects on tasks including supervised/semi-supervised image classification, image retrieval, unsupervised inpainting, image generation, and adversarial training (Bansal et al. 2018, Balestriero et al. 2018, Balestriero & Baraniuk 2020, Xie et al. 2017, Huang et al. 2018). Efforts to utilize orthogonality in network training have included penalizing the deviation of the gram matrix of each weight matrice from the identity matrix (Xie et al. 2017, Bansal et al. 2018, Balestriero et al. 2018, Kim & Yun 2022) and employing orthogonal initialization (Xie et al. 2017, Saxe et al. 2013). For more stringent norm preservation, some studies transform the convolutional layer into a doubly block-Toeplitz (DBT) matrix and enforce orthogonality (Qi et al. 2020, Wang et al. 2020).\nIn this work, we first time investigate the efficacy of two orthogonality regularizers, Soft Orthogonality (SO) and Spectral Restricted Isometry Property (SRIP) in SSL (Bansal et al. 2018). These regularizers aim to minimize the distance between the gram matrix of each weight matrix and the identity matrix-measured in Frobenius and spectral norms, respectively."}, {"title": "Preliminaries", "content": "In this section, we present the general settings for joint-embedding SSL methods. We consider a large unlabelled dataset $X \\in \\mathbb{R}^{N\\times D}$ comprising $N$ samples each of dimensionality $D$. The objective of SSL methods is to construct an effective encoder $f$ that transforms raw data into meaningful representations $Z = f(X)$, where $Z \\in \\mathbb{R}^{N\\times M}$ and $M$ denotes the representation dimensionality.The learning process of SSL methods is visually represented in Figure 2, where data augmentations transform $X$ into two augmented views $X_{aug1}, X_{aug2} \\in \\mathbb{R}^{D\\times N}$. A typical joint-embedding SSL architecture encompasses an encoder $f$ and a projector $p$. These components yield encoder features $Z_{aug1} = f(X_{aug1})$ and $Z_{aug2} = f(X_{aug2})$, as well as projection features $H_{aug1} = p(Z_{aug1})$and $H_{aug2} = p(Z_{aug2})$. During training, the parameters of $f$ and $p$ are optimized via backpropagation to minimize the discrepancy between $H_{aug1}$ and $H_{aug2}$. To prevent the encoder $f$ from producing a constant feature vector, contrastive methods utilize negative samples, and non-contrastive methods employ strategies such as the self-distillation technique.\nThe efficacy of the encoder $f$ is usually assessed by the performance of the $C$-class classification as downstream tasks. Specifically, given a labeled dataset containing samples $X_s \\in \\mathbb{R}^{S\\times D}$ and their corresponding labels $Y_s \\in \\mathbb{R}^{S\\times C}$, where $S$ is the sample number. Then, a linear layer $g$ parameterized by $W_l \\in \\mathbb{R}^{C\\times M}$ is appended on top of the learned representations $Z_s = f(X_s)$, and thus the classification task can be fulfilled by minimizing the cross-entropy between $softmax(g(Z_s))$ and $Y_s$. There are two strategies for the fine-tuning: 1) non-linear fine-tuning, which trains both $g$ and $f$ in the downstream tasks, and 2) linear evaluation, which freezes $f$ and only trains $g$ (referred to as the linear probe)."}, {"title": "Orthogonality Regularizers", "content": "We introduce two orthogonality regularizers: Soft Orthogonality (SO) and Spectral Restricted Isometry Property Regularization (SRIP), which are seamlessly integrable with linear and convolutional layers.\nConsider a weight matrix $W\\in \\mathbb{R}^{input\\times output}$ in a linear layer, where $input$ and $output$ denote the number of input and output features, respectively. In line with Bansal et al. (2018), Xie et al. (2017), Huang et al. (2018), we reshape the convolutional filter to a two-dimensional weight matrix $W\\in \\mathbb{R}^{input\\times output}$, while we still use the same notation $W$ for consistency. To be specific, $input = S \\times H \\times C_{in}$ and $output = C_{out}$, with $C_{in}$ and $C_{out}$ being the number of input and output channels, and $S$ and $H$ representing the width and height of the filter, respectively.\nThe SO regularizer encourages the weight matrix $W$ to approximate orthogonality by minimizing the distance between its Gram matrix and the identity matrix. This is quantified by the Frobenius norm as follows:\n$SO(W) = \\begin{cases} ||W^TW - I||_F, & \\text{if } input > output,\\\\ ||WW^T - I||_F, & \\text{otherwise,} \\end{cases}$ (1)\nwhere $I$ is the identity matrix of appropriate size.\nThe SRIP regularizer employs the spectral norm to measure the deviation from orthogonality, which is defined as:\n$SRIP(W) = \\begin{cases} \\sigma(W^TW - I), & \\text{if } input > output,\\\\ \\sigma(WW^T - I), & \\text{otherwise.} \\end{cases}$ (2)\nwhere $\\sigma(\\cdot)$ denotes the spectral norm operator. Due to the high computational cost posed by the spectral norm, the power iteration method (Yoshida & Miyato 2017, Bansal et al. 2018) with two iterations is used for the estimation. The process for estimating $\\sigma(WTW - I)$ is:\n$u = (W^TW - I)v, v = (W^TW - I)u, \\sigma(W^TW - I) = \\frac{||v||_2}{||u||_2}$ (3)\nwhere $v \\in \\mathbb{R}^{input}$ is a vector initialized randomly from a normal distribution."}, {"title": "Incorporating OR into SSL", "content": "This section details the integration of OR with SSL methods. To be specific, we employ OR across the encoder, targeting both convolutional and linear layers during pretraining. We represent the SSL method's loss function as $Loss_{SSL}$. Our overall optimization objective is the minimization of the combined loss equation:\n$Loss = Loss_{SSL} + \\gamma Loss_{OR}$, (4)\nwhere $Loss_{OR}$ is defined as $\\sum_{W\\in f} SO(W)$ or $\\sum_{W\\in f} SRIP(W)$, depending on the selected orthogonality regularizers. The term $\\gamma$ serves as a hyperparameter that balances the SSL objective and OR loss. Notably, we only perform OR on the weight matrices located within the linear and convolutional layers of the encoder $f$. OR provides a versatile regularization strategy for the encoder $f$, facilitating its application across various SSL methods without necessitating modifications to the network designs or existing training protocols."}, {"title": "Analysis of Dimensional Collapse and the Effects of OR in SSL", "content": "In this section, we show that dimensional collapse happens not only to the representations (i.e. output of the encoder), but also to weight matrices and hidden features of the encoder. We also compare the feature whitening technique used by one previous method, VICREG (Bardes et al. 2022), with OR. Migrating to BYOL, we find that the feature whitening technique only solves the dimensional collapse at the feature level, but instead accelerates the collapse of the weight matrices, and it even leads to lower performance of the downstream tasks as shown in Table 1. In contrast, OR can"}, {"title": "Numerical Experiments", "content": "We study the effects of OR on SSL methods through extensive experiments. we first demonstrate that OR improves the classification accuracy on CIFAR-10, CIFAR-100, and IMAGENET100, and the improvement is consistent across different backbones and SSL methods. On the large-scale dataset IMAGENET-1k (Deng et al. 2009), OR boosts the classification accuracy on both in-distribution and out-distribution datasets (i.e. transfer learning datasets), demonstrating consistent improvement. Moreover, OR also enhances the performance in downstream tasks(e.g. object detection).\nBaseline methods and datasets. We evaluated the effect of adding OR to 13 modern SSL methods, including 6 methods implemented by solo-learn (MOCOv2plus, MOCOv3, DINO, NNBYOL, BYOL, VICREG) (Chen & He 2021, Chen et al. 2021, Grill et al. 2020, Dwibedi et al. 2021, Caron et al. 2021) and 10 methods implemented by LightlySSL (BarlowTwins, BYOL, DCL, DCLW, DINO, Moco, NNCLR, SimCLR, SimSiam, SwaV) (Zbontar et al. 2021, Yeh et al. 2022, Caron et al. 2020, Chen & He 2021). We pretrain SSL methods on CIFAR-10, CIFAR-100, IMAGENET-100 and IMAGENET-1k and evaluate transfer learning scenarios on datasets including CIFAR-100, CIFA-10 (Krizhevsky et al. 2009), Food-101 (Bossard et al. 2014), Flowers-102 (Xia et al. 2017), DTD (Sharan et al. 2014), GTSRB (Haloi 2015). We evaluate the objection detection task on PASCAL VOC2007 and VOC2012 (Everingham et al. 2010). Detailed descriptions of datasets and baseline SSL methods are shown in Appendix A.4 and A.5, respectively.\nTraining and evaluation settings. For each SSL method, we use the original settings in solo-learn (Da Costa et al. 2022) and LightlySSL. These settings include the network structure, loss function, training policy (training epochs, optimizers, and learning rate schedulers) and data augmentation policy. The splits of the training and test set follow torchvision Marcel & Rodriguez (2010). For all the classification tasks, we report the linear probe or KNN accuracy; for the objection detection task, we perform non-linear fine-tuning. Details of training, parameter tuning, and evaluation are presented in Appendix A.6. It is worth noting that the Solo-learn and LightlySSL setups are not the same as the official implementation of the SSL methods, e.g., there is no use of multi-crop augmentation in DINO, and there is no exceptionally long training epoch. We leave experiments on migrating OR to the official implementation for future work.\nRecipe of adding OR. For OR, $\\gamma$ of SRIP is tuned from $\\{1e-3, 1e-4, 1e \u2013 5\\}$ and $\\gamma$ of SO is tuned from $\\{1e \u2013 5, 1e \u2013 6, 1e 7\\}$ on a validation set. When you want to add OR to your SSL pre-training, you simply pass the encoder into the loss function, and then you just need to set $\\gamma$ of the OR according to the backbone and regularizer you use as shown in Table 9 of Appendix A.6."}, {"title": "OR is Suitable for Different Backbones and SSL Methods", "content": "After pretraining on CIFRA-100, for each SSL method, we report the corresponding classification accuracy as shown in Table 2. Both two orthogonality regularizers consistently improve the linear classification accuracy. Note that OR boosts the performance of both constrastive (MoCov2plus,"}, {"title": "OR Works on Large-scale Dataset", "content": "We demonstrate the effects of OR on the large-scale dataset IMAGENET-1k. Specifically, we pre-train three BYOL models- BYOL without OR, BYOL with SO, and BYOL with SRIP on IMAGENET-1k"}, {"title": "Conclusions", "content": "The existing studies focus on the dimensional collapse of representations and overlook whether weight matrices and hidden features also undergo dimensional collapse. We first time propose a mitigation approach to employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weights, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances SSL method performance across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures as the backbones. Importantly, the time complexity and required efforts on fine-tuning are low and the performance improvement is significant, enabling it to become a useful plug-in in various SSL methods.\nIn terms of future research, we wish to examine the effect of OR on other pre-training foundation models, such as vision generative SSL models such as MAE (He et al. 2022), auto-regression models like GPTs and LLaMAs (Radford et al. 2018, 2019, Brown et al. 2020, Touvron et al. 2023), and Contrastive Language-Image Pre-training models (Radford et al. 2021, Li et al. 2022). We believe OR is a pluggable and useful module to boost the performance of vision and language foundation models. In fact, this paper is the first to test the effectiveness of OR in a Transformer-based architecture and it is reasonable to believe that it will perform well in these domains."}, {"title": "Appendix / supplemental material", "content": null}, {"title": "Effects of Representation Whitening on the Encoder", "content": "In this section, we explore the effect of whitening representations on hidden features and weight matrices in the encoder. To be specific, similar to the settings of 5, we train three VICREG Bardes et al. (2022) models: original VICREG, VICREG without projector (Li, Chen & Yang 2020), and VICREG with OR.\nOriginal VICREG adds two regularization terms (variance and covariance regularization) to whiten the projector features. We use $X_{aug1}$ as an example to introduce them.\nVariance regularization. The variance regularization term ensures that each dimension of the learned representation $Z$ maintains a non-trivial variance. This is critical to prevent the collapse of dimensions, where a model might ignore certain informative variations in the data. Mathematically, the variance regularization can be expressed as follows:\n$L_{var} = \\frac{1}{D} \\sum_{d=1}^D max(0, \\gamma \u2013 S(z_d, \\epsilon))$ (5)\nwhere $D$ is the dimensionality of $Z_{aug1} = f(X_{aug1})$, $z_d$ represents the $d$-th dimension of $Z_{aug1}$, $S(z_d, \\epsilon) = \\sqrt{Var(z_d)) + \\epsilon}$ is the regularized standard deviation of $z_d$ across different samples, and $\\gamma$ is a threshold parameter that dictates the minimum desired standard deviation for each dimension.\nCovariance regularization. The covariance regularization term is designed to decorrelate the different dimensions of $Z_{aug1}$. By minimizing the off-diagonal elements of the covariance matrix of $Z_{aug1}$, this term helps ensure that different dimensions capture distinct aspects of the data, thereby preventing redundancy in the representation. The covariance regularization is defined as:\n$L_{cov} = \\sum_{i\\neq j} (Cov(z_i, z_j))^2$ (6)\nwhere $Cov(z_i, z_j)$ denotes the covariance between the $i$-th and $j$-th dimensions of $Z_{aug1}$. This term effectively encourages the representation to have orthogonal dimensions, which is beneficial for learning independent features.\nAs for the VICREG without projector, we discard the projector and apply the SSL objective directly to the representations, which ensures that the representations are whitened (no dimensional collapse in representations), i.e., minimize the correlation among dimensions and make each dimension rich in information. For OR, we choose SO as the regularizer and set $\\gamma$ as le \u2013 6. We then experimentally observe that guaranteeing that dimensional collapses do not occur in representations or projector features (i.e., VICREG without projector and projector features) does not guarantee that dimensional collapses do not occur in weight matrices in the encoder. Moreover, discarding the projector even damages the performance of the original VICREG, while OR still boosts the performance as shown in Table 8."}, {"title": "Visualization of Weight Matrices", "content": "In this section, layer4 of ResNet18 pretrained with BYOL on CIFAR-10 is visualized. To be specific, we calculate the correlation coefficient matrix of the weight matrix and then plot the HeatMap of the correlation coefficient matrix and the results of Spectral Biclustering. As shown in Figure 5,"}, {"title": "Visualization of Representations", "content": "We used BYOL (ResNet18) for pretraining on CIFAR-10. After pretraining, we perform dimension reduction and visualization of learned representations using UMAP (McInnes et al. 2018). As shown in Figure 6, in the absence of OR, there is a tendency for the cluster centers of each category to move closer together and more outliers appear. This is due to the fact that in the absence of OR, BYOL produces representations dominated by some extremely large eigenvalues (i.e. dimensional collapse), which is consistent with results in Section 5.2."}, {"title": "Datasets", "content": "We utilized several datasets for pretraining and evaluating SSL methods. Below we provide a detailed description of these datasets:\n\u2022 IMAGENET-1k (Deng et al. 2009): A large dataset contains 1,281,167 training images, 50,000 validation images, and 100,000 test images, which spans 1000 object classes.\n\u2022 IMAGENET-100 (Deng et al. 2009): A subdataset of IMAGENET-1K, containing 100 classes with 1000 training data and 300 test data per class.\n\u2022 CIFAR-10 (Krizhevsky et al. 2009): Comprising 60,000 images in 10 classes, with each class containing 6,000 images. The split includes 50,000 training images and 10,000 test images.\n\u2022 CIFAR-100 (Krizhevsky et al. 2009): This dataset consists of 60,000 images divided into 100 classes, with 600 images per class. The dataset is split into 50,000 training images and 10,000 test images.\n\u2022 Food-101 (Bossard et al. 2014): This dataset includes 101,000 images of food dishes categorized into 101 classes, with each class having approximately 1,000 images.\n\u2022 Flowers-102 (Xia et al. 2017): Contains 8,189 images of flowers from 102 different categories. Each class consists of between 40 and 258 images.\n\u2022 DTD (Sharan et al. 2014): The Describable Textures Dataset (DTD) includes 5,640 images categorized into 47 different texture categories.\n\u2022 GTSRB (Haloi 2015): The German Traffic Sign Recognition Benchmark (GTSRB) dataset consists of over 50,000 images of traffic signs across 43 categories.\n\u2022 PASCAL VOC2007 and VOC2012 (Lin et al. 2014): Used for evaluating objection tasks, this dataset includes complex everyday scenes with annotated objects in their natural context. The objection detection task contains 20 categories. We use the VOC2007 and VOC2012 train-val (16551 images) as the training set and then report the performance on the VOC2007 test set (4952 images).\nEach dataset was carefully curated to support the training and validation of our models, ensuring a comprehensive evaluation across various image classification and segmentation tasks."}, {"title": "Joint-embedding SSL methods", "content": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without the need for labeled data. This appendix provides a concise overview of several SSL methods used in this paper.\n\u2022 Bootstrap Your Own Latent (BYOL), proposed by Grill et al. (2020), introduces a novel approach to SSL that does not rely on negative pairs. Instead, BYOL employs a dual-network architecture where the encoder learns to predict the representations of the momentum encoder. Through a series of updates (i.e. EMA), where the momentum encoder gradually assimilates the encoder's weights, BYOL effectively learns robust representations. The success of BYOL depends not only on the EMA, but also on its additional projector and the BN in the projector to avoid a complete collapse of the encoder. This method challenges the conventional wisdom that contrastive learning requires negative pairs, opening new avenues for SSL research."}, {"title": "Hyper-parameters of Pretraining and Evaluation", "content": "For each SSL method, we use the original settings of Solo-learn and LightlySSL (Da Costa et al. 2022). These settings include the network structure, loss function, training policy, and data augmentation policy. Considering that we use numerous SSL methods and that our setup is exactly the same as them, please go to their official implementation.\nFor OR, the appropriate regularization term $\\gamma$ generally depends only on the backbone used by SSL and the orthogonality regularizer (SRIP or SO) chosen. As shown in Table 9, when you want to add OR to your SSL pre-training, you simply pass the encoder into the loss function, and then you just need to set $\\gamma$ of the OR according to the backbone and regularizer you use.\nFor the classification tasks, due to computational constraints, we do not perform non-linear fine-tuning in classification tasks. Instead, we perform a linear probe or KNN to evaluate the quality of obtained representations as typically done in the literature (Huang et al. 2024, Li,"}, {"title": "Time Cost of OR", "content": "Implementing OR requires computing the OR loss in the backbone at each gradient update, we count the time overhead required by the different backbones to compute OR at one time, and we have averaged over 10 times as shown in Table A.7. In the pre-training phase, the time overhead of OR is only related to the backbone and the steps that need to be updated, IMAGENET-1k (100 epochs, batchsize 128) has a total of 62599 steps, and CIFAR-100 (1000 epochs, batchsize 256) has a total of 194999 steps. As you can see, compared to the original pre-training overhead of dozens and hundreds of hours, the additional time added by OR is very small, steadily improving SSL's performance. Notably, if we use a larger batchsize such as 4096, our time overhead will be reduced by 64 on IMAGENET-1k and 16 on CIFAR-100."}]}