{"title": "ALKAFI-LLAMA3: FINE-TUNING LLMS FOR PRECISE LEGAL\nUNDERSTANDING IN PALESTINE", "authors": ["Rabee Al-Qaesm", "Mohannad Hendi", "Banan Tantour"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet\ntheir application in the legal sector, particularly in low-resource contexts, remains limited. This\nstudy addresses the challenges of adapting LLMs to the Palestinian legal domain, where political\ninstability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning\napplications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct,\ntrained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution\nthat provides accurate and contextually relevant legal guidance. Our experiments demonstrate\npromising performance on various query types, ranging from yes / no questions and narrative\nexplanations to complex legal differentiations, while highlighting areas for improvement, such as\nhandling calculation-based inquiries and structured list formatting. This work provides a pathway\nfor the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained\nenvironments.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have gained significant attention over the past few years, particularly following the\nemergence of ChatGPT, both from researchers Movva et al. [2024] and in terms of their adoption in the private sector.\nThis hype has revolutionized how we use AI in different fields and helped redefine how various domains utilize AI,\nsuch as in medicine Alghamdi and Mostafa [2024], Yuan et al. [2024], finance Xie et al. [2024], Malaysha et al. [2024],\nand even agriculture Gupta et al. [2024]. These advancements demonstrate the profound potential of AI to transform\nindustries, driving innovation and efficiency in ways previously unimaginable. However, one domain that still has room\nfor growth and the potential to bring about significant change is the legal domain Martin et al. [2024], Maree et al.\n[2024].\nAlthough sectors such as healthcare and finance have rapidly adopted AI to address their unique challenges, the\nlegal industry has been relatively slow to embrace these technologies Legg and Bell [2020]. The complex nature of\nlegal language, coupled with jurisdictional variations and the high stakes involved, and the lack of AI regulations\nde Almeida et al. [2021], Nadjia [2024], has presented significant obstacles to developing effective AI-powered legal"}, {"title": "Problem statement", "content": "Over the years, the Palestinian legal system has undergone numerous changes due to political instability and prolonged\noccupation. As a result, legal frameworks lack consistency and clarity, leaving many people struggling to understand\ntheir rights and obligations. This situation is further complicated by citizens' general lack of legal knowledge and the\nabsence of accessible, 24/7 legal consultation services. These issues create significant barriers for people seeking timely\nand reliable legal guidance, often leaving them without the support to navigate their legal concerns effectively.\nAn AI-powered legal support chatbot can help bridge these gaps by providing accessible assistance around the clock,\nempowering citizens to address their legal issues confidently and clearly."}, {"title": "Objectives of the Study", "content": "This paper presents a solution for adapting AI technologies, such as large language models (LLMs), to the legal domain.\nWe also explore how such solutions can be developed in countries facing significant AI integration challenges. These\nchallenges include a lack of funding for AI-based projects, limited access to the computational resources required\nfor training such models Omar et al. [2024], and the absence of AI regulations in these regions Khan et al. [2024],\nMelkamu [2025]. We demonstrate the feasibility of leveraging smaller models that can be trained and fine-tuned on\nlocal machines while utilizing the capabilities of existing LLMs to generate data tailored to their specific problems.\nThis approach facilitates adapting AI applications to process official documents, making them accessible to the public\nand addressing these countries' unique constraints."}, {"title": "Related Work", "content": "Many studies have explored the complexities of domain-specific training and fine-tuning, mainly aimed at improving\nthe effectiveness of specialized datasets in the legal field. A notable example is SaulLM-7B Colombo et al. [2024],\nwhich marks a groundbreaking step forward as a large language model designed to comprehend and generate legal texts.\nThis model is built on the robust Mistral 7B architecture. It harnesses an impressive corpus of more than 30 billion\nEnglish legal tokens, enabling it to deliver state-of-the-art performance in processing and interpreting complex legal\ndocuments. Furthermore, the research introduces an innovative instructional fine-tuning strategy that further bolsters\nSaulLM-7B's aptitude for various legal tasks.\nIn a similar vein, Juru Junior et al. [2024] concentrates specifically on Brazilian legal texts to minimize the computational\ncosts associated with pre-training. This model utilizes a collection of 1.9 billion unique tokens from credible legal\nreferences, illustrating the advantages of domain specialization even when relying on a smaller pool of pretraining\ndata. While this approach can yield effective results within its specific area, it may sacrifice performance in broader\nknowledge domains.\nInternLM-Law Fei et al. [2024] takes a targeted approach to tackle the challenges posed by Chinese legal inquiries. The\nmodel is trained on an extensive dataset comprising over 1 million legal queries. It employs a two-stage fine-tuning\nprocess, beginning with an initial phase incorporating legal and general-purpose content, followed by a focused fine-\ntuning stage on high-quality legal data. This methodology has achieved state-of-the-art outcomes on the LawBench\nbenchmark, demonstrating superior performance compared to GPT-4 across various subtasks. This highlights the\nadvantages of employing tailored training strategies for specialized legal applications."}, {"title": "Data", "content": "Laws are often enacted to address and regulate various situations that arise to meet human needs. Legal systems can\ndiffer significantly from country to country, aligning with the specific requirements of each state's legislation and\npolitical orientation. This variation in regulations reflects each country's unique circumstances, priorities, and cultural\nvalues Robinson [1997]. Given the diverse nature of legal systems, it is essential to understand the specificities of\neach one, especially when creating contextually relevant datasets. Due to the lack of publicly available legal datasets,\nwe created our own, which was vital for effectively fine-tuning our model for accurate legal interpretation within the\nPalestinian context."}, {"title": "Legal Documents", "content": "In this study, we decided to work with the basic laws adopted within the hierarchy of legislation. These laws are the\nfoundation for regulating legal matters and based on them, the regulations and bylaws that interpret these laws are\nestablished and implemented within the Palestinian framework.\nWe gathered the original law texts used to build the synthetic dataset from the Official Gazette Bureau's website,\nthe government's official source for publishing new laws, amendments, government decisions, and other important\nannouncements. We extracted 1,277 text files from this resource, including applicable laws, repealed laws, and\namendments to specific laws. We included repealed laws to enlarge the dataset and help the LLM understand Palestine's\nlegal language, including its terminology, phrasing, and structure, ensuring accurate understanding and interpretation.\nFollowing this, we cleaned the text Liu et al. [2021], Tabassum and Patil [2020] by removing Unicode characters,\nbroken lines of text, repeated characters, multiple new lines, dashes, and any extra spaces before and after the text,\nthough further cleaning steps could still be applied."}, {"title": "Synthetic data creation", "content": "Each text file was converted into a structured JSON file to facilitate faster document access. These JSON files contain\nthe article number and corresponding legal text, enabling efficient and organized retrieval.\nTo create question-and-answer pairs for instruct fine-tuning, we used the ChatGPT API and Gemma API to generate\nquestions and answers based on the legal article text. We designed the prompt, as presented in Table 1, to guide the\nmodel to generate the pairs in a format that would be easy to integrate with the original JSON files Wu et al. [2024]. We\nalso used a one-shot learning method Kim and Lu [2024], Nfaoui and Elfaik [2024], Isaradech et al. [2024] to leverage\nthe model to understand the intended output.\nThis method ensured the consistent generation of question-and-answer pairs tailored to the legal text. It helped establish\na robust dataset to support instruct fine-tuning for legal language tasks. Additionally, it streamlined the data generation\nprocess and offered a scalable approach for extending similar datasets in future work."}, {"title": "Analysis of Dataset Characteristics", "content": "We initially analyzed the question-answer dataset to understand the generated synthetic data better. The dataset\ncontains 243,841 records, each comprising three components: the system message (which includes the legal article and\ninstructions), the user question, and the assistant's answer.\nUsing the split function, we calculated the total number of words generated, which amounts to approximately 5 million\nwords, with a vocabulary size of 208,835 unique words.\nNext, we analyzed the length of each record for deeper insights into the dataset. The shortest record contains 95 tokens,\nwhile the longest spans 2,008 tokens. The median record length is 184 tokens, with 25% of the records having fewer\nthan 157 tokens and 75% containing fewer than 225 tokens. On average, the records have a length of approximately\n210 tokens. Notably, 90% of the dataset consists of records with fewer than 299 tokens. This indicates that while most\nrecords are concise, a few longer ones significantly exceed the average length, as illustrated in the accompanying graph\n1."}, {"title": "Data Availability", "content": "One of the main challenges we faced while conducting this research was the lack of available datasets related to the legal\ndomain, especially in Arab countries. For this reason, we have made the dataset created for this research available for\nfree on Hugging Face 1. We hope this dataset can help boost research in the legal field in Arab countries and Palestine."}, {"title": "Methodology", "content": "There are numerous open-source LLMs available that can be utilized in our case, ranging from models with one billion\nparameters, such as those mentioned in Dubey et al. [2024], to larger models with up to 180 billion parameters, like\nFalcon Almazrouei et al. [2023]. We decided to use Unsloth's pre-quantized 4bit Llama-3.2-1B-Instruct model\n[unsloth Llama-3.2-1B-Instruct-bnb-4bit] for the following reasons:\n1. Their strong performance in the Arabic language across various benchmarks Khondaker et al. [2024].\n2. Their ability to run efficiently on small GPUs, allows us to operate the model locally on our machines without\nrelying on cloud services.\n3. The quantized version of the model reduces memory usage by approximately 70%, as demonstrated using\nUnsloth.\nThese factors make the Llama models with one billion parameters a compelling choice for our project, as they strike an\noptimal balance between performance, resource efficiency, and accessibility."}, {"title": "Fine-Tuning Llama for Palestinian Legal Context", "content": "Even though most generic models Touvron et al. [2023], Jiang et al. [2023], Achiam et al. [2023] rarely encounter\nArabic legal data, their responses are often unsatisfactory concerning Palestinian law. They tend to provide answers\nbased on laws from other Arabic countries, making it difficult to deliver accurate responses unless we use the Retrieval-\nAugmented Generation (RAG) approach. One of the main approaches to enhance these LLMs to have a more accurate\nanswer is to fine-tune Jeong [2024] these models with domain-specific data, in our case, the Palestinian law."}, {"title": "Implementation Details", "content": "Our code builds upon various open-source frameworks, primarily utilizing Unsloth. This framework leverages Flash\nAttention from Xformers to optimize transformer-based model training and inference, enabling efficient execution on\nlocal machines. It is developed using PyTorch, and all fine-tuned models will be made available on the Hugging Face\nHub for accessibility and collaboration."}, {"title": "Compute", "content": "Our experiments were conducted locally on a system equipped with an x86_64 architecture, featuring an 11th Gen\nIntel\u00ae Core\u2122\u043c \u04565-11600K processor running at 3.90GHz, 16 GB of RAM, and an NVIDIA GeForce RTX 3060 Ti\nHash Rate GPU with 8 GB of VRAM. The operating system used was Ubuntu 22.04.5 LTS. This locally hosted\nsetup provided a robust and efficient environment for training and fine-tuning transformer-based models, enabling the\nseamless execution of our experiments."}, {"title": "Enhancing Legal Instruction Adherence", "content": "Since our intention for this model is to use it in the future to support Retrieval-Augmented Generation (RAG) applications\nfor answering legal questions for Palestinian citizens, we decided to fine-tune the Llama instruct model to simulate how\nRAG works. Specifically, the model is designed to answer questions directly from legal articles provided during the\nfine-tuning process, mimicking how lawyers respond to people's inquiries based on legal texts."}, {"title": "Fine-Tuning Arguments", "content": "For the fine-tuning, we ran the model for 10 epochs with a learning rate (LR) of 2 \u00d7 10\u20136 with a linear learning rate\nscheduler and a 10% warmup ratio to prevent overfitting and complement the existing learned representations in the\npre-trained model Wen et al. [2024], Singh Kalra and Barkeshli [2024]. We employed the Adam optimizer with 8-bit\nprecision for efficiency and LoRA with a rank of 64 for parameter-efficient training. Finally, we set the batch size to\none to ensure the model could run without memory problems."}, {"title": "Results", "content": "The fine-tuning process for the model was conducted over 10 days. During this period, the loss function was used to\nevaluate the performance during the training and evaluation phases. As shown in graph 3 and graph 4, the training loss\ndemonstrates a decrease over the epochs. In the initial stages, the model struggled to learn, resulting in significant\nspikes in the graph. However, the model stabilized despite minor fluctuations in the later stages, particularly from the\nsecond epoch onward. Ultimately, a loss value of 0.33 was achieved."}, {"title": "Experiments", "content": "To experiment with the model's understanding of legal knowledge in the Palestinian context, we asked our domain expert\nto pose different questions leading to various categories of answers, including Yes/No Answers, Narrative/Explanatory\nAnswers, List-Based Answers, Conditional Answers, and Calculation Answers. This categorization helps ensure a\ncomprehensive analysis of the model's performance across varying scenarios, ranging from narrative explanations to\ncalculations. Each category reflects a unique dimension of legal inquiry."}, {"title": "Yes/No Answers", "content": "These responses address legal or procedural questions, typically beginning with \"Yes\" or \"No,\" and often include a brief\njustification or reference to applicable laws or regulations. For example, in Graph 6, when we asked the model, \"Can a\nprofessional doctor discuss my medical condition with anyone after I have completed my treatment with them?\" the\nmodel, referencing the Palestinian Evidence Law No. 4 of 2001, answered \"No.\" While the response was correct, it can\nbe noted that the model repeated part of the last sentence unnecessarily."}, {"title": "Narrative/Explanatory Answers", "content": "This test evaluated the model's capacity to generate detailed explanations, legal interpretations, or scenario descriptions\nrather than providing simple yes/no answers or calculations. Graph 7 shows the model's response when we asked it\nabout the period in which the model gave the correct answer for the text and mentioned the full article text."}, {"title": "Calculation Answers", "content": "Some articles in various laws include equations to calculate specific matters, such as financial entitlements for workers\nupon resignation, determining allowances for government employees and even formulas for calculating the number of\nyears of imprisonment for committing a crime.\nIn this context, we wanted to test the ability to find the exact result using the equation provided in Article 42 of the\nPalestinian Labor Law No. (7) of 2000, which outlines financial entitlements upon resignation. This article states that\nwhen employees resign, they receive one-third of their annual salary for each year worked if they have been employed\nfor less than five years.\nTo test this, we posed the following question: \"I am an employee working at Rabee Al-Qasem General Trading Company,\nand my salary is 5,000 shekels. I have worked for three years. How much are my entitlements?\" Unfortunately, the\nmodel gave an incorrect answer, applying the wrong equation. This outcome highlights the need for more data on\nsimilar situations to help the model understand these questions accurately."}, {"title": "List-Based Answers", "content": "Many articles include conditions and enumerated points, such as the conditions required to register a company or a\nmember's qualifications to join a cooperative. We aimed to test the model's ability to answer questions that require\nlisting multiple points, items, or conditions.\nIn this test, we asked, \"What areas of work are considered the most dangerous?\" The expected answer was a list of\ndangerous areas. While the model was able to identify and provide the relevant areas, it failed to format the response as\na proper list. Instead, it presented the items in a single paragraph without numbering or bullet points. This behavior is\nillustrated in Figure 9."}, {"title": "Comparative or Differentiation Answers", "content": "Finally, one of the most important aspects of model prediction is to help distinguish between two things and highlight\nthe differences. To test this, we asked the model: \"What is the difference between the testimony of one person and\nthat of a group of people?\" The model provided a very good, simple, yet clear answer: \"According to Article 1735\nof the Ottoman Code of Civil Law, the testimony of a single person is generally valid but may be prone to deception\ndue to individual reasoning. On the other hand, testimony from a group of people is less likely to involve falsehood or\ndeception.\""}, {"title": "Conclusion", "content": "This study presents a pioneering approach to adapting large language models (LLMs) for the Palestinian legal domain.\nBy fine-tuning a quantized version of the Llama-3.2-1B-Instruct model on a synthetic dataset derived from Palestinian\nlegal texts, we demonstrated the feasibility of using resource-efficient models to address the unique challenges of\nlow-resource languages and specialized legal domains. Our results highlight the potential of fine-tuned LLMs to provide\naccurate legal guidance, overcoming barriers such as limited data availability and computational resources. While the\nmodel successfully answered various legal queries, the evaluation also revealed areas where performance could be\nfurther enhanced, particularly for complex scenarios like calculations and structured list-based answers."}]}