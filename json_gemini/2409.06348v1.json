{"title": "VoiceWukong: Benchmarking Deepfake Voice Detection", "authors": ["Ziwei Yan", "Yanjie Zhao", "Haoyu Wang"], "abstract": "With the rapid advancement of technologies like text-to-speech (TTS) and voice conversion (VC), detecting deepfake voices has become increasingly crucial. However, both academia and industry lack a comprehensive and intuitive benchmark for evaluating detectors. Existing datasets are limited in language diversity and lack many manipulations encountered in real-world production environments.\nTo fill this gap, we propose VoiceWukong, a benchmark designed to evaluate the performance of deepfake voice detectors. To build the dataset, we first collected deepfake voices generated by 19 advanced and widely recognized commercial tools and 15 open-source tools. We then created 38 data variants covering six types of manipulations, constructing the evaluation dataset for deepfake voice detection. VoiceWukong thus includes 265,200 English and 148,200 Chinese deepfake voice samples. Using VoiceWukong, we evaluated 12 state-of-the-art detectors. AASIST2 achieved the best equal error rate (EER) of 13.50%, while all others exceeded 20%. Our findings reveal that these detectors face significant challenges in real-world applications, with dramatically declining performance. In addition, we conducted a user study with more than 300 participants. The results are compared with the performance of the 12 detectors and a multimodel large language model (MLLM), i.e., Qwen2-Audio, where different detectors and humans exhibit varying identification capabilities for deepfake voices at different deception levels, while the LALM demonstrates no detection ability at all. Furthermore, we provide a leaderboard for deepfake voice detection, publicly available at https://voicewukong.github.io.", "sections": [{"title": "1 Introduction", "content": "The rapid development of technologies such as TTS (Text-to-Speech) and VC (Voice Conversion) has brought great convenience to people in areas like entertainment and accessibility services. However, it is a double-edged sword. Illegal actors may exploit deepfake voices for various criminal activities. For example, in 2019, criminals used AI-based software to impersonate the voice of a U.K.-based energy firm's chief executive and requested a fraudulent transfer of \u20ac220,000 [3].\nTo counter the growing threats posed by deepfake voice technology, researchers are actively developing deepfake voice detection methods and creating open-source datasets for evaluation. For instance, traditional pipeline detection methods like LFCC-LCNN [69], the one-class learning-based detection method [79], emerging end-to-end detection models such as AASIST [30] and RawNet2 [59], as well as open-source deepfake voice datasets like ASVspoof [63, 73], have all garnered widespread attention.\nUnfortunately, academic deepfake voice detection methods often excel on specific datasets but fall short in real-world scenarios [47]. The rise of commercial tools and the latest generative models has produced increasingly convincing synthetic voices, outpacing current detection capabilities [49]. A key issue is the reliance on outdated or generic datasets for evaluation, which fail to reflect the sophistication of modern deepfake technologies. In practical applications, detectors struggle with poor generalization to unknown attacks and lack large-scale in-the-wild datasets [76]. Additionally, most methods focus solely on original content, overlooking the impact of post-processing manipulations (such as noise injection) on detection accuracy [72]. Given these challenges, there is a pressing need for a comprehensive benchmark to objectively evaluate various detection methods, thereby bridging the gap between academic research and real-world applications.\nTo address this gap, we introduce VoiceWukong, a comprehensive deepfake voice detection benchmark that incorporates various voice manipulations. VoiceWukong focuses on English and Chinese, the two most widely spoken languages globally, and features voices synthesized by advanced commercial tools and open-source models. We evaluated 12 state-of-the-art detection models, visually presenting their performance differences. Our fine-grained analysis of detector performance across different manipulations reveals potential avenues for optimization and improvement. Recognizing that humans are the primary targets of deepfakes, we conducted a user study involving over 300 participants. Based on the results, we classified deepfake voices into three levels. We then analyzed the performance of the detectors at each level, comparing the detection capabilities of users versus automated systems for these synthesized voices.\nOur main contributions can be summarized as follows:\n\u2022 A dataset addressing gaps. Our dataset encompasses both English and Chinese languages, leveraging 19 advanced commercial tools and 15 open-source models. Through six types of manipulations, it has accumulated 38 data variants, resulting in a total of 265,200 English and 148,200 Chinese deepfake voice samples. To our knowledge, this dataset is the first to extensively incorporate manipulation variants and compile the largest collection of commercially generated voice samples.\n\u2022 A comprehensive benchmark. We evaluated 12 advanced deepfake voice detectors using VoiceWukong. Results show that most detectors have an equal error rate (EER) above 20%, with three detectors exhibiting random performance on either the Chinese or English dataset. AASIST2 [60] achieved the best EER (13.50% for English and 13.54% for Chinese), yet this falls significantly short of the 0.82% EER reported on its original evaluation dataset, underscoring the challenges these detectors face in real-world applications. We further compared detector performance and conducted a fine-grained analysis to identify specific manipulations that cause performance degradation for each detector.\n\u2022 A large-scale user study. We conducted a user study involving over 300 participants to categorize deepfake voices into three levels of increasing difficulty (levels 0-2) based on their actual deception effectiveness. We then evaluated the performance of the 12 detectors and a multimodel large language model (MLLM), Qwen2-audio [14], across these levels. Results show that humans have false acceptance rates (FARs) of 18.97% for level 0 deepfakes in English and 4.20% in Chinese, outperforming most detectors. For level 2 deepfake voices, human FARs exceed 82% in both languages, falling behind most detectors. Qwen2-audio has an F1-Score of zero on the English dataset, indicating its inability to detect deepfake voices. We also examined human-focused features in deepfake voice detection to enhance detector-human collaboration in identifying synthetic audio."}, {"title": "2 Background and Related Work", "content": "Deepfake voice detectors primarily fall into two categories: traditional pipeline detectors and the increasingly researched end-to-end detectors [76]. The pipeline consists of a frontend feature extractor and a backend classifier. The classifier determines authenticity based on the features extracted by the feature extractor. Common features include spectral features represented by mel frequency cepstral coefficient (MFCC) [9], linear frequency cepstral coefficients (LFCC) [62], constant-Q transform (CQT) [11]; supervised embedding features [48]; and self-supervised embedding features represented by Wav2vec based features [60], XLS-R [5] based features [45]. Moreover, researchers have also attempted to explore some non-traditional features. Wang et al. [67] analyzed pop noise from close microphone speaking to detect deepfake voices. Doan et al. [19] detected deepfakes by evaluating the correlation between breathing, talking (speaking), and silence sounds. Common backend classifiers include traditional classifiers represented by GMM-based classifiers [16], and deep learning classifiers represented by ResNet [27] based classifiers [64], Res2Net [23] based classifiers [36], and DARTS [40] based classifiers [24].\nEnd-to-end detectors have also garnered widespread attention in the research community. RawNet2 [31] is a neural network designed for end-to-end speech recognition and speaker verification that directly processes raw audio waveforms. Tak et al. [59] were the first to apply RawNet2 to anti-spoofing. Wang et al. [70] proposed a joint optimization approach based on the weighted additive angular margin loss to extend and optimize the RawNet2 based deepfake voice detector. Tak et al. [57] leveraged the merit of graph attention networks (GATs) to learn the relationships between cues located in different sub-bands or different temporal intervals [61], proposing RawGAT-ST, which achieved excellent performance on ASVspoof2019. RawGAT-ST uses a pair of parallel graphs to simultaneously model temporal and spectral information, then combines them with element-wise multiplication. Jung et al. [30] proposed integrating these two heterogeneous graphs with heterogeneity-aware techniques and developed the AASIST, which achieved superior performance on ASVspoof2019.\nUnfortunately, most existing detectors are limited to pursuing performance on a single dataset, neglecting many challenges encountered in real-world applications. Ba et al. [4] highlighted these limitations in cross-language detection and proposed adaptation strategies. Zhang et al. [78] pointed out the insufficiency of models in adapting to unknown new attacks and conducted research on continual learning in deepfake voice detection. Wang et al. [68] and Wu et al. [72] considered the impact of manipulations on detector robustness, an aspect that most people have not taken into account."}, {"title": "2.1 Deepfake Voice Detection", "content": "Deepfake voice detectors primarily fall into two categories: traditional pipeline detectors and the increasingly researched end-to-end detectors [76]. The pipeline consists of a frontend feature extractor and a backend classifier. The classifier determines authenticity based on the features extracted by the feature extractor. Common features include spectral features represented by mel frequency cepstral coefficient (MFCC) [9], linear frequency cepstral coefficients (LFCC) [62], constant-Q transform (CQT) [11]; supervised embedding features [48]; and self-supervised embedding features represented by Wav2vec based features [60], XLS-R [5] based features [45]. Moreover, researchers have also attempted to explore some non-traditional features. Wang et al. [67] analyzed pop noise from close microphone speaking to detect deepfake voices. Doan et al. [19] detected deep-fakes by evaluating the correlation between breathing, talking (speaking), and silence sounds. Common backend classifiers include traditional classifiers represented by GMM-based classifiers [16], and deep learning classifiers represented by ResNet [27] based classifiers [64], Res2Net [23] based classifiers [36], and DARTS [40] based classifiers [24].\nEnd-to-end detectors have also garnered widespread attention in the research community. RawNet2 [31] is a neural network designed for end-to-end speech recognition and speaker verification that directly processes raw audio waveforms. Tak et al. [59] were the first to apply RawNet2 to anti-spoofing. Wang et al. [70] proposed a joint optimization approach based on the weighted additive angular margin loss to extend and optimize the RawNet2 based deepfake voice detector. Tak et al. [57] leveraged the merit of graph attention networks (GATs) to learn the relationships between cues located in different sub-bands or different temporal intervals [61], proposing RawGAT-ST, which achieved excellent performance on ASVspoof2019. RawGAT-ST uses a pair of parallel graphs to simultaneously model temporal and spectral information, then combines them with element-wise multiplication. Jung et al. [30] proposed integrating these two heterogeneous graphs with heterogeneity-aware techniques and developed the AASIST, which achieved superior performance on ASVspoof2019.\nUnfortunately, most existing detectors are limited to pursuing performance on a single dataset, neglecting many challenges encountered in real-world applications. Ba et al. [4] highlighted these limitations in cross-language detection and proposed adaptation strategies. Zhang et al. [78] pointed out the insufficiency of models in adapting to unknown new attacks and conducted research on continual learning in deepfake voice detection. Wang et al. [68] and Wu et al. [72] considered the impact of manipulations on detector robustness, an aspect that most people have not taken into account."}, {"title": "2.2 Benchmarks and Datasets", "content": "Benchmarks. Recent research in deepfake detection has primarily focused on benchmarking face detection methods. Notable contributions include the CDDB benchmark by Li et al. [37], which simulates real-world scenarios, and the comprehensive evaluation by Deng et al. [17] using multiple generation methods and detection metrics. Pei et al. [50] provided a thorough survey and evaluation of deepfake face generation and detection techniques across various datasets and sub-fields. In the voice domain, Zang et al. [77] introduced CtrSVDD, a large-scale benchmark for detecting singing voice synthesis models. To the best of our knowledge, VoiceWukong is the first comprehensive and in-depth benchmark focusing on deepfake voice detection.\nDatasets. The commonly used evaluation datasets for deepfake voice detectors include FoR [53], ASVspoof2019 [63], WaveFake [21], ASVspoof2021 [73], ADD2022 [74], and In-the-Wild [47], as shown in Table 1. ASVspoof2019 is constructed for automatic speaker verification (ASV) tasks and includes a replay subset (ASVspoof2019-PA). It has received a lot of attention in the field of deepfake detection (DD). ASVspoof2021 builds upon ASVspoof2019 by adding a section specifically for deepfake detection (ASVspoof2021-DF). Only WaveFake is constructed across multiple languages, but it does not focus on the most widely used languages. In-the-wild has a limited scope, focusing only on deepfake voices of celebrities and politicians. FoR is derived from seven open-source and commercial methods. Only a few datasets include manipulations like noise (ASVspoof2021, ADD2022, In-the-wild) and replay attacks (ASVspoof2019, ASVspoof2021). Overall, our dataset offers the broadest coverage of commercial tools, encompasses the widest range of voice manipulation variants, and targets the most representative languages."}, {"title": "2.3 Threat Model", "content": "The threat model in this study focuses on the malicious use of deepfake voice technology such as fraud and impersonation. Adversaries are assumed to have access to advanced commercial and open-source voice synthesis tools, enabling them to generate highly convincing synthetic voices in English and Chinese, and employ post-processing techniques to enhance realism and evade detection. The rapid advancement of voice synthesis technology presents a growing threat, often outpacing the development of detectors [49]. Current academic detectors may fail in the real world due to poor generalization and outdated datasets, creating a gap between lab results and practical effectiveness against sophisticated deepfake voices. Our goal is to bridge this gap by providing a comprehensive benchmark that reflects real-world threats, evaluates state-of-the-art detection methods, and incorporates human perception in assessing deepfake voice detection effectiveness."}, {"title": "3 Benchmark Construction", "content": "In this section, we introduce the construction process of VoiceWukong, as illustrated in Figure 1. \u00a7 3.1 introduces the dataset construction process. \u00a7 3.2 presents our unified training and evaluation of detectors. \u00a7 3.3 details our large-scale user study. Finally, \u00a7 3.4 discusses the evaluation metrics used in VoiceWukong."}, {"title": "3.1 Dataset Construction", "content": null}, {"title": "3.1.1 Voice Collection", "content": "Generation Methods. Since TTS and VC are mainstream methods for generating deepfake voices [34], our dataset focuses on these two types. We collected 15 open-source generation models that are either prominent in the research field or have the highest star ratings on GitHub. Given that adversaries might use commercial tools to synthesize deepfake voices in real-world scenarios, we additionally collected 19 such tools capable of generating deepfake voices for research purposes and paid the necessary fees for their use. We carefully examined the terms of service for each commercial tool to confirm their permissibility for research purposes. VoiceWukong is a non-commercial resource, thereby safeguarding against any potential infringement of intellectual property rights. To our knowledge, our dataset involves the most extensive range of commercial tools. The 34 methods (29 for TTS and 5 for VC) are listed in Table 7 in the Appendix, all supporting English and 19 supporting Chinese.\nGeneration Process. VoiceWukong encompasses English and Chinese, the two most widely used languages globally. Deepfake voice generation utilizes the English VCTK [65] and Chinese MAGICDATA [44] datasets. VCTK is widely used in voice cloning and conversion research, featuring diverse English textual content. MAGICDATA is a large-scale Mandarin speech dataset with extensive read speech data, covering domains like news, dialogues, and question-answering. After deduplication and length filtering (5-30 words for English, 5-35 characters for Chinese), we selected 100 sentences each from VTCK and MAGICDATA for fixed-text generation, and extracted 3,400 English and 1,900 Chinese sentences for random-text generation. For each method, we produced 200 deepfake voices per supported language: 100 with fixed text and 100 with random text. Methods with 10 or more speakers yielded 10 fixed-text and 10 random-text voices per speaker from 10 speakers. For methods with fewer speakers, we distributed the 200 voices evenly among available speakers. Generation was automated for open-source models and API-based tools, while web-based tools required manual input. For the five open-source VC models, we provided corresponding original voices. In total, we collected 6,800 English deepfake voices across 34 methods and 3,800 Chinese deepfake voices across 19 Chinese-supporting methods.\nReal Voice Collection. We also collected real voices from the VCTK and MAGICDATA datasets. To ensure data balance, we randomly selected text content not present in any deepfake voices and collected an equal number of real voices: 6,800 from VCTK and 3,800 from MAGICDATA."}, {"title": "3.1.2 Data Standardization", "content": "Inspired by previous research [53], the diversity of our dataset sources necessitates data standardization to eliminate biases. We implemented a standardized preprocessing pipeline for all voice samples, which included converting files to WAV format, resampling to 22,050 Hz, transforming to Mono channel, trimming silent segments from the beginning and end, and normalizing the volume to 0 dBFS.\nFormat Unification. Our collection methods include various open-source models and commercial tools, resulting in non-uniform audio formats, mainly WAV and MP3. To eliminate format bias, we used pydub\u00b9 to convert all audio files to WAV format, as it supports conversion between multiple formats with minimal loss.\nSample Rate Standardization. Regarding sample rates, the voices from different sources also vary. To evaluate the impact of sample rate changes on detectors, we standardized all data to a sample rate of 22,050 Hz. This rate captures frequencies up to 11,025 Hz, which is sufficient for the main spectral range of human speech (300 Hz to 3,400 Hz). It provides adequate frequency resolution while saving space and improving processing efficiency compared to higher rates like 44,100 Hz and 48,000 Hz. We used the commonly applied librosa2 for resampling all voice files to 22,050 Hz.\nMono Channel Conversion. Since the VCTK dataset uses two microphones for recording, we consistently used its data from Microphone 2. Additionally, the channel settings vary across different audio sources. To avoid the impact of different channels on the detectors, we used librosa to convert all voice files to Mono, eliminating bias.\nSilence Removal. Real speech recordings often include silent segments at the beginning and end, which speech synthesis methods may not always replicate. To eliminate bias, we removed these silent parts from all voice files. A Python script was used to calculate the smoothed energy envelope of each voice, removing segments below the 20th percentile at the beginning and end.\nVolume Normalization to 0 dBFS. When collecting voices from various commercial tools, some offer volume adjustment features while others do not. Moreover, the volume levels of voices generated by open-source models may vary. To eliminate the impact of volume differences on detection results, we normalize all voices to 0 dBFS."}, {"title": "3.1.3 Data Variant Generation", "content": "To cover various real-world scenarios that detectors might encounter, we applied additional manipulations to the standardized datasets. These manipulations reflect basic techniques malicious actors could use on deepfake voices and include six types: noise injection (NI), volume control (VC), time stretching (TS), sample rate changes (SR), replay (RE), and fade in & out (FD) effects. Except for replay, each type underwent fine-grained manipulations to create multiple variants.\nNoise Injection [15,53]. ESC-50 [51] is a widely recognized environmental sound classification dataset that divides sounds into five categories: animal sounds, natural soundscapes and water sounds, human (non-speech) sounds, interior/domestic sounds, and exterior/urban noises, each containing 400 five-second recordings. For each voice in our standardized dataset, we randomly selected a noise audio from each category and injected noise at Signal-to-Noise Ratios (SNRs) of 15 dB, 20 dB, and 25 dB. We also added Gaussian white noise to each voice using the same SNR settings.\nVolume Control [71]. To examine if varying volume levels affect deepfake voice detection, we applied multiple volume adjustments to the standardized dataset. Since we were unaware of the adjustment outcomes, we conducted listening tests to ensure the voice content remained clearly audible. We set the lowest volume at 0.5 times and the highest at 1.5 times the standardized dataset, with intermediate levels at 0.75 and 1.25 times the original volume.\nTime Stretching [20]. Time stretching is a technique that changes the playback speed of audio without altering its other attributes, effectively adjusting the speech rate. We applied time stretching to the standardized dataset at 0.8, 0.9, 1.1, and 1.2 times the original speed to examine the impact of accelerated and decelerated playback on deepfake voice detection.\nVoice Resampling. Generally, a higher sample rate [33] provides greater detail and quality in voice, making it sound more realistic and natural. However, higher sample rates also increase storage requirements. Therefore, generation tools often balance sample rates based on their needs. Lower sample rates lose some high-frequency information, potentially impacting detectors that rely on this data. We previously set 22,050 Hz as the sample rate for our standardized dataset and further resampled it at higher rates of 32,000 Hz and 44,100 Hz.\nReplay [71]. Replay attacks [66] have long been exploited as a simple yet highly challenging method to test system robustness. To assess the robustness of different detectors against replay attacks, we re-recorded the standardized dataset. We played all voices at maximum volume on a Lenovo Xiaoxin Pro14 2023 laptop and recorded them simultaneously with a Deli 14870 omnidirectional microphone placed 1 meter away at the same height. The recordings were conducted in a quiet, unoccupied indoor environment.\nFade In & Out. Fade in and fade out are common voice editing techniques that create smooth transitions and more natural voice connections [42,43]. Fade in gradually increases the volume from zero at the beginning of a voice clip, while fade out gradually decreases it to zero at the end, creating a smooth transition and more natural voice connections. This is a common voice editing technique. We applied fade in & out to each voice in the standardized dataset using three fade shapes: linear, logarithmic, and exponential at ratios of 0.1, 0.2, and 0.3."}, {"title": "3.2 Evaluation Methods", "content": "We now describe the evaluation of existing deepfake voice detectors using our constructed dataset. Our detector selection process is detailed in \u00a7 4.1. All detectors are trained on the ASVspoof2019-LA dataset (if necessary). We use published pre-trained models when available, or retrain models using the original authors' hyperparameters and settings, selecting the best-performing checkpoint for evaluation. This approach preserves each detector's optimal performance as determined by its creators. During evaluation, we maintain the input sequence length specified by the original authors, rather than fixing the voice sampling duration, due to differences in sample rates between our dataset and ASVspoof2019. This ensures consistency between training and evaluation sequence lengths, enabling fair comparisons across detectors. This methodology allows us to compare each detector's performance while maintaining its peak capabilities and avoiding inconsistencies that could arise from fixed sampling durations."}, {"title": "3.3 User Study", "content": "To examine the real-world deception effectiveness of deepfake voices generated by the selected methods and manipulation variants, we conducted a user study involving 318 participants. Based on the deception performance of these voices in the study, we divided the dataset into three levels to enable further evaluation of detectors in \u00a7 5.2. This section outlines the data sampling, study design, and grading method, while ethical considerations and participant details are provided in Appendix \u00a7.1 and \u00a7.2."}, {"title": "3.3.1 Sample Dataset Construction", "content": "Given our dataset's large scale, we sampled 1% for the user study, balancing practicality with comprehensive representation. We focused on the random text portion to avoid potential bias from repeated content in the fixed text samples. As illustrated in Figure 1, our sampling process was: 1) For each subset, we randomly selected two random-text deepfake voices per generation method, ensuring unique texts across subsets. 2) We then sampled an equal number of real voices from the same subset. This approach resulted in 38 Chinese deepfake and 38 real voices (corresponding to the 19 generation methods supporting Chinese), as well as 68 English deepfake and 68 real voices (34 generation methods) per subset. With a total of 39 subsets, our final sample dataset comprised 2,964 Chinese voices and 5,304 English voices."}, {"title": "3.3.2 Study Design", "content": "Questionnaire Setup. To maintain full control over the dataset, we set up a questionnaire on our server and provided each participant with a unique link to access the survey. At the start, users were required to read instructions that explicitly directed them to use headphones to minimize environmental noise and interference. Participants could only proceed after reading, understanding, and agreeing to these instructions. Furthermore, the response time for the entire questionnaire was strictly limited. Questionnaires completed in less than 25 minutes or more than two hours were considered invalid. For each subset within the sample dataset, we randomly paired one deepfake voice with one real voice. To construct each questionnaire, we randomly selected one unused pair from each subset, resulting in 78 voices per questionnaire. This ensured that every voice from the sample dataset appeared in each questionnaire round. After one round of questionnaire generation, we produced 38 different questionnaires for the Chinese dataset and 68 for the English dataset. We conducted three rounds of data collection, ultimately generating 114 Chinese questionnaires and 204 English questionnaires.\nTasks. The user study is structured around three distinct tasks. In the first task, participants are presented with randomly selected voice samples and asked to determine their origin. As illustrated in Figure 5 in the Appendix, users listen to each voice and must categorize it as either \u201cHuman\u201d or \u201cNot human\". They are required to make a selection before proceeding and are not permitted to revise their previous choices. Upon completion of the initial assessment, the second task commences. Participants revisit each deepfake voice sample, with their earlier judgments displayed as a reminder. They are then tasked with evaluating the generation quality on a three-point scale: 1 indicates an easily detectable deepfake, 2 suggests a deepfake identifiable upon close scrutiny, and 3 represents a voice indistinguishable from a genuine human recording. Following this evaluation, participants are prompted to identify the factors that influenced their decision-making process. They can select from predefined options such as volume and background noise or provide custom responses. The third task serves as an attention check for participants. Additionally, attention tests are embedded once within both the first and second tasks. These tests require participants to listen to a specific recording and answer content-related questions. Incorrect responses result in immediate termination of the questionnaire to prevent random guessing, and these participants are subsequently disqualified from the study.\nDeepfake Voice Deception Grading. We developed a systematic approach to grade deepfake voices based on their effectiveness in deceiving human participants. The grading process for deepfake voices produced by a specific tool under particular manipulation conditions was as follows. Our study consisted of three experimental rounds (as illustrated in Figure 1), each considered a separate experiment within our\""}, {"title": "3.4 Metrics", "content": "In VoiceWukong, we employ a comprehensive set of quantitative metrics to analyze the performance of various detectors. These metrics include False Acceptance Rate (FAR), False Rejection Rate (FRR), Equal Error Rate (EER) [46], Accuracy (ACC), F1-score, and Area Under the Curve (AUC). This diverse array of metrics provides a thorough evaluation of each detector's prediction results, offering insights into different aspects of their performance.\nLet $\\theta$ be the score threshold at which the detector classifies a voice as genuine. The FRR($\\theta$) and the FAR($\\theta$) of the detector at the threshold $\\theta$ are defined as follows:\nFAR($\\theta$) = $\\frac{\\sum{\\text{fake voices with score} > \\theta}}{\\sum{\\text{fake voices}}}$\nFRR($\\theta$) = $\\frac{\\sum{\\text{real voices with score} < \\theta}}{\\sum{\\text{genuine voices}}}$\nEquation 1 and Equation 2 are respectively decreasing and increasing functions of the threshold $\\theta$. The EER is defined in Equation 3 as the error rate at the specific threshold $\\theta_{eer}$, where FAR($\\theta_{eer}$) and FRR($\\theta_{eer}$) are equal.\nEER = FAR($\\theta_{eer}$) = FRR($\\theta_{eer}$)\nTAR($\\theta$) = $\\frac{\\sum{\\text{real voices with score} > \\theta}}{\\sum{\\text{genuine voices}}}$\nThe AUC represents the area under the Receiver Operating Characteristic (ROC) curve. The ROC curve describes the trade-off between the False Acceptance Rate (FAR) and the True Acceptance Rate (TAR) (defined in Equation 4) of a detector at various decision thresholds ($\\theta$). AUC values range from 0 to 1, with higher values indicating better detector performance. An AUC close to 0.5 suggests the detector's performance is comparable to random guessing. ACC describes the detector's correct prediction rate on the dataset, as defined in Equation 5. The F1-Score reflects the detector's sensitivity to FAR and FRR. Together with ACC, it provides a more comprehensive analysis of detector performance on manipulation subsets (\u00a7 4). The definition of F1-score is given in Equation 6.\nACC($\\theta$) = $\\frac{\\sum{\\text{voices with correct score}}}{\\sum{\\text{fake voices}} + \\sum{\\text{real voices}}}$\nF1-Score($\\theta$) = $\\frac{2 \\cdot (1 - \\text{FAR}(\\theta)) \\cdot (1 - \\text{FRR}(\\theta))}{ (1 - \\text{FAR}(\\theta)) + (1 - \\text{FRR}(\\theta))}$\nIn the subsequent sections of this paper, all discussed metrics are under the condition of $\\theta$ = $\\theta_{eer}$."}, {"title": "4 Evaluation and Analysis", "content": null}, {"title": "4.1 Evaluated Detectors", "content": "We collected 12 open-source detectors that have gained significant attention in deepfake voice detection or demonstrated excellent performance in their original publications. Our focus was primarily on end-to-end detectors rather than traditional pipeline detectors, as the latter often require complex and time-intensive feature extraction processes for large-scale datasets. The selected detectors include AASIST [30] and RawNet 2 [59], used as baselines in well-known challenges [32,73-75], along with RawBoost [58], OC-Softmax [79], RawGAT-ST [57], SAMO [18], Res-TSSDNet [29], RawNet2-Vocoder [56], AASIST2 [60], Raw PC-DARTS [25] and the latest detectors RawBMamba [10] and CLAD [72]. We evaluate all detectors on our dataset as described in \u00a7 3.2."}, {"title": "4.2 Evaluation Results", "content": "We examine the overall performance of various detectors. On the English dataset, EERs range from 13.50% to 50.01%, and on the Chinese dataset, from 13.54% to 51.88%. The best-performing detector is AASIST2, with the lowest EER on both datasets: 13.50% for English and 13.54% for Chinese, while all other detectors have EERs above 20%. The worst performer on the English dataset is Res-TSSDNet (50.01% EER), while on the Chinese dataset, AASIST has the highest EER at 51.88%. Most detectors perform better on the English dataset than on the Chinese dataset, likely due to their training on English data. EER differences between datasets indicate varying adaptability in cross-lingual detection. AASIST2 has the smallest EER difference (-0.04%) between the English and Chinese datasets, while AASIST shows the largest (-23.69%). SAMO also has a significant gap, with its English EER 23.15 percentage points lower than its Chinese EER, second only to AASIST. Interestingly, OC-Softmax performs better on the Chinese dataset, with its EER 3.51 percentage points lower. Res-TSSDNet has EER values close to 50% on both datasets, indicating poor performance across languages. On the English dataset, aside from AASIST2 (best) and Res-TSSDNet (worst), most detectors have similar EERs around 25%. However,"}, {"title": "4.3 Effect of Manipulations", "content": "We analyze the detectors' performance across various manipulated subsets of our dataset, using each detector's performance on the standardized (std.) subset as a baseline. This approach helps identify specific differences under various manipulations and reveals potential optimization methods. Analysis of the std. subset performance reveals AASIST2 as the standout performer, achieving ACCs above 90% on both language datasets (Table 8 and Table 9). On the English dataset, most detectors exceed 80% ACC, with CLAD peaking at 85.25%. Only RawNet2 and AASIST fall slightly below 80%. The Chinese dataset shows a general performance decline, except for OC-Softmax. AASIST experiences the most severe drop to 48.72% ACC. Notably, CLAD, despite ranking fourth in AUC score, achieves only 68.67% ACC on the Chinese std. subset. These results align with the AUC performance evaluation in \u00a7 4.2, highlighting varied cross-lingual detection capabilities.\nEffect of Noise Injection. Figure 3 illustrates ACC differences between 15 dB noise-injected and std. subsets for the top four detectors. All detectors show ACC declines across both datasets, with some interesting exceptions. SAMO experiences a significant drop on the English dataset but a slight increase on the Chinese dataset post-noise injection. Different noise types impact detectors variably. For instance, Human noise notably affects AASIST, while Gaussian noise has the least impact. Conversely, RawBoost is most affected by Gaussian noise and least by Interior noise. The top four detectors show similar patterns of noise impact across languages. AASIST2 is least affected by noise on the English dataset, while CLAD is most resilient on the Chinese dataset. Overall, when facing noise injection, most detectors may experience a very significant performance"}]}