{"title": "Integrating Saliency Ranking and Reinforcement Learning for Enhanced Object Detection", "authors": ["Matthias Bartolo", "Dylan Seychell", "Josef Bajada"], "abstract": "With the ever-growing variety of object detection approaches, this study explores a series of experiments that combine reinforcement learning (RL)-based visual attention methods with saliency ranking techniques to investigate transparent and sustainable solutions. By integrating saliency ranking for initial bounding box prediction and subsequently applying RL techniques to refine these predictions through a finite set of actions over multiple time steps, this study aims to enhance RL object detection accuracy. Presented as a series of experiments, this research investigates the use of various image feature extraction methods and explores diverse Deep Q-Network (DQN) architectural variations for deep reinforcement learning-based localisation agent training. Additionally, we focus on optimising the detection pipeline at every step by prioritising lightweight and faster models, while also incorporating the capability to classify detected objects, a feature absent in previous RL approaches. We show that by evaluating the performance of these trained agents using the Pascal VOC 2007 dataset, faster and more optimised models were developed. Notably, the best mean Average Precision (mAP) achieved in this study was 51.4, surpassing benchmarks set by RL-based single object detectors in the literature.", "sections": [{"title": "I. INTRODUCTION", "content": "Object detection, within the area of computer vision, is a critical problem in recognising and localising objects inside an image or video [1]. At first sight, the challenge appears simple: identify objects in visual scenes. However, the difficulty stems from the sheer variation of objects in terms of size, shape, orientation, occlusion, and lighting. Furthermore, the context in which these objects occur complicates the process even more, mandating models that can generalise across varied contexts and views.\nWhile substantial progress has been made in advancing this field with technologies such as YOLOv10 [2], which utilise various computer algorithms to overcome these issues, further experimentation is required to achieve human-like competency. To this end, grasping the intricacies of human visual perception [3]-[5] and understanding how humans locate objects are crucial aspects of this effort.\nIn [6], T. Boger and T. Ullman conducted several tests to investigate individuals' capacity to locate objects. Their findings show that, regardless of object realism, judgements based on physical reasoning, namely centre of mass, are consistently the most significant element in perceived object location. Successfully showing a dependence on physical qualities to determine object location. In a broader sense, this study provides an excellent justification for investigating a fresh paradigm in the modelling of spatial perception inside visual frameworks, such as human perception in object detection [6].\nThus, this study aims to investigate whether the integration of reinforcement learning and saliency ranking techniques, which mimic human perception, can effectively address the problem of object detection. We also explore the individual components of reinforcement learning-based object detectors with the goal of developing faster and more optimised models, while also examining the accuracy of the proposed architecture in comparison to the literature. Moreover, this research investigates whether adopting a reinforcement learning framework facilitates more transparent detection by enabling monitoring of the object detection training process.\nThe first part of this paper consists of background information on methodologies employed by saliency ranking and reinforcement learning, as well as a comprehensive review of past reinforcement learning techniques for object detection. Chapter IV provides a full overview of the solution architecture and its modules. This is followed by a series of experiments, and a thorough comparative analysis in Chapter V. The paper summarises the findings in Chapter VII and discusses proposed directions for future work in Chapter VIII."}, {"title": "II. BACKGROUND", "content": "This chapter presents background knowledge that readers must familiarise themselves with before proceeding to the subsequent chapters, which served as the foundation for this endeavour. A brief synopsis of feature and classification learning, saliency ranking, and reinforcement learning is provided."}, {"title": "A. Saliency Ranking", "content": "Saliency Ranking is a fundamental process aimed at discerning the most visually significant features within an image. In [7], [8], the authors introduce a technique for separating images into segments, and ranking their saliency. Using a grid-based methodology, this technique successfully locates salient objects, that allows for automatic selection of objects without the need of training a model. SaRa operates in two separate stages. It first processes texture and, if available, depth maps with a defined grid G with a size of kxk, to generate a saliency"}, {"title": "B. Feature and Classification Learning", "content": "Feature Learning, particularly critical in image classification, entails extracting vital features from images to aid in accurate object and pattern detection, most notably through the use of CNNs [10]. CNNs accomplish this through convolutional layers, which extract features using kernels or filters, successfully detecting patterns in images by striding across and computing dot products to generate feature maps, while pooling layers down-sample these maps to reduce parameter size. These networks have revolutionised the area of computer vision by allowing feature learning without the need to train massive complicated neuron architectures streamlining the process. To this extent, popular CNN architectures designed for image classification include: ResNet50 [11], VGG16 [12], Inception [13], Xception [14], MobileNet [15], and EfficientNet [16], each characterised by its architectural design and feature learning methods. Earlier architectures include VGG16,"}, {"title": "C. Reinforcement Learning", "content": "Reinforcement Learning is the process of learning what to do-how to translate states into actions in order to maximise a numerical reward signal [20]. In RL, the learner is not taught which actions to perform but instead must experiment to determine which actions provide the highest return. Additionally, RL is distinguished by two key characteristics: reliance on trial-and-error search, and the capacity to maximise cumulative rewards over extended periods rather than seeking immediate satisfaction. The RL framework is often modelled as an interaction loop, as can be seen in Figure 2. In the loop, the RL agent performs actions in an environment based on a policy that transitions its current state to a new one while collecting rewards and observations in the process [20].\nThe policy, which is frequently represented as \u03c0, serves as a function that guides the agent's decision-making process in an environment. It maps each state st to an action At, determining the agent's movements from the current state st to the next state st+1 at a given time step t. Rt represents the numerical reward that the agent obtains as a result of each action in a state. These rewards are critical in refining the agent's policy, assisting it in determining the appropriate action to take in various states in order to achieve its ultimate goal: the optimal policy \u03c0*. Maximising the predicted cumulative reward, which is the sum of all rewards collected over multiple time steps, is required to achieve this optimum policy [21]. The Environment refers to the external system or context with which an agent interacts and learns. It encompasses everything"}, {"title": "1) Markov Decision Process", "content": "Formally, RL can be expressed as a MDP [22]. An MDP is a mathematical framework for modelling SDMP in which the present state of the system, potential actions, and probability of transitioning to the next state are known [23]. Moreover, an MDP can be modelled as the following tuple: S, A, P, R, \u03b3), whereby:\n\u2022 S is a set of possible states st \u2208 S observed by the agent at time step t.\n\u2022 A is a set of possible actions at \u2208 A that the agent can execute at time step t.\n\u2022 P is the transition dynamics p(St+1|St, at) that determine the probability of moving to st+1 after taking action at in state st.\n\u2022 R is the immediate reward function r(st, at, St+1) that provides the reward obtained after transitioning from st to st+1 through action at.\n\u2022 \u03b3 is the discount factor (\u03b3\u2208 [0,1]) that regulates the balance between immediate and future rewards, and shapes the agent's decision-making."}, {"title": "2) Deep Reinforcement Learning", "content": "Employing techniques such as bootstrapping, dynamic programming, monte carlo, temporal-difference, and tabular search, RL methods face the issue of handling and generalising large-scale data in broad environments. A solution to this problem pertains to the use of Deep Reinforcement Learning (DRL). DRL combines the concept of RL methods with deep learning, connecting their strengths to address shortcomings in classic tabular techniques. As described in [22], DRL effectively navigates away from the confines of manually built state representations by combining complex neural network topologies such as ANNs and CNNs. It excels at exploring huge and complicated search spaces, allowing RL agents to generalise their learning across different settings. Deep RL uses Neural Networks (NN) to estimate value functions or rules, as opposed to traditional tabular techniques, which suffer from scalability in contexts with exponentially vast state spaces. This in turn allows agents to learn, and take decisions in high-dimensional and continuous state spaces, enabling them to display flexible behaviour across a wide range of complex situations [22]."}, {"title": "3) Deep Q-Network", "content": "Deep Q-Network (DQN) is an extension of the Q-learning technique but replaces the tabular Q-values with a deep neural network. In Q-learning, the Q-value represents the mapping that assigns a numerical value to each state-action pair, while indicates the expected cumulative reward when taking action a in state s. Moreover, Q-learning is an off-policy temporal-difference control algorithm whereby after every time step, Q-values are updated using Equation 3.\n$Q(s, a) = Q(s, a) + \\alpha\\cdot (R + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) - Q(s, a))$ (3)\nDQN attempts to increase generalisation capabilities, particularly in cases with huge state spaces, which traditional"}, {"title": "4) Double Deep Q-Network", "content": "The Double Deep Q-Network (DDQN) algorithm is an improvement on the traditional DQN approach, designed to address overestimation difficulties in Q-learning methods and reducing maximisation bias [25]. DDQN works by separating action selection from action value estimation using two independent NN. In DDQN, the network with the highest Q-value determines the ideal action, whereas the alternate network evaluates the value of that action. The difference is apparent when comparing the target value functions in Equations 6 and 7. In these equations, Y represents the target value at time step t, used in the update rule, and composed of the immediate reward Rt+1 and a discounted"}, {"title": "5) Dueling Deep Q-Network", "content": "The Dueling Deep Q-Network (Dueling DQN) architecture presents a substantial improvement on the traditional DQN. It improves efficient learning whilst allowing the model a more precise identification of the crucial state values, and advantageous actions [22]. Moreover, Dueling DQN presents a revolutionary design that separates the estimation of state values from the assessment of each action taken, independently of the remaining state value in a specific state [26]."}, {"title": "6) Double Dueling Deep Q-Network", "content": "The Double Dueling Deep Q-Network (D3QN) architecture represents a significant advancement over both traditional DQN and Dueling DQN models, by enhancing learning efficiency and decision-making accuracy in DRL tasks [20], [21]. Through the integration of DDQN and Dueling DQN, D3QN presents a dual-network architecture that separates the estimation of state values, and action advantages. Additionally, double Q-learning techniques are utilised to reduce overestimation biases [20], [21]. This novel technique not only increases the model's capacity to discover critical state-action values, but also makes it much more resilient to sub-optimal action selection, rendering it ideal for complex and dynamic contexts."}, {"title": "III. LITERATURE REVIEW", "content": "This chapter will focus on the literature review necessary for understanding the subsequent chapters. It will identify key architectures and methodologies previously developed in reinforcement learning object detectors.\nPioneering reinforcement learning-based object detectors, and appearing around the same time RCNN was released (2015), Caicedo et al. present \u201cActive Object Localization with Deep Reinforcement Learning\u201d. In their study [27], DRL was utilised for active object localisation, whereby the object detection problem was transformed into the MDP framework, in an attempt to solve it. In their implementation, the authors took into consideration eight distinct actions (up, down, left, right, bigger, smaller, fatter, and taller) to enhance the bounding box's fit around the item and an extra action (trigger) to indicate that an object is correctly localised in the current box. The trigger also modifies the environment by using a black cross to mark the region covered by the box, which acts as an Inhibition of Return (IoR) mechanism to prevent re-attention to the currently attended region, a widely used strategy in visual attention models to suppress continuous attraction to highly salient stimuli [5]. Furthermore, in their approach, the authors tackled multiple object detection by allowing the RL agent to run for a maximum of 200 time steps, evaluating 200 regions for each given image. Caicedo et al. encoded the state as a tuple of feature vectors and action histories, while also employing shift in Intersection over Union (IoU) as a binary reward strucuture ([-1,+1]) across actions, and a subsequent reward for trigger actions. The authors also employed a guided exploration strategy based on apprenticeship learning principles [28]\u2013[30], which uses expert demonstrations to inform the agent's actions, particularly in determining positive and negative rewards based on IoU with the current bounding box, allowing the agent to choose positive actions at random during exploration.\nAn enhancement to the algorithm presented in [27] was put forth by [31], whose authors also treated the object detection problem as an MDP and employed a hierarchical technique for object detection. The RL agent's objective in their approach consisted of first identifying a Region of Interest (ROI) in the image, then slowly reducing the area of interest to obtain smaller regions, thus generating a hierarchy. Similar to [27], Bellver et al. [31] additionally represented the reward function as an IoU change in between actions. According to their paper, they utilised DQN as the agent's architecture while also employing Image-zooms and Pool45-crops with VGG16 [12] networks as the image feature extraction backbone. In addition to a memory vector that stores the last four actions, the extracted image feature vector from the aforementioned networks was incorporated into the DQN state.\nK. Simonyan et al. [32], use the sequential search technique to present a DRL-based object identification system. The model was trained by using a collection of image regions, where the agent returned fixate actions at each time step, identifying an area in the image that the actor should investigate next. The state was composed of three pieces in total: the focus history Ft, the selected evidence region history Et, and the observed region history Ht. The fixate action was composed of three elements: the image coordinate of the subsequent fixate zt, the index of the evidence region et, and the fixate action itself. As presented in their paper, K. Simonyan et al. specified the reward function to be responsive to the detection location, the confidence of the final state, and the imposition of a penalty for each region evaluation.\nAs described in [33], Z. Jie et al. proposed a tree-structured RL agent (Tree-RL) for object localisation in order to map"}, {"title": "IV. METHODOLOGY", "content": "To identify the most relevant object inside an image, this system combines a saliency ranking algorithm with reinforcement learning. It seeks to detect and identify critical aspects in visual data by integrating various methodologies. This chapter describes the implementation details of the proposed \"SaRLVision\" system architecture, explaining the design decisions and their justifications. The approach and techniques used in creating the system architecture are also described, drawing on knowledge gained from thorough research of pertinent literature. Moreover, all code utilized is made available on GitHub\u00b9.\nIn alignment with the primary goal, an RL framework was developed to achieve object localisation within images. To this extent, the developed system was built via the gymnasium\u00b2 API, which provided a platform for the problem formulation, inspired by the existing literature. Subsequently, DRL techniques were applied to approximate the object detection problem."}, {"title": "A. Reinforcement Learning", "content": "1) States: Similar to methodologies presented in several prior studies [27], [34], [43], [44], the state representation in this work comprises a tuple (o, h), where o denotes a feature vector corresponding to the observed region (current bounding box region), and h encapsulates the history of actions undertaken. Departing from the methodologies delineated in [27], [43], the evaluation of the developed system involved testing three distinct image feature extraction methods (initialised with the ImageNet weights): VGG16, ResNet50, and MobileNet, facilitated by the pytorch\u00b3 API. VGG16 and ResNet50 were selected due to their widespread adoption in the literature, while MobileNet was chosen for its lightweight architecture. However, unlike the approaches described in [34], [44], the feature maps undergo processing through a global average pooling layer, a technique utilised in both Inception [17], and Xception [14] architectures. This step serves to reduce dimensionality and expedite object detection, while facilitating the development of smaller models and preserving the essential features from the image without compromising quality through Dense layers. Notably, pre-trained models were favoured over training a convolutional DQNfrom scratch, as demonstrated in [43]. These pre-trained models had already been trained on larger datasets, and interestingly, [43] showed that utilising a pre-trained CNN achieved comparable results. Additionally, akin to the method introduced in [27], the history vector h is a binary vector encoding the ten most recent actions taken, implemented to mitigate the repetition of similar or opposing actions. Each action in the history vector is represented by a nine-dimensional binary vector. A comparative analysis of different state sizes is presented in Table I.\n2) Actions: In a manner akin to the methodologies described in [27], [34], [44], the action set A comprises of eight transformations applicable to the bounding box, alongside another action designed to terminate the search process. These transformations, illustrated in Figure 4, are categorised into four subsets: horizontal and vertical box movements, scale adjustment, and aspect ratio modification. Consequently, the agent possesses four degrees of freedom to adjust the bounding box coordinates ([x1, Y1, x2, y2]) during interactions with the environment."}, {"title": "3) Rewards", "content": "The reward function (R) quantifies the agent's progress in localising an object following a specific action, assessing improvement based on the IoU between the target object and the predicted box. This function, as described in [27], [34], [43], [44], is computed during the training phase using ground truth boxes. For an observable region with box b and ground truth box g, the IoU is calculated as shown in Equation 9.\n$IoU(b, g) = \\frac{area(b \\cap g)}{area(b \\cup g)}$ (9)\nEquation 10 defines the reward Ra(st, St+1), that is assigned to the agent when transitioning from state st to st+1, where each state s corresponds to a box b containing the attended region, and t denotes the current time step.\n$R_a (s_t, s_{t+1}) = sign (IoU (b_{t+1}, g) \u2013 IoU(b_t, g))$ (10)\nThe reward is determined by the sign of the change in IoU, encouraging positive rewards for improvements and negative rewards for reductions in IoU. This binary reward scheme Ra(St, St+1) \u2208 {\u22121,+1} applies to any action transforming the box, ensuring clarity in the agent's learning process. Moreover, to minimise redundant actions, in case the reward is O indicating no change, a negative reward is issued. The trigger action employs a distinct reward scheme due to its role in transitioning to a terminal state without altering the box, resulting in a zero differential IoU. Its reward is determined by a threshold function of IoU, expressed in Equation 11, where \u03b7 represents the trigger reward and is set to 3.0 based on experiments in [27]. The parameter \u03c4 denotes the minimum IoU required to classify the attended region as a positive detection, typically set to 0.5 for evaluation, but adjusted to 0.6 during training, in alignment with [27], to encourage better localisation. An enhancement to the pipeline in [27], [43], [44], involves multiplying the trigger reward by 2 \u00d7 current IoU to ensure a minimum reward of 3.0 (\u03b7) while allowing for variable rewards based on the final IoU, thereby providing the RL agent with additional incentive to improve prediction accuracy.\n$R_w (s_t, s_{t+1}) = \\begin{cases} +\\eta * 2 * IoU(b, g) & \\text{if } IoU(b, g) \\geq \\tau \\\\ -\\eta & \\text{otherwise} \\end{cases}$ (11)"}, {"title": "4) Episodes", "content": "In terms of episode structure, each episode is restricted to a maximum of 40 time steps, in alignment with previous research [27], [43], [44]. Additionally, applying the trigger action prematurely also results in terminating the episode. Upon completion of an episode for a particular image, the DRL agent provides a bounding box prediction for the detected object."}, {"title": "5) Network Architecture", "content": "The DQN architecture, introduced in the presented system, assumes responsibility for decision-making in object localisation. To this extent, the designed architecture draws inspiration from the methodologies outlined in [27], [44]. While [27] employs a 3-layer Dense Layer DQN representation, [44] extends this architecture by incorporating Dropout Layers. Our proposed approach, as illustrated in Figure 5 which was implemented using the pytorch API, advocates for a deeper DQN network to bolster decision-making capabilities and enhance learning complexity. To mitigate concerns regarding overfitting, Dropout Layers were integrated into the network architecture, following insights gained from the results presented in [43]. Furthermore, as shown in Figure 5, this work develops a Dueling DQN Agent to improve learning efficiency by decoupling state and advantage functions. The Dueling DQN design divides the Q-value function into two streams, allowing the agent to better comprehend the value of doing specific actions in different situations, as shown in [26]. To achieve better results, the proposed approach not only introduces Dueling DQN, a technique not explored in existing literature, but also implements DDQN and D3QN methods, both of which have not been previously examined. Additionally, while [27] adopts a strategy of employing class-specific DQN agents, [43] investigates retraining class-specific DQNs on additional categories. However, this research focused solely on single object detection using class-specific DQN agents and refrained from following the experimentation listed in [43]."}, {"title": "F. Datasets", "content": "Object detection datasets are valuable resources for training and evaluating object detection models, comprising of ground truth bounding boxes. These datasets are also equipped with labels for multiple objects inside a single image. As discussed in Chapter II and emphasised in the prevalent literature, the Pascal VOC 2007 [45] and 2012 [46] datasets were chosen for training the agents, since they are extensively used for training RL-based detectors, and tend to offer comprehensive annotations. Furthermore, the class-specific DQNagents underwent training for 15 epochs on the mentioned dataset, analogous to [27], [34], [44]. Additionally, the Pascal VOC 2007 test dataset was also used to evaluate the effectiveness of the trained agents, allowing for a more standardised comparison with previous approaches [27], [44]. The COCO dataset [47], particularly its 2017 version, stands as a renowned benchmark for object detection tasks. In experiments related to saliency ranking, the COCO 2017 dataset was employed to further evaluate the performance of the approach and ensure its ability to generalise across diverse datasets and scenarios. The Pascal VOC dataset was obtained using the torchvision API, while the COCO dataset was acquired through the coco API."}, {"title": "V. EVALUATION", "content": "This chapter evaluates the techniques described in the methodology by establishing a set of object detection metrics for evaluating the trained agents. Additionally, it delves into four experiments, culminating in a comparative analysis. The first experiment centres on identifying the optimal threshold and number of iterations for the saliency ranking algorithm within the pipeline design. The second experiment assesses the impact of varying exploration modes and the integration of the configured SaRa process. The third experiment compares the effects of employing different feature learning architectures, whilst the final experiment examines the evaluation of training variant DQN agents. Finally, the chapter presents a"}, {"title": "A. Metrics", "content": "Metrics are quantifiable measures characterised by specific mathematical properties, which are employed to assess the performance of models. In this evaluation, adhering to the Pascal VOC object detection benchmark, the IoU threshold utilised was that of 0.5. As the current problem focuses on single object detection, when multiple ground truth bounding boxes for the same class in the image were provided, the IoU for the closest ground truth was calculated, following the approach in [43], [44]. Furthermore, the following metrics serve to evaluate the performance of the developed models:\nTrue Positives (TP), False Positives (FP), and False Negatives (FN) are essential components in the calculation of Precision and Recall. Precision, a metric that signifies the proportion of relevant items retrieved by the model, is formally defined in Equation 12.\n$Precision = \\frac{TP}{TP + FP}  \\frac{IoU(b, g) > threshold}{IoU(b, g) > threshold + FP}$ (12)\nRecall, represented in Equation 13, quantifies the effectiveness of retrieving relevant items by the model.\n$Recall = \\frac{TP}{TP + FN}  \\frac{IoU(b, g) > threshold}{IoU(b, g) > threshold + FN}$ (13)\nAP, as represented by Equation 14, calculates the cumulative precision for each recall value Rk in the range from 0 to n, where n signifies the total number of relevant items.\n$Average Precision (AP) = \\sum_{k=0}^{n}(R_k - R_{k-1}) \\cdot P_k$ (14)\nThe mAP, expressed in Equation 15, computes the average precision across all classes N by summing up the individual average precision values AP\u00bf and dividing by the total number of classes.\n$Mean Average Precision (mAP) = \\frac{1}{N}  \\sum_{i=1}^{N}AP_i$ (15)"}, {"title": "B. Experiment 1 - Optimal Threshold and Number of Iterations", "content": "The first experiment focused on determining the optimal threshold and number of iterations required to determine the saliency ranking ranks with the highest significance, for the initial bounding box generation. This experiment comprised two main components: the optimisation of the threshold, and the determination of the optimal number of iterations for this process. Each component was further divided into two distinct experiments, both of which were evaluated on a different dataset. Firstly, a comprehensive evaluation was conducted across various thresholds, ranging from 10% to 100% at 10% intervals, and using both the Pascal VOC and COCO datasets. Following this, the identified thresholds were applied to sizeable subsets of the Pascal VOC 2007+2012 training dataset (approximately 12,880 images) and the COCO 2017 training dataset (approximately 78,018 images), with a maximum subset limit of 1000 images per class, selected for the COCO dataset to mitigate class imbalance. In this experiment, the average IoU between the generated bounding box and the closest ground truth object bounding box was utilised as a metric to ascertain the optimal threshold.\nThe second component involved determining the optimal number of iterations for the initial bounding box prediction process. Similar to the threshold optimisation, this experiment was conducted on both datasets, varying the number of iterations from 1 to 4 for each dataset, whilst utilising the same evaluation criteria as above. The results, presented in Table II and detailed in Figures 11 and 13, revealed that a 30% threshold yields optimal results, achieving an average IoU of 0.1 on the COCO dataset and 0.3 on the Pascal VOC dataset, respectively. Moreover, a minimal discrepancy was observed in the optimal number of iterations, with the evaluation on the COCO dataset suggesting 2 iterations and 1 iteration on the Pascal VOC dataset. This disparity was understandable considering the larger dataset, image size and broader categories of the COCO dataset, as illustrated in Table II."}, {"title": "C. Experiment 2 - Exploration and Saliency Ranking", "content": "The second experiment involved evaluating the effects of various exploration modes outlined in Chapter IV. It explored the integration of the saliency ranking algorithm into the system pipeline following the optimal SaRa configuration determined in the previous experiment (Section V-B). Throughout this experiment, the feature learning architecture, specifically VGG16 chosen for its smaller state size, and the standard DQN architecture remained consistent.\nThe results, summarised in Table III, indicated a significant disparity between the mAP metric, favouring random exploration over guided exploration. Notably, the most effective configuration involved random exploration without the inclusion of saliency ranking in the initial bounding box generation. This outcome suggests that restricting the agent's observability to the extracted feature vector of the observed region may have played a role. Introducing saliency ranking, which reduces the primary observed region, might have confused the agent rather than the aiding in narrowing down the region of interest. Furthermore, additional tests were conducted to evaluate the impact of applying or disabling the initial SaRa process during inference on mAP. As shown in Table III, the results varied. In some instances, adding the SaRa prediction process to agents not trained with it improved mAP, while in other cases, it led to poorer performance. Similarly, disabling the process for trained agents resulted in inconsistent outcomes, even after multiple tests.\nMoreover, when examining the average reward across episodes during the training process, as illustrated in Figure 14, it is clearly evident that the reward trajectory differs between random exploration and guided exploration. In random exploration, the reward initially starts at a relatively low value and gradually increases as the agent transitions from exploration to exploitation. Conversely, in guided exploration, the reward begins at a high value and gradually decreases as the agent shifts towards exploitation. This phenomenon is attributed to the nature of guided exploration, which benefits from full observability in calculating the IoU from the ground truth bounding boxes. In contrast, when predicting the location of an object in an unseen image, such full observability is not available, thus explaining the decline in value when transitioning to exploitation. This observation can also be credited to the nature of guided exploration, which relies on selecting random actions with positive rewards. It is worth noting that both modes eventually converge to approximately the same reward level. Figure 14 also highlights that the application of the SaRa initial prediction results in greater variation in the training period, as evidenced by the wide fluctuation in the total reward. Additionally, in line with previous studies [27], [43], [44], it is observed that the agent transitions from exploration to exploitation approximately occurs after the third epoch for the current category. Furthermore, it is also apparent that guided exploration leads to a faster convergence rate compared to random exploration. This finding is consistent with its directed approach of exploration, as illustrated in detail in Figure 15. Moreover, the utilisation of SaRa leads to slower execution, attributed to its algorithmic structure featuring multiple iterative processes. Upon comparing the training time with previous studies [44], it is noted that there is a significant reduction in training time in the proposed approach. Whilst, [44] reports approximately three hours of training time for each category, the designed approach reduces this to less than an hour. This improvement can be attributed to the reduction in state size, enabling the designed models to be more lightweight and faster."}, {"title": "D. Experiment 3 - Feature Learning Architectures", "content": "The third experiment evaluates the effects of employing diverse feature learning architectures as outlined in Chapter IV. Building upon the insights gained from the preceding experiment in Section V-C, which identified the top two performing streams utilising random exploration, one incorporating saliency ranking and the other without it, this experiment further investigates their effectiveness across diverse feature learning architectures. Throughout this experimentation, the random exploration mode and the standard DQN architecture were retained.\nThe findings from this experiment, as depicted in Table IV, indicate that VGG16 performed the best among the chosen image feature extraction networks, followed by MobileNet and ResNet50. Moreover, while most results showed improvement compared to the previous experiment with higher mAP values, there appears to be a correlation between the state size and mAP. Smaller state sizes not only facilitates lighter-weight models but also achieve higher mAP results compared to larger state sizes, such as when using ResNet50. Additionally, while the use of the SaRa initial bounding box prediction process generally led to decreased mAP accuracy, it is worth noting that this was not observed when employing the MobileNet feature learning network. Interestingly, employing SaRa resulted in a higher mAP score than when it was disabled. Although this could be considered an anomaly, repeated testing yielded consistent results, suggesting a potential correlation in employing SaRa in similar state sizes that achieve comparable results.\nFurthermore, when analysing the average reward over episodes during the training phase, as depicted in Figure 16, it becomes apparent that all configurations exhibit similar rates of convergence. This observation aligns with the findings reported in the previous experiment in Section V-C, where the utilisation of SaRa led to increased fluctuations during training. It is evident that VGG16 demonstrates the most gradual and rapid convergence, while the MobileNet configuration experiences the least fluctuations during training, albeit achieving slightly slower convergence compared to VGG16. Proof of this is exhibited in Figure 16. Conversely, the ResNet50 configuration exhibits a more linear convergence pattern than the typical curved convergence observed in such systems. Once again, this observation can be directly linked to the state size, suggesting a correlation between convergence speed, type, and fluctuations. Smaller state sizes tend to result in faster and more gradual convergence, whereas larger state sizes lead to slower, linear convergence. Remarkably, an optimal convergence, characterised by minimal fluctuations and a balanced blend of gradual and rapid convergence, may be achieved with a state size between those of the VGG16 and MobileNet networks.\nFinally, it is also worth mentioning that there is a direct correlation between increasing the state size and the corresponding augmentation in execution time, as supported by Figure 17. However, it is intriguing to observe that this augmentation is relatively marginal, resulting in only a modest disparity. Furthermore, it is discernible that the discrepancy in time overhead between employing SaRa as an initial prediction or not, is substantially smaller compared to the impact of increasing the state size. Evidently, the utilisation of the ResNet50 feature learning network yields swifter execution times in comparison to employing VGG16 with the use of SaRa, as illustrated in Figure 17."}, {"title": "E. Experiment 4 - DQN Agent Architectures", "content": "The fourth experiment assessed the impacts of utilising various DQN architecture variants, as detailed in Chapter IV. Expanding on the findings from the previous experiments outlined in Sections V-C and V-D, which pinpointed the optimal environment configuration involving random exploration and the exclusion of the initial SaRa prediction, this experiment delves deeper into their efficacy across different DQN architectures. Throughout this experimentation, the consistent random exploration mode, feature learning network, and absence of SaRa, were maintained.\nThe results obtained from this experiment, as shown in Table V, highlight D3QN as the top-performing agent, achieving a mAP of 51.37. This marks a significant improvement over the modest 46.86 mAP obtained by the standard DQN. This enhancement can largely be attributed to the architecture of the D3QN model. Interestingly, while Dueling DQN also demonstrated better performance compared to the standard DQN, DDQN yielded inferior results. However, when these architectures were combined to form D3QN, the resulting performance surpassed that of Dueling DQN, contrary to the observed pattern between standard DQN and DDQN, where the mAP decreased. Additionally, upon analysing the average reward across episodes during the training phase, as illustrated in Figure 18, it is evident that all configurations demonstrate comparable rates of convergence. However, key differences between the convergence pattern of Dueling DQN akin to the standard stable DQN were recorded, as evidenced in Figure 18. Notably, the convergence pattern of Dueling DQN exhibits more pronounced fluctuations in average reward compared to"}]}